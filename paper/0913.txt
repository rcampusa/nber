                   NBER WORKING PAPER SERIES




                           PANEL GAlA




                        Gary   chamberlain




                     working Paper NO. 913




              NATIONAL BUREAU OF ECONOMIC RESEARCH
                    1050 Massachusetts Avenue
                       Cambridge MA 02138


                            June 1982




The research reported here is part of the llBR's research program
in Labor Studies.  Any opinions expressed are those ot the author
and not those ot the National Bureau ot Economic Research.
                                                              FIBER Working Paper 11913
                                                                    June 1982


                                            Panel     Data




                                               ABSTRACT



       We consider   linear    predictor definitions of noncausality or strict exoge—

neity and show that it        is restrictive    to assert that there exists a time—

invariant    latent variable c such that x is strictly exogenous conditional on c.

A restriction ot this sort is necessary to justity standard techniques tor

controlling tor unobserved individual eftects.                 There is a parallel analysis tor

multivariate probit models, but now the distributional assumption for the indi-

vidual eftects is restrictive.          This restriction can be avoided by using a con-

ditional likelihood analysis in a logit model.                  Some ot these ideas are

illustrated by estimating union wage eftects tor a sample ot Young Men in the

National Longitudinal     Survey.      The results indicate that the lags and leads
could have been generated just by an unobserved individual eftect, which gives
some    support tor analysis ot covariance—type estimates.                 These estimates indi-

cate a substantial omitted variable bias.                We also present estimates ot a model

ot temale labor torce participation, focusing on the relationship between par-

ticipation and fertility. Unlike the wage example, there is evidence against

conditional strict exogeneity;         if   we ignore this evidence, the probit           and logit
approaches    give conflicting results.




                                                  Gary       Chamberlain
                                                  Department ot Economics
                                                  University of Wisconsin
                                                  Madison,       Wisconsin    53706
                                                  (608)      262—7789
                                       PANEL DATA

                                    TABLE OF CONTENTS


1.   INTRODUCTION AND S1Th2(ARY                                         1


2.   SPECIFICATION      AND    IDENTIFICATION:      LINEAR   MODELS    12


     2.1 A Production Function Excnpie                                 12


     2.2 Fi-zed     Effects   cmd   Incidental Parczneters
     2.3   Rcmdan Effects cmd Specification Analysis                   15

     2.4 A Consumer Demand Fxriple                                     19


           2.4.a Certainty

            2.4. b Uncertainty

           2.4.c     Labor    SuppZy

     2.5 strict     Eogentity Conditional on a Latent Variable         23

     2.6 Lagged De&dent Variables                                      26


     2.7 Reeithsal     Covairteea: Heteroskedasticity cud Serial
           Correlation                                                 31

            2.7. a ifeteroekedasticity

            2.7.b    SeriaZ Correlation

3.   SPECIFICATION     AND     IDENTIFICATION:      NONLINEAR MODELS   32


     3.1   A Rcrzdom Effects Probit Model                              32

     3.2   A Fixed Effects Logit Modet: Conditional Likelihood         37

     3.3 Serial     Correlation cznd Lagged Dependent tTartables       43

     3.4 Duration Models                                               51
4.   INFERENCE                                                            55


     4.1 The Estimation of Linear Predictors                              56


     4.2 Inrposing•Restrictions: The Minimum Distance Estimator           58

     4.3 Simultaneous Equations:        A Ggneralization of   Two—coid
           Three—Stage    Least   Squares                                 63

     44 Asymptotic       Efficiency: A Comparison with the Quasi-
          Maximum Likelihood      Estimator                               71

     4.5 Z&ltivariate Probit Madel8                                       74


5.   EMPIRICAL    APPLICATIONS                                            78

     5.1 Linear Models: Union Wage Effects                                78


     5.2ATonline'Mode1s: Labor Force Participation                       a5

6.   CONCLUSION                                                           90

     APPENDIX                                                            102

     FOOTNOTES                                                           104

     REFERENCES                                                          109
                        1.       INTRODUCTION AND SUt(ARY


        The paper has tour parts:             the specification ot linear models; the

specification    ot    nonlinear models; statistical                   interence;       and empirical

applications.      The choice of topics is highly selective. We shall focus

on a tew problems and try to develop solutions in some detail.

        The discussion of linear models begins with the following specification:


(1.1)                    + ci •


(1.2)       E(uj)xji# .      .   .
                                     . , c)     = 0
                                                            (1=1,             N; t-1,         I)




For example, in a panel of farms observed over several years, suppose that
                                                      .th
      is a measure of the output of the 1— farm in the t— season,                                     is a

measured input that varies over time, c. is an unmeasured, fixed input

reflecting soil quality and other characteristics ot the tarm's location,

and         reflects unmeasured inputs that vary over time such as rainfall.

        Suppose that data is available on (x11, .             .   .    . X, i'      .         y)
for each of a large number of units, but                          is not observed. A cross—

section regression of                 on      will give a biased estimate of                   if c is

correlated with x, as we would expect it to be in the production function

example.     Furthermore, with a single cross section, there may be no internal

evidence ot this bias.               If T > 1, we can solve this problem given the

assumption is (1.2). The change in y satisfies


                   -                 - x11)            -
                                                            x11)

and the leaSt squares regression of                    —              on      —          provides a
                                                                                  Xj1
                                                                2




consistent       estimator of B(as HT +                         )    if the change in x has sufficient

variation.            A generalization of this estimator when T > 2 can be obtained

from a least squares regression with individual specific intercepts.

        The restriction in (1.2) is necessary for this result. For example,

consider        the        tollowing               autoregressive        specification:


                       =                   +            + uit



               E(ujtIYj,t...j Cj) =
                                                         0.

It   is clear that a regression of                                   —            on                —             will    not

provide a consistent estimator of 6, since                                            —       u.        is correlated with


, -l       —
               ', 2T           Hence it is not sufficient to asstune that


                                   c.)



Much    of our discussion wi U be directed at testing the stronger restriction
in   (1.2).

        Consider the (inimtn mean—square error) linear predictor of

conditional on                 .   .   .   .




(1.3)           E*(cjIxil                              XIT)   1I +          + .   .    .+

Given    the assumptions that variances are finite and that the distribution
f    (x1, •.               x, c) dcas not depend upon i, there are no
additional        restrictions in (1.3); it is simply notation for the linear

predictor.            Now consider the linear predictor ot                                     given x1, .    .   .   .




                                                                    +                          .+
               E*(yjtlxil,         .   .       .   .                      Xj]• + .        .
Form the TXT matrix Ti with it                    as the (t,s) element.                Then the restriction

in (1.2) implies that rt has a distinctive structure:


                 =          +
             U

where I is the I'<T identity matrix, £ is a TX1 vector of ones, and

An (X1,                          A test for this structure could usefully accompany
                     .,
                          A,).
estimators of S based on change regressions or on regressions with

individual       specific        intercepts.           Moreover,         this tormulation suggests an

alternative estimator for ,                     which is developed in the inference section.

        This test is an exogeneity                 test     and it is useful to relate it to

Granger (1969) and Sims (1972) causality.                                The novel teature is that we

are testing tor noncausality conditional on                              a latent variable.      Suppose

that ta).    is       the first period of the individual's (economic) life. Within

the linear predictor context, a Granger definition of "y does not cause

x conditional on a latent variable &' is



(1.9)                                                    Yu'                  c)
                  =                             ...,    x,     c1)
                                                                               (t1,Z,...).



A Sims definition is



             E*(yjtIxj x12, $
                                            .
                                                , c) E*(yj(xji, •              •
                                                                                   ,       c) (t1,2,...).
In tact, these two definitions imply identical restrictions on the

covariance       matrix of (x1          .       . •, X, y1           .   .•            The Sims form fits


directly     into the IT matrix framework and implies the following restrictions:

                      B +
                                                         4




where B is a bwer triangular matrix and I is a T><1 vector.

        We show how these nonlinear restrictions can be transformed into

linear restrictions on a standard simultaneous equations model. We show

also how a           A.' term can arise in an autoregressive model from the pro-

jection of an initial condition onto the x's.

         In Sectin 3 we use a mui.tivariate probit model to illustrate the new

issues that arise in models that are nonlinear in the variables. Consider

the      tollowing         specification:


                     = S                   +     + ult


                       1 if it 10,

                           0       otherwise         (i1, .   . .   . N; t1, .   .   . .




where,     condit Lanai On                     C.1, ' Xv C1, the          distribution ot      .   .   .   .u
is multivariate normal (J(O, ) with mean 9 and covariance matrix =

We    observe          .       .   .   .
                                                 y11,
                                                                       for a large number of individuals,

but we do not observe                              For example, in the reduced form of a labor

force participation model, y can indicate whether or not the

individual wc'rked during period t, x1 can be a measure ot the presence

of young children, and                            can capture unmeasured characteristics of the

individual that are stable at least over the sample period.                                 In the

certainty model of Heckman and MaCurdy (1980), c, is generated by the

single lite-time budget constraint.

         If we treat the                         as parameters to be estimated, then there is a

severe incidental parameter problem.                                The consistency of the maximum
likelihood estimator requires that T +                        , but       we want to do asymptotio

inference with N +                  for fixed T, which reflects the sample sizes in the

panel data sets we are most interested in.                              So we consider a random

effects estimator, which is based on the following specification for the

distribution ot c conditional on x:


(1.4) c =n +                        + . .       .+          + vi,


where the distribution of                      conditional on                                is N(O, 02).

This is similar to our specification in (1.3) for the linear model, but

there is an important difference;                         (1.3) was just notation for the linear

predictor,        whereas         (1.4)        embodies     substantive     restrictions.            We are

assuming that the regression function of c on the x's is linear and that

the residual variation is homaskedastic and normal.                                     Given these asst.nuptions,

our analysis runs parallel to the linear case.                              There is a matrix It of

multivariate probit coefficients which has the following structure:


             II
                   diag{,                      T }[S I + - '1,
where d±ag{cC1, . . . .                   is    a diagonal matrix of normalization factors

with       =            +                 We can impose these restrictions to obtain an

estimator of                which is consistent as N                      for fixed T.          We can also
                    t

test whether It in tact has this structure.

       A quite different treatment of the incidental parameter problem is

                                                                            a
possible with a logit functional form for
                                                                                1]x1,        ci) .   The sum

zT           provides a sufficient statistic for ci. Hence we can use the
 t=1y.it
distribution ot               .                      conditional   on       .   .   .    .
to obtain a conditional likelihood function that does not depend upon a..

Maximizing it with respect to                         provides an estimator that is consistent as

N -    '     for   fixed T, and the other standard properties                 for maximtn likelihood

bold    as    well.        The power of the procedure is that it places no restrictions

on the conditional distribution ot c given x.                          It is perhaps the closest

analog to the change regression in the linear model.                              A shortcoming is that

the residual covariance matrix is constrained to be equicorrelated. Just

as in the probit model, a key assumption is


                                                                      =
(1.5)                                  .   .    .   . X1, c)               1Ix Cj)t
and    we discuss how it can be tested.

        It is natural to ask whether (1.5) is testable without imposing                                       the
various functional form restrictions that underlie our tests in the probit

and logit cases.              First,        some definitions.      Suppose that t =           1    is the initial

period of the individual's (economic)                       life; an extension of Sims' condition

for x to be strictly exogenous is that                           is independent of x1, x2, II!

conditional           on       ...,        x.       An extension of Granger's condition for "y

does not cause x" is that                           is independent ot yl                          conditional on

                           Unlike the linear predictor case, now strict exogeneity is

weaker than            noncausality.            Noncausality requires that                be independent of

x41,. x2, ...
                             conditional on x1, •.., xcznd onY1               .   .   .   .              If   x

is strictly exogenous and in addition                           is independent of                 i..,

conditional on                  then we shall say that the relationship of x to y is

static.
      Then our question is whether it is restrictive to assert that there

exists a latent variable c such that the relationship of x to y is static

conditional on c.           We know that this is restrictive in the linear predictor

case, since the weaker condition that x be strictly exogenous conditional

on c is restrictive.           Untortunately,    there are no restrictions when we

replace zero partial correlation by conditional independence,                       it follows

that conditional strict exogeneity is restrictive only when combined with

specific       tunctional     torms — a     truly nonparaxaetric test cannot exist.

      Section 4 presents our framework for inference. Let 4 3

(1,        .    .   .
                        . x1, y1,   .   .
                                            y)   and assume that         is independent and

identically distributed (1.1.4.) for i              1,2            Let        be the vector

formed from the squares and cross—products of the elements in                               Our

framework is based on a simple observation:               the matrix II of linear

predictor coefficients is a function of                    if      is i.i.d. then so is

w.; hence our problem is to make inferences about a function of a popu-

lation mean under random sampling.              This is straighttorward and provides

an asymptotic distribution theory for least squares that does not require

a linear regression function or homosked.asticity.

      Stack the columns of II' into a vector 11 and let It a               hQ.i),   where

  r                 Then   the limiting distribution for least squares is normal

with covariance matrix

                                Bit'




      We impose restrictions on II by using a minimum distance estimator.

The restrictions can be expressed es 1i              g(e), where e is free to vary
                                                 8




within some set T.        Given the sample ntean w =                 w./N, we choose 9
                                                               E!1
to minimize the distance between w and g(6) , using the following distance

tunction:



             miii [
             OcT
                      - g(Q)]t   r'(w.)
                                     :1-
                                                 [-
where V(w.,) is a consistent estimator of V(w1)                   This is a generalized

least squares estimator for a multivariate regression model with nonlinear

restrictions on the parameters; the only explanatory variable is a constant

term.      The limiting distribution of                is normal with covarlance matrix


              r   ' —1
                     V
              LW               wrj

          An asymptotic distribution theory is also available when we use some
                         A_i
matrix other than V (w1) in the distance function.                     This theory shows

that V (wi) is the optimal choice.                    However, by using suboptimal norms,

we can place a number of commonly used estimators within this framework.

          The results on efficient estimation have some surprising consequences.

The simplest example is a .uni-variate linear predictor: E*(3rtx1,                           =
                                                                                       x2)
      +           + 1t2x2. Consider imposing the restriction that               = 0;     we do
ITQ

not want to maintain any other restrictions, such as linear regression,

homoskedasticity,       or normality. How shall we estimate it1? Let

F                   be the estimator obtained from the least sares regression

of y on x1, x2. We want to find a vector of the form e, 0)                     as close as
            A   A                          'I.


possible to (iT1, ff2) using V             ('IT) in the distance function. Since we

are not using the conventional estimator of V(), the answer to this
minization problem is not, in general, to set                b       the estimator

obtained from the least squares regression of y on x1. We can do better

by using        + TTr2; the asymptotic mean of          is zero if iT2 = 0,   and if

b     and     are correlated, then we can choose t to reduce the asymptotic

variance below that ot b
                         PC1
      This point has a direct counterpart in the estimation of simultaneous

equations.    The restrictions on the reduced form can be imposed using a

minimin distance estimator.     This is more efticient than conventional

estimators since it is using the optimal norm.            In addition, there are

generalizations of two— and three—stage least squares that achieve this

efticiency gain at lower computational cost.

     A related application is to the estimation of restricted ovariance

matrices.    Here the assumption to be relaxed is multivariate ni ina1ity.

We show that the conventional maximum likelihood estimator, whch assumes

normality,   is asymptotically equivalent to         a minimum distance estimator.

But that m±ninu     distance estimator is not. in general, using the optimal

norm. Hence there is a feasible minimt.un distance estimator that is as

least es good as the maximt likelihood estimator; it is strictly better

in general for nonnorinal distributions.

      The minimum distance approach has an application to the multivariate

probit model of Section 3. We begin by estimating T separate probit

specifications in which all leads and lags of z are included in the

specification for each


                                      =          +          + . .   .+
                                            'U




where F is the standard normal distribution function.                  Each of the T

probit specifications is estimated using a maximum likelihood program

for univariate probit analysis.             There is some sacrifice ot efticiency

here, but it may be outweighed by the advantage of avoiding numerical

integration.     Given the estimator for 11, we derive its asymptotic

covariance matrix and then impose and test restrictions by using the

minimum   distance      estimator.


       Section 5 presents two empirical applications, which implement the,

specifications discussed in Sections 2 and 3 using the inference procedures

from Section 4.       The linear example is based on the panel of Young Men

in the National Longitudinal Survey (Parties); y is the logarithm of the

individual's hourly wage and               includes variables to indicate whether or

not the individual's wage is set by collective bargaining; whether or not

he lives in an SHSA; and whether or not he lives in the South. We present

unrestricted least squares regressions ot           on     .   .   .   .   xT,   and we

examine the form of the It matrix. There are significant leads and lags,

but there is evidence in favor of a static relationship conditional on a

latent    variable; the leads and lags could be interpreted as just due to c,

with             .
                     ..,. xT,   a) =   @   + c.   The estimates of B that control for

c are smaller in absolute value than the cross—section estimates. The

union coefficient declines by 402, with somewhat larger declines for the

SNSA and region coefficients.

       The second application presents estimates of a model of labor force

participation.       It is based on a sample of married women in the Michigan

Panel Study ot Income Dynamics.            We focus on the relationship between
                                     11




participation and the presence ot young children.          The    unrestricted

fl   matrix for the prcibit specification has significant leads and lags;

but, unlike the wage example, there is evidence here that the leads and

lags are not generated just by a latent variable.          It we do impose this

restriction,    then the resulting estimator of        indicates that the cross—

section estimates overstate the negative effect of young children on the

woman's    participation   probability.

       The estimates for the logit functional form present some interesting

contrasts to the probit results.          The cross—section estimates, as usual,

are in close agreement with the probit estimates.          But when we use the

 conditional maximum likelihood estimator to control for c, the effect of

an additional young child on participation becomes substantially more

negative than in the cross—section estimates; so the estimated sign of

the bias is Opposite to that of the probit results.          Here the estimation

method is having a first order ettect on the results.            There are a variety

ot possible explanations.       It may be that the unrestricted distribution

 for c in the logit form is the key.         Or, since there is evidence against

the restriction that



                                XjT) c) flftk' c)

perhaps we are finding that imposing this restriction simply leads to

different biases in the probit and logit estimates.
                                          12




          2.     SPECIFICATION AND    IDENTIFICATION:     LINEAR MODELS


2.1. A Production Function £xcnple


         We    shall begin with a production function example, due to Mundlak

(1961)          Suppose that a farmer is producing a product with a Cobb-Douglas

technology:



                                          (Q<$<1;          11, ....Nt4

where
                                                   th
                is the logarithm of output on the i—         farm   in season t,      is
the logarithm ot a variable input (labor), c represents an input that is

fixed over time (soil quality), and u1 represents a stochastic input

(rainfall), which is not under the farmer's control. We shall assume that

the farmer knows the product price (P) and the input price (W), which do

not depend on his decisions, and that ha knows ci.                  The factor input deci-

sion,    however, is made before knowing                and we shall assume that Xii: is

chosen to maximize expected profits.               Then the tactor demand equation is


(2.1)             =     S   +bt   [E(a'tIJ)]   +Zet (P/W)      + c}/(1—),


where          is the infortiation set available to the farmer when he chooses
and we have suppressed the i          subscript.

        Assume first that          is independent of Jt, so that the farmer cannot

do better than using the unconditional mean.               In that case we have


                                               +
                              X,   c) =            C.
                                                  13




So it c is observed, only one period ot data is needed: the least squares

regression ot          on       c provides a consistent estimator of S as                    N +   cz,

     Now suppose that c is not observed by the econometrician, although it

is known to the farmer. Consider the least squares regression of                                   on

using just a single cross—section ot the data.                         The      population   counterpart

is



             E*(y11x1) a        +

where E* is the minimum mean—square error linear predictor (the wide—sense

regression        tunction)



             11
                  = Cov(y1,   x1)/V(x.Q, it                     -   it E(xi2.
                                                        E(y1)

We see from (2.1) that c and                      are correlated; hence 't               and the least

squares estimator of S does              not converge to B as N +            , Furthermore,             with

a single cross section, there may be no internal evidence of this omitted—

variable bias.

     Now the panel can help to solve this problem.                           Mundlak's solution was

to include farm specific indicator variables:                         a least squares regression

of      ott               (i")., .   .   . , N;   t-l               T), where        is a bP<L vector ot

zeros except for a one in the                      position.        So this solution treats the

c. as a set of parameters to be estimated.                      It is a 11tixed ettects"           solu-


tion, which we shall contrast with 11random ettects."                            The distinction is

that under a fixed effects approach, we condition on the                                 so that their

distribution plays no role. A random effects approach invokes a distri-

bution tor c.         In a Bayesian framework, S and the Cj would be treated sym-

metrically, with a prior distribution tor both.                         Since I am only going to




                                                                                                               J.
                                                 14




use asymptotic results on inference, however, a !lgentleu                             prior     distribution

for       will be dominated.             That this need not be true for the c. is one of
                                                                                           1

the interesting aspects ot our problem.

      We shall do asymptotic inference as N tends to infinity for fixed T.

Since the number of parameters (c.) is increasing with sample size, there

is a potential "incidental parameters" problem in the fixed effects approach.

This does not, however, pose a deep problem in our example.                               The least

squares regression with the indicator variables is algebraically equivalent

to the least squares regression of                     —       on          —    (id                N;


t1              fl,   where          =
                                         f;=1 y/T, x         =                   If   T = 2, this reduces

to a least squares regression of                       —         on xi2 —              Since
                                                                               x11.

                      -                               $(x2 —
                              Ix.   .,   — x1)

the   least squares regrt ision will provide a consistent stimator of 8 if
there is sufficient variation in                        —
                                                            x11.
                                                                    3




2.2 Fixed ffeats and                Incidental Parameters

       The incidental parameters can create real difficulties. Suppose that
      is    independently and identically distributed (i.i.cl.) across farms and

periods    with V(u.)      a              Then under        a normality assi.ption,            the maximum

likelihood    estimator of a2 converges (almost surely) to                                        as

N +       with T fixed.        The failure to correct for degrees of freedom leads

to a serious inconsistency when T is small.                             For another example,       consider

the    tollowing      autoregression:
                                                         15



             v      =av.10+c.1 +u.1
              'ii

                     =
             yi2                     + ci + u2.


Assume that tj           and u are i.i.d. conditional on y.                            and ci, and that they
                    ii               il

follow a normal distribution (N(O, a2)).                                  Consider the likelihood tunction

corresponding to the distribution of (17 y12) conditional on                                          and C1.

The log-likelihood function is quadratic ir ,                              cl            c (given         and

the maximum likelihood estimator of S is obtained from the least squares

regression of                —            on.        —         (i-i             N)    Since        is correlated
                                 y11
with          and


                     —           =                — y10) +            •
             3niz



it     is clear that



(2.9)
                         a                      — Y10)        'il Y0)
and the maximum likelihood estimator of                               is not con°istent. It the dis-

tribution of yio conditional on                           does not depend on S or               then the

likelihood function based on the distribution of (y±, yil, y12) condi-

tional on Cj gives the same inconsistent maximum likelihood estimator of

8. If the distribution of (yio, yj1, 2)                                    is   stationary, then the estimator

obtained from the least squares regression of                                   —      on      a

converges, as N +            , to         (e—1)I2.5



2.3. Rtdom           Effects wtd Specification Analysis


         We have seen that the success of the fixed effects estimator in the

production function example must be vieweu with some caution. The mci—
                                             16




dental parameter problem will be even more serious when we consider nonlinear

models.      So we shall consider next a random effects treatment of the pro-

duction tunction example; this will also provide a convenient framework

for specification analysis.6

         Assume that there is some joint distribution for (xil                             XiT c.),
which does not depend upon i, and consider the regression function that does

not condition S c:


             E(YjFx±i .   •   , xt)                 +                 •
                                                                          .. Xfl)d
The regression function for c1 given x.                       .   .   .          will    generally be

some nonlinear tunction.        But we can specify a minimum mean—square error

linear      predictor:



(2.2)                                           P + X1xi1 + eli +                    +
                                     xiT)


where X     = 111(x1) Cov(x ci). No restrictions are being imposed here —
 (2.2)    is simply giving our notation for the linear predictor.

         Now we have


             E*(yjjx) a 4' +                +


Combining these linear predictors for the T                periods gives the following
                                        8
mulcivariate       linear predictor:



(2.3)        E*(zlxj) a       + It

             II   Cov(, x!) v1(x1)                 I + LA',
                                                                17




where y =                  ••,                    I       is the lxi identity matrix, and Z is a T'<l

vector ot ones.

        The ri matrix is a useful tool for analyzing this model. Consider first

the     estimation   of 6:        it I =              2    we have


                                      /           +
                                                           A]_ X2

              irk)_                   t•\
                                                 x1

Hence


              =ir     11
                           -n =71
                                 21              22
                                                          —iT
                                                             12


So given a consistent estimator for TI, we can obtain a consistent estimator

tor 5.      The estimation of lE                  is       almost a standard problem in multivariate

regression; but, due to the nonlinearity in                                                we are estimating only

a wide—sense regression tunction, and some care is needed.                                         It turns out that

there     is a way of looking at the problem which allows a straightforward
treatment, under very weak assumptions. We shall develop this in the section
on    interence.

        We see in (2.3) that there are restrictions on the rt matrix. The off—

diagonal elements within the same co1 of if are all equal. The Th elements

of It are functions of the t+1 parameters '                                   .   .   .   . Xpe   This suggests an

obvious     specification test.                       Or, backing up a bit, we could begin with the

specification that TI =                     I.        Then passing to (2.3) would be a test for

whether there is a time—invariant omitted variable that is correlated with

the x's. The test of rt                      $    I       + £ A'     against an unrestricted Tt would be an

oimibus test of a variety of misspecifications, some of which will be con—

sidered next.
                                                  18



        Suppose that there is serial correlation in u, with u Pu1 +

where        is independent of            and we have suppressed the i subscripts.

Now we have

                  u           Pu.           w
                E(e t1J) =
                                    t1   E(e t)




 So the factor         demand equation becomes

                  = (Zn     + £n [E(et)1 +                       +      +
                                                   Lvi
                                                         (PIW)

 Suppose    that there is no variation in prices across the farms, so that the
           term is captured in period specific intercepts, which we shall suppress.
 Then we have


                                                  + (1_P')(Xix, + . .   .+          +


 where      =


 So   the if    matrix would indicate a distributed lead, even              after   controlling

 tar c.     If instead there is a first order moving average,
                                                                             U      W+
 then

                   a           Pw           w
                E(e tlJ)     a           E(e t)


 and a bit at algebra gives
                                                         19




                   E(YIxi,
                                                     -                 +   .   .   .+      +


Once again there is a distributed lead, but now                                     is not identified from

the it    matrix.

2.4.          .4 CordurneP        Demand Excarrple
2.4.a.        Certainty

              We       shall follow Ghez and Becker (1975),                    Heckzan and MaCurdy (1980),

and     MaCurdy (1981) in presenting a life—cycle model under certainty,                                    suppose

that the consumer is maximizing

                             t
                        =
                            t-1
subject to

                   ¶
                    z 1—(t—1)                 c B,            0 (tnt                I),
              1
where P            —    1    is the rate of time preference, Y—l is the (nominal) interest

rate,             is consumption         in   period t,            is the price of the consumption good

in period t, and B is the present value in the initial period ot litetime

income.           In this certainty model, the consumer                        faces a    single lifetime

budget        constraint.
         It   the optimal consumption is positive in every period, then


                   U(C) =         (YP)


A convenient functional foro is u(c) *
                                                                   Atc6/o
                                                                                   (A>O 6 < 1);   then we have


(2.4)                             + p(t—1) + c + Ut
                                           20




where yt =    Z   Ct,
                           = Zn Pt, C =    (—1)
                                                    1


     =   (1_)_1 Z At,      8 = (—1) -1, and        = (1_o)_1 Let (yp).    Note that   c is
determined by the marginal utility of initial wealth: u(C1)/P1 = 3V/DB.

         We shall assume that At is not observed by the econometrician, and that

it is independent ot the F's.            Then the model is similar to the production

function example if there is price variation across consumers as well as

over time.      There will generally be correlation between c and (x1
                                                                                         XT).

As before we have the prediction that IT = $ I + Z X', which is testable.

A consistent estimator of S can be obtained with only two periods of data

since


                           = $
(2.25)        y                  (x -   xci)   +    + Ut —


We   shall see next how these results are affected when we allow for some

uncertainty.



2.4.b.      Uncertainty


            We shall present a highly simplified model in order to obtain

some explicit results in the uncertainty case. The consumer is maximizing


               E( Z pt_lu(c)l
                  tel

 subject to


                    + S1     B


                        + .Y            C 0, St >       0 (tol       t)
                                                    21




The only source of uncertainty is the future prices. The consumer is

allowed to borrow against his tuture income, which has a present value

of B in the initial period.                 The consumption plan must have                   a function

only ot intormation available at date t.

        It is convenient to set t                    and to assume that                     is i.i.d.

(t=l,2, .      •.).       If   u(C)
                                      =
                                          Atc6/6
                                                     then we have the following optimal plan:
                                                                                                          10




(2.5)           C1 a d13/P1,          5a   (1—d1) B,

                      a JyS I1', S                (].—d)                    (t2,3,


where

                      =   [1 +        +                  + 1 —1
                f         (PK A/Aj) [1/(1-)                Ka


It    follows that


                      -
                                 = (—l)(x     —
                                                  xi) ÷         +       a


where    y,    x, u are        defined as in (2.) and               •                ZYL (PK)+ZflY.

        We    see that, in this particular example, the                       appropriate    interpretation

of the change regression is very sensitive to the amount of information

available to the consumer.                 In the uncertainty case, a regression ot

(In          — en Ci) an <Liz              — en P1) does not provide a co:.sistent esti-

mator of                       in fact, the estimator converges to -1, with the implied

estimator of 6 converging to 0.
                                                    22




2.a.c.     Labor Supply


           We    shall consider a certainty model in which the consumer is maxi—

mi z ing



(2.6)                   =
                       t=1             t    tt
                                           (C ,L )


subject    to



                E                (PC + WL)               c B +         1-(t-1)        w
                t=1                                              t-1



                C>O, O<L<L (t=1, ....t),

vhere Lt is leisure,              is the wage rate, B is the present value                      in   the

initial period of non.i.abor income, and L is the time endowment.                              We shall

assume that the inequality constraints on L are not binding; the parti-

cipation decision will be discussed in tbe section on nonlinear models.

If         is additively separable,


             1J(C L) = tJ(C) +


and if U(L) PsL6fd then we have

(2.7)                        + p(t—1) + C +


where                            Let        C   =                Lit
                 £YLL, x a
     a                Let    $                  and          (1-6)           Ln (YP).     Once again c is

determined by the marginal utility of initial wealth:                                            Vf9B.

         We shall assume that               is not observed by the econometrician. There

will generally be a correlation between c and                            .    .   .   .     since L1 depends
                                                               25



hypothesis in (2.8) implies that if T >                                          4,    there are (T—3)(T—2)/2 over—
identitying       restrictions.
         Consider next a Granger definition of "y does not cause x conditional

on C":


(2.10)                                        .   .                 .                                              .           .       .    . x,c)
                                                                                                 (t=1, .       .           .       .       I-fl.

Define    the tollowing linear predictors:



                                                           ttt, + til, +...+ ttt,                                          t+].                    t+1'


              E*(vt+i Jx1,
                                     ... x, Y1'            .    .       .    .         c)   0    (t=1,     .           .       .       . I-fl.

Then (2.10) is equivalent to p                             0.               We can rewrite the system, imposing
                                                      Cs
         0, as tollows:


(2.11)                                   +                          x
                                                                            t—1
                                                                                      +tx   +v

                 ts
                           =
                               ts — '    ¼
                                                           —
                                                                    ,s            t   n tt +
                           av
                               t+1
                                     —
                                             ( t+1 t t/ )v ,        E(x t+1                                    ao

                                                                            (SC t—l; t—2                 1—1)



 In the equation for                          there are t unknown parameters,
                                                                                                                                                          It'
 and 2(t—1)       orthogonality conditions. Hence there are t—2 restrictions
 (3 5.   t < I        1)


         It follows that the Granger condition for "y does not cause x con-

 ditional on a" implies (T—3)(T—2)/Z restrictions, which is the same number

 of restrictions implied by the Sims condition. In fact, it is a consequence

 of Sims' (1972) theorem, as extended by Rosoya (1977), that the two sets of
                                                             26


restrictions        are     equivalent; this is not immediately Obvious from a direct

comparison of                     and tt.IQL


         In tens of tie                   matrix, conditional strict exogeneity implies that


                   =      + x




                             : :22                   •c;i•        ...


                            ST1            T2        ''                     B11
                                                                                     xa(:),                     )
These nonlinear restrictions can be imposed and tested using the minimum

distance estimator to be developed in the inference section. Alternatively,

we can use the transformations                       in      (2.9) or in (2.11). These       transfor—
nations give us "siniultanecus equations"                                systems with linear restrictions;

(2.9)     can be estimated using three—stage least squares. A generalization of
three—stage least squares, which does not require homoskedasticity assumptions,
is developed in the interence section.                                  It is asymptotically equivalent to

imposing the nonlinear                    restrictions            directly on II, using the minimum distance

estimator.


 2.6.      Lagged Dependent Variables

           For     a specific example, write the labor supply model in (2.7)
 as     tollows:



(2.12)                            +             +                   +


                                      •          a0           (tnt,


 this     reduces to (2.7)                 if 62 :            and         = 1. If we aSStt   that   V   3 V +
                                         23




upon wages in all periods.         If At is independent of the W's, then we have

the prediction that 11 =        I +JX • If,     however, wages         are partly   deter-

mined by the quantity of previous work experience, then there will be lags

and leads in addition to those generated by c, and It will not have this

simple        structu.

        It would be usetul at this point to extend the uncertainty model to

incorporate uncertainty about tuture wages.        Unfortunately,         a   comparably

simple explicit solution is not available.        But we may conjecture that the

correct interpretation of a regression of (Zn
                                                  Lt
                                                       —   Lit           on (Zn     W
                                                                                        —    Lit

is also sensitive to the amount of Information available to the consumer.


2.5.    Strict   E'ogeneity   Conditionat on a Latent Variable

        We shall relate the specification analysis of It to the

causality definitions of Granger (1969) and Sims            (1972) .     Consider   a sample
                                                                                        12
in which t1 is the first period of the individual's (economic) life.                          A
Sims definition of         is strictly exogenous" is



                              ..) s.E*(y lxi       x)

In this case 11 is lower triangular: he elements above the main diagonal

are all zero.      This fails to hold in the nEdels we have 'baen considering,

due to the omitted variable c.        But, in some cases, we do have the following

property:



(2.8)         E*(yjx1,x21. ..,c)               . •.>        c) (t1,2,.. -)


        It   was stressed by Granger (1969) that the assessment of noncausality

depends crucially on what other variables are being conditioned on. The
                                                                              24




novel teature at (2.8)                               is that we are asking whether there exists some

latent variable                  (c)    such that x is strictly exogenous conditional on c.

The question is not vacuous since c is restricted to be time invariant.

        Let us examine what restrictions are implied by (2.8) .                                                              Define   the

                                                                    13
following linear predictors:


                         =                                                              i C +
                             t1        l +                      + tT XT
                                                                                    +




                   E*(uIxj,...,                     x, c) =               0             (t1,       ..,    T).

Then     (2.8) is equivalent to                                            0 for s >           t.         If           # 0, we can choose

a scale normalization for a such. thai_ c,=.. 1.                                                    Then we can rewrite the

system       with                      0 Cs         >   t) as follows:

(2.9)                    a                      +                   +              .+   $ +                        +



                             S              a
                   St1

                   E(x u) '             0       (s—i                      T; t—2, .       .    .   . T)

        Consider the                    "instrumental variable"                               orthogonality conditions implied

by E(xu) 0.                       In the                   equation, we have T+1 unknown coefficients:

               .    .    .   , 8,.                   and        T        orthogonality conditions.                       So these coetti—

cients are not identified.                                    In the                     equation, however, we have just

enough orthogonality conditions; and in the                                                               equation (j < T—Z),          we

have j—l more than we need since there are T—j+1 unknown coefficients:


       ,i'                   2' '           8T—j                        1T—j'
                                                                                    and T orthogonality conditions:


E(xsuT.)
                a       0 (s—i.                          T) .           It follows that, subject to a rank condition,
we can identify 8,                                      and               for 2 C s C t 5 T 1. In addition the —
                                               27



where w is uncorrelated with the x's and                     is i.i.d. and uncorrelated with
the x's and w, then we have the autoregressive, variance-components model of
Bales tra and Merlove (1966) 14 In keeping with our general approach, we shall
avoid placing restrictions on the serial correlation structure of v, our
inference procedures will be based ou the strict exogeneity condition that

                      x) = 0.
      We can fit      this   model into   the II matrix     framework by using recursive

substitution      to obtain the reduced form:

             yt 1Xj+                +
                                                    Ytc+ Ut,

                                        0,
             E*(u1x11. ..xQ

where


                             +
                                                    tt      1' 1t =

                  C
                       62X+ 633Tt    uv+            53   V1   +.   .     +
                                                                                    vi


                                               (1 <s<t4. t—i.                     T).



(We are assuming         that (2.12) holds for t >          1, but data on (x3, y) are

not     available.)

Hence this model satisfies the cond..tional strict exogeneity restrictions,


             ft   - B+

where    B is lower triangular. The I A term is generated by the projection
                                                                             15
of the initial condition (d2x +              63y)
                                                    on xl              XT.
        Estimation can proceed by using the minimt distance procedure to impose
the nonlinear restrictions on fl Alternatively, we can complete the system
                                                       28




in     (2.12) with

                   =            +    •   +         +


this is just notation tar the identity


                   =
                                               X)+ [y1
                                                                   —
                                                                                  II, X)]•
Then    we can apply the generalized three—stage least squares estimator to be

developed in the inference section.                          It achieves the same limiting distri-

bution at lower computational coat, since the restrictions in this form

are linear and can be imposed without requiring iterative optimization

techniques.

        Now consider a £cond order autoregression:



                       61't 52tl +                           +           + ti
                                         XT)
                                               0              (t1          T).

Recursive     substitution          gives


                       etix].   + .. +                  +
                                                              tli      1t29
                                                                              +


                                         x,) a o              (tel

where



              C1       52X + 53Y0         + ¼'—i'           C2 —


and there are nonlinear restrictions on the parameters. .                            The if matrix   has

the tollowing torm:
                                            29




                    +        +


where B is lower triangular,            =        '''         and E*(cjx) =   Xx   (j=1,2)

     This specification suggests a natural extension ot the conditional

strict exogeneity idea, with the conditioning set indexed by the number of

latent   variables.     We shall say that "x is strictly exogenous conditional

on c1, c2" if


           E*(yt1... x1 x,                             c1, ca)— E*(ytjxt, x_1, ...,   c1, c2).

We can also introduce a Granger version of this condition and generalize the

analysis in Section 2.5.


Sr'iat Correlation or Partial Adjustment?


     Griliches'     (1967)   considered the problem ot distinguishing between
                                                                 16
the following two models: a partial adjustment model,


(2.13)         a        +        + Vt


and a model with no structural lagged dependett.variab].e but with a

residual following a first order M.aricov process:


(2.14)
                   ext + Ut,


           Ut Pu_1 + a, e L.i.d.;

in both cases x is strictly exogenous:


           E*(vtlxi, .           .xT)E(ulxll              xQsO        Ct—i     T).
                                            30




In   the   serial correlation case, we have


                =       — P8x.i + Py1 +

as   Griliches observed, the least squares regression will have a distinctive

pattern ——    the coefficient on lagged x equals (as N + cc) minus the product

at the coetticients on current x and lagged y.

      I want to point out that this prediction does not rest on the serial

correlation structure of ii.         It is a direct implication at the assumption'

that u is   uncorralated with   x.



             E*(ytIx,   ,c_1, '—i $xt + E*(utIxt,                    x1, i:i
                                        a         +


                                        5
                                            sxt   + 'Pt   ut_i
                                                  —
                                                                 +

Here p         is simply notation for the linear predictor. In general u

not a first order process (E*(uIui,                uz) #         E*(utIut_i)), but this does

not attect our argument.

      Within the II matrix framework, the distinction between the two models

is that (2.14) implies a diagonal It matrix, with no distributed lag, whereas

the partial adjustment specification in (213) implies that 11 ' B                 + y A',

with a distributed lag in the lower triangular B matrix and a rank one set

of lags and leads in I A'

      We    can generalize the serial correlation model to allow for an individual

specific effect that may be correlated with x:
                                                     31




             yt =       + C + Ut,


                                   .         =0.
             E*(utlxi                  xT)

Now both the serial correlation and the partial adjustment models have a

rank one st of lags and leads in ri, but we can distinguish between them

because oniy the partial adjustment model has a distributed lag in the B

matrix.      So the absence of structural lagged dependent variables is signalled

by the following special case of conditional strict exogeneity:


                                                                   a),
                        •., X,1, c) —              E*(ytlxt,
In this case the relationship of x to y is "static" conditional on C. We

shall pursLe this distinction in nonlinear models in Section 3.3.


2.7.      Resi   aL Covaricmaes:             Heteroskedasticity and Serial Corretation


2.7. a.    ffeteroakedastioity


            If E(cIx1) # E*(aIx) then there will be heteroskedasticity,

since the residual will contain                                a                 Another source of

heteroskedasticity      is       random       coefticients:



                             +          +


                    s                         a 0,
                                  E(w)

                    a        +         + (w1xj +


If w is independent of x, then 11                     0 I + £.      A',   and our previous discussion,

is relevant for the estimation of .                       We   shall handle the heteroskedasticity
                                                32




problem in the inference section by allowing,                     —
                                                                      U

to     be an arbitrary function of


2.7.b.     Serial      Correlation
            It    may be ot interest to impose restrictions on the residual

covariances, such          as a variance—components structure together with an
autoregressive—moving           average scheme.         Consider the homoskedastic case in
which

                            —
                                    35j)Zj —

does     not depend upon x.           Then   the restrictions can be expressed as
                                    g's are            functions and S is an unrestricted
jk       jk9)' where the                       known


parameter        vector.   We shall discuss a minimum distance procedure for
imposing such restrictions in the interence section.

3.        SPECIFICATION      AND     IDENTIFICATION:     NONLINEAR MODELS

3.1.     A Rwzd,m Effects Probit Model

         Our treatment of individual effects carries over with some important
qualifications to nonlinear models.                  We shall illustrate with a labor force
participation        example.       If the upper bound on leisure is binding in (2.6)
then


                                >


where    m is the Lagrange multiplier corresponding to the lifetime budget
constraint       (the marginal utility of initial wealth) .           Let   y. a   1   if
individual i       works   in period t,                0 otherwise.   Let
                                                  33




              •L;i W =             + e1.
                          c1x.
              £i      =            +
                                         ez.

where        contains measured variables that predirt wages and tastes tar

leisure.     We shall simplify the notation by supposing that              consists of

a single variable.        Then           = 1 if



                            —    (t-1)   Lvi (yp) + en


                            +    (1-6) en      t +       —   > 0,




which we shall write as



(31)           S      + (t1) +             + u

        Now we need a distributional assumption for the .z's. We shall assume

that (u,           Ut) is independent of c and the x's, with a mu.ltivariate

normal distribution (N(O, E)).              So we have a probit model (suppressing the

i subscripts and period-specific intercepts):


              P(y =                                          +
                      11x1,


where F( ) is the standard normal distribution function and                   is the
 th
t— diagonal element of Z.

        Next we shall specity a distribution tar c conditional on x •      (x, ,   .




                                          +XTXT+V

where v is independent of the x's and has a normal distribution (N(O, a2)).
                                                              34



There is a very important difference in this step compared with the linear

case.        In the linear case it was not restrictive to decompose c into its                                              P


linear projection on x and an orthogonal residual. Now, however, we are

assuming that the regression function E(cIx) is actually linear, that V is

 independent of x, and that v has a normal distribution.                                               These are restric-

 tive assumptions and there may be a payoft to relaxing them.

           Given these assumptions, the distribution for                                          conditional on

       •            but marginal on c also has a probit form:


                    P(Y a 11x1,        .   .   .    . x)       F[cLt(6x+     A1x +        .   .   .+

                         =
                             (at   +



Combining these T specifications gives the followthg matrix 0

coefficients: S

(3.2)          fl:diag{ct1, . .                . .ctT)I$IT+tXI.
This       differs from the linear case only in the diagonal matrix of normalization

factors
             OLt.
                     There are         now nonlinear restrictions on It, but the identi-
fication analysis is still straightforward.                                We      have


               c1t
                         a
                                  a11 t1                      -




               $ a tt             +                      +                   (t2




if + A1 +           A,       0.    Then,       as   in       the   linear case, we can solve
for          and a3A. Only             ratios        of coefficients are identified, and so we
can use a       scale        normalization such as                      1.
                                             35




        As   for   inference, a computationally simple approach is to estimate I

cross—sectional probit specifications by maximum likelihood, where                                ..,
                                                                                            x1,
are included in each ot the I specifications.                     This gives    (t=1              T)

and we can use       a Taylor expansion to derive the covarlance matrix of the

asymptotic normal distribution tor                                      Then restrictions can be

imposed on 11       using a minimum distance estimator, just as in the linear case.
        We shall conclude our discussion of this model by considering the

interpretation ot the coefticients.              We began with the probit specification

that




              P(y      i]xj           XT, c) -   Ft      (3x + c)1.


So     one might argue that the correct measure of the effect of                       is based

on        ,   whereas we have obtained             + a)½S,             which is then an under-

estimate.      But there is something curious about this argument, since the

!!omitted variable!! v is independent ot             .   .    .   .       Suppose that we

decompose          in (3.1 )   into     +         and that measurements on                  become

available.         Then this argument implies that the correct measure of the

effect of x is based on [V(uz) 4             . As            the data collection becomes

increasingly         successtul. there is less and lass variance left in the

residual             and [V(uz)       becomes arbitrarily large.

        The resolution of this puzzle is that the effect of x depends upon

the value of a, and the effect evaluated at the average value for c is not

equal to the average ot the eftects, averaging over the distribution tor c.

Consider the effect on the probability that                           1 of increasing x     from x'
to x"; using the average value for c gives
                                                       36




              F[a (ext' + E(c)) I             —             (3x'   + £(c)) 1

The     problem       with this measure is that it may be relevant for Only a small
traction     ot the population.              I think that a more appropriate                  measure    is
the 'ean effect for a randomly drain individual:

              f[P(yr              1Ix : X'1, a) —   P(y            1x a ••, c)]p(dc),

where    u(dc) gives the population probability measure for c.
        We shall see how to recover this measure within our framework.                                   Let
z X1x1 + .        .    .+            let p (dz) and A(dv)           give    the population probability
measures for the independent random variables z                            and    v.   Then



                        —ijx.,       c) a         — Liz1,          ••        c)

                        c           a iIx,    z, v);

            JP(y • 1 x, z, v).U(dz)U(dv)


                                        z, v)P(dv[xz)u(dz)


                                   t1x, z)J(dz),
             a



where
     1J(dvIxi z) is the conditional probability measure, which equals the
unconditional measure since V is independent of  and Z. (it is important
to note that the last integral does not, in general, equal                                     a   1
                                                                                                       I1).
For   if     end z          are    correlated, as   they      are in our case, then

             p(y        a lixe)                   1.Ix,      z)1.L(dzIx)

                       fP(y a               z)u(dz).)
                                                37




            We have shown that


 (3.3)            ! [P(y      = lx = :Ct,    c) -
                                                        P(y
                                                                =         = x', c)]p(dc)


                      = I {P(y 1x f, z)                     —                   =   x',   z)1p(dz).
                                                                        1Fx

The integration with respect to the marginal distribution for z can be

done using the empirical distribution fi.thction, which gives the following

consistent         (as N +cc) estimator of (3j•q,


 (3.4)                2                        +    .   . .+
                           {F[c(6x"+ X1x1

                          —            +       +. ..+

3.2 A Fixed Effects Logit Model: Conditional Likelihood

            'A weakness in the probit model was the specification                         of a distribution
tor         c conditional on x.       A convenient form was chosen, but it was only an

approximation, perhaps a poor one.                      We shall discuss a technique that does

not require us to specify a particular distribution for C                                 conditional   on

x;          it will, however, have its own weaknesses.

            Consider the tollowing specification:


  (3.5)
                              1x1            Xr a)
                                                        -           +   a),   G(z) a e5ci +

where                         are independent conditional on                                 c.
        y1

Suppose        that TaZ and compute the probability that                             1 conditional, on
        +      2 1:

                                               +            .1 1) =                 —
 (3.6)           P(y2 =       1x1   x2, C,
                                                     38




whicii does not depend upon c.                 Given a random sample of individuals, the

conditional       log-likelihood          tunction        is



              L =                  Zn            -               +                             I),
                                                                      (l-w) Ztt C(—$(x.2—x1)


where

                              1 if (y11, y12) =           (0,   1)
                    =
               1
                          (             y1x, i2 (1, 0) ,

              B          {iy.1 +

         This conditional likelihood function does not depend upon the

incidental        parameters.           It is in the form of a binary logit likelihood

function in which the two outcomes are (0.1) and (1,0) with e 1anatory

variables           —           This     is the analog of differencing in the wo period

linear     model.        The conditional tnaxizntun likelihood (ML) estimata of B

can be obtained simply from a ML binary loglt program.                             This   conditional

likelihood approach was used by Rasch (1960, 1961) in his model for

intelligence            tests.20


        The conditional ML estimator of                        is consistent provided that the

conditional likelihood function                  satisfies           regularity conditions,    which

impose mild restrictions on the c. These restrictions, which are

satisfied if the i are a random sample from some distribution, are dis-

cussed in Andersen (1970).                  Furthermore, the inverse of the information'

matrix based on the conditional likelihood function provides a covariance

matrix    for the asymptotic (N + cQ) normal                         distribution of the conditional

ML estimator of B.

        These results should be contrasted with the jnconsistenc'? of che

standard fixed effects ft estinator, in which the likelihood tunction is
                                             39




based on the distribution of                            conditional on X1              xT, c.

For example, suppose that T = 2,               = 0,          = 1 (i11 . . .    . N).     The

following    limits   exist with probability       one if the C1         are a random sample

tram some distribution:


              urn                              =



              urn        E                          =



where


                                     a                   a
              E[y1(1 e


              E[(1—y.1) y21ci a                         + ci).


Andersen (1973, p 66) shows that the                      estimator of      converges with

probability one to 2 as N                •     A   simple extension at his argument shows

that if G is replaced by any distribution function (C) corresponding to a

symmetric,      continuous, nonzero probability density, then the ML estimator of

   converges with probability one to



                             2


The logit case is special in that                       a a8 for any distribution for c.         In
general the limit depends on this distribution; but if all of the                           • 0, then

once again we obtain convergence to 26 as N                    OO•



        For general T, conditioning on Zy (is].                           N) gives the following

conditional       log-likelihood    tunction:
                                                   40



                                                                         T
               L =         tit [arp(3 E                           exp(jS E            I,
                      i1             t=1                  deB,          Ni


                              (d1.                                           I    I
               B. =    {d =
                                               dT)Jdt       0    or 1 and Edt     1'
                                                                         t=i     t=1



L   is   in the conditional logit          form considered by McFadden (1974), with                     the

         set (B1) varying across the observations. Hence it can be
alternative


maximized by standard programs. There are T+]. distinct alternative sets
corresponding         to           0,1,    ..,,    T.    Groups for     which              :   0 or T
contribute zero to L, however, and so only I—i alternative sets are relevant.
The alternative set for the group with                            = s   has (    ) elements,
corresponding to the distinct sequences of T tr als with s successes. For
example,      with :=3 and sal there are three alter ati.ves with the following
conditional p:obabjljtjes:

                P(1,O,01x1, Cj       E
                                          it       1)     exp[B(x1


                P(O1OIx         c., Z              1)     exp(B(x2 —


                                Cj Et
                                               •    1)     lID,


                D =
                       ex1E(x1—x.3)       i+   exp [(x2—x13) ] +, 1.
                                                  41




        A weakness in this apprOach is that it relies on the assumption that

the Yt are independent conditional on x, c, with an identical torm tor the

conditional probability each period:                   P(y =     1x,    c)             + c)

In the probit framework, these assumptions translate into                                         so

that V + u generates an equicorrelated matrix: a2                              £   + a21. We have
seen that it is straightforward to allow E to be unrestricted in the probit

framework; that is not true here.

        An additional weakness is that we are limited in the sorts ot probability

statements that can be made.            We obtain a clean estimate ot the eftect ot

   on the log odds:



                 I P(yta1xtxh1, c) /P(y slix                nx',
                 L"° kc—x" c)/ P(ytfbkttxl, C)j1 s" -
                                                                       c) 1
           Lit   I                       .1-                                            x



the special feature of the logistic functional form is that this functiOn
of the probabilities does not depend upon c; so the problem of integrating
over the marginal distribution of c (instead of the conditional distribution
of a given x) does not arise.         But this is not the only function of the
probabilities    that one might     want to know.  In the probit section we
considered


             P(Yt     lx = C, a)         —
                                              P(Y       11x a x',        c),



which   depends upon c    for   probit       or   logit,   and    we averaged over          the   marginal
distribution tor c:

(37)         •r[P(y   s         a x",   c) —
                                                           1x          x', c)]ii(dc).
                                                          42




 This requires us to specify a marginal distribution for c, which is what

 the conditioning argument trys to avoid.                             We cannot estimate (3.7) if all

we have is the conditional 1 estimate of 3.

         Our specification in (3.5) asserts that                               is independent of


                 x1, x1, .           .   .   .       conditional      on      c.     This can be relaxed

 somewhat,       but the conditional likelihood argument certainly requires more
 than


              P(y          JIx, c) G($x + c);

 to     see this, try to derive (3.6) with x2                              y1, We can, however, implement

 the following specification (with                         :
                                                               (x1

 (3.8)        P(y =        lix, c)           G(8 +                   + •. +               +


where                        are independent conditional on x, c.                             This    corresponds

 to our specification of "x is strictly exogenous conditional on c" in

Section 2.5, except that                         1 in the term Yc --                it   is   not straightforward

 to allow a time—varying coefficient on C in the conditional likelihood
approach.        The    extension        of (3.6) is

 (3.9)            a    1x,   C,          +       •   1)              +          + 8t2x2       + . .   .+

                                                                                T)

where        a         —
                                  (jnO,1)            So    it   x    has sutticient variation,
we    can obtain consistent estimates of                                      and         (s2              t).
                                                      43




Only these parameters are identified, since we can transtorm the model

replacing c by c                       + 8. x1 + c without violating any restrictions.


       The restrictions in (3.5) or in (3.8) can be tested against the

 tollowing              alternative:



 (3.10)           Ny,       1x, c) =    tO +               + .   .   .+   + c).


We    can identify only lt —                     and so we can normalize                  ..,
          •
                    'F)      The maximized values ot the conditional log likelihoods can
                                                 21
be used to form                   statistics.          There are (T—2)(T—1)/2 restrictions in

 passing from (3.10) to (3.8), and (3.5) imposes an additional

 (T—1)(T-1-4)/2 —            1   restrictions.




 3.3. Serial Correlation               And Lugged Dependent Va'iahles

       Consider the tollowing two models:

                             1           a
                                                 1
                                                            >
 (3.lla) y          t        (0 otherwise


                             (1 if y*.u >0
 (.           )
                        t    ( 0 otherwise;            pu1 +
                                            44




in both cases et is i.i.d. N(O, 2) Hecknan (1978) observed that we
                                                    22
can distinguish between these two models.                 In the first model,



                  P(y It1 't-2' • •• = P(y                     1tyi) = F(1Y
                                                           =
                                                                              i/cr),



where p(      )   is the standard normal distribution function.          In the second

model, however,               =
                                  1't1' —V ..)           depends upon the entire history

ot the process.           If we observed         then previous outcomes would be

irrelevant.        In fact, we observe only whether               > 0; hence conditioning

in addition on whether                 > 0 affects the distribution of           and

So the lagged y implies a Markov chain whereas the Markov assumption for

the probit residual does not imply a Markov chain for the binary sequence

that it generates.

     There is an analogy with the following linear models:


(3.12a)              a            +


(3.lZb)                  a
                             u, Ut    a +

where at is i.i.d. N(O, a2). We know that if Ut a                      +      then no

distinction would be possible, without introducing more structure, since

both models imply a linear Markov process. With the moVing

average    residual,         however, the serial correlation model implies that the

entire past history is relevant for predicting y. So the distinction

between the two models rests on the order of the dependence on previous

realizations of

      We can still distinguish between the two models even when (u1, .             .   .   .
                                                                                               UT)

has a general wultivarlate normal distribution (N(P, Z)). Given nor—
                                                               45




malizations such as V(u) = 1                        (e—1                    T), the serial correlation model

has T(T+1)/2 free parameters.                          Hence if T > 3, there are restrictions on
         •1
the 2         — 1 parameters of the multinomial distribution for (y,


In particular, the most general multivariate probit model cannot generate

a Markov chain.              So we can add a lagged dependent variable and

identity Y.

         This result relies heavily on the restrictive nature of the multi—

variate        probit functional                  form. A more robust distinction between the two

models        is   possible when there is variation over time in x.. We shall

pursue        this after first presenting a generalization ot strict exogeneity and
noncausality         tor nonlinear models.

         Let t=1 be the first period f the individual's (economic) life. An

extension of Granger' s                       definition   .   f    "y does not cause x"                 is   that         is


independent ot           .    .       .   .       condituna1          on            .   .   .    . x.   An extension of Sims'

strict exogeueity condition is thst                                  is independent of
                                                                                                              x2,
conditional on                    .   . ., x.        In contrast to the linear predictor case,
                                                                                            23
these two definitions are no longer equivalent.                                                    For consider the tollowing

counterexample: let y1, y2 be independent Bernoulli random variables with

fly           1) a           z —1)               1/2 (t1,2). Let x3                             71y2. Then      is indepen-

dent of x3 and                is independent if x3. Let all of the other random

variables be degenerate (equal to zero,                                    say) .           Then    x is strictly exogenous

but x3 is clearly not independent of yy y2 conditional on xl, x2. The

counterexample works for the following reason:                                                  it a random variable is

uncorrelated          with each of two other random variables, then it is                                            Un—

correlated with every linear combination of them; but If it is independent

of each of the other random variables, it need not be independent of every

tunction ot them.
                                                           46



       Consider the following modification of Sims' condition:

independent of x1, x2, ,,. conditional                                     on              •..,   x,   '1' •'
(t=1,2, .        ..) .     Chamberlain            (1982)     shows that, subject to a regularity

condition,         this is equivalent to our extended definition of Granger non—

causality.           The regularity condition is trivially satisfied whenever

has a degenerate distribution prior to some point.                                           So it is satisfied in

our case since y,                    .    .   .    have      degenerate             distributions.

       It is straightforward                        to     introduce a time—invariant latent variable

into   these definitions. We shall say that "rj                                     does   not caLae z conditional
an a latent variablec" if either
                         is independent of                      .
                                                                    , y, conditional on
                     ...,        C             (t=l,2,. .       .1,

or
                    is     independent OfC÷1 z2,                                •   conditional on
                     4•,     x   1'                             a


they   are equivalent. We shall say that "z is strictly exogenous conditional
an a Latent variable &'                   if


                   is    independent of                         x23. .      . conditional on
                     ..,         c       (t=l,2,


       Now       let us return to the problem of distinguishing between serial

correlation and structural lagged dependent variables. Assume throughout

the discussion that                      and             are not independent.                We shall say that the

relationship of x to y is static if

             x is strictly exogenous and                            is   independent         of        .   . .,
             conditional on x.
                                             47




Then    i   propose the following distinctions:




            There     is residual seria7 correlation i         f        is   not independent
               f                     conditzonal on        .   ..
            If       the relationship     to y is static, then there are no
                                            of
            structural lagged dependent variables.

        Suppose that y and              are binary arid consider the probability that

      = 1                                     (0, 0) and       conditional on (xl, x2)
y2        conditional on (x_ x2)                                                                 (1, 0)

Since          and         are   assumed to be dependent, the distribution of               is


generally      difterenit in the two cases.           If        has a structural effect on

then the conditional probability of                    1 should differ in the two cases,

so that            is not independent of          conditional on

        Note;that         this   condition is one-sided:           am   only offering a condition
for    there to be no structural effect of     on y. There can be distri-
buted lag relationships in which we would not want to say that    has a
structural effect on y. Consider the production unictioni example with
serial correlation in rainfall; assume for the moment that there                       is   no

variation      In    c.    If the serial correlation in rainfall is not incorporated

in the farmer's iniormationi set, then our definitions assert that there is

residual serial correlation but no structural lagged dependent variables,

since    the    relationship o x to y is static.               Now suppose that the farmer

does    ue      previous     rainfall to predict future rainfall. Then the relation-
ship    of x to y is not static since x is not strictly exogenous.                      But we

my not want to say that the relationship between                             and    is struc-

tural, since the technology does not depend upon Y1.
                                         48




        How are these distinctions affected by latent variables? It should

be clear that a time—invariant latent variable can produce residual

serial         correlation. A major theme of the paper has been that such a

latent variable can also produce a failure of strict exogeneity. So

consider conditional versions           f these properties:


            There is residual seriaZ. correlation conditional on a
            latent variable c     if is not independent of     ...,       y1.,
            conditional   on                  C;


            Th  relationship of : to y is atatvc conditional on a latent
            va'iahte a if    is strCctly exogenous conditional on a and
            if i.   independent        of x,
                                          ..,      conditional on    C;


          If the relationship of a to y is           static    conditional on a
        latent variable C, the there are             no structural lagged
            dependent txzriabiee.


        A      surprising feature of te linear predictor definition of strict
axogeneity        is that it is restrictive to assert that there exists some.

time—invariant latent variable c such that x is strictly exogenous

conditional on c.         This is no longer true when we use conditional

independence to define strict exogeneity. For a counterexample,                          suppose

that            is a binary variable and consider the conditional strict exogeneity

question,        "Does there exist a time—invariant random variable C               such that

      is independent ot                 xl conditional on         .   .   .   .   c?"   The

                                                      T
answer is         yes since we can order the 2.. possible outcomes of the binary
                                                          th                                       T
sequence (x1, . . .     . x,)   and set cj if the j— outcome occurs (31, ..., 2                    ).

Now         is independent of          ...,        conditional on c!

         For    a rtondagenerate counterexample, let y and x be binary random
variables        with
                                                              49


                                                                                 2
                     P(y           CL4,
                                             x             tik
                                                                 >     1              Tik = 1,
                                                                              j,k=1

                                       = 0.
where               =    1,   CL2
                                                  Let 1' =
                                                              (Ill,        tl2 T21 t22). Then
                                                                                              we can set


                     a
                     -
                               4
                               E
                              m1.
                                        ye,y
                                         ui-tn m
                                                 >0, Z
                                                                   4
                                                                           'y =1,


where e is a vector of zeros except for a one in the m- component.

Hence I is in the interior of the convex hull of                                         {e,   m1, •    . . , 4).    Now

consider            the vector



                                                     6(1—A)

                                                     (1—6)X

                                                     (1—6) (1—X)




The components f y(6,X) give the probabilities P(y —                                                x   =
                                                                                                            CLk)
                                                                                                                   when

y and x are independent with fly = 1) a 6, P(x a 1) a A.                                            Set aX
                                                                                                            -Ui

     in
           A
               m)
                  with 0<6 <1,0< A<1. mThen 4y will
                                                 —   be in the interior

of   the       convex hull of {e, ml,...,4} if we choose ç
                                                           Am
                                                              so that

is   sufficiently                   close to a • Hence
                                                      Ui


                                4                                      4
                        Ta         E      y*e*,y*>O, Z
                               rn1


Let the coñponents of e*.be (tT1,                                  t2, t1, tfl). Let             c be a random
variable with P(c a in) — i (mel, •                                  . .   , 4), and set

                        fly            CL,
                                              X               m) -
                                                                           tik
                                     50




Now y is independent of x conditional on c, and the conditional distri-

butions are nondegenerate.

         it (xi,      X, 1'               has a general multinomial distribution,

then a straightforward extension of this argument shows that there exists

a random variable c such that                    is independent ot (x1
                                                                                        XT)
conditional on c, and the conditional distributions are nondegenerate.

     A similar point applies to tactor analysis.              Consider a linear one—

tactor model.      The specification is that there exists a latent variable c

such that the partial correlations between            .   .   .   .    are zero given c.

This is restrictive if T ) 3.      But we now know that it is not restrictive

to assert that there exists a latent variable c such that                            are

independent conditional on c.

         It follows that we cannot test for conditional strict exogeneity

without imposing functional form restrictions; nor can we test for a

conditionally static relationship without restricting the functional

torms.


         This point is intimately related to the fundamental difficulties

created by incidental parameters in nonlinear models.                 The labor force

participation example is assumed to be static conditional on c. We

shall present some tests of this in Section 5, but we shall be jointly

testing that proposition and the functional forms — a                  truly nonparametric

test cannot exist.      We stressed in the probit model that the specification

for the distribution of c conditional on x is restrictive; we avoided

such a restrictive specification in the logit model but only by imposing

a restrictive functional form on the distribution of y conditional on x, c.
                                        51




3 . 4.    Evraticn   Models
          In many problems the basic data is the amount of tine spent in a

state.      For example, a complete description ot an individual's labor

torce participation history is the duration ot the first spell ot parti-

cipation and the date it began, the duration ot the tollowing spell ot non-

participation, and so on.        This complete history will generate a binary

sequence when it is cut up into fixed length periods, but these periods

may have little to do with the underlying process. 24

         In particular, the measurement ot serial correlation depends upon the

period ot observation.        As the period becomes shorter, the probability that

a person who worked last period will work this period approaches one. So

finding significant serial correlation may say very little about the under-

lying process.       Or consider a spell that begins near the end of—a period; then

it is likely to overlap into the next period, so that previous employment

raises the probability ot current employment.

         Consider the underlying process ot time spent in one state tollowed by

time spent in the other state.        It the individual's history does not help

to predict his future given his current state, then this is a Markov process.

Whereas serial independence in continuous time has the absurd Implication that

mean duration ot a spell is zero, the Marlcov property does provide a fruitful

starting point.        It has two Implications:   the individual's history

prior to the current spell should not attect the distribution ot the

length of the current spell; and the amount of time spent in the

current state should not attect the distribution ot remaining time

in that state.

         So the first requirement of the Markov property is that durations

ot the spells be independent ot each other.        Assuming stationarity, this
                                            52



implies     an alternating renewal process.           The second requirement is that the

distribution ot duration be exponential, so that we have an alternating Poisson

process.      We   shall   refer to departures from this model as duration dependence.

       A test   of this Markov      property     using binary sequences will depend upon
what    sampling scheme is being used.           The simplest case is point sampling,

where each period we determine the individual's state at a particular point

in time, such as July 1 ot each year.              Then it an individual is tollowing an

alternating     Poisson    process, her history prior to that point is irrelevant

in predicting her state at the next interview.               So the binary sequence

generated by point sampling should be a Marlwv chain.

        It is possible to test this in a fixed ettects model that allows each

individual to have her own two exponential rate parameters (Ca, c12) in

the alternating Poisson process.             The idea is related to the conditional

likelihood approach in the fixed effects logit model. Let $                    be the
                                                                         ijk
number of times that individual I is observed making a transition from state


j   to state k Ci, k =     1,2).       Then the initial state and these four transition

counts are sufficient statistics for the Markov chain.               Sequences with the

same initial state and the same transition counts should be equally likely.

This is     the Markov form of de Finettl's           (1975) partial exchangeability.25

So we can test whether the Markov property holds conditional on C11, C12

by testing whether there is significant variation in the sample frequencies

of sequences with the same transition counts.

       this analysis is relevant if, for example, each year the survey question

is "Did you have a job on July 1?"               In the Michigan Panel Study of Income

Dynamics,    however, the most commonly used question 'for generating parti-
cipation sequences is "Did your wife do any work for money last year?"                  This

interval    sap1ing.   leads to    a   more complex    analysis, since even If the

individual is follOwing an alternating Poisson process, the binary sequence
                                                    53




generated by this sampling scheme is not a Markov chain,                             suppose that

        = 1, so that we know that the individual worked at some point during the

previous period.             What    is   relevant, however, is the individualTs state at

the     end of the period, and                 will affect the probability that the spell
of work occurred early in period t—l instead of late in the period.
        Nevertheless,         it is possible to test whether the underlying process is
alternating         Poisson.        The reason is that if                : 0, we know that the
individual never worked during period t—l, and so we know the state at the
end of that period; hence Yt_2t 7t—3'                        .   . are irrelevant.     So we have


                           lila1,   C2,   7t-1' t—2'

                           NY, =                         =
                                    ljc1, C2                       = 7t—d       t—d—1         0)


                      z         = 1c1,      C2, d),


where    d is the number of consecutive preceding periods that the individual
was jfl state 1.
        Let S01 be the number of times in the sequence that 1 is preceded by
0; let S011 be the number of times that 1 is preceded by 0, 1; etc. Then
sufficient statistics are                                •.., as well as the number of
                                                                                                      26
consecutive        ones at the beginning (n1) and at the end                         of a sequence.

For an example with T =              5;   let Ti1     0, fl5 a     0, S01    1, S011   = 1,
         =.   .   . • 0;    then we have
S0111

              P(O, 1, 1, 0, ok)


              = NY1          OIc)P(1O,      c)P(1lO,1,c)P(OIO,1,1,c)P(OIO,c);


              p(Q, 0, 1, 1, Oe)


                           = O1c)P(OO,
                  fly1                 E'°' c)P(11O,1,c)P(OlO,1,1, c),
                                                 54



where c =                    Thus these two sequences are equally likely conditional
            (c1,     c2).

n c, and letting p be the probability measure for c gives


            p(O,1,1,O,O) =      I    P(O,1,1,O,OIc)u(dc)


                =   I P(O,Q,1,1,OIc)p(dc) = P(O,O,1,1,O)
So the alternating Poisson process implies restrictions on the multinonial

distribution tor the binary sequence.

     These tests are indirect. The duration dependence question is clearly

easier to answer using surveys that measure durations of spells.                      Such

duration data raises a number of new econometric problems, but we shall not
                        27
pursue them here.              would simply like to make one connection with the

methods that we have been discussing.

     Let us simplify to a one state process; for example,                       can be t e
                                                                                 th
duration of the time interval between the starting date of the i—

individual's            job and his                   job.   Suppose that we observe T > 1

jobs for each of the N individuals, a not innocuous assumption.                    Impose,

the restriction that             >    0    by using the following specification:


                        exp(x +      C1 +


            E*(ujIxj)
where       =                             Then
                Ct11,        x).

                E*(Z4 Yk5j) $xj +

and our Section 2 analysis applies.                   The strict exogenaity assumption has
                                               55




a surprising implication in this context.                 Suppose that           is the
individual's age at the beginning of the
                                         2                 8
                                                            job.
                                                                            it
                                                                   Then x. - x.
                                                                                    :tt—1
                                                                                            =

1i,t—1 -- age is not strictly exogenous.



                                       4.    INFERENCE


                                  =
        Consider a sample Ei          (xj, y!), 1=1             N,where          (x11           XIK)

y1a (y11,   .   .    y).     We       shall assume that         is independent and identically
distributed (i.1.d..)      according        to some mulcivariate     distribution       with finite
fourth moments and E(x x) nonsingular.                   Consider the minimum mean—square
error   linear predictors,29

                    iw-i.
                       x )——          x


which we    can write as

                E*&jIx1)                        ii
                                                      E(jx) 1E(xx)]1

We want to estimate fl subject to restrictions and to test those restrictions.

For example, we may want to test whether a submatrix ot II has the torm

6 I +LA'.

        We shall not assume that the regression function E(y1x1) is linear.

Far although E(yjx c1) may be linear (indeed, we hope that it is), there

is generally no reason to insist that                          is linear.    So we shall

present a theory of inference for linear predictors. Furthermore, even if

the regression tunction is linear, there may be heterdskedasticity —— due

to random coefticients,        tor example.          So we shall allow E[(-4tx) (y— IIx) '            Ix]
to be an arbitrary function of
                                                     56




       4.1      The Estimation of Linear ErecHctcrs
                Let       be the vector formed from the distinct elements of r.r
                                      30
that have nonzero variance.                  Since
                                                                   (j yP is         i.i.d., it follows

that w. is i.i.d.             This simple observation is the key to our results. Since

II is a function of                  our problem is to make inferences about a function

at a population mean, under random sampling.

         Let ii =             and let 'rr be the vector formed from the columns of II'

(a                       Then ¶ is a function of 1.1: a                   h(p).         Let w 4
then Tr h(w) is the least squares estimator:

                               N                 N
                                           —1
                        vec[( E      —           E
                             1=1            1=1


By the strong law of large numbers, ; converges almost surely to p° as N +

     A.S.> u°)         where p°    is the true value of p. Let ii ah(p°). Since

h(p)   is continuous at p =                we have 'Ti a.s.> ¶0               The central limit theorem

implies        that

                          -   v°) —2-->N(O, V(w))

                                                           0
 Since    hQj)    is   differentiable at             =         the    5—method gives

                          -         D
                                   —> N(O,        Q),


 where


                                                _______              31
                      c2= ______
                           j



          We     have derived the limiting distribution of the least squares estimator.
  This approach was used by Cramer                        (1946)   to obtain limiting normal distributions

  for sample correlation and regression coefficients                              (p.    367); he presents an
                                                           57




explicit formula for the variance of the limiting distribution of a sample

correlation coefficient (            p.   359)     . Kendall arid Stuart (1961, p. 293) and

Goidherger (1974) present the formula for the variance of the limiting distri-

bution of a simple regression coefficient.

        Evaluating the partial derivaives in the formula for 2 is tedious.

That calculation can be simplified since Tr has a "ratio" form. In the Case

of simple regression with a zero intercept, we have ir =                                 E(y.xj/E(x)   and

                                     N                              N                N
                      -             ( E yx. —                       E                Z x/N)11.
                                                                                         1
                                    i=1                         i-i


Since
            4/N
                      a.s.>               we     obtain the same limiting distribution by
working   with




             i=1
                     [(y1   a    O)j/(/(Zfl
                                                       0
The definition       of ita gives                  —
                                                           x1)x1]
                                                                         a 0,   and so the central limit
theorem implies that

                  ( -                    N(O E[(y1              -   Ox)2xZ]/[E(x2)I2}.

This approach was used by White (1980) to obtain the limiting distribution
                                                                32
for univariate regression coefficients.                                  In the Appendix (Proposition 7)       we

tollow White's approach to obtain


(4.1)            =              —              —
                                                                        ®!x1ii!x1]'
where       E(xx[). A consistent estimator of Q                                 is    readily available from

the corresponding sample moments:
                                                    38




(4.2)            =
                       4          [(y.   -   ft xp(y — ft xi)' ®          ç1(xx) sH


where                             x.x/N.


        If                 = It   x.,        that the regression function is linear, than


                   =
                       E(V(yx1)          ® !?cv
If    V(y.x.) is       uncorrelated          with        then


                   =
                       E(V(YIx)] ®
If the conditional variance Is homoskeda9tic,                       so   that V(y.Lx.)   a   E   does


not    depend on             then

                                   —1.
                       -
                            x


        4.2   Imposing Restrictions: The P#ininrtmi Distance Estimator

              Since II. is a function of                    restrictions on 11 imply restrictions

an E(w). Let the dimension of LI • E(w1) be q.                              We shall specify the restric-

tions by the condition that U depends only on a pXl vector e of unknoWn

parameters: i              g(®),    where g is a known function and p < q.               The domain ot

e is T, a subset of p-dimensional Euclidean space (B?) that contains the true

value          So      the restrictions Imply that              =            is   confined to a certain

subset of
                                                     59




        We can impose the restrictions by using a minimum distance estimator:

 choose B to

                          N
                  miii E [w. —          g(S)]'   AN[wj
                                                    — —
                   @cT    i=1


where                    P and         is positive definite.      This minimization problem is

equivalent to the ,following one:                  choose 6 to


                  mm          —   g(9)j'      -—
                                           A,1[
                                           -
                  OcT


The properties of 6 are developed, for example, in Malinvaud (1970, Chap. 9)

Since g does not depend on any exogenous variables, the derivation ot these

properties can be simplified considerably, as in Chiarig (1956: and Ferguson

 (1958).


        For completeness, we shall state a set of regDlarity concitions and

the properties that they imply:



Assunrptzon       2.            a.s.> g(O°); T is a compact subset of          that contains

     g is continuous on T, and t(ffi) =              g<!0) for   SET implies that 0

   a.s.>               where Y is positive definite.



Assumption        2.               —          —-— EU1 A); T contains a neighborhood

   of         in which g has continuous second partial derivatives; rank               (G)


p, where G


           Choose to


        min[ -
           e eT
                                                  60




Fro?ost tion. I. If Assumption 1            is satisfied, then        ____ a0


?rovosition2.        If assumptions 1 and 2 are satisfied, then vW(e                -    e° —a--> N(O, A),

where


               A =   (G''YG)G'       A ! (2' '_)—•

If   A is positive definite, then A -                             is positive semi-definite;

hence an optimal choice for ''        Is A1,

Proposition 3.

definite matrix, and if AN a.s.>


               NEaN —   () , NN         —
                                            ,
                       If Assumptions' and 2 are satisfied, if

                                                  then


                                            g(a) 1 _!_> x2(q—)
                                                                               is a qxq positive—




        Now   consider imposing additional restrictions, which are expressed by
the condition that 6 =       f(cO,   where      is sXl (s <   p) .     The.   domain of ci is
a subset of          that contains the true value Il             50   6° 2          is    confined
to a certain subset of R.

Asawnption 2'.           is a compact   subset   of      that contains        a?;   f is a
continuous    mapping    from    into T; 2(a) 6 for a E T1 implias                        C0;
     contains a neighborhood of CL0 in which £ has continuous second partial

derivatives; rank (F) = s, where y

        Let h(ct)    g(f(CS)J. Choose &to


                tin (a.a-h(cL)F
                     —     ."   &[a -h(csfl.
               —1
                                                        61




Proposition    3'.     If Assumptions 1,        2, and 2' are satisfied, if A ispositive
                            a.s.
definite, and if A\.                         then            -                    -   s)       where
                                                                 d2                        ,




               d1 =   N[aN
                                -
                                    ()]' 'NN - h(C)j,
                     = N(aN -       g(6)
                                       I       ®    -



Furthermore, d1 — d2 is independent of                           in their limiting joint distribution.


      Suppose that the restrictions involve only II. We specify the

restrictions by the condition that It =                      fL,       where 5 is sxl and the domain

of 6 is T1, a subset of                that includes the true value .                           Consider     the

 following estimator of 60: choose Ô to


                            —
                                !Q' ff1             — f(dfl,


where Q is iven in (4.2), and we assie that Q in (4.1) is

positive      definite.     If Tl and f satisfy assumptions 1 and 2, then                                  a.s.> (50



                      -   r) —L N(O, trJi' U1)
and

                            A                                          2
                N[1T —    f(5Y]'            —            —>
                                                         D
                                                                      x (MK—s),


where F


      We can also estimate                 by applying the minimum distance procedure to

w instead of to 7. Suppose that the components of                                 are arranged so that

                       where           contains the components of                               Partition
      <!I1
11    Mw1) conformably: ii' =           (.ij P. Set 2t                    2i' )            =   (5', p.
                                                     62




Assurne that V(w.) is positive definite. Now choose B to



                            —
                                () I'           —




                     a.s.       —1
where                       >V (wi)




                  g(@)


                                                 U
and                  =           Then e1 gives an estimator of C;           it   has the same

limiting distribution as the estimator 6 that we obtained by applying the
                                            A   36
minimt.      distance procedure to it.

        This framework leads to some surprising results on efticient estimation.

For a simple example, we shall use a univariate linear predictor model,



                  E*(YjIX1, X12)          iT0 + 1T1X1 + 2j2

Consider imposing the restriction '112                    0.   Then the conventional estimator

of iT is                 the slope coefticient in the least squares regression ot y

on x1. We shall show that this estimator is generally less efficient than

the minimum distance estimator if the regression function is nonlinear or

if there is hetaroskedasticity.

          Let iT1, ir2 be the slope coefficients in the least squares multiple

regression of y on x, x2. The minimum distance estimator of it1 under

the restriction             n   0    can be obtained as 6 a            + 2 where t is chosen
                                                                 ir1
to minimize the (estimated) variance of the limiting distribution of 6;

thi       gives
                                                            63




                         A            017
                 3   =            -
                              1               2
                                      022

where           is the estimated covarianca between . and                              in their Limiting

distribution. Since ?
                                       1
                                            =b      —   Qb         ,   we   have
                                               yx         2 x2x1


                =b           YX:J.
                                      -(b
                                            X2X1
                                                   +--—-)ir.
                                                          2
                                                            A


                                                    22
        If E(yjx.1, x12) is linear and if V(y.x.1, x12)                                a2 then
—
    Cav(x11,    x2)/V(x11) and                      b. But in general &            #          and 5       is more

efticient than b                       The source ot the etticiency gain is that the limiting
                     yxi
distribution     of IT2 has a zero mean (if 112                         0), and so we cart reduce

variance without introducing any bias if iT2 is correlated with b                                     .    Under
                                                                                               TX1
the assumptions ot linear regression and homoskedasticity, b                                   and ff2       are
                                                                                         PC1
uncorralated; but this need not be true in the more general framework that
we are using.


        4.3   Simultaneous Equations: A Generalization of 1x—and Three—
              Stag'e Least Squares
               Given the discussion on imposing restrictions, it is not sur—

prising       that two—stage least squares is not, in general, an efficient

procedure tar            combining instrumental variables.                    I shall demonstrate this

with a simple example.                      Assume that (1 z, x1, x.2) is i.i.d. according

to some distribution with finite tourth moments, and that


                             '5       + v.
                yi

                         =                   = 0.       Assume also that E(zx.1)       # 0,                    0.
where E(v.x,1)                E(v1x.2)                                                        E(z.x12)
                                                       64




Then there are two instrumental variable estimators that both converge                             a.s.
to 6:
                         N                N
                     =         y.x. ./ E z.x..
                                it).L]. 113
                                                                               (j=1, 2),
                         1=1



                                    )-

where the j, k element of A is


                          E[(y.—iSz )2x x
                                                                  (j,k         1,2).
               Xjk        E(zjxj.)E(zjxjk)

        The two—stage last squares estimator combines                              and 6., by forming

   c           +                   zsed   on the   least squares regression of z on               x2


 (assume     that E[(x1            t2)'(x1 x2)] is nonsingular):
                               N              N
                          =                                       +    (1—&)
               '5TSLS                  yizi/E z =

where

                               N                   N               A
                                                                         N
                                              A
               &               Ezxfl/Uri E                    +



Since    &   a.s.>        VW(STSLS — 6) has the same limiting distribution as


                                   +
                                       (1—cO(62—óuJ.

        This   suggests        finding the t       that     minimizes the variance of the limiting
distribution       of v'R(t(51—d) +           (1—i) (62—6) 1. The         answer leads to the minimum
                                                        65




distance estimator:                    choose 6 to




                               [()c:)]                          [C:) (:)]
                 g =               + (1—'r) 2

where


                   z       (A1'    +                 + 2X12 + A22),



aod           is the j,k element of A-i.                    The estimator obtained by using a

consistent estimator of A has the same limiting distribution.

           In general t            a since t is a function of fourth moments and                 is not.

Suppose, for example, that z. =                              Then a   a   0   but I   0 unless
                                                     x.2.
                                              2
                                        C,


                 E[(y.1        —
                                                                 )
                                                                     )] = 0.
                                             E(x2)

           It we add another equation, then we can consider the conventional

three-stage least squares estimator.                          Its limiting distribution is derived

in the Appendix (Proposition 7); however, viewed as a jjiu distance

estimator, it is using the wrong norm in general.

       Considar the standard simultaneous equations model:


                 y nfl Cj + u,                         E(ux) a 9,

                           +   Bxi S
                 Ezi

where FtI + Ba 0 and                                   We are continuing to assume that

y.    is   Nx1, x      I       s  (xj, y)s i.i.d. according to a distribution
                                   KX1, rI

with   finite tourth moments u=i, .        N) , and that E(xx) is rionsingular.
                                                       66



There arc restrictions on F and B: m(F, B) = 0, where tn is a known function.

Assume that the implied restrictions on 11 can be specified by the condition

that      =    vec(fl')   =    f(6),    where the domain of 6 is T1, a subset of                    that

includes the true value 5°(s .s MEC).                       Assume that Tl and f satisfy assumptions

1 and 2; these properties could be derived from regularity conditions on m,

as in Maljnvaud (197O, proposition 2, p. 670).

                   A
         Choose    6 to


                   —                 Al
         mm                       Q       [ii —
                                           -       —
        6cT1

where Q is given          in   (4.2) and we assume that              in (4.1) is
positive definite. Let F =                                      Then we have Ai(—6°)              r0, A),
where A = (F 1—1—].
              Q F) .                    This      generalizes Malinvaud s minrnum distanc
estimator (p. 676); it reduces to his estimator if t4u                             is   uncorrela       d
with             so that 2   E(u7u) (13 [E(xx)]1 (u7 = y a flO)
                                 =

        Now    suppose that the only restrictions on 1' and B are that certain
coefticients      are     zero, together with the normalization restrictions that

the coefficient of                   in the            structural equation i.s     one.   Then we can

give a* explicit formula for A.                     Write the        structural. equation as


                yin            Z• +17.in
                          -m -in

where the components of                    are the variables in            and x. that appear in

the            equation with unknown coefficients. Let there be F!                         structural



equations      and ast.e        that      the true value           is nonsingular. Let
   =
        (5j, ...,      5)be       sXl, and let £) and (d) be parametric repre-

sentations of r and B that satisfy the zero restrictions and the normalization

rule.    We can choose a               compact     set T1cR! containing a       neighborhood   o the
true value 6°,          such that                is nonsingular for
                                                                        5cT1.    Then iT =
                                                               67




where          )        vec [—1           ()   B(6)]'.
        Assume         that f(6) =
                                               0.                      =   0
                                                                                    so that the structural
                                                   implies that

parameters         are         identified.         Then          and f satisfy Assumptions 1 and 2, and

 vW(5   - 5°)      —a—> NU1 A). .s'The formula far                     3:/as' is       given in Rcthenberg

 (1973,      p. 69):




                           =   —
                                   (r1 ® 1K !zxM                      ® !;n'
where             is block diagonal:                       =                   x!), ...,
                                                     cV
                                                               diag{E(z1                   E(zm
and            E(x x!). So we have
       -X


(4.3)                       (!tyz                  ® r)fl t}1
where
           0
          v4                    00'.is uncorrelated with x4x4,
             n r0y. + BN0x1. If u4u.                                                            then   this
          —s       —                -I-


reduces      to

                   A   =
                           C! [i1(vv) ®                        cr1i   !X}'      I


which       is the conventional asymptotic covarianca matrix for three—stage
least squares (Zeilner and ThieJ. (1962)).
        I shall present a generalization of three—stage least squares that
has the same limiting distribution as the generalized minimum distance
estimator.             Let $ = vec(B)              and    note that 7 = — (F1 c' 1)6               Than we

have

                        +      (r1 ® I)            SI' Q_l[A +        (1 ®
                   =       ((F ® I)ir          +   8]' 9[(r ®          I) + B],
                                                                         68



  where


                            a    =    (I     £ C1) r                     .
                                                                     0 ,O r
                                                                          -             x     xx!)    (I '2'



  Let   S
        -,zx
                   be the tollowing block—diagonal matrix:

                                                           N
                            S
                            .zx
                                     a   diag          L
                                                        ir
                                                           z.1x,. •            •   .          Z
                                                                                            i=1

 and let

                                                                               N

                       sx=#i1
                       -.                                      -.
                                                               S     =


 Let


                            -,
                                 =
                                      E(vv°       ®         j:j'         W=—iN
                                                                         —  N
                                                                              S (vv  \ x
                                                                                 .Li _'
                                                                                       i=].
where


               •
                   -    ft.

                        v.ry1*Bxandr
                        -.1
                                         -        I'                 "
                                                                                       t;
Now    replace 0 by


                            s    (I      ® s_i) 'Y(i ® sb,
and note that


                       (I ®              s)((F ® ii +                     $]       s
                                                                                   'X7
                                                                                            —     V 6.
                                                                                                  mZX -

Then    we have the tollowing distance tunction:


                                                       "—1
                       (s            —   S'
                                         —zx
                                               5)'
                                               —
                                                   1Y
                                                       —
                                                               (s        — S' 5).
                                                                           -4%
                        _xy                                        —xy


This    corresponds to Basmaxin's (1965)                                       interpretation of three—stage least
            17
squares.
                                                                69



                  Zin±rnizing    with   respect to          gives
                        I'             "—1
                        —G3
                                = (S
                                   —zx'1'       S'
                                                -.zx
                                                    ) —1   (S
                                                                     "—1
                                                            —zx-- —xy ).
                                                                     1F   s
.   U-

         The      limiting distribution of this estimator is derived in the Appendix

          (Proposition 7) .       We    record it es


         Proposition 4.                       — 50)             N(O, A), where A is given in (4.3).

         This generalized three—stage least squares estimator is asymptotically

         efticient within the class ot minimum distance estimators.



         Our derivation of the limiting distribution of                                      relies on linearity.

         For a generalized nonlinear three—stage least squares estimator, see

         Hansen     (1982)



                  Finally, we shall consider the generalization ot two—stage least

         squares.38 Suppose that


                        'il z5'
                        't
                            -1-il +v II'
         where                   0, z is                   and rank [E(xjzh)] =
                                                                                                  s1.   We complete

         the system by setting


                        yin a             +


         where E(x.u.) :0 (in = 2                      M).        So          =   x.   (in    2            M), and



                                 diag fz(z1 !) hf—i® E(x1x) }.

         Let 5' a                  ••    :) and apply the minimum distance procedure to

         obtain ?; since we are ignoring any restrictions on Ttm (in = 2,                                     .,
                                                                   70




A
    is    a limited information minimum distance estimator.

         We       have                 a                     N(O, A11), and evaluating the partitioned

inverse           gives

(4.4)                  A11 =     {E(z               x) [Eft41)2 pi''             E(xz1)}1
where              =            . äj°z.1.

         We can obtain the same limiting distribution by using the following

generalization of two—stage least squares: Let


                          —
                                                "' Ni)' F              =


                                            •   •                and


                                        N                    A          2
                           a!.          z (Y1            —
                                                                            iI
                                       i1
where                                 (for example,               could be an instrumental variable
          61

estimator);              then



                       1G2 =                        !i   !ti)1 '                 F   z*
This is the estimator of                                 that we obtain by applying generalized three—

stage least squares to the completed system, with no restrictions on

              2                 N).        The limiting distribution of this estimator is derived

in the Appendix (Proposition 7):
                                                   71




ETotosition         5.                 -
                                                   NCJ      A11), where A11 is given in
(A .4).     This generalized twa—stage least squares estimator i
asymptotically efficient in the class of limited infonatior, minimum
distance        estimators.



      4.4 Asymptotic Efficiency: A Comparison uith                    th      Quasi—lt.vizrwn
           Likelihood Estimator


                Assume that            is i.i.d. (i=1,2....) from a distribution with

E(r.) =    r,            a    E,   where Z is   a JXJ   positive—deflalta matrix;        the fourth
                V(r1)
moments are finite.                Suppose that we wish to estimate functions of I

subject to restrictions.               Let cy = vec (Z)       and express the restrictions by

the condition that a                       where g is a function fr ui T Thto            with a

domain T C R that contains the true value G°(q =                          j   <   j;jj/), Let
                S        2    (r. -
                               —1.     -— -

and let ;   = vec(S).
      If the distribution of                 is niultivariate      normal, then the log-likelihood

tunction is



                    L =f6 lilt         -                +      -      -

If   there are no restrictions on                 then the maximum 1ike1thod estimator of

@0   is   a solution to the following problem: Choose e                   to soive


                ______
                             [C1 (9) (23 z_1(e)](; e g)) a

We shall derive the properties of this estimator when the distribution of
                                                  72




r, is not necessarily normal; in that case we shall reter to the estimator

as a asi-maximum likelihood estimator (eQ) .

        MaCurdy (1979) considered a version of this problem and showed that,

under suitable regularity conditions,                       -      has   a limiting normal

distribution;    the covariance matrix, however, is not given by the standard

intormation matrix     tormula.     We would like to compare this distribution

with the distribution ot the minimum distance estimator.

        This 'comparison can be readily made by using theorem 1 in Ferguson (1958).

In our notation, Ferguson considers the following problem:                  Choose S to

solve



              wG, 6)   [-   g(O)I         0.



He derives the limiting distribution of Vice - 0°) under regularity conditions

on the tunctions W and ,.         These        regula ity conditions are particularly simple

in our problem since W does not depend on ;. We can state them as tollows:


Assumption 3.     E C i1' is an open set containing                g is a continuous,

one-to-one mapping of E into                   with a continuous inverse; g has coütlnuous

second partial derivatives in E; rank (3g(G)/'] a p for 8c; E(e) is non—

singular for ec
                -0
                                  - as.
In addition, we shall need s               > g(B        and the central linIt theorem result

that VSiG -                  (2' ti), where A =
                                                          V((r1— I°      ® Q.
        Then Ferguson's theorem implies that the likelihood equations almost

surely have a unique solution within                   for sufficiently large N, and

             2°) _L N(O,    A), where
                                                        73




                        (G'4'G) QtU A F


and G = agç°) /e', 'V =(E° ® :°)                        .     U will    be convenient to rewrite

this, imposing the symmetry restrictions on E. Let o* be the J(J+1)/2x1

vector formed by stacking the columns of the lower triangle of E. We can

define a        x [J(a÷i)/2J matrix T such that a                  =T     a*.     The   elements in each

row of T are all zero except for a single element which is one; T has full

column     rank. Lets I s*, g(B) :                      I    g*(G), G
then     VIi[s* -                 ——> N(O,       A*),   where A* is the covariance matrix of the
vector    formed from the columns of the lower triangle of (r                                0       —    I   )


Now    we can set

               A        (Gt* 'p*                   ¶iJ* A '*    G*)(G'*   'jI*




        Consider      the following Mnimum distance estimator: Choose                                to




               miii. IS< —
               SC           -
                                       -- —
                                   - - A.[sk
                                   g*(9)J'

where T is a compact subset of                      that contains a neighborhood of

and                  'Y*.       Then the tollowing result is implied by Proposition 2.


Proposition     6.          If Assumption 3 is satisfied, then                          —   6°)   has the

same     limiting distribution as                       -

If    A* is nonsingular, an optimal minimum distance estimator has p a.s.>                                        ç*1
where       is an arbitrary positive real number.                      If the distribution of

is normal, then                    a   (1/2)   !*; but in general                j not proportional to

!*,    since A* depends on fourth moments and '1' is a tunction ot second moments.
                                                   74




 So in general                  is less efticient than the optimal minimum distance

estimator that uses


 (4.5)                 =               -           -

where s is the vector formed from the lower triangle of                                 —   ) Cr. ) ,
                                                                                                 —

         More    generally, we can consider the class of consistent estimators
                                                            A
 that are continuously differentiable functions of s:6 êG*)                                   chiang

 (1956) shows that the minimum distance estimator based on                                   has the

minimal         asymptotic covariance matrix within this class.                       The minfmt

 distance       estimator based on              in (4.5) attains this lower bound.

4.5. Multivricte Probit Models
          Suppose that

                        =   I   if TI' x   +      > 0,


                        = 0 otherwise (i1, ..., N;          opt, .   .   .    . H),

where the distribution of uj1 =                             Urn) conditional on               is multi—
                                                (Uj1   •.


                                      There may be restrictions on               =
variate normal, N(O, E).
                                                                                      (9
but we want to allow E to be unrestricted, except for the scale normalization

that the diagonal elements of E are equal to one.                            In that case, the maximum

likelihood estimator has the computational disadvantage of requiring

numerical integration over M—l dimensions.

         Our strategy is to avoid numerical integration. We estimate                               by

maximizing the marginal likelihood function that is based on the distri-

bution of y            conditional on x.
                  im                       -1
                                                          75




                P(y.im =        11x)
                                  -i
                                           =   p(TP   xj,
                                                 -rn-i.



where F is the standard normal distribution function.                              Then-under   standard

assumptions we have T a.s.Y. °, the true value.                               If 1ii(!-.Jr°) D N(o, £2),

then we can impose the restriction that 1                             f(6) by choosing 6 to minimize


                [11 —                  111[1r   -   Qfl.
We   only need to derive a formula                    for
        Our estimator of ir is solving the following equation:



                              3QçlI)




where

                               N       N
                               2{Z
                QOT)
                              i1       ri           Ln.F(IIT x) ÷    (1—y)    Lit [LP(111 x) IL.



Hence the asymptotic distribution of iT can be obtained from the theory

of "M—estimators." Huber (1967) provides general results, which do not

impose differentiability restrictions on SOT). His results cover, for

example, regression estimators based on minimizing the residual sum of

absolute    deviations.            We shall not need this generality here and shall

sketch the derivation tor the simpler, difterentiable case.                              This case

has been considered by Hansen (1982), NaCurdy (1981a), and White (1982).41

      Let        be i.1.d. according to a distribution with support Z C

Let G be an open, convex subset of                             and let 1P(z, 0), be a function from

Z X ®    into           its            component is 1C' 9). For each BEg, 4i is a

measurable function of z, and there is a                               with
                                                         76




                                        =   0,    E[1P(z, e°)p'cz          6°)]   =      C




For     each zZ, )           is    a twice continuously differentiable function of 6. In

addition,



                             raf(z1,
                                         e°)
              3=
                        EL
is nonsingular, and


                  3'(z, e)
                                                          (Ic,   £, in     1,     ..,    p)


far   6S®, where E[b(z1)j C QO

        Suppose       that         have a (measurable) estimator                        such that
                                                                                                             r
and

                  N               A
                  E   —
                                         s9
                      tP(z1. !)
              i=1


for   sufficiently large N a.s. By Taylor's theorem,


              A. £ 'k5i' ØO) +
              Si—i                                   -
                                                         + I     NU            c      i:v' 2NiJ1

where

                          1       N atPk(zj,       !°)             1     N
                                                                         E
                                                                           ki' k
                               1=1
                                            pa—
                                                                         i—i       ppv
                                                                                    —    —




and
      Ak is   on the line segment joining eN and G° (kal,                                .., F)     (The measur-

ability of             follows from lemma 3 of Jezrnrich (1969).) By the strong law

of large numbers,                     converges a.s. to the                row of J, and
                                                       77



               1       N ki' -NQ                            <
                                                                1
                                   ee                                E h(z.) a.s.> E[h(z1)]



(k, t, rnl                   p).   Hence (g         —e°'             + 0-   a.s. and
                         .
                                               -N--             Nk
                                                       N
                                                   1
                              =    _91
                                                   N
                                                       E qi(1.,
                                                       in
                                                                    - 6°))
                                                                    a.s
for N   sufficiently         large a.s., where                            > J. By   the   central limit

thea rem,


                   N
                   S
                                   oD
                                    —'-N(O,
                                    )                       A).
             vW i=i —


 Hence


                   &!N)                       N(O,



        Applying   this result to our inultivarlate probit estimator gives


                               D
                                        N(O,


where   J    diag{J1, •             J1,}
                                              is   a block—diagonal matrix with

                   -   Z[{(F')2/[F(1—F)]} xycj]

(F and its derivative F'                are    evaluated at—mox
                                                              -1.
                                                                             )


and


              A        Eta         x11xj

where the m, n element of the MXM matrix H is Ii                            n 2 mu
                                                                                a e with

                             -F
              em = 1(1—F) F'                           (n1,, ,.,M)
                                                       78




(F and pt are evaluated at °'x1). We obtain a 'oonsistent estimator (Q)

of                   by    replacing expectations by samp1 means and           using    in place

of IT0,        Then we can apply, the minimum distance theory of Section 4.2 to

impose restrictions on T.


                                         5.   EMPIRICAL       APPLICATIONS



5.1 Linear Models: Union Wage Effects

         We shall present an empirical example that illustrates some of the

preceding results.42 The data come from the panel of Young Men in the National

Longitudinal Survey (Panes)                         The sample consists ot 1454 young men

who were not enrolled in school in 1969, 1970, or 1971, and who had complete

data on the variables listed in Table 1.                         Table 2.1 presents an unrestricted

least squares regression ot the logarithm ot wage in 1969 on the union, SMSA,

and region variables for all three years.                         The regression also includes a

constant,        schooling,           experience,     experience squared, and race.    This     regression

is repeated using the 1970 wage and the 1971 wage.

          In Section 2 we discussed the implications of a random intercept (c).

If the leads and lags are due just to c, then the submatrices of j corres-

ponding to the union, SMSA, or region coefficients should have the form

S    I   + 4   A'.        Consider,     tor example, the 3x3 subinatrix of union coefficients ——

the off—diagonal elements in each co1un should be equal to each other. So

we compare .043 to .046, .042 to .041, and'—.009 to .010; not bad.

          In Table 2.2 we add a complete set ot union interactions, so that, tor'

the union variables at least, we have a general regression function.                          Now

the submatrix of union coefficients                         is 3x7 If it equals        0) + £    A', then
                                  79



                               Table   1


 CT-L4R4CTERISTICS OF NATIONAL LONGITUDINAL SURVEY YOUNG
     '4ZN NOT ENROLLED IN SCHOOL IN 1969, 1970, 1971:

                Means and Standard Deviations
                         N=1454
Variable                  Mean                   Standard      Deviation

LW1                      5.64                               .423
LR2                      5.74                               .426
LW3                      5.82                               .437
Ui                         .336
u2                         .362
u3                         .364
U1tJ2                      .270
thUS                       .262
U2U3                       .303
tJ1U2U3                    .243
SMSA1                      .697
SMSA2                      .627
SMSA3                      .622
RNS1                       .409
RNS2                       .404
RNs3                       .410
S                       11.7                           2.64
EXF69                    5.11                          3.71
Er692                   39.8                          46.6
RACE                       .264


Notes   to Tabte 1:
LW1, LW2, LW] — logarithm ot hourly earnings (in rents) on the
current   or last job in 1969, 1970, 1971; Ui,    112, U3 ——       I   if wages
on rurrent or last job set by rollertive bargaining, 0 it not, in
1969, 1970, 1971; SMSAJ., SMSA2, SMSA3 — 1 it respondent in SMSA.
0 if not, in 1969, 1970, 1971; RNS1, ENSZ, RNS3 —— 1 if respondent
in South, 0 if not, in 1969, 1970, 1971; 5 —— years ot srhooling
completed; ExPS9 —— (age in 1969 — S — 6); RACE —— 1 it respondent
blark, 0 it not.
                                                             80



                                                       TABLE 2

                                 UNRESTRICTED LEAST SQUARES REGRESSIONS

2.1
                                        Coefticients        (and Standard Errors) of:
Dependent
Variable             Ui          112     u3       SMSSAJ.         SNSA2 SNSA3               RNS1       RNS2    RNSJ

LW].                .171         .042 —.009        135            —.001       .032          —.016      —.020   —.108
               (.025) (.026) (.025) (.028)                        (.055)     (.054)         (.081) (.081) (.070)

LW2                 .048         .150     .010     .086            .053       .020            .065     —.039   —.155
               (.023) (.028) (.026) (.027)                        (.065)     (.061)         (.099) (.109) (.092)

LW3                 .046         .041     .132     .083            .003       .088            .074      .056   —.232
               (.023) (.030) (.030) (.031)                        (.058)     (.056)         (.079) (.093) (.078)


Notes to Table 2.1:
LU. regressions ic1ude (1,                 S, Ei69,         EX2692, RACE).      The standard errors are
calculated using 2 in (4.2)




                            Coefticients         (and Standard Errors)          at:
Dependent
Variable                   UI.            112          u3            tJ1U2           U1u3             tJ2US     U11J2133

LW1                    .127             —.047      —.072             i28               .092           .156       —.182
                      (.044)            (.042)     (.041)           (.072)            (.075)         (.070)      (.104)

LWZ                  —.019               .014      —.085             .181              .118           .227       —.229
                (.040)                  (.045)     (.040)           (.074)            (.092)         (.066)      (.116)

LW3                  —.050              —.072      —.022             .110              .264           .246       —.256
                      (.037)            (.053)     (.052)           (.079)            (.081)         (.079)      (.113)


Notes toTable 2.2:
All rressions include (SNSLI, SMSA2, SNSAS, RNSL, RNS2,RNS3, 1, S, EXP69,
E69,       RACE).      The standard errors are calculated using 12 in (4.2).
                                         81




in the first three columns, the oft—diagonal elements within a column should

be equal; in the last tour columns, all elements within a column should be

equal.

         I first imposed the restrictions on the SMSA and region coetticients,

using the minimum distance estimator.         Q is estimated using the formula in

(4.2), and       =        The   minimum distance statistic (Proposition 3) is            6.82,
which    is not a surprising value from a x2(1O)       distribution.     It we impose the
restrictions o the union coefficients as well, then the 21 coefficients in
Table 2.2 are replaced by 8: one         and seven X's.     This gives an increase in

the minimum distance statistic (Proposition 3')       of 19.36 —       6.82 i 12.54,     which

is not a surprising value from a                distribution.   So there is no evidence

here ak.ainst the hypothesis that all the lags and leads        are generated by         a.

In the :aninology of Section 3.3,        the (linear predictor) relationsht of x

to y arpears to be static conditional on c

         Consider a transtormation ot the model in which the dependent variables

are Ltd, LW2—LW1, and r..W3—LW2.      Start with a intiLtivariate      regression   on


all of the lags and leads (and union interactions); then impose the

restriction    that U, SMSA, and RNS appear in the LW2—LW1 and LW3-LW2

equations only as contemporaneous changes (S(Y_Y_jjXitX2X3)

This is equivalent to the restriction that c generates all of the 1ag: and

leads, and we have seen that it is supported by the data.               I also considered

imposing all ot the restrictions with the single exception ot allowing

separate coetticients tor entering and leaving union coverage in the wage

change     equations.   The estimates (standard errors) are .097 (.019) and
                                       82




—.119 (.022). The standard error on the sum of the coefficients is .024,

so again there is no evidence against the simple model with E(ytxiiX2X3C)

      + C. 43
ixt
        Table   3.1 exhibits the estimates that result from imposing the

restrictions using the optimal minimum distance estimator.                 We also give

the conventional generalized least squares estimates.       They are minimum

distance estimates in which the weighting matrix (As) is the inverse of


(5.1)
                       N                    A.

                                                   ® Nil                   —1




                                                           A_i         1
We    give the conventional standard errors based on (F' Q        F)        and the

standard errors calculated according to Proposition 2. which do not require

an assumption ot homoskedastic linear regression.       These standard errors

are larger than the conventional ones, by about 30%.        The estimated gain

 in efticiency from using the appropriate metric is not very large; the

 standard errors calculated according to Proposition 2 are about 10% larger

when we use conventional GLS instead of the optimum minimum distance

estimator.

        Table 3.1 also presents the estimated A's.    Consider,    tor example,

an individual who was covered by collective bargaining in 1969. The linear

predictor of a increases by .089 if he is also covered in 1970, and it

increases by an additional .036 if he is covered in all three years. The

predicted c for someone who is always covered is higher by .102 than for

someone who is never covered.

        Table 3.2 presents estimates under the constraint that A                0. The

increment in the distance statistic is 89.08 — 19.36 =       69.72, which is a
                                                 83



                                           TABLE 3

                                 RESTRICTED       ES TIMA TES




3.1
                          Coefticients        (and Standard Errors)          ot:


                    U                            SMSA                         tNS

B:                 .107                           .056                        —.082
                  (.016)                         (.020)                       (.045)

B                  .121                           .050                        —.085
-.GLS
                  (.013)                         (.017)                       (.040)
                  (.018)                         (.021)                       (.052)




                  U].            u2        1J3           U1u2        U1U3            1J2U3       U1U2U3

A:
—
                 —.02       —.067   —.082                .156        .152             .195
                  (.O3       (.040) (.037)              (.057)      (.062)           (.059)          (.085)


             SMSA1                SMSA2           SMSA3          RNS1          RNs2           BNS3


                  .086            —.008               .032        .100         —.021          -.128
                 (.025)           (.046)          (.046)         (.072)            (.077)     (.068)

x2(23) = 19.36


3.2 Restrict A a

                          Coefticients        (and Standard Errors)          ot:


                           11                      SMSA


:                        .157                         .120                    —.150
                        (.012)                    (.013)                      (.016)

x2(36) z 69.06
                                                            a4



Jotes    to Tahia 3:

E*(z Lx) -
                  tix   =
                             111x1 + F2x2, Xj
                                                   = (Ui,       U2, U, U1U2, 131133, U21J3, U1JJ2U3,

SMSA1,       SNSA2, SMSA3, RNS1,             R}S2, R}53); x = (1,              S, EXP69, E)692 FACE)

      = ' 9' SMSA Y 5 13)                                + Z     A'; fl     is unrestricted. The

restrictions are expressed                   as ii =   F   6, where 6        is unrestricted.              and A

are    minimum distance estimates with                            =        in (4.2);          and èGLS      are


minimum distance estimates with                            =          in   (5.1) (A        is not shown in

the table)              The first standard error for                          is the conventional one based

on                           the second standard error for                       is based on
      '.—l       -1.        '—1   A..i           —i F)—1                                   The X
                                                                                                   2
                                                                                                       statistics
             V                    Q                            (Proposition 2) .                                    are

                                         '
                                             A_i .¼
computed from NPT — P                        Q   [IT —          ) (Proposition        3)
                                              85




surprisingly large value to come from a                    (13)   distribution.    If we constrain

only the union XTs to be zero, then the increment is 57.06 — 19.36 = 37.7,

which      is surprisingly large coning from a x            (7)   distribution.     So there is
strong evidence for heterogeneity bias.
       The union coefficient declines from .157 to .107 when we relax the

A =    0    restriction.     The least squares estimates for the separate cross

sections, with no leads or lags, give union coefficients of .195, .189,

and .191      in    1969, 1970, and 1971.          So   the decline in the union coefficient,
when we allow for heterogeneity bias, is 32% or 44% depending on which
biased estimate (.16 or .19) one uses.                  The SMSA and region coefficients
also decline in absolute value.              The least squares estimates for the
separate       cross sections give an average SMSA coefficient of .147 and an

average region coefficient of — .131.              So the decline in the SMSA coefficient

is either 53% or 52%, and the decline in absolute value of the region

coefficient is either 45% or 37%.



5.2.       Nonlinear     Models:    Labor Force Participation


           We shall illustrate some of the results in Section 3.                  The sample

consists of 924 married women in the Michigan Panel Study of Income Dynamics.

The    sample selection criteria and the means and standard deviations of the
variables      are in Table 4. Participation status is measured by the question
"Did _______ do        any work for money last yeas?" We shall model participation
in 1968, 1970, 1972, and 1974.
        In terms of the model described in Section 3.1, the wage predictors are
schooling,         experience,     and experience squared, where experience is measured
                                             86




as age minus schooling minus six; the tastes for nortmarkat time are

predicted by these variables and by children.                The specification tor children

is a conventional one that uses the number of children of age less than six

(YS) and the total number of children in the family unit (K).46 Variables

that aftect only the litetime budget constraint in this certainty model

are captured by c.            In particular, nonlabor income and the husband's wage

are assumed to affect the wife's participation only through the lifetime

budget      constraint.       The individual effect Cc) will also capture unobserved

permanent components in wages or in tastes for nonmarket time.

     Table 5 presents maximum likelihood              (ML)   estimates of cross—section

probit specifications for each of the four years.                Table 6 presents un-

restricted ML estimates for all lags and leads in YI( and K.               It the

residuals                 in the latent variable model (3.1) have constant variance,


then a1     .    .   .=          (3.8). and the submatrices of TI corresponding to

YK and K should have the form            I + 1. X'.    There may be some indication

of this pattern in Table 6, but it is much weaker than in the wage

regressions in Table 2.

     we allow for unequal variances and provide formal tests by using the

minimum distance estimator developed in Section 4.5.                In Table 7.1 we

impose the restrictions that




                Jtadiag{cs1,      .,cZ4}[&f7I4+LXfl(, Kt4+tXKi

The minimum distance statistic is           53.8,   which is a very surprising value

coming from a X(19) distribution.              So the latent variable C does not appear

to provide an adequate interpretation ot the unrestricted leads and lags.
                                             a7




It may be that the distributed lag relationship between current parti-

cipation and previous births is more general than the one implied by

scing over the previous six years (YK) and over the previous eighteen

years    (K) .    It may be truittul to explore this in more detail in tuture
work.     Perhaps strict exogeneity conditional on c will hold when we use a

more general specification tor lagged births.                 But we must keep in mind

that this question is intrinsically tied to the functional form restrictions —

we   saw in Section 3.3 that there always exist specifications in which                     is


independent ot                        conditional on c.

        It we do impose the restrictions in Table 7.1, then there is strong

evidence that A           0. Constraining X a 9         in Table 7.2 gives an increase

in the distance statistic of 78.4 — 53.8 s 24.6, which is surprisingly

large to come from a X2(8) distribution.

        In Table 7.3 we constrain all of the residual variances to be equal

(at = 1)    .    An   alternative interpretation ot the time varying coefticients

is provided in Table 7.4, where                   and     vary freely over time and             1.


In principle, we could also glow the                    to vary freely, since they can be

identified from changes        over   time   in the coefficients     of   c.     In tact that
model gives very         imprecise   results and it is difficult to ensure numerical
accuracy.
        We shall interpret the coefficients on '(K and K by following the pro-

cedure in (3.4).          Table 8 presents estimates of the expected change in the

participation probability when we assign an additional young child to a

randomly chosen family, so that YK and K increase by one.                      We compute this

measure for the models in Tables 7.1, 7.3, and 7.4.                 The average change in
                                         88




the participation probability is —.096. We can get an indication of omitted

variable bias by comparing these estimates with the ones based on Table

1.2, where X is constrained to be zero.           Now the average change in the

participation     probability   is — .122,    so that the decline in absolute value

when we control for c is 21%. An alternative comparison can be based on the

cross—section estimates, with no leads or lags, in Table 5.          Now the average

change in the participation probability is —.144, giving an omitted variable bias

ot 33%.

        Next we shall consider estimates from the logit framework of Section 3.2.

Table     9 presents   (standard) maximum likelihood estimates of cross—section

logit specifications for each of the four years. We can use the cross—section

probit results in Table 5 to cc.iistruct estimates of the expected change in

the log odds of participation         en we add a young child to a randomly chosen

tamily.     Doing this in each of the four years gives —.502, —.598, —.683, and

—.703. With the logit estimates, we simply add together the coefficients on

YK and K in Table 9; this gives —.507, —.612, —.691, and —.729. The average

over the four years is —.621 for probit and —.635 for logit. so at this

point there is little difterence between the two functional torms.

        Now allow for the latent variable Cc). Table 10 presents the conditional

maximum likelihood estimates for the fixed effects logit model.          The striking

result here is that, unlike the probit case, allowing for c leads to an

increase in the absolute value ot the children coefticients.          If we constrain

    and         to be constant over time (Table 10.1), the estimatedehange in the

log odds of participation when we add an additional young child is —.898.

If we allow         and     to vary freely over time (Table 10.2), the average of
                                       89




the estimated changes is —.883.      So the absolute value ot the estimates

increases by about 40% when we control for c using the logit framework.

The estimation method is having a first order eftect on the results.

       It is commonly found that probit and logit           specifications,    when properly

interpreted, give very similar results; our cross—section estimates are an

example ot this.      But our attempt to incorporate latent variables has turned

up marked differences between the probit and logit           specifications.      There

are    a number of possible explanations for this. The probit specification
restricts       to have a normal distribution conditional on x with a linear
regression     tunction and constant variance.    The conditional likelihood

approach in the logit model does not impose this possibly false restriction.

On the other hand, the probit model has a more general specification for the

residual     covarianca matrix.
       We   have seen that the restrictions on the probit        IT matrix,   which under-

lie our estimate of , appear to be false. An analogous test in the logit

framework is based on (3J.O). We use conditional ML to estimate a model

that includes YKsDt KsD (s           1,... ,4; t •     2,3,4),   where D is a dummy

variable that is one in period t and zero otherwise.             It is not restrictive

:0 exclude YK5D1 and lC'D since they can be absorbed in c. We include

ilso
     fl S.Dti EXP68.D and EX26821]Dt (t—2,3,4). Then comparing the maximized
:on.ditional likelihoods for this specification and the specification in

Table 10.2 gives a conditional likelihood ratio statistic ot 47.5, which

is a very surprising value to come from a x2(16) distribution.                So the

restrictions     underlying our logit estimates   of      also appear to be false.
                                       90




It may be that the false restrictions simply imply different biases in

the probit and logit       specifications.

                                6.   CONCLUSION



      Our discussion has focused on models that are static conditional on

a latent variable.     The panel aspect of the data has primarily been used

to control tor the latent variable.          Much work needs to be done on models

that incorporate uncertainty and Interesting dynamics.                Exploiting        the

martingale implications ot time-additive utility seems truittul here,                     as'


in Hall (1978) and Hansen and Singleton (1981) . There is, however, a

potentially important distinction between time averages and cross—section

averages.     A time average of forecast arrors over T periods should converge

to zero as T +    C3 But   an average of forecast errors across N individuals

surely need not converge to zero as N +                   there my be common components

in those errors, due to economy—wide innovations. The same point applies

when we consider covariances of forecast errors with variables that are

in the agents' intormation sets.        It those conditioning variables are

discrete, we can think of averaging over subsets of the forecast errors;

as T -       these averages should converge to zero, but not necessarily

as N -

     As     for controlling for latent variables, I think that future work

will have to address the lack ot identitication that we have uncovered.

it is not restrictive to assert that (y1, .       .   .   .    and (x1, .   .   .   .    are

independent conditional on some latent variable c.
                                         91



                                       TABLE 4
       CYA&CTh'MSTICS OF [4rCHIGAN PAJVEE STUDY OF INCOME'
                         DYNAMICS M42&IED VOMEN

                     Mewts crid Stzdard Deviations
                                   N = 924




Variable                          Mean                 Standard          Deviation

LYP1                              .499
LFPZ                              .530
lFP3                              .529
lFP4                              .566

                                  .969                         1.200
11(2                              .764                         1.069
YK3                             .551                            .895
YK4                               .363                              .685

                                 2.38                          1.69
K2                               2.30                          1.64
K3                               2.11                          1.61
K4                               1.84                          1.52

S                               12.1                           2.1
Efl68                           17.2                           8.5
Ei682                          368.                          301.



Notes to Table 4:

                      ——
LFP1, . . . . LF24         1    it answered "yes" to !!Did               work for money

last year, 0 otherwise, reterring to 1968, 1970, 1972, 1974; YK.1,                   .




YK4    —   number ot children ot age less than six in 1968, 1970, 1972,

1974; Ki ,    ...., K4     —    number   of children of age less than eighteen

living in the family unit in 1968, 1970, 1972, 1974; S —— years of

schooling completed; E68 —— (age in 1968 —                   S—6)    .   The   sample selection

criteria required that the women be married to the same spouse from

1968 to 1976; not part of the low income subsample; between 20 and 50
                                   92




years old in 1968; white; out of school from 1968 to 1076; not disabled.

We required complete data on the variables in the Table, and that there

be no inconsistency between reported earnings and the answer to the parti—

cipation   question.
                                                 93



                                       TABLE 5

                       ML PROBIT CROSS-SECTION ESTIMATES




                             Coefticients        (and Standard Errors)    ot:

Dependent
Variable             YtU      YX2           YKJ         YK4       K].       K2       K3       K4
                                             a           —

LFP1                -.246                                        —.063          -     -
                    (.046)                                       (.031)

LF22                         —.293           —                             —.075      —        —

                              (.055)                                       (.031)

LF23                                   —.342             —                          —.077      —
                                       (.067)                                       (.032)

LF24                                                   — .366      —                         —.069
                                                       (.081)                                (.034)




iIQTES TO TABLE 5:

separate   fl.1   estimates each year. All specifications include (1, S,

EXP68, EXP682).
                                              94




                                          TABLE 6
                                                                                                    I
                       UNRESTRICTED ML PROBIT ESTIM4TES




                        Coefticients      (and Standard Errors)    ot:
Dependent
Variable                                                                         K3            K4
              YK1          YKZ       YK3            YK4      fl

ISP1        —.205      —.017       —.160    .420           .176    —.142       —.196        .063
              (.081)   (.119)      (p141) (.144)          (.076)   (.100)      (.110>      (.090)

LFP2        —.047      —.238       —.047           .093    .320    —.278       —.250        .177
              (.079) (.117)         (.140) (.142)         (.077)   (.102)      (.110)      (.090)

LFP3        —.254    .214          —.190       —.209       .204    —.210       —.045        .030
            (.080) (.116)           (.139) (141)          (.077)   (.102)      (.112)      (.090)

LFP4        —.195       .252       —.211       —.282        20      .083       —.181        .058
              (.079)   (.118)       (.139)     (.138)     (.)75)   (.100)      (.110)      (.090)



NOTES TO TABLE 6:

Separate xt    estimates   each   year.    All specifications include       (1, S,    EXP68,

Er682).
