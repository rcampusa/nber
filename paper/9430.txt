                                NBER WORKING PAPER SERIES




                     OPTIMAL POLICY WITH PARTIAL INFORMATION
                          IN A FORWARD-LOOKING MODEL:
                          CERTAINTY-EQUIVALENCE REDUX

                                        Lars E. O. Svensson
                                        Michael Woodford

                                        Working Paper 9430
                                http://www.nber.org/papers/w9430


                      NATIONAL BUREAU OF ECONOMIC RESEARCH
                               1050 Massachusetts Avenue
                                 Cambridge, MA 02138
                                     January 2003




We thank Kosuke Aoki and Ernst Schaumburg for comments on an earlier draft. The views expressed herein
are those of the authors and not necessarily those of the National Bureau of Economic Research.

© 2003 by Lars E. O. Svensson and Michael Woodford. All rights reserved. Short sections of text not to
exceed two paragraphs, may be quoted without explicit permission provided that full credit including, ©
notice, is given to the source.
Optimal Policy with Partial Information in a Forward-Looking Model:
Certainty-Equivalence Redux
Lars E. O. Svensson and Michael Woodford
NBER Working Paper No. 9430
January 2003
JEL No. E37, E47, E52, E58

                                           ABSTRACT



       This paper proves a certainty equivalence result for optimal policy under commitment with
symmetric partial information about the state of the economy in a model with forward-looking
variables. This result is used in our previous paper, "Indicator Variables for Optimal Policy," which
synthesizes what is known about the case of symmetric partial information, and derives useful
general formulas for computation of the optimal policy response coefficients and efficient estimates
of the state of the economy in the context of a fairly general forward-looking rational-expectations
model. In particular, our proof takes into account that, under commitment, the policymaker can
affect the future evolution of the observable variables, and thereby potentially affect the future
information available.


Lars E.O. Svensson                                    Michael Woodford
Department of Economics                               Department of Economics
106 Fisher Hall                                       111 Fisher Hall
Princeton University                                  Princeton University
Princeton, NJ 08544-1021                              Princeton, NJ 08544-1021
and NBER                                              and NBER
svensson@princeton.edu                                woodford@princeton.edu
1 Introduction

Monetary policy is inevitably conducted under considerable uncertainty about the state of the
economy and the nature of recent disturbances. Analyses of optimal policy that take no account
of this are therefore of doubtful practical utility. However, in the case of purely backward-
looking models of the kind exclusively used by central banks prior to the 1990s, powerful general
principles for efficient estimation of the state of the economy and for determining the optimal
use to make of such estimates have been well-understood since at least the 1970s. In the case of a
linear economic model, a quadratic loss function for the policymaker, uncertainty only about the
state of the economy (that is, the current values of specific additive terms in the economic model),
and Gaussian disturbances, a principle of certainty equivalence applies: the optimal policy is the
same as if the state of the economy were fully observable, except that one responds to an efficient
estimate of the state of the economy rather than to its actual value. Moreover, a separation
principle applies, according to which the determination of the optimal response coefficients to
be applied to one’s estimate of the state of the economy (the optimization problem) and the
estimation of the current state of the economy (the estimation or signal-extraction problem)
can be treated as separate problems. The optimal response coefficients are independent of the
specification of the central bank’s incomplete information; and the optimal weights to place
on alternative indicators in estimating the state of the economy are independent of the central
bank’s objective function.1
      However, the presence of forward-looking variables in the system to be controlled — a com-
mon feature of modern macroeconomic models, including the econometric models now used by
many central banks — complicates matters in a number of respects. For example, optimal policy
under commitment ceases in general to coincide with the outcome of discretionary optimization,
as demonstrated for the general linear model with quadratic objectives in Backus and Driffill [1]
and Currie and Levine [3]. Optimal policy under commitment (even in the deterministic case) is
no longer a function solely of the vector of predetermined variables that suffices to characterize
the set of possible future paths for the economy from a given date onward; thus one cannot
expect that in the case of partial information optimal policy can depend solely on the optimal
estimate of such a vector of predetermined variables.
      Moreover, in the presence of partial information, estimation of the current state of the system
is no longer so simple. For currently observable variables will generally depend not only on the
  1
      Important early treatments include Chow [2], Kalchbrenner and Tinsley [4], and Leroy and Waud [5].


                                                      1
current vector of predetermined variables and random observation error with known properties,
but also upon forward-looking variables, the values of which will depend on the private sector’s
expectations about future policy. This makes it far from obvious that a separation principle
should apply, even in a linear-quadratic Gaussian framework. Because the relation between the
unobserved state of the economy and the observable variables depends on expected policy, one
may not be able to solve the optimal filtering problem independently of the solution for the
optimal policy response to the estimated state of the economy.
   Nonetheless, analogs of the classical control-theoretic results have been obtained for certain
special kinds of forward-looking models with partial information. With regard to the estima-
tion problem, Pearlman, Currie and Levine [8] have shown in a linear (non-optimizing) model
with forward-looking variables and partial information that the state of the economy can still
be estimated using a Kalman filter, although the solution is much more complex than in the
purely backward-looking case. Pearlman [7] has used this solution in an optimizing model to
demonstrate that certainty equivalence applies under both discretion and commitment in the
presence of forward-looking variables and symmetric partial information, that is, in the case
that both the central bank and the private sector have access to the same partial information.
In the case of commitment, “certainty equivalence” means that the optimal instrument settings
are the same linear function of the current estimate of the predetermined variables describing
the state of the economy and specific Lagrange multipliers (related to the value that alterna-
tive expectations would have had in the previous period’s policy problem) as in the case of the
corresponding optimal policy problem under certainty.
   Our previous paper [9] synthesizes what is known about the case of symmetric partial in-
formation, and derives useful general formulas for computation of the optimal policy response
coefficients and efficient estimates of the state of the economy in the context of a fairly general
forward-looking (rational-expectations) model. We find that not only does certainty equivalence
continue to characterize optimal policy, but that a separation of the problems of optimal policy
and optimal estimation of the current state of the economy continues to be possible, in that the
coefficients of the optimal Kalman filter are again independent of the central bank’s objective
function.
   The proof of certainty equivalence under commitment was not included in [9], in order to
save space. The present paper provides this proof. The proof is for a more general model than
in Pearlman [7] and more intuitive. In particular, our proof explicitly takes into account that,


                                                2
under commitment, the policymaker can affect the future evolution of the observable variables,
and thereby potentially affect the future information available.
   Section 2 lays out the model, section 3 demonestrates certainty equivalence for the case
of full information, and section 4 proves certainty equivalence for partial information. Section
5 outlines the separation principle, and section 6 concludes. Appendix A contains technical
details regarding the degree to which the policymaker can affect the information revealed by the
observable variables.


2 The model

Consider the linear model
                                                                                      
                   Xt+1          1
                                      Xt   2
                                               Xt|t           ut+1 
                               =A     +A        + Bit +       ,                           (2.1)
                      Qxt+1|t         xt       xt|t              0

where Xt is a vector of nX predetermined variables, xt is a vector of nx forward-looking variables,
it is a vector of the central bank’s ni policy instruments, ut is a vector of nX iid shocks with mean
zero and covariance matrix Σuu , and A1 , A2 , B and Q are matrices of appropriate dimension.
The nx × nx matrix Q may be singular (this is a slight generalization of usual formulations when
Q is the identity matrix). For any variable zt , zτ |t denotes E[zτ |It ], the rational expectation
(the best estimate) of zτ given the information It , the information available in period t. The
information is specified below. Let Yt denote a vector of nY target variables given by
                                                                               
                                           Xt   2
                                                     Xt|t 
                                Yt = C 1      +C        + Ci it ,                             (2.2)
                                                 xt                       xt|t

and let
                                                  Lt = Yt0 W Yt                                   (2.3)

be a period loss function, where W is a positive-semidefinite weight matrix.
   Let the vector of nZ observable variables, Zt , be given by
                                                                                  
                                              Xt   2
                                                        Xt|t 
                                Zt = D1          +D        + vt ,                             (2.4)
                                                  xt                       xt|t

where vt , the vector of noise, is iid with mean zero and covariance matrix Σvv . The information
It in period t is given by

                It = {Zτ , A1 , A2 , B, C 1 , C 2 , Ci , D1 , D2 , Q, W, δ, Σuu , Σvv ; τ ≤ t},   (2.5)

                                                              3
where δ (0 < δ < 1) is a discount factor. This incorporates the case when some or all of the
predetermined and forward-looking variables are observable.
   Note that (2.1) assumes that the expectations xt+1|t in the second block of equations are
conditional on the information It . The case when these expectations are replaced by a private
sector expectations E[xt+1 |Itp ] where the private-sector information Itp differs from It is treated
in Svensson and Woodford [10].
   Suppose that the central bank commits itself in an initial ex ante state (prior to the real-
ization of any period zero random variables) to a state-contingent plan for the indefinite future
that minimizes the expected discounted losses
                                                         ∞
                                                         X
                                                     E         δ t Lt .
                                                         t=0

Here E indicates the expectation with respect to information in the initial state, in which the
commitment is made. It is important to consider optimal commitment from such an ex ante
perspective, because, in the case of partial information, the information that the central bank
possesses in any given state depends upon the way that it has committed itself to behave in
other states that might have occurred instead.
   We begin by reviewing the form of the commitment equilibrium under full information, when
Zt includes all elements of Xt and xt . We then turn to the consequences of partial information,
when the information is given by (2.5).


3 Certainty equivalence under full information

Note that in the case of full information, Xt|t = Xt , xt|t = xt , as a result of which it is obvious
that only the aggregated matrices A ≡ A1 + A2 and C ≡ C 1 + C 2 matter to the optimization
problem.
   The Lagrangian for the commitment problem can be written in the form
                 "∞               ∞
                  X               X
       L = E           δ t Lt −         δ t ξ 0t+1 (Xt+1 − A11 Xt − A12 xt − B1 it − ut+1 )
                  t=0             t=0
                     ∞
                                                                                                                #
                    X
                  −         δ t Ξ0t (QEt xt+1   − A21 Xt − A22 xt − B2 it ) −   δ −1 ξ 00 (X0   − X̄0 − u0 ),
                      t=0

where in each period t, ξ t and Ξt are vectors of Lagrange multipliers conformable to Xt and xt
respectively, and where Et [·] ≡ E[·|Itf ] denotes expectations conditional on the full information,
Itf . The dating of the multipliers indicate the period information with which they are measurable


                                                           4
(that is, depend on). Thus, the constraint for Xt+1 , the predetermined variables, depends on
                                        f
information available in period t + 1, It+1 , whereas the constraint for QEt xt+1 , the forward-
looking variables, depends on the information available in period t, Itf . (That is, there is only
one such latter constraint for each information Itf , that may be reached in period t, so there is
only one vector of multipliers Ξt for each Itf ; in other words, Ξt depends only on the information
available in period t.) The final term on the right-hand side corresponds to the constraint
imposed by the vector of initial conditions on X0 ,

                                                     X0 = X̄0 + u0 ,                                                                  (3.1)

where X̄0 is known at the time of commitment.
       Using the law of iterated expectations (EEt xt+1 = Ext+1 )2 , we may equivalently write the
Lagrangian in the form
           (∞                                                                                                                          )
            X         h                                                                                  i
 L=E              t
                  δ Lt +   (ξ 0t+1 , Ξ0t )(Ayt   + Bit +   [u0t+1        0 0
                                                                     0])−δ      −1                  ˆt
                                                                                     (ξ 0t , Ξ0t−1 )Iy       +   δ −1 ξ 00 (X̄0   + u0 ) ,
            t=0
                                                                                                                                      (3.2)
where                                                                    
                                                                Xt 
                                                      yt ≡         ,
                                                                    xt
and                                                                       
                                                              I     0 
                                                     Iˆ ≡                 .
                                                               0 Q

We have added a term − δ −1 Ξ0−1 Qx0 to the right-hand side, for the sake of symmetry in notation,
but now correspondingly stipulate the initial condition

                                                           Ξ−1 = 0.                                                                   (3.3)

(Note that these Lagrange multipliers do not correspond to any actual constraint upon the
planning problem.) Finally, note that equations (2.2)–(2.3) define a quadratic function Lt =
L(yt , yt|t , it ). Because yt|t = yt in the case of full information, we can here write Lt = L(yt , yt , it ).
Thus the Lagrangian (3.2) is a quadratic function of the evolution of the vectors yt and it .
       Differentiation of (3.2) with respect to yt and it then yields the first-order conditions

                                                                                      ˆ
                                      Lyt + Et (ξ 0t+1 , Ξ0t )A = δ −1 (ξ 0t , Ξ0t−1 )I,                                              (3.4)
   2
     More precisely, E[Ξt Et xt+1 ] = EEt [Ξt xt+1 ] = E[Ξt xt+1 ] (where the first equality follows since Ξt is measur-
able with respect to Itf ).


                                                                5
                                                          Lit + Et (ξ 0t+1 , Ξ0t )B = 0,                                                             (3.5)

respectively, where for each of the two arguments z = y, i, Lzt ≡ ∂L(yt , yt , it )/∂zt . Recalling the
form of the quadratic function L, we have
                                                                                                                                             
                        ·             ¸                      ·            ¸                         ·                 ¸
                                             C0                              yt   1                                    Lyy     Lyi   yt 
   L(yt , yt , it ) =       yt0 i0t                  W         C Ci             ≡                    yt0 i0t                                 ,
                                              Ci0                                 it            2                             Liy   Lii        it
                                                                                                                                                     (3.6)
so that we can write                                                                               
                                                     0
                                                   Lyt           Lyy Lyi   yt 
                                                            =                                      ,
                                                      L0it            Liy         Lii           it

where the Ljk are matrices of constant coefficients (corresponding to the second partial deriva-
tives of L), that depend only upon the matrices C, Ci , and W as above. Using this notation, we
can equivalently write the first-order conditions (3.4) – (3.5) as
                                                                                                                    
                                                                       ξ t+1     −1 0 
                                                                                                                 ξt    
                                 Lyy yt + Lyi it + A0 Et                      = δ Iˆ                                ,                            (3.7)
                                                                          Ξt                                 Ξt−1
                                                                                               
                                                                                   ξ t+1 
                                                  Liy yt + Lii it + B 0 Et                      = 0.                                               (3.8)
                                                                                        Ξt

   Assuming that Lii is of full rank (see Svensson and Woodford [10, appendix B] for a the case
when Lii is not of full rank), we can solve (3.8) for it , obtaining
                                                                                                            
                                                                                             ξ t+1 
                                              it = −L−1           −1 0
                                                     ii Liy yt − Lii B Et                          .                                               (3.9)
                                                                                                    Ξt

   Substituting (3.9) into (2.1) and (3.7) to eliminate it , we then obtain a system of equations
for the evolution of yt and (ξ 0t+1 , Ξ0t )0 , that can be written in the form
                                                                                                                             
                                                    Et yt+1                                                       yt
                                                                       
                         0     R0               V δ −1 Iˆ0           
                                                
                                         ξ t+1   =             
                                                                   ξt  
                                                                         ,                                                                     (3.10)
                             Iˆ U  Et              R  0               
                                          Ξt                          Ξt−1

where

                                                      R ≡ A − BL−1
                                                                ii Liy ,

                                                      U      ≡ BL−1  0
                                                                 ii B ,

                                                      V      ≡ − Lyy + Lyi L−1
                                                                            ii Liy .


                                                                          6
Here it is worth noting that U and V are symmetric matrices.
       Let us assume furthermore that the square matrix on the left-hand side of (3.10) is of full
rank.3 Then we can invert this matrix, to obtain a system of the form
                                                                                   
                                          Et yt+1                         yt       
                                                                                   
                                          Eξ       = M                    ξt       .                                  (3.11)
                                          t t+1                                    
                                                                                   
                                                   Ξt                       Ξt−1

We then wish to consider solutions to (3.11) that are consistent with given initial values for
X0 and Ξ−1 according to (3.1) and (3.3). We note that the number of variables in (3.11) is
2(nX + nx ), where nX and nx is the dimension of Xt and xt , respectively, and that there are
nX + nx initial conditions ((3.1) and (3.3)). We shall restrict our attention to bounded solutions,
by which we mean solutions in which for any t, Et yt+τ , Et ξ t+τ and Et Ξt+τ −1 satisfy a uniform
bound for all τ . Such solutions necessarily satisfy the transversality condition for an optimal
plan, and since our equations (2.1)–(2.4) will usually represent only a local approximation to
the true structural equations and true loss function, unbounded solutions need not correspond
at all closely to solutions to the true equations.
       As usual (and ignoring non-generic cases), there is a unique bounded solution to (3.11)
consistent with the initial conditions if the number of eigenvalues of M inside the unit circle
(that is, with modulus less than one) is exactly equal to the number of initial conditions, nX +nx .
The eigenvalues λ of M are the roots of the characteristic equation
                                                                                  
                                                                −1 ˆ0
                                                   V       δ       I −     λR0    
                                      Det                                          = 0.                                 (3.12)
                                              R − λIˆ               −λU

Multiplication of the right blocks of this matrix by −λ−1 , then multiplication of the lower blocks
by −λ−1 δ −1 , and finally transposition of the matrix does not change the sign of its determinant.
Thus we may equivalently write
                                                                                              
                                                                −1 ˆ0        −1 −1
                                             V             δ       I −λ          δ       R0   
                             Det                                                               = 0.
                                              −1 −1 ˆ                   −1 −1
                                      R−λ          δ    I           −λ       δ     U

Comparison of this with (3.12) shows that if λ is a root, λ−1 δ −1 must also be. It follows that M
has as many eigenvalues with |λ| >           √1    as with |λ| <         √1 .     Thus, since      √1    > 1, at most half of the
                                               δ                           δ                         δ
   3
     Even when Q is singular, so that this matrix also is, our conclusions below remain essentially valid. Equation
(3.12) is still the relevant characteristic equation, and again there is a unique bounded solution, in the generic
case, if and only if exactly nX + nx roots are inside the unit circle. Furthermore, it is again true that there are
necessarily no more than this number of such roots, and that for δ close enough to 1, the condition assumed here
almost inevitably holds. However, we omit the algebra for the more general case.


                                                                7
eigenvalues (that is, at most nX + nx ) are inside the unit circle (that is, with |λ| < 1), so there
is no possibility of multiple stationary solutions to (3.11). If δ is close to 1 (as will often be the
case), there are likely to be exactly half inside the unit circle. We shall assume this condition
from now on. Thus (3.11) has a unique bounded solution in which Et yt+τ and Et (ξ 0t+1 , Ξ0t )0 can
be expressed as linear functions of the initial conditions X0 and (3.3), for arbitrary τ ≥ 0.
     In particular, the optimal equilibrium involves evolution of the instrument according to a
relation of the form
                                         it = F Xt + ΦΞt−1 ,                                   (3.13)

the optimal reaction function in state-space form, where F and Φ are matrices of constant
coefficients. We have just argued that y0 and E0 ξ 1 can be expressed as linear functions of X0
and Ξ−1 ≡ 0; substitutions of these solutions into (3.9) then yields (3.13) for t = 0. However,
exactly the same reasoning can be applied to solve equations (3.11) for all τ ≥ t, given initial
values Xt and Ξt−1 , and the unique bounded solution will be linear in the initial values, with
exactly the same coefficients in period t = 0. Thus (3.13) must hold for all t.
     Similarly, the forward-looking variables evolve according to a relation of the form

                                        xt = GXt + ΓΞt−1 ,                                     (3.14)

while the Lagrange multipliers associated with the forward-looking variables evolve according
to
                                        Ξt = SXt + ΣΞt−1 ,                                     (3.15)

starting from the initial condition (3.3). Substitution of these equations into (2.1) then implies
that the predetermined state variables evolve according to

                  Xt+1 = (A11 + A12 G + B1 F )Xt + (A12 Γ + B1 Φ)Ξt−1 + ut+1 ,                 (3.16)

starting from the initial condition X0 and (3.3). Note that (3.15) can be integrated to yield
                                                 t
                                                 X
                                        Ξt =            Στ SXt−τ .
                                                 τ =0

Thus, we have
                                                 t
                                                 X
                                          it =          Fτ Xt−τ ,                              (3.17)
                                                 τ =0

the optimal reaction function in integrative form, where F0 ≡ F and Fτ ≡ ΦΣτ −1 S, τ ≥ 1. Thus
the most fundamental difference with respect to the discretion case is that under the optimal

                                                        8
commitment, it (and xt ) are no longer a linear function of the current state Xt alone, but instead
depends upon past state vectors Xt−τ as well. The inertial character of optimal policy that this
can result in is illustrated in Woodford [11].
   Equations (3.13)–(3.16) then completely describe the evolution of the predetermined vari-
ables, the forward-looking variables, and the policy settings it , as a function of the sequence of
realizations of the disturbances ut (and the initial conditions (3.1) and (3.3)). Note that (3.15)
implies that the Lagrange multipliers Ξt are predetermined variables.
   Note also that the matrices F, G, S, Φ, Γ, Σ depend on A, B, Q, C, Ci , W and δ, but that
they are independent of Σuu . Thus these coefficients are the same as in the optimal plan under
certainty. This is the certainty equivalence result for the case of full information.


4 Certainty equivalence under partial information

Now suppose instead that both the private sector and the central bank observe only the variables
Zt in period t, that is, have the information It rather than Itf . In this case the Lagrangian takes
the form
           "∞
            X                    ∞
                                 X                ³                                                                                       ´
L = E           δ t Lt −                δ t ξ 0t+1 Xt+1 − A111 Xt − A211 Xt|t − A112 xt − A212 xt|t − B1 it − ut+1
           t=0                    t=0
             ∞
                                                                                                                                                 #
             X
                         t
           −         δ       ψ 0t (xt+1|t   −   A121 Xt   −   A221 Xt|t   −   A122 xt   −   A222 xt|t   − B2 it ) −   δ −1 ξ 00 (X0   − X̄0 − u0 ) .
               t=0

We now distinguish between zt+1|t ≡ E[zt+1 | It ] and Et zt+1 ≡ E[zt+1 | Itf ], the expectation (of
any variable zt+1 ) conditional upon all Xτ , xτ and Zτ for all τ ≤ t. Note also that now the
distinction between the two components A1 and A2 is relevant for the problem’s constraints.
Now the multipliers ψ t are measurable with respect to the full period t information, Itf , as
the term in brackets represents a constraint that applies in period t. However, they are not
necessarily measurable with respect to It (that is, they do not necessarily depend only on It ) as
there is not a single constraint for each information It . Thus, ψ t 6= ψ t|t .
   Note also that we do not write explicitly, in the Lagrangian, the constraints indicating the way
in which the choice of stochastic processes for Xt , xt , and it affects the conditioning information
in the various conditional expectations zτ |t , as a result of (2.4). As shown in Appendix A, we
obtain the correct first-order conditions even when the additional constraints (discussed there)
are omitted.



                                                                          9
   This expression can be simplified, if we note that
      h                 i       n h                     io       h              i           n h                   io
    E ψ 0t xt+1|t = E E ψ 0t xt+1|t | It                     = E ψ 0t|t xt+1|t = E E ψ 0t|t xt+1 | It                  = E [Ξt xt+1 ] ,

using the law of iterated expectations and introducing Ξt ≡ ψ t|t . We can then equivalently
express the Lagrangian as
       (∞                                                                                                                                               )
        X           h                                                                                                     i
L=E             t
                δ Lt +         (ξ 0t+1 , Ξ0t )(A1 yt   + Bit ) +   (ξ 0t+1|t , Ξ0t )A2 yt   −δ   −1                  ˆt
                                                                                                      (ξ 0t , Ξ0t−1 )Iy       +   δ −1 ξ 00 (X̄0   + u0 ) ,
          t=0
                                                                                                                                               (4.1)
once again stipulating the initial condition (3.3). It should also be noted that now it must
be measurable with respect to the information It . Note also that Ξt ≡ ψ t|t is measurable with
respect to It , even though ψ t is not.
   Differentiation of (4.1) then yields the first-order conditions

                                                                                                           ˆ
                            L1t + L2t|t + Et (ξ 0t+1 , Ξ0t )A1 + (ξ 0t+1|t , Ξ0t )A2 = δ −1 (ξ 0t , Ξ0t−1 )I,                                  (4.2)

                                                       Lit|t + (ξ 0t+1|t , Ξ0t )B = 0,                                                         (4.3)

where for j = 1, 2, i, Ljt denotes the partial derivative of L(yt , yt|t , it ) with respect to its first,
second, or third argument respectively. Here we have used result (A.14) from appendix A to
differentiate functions of yt|t with respect to yt . More precisely, condition (4.2) should also
contain a term that is proportional to µ0t , the vector of Lagrange multipliers associated with the
constraint on how changes in the evolution of yt affect the information content of the observables
Zτ in (2.4), as shown in appendix A. However, as is established there in (A.19), µt|t = 0, as a
result of which the neglected term has no consequences for condition (4.4) below, which is all
that matters for our subsequent analysis.
   Note that the expectations in (4.2) and (4.3) are not conditioned upon the same information,
because yt may take a different value in each state of the world in period t, while it must have
the same value in each state of the world that corresponds to the same information It . (The
consequences of the latter constraint are also treated in the Appendix.) Finally, note that in
the case of full information, conditions (4.2)–(4.3) are identical to (3.7)–(3.8).
   As it turns out, only the conditional expectations of these first-order conditions with respect
to public information It matter for determination of the optimal evolution of yt and it . Each
term in (4.3) is already conditional upon It . However, taking the conditional expectation of (4.2)
with respect to It , we obtain the simpler expression

                                                                                                ˆ
                                       L1t|t + L2t|t + (ξ 0t+1|t , Ξ0t )A = δ −1 (ξ 0t , Ξ0t−1 )I.                                             (4.4)

                                                                     10
Furthermore, a calculation similar to (3.6) implies that
                                                                                           
                                L01t|t   +   L02t|t     Lyy               Lyi   yt|t 
                                                      =                                    ,
                                      L0it|t                       Liy       Lii         it|t

where the matrices Ljk are exactly the same as in (3.6). Thus conditions (4.3) and (4.4) can
alternatively be written                                                           
                                                                      ξ
                                                                   0  t+1|t 
                                   Liy yt|t + Lii it + B                            = 0,                (4.5)
                                                                             Ξt
                                                                                                  
                                                  ξ t+1|t     −1 0 
                                                                        ξ t|t 
                          Lyy yt|t + Lyi it + A0           = δ Iˆ          .                          (4.6)
                                                      Ξt               Ξt−1

   The pair of difference equations (4.5) and (4.6) will be observed to be of exactly the same
form as (3.7) and (3.8) in the full-information case, except that conditional expectations are
now with respect to It rather than Itf . Thus, exactly in the same way as above, we can obtain
a system of equations                                                            
                                           yt+1|t              yt|t              
                                                                                 
                                           ξ               = M ξ                 ,                    (4.7)
                                           t+1|t               t|t               
                                                                                 
                                                 Ξt                          Ξt−1
where the matrix M is the same as in (3.11).
   As above (under our assumption about the eigenvalues of M), there is a unique bounded
solution for yτ |t , ξ τ |t and Ξτ −1|t (τ ≥ t), given any initial values Xt|t and Ξt−1 . Solving (4.5) for
it , we can associate with any such solution to (4.7) a unique bounded solution for iτ |t as well.
This solution satisfies
                                                it = F Xt|t + ΦΞt−1 ,                                     (4.8)

                                               xt|t = GXt|t + ΓΞt−1 ,                                     (4.9)

                                                Ξt = SXt|t + ΣΞt−1 ,                                     (4.10)

where the matrices F, G, S, Φ, Γ, Σ are the same as in (3.13)–(3.15) for the full-information case.
Equation (4.10) can be integrated to yield Ξt as a distributed lag of past values of Xt−τ |t−τ ;
thus, using this with (4.8) , the optimal reaction function in state-space form, results in
                                                       t
                                                       X
                                                it =           Fτ Xt−τ |t−τ ,                            (4.11)
                                                       τ =0




                                                               11
the optimal reaction function in integrated form, where Fτ , τ ≥ 0, are the same as for the
full-information case. Again, a certain amount of inertia is introduced into the way that Xt|t
determines it (and xt ).
   Note that equations (4.8)–(4.10) and (4.11) take exactly the same form as (3.13)–(3.15) and
(3.17), once expectations conditional upon Itf are replaced by expectations conditional upon It .
This represents an extension of certainty equivalence to the partial-information case.


5 The separation principle

The second row of (2.1) implies that

                              A121 (Xt − Xt|t ) + A122 (xt − xt|t ) = 0.                     (5.1)

Assuming that A122 is non-singular, this can be solved for xt . Substituting (4.9) for xt|t , one
obtains
                                  xt = G1 Xt + G2 Xt|t + ΓΞt−1 ,                             (5.2)

where again
                                       G1 ≡ −(A122 )−1 A121 ,

                                          G2 ≡ G − G1 .

Note that the matrices G1 and G2 , like the others, are independent of the specifications of D,
Σuu and Σvv .
   Substitution of (4.8), (4.9) and (5.2) into the first row of (2.1) furthermore yields

                             Xt+1 = HXt + JXt|t + ΨΞt−1 + ut+1 ,                             (5.3)

where H and J are given by

                             H ≡ A111 + A112 G1 ,                                            (5.4)

                              J   ≡ B1 F + A112 G2 + A211 + A212 G,                          (5.5)

and
                                        Ψ ≡ A12 Γ + B1 Φ.                                    (5.6)

Equations (4.10) and (5.2)–(5.3) then describe the evolution of the state variables yt in equilib-
rium, once we determine the evolution of the estimates Xt|t of the predetermined variables.


                                                 12
   Substituting (5.2) into (2.4), we obtain

                               Zt = LXt + M Xt|t + ΛΞt−1 + vt ,                          (5.7)

where L and M are given by

                                    L ≡ D11 + D21 G1 ,                                   (5.8)

                                    M   ≡ D21 G2 + D12 + D22 G,                          (5.9)

and
                                              Λ ≡ D2 Γ.                                 (5.10)

Equations (5.3) and (5.7) are then the transition and measurement equations for an optimal
filtering problem. Again the transformation into a problem without forward-looking variables
allows us to derive the estimation equations in a manner that is simpler than that used in
Pearlman, Currie and Levine [8].
   As demonstrated in more detail in Svensson and Woodford [9], the optimal prediction of Xt|t
is then given by a Kalman filter,

                      Xt|t = Xt|t−1 + K(Zt − LXt|t−1 − M Xt|t − ΛΞt−1 ),                (5.11)

We can rationalize (5.11) by observing that Zt − M Xt|t − ΛΞt−1 = LXt + vt , hence,

                         Zt − LXt|t−1 − M Xt|t = L(Xt − Xt|t−1 ) + vt ,

so (5.11) can be written in the conventional form

                            Xt|t = Xt|t−1 + K[L(Xt − Xt|t−1 ) + vt ],                   (5.12)

which allows us to identify K as (one form of) the Kalman gain matrix. From (5.3) we get

                                Xt+1|t = (H + J)Xt|t + ΨΞt−1 ,                          (5.13)

and the dynamics of the model are given by (4.10), (5.2), (5.3), (5.11) and (5.13).
   The Kalman gain matrix is given by

                                    K = P L0 (LP L0 + Σvv )−1 ,                         (5.14)

where the matrix P ≡ Cov[Xt − Xt|t−1 ] is the covariance matrix for the prediction errors Xt −
Xt|t−1 and fulfills
                        P = H[P − P L0 (LP L0 + Σvv )−1 LP ]H 0 + Σuu .                 (5.15)

                                                 13
Thus P can be solved from (5.15), either numerically or analytically, depending upon the com-
plexity of the matrices H, L and Σuu . Then K is given by (5.14). Note that this implies that
the Kalman gain K depends only upon the matrices A, Σuu , D, and Σvv .


6 Conclusions

The above proof demonstrates the certainty-equivalence result that the optimal policy under
commitment given an estimate of the state of the economy is independent of the degree of
uncertainty and hence the same policy as under full information. Furthermore, a separation
principle holds, in that the problem of finding the optimal policy and the problem of optimally
estimating the current state of the economy can be treated as separate problems (in particular,
the optimal estimation does not depend on the loss function or the reaction function).
   These results hold under symmetric partial information about the economy. As demonstrated
in Svensson and Woodford [10], in the asymmetric case in which the policymaker has partial
information and the private sector has full information, the certainty-equivalence result holds
only for the reaction function in state-space form, (4.8), but not for the reaction function in
integrative form, (4.11). Furthermore, the separation principle does not hold, since the optimal
estimation is no longer independent of the loss-function parameters and the reaction function.


A Differentiation results for conditional expectations

Here we address some technical issues that arise in the characterization of the optimal commit-
ment problem in the case of partial information. These relate to the fact that the policymaker
should recognize that his or her pattern of action affects the statistical relation between the
observables and underlying (exogenous) shocks, and thus might affect the information that is
publicly available about those shocks. This problem can be ignored in the case of discretion
because an independent optimization problem is solved in each state, and we may suppose that
behavior in any single state is of only negligible importance for the correlations that determine
the optimal linear estimates of unobserved states. But we must consider the matter more care-
fully in the case of commitment. In fact, we show here that the results presented in the text,
derived without taking into account the effects of policy upon the content of public information,
continue to be correct.




                                               14
   We wish to consider the problem of minimizing a discounted sum of expected losses
                                              ∞
                                              X
                                          E         δ t L(yt , yt|t , it )                (A.1)
                                              t=0

subject to a series of constraints of the form

                        M0 yt+1 + M1 yt+1|t = M2 yt + M3 yt|t + M4 it + ut+1              (A.2)

for each period t ≥ 0, where M0 , ..., M4 are matrices of appropriate dimension. Here it is
a vector of control variables chosen in period t, yt is a vector of endogenous state variables
determined by structural equations (A.2), and ut+1 of random disturbances in period t + 1,
assumed independently distributed over time. (For present purposes it is not necessary to
distinguish between the predetermined and non-predetermined elements of yt .) There is also a
set of initial conditions specifying
                                               M0 y0 = u0 ,                               (A.3)

where u0 is another random vector, distributed independently of the ut+1 vectors for all later
periods.
   For any random variable zT , the conditional expectation zT |t denotes

                                   zT |t ≡ E[zT | Zt , Zt−1 , . . . , Z0 ],               (A.4)

where Zt is a vector of observables in period t, implicitly defined by

                                       Zt = D1 yt + D2 yt|t + vt ,                        (A.5)

where vt is a vector of additional iid random disturbances. We introduce the notation ūt ≡
(u0t , vt0 )0 for the complete vector of exogenous random disturbances in period t. The controls
must be chosen on the basis of public information, so that

                                                    it = it|t                             (A.6)

is also a constraint.
   We may incorporate constraint (A.6) by instead writing the objective (A.1) as
                                            ∞
                                            X
                                        E         δ t L(yt , yt|t , it|t ),               (A.7)
                                            t=0

and making a similar substitution into (A.2). Then substituting (A.4) into (A.7) for the con-
ditional expectations, we obtain an objective that depends upon the state-contingent paths of

                                                        15
the variables yt , it and Zt . Our problem is then to choose yt , it and, in particular, Zt in each
period t ≥ 0, as functions of the history of realizations, the state st ≡ (ū0t , ū0t−1 , . . . , ū00 )0 , subject
to constraints (A.2), (A.3), and (A.5), so as to minimize (A.7). This will only determine the
path of it|t , but by then imposing (A.6) as well we can determine the complete evolution of the
control it .
    The first-order conditions for such a problem can, as usual, be obtained by writing a La-
grangian
                   ∞
                   X
          L =            δ t EL(yt , yt|t , it|t )
                   t=0
                             ∞
                             X
                         −         δ t Eϕ0t+1 (M0 yt+1 + M1 yt+1|t − M2 yt − M3 yt|t − M4 it|t − ut+1 )         (A.8)
                             t=0
                                                                ∞
                                                                X
                              −1
                         −δ        Eϕ00 (M0 y0       − u0 ) −         δ t Eµ0t (Zt − D1 yt − D2 yt|t − vt ),    (A.9)
                                                                t=0

where ϕt+1 , ϕ0 , and µt are the Lagrange multipliers associated with constraints (A.2), (A.3),
and (A.5) respectively. (Thus, ϕt corresponds to (ξ 0t , ψ 0t−1 ) in the text. There is a separate
vector of multipliers ϕt+1 for each possible state st+1 ≡ (ū0t+1 , ū0t , . . . , ū00 )0 , a separate ϕ0 for
each possible s0 ≡ ū0 , and a separate µt for each possible state st . The expectation operator
E[·] indicates unconditional expectations (expectations in the ex ante state in which the optimal
commitment is chosen). The commitment is chosen prior to the realization of any period 0
states, as the systematic pattern of behavior committed to in period 0 affects the information
revealed in alternative states in that period, as in others. The aim of this appendix is to explain
the calculation of the partial derivatives of such a Lagrangian with respect to the random paths
specified for yt , it and Zt .


A.1 Properties of the conditional expectation zT |t

We first note, that the conditional expectation (A.4) is a linear operator of the form

                                          zT |t (st ) ≡ EsT [Pt,T (st , sT )zT (sT )],                         (A.10)

where Pt,T (st , sT ) is a (scalar) kernel. Here sT ≡ (ū0T , . . . , ū00 )0 is an arbitrary state in period
T , st is an arbitrary state in period t ≤ T, and EsT denotes the expectation over the possible
states sT , under the ex ante or unconditional probability measure. (Note that EsT [·] should not
be confused with Et [·] ≡ E[·|Itf ], the expectation conditional on the full information in period
t, Itf .) Under our assumption of normally distributed disturbances, the conditional expectation

                                                                 16
is simply a linear projection upon the observables. This means that the stochastic kernel takes
the form
                                           Pt,T (st , sT ) = Pt,t (st , sT,t ),

where sT,t is the predecessor of state sT in period t (that is, the part of the history that has oc-
curred as of period t, that is, the unique state sT,t in period t that fulfills sT ≡ (ū0T , ..., ū0t+1 , s0T,t )0 ).
Furthermore, Pt,t can be expressed in the standard form for linear projections,

                                    Pt,t (s1t , s2t ) = Ẑt0 (s2t )E[Ẑt Ẑt0 ]−1 Ẑt (s1t ),                 (A.11)

where s1t and s2t are two states in period t and Ẑt (st ) is a vector of observables in period t and
earlier (including a constant) that results under st and spans the entire public information space
in period t, but that includes no redundant variables (so that the matrix E[Ẑt Ẑt0 ] is non-singular).
                                                                                                ¡ ¢
(Note that if s1t and s2t correspond to the same information It , we have Ẑt s1t = Ẑt (s2t ). Also,
                                                     0 , ..., Z 0 )0 .)
if there are no redundant observables, Ẑt = (Zt0 , Zt−1       0

    The choice of a particular representation Ẑt need not concern us here. We do, however, wish
to observe certain consequences of the general form (A.11) for the stochastic kernel. One is that
the kernel is symmetric, that is,

                                             Pt,t (s1t , s2t ) = Pt,t (s2t , s1t ).                           (A.12)

Another is that, for each fixed state s2t , the random variable Pt,t (·, s2t ) is a linear combination
of the vector of observables as of period t, so that Pt,t (·, s2t ) is in the linear space spanned by
the vector of observables in period t. More generally, for any period T ≥ t and any state sT in
period T , Pt,T (·, sT ) is a random variable that is measurable with respect to the information It .
These properties of the kernel suffice for the calculations that we need to do here.


A.2 Differentiation of expressions involving conditional expectations

We now consider differentiation of expressions involving conditional expectations, of the sort that
appear in the Lagrangian (A.9). We first consider the effect upon an expression of the form zT |t
of varying the state-contingent path of random variable zT , holding fixed the state-contingent
paths of the variables (Zt , . . . , Z0 ), and hence keeping the stochastic kernel Pt,T unchanged.
Suppose we let
                                                    zT = z̄T + φν T ,                                         (A.13)



                                                              17
where z̄T is the value of the random variable at which we wish to evaluate the partial derivative,
φ is a scalar constant, and ν T is another random variable (that is, another function of the state
in period T , sT ) indicating the direction in which we wish to vary zT . Then we define the partial
derivative of a functional g(zT ) with respect to perturbations of the random variable zT as the
function [∂g(zT )/∂zT ](sT ) with the property that
                                                          ·                          ¸
                                      ∂g(z̄T )       ∂g(z̄T )
                                               = EsT          (sT )ν T (sT )
                                        ∂φ            ∂zT
in the case of any perturbation of the form (A.13), that is, for any random variable ν T in period
T.
     From (A.10), we then can observe that
                                           ∂zT |t (st )
                                                        (sT ) = Pt,T (st , sT ),                          (A.14)
                                             ∂zT
where st is any state in period t ≤ T . By substituting (A.13) into (A.10), differentiating with
respect to φ and using (A.14), it follows that, for any process ν T ,
                       "                             #
                           ∂zT |t (st )
                 EsT                    (sT )ν T (sT ) = EsT [Pt,T (st , sT )ν T (sT )] = ν T |t (st ).   (A.15)
                             ∂zT

Note that (A.15) is just the conditional expectation with respect to It of the expression that
would be obtained if one were differentiating zT instead of zT |t . This is intuitive since, by the
law of iterated expectations,

                                      E[zT |t ν T ] = E[zT |t ν T |t ] = E[zT ν T |t ].

Thus the partial derivatives of each of these equivalent expressions with respect to zT should be
the same; but the partial derivative of the final expression is obviously ν T |t .


A.3 The effect of variation in the dependence of Zt on the state st

We turn next to the effect upon conditional expectations of variation in the way that the variables
Zt depend upon the state st ≡ (ū0t , . . . , ū00 )0 . We consider the effect of variation in a particular
(scalar) random observable Zjt (for a given j, 1 ≤ j ≤ nZ ) in the information Iτ for some τ ≥ t.
By analogy with (A.13), we consider perturbations of the form

                                                   Zjt = Z̄jt + φζ t

around the random observable Z̄jt for any random variable ζ t . We wish to consider the effect of
variation in φ on a conditional expectation zT |τ . We shall assume that our baseline pattern of

                                                              18
variation in the observables Z̄t is such that no small perturbation changes the dimension of the
linear space spanned by the observables; that is, we assume that we are not in a degenerate case
in which Z̄jt is an exact linear combination of other observables, but would cease to be under
an infinitesimal perturbation. This is necessary in order for a partial derivative of a conditional
expectation with respect to Zjt to exist. Our assumption means that the first-order conditions
that we derive here are necessary conditions for an optimal commitment under the assumption
that optimal policy does not involve a degeneracy of this kind. (It does not seem to us likely
that it should, in general; but here we must simply note that our methods apply only to the
case in which it does not.)
    Now, under the above observation, one observes that for any ζ t in the linear space spanned
by the observables Z̄t in period t, the linear space spanned by Z̄lt , l 6= j and Zjt = Z̄jt + φζ t
coincides with the linear space spanned by Z̄t , regardless of the value of φ. Thus, in such a case,
                                                         "                           #
                                ∂zT |τ (sτ )        ∂zT |τ (sτ )
                                             = Es̃t              (s̃t )ζ t (s̃t ) = 0.
                                   ∂φ                 ∂Zjt

This is true in particular for ζ t equal to the kernel Pt,t (st , s̃t ), where st is a fixed arbitrary state
in period t, so that                        "                                  #
                                                ∂zT |τ (sτ )
                                     Es̃t                    (s̃t )Pt,t (st , s̃t ) = 0.             (A.16)
                                                  ∂Zjt
    This property of the partial derivative with respect to Zjt suffices for our purposes. It means
that as long as we are only interested in the expectation of our first-order conditions conditional
upon public information, we can ignore the effects upon conditional expectations of variations in
the way that observables depend upon the history of realizations. This in turn makes constraint
(A.5) irrelevant to the optimization problem. We now show this explicitly by turning to the
first-order conditions associated with our problem.
    Differentiating the Lagrangian (A.9) with respect to it (st ), and using (A.14) together with
(A.12), we obtain the first-order condition

                                 Es̃t Pt,t (st , s̃t )[Lit (s̃t ) + ϕ0t+1 (s̃t )M4 ] = 0,

where Lit (st ) ≡ Li [yt (st ), yt|t (st ), it|t (st )]. This can equivalently be written

                                                  Lit|t + ϕ0t+1|t M4 = 0.                            (A.17)

Differentiating with respect to yt (st ), we obtain the corresponding first-order condition

   L1t + L2t|t + Et ϕ0t+1 M2 + ϕ0t+1|t M3 − δ −1 ϕ0t M0 − δ −1 ϕ0t|t−1 M1 + µ0t D1 + µ0t|t D2 = 0.   (A.18)

                                                              19
Condition (A.18) holds for any possible history in any period t ≥ 0, but for t = 0 we set
ϕ0|−1 = 0, so the term δ −1 ϕ00|−1 M1 is deleted.
   Finally, differentiating with respect to Zjt (s̃t ), we obtain,
                ∞
                                        (
                X                                                                                    ∂yτ |τ (sτ )
                           τ −t
µjt (s̃t ) =           δ          Esτ       [L2τ (sτ ) + Eτ ϕ0τ +1 (sτ )M3 + µ0τ (sτ )D2 ]                        (s̃t )
                τ =t
                                                                                                       ∂Zjt
                                                                                                                                               )
                                                                     ∂iτ |τ (sτ )                                 ∂yτ +1|τ (sτ )
                           + [Liτ (sτ ) +        Eτ ϕ0τ +1 (sτ )M4 ]              (s̃t )   −    Eτ ϕ0τ +1 (sτ )M1                (s̃t )              ,
                                                                       ∂Zjt                                           ∂Zjt
where for each period τ ≥ t, sτ indexes an arbitrary state in that period. But taking the
conditional expectation of this with respect to It , we obtain, for a state st consistent with the
information It ,
                ∞
                                        (                                                                  "                                         #
                X                                                                                              ∂yτ |τ (sτ )
µjt|t (st ) =          δ τ −t Esτ           [L2τ (sτ ) + Eτ ϕ0τ +1 (sτ )M3 + µ0τ (sτ )D2 ]Es̃t                              (s̃t )Pt,t (st , s̃t )
                τ =t
                                                                                                                 ∂Zjt
                                                                                           "                                      #
                                                                                               ∂iτ |τ (sτ )
                                             + [Liτ (sτ ) +   Eτ ϕ0τ +1 (sτ )M4 ]Es̃t                       (s̃t )Pt,t (st , s̃t )
                                                                                                 ∂Zjt
                                                                         "                                           #)
                                                                             ∂yτ +1|τ (sτ )
                                             − Eτ ϕ0τ +1 (sτ )M1 Es̃t                       (s̃t )Pt,t (st , s̃t )
                                                                                 ∂Zjt
          = 0,                                                                                                                             (A.19)

where we use (A.16) to evaluate each term in large square brackets. Then the conditional
expectation of (A.18) with respect to It is given simply by

                   L1t|t + L2t|t + ϕ0t+1|t (M2 + M3 ) − δ −1 ϕ0t|t M0 − δ −1 ϕ0t|t−1 M1 = 0,                                               (A.20)

for t ≥ 0, using (A.19). (Here, we let ϕ0|−1 = 0.)
   In fact, as discussed in the text (identifying ϕ0t with (ξ 0t , ψ 0t−1 )0 and ϕ0t|t with (ξ 0t|t , Ξ0t−1 )0 ),
the first-order conditions (A.17) and (A.20) suffice, along with the structural equations (A.2),
to completely determine the expected dynamics yτ |t , ϕτ |t for all τ ≥ t, given initial conditions
for period t. These results suffice, in turn, to completely determine the optimal evolution of
the state vector yt and the optimal actions it , as a function of the history of realizations of the
exogenous disturbances. Hence, only µt|t (which is always zero) matters for our purposes, rather
than µt itself. Consequently, the relevant first-order conditions, (A.17) and (A.20), are the same
as if we simply neglected the constraints implied by (A.5) in computing the optimal plan.


References

 [1] Backus, David, and John Driffill (1986), “The Consistency of Optimal Policy in Stochastic
     Rational Expectations Models,” CEPR Discussion Paper No. 124.

                                                                      20
 [2] Chow, Gregory C. (1975), Analysis and Control of Dynamic Economic Systems, John Wiley
    & Sons, New York.

 [3] Currie, David, and Paul Levine (1993), Rules, Reputation and Macroeconomic Policy Co-
    ordination, Cambridge University Press, Cambridge.

 [4] Kalchbrenner, J.H., and Peter A. Tinsley (1975), “On the Use of Optimal Control in the
    Design of Monetary Policy,” Special Studies Paper No. 76, Federal Reserve Board.

 [5] LeRoy, Stephen F., and Roger N. Waud (1977), “Applications of the Kalman Filter in
    Short-run Monetary Control,” International Economic Review 18, 195–207

 [6] Pearlman, Joseph G. (1986), “Diverse Information and Rational Expectations Models,”
    Journal of Economic Dynamics and Control 10, 333–338.

 [7] Pearlman, Joseph G. (1992), “Reputational and Nonreputational Policies under Partial
    Information,” Journal of Economic Dynamics and Control 16, 339–357.

 [8] Pearlman, Joseph G., David Currie and Paul Levine (1986), “Rational Expectations Models
    with Partial Information,” Economic Modelling 3, 90–105.

 [9] Svensson, Lars E.O., and Michael Woodford (2002a), “Indicator Variables for Optimal
    Policy,” working paper.

[10] Svensson, Lars E.O., and Michael Woodford (2002b), “Indicator Variables for Optimal
    Policy under Asymmetric Information,” working paper.

[11] Woodford, Michael, “Optimal Monetary Policy Inertia,” NBER working paper no. 7261,
    July 1999.




                                            21
