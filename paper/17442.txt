                                NBER WORKING PAPER SERIES




ROBUST INFERENCE FOR MISSPECIFIED MODELS CONDITIONAL ON COVARIATES

                                           Alberto Abadie
                                          Guido W. Imbens
                                           Fanyin Zheng

                                        Working Paper 17442
                                http://www.nber.org/papers/w17442


                      NATIONAL BUREAU OF ECONOMIC RESEARCH
                               1050 Massachusetts Avenue
                                 Cambridge, MA 02138
                                    September 2011




Financial support for this research was generously provided through NSF grant 0820361. The views
expressed herein are those of the authors and do not necessarily reflect the views of the National Bureau
of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2011 by Alberto Abadie, Guido W. Imbens, and Fanyin Zheng. All rights reserved. Short sections
of text, not to exceed two paragraphs, may be quoted without explicit permission provided that full
credit, including © notice, is given to the source.
Robust Inference for Misspecified Models Conditional on Covariates
Alberto Abadie, Guido W. Imbens, and Fanyin Zheng
NBER Working Paper No. 17442
September 2011
JEL No. C01

                                                ABSTRACT

Following the work by White (1980ab; 1982) it is common in empirical work in economics to report
standard errors that are robust against general misspecification. In a regression setting these standard
errors are valid for the parameter that in the population minimizes the squared difference between
the conditional expectation and the linear approximation, averaged over the population distribution
of the covariates. In nonlinear settings a similar interpretation applies. In this note we discuss an alternative
parameter that corresponds to the approximation to the conditional expectation based on minimization
of the squared difference averaged over the sample, rather than the population, distribution of a subset
of the variables. We argue that in some cases this may be a more interesting parameter. We derive
the asymptotic variance for this parameter, generally smaller than the White robust variance, and we
propose a consistent estimator for the asymptotic variance.


Alberto Abadie                                          Fanyin Zheng
John F. Kennedy School of Government                    Harvard University
Harvard University                                      fzheng@fas.harvard.edu
79 JFK Street
Cambridge, MA 02138
and NBER
alberto_abadie@harvard.edu

Guido W. Imbens
Department of Economics
Littauer Center
Harvard University
1805 Cambridge Street
Cambridge, MA 02138
and NBER
imbens@fas.harvard.edu
1     Introduction
Following the seminal work by White (1980ab, 1982), researchers in economics routinely
report standard errors that are robust to misspecification of the models that are being
estimated. Müller (2011) gives the corresponding confidence intervals a Bayesian inter-
pretation. A key feature of the approach developed by White (see also Eicker (1967) and
Huber (1967)) is that in regression settings it focusses on the best linear predictor (blp)
that minimizes the distance between the linear predictor and the true conditional expec-
tation, averaged over the joint distribution of all variables, with a similar interpretation
in nonlinear settings. However, in some regression settings it may be more appropriate
to focus on the conditional best linear predictor (cblp) defined by averaging over the
conditional distribution given the sample values of the covariates. The conceptual con-
tribution of this note is to extend the White results to such settings. For a large class
of estimators, including maximum likelihood and method of moment estimators, we first
formally characterize the generalization to nonlinear models of the conditional best lin-
ear predictor. We then derive a large sample approximation to the variance of the least
squares and method of moments estimators relative to this conditional estimand. In gen-
eral, in misspecified models, this robust variance for the conditional estimand is smaller
than the White robust variance. Finally, in the main technical contribution we propose
a consistent estimator for this variance so that asymptotically valid confidence intervals
can be constructed. The proposed estimator generalizes the variance estimator proposed
by Abadie and Imbens (2006) for matching estimators. In correctly specified models the
new variance estimator is simply an alternative to the standard White robust variance
estimator. In misspecified models the new variance estimator is the first estimator for
the robust variance for the conditional estimand.
    Whether conditional or unconditional estimand should be the primary focus is con-
text specific and we do not take the position that either the conditional or unconditional
estimand is always appropriate. This is related to discussions about “random” versus
“fixed” regressors. We discuss some examples to clarify the distinctions between the two
and to make an argument for our view that in at least some settings the conditional
estimand, corresponding to the fixed regression notion, is of interest. Most importantly,
we argue that there is a clear choice to be made by the researcher that has direct impli-
cations for inference. In making this choice the researcher should bear in mind that the
variance for the conditional estimand is generally smaller than that for the population or
unconditional estimand, and thus tests for the former will generally have better power
than tests for the latter.
    The rest of this note is organized as follows. In Section 2 we discuss the conceptual
issues raised by this note heuristically in a linear regression model setting. In Section 3 we
discuss the motivation for the conditional estimand. Next, in Section 4 we present formal
results covering least squares, maximum likelihood, and method of moments estimators.
In Section 5 we apply the methods developed in this note to a data set collected by
Imbens, Rubin and Sacerdote (2001). In Section 6 we present a small simulation study.


                                              1
Section 7 concludes. The appendix contains the proofs.


2    The Conditional Best Linear Predictor
In this section we lay out some of the conceptual issues in this note informally in the
setting of a linear regression model. In Section 4 we provide formal results, covering
both this linear model setting and more general cases including maximum likelihood and
method of moments.
    Consider the standard linear model

     Yi = Xi0 θ + εi ,                                                               (2.1)

with Yi the outcome of interest, Xi a K-vector of observed covariates, possibly including
an intercept, and εi an unobserved error. Let X, Y, and ε be the N × K matrix with
ith row equal to Xi0 , the N-vector with ith element equal to Yi , and the N-vector with
ith element equal to εi , respectively. Traditionally in this setting researchers assumed
homoskedasticity, independence of the errors terms, and Normality of the error terms,

     ε|X ∼ N (0, σ 2 · IN ),

where IN is the N × N identity matrix. Under those assumptions the exact (conditional)
distribution of the least squares estimator for θ,
                      −1
     θ̂ols = (X0 X)        (X0 Y) ,

is Normal:
                                   
     θ̂ols |X ∼ N θ, σ 2 · (X0 X)−1 .

However, the set of assumptions, linearity of the regression function, independence, ho-
moskedasticity, and Normality of the error terms is often unrealistic. White (1980ab),
Eicker (1967), and Huber (1967) considered the properties of the least squares estima-
tor θ̂ols under much weaker assumptions. For the most general case one needs to define
the estimand if the regression function is not linear. Suppose the sample (Yi , Xi )N  i=1
is a random sample from a large population satisfying some moment conditions. Let
µ(x) = E[Yi |Xi = x] be the conditional expectation of Yi given Xi = x, and let σ 2(x)
be the conditional variance. Even if this conditional expectation µ(x) is not linear, one
might still wish to approximate it by a linear function x0θ, and be interested in the
value of the slope coefficient of this linear approximation, θ. Traditionally the optimal
approximation is defined as the value of θ that minimizes the expectation of the squared
difference between the outcomes and the linear approximation to the regression function.




                                            2
This is generally referred to as the best linear predictor,1 formally defined as
                       h             i
                                   2
      θblp = arg min E (Yi − Xi0 θ) .                                                              (2.2)
                    θ

Writing this as
                       h                  i
                                        2               −1
       θblp = arg min E (µ(Xi ) − Xi0 θ) = (E [Xi Xi0 ]) (E [Xi µ(Xi )]) ,
                    θ

shows that this can be interpreted as the value of θ that minimizes the discrepancy
between the true regression function µ(x) and the linear approximation, weighted by the
population distribution of the covariates.
   White (1980ab) shows that, under some regularity conditions,
     √                
                          d
       N · θ̂ols − θblp −→ N (0, Vblp) ,

where the normalized large sample variance is
                          −1                                     −1
     Vblp = (E [Xi Xi0 ])    E (Yi − Xi0 θblp)2 Xi Xi0 (E [Xi Xi0 ]) .                             (2.3)

White also proposed a consistent estimator for Vblp,
                 N
                           !−1      N
                                                               !             N
                                                                                         !−1
              1 X                1 X                                      1 X
     V̂blp =        Xi Xi0            (Yi − Xi0 θ̂ols)2 Xi Xi0                  Xi Xi0         .   (2.4)
              N i=1             N i=1                                     N i=1

Using the White variance estimator V̂blp is currently standard practice in empirical work
in economics. The bootstrap (Efron, 1982; Efron and Tibshirani, 1993) can also be used
to construct confidence intervals for θblp.
    In this note we explore an alternative linear approximation to the possibly nonlinear
regression function µ(x). Instead of minimizing the marginal expectation of the squared
difference between the outcomes and the regression function, we minimize this expecta-
tion conditional on the observed covariates. Define the conditional best linear predictor
θcblp as
                         N
                         X  h             i
                                    0 2
       θcblp   = arg min   E (Yi − Xi θ) X .                                                       (2.5)
                     θ
                         i=1

Denoting the N-vector with i-th element equal to µ(Xi ) by µ(X), we can write θcblp as
                         N
                         X                   2             −1
       θcblp = arg min         (µ(Xi ) − Xi0 θ) = (X0 X)        (X0 µ(X)) ,
                     θ
                         i=1

   1
    As far as we can tell, this term originates in the department of economics at Wisconsin, perhaps
due to Art Goldberger (e.g., Goldberger 1991). The term is also used in Manski (1988). Earlier,
Chamberlain (1982) used the terms “minimum mean square error linear predictor,” and in the vector
case, “multivariate linear predictor” for the same concept.

                                                  3
to stress the interpretation of θcblp as the best approximation to the true regression func-
tion, now with the weights based on the empirical distribution of the covariates. Both
θblp and θcblp choose the linear approximation by minimizing the squared difference be-
tween the true regression function µ(x) and the linear approximation x0θ. The difference
between the two approximations is how they weight, as a function of the covariates, the
squared difference between the regression function and the linear approximation for each
x. The first approximation, leading to θblp, uses the population distribution of the co-
variates. The second approximation, leading to θcblp, uses the empirical distribution of
the covariates.
       We defer to Section 3 the question whether and why in a specific application θblp
or θcblp might be the object of interest. In some applications we argue that θblp is
unambiguously the estimand of interest. However, as discussed in detail in Section 3,
we also think that in at least some applications θcblp may be of more interest than θblp.
Therefore, given that the econometric literature has focused exclusively on inference
estimands like θblp, we view the question of inference for θcblp as potentially of interest.
       Next we point out the implications of the difference between θblp and θcblp. The first
issue to note is that for point estimation it is irrelevant whether we are interested in
θblp or θcblp. In both cases θ̂ols is the natural estimator. However, for inference it does
matter whether we are interested in estimating θblp or θcblp, unless E[ε|X] = 0 and the
conditional expectation is linear. Consider the variance of the least squares estimator
θ̂ols , viewed as an estimator of θcblp. The exact (conditional) variance of θ̂ols is
                                              0 
         V θ̂ols X = E θ̂ols − θcblp θ̂ols − θcblp X                                   (2.6)

                                          N
                                                             !
                    1          −1      1 X 2                                −1
                  =   (X0 X/N)               σ (Xi )Xi Xi0       (X0 X/N)        .
                    N                  N i=1

Because θ̂ols is unbiased for θcblp, it follows that the marginal variance is the expected
value of the conditional variance. Under random sampling this variance, normalized by
the sample size, converges to
                            −1                            −1
      Vcblp = (E [Xi Xi0 ])    E σ 2(Xi )Xi Xi0 (E [Xi Xi0 ]) ,                      (2.7)

and we have
     √                 
                          d
       N · θ̂ols − θcblp −→ N (0, Vcblp) .

The key difference between the robust variance Vblp proposed by White and the robust
variance Vcblp is the difference between the conditional variance σ 2(Xi ) in (2.9) and the
expectation of the squared residual E[(Yi − Xi0 θblp)2 |Xi ] in (2.3). For the overall variances
we have
                                                                 0
      Vblp = Vcblp + N · E (θcblp(X) − θblp) (θcblp(X) − θblp) ,

                                               4
where the last expectation is over the distribution of θcblp as a function of X. Note
that in general Vblp exceeds Vcblp. The difference arises from the misspecification in the
regression function, that is, the difference between the conditional expectation and the
best linear predictor, µ(x) − xθblp.
     The final question we address in this section is how to estimate Vcblp. The challenge
is that the conditional variance function σ 2(x) is generally unknown. Estimating this is
straightforward in the case with discrete covariates. One can simply calculate the sample
variance of Yi at each distinct value of the covariates. Often that is not feasible, how-
ever, because some of the covariates are (close to) continuous. In such cases estimating
σ 2 (x) consistently for all x would require nonparametric estimation involving bandwidth
choices. Such an estimator would be more complicated than the White robust variance
estimator which simply uses squared residuals to estimate the expectation of the squared
errors. Here we build on work by Abadie and Imbens (2006) in the context of matching
estimators to develop a general estimator for Vcblp that does not require consistent esti-
mation of σ 2(x), much like the White variance estimator does not consistently estimate
E[(Yi − Xi0 θblp)2 |Xi = x] for all x. First define or the M × N matrix A with (i, j)th
element equal to aij the norm kAk = maxi,j |aij |. Next, define `X (i) to be the index of
the unit closest to i in terms of X:

      `X (i) = arg        min            kXi − Xj k ,
                     j∈{1,...,N },j6=i


Then our proposed variance estimator is

                     N
                                   !−1
      b cblp =    1 X
      V                 Xi Xi0                                                                                 (2.8)
                  N i=1

                     N
                                                                                  !            N
                                                                                                           !−1
                  1 X                                                    0                1 X
            ·           ε̂i Xi − ε̂`X (i)X`X (i) ε̂iXi − ε̂`X (i) X`X (i)             ·           Xi Xi0           .
                 2N i=1                                                                     N i=1
In Section 4 we show in a more general setting that this variance estimator is consistent
for Vcblp. An alternative estimator for Vcblp exploits the fact that the conditional variance
of εi Xi conditional on Xi is the same as Xi times the conditional variance of εi given Xi ,
                     N
                                   !−1             N
                                                                              !              N
                                                                                                         !−1
      e cblp =    1 X                           1 X                 2                    1 X
      V                 Xi Xi0             ·          ε̂i − ε̂`X (i) Xi Xi0       ·             Xi Xi0         .
                  N i=1                        2N i=1                                     N i=1

   Although in this linear regression case with the conditioning on all covariates both
b
Vcblp and Ve cblp are consistent for Vcblp, for nonlinear settings, or with conditioning on a
subset of the covariates, only the first estimator Vb cblp generalizes. To be specific, suppose
                                                                 0
that the covariate vector Xi can be partitioned as Xi = (X1i       , X2i )0 and correspondingly
X = (X1 , X2 ), and suppose we wish to estimate the variance conditional on X1 only. In


                                                        5
this case the probability limit of the normalized variance for the least squares estimator
is
                              −1                                           −1
      Vcblp = (E [Xi Xi0 ])        (E [ V (εi Xi | X1i )]) (E [Xi Xi0 ])        .                  (2.9)

Our proposed estimator for this conditional variance is
                    N
                                    !−1
      b cblp =   1 X
      V                Xi Xi0                                                                  (2.10)
                 N i=1
                                                                              !                    !−1
               1 X                                                      0
                  N                                                                    N
                                                                                    1 X
          ·          ε̂i Xi − ε̂`X1 (i)X`X1 (i) ε̂i Xi − ε̂`X1 (i)X`X1 (i)      ·         Xi Xi0         .
              2N i=1                                                                N i=1
This estimator is consistent for the conditional variance Vcblp. In contrast, replacing
                                            e cblp would not lead to a consistent estimator for
ε̂`X (i) by ε̂`X1 (i) in the expression for V
the variance.
      In the remainder of this paper we will generalize the results in this section to maximum
likelihood and method of moments settings, and state formal results concerning the large
sample properties of the varaince estimators. In the general settings the estimators are
no longer least squares estimators, and we will modify the terminology to reflect this.
We will use θpop for population estimands that generalize the best linear predictor θblp in
the regression case, and θcond for the conditional version that generalizes the conditional
best linear predictor θcblp in the regression case.


3     Motivation for Conditional Estimands
In this section we address the question whether, when, and why the estimand conditional
on the covariates may be of interest. We emphatically do not wish to argue that in all
cases it is the conditional estimand is the appropriate object of interest. Rather, we
wish to make the case, through four examples, that it depends on the context what
the appropriate object is, and that at least in some settings, the conditional best linear
predictor may be more appropriate or at least a reasonable alternative, to the standard,
unconditional estimand.
    One way to frame the question is in terms of different repeated sampling perspectives
one can take. We can consider the distribution of the least squares estimator over repeated
samples where we redraw the pairs Xi and Yi (the random regressor case), or we can
consider the distribution over repeated samples where we keep the values of Xi fixed and
only redraw the Yi (the fixed regressor case). Under general misspecification both the
mean and variance of these two distributions will differ. The population estimand θpop
is the approximate (in a large sample sense) average over the repeated samples when we
redraw both Xi and Yi , and θcond is the approximate average over the repeated samples
where Xi is held fixed. Many introductory treatments of regression analyses briefly

                                                         6
introduce the fixed and random regressor concepts, with a variety of opinions on what
the most relevant perspective is. Wooldridge writes that “reliance on fixed regressors
... can have unintended consequences. ... Because our focus is on asymptotic analysis,
we have the luxury of allowing for random explanatory variables throughout the book”
(Wooldridge, 2002, p10-11). Cameron and Trivedi write “The fixed regressors assumption
is rarely appropriate for microeconometrics data” (Cameron and Trivedi, 2005, p. 77).
Stock and Watson (2003) focus on the random regressor case, arguing that “the i.i.d.
assumption is a reasonable one for many data collection schemes” but acknowledging that
“Not all sampling schemes produce i.i.d. observations on (Xi , Yi )” (Stock and Watson,
2003, p. 105). Goldberger (1991) takes a different position, assuming “X nonstochastic,
which says that the elements of X are constants, that is, degenerate random variables.
Their values are fixed in repeated samples ...” (Goldberger, p. 164). These discussions
are in the context of correctly specified regression models, however, where the averages
of the distributions under the two repeated sampling perspectives coincide, and their
variances agree in large samples. A point that has not received attention in the literature
is that under general misspecifiaction, the random versus fixed regressor distinction has
implications for inference that do not vanish with the sample size.
    Another point is that the sole difference between the population and conditional esti-
mands is the weight function used to measure the difference between the model and the
true data generating process. For the population estimand the weight function depends
on the population distribution of the potential conditioning variables, and for the condi-
tional estimand it is the sample distribution of these variables. Because the population
distribution of these variables, unlike the sample distribution, is unknown, in general
there is more uncertainty about the population estimand. Thus, in practial terms, focus-
ing on the conditional estimand θcond leads to smaller standard errors than focusing on
the population estimand θpop.
Example I (Finite versus Infinite Population)
In the first example we want to argue that if the sample is a random sample from a large
population θpop is of more interest than θcond, whereas in the case where the sample is
equal to the population, the conditional estimand θcond is of more interest.
    Consider estimation of the average effect of a binary treatment. Each unit in the
population is characterized by two potential outcomes Yi (c) and Yi (t), and a binary
covariate Xi ∈ {f, m}. Let Wi ∈ {c, t} denote the treatment received and Yi = Yi (Wi )
the realized outcome. Assume assignment to treatment is random given Xi . Define, for
w = c, t and x = f, m,
     µ(x, w) = E[Yi (w)|Xi = x],       and τ (x) = µ(x, t) − µ(x, c).
Let Nxw be the number of units in the sample with Xi = x and Wi = w, let q be the
population share of the Xi = f types and q̂ the sample share:
                                    N
                                 1 X              Nfc + Nft
     q = E[Wi ],      and q̂ =         Wi =                       .
                                 N i=1      Nfc + Nft + Nmc + Nmt

                                            7
Consider two estimands, first the population average treatment effect,

     θpop = E [Yi (t) − Yi (c)] = q · τ (f) + (1 − q) · τ (m),

where q is the population fraction of Xi = f types, and second, the conditional average
effect,
                  N
               1 X
     θcond   =       E [ Yi (t) − Yi (c)| Xi ] = q̂ · τ (f) + (1 − q̂) · τ (m).
               N i=1

What is the rationale for focusing on θpop versus θcond? If the sample is a random sample
from a large population, it seems natural to focus on the population average treatment
effect θpop as the object of interest. On the other hand, suppose the sample is the entire
population. For example, the population could be the 50 states of the United States
in which case we might have observations on all 50 states. The covariate could be an
indicator for a state being on the coast versus inland. In that case it would appear
reasonable to keep fixed the number of coastal versus inland states, rather than view the
share of coastal states as random. That perspective suggests focusing on θcond rather
than θpop in cases where the sample is the entire population. We may still wish to use
large sample approximations, but focus on estimation of θcond rather than θpop.
    Note that if we observe the entire population in this case, we cannot interpret the
uncertainty in the estimator as due to sampling variation in the units. Instead we can
interpret the uncertainty in the estimator as due to random variation in the treatment
assignment Wi (see, for example, Neyman, 1923, 1990). To justify large sample approxi-
mation, however, we will resort to a random sampling argument.
Example II (Convenience Sample)
In the second example we want to make the case that sometimes there is intrinsically
no more interest in θpop than θcond because neither the weighting scheme corresponding
to the population distribution, nor the weighting scheme corresponding to the empirical
distribution function, is obviously of primary interest.
    Consider the study of lottery winners by Imbens, Rubin and Sacerdote (2001). We
use data from this study in Section 5. Imbens, Rubin and Sacerdote surveyed individuals
who won large prizes in the lottery. Using a standard life-cycle model of labor supply
they focus on linear regressions of subsequent labor earnings on the annual prize and
some additional covariates including prior earnings. The coefficient on the prize in this
linear regression can be interpreted as the marginal propensity to consume out of un-
earned income, an economically meaningful parameter (e.g., Pencavel, 1986). Even if the
conditional expectation as a function of the prize is nonlinear, it may still be interesting
to focus on the coefficient in the linear regression, partly because it facilitates compar-
ison across studies. The question is whether the linear approximation should be based
on weighting the squared difference between the true regression function and the linear
predictor by the population or empirical distribution of lottery prizes. There does not

                                                   8
appear to be a strong substantive argument for preferring one weighting function (and
thus the corresponding estimand) over the other.
Example III (Experimental Design)
Karlan and List (2009) carried out an experimental evaluation of incentives for chari-
table giving. Among the results Karlan and List report are probit regression estimates
where the object of interest is the regression coefficient on the indicator for being offered
a matching incentive for charitable giving. The specification of the probit regression
function also includes characteristics of the matching incentives.
    In this case the difference between Vpop and Vcond is that Vpop takes into account
sampling variation in θ̂ due to variation in the sample values of the matching incentives
over the repeated samples, whereas Vcond conditions on these values. Given that the
distribution of these incentives in this experiment is fixed by the researchers there appears
to be no reason to take this uncertainty into account, and we submit that the appropriate
measure of uncertainty is Vcond rather than Vpop.
Example IV (Average Derivative in Sample versus Population)
In the last example we again want to make the case that there is no compelling reason
to prefer one estimand to the other.
    Suppose one estimates a parametricRbinary√response model, say a probit model with
                                          a
Pr(Yi = 1|Xi ) = Φ(Xi0 θ), where Φ(a) = −∞ (1/ 2π) exp(−z 2/2)dz. (The same argument
would apply to other nonlinear parametric models.) Parameter estimates are difficult
to interpret for such models, and often researchers report derivatives of the conditional
expectation of Yi given Xi with respect to Xi to facilitate comparisons with other models.
                                                                             0
For the probit model the derivative
                             2
                                   √ of the conditional expectation is φ(x θ) · θ, where
φ(a) = ∂Φ(a)/∂a = exp(−a /2)/ 2π. In nonlinear models the value of the derivative
depends on the value of the covariates, so often researchers report the average derivative
evaluated at the estimated parameters:
              N
           1 X
      γ̂ =       φ(Xi0 θ̂) · θ̂.
           N i=1

The variance of this estimator γ̂ for the average derivative differs depending on whether
we condition on the covariates or not. The two estimands are
                   N
                1 X
      γcond   =       φ(Xi0 θ) · θ,   and γpop = E [φ(Xi0 θ) · θ] .
                N i=1

Because the average derivative is presented primarily as a more interpretable parameter
than θ itself, taking into account the uncertainty in the distribution of the covariates that
is averaged over may not serve any useful purpose, suggesting that γcond may be just as
relevant as γpop.



                                             9
4      Inference for Conditional Estimands
In this section we present the main formal results of the paper, covering linear regression,
maximum likelihood, and method of moments estimators. We cover settings where we
condition on the full set of regressors as well as cases where we condition on a subset of
the regressors.
    Suppose we have a random sample of size N of a pair of random vectors, (Xi , Yi ),
i = 1, . . . , N. Let KX and KY be the dimensions of Xi and Yi , and let X and Y be
the N × KX and N × KY matrices with i-th rows equal to Xi0 and Yi0 respectively. We
are interested in a finite dimensional parameter θ, defined as some function of the joint
distribution of (Xi , Yi ). Under some economic model it follows that

       E [ψ(Yi , Xi , θ)] = 0.                                                                (4.1)

The model may have additional implications beyond this moment condition, but these
are not used for estimation. For example, it may be the case that the conditional moment
has expectation zero,

       E [ ψ(Yi , Xi , θ)| Xi ] = 0.

Alternatively, we may have specified the joint distribution of Yi and Xi , in which case
ψ(y, x, θ) could equal to the score function. In that case the model has the additional
implication that the expected value of the derivatives of ψ(y, x, θ) with respect to θ is
equal to the expected value of the second moments of ψ(y, x, θ). Based only on (4.1),
and not on any other implications of the motivating model, we may wish to estimate θ
by solving
       N
       X
             ψ(Yi , Xi , θ̂) = 0.
       i=1

We are interested in the properties of the estimator θ̂ under general misspecification of
the model that motivated the moment condition.
   The standard approach (Hansen, 1984; Newey and McFadden, 1994; Wooldridge,
2002) focuses on the value θpop that solves

       E [ψ(Yi , Xi , θpop)] = 0.

If the pairs (Xi , Yi ), for i = 1, . . . , N are independent and identically distributed, then
under regularity conditions,
      √               
                           d                                                       −1
        N θ̂ − θpop −→ N (0, Vgmm,pop) ,               where Vgmm,pop = Γ0 ∆−1 Γ       ,

with
                               
            ∂
       Γ=E     ψ(Yi , Xi , θpop) ,        and ∆ = E [ψ(Yi , Xi , θpop)ψ(Yi , Xi , θpop)0] .
           ∂θ0

                                              10
   Now we focus on the conditional estimand. Define θcond as the solution to
      " N                  #
       X
    E      ψ(Yi , Xi , θ) X = 0.                                                             (4.2)
           i=1

Note that implicitly θcond is a function of X. If the original model implied that the
conditional expectation of ψ(Yi, Xi , θ) given Xi is equal to zero, then θcond = θpop, but
this need not hold in general. The motivation for the estimand is the same as in the best-
linear-predictor case. In cases where the model implies a conditional moment condition,
but we are concerned about misspecification, we may wish to focus on the value for θ
that minimizes the discrepancy between E[ψ(Yi , Xi , θ)|Xi ] and zero. We can weight the
discrepancy by the population distribution of the Xi ’s, or by the empirical distribution.
The conditional estimand corresponds to the case where the weights are based on the
empirical distribution function.
    We make the following assumptions. These are closely related to standard assump-
tions used for establishing asymptotic properties for moment-based estimators. See for
example Newey and McFadden (1994).

Assumption 1 (Xi , Yi ), for i = 1, . . . , N, are independent and identically distributed.
The support of Xi is a compact subset of RL .

Assumption 2 (i) The K-component vector of moment conditions ψ(y, x, θ) is contin-
uously differentiable in θ for θ ∈ Θ with Θ a compact subset of RK , with both ψ(y, x, θ)
and its derivative with respect to θ, ∂θ∂ 0 ψ(y, x, θ), continuous in x and y for all θ ∈ Θ,
(ii) there is a unique value θpop ∈ int(Θ)
                                              such that E [ψ(Yi, X    i , θpop)] = 0, (iii) ∆ and
                                                     ∂
Γ hare finite and full rank, and
                             i   (iv) E   supθ∈Θ ∂θ0   ψ(Yi , Xi , θ)  , and for some positive δ,
E supθ∈Θ kψ(Yi , Xi , θ)k2+δ are finite.

Theorem 1 Suppose Assumptions 1 and 2 hold. Then (i), θ̂ − θcond = op (1), and (ii)
    √              
                      d
      N · θ̂ − θcond −→ N (0, Vgmm,cond) ,

where
                             −1
        Vgmm,cond = Γ0 ∆−1
                        condΓ    ,         and ∆cond = E [ V (ψ(Yi , Xi , θpop))| Xi ] .

If also E [ ψ(Yi , Xi , θpop)| Xi = x] = 0 for all x, then (iii),
                                   √            
                                                     d
       θcond = θpop,         and     N θ̂ − θpop −→ N (0, Vgmm,cond) .

Proof: See Appendix.
    Let us consider an additional example to illustrate the differences between the two
variances. This example is related to the discussion in Chow (1984).

                                                11
Example V (Maximum Likelihood Estimation)
Suppose we specify the conditional distribution of Yi given Xi as f(y|x; θ). We estimate
the model by maximum likelihood:
                      N
                      X
       θ̂ = arg max         ln f(Yi |Xi ; θ).
                  θ
                      i=1

The normalized asymptotic variance under correct specification, and under some regu-
larity conditions, is equal to the inverse of the information matrix Iθ−1 , where
                2                                                                      
                   ∂                           ∂                     ∂                  0
      Iθ = −E           ln f(Yi |Xi ; θ) = E      ln f(Yi |Xi ; θ) ·    ln f(Yi |Xi ; θ) .
                 ∂θ∂θ0                         ∂θ                    ∂θ

White (1982) analyzed the properties of the estimator under general misspecification of
the conditional density. Let

       θpop = arg max E [ln f(Yi |Xi ; θ)] .
                      θ

Then White (1982) showed that under general misspecification,
                         √                                 −1 
         p                                d
     θ̂ −→ θpop,    and    N · θ̂ − θpop −→ N 0, Γ0 ∆−1 Γ           ,

with
                                                                                                         
        ∂2                                                 ∂                        ∂                     0
Γ = −E       ln f(Yi |Xi ; θpop) ,               and ∆ = E    ln f(Yi |Xi ; θpop) ·    ln f(Yi |Xi ; θpop) .
       ∂θ∂θ0                                               ∂θ                       ∂θ

The conditional version of the estimand under general misspecification is
                            N
                            X
       θcond = arg max            E [ ln f(Yi |Xi ; θ)| Xi ] ,
                      θ
                            i=1

where the expectation is taken only over Yi . Theorem 1 implies that
     √                                    −1 
                         d          0 −1
       N · θ̂ − θcond −→ N 0, Γ ∆condΓ             ,

where
                                           
                     ∂
       ∆cond   =E V     ln f(Yi |Xi , θpop) Xi .
                     ∂θ

If the model is correctly specified, then ∆ = ∆cond, but if the model is misspecified then
                               
          ∂
      E      ln f(Yi |Xi , θpop) = 0,
          ∂θ

                                                         12
but it is not true that for all x
                                       
           ∂
      E       ln f(Yi |Xi , θpop) Xi = x = 0,
           ∂θ
implying that in general ∆ − ∆cond is positive semi-definite. 
   Next, we consider estimation of the variance in the general case. Estimation of Γ is
the same as for the population estimand:
              N
           1 X ∂
      Γ̂ =           ψ(Yi , Xi , θ̂).
           N i=1 ∂θ0

The key question concerns estimation of ∆cond. Our proposed estimator matches each
unit to the closest unit in terms of Xi , and then differences the values of the moment
function:

         1 X                                                                                    0
            N
ˆ
∆cond =        ψ(Yi , Xi , θ̂) − ψ(Y`X (i), X`X (i) , θ̂) ψ(Yi , Xi , θ̂) − ψ(Y`X (i), X`X (i), θ̂) .
        2N i=1

We then combine these estimates to get an estimator for the variance for the conditional
estimand:
                          −1
     b             0 ˆ −1
     Vgmm,cond = Γ̂ ∆condΓ̂    .

Theorem 2 (Conditional Variance for Method of Moments Estimators)
Suppose Assumptions 1 and 2 hold. Then
                  p
      b gmm,cond −→
      V             Vgmm,cond.

Proof: See Appendix.


5     An Application to the Imbens-Rubin-Sacerdote Lot-
      tery Data
To illustrate the issues raised in this note we look at some data previously analyzed by
Imbens, Rubing and Sacerdote (2001). Imbens, Rubin and Sacerdote collected data on
individuals who played the lottery in the mid-eighties. Here we focus on a subset of their
data for 194 individuals who won large prizes. We use three variables, the yearly prize
won by each individual, the average of yearly earnings over six years prior to winning the
lottery and the average of yearly earnings over the six years after winning the lottery.
Table 1 reports some summary statistics.
    Using a standard life-cycle model for consumption and savings Imbens, Rubin and
Sacerdote estimate a linear model relating subsequent labor earnings to prior earnings and

                                                13
the yearly prize. The coefficient on the yearly prize can be interpreted as the propensity
to earn out of unearned income, an economically meaningful parameter (e.g., Pencavel,
1986). Following the Imbens-Rubin-Sacerdote specification we focus on the regression
function

     Yi = θ0 + θ1 · Pi + θ2 · Xi + εi ,

where Yi is the average of post-lottery earnings, Xi is the average of pre-lottery earnings,
and Pi is the yearly prize. As we discussed in Example II in Section 3, we may wish to
estimate a linear regression function even if one does not believe the conditional expec-
tation is exactly linear. The question then arises how to approximate the conditional
expectation by a linear function: averaging the squared difference between the condi-
tional expectation and the linear approximation over the population or over the sample
distribution of the covariates. Arguably one is interested in estimating a representative
value for the marginal propensity to earn out of unearned income, acknowledging that
this parameter may vary between individuals, and, for a given individual, may vary by
income levels. There is in our view no compelling argument that the population distri-
bution in the lottery sample, or the sample distribution is closer to being representative
of the population of interest.
    In Table 2 we report estimates for this regression function, with both the conventional
robust standard errors and the standard error for the conditional estimand.


6    A Small Simulation Study
In this section we assess the small sample properties of the variance estimators. We
center our simulation study around the lottery data set. We focus on estimating a linear
regression function

     Yi = θ0 + θ1 · Pi + θ2 · Xi + εi .

The joint distribution of the two covariates in the population is
                                           
        Pi              32.0       443.1 54.8
               ∼N               ,                    .
        Xi              12.1        54.8 124.9

The means and variances of this joint distribution were estimated on the lottery data.
The conditional distribution of Yi given Pi and Xi is normal:
                             
     Yi |Xi , Pi ∼ N µi , σi2 ,

where
                                              δ                                 
     µi = 6.46 − 0.13 × Pi + 0.75 × Xi +         × Pi2 + 1420 − 87 × Pi − 5 × Xi ,
                                            1000

                                            14
and

      ln σi2 = 2.611 − 0.012 · Pi + 0.070 · Xi .

Again the parameter values are motivated by the lottery data. A non-zero value for δ
makes the model nonlinear. We use two values for δ. In the first design we fix δ = 1.43
corresponding most closely to the lottery data. In the second design we use a larger
value, δ = 14.3.
      Table 3 presents the results. We focus on the coefficient on the prize, θ1. For both
designs we report the the average of the population and conditional standard error for
θ̂1 , and four coverage rates. First the coverage frequency of the conventional (White
standard error based) 95% confidence interval for θpop. This coverage should be 0.95.
Next, the fequency with which the same confidence interval covers θcond. This should be
more than 0.95. In the next row we report the coverage rates for confidence intervals
based on the conditional standard errors. Now the coverage for θpop could be less than
0.95, but the coverage for θcond should be 0.95. In the first design the model is too
close to being linear to detect these effects, and all coverage rates are close to 0.95. In
the second design the average conditional standard error is about 10% less than the
average unconditional (White) standard error, and this shows up in the coverage rates of
the confidence intervals. The confidence interval based on White standard errors covers
θpop with probability 0.95, and θcond with probability 0.97, and the confidence interval
based on the conditional standard error covers θcond with probability 0.94, and θpop with
probability 0.91.


7     Conclusion
In this note we discuss inference for conditional estimands in misspecified models. Fol-
lowing the work by White (1980ab, 1982) it is common in empirical work to report robust
standard errors. These robust standard errors are valid for the population value of the
estimator given random sampling. We show that if one is interested in the conditional
estimand, conditional on all or a subset of the variables, robust standard errors are gener-
ally smaller than the White robust standard errors. We derive a general characterization
of the variance for the conditional estimand and propose a consistent estimator for this
variance. We argue that in some settings the conditional estimand may be of more
interest than the unconditional one.




                                               15
                                  Appendix: Proofs of Theorems

Proof of Theorem 1: Assumptions 1 and 2 imply the assumptions in Theorems 2.6 and
3.4 in Newey and McFadden (1994). Their results imply that θ̂ is consistent for θpop , and
     √                d
that N (θ̂ − θpop ) → N (0, (Γ0∆−1 Γ)−1 ). To prove part (i), that θ̂ − θcond = op (1), we first
prove that θcond − θpop = op (1). Then, by the triangle inequality, because θ̂ − θpop = op(1) by
Theorem 2.6 in Newey McFadden (1994), it follows that θ̂ − θcond = op (1). Define ρ(x, θ) =
E[ψ(Yi , Xi, θ)|Xi = x], so that E[ρ(Xi, θpop)] = 0, and θcond solves
        N
      1 X
          ρ(Xi, θ) = 0.
      N
           i=1

Hence θcond can be thought of as a method of moments estimator for θpop with moment condition
ρ(Xi, θ). Because of Assumption 2 it follows that ρ(x, θ) satisfies the conditions for consistency
of the method of moments estimator in Theorem 2.6 in Newey and McFadden (1994), and thus
θcond − θpop = op (1).
Next we prove part (ii) of the theorem. Theorem 3.4 in Newey and McFadden also implies that
θcond − θpop = Op (N −1/2). Because θ̂ − θpop = Op(N −1/2 ) it follows by the triangle inequality
that θcond − θpop = Op (N −1/2). By a mean value theorem it follows that
             N                        N                        N                       √
         1 X                      1 X                       1 X ∂
     0= √       ψ(Yi , Xi, θ̂) = √       ψ(Yi, Xi, θcond) +           ψ(Yi , X i , θ̃)   N(θ̂ − θcond ),
          N i=1                    N i=1                    N     ∂θ0
                                                              i=1

for some intermediate value θ̃. Because θcond − θpop = op(1) and θ̂ − θpop = op (1), it follows that
                                                                   P
the intermediate value θ̃ satisfies θ̃ − θpop = op (1), and thus N1 N    ∂
                                                                    i=1 ∂θ0 ψ(Yi, Xi, θ̃) = Γ + op (1).
Thus
              N                      √
          1 X
      0= √       ψ(Yi, Xi, θcond) + Γ N(θ̂ − θcond ) + op (1),
           N i=1

and therefore
                                      N
      √                     −1    1 X
          N (θ̂ − θcond ) = Γ    √       ψ(Yi, Xi, θcond) + op (1).                               (A.1)
                                   N i=1

Next we show that
           N
       1 X
      √       ψ(Yi, Xi, θcond) ∼ N (0, ∆cond).                                                    (A.2)
        N i=1

Because θcond is the solution to
       " N                  #
         X
     E       ψ(Yi, Xi, θ) X = 0,
            i=1




                                                    16
θcond is a function of X, i.e. θcond = θcond (X). Therefore conditional on X the ψ(Yi , Xi, θcond)
are independent, but not identically distributed. For ease of exposition we focus on the case
where K = 1. We first apply the Lyapunov Central Limit Theorem to show that
            PN 
       √1
         N    i=1 ψ(Yi, Xi, θcond) − E [ ψ(Yi, Xi, θcond)| X]  d
                 q P                                          −→ N (0, 1) .                 (A.3)
                    1   N
                   N    i=1 V  [ψ(Yi , X  , θ
                                         i cond )| X]

The Lyapounov condition E[|ψ(Yi, Xi, θcond) − E[ψ(Yi, Xi, θcond )|X]|2+δ ] < ∞ for some positive
δ followsPfrom Assumption 2(iv).
Because N   i=1 E [ ψ(Yi, Xi, θcond)| X] = 0 by the definition of θcond , it follows that the numerator
in (A.3) simplifies to
            N
        1 X
       √       ψ(Yi, Xi, θcond) − E [ ψ(Yi, Xi, θcond)| X]
         N i=1
                N                           N                                 N
            1 X                         1 X                                1 X
          =√       ψ(Yi , Xi, θcond) − √       E [ψ(Yi , Xi, θcond)| X] = √      ψ(Yi, Xi, θcond).
             N i=1                       N i=1                             N i=1
The denominator in (A.3) converges in probability
         N
       1 X                          p
           V [ψ(Yi, Xi, θcond)| X] −→ E {V [ψ(Yi , Xi, θcond)| Xi]} = Vcond .
       N
          i=1

In combination with (A.3) this implies
         PN
      √1
       N    i=1 ψ(Yi, Xi, θcond) d
              √                 −→ N (0, 1) ,
                Vcond
and thus (A.2) follows.
Combining (A.1) and (A.2) implies
      √                                    
       N θ̂ − θcond ∼ N 0, Γ0 ∆−1 cond Γ)
                                          −1
                                                .

finishing the proof of part (ii).
If also E [ ψ(Yi, Xi, θ)| Xi] = 0, θcond is the solution to
                              " N                  #
                               X
       E [ψ(Yi, Xi, θ)] = E        ψ(Yi, Xi, θ) X = 0.
                              i=1

Then θcond = θpop , and part (iii) of the theorem follows. 

Next we state a useful lemma from Abadie and Imbens (2010).
Lemma A.1 (Lemma 1, Abadie and Imbens (2010, page 180)) Suppose that W1 , W2 , . . .
is a sequence with Wi ∈ W where W a compact subset of RK . Then
             N
           1 X                      2
       lim     Wi − W`W (i)             = 0.
      N →∞ N
                i=1


                                                  17
Lemma A.2 (Average Conditional Moments) Let (Vi, Wi), i = 1, . . . , N , be a sequence
of independent, identically distributed random vectors, with dimension KV and KW respectively,
and compact support for Wi . For some positive integer n, and for j = 1, 2, . . . , n, let µj (w) =
E[Vij |Wi = w] be Lipschitz in w with constant Cj , and suppose all moments of Vi up to the
2n-th moment exist. Then for all nonnegative k, m such that min(k, m) ≤ n,
        N
      1 X k             p
                            h                         i
          Vi · V`m
                 W (i)
                       −→ E  E  V i
                                   k
                                     W i  · E V i
                                                 m
                                                   | W i    .
      N
         i=1

Proof of Lemma A.2: We focus on the scalar case. The vector case can be shown by the
same argument. First we show
       "                                             #
           N
         1 X k                h                 i
     E        Vi · V`m
                     W (i)
                           − E E Vik Wi · E Vim Wi     = o(1).                 (A.4)
         N
               i=1

Because Vi and V`W (i) are independent conditional on W = (W1 , ..., WN )0 ,
        "               #
                           1 X n h k              io
          N                  N
        1 X k      m
      E     Vi · V`W (i) =     E E Vi · V`m
                                          W (i)
                                                W
        N                  N
               i=1                    i=1


                  1 X n                        o
                    N
              =       E E( Vik W) · E V`m
                                        W (i)
                                              W
                  N
                     i=1

                  1 X n                                 o
                    N
              =       E E( Vik Wi ) · E V`m
                                          W (i)
                                                W `W (i)
                  N
                     i=1
                  N
                1 X                        
              =     E µk (Wi ) · µm W`W (i)
                N
                     i=1
                   N
                1 X                                                 
              =       E µk (Wi ) · µm (Wi) + µm W`W (i) − µm (Wi )
                N
                  i=1
                   N
                                                  (    N
                                                                                          )
                1 X                                 1 X                               
              =       E [µk (Wi ) · µm (Wi )] + E         µm (Wi ) µm W`W (i) − µm (Wi)
                N                                   N
                  i=1                                 i=1
                                                      (                                       )
              1 X h  k                         i
                N                                          N
                                         m               1 X                              
            =      E E Vi |Wi · E (Vi |Wi) + E                µm (Wi) µm W`W (i) − µm (Wi ) .
              N                                         N
                  i=1                                    i=1

Therefore,
        "                                               #
           N
         1 X k               h                       i
       E     Vi · V`m
                    W (i)
                          − E E Vik |Wi · E (Vim |Wi )
         N
               i=1
                     (     N
                                                                )
                         1 X                             
              ≤ E            µm (Wi ) µm W`W (i) − µm (Wi)
                         N
                           i=1


                                                 18
                 (    N
                                                                        )
                   1 X                         
            ≤E           |µm (Wi)| · µm W`W (i) − µm (Wi )
                  N
                     i=1
                               (    N
                                                       )
                                 1 X
            ≤ sup |µm (w)| · E        Cm Wi − W`W (i)
               w                 N
                                            i=1
            = o(1),
by Lemma A.1. This finishes the proof of (A.4).
Next, we will show that
        "                                                  # 
         1 X  N                   h                   i 2 
      E           Vik · V`m
                          W (i)
                                − E E Vik Wi · E Vim | Wi        = o(1),                     (A.5)
         N                                                    
                i=1

which, together with (A.4), proves the claim in the Lemma. First we expand the square:
        "                                                 # 
         1 X  N                  h                   i 2 
      E          Vik · V`m
                         W (i)
                               − E E Vik Wi · E Vim | Wi
         N                                                   
                i=1
               "                  #2 
                1 X
                   N                   n h                              io2
            =E       Vik · V`m
                             W (i)
                                        + E E V i
                                                 k
                                                   |W i   · E (V i
                                                                  m
                                                                    |W i )
                N                    
                            i=1
                            (                                                            )
                                  N
                                1 X k                h                             i
                      −2E           Vi · V`m
                                           W (i)
                                                 · E  E V i
                                                           k
                                                             |W i   · E (V i
                                                                            m
                                                                              |W i )
                                N
                                   i=1
By (A.4), this is equal to
                                !2 
            1  XN                      n h                   io2
     E            Vik · V`m
                           W (i)
                                     − E E Vik |Wi · E Vim |Wi      + o(1)
           N
                i=1


                1 X h 2k              i   1 XX h k m                 i
                   N                         N
                                2m                           k m
            =        E V i · V `W (i)   +      E V  V      V  V
                                                   i `W (i) j `W (j)
                N2                        N2
                      i=1                              i=1 j6=i
                       n h                   io2
                      − E E Vik |Wi · E Vim |Wi      + o(1).
Because the moments of Vi up to at least the 2m-th and 2k-th moments exist, it follows that
the first term is op (1), and the entire expression is

            1 XX h k m                 i n h                         io2
               N
                               k m                k              m
                 E V  V      V  V
                     i `W (i) j `W (j)  − E E  V i  |W i  · E V i  |W i      + o(1).
            N2
                 i=1 j6=i

Because pr {`W (i) = `W (j)} −→ 0, i 6= j, when N −→ ∞, this is equal to
     "                 #                      
           N
       1 X k m                1 X                 n h                   io2
   E         Vi V`W (i) · E       Vjk V`m
                                         W (j)
                                                − E E Vik |Wi · E Vim |Wi      + o(1) = o(1),
       N                      N
          i=1                            j6=i

by (A.4). This finishes the proof of (A.5), and thus the claim in the lemma. 

                                                        19
Lemma A.3 (Average Conditional Variances) Let (Vi, Wi ), i = 1, . . ., N , be a random
sample from the distribution of (V, W ) where (V, W ) are a pair of random vectors, with dimen-
sion KV and KW respectively, with compact support for Wi . Suppose that µk (w) = E[Vik |Wi =
w] is Lipschitz in w with constant Ck for k ≤ 2, and that the fourth moment of Vi is finite.
Define
                   N
                   X                         0
        b cond = 1
        V            Vi − V`W (i) Vi − V`W (i) .
                2N
                       i=1

Then:
                p
        b cond −→
        V         E [V(Vi|Wi )] .                                                                (A.6)
                                     p
                             b cond −→
Proof of Lemma A.3: To prove V         E [V(Vi|Wi )], we show
         n                       o2
        E Vb cond − E [V(Vi|Wi )] = o(1).

Without loss of generality we focus on the case with KV = 1:
                   N
                   X                   N         N             N
        b cond = 1
                                 2  1 X 2     1 X 2         1 X
        V            Vi − V`W (i) =      Vi +      V`W (i) −     ViV`W (i) ,
                2N                  2N        2N             N
                       i=1                       i=1           i=1             i=1

and
                         n                         o         h             i
        E [V(Vi|Wi )] = E E Vi2 Wi − [E (Vi | Wi )]2 = E Vi2 − E E (Vi | Wi )2 .

          PN                 p
Because      i=1   Vi2 /N −→ E[Vi2 ] by a law of large numbers, it is sufficient to show

          N
        1 X 2        p                           N
                                                 1 X               p
                                                                      h             i
            V`W (i) −→ E Vi2 ,             and       Vi · V`W (i) −→ E E (Vi | Wi )2 .           (A.7)
        N                                        N
           i=1                                     i=1

The first part of (A.7) follows from applying Lemma A.2 with k = 0 and m = 2, and the second
part follows from applying Lemma A.2 with k = m = 1. 
                                       p                                                   p
Proof of Theorem 2: Since θ̂ −→ θpop and ψ(Yi , Xi, θ) is differentiable in θ, Γ̂ −→ Γ by the
                                                            p
                                                    ˆ cond −→
law of large numbers. Then it is sufficient to show ∆         ∆cond. Define
           N
˜        1 X                                                                                               0
∆cond =      ψ(Yi , Xi, θcond) − ψ(Y`X (i), X`X (i), θcond) ψ(Yi , Xi, θcond ) − ψ(Y`X (i), X`X (i) , θcond) .
        2N
                 i=1

                                                                      p
Let Vi = ψ(Yi , Xi, θcond), and Wi = Xi . By Lemma A.3, ∆     ˜ cond −→  V (ψ(Yi , Xi, θpop)). Because
    p                                                                          p
                                                                        ˆ
θ̂ −→ θcond and ψ(Yi , Xi, θ) is differentiable in θ, it follows that ∆cond −→ ∆    ˜ cond . Therefore,
                                   p
                  ˆ cond (Γ̂0 )−1 −→ Γ−1 ∆cond (Γ0 )−1 = Vgmm,cond . 
b gmm,cond = Γ̂−1 ∆
V




                                                   20
                                      References

Abadie, A., and G. Imbens (2006), “Large Sample Properties of Matching Estimators for
   Average Treatment Effects,” Econometrica, Vol. 74(1) 235-267.

Abadie, A., and G. Imbens (2010), “Estimation of the Conditional Variance in Paired
   Experiments,” Annales dEconomie et de Statistique, No 91, 175-187.

Angrist, J., and S. Pischke, (2009), Mostly Harmless Econometrics, Princeton University
   Press, Princeton, NJ.

Cameron, C., and P. Trivedi, (2005), Microeconometrics, Methods and Applications, Cam-
   bridge University Press, Cambridge.

Chamberlain, G., (1982), “Multivariate Regression Models for Panel Data,” Journal of
   Econometrics, Vol. 18, 5-46.

Chow, G., (1984), “Maximum-likelihood estimation of misspecified models,” Economic Mod-
   elling, Vol. 1(2): 134-138.

Efron, B., (1982), The Bootstrap and other Resampling Plans, Philadelphia, Society for
   Industrial and Applied Mathematics.

Efron, B., and Tibshirani, (1993), An Introduction to the Bootstrap, Chapman and Hall,
   New York.

Eicker, F., (1967), “Limit Theorems for Regression with Unequal and Dependent Errors,”
    Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability,
    Vol. 1, 59-82, University of California Press, Berkeley.

Goldberger, A., (1991), A Course in Econometrics, Harvard University Press.

Hansen, L–P., (1982), ”Large Sample Properties of Generalized Method of Moment Estima-
   tors”, Econometrica, vol. 50, 1029–1054.

Huber, P., (1967), “The Behavior of Maximum Likelihood Estimates Under Nonstandard
   Conditions,” Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics
   and Probability, Vol. 1, 221-233, University of California Press, Berkeley.

Imbens, G., and M. Kolesár, (2010), “The Behrens-Fisher Problem and Robust Standard
    Errors in Small Samples With and Without Clustering,” Unpublished Manuscript.

Imbens, G., D. Rubin, and B. Sacerdote, (2001), “Estimating the Effect of Unearned
    Income on Labor Supply, Earnings, Savings and Consumption: Evidence from a Survey
    of Lottery Players,” American Economic Review 91, 778-794.

Karlan, D., and J. List, (2001), “Does Price Matter in Charitable Giving? Evidence from
   a Large-Scale Natural Field Experiment,” American Economic Review 97(5): 1774-1793.

MacKinnon, J., and H. White, (1985), “Some Heteroskedasticity-consistent Covariance
   Matrix Estimators with Improved Finite Sample Properties,” Journal of Econometrics,
   Vol. 29, 305-325.

                                           21
Manski, C., (1988), Analogue Estimation Methods, Chapman and Hall.

Müller, U., (2011) ”Risk of Bayesian Inference in Misspecified Models, and the Sandwich
    Covariance Matrix”, Unpublished Manuscript, Princeton University.

Newey, W., and D. McFadden, (1994) ”Estimation in Large Samples”, in: McFadden and
   Engle (Eds.), The Handbook of Econometrics, Vol. 4.

Neyman, J., (1923, 1990), “On the Application of Probability Theory to Agricultural Experi-
   ments. Essay on Principles. Section 9,”translated in Statistical Science, (with discussion),
   Vol 5, No 4, 465–480, 1990.

Pencavel, J., (1986) “Labor Supply of Men: A Survey”, in O. Ashenfelter and R. Layard
   eds., Handbook of Labor Economics, North Holland: Elsevier, pp. 3-102.

Stock, J., and M. Watson, (2003), Introduction to Econometrics, Addison Wesley.

White, H., (1980a), “Using Least Squares to Approximate Unknown Regression Functions,”
   International Economic Review, Vol. 21(1):149-170.

White, H. (1980b), “A Heteroskedasticity-Consistent Covariance Matrix Estimator and a
   Direct Test for Heteroskedasticity,” Econometrica, 48, 817-838.

White, H., (1982), “Maximum likelihood estimation of misspecified models,” Econometrica,
   Vol 50(1): l-25.

Wooldridge, J., (2002), Econometric Analysis of Cross Section and Panel Data, MIT Press,
   Cambridge, MA.




                                             22
  Table 1: Summary Statistics for Lottery Data (N=194)

                             Average Standard Deviation

  Earnings Post Lottery         11.6                12.3
  Earnings Pre Lottery          12.1                11.2
  Yearly Prize                  32.0                21.1



           Table 2: Estimates for Lottery Data
                                            p            p
                                     est.       V̂blp        V̂cblp

           intercept        6.497           1.429         1.396
          yearly prize      -0.127          0.032         0.028
    average lagged earnings 0.755           0.077         0.079



     Table 3: Coverage Rate 95% Confidence Interval

Design I: (δ = 1.43)    q            average s.e.       θblp      θcblp
                            V̂blp       0.0466          0.951     0.951
                        q
                            V̂cblp      0.0450          0.938     0.940

Design II: (δ = 14.3)   q            average s.e.       θblp      θcblp
                            V̂blp       0.0512          0.953     0.969
                        q
                            V̂cblp      0.0451          0.914     0.941




                                23
