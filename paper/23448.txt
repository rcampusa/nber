                             NBER WORKING PAPER SERIES




                            TEMPERED PARTICLE FILTERING

                                       Edward Herbst
                                      Frank Schorfheide

                                     Working Paper 23448
                             http://www.nber.org/papers/w23448


                    NATIONAL BUREAU OF ECONOMIC RESEARCH
                             1050 Massachusetts Avenue
                               Cambridge, MA 02138
                                    May 2017




Schorfheide gratefully acknowledges financial support from the National Science Foundation
under the grant SES 1424843. The views expressed herein are those of the authors and do not
necessarily reflect the views of the National Bureau of Economic Research, the Board of
Governors, or the Federal Reserve System.

NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.

¬© 2017 by Edward Herbst and Frank Schorfheide. All rights reserved. Short sections of text, not
to exceed two paragraphs, may be quoted without explicit permission provided that full credit,
including ¬© notice, is given to the source.
Tempered Particle Filtering
Edward Herbst and Frank Schorfheide
NBER Working Paper No. 23448
May 2017
JEL No. C11,C32,E32

                                            ABSTRACT

The accuracy of particle filters for nonlinear state-space models crucially depends on the proposal
distribution that mutates time t-1 particle values into time t values. In the widely-used bootstrap
particle filter, this distribution is generated by the state-transition equation. While straightforward
to implement, the practical performance is often poor. We develop a self-tuning particle filter in
which the proposal distribution is constructed adaptively through a sequence of Monte Carlo
steps. Intuitively, we start from a measurement error distribution with an inflated variance, and
then gradually reduce the variance to its nominal level in a sequence of tempering steps. We show
that the filter generates an unbiased and consistent approximation of the likelihood function.
Holding the run time fixed, our filter is substantially more accurate in two DSGE model
applications than the bootstrap particle filter.


Edward Herbst
Board of Governors of the Federal Reserve System
20th Street and Constitution Avenue N.W.
Washington, DC 20551
edward.p.herbst@frb.gov

Frank Schorfheide
University of Pennsylvania
Department of Economics
3718 Locust Walk
Philadelphia, PA 19104-6297
and NBER
schorf@ssc.upenn.edu
                                                                                                            1


1         Introduction

Estimated dynamic stochastic general equilibrium (DSGE) models are now widely used by
academics to conduct empirical research in macroeconomics as well as by central banks
to interpret the current state of the economy, to analyze the impact of changes in mone-
tary or fiscal policies, and to generate predictions for macroeconomic aggregates. In most
applications, the estimation uses Bayesian techniques, which require the evaluation of the
likelihood function of the DSGE model. If the model is solved with a (log)linear approx-
imation technique and driven by Gaussian shocks, then the likelihood evaluation can be
efficiently implemented with the Kalman filter. If, however, the DSGE model is solved using
a nonlinear technique, the resulting state-space representation is nonlinear and the Kalman
filter can no longer be used.

        FernaÃÅndez-Villaverde and Rubio-Ramƒ±ÃÅrez (2007) proposed using a particle filter to eval-
uate the likelihood function of a nonlinear DSGE model, and many other papers have since
followed this approach. However, configuring the particle filter so that it generates an ac-
curate approximation of the likelihood function remains a key challenge. The contribution
of this paper is to propose a self-tuning particle filter, which we call a tempered particle
filter, that in our applications is substantially more accurate than the widely-used bootstrap
particle filter.

        Our starting point is the state-space representation of a potentially nonlinear DSGE
model given by a measurement equation and a state-transition equation in form of

                                                                                    
                                yt = Œ®(st ; Œ∏) + ut ,          ut ‚àº N 0, Œ£u (Œ∏)                           (1)
                                st = Œ¶(st‚àí1 , t ; Œ∏),         t ‚àº F (¬∑; Œ∏).

The functions Œ®(st ; Œ∏) and Œ¶(st‚àí1 , t ; Œ∏) are generated numerically when solving the DSGE
model. Here yt is a ny √ó 1 vector of observables, ut is a ny √ó 1 vector of normally distributed
measurement errors, and st is an ns √ó 1 vector of hidden states.1 To obtain the likelihood
increments p(yt+1 |Y1:t , Œ∏), where Y1:t = {y1 , . . . , yt }, it is necessary to integrate out the latent
states:                                Z Z
                 p(yt+1 |Y1:t , Œ∏) =         p(yt+1 |st+1 , Œ∏)p(st+1 |st , Œ∏)p(st |Y1:t , Œ∏)dst+1 dst ,   (2)

    1
    In principle both Œ®(¬∑) and Œ¶(¬∑) could depend on the time period t in a deterministic manner. We omit
this dependency in our notation. The ut ‚Äôs do not literally have to be measurement errors. They could
also be innovations to fundamentals. All we require is a non-degenerate distribution of yt |st with a scalable
covariance matrix.
                                                                                                   2


which can be done recursively with a filter.

       Particle filters represent the distribution of the hidden state vector st conditional on time
t information Y1:t = {y1 , . . . , yt } through a swarm of particles {sjt , Wtj }M
                                                                                 j=1 such that


                                   M            Z
                                1 X      j    j
                                      h(st )Wt ‚âà h(st )p(st |Y1:t , Œ∏).                          (3)
                                M j=1

The approximation here is in the sense of a strong law of large numbers (SLLN) or a central
limit theorem (CLT). The approximation error vanishes as the number of particles M tends to
infinity. The filter recursively generates approximations of p(st |Y1:t , Œ∏) for t = 1, . . . , T and
produces approximations of the likelihood increments p(yt |Y1:t , Œ∏) as a by-product. There
exists a large volume of literature on particle filters. Surveys and tutorials are provided,
for instance, by Arulampalam, Maskell, Gordon, and Clapp (2002), CappeÃÅ, Godsill, and
Moulines (2007), Doucet and Johansen (2011), Creal (2012), and Herbst and Schorfheide
(2015). Textbook treatments of the statistical theory underlying particle filters can be found
in Liu (2001), CappeÃÅ, Moulines, and Ryden (2005), and Del Moral (2013).

       The conceptually most straightforward version of the particle filter is the bootstrap par-
ticle filter proposed by Gordon, Salmond, and Smith (1993). This filter uses the state-
transition equation to turn sjt‚àí1 particles onto sjt particles, which are then reweighted based
on their success in predicting the time t observation, measured by p(yt |sjt , Œ∏). While the
bootstrap particle filter is easy to implement, it relies on the state-space model‚Äôs ability to
accurately predict yt by forward simulation of the state-transition equation. In general, the
lower the average density p(yt |sjt , Œ∏), the more uneven the distribution of the updated particle
weights, and the less accurate the approximation in (3).

       Ideally, the proposal distribution for sjt should not just be based on the state-transition
equation p(st |st‚àí1 , Œ∏) but also account for the observation yt . In fact, conditional on sjt‚àí1 the
optimal proposal distribution is the posterior2

                               p(st |yt , sjt‚àí1 , Œ∏) ‚àù p(yt |st , Œ∏)p(st |sjt‚àí1 , Œ∏),            (4)


       where ‚àù denotes proportionality. Unfortunately, in a generic nonlinear state-space model,
it is not possible to directly sample from this distribution. Constructing an approximation for
   2
   It is optimal in the sense that it minimizes the variance of the particle weights conditional on
{sjt‚àí1 , Wt‚àí1
          j
              }M
               j=1 .
                 In importance sampling it is approximately true that the smaller the variance of the
importance weights, the smaller is the asymptotic variance of the Monte Carlo approximation.
                                                                                                        3


p(st |yt , sjt‚àí1 , Œ∏) in a generic state-space model is difficult and often involves tedious model-
specific calculations that have to be executed by the user of the algorithm prior to its
implementation.3 The innovation in our paper is to generate this approximation in a sequence
of Monte Carlo steps. The basic idea goes back to Godsill and Clapp (2001). Our starting
point is the observation, that the larger the measurement error variance the more accurate the
filter becomes, because holding everything else constant, the variance of the particle weights
decreases. Building on this insight, in each period t, we generate sjt by forward simulation but
then update the particle weights based on a density p1 (yt |st , Œ∏) with an inflated measurement
error variance. In a sequence of tempering iterations we reduce this inflated measurement
error variance to its nominal level. These iterations mimic a sequential Monte Carlo (SMC)
algorithm designed for a static parameter and involve correction, selection, and mutation
steps. Such algorithms have been successfully used to approximate posterior distributions
for parameters of econometric models.4

    We show that our proposed tempered particle filter produces a valid approximation of the
likelihood function and substantially reduces the Monte Carlo error relative to the bootstrap
particle filter, even after controlling for computational time. Our algorithm can be embed-
ded into particle Markov chain Monte Carlo algorithms that replace the true likelihood by
a particle-filter approximation; see, for instance, FernaÃÅndez-Villaverde and Rubio-Ramƒ±ÃÅrez
(2007) for DSGE model applications and Andrieu, Doucet, and Holenstein (2010) for the
underlying statistical theory.

    The idea of adding tempering steps to the particle filter dates back to Godsill and Clapp
(2001), but it has not been used in the DSGE model literature. Contemporaneously with our
paper, Johansen (2016) developed a particle filter that involves tempering iterations to track
p(st , st‚àí1 , . . . , st‚àíL |Y1:t ). While his algorithm allows for the mutation of particles representing
blocks of lagged states, no clear guidance is provided on how the algorithm should be tailored
in a specific application and whether the additional computational cost of mutating lagged
states is compensated by improvements in the accuracy of the likelihood approximation.
Moreover, the paper does not contain any theoretical results and the numerical illustration
is restricted to a univariate model rather than DSGE models with multidimensional state
    3
      Attempts include approximations based on the one-step Kalman filter updating formula applied to a
linearized version of the DSGE model. Alternatively, one could use the updating step of an approximate
filter, e.g., the ones developed by Andreasen (2013) or Kollmann (2015).
    4
      Chopin (2002) first showed how to use sequential Monte Carlo methods to conduct inference on a
parameter that does not evolve over time. Applications to the estimation of DSGE model parameters
have been considered in Creal (2007) and Herbst and Schorfheide (2014). Durham and Geweke (2014) and
Bognanni and Herbst (2015) provide applications to the estimation of other econometric time-series models.
                                                                                             4


spaces. In addition, in each time period t we are choosing the tempering schedule adaptively,
building on work by Jasra, Stephens, Doucet, and Tsagaris (2011), Del Moral, Doucet,
and Jasra (2012), SchaÃàfer and Chopin (2013), Geweke and Frischknecht (2014), and Zhou,
Johansen, and Aston (2015).

    There are essentially two methods of establishing theoretical properties of SMC approx-
imations. On the one hand, Del Moral (2004) and Del Moral (2013) use high-level random
field theory. These authors establish theoretical properties of SMC methods through the
lens of the Feynman-Kac formula and its role in stochastic differential equations. While this
approach is mathematically elegant, it relies on theory that is unfamiliar to most econometri-
cians. On the other hand, Chopin (2004) proves a CLT for SMC approximations recursively,
using familiar (to econometricians) CLTs for non-identically and independently distributed
random variables. In a similar fashion, Pitt, Silva, Giordani, and Kohn (2012) show how
one can prove the unbiasedness of particle filter approximation without making use of the
Feynman-Kac formula. We follow this second route and show how the arguments in Chopin
(2004) and Pitt, Silva, Giordani, and Kohn (2012) can be extended to account for the spe-
cific tempering iterations used in our algorithm. While the theoretical results in our paper
are restricted to a non-adaptive version of the filter, theoretical results for adaptive SMC
algorithms have recently been obtained by Beskos, Jasra, Kantas, and Thiery (2014).

    The remainder of the paper is organized as follows. The proposed tempered particle
filter is presented in Section 2. We provide a SLLN for the particle filter approximation of
the likelihood function in Section 3 and show that the approximation is unbiased. Here we
are focusing on a version of the filter that is non-adaptive. The filter is applied to a small-
scale New Keynesian DSGE model and the Smets-Wouters model in Section 4 and Section 5
concludes. Theoretical derivations, computational details, DSGE model descriptions, and
data sources are relegated to the Online Appendix. To simplify the notation, we often drop
Œ∏ from the conditioning set of densities p(¬∑|¬∑).



2     The Tempered Particle Filter

A key determinant of the accuracy of a particle filter is the distribution of the normalized
weights
                                                    wÃÉtj Wt‚àí1
                                                           j
                                    WÃÉtj   =   1
                                                   PM j j ,
                                               M     j=1 wÃÉt Wt‚àí1
                                                                                                         5

        j
where Wt‚àí1 is the (normalized) weight associated with the jth particle at time t ‚àí 1, wÃÉtj is
the incremental weight after observing yt , and WÃÉtj is the normalized weight accounting for
this new observation.5 For the bootstrap particle filter, the incremental weight is simply the
likelihood of observing yt given the jth particle, p(yt |sjt ). It is approximately true that, all
else equal, the larger the variance of WÃÉtj ‚Äôs, the less accurate the Monte Carlo approximations
generated by the particle filter.

       One can show that, as the measurement error variance increases the variance of the
particle weights {WÃÉt }M
                       j=1 decreases. Let Œ£u /œÜn , 0 < œÜn ‚â§ 1 be an inflated measurement
error covariance matrix. Then,
                                                                            
                                           1                0 ‚àí1
                       pn (yt |st ) ‚àù exp ‚àí œÜn (yt ‚àí Œ®(st )) Œ£u (yt ‚àí Œ®(st )) .                        (5)
                                           2

                                                                 j
Assuming that a resampling step equalized the particle weights Wt‚àí1 = 1, it is straightfor-
ward to verify that
                                                         pn (yt |sjt )
                                  lim    WÃÉtj   =   1
                                                        PM             j
                                                                           = 1.                        (6)
                                                          j=1 pn (yt |st )
                                œÜn ‚àí‚Üí0
                                                    M

Thus, in the limit, the variance of the particle weights is equal to zero. With a bit more
algebra, it can be verified that the variance of the particle weights monotonically decreases as
œÜn ‚àí‚Üí 0 (see Appendix for details). We use this insight to construct a tempered particle filter
in which we generate proposed particle values sÃÉjt sequentially, by reducing the measurement
error variance from an inflated initial level Œ£u /œÜ1 to the nominal level Œ£u using a sequence
of scale factors
                                    0 < œÜ1 < œÜ2 < . . . < œÜNœÜ = 1.

The reduction of the measurement error variance is achieved by a sequence of Monte Carlo
steps that we borrow from the literature of SMC approximations for posterior moments
of static parameters (see Chopin (2002) and, for instance, the treatment in Herbst and
Schorfheide (2015)).

       By construction, pNœÜ (yt |st ) = p(yt |st ). Based on pn (yt |st ), we can define the bridge
distributions

                               pn (st |yt , st‚àí1 ) ‚àù pn (yt |st )p(st |st‚àí1 ).                         (7)
   5
    In the notation developed subsequently, the tilde on WÃÉtj indicates that this is weight associated with
particle j before any resampling of the particles.
                                                                                                        6


Integrating out st‚àí1 under the distribution p(st‚àí1 |Y1:t‚àí1 ) yields the bridge posterior density
for st conditional on the observables:
                                              Z
                           pn (st |Y1:t ) =       pn (st |yt , st‚àí1 )p(st‚àí1 |Y1:t‚àí1 )dst‚àí1 .          (8)

In the remainder of this section, we describe the proposed tempered particle filter. We do so
in two steps: Section 2.1 presents the main algorithm that iterates over periods t = 1, . . . , T
to approximate the likelihood increments p(yt |Y1:t‚àí1 ) and the filtered states p(st |Y1:t ). In
Section 2.2, we focus on the novel component of our algorithm, which in every period t uses
NœÜ steps to reduce the measurement error variance from Œ£u /œÜ1 to Œ£u . We provide specific
guidance for practitioners on tuning the tempered particle filter in Section 2.3. Finally, in
Section 2.4 we briefly discuss the relationship between the tempered and the conditionally-
optimal particle filter.


2.1    The Main Iterations

The tempered particle filter has the same structure as the bootstrap particle filter. In
every period t, we draw innovations t and use the state-transition equation to simulate the
state vector forward; we update the particle weights; and we resample the particles. The
key innovation is to start out with a fairly large measurement error variance, which is then
iteratively reduced to the nominal measurement error variance Œ£u . As the measurement error
variance is reduced (tempering), we adjust the innovations to the state-transition equation
as well as the particle weights.

   The number of tempering stages may differ for every time period t. However, to keep
the notation as simple as possible, we write NœÜ instead of NœÜ,t . Starting from the distribu-
tion p(st‚àí1 |Y1:t‚àí1 ) and using a sequence of tempering iterations our filter tracks the bridge
distributions pn (st |Y1:t ) defined in (8) for n = 1, . . . , NœÜ . As our filter cycles through the
                                                                                                      j,N
tempering iterations and mutates the particle values sj,n
                                                      t , it keeps the particle values st‚àí1
                                                                                           œÜ



unchanged (except during resampling operations so that st states are not separated from the
                                                                                                j,N
corresponding st‚àí1 states). The filter is designed such that the pairs (sj,n     œÜ
                                                                         t , st‚àí1 ) with their

associated particle weights approximate the distributions

               pn (st , st‚àí1 |Y1:t ) = pn (st |st‚àí1 , Y1:t )p(st‚àí1 |Y1:t ) for n = 1, . . . , NœÜ .

Because it is convenient for the implementation of the mutation steps, we will include t
                                                                                                        7


in the vector of particle values and track the triplet (st , t , st‚àí1 ) with the understanding
that this triplet always satisfies the state-transition equation st = Œ¶(st‚àí1 , t ). Algorithm 1
summarizes the iterations over periods t = 1, . . . , T .

Algorithm 1 (Tempered Particle Filter)

                                                                                                       iid
  1. Period t = 0 Initialization. Draw the initial particles from the distribution sj0 ‚àº
                                       j,NœÜ                  j,NœÜ
      p(s0 ), j = 1, . . . , M . Let s0       = sj0 and W0          = 1.

  2. Period t Iterations. For t = 1, . . . , T :

       (a) Particle Initialization.
                                       j,N      j,N
              i. Starting from {st‚àí1œÜ , Wt‚àí1œÜ }, generate Àúj,1
                                                            t ‚àº F (¬∑) and define


                                                                     j,N
                                                        sÃÉj,1       œÜ
                                                                       Àúj,1
                                                          t = Œ¶(st‚àí1 , t ).


             ii. Compute the incremental weights:

                                wÃÉtj,1 = p1 (yt |sÃÉj,1 )                                              (9)
                                              t                                 
                                                       1   j,1 0 ‚àí1
                                                                          j,1
                                                                               
                                       ‚àù exp ‚àí œÜ1 yt ‚àí Œ®(sÃÉt ) Œ£u yt ‚àí Œ®(sÃÉt ) .
                                                       2

            iii. Normalize the incremental weights:

                                                                           j,N
                                                                  wÃÉtj,1 Wt‚àí1œÜ
                                                  WÃÉtj,1 =       PM j,1 j,NœÜ                         (10)
                                                             1
                                                             M     j=1 wÃÉt Wt‚àí1

                                                                     j,N
                 to obtain the particle swarm {sÃÉj,1
                                                 t ,Àúj,1     œÜ     j,1
                                                      t , st‚àí1 , WÃÉt }, which leads to


                                 M                       Z
                              1 X       j,1 j,NœÜ     j,1
                   hÃÉ1t,M   =       h(sÃÉt , st‚àí1 )WÃÉt ‚âà h(st , st‚àí1 )p1 (st , st‚àí1 |Y1:t )dst dst‚àí1 . (11)
                              M j=1

                 Moreover,
                                                  M
                                               1 X j,1 j,NœÜ
                                                    wÃÉ Wt‚àí1 ‚âà p1 (yt |Y1:t‚àí1 ).                      (12)
                                               M j=1 t

             iv. Resample the particles:

                                                  j,N                            j,N
                                     {sÃÉj,1
                                        t ,Àúj,1     œÜ     j,1     j,1 j,1      œÜ    j,1
                                             t , st‚àí1 , WÃÉt } 7‚Üí {st , t , st‚àí1 , Wt },
                                                                                                              8


                 to obtain the approximation

                                M                     Z
                             1 X      j,1 j,NœÜ    j,1
                  hÃÑ1t,M   =       h(st , st‚àí1 )Wt ‚âà h(st , st‚àí1 )p1 (st , st‚àí1 |Y1:t )dst dst‚àí1 . (13)
                             M j=1

         (b) Tempering Iterations: Execute Algorithm 2 (see next section) to
              i. convert the particle swarm

                                             j,N                     j,NœÜ      j,NœÜ     j,N      j,NœÜ
                                  {sj,1  j,1     œÜ    j,1
                                    t , t , st‚àí1 , Wt } 7‚Üí {st             , t      , st‚àí1œÜ , Wt      }

                 to approximate

                                                  M
                                      N  œÜ     1 X         j,N    j,N       j,N
                                     hÃÑt,M =           h(st œÜ , st‚àí1œÜ )Wt œÜ                                 (14)
                                              M j=1
                                              Z
                                            ‚âà    h(st , st‚àí1 )p(st , st‚àí1 |Y1:t )dst dst‚àí1 ;

              ii. compute the approximation pÃÇM (yt |Y1:t‚àí1 ) of the likelihood increment.

   3. Likelihood Approximation

                                                      T
                                                      Y
                                      pÃÇM (Y1:T ) =         pÃÇM (yt |Y1:t‚àí1 ).                             (15)
                                                      t=1


      If we were to set œÜ1 = 1, NœÜ = 1, and omit Step 2.(b) for all t, then Algorithm 1
is exactly identical to the bootstrap particle filter: the sjt‚àí1 particle values are simulated
forward using the state-transition equation; the weights are then updated based on how
well the new state sÃÉjt predicts the time t observations, measured by the predictive density
p(yt |sÃÉjt ); and finally the particles are resampled using a standard resampling algorithm, such
as multinominal resampling, or systematic resampling.6 Once the resampling step has been
executed, the particle weights are equalized and the variance of the particle weights is equal
to zero: Wtj,1 = 1 for j = 1, . . . , N . Thus, in principle, we could drop the Wtj,1 weights from
the formulas.

      The drawback of the bootstrap particle filter is that the proposal distribution for the
innovation Àújt ‚àº F (¬∑) is ‚Äúblind,‚Äù in that it is not adapted to the period t observation yt . This
  6
   Detailed textbook treatments of resampling algorithms can be found in the by Liu (2001) and CappeÃÅ,
Moulines, and Ryden (2005).
                                                                                                            9


typically leads to a large variance in the incremental weights wÃÉtj , which in turn translates into
inaccurate Monte Carlo approximations. Taking the states {sjt‚àí1 }M
                                                                 j=1 as given and assuming
                                                                            j
that a t ‚àí 1 resampling step has equalized the particle weights, that is, Wt‚àí1 = 1, the
conditionally optimal choice for the proposal distribution is p(Àújt |sjt‚àí1 , yt ). However, because
of the nonlinearity in state-transition and measurement equation, it is not possible to directly
generate draws from this distribution. The main idea of our algorithm is to sequentially adapt
the proposal distribution for the innovations to the current observation yt by raising œÜn from
a small initial value to œÜNœÜ = 1. This is done in Step 2(b), which is described in detail in
Algorithm 2 in the next section.


2.2    Tempering the Measurement Error Variance

The idea of including tempering iterations into a particle filter dates back to Godsill and
Clapp (2001). These iterations build on Neal (1998)‚Äôs annealed importance sampling and
mimic the steps of SMC algorithms that have been developed for static parameters (e.g.,
Chopin (2002), Del Moral, Doucet, and Jasra (2006), Durham and Geweke (2014), and
Herbst and Schorfheide (2014, 2015)). The goal of SMC algorithms for static parameters
is to generate draws from a posterior distribution p(Œ∏|Y ) by sampling from a sequence of
bridge posteriors pn (Œ∏|Y ). These bridge posteriors are either generated by sequentially adding
observations to the likelihood function or by tempering the likelihood function, i.e., pn (Œ∏|Y ) ‚àù
       œÜn
 p(Y |Œ∏) p(Œ∏), n = 1, . . . , NœÜ . In the latter case, the sequence {œÜn } is chosen such that the
bridge posterior is equal to the actual posterior for n = NœÜ (i.e, when œÜNœÜ = 1.) At
each iteration, the algorithm cycles through three stages: particle weights are updated in
the correction step; the particles are being resampled and particle weights are equalized in
the selection step; and particle values are changed in the mutation step. The analogue of
        œÜn
 p(Y |Œ∏)     in our algorithm is pn (yt |st ) given in (5), which reduces to p(yt |st ) for œÜn = 1.
Algorithm 2 summarizes the correction, selection, and mutation steps. Note that the number
of stages, NœÜ , is an output of the algorithm that is determined in Step 1(a)iii. in conjunction
with the termination condition n = NœÜ of the do-loop. Thus, the filter is adaptive with
respect to the tempering schedule.


Algorithm 2 (Tempering Iterations) This algorithm receives as input the particle swarm
           j,N                                                        j,NœÜ      j,NœÜ     j,N      j,NœÜ
{sj,1  j,1     œÜ    j,1
  t , t , st‚àí1 , Wt } and returns as output the particle swarm {st          , t      , st‚àí1œÜ , Wt      } and
the likelihood increment pÃÇM (yt |Y1:t‚àí1 ). Set n = 2 and NœÜ = 0.
                                                                                              10


1. Do until n = NœÜ :

   (a) Correction:
         i. For j = 1, . . . , M define the incremental weights

                                            pn (yt |sj,n‚àí1 )
                           wÃÉtj,n (œÜn ) =            t
                                                       j,n‚àí1                                 (16)
                                           pn‚àí1 (yt |st      )
                                                   d/2       
                                              œÜn                    1             0
                                         =                exp ‚àí yt ‚àí Œ®(sj,n‚àí1
                                                                            t     )
                                             œÜn‚àí1                   2
                                                                                
                                                               ‚àí1      j,n‚àí1
                                                                             
                                           √ó(œÜn ‚àí œÜn‚àí1 )Œ£u yt ‚àí Œ®(st         ) .


         ii. Define the normalized weights

                                                              wÃÉtj,n (œÜn )Wtj,n‚àí1
                                      WÃÉtj,n (œÜn )   =   1
                                                             PM j,n              j,n‚àí1
                                                                                       ,     (17)
                                                         M     j=1 wÃÉt (œÜn )Wt


            (Wtj,n‚àí1 = 1 because the resampling step was executed in iteration n ‚àí 1), and
            the inefficiency ratio

                                                                M
                                                             1 X               2
                                        InEff(œÜn ) =               WÃÉtj,n (œÜn ) .            (18)
                                                             M j=1


        iii. If InEff(œÜn = 1) ‚â§ r‚àó , then set œÜn = 1, NœÜ = n, and WÃÉtj,n = WÃÉtj,n (œÜn = 1).
            Otherwise, let n = n + 1, œÜ‚àón be the solution to InEff(œÜ‚àón ) = r‚àó , and WÃÉtj,n =
            WÃÉtj,n (œÜn = œÜ‚àón ).
                                                             j,N
        iv. The particle swarm {sj,n‚àí1
                                 t     , j,n‚àí1
                                          t     , st‚àí1œÜ , WÃÉtj,n } approximates

                                              M
                                           1 X                 j,N
                                  hÃÉnt,M =         h(sj,n‚àí1
                                                       t    , st‚àí1œÜ )WÃÉtj,n                  (19)
                                          M j=1
                                          Z
                                        ‚âà    h(st , st‚àí1 )pn (st , st‚àí1 |Y1:t )dst dst‚àí1 .

   (b) Selection: Resample the particles:

                                             j,N                                 j,N
                        {sj,n‚àí1
                          t     , j,n‚àí1
                                   t     , st‚àí1œÜ , WÃÉtj,n } 7‚Üí {sÃÇj,n
                                                                  t ,ÀÜj,n     œÜ    j,n
                                                                       t , st‚àí1 , Wt },


       which leads to Wtj,n = 1 for j = 1, . . . , N . Keep track of the correct ancestry infor-
                                                                                                              11

                                               j,N
           mation such that sÃÇj,n       œÜ
                                           ÀÜj,n
                              t = Œ¶(st‚àí1 , t ) for each j. This leads to the approximation



                            M                      Z
                         1 X       j,n j,NœÜ    j,n
              hÃÇnt,M   =       h(sÃÇt , st‚àí1 )Wt ‚âà h(st , st‚àí1 )pn (st , st‚àí1 |Y1:t )dst dst‚àí1 .           (20)
                         M j=1


       (c) Mutation: Use a Markov transition kernel Kn (st |sÃÇt ; st‚àí1 ) with the invariance
           property                                    Z
                               pn (st |yt , st‚àí1 ) =       Kn (st |sÃÇt ; st‚àí1 )pn (sÃÇt |yt , st‚àí1 )dsÃÇt   (21)

           to mutate the particle values (see Algorithm 3 for an implementation). This leads
                                                       j,N
           to the particle swarm {sj,n  j,n     œÜ    j,n
                                   t , t , st‚àí1 , Wt }, which approximates


                            M                     Z
                         1 X      j,n j,NœÜ    j,n
              hÃÑnt,M   =       h(st , st‚àí1 )Wt ‚âà h(st , st‚àí1 )pn (st , st‚àí1 |Y1:t )dst dst‚àí1 .            (22)
                         M j=1

  2. Approximate the likelihood increment:

                                                       NœÜ          M
                                                                                          !
                                                       Y        1 X j,n j,n‚àí1
                                 pÃÇM (yt |Y1:t‚àí1 ) =                 wÃÉ Wt                                (23)
                                                       n=1
                                                                M j=1 t

                                                       j,N
      with the understanding that Wtj,0 = Wt‚àí1œÜ . 


   The correction step adapts the stage n ‚àí 1 particle swarm to the reduced measurement
error variance in stage n by reweighting the particles. The incremental weights in (16)
capture the change in the measurement error variance from Œ£u /œÜn‚àí1 to Œ£u /œÜn and yield an
importance sampling approximation of pn (st |Y1:t ) based on the stage n ‚àí 1 particle values.
                                                                                        œÜ                 N
As mentioned earlier, rather than relying on a fixed exogenous tempering schedule {œÜn }n=1 ,
we choose œÜn to achieve a targeted inefficiency ratio r‚àó > 1. This approach of adaptively
choosing the tempering schedule has been used in the SMC literature by Jasra, Stephens,
Doucet, and Tsagaris (2011), Del Moral, Doucet, and Jasra (2012), SchaÃàfer and Chopin
(2013), and Zhou, Johansen, and Aston (2015). It also has proven useful in the context of
global optimization of nonlinear functions; see Geweke and Frischknecht (2014). To relate
the inefficiency ratio to œÜn , we begin by defining

                                  1
                            ej,t = (yt ‚àí Œ®(sj,n‚àí1
                                            t     ))0 Œ£‚àí1         j,n‚àí1
                                                       u (yt ‚àí Œ®(st     )).
                                  2
                                                                                                    12


Assuming that the particles were resampled in iteration n ‚àí 1 and Wtj,n‚àí1 = 1, we can then
express the inefficiency ratio as

                                M               1
                                                  PM
                             1 X      j,n
                                            2  M  j=1 exp[‚àí2(œÜn ‚àí œÜn‚àí1 )ej,t ]
                InEff(œÜn ) =       WÃÉt (œÜn ) =  P                            2 .                (24)
                             M j=1              1  M
                                                M  j=1 exp[‚àí(œÜn ‚àí œÜn‚àí1 )ej,t ]


It is straightforward to verify that for œÜn = œÜn‚àí1 the inefficiency ratio InEff(œÜn ) = 1 < r‚àó .
Moreover, we show in the Online Appendix that the function is monotonically increasing
on the interval [œÜn‚àí1 , 1], which is the justification for Step 1(a)iii of Algorithm 3. Thus, we
are raising œÜn as closely to one as we can without exceeding a user-defined bound on the
variance of the particle weights. Note that we can use the same approach to set the initial
scaling factor œÜ1 in Algorithm 1.
     The selection step is executed in every iteration n to ensure that we can find a unique
œÜn+1 based on (24) in the subsequent iteration. Thus, Wtj,n = 1 and in principle we could
drop the weights from the formulas. The equalization of the particle weights allows us to
characterize the properties of the function InEff(œÜn ).
     Finally, in the mutation step, we are using a Markov transition kernel to change the parti-
cle values from (sÃÇj,n
                   t ,ÀÜj,n      j,n j,n
                        t ) to (st , t ) in a way to maintain an approximation of pn (st , st‚àí1 |Y1:t ).

In the absence of the mutation step, the initial particle values (sj,1  j,1
                                                                   t , t ) generated in Step 2(a)

of Algorithm 2 would never change and we would essentially reproduce the bootstrap particle
filter by computing p(yt |sÃÉjt ) sequentially under a sequence of measurement error covariance
matrices that converges to Œ£u . The mutation can be implemented with NM H steps of a
random walk Metropolis-Hastings (RWMH) algorithm; see Algorithm 3 below. Unlike in
                                                                                                  j,N
the algorithm proposed by Johansen (2016), we do not mutate the particle values st‚àílœÜ ,
l = 1, . . . , L. The advantage is that the state vector that we are mutating has a smaller
dimension, which tends to increase the probability that a particle value changes during
the mutation step. Thus, our algorithm should be able to attain a desired probability of
mutating the particle values with fewer steps of the RWMH algorithm and therefore be
faster. A potential disadvantage is that we are not adapting as well to the joint distribution
pn (st , st‚àí1 , . . . , st‚àíL |Y1:t ).

Algorithm 3 (RWMH Mutation Step) This algorithm receives as input the particle swarm
                j,N                                                                 j,N
{sÃÇj,n
   t ,ÀÜj,n     œÜ    j,n                                           j,n j,n      œÜ    j,n
        t , st‚àí1 , Wt } and returns as output the particle swarm {st , t , st‚àí1 , Wt }.


    1. Execute NM H Metropolis-Hastings Steps for Each Particle: For j = 1, . . . M :
                                                                                                                              13


         (a) Set ÀÜj,n,0
                   t     = ÀÜj,n
                             t . Then, for l = 1, . . . , NM H :

               i. Generate a proposed innovation:

                                                             ejt ‚àº N ÀÜj,n,l‚àí1 , c2n In .
                                                                                        
                                                                       t


               ii. Compute the acceptance rate:
                                                                 (                          j,N
                                                                                                                     )
                                                                              pn (yt |ejt , st‚àí1œÜ )p (ejt )
                               Œ±(ejt |ÀÜj,n,l‚àí1
                                        t       )   = min          1,                       j,N
                                                                                                                         .
                                                                        pn (yt |ÀÜj,n,l‚àí1
                                                                                  t       , st‚àí1œÜ )p (ÀÜj,n,l‚àí1
                                                                                                         t       )

              iii. Update particle values:
                                                   (
                                                       ejt           with prob. Œ±(ejt |ÀÜj,n,l‚àí1
                                                                                         t       )
                                     ÀÜj,n,l
                                       t       =
                                                       ÀÜj,n,l‚àí1
                                                         t       with prob. 1 ‚àí Œ±(ejt |ÀÜj,n,l‚àí1
                                                                                         t       )

         (b) Define
                                                                                    j,N
                                      j,n ÀÜj,n,N
                                       t = t
                                                 MH
                                                    ,                sj,n       œÜ   j,n
                                                                      t = Œ¶(st‚àí1 , t ). 


       The covariance matrix for the proposal distribution in the RWMH algorithm is simply
the identity matrix In scaled by cn .7 We set adaptively by cn to achieve a desired acceptance
rate. In particular, we compute the average empirical rejection rate RÃÇn‚àí1 (cn‚àí1 ), based on
the mutation phase in iteration n ‚àí 1. The average is computed across the NM H RWMH
steps. We set c1 = c‚àó and, for n > 2, adjust the scaling factor according to

                                                                                             e20(x‚àí0.40)
                 cn = cn‚àí1 f 1 ‚àí RÃÇn‚àí1 (cn‚àí1 ) ,                f (x) = 0.95 + 0.10                         .                (25)
                                                                                            1 + e20(x‚àí0.40)

This function is designed to increase the scaling factor by 5 percent if the acceptance rate
is well above 0.40, and decrease the scaling factor by 5 percent if the acceptance rate is well
below 0.40. For acceptance rates near 0.40, the increase (or decrease) of cn is attenuated by
the logistic component of f (x). In our empirical applications, the performance of the filter
was robust to variations on the rule.
   7
    Herbst and Schorfheide (2014) use the particle approximation of the posterior covariance matrix from
the selection step to specify the stage-n proposal covariance matrix. In the tempered particle filter, the cost
of computing this object tends to outweigh the gains from adapation, so we instead use the identity matrix.
                                                                                              14


2.3    Tuning of the Algorithm

In order to run Algorithm 3, the user has to specify the number of particles M , the initial
measurement error precision scaling œÜ1 in Algorithm 1, the targeted inefficiency ratio r‚àó , the
initial scaling of the proposal covariance matrix c‚àó , and the number of Metropolis-Hastings
steps NM H . In principle, the user can also adjust the target acceptance rate (and potentially
the speed of adjustment) in (25). Each of these tuning parameters affects the statistical
properties of the filter, and can potentially affect the computational cost associated with the
filter. We now discuss some issues in selecting each of these parameters.

   The selection of M is an issue for any particle filter. A higher M is associated with a
more precise approximation at the cost of a longer run time of the filter. In practice, this is
usually done through experimentation. If the particle filter is embedded in a Markov chain
Monte Carlo (MCMC) algorithm, a heuristic suggested by Pitt, Silva, Giordani, and Kohn
(2012), is to increase M until the standard deviation of the filter‚Äôs log likelihood estimate
at some parameter value is less than one. Particle filter approximations typically satisfy a
CLT according to which the variance is proportional to 1/M .

   The initial measurement error precision œÜ1 can either be user-specified or determined
adaptively by targeting a desired variance of particle weights as in Step 1(a)iii of Algorithm 2.
The targeted inefficiency ratio, r‚àó ‚àà (1, ‚àû) controls the targeted degree of ‚Äúunevenness‚Äù of
the distribution of particle weights that pins down a particular œÜn . If r‚àó is close to 1,
loosely speaking, œÜn will be ‚Äúclose‚Äù to œÜn‚àí1 and generally there will be many stages (NœÜ
will be large.) In contrast, if r‚àó is very large, bridge distributions can be very different,
and in general NœÜ will be small. In the limit, as r‚àó ‚àí‚Üí ‚àû, the algorithm converges to the
resample-move variant of the bootstrap particle filter, where NœÜ = 1 for all t. The particles
are mutated at each time t, but there are no intermediate bridge distributions.

   A low r‚àó delivers weighted particles with low variance, which all else equal are associated
with more precise Monte Carlo estimates. Of course, a low r‚àó is also associated with many
bridge distributions, which increases the run time of the filter. At some point, increasing
the number of tempering iterations further, could in principle result in less precise estimates
because of the variability induced by the additional resampling and mutation steps. In
practice, we don‚Äôt find this be an issue, and so r‚àó works as a complement to M , with both
having a trade-off between statistical precision and computational cost. In Section 4 we
examine the effects of different choices of M and r‚àó in two DSGE models.
                                                                                                     15


    The other two tuning parameters, namely, the initial scaling of the proposal covariance
matrix c‚àó and the number of RWMH steps NM H , are less important. If there are many
bridge distributions, the influence of the initial scaling factor c‚àó is diminished because it is
adjusted in each subsequent iteration. While many intermediate RWMH steps help to ensure
that the particles are both diverse and well-adapted to any given bridge distribution, often
this effect can be achieved by choosing a lower r‚àó . Of course, this is not to say that c‚àó and
NM H do not affect the variance of the Monte Carlo estimates. In any particular application,
experimentation with these parameters may enhance the performance of the algorithm. In
Section 4.2 we analyze the effects of increasing NM H .

    Finally, we could replace the draws of Àúj,1
                                             t   from the innovation distribution F (¬∑) in
                                                                                                   j,N
Step 2(a)i of Algorithm 2 with draws from a tailored distribution with density gt1 (Àúj,1    œÜ
                                                                                      t |st‚àí1 )
                                                                                   j,N
and then adjust the incremental weight œâÃÉtj,1 by the ratio p (Àúj,1  1 j,1
                                                                         t |st‚àí1œÜ ), as it is done
                                                                 t )/gt (Àú

in the generalized version of the particle filter. Here the gt (¬∑) density might be constructed
based on a linearized version of the DSGE model or be obtained through the updating steps
of a conventional nonlinear filter, such as an extended Kalman filter, unscented Kalman fil-
ter, or a Gaussian quadrature filter; see Herbst and Schorfheide (2015). Thus, the proposed
tempering steps can be used either to relieve the user from the burden of having to construct
          j,N
a gt1 (Àúj,1    œÜ
         t |st‚àí1 ) in the first place, or it could be used to improve upon the accuracy obtained
                                    j,N
with a readily available gt1 (Àúj,1    œÜ
                                t |st‚àí1 ).




2.4     Relationship to Conditionally-Optimal Particle Filter

We mentioned in the introduction that conditional on the sjt‚àí1 particles it is optimal to
generate draws from the proposal distribution p(st |yt , st‚àí1 ) given in (4). The tempered
particle filter generates a sequence of approximations pn (st , st‚àí1 |yt , Y1:t‚àí1 ) that converge to
p(st , st‚àí1 |yt , Y1:t‚àí1 ) as n ‚àí‚Üí NœÜ . This raises the question to what extent this filter can
achieve conditionally optimality. Because p(st |yt , st‚àí1 ) and pn (st , st‚àí1 |yt , Y1:t‚àí1 ) are not the
same objects, we provide a comparison of the two approaches by embedding the conditionally-
optimal proposal distribution into Algorithm 1.

    Suppose in Step 2(a)i we draw sÃÉj,1
                                    t,‚àó (we are using the ‚àó subscript to indicate draws and

weights associated with the conditionally-optimal proposal) from

                                             j,N                         j,N
                               p1 (st |yt , st‚àí1œÜ ) ‚àù p1 (yt |st )p(st |st‚àí1œÜ ),                   (26)
                                                                                                              16


where p1 (yt |st ) is based on scaling the precision of the measurement errors by œÜ1 .8 The
incremental weights for sÃÉj,1
                          t,‚àó are given by

                                                                     j,N
                             j,1                           p(sÃÉj,1      œÜ
                                                               t,‚àó |st‚àí1 )                   j,N
                           wÃÉt,‚àó   =   p1 (yt |sÃÉj,1
                                                 t,‚àó )                    j,NœÜ
                                                                                  = p1 (yt |st‚àí1œÜ ).         (27)
                                                         p1 (sÃÉj,1
                                                               t,‚àó |yt , st‚àí1 )


                                                                                      j,1
       For every choice œÜÃÉ1 of the measurement error precision, the variance of the wÃÉt,‚àó weights is
smaller than the variance of the weights wÃÉtj,1 obtained under the bootstrap proposal, because
the conditionally-optimal proposal is designed to minimize the variance of the particle weights
                                   j,N          j,N
conditional on the swarm {st‚àí1œÜ , Wt‚àí1œÜ }. Moreover, we know from (24) that the variance of
the particle weights is an increasing function of œÜ1 . Thus, if we choose œÜ1 adaptively according
to Step 1(a)iii of Algorithm 2 by targeting a specific variance of the particle weights (or the
a particular inefficiency ratio), then it has to be the case that the precision chosen under the
conditionally-optimal proposal, say œÜ1,‚àó , is larger (and closer to one) than the precision œÜ1
chosen under the bootstrap proposal.
                                                                             j,1
       This leads to the following conclusions: (i) if the variance of the wÃÉt,‚àó is sufficiently small,
                                                                                          j,1
then œÜ1,‚àó = 1 and the tempering iterations become obsolete. (ii) If the variance of the wÃÉt,‚àó is
large enough such that œÜ1,‚àó < 1, then, because œÜ1,‚àó ‚â• œÜ1 , the tempered particle filter with the
conditionally-optimal proposal distribution will be more accurate than the tempered particle
filter based on the bootstrap proposal. The former will either use fewer iterations to bridge
the discrepancy between p1 (st , st‚àí1 |Y1:t ) and p(st , st‚àí1 |Y1:t ) or it will use the same number of
iterations with smaller gaps between pn‚àí1 (st , st‚àí1 |Y1:t ) and pn (st , st‚àí1 |Y1:t ).

       The implementation of the conditionally-optimal particle filter is typically infeasible in
practice. Thus, the tempered particle filter is meant to be a feasible alternative that domi-
nates the widely-used bootstrap particle filter. However, the discussion emphasizes an im-
                                                                                                       j,N
portant point made at the end of Section 2.3: if a better proposal than p(st |st‚àí1œÜ ) is available,
then it should be used along with the tempering iterations.
   8
    If one can sample from the conditionally-optimal proposal for œÜn = 1, then it is reasonable to assume
that one can sample from this density for 0 < œÜn ‚â§ 1. This is certainly true for normally distributed
measurement errors.
                                                                                                          17


3       Theoretical Properties of the Filter

We will now examine asymptotic (with respect to the number of particles M ) and finite
sample properties of the particle filter approximation of the likelihood function. Section 3.1
provides a SLLN, and Section 3.2 shows that the likelihood approximation is unbiased.
Throughout this section, we will focus on a version of the filter that is non-adaptive.9 This
version of the filter replaces Algorithm 2 with Algorithm 4 and Algorithm 3 with Algorithm 5:

Algorithm 4 (Tempering Iterations ‚Äì Non-Adaptive) This algorithm is identical to
                                                                  œÜ           N
Algorithm 2, with the exception that the tempering schedule {œÜn }n=1 is pre-determined. The
Do until n = NœÜ -loop is replaced by a For n = 1 to NœÜ -loop and Step 1(a)iii is eliminated. 

Algorithm 5 (RWMH Mutation Step ‚Äì Non-Adaptive) This algorithm is identical
                                                            œÜ         N
to Algorithm 3 with the exception that the sequence {cn , }n=1 is pre-determined. 


3.1     Asymptotic Properties

Under suitable regularity conditions, the Monte Carlo approximations generated by a particle
filter satisfy a SLLN and a CLT. Proofs for a generic particle filter are provided in Chopin
(2004). We will subsequently establish a SLLN for the tempered particle filter by modifying
the recursive proof developed by Chopin (2004) to account for the tempering iterations of
Algorithm 4. In this paper, we are primarily interested in establishing an almost-sure limit
for the Monte Carlo approximation of the likelihood function:
                                                 Ô£´                                       Ô£∂
                    T                        T                     NœÜ
                    Y                   a.s.
                                             Y                     Y   pn (yt |Y1:t‚àí1 ) Ô£∏
    pÃÇM (Y1:T ) =     pÃÇM (yt |Y1:t‚àí1 ) ‚àí‚Üí       Ô£≠p1 (yt |Y1:t‚àí1 )                         = p(Y1:T ).   (28)
                  t=1                        t=1
                                                                      p (y |Y
                                                                   n=2 n‚àí1 t 1:t‚àí1
                                                                                       )

Here the last equality follows because pNœÜ (yt |Y1:t‚àí1 ) = p(yt |Y1:t‚àí1 ) by definition. The limit
is obtained by letting the number of particles M ‚àí‚Üí ‚àû. We assume that the length of the
sample T is fixed. As a by-product, we also derive an almost-sure limit for Monte Carlo
approximations of moments of the filtered states:

                        M                          Z Z
                     1 X      j,n j,NœÜ    j,n a.s.
          hÃÑnt,M   =       h(st , st‚àí1 )Wt ‚àí‚Üí          h(st , st‚àí1 )pn (st , st‚àí1 |Y1:t )dst dst‚àí1 .     (29)
                     M j=1
    9
    Some asymptotic results for adaptive SMC algorithms are available in the literature, e.g., Herbst and
Schorfheide (2014) and Beskos, Jasra, Kantas, and Thiery (2014).
                                                                                                                            18


We use h(¬∑) to denote a generic function of both st and st‚àí1 for technical reasons that will
be explained below.10 Of course, a special case is a function that is constant with respect to
st‚àí1 . We simply denote such functions by h(st ).

       To guarantee the almost-sure convergence, we need to impose some regularity conditions
on the functions h(st , st‚àí1 ). We define the following classes of functions:
                                    Z
             Ht1   =  h(st , st‚àí1 )       Ep(¬∑|st‚àí1 ) [|h(st , st‚àí1 )|]p(st‚àí1 |Y1:t‚àí1 )dst‚àí1 < ‚àû,                          (30)
                                                                                             
                                                                                          1+Œ¥
                             ‚àÉŒ¥ > 0 s.t. Ep(¬∑|st‚àí1 ) h(st , st‚àí1 ) ‚àí Ep(¬∑|st‚àí1 ) [h]            < C < ‚àû,
                                                        
                                                  NœÜ
                             Ep(¬∑|st‚àí1 ) [h] ‚àà Ht‚àí1


and for n = 2, . . . , NœÜ :
                   
       Htn    =        h(st , st‚àí1 ) h(st , st‚àí1 ) ‚àà Htn‚àí1 ,                                                               (31)
                                                                                                           
                                                                                                      1+Œ¥
                              ‚àÉŒ¥ > 0 s.t. EKn (¬∑|sÃÇt ,st‚àí1 ) h(st , st‚àí1 ) ‚àí EKn (¬∑|sÃÇt ,st‚àí1 ) [h]             < C < ‚àû,
                                                                        
                                                                    n‚àí1
                              EKn (¬∑|sÃÇt ,st‚àí1 ) [h(st , st‚àí1 )] ‚àà Ht    .


By definition, HtnÃÉ ‚äÜ Htn for nÃÉ > n. The classes Htn are chosen such that the moment bounds
                                                                                                                    j,N
that guarantee the almost sure convergence of Monte Carlo averages of h(sj,n     œÜ
                                                                         t , st‚àí1 ) are

satisfied. The key assumption here is that there exists a uniform bound for the centered 1+Œ¥
conditional moment of the function h(st , st‚àí1 ) under the state-transition density p(st |st‚àí1 )
and the transition kernel of the mutation step of Algorithm 5, Kn (st |sÃÇt , st‚àí1 ). This will allow
us to apply a SLLN to the particles generated by the forward simulation of the model and
the mutation step in the tempering iterations.
                                                                                                                     N
       For the class H11 to be properly defined according to (30), we need to define H0 œÜ . Let
             N
H0 = H0 œÜ and note that Ep(¬∑|s0 ) [h] is a function of s0 only. Thus, we define
                                                         Z                        
                                     H0 = h(s0 )              |h(s0 )|p(s0 )ds0 < ‚àû .                                      (32)


Under the assumption that the initial particles are generated by i.i.d. sampling from p(s0 ),
  10
    Spoiler alert: we need the st‚àí1 because the Markov transition kernel generated by Algorithm 4 (or
Algorithm 2) is invariant under the distribution pn (st |yt , st‚àí1 ), which is conditioned on st‚àí1 , instead of the
distribution pn (st |Y1:t ).
                                                                                                19


the integrability conditions ensure that we can apply Kolmogorov‚Äôs SLLN. Throughout this
paper, we use C to denote a generic constant. Notice that any bounded function |h(¬∑)| < h
is an element of Htn for all t and n. Under the assumption that the measurement er-
rors have a multivariate normal distribution, the densities pn (yt |st ) and the density ratios
pn (yt |st )/pn‚àí1 (yt |st ) are bounded uniformly in st , which means that these functions are ele-
ments of all Htn .

   By changing the definition of the classes Htn and requiring moments of order 2+Œ¥ to exist,
the subsequent theoretical results can be extended to a CLT following arguments in Chopin
(2004) and Herbst and Schorfheide (2014). The CLT provides a justification for computing
numerical standard errors from the variation of Monte Carlo approximations across multiple
independent runs of the filter, but the formulas for the asymptotic variances have an awkward
recursive form that makes it infeasible to evaluate them. Thus, they are of limited use in
practice.


3.1.1     Algorithm 1

To prove the convergence of the Monte Carlo approximations generated in Step 2(a) of
Algorithm 1, we can use well established arguments for the bootstrap particle filter, which we
                                                                            a.s.
adapt from the presentation in Herbst and Schorfheide (2015). We use ‚àí‚Üí to denote almost-
sure convergence as M ‚àí‚Üí ‚àû. The starting point is the following recursive assumption:

                                          j,N    j,N
Assumption 1 The particle swarm {st‚àí1œÜ , Wt‚àí1œÜ } generated by the period t ‚àí 1 iteration of
Algorithm 1 approximates:

                              M                      Z
                 NœÜ        1 X      j,NœÜ   j,NœÜ a.s.
               hÃÑt‚àí1,M   =       h(st‚àí1 )Wt‚àí1 ‚àí‚Üí h(st‚àí1 )p(st‚àí1 |Y1:t‚àí1 )dst‚àí1 .              (33)
                           M j=1

                            œÜN
for functions h(st‚àí1 ) ‚àà Ht‚àí1 .


   In our statement of the recursive assumption, we only consider functions that vary with
st‚àí1 , which is why we write h(st‚àí1 ) (instead of h(st‚àí1 , st‚àí2 )). As discussed previously, if the
filter is initialized by direct sampling from p(s0 ), then the recursive assumption is satisfied
for t = 1. Conditional on the recursive assumption, we can obtain the following convergence
result:
                                                                                                            20


Lemma 1 Suppose that Assumption 1 is satisfied. Then for h ‚àà Ht1 :

                  M                              Z Z
               1 X       j,1 j,NœÜ     j,NœÜ a.s.
    hÃÇ1t,M   =       h(sÃÉt , st‚àí1 )Wt‚àí1 ‚àí‚Üí             h(st , st‚àí1 )p(st , st‚àí1 |Y1:t‚àí1 )dst dst‚àí1         (34)
               M j=1
               1
                 PM         j,1 j,NœÜ   j,1   j,NœÜ
                   j=1 h(sÃÉt , st‚àí1 )wÃÉt Wt‚àí1
                                                       Z Z
               M                                  a.s.
    hÃÉ1t,M   =         PM j,1 j,NœÜ                ‚àí‚Üí           h(st , st‚àí1 )p1 (st , st‚àí1 |Y1:t )dst dst‚àí1 (35)
                     1
                    M     j=1 twÃÉ  W t‚àí1
                  M                            Z Z
               1 X       j,1 j,NœÜ     j,1 a.s.
    hÃÑ1t,M   =       h(st , st‚àí1 )Wt ‚àí‚Üí              h(st , st‚àí1 )p1 (st , st‚àí1 |Y1:t )dst dst‚àí1 .         (36)
               M j=1

Moreover,

                                        M              Z
                                     1 X j,1 j,NœÜ a.s.
                 pÃÇ1 (yt |Y1:t‚àí1 ) =      wÃÉ Wt‚àí1 ‚àí‚Üí p1 (yt |st )p1 (st |Y1:t‚àí1 )dst .                    (37)
                                     M j=1 t


   A formal proof of Lemma 1 that verifies the moment conditions required for the almost-
sure convergence is provided in the Online Appendix. Subsequently, we provide the key steps
of the argument. Because we need to keep track of joint densities (st , st‚àí1 ), we define

                                                     pn (yt |st )p(st |st‚àí1 )p(st‚àí1 |Y1:t‚àí1 )
                 pn (st , st‚àí1 |Y1:t ) = R               R                                    .
                                             pn (yt |st ) p(st |st‚àí1 )p(st‚àí1 |Y1:t‚àí1 )dst‚àí1 dst

Here we used the fact that, according to the state-space model, the distribution of yt con-
ditional on st does not depend on (st‚àí1 , Y1:t‚àí1 ). Moreover, the distribution of st conditional
on st‚àí1 does not depend on Y1:t‚àí1 . Thus, integrating with respect to st‚àí1 yields
                                   Z
                                       pn (st , st‚àí1 |Y1:t )dst‚àí1 = pn (st |Y1:t ),

where pn (st |Y1:t ) was previously introduced in (8).

   The forward iteration of the state-transition equation amounts to drawing sjt from the
                j,N
density p(st |st‚àí1œÜ ). Use Ep(¬∑|sj,NœÜ ) [h] to denote expectations under this density and consider
                                    t‚àí1
                                                                                                         21


the decomposition:
                     Z Z
        hÃÇ1t,M   ‚àí         h(st , st‚àí1 )p(st , st‚àí1 |Y1:t‚àí1 )dst dst‚àí1                                  (38)
                M                                    
              1 X        j,1 NœÜ                           j,N
           =         h(sÃÉt , st‚àí1 ) ‚àí Ep(¬∑|sj,NœÜ ) [h] Wt‚àí1œÜ
             M j=1                           t‚àí1


                  M                             Z Z                                                
               1 X                       j,NœÜ
             +         Ep(¬∑|sj,NœÜ ) [h]Wt‚àí1 ‚àí           h(st , st‚àí1 )p(st , st‚àí1 |Y1:t‚àí1 )dst dst‚àí1
               M j=1         t‚àí1


           = I + II

                                                                                j,N         j,N
Both terms converge to zero. First, conditional on the particles {st‚àí1œÜ , Wt‚àí1œÜ }, the weights
  j,N
Wt‚àí1œÜ are known, and term I is an average of mean-zero random variables that are inde-
                                                                                                  N
pendently distributed. Second, the definition of Ht1 implies that Ep(¬∑|sj,NœÜ ) [h] ‚àà Ht‚àí1
                                                                                        œÜ
                                                                                          . Thus,
                                                                                      t‚àí1
we can deduce from Assumption 1 that term II converges to zero. This delivers (34). In
slight abuse of notation, we can now set h(¬∑) to either h(st )p1 (yt |st ) or p1 (yt |st ) to deduce
the convergence result required to justify the approximations in (35) and (37). Finally, the
SLLN is preserved by the resampling step, which delivers (36).


3.1.2      Algorithm 4

The convergence results for the tempering iterations rely on the following recursive assump-
tion, which according to Lemma 1 is satisfied for n = 2.

                                                             j,N
Assumption 2 The particle swarm {sj,n‚àí1
                                  t     , st‚àí1œÜ , Wtj,n‚àí1 } generated by iteration n ‚àí 1 of
Algorithm 4 approximates:

                M                               Z Z
             1 X      j,n‚àí1 j,NœÜ     j,n‚àí1 a.s.
   hÃÑn‚àí1
     t,M   =       h(st    , st‚àí1 )Wt      ‚àí‚Üí       h(st , st‚àí1 )pn‚àí1 (st , st‚àí1 |Y1:t )dst dst‚àí1       (39)
             M j=1

for functions h ‚àà Htn .


   The convergence results are stated in the following lemma:
                                                                                                                   22


Lemma 2 Suppose that Assumption 2 is satisfied. Then for h ‚àà Htn‚àí1 :
                            PM         j,n‚àí1 j,NœÜ
                       1
                       M        j=1 h(st     , st‚àí1 )wÃÉtj,n Wtj,n‚àí1
         hÃÉnt,M    =               1
                                     PM j,n j,n‚àí1                                                                 (40)
                                  M     j=1 wÃÉt Wt
                               Z Z
                        a.s.
                       ‚àí‚Üí              h(st , st‚àí1 )pn (st , st‚àí1 |Y1:t )dst dst‚àí1
                        M                           Z Z
                     1 X       j,n j,NœÜ    j,n a.s.
         hÃÇnt,M    =       h(sÃÇt , st‚àí1 )Wt ‚àí‚Üí          h(st , st‚àí1 )pn (st , st‚àí1 |Y1:t )dst dst‚àí1 .             (41)
                     M j=1

Moreover,
                                                         M
                                 t |Y1:t‚àí1 )
                            pn (y\                    1 X j,n j,n‚àí1 a.s. pn (yt |Y1:t‚àí1 )
                                               
                                                    =      wÃÉ Wt    ‚àí‚Üí                                            (42)
                           pn‚àí1 (yt |Y1:t‚àí1 )         M j=1 t            pn‚àí1 (yt |Y1:t‚àí1 )

and for h ‚àà Htn ,

                        M                          Z Z
                     1 X      j,n j,NœÜ    j,n a.s.
          hÃÑnt,M   =       h(st , st‚àí1 )Wt ‚àí‚Üí          h(st , st‚àí1 )pn (st , st‚àí1 |Y1:t )dst dst‚àí1 .              (43)
                     M j=1


   The convergence in (43) implies that the recursive Assumption 2 is satisfied for iteration
n + 1 of Algorithm 4. Thus, we deduce that the convergence in (43) holds for n = NœÜ .
This, in turn, implies that if the recursive Assumption 2 for Algorithm 1 is satisfied at the
beginning of period t, it will also be satisfied at the beginning of period t + 1. A formal proof
of Lemma 2 is provided in the Online Appendix. We will provide an outline of the argument
below.

Correction and Selection Steps. The convergence of the approximations in (40) and
(42), obtained after executing the correction step, follows from the recursive Assumption 2
                                         j,N                                        j,N
and the fact that h(sj,n‚àí1
                     t     , st‚àí1œÜ ) ‚àà Htn‚àí1 and h(sj,n‚àí1
                                                    t     , st‚àí1œÜ )wÃÉtj,n ‚àà Htn‚àí1 . Furthermore, it
relies on the following calculation:
                       RR                   pn (yt |st )
                             h(st , st‚àí1 ) pn‚àí1           p (s , s |Y )dst dst‚àí1
                                                (yt |st ) n‚àí1 t t‚àí1 1:t
                                      R pn (yt |st )                                                              (44)
                                                         p (s |Y )dst
                                         pn‚àí1 (yt |st ) n‚àí1 t 1:t
                               RR                     pn (yt |st ) pn‚àí1 (yt |st )p(st ,st‚àí1 |Y1:t‚àí1 )
                                     h(st , st‚àí1 ) pn‚àí1    (yt |st )     pn‚àí1 (yt |Y1:t‚àí1 )
                                                                                                      dst dst‚àí1
                           =                 R pn (yt |st ) pn‚àí1 (yt |st )p(st |Y1:t‚àí1 )
                                                 pn‚àí1 (yt |st )      pn‚àí1 (yt |Y1:t‚àí1 )
                                                                                             dst
                               RR
                                     h(st , st‚àí1 )pn (yt |st )p(st , st‚àí1 |Y1:t‚àí1 )dst dst‚àí1
                           =                  R
                                                  pn (yt |st )p(st |Y1:t‚àí1 )dst
                              Z Z
                           =          h(st , st‚àí1 )pn (st , st‚àí1 |Y1:t )dst dst‚àí1 .
                                                                                                               23


The first equality is obtained by reversing Bayes Theorem and expressing the ‚Äúposterior‚Äù
pn‚àí1 (st , st‚àí1 |Y1:t ) as the product of ‚Äúlikelihood‚Äù pn‚àí1 (yt |st ) and ‚Äúprior‚Äù p(st , st‚àí1 |Y1:t‚àí1 ) di-
vided by the ‚Äúmarginal likelihood‚Äù pn‚àí1 (yt |Y1:t‚àí1 ). We then cancel the pn‚àí1 (yt |st ) and the
marginal likelihood terms to obtain the second equality. Finally, an application of Bayes
Theorem leads to the third equality.

     Recall that pNœÜ (yt |Y1:t‚àí1 ) = p(yt |Y1:t‚àí1 ) by construction and that an approximation of
p1 (yt |Y1:t‚àí1 ) is generated in Step 2(a)iii of Algorithm 1. Together, this leads to the approxi-
mation of the likelihood increment p(yt |Y1:t‚àí1 ) in (23). Resampling after the correction step
preserves the SLLN, which delivers (41).

Mutation Step. We now outline how to establish (43). Let
                                                            Z
                                 EKn (¬∑|sÃÇt ;st‚àí1 ) [h] =       h(st , st‚àí1 )Kn (st |sÃÇt ; st‚àí1 )dst ,

which is a function of (sÃÇt , st‚àí1 ). We can decompose the Monte Carlo approximation from
the mutation step as follows:

            M                            Z Z
         1 X      j,n j,NœÜ     j,n
               h(st , st‚àí1 )Wt ‚àí                 h(st , st‚àí1 )pn (st , st‚àí1 |Y1:t )dst dst‚àí1                  (45)
         M j=1
                 M                                               
               1 X        j,n j,NœÜ
          =            h(st , st‚àí1 ) ‚àí EK (¬∑|sÃÇj,n ;sj,NœÜ ) [h] Wtj,n
              M j=1                              n  t     t‚àí1


                    M                              Z Z                                                
                1 X
             +           EK (¬∑|sÃÇj,n ;sj,NœÜ ) [h] ‚àí         h(st , st‚àí1 )pn (st , st‚àí1 |Y1:t )dst dst‚àí1 Wtj,n
                M j=1       n    t     t‚àí1


            = I + II,          say.

                                                                                 j,N
By construction, conditional on the particles {sÃÇj,n     œÜ    j,n
                                                 t , st‚àí1 , Wt }, term I is an average of

independent mean-zero random variables, which converges to zero.

     The analysis of term II is more involved for two reasons. First, as previously highlighted,
EK          j,n j,NœÜ    [h] is a function not only of sÃÇt but also of st‚àí1 . Second, while the invariance
     n (¬∑|sÃÇt ;st‚àí1 )
                                                                                                              24


property (21) implies that
                     Z
                         EKn (¬∑|sÃÇt ;st‚àí1 ) [h]pn (sÃÇt |yt , st‚àí1 )dsÃÇt                                      (46)
                           Z Z                                                 
                         =              h(st , st‚àí1 )Kn (st |sÃÇt ; st‚àí1 )dst pn (sÃÇt |yt , st‚àí1 )dsÃÇt
                           Z                     Z                                                  
                         =      h(st , st‚àí1 )            Kn (st |sÃÇt ; st‚àí1 )pn (sÃÇt |yt , st‚àí1 )dsÃÇt dst
                           Z
                         =      h(st , st‚àí1 )pn (st |yt , st‚àí1 )dst ,

                                     j,N
the summation over (sÃÇj,n     œÜ    j,n
                      t , st‚àí1 , Wt ) generates an integral with respect to pn (st , st‚àí1 |Y1:t )

instead of pn (st |yt , st‚àí1 ); see (41).

    To obtain the expected value of EKn (¬∑|sÃÇt ;st‚àí1 ) [h] under the distribution pn (sÃÇt , st‚àí1 |Y1:t ),
notice that

                      pn (st , st‚àí1 |Y1:t ) = pn (st , st‚àí1 |yt , Y1:t‚àí1 )                                   (47)
                                               = pn (st |st‚àí1 , yt , Y1:t‚àí1 )p(st‚àí1 |yt , Y1:t‚àí1 )
                                               = pn (st |st‚àí1 , yt )p(st‚àí1 |yt , Y1:t‚àí1 ).

The last equality holds because, using the first-order Markov structure of the state-space
model, we can write

                                                       pn (yt |st , st‚àí1 , Y1:t‚àí1 )p(st |st‚àí1 , Y1:t‚àí1 )
              pn (st |yt , st‚àí1 , Y1:t‚àí1 ) = R
                                                       p (y |s , s , Y1:t‚àí1 )p(st |st‚àí1 , Y1:t‚àí1 )dst
                                                     st n t t t‚àí1
                                                       pn (yt |st )p(st |st‚àí1 )
                                              = R
                                                       p (y |s )p(st |st‚àí1 )dst
                                                     st n t t
                                              = pn (st |yt , st‚àí1 ).


    Therefore, we obtain
               Z Z
                   EKn (¬∑|sÃÇt ;st‚àí1 ) [h]pn (sÃÇt , st‚àí1 |Y1:t )dsÃÇt dst‚àí1                                    (48)
                   Z Z                                                      
                 =             EKn (¬∑|sÃÇt ;st‚àí1 ) [h]pn (sÃÇt |yt , st‚àí1 )dsÃÇt pn (st‚àí1 |yt , Y1:t‚àí1 )dst‚àí1
                   Z Z                                                
                 =             h(st , st‚àí1 )pn (st |yt , st‚àí1 )dst pn (st‚àí1 |yt , Y1:t‚àí1 )dst‚àí1
                   Z Z
                 =        h(st , st‚àí1 )pn (st , st‚àí1 |Y1:t )dst dst‚àí1 .
                                                                                                         25


The first equality uses (47). The second equality follows from the invariance property (46).
For the third equality, we used (47) again. Thus, under suitable regularity conditions, term
II also converges to zero almost surely, which leads to the convergence in (43).

   We can deduce from Lemmas 1 and 2 that we obtain almost-sure approximations of the
likelihood increment for every period t = 1, . . . , T . Because T is fixed and pNœÜ (yt |Y1:t‚àí1 ) =
p(yt |Y1:t‚àí1 ), we obtain the following theorem:


Theorem 1 Consider the nonlinear state-space model (1) with Gaussian measurement er-
rors. Suppose that the initial particles are generated by i.i.d. sampling from p(s0 ). Then
the Monte Carlo approximation of the likelihood function generated by Algorithms 1, 4, 5 is
consistent:
                                                Ô£´                                       Ô£∂
                 T                          T                     NœÜ
                 Y                     a.s.
                                            Y                     Y   pn (yt |Y1:t‚àí1 ) Ô£∏
   pÃÇM (Y1:T ) =     pÃÇM (yt |Y1:t‚àí1 ) ‚àí‚Üí       Ô£≠p1 (yt |Y1:t‚àí1 )                         = p(Y1:T ).   (49)
                 t=1                        t=1
                                                                     p (y |Y
                                                                  n=2 n‚àí1 t 1:t‚àí1
                                                                                      )


3.2    Unbiasedness

Particle filter approximations of the likelihood function are often embedded into posterior
samplers for the parameter vector Œ∏, e.g., a Metropolis-Hastings algorithm or a SMC algo-
rithm; see Herbst and Schorfheide (2015) for a discussion and further references in the context
of DSGE models. A necessary condition for the convergence of the posterior sampler is that
the likelihood approximation of the particle filter is unbiased.


Theorem 2 Suppose that the tempering schedule is deterministic and that the number of
stages NœÜ is the same for each time period t ‚â• 1. Then, the particle filter approximation of
the likelihood generated by Algorithm 1 is unbiased:
                                 Ô£Æ Ô£´                                  !Ô£∂Ô£π
                                   T   NœÜ              M
                                  Y    Y            1 X
                                                         wÃÉj,n Wtj,n‚àí1 Ô£∏Ô£ª = p(Y1:T ).
                           
               E pÃÇM (Y1:T ) = E Ô£∞   Ô£≠                                                                  (50)
                                      t=1    n=1
                                                    M j=1 t


   A proof of Theorem 2 unbiasedness is provided in the Online Appendix. Our proof ex-
ploits the recursive structure of the algorithm and extends the proof by Pitt, Silva, Giordani,
and Kohn (2012) to account for the tempering iterations.
                                                                                                       26


4        DSGE Model Applications

In this section, we assess the performance of the tempered particle filter (TPF) and the
bootstrap particle filter (BSPF). The principle point of comparison is the accuracy of the
approximation of the likelihood function, but we will also assess each filter‚Äôs ability to track
the filtered states. We consider two models in the subsequent analysis. The first is a small-
scale New Keynesian DSGE model that comprises a consumption Euler equation, a New
Keynesian Phillips curve, a monetary policy rule, and three exogenous shock processes. The
second model is the medium-scale DSGE model by Smets and Wouters (2007), which is the
core of many of the models that are being used in academia and at central banks.

       While the exposition of the algorithms in this paper focuses on the nonlinear state-space
model (1), the numerical illustrations are based on linearized versions of the DSGE models.
Linearized DSGE models (with normally distributed innovations) lead to a linear Gaussian
state-space representation. This allows us to use the Kalman filter to compute the exact
values of the likelihood function p(Y1:T |Œ∏) and the filtered states E[st |Y1:t , Œ∏].

       We assess the accuracy of the particle filter approximations by running the filters repeat-
edly and studying the sampling distribution of their output across independent runs. To
evaluate the accuracy of pÃÇM (Y1:T |Œ∏) we consider two statistics. The first is the log likelihood
approximation error,
                                  ÀÜ 1 = ln pÃÇM (Y1:T |Œ∏) ‚àí ln p(Y1:T |Œ∏).
                                  ‚àÜ                                                                  (51)

Because the particle filter approximation of the likelihood function is unbiased (see Theo-
rem 2), Jensen‚Äôs inequality applied to the concave logarithmic transformation implies that
                      ÀÜ 1 is negative. Second, we consider the following statistic:11
the expected value of ‚àÜ

                   ÀÜ 2 = pÃÇM (Y1:T |Œ∏) ‚àí 1 = exp[ln pÃÇM (Y1:T |Œ∏) ‚àí ln p(Y1:T |Œ∏)] ‚àí 1.
                   ‚àÜ                                                                                 (52)
                           p(Y1:T |Œ∏)

                   ÀÜ 2 requires us to exponentiate the difference in log-likelihood values,
The computation of ‚àÜ
which is feasible if the particle filter approximation is reasonably accurate. The unbiasedness
result implies that the sampling mean of ‚àÜ    ÀÜ 2 should be close to zero.

   In our experiments, we run the filters Nrun = 100 times and examine the sampling
                                ÀÜ 1 and ‚àÜ
properties of the discrepancies ‚àÜ       ÀÜ 2 . Because there is always a trade-off between
  11
   Assessing the bias of the likelihood function pÃÇM (Y1:T |Œ∏) directly is numerically challenging because
exponentiating a log-likelihood value of around ‚àí300 leads to a missing value using standard software.
                                                                                                            27


accuracy and speed, we also assess the run-time of the filters.12 The run-time of any particle
filter is sensitive to the exact computing environment used. Thus, we provide details about
the implementation in the Online Appendix. In this regard, it is important to note that
the tempered particle filter is designed to work with a small number of particles (i.e., on a
desktop computer). Therefore, we restrict the computing environment to a single machine,
and we do not try to leverage large-scale parallelism via a computing cluster, as in Gust,
Herbst, Lopez-Salido, and Smith (forthcoming).
       As described in Section 2.3, implementing the bootstrap particle filter requires choosing
the number of particles M , while the tempered particle filter requires additionally choosing
the tuning parameters r‚àó , c‚àó , and NM H . We discuss these choices and their effect on the
accuracy of the filters below. Results for the small-scale New Keynesian DSGE model are
presented in Section 4.1. In Section 4.2, the tempered particle filter is applied to the Smets-
Wouters model.


4.1       A Small-Scale DSGE Model

We first use the BSPF and the TPF to evaluate the likelihood function associated with
the small-scale New Keynesian DSGE model used in Herbst and Schorfheide (2015). The
details about the model can be found in the Online Appendix. From the perspective of the
particle filter, the key feature of the model is that it has three observables (output growth,
inflation, and the federal funds rate). To facilitate the use of particle filters, we augment
the measurement equation of the DSGE model by independent measurement errors, whose
standard deviations we set to be 20% of the standard deviation of the observables.13
Great Moderation Sample. The data span is 1983Q1 to 2002Q4, for a total of 80 obser-
vations for each series. We assess the performance of the particle filters for two parameter
vectors, which are denoted by Œ∏m and Œ∏l and tabulated in Table 1. The value Œ∏m is chosen as
a high likelihood point, close to the posterior mode of the model. The log likelihood at Œ∏m
is ln p(Y |Œ∏m ) = ‚àí306.49. The second parameter value, Œ∏l , is chosen to be associated with a
lower log-likelihood value. Based on our choice, ln p(Y |Œ∏l ) = ‚àí313.36. The sample and the
parameter values are identical to those used in Chapter 8 of Herbst and Schorfheide (2015).
  12
     The run-times reported below do not account for the fact that the user of the TPF might experiment
with the choice of tuning constants. Moreover, the computing times for both filters will increase if the linear
solution is replaced by a nonlinear solution. The larger the time it takes to solve the model, the smaller the
percentage reduction in combined run-time for solution and filter attainable by the TPF.
  13
     The measurement error standard deviations are 0.1160 for output growth, 0.2942 for inflation, and 0.4476
for the interest rates.
                                                                                              28


                       Table 1: Small-Scale Model: Parameter Values

                  Parameter    Œ∏m     Œ∏l            Parameter      Œ∏m       Œ∏l
                  œÑ           2.09   3.26           Œ∫             0.98     0.89
                  œà1          2.25   1.88           œà2            0.65     0.53
                  œÅr          0.81   0.76           œÅg            0.98     0.98
                  œÅz          0.93   0.89           r(A)          0.34     0.19
                  œÄ (A)       3.16   3.29           Œ≥ (Q)         0.51     0.73
                  œÉr          0.19   0.20           œÉg            0.65     0.58
                  œÉz          0.24   0.29           ln p(Y |Œ∏)   -306.5   -313.4



   We compare the BSPF with two variants of the TPF, which differ with respect to the
targeted inefficiency ratio: r‚àó = 2 and r‚àó = 3. For the BSPF, we use M = 40, 000 particles,
and for the TPF, we consider M = 4, 000 and M = 40, 000 particles, respectively. In
Algorithm 3, we use NM H = 1 Metropolis-Hastings steps and set the initial scale of the
proposal covariance matrix to c‚àó = 0.3. We also report results for two related algorithms.
The first algorithm is the resample-move variant of the bootstrap particle filter (RMPF)
described in Section 2.3 which sets r‚àó = ‚àû. Recall that this algorithm does not utilize
any bridge distributions, but unlike the BSPF it involves a mutation step that changes the
particle values. The second algorithm is the conditionally-optimal particle filter (COPF).
Recall that the implementation of the COPF is generally infeasible for nonlinear DSGE
models. However, in our particular setting in which we are using a linearized DSGE model,
we can directly sample from p(st |yt , sjt‚àí1 ) using a Kalman filter updating step and compare
the accuracy of the proposed TPF to this infeasible benchmark.
                                                                        ÀÜ 1 associated with
   Figure 1 displays density estimates for the sampling distribution of ‚àÜ
the BSPF and the four variants of the TPF for Œ∏ = Œ∏m (left panel) and Œ∏ = Œ∏l (right panel).
For Œ∏ = Œ∏m , the T P F (r‚àó = 2) with M = 40, 000 (the green line) is the most accurate of
                                 ÀÜ 1 distributed tightly around zero. The distribution of ‚àÜ
all the filters considered, with ‚àÜ                                                        ÀÜ1
associated with T P F (r‚àó = 3) with M = 40, 000 is slightly more disperse, with a larger left
tail, as the higher tolerance for particle inefficiency translates into a higher variance for the
likelihood estimate. Reducing the number of particles to M = 4, 000 for both of these filters
results in a higher variance estimate of the likelihood. The most poorly performing TPF
                                                                   ÀÜ 1 that is similar to the
(with r‚àó = 3 and M = 4, 000) is associated with a distribution for ‚àÜ
one associated with the BSPF that uses M = 40, 000. Overall, the TPF compares favorably
with the BSPF when Œ∏ = Œ∏m .
                                                                                                                                                                          29


            Figure 1: Small-Scale Model: Distribution of Log-Likelihood Approximation Errors

                                       Œ∏ = Œ∏m                                                                            Œ∏ = Œ∏l
                                                                                                    0.6
            0.8
                                                                                                                                                 TPF (r ‚àó = 2)M = 40000
            0.7                                                                                     0.5

            0.6                                                TPF (r ‚àó = 2), M = 40000
                                                                                                    0.4
            0.5                 TPF (r ‚àó = 3), M = 40000
                                                                                                                            TPF (r ‚àó = 3), M = 40000




                                                                                          Density
  Density




            0.4                                                                                     0.3


            0.3                TPF (r ‚àó = 2), M = 4000                                                                      TPF (r ‚àó = 2), M = 4000
                                                                                                    0.2

            0.2                                                                                                      TPF (r ‚àó = 3), M = 4000
                          TPF (r ‚àó = 3), M = 4000
                                                                                                    0.1
                                                                                                                BSPF , M = 40000
            0.1
                    BSPF , M = 40000
            0.0                                                                                     0.0
              ‚àí10   ‚àí8       ‚àí6        ‚àí4           ‚àí2     0        2         4                           ‚àí15      ‚àí10                 ‚àí5                  0




                           ÀÜ 1 = ln pÃÇ(Y1:T |Œ∏m ) ‚àí ln p(Y1:T |Œ∏m ) based on Nrun = 100 runs of
Notes: Density estimate of ‚àÜ
the particle filter.

        The performance differences become even more stark when we consider Œ∏ = Œ∏l ; depicted
in the right panel of Figure 1. While the sampling distributions indicate that the likelihood
estimates are less accurate for all the particles filters, the BSPF deteriorates by the largest
amount. The TPF, by targeting an inefficiency ratio, adaptively adjusts to account for the
relatively worse fit of Œ∏l .

        The results are also shown in Table 2, which displays summary statistics for the two
types of likelihood approximation errors as well as information about the average number
                                                       ÀÜ 1 convey essentially the same story
of stages and run time of each filter. The results for ‚àÜ
                                      ÀÜ 2 highlights the performance deterioration of the
as Figure 1. The bias associated with ‚àÜ
BSPF when considering Œ∏ = Œ∏l . The bias of 2.57 is substantially larger than for any of the
TPFs. Using a resample-move step without the bridge distribution (RMPF) leads to only
a slightly more accurate likelihood estimate than the BSPF, despite having a sustantially
longer run time (we are using NM H = 10 for this algorithm). Finally, the generally infeasible
COPF with M = 400 particles is an order of magnitude more accurate than all the other
filters. Thus, while the use of the tempering iterations leads to a significant improvement in
accuracy, it remains substantially worse than the COPF.

        The results in Table 2 are comparable to those reported in Table 8.2 of Herbst and
Schorfheide (2015). For the COPF the numbers are very similar, while for the BSPF the
differences are a bit larger. The discrepancies are due to the fact that we are computing
                                                                                         30


                   Table 2: Small-Scale Model: PF Summary Statistics


                               BSPF                     TPF                RMPF COPF
  Number of Particles M        40,000     4,000 4,000 40,000 40,000        40,000 400
  Target Ineff. Ratio r‚àó                       2     3       2    3            ‚àû
                                                            m
                              High Posterior Density: Œ∏ = Œ∏
       ÀÜ1
  Bias ‚àÜ                        -1.48      -1.19 -1.48 -0.15 -0.18           -1.42   -0.12
  StdD ‚àÜÀÜ1                       1.91       1.39 1.70 0.46 0.58               1.79    0.35
       ÀÜ2
  Bias ‚àÜ                        -0.14      -0.28 -0.28 -0.05 -0.01           -0.16   -0.05
  T ‚àí1 Tt=1 NœÜ,t
      P
                                 1.00       4.31 3.24 4.31 3.24               1.00    1.00
  Average Run Time (s)           0.62       0.29 0.23 3.28 2.19               2.09    0.02
                              Low Posterior Density: Œ∏ = Œ∏l
       ÀÜ1
  Bias ‚àÜ                        -6.56      -2.67 -4.14 -0.53 -0.72           -5.59   -0.16
  StdD ‚àÜÀÜ1                       5.27       2.02 2.57 0.95 1.16               4.07    0.40
       ÀÜ2
  Bias ‚àÜ                         2.57      -0.63 -0.48 -0.07 -0.13            0.10   -0.08
  T ‚àí1 Tt=1 NœÜ,t
      P
                                 1.00       4.37 3.29 4.35 3.28               1.00    1.00
  Average Run Time (s)           0.60       0.30 0.24 2.82 2.33               2.08    0.02

Notes: The results are based on Nrun = 100 independent runs of the particle filters. The
                         ÀÜ 1 and ‚àÜ
likelihood discrepancies ‚àÜ       ÀÜ 2 are defined in (51) and (52).


the means and standard deviations of the approximation errors based on ‚Äúonly‚Äù Nrun = 100
independent runs of the algorithms. In Table 8.2 of Herbst and Schorfheide (2015) we also
showed results for an auxiliary PF (see Pitt and Shephard (1999) and Doucet and Johansen
(2011)), but we found that the auxiliary PF did not improve much over the BSPF.

   The row labeled T ‚àí1 Tt=1 NœÜ,t shows the average number of tempering iterations asso-
                        P

ciated with each particle filter. The BSPF has, by construction, always an average of one.
When r‚àó = 2, the TPFs use a bit more than 4 stages per time period. With a higher tol-
erance for inefficiency, when r‚àó = 3, the average number of stages falls to about 3.25. Note
that when considering Œ∏l , the TPF always uses a greater number of stages, reflecting the
relatively worse fit of the model under Œ∏ = Œ∏l compared with Œ∏ = Œ∏m . Table 2 also displays
the average run time of each filter (in seconds). When using the same number of particles,
the BSPF runs much more quickly than the TPFs, reflecting the fact that the additional
tempering iterations require many more likelihood evaluations, in addition to the computa-
tional costs associated with the mutation phase. For a given level of accuracy, however, the
TPF requires many fewer particles. For instance, using M = 4, 000, the TPF yields more
precise likelihood estimates than the BSPF using M = 40, 000 and takes less than half the
                                                                                               31


                  Figure 2: Small-Scale Model: Accuracy of Filtered State

                         3.5
                         3.0
                         2.5
                         2.0
                         1.5
                         1.0
                         0.5                       BSPF, M=40k          TPF(r*=2), M=4k
                                                   TPF(r*=2), M=40k     TPF(r*=3), M=4k
                                                   TPF(r*=3), M=40k
                         0.0
                               1984    1989        1994               1999

Notes: The figure depicts RMSEs associated with EÃÇ[gÃÇt |Y1:t ]. Results are based on Nrun = 100
independent runs of the particle filters.

time to run.

   Finally, we consider the accuracy of the filtered state estimates. We consider the latent
government spending shock as a prototypical hidden state. Using the Kalman filter, we
can compute E[gÃÇt |Y1:T ], which we compare to the particle filter approximation, denoted by
EÃÇ[gÃÇt |Y1:T ]. Figure 2 plots root-mean-squared errors (RMSEs) for EÃÇ[gÃÇt |Y1:T ]. The ranking of
the filters is consistent with the ranking based on the accuracy of the likelihood approxima-
tions. The BSPF performs the worst. Using the TPF with M = 40, 000 particles reduces
the RMSE roughly by a factor of three.

Great Recession Sample. It is well known that the BSPF is very sensitive to outliers.
To examine the extent to which this is also true for the tempered particle filter, we re-run
the previous experiments on the sample 2003Q1 to 2013Q4. This period includes the Great
Recession, which was a large outlier from the perspective of the small-scale DSGE model
(and most other econometric models).

   Figure 3 plots the density of the approximation errors of the log likelihood estimates
associated with each of the filters. The difference in the distribution of approximation errors
between the BSPF and the TPFs is massive. For Œ∏ = Œ∏m and Œ∏ = Œ∏l , the approximation
errors associated with the BSPF are concentrated in the range of ‚àí200 to ‚àí300, almost two
orders of magnitude larger than the errors associated with the TPFs. This happens because
the large drop in output in 2008Q4 is not predicted by the forward simulation in the BSPF.
                                                                                                                                                                            32


Figure 3: Small-Scale Model: Distribution of Log-Likelihood Approximation Errors, Great
Recession Sample

                                   Œ∏ = Œ∏m                                                                                  Œ∏ = Œ∏l
            0.30                                                                                 0.25


            0.25                                        TPF (r ‚àó = 2), M = 40000
                                                                                                 0.20                                        TPF (r ‚àó = 2), M = 40000


            0.20
                                                                                                 0.15
                                                        TPF (r ‚àó = 3), M = 40000                                                             TPF (r ‚àó = 3), M = 40000
  Density




                                                                                       Density
            0.15

                                                                                                 0.10
            0.10                                        TPF (r ‚àó = 2), M = 4000
                                                                                                                                             TPF (r ‚àó = 2), M = 4000

                                                                                                 0.05
            0.05                                        TPF (r ‚àó = 3), M = 4000
                                                                                                                                             TPF (r ‚àó = 3), M = 4000
                              BSPF , M = 40000
                                                                                                            BSPF , M = 40000
            0.00                                                                                 0.00
              ‚àí350   ‚àí300   ‚àí250    ‚àí200         ‚àí150    ‚àí100        ‚àí50           0               ‚àí350   ‚àí300    ‚àí250         ‚àí200   ‚àí150      ‚àí100        ‚àí50         0




                           ÀÜ 1 = ln pÃÇ(Y1:T |Œ∏m ) ‚àí ln p(Y1:T |Œ∏m ) based on Nrun = 100 runs of
Notes: Density estimate of ‚àÜ
the particle filters.

In turn, the filter collapses, in the sense that the likelihood increment in that quarter is
estimated using essentially only one particle.

       Table 3 tabulates the results for each of the filters. Consistent with Figure 3 the bias
associated with the log-likelihood estimate is ‚àí215 and ‚àí279 for Œ∏ = Œ∏m and Œ∏ = Œ∏l ,
respectively, compared with about ‚àí8 and ‚àí10 for the worst performing TPF. For Œ∏ = Œ∏m ,
the T P F (r‚àó = 2) with M = 40, 000 has a bias only of ‚àí2.8 with a standard deviation of
1.5, which is about 25 times smaller than the BSPF. It is true that this variant of the filter
takes about 6 times longer to run than the BSPF, but even when considering M = 4, 000
particles, the TPF estimates are still overwhelmingly more accurate ‚Äì and are computed
more quickly ‚Äì than the BSPF estimates. A key driver of this result is the adaptive nature
of the tempered particle filter. While the average number of stages used is about 5 for r‚àó = 2
and 4 for r‚àó = 3, for t = 2008Q4 ‚Äì the period with the largest outlier ‚Äì the tempered particle
filter uses about 15 stages, on average.

       Figure 4 provides an illustration of why the TPF provides much more accurate approxima-
tions than the BSPF. We focus on one particular state, namely model-implied output growth,
which is observed output growth minus its measurement error. We focus on t = 2008Q4. The
left panel depicts the BSPF approximations pÃÇ(st |Y1:t‚àí1 ) and pÃÇ(st |Y1:t ) as well as the ‚Äútrue‚Äù
density p(st |Y1:t ). The BSPF essentially generates draws from the forecast density pÃÇ(st |Y1:t‚àí1 )
and reweights them to approximate the density p(st |Y1:t ). In 2008Q4, these densities have
                                                                                                                                     33


        Table 3: Small-Scale Model: PF Summary Statistics ‚Äì The Great Recession


                                                            BSPF     TPF
                 Number of Particles M                      40,000
                                                          4,000 4,000 40,000 40,000
                 Target Ineff. Ratio r‚àó                       2     3      2      3
                                                                 m
                                   High Posterior Density: Œ∏ = Œ∏
                      ÀÜ
                 Bias ‚àÜ1                     -215.63      -5.93 -7.91 -2.84 -4.27
                       ÀÜ
                 StdD ‚àÜ1                       36.74       3.01 3.36 1.55 1.80
                      ÀÜ
                 Bias ‚àÜ2                       -1.00      -0.85 -0.95 -0.71 -0.91
                   ‚àí1
                      PT
                 T      t=1 NœÜ,t                1.00       5.12 3.87 5.08 3.86
                 Average Run Time (s)           0.38       0.28 0.18 2.28 2.09
                                    Low Posterior Density: Œ∏ = Œ∏l
                      ÀÜ1
                 Bias ‚àÜ                      -279.12      -7.26 -9.98 -3.81 -5.82
                       ÀÜ
                 StdD ‚àÜ1                       41.74       3.44 4.22 1.68 2.15
                      ÀÜ
                 Bias ‚àÜ2                       -1.00      -0.89 -0.99 -0.86 -0.98
                   ‚àí1
                      PT
                 T      t=1 NœÜ,t                1.00       5.36 4.04 5.33 4.03
                 Average Run Time (s)           0.37       0.29 0.23 2.40 2.10

Notes: The results are based on Nrun = 100 independent runs of the particle filters. The
                         ÀÜ 1 and ‚àÜ
likelihood discrepancies ‚àÜ       ÀÜ 2 are defined in (51) and (52).


                     Figure 4: Small-Scale Model: BSPF versus TPF in 2008Q4

                              BSPF                                                                  TPF

           3.5                                                  3.5
                                    Forecast Density
           3.0                      BSPF Filtered Density       3.0
                                    True Filtered Density       2.5
           2.5
                                                                2.0
           2.0                                                  1.5
                                                                1.0
           1.5
                                                                0.5
           1.0                                                   0.0
                                                                   1.0
           0.5                                                           0.8
                                                                                                                                 2
                                                                               0.6                                           1
                                                                               œÜn    0.4                            ‚àí1   0
           0.0                                                                             0.2                 ‚àí2
                 4    3   2     1       0        1          2                                    0.0 ‚àí4   ‚àí3



Notes: Here t = 2008Q4 and st equals model-implied output growth. Left panel: forecast
density pÃÇ(st |Y1:t‚àí1 ), BSPF filtered density pÃÇ(st |Y1:t ), and true filtered density p(st |Yt‚àí1 ).
Right panel: forecast density pÃÇ(st |Y1:t‚àí1 ) (blue), waterfall plot of density estimates pÃÇn (st |Y1:t )
for n = 1, . . . , NœÜ , and true filtered density p(st |Y1:t ) (red).

very little overlap. This implies that essentially one draw from the forecast density receives
all the weight and the BSPF filtered density is a point mass. This point mass provides a
                                                                                              34


poor approximation of p(st |Y1:t ).

   The right panel of Figure 4 displays a waterfall plot of density estimates pÃÇn (st |Y1:t ) for
n = 1, . . . , NœÜ = 15. The densities are placed on the y-axis at the corresponding value of
œÜn . The first iteration in the tempering phase has œÜ1 = 0.002951, which corresponds to an
inflation of the measurement error variance by a factor over 300. This density looks similar
to the predictive distribution p(st |Y1:t‚àí1 ), with a 1-step-ahead prediction for output growth
of about ‚àí1% (in quarterly terms). As we move through the iterations, œÜn increases slowly
at first and pÃÇn (st |Y1:t ) gradually adds more density where st ‚âà ‚àí2.5. Each correction step of
Algorithm 2 requires only modest reweighting of the particles and the mutation steps refresh
the particle values. The filter begins to tolerate relatively large changes from œÜn to œÜn+1 ,
as more particles lie in this region, needing only three stages to move from œÜn ‚âà 0.29 to
œÜN = 1. Alongside pÃÇNœÜ (st |Y1:t ) we also show the true filtered density in red, obtained from
the Kalman filter recursions. The TPF approximation at n = NœÜ matches the true density
extremely well.


4.2    The Smets-Wouters Model

We next assess the performance of the tempered particle filter for the Smets and Wouters
(2007), henceforth SW, model. This model forms the core of the latest vintage of DSGE
models. While we leave the details of the model to the Online Appendix, it is important
to note that the SW model is estimated over the period 1966Q1 to 2004Q4 using seven
observables: the real per capita growth rates of output, consumption, investment, wages;
hours worked; inflation; and the federal funds rate. Moreover, the SW model has a high-
dimensional state space st with more than a dozen state variables. The performance of the
BSPF deteriorates quickly due to the increased state space and the fact that it is much more
difficult to predict seven observables than it is to predict three observables with a DSGE
model. As a consequence, the estimation of nonlinear variants of the SW model has proven
to be extremely difficult.

   We compute the particle filter approximations conditional on two sets of parameter val-
ues, Œ∏m and Œ∏l , which are summarized in Table 4. Œ∏m is the parameter vector associated with
the highest likelihood value among the draws that we generated with a posterior sampler.
Œ∏l is a parameter vector that attains a lower likelihood value. The log-likelihood difference
between the two parameter vectors is approximately 13. The standard deviations of the
measurement errors are chosen to be approximately 20% of the sample standard deviation
                                                                                                          35


                               Table 4: SW Model: Parameter Values

                           Œ∏m         Œ∏l                             Œ∏m         Œ∏l
                       Œ≤ÃÉ 0.159     0.182               œÄÃÑ         0.774      0.571
                      ¬Øl ‚àí1.078     0.019               Œ±          0.181      0.230
                       œÉ 1.016      1.166               Œ¶           1.342    1.455
                       œï 6.625      4.065               h          0.597      0.511
                       Œæw 0.752     0.647               œÉl         2.736      1.217
                       Œæp 0.861     0.807               Œπw         0.259      0.452
                       Œπp 0.463     0.494               œà          0.837      0.828
                       rœÄ 1.769     1.827               œÅ          0.855      0.836
                       ry 0.090     0.069               r‚àÜy        0.168      0.156
                       œÅa 0.982     0.962               œÅb         0.868      0.849
                       œÅg 0.962     0.947               œÅi         0.702      0.723
                       œÅr 0.414     0.497               œÅp         0.782      0.831
                       œÅw 0.971     0.968               œÅga        0.450      0.565
                       ¬µp 0.673     0.741               ¬µw         0.892      0.871
                       œÉa 0.375     0.418               œÉb         0.073      0.075
                       œÉg 0.428     0.444               œÉi         0.350      0.358
                       œÉr 0.144     0.131               œÉp         0.101      0.117
                       œÉw 0.311     0.382               ln p(Y |Œ∏) ‚àí943.0    ‚àí956.1

Notes: Œ≤ÃÉ = 100(Œ≤ ‚àí1 ‚àí 1).

of the time series.14 For comparison purposes, the parameter values and the data are iden-
tical to the ones used in Chapter 8 of Herbst and Schorfheide (2015). We run each filter
Nrun = 100 times.

       Figure 5 displays density estimates of the approximation errors associated with the log-
likelihood estimates under Œ∏ = Œ∏m and Œ∏ = Œ∏l . We use M = 40, 000 particles for the BSPF.
For the TPF, we use M = 4, 000 or M = 40, 000 and consider r‚àó = 2 and r‚àó = 3. Moreover,
in the mutation step (Algorithm 3), we set NM H = 1 and c‚àó = 0.3. Under both parameter
values, the BSPF exhibits the most bias, with its likelihood estimates substantially below
the true likelihood value. The distribution of the bias falls mainly between ‚àí400 and ‚àí100.
This means that eliciting the posterior distribution of the SW model using, for example, a
particle Markov chain Monte Carlo algorithm with likelihood estimates from the bootstrap
particle filter, would be nearly impossible. The TPFs perform better, although they also
underestimate the likelihood by a large amount.
  14
    The standard deviations for the measurement errors are 0.1731 (output growth), 0.1394 (consumption
growth), 0.4515 (investment growth), 0.1128 (wage growth), 0.5838 (log hours), 0.1230 (inflation), and 0.1653
                                                                                                                                                                               36


  Figure 5: Smets-Wouters Model: Distribution of Log-Likelihood Approximation Errors

                                      Œ∏ = Œ∏m                                                                                 Œ∏ = Œ∏l
                                                                                                0.016
             0.018

             0.016
                                                      ‚àó
                                               TPF (r = 2), M = 40000                           0.014
                                                                                                                                                    TPF (r ‚àó = 2), M = 40000
             0.014                            TPF (r ‚àó = 3), M = 40000                          0.012
                                                                                                                     TPF (r ‚àó = 3), M = 40000
             0.012
                                                                                                0.010
             0.010                           TPF (r ‚àó = 2), M = 4000                                                    TPF (r ‚àó = 2), M = 4000




                                                                                      Density
   Density




                                                                                                0.008
                                                                                                                  TPF (r ‚àó = 3), M = 4000
             0.008
                                       TPF (r ‚àó = 3), M = 4000                                  0.006
             0.006
                                                                                                0.004       BSPF , M = 40000
             0.004
                               BSPF , M = 40000
             0.002                                                                              0.002


             0.000                                                                              0.000
                ‚àí700   ‚àí600   ‚àí500   ‚àí400     ‚àí300        ‚àí200   ‚àí100    0      100                ‚àí600   ‚àí500   ‚àí400     ‚àí300      ‚àí200     ‚àí100       0       100      200




                           ÀÜ 1 = ln pÃÇ(Y1:T |Œ∏m ) ‚àí ln p(Y1:T |Œ∏m ) based on Nrun = 100 runs of
Notes: Density estimate of ‚àÜ
the particle filters.

                                             Table 5: SW Model: PF Summary Statistics


                                                                             BSPF                                       TPF
                       Number of Particles M                      4,000      40,000                                  4,000 40,000 40,000
                       Target Ineff. Ratio r‚àó                         2                                                  3      2      3
                                           High Posterior Density: Œ∏ = Œ∏m
                            ÀÜ1
                       Bias ‚àÜ                      -235.50      -126.09 -144.57                                                     -55.71 -65.94
                             ÀÜ
                       StdD ‚àÜ1                       60.30        46.55 44.32                                                        20.73 23.81
                            ÀÜ
                       Bias ‚àÜ2                       -1.00        -1.00 -1.00                                                        -1.00 -1.00
                         ‚àí1
                            PT
                       T      t=1 NœÜ,t                1.00         6.19    4.75                                                       6.14 4.71
                       Average Run Time (s)           4.28         2.75    2.11                                                      28.83 22.40
                                                                         l
                                            Low Posterior Density: Œ∏ = Œ∏
                            ÀÜ
                       Bias ‚àÜ1                     -263.31      -138.69 -168.76                                                     -66.92 -83.08
                             ÀÜ
                       StdD ‚àÜ1                       78.14        48.18 50.15                                                        24.26 29.14
                            ÀÜ
                       Bias ‚àÜ2                       -1.00        -1.00 -1.00                                                        -1.00 -1.00
                         ‚àí1
                            PT
                       T      t=1 NœÜ,t                1.00         6.25    4.81                                                       6.21 4.78
                       Average Run Time (s)           4.17         2.34    2.16                                                      26.01 20.14

                                    ÀÜ 1 and ‚àÜ
Notes: The likelihood discrepancies ‚àÜ          ÀÜ 2 are defined in (51) and (52). Results are
based on Nrun = 100 runs of the particle filters.

         Table 5 underscores the results in Figure 5. The best-performing TPF, while three to four
(interest rates).
                                                                                          37


                 Table 6: SW Model: PF Summary Statistics (NM H = 10)


                                         BSPF                   TPF
           Number of Particles M         40,000      4,000 4,000 40,000 40,000
                                 ‚àó
           Target Ineff. Ratio r                         2      3     2      3
                              High Posterior Density: Œ∏ = Œ∏m
                ÀÜ1
           Bias ‚àÜ                      -235.50      -21.06 -25.20 -6.45 -9.00
           StdD ‚àÜÀÜ1                      60.30       10.55 11.92 4.01 5.55
                ÀÜ
           Bias ‚àÜ2                       -1.00       -1.00 -1.00 1.32 -0.61
             ‚àí1
                PT
           T      t=1 NœÜ,t                1.00        6.11 4.69 6.07 4.69
           Average Run Time (s)           3.92        8.45 6.07 81.70 62.33
                               Low Posterior Density: Œ∏ = Œ∏l
                ÀÜ1
           Bias ‚àÜ                      -263.31      -26.41 -34.48 -9.66 -13.72
           StdD ‚àÜÀÜ1                      78.14       10.85 12.66 5.51 6.31
                ÀÜ
           Bias ‚àÜ                        -1.00       -1.00 -1.00 0.17 -0.66
             ‚àí1
                PT2
           T      t=1 NœÜ,t                1.00        6.16 4.74 6.14 4.71
           Average Run Time (s)           3.69        7.76 6.56 80.52 62.97

                                    ÀÜ 1 and ‚àÜ
Notes: The likelihood discrepancies ‚àÜ          ÀÜ 2 are defined in (51) and (52). Results are
based on Nrun = 100 runs of the particle filters.

times more accurate than the BSPF, still generates a bias in the log-likelihood approximation
of about ‚àí55 and a standard deviation of about 21 for Œ∏ = Œ∏m . Moreover, this increased
performance comes at a cost: the T P F (r‚àó = 2), M = 40, 000 filter takes about 29 seconds,
while the BSPF takes only 4 seconds. Even the variants of the TPF, which run more quickly
than the BSPF, still have wildly imprecise estimates of the likelihood; but these estimates
are in general better than those of the BSPF.

   It is well known that in SMC algorithms for static parameters the mutation phase is
crucial. For example, Bognanni and Herbst (2015) show that tailoring the mutation step to
model can substantially improve performance. The modification of the mutation step is not
immediately obvious. One clear way to allow the particles to better adapt to the current
density is to increase the number of Metropolis-Hastings steps. While all of the previous
results are based on NM H = 1, we now consider NM H = 10. Table 6 displays the results
associated with this choice for variants of the TPF, along with the BSPF, which is unchanged
from the previous exercise.

   The bias shrinks dramatically. For the T P F (r‚àó = 2), M = 40, 000, when Œ∏ = Œ∏m , the bias
falls from about ‚àí55 to about ‚àí6, with the standard deviation of the estimator decreasing
                                                                                            38


by a factor of 6. Of course, this increase in performance comes at a computational cost.
Each filter takes about three times longer than its NM H = 1 counterpart. Note that this
is less than you might expect, given the fact that the number of Metropolis-Hastings steps
at each iteration has increased by 10. This reflects two things. First, the mutation phase
is easily parallelizable on a multi-core desktop computer. Second, a substantial fraction of
computational time is spent during the resampling (selection) phase, which is not affected
by increasing the number of Metropolis-Hastings steps.



5      Conclusion

We developed a particle filter that automatically adapts the proposal distribution for the
particles sjt to the current observation yt . We start with a forward simulation of the state-
transition equation under an inflated measurement error variance and then gradually reduce
the variance to its nominal level. In each step, the particle values and weights change so
that the distribution slowly adapts to p(sjt |yt , sjt‚àí1 ). We demonstrate that the algorithm
improves upon the standard bootstrap particle filter, in particular in instances in which the
model generates very inaccurate one-step-ahead predictions of yt . The tempering iterations
can also be used to improve a particle filter with a more general initial proposal distribution
than the BSPF. Morever, our filter can be easily embedded in particle MCMC algorithms.



References
Andreasen, M. M. (2013): ‚ÄúNon-Linear DSGE Models and the Central Difference Kalman
    Filter,‚Äù Journal of Applied Econometrics, 28(6), 929‚Äì955.

Andrieu, C., A. Doucet, and R. Holenstein (2010): ‚ÄúParticle Markov Chain Monte
    Carlo Methods,‚Äù Journal of the Royal Statistical Society Series B, 72(3), 269‚Äì342.

Arulampalam, S., S. Maskell, N. Gordon, and T. Clapp (2002): ‚ÄúA Tutorial on
    Particle Filters for Online Nonlinear/Non-Gaussian Bayesian Tracking,‚Äù IEEE Transac-
    tions on Signal Processing, 50(2), 174‚Äì188.

Beskos, A., A. Jasra, N. Kantas, and A. H. Thiery (2014): ‚ÄúOn the Convergence of
    Adaptive Sequential Monte Carlo Methods,‚Äù arXiv Working Paper.
                                                                                         39


Bognanni, M., and E. P. Herbst (2015): ‚ÄúEstimating (Markov-Switching) VAR Models
  without Gibbs Sampling: A Sequential Monte Carlo Approach,‚Äù Finance and Economics
  Discussion Paper Series, Board of Governors, 2015-116(116), 154.

CappeÃÅ, O., S. J. Godsill, and E. Moulines (2007): ‚ÄúAn Overview of Existing Methods
  and Recent Advances in Sequential Monte Carlo,‚Äù Proceedings of the IEEE, 95(5), 899‚Äì
  924.

CappeÃÅ, O., E. Moulines, and T. Ryden (2005): Inference in Hidden Markov Models.
  Springer Verlag, New York.

Chopin, N. (2002): ‚ÄúA Sequential Particle Filter for Static Models,‚Äù Biometrika, 89(3),
  539‚Äì551.

          (2004): ‚ÄúCentral Limit Theorem for Sequential Monte Carlo Methods and Its
  Application to Bayesian Inference,‚Äù Annals of Statistics, 32(6), 2385‚Äì2411.

Creal, D. (2007): ‚ÄúSequential Monte Carlo Samplers for Bayesian DSGE Models,‚Äù
  Manuscript, University Chicago Booth.

         (2012): ‚ÄúA Survey of Sequential Monte Carlo Methods for Economics and Finance,‚Äù
  Econometric Reviews, 31(3), 245‚Äì296.

Del Moral, P. (2004): Feynman-Kac Formulae. Springer Verlag.

         (2013): Mean Field Simulation for Monte Carlo Integration. Chapman & Hall/CRC,
  Boca Raton.

Del Moral, P., A. Doucet, and A. Jasra (2006): ‚ÄúSequential Monte Carlo Samplers,‚Äù
  Journal of the Royal Statistical Society, Series B, 68(Part 3), 411‚Äì436.

         (2012): ‚ÄúAn AdaptiveSequential Monte Carlo Method for Approximate Bayesian
  Computation,‚Äù Statistical Computing, 22, 1009‚Äì1020.

Doucet, A., and A. M. Johansen (2011): ‚ÄúA Tutorial on Particle Filtering and Smooth-
  ing: Fifteen Years Later,‚Äù in Handook of Nonlinear Filtering, ed. by D. Crisan, and B. Ro-
  zovsky. Oxford University Press, Oxford.
                                                                                        40


Durham, G., and J. Geweke (2014): ‚ÄúAdaptive Sequential Posterior Simulators for
  Massively Parallel Computing Environments,‚Äù in Advances in Econometrics, ed. by I. Jeli-
  azkov, and D. Poirier, vol. 34, chap. 6, pp. 1‚Äì44. Emerald Group Publishing Limited, West
  Yorkshire.

FernaÃÅndez-Villaverde, J., and J. F. Rubio-Ramƒ±ÃÅrez (2007): ‚ÄúEstimating Macroeco-
  nomic Models: A Likelihood Approach,‚Äù Review of Economic Studies, 74(4), 1059‚Äì1087.

Geweke, J., and B. Frischknecht (2014): ‚ÄúExact Optimization By Means of Sequen-
  tially Adaptive Bayesian Learning,‚Äù Mimeo.

Godsill, S. J., and T. Clapp (2001): ‚ÄúImprovement Strategies for Monte Carlo Particle
  Filters,‚Äù in Sequential Monte Carlo Methods in Practice, ed. by A. Doucet, N. De Freitas,
  and N. Gordon, pp. 139‚Äì158. Springer Verlag, New York.

Gordon, N., D. Salmond, and A. F. Smith (1993): ‚ÄúNovel Approach to Nonlinear/Non-
  Gaussian Bayesian State Estimation,‚Äù Radar and Signal Processing, IEE Proceedings F,
  140(2), 107‚Äì113.

Gust, C., E. Herbst, D. Lopez-Salido, and M. E. Smith (forthcoming): ‚ÄúThe Em-
  pirical Implications of the Interest-Rate Lower Bound,‚Äù American Economic Review.

Herbst, E., and F. Schorfheide (2014): ‚ÄúSequential Monte Carlo Sampling for DSGE
  Models,‚Äù Journal of Applied Econometrics, 29(7), 1073‚Äì1098.

         (2015): Bayesian Estimation of DSGE Models. Princeton University Press, Prince-
  ton.

Jasra, A., D. A. Stephens, A. Doucet, and T. Tsagaris (2011): ‚ÄúInference for Levy-
  Driven Stochastic Volatility Models via Adaptive Sequential Monte Carlo,‚Äù Scandinavian
  Journal of Statistics, 38, 1‚Äì22.

Johansen, A. M. (2016): ‚ÄúOn Blocks, Tempering and Particle MCMC for Systems Iden-
  tification,‚Äù Manuscript, University of Warwick.

Kollmann, R. (2015): ‚ÄúTractable Latent State Filtering for Non-Linear DSGE Models
  Using a Second-Order Approximation and Pruning,‚Äù Computational Economics, 45(2),
  239‚Äì260.
                                                                                    41


Liu, J. S. (2001): Monte Carlo Strategies in Scientific Computing. Springer Verlag, New
  York.

Neal, R. M. (1998): ‚ÄúAnnealed Importance Sampling,‚Äù Technical Report, Department of
  Statistics, University of Toronto, 9805.

Pitt, M. K., and N. Shephard (1999): ‚ÄúFiltering via Simulation: Auxiliary Particle
  Filters,‚Äù Journal of the American Statistical Association, 94(446), 590‚Äì599.

Pitt, M. K., R. d. S. Silva, P. Giordani, and R. Kohn (2012): ‚ÄúOn Some Properties
  of Markov Chain Monte Carlo Simulation Methods Based on the Particle Filter,‚Äù Journal
  of Econometrics, 171(2), 134‚Äì151.

SchaÃàfer, C., and N. Chopin (2013): ‚ÄúSequential Monte Carlo on Large Binary Sampling
  Spaces,‚Äù Statistical Computing, 23, 163‚Äì184.

Smets, F., and R. Wouters (2007): ‚ÄúShocks and Frictions in U.S. Business Cycles: A
  Bayesian DSGE Approach,‚Äù American Economic Review, 97(3), 586‚Äì608.

White, H. (2001): Asymptotic Theory For Econometricians. Academic Press, New York.

Zhou, Y., A. M. Johansen, and J. A. Aston (2015): ‚ÄúTowards Automatic Model
  Comparison: An Adaptive Sequential Monte Carlo Approach,‚Äù arXiv Working Paper,
  1303.3123v2.
Online Appendix                                                                             A-1


                            Online Appendix for
                         Tempered Particle Filtering
                    Edward Herbst and Frank Schorfheide


A      Theoretical Derivations

A.1     Monotonicity of Inefficiency Ratio

Recall the definitions

                                1
                          ej,t = (yt ‚àí Œ®(sj,n‚àí1
                                          t     ))0 Œ£‚àí1         j,n‚àí1
                                                     u (yt ‚àí Œ®(st     ))
                                2

and                                                   ny /2
                                                 œÜn
                         wÃÉtj,n (œÜn )   =                       exp[‚àí(œÜn ‚àí œÜn‚àí1 )ej,t ].
                                                œÜn‚àí1

Provided that the particles had been resampled and Wtj,n‚àí1 = 1, the inefficiency ratio can
be manipulated as follows:

                                            PM      2
                                        wÃÉtj,n (œÜn )
                                        1
                                        M       j=1
                InEff(œÜn ) =  P                    2
                               1    M      j,n
                              M     j=1 wÃÉt (œÜn )

                                 1
                                   PM  œÜn d
                                M     j=1 œÜn‚àí1         exp[‚àí2(œÜn ‚àí œÜn‚àí1 )ej,t ]
                           =                                                  2
                                1
                                   PM  œÜn d/2
                               M     j=1 œÜn‚àí1           exp[‚àí(œÜn ‚àí œÜn‚àí1 )ej,t ]
                               1
                                  PM
                              M     j=1 exp[‚àí2(œÜn ‚àí œÜn‚àí1 )ej,t ]
                           =  P                                       2
                               1    M
                              M     j=1 exp[‚àí(œÜ      n ‚àí  œÜn‚àí1 )ej,t ]
                                        A1 (œÜn )
                               =                 .
                                        A2 (œÜn )


    Note that for œÜn = œÜn‚àí1 , we obtain ESS(œÜn ) = 1. We now will show that the inefficiency
ratio is monotonically increasing on the interval [œÜn‚àí1 , 1]. Differentiating with respect to œÜn
Online Appendix                                                                               A-2


yields

                                                                       (1)
                             (1)      A(1) (œÜn )A2 (œÜn ) ‚àí A1 (œÜn )A2 (œÜn )
                        InEff (œÜn ) =                                       ,
                                                    [A2 (œÜn )]2

where

                   M
     (1)        2 X
   A (œÜn ) = ‚àí        ej,t exp[‚àí2(œÜn ‚àí œÜn‚àí1 )ej,t ]
                M j=1
                   M
                                             !          M
                                                                                       !
                2 X                                  1 X
   A(2) (œÜn ) =       exp[‚àí(œÜn ‚àí œÜn‚àí1 )ej,t ]      ‚àí       ej,t exp[‚àí(œÜn ‚àí œÜn‚àí1 )ej,t ] .
                M j=1                                M j=1


The denumerator in InEff(1) (œÜn ) is always non-negative and strictly different from zero. Thus,
we can focus on the numerator:

                                      (1)
         A(1) (œÜn )A2 (œÜn ) ‚àí A1 (œÜn )A2 (œÜn )
                     M
                                                      !       M
                                                                                         !2
                 2 X                                       1 X
           =   ‚àí        ej,t exp[‚àí2(œÜn ‚àí œÜn‚àí1 )ej,t ]            exp[‚àí(œÜn ‚àí œÜn‚àí1 )ej,t ]
                 M j=1                                     M j=1
                     M
                                                  !        M
                                                                                     !
                  1 X                                    2 X
             ‚àí           exp[‚àí2(œÜn ‚àí œÜn‚àí1 )ej,t ]             exp[‚àí(œÜn ‚àí œÜn‚àí1 )ej,t ]
                 M j=1                                  M j=1
                       M
                                                        !
                    1 X
             √ó ‚àí           ej,t exp[‚àí(œÜn ‚àí œÜn‚àí1 )ej,t ]
                   M j=1
                     M
                                                !
                 1  X
           = 2          exp[‚àí(œÜn ‚àí œÜn‚àí1 )ej,t ]
                 M j=1
                      M
                                                       !      M
                                                                                          !
                   1 X                                      1 X
             √ó            ej,t exp[‚àí(œÜn ‚àí œÜn‚àí1 )ej,t ]           exp[‚àí2(œÜn ‚àí œÜn‚àí1 )ej,t ]
                   M j=1                                   M j=1
                     M
                                                       !      M
                                                                                         !
                  1 X                                      1 X
             ‚àí           ej,t exp[‚àí2(œÜn ‚àí œÜn‚àí1 )ej,t ]           exp[‚àí(œÜn ‚àí œÜn‚àí1 )ej,t ]
                 M j=1                                     M j=1


   To simplify the notation, we now define


                                     xj,t = exp[‚àí(œÜn ‚àí œÜn‚àí1 )ej,t ].
Online Appendix                                                                                           A-3


Note that 0 < xj,t ‚â§ 1, which implies that x2j,t ‚â§ xj,t . Moreover, ej,t ‚â• 0. We will use these
properties to establish the following bound:

                               (1)
A(1) (œÜn )A2 (œÜn ) ‚àí A1 (œÜn )A2 (œÜn )
               M
                        !"       M
                                               !      M
                                                               !        M
                                                                                        !       M
                                                                                                          !#
           1 X                1 X                  1 X 2             1 X                     1 X
 = 2               xj,t              ej,t xj,t             x     ‚àí         ej,t x2j,t              xj,t
           M j=1             M j=1                 M j=1 j,t         M j=1                   M j=1
               M
                        !"       M
                                               !      M
                                                               !        M
                                                                                        !       M
                                                                                                          !#
           1 X                1 X                  1 X 2             1 X                     1 X 2
 ‚â• 2               xj,t              ej,t xj,t             x     ‚àí         ej,t x2j,t             x
           M j=1             M j=1                 M j=1 j,t         M j=1                   M j=1 j,t
               M
                        !       M
                                       !"          M
                                                               !        M
                                                                                        !#
           1 X              1 X 2                1 X                 1 X
 = 2               xj,t            x                  ej,t xj,t ‚àí          ej,t x2j,t
           M j=1           M j=1 j,t            M j=1                M j=1
  ‚â• 0.


We conclude that the inefficiency ratio InEff(œÜn ) is increasing in œÜn . 



A.2     Proofs for Section 3.1

The proofs in this section closely follow Chopin (2004) and Herbst and Schorfheide (2014).
Throughout this section, we will assume that h(Œ∏) is scalar and we use absolute values |h|
instead of a general norm khk. Extensions to vector-valued-h functions are straightforward.
We use C to denote a generic constant. We will make repeated use of the following moment
bound for r > 1

                                    r         r     r
                          E X ‚àí E[X]    ‚â§ Cr E X + E[X]                                               (A.1)
                                                r
                                        ‚â§ 2Cr E X ,


where Cr = 1 for r = 1 and Cr = 2r‚àí1 for r > 1. The first inequality follows from the Cr
inequality and the second inequality follows from Jensen‚Äôs inequality.

   We will make use of the following SLLN (Markov, see White (2001) p. 33): Let {Zj } be
a sequence of independent random variables with finite means ¬µj = E[Zj ]. If for some Œ¥ > 0,
P‚àû                                       PM            a.s
                             < ‚àû, then M1
                   1+Œ¥
                        1+Œ¥                                                        1+Œ¥
                                                                                         
   j=1 E |Zj ‚àí ¬µj |     /j                 j=1 Zj ‚àí ¬µj ‚àí‚Üí 0. Note that E |Zj ‚àí ¬µj |        <
                                                                  P‚àû
C < ‚àû implies that the summability condition is satisfied because j=1 (1/j)1+Œ¥ < ‚àû.
Online Appendix                                                                                             A-4


    Recall that under a multivariate Gaussian measurement error distribution, the density
pn (yt |st ) can be bounded from above. Thus, pn (yt |st ) ‚àà HtÃÉnÃÉ and œâÃÉtj,n ‚àà HtÃÉnÃÉ for any tÃÉ and nÃÉ.
Moreover, for any h(st , st‚àí1 ) ‚àà Htn , we can deduce that hÃÉ(¬∑) = h(¬∑)œâÃÉtj,n ‚àà Htn .

Proof of Lemma 1. (i) (34) can be established as follows. Consider the summands I and
II in (38). Recall that

                                     M
                                  1 X  j j,NœÜ                        
                                                                         j,N
                           I=           h(sÃÉt , st‚àí1 ) ‚àí Ep(¬∑|sj ) [h] Wt‚àí1œÜ .
                                  M j=1                        t‚àí1




                                     j,N       j,N
Conditional on the particles {st‚àí1œÜ , Wt‚àí1œÜ } the summands in term I form a triangular array
of mean-zero random variables that within each row are independently distributed. In Al-
gorithm 4 the particles were resampled during the t ‚àí 1 tempering iteration NœÜ , such that
  N
Wt‚àí1œÜ = 1. Using the bound
                                                                                   
                                                                              1+Œ¥
                    Ep(¬∑|st‚àí1 ) h(st , st‚àí1 ) ‚àí Ep(¬∑|st‚àí1 ) [h(st , st‚àí1 )]             <C<‚àû

                                                                                               a.s.
implied by the definition of Ht1 , we can apply the SLLN to deduce that term I ‚àí‚Üí 0. The
second term in (38) takes the form

                    M                  Z Z                                            
                 1 X
            II =        Ep(¬∑|sj ) [h] ‚àí     h(st , st‚àí1 )p(st , st‚àí1 |Y1:t‚àí1 )dst dst‚àí1 .
                 M j=1        t‚àí1




                                    œÜ N
By assumption, Ep(¬∑|st‚àí1 ) [h] ‚àà Ht‚àí1 . The almost-sure convergence to zero of term II can now
be deduced from the recursive Assumption (33). By combining the convergence results for
terms I and II we have established (34).

    (ii) The convergences in (35) and (37) follow immediately from the fact that p1 (yt |st ) ‚àà
Ht1 . Moreover, if h(st , st‚àí1 ) ‚àà Ht1 , then h(st , st‚àí1 )p1 (yt |st ) ‚àà Ht1 . Finally, p1 (yt |st ) > 0,
which implies that the almost-sure limit of the denominator in (35) is strictly positive.
                                                                                                      j,N
    (iii) To verify (36), let FÃÉt,1,M denote the œÉ-algebra generated by the particles {sÃÉj,1     œÜ    j,1
                                                                                         t , st‚àí1 , Wt }.
Online Appendix                                                                                                   A-5


Moreover, define
                                                           M
                                                        1 X            j,NœÜ
                                        E[h|FÃÉt,1,M ] =       h(sÃÉj,1          j,1
                                                                  t , st‚àí1 )WÃÉt ,
                                                        M j=1

and write
                         Z
            hÃÑ1t,M   ‚àí       h(st , st‚àí1 )p1 (st , st‚àí1 |Y1:t )dst                                               (A.2)
                             M
                 1 X            j,NœÜ
                      h(sj,1
                                                     
              =           t , st‚àí1 ) ‚àí E[h|FÃÉt,1,M ]
                M j=1
                   X M                       Z Z                                                  
                   1         j,1 NœÜ       j,1
                +        h(sÃÉt , st‚àí1 )WÃÉt ‚àí           h(st , st‚àí1 )p1 (st , st‚àí1 |Y1:t )dst dst‚àí1
                   M j=1
                    M
                 1 X            j,NœÜ
                        h(sj,1
                                                         
              =            t , st‚àí1 ) ‚àí E[h|FÃÉt,1,M ]
                M j=1
                         Z Z                                                
                    1
                + hÃÉt,M ‚àí        h(st , st‚àí1 )p1 (st , st‚àí1 |Y1:t )dst dst‚àí1

              = I + II.


From (35), we can immediately deduce that term II converges to zero almost surely. Re-
                                                                j,N
call that we are resampling the pairs (sÃÉj,n     œÜ
                                         t , st‚àí1 ). Thus, under multinomial resampling the
        j,NœÜ
h(sj,1
   t , st        )‚Äôs form a triangular array of i.i.d. random variables conditional on FÃÉt,1,M . Using
Kolmogorov‚Äôs SLLN for i.i.d. sequences, it suffices to verify for that
                                                                          
                                                      j,NœÜ
                                            E h(sj,1
                                                 t , st    )          FÃÉt,1,M < ‚àû.


We can manipulate the moment as follows

                                                           M
                         j,NœÜ                              1 X              j,NœÜ
        E        h(sj,1
                    t , st‚àí1 )         FÃÉt,1,M        =            h(sÃÉj,1
                                                                       t , st‚àí1 ) WÃÉt
                                                                                       j,1
                                                          M j=1
                                                          Z
                                                      a.s
                                                     ‚àí‚Üí      h(st , st‚àí1 ) p1 (st , st‚àí1 |Y1:t )dst dst‚àí1 < ‚àû.


Because h ‚àà Ht1 implies |h| ‚àà Ht1 , we obtain the almost-sure convergence from (35). 

Proof of Lemma 2. (i) We begin with the correction step. Recall that for any h(st , st‚àí1 ) ‚àà
Online Appendix                                                                                              A-6


Htn‚àí1 :


           pn (yt |st )/pn‚àí1 (yt |st ) ‚àà Htn‚àí1      and h(st , st‚àí1 )pn (yt |st )/pn‚àí1 (yt |st ) ‚àà Htn‚àí1 .


Thus, the recursive Assumption 2 yields the almost-sure convergence in (40) and (42).

    (ii) We proceed with the selection step. The convergence in (41) can be established with
an argument similar to the one used in Step (iii) of the proof of Lemma 1.

    (iii) Finally, consider the mutation step. To establish the convergence in (43), we need to
construct moment bounds for the terms I and II that appear in (45). Under the assumption
that the resampling step is executed at every iteration n, term I takes the form:

                                  M                                          
                               1 X       j,n j,NœÜ
                            I=        h(st , st‚àí1 ) ‚àí EK (¬∑|sÃÇj,n ;sj,NœÜ ) [h] .
                               M j=1                    n     t     t‚àí1




Conditional on the œÉ-algebra generated by the particles of the selection step, term I is an
average of independently distributed mean-zero random variables. By virtue of h ‚àà Htn , we
can deduce that, omitting the j and n superscripts,

                                                                                 1+Œ¥ 
                      EKn (¬∑|sÃÇt ;st‚àí1 ) h(st , st‚àí1 ) ‚àí EKn (¬∑|sÃÇt ;st‚àí1 ) [h]           < C < ‚àû.

                                                 a.s.
Therefore, the SLLN implies that I ‚àí‚Üí 0. For term II, we have

                   M                            Z Z                                            
                1 X
           II =       EK (¬∑|sÃÇj,n ;sj,NœÜ ) [h] ‚àí     h(st , st‚àí1 )pn (st , st‚àí1 |Y1:t )dst dst‚àí1 .
                M j=1   n     t     t‚àí1




Now define
                                         œà(sÃÇt , st‚àí1 ) = EKn (¬∑|sÃÇt ;st‚àí1 ) [h].

The definition of Htn implies that œà(sÃÇt , st‚àí1 ) ‚àà Htn‚àí1 . Thus, we can deduce from (41) that
    a.s.
II ‚àí‚Üí 0. 
Online Appendix                                                                                                   A-7


A.3      Proofs for Section 3.2

The subsequent proof of the unbiasedness of the particle filter approximation uses Lemmas 3
                                                                                                           j,N
and 5 below. Throughout this section, we use the convention that Wtj,0 = Wt‚àí1œÜ . Moreover,
we often use the j subscript to denote a fixed particle as well as a running index in a sum-
mation. That is, we write aj / M
                              P      j            j
                                                     PM l
                                j=1 a instead of a /    l=1 a . We also define the information

set

                                       j,NœÜ      j,NœÜ                             j,NœÜ      j,NœÜ
                                                        ), (sj,1  j,1
                                 
               Ft‚àí1,n,M =            (s0      , W0           1 , W1 ), . . . , (s1       , W1      ), . . . ,    (A.3)
                                                                            M
                                 (sj,1     j,1             j,n     j,n
                                   t‚àí1 , Wt‚àí1 ), . . . , (st‚àí1 , Wt‚àí1 )     j=1
                                                                                .


A.3.1    Additional Lemmas

Lemma 3 Suppose that the incremental weights wÃÉtj,n are defined as in (9) and (16) and that
there is no resampling. Then
                                                                    Ô£´           Ô£∂
                        NœÜ                                            NœÜ
                                  M
                                                         !        M
                        Y      1 X j,n j,n‚àí1                   1 X    Y            j,N
                                    wÃÉ W                     =      Ô£≠    wÃÉTj,n Ô£∏ WT ‚àí1œÜ                         (A.4)
                        n=1
                               M j=1 T T                       M j=1 n=1


and                                                                     Ô£´                  Ô£∂
                          NœÜ                                                NœÜ
                                      M
                                                                 !
                  j,N
                          Y     1     X                                     Y                      j,N
                   œÜ
               WT ‚àíh‚àí1                      wÃÉTj,n‚àíh‚àí1 WTj,n‚àí1
                                                          ‚àíh‚àí1       =Ô£≠          wÃÉTj,n‚àíh‚àí1 Ô£∏ WT ‚àíh‚àí2
                                                                                                  œÜ
                                                                                                      .          (A.5)
                          n=1
                                M     j=1                                 n=1



Proof of Lemma 3. The lemma can be proved by induction. If there is no resampling,
then Wtj,n = WÃÉtj,n .

Part 1. The inductive hypothesis to show (A.4) takes the form
                                                                 Ô£´           Ô£∂
                    NœÜ                                             NœÜ
                                 M
                                                     !         M
                    Y         1 X j,n j,n‚àí1                 1 X    Y
                                   wÃÉ W                   =      Ô£≠    wÃÉTj,n Ô£∏ WTj,n‚àó ‚àí1 .                       (A.6)
                   n=n‚àó
                              M j=1 T T                     M j=1 n=n
                                                                             ‚àó
Online Appendix                                                                                             A-8


If the hypothesis is correct, then

         NœÜ          M
                                      !
         Y        1 X j,n j,n‚àí1
                        wÃÉ W                                                                             (A.7)
       n=n‚àó ‚àí1
                 M j=1 T T
             Ô£´         Ô£´             Ô£∂            Ô£∂
                          NœÜ
                     M                                       M
                                                                                      !
                 1  X     Y                              1  X
         = Ô£≠           Ô£≠       wÃÉj,n Ô£∏ WTj,n‚àó ‚àí1 Ô£∏                wÃÉj,n‚àó ‚àí1 WTj,n‚àó ‚àí2
                M j=1 n=n T                             M j=1 T
                             ‚àó
             Ô£´         Ô£´             Ô£∂                                  Ô£∂
                          NœÜ
                     M                                                             M
                                                                                                       !
                                                j,n‚àó ‚àí1   j,n‚àó ‚àí2
                 1  X     Y      j,n         wÃÉ         W                     1   X      j,n ‚àí1 j,n ‚àí2
         = Ô£≠           Ô£≠       wÃÉ Ô£∏ 1 PMT j,n‚àó ‚àí1         T             Ô£∏              wÃÉT ‚àó WT ‚àó
                M j=1 n=n T             M     j=1 T wÃÉ       W  j,n‚àó ‚àí2
                                                                T
                                                                             M    j=1
                             ‚àó
                      Ô£´               Ô£∂
                   M     NœÜ
               1 XÔ£≠ Y
         =                     wÃÉTj,n Ô£∏ WTj,n‚àó ‚àí2 .
              M j=1 n=n ‚àí1
                                    ‚àó




The first equality follows from (A.6), and the second equality is obtained by using the
definition of WTj,n‚àó ‚àí1 .

    It is straightforward to verify that the inductive hypothesis (A.6) is satisfied for n‚àó = NœÜ .
                                                                     j,N
Setting n‚àó = 1 in (A.6) and noticing that WTj,0 = WT ‚àí1œÜ leads the desired result.

Part 2. To show (A.5), we can use the inductive hypothesis
                                                                       Ô£´                  Ô£∂
                              NœÜ                                           NœÜ
                                           M
                                                                 !
                    j,N
                              Y         1 X j,n                            Y
                                                                                                    ‚àó ‚àí1
                   œÜ
               WT ‚àíh‚àí1                       wÃÉ     W j,n‚àí1          =Ô£≠           wÃÉTj,n‚àíh‚àí1 Ô£∏ WTj,n
                                                                                                  ‚àíh‚àí1 .   (A.8)
                             n=n‚àó
                                        M j=1 T ‚àíh‚àí1 T ‚àíh‚àí1                n=n‚àó



If the inductive hypothesis is satisfied, then

                           NœÜ              M
                                                                 !
              j,N
                           Y            1 X j,n
               œÜ
           WT ‚àíh‚àí1                            wÃÉT ‚àíh‚àí1 WTj,n‚àí1
                                                          ‚àíh‚àí1                                             (A.9)
                          n=n‚àó ‚àí1
                                        M j=1
                                NœÜ      M
                                                                !     M
                                                                                                  !
                  j,NœÜ
                                Y   1  X                          1  X
                                                                                ‚àó ‚àí1        ‚àó ‚àí2
              = WT ‚àíh‚àí1                    wÃÉTj,n‚àíh‚àí1 WTj,n‚àí1
                                                           ‚àíh‚àí1          wÃÉTj,n‚àíh‚àí1   WTj,n
                                                                                         ‚àíh‚àí1
                         n=n‚àó
                                   M   j=1
                                                                  M  j=1
                Ô£´                 Ô£∂
                   NœÜ
                                                                                                    !
                                                 ‚àó ‚àí1      ‚àó ‚àí2         M
                   Y     j,n              wÃÉTj,n‚àíh‚àí1  WTj,n
                                                         ‚àíh‚àí1       1  X       j,n ‚àó ‚àí1    j,n ‚àó ‚àí2
              = Ô£≠      wÃÉT ‚àíh‚àí1 Ô£∏ 1 PM j,n‚àó ‚àí1 j,n‚àó ‚àí2                      wÃÉ          W
                  n=n‚àó                M    j=1 wÃÉT ‚àíh‚àí1 WT ‚àíh‚àí1
                                                                   M j=1 T ‚àíh‚àí1 T ‚àíh‚àí1
                Ô£´                    Ô£∂
                    NœÜ
                    Y
                                              ‚àó ‚àí2
              = Ô£≠         wÃÉTj,n‚àíh‚àí1 Ô£∏ WTj,n
                                           ‚àíh‚àí1 .
                      n=n‚àó ‚àí1
Online Appendix                                                                                         A-9


   For n‚àó = NœÜ the validity of the inductive hypothesis can be verified as follows:

                                           M
                                                                  !
                            œÜj,N        1 X j,NœÜ        j,NœÜ ‚àí1
                        WT ‚àíh‚àí1               wÃÉT ‚àíh‚àí1 WT ‚àíh‚àí1                                       (A.10)
                                        M j=1
                                          j,N      j,N ‚àí1            M
                                                                                            !
                                            œÜ        œÜ
                                       wÃÉT ‚àíh‚àí1 WT ‚àíh‚àí1           1 X j,NœÜ        j,NœÜ ‚àí1
                             =        PM j,NœÜ          j,NœÜ ‚àí1
                                                                        wÃÉT ‚àíh‚àí1 WT ‚àíh‚àí1
                                    1                             M j=1
                                    M   j=1 wÃÉT ‚àíh‚àí1 WT ‚àíh‚àí1
                                     j,NœÜ    j,NœÜ ‚àí1
                             =     wÃÉT ‚àíh‚àí1 WT ‚àíh‚àí1  .


Setting n‚àó = 1 in (A.8) leads to the desired result. 

   The following lemma simply states that the expected value of a sum is the sum of the
expected values, but it does so using a notation that we will encounter below.

                                                                                                   QM
Lemma 4 Suppose sj , j = 1, . . . , M , is a sequence of random variables with density               j=1   p(sj ),
then
             Z         Z       M          YM                        M Z
                             1 X       j            j    1        M  1 X
                 ¬∑¬∑¬∑               f (s )        p(s ) ds ¬∑ ¬∑ ¬∑ ds =        f (sj )p(sj )dsj .
                             M j=1           j=1
                                                                     M j=1


Proof of Lemma 4. The statement is trivially satisfied for M = 1. Suppose that it is true
for M ‚àí 1, then

       Z         Z      M            Y M           
                      1 X          j
           ¬∑¬∑¬∑                f (s )          p(s ) ds1 ¬∑ ¬∑ ¬∑ dsM
                                                  j
                                                                                                       (A.11)
                    M j=1                j=1
                                                            M  ‚àí1               M ‚àí1
                                           M ‚àí1 1
               Z      Z                                                                
                             1       M
                                                             X
                                                                      j       M
                                                                                Y
           =      ¬∑¬∑¬∑           f (s ) +                          f (s ) p(s )       p(s ) ds1 ¬∑ ¬∑ ¬∑ dsM
                                                                                        j
                            M                 M M ‚àí 1 j=1                       j=1
                Z                             MY ‚àí1 Z
                 1
           =            f (sM )p(sM )dsM                p(sj )dsj
                 M                               j=1
                                      M ‚àí1
                   M ‚àí1 1
                                     X    Z                   Z
                                                    j   j    j
               +                               f (s )p(s )ds         p(sM )dsM
                      M M ‚àí 1 j=1
                   M Z
                1 X
           =               f (sj )p(sj )dsj ,                                                          (A.12)
               M j=1

which verifies the claim for all M by induction. 
Online Appendix                                                                                      A-10


Lemma 5 Suppose that the incremental weights wÃÉtj,n are defined as in (9) and (16) and
that the selection step is implemented by multinomial resampling for a predetermined set of
iterations n ‚àà N . Then,
              Ô£Æ                                            Ô£π
                  NœÜ      M
                                        !                    M
                  Y    1 X j,n j,n‚àí1                      1 X          j,N     j,N
          EÔ£∞                wÃÉ W            FT ‚àí1,NœÜ ,M =
                                                       Ô£ª        p(yT |sT ‚àí1œÜ )WT ‚àí1œÜ                (A.13)
              n=1
                       M j=1 T T                          M j=1


and
              Ô£Æ                                                                               Ô£π
                                           NœÜ
         M                                                 M
                                                                              !
      1 X                  j,NœÜ     j,NœÜ
                                           Y            1 X j,n
            E Ô£∞p(YT ‚àíh:T |sT ‚àíh‚àí1 )WT ‚àíh‚àí1                   wÃÉ     W j,n‚àí1       FT ‚àíh‚àí2,NœÜ ,M Ô£ª (A.14)
      M j=1                                n=1
                                                        M j=1 T ‚àíh‚àí1 T ‚àíh‚àí1
                                                               M
                                                            1 X                 j,NœÜ     j,NœÜ
                                                          =       p(YT ‚àíh‚àí1:T |sT ‚àíh‚àí2 )WT ‚àíh‚àí2 .
                                                            M j=1


Proof of Lemma 5. We first prove the lemma under the assumption of no resampling, i.e.,
N = ‚àÖ. We then discuss how the proof can be modified to allow for resampling.

Part 1 (No Resampling). We deduce from Lemma 3 that
                                                                  Ô£´           Ô£∂
     NœÜ                                                             NœÜ
    Y               M
                                  !                         M    Y                           
                  1 X j,n j,n‚àí1                           1 X             j,n   j,NœÜ
   E                   wÃÉ W           FT ‚àí1,NœÜ ,M       =       E Ô£≠     wÃÉT Ô£∏ WT ‚àí1 FT ‚àí1,NœÜ ,M .
        n=1
                  M j=1 T T                               M j=1     n=1
                                                                                                    (A.15)
The subsequent derivations focus on the evaluation of the expectation on the right-hand
                                                                                               1:M,NœÜ ‚àí1
side of this equation. We will subsequently integrate over the particles sT1:M,1 , . . . , sT              ,
which enter the incremental weights wÃÉTj,n . We use s1:M,n
                                                     T     to denote the set of particle values
                               j,N
{s1,n          M,n              œÜ
  T , . . . , sT }. Because WT ‚àí1 ‚àà FT ‚àí1,NœÜ ,M it suffices to show that

                              Ô£´          Ô£∂
                              YNœÜ                    
                                     j,n Ô£∏                      j,N
                            E Ô£≠    wÃÉT     FT ‚àí1,NœÜ ,M = p(yT |sT ‚àí1œÜ ).                            (A.16)
                                  n=1
Online Appendix                                                                                                     A-11


   Recall that the initial state particle sj,1
                                           T is generated from the state-transition equation
      j,N
p(sT |sT ‚àí1œÜ ). The first incremental weight is defined as


                                                  wÃÉTj,1 = p1 (yT |sj,1
                                                                    T ).



The incremental weight in tempering iteration n is given by

                                                          pn (yT |sj,n‚àí1 )
                                              wÃÉTj,n   =           T
                                                                     j,n‚àí1 .
                                                         pn‚àí1 (yT |sT )

Because we are omitting the selection step, the new particle value is generated in the mutation
step by sampling from the Markov transition kernel

                                                                           j,N
                                             sj,n      n j,n‚àí1
                                              T ‚àº Kn (sT |sT   , sT ‚àí1œÜ ),                                         (A.17)


which has the invariance property
                                                Z
                      pn (sT |yT , sT ‚àí1 ) =           Kn (sT |sÃÇT ; sT ‚àí1 )pn (sÃÇT |yT , sT ‚àí1 )dsÃÇT .            (A.18)


   Using the previous notation, we can write
                     Ô£ÆÔ£´                  Ô£∂                Ô£π
                          NœÜ
                          Y
                  E Ô£∞Ô£≠          wÃÉTj,n Ô£∏ FT ‚àí1,NœÜ ,M Ô£ª                                                             (A.19)
                          n=1
                                         Ô£´                                                  Ô£∂
                                           NœÜ             j,n‚àí1
                                                pn (yT |sT )
                          Z          Z
                                                                         j,n‚àí1 j,n‚àí2 j,NœÜ Ô£∏
                                           Y
                     =         ¬∑¬∑¬∑       Ô£≠
                                                            j,n‚àí1 Kn‚àí1 (sT    |sT , sT ‚àí1 )
                                           n=3
                                               p n‚àí1 (yT |s T    )
                            p2 (yT |sj,1
                                     T )          j,1   j,1 j,NœÜ    j,1       j,NœÜ ‚àí1
                          √ó          j,1 p1 (yT |sT )p(sT |sT ‚àí1 )dsT ¬∑ ¬∑ ¬∑ dsT       .
                            p1 (yT |sT )

The bridge posterior densities were defined as

                           pn (yt |st )p(st |st‚àí1 )
                                                                               Z
     pn (st |yt , st‚àí1 ) =                          ,       pn (yt |st‚àí1 ) =       pn (yt |st )p(st |st‚àí1 )dst .   (A.20)
                               pn (yt |st‚àí1 )
Online Appendix                                                                                              A-12


Using the invariance property of the transition kernel in (A.18) and the definition of the
bridge posterior densities, we deduce that
         Z
                                          j,N                                 j,N
             Kn‚àí1 (sj,n‚àí1
                    T     |sj,n‚àí2
                            T     , sT ‚àí1œÜ )pn‚àí1 (yT |sj,n‚àí2
                                                        T      )p(sj,n‚àí2
                                                                    T     |sT ‚àí1œÜ )dsj,n‚àí2
                                                                                     T                      (A.21)
               Z
                                              j,N                        j,N               j,N
             =     Kn‚àí1 (sj,n‚àí1
                            T    |sj,n‚àí2
                                    T      , sT ‚àí1œÜ )pn‚àí1 (sj,n‚àí2
                                                            T     |yT , sT ‚àí1œÜ )pn‚àí1 (yT |sT ‚àí1œÜ )dsj,n‚àí2
                                                                                                    T

                                         j,N               j,N
             = pn‚àí1 (sTj,n‚àí1 |yT , sT ‚àí1œÜ )pn‚àí1 (yT |sT ‚àí1œÜ )
                                                   j,N
             = pn‚àí1 (yT |sTj,n‚àí1 )p(sTj,n‚àí1 |sT ‚àí1œÜ ).


The first equality follows from Bayes Theorem in (A.20). The second equality follows from
the invariance property of the transition kernel. The third equality uses Bayes Theorem
again.

   We can now evaluate the integrals in (A.19). Consider the terms involving sj,1
                                                                              T :


                                                                j,1
                                          j,1 j,NœÜ p2 (yT |sT )
                          Z
                                                                                     j,1 j,NœÜ
                              K2 (sj,2
                                   T   |s T  , sT ‚àí1 )                       j,1
                                                                j,1 p1 (yT |sT )p(sT |sT ‚àí1 )dsT
                                                                                                j,1
                                                                                                            (A.22)
                                                       p1 (yT |sT )
                                 Z
                                                  j,1 j,NœÜ                     j,1 j,NœÜ
                              =     K2 (sj,2                           j,1
                                             T |sT , sT ‚àí1 )p2 (yT |sT )p(sT |sT ‚àí1 )dsT
                                                                                            j,1


                                                         j,N
                              = p2 (yT |sj,2   j,2    œÜ
                                         T )p(sT |sT ‚àí1 ).



Thus,
                 Ô£ÆÔ£´                 Ô£∂              Ô£π
                      NœÜ
                      Y
               E Ô£∞Ô£≠           wÃÉTj,n Ô£∏ FT ‚àí1,NœÜ ,M Ô£ª                                                        (A.23)
                      n=1
                                    Ô£´                                               Ô£∂
                                      NœÜ           j,n‚àí1
                                          pn (yT |sT )
                      Z         Z
                                                                  j,n‚àí1 j,n‚àí2 j,NœÜ Ô£∏
                                      Y
                 =        ¬∑¬∑¬∑       Ô£≠
                                                     j,n‚àí1 Kn‚àí1 (sT    |sT , sT ‚àí1 )
                                         p (y |s
                                     n=4 n‚àí1 T T
                                                          )
                          p3 (yT |sj,2
                                   T )          j,2   j,2 j,NœÜ    j,2       j,NœÜ ‚àí1
                      √ó            j,2 p2 (yT |sT )p(sT |sT ‚àí1 )dsT ¬∑ ¬∑ ¬∑ dsT
                          p2 (yT |sT )
                                         j,NœÜ ‚àí1
                              pNœÜ (yT |sT   )
                      Z
                                                             j,N ‚àí1  j,N ‚àí1 j,N     j,N ‚àí1
                 =                    j,NœÜ ‚àí1
                                                pNœÜ ‚àí1 (yT |sT œÜ )p(sT œÜ |sT ‚àí1œÜ )dsT œÜ
                         pNœÜ ‚àí1 (yT |sT       )
                             j,NœÜ
                 =    p(yT |sT ‚àí1 ).
Online Appendix                                                                                      A-13


The first equality follows from (A.22). The second equality is obtained by sequentially
                              j,NœÜ‚àí2
integrating out sj,2
                 T , . . . , sT        , using a similar argument as for sj,1
                                                                          T . This proves the first

part of the lemma.

Part 2 (No Resampling). Using Lemma 3, we write

                                               NœÜ        M                                
                             j,NœÜ        j,NœÜ
                                                Y       1 X j,n         j,n‚àí1
               E p(YT ‚àíh:T |sT ‚àíh‚àí1 , Œ∏)WT ‚àíh‚àí1               wÃÉT ‚àíh‚àí1 WT ‚àíh‚àí1 FT ‚àíh‚àí2,NœÜ ,M
                                                n=1
                                                       M  j=1
                                              Ô£´              Ô£∂
                                               NœÜ                                   
                                    j,NœÜ                          j,NœÜ
                                                Y     j,n
                = E p(YT ‚àíh:T |sT ‚àíh‚àí1 , Œ∏) Ô£≠       wÃÉT ‚àíh‚àí1 Ô£∏ WT ‚àíh‚àí2 FT ‚àíh‚àí2,NœÜ ,M         (A.24)
                                                         n=1



To prove the second part of the lemma, we slightly modify the last step of the integration
in (A.23):
               Ô£Æ                          Ô£´                Ô£∂               Ô£π
                                              NœÜ
                             j,N
                                              Y
                               œÜ
             E Ô£∞p(YT ‚àíh:T |sT ‚àíh‚àí1 )Ô£≠              wÃÉTj,n‚àíh‚àí1 Ô£∏ FT ‚àí2,NœÜ ,M Ô£ª                       (A.25)
                                           n=1
                   Z
                                     j,N
                                      œÜ                      œÜ  j,N ‚àí1  œÜ   j,N ‚àí1
                                                                                 œÜ   j,N   j,N ‚àí1
                                                                                           œÜ
               =       p(YT ‚àíh:T |sT ‚àíh‚àí1 )pNœÜ (yT ‚àíh‚àí1 |sT ‚àíh‚àí1 )p(sT ‚àíh‚àí1 |sT ‚àíh‚àí2 )dsT ‚àíh‚àí1
                                  œÜ j,N
               = p(YT ‚àíh‚àí1:T |sT ‚àíh‚àí2 ),


as required.

Part 1 (Resampling in tempering iteration nÃÑ). We now assume that the selection step is
executed once, in iteration nÃÑ, i.e., N = {nÃÑ}. For reasons that will become apparent subse-
quently, we will use i subscripts for particles in stages n = 1, . . . , nÃÑ ‚àí 1. Using Lemma 3, we
deduce that it suffices to show:
                      nÃÑ‚àí1
                        Y 1 X  M               X M                
                                     i,n i,n‚àí1    1       j,nÃÑ j,nÃÑ‚àí1
                    E              wÃÉT WT               wÃÉT WT                                      (A.26)
                        n=1
                             M i=1
                                                  M j=1
                                           NœÜ
                                        M  Y                              
                                     1 X            j,n   j,nÃÑ
                                   √ó             wÃÉ      WT      FT ‚àí1,NœÜ ,M
                                     M j=1 n=nÃÑ+1 T
                                                               M
                                                            1 X          j,N     j,N
                                                          =       p(yT |sT ‚àí1œÜ )WT ‚àí1œÜ .
                                                            M j=1
Online Appendix                                                                                                 A-14

                                                                                                      1:M,NœÜ
To evaluate the expectation, we need to integrate over the particles s1:M,1
                                                                      T     , . . . , sT                       as well
as the particles sÃÇ1:M,nÃÑ
                   T      generated during the selection step. We have to distinguish two cases:

                                                                    j,NœÜ
                Case 1, n 6= nÃÑ : sj,n      j,n j,n‚àí1
                                   T ‚àº Kn (sT |sT     , sT                    ),   j = 1, . . . , M
                                                                  j,NœÜ
                Case 2, n = nÃÑ : sj,n      j,n j,n
                                  T ‚àº Kn (sT |sÃÇT , sT                  j = 1, . . . , M ;
                                                                         ),

                                       sÃÇj,n   ‚àº M N s1:M,n‚àí1 , WÃÉT1:M,n , j = 1, . . . , M
                                                                        
                                         T            T



where M N (¬∑) here denotes the multinomial distribution.

    In a preliminary step, we are integrating out the particles sÃÇ1:M,nÃÑ
                                                                  T      . These particles enter the
                                                 j,N
Markov transition kernel KnÃÑ (sj,nÃÑ j,nÃÑ    œÜ                                          j,nÃÑ 1:M,nÃÑ‚àí1
                               T |sÃÇT , sT ‚àí1 ) as well as the conditional density p(sÃÇT |sT         ).
Under the assumption that the resampling step is executed using multinomial resampling,

                                                          M
                                                       1 X j,nÃÑ j,nÃÑ
                           p(sÃÇj,nÃÑ
                               T |sT
                                    1:M,nÃÑ‚àí1
                                             )=              WÃÉT Œ¥(sÃÇT ‚àí si,nÃÑ‚àí1
                                                                          T      ),
                                                       M i=1

                                                                                                      R
where Œ¥(x) is the Dirac function with the property that Œ¥(x) = 0 for x 6= 0 and                           Œ¥(x)dx = 1.
Integrating out the resampled particles yields


               p(s1:M,nÃÑ
                  T      |sT1:M,nÃÑ‚àí1 )                                                                         (A.27)
                     Z Y   M                            X    M                      
                                      j,nÃÑ j,nÃÑ j,NœÜ     1
                 =             KnÃÑ (sT |sÃÇT , sT ‚àí1 )            WÃÉT Œ¥(sÃÇT ‚àí sT ) dsÃÇ1:M,nÃÑ
                                                                   i,nÃÑ  j,nÃÑ i,nÃÑ‚àí1
                                                                                       T
                          j=1
                                                        M i=1
                      M Z                               X    M                      
                     Y                j,nÃÑ j,nÃÑ j,NœÜ     1
                 =             KnÃÑ (sT |sÃÇT , sT ‚àí1 )            WÃÉT Œ¥(sÃÇT ‚àí sT ) dsÃÇj,nÃÑ
                                                                   i,nÃÑ  j,nÃÑ i,nÃÑ‚àí1
                                                                                       T
                     j=1
                                                        M    i=1
                      M          M                                   
                     Y        1 X i,nÃÑ             j,nÃÑ i,nÃÑ‚àí1 i,NœÜ
                 =                     WÃÉ KnÃÑ (sT |sT , sT ‚àí1 ) .
                     j=1
                              M i=1 T

In the last equation, the superscript for sT ‚àí1 changes from j to i because during the resam-
pling, we keep track of the history of the particle. Thus, if for particle j = 1 the value sÃÇ1,nÃÑ
                                                                                             T
                                 3,N
is set to sT3,nÃÑ‚àí1 , we also use sT ‚àí1œÜ for this particle.
Online Appendix                                                                                   A-15


   We can now express the expected value, which we abbreviate as E, as the following
integral:

       nÃÑ‚àí1
         Y 1 X  M              X M               
                     i,n i,n‚àí1    1      j,nÃÑ j,nÃÑ‚àí1
 E = E             wÃÉ W                wÃÉ W                                                     (A.28)
         n=1
              M i=1 T T ‚àí1        M j=1 T T
            X     NœÜ
               M  Y                              
            1              j,n   j,nÃÑ
          √ó             wÃÉ      WT      FT ‚àí1,NœÜ ,M
            M j=1 n=nÃÑ+1 T
            Z  nÃÑ‚àí1       M                       M                         M  Y  NœÜ
          Z      Y 1 X         i,n  i,n‚àí1
                                            X
                                                1       j,nÃÑ j,nÃÑ‚àí1
                                                                     X
                                                                          1               j,n
                                                                                              
    =   ¬∑¬∑¬∑                   wÃÉ W                    wÃÉ W                             wÃÉ
                 n=1
                       M i=1 T T ‚àí1             M j=1 T T               M j=1 n=nÃÑ+1 T
        nÃÑ‚àí1 M                         Y M     M                               
         YY           i,n i,n‚àí1 i,NœÜ            1 X i,nÃÑ          j,nÃÑ i,nÃÑ‚àí1 i,NœÜ
      √ó          Kn (sT |sT , sT ‚àí1 )                 WÃÉT KnÃÑ (sT |sT , sT ‚àí1 )
         n=1 j=1                           j=1
                                                M i=1
              œÜ ‚àí1 M
            NY                          
                          j,n j,n‚àí1 j,NœÜ                  1:M,NœÜ ‚àí1
                   Y
          √ó          Kn (sT |sT , sT ‚àí1 ) ds1:M,1
                                            T     ¬∑ ¬∑ ¬∑ dsT         .
                n=nÃÑ+1 j=1



For the second equality, we used the fact that WTj,nÃÑ = 1.

   Using Lemma 4, we can write

     Z         Z           NœÜ
                        M  Y               œÜ ‚àí1 M
                                         NY                            
                     1 X            j,n
                                                 Y        j,n j,n‚àí1 j,NœÜ                      1:M,NœÜ ‚àí1
         ¬∑¬∑¬∑                     wÃÉ                  Kn (sT |sT , sT ‚àí1 ) dsT1:M,nÃÑ+1 ¬∑ ¬∑ ¬∑ dsT
                     M j=1 n=nÃÑ+1 T       n=nÃÑ+1 j=1
              M Z         NœÜ
                      Z  Y               œÜ ‚àí1
                                       NY                        
           1 X                    j,n               j,n j,n‚àí1 j,NœÜ                   j,N ‚àí1
         =        ¬∑¬∑¬∑           wÃÉT            Kn (sT |sT , sT ‚àí1 ) dsj,nÃÑ+1
                                                                      T      ¬∑ ¬∑ ¬∑ dsT œÜ
           M j=1         n=nÃÑ+1         n=nÃÑ+1
              M
           1 X            j,NœÜ
         =       F (sj,nÃÑ
                     T , sT ‚àí1 ).                                                               (A.29)
           M j=1
Online Appendix                                                                                                 A-16


    Now consider the following integral involving terms that depend on s1:M,nÃÑ
                                                                        T      :

                      Z   M                       X      M                       
                        1 X          j,nÃÑ j,NœÜ
                                                       1            j,nÃÑ    j,nÃÑ‚àí1
           I1   =             F sT , sT ‚àí1                        wÃÉ W                                           (A.30)
                      M j=1                            M j=1 T T
                     M       M                                          
                    Y     1 X i,nÃÑ               j,nÃÑ i,nÃÑ‚àí1 i,NœÜ
                  √ó               WÃÉT KnÃÑ (sT |sT , sT ‚àí1 ) ds1:M,nÃÑ          T
                    j=1
                         M   i=1
                   X   M Z                              M                                               
                    1                j,nÃÑ j,NœÜ
                                                  1 X            i,nÃÑ         j,nÃÑ i,nÃÑ‚àí1 i,NœÜ       j,nÃÑ
                =             F sT , sT ‚àí1                   WÃÉ KnÃÑ (sT |sT , sT ‚àí1 ) dsT
                    M j=1                            M i=1 T
                     X   M                    
                      1        j,nÃÑ     j,nÃÑ‚àí1
                  √ó          wÃÉ W
                      M j=1 T T
                      M Z                             M                                               
                   1 X            j,nÃÑ j,NœÜ
                                                1 X         i,nÃÑ      i,nÃÑ‚àí1        j,nÃÑ i,nÃÑ‚àí1 i,NœÜ
                =           F sT , sT ‚àí1                   wÃÉT WT             KnÃÑ (sT |sT , sT ‚àí1 ) dsj,nÃÑ   T .
                  M j=1                           M i=1

The first equality is the definition of I1 . The second equality is a consequence of Lemma 4.
The last equality is obtained by recalling that

                                                              wÃÉi,nÃÑ WTi,nÃÑ‚àí1
                                             WÃÉTi,nÃÑ =   1
                                                             PMT i,nÃÑ        i,nÃÑ‚àí1
                                                                                    .
                                                         M     i=1 wÃÉT WT



    We proceed in the evaluation of the expected value E by integrating over the particle
values s1:M,1
        T     , . . . , sT1:M,nÃÑ‚àí1 :

                                Z        Z         M
                                                  nÃÑ‚àí1
                                                   Y          
                                               1 X i,n i,n‚àí1
                      E =   ¬∑ ¬∑ ¬∑ I1 ¬∑               wÃÉ W                                                     (A.31)
                                        n=1
                                              M i=1 T T
                            nÃÑ‚àí1 M                      
                                          i,n i,n‚àí1 i,NœÜ
                              YY
                          √ó          Kn (sT |sT , sT ‚àí1 ) ds1:M,1
                                                            T     ¬∑ ¬∑ ¬∑ ds1:M,nÃÑ‚àí1
                                                                          T        ,
                                       n=1 j=1
Online Appendix                                                                                                        A-17


where
             nÃÑ‚àí1
              Y         M                  
                    1 X i,n i,n‚àí1
        I1 ¬∑                 wÃÉ W
             n=1
                   M i=1 T T
              X    M Z                            M                                                    
               1                  j,nÃÑ j,NœÜ
                                             1 X         i,nÃÑ    i,nÃÑ‚àí1       j,nÃÑ i,nÃÑ‚àí1 i,NœÜ      j,nÃÑ
          =                F sT , sT ‚àí1                wÃÉ W              KnÃÑ (sT |sT , sT ‚àí1 ) dsT
               M j=1                            M i=1 T T
                nÃÑ‚àí1
                 Y 1 X        M                 
                                      i,n  i,n‚àí1
             √ó                     wÃÉT WT
                 n=1
                         M    i=1
                 M    Z                          M                    nÃÑ‚àí1
                                                                         Y 1 X       M               
              1 X              j,nÃÑ j,NœÜ
                                           1 X        i,nÃÑ    i,nÃÑ‚àí1                      i,n  i,n‚àí1
          =              F sT , sT ‚àí1                wÃÉ W                                wÃÉT WT
             M j=1                            M i=1 T T                  n=1
                                                                                M    i=1
                                          
                          i,nÃÑ‚àí1 i,NœÜ
             √óKnÃÑ (sj,nÃÑ
                     T |sT       , sT ‚àí1 ) dsj,nÃÑ
                                              T

               M Z                           M          nÃÑ‚àí1
                                                          Y i,n  i,N
            1 X              j,nÃÑ j,NœÜ
                                        1 X       i,nÃÑ
          =            F sT , sT ‚àí1             wÃÉ             wÃÉ WT ‚àí1œÜ
            M j=1                         M i=1 T n=1 T
                                       
                   j,nÃÑ i,nÃÑ‚àí1 i,NœÜ
            √óKnÃÑ (sT |sT , sT ‚àí1 ) dsj,nÃÑ T .



The last equality follows from the second part of Lemma 3. Notice the switch from j to i
superscript for functions of particles in stages n < nÃÑ. Thus,

           M Z                       Z      Z  X   M          nÃÑ‚àí1
                                                                Y i,n  i,N
        1 X              j,nÃÑ j,NœÜ
                                              1         i,nÃÑ
    E =            F sT , sT ‚àí1         ¬∑¬∑¬∑            wÃÉT           wÃÉT WT ‚àí1œÜ                  (A.32)
        M j=1                                  M i=1            n=1
                                    nÃÑ‚àí1  M                          
               j,nÃÑ i,nÃÑ‚àí1 i,NœÜ                    i,n i,n‚àí1 i,NœÜ
                                       YY
        √óKnÃÑ (sT |sT , sT ‚àí1 )                Kn (sT |sT , sT ‚àí1 ) ds1:M,1
                                                                         T    ¬∑ ¬∑ ¬∑ ds1:M,nÃÑ‚àí1
                                                                                      T        dsj,nÃÑ
                                                                                                 T
                                                  n=1 i=1
             M Z                   M Z     Z YnÃÑ       
          1 X       j,nÃÑ j,NœÜ
                               1 X                  i,n   i,N
        =        F sT , sT ‚àí1           ¬∑¬∑¬∑        wÃÉT WT ‚àí1œÜ
          M j=1                  M i=1         n=1
                                               nÃÑ‚àí1                                                      
                        i,nÃÑ‚àí1 i,NœÜ                            i,n‚àí1 i,NœÜ
                                               Y
            √óKnÃÑ (sj,nÃÑ
                   T |sT      , sT ‚àí1 )               Kn (si,n
                                                           T |sT    , sT ‚àí1 )dsi,1
                                                                               T        ¬∑ ¬∑ ¬∑ dsi,nÃÑ‚àí1
                                                                                                T            dsj,nÃÑ
                                                                                                               T .
                                               n=1


The second equality follows from Lemma 4. The calculations in (A.23) imply that

             Z            nÃÑ
                       Z Y                            nÃÑ‚àí1
                                               i,N                                i,N
                                                        Y
                 ¬∑¬∑¬∑             wÃÉTi,n       WT ‚àí1œÜ           Kn (si,n i,n‚àí1
                                                                    T |sT     , sT ‚àí1œÜ )dsi,1       i,nÃÑ‚àí2
                                                                                          T ¬∑ ¬∑ ¬∑ dsT                 (A.33)
                           n=1                          n=1
                                                                                             i,N         i,N
                                                        = pnÃÑ‚àí1 (yT |si,nÃÑ‚àí1
                                                                      T      )p(si,nÃÑ‚àí1
                                                                                 T      |sT ‚àí1œÜ )WT ‚àí1œÜ .
Online Appendix                                                                                               A-18


In turn,

              M Z                          M Z
         1 X               j,nÃÑ j,NœÜ
                                       1 X                    i,nÃÑ‚àí1 i,NœÜ
     E =             F sT , sT ‚àí1                  KnÃÑ (sj,nÃÑ
                                                         T |sT       , sT ‚àí1 )
         M j=1                           M i=1
                                                           
                    i,nÃÑ‚àí1      i,nÃÑ‚àí1 i,NœÜ   i,NœÜ  i,nÃÑ‚àí1
         √ópnÃÑ (yT |sT )p(sT |sT ‚àí1 )WT ‚àí1 dsT                 dsj,nÃÑ
                                                                 T

                  M         M Z
             1 X 1 X                        j,NœÜ      j,nÃÑ i,nÃÑ‚àí1 i,NœÜ
           =                      F sj,nÃÑ
                                       T , sT ‚àí1 KnÃÑ (sT |sT      , sT ‚àí1 )dsj,nÃÑ
                                                                             T                           (A.34)
             M i=1 M j=1
                                                              
                        i,nÃÑ‚àí1   i,nÃÑ‚àí1 i,NœÜ     i,NœÜ  i,nÃÑ‚àí1
             √ópnÃÑ (yT |sT )p(sT |sT ‚àí1 )WT ‚àí1 dsT
                M Z
             1 X               i,NœÜ                   i,nÃÑ i,NœÜ    i,NœÜ
           =        F si,nÃÑ
                         T  , s               i,nÃÑ
                               T ‚àí1 pnÃÑ (yT |sT )p(sT |sT ‚àí1 )WT ‚àí1 dsT
                                                                         i,nÃÑ
             M i=1
                M Z     Z  Y    NœÜ           NY œÜ ‚àí1                       
             1 X                         j,n                   j,n j,n‚àí1 j,NœÜ
           =        ¬∑¬∑¬∑               wÃÉT                Kn (sT |sT , sT ‚àí1 )
             M j=1             n=nÃÑ+1            n=nÃÑ+1
                                      j,N     j,N                j,NœÜ ‚àí1                        j,N
               √ópnÃÑ (yT |sj,nÃÑ  j,nÃÑ   œÜ      œÜ   j,nÃÑ+1
                          T )p(sT |sT ‚àí1 )WT ‚àí1 dsT      ¬∑ ¬∑ ¬∑ dsT         pnÃÑ (yT |sj,nÃÑ  j,nÃÑ   œÜ    j,nÃÑ
                                                                                     T )p(sT |sT ‚àí1 )dsT
                M
             1 X          j,N     j,N
           =       p(yT |sT ‚àí1œÜ )WT ‚àí1œÜ .
             M j=1

The second equality is obtained by changing the order of two summations. To obtain the
third equality, we integrate out the si,nÃÑ‚àí1
                                      T      terms along the lines of (A.21). Notice that the
value of the integral is identical for all values of the j superscript. Thus, we simply set j = i
                                                                                         i,NœÜ 
and drop the average. For the fourth equality, we plug in the definition of F si,nÃÑ T , sT ‚àí1 and

replace the i index with a j index. The last equality follows from calculations similar to
those in (A.23). This completes the analysis of Part 1.

Part 2 (Resampling in tempering iteration nÃÑ). A similar argument as for Part 1 can be used
to extend the result for Part 2.

Resampling in multiple tempering iterations. The previous analysis can be extended to the
case in which the selection step is executed in multiple tempering iterations n ‚àà N , assuming
that the set N does not itself depend on the particle system. 
Online Appendix                                                                                  A-19


A.3.2    Proof of Main Theorem

Proof of Theorem 2. Suppose that for any h such that 0 ‚â§ h ‚â§ T ‚àí 1

                                                     M
                                                1 X               j,NœÜ        j,NœÜ
     E pÃÇ(YT ‚àíh:T |Y1:T ‚àíh‚àí1 , Œ∏)|FT ‚àíh‚àí1,NœÜ ,M =       p(YT ‚àíh:T |sT ‚àíh‚àí1 , Œ∏)WT ‚àíh‚àí1 ,        (A.35)
                                                  M j=1

where                                                    Ô£´                           !Ô£∂
                                                 T           NœÜ       M
                                                 Y           Y     1 X
                 pÃÇ(YT ‚àíh:T |Y1:T ‚àíh‚àí1 , Œ∏) =            Ô£≠              wÃÉj,n Wtj,n‚àí1 Ô£∏ .
                                                t=T ‚àíh       n=1
                                                                   M j=1 t

Then, by setting h = T ‚àí 1, we can deduce that

                                                 M
                                            1 X            j,N      j,N
                     E pÃÇ(Y1:T |Œ∏)|F0,NœÜ ,M =       p(Y1:T |s0 œÜ , Œ∏)W0 œÜ .                     (A.36)
                                              M j=1

Recall that for period t = 0, we adopted the convention that NœÜ = 1 and assumed that the
                                                    j,NœÜ                    j,NœÜ
states were initialized by direct sampling: s0               ‚àº p(s0 ) and W0       = 1. Thus,
                                                                
                                                             
                       E pÃÇ(Y1:T |Œ∏) = E E pÃÇ(Y1:T |Œ∏)|F0,NœÜ ,M                                 (A.37)
                                             X  M                         
                                              1                j,NœÜ   j,NœÜ
                                        = E           p(Y1:T |s0 , Œ∏)W0
                                             M j=1
                                          Z
                                        =    p(Y1:T |s0 , Œ∏)p(s0 )ds0

                                        = p(Y1:T |Œ∏),


as desired.

   In the remainder of the proof, we use an inductive argument to establish (A.35). If (A.35)
Online Appendix                                                                                          A-20


holds for h, it also has to hold for h + 1:

                                            
E pÃÇ(YT ‚àíh‚àí1:T |Y1:T ‚àíh‚àí2 , Œ∏)|FT ‚àíh‚àí2,NœÜ ,M
                                                                                        
                                              
 = E E pÃÇ(YT ‚àíh:T |Y1:T ‚àíh‚àí1 , Œ∏) FT ‚àíh‚àí1,NœÜ ,M pÃÇ(yT ‚àíh‚àí1 |Y1:T ‚àíh‚àí2 , Œ∏) FT ‚àíh‚àí2,NœÜ ,M
       M
    1 X                j,NœÜ          j,NœÜ                                            
  =       E p(YT ‚àíh:T |sT ‚àíh‚àí1 , Œ∏)WT ‚àíh‚àí1   pÃÇ(yT ‚àíh‚àí1 |Y1:T ‚àíh‚àí2 , Œ∏) FT ‚àíh‚àí2,NœÜ ,M
    M j=1
           Ô£Æ                                   Ô£´                                  !Ô£∂                 Ô£π
       M                                          NœÜ         M
    1 X Ô£∞                 j,NœÜ          j,NœÜ
                                                 Y 1 X j,n
  =       E p(YT ‚àíh:T |sT ‚àíh‚àí1   , Œ∏)WT ‚àíh‚àí1   Ô£≠                wÃÉT ‚àíh‚àí1 WTj,n‚àí1
                                                                             ‚àíh‚àí1
                                                                                     Ô£∏ FT ‚àíh‚àí2,NœÜ ,M Ô£ª
    M j=1                                        n=1
                                                        M   j=1
         M
      1 X                 j,NœÜ        j,NœÜ
  =         p(YT ‚àíh‚àí1:T |sT ‚àíh‚àí2 , Œ∏)WT ‚àíh‚àí2 .
      M j=1

Note that FT ‚àíh‚àí2,NœÜ ,M ‚äÇ FT ‚àíh‚àí1,NœÜ ,M . Thus, the first equality follows from the law of iter-
ated expectations. The second equality follows from the inductive hypothesis (A.35). The
third equality uses the definition of the period-likelihood approximation in (23) of Algo-
rithm 2. The last equality follows from the second part of Lemma 5.

    We now verify that the inductive hypothesis (A.35) holds for h = 0. Using the definition
of pÃÇ(yT |Y1:T ‚àí1 , Œ∏), we can write
                                                  Ô£Æ                                                 Ô£π
                                                      NœÜ       M
                                                                                      !
                                                Y          1   X
                                                                     wÃÉTj,n WTj,n‚àí1
                                        
        E pÃÇ(yT |Y1:T ‚àí1 , Œ∏)|FT ‚àí1,NœÜ ,M = E Ô£∞                                           FT ‚àí1,NœÜ ,M Ô£ª (A.38)
                                                    n=1
                                                           M   j=1
                                                    M
                                                 1 X          j,N     j,N
                                          =            p(yT |sT ‚àí1œÜ )WT ‚àí1œÜ .
                                                 M j=1


The second equality follows from the first part of Lemma 5. Thus, we can deduce that (A.35)
holds for h = T ‚àí 1 as required. This completes the proof. 



B      Computational Details

The code for this project is available at http://github.com/eph/tempered_pf. The appli-
cations in Section 4 were coded in Fortran and compiled using the Intel Fortran Compiler
Online Appendix                                                                                   A-21


(version: 13.0.0), including the math kernel library. The calculations in Algorithm 1, part
2(a)ii, Algorithm 2, part 1(a)i, and Algorithm 2, part 2(c) were implemented using OpenMP
(shared memory) multithreading.



C       DSGE Models and Data Sources

C.1     Small-Scale DSGE Model

C.1.1    Equilibrium Conditions

We write the equilibrium conditions by expressing each variable in terms of percentage
deviations from its steady state value. Let xÃÇt = ln(xt /x) and write

                                      h                                  i
                           1 = Œ≤Et e‚àíœÑ cÃÇt+1 +œÑ cÃÇt +RÃÇt ‚àízÃÇt+1 ‚àíœÄÃÇt+1                           (A.39)
                                                                         
                                  œÄÃÇt
                                                        1      œÄÃÇt      1
                           0 = e ‚àí1            1‚àí            e +                                 (A.40)
                                                        2ŒΩ             2ŒΩ
                                         œÄÃÇt+1          ‚àíœÑ cÃÇt+1 +œÑ cÃÇt +yÃÇt+1 ‚àíyÃÇt +œÄÃÇt+1 
                                ‚àíŒ≤Et e          ‚àí1 e
                                  1‚àíŒΩ             œÑ cÃÇt
                                                        
                                +           1 ‚àí e
                                  ŒΩœÜœÄ 2
                                         œÜœÄ 2 g œÄÃÇt         2
                    ecÃÇt ‚àíyÃÇt = e‚àígÃÇt ‚àí         e ‚àí1                                             (A.41)
                                           2
                         RÃÇt = œÅR RÃÇt‚àí1 + (1 ‚àí œÅR )œà1 œÄÃÇt                                        (A.42)

                                   +(1 ‚àí œÅR )œà2 (yÃÇt ‚àí gÃÇt ) + R,t

                        gÃÇt = œÅg gÃÇt‚àí1 + g,t                                                    (A.43)

                         zÃÇt = œÅz zÃÇt‚àí1 + z,t .                                                 (A.44)


    Log-linearization and straightforward manipulation of Equations (A.39) to (A.41) yield
the following representation for the consumption Euler equation, the New Keynesian Phillips
Online Appendix                                                                                    A-22


curve, and the monetary policy rule:
                                                                         
                                       1
                 yÃÇt   = Et [yÃÇt+1 ] ‚àí     RÃÇt ‚àí Et [œÄÃÇt+1 ] ‚àí Et [zÃÇt+1 ]                        (A.45)
                                       œÑ
                         +gÃÇt ‚àí Et [gÃÇt+1 ]

                 œÄÃÇt = Œ≤Et [œÄÃÇt+1 ] + Œ∫(yÃÇt ‚àí gÃÇt )

                RÃÇt = œÅR RÃÇt‚àí1 + (1 ‚àí œÅR )œà1 œÄÃÇt + (1 ‚àí œÅR )œà2 (yÃÇt ‚àí gÃÇt ) + R,t


where
                                                          1‚àíŒΩ
                                                  Œ∫=œÑ            .                                (A.46)
                                                          ŒΩœÄ 2 œÜ
To construct a likelihood function, we have to relate the model variables to a set of ob-
servables yt . We use the following three observables for estimation: quarter-to-quarter per
capita GDP growth rates (YGR), annualized quarter-to-quarter inflation rates (INFL), and
annualized nominal interest rates (INT). The three series are measured in percentages, and
their relationship to the model variables is given by the following set of equations:


                              Y GRt = Œ≥ (Q) + 100(yÃÇt ‚àí yÃÇt‚àí1 + zÃÇt )                             (A.47)

                            IN F Lt = œÄ (A) + 400œÄÃÇt

                               IN Tt = œÄ (A) + r(A) + 4Œ≥ (Q) + 400RÃÇt .


The parameters Œ≥ (Q) , œÄ (A) , and r(A) are related to the steady states of the model economy
as follows:
                                  Œ≥ (Q)                     1                           œÄ (A)
                       Œ≥ =1+            ,    Œ≤=                       ,    œÄ =1+              .
                                  100               1 + r(A) /400                       400
The structural parameters are collected in the vector Œ∏. Since in the first-order approximation
the parameters ŒΩ and œÜ are not separately identifiable, we express the model in terms of Œ∫,
defined in (A.46). Let


                  Œ∏ = [œÑ, Œ∫, œà1 , œà2 , œÅR , œÅg , œÅz , r(A) , œÄ (A) , Œ≥ (Q) , œÉR , œÉg , œÉz ]0 .
Online Appendix                                                                     A-23


C.1.2    Data Sources

  1. Per Capita Real Output Growth Take the level of real gross domestic product,
      (FRED mnemonic ‚ÄúGDPC1‚Äù), call it GDPt . Take the quarterly average of the Civilian
      Non-institutional Population (FRED mnemonic ‚ÄúCNP16OV‚Äù / BLS series ‚ÄúLNS10000000‚Äù),
      call it P OPt . Then,


                              Per Capita Real Output Growth
                                                               
                                            GDPt           GDPt‚àí1
                               = 100 ln             ‚àí ln              .
                                            P OPt          P OPt‚àí1


  2. Annualized Inflation. Take the CPI price level, (FRED mnemonic ‚ÄúCPIAUCSL‚Äù),
      call it CP It . Then,
                                                                           
                                                                   CP It
                              Annualized Inflation = 400 ln                     .
                                                                  CP It‚àí1


  3. Federal Funds Rate. Take the effective federal funds rate (FRED mnemonic ‚ÄúFED-
      FUNDS‚Äù), call it F F Rt . Then,


                                    Federal Funds Rate = F F Rt .



C.2     The Smets-Wouters Model

C.2.1    Equilibrium Conditions

The log-linearized equilibrium conditions of the Smets and Wouters (2007) model take the
following form:
Online Appendix                                                                      A-24




                  yÃÇt = cy cÃÇt + iy iÃÇt + zy zÃÇt + Œµgt                              (A.48)
                            h/Œ≥                     1
                  cÃÇt =             cÃÇt‚àí1 +                Et cÃÇt+1                 (A.49)
                         1 + h/Œ≥               1 + h/Œ≥
                            wlc (œÉc ‚àí 1) ÀÜ
                         +                    (lt ‚àí Et ÀÜlt+1 )
                           œÉc (1 + h/Œ≥)
                               1 ‚àí h/Œ≥                                1 ‚àí h/Œ≥ b
                         ‚àí                    (rÃÇt ‚àí Et œÄÃÇt+1 ) ‚àí              Œµ
                           (1 + h/Œ≥)œÉc                              (1 + h/Œ≥)œÉc t
                                 1                       Œ≤Œ≥ (1‚àíœÉc )
                   iÃÇt =                  iÃÇ t‚àí1  +                  Et iÃÇt+1       (A.50)
                         1 + Œ≤Œ≥ (1‚àíœÉc )              1 + Œ≤Œ≥ (1‚àíœÉc )
                                        1
                         + 2                          qÃÇt + Œµit
                           œïŒ≥ (1 + Œ≤Œ≥ (1‚àíœÉc ) )
                  qÃÇt = Œ≤(1 ‚àí Œ¥)Œ≥ ‚àíœÉc Et qÃÇt+1 ‚àí rÃÇt + Et œÄÃÇt+1                     (A.51)

                           +(1 ‚àí Œ≤(1 ‚àí Œ¥)Œ≥ ‚àíœÉc )Et rÃÇt+1
                                                     k
                                                         ‚àí Œµbt

                  yÃÇt = Œ¶(Œ±kÃÇts + (1 ‚àí Œ±)ÀÜlt + Œµat )                                (A.52)

                  kÃÇts = kÃÇt‚àí1 + zÃÇt                                                (A.53)
                         1‚àíœà k
                   zÃÇt =        rÃÇ                                                  (A.54)
                            œà t
Online Appendix                                                                       A-25




                              (1 ‚àí Œ¥)
                    kÃÇt =             kÃÇt‚àí1 + (1 ‚àí (1 ‚àí Œ¥)/Œ≥)iÃÇt                     (A.55)
                                 Œ≥
                              +(1 ‚àí (1 ‚àí Œ¥)/Œ≥)œïŒ≥ 2 (1 + Œ≤Œ≥ (1‚àíœÉc ) )Œµit

                   ¬µÃÇpt = Œ±(kÃÇts ‚àí ÀÜlt ) ‚àí wÃÇt + Œµat                                 (A.56)
                             Œ≤Œ≥ (1‚àíœÉc )                         Œπp
                   œÄÃÇt =              (1‚àíœÉ   )
                                               Et œÄÃÇt+1 +                 œÄÃÇt‚àí1      (A.57)
                          1 + Œπp Œ≤Œ≥        c               1 + Œ≤Œ≥ (1‚àíœÉc )
                                  (1 ‚àí Œ≤Œ≥ (1‚àíœÉc ) Œæp )(1 ‚àí Œæp )
                          ‚àí                                             ¬µÃÇpt + Œµpt
                            (1 + Œπp Œ≤Œ≥    (1‚àíœÉ  c ) )(1 + (Œ¶ ‚àí 1)Œµp )Œæp
                     k    ÀÜ
                   rÃÇ = lt + wÃÇt ‚àí kÃÇ    s
                                                                                     (A.58)
                     t                     t
                                                  1
                   ¬µÃÇw
                     t = wÃÇt ‚àí œÉl ÀÜlt ‚àí                (cÃÇt ‚àí h/Œ≥cÃÇt‚àí1 )             (A.59)
                                               1 ‚àí h/Œ≥
                               Œ≤Œ≥ (1‚àíœÉc )
                   wÃÇt =                   (Et wÃÇt+1                                 (A.60)
                            1 + Œ≤Œ≥ (1‚àíœÉc )
                                                   1
                            +Et œÄÃÇt+1 ) +                 (wÃÇt‚àí1 ‚àí Œπw œÄÃÇt‚àí1 )
                                           1 + Œ≤Œ≥ (1‚àíœÉc )
                               1 + Œ≤Œ≥ (1‚àíœÉc ) Œπw
                            ‚àí                    œÄÃÇt
                                1 + Œ≤Œ≥ (1‚àíœÉc )
                                    (1 ‚àí Œ≤Œ≥ (1‚àíœÉc ) Œæw )(1 ‚àí Œæw )
                            ‚àí                                           ¬µÃÇw + Œµw
                               (1 + Œ≤Œ≥ (1‚àíœÉc ) )(1 + (Œªw ‚àí 1)w )Œæw t          t

                    rÃÇt   = œÅrÃÇt‚àí1 + (1 ‚àí œÅ)(rœÄ œÄÃÇt + ry (yÃÇt ‚àí yÃÇt‚àó ))              (A.61)

                              +r‚àÜy ((yÃÇt ‚àí yÃÇt‚àó ) ‚àí (yÃÇt‚àí1 ‚àí yÃÇt‚àí1
                                                               ‚àó
                                                                   )) + Œµrt .




The exogenous shocks evolve according to
Online Appendix                                     A-26




                  Œµat = œÅa Œµat‚àí1 + Œ∑ta             (A.62)

                  Œµbt = œÅb Œµbt‚àí1 + Œ∑tb             (A.63)

                  Œµgt = œÅg Œµgt‚àí1 + œÅga Œ∑ta + Œ∑tg   (A.64)

                  Œµit = œÅi Œµit‚àí1 + Œ∑ti             (A.65)

                  Œµrt = œÅr Œµrt‚àí1 + Œ∑tr             (A.66)

                  Œµpt = œÅr Œµpt‚àí1 + Œ∑tp ‚àí ¬µp Œ∑t‚àí1
                                             p
                                                   (A.67)

                  Œµw
                   t = œÅw Œµw      w       w
                           t‚àí1 + Œ∑t ‚àí ¬µw Œ∑t‚àí1 .    (A.68)
Online Appendix                                                                        A-27


The counterfactual no-rigidity prices and quantities evolve according to


                       yÃÇt‚àó = cy cÃÇ‚àót + iy iÃÇ‚àót + zy zÃÇt‚àó + Œµgt                       (A.69)
                                  h/Œ≥ ‚àó                    1
                       cÃÇ‚àót =             cÃÇt‚àí1 +                  Et cÃÇ‚àót+1          (A.70)
                               1 + h/Œ≥                1 + h/Œ≥
                                  wlc (œÉc ‚àí 1) ÀÜ‚àó
                               +                     (l ‚àí Et ÀÜlt+1‚àó
                                                                     )
                                 œÉc (1 + h/Œ≥) t
                                      1 ‚àí h/Œ≥ ‚àó                 1 ‚àí h/Œ≥ b
                               ‚àí                     rt ‚àí                    Œµ
                                 (1 + h/Œ≥)œÉc                (1 + h/Œ≥)œÉc t
                                        1                       Œ≤Œ≥ (1‚àíœÉc )
                        iÃÇ‚àót =                   iÃÇ ‚àó
                                                         +                   Et iÃÇ‚àó
                               1 + Œ≤Œ≥ (1‚àíœÉc ) t‚àí1 1 + Œ≤Œ≥ (1‚àíœÉc ) t+1
                                               1
                               + 2                           qÃÇ ‚àó + Œµit               (A.71)
                                 œïŒ≥ (1 + Œ≤Œ≥ (1‚àíœÉc ) ) t
                       qÃÇt‚àó = Œ≤(1 ‚àí Œ¥)Œ≥ ‚àíœÉc Et qÃÇt+1    ‚àó
                                                            ‚àí rt‚àó                     (A.72)

                                 +(1 ‚àí Œ≤(1 ‚àí Œ¥)Œ≥ ‚àíœÉc )Et rt+1
                                                          k‚àó
                                                              ‚àí Œµbt

                       yÃÇt‚àó = Œ¶(Œ±kts‚àó + (1 ‚àí Œ±)ÀÜlt‚àó + Œµat )                           (A.73)
                                ‚àó
                       kÃÇts‚àó = kt‚àí1  + zt‚àó                                            (A.74)
                               1 ‚àí œà k‚àó
                        zÃÇt‚àó =        rÃÇ                                              (A.75)
                                  œà t
                               (1 ‚àí Œ¥) ‚àó
                        kÃÇt‚àó =           kÃÇt‚àí1 + (1 ‚àí (1 ‚àí Œ¥)/Œ≥)iÃÇt                   (A.76)
                                   Œ≥
                               +(1 ‚àí (1 ‚àí Œ¥)/Œ≥)œïŒ≥ 2 (1 + Œ≤Œ≥ (1‚àíœÉc ) )Œµit

                       wÃÇt‚àó = Œ±(kÃÇts‚àó ‚àí ÀÜlt‚àó ) + Œµat                                  (A.77)

                       rÃÇtk‚àó = ÀÜlt‚àó + wÃÇt‚àó ‚àí kÃÇt‚àó                                     (A.78)
                                             1
                        wÃÇt‚àó = œÉl ÀÜlt‚àó +          (cÃÇ‚àó + h/Œ≥cÃÇ‚àót‚àí1 ).                 (A.79)
                                          1 ‚àí h/Œ≥ t
Online Appendix                                                                   A-28


The steady state (ratios) that appear in the measurement equation or the log-linearized
equilibrium conditions are given by




                               Œ≥ = Œ≥ÃÑ/100 + 1                                   (A.80)

                              œÄ ‚àó = œÄÃÑ/100 + 1                                  (A.81)

                                rÃÑ = 100(Œ≤ ‚àí1 Œ≥ œÉc œÄ ‚àó ‚àí 1)                     (A.82)
                               k
                             rss = Œ≥ œÉc /Œ≤ ‚àí (1 ‚àí Œ¥)                            (A.83)
                                    Œ±                1
                                      Œ± (1 ‚àí Œ±)(1‚àíŒ±) 1‚àíŒ±
                             wss =            k Œ±
                                                                                (A.84)
                                           Œ¶rss
                              ik = (1 ‚àí (1 ‚àí Œ¥)/Œ≥)Œ≥                             (A.85)
                                            k
                                   1 ‚àí Œ± rss
                              lk =                                              (A.86)
                                      Œ± wss
                                      (Œ±‚àí1)
                              ky = Œ¶lk                                          (A.87)

                               iy = (Œ≥ ‚àí 1 + Œ¥)ky                               (A.88)

                               cy = 1 ‚àí gy ‚àí iy                                 (A.89)
                                     k
                               zy = rss ky                                      (A.90)
                                              k
                                     1 1 ‚àí Œ± rss ky
                              wlc =                 .                           (A.91)
                                    Œªw Œ±       cy


The measurement equations take the form:




                                  Y GRt = Œ≥ÃÑ + yÃÇt ‚àí yÃÇt‚àí1                      (A.92)

                                      IN Ft = œÄÃÑ + œÄÃÇt

                                  F F Rt = rÃÑ + RÃÇt

                                  CGRt = Œ≥ÃÑ + cÃÇt ‚àí cÃÇt‚àí1

                                      IGRt = Œ≥ÃÑ + iÃÇt ‚àí iÃÇt‚àí1

                                 W GRt = Œ≥ÃÑ + wÃÇt ‚àí wÃÇt‚àí1

                              HOU RSt = ¬Øl + ÀÜlt .
Online Appendix                                                                      A-29


C.2.2   Data

The data cover 1966Q1 to 2004Q4. The construction follows that of Smets and Wouters
(2007). Output data come from the NIPA; other sources are noted in the exposition.


  1. Per Capita Real Output Growth. Take the level of real gross domestic prod-
     uct (FRED mnemonic ‚ÄúGDPC1‚Äù), call it GDPt . Take the quarterly average of the
     Civilian Non-institutional Population (FRED mnemonic ‚ÄúCNP16OV‚Äù and BLS series
     ‚ÄúLNS10000000‚Äù) normalized so that its 1992Q3 value is 1 and call it P OPt . Then,


                          Per Capita Real Output Growth
                                                           
                                        GDPt           GDPt‚àí1
                           = 100 ln             ‚àí ln              .
                                        P OPt          P OPt‚àí1


  2. Per Capita Real Consumption Growth. Take the level of personal consumption
     expenditures (FRED mnemonic ‚ÄúPCEC‚Äù), call it CON St . Take the level of the GDP
     price deflator (FRED mnemonic ‚ÄúGDPDEF‚Äù) and call it GDP Pt . Then,


                          Per Capita Real Consumption Growth
                                                       
                                              CON St
                               = 100 ln
                                            GDP Pt P OPt
                                                          
                                             CON St‚àí1
                                   ‚àí ln                      .
                                          GDP Pt‚àí1 P OPt‚àí1


  3. Per Capita Real Investment Growth. Take the level of fixed private investment
     (FRED mnemonic ‚ÄúFPI‚Äù) and call it IN Vt . Then,


                          Per Capita Real Investment Growth
                                                       
                                                IN Vt
                               = 100 ln
                                            GDP Pt P OPt
                                                          
                                              IN Vt‚àí1
                                   ‚àí ln                      .
                                          GDP Pt‚àí1 P OPt‚àí1


  4. Per Capita Real Wage Growth. Take the BLS measure of compensation per
     hour for the nonfarm business sector (FRED mnemonic ‚ÄúCOMPNFB‚Äù / BLS series
Online Appendix                                                                      30


     ‚ÄúPRS85006103‚Äù) and call it Wt . Then,


                         Per Capita Real Wage Growth
                                                            
                                         Wt             Wt‚àí1
                          = 100 ln              ‚àí ln               .
                                       GDP Pt          GDP Pt‚àí1


  5. Per Capita Hours Index. Take the index of average weekly nonfarm business hours
     (FRED mnemonic / BLS series ‚ÄúPRS85006023‚Äù) and call it HOU RSt . Take the number
     of employed civilians (FRED mnemonic ‚ÄúCE16OV‚Äù), normalized so that its 1992Q3 value
     is 1 and call it EM Pt . Then,
                                                                           
                                                          HOU RSt EM Pt
                        Per Capita Hours = 100 ln                               .
                                                             P OPt

     The series is then demeaned.

  6. Inflation. Take the GDP price deflator, then
                                                                   
                                                          GDP Pt
                                Inflation = 100 ln                      .
                                                         GDP Pt‚àí1


  7. Federal Funds Rate. Take the effective federal funds rate (FRED mnemonic ‚ÄúFED-
     FUNDS‚Äù) and call it F F Rt . Then,


                                Federal Funds Rate = F F Rt /4.
