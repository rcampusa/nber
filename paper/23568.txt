                              NBER WORKING PAPER SERIES




 USING INSTRUMENTAL VARIABLES FOR INFERENCE ABOUT POLICY RELEVANT
                        TREATMENT EFFECTS

                                       Magne Mogstad
                                        Andres Santos
                                    Alexander Torgovitsky

                                      Working Paper 23568
                              http://www.nber.org/papers/w23568


                    NATIONAL BUREAU OF ECONOMIC RESEARCH
                             1050 Massachusetts Avenue
                               Cambridge, MA 02138
                                    July 2017




We thank Liran Einav, Derek Neal, Ed Vytacil, Chris Walters, and seminar participants at several
universities and conferences for valuable feedback and suggestions. Bradley Setzler provided
excellent research assistance. We are grateful to Pascaline Dupas for her help in accessing the
data and in understanding the institutional details. The views expressed herein are those of the
authors and do not necessarily reflect the views of the National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.

© 2017 by Magne Mogstad, Andres Santos, and Alexander Torgovitsky. All rights reserved.
Short sections of text, not to exceed two paragraphs, may be quoted without explicit permission
provided that full credit, including © notice, is given to the source.
Using Instrumental Variables for Inference about Policy Relevant Treatment Effects
Magne Mogstad, Andres Santos, and Alexander Torgovitsky
NBER Working Paper No. 23568
July 2017
JEL No. C21,C36

                                          ABSTRACT

We propose a method for using instrumental variables (IV) to draw inference about causal effects
for individuals other than those affected by the instrument at hand. Policy relevance and external
validity turns on the ability to do this reliably. Our method exploits the insight that both the IV
estimand and many treatment parameters can be expressed as weighted averages of the same
underlying marginal treatment effects. Since the weights are known or identified, knowledge of
the IV estimand generally places some restrictions on the unknown marginal treatment effects,
and hence on the values of the treatment parameters of interest. We show how to extract
information about the average effect of interest from the IV estimand, and, more generally, from
a class of IV-like estimands that includes the two stage least squares and ordinary least squares
estimands, among many others. Our method has several applications. First, it can be used to
construct nonparametric bounds on the average causal effect of a hypothetical policy change.
Second, our method allows the researcher to flexibly incorporate shape restrictions and
parametric assumptions, thereby enabling extrapolation of the average effects for compliers to the
average effects for different or larger populations. Third, our method can be used to test model
specification and hypotheses about behavior, such as no selection bias and/or no selection on
gain. To accommodate these diverse applications, we devise a novel inference procedure that is
designed to exploit the convexity of our setting. We develop uniformly valid tests that allow for
an infinite number of IV--like estimands and a general convex parameter space. We apply our
method to analyze the effects of price subsidies on the adoption and usage of an antimalarial bed
net in Kenya.


Magne Mogstad                                         Alexander Torgovitsky
Department of Economics                               Department of Economics
University of Chicago                                 Northwestern University
1126 East 59th Street                                 2001 Sheridan Rd, Room 302
Chicago, IL 60637                                     Evanston, IL 60208-2600
and NBER                                              a-torgovitsky@northwestern.edu
magne.mogstad@gmail.com

Andres Santos
Department of Economics
University of California - Los Angeles
8283 Bunche Hall, 315 Portola Plaza
Los Angeles, CA, 90095
andres@econ.ucla.edu
       1   Introduction

       In an influential paper, Imbens and Angrist (1994) provided conditions under which an
       instrumental variables (IV) estimand can be interpreted as the average causal effect for
       the subpopulation of compliers, i.e. for those whose treatment status would be affected
       by an exogenous manipulation of the instrument. In some cases, this local average
       treatment effect (LATE) is of intrinsic interest, for example if the instrument itself
       represents an intervention or policy change of interest. On the other hand, in many
       situations, the causal effect for individuals induced to treatment by the instrument at
       hand might not be representative of the causal effect for those who would be induced
       to treatment by a given policy change of interest to the researcher. In these cases, the
       LATE is not the relevant parameter for evaluating the policy change.
           In this paper, we show how to use instrumental variables to draw inference about
       treatment parameters other than the LATE, thereby learning about causal effects for
       individuals other than those affected by the instrument at hand. Policy relevance and
       external validity turn on the ability to do this reliably. Our setting is the canonical pro-
       gram evaluation problem with a binary treatment D ∈ {0, 1} and a scalar, real-valued
       outcome, Y .1 Corresponding to the two treatment arms are unobservable potential
       outcomes, Y0 and Y1 . These represent the realization of Y that would have been ex-
       perienced by an individual had their treatment status been exogenously set to 0 or 1.
       The relationship between observed and potential outcomes is given by

                                        Y = DY1 + (1 − D)Y0 .                                  (1)

           Following Heckman and Vytlacil (1999, 2005), we assume that treatment is deter-
       mined by the weakly separable selection or choice equation

                                         D = 1[ν(Z) − U ≥ 0],                                  (2)

       where ν is an unknown function, U is a continuously distributed random variable, and
       Z is a vector of observable regressors. Suppose that Z is independent of (Y0 , Y1 , U ),
       perhaps conditional on some subvector X of Z. Under this assumption, the IV model
       given by (1)–(2) is equivalent to the IV model used by Imbens and Angrist (1994) and
   1
     For discussions of heterogeneous effects IV models with multiple discrete treatments, we refer to
Angrist and Imbens (1995), Heckman, Urzua, and Vytlacil (2006), Heckman and Vytlacil (2007b), Heckman
and Urzua (2010), Kirkeboen, Leuven, and Mogstad (2016), and Lee and Salanié (2016), among others.
Heterogeneous effects IV models with continuous treatments have been considered by Angrist, Graddy, and
Imbens (2000), Chesher (2003), Florens, Heckman, Meghir, and Vytlacil (2008), Imbens and Newey (2009),
Torgovitsky (2015), Masten (2015), and Masten and Torgovitsky (2016), among others.


                                                    2
many subsequent authors (Vytlacil, 2002). In particular, the instrument monotonicity
condition of Imbens and Angrist (1994) is embedded in the separability of U and Z in
the latent index ν(Z) − U . An important feature of the model is that treatment effects
Y1 − Y0 can vary across individuals with the same observable characteristics, X, in a
way that depends on the unobservable component of treatment choice, U .
   Our goal is to develop a method that uses a random sample of (Y, D, Z) together
with the structure of the model to draw inference about a parameter of interest, β ? ,
that a researcher has decided is relevant for evaluating a hypothetical policy change or
intervention. Our method builds on the work of Heckman and Vytlacil (1999, 2001a,b,c,
2005, 2007a,b). Those authors showed that many different treatment parameters can
be expressed in terms of the marginal treatment effect (MTE) function

                           MTE(u, x) ≡ E [Y1 − Y0 |U = u, X = x] .                                   (3)

The MTE can be interpreted as the average treatment effect indexed as a function
of an individual’s latent propensity to receive treatment, U , and conditional on other
covariates, X. Heckman and Vytlacil (2005) showed that common parameters of inter-
est can be expressed as weighted averages of the MTE function, with weights that are
either known or identified. They showed that the same is also true of the IV estimand.
   These insights suggest that even if the IV estimand is not of direct interest, it
still carries information about the underlying MTE function, and hence about the
parameter of interest, β ? . In particular, since the weights for both the IV estimand
and the parameter of interest are identified, knowledge of the IV estimand generally
places some restrictions on the unknown MTE function, and hence on the range of
values for β ? that are consistent with the data. This can be seen by writing:
                                                Z
                           βIV              ≡       MTE(u) ×            ωIV (u)           du
                           |{z}                     | {z }              | {z }
                   identified IV estimand           unknown       identified IV weights
                                                Z
                          β?                ≡       MTE(u) ×              ω ? (u)              du,   (4)
                         |{z}                       | {z }                | {z }
            unknown target parameter                unknown       identified target weights


where we are assuming for the moment that there are no covariates X, just for sim-
plicity. Equation (4) suggests that we can extract information about the parameter
of interest, β ? , from the IV estimand, βIV , by solving an optimization problem. In
particular, β ? must be smaller than
               Z                                              Z
         max        MTE(u)ω ? (u) du subject to                   MTE(u)ωIV (u) du = βIV ,           (5)
         MTE



                                                      3
where the maximum is taken over a set of potential MTE functions that also incor-
porates any additional a priori assumptions that the researcher chooses to maintain.
Similarly, β ? must be larger than the solution to the analogous minimization problem.
   The optimization problem (5) has only the single constraint involving βIV . Using
the same logic, one can also include similar constraints for other IV estimands that
correspond to different functions of Z. Upon doing so, the bounds on β ? will necessarily
tighten, because each new IV estimand reduces the feasible set in (5). We show that,
more generally, any cross moment between Y and a known function of D and Z can
also be written as a weighted average of the two marginal treatment response (MTR)
functions that constitute an MTE function. We refer to this class of cross moments as
“IV–like” estimands.
   The class of IV–like estimands is general enough to contain the estimands corre-
sponding to any weighted linear IV estimator. This includes, as special cases, the
two stage least squares (TSLS), optimal generalized method of moments, and ordi-
nary least squares (OLS) estimands. Each moment in this class provides a different
weighted average of the same underlying MTR functions, and therefore carries some
distinct information about the possible values of the parameter of interest, β ? . We
show how these IV–like estimands can be chosen systematically so as to provide the
tightest possible bounds on β ? .
   Our method has several applications. First, it can be used to construct nonpara-
metric bounds on the average causal effect of a hypothetical policy change. Second, our
method enables extrapolation of the average effects for compliers to the average effects
for different or larger populations. Third, our method can be used to perform tests of
model specification and of individual behavior, such as testing the null hypotheses of
no selection bias and/or no selection on gains. In all of these applications, our method
provides a researcher the option to impose parametric and/or shape restrictions, if
desired.
   To accommodate these diverse applications, we develop a novel inference framework
that allows for (but does not require) nonparametric and/or shape constrained spec-
ifications for the MTR functions, as well as an infinite number of IV–like estimands.
Our approach is specifically designed to take advantage of the convexity of our setting.
We show that it satisfies two key requirements. First, it provides uniform size control
over a wide class of distributions, a feature which is critically important in partially
identified settings (Imbens and Manski, 2004). Second, implementing our procedure
involves solving optimization problems for which there exist algorithms that provably
converge to the global optimum. The generality of our inference results make them of
independent interest, and we state them in a manner that facilitates their portability.

                                           4
          We apply our method using data from Dupas (2014). This data comes from a
      randomized pricing experiment for a preventative health product, conducted in Kenya.
      The goal of our empirical analysis is to assess how a class of potential subsidy regimes
      can promote the use of the health product, and to compare increases in usage to the
      costs of subsidization. For example, we measure the effect of a policy that offers free
      provision to each household as compared to a policy under which all households can
      purchase the product at a given price. This comparison does not correspond to the
      variation in prices induced by the experiment. As a result, it is not point identified
      under standard instrumental variables assumptions. However, our method can be used
      to estimate bounds on the average causal effect of this comparison. Our results show
      that these bounds can be very informative.
          Our paper contributes to several literatures. A large body of work is concerned
      with using instrumental variables to draw nonparametric inference about treatment
      parameters other than the LATE. Heckman and Vytlacil (2005) observe that if Z is
      continuously distributed and has a sufficiently large impact on treatment choices D,
      so that the propensity score P (D = 1|Z = z) varies over the entire [0, 1] interval, then
      the MTE function is nonparametrically point identified. As a consequence, any target
      parameter β ? is also nonparametrically point identified. In practice, however, instru-
      ments have limited support and are often discrete or even binary. For these situations,
      many common target parameters of interest, such as the average treatment effect, are
      not nonparametrically point identified. Analytic expressions for sharp bounds on the
      average treatment effect have been derived by Manski (1989, 1990, 1994, 1997, 2003),
      Balke and Pearl (1997), Heckman and Vytlacil (2001b) and Kitagawa (2009), among
      others.2
          Analytic expressions for bounds are useful because they provide intuition on the
      source and strength of identification. However, it can be difficult to derive analytic
      bounds for more complicated parameters, such as the policy relevant treatment effects
      (PRTEs) studied by Heckman and Vytlacil (2001a, 2005) and Carneiro, Heckman, and
      Vytlacil (2010, 2011). Our methodology is particularly useful in such settings. In ad-
      dition, our method provides a unified framework for imposing shape restrictions such
      as monotonicity, concavity, monotone treatment selection (Manski, 1997; Manski and
      Pepper, 2000, 2009) and separability between observed and unobserved factors in the
      MTE function (Brinch, Mogstad, and Wiswall, 2015). It can be especially difficult to
    2
      Note that Manski’s analyses did not impose the separable first stage equation (2), see Heckman and
Vytlacil (2001b) and Kitagawa (2009) for further discussion. Also related is work by Shaikh and Vytlacil
(2011), Bhattacharya, Shaikh, and Vytlacil (2012), and Mourifié (2015), who augment (2) with a similar
assumption for the potential outcomes, (Y0 , Y1 ).



                                                   5
       derive analytic bounds for treatment parameters that incorporate these types of as-
       sumptions in flexible combinations. In contrast, our general computational approach
       allows one to flexibly adjust the parameter of interest, as well as the maintained as-
       sumptions, without requiring additional identification analysis.
          In addition, our paper is related to recent work that considers extrapolation in in-
       strumental variables model under additional assumptions. While our method delivers
       bound on the target parameter in general, these bounds nest important point identifi-
       cation results as special cases. For example, our method nests existing approaches that
       extrapolate by assuming no unobserved heterogeneity in the treatment effect (Heckman
       and Robb, 1985; Angrist and Fernandez-Val, 2013), and those that parameterize this
       unobserved heterogeneity (Heckman, Tobias, and Vytlacil, 2003; Brinch et al., 2015).3
       One attractive feature of our method is that the constraints in (5) require an MTE
       function to also yield the usual, nonparametrically point identified LATE. Hence, our
       method allows for extrapolation to other parameters of interest without sacrificing the
       internal validity of the LATE.
          Our paper also relates to a literature on specification tests in settings with instru-
       mental variables. To see this, suppose that (5) is infeasible, so that there does not
       exist an MTE function that can both satisfy the researcher’s assumptions and lead
       to the observed IV estimand. Then the model is misspecified: Either the researcher’s
       assumptions are invalid, Z is not exogenous, the selection equation (2) is rejected by
       the data, or some combination of the three. Balke and Pearl (1997) and Imbens and
       Rubin (1997) noted that (2) has testable implications, while Machado, Shaikh, and
       Vytlacil (2013), Huber and Mellace (2014), Kitagawa (2015), and Mourifié and Wan
       (2016) have developed this observation into formal statistical tests. Our method builds
       on the work of these authors by allowing the researcher to maintain additional assump-
       tions, such as parametric and/or shape restrictions. In addition to testing whether the
       model is misspecified, our method can also be used to test null hypotheses such as no
       selection bias and/or no selection on gains.
          Lastly, our paper contributes to a growing literature on inference for functionals of
       partially identified parameters. Our inference procedure is based on a profile statistic.
       Romano and Shaikh (2008) and Bugni, Canay, and Shi (2015) proposed using pro-
       file statistics for moment inequality models, while Chernozhukov, Newey, and Santos
       (2015) considered models with conditional moment inequalities. Our model contains
       additional structure not present in moment inequality models.4 We utilize this spe-
   3
     For a completely different Bayesian approach to extrapolation in instrumental variables models, see
Chamberlain (2011).
   4
     Loosely speaking, in a moment inequality model, the inequalities are random, whereas in our context


                                                   6
      cial structure to develop distributional approximations that are uniformly valid under
      low level conditions, and which only require the parameter space to be convex. An
      important alternative to profiling test statistics has been recently proposed by Kaido,
      Molinari, and Stoye (2016), who instead construct confidence regions through an ad-
      justed projection algorithm. Their work, in common with many other papers in the
      moment inequalities literature, focuses on finite dimensional parameters and a finite
      number of moment restrictions. Both of these conditions are restrictive for our ap-
      plications of interest. Our analysis is also related to Beresteanu and Molinari (2008),
      Bontemps, Magnac, and Maurin (2012), and Kaido and Santos (2014), who also exploit
      convexity for statistical inference.
          The remainder of the paper is organized as follows. In Section 2, we present the
      model and develop our method for bounding a target parameter of interest while po-
      tentially maintaining additional shape constraints. In Section 3, we discuss key appli-
      cations of our method, which we illustrate in Section 4 through a numerical example.
      We develop our statistical inference procedure in Section 5. In Section 6, we apply our
      method to study the effects of price subsidies on the adoption of preventative health
      products. We provide some concluding remarks in Section 7. Proofs for all results
      presented in the main text are contained in Appendices A and C.

      2     Identification

      Throughout this section, we assume that the researcher knows the joint distribution of
      the observed data (Y, D, Z). We address issues of statistical inference in Section 5.

      2.1     Model
      Our analysis uses the IV model consisting of (1)–(2), which is also often referred to as
      the two-sector generalized Roy model. The observable variables in the model are the
      outcome Y ∈ R, the binary treatment D ∈ {0, 1}, and a vector of observables Z ∈ Rdz .
      We decompose Z into Z = (X, Z0 ), where Z0 ∈ Rdz0 are exogenous instruments and
      X ∈ Rdx are control variables. The unobservables are the potential outcomes (Y0 , Y1 ),
      and the variable U in the selection equation, which represents unobservable factors
      that affect treatment choice.
          We maintain the following assumptions throughout the paper.

             Assumptions I
       I.1 U ⊥
             ⊥ Z0 |X, where ⊥
                            ⊥ denotes (conditional) statistical independence.

the inequalities are deterministic, because they arise from the specification of the parameter space.


                                                      7
 I.2 E[Yd |Z, U ] = E[Yd |X, U ] and E[Yd2 ] < ∞ for d ∈ {0, 1}.
 I.3 U is continuously distributed, conditional on X.

Assumptions I.1 and I.2 require Z0 to be exogenous with respect to both the selection
and outcome processes. Vytlacil (2002) showed that, given I.1, the assumption that
the index of the selection equation is additively separable as in (2) is equivalent to
the assumption that Z0 affects D monotonically in the sense introduced by Imbens
and Angrist (1994). Hence, I.1 combined with (2) imposes substantive restrictions on
choice behavior. Assumption I.2 imposes an exclusion restriction that the conditional
means of Y0 and Y1 depend on Z = (Z0 , X) only through the covariates X.
   Assumption I.3 is a weak regularity condition that enables us to impose a standard
normalization. As is well known, equation (2) may be rewritten as

                 D = 1 FU |X (U |X) ≤ FU |X (ν(Z)|X) ≡ 1[U
                                                   
                                                         e ≤ νe(Z)],                 (6)

where we are using the notation FU |X (u|x) ≡ P (U ≤ u|X = x) and we have defined
e ≡ FU |X (U |X) and νe(Z) ≡ FU |X (ν(Z)|X). Under Assumptions I.1 and I.3, U
U                                                                            e is
uniformly distributed on [0, 1], conditional on Z = (Z0 , X). Working with this normal-
ized model simplifies the analysis and does not affect its empirical content. Hence, we
drop the tilde and maintain throughout the paper the normalization that U itself is
distributed uniformly over [0, 1] conditional on Z. A consequence of this normalization
is that

                    p(z) ≡ P (D = 1|Z = z) = FU |Z (ν(z)|z) = ν(z),                  (7)

where p(z) is the propensity score.
   It is important to observe what is not being assumed under Assumptions I. First, we
do not impose any conditions on the support of Z: Both the control (X) and exogenous
(Z0 ) components of Z may be either continuous, discrete and ordered, categorical, or
binary. Second, the IV model as specified here allows for rich forms of observed and
unobserved heterogeneity. In particular, it allows Y1 − Y0 to vary not only across
individuals with different values of X, but also among individuals with the same X.
The treatment D may be statistically dependent with Y0 (indicating selection bias),
or Y1 − Y0 (indicating selection on the gain), or both, even conditional on X. Third,
the model does not specify why individuals make the treatment choice that they do,
in contrast to a stylized Roy model in which D = 1[Y1 > Y0 ]. However, the model also
does not preclude the possibility that individuals choose treatment with full or partial
knowledge of the potential outcomes (Y0 , Y1 ). Any such knowledge will be reflected

                                            8
through dependence between the potential outcomes, (Y0 , Y1 ), and the unobserved
component treatment choice, U . Assumption I does not restrict this dependence.

2.2       What We Want to Know: The Target Parameter
As observed by Heckman and Vytlacil (1999, 2005), a wide range of treatment param-
eters can be written as weighted averages of the underlying MTE function. We use a
slight generalization of their observation. Instead of working with the MTE function
(3) directly, we consider treatment parameters that can be expressed as functions of
the two marginal treatment response (MTR) functions, defined as

   m0 (u, x) ≡ E [Y0 | U = u, X = x]             and          m1 (u, x) ≡ E [Y1 | U = u, X = x] .                   (8)

Of course, each pair m ≡ (m0 , m1 ) of MTR functions generates an associated MTE
function MTE(u, x) ≡ m1 (u, x)−m0 (u, x). One benefit of working with MTR functions
instead of MTE functions is that it allows us to consider parameters that weight m0
and m1 asymmetrically. Another benefit is that it allows the researcher to impose
assumptions on m0 and m1 separately.
   We assume that the researcher is interested in a target parameter β ? that can be
written for any candidate pair of MTR functions m ≡ (m0 , m1 ) as
             Z    1                                             Z      1                                 
      ?
      β ≡E             m0 (u, X)ω0? (u, Z) dµ? (u)       +E                   m1 (u, X)ω1? (u, Z) dµ? (u)       ,   (9)
               0                                                      0

where ω0? and ω1? are identified weighting functions, and µ? is an integrating measure
that is chosen by the researcher and usually taken to be the Lebesgue measure on [0, 1].
For example, to set β ? to be the average treatment effect (ATE), observe that
                                                             Z   1                  Z            1              
E[Y1 − Y0 ] = E[m1 (U, X) − m0 (U, X)] = E                            m1 (u, X)du − E                   m0 (u, X)du ,
                                                              0                                 0

take ω1? (u, z) = 1, ω0? (u, z) = −1, and let µ? be the Lebesgue measure on [0, 1]. Simi-
larly, to set β ? to be the ATE conditional on X lying in some known set X ? , take

                                                                       1[x ∈ X ? ]
                               ω1? (u, z) ≡ ω1? (u, x, z0 ) =                      ,
                                                                      P (X ∈ X ? )

ω0? (u, z) = −ω1? (u, z), and let µ? be as before. The resulting target parameter is then
the population average effect of assigning treatment randomly to every individual with
covariates x ∈ X ? , assuming full compliance.



                                                         9
                                          Table 1: Weights for a Variety of Target Parameters

                                                                                                                Weights
     Target Parameter                              Expression                  ω0 (u, z) ≡ ω0 (u, x, z0 )              ω1 (u, z) ≡ ω1 (u, x, z0 )       Measure µ?



     Average Untreated Outcome                        E[Y0 ]                                 1                                        0                  Leb.[0, 1]


     Average Treated Outcome                          E[Y1 ]                                 0                                        1                  Leb.[0, 1]


     Average Treatment Effect                                                                −1                                       1                  Leb.[0, 1]
                                                   E[Y1 − Y0 ]
     (ATE)

     Average Treatment Effect                                                            ?                                       1[x ∈ X ? ]
                                               E[Y1 − Y0 |X ∈ X ? ]                    −ω1 (u, z)                                                        Leb.[0, 1]
     (ATE) given X ∈ X ?                                                                                                        P (X ∈ X ? )


     Average Treatment on the                                                            ?                                      1[u ≤ p(z)]
                                                E[Y1 − Y0 |D = 1]                      −ω1 (u, z)                                                        Leb.[0, 1]
     Treated (ATT)                                                                                                               P (D = 1)


     Average Treatment on the                                                            ?                                      1[u > p(z)]
                                                E[Y1 − Y0 |D = 0]                      −ω1 (u, z)                                                        Leb.[0, 1]
     Untreated (ATU)                                                                                                             P (D = 0)


     Marginal Treatment Effect at u             E[Y1 − Y0 |U = u]                            -1                                       1                  Dirac(u)
10




     Local Average Treatment Effect                                                      ?                                      1[u ∈ [u, u]]
                                              E[Y1 − Y0 |U ∈ [u, u]]                   −ω1 (u, z)                                                        Leb.[0, 1]
     for U ∈ [u, u] (LATE(u, u))                                                                                                  (u − u)


     Policy Relevant Treatment                                                                                        1[u ≤ p? (z ? )] − 1[u ≤ p(z)]
                                                  E[Y ? ] − E[Y ]                        ?
     Effect (PRTE) for new policy                                                      −ω1 (u, z)                                                        Leb.[0, 1]
                                                  E[D ? ]   − E[D]                                                         E[p? (Z ? )] − E[p(Z)]
     (p? , Z ? )

     Additive PRTE with magnitude            PRTE with Z ? = Z and                       ?                           1[u ≤ p(z) + α] − 1[u ≤ p(z)]
                                                                                       −ω1 (u, z)                                                        Leb.[0, 1]
     α                                         p? (z) = p(z) + α                                                                      α


     Proportional PRTE with                  PRTE with Z ? = Z and                       ?                          1[u ≤ (1 + α)p(z)] − 1[u ≤ p(z)]
                                                                                       −ω1 (u, z)                                                        Leb.[0, 1]
     magnitude α                              p? (z) = (1 + α)p(z)                                                               αE[p(Z)]


     PRTE for an additive α shift of      PRTE with Z ? = Z + αej and                    ?                          1[u ≤ p(z + αej )] − 1[u ≤ p(z)]
                                                                                       −ω1 (u, z)                                                        Leb.[0, 1]
     the j th component of Z                     p? (z) = p(z)                                                            E[p(Z + αej )] − E[p(Z)]

                            ?    ?
     Sum of two quantities βA , βB                    ?    ?                     ?
                                                                                ωA,0           ?
                                                                                     (u, z) + ωB,0 (u, z)                  ?
                                                                                                                          ωA,1           ?
                                                                                                                               (u, z) + ωB,1 (u, z)     Common µ?
                                                     βA + βB
     with common measure µ?

                                                                               1[u ≤ p(z)]        1[u > p(z)]
     Average Selection Bias                E[Y0 |D = 1] − E[Y0 |D = 0]                       −                                        0                  Leb.[0, 1]
                                                                               P (D = 1)          P (D = 0)
                                                                                         ?                            1[u ≤ p(z)]         1[u > p(z)]
     Average Selection on the Gain     E[Y1 − Y0 |D = 1] − E[Y1 − Y0 |D = 0]           −ω1 (u, z)                                     −                  Leb.[0, 1]
                                                                                                                          P (D = 1)       P (D = 0)
   In Table 1, we provide formulas for the weights ω0? and ω1? that correspond to a
variety of different treatment parameters. Any of these can be taken to be the target
parameter β ? . Examples include (i) the average treatment effect for the treated (ATT),
i.e. the average impact of treatment for individuals who actually take the treatment;
(ii) the average treatment effect for the untreated (ATU), i.e. the average impact of
treatment for individuals who do not take treatment; (iii) LATE[u, u], i.e. the average
treatment effect for individuals who would take the treatment if their realization of
the instrument yielded p(z) = u, but not if it yielded p(z) = u; and (iv) the policy
relevant treatment effect (PRTE), i.e. the average impact on Y (either gross or per
net individual affected) due to a change from the baseline policy to some alternative
policy.
   For most of the parameters in Table 1, the integrating measure µ? is taken to
be Lebesgue measure on [0, 1]. However, researchers are sometimes interested in the
MTE function itself. For example, Carneiro et al. (2011) and Maestas, Mullen, and
Strand (2013) both report estimates of the MTE function for various values of u. Our
specification of β ? accommodates this by replacing µ? with the Dirac measure (i.e.,
a point mass) at some specified point u and taking ω0? (u, z) = −ω1? (u, z) = −1. The
resulting target parameter is the MTE function averaged over X, i.e. E[m(u, X)].

2.3       What We Know: IV–Like Estimands
A key point for our method is that a set of identified quantities can also be written
in a form similar to (9). Consider, for example, the IV estimand that results from
using Z as an instrument for D in a linear instrumental variables regression that
includes a constant term, but which does not include any other covariates X. Assuming
Cov(D, Z) 6= 0, this estimand is given by

                                           Cov(Y, Z)
                                   βIV ≡             .                             (10)
                                           Cov(D, Z)

For example, if Z ∈ {0, 1} is binary, then βIV reduces to the standard Wald estimand

                                 E [Y | Z = 1] − E [Y | Z = 0]
                         βIV =                                 .                   (11)
                                 E [D | Z = 1] − E [D | Z = 0]

Heckman and Vytlacil (2005) show that βIV can also be written in the form (9) as a
weighted average of the MTE function. This observation forms the foundation for our
intuition that useful information about β ? can be extracted from knowledge of βIV .
The next proposition shows that, more generally, any cross moment of Y with a known
or identified function of (D, Z) ≡ (D, X, Z0 ) can also be expressed as the weighted sum

                                            11
       of the two MTR functions, m0 and m1 . We refer to such cross moments as IV–like
       estimands.

       Proposition 1. Suppose that s : {0, 1} × Rdz 7→ R is a known or identified function of
       (D, Z) that is measurable and has a finite second moment. We refer to such a function
       s as an IV–like specification and to βs ≡ E[s(D, Z)Y ] as an IV–like estimand. If
       (Y, D) are generated according to (1) and (2) under Assumptions I, then
                          Z   1                             Z         1                         
                 βs = E            m0 (u, X)ω0s (u, Z) du + E                m1 (u, X)ω1s (u, Z) du ,   (12)
                           0                                         0

             where ω0s (u, Z) ≡ s(0, Z)1[u > p(Z)]
              and ω1s (u, Z) ≡ s(1, Z)1[u ≤ p(Z)].

          The weights in Proposition 1 can be shown to reduce to the weights for βIV derived
       by Heckman and Vytlacil (2005) by taking

                                                                 z0 − E[Z0 ]
                                      s(d, z) ≡ s(d, x, z0 ) =               ,                          (13)
                                                                 Cov(D, Z0 )

       which is an identified function of D (trivially) and Z. As we elaborate further in
       Appendix B, Proposition 1 applies more broadly to include any well-defined weighted
       linear IV estimand that uses some function of D and Z as included and excluded
       instruments for a set of endogenous variables also constructed from D and Z.5 For
       example, the ordinary least squares (OLS) estimand corresponds to taking s to be

                                                          d − E[D]
                                              s(d, z) =            .
                                                           Var(D)

       More generally, any subvector of the TSLS or optimal GMM estimands can also be
       written as an IV–like estimand. Table 2 contains expressions for some notable IV–like
       estimands.

       2.4    From What We Know to What We Want to Know
       We now show how to extract information about the target parameter β ? from the gen-
       eral class of IV-like estimands introduced in Section 2.3. Let S denote some collection
       of IV–like specifications (i.e. functions s : {0, 1} × Rdz 7→ R) chosen by the researcher,
       that each satisfy the conditions set out in Proposition 1. Corresponding to each s ∈ S
       is an IV–like estimand βs ≡ E[s(D, Z)Y ]. We assume that the researcher has restricted
   5
     The phrases “included” and “excluded” instrument are meant in the sense typically introduced in
textbook treatments of the linear IV model without heterogeneity.

                                                        12
the pair of MTR functions m ≡ (m0 , m1 ) to lie in some admissible set M, which incor-
porates any a priori assumptions that the researcher wishes to maintain about m, such
as parametric or shape restrictions. Our goal is to characterize bounds on the values
of the target parameter β ? that could have been generated by MTR functions m ∈ M
that also could have delivered the collection of identified IV–estimands through (12).
   To this end, we denote the weighting expression in Proposition 1 as a linear map
Γs : M 7→ R, defined for any IV–like specification s ∈ S as
                    Z       1                             Z       1                         
       Γs (m) ≡ E                m0 (u, X)ω0s (u, Z) du + E              m1 (u, X)ω1s (u, Z) du ,         (14)
                         0                                     0

where we recall that ω0s (u, z) ≡ s(0, z)1[u > p(z)] and ω1s (u, z) ≡ s(1, z)1[u ≤ p(z)].
By Proposition 1, if (Y, D) are generated according to (1) and (2) under Assumptions
I, then the MTR functions m ≡ (m0 , m1 ) must satisfy Γs (m) = βs for every IV–like
specification s ∈ S. As a result, m must lie in the set

                                 MS ≡ {m ∈ M : Γs (m) = βs for all s ∈ S} .

   The target parameter, β ? , can also be expressed as an identified linear map of the
MTR functions. From (9), we define this map as Γ? : M 7→ R, with
              Z     1                                       Z     1                              
   ?
  Γ (m) ≡ E              m0 (u, X)ω0? (u, Z)dµ? (u)      +E              m1 (u, X)ω1? (u, Z)dµ? (u)     . (15)
                 0                                               0

It follows that if (Y, D) is generated according to (1) and (2) under Assumptions I,
then the target parameter must belong to the identified set

                             BS? ≡ {b ∈ R : b = Γ? (m) for some m ∈ MS }.

Intuitively, BS? is the set of values for the target parameter that could have been
generated by MTR functions that are consistent with the IV–like estimands. The next
result shows that if M is convex, then BS? is an interval that can be characterized by
solving two convex optimization problems.

Proposition 2. Suppose that M is convex. Then either MS is empty, and hence BS?
                                                                                            ?
is empty, or else the closure of BS? (in R) is equal to the interval [β ? , β ], where

                                                               ?
                         β ? ≡ inf Γ? (m)           and       β ≡ sup Γ? (m).                             (16)
                                   m∈MS                                   m∈MS




                                                    13
                         Table 2: Notable IV–Like Estimands


Estimand                                  βs                       s(D, Z)                    Notes



                                     Cov(Y, Z0 )                  Z0 − E[Z0 ]
IV slope                                                                                    Z0 scalar
                                     Cov(D, Z0 )                  Cov(D, Z0 )
                                                                                         e ≡ [1, D, X 0 ]0
                                                                                         X
IV (jth component)               e0j E[Z
                                       eXe 0 ]−1 E[ZY
                                                   e ]          e0j E[Z
                                                                      eXe 0 ]−1 Z
                                                                                e        e ≡ [1, Z0 , X 0 ]0
                                                                                         Z
                                                                                           Z0 scalar

                                     Cov(Y, D)                     D − E[D]
OLS slope                                                                                        —
                                      Var(D)                        Var(D)


OLS (jth component)             e0j E[X
                                      eXe 0 ]−1 E[XY
                                                  e ]           e0j E[X
                                                                      eXe 0 ]−1 X
                                                                                e        e ≡ [1, D, X 0 ]0
                                                                                         X

                                                                                      Π ≡ E[XeZe0 ]E[Z
                                                                                                     eZe0 ]−1
                                         −1        
TSLS (jth component)     e0j    ΠE[Z
                                   eXe 0]      ΠE[ZY
                                                  e ]         e0j (ΠE[Z
                                                                      eXe 0 ])−1 ΠZ
                                                                                  e   Included variables X e
                                                                                         Instruments Z e



2.5        Sharpness and Point Identification
The set MS consists of all MTR functions in M that are consistent with the IV–like
estimands chosen by the researcher. However, MS does not necessarily exhaust all of
the information available in the data. In particular, MS may contain MTR functions
that would be ruled out if S were expanded to include additional IV–like specifications.
If this is the case, then incorporating these further specifications could shrink BS? .
     We examine this issue by considering the conditional means of Y that would be
generated through (1) and (2) under Assumptions I by a given MTR pair m = (m0 , m1 ).
Whenever 0 < p(Z) < 1, these conditional means can be written as
                                                                Z 1
                                                         1
              E[Y |D = 0, Z] = E[Y0 |U > p(Z), Z] =                   m0 (u, X) du,                      (17)
                                                    (1 − p(Z)) p(Z)
                                                          Z p(Z)
                                                      1
     and      E[Y |D = 1, Z] = E[Y1 |U ≤ p(Z), Z] =              m1 (u, X) du.                           (18)
                                                    p(Z) 0

MTR pairs that (almost surely) satisfy (17)–(18) are compatible with the observed
conditional means of Y . Our next result shows that any such MTR pair will be in MS
for any choice of S. Moreover, we show that if S is chosen correctly, then MS will
contain only MTR pairs that are compatible with the observed conditional means of
Y.

                                                         14
                                                    R1
Proposition 3. Suppose that every m ∈ M satisfies E[ 0 md (u, X)2 du] < ∞ for
d ∈ {0, 1}. If S contains functions that satisfy the conditions of Proposition 1, then

              {m ∈ M : m satisfies (17) and (18) almost surely} ⊆ MS .               (19)

Moreover, suppose that S ≡ {s(d, z) = 1[d = d0 ]f (z) for (d0 , f ) ∈ {0, 1} × F}, where
F is a collection of functions. If the linear span of F is norm dense in L2 (Z) ≡ {f :
Rdz 7→ R s.t. E[f (Z)2 ] < ∞}, then

              {m ∈ M : m satisfies (17) and (18) almost surely} = MS .               (20)

   Proposition 3 shows that if S is a sufficiently rich class of functions, then MS
is sharp in the sense of being the smallest subset of M that is compatible with the
observed conditional means of Y . It follows that BS? is also the smallest set of values
for the target parameter that are consistent with both the conditional means of Y and
the assumptions of the model. For example, if D ∈ {0, 1} and Z ∈ {0, 1}, then (20)
holds if we take F = {1[z = 0], 1[z = 1]}, so that

        S = {1[d = 0, z = 0], 1[d = 0, z = 1], 1[d = 1, z = 0], 1[d = 1, z = 1]} .

The information contained in the corresponding IV–like estimands is the same as that
contained in the coefficients of a saturated regression of Y on D and Z. More generally,
if Z is continuous, then (20) can be satisfied by taking F to be certain parametric
families of functions of Z. For example, if Z ∈ Rdz , then one such family is the set of
half spaces, F = {1[z ≤ z 0 ] : z 0 ∈ Rdz }. Other examples can be found in e.g. Bierens
(1990) and Stinchcombe and White (1998).
   While we view partial identification as the standard case, we emphasize that our
analysis does not preclude point identification. Letting |S| denote the cardinality of S,
notice that the restrictions

                                Γs (m) = βs for all s ∈ S                            (21)

constitute a linear system of |S| equations in terms of m. Thus, if M is finite dimen-
sional, then point identification of the MTR functions is determined by the rank of this
linear system. Note that if the MTR functions are point identified, then any target
parameter β ? is also point identified.
   These observations about point identification are implicit in the work of Brinch et al.
(2015). Those authors show that if M is restricted to be a set of polynomials, then point


                                           15
       identification of the MTR functions can be established by considering regressions of Y
       on p(Z) and D. Their results allow for Z to be discrete, but require the specification of
       M to be no richer than the support of p(Z). For example, if Z is binary, their results
       require M to only contain MTR pairs that are linear in u.6 In contrast, our results
       allow the researcher to specify M independently of the data.
          In some situations, point identification of the target parameter, β ? , can also be
       established when both |S| and M are infinite dimensional. Indeed, relationships similar
       to (17) and (18) have been used previously to establish point identification of the
       MTE function. For example, Heckman and Vytlacil (1999, 2001c, 2005) and Carneiro
       et al. (2010, 2011) show that if Z0 is continuously distributed, then the MTE is point
       identified over the support of the propensity score. As a consequence, any target
       parameter β ? whose weights have support contained within the interior of the support
       of the propensity score will also be point identified. Proposition 3 implies that the same
       is true in our framework if S is chosen to be a sufficiently rich collection of functions.

       2.6   Nonparametric Shape Restrictions
       Our method allows researchers to easily incorporate nonparametric shape restrictions
       into their specification of the MTR functions. These restrictions can be imposed either
       on the MTR functions m = (m0 , m1 ) or directly on the MTE function m1 − m0 .
       For example, to impose the monotone treatment response assumption considered by
       Manski (1997), i.e. that Y1 ≥ Y0 with probability 1, the set M should be specified
       to only contain MTR pairs for which m1 − m0 is non-negative. Similarly, one could
       assume that m(·, x) is weakly decreasing for every x. This restriction would reflect the
       assumption that those more likely to select into treatment (those with small realizations
       of U ) are also more likely to have larger gains from treatment. This is similar to the
       monotone treatment selection assumption of Manski and Pepper (2000). Maintaining
       combinations of assumptions simultaneously (e.g. both monotone treatment response
       and monotone treatment selection) is simply a matter of imposing both restrictions on
       M at the same time.
          Another type of nonparametric shape restriction that can be used to tighten the
       bounds is separability between the observed (X) and unobserved (U ) components.
       Although restrictive, separability of this sort is standard (often implicit) in applied
       work using instrumental variables. In our framework, separability between X and U
       can be imposed by restricting M to only contain MTR pairs (m0 , m1 ) that can be
   6
     Recently, Kowalski (2016) has applied the linear case, studied in depth by Brinch et al. (2015), to
analyze the Oregon Health Insurance Experiment.



                                                  16
decomposed as

                         md (u, x) = mU        X
                                      d (u) + md (x)            for d = 0, 1,                       (22)

where mU      X
       d and md are some other functions that can themselves satisfy some shape
restrictions. This type of separability implies that the slopes of the MTR functions
with respect to u do not vary with x. Alternatively, it is straightforward to interact u
and x fully or partially if complete separability is viewed as too strong of a restriction.
Specifications like (22) can also be used to mitigate the curse of dimensionality, for
example by specifying mX
                       d (x) to be a linear function of x.


2.7   Computing the Bounds
Proposition 2 provides convenient numerical methods for computing bounds on the
target parameter. In this section, we focus on a particularly tractable computational
approach in which we replace the possibly infinite dimensional admissible set of func-
tions M by a finite dimensional subset Mfd ⊆ M. The upper bound for the target
parameter with this finite dimensional subset is given by

                     ?
                  β fd ≡ sup {Γ? (m) s.t. Γs (m) = βs for all s ∈ S},                               (23)
                           m∈Mfd


while β ?fd is defined as the analogous infimum.
   Suppose that we specify Mfd as the finite linear basis

                                                       Kd
                 (                                                                              )
                                                       X
         Mfd ≡    (m0 , m1 ) ∈ M : md (u, x) =               θdk bdk (u, x) for d ∈ {0, 1} ,        (24)
                                                       k=1


where {bdk }K                                       0    0 0
            k=1 are known basis functions and θ ≡ (θ0 , θ1 ) parameterizes functions in
             d


Mfd with θd ≡ (θd1 , . . . , θdKd )0 . The admissible set M generates an admissible set

                                                   K0                K1
                 (                                                                   !     )
                                                   X                 X
                                   K0    K1
            Θ≡       (θ0 , θ1 ) ∈ R     ×R    :          θ0k b0k ,         θk1 b1k       ∈M ,
                                                   k=1               k=1

for the finite dimensional parameter θ. Using the linearity of the mappings Γ? and Γs




                                                  17
        defined in (14) and (15), we write (23) as

                                         K0                           K1
                     ?                   X                            X
                   β fd   ≡     sup             θ0k Γ?0 (b0k )    +        θ1k Γ?1 (b1k )
                              (θ0 ,θ1 )∈Θ k=1                         k=1
                                         K0
                                         X                             K1
                                                                       X
                                  s.t.          θ0k Γ0s (b0k ) +            θ1k Γ1s (b1k ) = βs for all s ∈ S,       (25)
                                         k=1                          k=1

        where we have decomposed Γ? (m) ≡ Γ?0 (m0 ) + Γ?1 (m1 ) with
                                                       Z     1                             
                                    Γ?d (md ) ≡ E                 md (u, X)ωd? (u, Z)dµ? (u) ,
                                                          0

        and similarly for the map Γs . If Θ is a polyhedral set, then (25) is a linear program.
        Linear programs are used routinely in empirical work involving quantile regressions, e.g.
        Abadie, Angrist, and Imbens (2002), in part because they can be solved quickly and
        reliably. Whether a given shape restriction on M translates into Θ being polyhedral
        depends on the basis functions. In Appendix F, we discuss the Bernstein polynomial
        basis, which is particularly attractive in this regard.
           In some situations, M can be replaced by a finite dimensional set Mfd without
                                                                                                        ?        ?
        affecting the bounds on the target parameter, i.e. while ensuring β fd = β . This
        can be interpreted as an exact computational approach for determining nonparametric
        bounds on the target parameter. For example, suppose that Z has discrete support
        and that the weight functions ωd? (u, z) for the target parameter are piecewise constant
        in u. Then define a partition {Aj }Jj=1 of [0, 1] such that ωd? (u, z) and 1[u ≤ p(z)] are
        constant (as functions of u) on each Aj .7 Let {x1 , . . . xL } denote the support of X and
        then use
                          bjl (u, x) ≡ 1[u ∈ Aj , x = xl ] for 1 ≤ j ≤ J and 1 ≤ l ≤ L                               (26)

        as the basis functions employed in the construction of Mfd in (24), with Kd = JL.
        The basis formed by the functions defined in (26) is known as a constant spline, or a
        Haar basis.
           The element of the Haar basis that provides the best mean squared error approxi-
        mation to a given function md (u, x) can be shown to be

                                            J X
                                            X L
                          Πmd (u, x) ≡                  E[md (U, X)|U ∈ Aj , X = xl ]bjl (u, x).                     (27)
                                            j=1 l=1

    7
     For example, take A1 ≡ [u0 , u1 ] and Aj ≡ (uj−1 , uj ] for 2 ≤ j ≤ J, where {uj }Jj=0 are the ordered
unique elements of the union of {0, 1}, supp{p(Z)}, and the discontinuity points of {ωd? (·, z) : d ∈ {0, 1}, z ∈
supp{Z}}.


                                                                      18
This corresponds to taking θd(j,l) = E[md (U, X)|U ∈ Aj , X = xl ] for an element of
(24), with the slight abuse of notation that k = (j, l). The next proposition uses (27)
to show that the Haar basis, despite being a finite basis, reproduces nonparametric
bounds on the target parameter.

Proposition 4. Suppose that Z has discrete support and that ωd? (u, z) are piecewise
constant in u. Let {Aj }Jj=1 be a partition of [0, 1] such that ωd? (u, z) and 1[u ≤ p(z)]
are constant on u ∈ Aj for any z. Suppose that Mfd ⊆ M and that (Πm0 , Πm1 ) ∈ M
                                          ?        ?
for every (m0 , m1 ) = m ∈ M. Then β fd = β and β ?fd = β ? .

        Proposition 4 shows that one can solve the infinite dimensional problems defining
    ?        ?
β and β exactly by solving (23) with a Haar basis for Mfd . Besides requiring Z to
have discrete support, the result also requires (Πm0 , Πm1 ) ∈ M for every m ∈ M,
as well as Mfd ⊆ M. Intuitively, this requires an MTR pair formed from the Haar
basis to itself be admissible, as well as to maintain the restrictions encoded in M. For
certain restrictions, such as boundedness or monotonicity, this is immediately implied
by (27). We demonstrate the use of the Haar basis in both the numerical illustration
in Section 4 and the empirical application in Section 6.

3       Applications of the Method

3.1       Partial Identification of Policy Relevant Treatment Effects
The policy relevant treatment effect (PRTE) is the mean effect of changing from a
baseline policy to an alternative policy that provides different incentives to participate
in treatment (Heckman and Vytlacil, 1999, 2005). In many situations, this policy
comparison does not directly correspond to the variation in treatment induced by the
instrument, so the PRTE is not point identified. In such cases, researchers can use our
method to construct bounds on the PRTE.
        To see how one can use our method to draw inference about PRTEs, consider a
policy a that operate by changing factors that affect an agent’s treatment decision.
We follow Heckman and Vytlacil (1999, 2005) in assuming that a has no direct effect
on the potential outcomes (Y0 , Y1 ), and in particular that it does not affect the set M
of admissible MTR functions. This assumption is similar to the exclusion restriction.
The policy can then be summarized by a propensity score and instrument pair (pa , Z a ).
Treatment choice under policy a is given by

                                    Da ≡ 1[U ≤ pa (Z a )],



                                              19
       where U is the same unobservable term as in the selection equation for the status quo
       policy, D. The outcome of Y that would be observed under the new policy is therefore

                                       Y a = Da Y1 + (1 − Da )Y0 .

       The PRTE of policy a1 relative to another policy a0 is defined as

                                                 E[Y a1 ] − E[Y a0 ]
                                       PRTE ≡                                                  (28)
                                                 E[Da1 ] − E[Da0 ]

       where we assume that E[Da1 ] 6= E[Da0 ].8
          In Table 1, we provide weights ω0? and ω1? that can be used to express the PRTE as
       a target parameter β ? with the form given in (9) for different policies a0 and a1 . The
       choice of weights depends on the policies being compared. The way in which different
       policy comparisons translate into different weights is illustrated in Table 1 through the
       three specific examples considered by Carneiro et al. (2011). Each of these comparisons
       is between a hypothetical policy a1 and the status quo policy a0 , the latter of which
       is characterized by the pair (pa0 , Z a0 ) = (p, Z) observed in the data. The comparisons
       are: (i) an additive α change in the propensity score, i.e. pa1 = p+α; (ii) a proportional
       (1 + α) change in the propensity score, i.e. pa1 = (1 + α)p; and (iii) an additive α shift
       in the distribution the jth component of Z, i.e. Z a1 = Z + αej , where ej is the jth
       unit vector. The first and second of these represent policies that increase (or decrease)
       participation in the treatment by a given amount α or a proportional amount (1 + α).
       The third policy represents the effect of shifting the distribution of a variable that
       impacts treatment choice. In all of these definitions, α is a quantity that could either
       be hypothesized by the researcher, estimated from some auxiliary choice model, or
       predicted from the estimated p(Z) under parametric assumptions.
          After choosing the weights that correspond to the policy comparison of interest, the
       procedure in Section 2.7 can be used to estimate bounds for the PRTE. These bounds
       can be fully nonparametric, but they can also incorporate a priori parametric or shape
       restrictions if desired. Our statistical inference results in Section 5 allow us to build
       confidence intervals for the target parameter. In Section 6, we apply these insights to
       evaluate the PRTEs of alternative subsidy regimes for malaria bed nets.
   8
     If this assumption is concerning, one can also define the PRTE as E[Y a1 ] − E[Y a0 ], see Heckman and
Vytlacil (2001a) or pp. 380–381 of Carneiro et al. (2010). Our approach directly applies to this definition
as well.




                                                    20
3.2   Extrapolation of Local Average Treatment Effects
Imbens and Angrist (1994) showed that the LATE is point identified under the assump-
tions considered in this paper. As argued by Imbens (2010, pp. 414–415), it is desirable
to report both the LATE, with its high degree of internal validity, but possibly limited
external validity, and extrapolations of the LATE to larger or different populations.
We now show how to use our method to perform this type of extrapolation, thereby
allowing researchers to assess the sensitivity of a given LATE estimate to an expansion
(or contraction) of the complier subpopulation.
   To extrapolate the LATE, it is useful to connect the LATE parameter to the PRTE.
To see the relationship, suppose that there are no covariates X, i.e. Z = Z0 , and
suppose that Z0 is binary. Consider the PRTE that results from comparing a policy
a1 under which every agent receives Z = 1 against a policy a0 under which every agent
receives Z = 0. Choices under these policies are

                    Da0 ≡ 1[U ≤ p(0)]     and   Da1 ≡ 1[U ≤ p(1)],

where p(1) > p(0) are the propensity score values in the observed data. The PRTE for
this policy comparison is

  E[Y a1 ] − E[Y a0 ]   E [(Da1 − Da0 )(Y1 − Y0 )]
                      =                            = E [Y1 − Y0 | U ∈ (p(0), p(1)]] , (29)
  E[Da1 ] − E[Da0 ]            p(1) − p(0)

where we used Da1 − Da0 = 1[U ∈ (p(0), p(1)]]. The right-hand side of (29) is precisely
the LATE as defined by Imbens and Angrist (1994).
   Extrapolation of the LATE amounts to changing p(0), p(1), or both. For example,
suppose that the researcher wants to examine the sensitivity of the LATE to an ex-
pansion of the complier subpopulation that includes individuals with lower willingness
to pay for treatment. This sensitivity check corresponds to shifting p(1) to p(1) + α
for α > 0. Arguing as in (29), the extrapolated LATE can be shown to be

                    PRTE(α) = E [Y1 − Y0 | U ∈ (p(0), p(1) + α]] .                   (30)

This PRTE is still a LATE as defined by Heckman and Vytlacil (2005), but one that
is not point identified by the IV estimand unless α = 0.
   While PRTE(α) is not point identified, we show in Table 1 that it can be expressed
as a target parameter β ? in the form (9). As a result, we can use our approach to




                                           21
bound PRTE(α). To gain some intuition into such approach, we write PRTE(α) as


  PRTE(α)
                                                 Z p(1)+α
          p(1) − p(0)                    α
    =                     LATE +                              {m1 (u) − m0 (u)} du,
        α + p(1) − p(0)            α + p(1) − p(0)    p(1)


where LATE is the usual point identified LATE in (29). This decomposition shows
that the conclusions that can be drawn about PRTE(α) depend on what can be said
about m1 (u) − m0 (u) for u ∈ [p(1), p(1) + α]. Therefore, restrictions on the set M
of admissible MTR pairs translate directly into restrictions on the possible values of
PRTE(α). For example, if we possess an a priori bound on the support of Y , e.g. Y is
binary, then even nonparametric bounds can be informative about PRTE(α).
   Our method allows a researcher to formally and transparently balance their de-
sire for robust assumptions against their desire for broader extrapolation. Stronger
assumptions are reflected through a more restrictive set of admissible MTR pairs, M.
Less ambitious extrapolations are reflected through smaller values of α. For a given α,
more restrictive specifications of M yield smaller bounds, while for a given specification
of M, smaller values of α also yield smaller bounds. Both margins can be smoothly
adjusted, with point identification obtained as a limiting case as α → 0. We illustrate
this tradeoff in our numerical example in Section 4.

3.3   Testable Implications
If the set MS is empty, then the model is misspecified: There does not exist a pair of
MTR functions m that is both admissible (m ∈ M), and which could have generated
the observed data. If M is an unrestricted class of functions, then this is attributable
to a falsification of selection equation (2) together with Assumptions I. The testable
implications of these assumptions for the IV model are well-known, see e.g. Balke and
Pearl (1997), Imbens and Rubin (1997) or Kitagawa (2015). On the other hand, if
other restrictions have been placed on M, then misspecification could be due either to
the failure of Assumptions I, or the specification of M, or both. Our inference results
in Section 5 provide a formal test of the null hypothesis that MS is nonempty.
   This test can also be used to test a variety of null hypotheses about the underlying
MTR functions. For example, Table 1 reports the weights that correspond to the
quantity E[Y0 |X, D = 1] − E[Y0 |X, D = 0]. This quantity is often described as a
measure of selection bias, since it captures the extent to which average untreated
outcomes differ solely on the basis of treatment status, conditional on observables.
                                      ? (m) for the amount of selection bias under
The weights provide a linear mapping βsel


                                           22
       an MTR pair m. Suppose that we restrict M to contain only MTR pairs m for
              ? (m) = 0. Then rejecting the null hypothesis that M is nonempty could be
       which βsel                                                 S
       interpreted as rejecting the hypothesis that there is no selection bias, at least as long
       as Assumptions I and any other restrictions on M are not deemed suspect.
           Alternatively, one might be interested in testing the joint hypothesis that there is
       no selection on unobservables; that is, no selection bias and no selection on the gain.
       Weights for a typical measure of selection on the gain are provided in Table 1. These
                                     ?
       too provide a linear mapping βgain on the set of MTR pairs M. The hypothesis that
       there is no selection on unobservables would be rejected if we were to reject the null
       hypothesis that MS is nonempty when M is restricted to contain only MTR pairs m
                  ? (m) = β ? (m) = 0.
       for which βsel      gain


       4     Numerical Illustration

       In this section, we illustrate how to use our method to construct nonparametric bounds
       on treatment parameters of interest, and how shape restrictions and parametric as-
       sumptions can be used to tighten these bounds.

       4.1    The Data Generating Process
       We consider a simple example with a trinary instrument, Z ∈ {0, 1, 2}, with P (Z =
       0) = .5, P (Z = 1) = .4, and P (Z = 2) = .1. The propensity score is specified as
       p(0) = .35, p(1) = .6, and p(2) = .7. We take the outcome Y ∈ {0, 1} to be binary and
       restrict M to contain only MTR pairs that are each bounded between 0 and 1. The
       data is generated using the MTR functions

                                    m0 (u) = .6b20 (u) + .4b21 (u) + .3b22 (u)
                             and m1 (u) = .75b20 (u) + .5b21 (u) + .25b22 (u),                  (31)

       where b2k is the kth Bernstein basis polynomial of degree 2.9

       4.2    IV Estimand, Weights, and Parameter of Interest
       Figure 1 contains two plots with two vertical axes. The left plot is for d = 0, while
       the right plot is for d = 1, and both vertical axes apply to both plots. The left axis
       measures weight functions, which are indicated with colored curves and, for the sake of
       clarity, are not drawn over regions where they are zero. The blue weights correspond
   9
     Appendix F contains the definition of the Bernstein polynomials, along with a discussion of some useful
properties of the Bernstein polynomial basis.


                                                     23
                   Figure 1: MTRs Used in the Data Generating Process (DGP)
           Weights
         (where 6= 0)                                                                                 MTR
                              d=0                                                   d=1
                                                                                                                1
 4
                                                                                                                0.75
 2

 0                                                                                                              0.5

−2
                                                                                                                0.25
−4
                                                                                                                0
     0       0.2        0.4         0.6    0.8       1        0      0.2      0.4         0.6   0.8         1
                               u                                                   u
                                     DGP MTRs         LATE(0.35, 0.90)      IV slope


         to ωds when s(D, Z) is taken to be (13) so that βs is the IV slope coefficient estimand
         (10) from using Z as an instrument for D. These weights are positive between the
         smallest and largest values of the propensity score, i.e. from p(0) = .35 to p(2) = .7,
         and they change value at p(1) = .6.
            As shown by Imbens and Angrist (1994), three LATEs are nonparametrically point
         identified in this setting: LATE(.35, .6), LATE(.35, .7) and LATE(.6, .7). Suppose that
         the researcher wants to examine the sensitivity of these average causal effects to an
         expansion of the complier subpopulation. Then they might be interested in the target
         parameter

                                     LATE(.35, .9) ≡ E [Y1 − Y0 |U ∈ [.35, .9]] .

         The weights for this parameter are drawn in red in Figure 1. As shown in Table 1, the
         weights are constant over [.35, .9] with magnitude (.9 − .35)−1 ≈ 1.81.
            The right vertical axis in Figure 1 measures MTR functions for both the d = 0 and
         d = 1 plots. The MTR functions that were used to generate the data, i.e. (31), are
         plotted in black. These MTR functions imply a value of approximately .074 for the IV
         slope coefficient. By Proposition 1, this value is equal to the integral of the product
         of the black and blue curves, summed over the d = 0 and d = 1 plots. Similarly,
         these MTR function imply a value of approximately .046 for LATE(.35, .9) through an
         analogous sum of integrals using the red curves.



                                                         24
             Figure 2: Maximizing MTRs When Using Only the IV Slope Coefficient
                                           Nonparametric bounds: [-0.421,0.500]

            Weights
          (where 6= 0)                                                                                    MTR
                               d=0                                                      d=1
                                                                                                                    1
  4
                                                                                                                    0.75
  2

  0                                                                                                                 0.5

−2
                                                                                                                    0.25
−4
                                                                                                                    0
      0         0.2      0.4         0.6       0.8      1        0     0.2        0.4         0.6   0.8         1
                                u                                                      u
                                    Maximizing MTRs         LATE(0.35, 0.90)       IV slope


          4.3     Bounds on the Target Parameter
          We now illustrate how the researcher can extract information about the target param-
          eter LATE(.35, .9) from the class of IV-like estimands introduced in Section 2.3.
             Figure 2 is like Figure 1, except the MTR functions that are plotted yield the
          nonparametric upper bound on LATE(.35, .9). The nonparametric upper bounds are
          computed using the Haar basis discussed in Section 2.7.10 The pair m ≡ (m0 , m1 ) in
          this plot is generated by trying to make m1 as large as possible, and m0 as a small
          as possible, on the support of the red weights, while still yielding a value of .074 for
          the IV slope coefficient determined by the blue weights. These MTR functions imply
          a value of .5 for the target parameter LATE(.35, .9), which is the largest value that is
          consistent with the IV slope estimand. There are multiple pairs of MTR functions with
          this property. In particular, notice that neither weights are positive over the region
          [0, .35], so that MTR pairs may be freely adjusted on this region without changing the
          implied values of the IV slope estimand or LATE(.35, .9). The lower bound of −.421
          indicated in Figure 2 is obtained through an analogous minimization problem that
          follows the same logic as the upper bound.
             Figure 3 repeats this exercise while including a second IV–like estimand. The second
  10
     As discussed in Section 2.7, this amounts to solving two linear programs. We used Gurobi (Gurobi Op-
timization, 2015) to solve all linear programs reported in the paper.



                                                            25
     Figure 3: Maximizing MTRs When Using Both the IV and OLS Slope Coefficients
                                          Nonparametric bounds: [-0.411,0.500]

           Weights
         (where 6= 0)                                                                                         MTR
                              d=0                                                      d=1
                                                                                                                        1
 4
                                                                                                                        0.75
 2

 0                                                                                                                      0.5

−2
                                                                                                                        0.25
−4
                                                                                                                        0
     0       0.2        0.4         0.6       0.8      1          0      0.2     0.4         0.6        0.8         1
                            u                                                           u
                        Maximizing MTRs             LATE(0.35, 0.90)       IV slope         OLS slope




     Figure 4: Maximizing MTRs When Breaking the IV Slope into Two Components
                                          Nonparametric bounds: [-0.320,0.407]

           Weights
         (where 6= 0)                                                                                         MTR
                              d=0                                                      d=1
 6                                                                                                                      1
 4
                                                                                                                        0.75
 2

 0                                                                                                                      0.5
−2
                                                                                                                        0.25
−4

−6                                                                                                                      0
     0       0.2        0.4         0.6       0.8      1          0      0.2     0.4         0.6        0.8         1
                        u                                                               u
           Maximizing MTRs                LATE(0.35, 0.90)        IV slope (1[Z = 2])       IV slope (1[Z = 3])




                                                             26
     Figure 5: Maximizing MTRs When Using All IV–like Estimands (Sharp Bounds)
                                          Nonparametric bounds: [-0.138,0.407]

           Weights
         (where 6= 0)                                                                                     MTR
                              d=0                                                      d=1
                                                                                                                    1
 2
                                                                                                                    0.75

 0                                                                                                                  0.5

                                                                                                                    0.25
−2
                                                                                                                    0
     0       0.2        0.4         0.6       0.8      1        0       0.2      0.4         0.6    0.8         1
                         u                                                              u
              Maximizing MTRs               LATE(0.35, 0.90)        (1 − D)1[Z = 1]         (1 − D)1[Z = 2]
              (1 − D)1[Z = 3]               D1[Z = 1]               D1[Z = 2]               D1[Z = 3]


         estimand is the OLS slope coefficient, whose weights are drawn in light blue. Notice
         that, whereas the blue and red weights are symmetric between d = 0 and d = 1 in the
         sense of having different signs, but the same magnitude, the light blue weights for the
         OLS slope coefficient are asymmetric. A maximizing or minimizing MTR pair must
         yield the implied values for both the IV slope coefficient and the OLS slope coefficient,
         which is approximately .253 in the simulation. In this DGP, the additional constraint
         from the OLS slope coefficient has no effect on the upper bound of LATE(.35, .9), but
         does tighten the lower bound slightly from −.421 to −.411. In Figure 4, instead of
         using Z as a single instrument, we split Z into two binary indicators, 1[Z = 2] and
         1[Z = 3], to create two IV slope estimands. This tightens both bounds on LATE(.35, .9)
         considerably. The tightest possible nonparametric bounds are obtained in Figure 5,
         which includes a collection of six IV–like specifications that is rich enough to satisfy
         the conditions of Proposition 3. The resulting bounds of [−.138, .407] are the sharp
         nonparametric bounds for LATE(.35, .9).
            The bounds in Figure 5 can be tightened considerably by imposing nonparametric
         shape restrictions. For example, in Figure 6, the MTR functions are restricted to be
         decreasing like the DGP MTR functions shown in Figure 1. This tightens the sharp
         identified set for LATE(.35, .9) by ruling out non-decreasing MTR pairs like the one
         shown in Figure 5.


                                                           27
                     Figure 6: Maximizing MTRs When Restricted to be Decreasing
                               Nonparametric bounds, MTRs decreasing: [-0.095,0.077]

           Weights
         (where 6= 0)                                                                                   MTR
                               d=0                                                   d=1
                                                                                                                  1
 2
                                                                                                                  0.75

 0                                                                                                                0.5

                                                                                                                  0.25
−2
                                                                                                                  0
     0         0.2       0.4         0.6     0.8      1        0       0.2     0.4         0.6    0.8         1
                          u                                                           u
               Maximizing MTRs              LATE(0.35, 0.90)       (1 − D)1[Z = 1]        (1 − D)1[Z = 2]
               (1 − D)1[Z = 3]              D1[Z = 1]              D1[Z = 2]              D1[Z = 3]


            Even tighter bounds can be obtained by also requiring the MTR functions to be
         smooth. This may be a desirable a priori assumption if one believes that the jump-
         discontinuous MTR pairs in Figure 6 are sufficiently poorly behaved as to be an unlikely
         description of the relationship between selection unobservables, U , and potential out-
         comes, Y0 and Y1 . For example, in Figure 7, the MTR functions are restricted to be
         decreasing and characterizable by a polynomial of order 10 or lower. This eliminates
         the possibility of non-smooth MTR functions like those in Figure 6, and in this example
         reduces the bounds to [0, 0.067].

         4.4     Tradeoffs between Tightness and the Target Parameter
         Figure 8 illustrates how the bounds change as the parameter of interest changes. In
         particular, instead of LATE(.35, .9), we construct bounds on

                                       LATE(.35, u) ≡ E [Y1 − Y0 |U ∈ [.35, u]] ,

         for different values of u, using the same specification as in Figure 7. Sharp lower and
         upper bounds on this parameter are given by the blue and red curves, respectively.
            As evident from Figure 8, the bounds collapse to a point for u = .6 and .7, i.e the
         two other points of support for the propensity score. For these values of u, LATE(.35, u)


                                                          28
Figure 7: Maximizing MTRs When Further Restricted to be a 10th Order Polynomial
                         Order 9 polynomial bounds, MTRs decreasing: [0.000,0.067]

           Weights
         (where 6= 0)                                                                                 MTR
                              d=0                                                  d=1
                                                                                                                1
 2
                                                                                                                0.75

 0                                                                                                              0.5

                                                                                                                0.25
−2
                                                                                                                0
     0         0.2      0.4         0.6    0.8     1         0       0.2     0.4         0.6    0.8         1
                          u                                                         u
               Maximizing MTRs            LATE(0.35, 0.90)       (1 − D)1[Z = 1]        (1 − D)1[Z = 2]
               (1 − D)1[Z = 3]            D1[Z = 1]              D1[Z = 2]              D1[Z = 3]


         is a usual point identified LATE as in Imbens and Angrist (1994). For other values of
         u this parameter is not point identified, such as for u = .9, which is indicated by the
         dotted vertical line. For u between .6 and .7 the bounds are very tight, as shown in
         the magnified region. As u decreases from .6 or increases above .7, the bounds widen.
         This reflects the increasing difficulty of drawing inference about a parameter the more
         dissimilar it is from what was observed in the data.

         5     Statistical Inference

         In this section, we develop a general testing procedure that enables us to conduct
         statistical inference for the methods and applications described in Sections 2 and 3.

         5.1     Notation and Null Hypothesis
         Our results will be sufficiently flexible to allow for possible nonparametric specifications
         of M, potentially with additional shape restrictions, as well as a finite or infinite
         collection of IV–like specifications, S. To accommodate these possibilities formally,
         we abstract from finite dimensional Euclidean spaces and work with general complete
         normed vector spaces, i.e. Banach spaces. For the MTR functions, we assume that M
         is a subset of a Banach space M with norm k · kM .


                                                       29
                                Figure 8: Bounds on a Family of PRTEs
                                                Bounds on LATE(.35, u)                                           Upp
                                                                                     Upper Bound
              .125                                                                   Lower Bound
                                                                                     Actual Value
                                                                                                                 Low
                .1                                                                                               Act
              .075


               .05


              .025


                0


            −.025


                     .4   .45        .5   .55     .6   .65    .7    .75   .8   .85     .9   .95        1
                                                              u


            For the IV–like estimands, we identify the relationship between βs and its specifi-
         cation s ∈ S with a function s 7→ βs that maps each s ∈ S into βs ≡ E[s(D, Z)Y ]. As
         a notational device, it will sometimes be convenient to extend the domain of the map
         s 7→ βs . For example, when testing whether a target parameter β ? ∈ R is equal to
         zero, we let β ≡ {βs : s ∈ S} ∪ {0} so that β then represents the collection of IV–like
         estimands together with the hypothesized value for the target parameter. On the other
         hand, when conducting a specification test, we only need to set β = {βs : s ∈ S}. As
         another example, if we were testing the joint hypothesis of no unobserved heterogene-
         ity, we might augment β with two additional elements—one for selection bias and the
         other for selection on the gain—as per our discussion in Section 3.3. In order to ac-
         commodate these different applications, as well as a finite or infinite number of IV–like
         specifications S, we will view β as an element of a Banach space B with norm k · kB .
            A primary concern in our development of a statistical procedure is uniform validity
         in the underlying distribution of the data. As argued by Imbens and Manski (2004),
         uniformity considerations are of particular concern when conducting inference in the
5   .5     .55                  .6              .65                .7          .75
         presence of partial identification. In order to discuss uniformity formally, we now      .8       .85     .
         explicitly denote the dependence of various quantities on the distribution of the data,
                                                              u
         P . So, for example, we now write the IV–like estimand βs ≡ E[s(D, Z)Y ] as βP,s to
         emphasize its dependence on the distribution of (Y, D, Z). Similarly, we now write β


                                                         30
      with a subscript as βP . We assume that P lies in a set P of possible distributions
      which satisfy regularity conditions that are introduced subsequently.
          As discussed in Section 2, both the IV–like estimands and the target parameter are
      linear functions of the admissible MTR pair m = (m0 , m1 ) ∈ M ⊆ M. We denote
      these linear functions by a single map ΓP : M 7→ B. For example, if S = {s1 , . . . , s|S| }
      is finite and βP = (βP,s1 , . . . , βP,s|S| )0 , then B = R|S| and

                                   ΓP (m) ≡ (ΓP,s1 (m), . . . , ΓP,s|S| (m))0 ,

      where ΓP,s is as defined in (14), but now carries a P subscript to emphasize its depen-
      dence on the distribution of the data. To test whether a target parameter is equal to
      a given hypothesized value β0? , we let βP = (βP,s1 , . . . , βP,s|S| , β0? )0 and

                              ΓP (m) ≡ (ΓP,s1 (m), . . . , ΓP,s|S| (m), Γ?P (m))0 ,               (32)

      where Γ?P is as defined in (15), but now carries a P subscript as well.
          Given the flexibility of the parameter βP ∈ B and the map ΓP : M 7→ B, we can
      encompass the applications discussed in Section 3 as special cases of the null hypothesis

                                    H0 : P ∈ P0         H1 : P ∈ P \ P0 ,                         (33)

      where the set P0 is defined as

                            P0 ≡ {P ∈ P : ΓP (m) = βP for some m ∈ M}.                            (34)

      The set P0 consists of distributions of the observed data for which there exists an
      admissible MTR pair that generates βP . In the following, we propose a test of (33)
      that provides uniform size control over the set of distributions P.
          By specifying βP appropriately, we can use our test of (33) for a variety of purposes.
      For example, a confidence region for a target parameter can be obtained by setting ΓP
      as in (32) and conducting test inversion of (33) for βP = ((βP,s1 , . . . , βP,s|S| , β0? )0 over
      different hypothesized values β0? of the target parameter. Alternatively, to conduct a
      specification test that employs an infinite collection of moments, we would let βP =
      {βP,s : s ∈ S} and ΓP (m) = {ΓP,s (m) : s ∈ S}.11
11
     We discuss this application more formally in Example 5.2.




                                                       31
5.2   The Test Statistic
The applications in Section 3 highlight both the wide array of empirically relevant
hypotheses encompassed by (33) and the importance of being flexible in the definitions
of βP ∈ B and M ⊆ M. To ensure that our general results apply to such examples,
we will simply assume that we posses estimators β̂ ∈ B for the parameter βP and
Γ̂ : M 7→ B for the map ΓP : M 7→ B. Given such estimators, we then construct a
minimum distance test statistic for the null hypothesis in (33) as

                                          √
                             Tn ≡ inf         nkβ̂ − Γ̂(m)kB ,                       (35)
                                   m∈Mn


where Mn is a subset of M that grows dense in M. When M is finite dimensional,
we can set Mn = M. We note that, provided Γ̂ is linear and Mn is convex, as they
are in our applications of interest, Tn is the solution to a convex optimization problem.
In Appendix G.2, we describe situations in which (35) can be reformulated as a linear
program.
   Our analysis uses some properties of convex sets. In order to introduce these prop-
erties, we need some additional notation. Let B∗ denote the dual space of B, i.e.

                  B∗ ≡ {b∗ : B 7→ R s.t. b∗ is linear and continuous},

endowed with the norm kb∗ kB∗ ≡ supkbkB ≤1 |b∗ (b)|. By definition, every b∗ ∈ B∗ is a
linear map on B. Note too, that every b ∈ B also induces a linear map on B∗ given by
b∗ 7→ b∗ (b). We emphasize this bilinear relationship with the notation hb∗ , bi ≡ b∗ (b).
The weak topology on B is then the weakest topology under which all of the linear
maps b∗ ∈ B∗ (i.e. all hb∗ , ·i) are continuous. The weak topology is important for our
purposes because it arises naturally in the study of both linear maps and convex sets.
   Let M∗ denote the dual space of M. The adjoint of a linear map ΓP : M 7→ B is
defined as the unique linear map Γ∗P : B∗ 7→ M∗ that satisfies

                               hb∗ , ΓP (m)i = hΓ∗P (b∗ ), mi                        (36)

for every b∗ ∈ B∗ and m ∈ M. For example, if M = Rdm and B = Rdβ , then ΓP can
be identified with a dβ × dm matrix, and its adjoint Γ∗P is simply its dm × dβ transpose.
Similarly, we denote the adjoint of Γ̂ as Γ̂∗ . If ΓP : M 7→ B is continuous, then the
bilinear maps (b∗ , m) 7→ hb∗ , ΓP (m)i and (b∗ , m) 7→ hΓ∗P (b∗ ), mi are bounded on any




                                            32
bounded subsets D × M ⊂ B∗ × M. Hence, these maps are elements of
                            (                                                     )
           ∞
          ` (D × M) ≡         f : D × M 7→ R s.t.          sup       |f (b, m)| < ∞ .    (37)
                                                         (b,m)∈D×M


For our purposes, it will be useful to let D be the unit ball in B∗ , so we define

                                D ≡ {b∗ ∈ B∗ : kb∗ kB∗ ≤ 1}.

Intuitively, one can interpret b∗ ∈ D as a “direction” in the original space B.
   For any convex set C ⊆ B, we define its support function ν(·, C) : D 7→ R as

                                     ν(b∗ , C) ≡ suphb∗ , bi.                            (38)
                                                   b∈C


Intuitively, ν(b∗ , C) indicates how far one can move in the direction b∗ while staying
within the set C. Letting Γ̂(Mn ) ≡ {b ∈ B : b = Γ̂(m) for some m ∈ Mn }, we obtain
by duality (see, e.g., Theorem 5.13.1 of Luenberger (1969))

                 √ n ∗                        o                  √ n ∗              o
   Tn = sup       n hb , β̂i − ν(b∗ , Γ̂(Mn )) = sup       inf    n hb , β̂ − Γ̂(m)i .   (39)
         b∗ ∈D                                     b∗ ∈D m∈Mn

In other words, the minimum distance test statistic Tn can also be computed by finding
the direction b∗ ∈ D for which β̂ is the farthest away from its projection onto Γ̂(Mn ).
We will heavily rely on the dual characterization in (39) in our analysis.
   We illustrate our results with two examples that we will return to throughout our
discussion. The first example is extremely simple and intended to exposit the nature
of our statistical approximations, while the second example uses infinite dimensional
spaces, and is used to clarify the more abstract aspects of our analysis.

Example 5.1. Suppose that there are no covariates (Z = Z0 ), Y ∈ {0, 1} is binary, we
have chosen a single IV–like specification (S = {s}), and we have modeled the MTR
functions as md (u) = θd u for θd ∈ R and d = 0, 1. A given MTR function (m0 , m1 ) is
then fully characterized by a pair (θ0 , θ1 ). Therefore, we set M = R2 and define

                      M ≡ {(θ0 , θ1 ) ∈ R2 : θd ∈ [0, 1] for d ∈ {0, 1}},                (40)

where the restriction θd ∈ [0, 1] reflects Y ∈ {0, 1}. Suppose that our goal is to test
whether the specified model is compatible with the single chosen IV–like restriction
(recall S = {s}). To do this, we set B = R, so that βP = E[s(D, Z)Y ], and use some



                                              33
      calculus to write the map ΓP : R2 7→ R as

                                     X         Z    1                     
                    ΓP (m) =               E             θd u ωds (u, Z) du
                                 d∈{0,1}         0
                                 "                                    #0 " #       "     #0 " #
                                     1                2
                                     2 E[s(0, Z)(1 − p (Z))]              θ0        ΓP,0     θ0
                             =          1           2
                                                                               ≡                  ≡ ΓP θ.   (41)
                                        2 E[s(1, Z)p (Z)]                θ1         ΓP,1   θ1

      Hence, the linear map ΓP is just the two dimensional row vector ΓP ≡ (ΓP,0 , ΓP,1 )
      defined in (41). Given a parametric or nonparametric estimator p̂(Z) for p(Z), we
      then set β̂ ∈ R to be the sample analog of E[s(D, Z)Y ] and take Γ̂ ≡ (Γ̂0 , Γ̂1 ) ∈ R2 to
      be the sample analog of ΓP . For this example, the duality result in (39) states that
                                           n√                             o
                      Tn =        inf   n|β̂ − Γ̂(m)| s.t. θd ∈ [0, 1]
                           m=(θ0 ,θ1 )0
                                        n√                                              o
                          = sup inf         nb∗ (β̂ − θ0 Γ̂0 − θ1 Γ̂1 ) s.t. θd ∈ [0, 1] ,                  (42)
                             −1≤b∗ ≤1 θ0 ,θ1


      where we used that B∗ = B = R, and D = {b∗ ∈ R : |b∗ | ≤ 1}. 

      Example 5.2. As in Example 5.1, we assume that there are no covariates (Z = Z0 )
      and that Y ∈ {0, 1} is binary. However, now we consider testing the null hypothesis
      that agents more likely to select into treatment also have a higher expected benefit
      from treatment. To this end, we take M to be the infinite dimensional set of functions

         M = {(m0 , m1 ) s.t. md : [0, 1] 7→ [0, 1], m1 − m0 monotonically increasing} ,                    (43)

      and Mn ⊂ M a finite dimensional subset, such as pairs of Bernstein polynomials
      of finite order K < ∞; see Appendix F. For any m = (m0 , m1 ), we let kmk2M ≡
        (m0 (u)2 + m1 (u)2 )du and note that M ⊂ M for M ≡ L2 ([0, 1]) × L2 ([0, 1]) where
      R
                                          R1
      L2 ([0, 1]) ≡ {f : [0, 1] 7→ R s.t. 0 f (u)2 du < ∞}.
          We also consider an infinite set of IV–like specifications S that satisfies the condi-
      tions of Proposition 3. By Stinchcombe and White (1998), such a set S can be of the
      form
                                           S = {s(·; λ) : λ ∈ Λ ⊂ Rdλ },                                    (44)

      where s(·; λ) : {0, 1} × Rdz 7→ R is known and λ ∈ Λ for Λ ⊂ Rdλ a finite dimensional
      compact parameter space.12 We can then identify βP with a function on Λ via

                                               βP (λ) ≡ E[s((D, Z); λ)Y ].                                  (45)
12
     E.g., take s((d, z); λ) = (1 − d) exp{λ0 + z 0 λ1 } + d exp{λ2 + z 0 λ3 } with λ = (λ0 , λ1 , λ2 , λ3 ).


                                                                 34
      Provided λ 7→ E[s((D, Z); λ)] is continuous, we can set B = C(Λ) with C(Λ) ≡ {f :
      Λ 7→ R s.t. f is continuous} and k · kB = k · k∞ with kf k∞ ≡ supλ∈Λ |f (λ)|. The map
      ΓP : M 7→ C(Λ) assigns to each MTR pair (m0 , m1 ) the continuous function on Λ
      defined by
                         "Z                                 #          "Z                                  #
                              1                                                 p(Z)
        ΓP (m)(λ) ≡ E               m0 (u)s0 ((0, Z); λ)du + E                         m1 (u)s1 ((0, Z); λ)du . (46)
                             p(Z)                                           0


          As in Example 5.1, given an estimator p̂(Z) for the propensity score p(Z), we may
      obtain estimators β̂ and Γ̂ through the sample analogs of (45) and (46). The dual
      space of B = C(Λ) is the set of signed Borel measures with bounded variation, i.e.
      T(Λ) ≡ {τ : Λ d|τ |(λ) ≤ 1}.13 For this example, the duality result in (39) states that
                 R

                                            √
                              Tn = inf        nkβ̂ − Γ̂(m)k∞
                                     m∈Mn
                                                   √
                                                       Z
                                  = sup       inf    n (β̂(λ) − Γ̂(m)(λ))dτ (λ)
                                     τ ∈T(Λ) m∈Mn
                                                √
                                  = sup inf         n|β̂(λ) − Γ̂(m)(λ)|,                                       (47)
                                     λ∈Λ m∈Mn

      where the final equality follows after noting that, for any m ∈ Mn , the optimal τ puts
      measure plus or minus one on the λ ∈ Λ that maximizes |β̂(λ) − Γ̂(m)(λ)|. 

      5.3     Distributional Approximation
      In this section, we obtain an approximation for the distribution of the test statistic,
      Tn . To do this, we maintain the following assumptions.

            Assumptions S
       S.1 M ⊆ M is convex and compact in the weak topology.
       S.2 The maps ΓP : M 7→ B and Γ̂ : M 7→ B are linear and continuous.
       S.3 There are tight and centered jointly Gaussian processes (GP,β , GP,Γ ) ∈ B×`∞ (D×
                          √                                    √
           M) such that n{Γ̂ − ΓP } = GP,Γ + Op (δnc ) and n{β̂ − βP } = GP,β + Op (δnc )
            uniformly in P ∈ P for some sequence δnc ↓ 0.
       S.4 The Gaussian processes (GP,β , GP,Γ ) satisfy
                                                                                                  
                       sup E[kGP,β kB ] < ∞          and        sup E       sup kGP,Γ (m)kB < ∞.
                      P ∈P                                      P ∈P        m∈M

13
     See, e.g., Corollary 14.15 in Aliprantis and Border (2006).



                                                           35
S.5 For every m ∈ M there is a Πn m ∈ Mn ⊆ M and a sequence δns ↓ 0 such that

                                      √
                               sup  nkΓP (m − Πn m)kB ≤ δns
                               P ∈P
                                                                 
                       and     sup E sup kGP,Γ (m) − GP,Γ (Πn m)kB ≤ δns .
                               P ∈P        m∈M


   Assumption S.1 is our main requirement for M, which is that it is convex and
compact with respect to the weak topology. If M is reflexive (i.e. M = (M∗ )∗ ), as in
Example 5.2, then S.1 is satisfied if M is convex, closed, and bounded under k · kM .
Assumption S.2 formalizes our requirement that the maps ΓP and Γ̂ are linear and
continuous. Our main statistical assumption is S.3, which can be interpreted as requir-
ing that a central limit theorem applies to the estimators β̂ and Γ̂ uniformly in P ∈ P
                                       √               √
at a rate δnc ↓ 0. In our applications, n{β̂ − βP } and n{Γ̂ − ΓP } are asymptotically
equivalent to empirical processes and Assumption S.3 can hold with rates as fast as
             √
δnc = log(n)/ n (Koltchinskii, 1994; Rio, 1994); see also Chernozhukov et al. (2015).
Assumption S.4 imposes moment conditions on the processes (GP,β , GP,Γ ). Assump-
tion S.5 requires the approximation error δns introduced from employing Mn in place
of M to vanish sufficiently fast. We note that, due to Assumption S.3, there are no
constraints on the rate of growth of the sieve Mn .
   Under the null hypothesis, there exists an m ∈ M such that ΓP (m) = βP . By the
definition of the support function, it follows that

                          hb∗ , βP i − ν(b∗ , ΓP (M)) ≤ 0     for all b∗ ∈ D.                     (48)

Since 0 ∈ D, there is always a b∗ ∈ D for which (48) holds with equality. This suggests
that the supremum in the dual representation of Tn (see (39)) is attained at a b∗ in

                    DP (κun ) ≡ {b∗ ∈ D : hb∗ , βP i − ν(b∗ , ΓP (M)) ≥ −κun },                   (49)

for any positive sequence κun that converges to zero, but not too quickly. The set
DP (κun ) shares a similarity with the moment inequalities literature, in which the set of
moment inequalities that are “close” to binding plays a crucial role in inference; see
Canay and Shaikh (2016) and the references cited therein. Analogously, we use DP (κun )
to define the distributional approximation

                                                                      √
  UP,n (κun ) ≡                         hb∗ , GP,β − GP,Γ (m)i s.t.       nhb∗ , βP − ΓP (m)i ≤ δns .
                                    
                    sup       inf
                  b∗ ∈DP (κu  m∈M
                           n)


Our next result provides some properties of UP,n (κun ) as an approximation to Tn .

                                                    36
Theorem 1. Suppose that Assumptions S.1–S.5 hold. Let κun be any sequence for
     √
which nκun ↑ ∞, and define DP (κun ) as in (49). Then, uniformly over P ∈ P,

                                                            √
   Tn =       sup           inf {hb∗ , GP,β − GP,Γ (m)i +       nhb∗ , βP − ΓP (m)i} + op (1).    (50)
          b∗ ∈D       u     m∈M
                  P (κn )



Moreover, there exists a sequence ξn = Op (δnc + δns ) such that for any P ∈ P0 ,

                                         Tn ≤ UP,n (κun ) + ξn ,                                  (51)

uniformly over P ∈ P.

   The first result in Theorem 1 provides an asymptotic expansion for our test statis-
tic, Tn . We will use this expansion to assess the quality of our subsequent bounds. In
particular, (50) can be used to conclude that the quantiles of the pointwise asymptotic
distribution of Tn can fail to deliver uniform size control without additional assump-
tions; see Example 5.1 below. As a result, we will instead construct a test with a critical
value based on the quantiles of UP,n (κun ). By showing that UP,n (κun ) is an asymptotic
upper bound for Tn , Theorem 1 lays the foundations for establishing that such a test
can deliver uniform size control. It is worth noting that the results of Theorem 1 hold
for any sequence κun that satisfies the stated conditions. Thus, Theorem 1 actually
establishes a family (indexed by sequences κun ) of uniformly valid upper bounds for Tn .
   We now revisit Example 5.1, which is useful for clarifying the nature of our approx-
imations, and then Example 5.2, which helps illustrate the content of our assumptions.

Example 5.1 (continued). Consider a sequence of distributions Pn,γ , such that βPn,γ =
       √                     √
(1 + γ/ n) and ΓPn,γ = (1, γ/ n) for some γ ≥ 0. Suppose for simplicity that Zβ and
ZΓ are independent standard normal random variables, and that

          √                                         √
              n{β̂ − βPn,γ } = Zβ + op (1)           n{Γ̂ − ΓPn,γ } = (0, ZΓ ) + op (1).          (52)

                                                                       √
Direct calculation shows that for any κun ↓ 0 that satisfies               nκun ↑ ∞, (50) simplifies to

                            Tn = max {min {Zβ + γ, Zβ − ZΓ } , 0} + op (1).                       (53)

It is important to notice that the quantiles of the distributional approximation in (53)
are increasing in γ. As a result, any critical value cn that satisfies

                                         lim Pn,0 (Tn > cn ) = α                                  (54)
                                        n→∞

will deliver an asymptotic rejection probability larger than α along any contiguous

                                                   37
      sequence Pn,γ with γ > 0. This result contrasts with, e.g., Andrews and Soares (2010),
      in which the pointwise limit is also the “least favorable.” Employing the upper bound
      UP,n (κun ) addresses this difficulty. In particular,

                                UPn,γ ,n (κun ) = max{Zβ − ZΓ , 0} + op (1)                   (55)

      for any sequence κun ↓ 0. In this example, (55) corresponds to the “least favorable”
      value of the local parameter γ in (53), i.e. γ = +∞. 

      Example 5.2 (continued). Since M is reflexive, Assumption S.1 is satisfied whenever
      M is closed and bounded, which is the case for M in (43). Assumption S.2 is satisfied
      for ΓP : M 7→ B as in (46) (and its plug-in analog) by Jensen’s inequality, provided
      that S has a square integrable envelope. Sufficient conditions for Assumption S.3 can
      be found by establishing a (uniform over P ∈ P) asymptotic expansion

                                                         n
                        √                        1 X
                            n{Γ̂ − ΓP }(m)(λ) = √    ψ(Di , Zi , m, λ) + op (1)               (56)
                                                  n
                                                      i=1

      and ensuring that {f (y, d, z) = ys(d, z) : s ∈ S} and {f (d, z) = ψ(d, z, m, λ) : (m, λ) ∈
      M × Λ} are suitably Donsker classes.14 Assumption S.4 reduces to a uniform (in
      P ∈ P) moment bound on the supremum of the approximating Gaussian processes.
      To verify Assumption S.5, one can apply results on sieve approximation errors, such
      as those available in Chen (2007) and the references cited therein. 

      5.4    Bootstrap Approximation
      The important implication of Theorem 1 is that the quantiles of UP,n (κun ) are valid
      critical values for the test statistic Tn . We now address the problem of estimating these
      quantiles. For clarity of exposition, we divide our analysis into two steps, each of which
      addresses a distinct challenge.
14
     Such requirement may necessitate further restricting M by, e.g., imposing a bound on derivatives.




                                                    38
5.4.1    An Infeasible Approximation
We first derive an infeasible bootstrap approximation for the distribution of UP,n (κun ).
To this end, we first rewrite UP,n (κun ) as the saddle point problem

                    UP,n (κun ) = sup inf hb∗ , GP,β − GP,Γ (m)i
                                  b∗ ∈D m∈M

                                s.t. (i) hb∗ , βP i − ν(b∗ , ΓP (M)) ≥ −κun ,
                                          √
                                     (ii) nhb∗ , βP − ΓP (m)i ≤ δns ,                     (57)

where constraint (i) corresponds to b∗ ∈ DP (κun ), as defined in (49). Characterizing
UP,n (κun ) as (57) emphasizes that its distribution is determined by only three essential
unknowns: The objective function and the two constraints of optimization.
   The objective function in (57) depends on the unknown distribution P of the data
only through the asymptotic distribution of the chosen estimators, i.e. (GP,β , GP,Γ ).
While the bootstrap is not valid for estimating the distribution of Tn , it can nonetheless
often consistently estimate the distribution of these estimators (Fang and Santos, 2014).
We therefore assume the existence of suitable estimators (Ĝβ , ĜΓ ) of the distribution
of (GP,β , GP,Γ ) as follows.

      Assumptions S (continued)

                     P,β , GP,Γ ) + Op (δn ) in B × ` (D × M) uniformly in P ∈ P, with
S.6 (Ĝβ , ĜΓ ) = (Gbs     bs           c           ∞


        P,β , GP,Γ ) independent of {(Yi , Di , Zi )}i=1 and equal in law to (GP,β , GP,Γ ).
      (Gbs     bs                                    ∞


S.7 Mn ⊆ M is convex and closed.

   Assumption S.6 is our main bootstrap requirement. It requires the existence of
a consistent bootstrap procedure that estimates the law of (GP,β , GP,Γ ) uniformly in
P ∈ P at the rate δnc , i.e. the same rate as in Assumption S.3. Typically, for standard
bootstrap analogs β̂ bs and Γ̂bs of β̂ and Γ̂ respectively, the estimators Ĝβ and ĜΓ would
                              √                           √
correspond to setting Ĝβ = n{β̂ bs − β̂} and ĜΓ = n{Γ̂bs − Γ̂}. However, note that
Assumption S.6 also allows for estimation of the law of (GP,β , GP,Γ ) through alternative
resampling procedures, such as a score or weighted bootstrap, subsampling, or the m
out of n bootstrap. Assumption S.7 imposes the regularity condition that Mn is closed
and convex.
   Having estimators (Ĝβ , ĜΓ ) enables us to mimic the stochastic behavior of the ob-
jective function in (57) by simply employing a plug-in sample analog, i.e. by replacing
hb∗ , GP,β −GP,Γ (m)i with hb∗ , Ĝβ − ĜΓ (m)i. A similar approach is also effective for han-
dling constraint (i) in (57), which corresponds to imposing b∗ ∈ DP (κun ). Specifically,


                                                39
       we replace constraint (i) in (57) with the constraint that b∗ ∈ D̂n , where

                           D̂n ≡ {b∗ ∈ D : hb∗ , β̂i − ν(b∗ , Γ̂(Mn )) ≥ −κ̂un }                 (58)

       and κ̂un is a bandwidth selected by the researcher. We discuss data-driven choices of
       κ̂un in Section 5.5.2. However, we note here that setting D̂n = D (κ̂un = ∞) is always a
       valid choice.
          Turning to (ii) in (57), we note that using a simple plug-in analog of this constraint
       can lead to a lack of size control. Instead, we construct a possibly random set M̂n
       that, heuristically, should satisfy the following condition asymptotically:

                                      {m ∈ M : ΓP (m) = βP } ⊆ M̂n .                             (59)

       Under the null hypothesis, there exists an mP ∈ M such that ΓP (mP ) = βP , and
       therefore any m ∈ M̂n that satisfies hb∗ , ΓP (mP − m)i ≤ 0 must also satisfy constraint
       (ii) in (57). These observations imply that under the null hypothesis

        {m ∈ M̂n : hb∗ , ΓP (m)i = ν(b∗ , ΓP (M̂n ))} ⊆ {m ∈ M : hb∗ , βP − ΓP (m)i ≤ 0} (60)

       whenever (59) holds. While setting M̂n = M guarantees condition (59), for power
       considerations it may be advisable to instead set

                                 M̂n ≡ {m ∈ Mn : kβ̂ − Γ̂(m)kB ≤ κ̂m
                                                                   n}                            (61)

       for some bandwidth κ̂m
                            n that is chosen by the researcher.
                                                                15 Note that taking κ̂m = ∞
                                                                                      n
       corresponds to choosing M̂n = M. While we allow κ̂un and κ̂m
                                                                  n to differ from each
       other, these bandwidths are in fact closely related and can often be selected through a
       common procedure. See Section 5.5.2 for additional details.
          At this point, our discussion suggests that the distribution of
                                   n                                                           o
         In (ΓP ) ≡ sup    inf      hb∗ , Ĝβ − ĜΓ (m)i s.t. hb∗ , ΓP (m)i = ν(b∗ , ΓP (M̂n )) , (62)
                   b∗ ∈D̂n m∈M̂n


       conditional on the data, should approximate the distribution of UP,n (κun ). Intuitively,
       In (ΓP ) imitates (57) but replaces constraint (i) with b∗ ∈ D̂n , and replaces constraint
       (ii) with m ∈ {m̃ ∈ M̂n : hb∗ , ΓP (m̃)i = ν(b∗ , ΓP (M̂n ))}. The bootstrap statistic
  15
      Loosely speaking, we should expect the minimizer of Tn = inf m∈Mn kβ̂ − Γ̂(m)kB to “converge” to
{m ∈ M : βP = ΓP (m)}. We emphasize, however, that the set of distributions P that we consider allows
for the possibility that the set {m ∈ M : βP = ΓP (m)} is not consistently estimable uniformly in P ∈ P.



                                                      40
In (ΓP ) is infeasible due to its dependence on the unknown map ΓP : M 7→ B. We
address this challenge in the next section. Nevertheless, establishing the properties of
In (ΓP ) provides us with a useful intermediate result before analyzing the bootstrap
statistic itself. For this purpose, we impose the following requirements on κ̂un and κ̂m
                                                                                       n:

     Assumptions S (continued)
S.8 κ̂un satisfies

                                                       κ̂un
                                                                    
                                   lim inf inf P            > (1 + δ) = 1
                                   n→∞ P ∈P            κun
                                                                     √
    for some δ > 0 and some non-random sequence κun such that nκun ↑ ∞.
                                                      √ m
S.9 κ̂m
      n satisfies lim inf C↑∞ lim inf n→∞ inf P ∈P P ( nκ̂n ≥ C) = 1 for every C > 0.

   Assumption S.8 allows κ̂un to be random, but requires it to be (asymptotically) larger
than the nonrandom sequence κun , which determines the random variable UP,n (κun )
                                                                 √
whose distribution we aim to approximate. Assumption S.9 requires nκ̂m
                                                                     n to diverge
to infinity asymptotically as well. By allowing κ̂un and κ̂m
                                                           n to be random, Assumptions
S.8 and S.9 enable us to employ data-driven choices of bandwidths. We discuss possible
choices in Section 5.5.2.
   Let Ubs    u
        P,n (κn ) be the random variable defined by

                                   n                            √ ∗                      o
  Ubs    u
   P,n (κn ) ≡       sup    inf     hb∗ , Gbs
                                           P,β − GP,Γ (m)i s.t.
                                                  bs
                                                                 nhb , βP − ΓP (m)i ≤ δns .
                 b∗ ∈DP (κu  m∈M
                          n)



                                                         P,n (κn ) and UP,n (κn ) share
Notice that, given Assumption S.6, the random variables Ubs    u              u

the same distribution, and hence the same quantiles. However, in contrast to UP,n (κun ),
the random variable Ubs    u
                     P,n (κn ) is independent of the data, and hence its quantiles
conditional on the data are equal to the unconditional quantiles of UP,n (κun ) that we
desire for inference. These observations motivate our next intermediate result.

Lemma 5.1. Suppose that Assumptions S.1–S.7 and S.9 hold. Then, for any sequence
                                                √
κun that satisfies Assumption S.8, and for which nκun ↑ ∞, there exists a sequence
ξnbs ∈ R such that ξnbs = Op (δnc ) uniformly over P ∈ P0 , and such that

                                    Ubs    u                 bs
                                     P,n (κn ) ≤ In (ΓP ) + ξn                          (63)

for any P ∈ P0 .

   Together with Theorem 1, Lemma 5.1 suggests that the quantiles of the bootstrap
statistic In (ΓP ), conditional on the data, can be used as critical values for the test

                                                   41
statistic, Tn . In the next section, we use Lemma 5.1 to establish an analogous result
for a feasible bootstrap statistic. Before proceeding, we revisit Example 5.1 to illustrate
the approximations in Lemma 5.1, and Example 5.2 to clarify our assumptions.

Example 5.1 (continued). We continue to consider the sequence Pγ,n , which satisfies
                          √                      √
(52) with βPγ ,n = (1 + γ/ n) and ΓPγ ,n = (1, γ/ n) for some γ > 0. Let

                      Ĝβ = Zbs
                             β + op (1)       ĜΓ = (0, Zbs
                                                         Γ ) + op (1)                  (64)

where Zbs
       β and ZΓ are independent standard normal random variables. Provided that
              bs

κ̂un satisfies Assumption S.8 and converges in probability to zero, it is straightforward
to verify that D̂n converges in probability to [0, 1], which is the set of “directions” b∗ ∈
[−1, 1] satisfying hb∗ , βPγ,n i = ν(b∗ , ΓPγ,n (M)). Similarly, provided that κ̂m
                                                                                 n converges
in probability to zero, it follows that with probability tending to one along Pγ,n

                 {(θ0 , θ1 ) = m ∈ M : m = (1, θ1 ) for θ1 ∈ [0, 1]} ⊂ M̂n .           (65)

However, (65) implies {m ∈ M̂n : hb∗ , ΓP (m)i = ν(b∗ , ΓP (M̂n ))} = {(1, 1)}, and hence

           In (ΓPγ,n ) = sup hb∗ , Zbs
                                    β − ZΓ i + op (1) = UPγn ,n (κn ) + op (1),
                                         bs              bs       u
                                                                                       (66)
                         b∗ ∈[0,1]


where the final equality follows from (55). Therefore, in this simple example, In (ΓP )
and Ubs    u
     P,n (κn ) are asymptotically equivalent. 

Example 5.2 (continued). As in the verification of Assumption S.3, sufficient condi-
tions for establishing Assumption S.6 can be found by guaranteeing that an asymp-
totically linear expansion such as (56) also holds for the bootstrap estimators. For
verifying Assumption S.7, note that convex sets are closed in the weak topology of M
if and only if they are closed under k · kM . 


5.4.2    The Bootstrap Statistic
The main obstacle to using the statistic In (ΓP ) for inference is that it depends on the
unknown linear map ΓP : M 7→ B. We now address this challenge and construct a
feasible bootstrap statistic. Before proceeding, we present a lemma that makes the
dependence of In (ΓP ) on the unknown map ΓP more transparent. Recall from (36)
that Γ∗P : B∗ 7→ M∗ denotes the adjoint of ΓP .




                                             42
       Lemma 5.2. Suppose that Assumptions S.1–S.5, S.7, and S.9 are satisfied. Then
                                             n                                                  o
           In (ΓP ) = sup     sup      inf    hb∗ , Ĝβ − ĜΓ (m)i s.t. hΓ∗P (b∗ ), m − m̃i ≥ 0      (67)
                      b∗ ∈D̂n m̃∈M̂n m∈M̂n


       with probability tending to one, uniformly over P ∈ P0 .

          Lemma 5.2 shows that ΓP enters the optimization problem that defines In (ΓP )
       solely through the bilinear constraint involving b∗ , m, and m̃. Given an estimator, Γ̂, it
       would be natural to approximate In (ΓP ) by simply employing the plug-in analog In (Γ̂).
       However, the feasible set in (67) changes discontinuously in ΓP . As a result of this,
       using the quantiles of In (Γ̂) as critical values can fail to control size. For this reason,
       we instead consider the least favorable critical value obtained from a “neighborhood”
       of our estimator Γ̂.
          Defining an appropriate neighborhood of Γ̂ can be challenging when M and/or
       B are infinite dimensional. In particular, Assumption S.3 is too weak to guarantee
       that Γ̂ is consistent for ΓP with respect to the operator norm.16 Instead, we build
       neighborhoods using the weak (operator) topology. Let Mn be the closed linear span
       of Mn (in M), and let M∗n denote its dual space. For any b∗ ∈ B∗ , define

                     Ĝn (b∗ ) ≡ {g ∈ M∗n : |hg − Γ̂∗ (b∗ ), vi| ≤ κ̂gn for all v ∈ Vn }             (68)
                              = {g ∈ M∗n : |hg, vi − hb∗ , Γ̂(v)i| ≤ κ̂gn for all v ∈ Vn },          (69)

       where κ̂gn ↓ 0 and Vn ⊆ M are chosen by the researcher. Typically, we take Vn ⊆ Mn
       and use specification (69) in place of (68) so as to avoid having to compute the adjoint
       Γ̂∗ . We provide guidance for choosing κ̂gn in Section 5.5.2.
          We can now define our feasible bootstrap statistic as
                                                      n                                         o
          Tnbs ≡ sup           sup           inf       hb∗ , Ĝβ − ĜΓ (m)i s.t. hg, m − m̃i ≥ 0 .   (70)
                 b∗ ∈D̂n (g,m̃)∈Ĝn (b∗ )×M̂n m∈M̂n


       While (70) looks like an unwieldy optimization problem, we show in Appendix G.3
       that it can be reformulated as a bilinear program. While bilinear programs are not
       convex, they can be provably solved to global optimality by algorithms that combine
       McCormick relaxations (McCormick, 1976) with spatial branch-and-bound strategies.17
  16
     More precisely, supkmkM ≤1 k{Γ̂ − ΓP }(m)kB need not converge to zero in probability.
  17
     A good software implementation of these types of algorithms is the BARON solver developed by
Tawarmalani and Sahinidis (2005). Although not provably convergent to global optimality, we have found
that more common locally optimal solvers, such as KNITRO (Byrd, Nocedal, and Waltz, 2006), almost
always find the global optimum for our problem, and much more quickly than BARON. We use KNITRO


                                                          43
      For our purposes, an attractive property of these algorithms is that termination after a
      set amount of time always provides an upper bound on the actual optimal value, Tnbs .
      Critical values constructed from these upper bounds may be conservative, however they
      will still provide size control.
          We use the following two assumptions to establish the properties of Tnbs .
            Assumptions S (continued)
                                                             √
     S.10 κ̂gn satisfies lim inf C↑∞ lim inf n→∞ inf P ∈P P ( nκ̂gn ≥ C) = 1 for every C > 0.
     S.11 There exists a λ ∈ R and an mc ∈ Mn such that Vn ⊆ λ(Mn − mc ) for every n.
      Assumption S.10 requires the bandwidth κ̂gn (which can be data-dependent) to not
                                            √
      converge to zero at a rate faster than n. Assumption S.11 relates the set of functions
      Vn used to construct Ĝn (b∗ ) to the sieve Mn . This requirement is imposed as a simple
                                                            √
      sufficient condition to ensure that the process hb∗ , n{Γ̂ − ΓP }(v)i is asymptotically
      tight on `∞ (D × V) for V = ∞
                                      S
                                        n=1 Vn . For applications in which B and M are finite
      dimensional, the sets Vn may chosen so that Ĝn (b∗ ) = {g : kg − Γ̂(v)k ≤ κ̂gn }, where
      k · k is the standard Euclidean norm. Notice that Assumption S.11 does not place any
      requirements on the “rate of growth” of Vn .
          The next theorem describes the properties of our proposed bootstrap statistic.
      Theorem 2. Suppose that Assumptions S.1–S.7 and S.9–S.11 are satisfied. Then, for
                                                                √
      any sequence κun that satisfies Assumption S.8, as well as nκun ↑ ∞, there exists a
      sequence ξnbs ∈ R, with ξnbs = Op (δnc ) uniformly in P ∈ P0 , and such that

                                          Ubs    u      bs   bs
                                           P,n (κn ) ≤ Tn + ξn                              (71)

      for any P ∈ P0 .
      Theorems 1 and 2 build the foundation for testing the null hypothesis in (33) by
      comparing the test statistic Tn to critical values obtained from the quantiles of the
      bootstrap statistic Tnbs , conditional on the data. We establish the properties of such a
      test in the next section.
          Our choice of Tnbs is informed by considerations of computational reliability in real-
      istically sized problems. For simple problems, a less conservative, but computationally
      more challenging critical value may also be available. To see this, start by defining the
      1 − α quantile of In (Γ) (conditional on the data) as

                      q1−α (Γ) ≡ inf {c s.t. P (In (Γ)|{Yi , Di , Zi }ni=1 ) ≥ 1 − α} .     (72)
                                  c∈R

for our empirical application in Section 6, but we have checked a subset of the results using BARON and
found them to be nearly identical.

                                                     44
In principle, it is possible to use the least favorable critical value given by
             n                                                                            o
      sup     q1−α (Γ) s.t. |hΓ∗ (b∗ ) − Γ̂∗ (b∗ ), vi| ≤ κ̂gn for all (b∗ , v) ∈ D̂n × Vn .           (73)
    Γ:M7→B


Intuitively, by using Tnbs , we are computing the quantile of the maximum over a neigh-
borhood of Γ̂, instead of the maximum of the quantile over such a neighborhood.
Whenever (73) is solvable, it will yield a critical value that provides better power than
Tnbs . We illustrate this in Example 5.1 below. Our recommendation is to use (73) when
feasible, but we focus on Tnbs due to its wider (computational) applicability.
   We conclude this section by revisiting Examples 5.1 and 5.2.

Example 5.1 (continued). We return again to the sequences Pγ,n , which satisfied
                                   √                      √
(52) and (65) with βPγ ,n = (1 + γ/ n) and ΓPγ ,n = (1, γ/ n). In this simple example,
M = R2 and B = R, so that Γ̂∗ : R 7→ R2 and Γ∗P : R 7→ R2 are given by

                             γ 0
                                                                                       0
                                                                              ZΓ + γ
                                                                      
         Γ∗P (b∗ )     ∗
                     = b 1, √                   and        ∗   ∗
                                                       Γ̂ (b ) = b  ∗
                                                                            1, √            + op (1)   (74)
                              n                                                  n

for any b∗ ∈ R. Setting Vn = {(1, 0), (0, 1)} implies that for any b∗ ≥ 0,
                                                                             
                            0                              0    1
                                            2
                     (g1 , g2 ) = g ∈ R : g = (1, c) and |c| ≤ √                  ⊆ Ĝn (b∗ )          (75)
                                                                 n

with probability tending to one along Pγ,n . For notational clarity, we also define

     L(g) ≡           sup            inf {hb∗ , Gbs
                                                 P,β − GP,Γ (m)i s.t. hg, m − m̃i ≥ 0},
                                                        bs
                                                                                                       (76)
               (b∗ ,m̃)∈[0,1]×M̂n   m∈M̂n


and note that the set inclusion in (65) implies that

                                                 β − ZΓ }
                                         max{0, Zbs
                                    
                                                      bs                if g2 > 0
                                    
                                    
                        L(g) =          max{0, Zbs
                                                β     − (Zbs
                                                           Γ )+ }       if g2 = 0                      (77)
                                    
                                                max{0, Zbs
                                                        β }             if g2 < 0
                                    


for any (1, g2 )0 = g ∈ R2 , where (Zbs          bs
                                     Γ )+ = max{ZΓ , 0}. We conclude that


                            Tnbs = max{0, Zbs
                                           β , Zβ − ZΓ } + op (1).
                                                bs   bs



Note that, at least for this example, the less conservative critical values given in (73)
can actually be solved for analytically. These critical values correspond to the quantiles
of max{0, Zbs
           β − ZΓ }, which are also equal to the quantiles of UP,n (κn ). 
                bs                                                   u




                                                      45
Example 5.2 (continued). We use this example to illustrate the computation of Tnbs
in a more abstract setting. Recall that B = C(Λ) and M = L2 ([0, 1]) × L2 ([0, 1])
so that B∗ = T(Λ) is the set of signed Borel measures of bounded variation, while
M∗ = L2 ([0, 1]) × L2 ([0, 1]). Hence, for this example we obtain
                        Z                           Z                           Z                      
      D̂n =       b∗ :        d|b∗ |(λ) ≤ 1 and          β̂(λ)db∗ (λ) ≥ sup          Γ̂(m)db∗ (λ) − κ̂un ,
                          Λ                          Λ                    m∈Mn   Λ
            n                                             o
    M̂n = m ∈ Mn : |β̂(λ) − Γ̂(m)(λ)| ≤ κ̂mn for all λ ∈ Λ  ,
                           1
            (            "
                      Z 1 X              #      Z                                )
 Ĝn (b∗ ) = g ∈ Mn :        gd (u)vd (u) du −     Γ̂(v)(λ)db∗ (λ) ≤ κ̂gn ∀v ∈ Vn ,
                                   0   d=0                            Λ


where we used the fact that Mn = M∗n . Therefore, the bootstrap statistic reduces to
                                                               Z
              Tnbs = sup                sup              inf       {Ĝβ (λ) − ĜΓ (m)(λ)}db∗ (λ)
                          b∗ ∈D̂n (g,m̃)∈Ĝn (b∗ )×M̂n m∈M̂n   Λ
                                                 Z    1
                                                     1X
                                          s.t.             [gd (u)(md (u) − m̃d (u))] du ≥ 0.           (78)
                                                  0 d=0


We note that, even though b∗ is infinite dimensional, it is possible to instead optimize
Tnbs over a sieve for D. We provide a formal development of this sieve approach for D in
Appendix E. A natural choice of a sieve for D would be Dn ≡ {b∗ (A) = Jj=1 wj 1{λj ∈
                                                                         P

A} : Jj=1 |wj | ≤ 1} for {λj }Jj=1 ⊆ Λ. Similarly, using Mn in place of M renders
      P

all parameters finite dimensional and all constraints bilinear. See Appendix G for
additional discussion of computation. 

5.5     The Test
In this section, we apply the results obtained in Sections 5.3 and 5.4 to construct a
consistent test. We then discuss choosing bandwidths for this test.


5.5.1    Basic Properties
Theorem 1 implies that the unconditional quantiles of UP,n (κun ) could be used as critical
values for the test statistic Tn . While UP,n (κun ) is infeasible, Theorem 2 suggests that
the quantiles of Tnbs conditional on the data can be used instead. We therefore define
the critical value ĉ1−α for our test to be given by
                                    n                                        o
                         ĉ1−α ≡ inf c : P Tnbs ≤ c|{Yi , Di , Zi }ni=1 ≥ 1 − α .                       (79)




                                                          46
In the following, it will also be useful to define the quantiles of UP,n (κun ) itself, which
we denote by

                  c1−α (UP,n (κun )) ≡ inf{c : P (UP,n (κun ) ≤ c) ≥ 1 − α}.              (80)

    As usual, a final regularity condition is required to establish that a distributional
approximation also delivers consistent estimators of the desired quantiles; see, e.g.,
Romano and Shaikh (2012) and Chernozhukov, Lee, and Rosen (2013). In the present
context, this regularity condition is the following.
      Assumptions S (continued)
S.12 There is a δ > 0 and sequence ζn with ζn (δnc + δns ) = o(1) and

                                    1                                          
               sup sup sup            P |UP,n (κun ) − c1−α−η (UP,n (κun ))| ≤  ≤ ζn .
              η∈[0,δ] 0<<1 P ∈P0   

Notice that if ζn is bounded, then Assumption S.12 corresponds to the requirement that
the cumulative distribution functions of UP,n (κun ) are uniformly (in n) continuous in a
neighborhood of their 1 − α quantiles. However, by allowing ζn to diverge, Assumption
S.12 also allows the distribution of UP,n (κun ) to become increasingly discontinuous.
The “rate” of this loss of continuity (ζn ) must be slower than the rate of convergence
of our stochastic approximation (δnc + δns ).
    We now state our final and main result on statistical inference.

Theorem 3. Suppose that Assumptions S.1–S.7 and S.9–S.11 are satisfied. Then, for
any sequence κun that satisfies Assumption S.8 and S.12,

                              lim sup sup P (Tn > ĉ1−α ) ≤ α.
                                n→∞ P ∈P0


Moreover, under the same assumptions,

                                      lim P (Tn > ĉ1−α ) = 1.
                                     n→∞

for any P ∈ P \ P0 .

    Theorem 3 says that our proposed test is consistent and controls size uniformly in
P ∈ P0 . We note that the class P0 over which size control is ensured is fairly broad. In
particular, the class P0 is such that the set of solutions {m ∈ M : kβP −ΓP (m)kB = 0}
cannot even be consistently estimated uniformly in P0 . We believe that this quality
is particularly important when M is defined by linear inequality constraints and the


                                                47
target parameter is partially identified. Moreover, we emphasize that we have not
placed any conditions that prohibit shape restrictions on M, other than requiring M
to be a convex set. As such, we believe Theorem 3 may be of independent interest
for applications other than the one studied in this paper. Finally, we note that in
establishing Theorem 3, we have not assumed that the data is i.i.d. Instead, we have
assumed that there is a bootstrap (Ĝβ , ĜΓ ) that is capable of handling the dependence
structure in the data. This allows for clustering and other types of dependence.


5.5.2      Bandwidth Guidance
Constructing the bootstrap statistic Tnbs requires specifying the bandwidths κ̂un , κ̂m
                                                                                      n , and
κ̂gn . While Assumptions S.8, S.9, and S.10 do not provide much direction in choosing
these bandwidths, it is clear that their values should be related to the distribution of the
data. In the following, we provide some guidance on the selection of these quantities.
In future work, we hope to formalize a data-driven procedure for determining these
bandwidths by applying the insights of Romano, Shaikh, and Wolf (2014). However,
at present we keep the discussion informal.
   The sequence κun and its feasible counterpart κ̂un were introduced in the derivation
of the distributional approximation in Theorem 1. The role of κun is related to

                                              √ n ∗                        o
                          b̂∗ ∈ arg max
                                    ∗
                                               n hb , β̂i − ν(b∗ , Γ̂(Mn )) ,                        (81)
                                      b ∈D


which represents the direction in which β̂ is farthest away from the set Γ̂(Mn ). In
particular, while κun is allowed to converge to zero, it must do so slowly enough for
                                                                   
                                                       ∗
                               lim inf inf P b̂ ∈           DP (κun )   = 1,                         (82)
                                n→∞ P ∈P0


where DP (κun ) is the set defined in (49). A bound for the probability in (82) is

                                                                                               √
                                                                                                       
                     ∗
lim inf inf P b̂ ∈       DP (κun )     ≥ lim inf inf P          sup kGP,β − GP,Γ (m)kB ≤           nκun       ,
 n→∞ P ∈P0                                  n→∞ P ∈P0         m∈Mn
                                                                                                     (83)
where we are assuming that           δns   = 0 for simplicity. This suggests taking   κ̂un   to be
                                                                              
           1
 κ̂un   = √   × inf c : P   sup kĜβ − ĜΓ (m)kB ≤ c|{Yi , Di , Zi }i=1 ≥ 1 − αn
                                                                    n
                                                                                   (84)
            n              m∈Mn


for some sequence αn ↓ 0.
   The bandwidth κ̂m
                   n was introduced in Assumption S.9 with the purpose of ensuring




                                                       48
      that M̂n contained the set {m ∈ M : kβP − ΓP (m)kB = 0} with probability tending
      to one. In analogy to (83), it is possible to show that
                                                                           
                    lim inf inf P Πn m ∈ M̂n for all m ∈ M s.t. ΓP (m) = βP
                     n→∞ P ∈P0
                                                                   √ m
                                                                       
                       ≥ lim inf inf P   sup kGP,β − GP,Γ (m)kB ≤ nκ̂n .                                    (85)
                              n→∞ P ∈P0      m∈Mn


      From (83) and (85), we see that the choices of κ̂un and κ̂m
                                                                n are closely related. For
      instance, given that κ̂un is selected according to (84), a simple rule is to let κ̂m
                                                                                         n =
                 √                             √
      κ̂un + Tn / n. Here, the addition of Tn / n to κ̂un ensures that M̂n 6= ∅.
          Lastly, recall that κ̂gn was introduced in the construction of neighborhoods for the
      adjoint Γ∗P : B∗ 7→ M∗ in the weak (operator) topology. A natural choice for κ̂gn is
                          (                                                             !              )
                 1                                ∗
       κ̂gn   = √ × inf   c:P       sup sup |hb , ĜΓ (v)i| ≤   c|{Yi , Di , Zi }ni=1       ≥ 1 − αn       , (86)
                  n                b∗ ∈D̂n v∈Vn

                                                          √
      where again αn ↓ 0. Together, (84), κ̂m     u
                                            n = κ̂n + Tn / n, and (86) provide a simple
      heuristic way to relate bandwidth selection to features of the distribution of the data.
      In Appendix G.4, we show that implementing these bandwidth choices amounts to
      solving a large number of small mixed integer linear programs. We leave a more
      detailed analysis of bandwidth selection to future work.

      6       The Efficacy of Price Subsidies for Bed Nets

      In this section, we apply our method to analyze how price subsidies affect the adoption
      and usage of a preventative health product.

      6.1      Background, Data, and Experiment
      According to the World Health Organization (WTO), approximately 5.9 million chil-
      dren under the age of 5 died in 2015. WTO estimates that a majority of these early
      childhood deaths could be prevented or treated if households were regularly using ex-
      isting health products, such as deworming medication, mosquito nets, water treatment
      solution, or latrines.18 An important, and largely unanswered, question for developing
      countries is how to design cost effective policies that promote access to (and usage of)
      preventive health products.
          While highly subsidizing health products has been shown to markedly increase
      access in developing countries, researchers and policymakers alike have expressed con-
18
     See http://www.who.int/mediacentre/factsheets/fs178/en/.


                                                      49
      cerns about the cost effectiveness of such policies (see e.g. Cohen and Dupas (2010)
      and Dupas and Zwane (2016)). One concern is the financial cost of subsidizing in-
      framarginal consumers who would have still purchased the product under a smaller
      subsidy. Another concern is that households that are unwilling to pay a monetary
      price for a product might also be unwilling to pay the non-monetary costs associated
      with using the product on a regular basis.19 On the other hand, charging a higher
      price to screen out non-users may exclude poor or credit-constrained individuals who
      would benefit from using the product.20
          The goal of our empirical analysis is to assess these tradeoffs by evaluating the
      effects of potential subsidy regimes on usage of a preventative health product, taking
      into account differences in subsidization costs across the regimes. Building on Dupas
      (2014), we use data from a randomized controlled experiment in Kenya that randomly
      assigned prices for a new and improved antimalarial bed net called the Olyset net.21
      The experimenters randomized prices across a total of 1,200 households in six villages.
      Households had a three month opportunity to purchase the Olyset net at their assigned
      subsidized price. Prices for the net varied from 0 to 250 Kenyan schillings (250 Ksh,
      or approximately $3.80), which is roughly twice the average daily wage for agricultural
      work in the area. Seventeen different prices were offered in total, but each area was
      assigned only four or five of these prices. For example, if an area was assigned the
      price set (Ksh 50, 100, 150, 200, 250), then all of the study households in the area were
      randomly assigned to one of these five prices. Price sets for every area included low,
      medium, and high prices. Only two areas had a price set that included free provision
      for some households.
          Two months after the experiment, Dupas (2014) collected data on household pur-
      chase and usage of the Olyset net.22 Usage was assessed by whether a household stated
      having started using the net, and whether the net was observed hanging above their
      bedding at the time of the visit. Table 1 in Dupas (2014) presents summary statistics of
      household characteristics and their correlation with the randomized price assignment.
      This table suggests that randomization was successful in making the price assignment
    19
       See, for example, Ashraf, Berry, and Shapiro (2010), who study the provision of chlorine solution for
water treatment. They find that higher prices tend to screen out those who use the product less.
    20
       For example, Dupas and Zwane (2016) study alternative screening mechanisms for the provision of
chlorine solution. They find that higher prices may prevent many households that would use the product
from obtaining it.
    21
       Dupas (2014) uses this data to examine how short-run subsidies affect long-run adoption through
learning. We refer to Dupas (2014) for a detailed discussion of the background, data, and experimental
design.
    22
       To study effects on long-run adoption, Dupas also conducted a one year follow-up. However, the
follow-up survey only included a subset of the villages. We therefore focus on the short-run effects.


                                                    50
             Figure 9: Impact of Price on the Household’s Purchase of Bed Net

                                     1.0
                                           ●
                                     0.8
                                               ●        ●
                   Share Purchased
                                     0.6
                                                   ●            ●●
                                                            ●




                                     0.4
                                                                                       ●


                                                                           ●
                                                                     ●         ●           ●
                                     0.2                                           ●
                                                                                                     ●●
                                                                                                 ●




                                                                                                           ●
                                     0.0

                                           0       50                100                   150       200   250
                                                                       Price (in Ksh)


 Notes: This figure plots purchase rates against prices. The size of the circles reflect the relative size of
the sample at each price point. The lines show predicted values from logit regressions of the households’
decision to purchase the bed net on the randomly assigned prices. The dashed lines indicate 90% confidence
intervals.


       independent of observable baseline characteristics.
          Figure 9 replicates Dupas’ experimental estimates of the impact of price on the
       household’s purchase decision. This figure plots purchase rates for the Olyset net
       against the price assignment. The sizes of the circles reflect the relative sample sizes
       at each price point. The lines indicate predicted values (and confidence intervals) from
       logistic regressions of the households’ purchase decisions on their randomly assigned
       prices. The demand function is quite steep. The likelihood of purchasing the bed net
       increases from .04 to over .23 as the price decreases from Ksh 250 to 150, reaching
       nearly .70 at Ksh 50.

       6.2   Evaluating a Class of Subsidy Regimes
       We use the randomly assigned prices as instruments, and use their exogenous variation,
       together with our method, to study the effectiveness of different subsidy regimes.23 As
       discussed in Section 3.1, we can use our method to compute bounds on PTREs that
       measure the causal effect on usage of one subsidy regime, a0 , relative to another subsidy
  23
     A possible threat to the exclusion restriction (Assumption I.2) is the psychological “sunk cost” effect,
whereby individuals who have paid more for a product feel more compelled to use it. However, recent
experiments conducted in developing countries find no evidence of a sunk cost effect in settings with health
products (Cohen and Dupas (2010) and Ashraf et al. (2010)).


                                                                           51
regime, a1 . For example, consider the effect of a policy that offers free provision
of the Olyset net to all households, compared to a policy under which households
have the option to buy the net at a given price. This comparison does not directly
correspond to the variation in prices induced by Dupas’ experiment, and is therefore not
point identified under standard instrumental variables assumptions. Nevertheless, our
method can be used to establish bounds, which we now show can be quite informative.
   In the notation of Section 2.1, Z is the randomly assigned price, D is an indicator
for whether the household purchases the Olyset net, and Y is an indicator for whether
the household uses the net. We consider PRTEs that contrast a regime a0 under which
the propensity score is constant at pa0 , to a regime a1 with a constant propensity score
of pa1 . In Table 3, we focus on two particular choices of pa0 and pa1 . The first choice is
pa0 = 0 and pa1 = 1, which can be thought of as the contrast between a regime a1 with
free provision of the Olyset set, and another regime a0 under which there is no access
to the Olyset net. In this case, the PRTE we consider is just the average treatment
effect (ATE). For the second choice, the a0 regime is still free provision, but in the a1
regime the Olyset net is offered at a price of Ksh 150, which is roughly the observed
market price a year after the experiment (Dupas, 2014). To implement this PRTE, we
use the estimated propensity score to predict pa0 and pa1 . As discussed in Section 3.2,
the PRTE in this case can be interpreted as the LATE from a hypothetical experiment
in which households are either freely provided an Olyset net or able to purchase one
at 150 Ksh.
   Table 3 demonstrates the way in which our method allows the researcher to trans-
parently substitute the strength of their assumptions with the strength of their con-
clusions. Comparing the bounds across columns clarifies how the strength of the con-
clusions (the width of the bounds) depends on two aspects. First, for a fixed set of
assumptions (indexed here by the Bernstein polynomial order, K), bounds based on a
broader class of IV-like estimates are substantially more informative. In the first five
columns, we restrict attention to the information contained in the IV estimand that
uses the propensity score p(Z) as an instrument for D. By comparison, the next five
columns demonstrate that both the OLS and IV estimands carry independent informa-
tion. The last five columns demonstrate that the bounds can be tightened considerably
by using richer specifications of the multi-valued price instrument.
   The other aspect that affects the strength of the conclusions is the set of maintained
assumptions on the MTR functions, m = (m0 , m1 ). In columns (1)-(4), (6)-(9) and
(11)-(14), we model m0 and m1 with Bernstein polynomials of order K for various




                                            52
                                Table 3: The Effects of Purchase on Net Usage
                                (1)     (2)        (3)         (4)   (5)     (6)     (7)     (8)     (9)     (10)    (11)    (12)    (13)    (14)    (15)
                                                                             Information Specification
      Intercept                  3       3          3          3      3       3     3      3        3         3       3       3       3        3      3
      Linear in p(Z)             3       3          3          3      3       3     3      3        3         3       3       3       3        3      3
      OLS                                                                     3     3      3        3         3       3       3       3        3      3
      1(Z ≤ 50)                                                                                                       3       3       3        3      3
      1(Z ≤ 150)                                                                                                      3       3       3        3      3
      Panel A.                                                       Population Average Treatment Effect
      K (polynomial order)       2       6         10          20    NP     2     6      10    20     NP              2       6       10      20     NP
      Bounds
      Lower                     .6521   .4646     .3857    .3275     .2533   .6521   .4956   .4700   .4537   .3954    ∅      .6365   .5602   .5269   .4487
      Upper                     .6772   .7269     .7362    .7445     .7515   .6521   .7269   .7362   .7445   .7515    ∅      .7104   .7178   .7229   .7253
      90% Confidence Interval
      Lower                     .5486   .3761     .2995    .2421             .4282   .4032   .3511   .3204           .5206   .4130   .3652   .3260
      Upper                     .7462   .8019     .8102    .8139             .7516   .8093   .8179   .8209           .7491   .7910   .7941   .7978
      Panel B.                                                  PRTE at Free Provision versus a Price of 150 Ksh
      K (polynomial order)       2       6         10          20   NP      2      6      10     20     NP      2             6       10      20     NP
      Bounds
      Lower                     .6600   .5881     .5626    .5444     .4817   .6600   .5881   .5626   .5444   .4856    ∅      .6758   .6506   .6214   .5573
      Upper                     .7049   .8140     .8469    .8817     .9732   .6600   .7085   .7172   .7275   .7941    ∅      .6895   .6988   .7140   .7492
      90% Confidence Interval
      Lower                     .5417   .5005     .4695    .4479             .3890   .3472   .3414   .3320           .5079   .4755   .4584   .4281
      Upper                     .7686   .9161     .9519    .9746             .7732   .9263   .9616   .9838           .7713   .9093   .9291   .9511
                                                                     Specifications of the IV-like Estimands
      Intercept                                s(d, z) = 1                               s(d, z) = 1                               s(d, z) = 1
      Linear in p(Z)                          s(d, z) = p(z)                           s(d, z) = p(z)                            s(d, z) = p(z)
      OLS                                                                                s(d, z) = d                               s(d, z) = d
      1(Z ≤ 50)                                                                                                               s(d, z) = 1(z ≤ 50)
      1(Z ≤ 150)                                                                                                             s(d, z) = 1(z ≤ 150)



Notes: This table reports bounds and 90% confidence intervals for the effects of purchase on usage of the
Olyset net. We estimate the propensity score, p, using the fitted logistic regression from Figure 9. K
denotes the order of the Bernstein polynomial specification for the MTR functions. The confidence intervals
are based on 200 bootstrap replicates, and the tuning parameters are specified as 0.05.




                                                                                53
      choices of K.24 In columns (5), (10), and (15), we use the Haar basis (constant spline)
      specification discussed in Section 2.7, which was shown in Proposition 4 to provide
      exact nonparametric bounds in the sample. As evident from Table 3, the stronger the
      restrictions, the tighter the bounds.
          At one extreme, it is possible to specify K to achieve point identification. As shown
      in Brinch et al. (2015), the information used in column (6)-(15) are sufficient to point
      identify the MTE (and hence any PRTE), provided that K = 2, so that the MTR
      functions are quadratic. For example, the results in column (6) imply a 65% usage
      rate among individuals induced to purchase by a change to free provision from a price
      of Ksh 150. However, the bounds for this specification are empty in column (11),
      suggesting that K = 2 might be too restrictive in our setting.
          At the other extreme, one can impose few or no restrictions on m0 and m1 , which
      yields wider, but more robust bounds. However, even with a very flexible specifica-
      tion, the bounds remain quite informative. For example, with a 10th order Bernstein
      polynomial (K = 10), the results in column (13) show that the ATE can be bounded
      between .56 and .72, whereas the PRTE of free provision compared to a price of Ksh
      150 is bounded between .65 and .70. These results imply that the usage rate in the
      overall population is relatively similar to that among individuals induced to purchase
      by a change to free provision from a price of Ksh 150.
          Figure 10 reports bounds on a range of PRTEs. Each PRTE takes pa0 to be
      the propensity score associated with a regime in which all households can purchase
      the bed net at a uniform price of Ksh 150.25 The alternative policy is specified by
      pa1 = pa0 + α. Panel A plots bounds on this PRTE as a function of α. The predicted
      price levels associated with a given α (predicted using the logistic regression in Figure
      9) are indicated in parentheses. To be conservative, we set K = 10 as in column (13)
      of Table 3, which allows a very flexible functional form for m0 and m1 .
          If households that are unwilling to pay larger monetary prices for the Olyset net
      are less likely to use the net after purchasing it, then we would expect to see the
      PRTEs declining in α. In contrast, Panel A of Figure 10 suggests that the PRTEs are
      increasing in α. This finding is consistent with higher prices excluding poor or credit
      constrained individuals who would use an Olyset net if they were able to purchase one.
      For example, among those induced to purchase the net by lowering prices from Ksh
      150 to Ksh 80, the usage rate is bounded between .57 and .62. By comparison, among
      the larger set of individuals induced to purchase by a change from Ksh 150 to free
      provision, the usage rate is bounded between .65 and .70.
24
     See Appendix F for a definition and discussion of the Bernstein polynomials.
25
     That is, pa0 = p̂(150), where p̂ is the estimated logistic model plotted in Figure 9.


                                                     54
                                                           Figure 10: Bounds on Policy Relevant Treatment Effects

                                                       1.0



                                                       0.8
              PRTE on Usage of Bed Net

                                                                                                                                                                                                                                ●●
                                                                                                                                                                                                      ●●●●●●●●●●●●●●       ●●●●
                                                                                                                                                                                               ●●●●                   ●●●●
                                                                                                                                                                                                                           ●●●●●●
                                                                                                                                                                               ●●        ●●●                    ● ●●●
                                                                                                                                                                                                              ●
                                                                                                                                                                           ●●●                          ●●●
                                                                                                                                                                                                            ●
                                                       0.6                                               ●●                                                                                        ●●●●
                                                                                                     ●●●    ●●●●
                                                                   ●●●●●●●●●●●●●●●●●●●●●●●●● ●●●●●●●●●
                                                                                               ● ● ●●●
                                                                                        ● ● ●●
                                                                                     ●●
                                                                                 ●●●
                                                                            ●●●●
                                                                   ●●●●●●●●


                                                       0.4



                                                       0.2



                                                       0.0

                                                             0 (150)                   0.099 (125)                                     0.216 (100)                         0.338 (75)                   0.453 (50)                     0.548 (25) 0.621 (0)
                                                                                                                                                                      α (Ksh)

                                                                                                               (a) PRTE: Effect on Usage Rates

                                                               ●
                                                                   ●
              PRTE on Usage of Bed Net per 1,000 Ksh




                                                       8               ●
                                                                           ●
                                                                               ●
                                                                                   ●
                                                                                       ●
                                                                                           ●
                                                       7       ●
                                                                                               ●
                                                                                                   ●
                                                                   ●                                   ●
                                                                       ●                                   ●
                                                                           ●                                   ●
                                                                               ●                                   ●
                                                                                   ●                                   ●
                                                                                       ●                                   ●
                                                       6                                   ●                                   ●
                                                                                               ●                                   ●
                                                                                                   ●                                   ●
                                                                                                       ●                                   ●
                                                                                                           ●●
                                                                                                                   ●●                          ●
                                                                                                                                                   ●
                                                                                                                           ●●                          ●
                                                                                                                                   ●●                      ●●
                                                                                                                                           ●●                   ●●
                                                                                                                                                   ●●                ●●●
                                                                                                                                                           ●●              ●●●
                                                       5                                                                                                        ●●
                                                                                                                                                                     ●●
                                                                                                                                                                                    ●●
                                                                                                                                                                                         ●●
                                                                                                                                                                          ●●                  ●●
                                                                                                                                                                                                   ●●
                                                                                                                                                                               ●●                       ●●
                                                                                                                                                                                    ●●                       ●●
                                                                                                                                                                                         ●●                       ●●
                                                                                                                                                                                              ●●
                                                                                                                                                                                                   ●●                  ●
                                                                                                                                                                                                        ●●                 ●
                                                                                                                                                                                                             ●●                ●
                                                                                                                                                                                                                  ●●               ●
                                                       4                                                                                                                                                               ●●              ●
                                                                                                                                                                                                                               ●●        ●
                                                                                                                                                                                                                                       ●●●●
                                                                                                                                                                                                                                           ●●●
                                                                                                                                                                                                                                             ●●●●
                                                                                                                                                                                                                                                ●●●●
                                                                                                                                                                                                                                                  ●
                                                                                                                                                                                                                                                    ● ●
                                                                                                                                                                                                                                                     ●
                                                                                                                                                                                                                                                       ●
                                                       3
                                                           0 (150)                 0.099 (125)                                     0.216 (100)                            0.338 (75)                    0.453 (50)                 0.548 (25) 0.621 (0)
                                                                                                                                                                     α (Ksh)

                                                                                                       (b) PRTE: Effects Relative to Costs

Notes: Panel A displays bounds on usage for a range of PRTEs. Each PRTE takes pa0 = p̂(150), where p̂ is
the fitted logistic regression from Figure 9. This corresponds to a policy regime under which all households
can purchase the Olyset bed net at a uniform price of Ksh 150. The alternative policy regime is pa1 = pa0 +α.
The x-axis shows how the PRTE varies with changes in α or prices in Ksh (z), where α and z are related
through α = p̂(z)−pa0 . Panel B divides the PRTE on usage by the PRTE on subsidy costs. The specification
of the IV-like estimands and Bernstein polynomial order correspond to column (13) in Panel B of Table 3.

                                                                                                                                                                     55
          Even if a subsidy regime with low prices leads to relatively high usage among those
      who purchase the Olyset net, it still comes at the cost of subsidizing inframarginal
      consumers who would have also purchased the net at a higher price. To compare the
      effects on usage to the costs of lowering prices, we divide the PRTE on usage by the
      PRTE on subsidy costs. Panel B of Figure 10 plots the results, which show that subsidy
      regimes with low prices or free provision induce relatively few individuals to use the
      Olyset net per Ksh spent. For example, if prices are lowered from Ksh 150 to 80, then
      each 1,000 Ksh in subsidy costs induces at least 4.70 households (lower bound) to use
      the bed net. By comparison, moving from a regime with a price of Ksh 150 to one
      with free provision induces at most 3.39 households (upper bound) to use the bed net
      for every 1,000 Ksh in subsidy costs.26

      7    Conclusion

      We proposed a method for using instrumental variables (IVs) to draw inference about
      treatment parameters other than the LATE. Our method uses the observation that
      both the IV estimand and many treatment parameters can be expressed as weighted
      averages of the same underlying marginal treatment effects. Since the weights are
      known or identified, this observation implies that knowledge of the IV estimand places
      some restrictions on the unknown marginal treatment response functions, and hence
      on the possible values of other treatment parameters of interest. We showed how
      to extract this information from a class of objects described as IV-like estimands,
      which includes the TSLS and OLS estimands, among others. An important aspect of
      our method is that it allows the researcher a large degree of flexibility in choosing a
      parameter of interest, and in choosing auxiliary identifying assumptions that can be
      used to help tighten their empirical conclusions. Another important feature is that it
      is computationally straightforward.
          We considered three main applications of our method. We showed that it can be
      used for counterfactual policy analysis, including extrapolation away from the variation
      in treatment induced by the instrument at hand. In addition, we showed that the
      general framework facilitates tests of both model specification and economic behavior.
      To implement our method, we developed a novel inference procedure that exploits the
      convexity of our setting, while remaining uniformly valid and computationally reliable
      under weak conditions. We applied our method to analyze how price subsidies affect
    26
       Of course, high subsidization or free provision of the Olyset net may still be optimal depending on its
effects on health outcomes. Unfortunately, we do not have the data needed to assess the effects on health
outcomes.



                                                     56
the adoption and usage of antimalarial bed nets in Kenya. Our results suggest that
generous subsidy regimes encourage usage among individuals who would otherwise
not use a bed net, albeit at a potentially high cost of subsidizing other inframarginal
individuals.
   The overall message from our analysis is that it is possible, both in theory and
in practice, to use IVs to draw informative inference about a wide range of treatment
parameters other than the LATE. This enables researchers to learn about causal effects
for a broad range of individuals, not just those who are affected by the instrument
observed in the data. The ability to do this is critical to ensuring that estimates
obtained through IV strategies are both externally valid and relevant to policy.




                                          57
A      Proofs for Section 2

Proof of Proposition 1. Using equation (1), we first note that

                       βs = E[s(D, Z)DY1 ] + E[s(D, Z)(1 − D)Y0 ].                 (87)

Using equation (2) with Assumptions I.1 and I.2, observe that the first term of (87)
can be written as

                    E[s(D, Z)DY1 ] = E[s(D, Z)1[U ≤ p(Z)]E[Y1 |U, Z]]
                                  ≡ E[s(1, Z)1[U ≤ p(Z)]m1 (U, X)],                (88)

where the first equality follows because s(D, Z)D is a deterministic function of (U, Z),
and the second equality uses the definition of m1 and I.2, together with the identity
that

       s(D, Z)1[U ≤ p(Z)] ≡ s (1[U ≤ p(Z)], Z) 1[U ≤ p(Z)] = s(1, Z)1[U ≤ p(Z)].

Using the normalization that U |Z is uniformly distributed on [0, 1] for any realization
of Z, it follows from (88) that

                E[s(D, Z)DY1 ] = E [E [s(1, Z)1[U ≤ p(Z)]m1 (U, Z)|Z]]
                                   Z 1                               
                               =E       s(1, Z)1[u ≤ p(Z)]m1 (u, Z) du
                                     0
                                   Z 1                       
                               ≡E       ω1s (u, Z)m1 (u, Z) du .
                                        0

The claimed result follows after applying a symmetric argument to the second term on
the right hand side of equation (87).                                           Q.E.D.

Proof of Proposition 2. Since Γs : M 7→ R is linear for every s ∈ S, it follows
from convexity of M that MS is convex as well. (Note that the empty set is trivially
convex.) If MS is empty, then by definition we also have BS? = ∅. On the other hand,
if MS 6= ∅, then the linearity of Γ? : M 7→ R implies that BS? ≡ Γ? (MS ) ⊆ R is a
                                                                            ?
convex set, and so its closure is [inf m∈MS Γ? (m), supm∈MS Γ? (m)] ≡ [β ? , β ]. Q.E.D.

Proof of Proposition 3. For notational simplicity, we define the set

               Mid ≡ {m ∈ M : m satisfies (17) and (18) almost surely}.



                                            58
For any m ≡ (m0 , m1 ) ∈ Mid and s ∈ S we obtain from the definition of βs that
                                        X
        βs = E[s(D, Z)E[Y |D, Z]] =             E[1[D = d]s(d, Z)E[Y |D = d, Z]].        (89)
                                      d∈{0,1}


Examining the d = 0 term in the summation, we obtain
                                        "                       Z 1               #
                                                           1
 E[1[D = 0]s(0, Z)E[Y |D = 0, Z]] = E 1[D = 0]s(0, Z)                m0 (u, X)du
                                                      1 − p(Z) p(Z)
                                                        Z 1                       
                                                   1
                                  = E 1[D = 0]               m0 (u, X)ω0s (u, Z)du
                                               1 − p(Z) 0
                                     Z 1                      
                                  =E      m0 (u, X)ω0s (u, Z)du ,                (90)
                                            0

where the first equality follows from m ∈ Mid satisfying (17), the second equality
uses the definition ω0s (u, z) = s(0, z)1{u > p(z)}, and the final equality is implied by
P (D = 0|Z) = 1 − p(Z). By analogous arguments, we also obtain
                                                    Z     1                        
          E[1[D = 1]s(1, Z)E[Y |D = 1, Z]] = E                 m1 (u, X)ω1s (u, Z)du .   (91)
                                                       0

Together, (89), (90), and (91) imply that Γs (m) = βs . In particular, since s ∈ S and
m ∈ Mid were arbitrary, we conclude Mid ⊆ MS as claimed.
   Next, suppose S = {s(d, z) = 1[d = d0 ]f (z) : (d0 , f ) ∈ {0, 1} × F} and that the
closed linear span of F is equal to L2 (Z). Then note that for any m ∈ MS and s ∈ S
with the structure s(d, z) = 1[d = 0]f (z) we obtain by definition of βs and Γs that
                                                       "Z                          #
                                                               1
           E[Y 1[D = 0]f (Z)] ≡ βs = Γs (m) = E                    m0 (u, X)duf (Z)      (92)
                                                           p(Z)


where the second equality follows from m ∈ MS and the final equality is due to
ω0s (u, z) ≡ 1{u > p(z)}s(0, z) and s(0, z) = 1[0 = 0]f (z). Furthermore, define
                                                   Z   1
                     ∆(Z) ≡ E[Y 1[D = 0]|Z] −                  m0 (u, X)du               (93)
                                                     p(Z)


and note that (92) implies that E[∆(Z)f (Z)] = 0 for all f ∈ F. Since E[Y02 ] < ∞ by
Assumption I.2 and E[ m2d (u, X)du] < ∞, Jensen’s inequality implies that ∆ ∈ L2 (Z).
                     R

Thus, since E[∆(Z)f (Z)] = 0 for all f ∈ F and the closed linear span of F equals
L2 (Z), we conclude that ∆(Z) = 0 almost surely. Equivalently, since P (D = 0|Z) =




                                            59
1 − p(Z) by definition of p(Z), we obtain whenever 1 − p(Z) > 0 that
                                                       Z   1
                                                1
                           E[Y |D = 0, Z] =                    m0 (u, X)du                 (94)
                                            1 − p(Z)   p(Z)


almost surely, i.e. m0 satisfies (17). Analogous arguments imply that m1 satisfies (18).
Since (m0 , m1 ) = m ∈ MS was arbitrary, we conclude that MS ⊆ Mid , which together
with (19) establishes (20).                                                              Q.E.D.

Proof of Proposition 4. We prove the proposition for the upper bound of the target
parameter. The proof for the lower bound follows by identical arguments.
                                                                                 ?   ?
   Observe that since Mfd ⊆ M, we can immediately conclude that β fd ≤ β . As for
the opposite inequality, let {z1 , . . . , zK } denote the discrete support of Z. Then notice
that for any (m0 , m1 ) = m ∈ M we have that
                                                           
                 XJ X
                    K
  Γds (md ) = E      1[U ∈ Aj , Z = zk ]md (U, X)ωds (U, Z)
                     j=1 k=1
                                                                             
               XJ X
                  K
            =E     E[md (U, X)|U ∈ Aj , Z = zk ]ωds (U, Z)1[U ∈ Aj , Z = zk ]
                     j=1 k=1
                                                                             
               XJ X
                  L
            =E     E[md (U, X)|U ∈ Aj , X = xl ]ωds (U, Z)1[U ∈ Aj , X = xl ] .
                     j=1 l=1

                                                                                           (95)

The first equality here follows from {Aj }Jj=1 being a partition of [0, 1]. The second
equality follows because ωds (u, z) is constant on each set {u ∈ Aj , z = zk } given the
assumption that 1[u ≤ p(z)] is constant Aj —recall Proposition 1. The third equality
in (95) follows from X being a subvector of Z = (Z0 , X).
   Given the definition of bjl (u, x) in (26), we can conclude from (95) that Γds (md ) =
Γds (Πmd ) for Πmd as defined in (27). An identical argument also yields Γ?d (md ) =
Γ?d (Πmd ). Hence, given the assumption that Πm ≡ (Πm0 , Πm1 ) ∈ Mfd , we have

                 ?
                β ≡ sup {Γ? (m) s.t. Γs (m) = βs for all s ∈ S}
                       m∈M

                     = sup {Γ? (Πm) s.t. Γs (Πm) = βs for all s ∈ S}
                       m∈M
                                                                             ?
                     ≤ sup {Γ? (m) s.t. Γs (m) = βs for all s ∈ S} ≡ β fd .                (96)
                       m∈Mfd

                       ?       ?
We conclude that β fd = β .                                                              Q.E.D.


                                              60
B    MTR Weights for Linear IV Estimands

In this appendix, we show that linear IV estimands are a special case of our notion of
an IV–like estimand. For the purpose of this discussion, we adopt some of the standard
textbook terminology regarding “endogenous variables” and “included” and “excluded”
instruments in the context of linear IV models without heterogeneity. Consider a linear
IV specification with endogenous variables X     e1 , included instruments Ze1 , and excluded
                         e ≡ [X
instruments Ze2 . We let X      e1 , Ze1 ] and Ze ≡ [Ze2 , Ze1 ]0 . We assume that both E[ZeZe0 ]
                                          0

        e 0 ] have full rank.
and E[ZeX
    As long as these two conditions hold, all of the variables in X
                                                                  e and Ze can be functions
of (D, Z). Usually, one would expect that X      e1 would include D and possibly some
interactions between D and other covariates X. The instruments, Z,        e would usually
consist of functions of the vector Z, which contains X, by notational convention. The
included portion of Z,e i.e. Ze1 , would typically also include a constant term as one
of its components. However, whether Ze is actually “exogenous” in the usual sense of
the linear instrumental variables model is not relevant to our definition of an IV–like
estimand or the derivation of the weighting expression (12). In particular, OLS is
nested as a linear IV specification through the case in which Ze1 = [1, D]0 and both X
                                                                                     e1
and Ze2 are empty vectors.
    It may be the case that Ze has dimension larger than X,
                                                         e as in “overidentified” lin-
ear models. In such cases, a positive definite weighting matrix Π is used to generate
instruments ΠZe that have the same dimension as X.      e A common choice of Π is the
two-stage least squares weighting ΠTSLS ≡ E[X    e Ze ]E[ZeZe0 ]−1 which has as its rows the
                                                     0

first stage coefficients corresponding to linear regressions of each component of X    e on
the entire vector Z.
                  e We assume that Π is a known or identified non-stochastic matrix
with full rank. This covers ΠTSLS and the optimal weighting under heteroskedasticity
(optimal GMM) as particular cases given standard regularity conditions. The instru-
mental variables estimator that uses ΠZe as an instrument for X
                                                              e in a regression of Y
on X
   e has corresponding estimand

                                  −1                      −1    
             βIV,Π          e 0]
                     ≡ ΠE[ZeX                 e           e e 0
                                           ΠE[ZY ] = E ΠE[Z X ]     ΠZY ,
                                                                     e


                                                  e 0 ])−1 ΠZ.
which is an IV–like estimand with s(D, Z) ≡ (ΠE[ZeX         e




                                               61
C     Proofs for Section 5

Proof of Theorem 1. First, we show in Lemma D.1 that

                                          √ n ∗                      o
                        Tn = sup           n hb , β̂i − ν(b∗ , Γ̂(M)) + Op (δns + δnc ),                         (97)
                                  b∗ ∈D


uniformly over P ∈ P. Next, recalling that D is the unit sphere in B∗ and DP (κun ) is
defined as in (49), we define the event
              "                                                                                                    #
                                   n                         o                          n                         o
Ωn (P ) ≡             sup           hb∗ , β̂i − ν(b∗ , Γ̂(M)) ≤             sup          hb∗ , β̂i − ν(b∗ , Γ̂(M)) .
                  b∗ ∈D\DP (κu
                             n)                                          b∗ ∈DP (κu
                                                                                  n)
                                                                                                                 (98)
For any b∗ ∈ B∗ , let

                                 √                        n                              o
                   ∆n (b∗ ) ≡          n hb∗ , β̂ − βP i − ν(b∗ , Γ̂(M)) − ν(b∗ , ΓP (M)) .

Observe that, since hb∗ , βP i − ν(b∗ , ΓP (M)) ≤ −κun for all b∗ ∈ D \ DP (κun ), we have
                                                                                                                  !
                                                  √                             √
 P (Ωn (P )c ) ≤ P             sup 2∆n (b∗ ) −        nκun >         sup            n {hb∗ , βP i − ν(b∗ , ΓP (M))}
                               b∗ ∈D                              b∗ ∈DP (κu
                                                                           n)

                                                   √
                                                             
                                             ∗
                    ≤P         sup 2∆n (b ) >          nκun                                                      (99)
                               b∗ ∈D


where Ωn (P )c denotes the complement of Ωn (P ) and in the final inequality we used
                           √
0 ∈ DP (κun ). Hence, since nκun ↑ ∞, (99) and Lemma D.2 yield

                                             lim sup sup P (Ωn (P )c ) = 0.                                     (100)
                                              n→∞ P ∈P


In addition, note that the definition of ν(b∗ , Γ̂(M)) and Assumption S.3 imply

                   √ n ∗                      o
        sup         n hb , β̂i − ν(b∗ , Γ̂(M))
     b∗ ∈DP (κu
              n)
                           n     √             √                    √                   o
 =      sup          inf    hb∗ , n{β̂ − βP } − n{Γ̂(m) − ΓP (m)}i + nhb∗ , βP − ΓP (m)i
     b∗ ∈DP (κu  m∈M
              n)
                            ∗                     √
 =      sup          inf    hb , GP,β − GP,Γ (m)i + nhb∗ , βP − ΓP (m)i + Op (δnc )                             (101)
     b∗ ∈DP (κu  m∈M
              n)


uniformly in P ∈ P. The first claim of Theorem 1 now follows from (97), (100), and
(101) together with δnc ↓ 0 and δns ↓ 0 from Assumptions S.3 and S.5.
     To establish the second claim, we note that if P ∈ P0 , then the set {m ∈ M :



                                                              62
√
    nhb∗ , βP − ΓP (m)i ≤ δns } cannot be empty for any b∗ ∈ B∗ , because for such P there
exists an mP ∈ M such that βP = ΓP (mP ). Hence, we conclude from (97), (100), and
(101) that

                                                                     √
                                         hb∗ , GP,β − GP,Γ (m)i +        nhb∗ , βP − ΓP (m)i + Op (δnc + δnc )
                                     
    Tn =         sup           inf
           b∗ ∈D         u     m∈M
                     P (κn )

       ≤ UP,n (κun ) + Op (δnc + δns ) + δns                                                              (102)

uniformly in P ∈ P0 , where the inequality follows by set inclusion and the definition
of UP,n (κun ). This implies the second claim of Theorem 1.                                             Q.E.D.

Proof of Lemma 5.1. Note that D̂n ⊆ D, M̂n ⊆ M, and |hb∗ , bi| ≤ kb∗ kB∗ kbkB ≤
kbkB for any b∗ ∈ D. These observations with Assumption S.6 imply

         sup          sup       hb∗ , Ĝβ − ĜΓ (m)i − hb∗ , Gbs
                                                              P,β − GP,Γ (m)i
                                                                     bs

        b∗ ∈D̂   n   m∈M̂n
                                              ∗
                 ≤ kĜβ − Gbs
                           P,β kB + sup sup hb , ĜΓ (m) − GP,Γ (m)i = Op (δn )
                                                            bs              c
                                                                                                          (103)
                                                b∗ ∈D m∈M


uniformly in P ∈ P. Next, define the event Ωn (P ) ≡ Ω1n (P ) ∩ Ω2n (P ) where
                          h               i
                Ω1n (P ) ≡ DP (κn ) ⊆ D̂n
                          h                                             i
            and Ω2n (P ) ≡ Πn mP ∈ M̂n for all mP ∈ M s.t. ΓP (mP ) = βP .

Observe that Lemmas D.4 and D.5 imply
                                                                                             
                                     c                                     c                    c
 lim sup sup P (Ωn (P ) ) ≤ lim sup                      sup P (Ω1n (P ) ) + sup P (Ω2n (P ) ) = 0. (104)
    n→∞ P ∈P0                                 n→∞        P ∈P                   P ∈P0


Moreover, {m ∈ M : ΓP (m) = βP } 6= ∅ for every P ∈ P0 . It follows that, for any
P ∈ P0 , if Ωn (P ) occurs, then it is also true that M̂n 6= ∅. In this event, Lemma
D.3 implies that for any b∗ ∈ B∗ there exists an m ∈ M̂n such that hb∗ , ΓP (m)i =
ν(b∗ , ΓP (M̂n )), so that In (ΓP ) is indeed well defined. Thus, (103) and (104) yield
                                            n
        In (ΓP ) = sup               inf     hb∗ , Gbs
                                                    P,β − GP,Γ (m)i
                                                           bs

                          b∗ ∈D̂n m∈M̂n
                                                                                     o
                                               s.t. hb∗ , ΓP (m)i = ν(b∗ , ΓP (M̂n )) + Op (δnc )         (105)

uniformly in P ∈ P0 .
     Next, note that, for any b∗ ∈ D, if m̂ ∈ M̂n satisfies hb∗ , ΓP (m̂)i = ν(b∗ , ΓP (M̂n )),
then it also satisfies hb∗ , ΓP (m̂ − m)i ≥ 0 for all m ∈ M̂n . Hence, when Ω2n (P ) is true,

                                                                63
we obtain

                  √                             √
                      nhb∗ , βP − ΓP (m̂)i ≤        nhb∗ , ΓP (mP ) − ΓP (Πn mP )i
                                                √
                                            ≤       nkΓP (mP − Πn mP )kB ≤ δns                (106)

for any b∗ ∈ D, any m̂ ∈ Mn satisfying hb∗ , ΓP (m̂)i = ν(b∗ , ΓP (M̂n )), and every
mP ∈ M such that ΓP (mP ) = βP . The second equality in (106) used hb∗ , bi ≤ kbkB
for all b∗ ∈ D, while the third inequality followed from Assumption S.5. Since under
Ω1n (P ) we have DP (κn ) ⊆ D̂n , (106) and Ωn (P ) ≡ Ω1n (P ) ∩ Ω2n (P ) imply that
whenever Ωn (P ) occurs
                               n                                                                o
 Ubs    u
  P,n (κn ) ≤ sup       inf     hb∗ , Gbs
                                       P,β − G bs
                                               P,Γ (m)i s.t. hb ∗
                                                                  , ΓP (m)i = ν(b∗
                                                                                   , ΓP ( M̂n ))  .
               b∗ ∈D̂n m∈M̂n
                                                                                              (107)
The result now follows from (107) together with (104) and (105).                            Q.E.D.

Proof of Lemma 5.2. Since {m ∈ M : βP = ΓP (m)} 6= ∅ for all P ∈ P0 , Lemma
D.5 implies that
                                    lim inf inf P (M̂n 6= ∅) = 1.                             (108)
                                    n→∞ P ∈P0

Given (108), we establish the claim by showing that (67) holds whenever M̂n 6= ∅.
   To this end, note that if M̂n 6= ∅, then by definition of the support function, an m ∈
M̂n satisfies hb∗ , ΓP (m)i = ν(b∗ , ΓP (M̂n )) if and only if it satisfies hb∗ , ΓP (m− m̃)i ≥ 0
for all m̃ ∈ M̂n . Equivalently, since hb∗ , ΓP (m)i = hΓ∗P (b∗ ), mi for all (b∗ , m) ∈ B∗ ×M,
we obtain by set inclusion that
                            n                                                                  o
 In (ΓP ) = sup       inf    hb∗ , Ĝβ − ĜΓ (m)i s.t. hΓ∗ (b∗ ), m − m̃i ≥ 0 for all m̃ ∈ M̂n
            b∗ ∈D̂n m∈M̂n
                                    n                                                o
         ≥ sup        sup     inf    hb∗ , Ĝβ − ĜΓ (m)i s.t. hΓ∗ (b∗ ), m − m̃i ≥ 0 .       (109)
            b∗ ∈D̂n m̃∈M̂n m∈M̂n


For any (b∗ , m̃) ∈ D̂n × M̂n , define

                        C(b∗ , m̃) ≡ {m ∈ M̂n : hΓ∗P (b∗ ), m − m̃i ≥ 0}.

Recall that Γ∗P (b∗ ) ∈ M∗ and that M̂n is compact in the weak topology by Lemma D.3.
It follows that, provided M̂n 6= ∅, there exists an mob∗ ∈ M̂n such that hΓ∗P (b∗ ), mob∗ i =
supm∈M̂n hΓ∗ (b∗ ), mi. Such an mob∗ satisfies C(b∗ , mob∗ ) ⊆ C(b∗ , m̃) for all m̃ ∈ M̂n .




                                                    64
Using set inclusion and hΓ∗P (b∗ ), mob∗ − m̃i ≥ 0 for all m̃ ∈ M̂n , we obtain

     sup       sup         inf {hb∗ , Ĝβ − ĜΓ (m)i s.t. hΓ∗ (b∗ ), m − m̃i ≥ 0}
    b∗ ∈D̂   n m̃∈M̂n
                      m∈M̂n

       = sup                 inf
                             o
                                         hb∗ , Ĝβ − ĜΓ (m)i                                            (110)
                          ∗
             b∗ ∈D̂n m∈C(b ,mb∗ )

       = sup             inf {hb∗ , Ĝβ − ĜΓ (m)i s.t. hΓ∗P (b∗ ), m − m̃i ≥ 0 for all m̃ ∈ M̂n }.
             b∗ ∈D̂   n
                        m∈M̂n


The claim now follows from (109) and (110).                                                             Q.E.D.

Proof of Theorem 2. For any b∗ ∈ B∗ , define the set

                       G̃n (b∗ ) ≡ {g ∈ M∗ : |hg − Γ̂∗ (b∗ ), vi| ≤ κ̂qn for all v ∈ Vn }.

and define

   T̃nbs ≡ sup                     sup           inf {hb∗ , Ĝβ − ĜΓ (m)i s.t. hg̃, m − m̃i ≥ 0}.       (111)
              b∗ ∈D̂                   ∗
                       n (g̃,m̃)∈G̃n (b )×M̂n
                                                m∈M̂n


Define the event Ωn (P ) ≡ Ω1n (P ) ∩ Ω2n (P ), where
                                          h                                       i
                                Ω1n (P ) ≡ Γ∗P (b∗ ) ∈ G̃n (b∗ ) for all b∗ ∈ D̂n                        (112)
                                          h             i
                                Ω2n (P ) ≡ Tnbs = T̃nbs .                                                (113)

We show that Ωn (P ) occurs with probability approaching 1, uniformly over P ∈ P0 .
   Using the definition of G̃n (b∗ ) and Assumption S.10 we have

     lim inf inf P (Ω1n (P ))
      n→∞ P ∈P
                                                                                            !
                                                   √                               √
        ≥ lim inf inf P                  sup sup |h n{Γ∗P (b∗ ) − Γ̂∗ (b∗ )}, vi| ≤ nκ̂gn
               n→∞ P ∈P
                                     b∗ ∈D̂n v∈Vn
                                                                                                !
                                                           √
        ≥ lim inf lim inf inf P                  sup sup |h n{Γ∗P (b∗ ) − Γ̂∗ (b∗ )}, vi| ≤ C       .    (114)
                C↑∞        n→∞ P ∈P
                                                b∗ ∈D̂n v∈Vn


Recall that under Assumption S.11, Vn ⊆ λ(Mn − mc ) for some mc ∈ Mn , while
Mn ⊆ M by Assumption S.5, and D̂n ⊆ D by construction. Using these inclusions,




                                                               65
we have

                            √
                   sup sup h n{Γ∗P (b∗ ) − Γ̂∗ (b∗ )}, vi
                b∗ ∈D̂n v∈Vn
                                            √
                    ≤ sup        sup       h n{Γ∗P (b∗ ) − Γ̂∗ (b∗ )}, λ(m1 − m2 )i
                       b∗ ∈D   m1 ,m2 ∈M
                                        √
                    ≤ sup sup 2|λ| hb∗ , n{Γ̂ − ΓP }(m)i
                       b∗ ∈D m∈M

                    = sup sup 2|λ| |hb∗ , GP,Γ (m)i| + Op (δnc )                              (115)
                       b∗ ∈D m∈M


where the second inequality follows from Γ̂∗ and Γ∗P being the adjoints of Γ̂ and ΓP
respectively, while the equality follows from Assumption S.3, and holds uniformly over
P ∈ P. Since |hb∗ , bi| ≤ kbkB for all b∗ ∈ D and b ∈ B, (115) implies that
                                                                                          !
                                                   √
            lim inf lim inf inf P        sup sup |h n{Γ∗P (b∗ ) − Γ̂∗ (b∗ )}, vi| ≤ C
             C↑∞     n→∞ P ∈P
                                        b∗ ∈D̂n v∈Vn
                                                                     
                                                          C
              ≥ lim inf inf P       sup 2|λ|kGP,Γ (m)kB ≤                 =1                  (116)
                   C↑∞ P ∈P         m∈M                   2

where we have applied Markov’s inequality with Assumption S.4. From (114), (116),
and Lemma D.6 we conclude that
                                                                                    
                          c                                    c                      c
 lim sup sup P (Ωn (P ) ) ≤ lim sup             sup P (Ω1n (P ) ) + sup P (Ω2n (P ) ) = 0. (117)
  n→∞ P ∈P0                        n→∞          P ∈P                 P ∈P0


    Next, we recall that Lemma 5.1 establishes that for any κun satisfying Assumption
        √
S.8 and nκun ↑ ∞, there exists a ξ˜nbs ∈ R satisfying ξ˜nbs = Op (δnc ) uniformly in P ∈ P0
such that
                                       Ubs    u                ˜bs
                                        P,n (κn ) ≤ In (ΓP ) + ξn                             (118)

for all P ∈ P0 . From Lemma 5.2 we can conclude that In (ΓP ) ≤ T̃nbs whenever the
event Ω1n (P ) occurs, whereas by definition Tnbs = T̃nbs whenever the event Ω2n (P )
occurs. Therefore, the claim of the result follow from (117), (118), and setting ξnbs =
ξ˜nbs + 1{{Yi , Zi }ni=1 ∈ Ωn (P )c }(In (ΓP ) − Tnbs ).                       Q.E.D.

Proof of Theorem 3. The proof of the first claim closely follows the arguments in
Lemma D.6 of Chernozhukov et al. (2015).
   First, note that Theorems 1 and 2 imply that

            Tn ≤ UP,n (κun ) + Op (δnc + δns )         and Ubs    u      bs       c
                                                            P,n (κn ) ≤ Tn + Op (δn ),        (119)



                                                       66
both uniformly over P ∈ P. Together with Markov’s inequality, this implies that for
any  > 0 and Cn ↑ ∞ we have
                                                                          
           lim sup sup P P Ubs (κ
                            P,n n
                                 u
                                   ) > Tn
                                         bs
                                            + C  δ
                                                n n
                                                   c
                                                     |{Yi , Di , Z }n
                                                                  i i=1   > 
             n→∞ P ∈P0
                                1  bs u                      
              ≤ lim sup sup       P UP,n (κn ) > Tnbs + Cn δnc = 0.                          (120)
                   n→∞ P ∈P0    

In particular, this implies that there is a sequence ηn ↓ 0 (which depends on Cn ↑ ∞)
such that the event
           h                                                                           i
  Ωn (P ) ≡ {Yi , Di , Zi }ni=1 : P Ubs (κ
                                     P,n n
                                          u
                                            ) > Tn
                                                  bs
                                                     + C  δ
                                                         n n
                                                            c
                                                              |{Yi , Di , Z }n
                                                                           i i=1   ≤ η n     (121)

satisfies supP ∈P0 P (Ωn (P )c ) = o(1). Furthermore, for any t ∈ R,
                                       
          P Tnbs ≤ t|{Yi , Di , Zi }ni=1 1 [{Yi , Di , Zi }ni=1 ∈ Ωn (P )]
                                                                                   
            ≤ P Tnbs ≤ t and Ubs         (κ
                                      P,n n
                                           u
                                              ) ≤ Tn
                                                    bs
                                                       +  C   δ
                                                             n n
                                                                c
                                                                  |{Yi , Di , Z } n
                                                                               i i=1 + ηn
                                             
            ≤ P Ubs        u                c
                    P,n (κn ) ≤ t + Cn δn + ηn ,                                             (122)

where the final inequality used the independence of Ubs    u                      n
                                                     P,n (κn ) and {Yi , Di , Zi }i=1 under
Assumption S.6.
   By evaluating (122) at t = ĉ1−α , we obtain
                                             
      P ĉ1−α + Cn δnc ≥ c1−α−ηn (UP,n (κun )) ≥ P {Yi , Di , Zi }ni=1 ∈ Ωn (P )
                                                                                
                                                                                             (123)

where used the equality in distribution of UP,n (κun ) and Ubs    u
                                                            P,n (κn ) under Assumption
S.6. Theorem 1, (123), and supP ∈P0 P (Ωn (P )c ) = o(1) then yield

      lim sup sup P (Tn > ĉ1−α )
       n→∞ P ∈P0

         ≤ lim sup sup P UP,n (κun ) + Cn (δns + δnc ) > ĉ1−α
                                                                   
             n→∞ P ∈P0

         ≤ lim sup sup P UP,n (κun ) > c1−α−ηn (UP,n (κun )) − 2Cn (δnc + δns ) .
                                                                               
                                                                                             (124)
             n→∞ P ∈P0


Notice, however, that because ζn (defined in Assumption S.12) satisfies ζn (δns + δnc ) =
o(1), we can select Cn ↑ ∞ slowly enough so that ζn Cn (δns + δnc ) = o(1). Since ηn ↓ 0,




                                                67
we conclude that for such a choice of Cn ,
                                                                                            
lim sup sup P c1−α−ηn (UP,n (κun )) ≥ UP,n (κun ) > c1−α−ηn (UP,n (κun )) − 2Cn (δnc + δns )
 n→∞ P ∈P0

  ≤ lim sup ζn × 2Cn (δnc + δns ) = 0.                                                             (125)
      n→∞

Combining (124) and (125) establishes the first claim, since
                                                                               
 lim sup sup P (Tn > ĉ1−α ) ≤ lim sup sup P UP,n (κun ) > c1−α−ηn (UP,n (κun )) ≤ α.
   n→∞ P ∈P0                             n→∞ P ∈P0
                                                                                                   (126)
   To establish the second claim, we first note that for any            b∗   ∈   B∗ ,   and any sequence
mn that converges (in the weak topology) to an m ∈ M, we have

                lim hb∗ , ΓP (mn ) − ΓP (m)i = lim hΓ∗P (b∗ ), mn − mi = 0,                        (127)
               n→∞                                      n→∞

which implies ΓP : M 7→ B is continuous when both M and B are equipped with their
respective weak topologies. Also, note that m 7→ kβP − ΓP (m)kB is lower semicon-
tinuous (with respect to the weak topologies), which follows because ΓP : M 7→ B is
continuous, and because the norm functional k·kB is lower semicontinuous with respect
to the weak topology in B (see Lemma 6.22 in Aliprantis and Border (2006)). Since
M is compact in the weak topology by Assumption S.1, we conclude that

               ∆0 ≡ inf kβP − ΓP (m)kB = min kβP − ΓP (m)kB > 0                                    (128)
                        m∈M                             m∈M


for any P ∈ P \ P0 .
   Next, observe that Lemmas D.1 and D.2 imply that for any P ∈ P \ P0

                               √ n ∗                      o
               Tn = sup         n hb , β̂i − ν(b∗ , Γ̂(M)) + op (1)
                       b∗ ∈D
                               √
                  = sup            n {hb∗ , βP i − ν(b∗ , ΓP (M))} + Op (1)
                       b∗ ∈D
                               √                                   √
                  = inf            nkβP − ΓP (m)kB + Op (1) =          n∆0 + Op (1),               (129)
                       m∈M


where the third equality holds by Theorem 5.13.1 in Luenberger (1969), and the final
equality in (129) follows from (128). On the other hand, for any C > 0 we have
                                                       1
     P (ĉ1−α > C) ≤ P P Tnbs > C|{Yi , Di , Zi }ni=1 > α ≤ P (Tnbs > C),                          (130)
                                                           α

where the first inequality follows by definition of ĉ1−α and the second by Markov’s


                                                   68
inequality. From D̂ ⊆ D, M̂n ⊆ M, and supb∗ ∈D hb∗ , bi = kbkB for any b ∈ B (see
Lemma 6.10 in Aliprantis and Border (2006)), and Assumption S.6, we obtain

                     Tnbs ≤ kĜβ kB + sup sup hb∗ , −ĜΓ (m)i
                                       b∗ ∈D m∈M
                                                ∗
                         = kGbs                      bs
                             P,β kB + sup sup hb , −GP,Γ (m)i + op (1)
                                           b∗ ∈D m∈M

                         = kGbs             bs
                             P,β kB + sup kGP,Γ (m)kB + op (1).                             (131)
                                           m∈M


Assumptions S.4 and S.6 together with (131) then imply that Tnbs = Op (1) under any
P ∈ P. We conclude from (130) that

                               lim sup lim sup P (ĉ1−α > C) = 0.                           (132)
                                C↑∞        n→∞


The second claim now follows from (129) and (132).                                        Q.E.D.

D    Proofs of Auxiliary Results

Lemma D.1. If Assumptions S.1, S.2, S.3, and S.5 hold, then

                                      √ n ∗
                                                               o             
                                                       ∗              c    s
 lim sup lim sup sup P        Tn − sup n hb , β̂i − ν(b , Γ̂(M)) > C(δn + δn ) = 0.
    C↑∞    n→∞ P ∈P                b∗ ∈D


Proof of Lemma D.1. Assumption S.5 implies that

           √                           √
     inf       nkβ̂ − Γ̂(m)kB ≤ inf        nkβ̂ − Γ̂(Πn m)kB                                (133)
    m∈Mn                         m∈M
                                       √                             √
                               ≤ inf       nkβ̂ − Γ̂(m)kB + sup          nkΓ̂(m) − Γ̂(Πn m)kB ,
                                 m∈M                           m∈M

where the first inequality follows from Πn m ∈ Mn for all m ∈ M, and the second
inequality applies the triangle inequality. Note that

                                       kbkB = sup hb∗ , bi                                  (134)
                                                 b∗ ∈D


for any b ∈ B; see, e.g., Lemma 6.10 in Aliprantis and Border (2006). Assumption S.3




                                                 69
with (134) imply

                              √
                      sup         nkΓ̂(m − Πn m) − ΓP (m − Πn m)kB
                      m∈M
                                           √
                            = sup sup hb∗ , n{Γ̂ − ΓP }(m − Πn m)i
                                  m∈M b∗ ∈D

                            = sup sup hb∗ , GP,Γ (m) − GP,Γ (Πn m)i + Op (δnc )                    (135)
                                  m∈M b∗ ∈D


uniformly in P ∈ P. Similarly, Assumption S.5 with (134) imply
                                                          
                                            ∗
                 sup E sup sup hb , GP,Γ (m) − GP,Γ (Πn m)i
                 P ∈P   m∈M b∗ ∈D
                                                            
                      = sup E sup kGP,Γ (m) − GP,Γ (Πn (m))kB = O(δns ).                           (136)
                            P ∈P       m∈M


Combining (133), (135), and (136) with Assumption S.5 and applying Markov’s in-
equality, we obtain

                      √                                   √
                inf       nkβ̂ − Γ̂(m)kB ≤ inf                nkβ̂ − Γ̂(m)kB + Op (δns + δnc )
               m∈Mn                                 m∈M
                                                           √
                                                 ≤ inf         nkβ̂ − Γ̂(m)kB + Op (δns + δnc )    (137)
                                                    m∈Mn


uniformly in P ∈ P, where the second inequality in (137) used Mn ⊆ M.
      Since Γ̂ : M 7→ B is continuous under Assumption S.2, we can define its adjoint
Γ̂∗   : B∗ 7→ M∗ , so that hb∗ , Γ̂(m)i = hΓ̂∗ (b∗ ), mi. Note that Γ̂∗ (b∗ ) ∈ M∗ , and therefore
hb∗ , Γ̂(m)i = hΓ̂∗ (b∗ ), mi implies m 7→ hb∗ , Γ̂(m)i is continuous in the weak topology.
Then, since M is compact in the weak topology under Assumption S.1, we obtain

                                            √
                      Tn = inf sup              nhb∗ , β̂ − Γ̂(m)i + Op (δnc + δns )
                              m∈M b∗ ∈D
                                            √
                          = sup inf nhb∗ , β̂ − Γ̂(m)i + Op (δnc + δns )
                              b∗ ∈D   m∈M
                               √ n                         o
                          = sup n hb∗ , β̂i − ν(b∗ , Γ̂(M)) + Op (δnc + δns ),                     (138)
                              b∗ ∈D


uniformly in P ∈ P, where the first equality follows from results (134) and (137),
the second equality results from Theorem 4.2 in Sion (1958), and the final equality is
implied by the definition of ν(b∗ , Γ̂(M)). The claim now follows from (138).                     Q.E.D.




                                                         70
Lemma D.2. If Assumptions S.3 and S.4 hold, then

      lim lim sup sup
      C↑∞ n→∞ P ∈P
                                                                                      !
                                        n                              o  C
        P       sup sup hb , β̂ − βP i − ν(b∗ , Γ̂(C)) − ν(b∗ , ΓP (C)) > √
                                  ∗
                                                                                          = 0.
                ∗
               b ∈D C⊆M                                                     n

Proof of Lemma D.2. Throughout the proof, we use (134) from Lemma D.1, see
e.g. Lemma 6.10 in Aliprantis and Border (2006). Together with Assumption S.3, this
implies that

                        √                      √
                sup      n|hb∗ , β̂ − βP i| = k n{β̂ − βP }kB = kGP,β kB + op (1)                (139)
                b∗ ∈D

uniformly in P ∈ P. Similarly,

                       √
       sup sup             n|ν(b∗ , Γ̂(C)) − ν(b∗ , ΓP (C))|
       b∗ ∈D   C⊆M
                              √
            = sup sup             n sup hb∗ , Γ̂(m)i − sup hb∗ , ΓP (m)i
               b∗ ∈D   C⊆M             m∈C              m∈C
                                   ∗   √
            ≤ sup sup |hb ,                n{Γ̂ − ΓP }(m)i| = sup kGP,Γ (m)kB + op (1),          (140)
               b∗ ∈D m∈M                                       m∈M

uniformly in P ∈ P, where the inequality follows from C ⊆ M, and the final equality
follows from Assumption S.3. From (139) and (140), we conclude that for any C > 0,
                                                                                                 !
                                                                               C
    lim sup sup P sup sup hb∗ , β̂ − βP i − {ν(b∗ , Γ̂(C)) − ν(b∗ , ΓP (C))} > √
      n→∞ P ∈P    b∗ ∈D C⊆M                                                      n
                                                    
                                                 C
       ≤ sup P kGP,β kB + sup kGP,Γ (m)kB >
         P ∈P              m∈M                   2

so that the result follows from Markov’s inequality with Assumption S.4.                    Q.E.D.

Lemma D.3. If Assumptions S.1, S.2, and S.7 hold, then M̂n is compact in the weak
topology. Moreover, if M̂n 6= ∅, then for all b∗ ∈ B∗
                           n                                           o
                            m ∈ M̂n : hb∗ , ΓP (m)i = ν(b∗ , ΓP (M̂n )) 6= ∅.

Proof of Lemma D.3. We note that {m ∈ M : kβ̂ − Γ̂(m)kB ≤ κ̂m
                                                            n } is closed in the
weak topology of M. This follows by Lemma 6.22 in Aliprantis and Border (2006),
which implies that {b ∈ B : kβ̂ − bkB ≤ κ̂m
                                          n } is closed in the weak topology of B,
and because Γ̂ : M 7→ B is continuous and linear under Assumption S.2. Hence,


                                                       71
{m ∈ M : kβ̂ − Γ̂(m)kB ≤ κ̂m
                           n } being closed, together with Assumption S.7 and


                            M̂n = Mn ∩ {m ∈ M : kβ̂ − Γ̂(m)kB ≤ κ̂m
                                                                  n },                                        (141)

shows that M̂n is closed in the weak topology, and therefore compact in the weak
topology, since M̂n ⊆ M and M is compact under Assumption S.1. Provided that
M̂n 6= ∅, it follows that for any b∗ ∈ B∗

                   ν(b∗ , ΓP (M̂n )) ≡ sup hb∗ , ΓP (m)i = max hΓ∗P (b∗ ), mi,                                (142)
                                          m∈M̂n                   m∈M̂n


where attainment in the final equality is guaranteed by the compactness of M̂n in the
weak topology and the continuity of m 7→ hΓ∗P (b∗ ), mi in the weak topology.                              Q.E.D.

Lemma D.4. Suppose that Assumptions S.3 and S.4 hold. Then

                                    lim inf inf P (DP (κun ) ⊆ D̂n ) = 1
                                     n→∞ P ∈P

                                                                                          √
for any non-random sequence κun that satisfies Assumption S.8 and                             nκun ↑ ∞.

Proof of Lemma D.4. For any b∗ ∈ B∗ and Mn ⊆ M, define

                            √                      n                                  o
               ∆n (b∗ ) ≡       n hb∗ , β̂ − βP i − ν(b∗ , Γ̂(Mn )) − ν(b∗ , ΓP (Mn )) .                      (143)

Using the definition of DP (κun ), and noting that Mn ⊆ M implies ν(b∗ , ΓP (Mn )) ≤
ν(b∗ , ΓP (M)) for all b∗ ∈ B∗ , we have

                                                                                          ∆n (b∗ )
              n                           o                                                       
   inf         hb∗ , β̂i − ν(b∗ , Γ̂(Mn )) ≥       inf        ∗                     ∗
                                                             hb , βP i − ν(b , ΓP (Mn )) − √
b∗ ∈DP (κu
         n)                                    b∗ ∈DP (κu
                                                        n)                                    n
                                                                      ∗
                                                                             
                                                                ∆n (b )
                                             ≥ − sup              √     + κun .               (144)
                                                   ∗       u
                                                  b ∈DP (κn )       n

The definition of D̂n , together with (144) and Lemma D.2, then implies that
                                                                                                                  !
                                                                                    ∆n (b∗ )
                                                                                                    
  lim inf inf P (DP (κun ) ⊆ D̂n ) ≥ lim inf inf P                sup                √       + κun       ≤ κ̂un
   n→∞ P ∈P0                                 n→∞ P ∈P0        b∗ ∈D       u
                                                                      P (κn )
                                                                                        n

                                          ≥ lim inf inf P ((1 + δ)κun ≤ κ̂un ) = 1,                           (145)
                                             n→∞ P ∈P0


where the final equality follows by Assumption S.8 for some δ > 0.                                         Q.E.D.




                                                    72
Lemma D.5. Suppose that Assumptions S.3–S.5 and S.9 hold. Then

        lim inf inf P (Πn mP ∈ M̂n for all mP ∈ M s.t. ΓP (mP ) = βP ) = 1.
         n→∞ P ∈P0


Proof of Lemma D.5. Note that if βP = ΓP (mP ), then by the triangle inequality


  kβ̂ − Γ̂(Πn mP )kB
     ≤ kβ̂ − βP kB + kΓP (mP ) − ΓP (Πn mP )kB + kΓP (Πn mP ) − Γ̂(Πn mP )kB . (146)

Using (134) from Lemma D.1 together with Assumption S.3, we have

                            √
                       sup k n{Γ̂ − ΓP }(m) − GP,Γ (m)kB = op (1),                             (147)
                       m∈M


uniformly over P ∈ P. Assumptions S.3 and S.5, together with (146) and (147), imply

                                 √
                     sup             nkβ̂ − Γ̂(Πn mP )kB
           mP ∈M:ΓP (mP )=βP

                 ≤           sup         {kGP,β kB + kGP,Γ (Πn mP )kB } + op (1),              (148)
                     mP ∈M:ΓP (mP )=βP


uniformly over P ∈ P. Then by Assumption S.9, (148), and the definition of M̂n ,

  lim inf inf P (Πn mP ∈ M̂n for all mP ∈ M s.t. ΓP (mP ) = βP )                               (149)
   n→∞ P ∈P0
                                                                                    √          !
                                                                                        nκ̂m
                                                                                           n
     ≥ lim inf inf P               sup         {kGP,β kB + kGP,Γ (Πn mP )kB } ≤                  .
        n→∞ P ∈P0          mP ∈M:ΓP (mP )=βP                                            2

Moreover, by Assumptions S.4, S.9, and Markov’s inequality,
                                                                     √
                                                                         nκ̂m
                                                                               
                                                                            n
           lim inf inf P     sup {kGP,β kB + kGP,Γ (m)kB } ≤
            n→∞ P ∈P0            m∈M                                     2
                                                               
               ≥ lim inf inf P sup {kGP,β kB + kGP,Γ (m)kB } ≤ C = 1.                          (150)
                  C↑∞ P ∈P0            m∈M


The result follows from (149) and (150).                                                  Q.E.D.

Lemma D.6. Suppose that Assumptions S.2–S.5, S.9, and S.11 hold. Define the set

               G̃n (b∗ ) ≡ {g ∈ M∗ : |hg − Γ̂∗ (b∗ ), vi| ≤ κ̂qn for all v ∈ Vn }




                                                73
and the statistic
                                                   n                                          o
      T̃nbs ≡ sup          sup            inf       hb∗ , Ĝβ − ĜΓ (m)i s.t. hg̃, m − m̃i ≥ 0 .
             b∗ ∈D̂n (g̃,m̃)∈G̃n (b∗ )×M̂n m∈M̂n


Then Tnbs = T̃nbs with probability tending to one, uniformly in P ∈ P0 .
Proof of Lemma D.6. Since {m ∈ M : βP = ΓP (m)} 6= ∅ for all P ∈ P0 , Lemma
D.5 implies that
                                   lim inf inf P (M̂n 6= ∅) = 1.                                   (151)
                                    n→∞ P ∈P0

Hence, the claim follows provided that Tnbs = T̃nbs whenever M̂n 6= ∅.
     To show this, note that because Mn ⊆ M, the restriction of any g ∈ M∗ to Mn is
a continuous linear functional on Mn . That is, for any g ∈ M∗ there exists a gn ∈ M∗n
such that hg − gn , mi = 0 for all m ∈ Mn . Since Vn ⊆ Mn by Assumption S.11,
and because ∅ 6= M̂n ⊆ Mn ⊆ Mn , it follows that for any g̃ ∈ G̃n (b∗ ) there exists a
g ∈ Ĝn (b∗ ) such that hg − g̃, mi = 0 for all m ∈ M̂n . As a consequence,
                                                n                                         o
    T̃nbs ≤ sup          sup            inf      hb∗ , Ĝβ − ĜΓ (m)i s.t. hg, m − m̃i ≥ 0 . (152)
           b∗ ∈D̂n (g,m̃)∈Ĝn (b∗ )×M̂n m∈M̂n


     Conversely, note that any gn ∈ M∗n is a continuous linear functional defined on
Mn , which is a subspace of M. By the Hahn-Banach Theorem (see e.g. Theorem 5.4.1
of Luenberger (1969)), there exists an extension g ∈ M∗ such that hgn − g, mi = 0
for all m ∈ Mn . As before, since Vn ⊆ Mn under Assumption S.11, and because
∅ 6= M̂n ⊆ Mn , we can conclude that for any g ∈ Ĝn (b∗ ) there exists a g̃ ∈ G̃n (b∗ ) such
that hg − g̃, mi = 0 for all m ∈ M̂n . Hence, we conclude that
                                                n                                         o
    T̃nbs ≥ sup          sup            inf      hb∗ , Ĝβ − ĜΓ (m)i s.t. hg, m − m̃i ≥ 0 . (153)
           b∗ ∈D̂n (g,m̃)∈Ĝn (b∗ )×M̂n m∈M̂n


     The result now follows from (152), (153), and the definition of Tnbs .                  Q.E.D.

E     Computation for Infinite B

Whenever B is infinite dimensional, i.e. we are employing an infinite number of IV-
like specifications S, the dual space B∗ is infinite dimensional as well. Computing
the bootstrap statistic Tnbs may be challenging in this situation. In this appendix, we
provide a supplemental result that establishes appropriate conditions under which Tnbs
can be approximated by a finite dimensional optimization problem when B∗ is infinite
dimensional.

                                                     74
   Let Dn ⊆ D be a finite dimensional subset of D and, in analogy to D̂n , define

                           D̃n ≡ {b∗ ∈ Dn : hb∗ , β̂i − ν(b∗ , Γ̂(Mn )) ≥ −κ̂un }.                   (154)

Heuristically, D̃n consists of the set of “directions” b∗ in the sieve Dn that are “close”
to binding. Notice the contrast to D̂n , which contains all such directions in D and
hence is potentially infinite dimensional. Define the bootstrap statistic

   Cnbs ≡ sup                 sup                inf {hb∗ , Ĝβ − ĜΓ (m)i s.t. hg, m − m̃i ≥ 0},    (155)
          b∗ ∈D̃   n   (g,m̃)∈Ĝn   (b∗ )×M̂   n
                                                 m∈M̂n


where M̂n and Ĝn (b∗ ) remain as defined in (61) and (68), respectively. That is, Cnbs is
identical to Tnbs , with the exception that D̂n has been replaced by D̃n .
   In the following, we show that a version of Theorem 2 continues to hold with Cnbs
in place of Tnbs . This implies that Cnbs is also a valid bootstrap statistic. To do this,
we impose the following conditions on the sieve Dn for D:

     Assumption U
     For every b∗ ∈ D there exists a Πn b∗ ∈ Dn ⊆ D such that

                                           √
                             sup sup            n|hb∗ − Πn b∗ , βP i| ≤ δnb ,
                             P ∈P b∗ ∈D
                                     "                                                        #
                                                               ∗        ∗
                             sup E               sup        |hb − Πn b , GP,β − GP,Γ (m)i| ≤ δnb ,
                             P ∈P         (b∗ ,m)∈D×M
                                                       √
                   and       sup          sup              n|hb∗ − Πn b∗ , ΓP (m)i| ≤ δnb ,
                             P ∈P     (b∗ ,m)∈D×M


     for some δnb ↓ 0.

   Assumption U places conditions on the sieve Dn that are analogous to those required
of Mn by Assumption S.5. We note that Assumption U imposes no constraints on the
rate of growth of Dn . Instead, Assumption U demands that the rate of growth of Dn be
sufficiently fast so that the approximation error introduced from optimizing over Dn in
place of D is asymptotically negligible. This is likely to be the case when computation
is the primary reason for using Dn over D. Our next result provides an analog to
Theorem 2 for situations when a sieve Dn is used in place of D.

Lemma E.1. Suppose that Assumptions S.1–S.7, S.9–S.11, and U hold. Then, for
                                                          √
any sequence κun that satisfies Assumption S.8, as well as nκun ↑ ∞, there exists a




                                                              75
sequence ξnbs ∈ R, with ξnbs = Op (δnc + δnb ) uniformly in P ∈ P0 , and such that

                                        Ubs    u      bs   bs
                                         P,n (κn ) ≤ Cn + ξn                                   (156)

for any P ∈ P0 .

Proof of Lemma E.1. We begin by defining an auxiliary lower bound CnL . Note
that since κ̂un satisfies Assumption S.8, there exists an η ∈ (0, 1) such that

                                                     ηκ̂un
                                                                    
                                                                   L
                               lim inf inf P               > (1 + δ ) = 1                      (157)
                                n→∞ P ∈P              κun

for some δ L > 0. Define

                       D̂nL ≡ {b∗ ∈ D : hb∗ , β̂i − ν(b∗ , Γ̂(Mn )) ≥ −ηκ̂un }
           and ĜnL (b∗ ) ≡ {g ∈ M∗n : |hg, vi − hb∗ , Γ̂(v)i| ≤ ηκ̂gn for all v ∈ Vn },

then let
                                                n                                         o
  CnL ≡ sup              sup           inf       hb∗ , Ĝβ − ĜΓ (m)i s.t. hg, m − m̃i ≥ 0 .   (158)
                 L (g,m̃)∈Ĝ L (b∗ )×M̂ m∈M̂n
           b∗ ∈D̂n          n          n



Note that Assumption S.10 is satisfied with ηκ̂gn replacing κ̂gn , and that (157) implies
that Assumption S.8 is also satisfied with ηκ̂un in place of κ̂un . Hence, from Theorem 2,
we can conclude that
                                        Ubs    u      L    L
                                         P,n (κn ) ≤ Cn + ξn ,                                 (159)

uniformly over P ∈ P0 , with ξnL = Op (δnc ).
   Next, note that because Dn ⊆ D by Assumption U, and since |hb∗ , bi| ≤ kbkB for
any b∗ ∈ D, we have

       sup sup |hb∗ − Πn b∗ , Ĝβ − ĜΓ (m)i − hb∗ − Πn b∗ , Gbs
                                                              P,β − GP,Γ (m)i|
                                                                     bs
      b∗ ∈D m∈M
                                           ∗
           ≤ 2kĜβ − Gbs
                      P,β kB + sup sup 2|hb , ĜΓ (m) − GP,Γ (m)i| = Op (δn ),
                                                         bs               c
                                                                                               (160)
                                    b∗ ∈D m∈M

uniformly over P ∈ P, where the final equality is due to Assumption S.6. Moreover,

                       sup sup |hb∗ − Πn b∗ , Gbs
                                               P,β − GP,Γ (m)i| = Op (δn )
                                                      bs               b
                                                                                               (161)
                      b∗ ∈D m∈M


uniformly in P ∈ P by Assumptions S.6, U, and Markov’s inequality. Together, (160),



                                                      76
(161) imply that
                                                     n                                            o
    CnL = sup             sup              inf        hΠn b∗ , Ĝβ − ĜΓ (m)i s.t. hg, m − m̃i ≥ 0
                  L (g,m̃)∈Ĝ L (b∗ )×M̂ m∈M̂n
            b∗ ∈D̂n          n          n

            + Op (δnc + δnb )                                                                               (162)

uniformly in P ∈ P.
   Define the event Ωn (P ) ≡ Ω1n (P ) ∩ Ω2n (P ), where
                                  h                                  i
                        Ω1n (P ) ≡ Πn b∗ ∈ D̃n for all b∗ ∈ D̂nL                                            (163)
                                  h                                        i
                        Ω2n (P ) ≡ ĜnL (b∗ ) ⊆ Ĝn (Πn b∗ ) for all b∗ ∈ D .                               (164)

Then note that by definition of D̃n and D̂nL , with η ∈ (0, 1), we obtain

  P (Ω1n (P ))
                                  n                                  o              
                       ∗   ∗              ∗                ∗                       u
     ≥ P inf      hΠn b − b , β̂i − ν(Πn b , Γ̂(Mn )) − ν(b , Γ̂(Mn )) ≥ −(1 − η)κ̂n
            b∗ ∈D
                                                               !
       ≥P         sup       |hΠn b∗ − b∗ , β̂ − Γ̂(m)i| ≤ (1 − η)κ̂un         .                             (165)
              (b∗ ,m)∈D×M


By Assumptions S.3, U, and Markov’s inequality we obtain

     sup       |hΠn b∗ − b∗ , β̂ − Γ̂(m)i|                                                                  (166)
(b∗ ,m)∈D×M

                                                                            δnc + δnb                  δnc + δnb
                                                                                                                
                       1
   ≤        sup       √ hΠn b∗ − b∗ , GP,β − GP,Γ (m)i + Op                    √            = Op          √
       (b∗ ,m)∈D×M      n                                                        n                          n

uniformly in P ∈ P. Since δnc ↓ 0 by Assumption S.3, δnb ↓ 0 by Assumption U, and κ̂un
                              √
satisfies Assumption S.8 with nκun ↑ ∞, we conclude from (165) and (166) that

                                       lim inf inf P (Ω1n (P )) = 1.                                        (167)
                                        n→∞ P ∈P


Similarly, the definitions of ĜnL (b∗ ) and Ĝn (Πn b∗ ), η ∈ (0, 1), and Vn ⊆ λ(M − M) yield
                                                                                                  
                                                 ∗     ∗
           P (Ω2n (P )) ≥ P         sup |hΠn b − b , Γ̂(v)i| ≤ (1 −     η)κ̂gn
                                                                         for all v ∈ Vn
                                b∗ ∈D
                                                                                  !
                                                      ∗   ∗            (1 − η) g
                        ≥P                sup   |hΠn b − b , Γ̂(m)i| ≤         κ̂n .                        (168)
                                    (b∗ ,m)∈D×M                          2λ




                                                        77
After arguments analogous to those in (166) (evaluated with β̂ = 0), (168) implies

                                        lim inf inf P (Ω2n (P )) = 1.                           (169)
                                        n→∞ P ∈P


Since Ωn (P ) ≡ Ω1n (P ) ∩ Ω2n (P ), results (167) and (169) establish that Ωn (P ) occurs
with probability tending to one, uniformly over P ∈ P.
    To conclude, observe that when Ωn (P ) occurs, we have
                                         n                                            o
     sup           sup            inf     hΠn b∗ , Ĝβ − ĜΓ (m)i s.t. hg, m − m̃i ≥ 0
          L (g,m̃)∈Ĝ L (b∗ )×M̂ m∈M̂n
    b∗ ∈D̂n          n          n
                                                n                                         o
         ≤ sup           sup             inf     hb∗ , Ĝβ − ĜΓ (m)i s.t. hg, m − m̃i ≥ 0 .    (170)
           b∗ ∈D̃n (g,m̃)∈Ĝn (b∗ )×M̂n m∈M̂n


The claim now follows from the definition of Cnbs by combining (159), (162), (169), and
(170).                                                                                         Q.E.D.

F    Bernstein Polynomials

The kth Bernstein basis polynomial of degree K is defined as
                                                                  !
                                                              K
                         bK
                          k    : [0, 1] 7→ R :   bK
                                                  k (u)   ≡           uk (1 − u)K−k
                                                              k

for k = 0, 1, . . . , K. A degree K Bernstein polynomial B is a linear combination of
these K + 1 basis polynomials:

                                                                  K
                                                                  X
                              B(u) : [0, 1] 7→ R : B(u) ≡               θk bK
                                                                            k (u),
                                                                  k=0

for some constants θ0 , θ1 , . . . , θK . As is well-known, any continuous function on [0, 1]
can be uniformly well approximated by a Bernstein polynomial of sufficiently high
order.
    The shape of B can be constrained by imposing linear restrictions on θ0 , θ1 , . . . , θK .
This computationally appealing property of the Bernstein polynomials has been noted
elsewhere by Chak, Madras, and Smith (2005), Chang, Chien, Hsiung, Wen, and Wu
(2007) and McKay Curtis and Ghosh (2011), among others. The following constraints
are especially useful in the current application. Derivations of these properties can be
found in Chang et al. (2007) and McKay Curtis and Ghosh (2011).
      Shape Constraints



                                                     78
S.1 Bounded below by 0: θk ≥ 0 for all k.
S.2 Bounded above by 1: θk ≤ 1 for all k.
S.3 Monotonically increasing: θ0 ≤ θ1 ≤ · · · ≤ θK .
S.4 Concave: θk − 2θk+1 + θk+2 < 0 for k = 0, . . . , K − 2.
    Each Bernstein basis polynomial is itself an ordinary degree K polynomial. The
coefficients on this ordinary polynomial representation (i.e. the power basis represen-
tation) can be computed by applying the binomial theorem:

                                            K
                                                                !       !
                                            X               K       i
                                 bK
                                  k (u) =         (−1)i−k                   ui .                              (171)
                                            i=k
                                                            i       k

Representation (171) is useful for computing the terms Γ?d (bdk ) and Γds (bdk ) that appear
in the finite-dimensional program (25). To see this note for example that with d = 1,
                                                           "                                              #
                       Z   1                                      Z                  p(Z)
      Γ1s (b1k ) ≡ E            b1k (u, Z)ω1s (u, Z) du = E s(1, Z)                           b1k (u, Z) du
                        0                                                          0

                                                                             R p(Z)
If b1k (u, Z) = b1k (u) is a Bernstein basis polynomial, then                  0        b1k (u) du can be com-
puted analytically through elementary calculus using (171). The result of this integral
is a known function of p(Z). The coefficient Γ1s (b1k ) is then simply the population
average of the product of this known function with s(0, Z), which is also known or
identified. Thus, no numerical integration is needed to compute or estimate the γdks
terms. This conclusion depends on the form of the weights, and may not hold for all
                ? , although it holds for all of the parameters listed in Table 1. When
target weights ωdk
it does not, one dimensional numerical integration can be used instead.

G     Implementation and Computation

In this appendix, we discuss computation for the sample analog bounds, the test statis-
tic, the bootstrap statistic, and our data-driven choices of tuning parameters for the
bootstrap statistic.

G.1    Estimating Bounds
Consider the finite dimensional problem (25). We assume throughout Appendix G that
Θ is polyhedral, so that it can be represented as

                                      Θ ≡ {θ ∈ Rdθ : Rθ ≤ q}                                                  (172)


                                                     79
for a known vector q ∈ Rdq and a known dq × (K0 + K1 ) dimensional matrix R, where
θ ≡ (θ0 , θ1 ), and dθ ≡ K0 + K1 . We also assume throughout Appendix G that Θ ⊂ Rdθ
is a bounded set.
     Let Γ̂?d (bdk ) and Γ̂ds (bdk ) denote estimators of Γ?d (bdk ) and Γds (bdk ), respectively.
For the target parameter, these can be constructed as, e.g.
                                              n    Z    1
                                        1X
                        Γ̂?d (bdk )   ≡                     bdk (u, Xi )ω̂d? (u, Zi ) dµ? (u),               (173)
                                        n           0
                                             i=1

where ω̂d? is an estimator of the known or identified weighting function, ω̂d? . Depending
on the choices of basis and target parameter, the integral can often be evaluated an-
alytically. An estimator analogous to (173) can also be constructed for each Γds (bdk ).
These require an estimator ω̂ds , which in turn requires an estimator for the propensity
score, p(z), and possibly the functions s(d, z) that define the IV–like estimand. Letting
ŝ(d, z) be the latter estimator, we then define

                                                             n
                                                       1X
                                            β̂s ≡         ŝ(Di , Zi )Yi
                                                       n
                                                            i=1

as an estimator of βs .
                                                            ?
     Given these estimators, the bound β fd can be estimated by solving the linear pro-
gram

                        K0                  K1
               ?
             ˆ
             β fd ≡ max
                        X
                                 ?
                           θ0k Γ̂0 (b0k ) +
                                            X
                                               θ1k Γ̂?1 (b1k )
                    θ≡(θ0 ,θ1 )
                                  k=1                           k=1
                                  K0
                                  X                              K1
                                                                 X
                       s.t.             θ0k Γ̂0s (b0k ) +              θ1k Γ̂1s (b1k ) = β̂s for all s ∈ S
                                  k=1                            k=1

                      and Rθ ≤ q.                                                                            (174)

Solving the analogous minimization problem yields an estimator of the lower bound,
 ?
β̂ fd . Both of these problems are linear programs with dθ ≡ K0 + K1 variables and
|S| + dq constraints.
     It is possible that no solution to (174) exists. This could be an indication that
no solution to the population problem (25) exists either, which would imply that the
model is misspecified. However, it could also be that (25) is feasible, but that (174)
is infeasible due to statistical error in the estimation of the Γds (bdk ) and βs terms.
Our results in Section 5 provide a formal statistical test of the null hypothesis that
the model is not misspecified. Those results also provide a procedure for building a


                                                                 80
confidence region for β ? . It is possible for this confidence region to be nonempty even
when (174) is infeasible.

G.2    Reformulation of the Test Statistic
We continue to assume that m has been parameterized by some finite dimensional
θ ∈ Θ, where Θ is polyhedral and characterized by the linear constraints Rθ ≤ q, as in
(172). We also assume that β is a finite dimensional parameter so that B = Rdβ .
   The test statistic, Tn , is defined by the optimization problem (35). The choice of
norm on B affects the nature of the objective in (35), and will affect both the objective
and constraints for the bootstrap statistic problem discussed in the next section. For
computational reasons, it turns out to be convenient to choose a norm k · kB for which
the unit ball is polyhedral. This suggests taking k · kB to be either the 1–norm or the
max–norm on Rdβ . For concreteness, we will take k · kB to be the 1–norm, so that
               Pdβ
kβ̂kB = kβ̂k1 ≡ l=1 |β̂l |.
   Given these choices, we can rewrite (35) as

                                √
                        Tn = min nkβ̂ − Γ̂θk1       s.t. Rθ ≤ q.                    (175)
                                θ

This problem can be reformulated as a linear program by introducing non-negative
slack variables, w+ , w− ∈ Rdβ . Specifically, it can be shown that (175) is equivalent to

                                           √ Pdβ +      −
                                                           
                        Tn = minθ,w+ ,w−    n     w
                                               l=1 l + w l
                                      s.t. Rθ ≤ q
                                                                                    (176)
                                           w+ − w− = β̂ − Γ̂θ
                                           w− , w+ ≥ 0.

For a discussion of this reformulation see e.g. Boyd and Vandenberghe (2004, pg. 294).

G.3    Reformulation of the Bootstrap Statistic
In this section, we show that the optimization problem that defines the bootstrap
statistic, i.e. (70), can be reformulated as a bilinear program. Throughout this section,
we assume that the researcher has selected values of the tuning parameters κ̂m     u
                                                                             n , κ̂n ,
and κ̂gn . In the next section, we discuss the computational aspects of our data-driven
recommendations for choosing these parameters.
   Consider the inner minimization problem in (70). We rewrite this problem here in




                                           81
terms of θ ∈ Θ̂ (replacing m ∈ M̂n ) as

                         bs
                        Tn,inner (b∗ , g, θ̃) ≡ minθ (Ĝβ − ĜΓ θ)0 b∗
                                                s.t. g 0 (θ − θ̃) ≥ 0
                                                                                            (177)
                                                        Rθ ≤ q
                                                        kβ̂ − Γ̂θk1 ≤ κ̂m
                                                                        n,


where we continue to use the notation κ̂m
                                        n despite having exchanged m’s for θ’s else-
where. We write this inner problem as a function of (b∗ , g, θ̃) to emphasize that these
variables are fixed from the outer maximization problem in (70). Using a similar idea
as in the reformulation (176) for the test statistic, we rewrite (177) as the following
linear program:

                   bs
                  Tn,inner (b∗ , g, θ̃) = minθ,w+ ,w−    (Ĝβ − ĜΓ θ)0 b∗
                                                 s.t. g 0 (θ − θ̃) ≥ 0
                                                         Rθ ≤ q
                                                                                            (178)
                                                         w+ − w− = β̂ − Γ̂θ
                                                         Pdβ +       −      m
                                                           l=1 wl + wl ≤ κ̂n
                                                         w− , w+ ≥ 0.

   As long as Θ̂n ≡ {θ ∈ Θ : kβ̂ − Γ̂θk1 ≤ κ̂m
                                             n } is nonempty, a feasible solution to
(178) can always be achieved by taking θ = θ̃ since θ̃ ∈ Θ̂n from the outer optimization
constraint in (70). We assume that κ̂m
                                     n has been chosen to be sufficiently large to ensure
                                                  √
this is the case, which simply requires κ̂m
                                          n ≥ Tn / n. Together with our assumption
that Θ is bounded, we can conclude that strong duality holds; see, e.g. Corollary 5.3.7
in Borwein and Lewis (2010). The dual of (178) can be shown to be

              bs
             Tn,inner (b∗ , g, θ̃) = maxσ Ĝ0β b∗ + q 0 σ1 + κ̂m       0         0
                                                               n σ2 − g θ̃σ3 + β̂ σ4
                                     s.t. σ1 , σ2 , σ3 ≤ 0
                                                                                            (179)
                                           R0 σ1 + Γ̂0 σ4 − gσ3 = −Ĝ0Γ b∗
                                           σ2 ≤ σ4,l ≤ −σ2        for l = 1, . . . , dβ ,

where σ = (σ10 , σ2 , σ3 , σ40 )0 with σ1 ∈ Rdq , σ2 ∈ R, σ3 ∈ R, and σ4 ∈ Rdβ . Given (179),




                                                82
we can now write (70) as a single maximization problem:

                 Tnbs = maxb∗ ,g,θ̃,σ Ĝ0β b∗ + q 0 σ1 + κ̂m       0         0
                                                           n σ2 − g θ̃σ3 + β̂ σ4
                                 s.t. σ1 , σ2 , σ3 ≤ 0
                                        R0 σ1 + Γ̂0 σ4 − gσ3 = −Ĝ0Γ b∗
                                        σ2 ≤ σ4,l ≤ −σ2         for l = 1, . . . , dβ .           (180)
                                        b∗ ∈ D̂n     g ∈ Ĝn (b∗ )
                                        Rθ̃ ≤ q
                                        kβ̂ − Γ̂θ̃k1 ≤ κ̂m
                                                         n.


Our next task is to reformulate (180) into a bilinear maximization problem. This
involves several steps.
   First, we reformulate the constraint b∗ ∈ D̂n . To this end, note that B∗ = Rdβ but
equipped with the max–norm k · k∞ , defined as kb∗ k∞ ≡ maxl |b∗l |. Hence, by definition
of D̂n (see (58)) the constraint b∗ ∈ D̂n is equivalent to

                      kb∗ k∞ ≤ 1       and β̂ 0 b∗ − ν(b∗ , Γ̂(Θ)) ≥ −κ̂un .                      (181)

The first constraint in (181) can be rewritten as the set of linear constraints −1 ≤ b∗l ≤ 1
for l = 1, . . . , dβ . To reformulate the second constraint in (181), we rephrase the
definition of a support function (see (38)) as a minimization problem by applying
strong duality; see, e.g., Corollary 5.3.7 in Borwein and Lewis (2010). That is,
                                                   !                                      !
                               maxθ ((b∗ )0 Γ̂)θ            minσ5 ≥0           q 0 σ5
            ν(b∗ , Γ̂(Θ)) =                            =                                      .
                                s.t.     Rθ ≤ q                s.t.      R0 σ5 = Γ̂0 b∗

Using the minimization form of the support function, we conclude that the second
constraint in (181) is equivalent to the existence of a σ5 ∈ Rdq such that

                  σ5 ≥ 0      and R0 σ5 = Γ̂0 b∗       and q 0 σ5 ≤ β̂ 0 b∗ + κ̂un .              (182)

Notice that the constraints in (182) are linear in the variables of optimization, which
now include the dual variables, σ5 .
   Second, we reformulate the constraint that g ∈ Gn (b∗ ). To implement this con-
straint, we take Vn = {±ej }dj=1
                              θ
                                 , where ej is the jth unit basis vector in Rdθ . Then,
from (69), g ∈ Gn (b∗ ) is equivalent to kg − (b∗ )0 Γ̂k∞ ≤ κ̂gn . As noted above, this max
norm constraint can also be written as a set of linear inequalities.




                                                83
   Third, we observe that the transpose of the equality constraint in (180) implies that

                                    σ3 g 0 = σ10 R + σ40 Γ̂ + (b∗ )0 ĜΓ .

We substitute this expression into the term g 0 θ̃σ3 = σ3 g 0 θ̃ in the objective of (180).
This allows us to rewrite the objective as

                   Ĝ0β b∗ + q 0 σ1 + κ̂m       0        0          ∗ 0           0
                                        n σ2 − σ1 Rθ̃ − σ4 Γ̂θ̃ − (b ) ĜΓ θ̃ + β̂ σ4 .      (183)

The benefit of this substitution is that, whereas the term σ3 γ 0 θ̃ in the objective of (180)
is the product of three variables of optimization, every term in (183) is the product of
at most two variables of optimization.
   Fourth, we recall the reformulation that we used on the inner problem to move from
(177) to (178). Here, it will be applied to the constraint kβ̂ − Γ̂θ̃k1 ≤ κ̂m
                                                                            n.
   Incorporating these four observations into (180), we reformulate the program as

                                           n σ2 − σ1 Rθ̃ − σ4 Γ̂θ̃ − (b ) ĜΓ θ̃ + β̂ σ4
           Tnbs = max Ĝ0β b∗ + q 0 σ1 + κ̂m       0        0          ∗ 0           0

     as a function of b∗ , g, θ̃, σ1 , σ2 , σ3 , σ4 , σ5 , w̃+ , w̃−
                     s.t. σ1 , σ2 , σ3 ≤ 0,          σ5 , w̃+ , w̃− ≥ 0
                           R0 σ1 + Γ̂0 σ4 − gσ3 = −Ĝ0Γ b∗
                           σ2 ≤ σ4,l ≤ −σ2             for l = 1, . . . , dβ .
                           −1 ≤    b∗l   ≤1    for l = 1, . . . , dβ
                                                                                             (184)
                           R0 σ5 = Γ̂0 b∗
                           q 0 σ5 ≤ β̂ 0 b∗ + κ̂un
                           −κ̂gn ≤ e0j (g − (b∗ )0 Γ̂) ≤ κ̂gn        for j = 1, . . . , dθ
                           Rθ̃ ≤ q
                           w̃+ − w̃− = β̂ − Γ̂θ̃
                           Pdβ +          −      m
                              l=1 w̃l + w̃l ≤ κ̂n .

This program is almost linear in both its objective and constraints. However, it does
contain the following terms that are bilinear in the sense of being the product of two
different variables of optimization: σ1 Rθ̃, σ40 Γ̂θ̃, (b∗ )0 ĜΓ θ̃, and gσ3 . As a result, (184)
is a bilinear programming problem. Despite being non-convex, bilinear programs like
these can be reliably solved to global optimality, see e.g. Tawarmalani and Sahinidis
(2005) and the references cited therein.




                                                        84
G.4    Reformulation of the Tuning Parameter Selection Problems
                                                                               √
We use (84) to choose κ̂un , as well as κ̂m                      m     u
                                          n , which we take as κ̂n = κ̂n + Tn / n. To
do this, we compute supm∈Mn kĜβ − ĜΓ (m)kB . In terms of the finite dimensional
framework used throughout this section, this problem can be written as

                            max kĜβ − Ĝβ θk1      s.t.   Rθ ≤ q.                         (185)
                             θ

This problem looks superficially similar to the reformulated test statistic problem,
(175), with the important difference that it is a maximization problem, rather than
a minimization problem. Since k · k1 is a convex function, this means that (185) is a
non-convex optimization problem.
   However, (185) can be reformulated as a mixed integer linear program (MILP) by
applying a fairly standard argument. The argument is based on the observation that
the nonlinearity of the objective comes from the absolute value function, i.e.:

                                             dβ                  
                                             X
                          kĜβ − Ĝβ θk1 ≡         e0l Ĝβ − Ĝβ θ ,
                                             l=1


where el are unit vectors in Rdβ . As a result, the objective can be linearized by
introducing dβ binary variables that indicate whether each absolute value is obtained
for a positive or negative number, together with dβ slack variables to stand in for the
magnitude of the absolute value itself. Specifically, (185) is equivalent to
                           Pdβ
             maxθ,ζ,σ,w      l=1 σl
                     s.t. Rθ ≤ q
                           ζl ∈ {0, 1}   for l = 1, . . . , dβ
                                                                                           (186)
                           w = Ĝβ − Ĝβ θ
                           wl ≤ σl ≤ wl + (1 − ζl )BigMl           for l = 1, . . . , dβ
                           −wl ≤ σl ≤ −wl + ζl BigMl             for l = 1, . . . , dβ

   In (186), ζ are binary variables, w is a definitional variable that helps with notation,
and BigMl are large numbers, referred to as “big M” parameters in the operations
research literature (e.g. pp. 136-137 of Schrijver (1998)). The big M parameters are
chosen by the researcher in such a way as to ensure that constraints in which they
enter are never binding when BigMl is multiplied by 1. It is important to note that
these parameters are not tuning parameters in the usual statistical sense. In particular,
while the choice of the big M parameters can impact the speed at which (186) is solved,
these parameters can (and should) always be chosen so as not to impact the optimal

                                              85
value of (186).
   To see how (186) reformulates (185), first note that we have set wl = e0l (Ĝβ − Ĝβ θ)
in (186) as a (notational) constraint. Next, observe that if wl ≥ 0, then wl ≤ σl ≤ −wl
is a contradiction, so the only feasible solution in this case is to take ζl = 1. However,
with ζl = 1, the constraints enforce σl = wl . Similarly, if wl ≤ 0, then the binary
constraints enforce any feasible solution to have ζl = 0 and hence σl = −wl . In both
cases, σl = |e0l (Ĝβ − Ĝβ θ)|, so that the objective in (186) is always identical to that in
(185).
   Using (86) to select κ̂gn involves solving a problem similar to (185), which can also
be reformulated as a MILP. In finite dimensions, and with the choice of Vn discussed
in the previous section, this problem can be written as

         max
          ∗
             k(b∗ )0 ĜΓ k∞      s.t. kb∗ k∞ ≤ 1          and ν(b∗ , Γ̂(Θ)) ≤ β̂ 0 b∗ + κ̂un .          (187)
          b

We have already shown how to reformulate the constraints in this problem as linear
constraints; recall (182). However, as with (185), the objective in (187) is a nonlinear,
convex function, so maximizing it subject to linear constraints is a non-convex problem.
   We reformulate (187) as the following MILP:

                    max σ1
      as a function of b∗ , σ1 , σ2 , w, π, ζ1 , ζ2
                     s.t. −1 ≤ b∗l ≤ 1            for l = 1, . . . , dβ
                              σ2 ≥ 0
                              q 0 σ2 ≤ β̂ 0 b∗ + κ̂un
                              R0 σ2 = Γ̂0 b∗
                                                                                                        (188)
                              ζ1,j , ζ2,j ∈ {0, 1}      for j = 1, . . . , dθ
                              w = (b∗ )0 ĜΓ
                              wj ≤ πj ≤ wj + (1 − ζ1,j )BigM1,j                 for j = 1, . . . , dθ
                              −wj ≤ πj ≤ −wj + ζ1,j BigM1,j                for j = 1, . . . , dθ
                              πj ≤ σ1 ≤ πj + (1 − ζ2,j )BigM2,j                 for j = 1, . . . , dθ
                              Pdθ
                                j=1 ζ2,j = 1


The justification of this reformulation is similar to the one discussed for (186). The
constraints involving the ζ1 binary variables ensure that πj is always the absolute value
of wj , which is constrained (defined) as the jth element of (b∗ )0 ĜΓ . The additional
constraints involving the ζ2 binary variables then ensure that σ1 is always the maximum
of πj , since dj=1
             Pθ
                   ζ2,j = 1 can be satisfied if and only if a single ζ2,j is equal to 1.



                                                        86
G.5    Some Notes on Computation
We have shown that the main optimization problems in our methodology can all be
reformulated as problems with well-understood properties for which there exist globally
optimal algorithms. Admittedly, this has taken a substantial amount of work. However,
once the theoretical reformulation work has been done once, it does not need to be
performed again by a practitioner. The reformulated problems can be implemented
directly and solved using appropriate software. We are in the process of developing a
software package that processes the necessary optimization problems in the background
without requiring additional input from the user.
   We summarize the computational steps involved in statistical inference. First, con-
struct consistent estimators of identified population quantities, as discussed in Section
G.1. The exact definition of the terms involved here will depend on the null hypothesis
of interest. Second, compute the test statistic, Tn , by solving (176). Third, solve the
MILP (186) one time each for a large number of bootstrap draws. Given a choice of
quantile αn , this provides a data-driven choice of κ̂un through (84), as well as a data-
driven choice of κ̂m     u
                   n = κ̂n + Tn . Fourth, solve the MILP (188) one time each for a large
number of bootstrap draws. This yields a data-driven choice of κ̂gn . Fifth, with all tun-
ing parameters selected, solve the bilinear maximization problem (184) one time each
for a large number of bootstrap draws. This provides the critical value ĉ1−α defined in
(79). The null hypothesis is then rejected at level α if Tn > ĉ1−α .
   In practice, we have found that the part of this procedure that takes the longest is
by far the bilinear program (184). The MILPs used to select the tuning parameters are
relatively small. Even though these programs must be solved a large number of times,
we have found that they can be solved extremely quickly using modern algorithms like
Gurobi (Gurobi Optimization, 2015), which is the software we use for this step in our
empirical application.

References
Abadie, A., J. Angrist, and G. Imbens (2002): “Instrumental Variables Estimates of
  the Effect of Subsidized Training on the Quantiles of Trainee Earnings,” Econometrica, 70,
  91–117. 18

Aliprantis, C. D. and K. C. Border (2006): Infinite Dimensional Analysis – A Hitch-
  hiker’s Guide, Berlin: Springer-Verlag. 35, 68, 69, 71

Andrews, D. W. and G. Soares (2010): “Inference for parameters defined by moment
  inequalities using generalized moment selection,” Econometrica, 78, 119–157. 38

Angrist, J. D. and I. Fernandez-Val (2013): “ExtrapoLATE-ing: External Validity



                                            87
  and,” in Advances in Economics and Econometrics: Volume 3, Econometrics: Tenth World
  Congress, Cambridge University Press, vol. 51, 401–. 6

Angrist, J. D., K. Graddy, and G. W. Imbens (2000): “The Interpretation of Instru-
  mental Variables Estimators in Simultaneous Equations Models with an Application to the
  Demand for Fish,” The Review of Economic Studies, 67, 499–527. 2

Angrist, J. D. and G. W. Imbens (1995): “Two-Stage Least Squares Estimation of Aver-
  age Causal Effects in Models with Variable Treatment Intensity,” Journal of the American
  Statistical Association, 90, 431–442. 2

Ashraf, N., J. Berry, and J. Shapiro (2010): “Can Higher Prices Stimulate Product Use?
  Evidence from a Field Experiment in Zambia,” American Economic Review, 100, 2383–2413.
  50, 51

Balke, A. and J. Pearl (1997): “Bounds on Treatment Effects From Studies With Imperfect
  Compliance,” Journal of the American Statistical Association, 92, 1171–1176. 5, 6, 22

Beresteanu, A. and F. Molinari (2008): “Asymptotic properties for a class of partially
  identified models,” Econometrica, 76, 763–814. 7

Bhattacharya, J., A. M. Shaikh, and E. Vytlacil (2012): “Treatment effect bounds:
  An application to SwanGanz catheterization,” Journal of Econometrics, 168, 223–243. 5

Bierens, H. J. (1990): “A consistent conditional moment test of functional form,” Economet-
  rica: Journal of the Econometric Society, 1443–1458. 15

Bontemps, C., T. Magnac, and E. Maurin (2012): “Set identified linear models,” Econo-
  metrica, 80, 1129–1155. 7

Borwein, J. and A. S. Lewis (2010): Convex analysis and nonlinear optimization: theory
  and examples, Springer Science & Business Media. 82, 83

Boyd, S. and L. Vandenberghe (2004): Convex optimization, Cambridge university press.
  81

Brinch, C. N., M. Mogstad, and M. Wiswall (2015): “Beyond LATE with a Discrete
  Instrument,” Journal of Political Economy, forthcoming. 5, 6, 15, 16, 54

Bugni, F., I. Canay, and X. Shi (2015): “Inference for functions of partially identified
  parameters in moment inequality models,” Tech. rep., cemmap working paper, Centre for
  Microdata Methods and Practice. 6

Byrd, R. H., J. Nocedal, and R. A. Waltz (2006): “KNITRO: An integrated package for
  nonlinear optimization,” in Large-scale nonlinear optimization, Springer, 35–59. 43

Canay, I. and A. Shaikh (2016): “Practical and theoretical advances in inference for partially
  identified models,” Tech. rep., cemmap working paper, Centre for Microdata Methods and
  Practice. 36

Carneiro, P., J. J. Heckman, and E. Vytlacil (2010): “Evaluating Marginal Policy
  Changes and the Average Effect of Treatment for Individuals at the Margin,” Econometrica,
  78, 377–394. 5, 16, 20



                                             88
Carneiro, P., J. J. Heckman, and E. J. Vytlacil (2011): “Estimating Marginal Returns
  to Education,” American Economic Review, 101, 2754–81. 5, 11, 16, 20

Chak, P. M., N. Madras, and B. Smith (2005): “Semi-nonparametric estimation with
  Bernstein polynomials,” Economics Letters, 89, 153–156. 78

Chamberlain, G. (2011): “Bayesian aspects of treatment choice,” The Oxford Handbook of
  Bayesian Econometrics, 11–39. 6

Chang, I.-S., L.-C. Chien, C. A. Hsiung, C.-C. Wen, and Y.-J. Wu (2007): “Shape
  restricted regression with random Bernstein polynomials,” in Lecture Notes–Monograph Se-
  ries, ed. by R. Liu, W. Strawderman, and C.-H. Zhang, Beachwood, Ohio, USA: Institute
  of Mathematical Statistics, vol. Volume 54, 187–202. 78

Chen, X. (2007): “Chapter 76 Large Sample Sieve Estimation of Semi-Nonparametric Mod-
  els,” in Handbook of Econometrics, ed. by J. J. Heckman and E. E. Leamer, Elsevier, vol.
  Volume 6, Part 2, 5549–5632. 38

Chernozhukov, V., S. Lee, and A. M. Rosen (2013): “Intersection bounds: estimation
  and inference,” Econometrica, 81, 667–737. 47

Chernozhukov, V., W. K. Newey, and A. Santos (2015): “Constrained conditional
  moment restriction models,” arXiv preprint arXiv:1509.06311. 6, 36, 66

Chesher, A. (2003): “Identification in Nonseparable Models,” Econometrica, 71, 1405–1441.
  2

Cohen, J. and P. Dupas (2010): “Free Distribution or Cost-Sharing? Evidence from a
  Randomized Malaria Prevention Experiment,” The Quarterly Journal of Economics, 125,
  1–45. 50, 51

Dupas, P., H. V. K. M. and A. P. Zwane (2016): “Targeting health subsidies through a
  non-price mechanism: A randomized controlled trial in Kenya,” Science, 353, 889–895. 50

Dupas, P. (2014): “ShortRun Subsidies and LongRun Adoption of New Health Products:
  Evidence From a Field Experiment,” Econometrica, 82, 197–228. 5, 50, 52

Fang, Z. and A. Santos (2014): “Inference on directionally differentiable functions,” arXiv
  preprint arXiv:1404.3763. 39

Florens, J. P., J. J. Heckman, C. Meghir, and E. Vytlacil (2008): “Identification
  of Treatment Effects Using Control Functions in Models With Continuous, Endogenous
  Treatment and Heterogeneous Effects,” Econometrica, 76, 1191–1206. 2

Gurobi Optimization, I. (2015): “Gurobi Optimizer Reference Manual,” . 25, 87

Heckman, J., J. L. Tobias, and E. Vytlacil (2003): “Simple Estimators for Treatment
  Parameters in a Latent-Variable Framework,” Review of Economics and Statistics, 85, 748–
  755. 6

Heckman, J. J. and R. J. Robb (1985): “Alternative methods for evaluating the impact of
  interventions,” in Longitudinal Analysis of Labor Market Data, ed. by J. J. Heckman and
  B. Singer, Cambridge University Press. 6



                                            89
Heckman, J. J. and S. Urzua (2010): “Comparing IV with structural models: What simple
  IV can and cannot identify,” Journal of Econometrics, 156, 27–37. 2

Heckman, J. J., S. Urzua, and E. Vytlacil (2006): “Understanding Instrumental Vari-
  ables in Models with Essential Heterogeneity,” Review of Economics and Statistics, 88, 389–
  432. 2

Heckman, J. J. and E. Vytlacil (2001a): “Policy-Relevant Treatment Effects,” The Amer-
  ican Economic Review, 91, 107–111. 3, 5, 20

——— (2005): “Structural Equations, Treatment Effects, and Econometric Policy Evaluation,”
 Econometrica, 73, 669–738. 2, 3, 5, 9, 11, 12, 16, 19, 21

Heckman, J. J. and E. J. Vytlacil (1999): “Local Instrumental Variables and Latent Vari-
  able Models for Identifying and Bounding Treatment Effects,” Proceedings of the National
  Academy of Sciences of the United States of America, 96, 4730–4734. 2, 3, 9, 16, 19

——— (2001b): “Instrumental Variables, Selection Models, and Tight Bounds on the Average
 Treatment Effect,” in Econometric Evaluations of Active Labor Market Policies in Europe,
 ed. by M. Lechner and F. Pfeiffer, Heidelberg and Berlin: Physica. 3, 5

——— (2001c): “Local Instrumental Variables,” in Nonlinear Statistical Modeling: Proceedings
 of the Thirteenth International Symposium in Economic Theory and Econometrics: Essays
 in Honor of Takeshi Amemiya, ed. by K. M. C Hsiao and J. Powell, Cambridge University
 Press. 3, 16

——— (2007a): “Chapter 70 Econometric Evaluation of Social Programs, Part I: Causal Mod-
 els, Structural Models and Econometric Policy Evaluation,” in Handbook of Econometrics,
 ed. by J. J. Heckman and E. E. Leamer, Elsevier, vol. Volume 6, Part 2, 4779–4874. 3

——— (2007b): “Chapter 71 Econometric Evaluation of Social Programs, Part II: Using the
 Marginal Treatment Effect to Organize Alternative Econometric Estimators to Evaluate So-
 cial Programs, and to Forecast their Effects in New Environments,” in Handbook of Econo-
 metrics, ed. by J. J. Heckman and E. E. Leamer, Elsevier, vol. Volume 6, Part 2, 4875–5143.
 2, 3

Huber, M. and G. Mellace (2014): “Testing Instrument Validity for LATE Identification
  Based on Inequality Moment Constraints,” Review of Economics and Statistics, 97, 398–411.
  6

Imbens, G. W. (2010): “Better LATE Than Nothing: Some Comments on Deaton (2009) and
  Heckman and Urzua (2009),” Journal of Economic Literature, 48, 399–423. 21

Imbens, G. W. and J. D. Angrist (1994): “Identification and Estimation of Local Average
  Treatment Effects,” Econometrica, 62, 467–475. 2, 3, 8, 21, 24, 29

Imbens, G. W. and C. F. Manski (2004): “Confidence intervals for partially identified
  parameters,” Econometrica, 72, 1845–1857. 4, 30

Imbens, G. W. and W. K. Newey (2009): “Identification and Estimation of Triangular
  Simultaneous Equations Models Without Additivity,” Econometrica, 77, 1481–1512. 2

Imbens, G. W. and D. B. Rubin (1997): “Estimating Outcome Distributions for Compliers
  in Instrumental Variables Models,” The Review of Economic Studies, 64, 555–574. 6, 22


                                             90
Kaido, H., F. Molinari, and J. Stoye (2016): “Confidence intervals for projections of
  partially identified parameters,” arXiv preprint arXiv:1601.00934. 7

Kaido, H. and A. Santos (2014): “Asymptotically efficient estimation of models defined by
  convex moment inequalities,” Econometrica, 82, 387–413. 7

Kirkeboen, L., E. Leuven, and M. Mogstad (2016): “Field of Study, Earnings and Self-
  Selection,” The Quarterly Journal of Economics, 131, 1057–1111. 2

Kitagawa, T. (2009): “Identification Region of the Potential Outcome Distributions under
  Instrument Independence,” Cemmap working paper. 5

——— (2015): “A Test for Instrument Validity,” Econometrica, 83, 2043–2063. 6, 22

Koltchinskii, V. I. (1994): “Komlos-Major-Tusnady approximation for the general empirical
  process and Haar expansions of classes of functions,” Journal of Theoretical Probability, 7,
  73–118. 36

Kowalski, A. (2016): “Doing More When You’re Running LATE: Applying Marginal Treat-
  ment Effect Methods to Examine Treatment Effect Heterogeneity in Experiments,” NBER
  Working paper 22363. 16

Lee, S. and B. Salanié (2016): “Identifying Effects of Multivalued Treatments,” Working
  paper. 2

Luenberger, D. G. (1969): Optimization by vector space methods, John Wiley & Sons. 33,
  68, 74

Machado, C., A. M. Shaikh, and E. J. Vytlacil (2013): “Instrumental Variables and
 the Sign of the Average Treatment Effect,” Working paper. 6

Maestas, N., K. J. Mullen, and A. Strand (2013): “Does Disability Insurance Receipt
 Discourage Work? Using Examiner Assignment to Estimate Causal Effects of SSDI Receipt,”
 The American Economic Review, 103, 1797–1829. 11

Manski, C. (1994): “The selection problem,” in Advances in Econometrics, Sixth World
 Congress, vol. 1, 143–70. 5

Manski, C. F. (1989): “Anatomy of the Selection Problem,” The Journal of Human Resources,
 24, 343–360. 5

——— (1990): “Nonparametric Bounds on Treatment Effects,” The American Economic Re-
 view, 80, 319–323. 5

——— (1997): “Monotone Treatment Response,” Econometrica, 65, 1311–1334. 5, 16

——— (2003): Partial identification of probability distributions, Springer. 5

Manski, C. F. and J. V. Pepper (2000): “Monotone Instrumental Variables: With an
 Application to the Returns to Schooling,” Econometrica, 68, 997–1010. 5, 16

——— (2009): “More on monotone instrumental variables,” Econometrics Journal, 12, S200–
 S216. 5



                                             91
Masten, M. A. (2015): “Random Coefficients on Endogenous Variables in Simultaneous
 Equations Models,” cemmap working paper 25/15. 2

Masten, M. A. and A. Torgovitsky (2016): “Identification of Instrumental Variable Cor-
 related Random Coefficients Models,” The Review of Economics and Statistics, forthcoming.
 2

McCormick, G. P. (1976): “Computability of global solutions to factorable nonconvex pro-
 grams: Part IConvex underestimating problems,” Mathematical programming, 10, 147–175.
 43

McKay Curtis, S. and S. K. Ghosh (2011): “A variable selection approach to monotonic
 regression with Bernstein polynomials,” Journal of Applied Statistics, 38, 961–976. 78

Mourifié, I. (2015): “Sharp bounds on treatment effects in a binary triangular system,”
 Journal of Econometrics, 187, 74–81. 5

Mourifié, I. and Y. Wan (2016): “Testing Local Average Treatment Effect Assumptions,”
 The Review of Economics and Statistics, 99, 305–313. 6

Rio, E. (1994): “Local invariance principles and their application to density estimation,”
  Probability Theory and Related Fields, 98, 21–45. 36

Romano, J. P. and A. M. Shaikh (2008): “Inference for identifiable parameters in partially
  identified econometric models,” Journal of Statistical Planning and Inference, 138, 2786–
  2807. 6

——— (2012): “On the uniform asymptotic validity of subsampling and the bootstrap,” The
 Annals of Statistics, 40, 2798–2822. 47

Romano, J. P., A. M. Shaikh, and M. Wolf (2014): “A Practical Two-Step Method for
  Testing Moment Inequalities,” Econometrica, 82, 1979–2002. 48

Schrijver, A. (1998): Theory of linear and integer programming, John Wiley & Sons. 85

Shaikh, A. M. and E. J. Vytlacil (2011): “Partial Identification in Triangular Systems of
  Equations With Binary Dependent Variables,” Econometrica, 79, 949–955. 5

Sion, M. (1958): “On general minimax theorems,” Pacific J. Math, 8, 171–176. 70

Stinchcombe, M. B. and H. White (1998): “Consistent specification testing with nuisance
  parameters present only under the alternative,” Econometric theory, 14, 295–325. 15, 34

Tawarmalani, M. and N. V. Sahinidis (2005): “A polyhedral branch-and-cut approach to
  global optimization,” Mathematical Programming, 103, 225–249. 43, 84

Torgovitsky, A. (2015): “Identification of Nonseparable Models Using Instruments With
  Small Support,” Econometrica, 83, 1185–1197. 2

Vytlacil, E. (2002): “Independence, Monotonicity, and Latent Index Models: An Equiva-
  lence Result,” Econometrica, 70, 331–341. 3, 8




                                            92
