                              NBER WORKING PAPER SERIES




   POORLY MEASURED CONFOUNDERS ARE MORE USEFUL ON THE LEFT THAN
                          ON THE RIGHT

                                           Zhuan Pei
                                      Jörn-Steffen Pischke
                                       Hannes Schwandt

                                      Working Paper 23232
                              http://www.nber.org/papers/w23232


                     NATIONAL BUREAU OF ECONOMIC RESEARCH
                              1050 Massachusetts Avenue
                                Cambridge, MA 02138
                                    March 2017




We thank Suejin Lee for excellent research assistance and Alberto Abadie, Josh Angrist, Matias
Cattaneo, Bernd Fitzenberger, Brigham Frandsen, Daniel Hungerman, Francesca Molinari, Pedro
Souza, and participants at various seminars and conferences for helpful comments. The views
expressed herein are those of the authors and do not necessarily reflect the views of the National
Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.

© 2017 by Zhuan Pei, Jörn-Steffen Pischke, and Hannes Schwandt. All rights reserved. Short
sections of text, not to exceed two paragraphs, may be quoted without explicit permission
provided that full credit, including © notice, is given to the source.
Poorly Measured Confounders are More Useful on the Left Than on the Right
Zhuan Pei, Jörn-Steffen Pischke, and Hannes Schwandt
NBER Working Paper No. 23232
March 2017
JEL No. C31,C52

                                           ABSTRACT

Researchers frequently test identifying assumptions in regression based research designs (which
include instrumental variables or difference-in-differences models) by adding additional control
variables on the right hand side of the regression. If such additions do not affect the coefficient of
interest (much) a study is presumed to be reliable. We caution that such invariance may result
from the fact that the observed variables used in such robustness checks are often poor measures
of the potential underlying confounders. In this case, a more powerful test of the identifying
assumption is to put the variable on the left hand side of the candidate regression. We provide
derivations for the estimators and test statistics involved, as well as power calculations, which can
help applied researchers interpret their findings. We illustrate these results in the context of
various strategies which have been suggested to identify the returns to schooling.


Zhuan Pei                                                Hannes Schwandt
Dept. of Policy Analysis and Management                  University of Zurich
134 Martha Van Rensselaer Hall                           Department of Economics
Cornell University                                       Schönberggasse 1
Ithaca, NY 14853-4401                                    8001 Zürich, CH
peizhuan@gmail.com                                       and IZA
                                                         hannes.schwandt@uzh.ch
Jörn-Steffen Pischke
CEP
London School of Economics
Houghton Street
London WC2A 2AE
UNITED KINGDOM
and IZA
and also NBER
s.pischke@lse.ac.uk
1     Introduction
The identification of causal effects depends on explicit or implicit assumptions
which typically form the core of a debate about the quality and credibility of a
particular research design. In regression based strategies, this is the claim that
variation in the regressor of interest is as good as random after conditioning on
a sufficient set of control variables. In instrumental variables models it involves
the assumption that the instrument is as good as randomly assigned. In panel
or differences-in-differences designs it is the parallel trends assumption, pos-
sibly after suitable conditioning. The credibility of a design can be enhanced
when researchers can show explicitly that potentially remaining sources of se-
lection bias have been eliminated. This is often done through some form of
balancing tests or robustness checks.
    The research designs mentioned above can all be thought of as variants of
regression strategies. If the researcher has access to a variable for a potentially
remaining confounder, tests for the identifying assumption take two canonical
forms. The variable can be added as a control on the right hand side of the
regression. The identifying assumption is confirmed if the estimated causal
effect of interest is insensitive to this variable addition—we call this the coef-
ficient comparison test. Alternatively, the variable can be placed on the left
hand side of the regression instead of the outcome variable. A zero coefficient
on the causal variable of interest then confirms the identifying assumption.
This is the balancing test which is typically carried out using baseline char-
acteristics or pre-treatment outcomes in a randomized trial or in a regression
discontinuity design.
    Researchers often rely on one or the other of these tests. The main point of
our paper is to show that the balancing test, using the proxy for the candidate
confounder on the left hand side of the regression, is generally more powerful.
This is particularly the case when the available variable is a noisy measure
of the true underlying confounder. The attenuation due to measurement error
often implies that adding the candidate variable on the right hand side as a
regressor does little to eliminate any omitted variables bias. The same mea-


                                        1
surement error does comparatively less damage when putting this variable on
the left hand side. Regression strategies work well in finding small but relevant
amounts of variation in noisy dependent variables.
   These two testing strategies are intimately related through the omitted
variables bias formula. The omitted variables bias formula shows that the
coefficient comparison test involves two regression parameters, the coefficient
from the balancing test and the coefficient from the added regressor in the
outcome equation. If the researcher has a strong prior that the added regressor
ought to matter for the outcome under study, then the balancing test will
provide the remaining information necessary to assess the research design.
This maintained assumption is the ultimate source of the superior power of the
balancing test. However, we show that quantitatively meaningful differences
emerge particularly when there is some substantial amount of measurement
error in the added regressor. We derive the relevant parameters in the presence
of measurement error in Section 3.
   Of course, sometimes researchers may be more agnostic about whether the
added regressor matters for the outcome. In case it does not matter, rejecting
balance for this variable is of no consequence for this particular research design.
In this view, only the coefficient comparison test is really relevant while the
balancing test provides no additional information. However, this strikes us as
a narrow view and not one shared by many in the experimental community,
where balancing tests are commonly used. Lack of balance is seen as an
indictment of the randomization in an experiment irrespective of whether the
variable in question affects the outcome. Lack of balance with respect to one
or more observed covariates raises the possibility that there may also be lack
of balance for other unobservables, and would lead a prudent researcher to
reassess the credibility of their research design. The same should be true for
quasi-experimental research based on observational data.
   A second point we are making is that the two strategies, coefficient com-
parison and balancing, both lead to explicit statistical tests. The balancing
test is a simple t-test used routinely by researchers. When adding a covariate
on the right hand side, comparing the coefficient of interest across the two re-

                                        2
gressions can be done using a generalized Hausman test. In practice, we have
not seen this test carried out in applied papers, where researchers typically
just eye-ball the results.2 We provide the relevant test statistics and discuss
how they behave under measurement error in Section 4. We also show how
the coefficient comparison test is simple to implement for varying identification
strategies. We demonstrate the superior power of the balancing test under a
variety of scenarios in Section 5.
      The principles underlying the points we are making are not new but the
consequences do not seem to be fully appreciated in much applied work.
Griliches (1977) is a classic reference for the issues arising when regression
controls are measured with error. A subsequent literature, for example Rosen-
baum and Rubin (1983) and Imbens (2003), has considered omitted variables
bias in non-linear models without measurement error. More closely related is
Battistin and Chesher (2014), as it discusses identification in the presence of
a mismeasured covariate in non-linear models. Like in the literature following
Rosenbaum and Rubin (1983), they discuss identification given assumptions
about a missing parameter, namely the degree of measurement error in the
covariate. We follow Griliches (1977) in framing our discussion around the
omitted variables bias arising in linear regressions, the general framework used
most widely in empirical studies. Unlike this literature, we are less interested
in point identification in the presence of missing information. We go beyond
the analysis in all of these papers in our explicit discussion of testing, which
forms the core of our study.
      Altonji, Elder and Taber (2005) discuss an alternative but closely related
approach to the problem. As we noted above, applied researchers often argue
that relative stability of regression coefficients when adding additional con-
trols provides evidence for credible identification. Implicit in this argument is
the idea that other confounders not controlled for are similar to the controls
just added to the regression. The paper by Altonji, Elder and Taber (2005)
formalizes this argument. In practice, adding controls will typically move the
coefficient of interest somewhat even if it is not by much. Altonji et al. (2013)
  2
      An exception is Gelbach (2016), who discusses the Hausman test in this context.


                                             3
and Oster (forthcoming) extend the original Altonji, Elder and Taber work by
providing more precise conditions for bounds and point identification in this
case. The approach in these papers relies on an assumption about how the
omitted variables bias due to the observed regressor is related to any remain-
ing omitted variables bias due to unobserved confounders.
   The remaining unobserved confounders in this previous work can be thought
of as the source of measurement error in the covariate which is added to the
regression in our analysis. For example, in our empirical example below, we
use mother’s education as a measure for family background but this variable
may only capture a small part of all the relevant family background informa-
tion, a lot of which may be orthogonal to mother’s education. In fact, we show
that our formulation and Oster’s (forthcoming) are isomorphic. This means
that our framework is a useful starting point for researchers who are willing to
make the type of assumptions in Altonji, Elder and Taber (2005) and follow-up
papers as well.
   Another related strand of work is by Belloni, Chernozhukov and Hansen
(2014a, b), who tackle the opposite problem from Altonji, Elder and Taber
(2005), namely choosing the best controls when the researcher has a poten-
tially bigger set of candidate controls available than is necessary. This large
dimensional set may come from nonlinearities and interactions among regres-
sors. Belloni, Chernozhukov and Hansen (2014b) use Lasso to select regressors
which are highly correlated with either the treatment or the outcome condi-
tional on other covariates. They then estimate an outcome equation including
as controls all the regressors selected in this preliminary step. In a sense, this is
more closely related to our setup than the Altonji, Elder and Taber approach
as Belloni, Chernozhukov and Hansen (2014b) also postulate that identifica-
tion can be achieved when using a subset of the available covariates as controls.
Their variable selection problem is related to the two testing strategies we dis-
cuss in this paper. However, like Altonji et al. (2013) and Oster (forthcoming),
their ultimate interest is in point identification and inference for the treatment
effects parameter, not in testing whether a particular specification is subject
to remaining confounders. Their setup is also not specifically geared towards

                                         4
dealing with control variables which are subject to error, which is our focus.
    An older literature by Hausman (1978), Hausman and Taylor (1980), and
Holly (1982) (see also the summary in MacKinnon, 1992, section II.9) considers
the relative power of the Hausman test compared to alternatives, in particular
an F -test for the added covariates in the outcome equation when potentially
multiple covariates are added. This comparison effectively maintains that there
is a lack of balance, and instead tests whether the added regressors matter
for explaining the outcome. While this is a different exercise from ours, this
literature highlights the potential power of the Hausman test when it succinctly
transforms a test with multiple restrictions (like the F -test for the added
covariates) into a test with a single restriction (the coefficient comparison
test). We discuss how to extend our framework to multiple added controls in
Section 5.3. Our basic findings largely carry over to this setting but we also
reach the conclusion that the Hausman test has a role to play when the goal
is to summarize a large number of restrictions.
    Griliches (1977) uses estimates of the returns to schooling as example for
the methodological points he makes. Such estimates have formed a staple of
labor economics ever since. We use Griliches’ data from the National Longi-
tudinal Survey of Young Men to illustrate our power results in Section 6. In
addition to Griliches (1977), this data set has been used in a well known study
by Card (1995). It is well suited for our purposes because the data contain
various test score measures which can be used as controls in a regression strat-
egy (as investigated by Griliches, 1977), a candidate instrument for college
attendance (investigated by Card, 1995), as well as a myriad of other useful
variables on individual and family background. The empirical results support
and illustrate our theoretical claims.


2     A Simple Framework
Consider the following simple framework starting with a population regression
equation
                              yi = αs + β s si + esi                        (1)


                                         5
where yi is an outcome like log wages, si is the causal variable of interest, like
years of schooling, and esi is the regression residual. The researcher proposes
this short regression model to be causal. This might be the case because the
data come from a randomized experiment, so the simple bivariate regression
is all we need. More likely, the researcher has a particular research design
applied to observational data. For example, in the case of a regression strategy
controlling for confounders, yi and si would be residuals from regressions of
the original outcome and treatment variables on the chosen controls. In the
case of panel data or differences-in-differences designs the controls are sets of
fixed effects. In the case of instrumental variables, si would be the predicted
value from a first stage regression. In practice, (1) encompasses a wide variety
of empirical approaches, and should be thought of as a short-hand for these.3
       Now consider the possibility that the population regression parameter β s
from (1) may not actually capture a causal effect. There may be a candidate
confounder xi , so that the causal effect of si on yi would only be obtained
conditional on xi , as in the long regression

                                yi = α + βsi + γxi + ei                                 (2)

and the researcher would like to probe whether this is a concern. For example,
in the returns to schooling context, xi might be some remaining part of an
individual’s earnings capacity which is also related to schooling, like ability or
family background.
       Researchers who find themselves in a situation where they start with a pro-
posed causal model (1) and a measure for a candidate confounder xi typically
do one of two things: They either regress xi on si and check whether si is signif-
icant, or they include xi on the right hand side of the original regression as in
(2), and check whether the estimate of β changes materially when xi is added
to the regression of interest. The first strategy constitutes a test for “balance,”
a standard check for successful randomization in an experiment. In principle,
the second strategy has the advantage that it goes beyond testing whether (1)
   3
    Of course, all subsequent regression equations and results also inherit the structure of
the actual underlying research design.


                                             6
qualifies as a causal regression. An appreciable change in β suggests that the
original estimate β s is biased. The results obtained with xi as an additional
control should be closer to the causal effect we seek to uncover. In particular,
if xi were the only relevant confounder and if we measure it without error, the
β parameter from the controlled regression is the causal effect of interest. In
practice, there is usually little reason to believe that these two conditions are
met, and hence a difference between β and β s again only indicates a flawed
research design.
   The relationship between these two strategies is easy to see. Write the
regression of xi on si , which we will call the balancing regression, as

                               xi = δ0 + δsi + ui .                          (3)

The change in the coefficient β from adding xi to the regression (1) is given
by the omitted variables bias formula

                                   β s − β = γδ.                             (4)

The change in the coefficient of interest β from adding xi consists of two com-
ponents, the coefficient γ on xi in the outcome equation (2) and the coefficient
δ from the balancing regression.
   Here we consider the relationship between these two approaches: the bal-
ancing test, consisting of an investigation of the null hypothesis

                                   H0 : δ = 0,                               (5)

compared to the inspection of the coefficient movement β s − β. The latter
strategy of comparing β s and β is often done informally, but it can be formal-
ized as a statistical test of the null hypothesis

                                H0 : β s − β = 0,                            (6)

which we will call the coefficient comparison (CC) test. From (4) it is clear
that (6) amounts to

                       H0 : β s − β = 0 ⇔ γ = 0 or δ = 0.                    (7)

                                        7
       This highlights that the two approaches formally test the same hypothesis
under the maintained assumption γ 6= 0. We may often have a strong sense
that γ 6= 0; i.e. we are dealing with a variable xi which we believe affects the
outcome, but we are unsure whether it is related to the regressor of interest
si . In this case, both tests would seem equally suitable.4 Nevertheless, in
other cases γ may be zero, or we may be unsure. In this case, the coefficient
comparison test seems to dominate because it directly addresses the question
we are after, namely whether the coefficient of interest β is affected by the
inclusion of xi in the regression.5
       Here we make the point that the balancing test adds valuable information
particularly when the true confounder is measured with error. In general, xi
may not be easy to measure. If the available measure for xi contains classi-
cal measurement error, the estimator of γ in (2) will be attenuated, and the
comparison β s − β will be too small (in absolute value) as a result. The esti-
mator of δ from the balancing regression is still consistent in the presence of
measurement error; this regression simply loses precision because the mismea-
sured variable is on the left hand side. Under the maintained assumption that
0 < γ < ∞, the balancing test is more powerful than the coefficient compari-
son test. In order to make these statements precise, we collect results for the
relevant population parameters for the case of classical measurement error in
the following section, before moving on to the test statistics.
   4
      One might argue that researchers should only carry out the long regression and not
the short regression if they know that γ 6= 0: if δ 6= 0, not including x in the regression
will lead to omitted variable bias; if δ = 0, both β̂ s and β̂ are consistent but β̂ s is less
efficient than β̂. As we emphasized in the Introduction, however, the focus of this paper is
on testing whether the treatment is plausibly randomly assigned in an (quasi-)experimental
design. In the analysis of a randomized controlled trial, for example, researchers may include
covariates when estimating the treatment effect but that does not come before a formal test
of covariate balance.
    5
      Equations (4) and (7) highlight that a regressor ought to be included in the long re-
gression when both γ 6= 0 and δ 6= 0. This differs from the selection rule chosen by Belloni,
Chernozhukov and Hansen (2014b), who include a regressor when either γ 6= 0 or δ 6= 0 is
true.




                                              8
3         Population Parameters in the Presence of
          Measurement Error
The candidate variable xi is not observed. Instead, the researcher works with
the mismeasured variable
                                         xm
                                          i = xi + mi .                        (8)
Here we assume the measurement error mi is classical, i.e. E (mi ) = 0,
Cov (xi , mi ) = 0. In Section 5 below we also investigate the impact of non-
classical errors. As a result of the measurement error, the researcher compares
the regressions

                              yi = αs + β s si + esi
                              y i = α m + β m si + γ m x m    m
                                                         i + ei .              (9)

        Notice that the short regression does not involve the mismeasured xi , so
that β s = β + γδ as before. However, the population regression coefficients
β m and γ m are now different from β and γ from equation (2), and they are
related in the following way:
                                             1−λ
                              β m = β + γδ         = β + γδθ
                                            1 − R2
                                       λ − R2
                              γm   = γ        = γ (1 − θ)                     (10)
                                       1 − R2
where R2 is the population R2 of the regression of si on xm
                                                          i and

                                               V ar (xi )
                                         λ=
                                               V ar (xm
                                                      i )

is the reliability of xm  6
                       i . λ measures the amount of measurement error present
as the fraction of the variance in the observed xm
                                                 i , which is due to the signal
in the true xi . It is also the attenuation factor in a simple bivariate regression
on xm
    i . In the multivariate model (9), an alternative way to parameterize the
amount of measurement error is
                                                       2
                                          1−λ         σm
                                   θ=           =          .
                                         1 − R2   σu2 + σm
                                                         2

    6
        Note R2 is also the population R2 of the regression of xm
                                                                i on si .


                                                 9
where σ 2 denotes the variance of the random variable in the subscript. 1 − θ
       ·
is the multivariate attenuation factor. Recall that ui is the residual from the
balancing regression (3).
   With the mismeasured xm
                         i , the balancing regression becomes


                            xm    m    m
                             i = δ0 + δ si + ui + mi ,                      (11)

which implies that
                                  2
                                σm            σu2 + σm2
                  λ=1−              m
                                        > 1 −        m
                                                         = R2 .
                             V ar (xi )       V ar (xi )

As a result
                                    1−λ
                               0 <        <1
                                   1 − R2
                                   λ − R2
                               0 <        < λ.
                                   1 − R2
   θ is an alternative way to parameterize the degree of measurement error in
xi compared to λ and R2 . The θ parameterization uses only the variation in
xm                                                                   m
 i which is orthogonal to si . This is the part of the variation in xi relevant
to the estimate of γ m in regression (9), which also has si as a regressor. θ
turns out to be a useful parameter in many of the derivations that follow.
   The population coefficient β m differs from β but less so than β s . In fact,
β m lies between β s and β, as can be seen from (10). The parameter γ m
is attenuated compared to γ; the attenuation is bigger than in the case of
a bivariate regression of yi on xm                              m
                                 i without the regressor si if xi and si are
correlated (R2 > 0).
   These results highlight a number of issues. The gap β s − β m is too small
compared to the desired β s − β, directly affecting the coefficient comparison
test. This is a consequence of the fact that γ m is biased towards zero. Ce-
teris paribus, this is making the assessment of the hypothesis γ = 0 more
difficult. Finally, the balancing regression (11) with the mismeasured xm
                                                                        i in-
volves measurement error in the dependent variable, which has no effect on
the population parameter δ m = δ, but the estimator δbm is less efficient than δ.
                                                                               b


                                        10
    The results here are also useful for thinking about the identification of β
and γ in the presence of measurement error. Rearranging (10) yields

                                           1 − R2
                               γ = γm
                                           λ − R2
                                                     1−λ
                               β = β m − δγ m              .                           (12)
                                                    λ − R2
Since R2 can be estimated from the data, these expressions only involve the
unknown parameter λ. If we are willing to make an assumption about the
measurement error, we are able to point identify β. Even if λ is not known
precisely, (12) can be used to bound β for a range of plausible reliabilities.
Alternatively, (10) can be used to derive the value of λ for which β = 0. These
calculations are similar in spirit to the ones suggested by Oster (forthcoming)
in a setting that is closely related.


4     Inference
In this section, we consider how conventional standard errors and test statis-
tics for the quantities of interest are affected in the homoskedastic case.7 We
present the theoretical power functions for the two alternative test statistics;
derivations are in Appendix A, which also shows that our results carry over
to robust standard errors. We extend the power results to the heteroskedastic
case and non-classical measurement error in simulations. Our basic conclusions
are the same in all these different scenarios.
    Start with the standard error of estimator δbm from the balancing regression:
                                   s
                    √      
                                 p    σu2 + σm2       1 σu
                       b δbm →
                      nse                       =  √          ,
                                          σs2        1 − θ σs

where we use se(   ·
             b ) to denote the estimated standard error of a given estimator.
        ·
Let se( ) denote the asymptotic standard error of an estimator, i.e., se( ) ≡
    7
                                                                                      ·
      See Appendix A for the precise setup of the model. The primitive disturbances are si ,
ui , ei , and mi , which we assume to be uncorrelated with each other. Other variables are
determined by (2), (3), and (8).



                                            11
  n
        √
√1 plim{ nse(
          b    ·)}. In the case of δbm ,
                                   1   1 σu
                           se δbm = √ √          .
                                      n 1 − θ σs

   Comparing the asymptotic standard error of δbm to its counterpart in the
case with no measurement error,
                                       1 σu
                                se δb = √      ,
                                          n σs

we have                                      
                                       se δb
                            se δbm = √          .
                                           1−θ
Since 0 < θ < 1, the standard error is inflated compared to the case with no
measurement error.
   A test based on the t-statistic

                                              δbm
                                 tδm =         
                                           b δbm
                                           se

remains consistent because mi is correctly accounted for in the residual of the
balancing regression (11), but the t-statistic is asymptotically smaller than in
the error free case: As n → ∞, the comparison of the scaled t-statistics when
δ > 0 is (without loss of generality, we are assuming that δ is either zero or
positive)
                    1    p √      δ     δ  p  1
                   √ tδm → 1 − θ   <   ← √ tδ
                     n            σu    σu     n
                                           σs       σs

This means the null hypothesis (5) is rejected less often. The test is less power-
                                                                           √
ful than in the error free case; the power loss is captured by the term 1 − θ.
                   bm , the estimator for the coefficient on the mismeasured
   We next turn to γ
xm
 i in (9). The parameter γ is of interest since it determines the coefficient
movement β s − β = γδ in conjunction with the result from the balancing
regression. Let x̃m                                                    m
                  i be the residual from the population regression of xi on si .
For ease of exposition, we impose conditional homoskedasticity of em
                                                                   i given si




                                           12
and xm
     i here and leave the more general case to Appendix A.2.3. The standard
          bm in the limit is
error for γ
                                    p
                           m     1    V ar (em i )
                      se (b
                          γ ) = √   p
                                  n V ar (e   xm
                                               i )
                                    s
                                 1    γ 2 θσu2 + σe2
                              = √
                                  n     σu2 + σm 2
                                            s
                                 1 √                 σ2
                              = √    1 − θ θγ 2 + e2 ,
                                  n                  σu

while                                            s
                                          1            σe2
                                    γ) = √
                                se (b                      .
                                           n           σu2
   γ m ) involves two terms: the first term is an attenuated version of se(b
se(b                                                                       γ ) from
the corresponding regression with the correctly measured xi , while the second
term depends on the value of γ. The parameters in the two terms are not
                         γ m ) ≷ se (b
directly related, so se (b           γ ). Measurement error does not necessarily
inflate the standard error here.
   The two terms have a simple, intuitive interpretation. Measurement error
attenuates the parameter γ m towards zero, the attenuation factor is 1 − θ.
The standard error is attenuated in the same direction; this is reflected in the
√
  1 − θ factor, which multiplies the remainder of the standard error calcula-
tion. The second influence from measurement error comes from the term θγ 2 ,
which results from the fact that the residual variance V ar (em
                                                              i ) is larger when
there is measurement error. The increase in the variance is related to the true
γ, which enters the residual.
   The t-statistic for testing whether γ m = 0 is

                                               bm
                                               γ
                                   tγ m =
                                            se
                                            b (b γ m)

and it follows that when γ > 0
                 1     p √          γ                    γ           p  1
                √ tγ m → 1 − θ q                       <q            ← √ tγ .
                  n              θγ 2 +          σe2           σe2       n
                                                  2
                                                 σu             2
                                                               σu



                                            13
    As in the case of δbm from the balancing regression, the t-statistic for γ
                                                                             bm
is smaller than tγ for the error free case. But in contrast to the balancing test
statistic tδm , measurement error reduces tγ m relatively more, namely due to
                                                                       √
the term θγ 2 in the denominator, in addition to the attenuation factor 1 − θ.
This is due to the fact that measurement error in a regressor both attenuates
the relevant coefficient towards zero and introduces additional variance into the
residual. Though interestingly, θγ 2 captures the additional residual variance
                 √
while the factor 1 − θ now captures the attenuation of γ m . In the balancing
               √
test statistic, 1 − θ accounted for the residual variance. The upshot from
this discussion is that classical measurement error makes the assessment of
whether γ = 0 more difficult compared to the assessment of whether δ = 0.
As we will see, this is the source of the greater power of the balancing test
statistic.
    Finally, consider the quantity β s − β m , which enters the coefficient compar-
ison test. To form a test statistic for this quantity we need the expression for
the asymptotic variance of βbs − βbm , which we derive through an application
of the delta method to the omitted variables bias formula

                                   βbs − βbm = δbm γ
                                                   bm .

Specifically, we can relate V ar(βbs − βbm ) to the asymptotic variances of δbm and
bm and their asymptotic covariance:
γ
                                            
                   s   m               2
           V ar β − β
                  b   b    = γ (1 − θ) V ar δbm + δ 2 V ar (b
                               2
                                                            γ m)
                                                       
                              + 2δγ (1 − θ) Cov δbm , γ
                                                      bm .                    (13)
              
             m
Using V ar δb              γ m ), which we derived above, and the fact that
                 and V ar (b
           
Cov δbm , γ
          bm = 0, which we show in Appendix A.2.2, we get

                                                   2             2
                                                                  
                 
                     bs   bm
                                 1             2 σu     2 2  2 σe
             V ar β − β        = (1 − θ) γ 2 + θδ γ + δ 2 .
                                  n               σs            σu
                                                       
                                   γ m ), V ar βbs − βbm has both an attenuation
It is easy to see that, like V ar (b
factor as well as an additional positive term compared to the case where θ =

                                           14
                 
              s
0, i.e. V ar β − β . Measurement error may therefore raise or lower the
             b   b
sampling variance for the coefficient comparison test.
   Before we proceed to discuss the power of the coefficient comparison test,
we note that the covariance term in
                                                        
         V ar βbs − βbm = V ar βbs + V ar βbm − 2Cov βbs , βbm

reduces the sampling variance of βbs − βbm . In fact,
                                                    this
                                                        covariance
                                                                  term
                                                                      is posi-
tive, and it is generally sizable compared to V ar βbs and V ar βbm since the
                                                                            
regression residuals es and em are highly correlated. Because 2Cov βbs , βbm
                      i         i

gets subtracted, looking at the standard errors of βbs and βbm alone can poten-
tially mislead the researcher into concluding that the two coefficients are not
significantly different from each other when in fact they are.
   The coefficient comparison test itself can be formulated as a t-test as well,
since we are interested in the movement in a single parameter. Define
                                               βbs − βbm
                            t(β s −β m )   ≡
                                             b βbs − βbm )
                                             se(
      b βbs − βbm ) is a consistent standard error estimator. Since
where se(

                          β s − β m = δγ m = δγ (1 − θ)

we have
                 1             p                      δγ (1 − θ)
                √ t(β s −β m ) → r
                  n                                         2                             2
                                                                                               
                                      (1 − θ)         γ 2 σσu2   +   θδ 2 γ 2   +   δ 2 σσe2
                                                            s                             u

                                    √                            δγ
                            =        1 − θq              2                             2
                                                                                           .       (14)
                                                  γ 2 σσu2   + θδ 2 γ 2 + δ 2 σσe2
                                                        s                              u


   Under the alternative hypothesis (δ 6= 0) and the maintained assumption
γ 6= 0, the limits for the other two test statistics can be written as
                        1       p   √         δγ
                       √ tδ m →       1 − θq 2
                         n                    γ 2 σσu2
                                                    s

                        1       p   √                 δγ
                       √ tγ m →       1 − θq                      .
                         n                                      2
                                              θδ 2 γ 2 + δ 2 σσe2
                                                                            u



                                             15
Hence, using (14), it is apparent that under these conditions the three tests
are asymptotically related in the following way:
                               !2               !2                     !2
                     1                     1                     1
        plim 1                    = plim 1         + plim      1             (15)
                √ t(β s −β m )           √ tδ m               √ tγ m
                  n                       n                     n

   These results highlight a number of things. First of all, under the main-
tained hypothesis γ 6= 0, the balancing test alone is more powerful. This is
not surprising at all, since the balancing test only involves estimating the pa-
rameter δ while the coefficient comparison test involves estimating both δ and
γ. Imposing γ 6= 0 in the coefficient comparison test is akin to tγ m → ∞, and
this would restore the equivalence of the balancing and coefficient comparison
tests. Note that the power advantage from imposing γ 6= 0 exists regardless
of the presence of measurement error.
   The second insight is that measurement error affects the coefficient com-
parison test in two ways. The test statistic is subject to both the attenuation
      √
factor 1 − θ and the term θδ 2 γ 2 in the variance, which is inherited from the
                bm . Importantly, however, all these terms interact in the coef-
t-statistic for γ
ficient comparison test. In our numerical exercises below, it turns out that
the way in which measurement error attenuates γ m compared to γ is a ma-
jor source of the power disadvantage of the coefficient comparison test. Our
simulations demonstrate that the differences in power between the coefficient
comparison and balancing tests can be substantial when there is considerable
measurement error in xm
                      i . Before we turn to these results, we briefly note how
the coefficient comparison test can be implemented in practice.

4.1    Implementing the Coefficient Comparison Test
The balancing test is a straightforward t-test, which regression software cal-
culates routinely. We noted that the coefficient comparison test is a general-
ized Hausman test. Regression software will typically calculate this as well if
it allows for seemingly unrelated regression estimation (SURE). SURE takes
Cov (esi , em
            i ) into account and therefore facilitates the test. In Stata, this is




                                       16
implemented via the suest command. Generically, the test would take the
following form:
        reg y s
        est store reg1
        reg y s x
        est store reg2
        suest reg1 reg2
        test[reg1 mean]s=[reg2 mean]s
        The test easily accommodates covariates or can be carried out with the
variables y, s, and x being residuals from a previous regression (hence facil-
itating large numbers of fixed effects though degrees of freedom may have to
be adjusted in this case).
        As far as we can tell, the Stata suest or 3reg commands don’t work for
the type of IV regressions we might be interested in here. An alternative, which
also works for IV, is to take the regressions (1) and (2) and stack them:

                                                                                                     
        yi               1 0        αs               si 0        βs               0 0         0               esi
                 =                           +                            +                           +                 .
        yi               0 1        α                0 si        β                0 xi        γ               ei

Testing β s − β = 0 is akin to a Chow test across the two specifications (1) and
(2). Of course, the data here are not two subsamples but rather duplicates of
the original data set. To take account of this and allow for the correlation in
the residuals across duplicates, it is crucial to cluster standard errors on the
observation identifier i.


5         Power Comparisons
5.1          Asymptotic and Monte Carlo Results with Classical
             Measurement Error
The ability of a test to reject when the null hypothesis is false is described by
the power function of the test. The power functions here are functions of d, the
values the parameter δ might take on under the alternative hypothesis. Because
the joint distribution between the coefficient and standard error estimators is

                                                            17
difficult to characterize, especially in the case of the coefficient comparison
test, we abstract away from the sampling variation in estimating the standard
errors in the theoretical derivations of this section. The resulting t-statistic
for the null hypothesis that the coefficient δ is zero in the balancing test is
                                          √ bm    √ bm
                                 δbm       n·δ     n·δ
                       tδm   =          = √ 2 2 =   σu  .
                               se(δbm )    σu +σm  √
                                                  σ 1−θ      s
                                                   σs


Similarly, we use
                                                     √ bs bm
                                         βbs − βbm    n(β − β )
                 t(β s −β m ) (d; γ) =              = p
                                            s     m
                                       se(βb − βb )     Vβ (d; γ)

where
                                              γ 2 σu2            d2 σe2
                                                                         
                                                           2 2
                    Vβ (d; γ) = (1 − θ)               + θd  γ  +
                                               σs2                σu2
in the derivation of the power function for the coefficient comparison test.
    As shown in Appendix A, the power function for a 5% critical value of the
balancing test is
                                                   √    √ 
                                                 nσs 1 − θ
                P owertδm (d) = 1 − Φ 1.96 − d
                                                    σu
                                              √    √      
                                                nσs 1 − θ
                                + Φ −1.96 − d                ,                (16)
                                                   σu

         ·
where Φ ( ) is the standard normal cumulative distribution function. The
power function for the coefficient comparison test is
                                                     √            !
                                                       nγ (1 − θ)
             P owert(βs −βm ) (d; γ) = 1 − Φ 1.96 − d p
                                                        Vβ (d; γ)
                                                     √            !
                                                       nγ (1 − θ)
                                       + Φ −1.96 − d p              .         (17)
                                                        Vβ (d; γ)

   Note that the power function for the balancing test does not involve the




                                              18
parameter γ. Using our results above, for 0 < γ < ∞ it can be written as
                                               √            !
                                                 nγ (1 − θ)
              P owertδm (d) = 1 − Φ 1.96 − d p
                                                  Vδ (d; γ)
                                               √            !
                                                 nγ (1 − θ)
                              + Φ −1.96 − d p                 ,         (18)
                                                  Vδ (d; γ)

where
                                                         γ 2 σu2
                                  Vδ (d; γ) = (1 − θ)            .
                                                          σs2
It is hence apparent that Vβ (d; γ) > Vδ (d; γ), i.e. the coefficient comparison
test has a larger variance. As a result, when d 6= 08

                          P owertδm (d) > P owert(βs −βm ) (d; γ) .                          (19)

       In practice, this result may or may not be important. In addition, when
the standard error is estimated, the powers of the two tests may differ from the
theoretical results above. Therefore, we carry out a number of Monte Carlo
simulations to assess the performance of the two tests. Table 1 displays the
parameter values we use as well as the implied values of the population R2
of regression (9). The values were chosen so that for intermediate amounts of
measurement error in xm      2
                      i the R s are reflective of regressions fairly typical of
those in applied microeconomics, for example, a wage regression. Note that
the amounts of measurement error we consider are comparatively large. In our
empirical application we use mother’s education and the presence of a library
card in the household as measures of family background. We suspect that
these variables pick up at most a minor part of the true variation of family
background, even in the presence of other covariates, so that values of θ = 0.7
or θ = 0.85 for the measurement error are not unreasonable.
   8
      To see this, define f (t) = 1 − Φ(1.96 − t) + Φ(−1.96 − t) and denote the probability
density function of a standard normal distribution by φ. The f notation allows us to rewrite
the expressions for the power functions P owertδm (d) and P owert(βs −βm ) (d; γ) in equations
(17) and (18) simply as f (t1 ) and f (t2 ). When d 6= 0, Vβ (d; γ) > Vδ (d; γ) implies that
|t1 | > |t2 | > 0. Since f 0 (t) = φ(1.96 − t) − φ(1.96 + t) is positive for all t > 0 and negative
for all t < 0, f (t1 ) > f (t2 ) given |t1 | > |t2 | > 0, and equation (19) follows.




                                                19
       In Figure 1, we start by plotting the the theoretical power functions for both
tests for three different magnitudes of the measurement error.9 The black/thin
lines show the power functions with no measurement error. The power func-
tions can be seen to increase quickly with d, and both tests reject with virtual
certainty once d exceeds values of 1. The balancing test is slightly more power-
ful but this difference is small, and only visible in the figure for a small range
of d.
       The blue/medium thick lines correspond to θ = 0.7, i.e. 70% of the variance
of xm
    i is measurement error after partialling out si . Measurement error of that
magnitude visibly affects the power of both tests. The balancing test still
rejects with certainty for d > 1.5, while the coefficient comparison test does
not reject with certainty for the parameter values considered in the figure. This
discrepancy becomes even more pronounced when we set θ = 0.85 (red/thick
lines). The power of the coefficient comparison test does not rise above 0.65
in this case, while the balancing test still rejects with probability 1 when d is
around 2.
       The results in Figure 1 highlight that there are parameter combinations
where the balancing test has substantially more power than the coefficient
comparison test. In other regions of the parameter space, the two tests have
more similar power, for example, when d < 0.5.10
       Before going on to simulations of more complicated cases, we contrast the
theoretical power functions in Figure 1, based on asymptotic approximations,
to simulated rejection rates of the same tests in Monte Carlo samples. Figure
   9
      The power function for the balancing test in equation (16) is written using the normal
distribution, but we actually calculate it using the t-distribution with n − 2 degrees of
freedom. This is consistent with how Stata version 14 performs the balancing test following
the command reg x s or reg x s, r, even though this distribution choice makes little
difference given our sample size (n = 100).
   10
      While we highlight the consequences of measurement error throughout the paper, we
should note that formally any particular value of θ can be mimicked by an appropriate
combination of values for γ and σu2 . This is an immediate consequence of the fact that
the classical measurement error model is underidentified by one parameter. In that sense
“measurement error” is simply a label for a certain set of parameter values. It is always
difficult to choose empirically relevant values for simulations and we take comfort from the
fact that the results emerging from this section are also reflected in the empirical example
in Section 6.


                                            20
2 shows the power functions for the two tests without measurement error
(θ = 0) and with (θ = 0.85), as well as their simulated counterparts.11 Without
measurement error, the theoretical power functions are closely aligned with the
empirical rejection rates (black lines). Adding measurement error, this is also
true for the balancing test (solid red and blue/thick lines) but not for the
coefficient comparison test (broken red and blue/thick lines).
       Figure 2 reveals that the empirical rejection rates of the coefficient com-
parison test in the presence of measurement error deviate substantially from
the power function calculation based on the asymptotic approximation. This
discrepancy is almost completely explained by the fact that we use the asymp-
totic values of standard errors in the calculations but estimated standard errors
in the simulations. The empirical test is severely distorted under the null; it
barely rejects more than 1% of the time for a nominal size of 5%. While this
problem leads to too few rejections under the null, it is important to note that
the same issue arises for positive values of d until about d < 1.5. For larger
values of d the relationship reverses. In other words, for moderate values of d
the coefficient comparison test statistic is biased downwards under the alter-
native, and the test has too little power. This highlights another advantage
of the balancing test—a standard t-test where no such problem arises. We
note that this is a small sample problem, which goes away when we increase
the sample size (in unreported simulations). We suspect that this problem is
related to the way in which the coefficient comparison test effectively combines
the simple tδm and tγ m test statistics in a non-linear fashion, as can be seen in
equation (15), and the fact that tγ m sometimes is close to 0 in small samples
despite the fact that we fix γ substantially above 0.

5.2       Monte Carlo Results beyond the Benchmark Model
The homoskedastic case with classical measurement error might be highly
stylized and not correspond well to the situations typically encountered in
empirical practice. We therefore explore some other scenarios using simulations
  11
    We did 25,000 replications in these simulations, and each repeated sample contains 100
observations.


                                           21
in this section. Figure 3 shows the original theoretical power functions for the
case with no measurement error from Figure 1. It adds empirical rejection
rates from simulations with heteroskedastic errors ui and ei of the form
                                                        2
                                             e|si |
                                       
                            2                                 2
                           σu,i    =                         σ0u
                                           1 + e|si |
                                                        2
                                             e|si |
                                       
                             2                                2
                            σe,i   =                         σ0e .
                                           1 + e|si |
                               2       2
We set the baseline variances σ0u and σ0e so that the unconditional variances
σ 2u = 3 and σ 2e = 30 match the variances in Figure 1. The test statistics used in
the simulations employ robust standard errors. We plot the rejection rates for
data with no measurement error and for the more severe measurement error
scenario given by θ = 0.85. As can be seen in Figure 3, both the balancing and
the coefficient comparison tests lose some power with heteroskedastic residuals
and a robust covariance matrix compared to the conventional, homoskedastic
baseline (black/thin lines). Otherwise, the main findings look very similar
to those in Figure 1. Heteroskedasticity does not seem to alter the basic
conclusions appreciatively.
    Next, we explore mean reverting measurement error (Bound et al., 1994).
We generate measurement error as

                                   mi = κxi + µi

where κ is a parameter and Cov (xi , µi ) = 0, so that κxi captures the error
related to xi and µi the unrelated part. When −1 < κ < 0, the error is mean
reverting, i.e. the κxi -part of the error reduces the variance in xm
                                                                    i compared
to xi .
    The case of mean reverting measurement error captures a variety of ideas,
including the one that we may observe only part of a particular confounder
made up of multiple components. Imagine we would like to include in our
regression a variable xi = w1i + w2i , where w1i and w2i are two orthogonal
variables. We observe xm
                       i = w1i . For example, xi may be family background,
w1i is mother’s education and other parts of family background correlated


                                           22
with it, and w2i are all relevant parts of family background which are uncorre-
lated with mother’s education. As long as selection bias due to w1i and w2i is
the same, this amounts to the mean reverting measurement error formulation
above. Note that λ = V ar (xi ) /V ar (xm
                                        i ) > 1 in this case, so the mismeasured
xm
 i has a lower variance than the true xi . This scenario is also isomorphic to
the model studied by Oster (forthcoming). See Appendix B for details.
   Notice that xm
                i can now be written as


                xm
                 i = (1 + κ) δ0 + (1 + κ) δsi + (1 + κ) ui + µi ,


so this parameterization directly affects the coefficient in the balancing regres-
sion, which will be smaller than δ for a negative κ. As a result, the balancing
test will reject less often. At the same time, a negative κ offsets and possibly
reverses the attenuation bias on γ. This should bring the power functions of
the balancing and coefficient comparison tests closer together.
   For the simulations we set κ = −0.5, so the error is mean reverting. We
also fix σµ2 in the simulations. However, it is important to note that the
nature of the measurement error will change as we change the value of d under
the alternative hypotheses. xi depends on δ and the correlated part of the
measurement error depends in turn on xi . We show results for two cases
with σµ2 = 0.75 and σµ2 = 2.25. Under the null, these two parameter values
correspond to λ = 2 and λ = 1, respectively. The case λ = 2 corresponds
to the Oster (forthcoming) model just described with V ar (w1i ) = V ar (w2i ).
These models exhibit relatively large amounts of mean reversion. Figure 4
demonstrates that the balancing test again dominates. The gap is small for
the σµ2 = 0.75 case but grows with σµ2 , the classical portion of the measurement
error. This finding is not surprising as mean-reverting measurement error does
less damage in terms of biasing the estimate of γ.
   A particular case of mean reverting measurement error is the one where xi
is a dummy variable, so we provide some simulation results for this case. In
this case, the balancing equation is a binary choice model, and hence inherently
non-linear. While we assume that the researcher continues to estimate (3) as



                                       23
a linear probability model, we generate xi as follows:

                             Pr (xi = 1) = Φ (δsi ) ,                       (20)

         ·
where Φ ( ) is the normal distribution function as before. Measurement error
takes the form of misclassification, and we assume the misclassification rate to
be symmetric:

                Pr (xm                    m
                     i = 1|xi = 0) = Pr (xi = 0|xi = 1) = τ.


Compared to the baseline parameters in Table 1, we set σs2 = 0.25, and τ = 0.1
in our simulations. The model remains the same in all other respects. We use
robust standard errors in estimating (9) and (11).
    Various
       issues arise from the nonlinear nature of (20). One is the fact that
plim δ̂ from estimating (11) linearly is not going to equal the δ we generated
                                                                           
in the probit equation (20) to generate x. The relationship between plim δ̂
and δ is concave. In Figure
                           5, we plot rejection rates against values of δ,
although the quantity plim δ̂ is probably more comparable to what we put
on the x-axis in the previous figures that summarize the simulation results
from linear models. We note that results look qualitatively very similar when
we plot rejection rates against the empirical averages of δ̂ from estimating (11)
as a linear probability model.
   Another issue is that measurement error in xi will now lead to a biased
estimate of δ in estimating (11). This is true even if we were to use a probit
and estimated a model like (20). The bias takes the form of attenuation, just
as in the case of a binary regressor with measurement error (see Hausman,
Abrevaya and Scott-Morton, 1998). This is the corollary of our result that
mean reverting measurement error also reduces the power of the balancing
test. Of course, we know from the relationship (15) between the test statistics
that the coefficient comparison test will also suffer from the same power loss.
   The black/thin lines in Figure 5 reveal a sizable power advantage for the
balancing test even without any misclassification. This result is in stark con-
trast to the linear models we have analyzed, where a large power loss for the


                                       24
coefficient comparison test only resulted once we introduced measurement er-
ror. In fact, it is possible to think of the binary nature of xi itself as a form of
mismeasurement. Equation (20) defines Pr (xi = 1) as a latent index, but the
outcome regression (2) uses a coarse version of this variable in the form of the
binary xi .
       In our parameterization, the coefficient comparison test never reaches a
rejection rate of 1, and the power function levels off at a far lower level. As
d increases, the power of the balancing test goes to 1. In the linear model,
the rejection rate of tγ is independent of d. Because of the nonlinear nature
of (20) this is no longer true here, and the average value of tγ across repeated
samples actually falls for higher values of d. Drawing on (15), the power of the
coefficient comparison test will equal the power of tγ when tδ → ∞. This is not
a specific feature of the binary case but is generally true for the relationship
between the three test statistics. However, in the binary case this implies that
the power of the coefficient comparison test may decline with d.12
       Adding measurement error to the binary regressor xi makes things worse
as is visible from the red/thick lines in Figure 5. The power loss of the balanc-
ing test is comparatively minor for the relatively low misclassification rate of
τ = 0.1 we are using. Much of the loss for the balancing test results from the
binary nature of the xi variable in the first place. The coefficient comparison
test is affected by misclassification error to a much higher degree because tγ
is affected, the Hausman, Abrevaya and Scott-Morton (1998) result notwith-
standing.

5.3       Multiple Controls
So far we have concentrated on the case of a single added regressor xi . Often
in empirical practice we may want to add a set of additional covariates at once.
  12
    The reason for the decline of tγ with d in our parameterization is as follows: the standard
error of γ̂ depends on the residual variance of the long regression, which is independent of
d, and on the variance of the residual from regressing xi on si (because si is partialled out
in the long regression). When d = 0, this latter residual is just equal to xi itself, which is
binary. But si is continuous, so as d increases, partialling out si transforms the binary xi
into a continuous variable, which has less variance than in the d = 0 case. As the effective
variance in this regressor falls, the standard error of γ̂ goes up and tγ goes down.

                                              25
It is straightforward to extend our framework to that setting. In this section,
we describe this multivariate extension, and provide some simulation results.
Some interesting new issues arise in this analysis.
   Suppose there are k added regressors, i.e. xi is a k × 1 vector, and

                               yi = α + βsi + x0i γ + ei
                               xi = δ 0 + δsi + ui                               (21)
                         β s − β = γ 0δ

where γ, δ 0 , δ and ui are k × 1 vector analogs of their scalar counterparts
in Section 2. Lee and Lemieux (2010) suggest a balancing test for multiple
covariates in the context of evaluating regression discontinuity designs. Let
x(j) denote the n × 1 vector of all the observations on the j-th x-variable. We
can stack all the x-variables on the left-hand-side of the regression to obtain
                                                            
           x(1)         ιδ01        s 0 0 0           δ1        u(1)
         x(2)   ιδ02   0 s 0 0   δ2   u(2) 
         ...  =  ...  +  0 0 ... 0   ...  +  ...  ,
                                                            

           x(k)         ιδ0k       0 0 0 s            δk        u(k)

where ι is an n × 1 vector of ones, s = [s1 , s2 , ..., sn ]0 , and u(j) the vector of
residuals corresponding to covariate x(j) . We can then perform an F -test for
the joint significance of the δ coefficients. This left-hand-side (LHS) balancing
test is similar to the way we implemented the coefficient comparison test above
in Section 4.1.
   The drawback of the LHS test is that stacking equations is non-standard,
and requires some extra programming to carry it out. It therefore seems
appealing to consider the alternative of regressing s on the covariates x

                                   si = π 0 xi + vi

and test whether the coefficient vector π is significantly different from zero.
This is a standard F -test. We refer to this test as the right-hand-side (RHS)
balancing test. Applied researchers sometimes use this RHS balancing test;
for example, Bruhn and McKenzie (2009) report it being used in some exper-
imental studies in development economics.

                                         26
       While putting the balancing variables on the RHS might at first glance
seem unusual, it turns out that the LHS and RHS tests deliver very similar
results. In the case of a single covariate xi (i.e. k = 1) the LHS and the
RHS tests using a conventional covariance matrix for homoskedastic residuals
are numerically identical.13 This is no longer true with multiple covariates
(k > 1). However, the scaled F -statistics of the two tests have the same
probability limit in the special case where the LHS regression has a spherical
error structure var(ui ) = σ 2 Ik and the RHS regression is homoskedastic, as
we show in Appendix C.
       How do the balancing tests with multiple covariates perform in practice?
Figure 6 shows simulations using a similar design as described in Table 1 for all
k balancing equations. However, with multiple covariates there are different
ways of specifying the alternative hypotheses now. The null hypothesis may
fail for one, various, or all of the k covariates. We show rejection rates under
two polar versions of the alternative hypothesis: first, for the case where all
covariates are unbalanced, i.e. δ1 = δ2 = . . . = δk = d, and then for the case
where only the first covariate is unbalanced while the others remain balanced,
i.e. δ1 = d, δ2 = . . . = δk = 0. We generate normally distributed, spherical
errors and impose homoskedasticity and independence when performing the
joint test of the δj ’s or the πj0 s. There are four panels in Figure 6: the top row
has 4 added covariates, and the bottom row 8; the left hand column shows the
case where all covariates are unbalanced while the right hand column displays
the case where only the first covariate is unbalanced.
       Figure 6 highlights a number of results. The LHS and RHS balancing tests
are indeed very similar as their power functions virtually lie on top of each
other in all four panels. When all covariates are unbalanced and when mea-
surement error is absent, the Hausman test turns out to be an efficient test
  13
     The F -test in this case amounts to the overall F -test for the significance of the regression.
This, in turn, is a function of the R2 of the regression. Since only two variables xi and si are
involved, this is the square of the correlation coefficient between the two. But the correlation
coefficient is not directional, so the forward and reverse regression have to deliver the same
F -statistic (in the case when there other covariates present in the regression, replace the R2
and correlation coefficient with their partial equivalents in this argument).



                                                27
in combining the k separate hypotheses into one single test-statistic, which is
generated from the estimates of only two parameters, the long and short β’s.
The balancing tests, on the other hand, have to rely on the estimation of k
parameters.14 In this case, the rejection rates for the coefficient comparison
test (black/thin broken lines) therefore lie above the ones for both the balanc-
ing tests (black/thin solid and dash-dot lines), as can be seen in the left-hand
panels. In the presence of measurement error, however, the balancing tests are
again more powerful than the coefficient comparison test as can be seen from
the juxtaposition of the thicker red lines.
       This power advantage of the balancing tests is greater when only one co-
variate is unbalanced. Both tests are less powerful in this case, but the power
loss for the coefficient comparison test is now much more pronounced. This
is particularly noticeable in the case with measurement error in the covariates
(red/thick lines) but the balancing tests outperform the coefficient comparison
test even without measurement error in this case. Empirically relevant cases
may often lie in between these extremes. Researchers may be faced with a set
of potential controls to investigate, some of which may be unbalanced with
the treatment while others are not. Figure 6 demonstrates that the balancing
tests will frequently be the most powerful tools in such a situation, but the
coefficient comparison test also has a role to play in the multivariate case.
       The simulations reveal a number of further insights. With measurement
error, the small sample issue of the coefficient comparison test, which we high-
lighted in Figure 2, arises again. On top of this, we found in unreported
simulations that both the LHS and RHS balancing tests with robust stan-
dard errors (clustered standard errors across equations for the LHS test and
heteroskedasticity-robust standard errors for the RHS test) have a size dis-
tortion under the null hypothesis and reject too often. This is the standard
small sample distortion of these covariance matrices discussed in the literature
(MacKinnon and White, 1985; Chesher and Jewitt, 1987; Angrist and Pis-
  14
     The analyses in Hausman (1978), Hausman and Taylor (1980), Holly (1982), and MacK-
innon (1992) section II.9, which compare the power of the coefficient comparison test to the
F -test for γ = 0, highlight a similar result.



                                            28
chke, 2009, chapter 8). We find that the bias tends to get worse when more
covariates are added. Applied researcher may be most interested in the test-
ing strategies discussed here when k is large (so that a series of single variable
balancing tests is unattractive), and will want to rely on a robust covariance
matrix. An upward size distortion may be less of an issue for a conservative
researcher in a balancing test (where it means the researcher will falsely de-
cide not to go ahead with a research design where the covariates are actually
balanced) than in a test for the presence of non-zero treatment effects (where
the same bias leads to false discoveries). Nevertheless, we suspect that most
applied researchers would prefer a test with a correct size under the null and
a steep power function. As a result, research on remedies for the bias problem
in multivariate tests is therefore particularly important.15
       While we find few differences between the power of the LHS and RHS
tests in our simulations, we know from the theoretical analysis in Appendix
C that the test statistics will differ asymptotically when the third and fourth
moments of the underlying data deviate from the normally distributed case.
It is therefore interesting to probe how the two tests perform in an example
with real data.
       We therefore pooled data from the 2010 - 2014 American Community Sur-
veys (ACS). Our data set consists of white and African American individuals
  15
    We find in unreported simulations that the classic small sample corrections HC2 and
HC3 by MacKinnon and White (1985) still have size distortions under the null. There
is currently an active literature on how to better deal with this small sample bias of the
robust or clustered covariance estimator. For example, Young (2016) suggests an adjustment
of the degrees of freedom of hypothesis tests but this adjustment is only implemented for
one coefficient at a time, so does not work for testing multiple linear restrictions at once.
Cattaneo, Jansson and Newey (2017) present an adjustment of the entire covariance matrix
but only consider the case of heteroskedasticity and do not allow for clustering. As a result,
neither of these can currently be applied to our LHS balancing test.
   Another alternative is to rely on a series of single coefficient tests and adjust the resulting
test statistics for multiple testing. Akin to the size distortion of robust test statistics,
without adjustment such multiple testing will reject too often under the null as first noted
by Bonferroni (1935). There is a sizable literature in statistics and theoretical econometrics
on this topic with modern approaches based either on the influential work by Westfall and
Young (1993) or by Benjamini and Hochberg (1995). Examples of empirical applications in
economics are Kling, Liebman and Katz (2007), Anderson (2008), and Duflo, Dupas and
Kremer (2017). But these examples remain rare, and no clear choices among the multitude
of theoretical alternatives have yet emerged among applied researchers.


                                               29
aged 21 to 64 with non-missing annual earnings. This data set has 5,644,865
observations. We generated a binary treatment si according to


                        Pr(si = 1) = ωF (educi ) + (1 − ω) U.

educi is the years of schooling of individual i, F (educi ) is its cumulative dis-
tribution function, U is a uniform random variable, and ω is a weight akin
to the parameter d in our earlier Monte Carlo experiments. Under the null
hypothesis, ω = 0, and the treatment si consists solely of the generated noise
U . For values of ω > 0, the treatment si is related to the education level of
the individual, which in turn is correlated with other individual covariates xi .
Our vector of covariates (xi ) contains the six variables: female, black, age, age
squared, log family size, and log income. These variables take on very different
distributions from simple binary for female and black to skewed distributions
for family size and income. They are also the types of variables researchers
will likely use to check for balance when working with individual household
data. The bigger the ω, the more likely the balancing test relating si and xi
should reject. In our simulations, we draw samples of size 1,000 with replace-
ment from the original 5,644,865 observations in the ACS dataset. We perform
10,000 replications and carry out the LHS and RHS tests for various values of
ω. Figure 7 shows the results for the two balancing tests. The rejection rates
are virtually indistinguishable. We find no evidence that the performance of
the two tests differs in this setting.16 This does not mean that the LHS and
RHS test statistics are identical in any given sample. Particularly under the
null we sometimes find sizable disparities in p-values.
       The upshot is that it is in principle straightforward to extend the balancing
test to multiple covariates. An interesting finding is that a RHS test offers a
computationally simple alternative that closely mimics the performance of the
more standard LHS balancing test. Yet, at this point implementation issues
related to the small sample bias of robust covariance estimators also hamper
our ability to confidently carry out balancing tests for multiple covariates.
  16
    We have also experimented with basing selection into treatment si on income and in-
cluding education among the added covariates instead. The results are very similar.


                                          30
Moreover, sometimes we are interested in the robustness of the original results
when the number of added regressors is very large. An example would be a
differences-in-differences analysis in a state-year panel, where the researcher is
interested in checking whether the results are robust to the inclusion of state
specific trends. The balancing test does not seem to be the right framework
to deal with this situation. The coefficient comparison test has an important
role to play in this scenario.


6     Empirical Analysis
We illustrate the theoretical results in the context of estimating the returns
to schooling using data from the National Longitudinal Survey of Young Men
(NLS). This is a panel study of about 5,000 male respondents interviewed
from 1966 to 1981. The data set has featured in many prominent analyses of
the returns to education, including Griliches (1977) and Card (1995). We use
the NLS extract posted by David Card and augment it with the variable on
body height measured in the 1973 survey. We estimate regressions similar to
equation (2). The variable yi is the log hourly wage in 1976 and si is the
number of years of schooling reported by the respondent in 1976. Our samples
are restricted to observations without missing values in any of the variables
used in a particular table or set of tables.
    We start in Table 2 by presenting simple OLS regressions controlling for
experience, race, and past and present residence. The estimated return to
schooling is 0.075. This estimate may not reflect the causal effect of edu-
cation on income because important confounders, such as ability or family
background, are not controlled for.
    In columns (2) to (5) we include variables which might proxy for the respon-
dent’s family background. In column (2) we include mother’s education, in
column (3) whether the household had a library card when the respondent was
14, and in column (4) we add body height measured in inches. Each of these
variables is correlated with earnings, and the coefficient on education moves
moderately when these controls are included. Mother’s education captures an


                                       31
important component of a respondent’s family background. The library card
measure has been used by researchers to proxy for important parental atti-
tudes (e.g. Farber and Gibbons, 1996). Body height is a variable determined
by parents’ genes and by nutrition and disease environment during childhood.
It is unlikely a particularly powerful control variable but it is predetermined
and correlated with family background, self-esteem, and ability (e.g. Persico,
Postlewaite and Silverman, 2004; Case and Paxson, 2008). The return to edu-
cation falls by 0.1 to 0.2 log points when these controls are added. In column
(5) we enter all three variables simultaneously. The coefficients on the controls
are somewhat attenuated, and the return to education falls slightly further to
0.071.
   It might be tempting to conclude from the relatively small change in the
estimated returns to schooling that this estimate should be given a causal
interpretation. We provide a variety of evidence that this conclusion is unlikely
to be a sound one. Below the estimates in columns (2) to (5), we display the
p-values from the coefficient comparison test, comparing each of the estimated
returns to education to the one from column (1). Although the coefficient
movements are small, the tests all reject at the 5% level, and in columns (4)
and (5) they reject at the 1% level. These results might not be expected from
the size of the coefficient movements and the individual standard errors on
the years of education coefficients alone, highlighting the importance for the
formal coefficient comparison test.
   The results in columns (6) to (8), where we regress maternal education,
the library card, and body height on education, further magnify the concern.
The education coefficient is positive and strongly significant in all three re-
gressions, with t-values ranging from 4.4 to 13.1, and both the LHS and RHS
joint balancing tests reject the hypothesis that all three controls are balanced
with a p-value of virtually zero. The magnitudes of the coefficients are sub-
stantively important. It is difficult to think of these results as causal effects:
the respondent’s education should not affect predetermined proxies of family
background. Instead, these estimates reflect selection bias. Individuals with
more education have significantly better educated mothers, were more likely

                                       32
to grow up in a household with a library card, and experienced more body
growth when young. Measurement error leads to attenuation bias when these
variables are used on the right-hand side which renders them fairly useless as
controls. The measurement error matters less for the estimates in columns (6)
to (8), and these are informative about the role of selection. Comparing the
p-values at the bottom of the table to the corresponding ones for the coefficient
comparison test in columns (2) to (4) demonstrates the superior power of the
balancing test.
   The following tables have the same general layout. In Table 3 we change
the baseline specification by including the respondent’s score on the Knowl-
edge of the World of Work test (KWW), a variable used by Griliches (1977) as
a proxy for ability. The sample size is reduced due to the exclusion of missing
IQ values in the test score for consistency with a subsequent table. Estimated
returns without the KWW score in this restricted sample (unreported) are
very similar to those in Table 2. Adding the KWW score reduces the coeffi-
cient on education by almost 20%, from 0.075 to 0.061. Adding our additional
controls maternal education, the library card, and body height to this new
specification does very little to the estimated returns to education. The coef-
ficient comparison test indicates that none of the small changes in the returns
to education are statistically significant. Controlling for the KWW scores has
largely knocked out the library card effect but done little to the coefficients
on maternal education and body height. The relatively small and insignifi-
cant coefficient movements in columns (2) to (5) suggest that the specification
controlling for the KWW score might solve the ability bias problem.
   Columns (6) to (8), however, show that the three covariates are still mostly
unbalanced with respect to education even when the KWW score is in the
regression. This raises the possibility that the estimated returns in columns
(1) to (5) might remain biased by selection. The estimated coefficients on
education for the three controls are on the order of half their value from Table 2,
and the body height measure is now only significant at the 10% level. But the
relationship between mother’s and own education is still sizable, so that this
measure continues to indicate the possibility of important selection. Balance

                                        33
in library card ownership is rejected despite the fact that a comparison of the
coefficients in columns (1) and (3) indicates no role for this variable at all. A
joint balancing test with all three controls strongly rejects the hypothesis that
they are balanced. The results in this table illustrate the message of our paper
in a powerful fashion.
       While the KWW score might be a potent control, it is likely also measured
with substantial error, for example, due to testing noise. Griliches (1977) pro-
poses to instrument this measure with an independent IQ test score variable,
which is also contained in the NLS data, to eliminate at least some of the
consequences of this measurement error. In Table 4, we take the specification
one step further by instrumenting the KWW score with IQ. The coefficient
on the KWW score almost triples, in line with the idea that an individual
test score is a very noisy measure of ability. The education coefficient now
falls to only about half its previous value from 0.061 to 0.034. This might be
due to positive omitted variable bias present in the previous regressions which
is eliminated by IQ-instrumented KWW (although there may be other possi-
ble explanations for the change as well, like measurement error in schooling).
Both the coefficient comparison tests and the balancing tests (individual and
joint) indicate no evidence of selection any more. This is due to a combination
of lower point estimates and larger standard errors. We note that the joint
LHS and RHS balancing tests produce somewhat different test statistics in
this case, although both p-values are well above conventional rejection levels.
The contrast between Tables 3 and 4 highlights the usefulness of the balancing
test: it warns about the Table 3 results, while the coefficient comparison test
delivers insignificant differences in either case.
       Finding an instrumental variable for education is an alternative to control
strategies, such as using test scores. In Table 5 we follow Card’s (1995) analysis
and instrument education using distance to the nearest college, while dropping
the KWW score. We use the same sample as in Table 2, which differs from
Card’s sample.17 Our IV estimates of the return to education are slightly higher
  17
   Unlike Card, who uses two dummies for proximity to a two- and a four-year college,
we use a single dummy variable for whether there is a four-year college in the county as


                                          34
than in Table 2 but lower than in Card (1995) at around 8%. The IV returns
estimates are relatively noisy, with a t-statistic of about 2. Columns 1-5 of
Table 5 show that the IV estimate on education, while bouncing around a bit,
does not change significantly when maternal education, the library card, or
body height is included. In particular, if these three controls are included at
the same time in column (5), the point estimate is indistinguishable from the
unconditional estimate in column (1) both substantively and statistically.
    IV regressions with pre-determined variables on the left hand side can be
thought of as a test for random assignment of the instruments. In this case the
selection regressions in columns (6)-(8) are imprecise, just like the IV returns
estimates, and as a result less informative. The coefficients in the regressions
for mother’s education and body height have the wrong sign but confidence
intervals cover anything ranging from zero selection to large positive amounts.
Only the library card measure is large, positive, with a p-value of around 0.06,
possibly indicative of some remaining potential for selection even in the IV
regressions. However, with p-values of around 0.29, both the LHS and RHS
joint balancing tests fail to reject the null hypothesis that all three controls are
balanced. In other words, the college distance instrument passes the balancing
test, but the data do not speak clearly in this particular case.


7     Conclusion
Using predetermined characteristics as dependent variables offers a useful spec-
ification check for a variety of identification strategies popular in empirical
economics. We argue that this is the case even for variables which might be
poorly measured and are of little value as control variables. Such variables
should be available in many data sets, and we encourage researchers to per-
form such balancing tests more frequently. We show that this is generally a
more powerful strategy than adding the same variables on the right hand side
of the regression as controls and looking for movement in the coefficient of
instrument, and we instrument experience and experience squared by age and age squared.
We restrict Card’s sample to non-missing values in maternal education, the library card,
and body height.


                                          35
interest.
   We have illustrated our theoretical results with an application to the re-
turns to education. Taking our assessment from this exercise at face value, a
reader might conclude that the results in Table 4, returns around 3.5%, can
safely be regarded as causal estimates. Of course, this is not the conclusion
reached in the literature, where much higher IV estimates like those in Table
5 are generally preferred (see e.g. Card, 2001 or Angrist and Pischke, 2015,
chapter 6). This serves as a reminder that the discussion here is focused on
sharpening one particular tool in the kit of applied economists. Successfully
passing the balancing test should be a necessary condition for a successful
research design but it is not sufficient.
   The balancing test and other statistics we discuss here are useful for gaug-
ing selection bias due to observed confounders, even when they are potentially
measured poorly. It does not address any other issues which may also haunt a
successful empirical investigation of causal effects. One possible issue is mea-
surement error in the variable of interest, which is also exacerbated as more
potent controls are added. Griliches (1977) shows that a modest amount of
measurement error in schooling may be responsible for the patterns of returns
we have displayed in Tables 2 to 4. Another issue, also discussed by Griliches,
is that controls like test scores might themselves be influenced by schooling,
which would make them bad controls. For all these reasons, other approaches
like IV estimates of the returns may be preferable.




                                        36
References
Altonji, Joseph G., Timothy Conley, Todd E. Elder, and Christo-
  pher R. Taber. 2013. “Methods for Using Selection on Observed Variables
  to Address Selection on Unobserved Variables.” mimeographed.

Altonji, Joseph G, Todd E Elder, and Christopher R Taber. 2005.
  “Selection on Observed and Unobserved Variables: Assessing the Effective-
  ness of Catholic Schools.” Journal of Political Economy, 113(1): 151–184.

Anderson, Michael L. 2008. “Multiple Inference and Gender Differences in
  the Effects of Early Intervention: A Reevaluation of the Abecedarian, Perry
  Preschool, and Early Training Projects.” Journal of the American Statistical
  Association, 103(484): 1481–1495.

Angrist, Joshua, and Jörn-Steffen Pischke. 2009. Mostly Harmless
  Econometrics: An Empiricist’s Companion. Princeton Univeristy Press.

Angrist, Joshua, and Jörn-Steffen Pischke. 2015. Mastering Metrics:
  The Path from Cause to Effect. Princeton Univeristy Press.

Battistin, Erich, and Andrew Chesher. 2014. “Treatment Effect Es-
  timation with Covariate Measurement Error.” Journal of Econometrics,
  178(2): 707–715.

Belloni, Alexandre, Victor Chernozhukov, and Christian Hansen.
  2014a. “High-Dimensional Methods and Inference on Structural and Treat-
  ment Effects.” Journal of Economic Perspectives, 28(2): 29–50.

Belloni, Alexandre, Victor Chernozhukov, and Christian Hansen.
  2014b. “Inference on Treatment Effects after Selection among High-
  Dimensional Controls.” The Review of Economic Studies, 81(2): 608–650.

Benjamini, Yoav, and Yosef Hochberg. 1995. “Controlling the False Dis-
  covery Rate: A Practical and Powerful Approach to Multiple Testing.” Jour-
  nal of the Royal Statistical Society. Series B (Methodological), 57(1): 289–
  300.

                                      37
Bonferroni, Carlo Emilio. 1935. Il Calcolo delle Assicurazioni su Gruppi
  di Teste. Tipografia del Senato.

Bound, John, Charles Brown, Greg J Duncan, and Willard L
  Rodgers. 1994. “Evidence on the Validity of Cross-sectional and Longi-
  tudinal Labor Market Data.” Journal of Labor Economics, 12(3): 345–368.

Bruhn, Miriam, and David McKenzie. 2009. “In Pursuit of Balance:
  Randomization in Practice in Development Field Experiments.” American
  Economic Journal: Applied Economics, 1(2): 200–232.

Card, David. 1995. “Using Geographic Variations in College Proximity to
  Estimate the Returns to Schooling.” In Aspects of Labor Market Behavior:
  Essays in Honor of John Vanderkamp. , ed. Loizos Nicolaou Christofides,
  John Vanderkamp, E. Kenneth Grant and Robert Swidinsky. Toronto: Uni-
  versity of Toronto Press.

Card, David. 2001. “Estimating the Return to Schooling: Progress on Some
  Persistent Econometric Problems.” Econometrica, 69(5): 1127–1160.

Case, Anne, and Christina Paxson. 2008. “Stature and Status: Height,
  Ability, and Labor Market Outcomes.” Journal of Political Economy,
  116(3): 499–532.

Cattaneo, Matias D., Michael Jansson, and Whitney K. Newey.
  2017. “Inference in Linear Regression Models with Many Covariates and
  Heteroskedasticity.” mimeographed, Michigan, Berkeley, and MIT.

Chesher, Andrew, and Ian Jewitt. 1987. “The Bias of a Heteroskedasticity
  Consistent Covariance Matrix Estimator.” Econometrica, 55(5): 1217–1222.

Duflo, Esther, Pascaline Dupas, and Michael Kremer. 2017. “The Im-
  pact of Free Secondary Education: Experimental Evidence from Ghana.”
  mimeographed, MIT, Stanford, and Harvard.

Farber, Henry S., and Robert Gibbons. 1996. “Learning and Wage Dy-
  namics.” The Quarterly Journal of Economics, 111(4): 1007–1047.

                                     38
Gelbach, Jonah B. 2016. “When Do Covariates Matter? And Which Ones,
  and How Much?” Journal of Labor Economics, 34(2): 509–543.

Griliches, Zvi. 1977. “Estimating the Returns to Schooling: Some Econo-
  metric Problems.” Econometrica, 45(1): 1–22.

Hausman, Jerry A. 1978. “Specification Tests in Econometrics.” Economet-
  rica, 46(6): 1251–71.

Hausman, Jerry A., and William E. Taylor. 1980. “Comparing Specifi-
  cation Tests and Classical Tests.” MIT Dept. of Economics Working Paper
  no. 266.

Hausman, Jerry A., Jason Abrevaya, and F.M. Scott-Morton. 1998.
  “Misclassification of the Dependent Variable in a Discrete-response Setting.”
  Journal of Econometrics, 87(2): 239–269.

Holly, Alberto. 1982. “A Remark on Hausman’s Specification Test.” Econo-
  metrica, 50(3): 749–759.

Imbens, Guildo W. 2003. “Sensitivity to Exogeneity Assumptions in Pro-
  gram Evaluation.” American Economic Review, 93(2): 126–132.

Kling, Jeffrey R, Jeffrey B Liebman, and Lawrence F Katz. 2007.
  “Experimental Analysis of Neighborhood Effects.” Econometrica, 75(1): 83–
  119.

Lee, David S., and Thomas Lemieux. 2010. “Regression Discontinuity
  Designs in Economics.” Journal of Economic Literature, 48(2): 281–355.

MacKinnon, James G. 1992. “Model Specification Tests and Artificial Re-
  gressions.” Journal of Economic Literature, 30(1): 102–146.

MacKinnon,       James       G.,   and     Halbert   White.     1985.   “Some
  Heteroskedasticity-consistent Covariance Matrix Estimators with Improved
  Finite Sample Properties.” Journal of Econometrics, 29(3): 305–325.


                                      39
Miller, Kenneth S. 1981. “On the Inverse of the Sum of Matrices.” Mathe-
  matics Magazine, 54(2): 67–72.

Oster, Emily. forthcoming. “Unobservable Selection and Coefficient Stabil-
  ity: Theory and Evidence.” Journal of Business & Economic Statistics.

Persico, Nicola, Andrew Postlewaite, and Dan Silverman. 2004. “The
  Effect of Adolescent Experience on Labor Market Outcomes: The Case of
  Height.” Journal of Political Economy, 112(5): 1019–1053.

Rosenbaum, Paul R., and Donald B. Rubin. 1983. “Assessing Sensitivity
  to an Unobserved Binary Covariate in an Observational Study with Binary
  Outcome.” Journal of the Royal Statistical Society. Series B (Methodologi-
  cal), 45(2): 212–218.

Westfall, Peter H., and S. Stanley Young. 1993. Resampling Based Mulit-
  ple Testing: Examples and Methods for p-value Adjustment. John Wiley and
  Sons.

Young, Alwyn. 2016. “Improved, Nearly Exact, Statistical Inference with
  Robust and Clustered Covariance Matrices using Effective Degrees of Free-
  dom Corrections.” mimeographed, LSE.




                                    40
                                  Figure 1: Theoretical Rejection Rates

           1          .8
Rejection probability
     .4    .2
           0 .6




                           0            .5                  1          1.5                2
                                                            d

                               Balancing test, θ=0 (baseline)   CC test, θ=0 (baseline)
                               Balancing test, θ=.7             CC test, θ=.7
                               Balancing test, θ=.85            CC test, θ=.85




                                                       41
                                Figure 2: Theoretical and Simulated Rejection Rates

            1          .8
  Rejection probability
.2     .4   0  .6




                            0                .5                  1          1.5                   2
                                                                 d

                                 Balancing test, baseline            CC test, baseline
                                 Balancing test, θ=0, simulated      CC test, θ=0, simulated
                                 Balancing test, θ=.85, asymptotic   CC test, θ=.85, asymptotic
                                 Balancing test, θ=.85, simulated    CC test, θ=.85, simulated


                      Note: Comparison of asymptotic rejection rates with rejection
                      rates based on Monte Carlo simulations. Baseline refers to the
                      theoretical rejection rates without measurement error.




                                                            42
                           Figure 3: Simulated Rejection Rates with Heteroskedasticity

           1          .8
Rejection probability
     .4    .2
           0 .6




                           0               .5                  1           1.5              2
                                                               d

                                   Balancing test, baseline        CC test, baseline
                                   Balancing test, θ=0, robust     CC test, θ=0, robust
                                   Balancing test, θ=.85, robust   CC test, θ=.85, robust


                     Note: Comparison of baseline rejection rates (from Figure 1) with
                     simulated rejection rates based on heteroskedastic errors and ro-
                     bust standard errors.




                                                          43
Figure 4: Simulated Rejection Rates with Mean Reverting Measurement Error

            1          .8
 Rejection probability
      .4    .2
            0 .6




                            0           .5                  1           1.5               2
                                                            d

                                     Balancing test, baseline     CC test, baseline
                                                      2                     2
                                     Balancing test, σ µ=.75      CC test, σ µ=.75
                                                      2                     2
                                     Balancing test, σ µ=2.25     CC test, σ µ=2.25


                      Note: Comparison of baseline rejection rates (from Figure 1) with
                      simulated rejection rates based on mean reverting measurement
                      error and robust standard errors.




                                                       44
Figure 5: Simulated Rejection Rates with Binary Control and Misclassification

               1        .8
  Rejection probability
      .4       .6
               .2
               0




                             0           .5                  1            1.5                2
                                                             d

                                         Balancing test, τ=0        CC test, τ=0
                                         Balancing test, τ=0.1      CC test, τ=0.1


                      Note: Rejection rates for a binary control variable that is misclas-
                      sified (i.e. its binary value is flipped) with probability τ .




                                                        45
                            Figure 6: Simulated Rejection Rates with Multiple Controls

                            (a) 4 covariates, x1-x4 not balanced                                        (b) 4 covariates, only x1 not balanced
Rejection probability




                                                                            Rejection probability
0 .2 .4 .6 .8 1




                                                                            0 .2 .4 .6 .8 1
                        0          .5        1        1.5          2                                0           .5        1        1.5           2
                                             d                                                                            d


                            (c) 8 covariates, x1-x8 not balanced                                        (d) 8 covariates, only x1 not balanced
Rejection probability




                                                                            Rejection probability
0 .2 .4 .6 .8 1




                                                                            0 .2 .4 .6 .8 1
                        0          .5        1        1.5          2                                0           .5        1        1.5           2
                                             d                                                                            d


                            LHS Bal test, θ=0                  RHS Bal test, θ=0                                          CC test, θ=0
                            LHS Bal test, θ=.85                RHS Bal test, θ=.85                                        CC test, θ=.85

                   Note: Rejection rates for simultaneous tests for adding 4 or 8 ad-
                   ditional covariates at once under different specifications for the
                   alternative hypotheses.




                                                                       46
Figure 7: Rejection Rates with Multiple Controls in Actual Data from the
ACS
             1
             .8
  Rejection probability
      .4     .2
             0 .6




                          0               .2                       .4                 .6
                                   Share of treatment determined by covariate

                                   LHS balancing test         RHS balancing test


                   Note: Rejection rates based on drawing random samples of size
                   1,000 from the American Community Surveys. See text for details.




                                                   47
         Table 1: Parameters for Power Calculations and Implied R2 s

 𝜎𝜎𝑠𝑠2 = 1                                    β=1
 𝜎𝜎𝑢𝑢2 = 3                                    γ=3
𝜎𝜎𝑒𝑒2 = 30                                   n = 100
                                                R2
    d                    θ=0                 θ = 0.7              θ = 0.85
    0                    0.48                  0.16                 0.09
   0.5                   0.53                  0.23                 0.16
   1.0                   0.59                  0.33                 0.27
   1.5                   0.66                  0.44                 0.39
   2.0                   0.72                  0.54                 0.50

     Note: The implied population R2 ’s do not depend on n, but the
     subsequent power calculations do.




                                     48
                              Table 2: Baseline Regressions for Returns to Schooling and Specification Checks
                                                                                   Dependent Variable
                                                                                                   Mother's years of      Library card        Body height
                                                        Log hourly earnings                                          	                  	  
                                                                                                      education            at age 14           in inches
                                            (1)       (2)       (3)      (4)       (5)                    (6)        	        (7)       	        (8)
                                                                                                                     	          	      	          	  
                                           0.0751 0.0728 0.0735 0.0740 0.0710                           0.3946               0.0371             0.1204
     Years of education
                                          (0.0040) (0.0042) (0.0040) (0.0040) (0.0042)                 (0.0300)             (0.0040)           (0.0273)

                                             	      0.0059      	       	      0.0044    	             	         	        	       	        	  
     Mother's years of education
                                             	     (0.0029)     	       	     (0.0030)   	             	         	        	       	        	  
                                             	         	       	       	         	     	             	         	        	       	        	  
                                             	         	    0.0428      	      0.0361    	             	         	        	       	        	  
     Library card at age 14
                                                             (0.0183)            (0.0184)




49
                                             	         	                	                	             	         	        	       	        	  
                                             	         	       	       	         	     	             	         	        	       	        	  
                                             	         	       	    0.0090     0.0084    	             	         	        	       	        	  
     Body height in inches
                                             	         	       	   (0.0027)   (0.0027)   	             	         	        	       	        	  
                                             	         	       	                         	             	         	        	       	        	  
     p-values                                                                               	             	         	        	       	        	  
        Coefficient comparison test          	      0.044     0.022    0.009     0.002     	             	         	        	       	        	  
        LHS balancing test: individual       	         	                                  	          0.000         	     0.000       	     0.000
        LHS balancing test: joint            	         	                                  	                        	     0.000       	  
        RHS balancing test: joint            	         	                                  	                        	     0.000       	  
         	  
       Note: The number of observations is 2,500 in all regressions. Heteroskedasticity robust standard errors in parentheses, and
       the joint LHS balancing test is conducted via the suest Stata command All regressions control for experience, experience-
       squared, indicators for black, for southern residence and residence in a standard metropolitan statistical area (SMSA) in
       1976, indicators for region in 1966 and living in an SMSA in 1966.
                Table 3: Regressions for Returns to Schooling and Specification Checks Controlling for the KWW Score

                                                                                    Dependent Variable
                                                                                                    Mother's years of      Library card          Body height
                                                      Log hourly earnings
                                                                                                       education      	    at age 14     	      in inches
                                           (1)       (2)        (3)        (4)      (5)                    (6)                  (7)                   (8)
                                                                                                                      	                  	  
                                                                                                                      	          	      	          	  
                                          0.0609 0.0596 0.0608 0.0603 0.0591                             0.2500               0.0133               0.0731
     Years of education
                                         (0.0059) (0.0060) (0.0059) (0.0059) (0.0060)                   (0.0422)             (0.0059)             (0.0416)

                                          0.0070 0.0068 0.0069 0.0069 0.0067                             0.0410                0.0076              0.0145
     KWW score
                                         (0.0015) (0.0016) (0.0016) (0.0015) (0.0016)                   (0.0107)              (0.0016)            (0.0117)

                                                    0.0053                         0.0048
     Mother's years of education
                                                   (0.0037)                       (0.0037)




50
                                                               0.0097              0.0045
     Library card at age 14
                                                              (0.0215)            (0.0216)

                                                                          0.0078 0.0075
     Body height in inches
                                                                         (0.0034) (0.0034)
                                            	         	         	                         	             	         	         	      	          	  

     p-values
                                                                                             	             	         	         	      	          	  
        Coefficient comparison test                 0.161      0.651      0.156    0.084
                                            	  
        LHS balancing test: individual                                                                   0.000                 0.025               0.079
                                            	  
        LHS balancing test: joint           	                                                                                 0.000
        RHS balancing test: joint           	                                                                                 0.000
         	  
     Note: The number of observations is 1,773 in all regressions, due to missing values in IQ. Heteroskedasticity robust standard
     errors in parentheses, and the joint LHS balancing test is conducted via the suest Stata command. All regressions control
     for experience, experience-squared, indicators for black, for southern residence and residence in an SMSA in 1976, indicators
     for region in 1966 and living in an SMSA in 1966.
                Table 4: Regressions for Returns to Schooling and Specification Checks Instrumenting the KWW Score
                                                                                                                          Dependent Variable
                                                                                                                                                   Mother's years of      Library card                            Body height
                                                                        Log hourly earnings
                                                                                                                                                      education      	    at age 14                   	          in inches
                                                   (1)                 (2)                 (3)                 (4)         (5)                            (6)                  (7)                                     (8)
                                                                                                                                                                     	                                	  
                                                                                                                                                                     	          	                    	                    	  
                                         0.0340 0.0339 0.0342 0.0343 0.0345                                                                             0.0234               0.0168                                      -0.0486
     Years of education
                                        (0.0139) (0.0139) (0.0138) (0.0139) (0.0138)                                                                   (0.0952)             (0.0134)                                     (0.0998)

                                         0.0199 0.0195 0.0200 0.0194 0.0191 	  	                                                                        0.1496                           0.0060                         0.0728
     KWW score instrumented by IQ
                                        (0.0062) (0.0063) (0.0063) (0.0062) (0.0064) 	  	                                                              (0.0422)                         (0.0060)                       (0.0449)

                                        	  	              0.0028 	  	                          	  	              0.0026 	  	   	  	                         	  	   	  	                	  	   	  	  
     Mother's years of education
                                        	  	             (0.0039) 	  	                         	  	             (0.0039) 	  	   	  	                        	  	   	  	                	  	   	  	  

                                        	  	             	  	               -0.0130 	  	                         -0.0154 	  	   	  	                        	  	   	  	                	  	   	  	  




51
     Library card at age 14
                                        	  	             	  	              (0.0245) 	  	                        (0.0243) 	  	   	  	                        	  	   	  	                	  	   	  	  

                                        	  	             	  	              	  	               0.0070 0.0069 	  	   	  	                                     	  	   	  	                	  	   	  	  
     Body height in inches
                                        	  	             	  	              	  	              (0.0034) (0.0034) 	  	   	  	                                  	  	   	  	                	  	   	  	  
                                                    	                  	                  	                                            	                 	         	                    	       	                    	  

     p-values
                                                                                                                                          	                 	         	                    	       	                    	  
        Coefficient comparison test                                   0.818               0.634               0.636     0.552
                                                    	  
        Balancing test: individual                         	  	                                                                        	  	   	      0.806       	  	   	          0.212     	  	   	          0.626
                                                    	  
        LHS balancing test: joint                   	                                                                                                                                     0.593
        RHS balancing test: joint                   	                                                                                                                                     0.137
         	  
     Note: The number of observations is 1,773 in all regressions, due to missing values in IQ. Heteroskedasticity robust
     standard errors in parentheses, and the joint LHS balancing test is conducted by a stacked IV regression with standard
     errors clustered across the three additional covariates. All regressions control for experience, experience-squared, indicators
     for black, for southern residence and residence in an SMSA in 1976, indicators for region in 1966 and living in an SMSA
     in 1966.
       Table 5: Regressions for Returns to Schooling and Specification Checks Instrumenting Schooling by Proximity to
       College
                                                                                                   Dependent Variable
                                                                                                                       Mother's years of      Library card                Body height
                                                              Log hourly earnings                                                        	                          	  
                                                                                                                          education            at age 14                   in inches
                                            (1)            (2)           (3)          (4)          (5)                        (6)        	        (7)               	        (8)
                                                                                                                                         	          	              	          	  
     Years of education instrumented by    0.0816 0.0818 0.0778 0.0845 0.0822                                              -0.0952               0.1015                     -0.3658
     college proximity                    (0.0431) (0.0417) (0.0518) (0.0418) (0.0466)                                     (0.3594)             (0.0542)                   (0.3681)

                                             	  	      0.0030          	  	       	  	      0.0012    	  	             	  	     	  	        	  	     	  	        	  	  
     Mother's years of education
                                             	  	     (0.0143)         	  	       	  	     (0.0140)   	  	             	  	     	  	        	  	     	  	        	  	  
                                               	             	           	           	           	       	                 	         	            	         	            	  
                                             	  	         	  	    0.0367          	  	      0.0237    	  	             	  	     	  	        	  	     	  	        	  	  




52
     Library card at age 14
                                             	  	         	  	   (0.0886)         	  	     (0.0581)   	  	             	  	     	  	        	  	     	  	        	  	  
                                               	             	           	           	           	       	                 	         	            	         	            	  
                                             	  	         	  	       	  	    0.0081         0.0079    	  	             	  	     	  	        	  	     	  	        	  	  
     Body height in inches
                                             	  	         	  	       	  	   (0.0044)       (0.0032)   	  	             	  	     	  	        	  	     	  	        	  	  
                                               	             	           	                                 	                 	         	            	         	            	  
     p-values                                                                                                 	                 	         	            	         	            	  
        Coefficient comparison test            	        0.873         0.686        0.380         0.908       	                 	         	            	         	            	  
        Balancing test: individual             	           	  	                                          	  	          0.791         	  	     0.061         	  	     0.321
        LHS balancing test: joint              	           	  	                                          	  	                          	       0.290           	  
        RHS balancing test: joint              	             	                                              	                            	       0.291           	  
         	  
       Note: The number of observations is 2,500 in all regressions. Heteroskedasticity robust standard errors in parentheses,
       and the joint LHS balancing test is conducted by a stacked IV regression with standard errors clustered across the three
       additional covariates. All regressions control for experience, experience-squared, indicators for black, for southern residence
       and residence in an SMSA in 1976, indicators for region in 1966 and living in an SMSA in 1966.
                   Appendix (For Online Publication Only)


    Appendix (For Online Publication Only)

A        Power Functions
A.1      The Balancing Test
The desired balancing regression is

                               xi = δ0 + δsi + ui ,

but xi is measured with error

                                  xm
                                   i = xi + mi .


Effectively, we run the balancing regression

                          xm    m    m
                           i = δ0 + δ si + ui + mi .


    As mentioned in Section 5.1, in the theoretical derivation of the power func-
tions we abstract away from the sampling variation in estimating the standard
errors by treating σu , σm and σs as known constants. In this case, the asymp-
totic variance of δbm can be directly calculated, and the resulting test statistic
for the null hypothesis that the balancing coefficient δ is zero is

                                   δbm                δbm
                         tδm =      =               √ 2   2
                                                                .
                                                       σu +σm
                                 se δbm         √1
                                                  n     σs

Define
                                         2
                                        σm
                            θ =
                                    σu2 + σm
                                           2

                                                       σu2
                                 ⇒ σu2 + σm
                                          2
                                            =
                                                      1−θ
Hence                                    √      √
                                             nσs 1 − θ
                             tδm = δbm                 .
                                                σu



                                         53
                  Appendix (For Online Publication Only)


The rejection probability when δ = d and when using critical value C is

Pr (|tδm | > C) = Pr (tδm > C) + Pr (tδm < −C)
                                                         
                             m                      m
                           δ
                           b                      δ
                                                  b
                = Pr    > C  + Pr    < −C 
                        se δbm                 se δbm
                                          √    √
                                                      
                          m
                         δ −d
                         b                  nσs 1 − θ 
                = Pr    > C − d
                        se δbm                 σu
                                              √     √
                                                         
                             m
                            δ −d
                            b                   nσs 1 − θ 
                  + Pr    < −C − d
                           se δbm                  σu
                                 √    √                  √    √      
                                    nσs 1 − θ                 nσs 1 − θ
                ≈ 1−Φ C −d                      + Φ −C − d
                                       σu                        σu
when n is large. This is the power function of the balancing test
                              √     √                      √    √      
                                 nσs 1 − θ                      nσs 1 − θ
P owertδm (d) = 1−Φ 1.96 − d                  +Φ −1.96 − d                  .
                                     σu                            σu

A.2       The Coefficient Comparison Test
The short and long regressions are

                         yi = αs + β s si + esi
                         yi = α + βsi + γxi + ei ,

and
                             xi = δ0 + δsi + ui .

Adding measurement error in xi :

                               xm
                                i = xi + mi ,


we have

                       yi = αs + β s si + esi
                       yi = αm + β m si + γ m xm    m
                                               i + ei

                      xm
                       i = δ0 + δsi + ui + mi .

                                     54
                     Appendix (For Online Publication Only)


   Treat si , ui , ei , and mi as the underlying random variables which determine
xi , yi , esi and em
                   i . We normalize si to a mean zero variable. For the derivations
in the remainder of this section, we make the following assumptions:
   Assumption A1: si , ui , ei and mi are mutually independent;
   Assumption A2: E[u3i ] = 0.
   Note that Assumptions A1 and A2 are satisfied in the DGP’s we adopt
for the Monte Carlo simulations underlying Figure 2, that is, when si , ui ,
ei , mi follow a joint normal distribution with the first two moments specified
according to
                                   2                  
                   si              0       σs 0 0 0
                  ui           0   0 σu2 0 0
                                                          
                  ei  ∼            ,
                                                         .              (A1)
                                   0   0 0 σe2 0          
                                                  2
                   mi              0       0 0 0 σm

A.2.1    Population Parameters

In this subsection, we derive the expressions of population regression coeffi-
cients β m and γ m in terms of the model parameters, as discussed in Section 3.
Performing an anatomy to the multiple regression (9), we have

                              Cov(yi , ui + mi )      σ2
                       γm =                      =γ 2 u 2 ,                   (A2)
                               V ar(ui + mi )      σu + σ m

where ui + mi is the residual from the population regression of xm
                                                                 i on si . Using
θ as defined above, equation (A2) becomes

                                  γ m = γ(1 − θ).                             (A3)

By the omitted variable bias formula, we have

                                 β s = β + γδ
                                 β s = β m + γ m δ,

and therefore
                                  β m = β + γδθ.                              (A4)



                                        55
                   Appendix (For Online Publication Only)


   As mentioned in the main text, an alternative representation of θ is
                                               1−λ
                                         θ=          ,                     (A5)
                                              1 − R2
where
                                              V ar (xi )
                                     λ=
                                              V ar (xm
                                                     i )

is the reliability of xm        2                   2                      m
                       i , and R is the population R of the regression of xi on
si . To see why (A5) holds, notice that

                         V ar(xi ) = δ 2 σs2 + σu2
                        V ar(xm      2 2     2    2
                              i ) = δ σs + σ u + σm
                                            σ2 + σ2
                              R2 = 1 − 2 2 u 2 m 2 ,
                                        δ σs + σu + σ m
from which equation (A5) mechanically follows.

A.2.2    Asymptotic Variance in the Coefficient Comparison Test un-
         der Homoskedasticity

For the coefficient comparison test β s − β m = 0, the test statistic is

                                                 βbs − βbm
                          t(β s −β m )   =q                       ,
                                               V ar(βbs − βbm )

which is asymptotically standard normal. As mentioned in Section 4, we rely
on the delta method equation (13) to derive V ar(βbs − βbm ). We have already
shown in the previous subsection that
                                         1    σu2
                            V ar(δbm ) =              ,                   (A6)
                                         n (1 − θ)σs2
                                          
                    γ m ) and Cov δbm , γ
and we derive V ar (b                   bm in the remainder of this subsection.
For simplicity of exposition, we make an additional assumption:
   Assumption A3: V ar(em        m
                        i |si , xi ) is constant.
   Like Assumptions A1 and A2, Assumption A3 is also satisfied in the DGP’s
underlying Figure 2. In the subsection below, we also derive the general ex-
pression of V ar(βbs − βbm ) when Assumption A3 is relaxed.

                                               56
                    Appendix (For Online Publication Only)


                           γ m ), first note that
   In order to derive V ar(b
                                          1 V ar (em  i )
                                γ m) =
                          V ar (b                           ,                 (A7)
                                          n V ar (ui + mi )
where, as mentioned above, ui + mi is the residual from the population regres-
sion of xm                                  2    2
         i on si . Since V ar (ui + mi ) = σu + σm , the missing piece in equation
(A7) is V ar (em
               i ). Plugging (A3) and (A4) into (9), we get

        yi = αm + β m si + γ m xm    m
                                i + ei

           = αm + (β + γδθ) si + γ (1 − θ) xm    m
                                            i + ei

           = (αm + γ (1 − θ) δ0 ) + (β + γδ) si + γ (1 − θ) (ui + mi ) + em
                                                                          i

Since

                   yi = α + βsi + γ (δ0 + δsi + ui ) + ei
                       = (α + γδ0 ) + (β + γδ) si + γui + ei ,

matching residuals yields

           γui + ei = γ (1 − θ) (ui + mi ) + em
                                              i

                em
                 i = γθui − γ (1 − θ) mi + ei
                                           2 2
          V ar (em       2 2 2    2
                 i ) = γ θ σu + γ (1 − θ) σm + σe
                                                     2

                                  2
                                       2                  2 !
                                                     σu2
                                              
                         2       σm        2                  2
                     = γ       2     2
                                          σu +     2     2
                                                             σm + σe2
                              σu + σ m           σ u + σm
                     = γ 2 θσu2 + σe2 .

   So
                                      1 γ 2 θσu2 + σe2
                             γ m) =
                       V ar (b
                                      n σu2 + σm   2

                                                       σe2
                                                          
                                      1−θ        2
                                    =           γ θ+ 2 .                      (A8)
                                        n              σu
As for Cov(δbm , γ
                 bm ), first note that
                                          P
                                           i (u mi )(si − s̄)
                                                i+
                         δbm − δ =            P         2
                                                                              (A9)
                                            i (si − s̄)
                                      P m m ¯m
                                       e (e x −x    e )
                       bm − γ m
                       γ            = P i i m 2                              (A10)
                                         em       ¯
                                         x i −x   e

                                          57
                    Appendix (For Online Publication Only)


             ¯ m are the sample averages of si and x
                                                   em       em      m   b bm
where s̄ and x
             e                                       i with x i = x i − δ0 − δ s i
being the residual from regressing xm
                                    i on si . By Assumption A1 along with
                   p                p
the fact that δ̂0 → δ0 and δ̂ m → δ, the asymptotic joint distribution of the
numerators in equations (A9) and (A10) is
                   P                        
               1      i (u i + mi )(si − s̄)
              √       P m m ¯m
                n         e (e x −x  e )
                   i i2 i 2 2                                      
            d             (σu + σm ) σs        E[si (ui + mi )2 em
                                                                 i ]
          −→N 0,                                                        .
                      E[si (ui + mi )2 em                  2 m 2
                                         i ] E[(ui + mi ) (ei ) ]

By Assumptions A1 and A2,

         E[si (ui + mi )2 em                    2
                           i ] = E[si (ui + mi ) (γθui − γ (1 − θ) mi + ei )]

                             = 0.

Since the denominators of equations (A9) and (A10) converge in probability
to positive constants,
                                  Cov(δbm , γ
                                            bm ) = 0.                           (A11)

Plugging equations (A6), (A8) and (A11) into (13) yields
                               1
             V ar(βbs − βbm ) ≡  Vβ (d; γ)
                               n          2 2
                                                        δ 2 σe2
                                                                
                               1           γ σu     2 2
                              = (1 − θ)         + θδ γ + 2 .                    (A12)
                               n            σs2          σu

   Recall that
                          β s − β m = δγ m = δγ (1 − θ) ,

so the power function of the coefficient comparison test is
                                       √            !             √            !
                                         nγ (1 − θ)                 nγ (1 − θ)
P owert(βs −βm ) (d; γ) = 1−Φ 1.96 − d p              +Φ −1.96 − d p             .
                                          Vβ (d; γ)                  Vβ (d; γ)

A.2.3    Relaxing Assumption A3

In this subsection, we provide the expression for V ar(βbs − βbm ) while relaxing
the conditional homoskedasticity of em
                                     i , i.e. Assumption A3. Our derivation




                                          58
                   Appendix (For Online Publication Only)


of this asymptotic variance expression still relies on equation (13). Since equa-
tions (A6) and (A11) are not affected by Assumption A3, we will only need
                                 γ m ).
the general expression for V ar (b
   Representing model (9) in matrix form,

                                  yi = Wi0 Γ + em
                                                i ,

                        0                     m 0
where Wi = (1, si , xm                 m  m
                     i ) and Γ = (α , β , γ ) . The asymptotic variance-
covariance matrix of the regression estimator Γ
                                              b is
                1
                  E[Wi Wi0 ]−1 E[Wi Wi0 (em    2       0 −1
                                            i ) ]E[Wi Wi ] .
                n
Expressing E[Wi Wi0 ] in terms of the fundamental model parameters is straight-
forward:
                                                     
                                 1    si     xm i
               E[Wi Wi0 ] = E  si    s2i   s i xmi
                                                      
                                 m       m      m 2
                                xi si xi (xi )
                                                              
                              1 0                   δ0
                          =  0 σs2                δσs2        .
                              δ0 δσs2 δ02 + δ 2 σs2 + σu2 + σm
                                                             2


As before, we set E[si ] = 0, which sacrifices no generality since the mean does
not enter the variance calculation in any case.
   Writing out the entries in the matrix E[Wi Wi0 (em  2
                                                    i ) ]:

                    E[Wi Wi0 (em    2
                                 i ) ]
                         (em    2
                                       si (em     2
                                                       xm       m 2
                                                                       
                            i )               i )         i (ei )
                        | {z  }       |    {z    }    |    {z     }    
                          (i)              (ii)            (iii)       
                       s (em )2
                       i i            s2i (em i )
                                                  2
                                                      s i xm
                                                           i (ei )
                                                                  m 2   
                  = E                                                  .
                                       | {z }         | {z }            
                                          (iv)             (v)         
                       m m 2                             m 2 m 2
                       xi (ei ) si xm           m 2                    
                                          i (ei )    (xi ) (ei )        
                                                     | {z }
                                                            (vi)

Below we express quantities (i) to (vi) in terms of the fundamental model
parameters. Letting κm = E[m4i ] and κu = E[u4i ] and utilizing Assumptions 1
and 2, we have the expressions for (i) to (vi):

                    E[(em  2                               2
                        i ) ] = E[(γθui − γ(1 − θ)mi + ei ) ]

                               = γ 2 θ2 σu2 + γ 2 (1 − θ)2 σm
                                                            2
                                                              + σe2 ,         (i)

                                           59
                   Appendix (For Online Publication Only)


                  E[si (em  2                                  2
                         i ) ] = E[si (γθui − γ(1 − θ)mi + ei ) ]

                             = 0,                                            (ii)

                E[xm   m 2                              m 2
                   i (ei ) ] = E[(δ0 + δsi + ui + mi )(ei ) ]

                             = δ0 E[(em  2            m 2
                                      i ) ] + δE[si (ei ) ]

                             = δ0 (γ 2 θ2 σu2 + γ 2 (1 − θ)2 σm
                                                              2
                                                                + σe2 ),     (iii)

                E[s2i (em  2       2                          2
                        i ) ] = E[si (γθui − γ(1 − θ)mi + ei ) ]

                            = σs2 (γ 2 θ2 σu2 + γ 2 (1 − θ)2 σm
                                                              2
                                                                + σe2 ),     (iv)

and

              E[si xm   m 2                                    m 2
                    i (ei ) ] = E[si (δ0 + δsi + ui + mi ) · (ei ) ]

                             = δ0 E[si (em  2        2 m 2
                                         i ) ] + δE[si (ei ) ]

                             + E[si ui (γθui − γ(1 − θ)mi + ei )2 ]
                             + E[si mi (γθui − γ(1 − θ)mi + ei )2 ]
                             = δσs2 (γ 2 θ2 σu2 + γ 2 (1 − θ)2 σm
                                                                2
                                                                  + σe2 ).   (v)

Finally, for the expression of (vi)

            E[(xm  2 m 2                             2 m 2
                i ) (ei ) ] = E[(δ0 + δsi + ui + mi ) (ei ) ]

                             = δ02 E[(em  2     2   2 m 2
                                       i ) ] + δ E[si (ei ) ]

                                 +E[u2i (γθui − γ(1 − θ)mi + ei )2 ]
                                 +E[m2i (γθui − γ(1 − θ)mi + ei )2 ]
                                 +2δ0 δE[si (em  2               m 2
                                              i ) ] + 2δ0 E[ui (ei ) ]

                                 +2δ0 E[mi (em  2                m 2
                                             i ) ] + 2δE[si ui (ei ) ]

                                 +2δE[si mi (em  2               m 2
                                              i ) ] + 2E[ui mi (ei ) ].

Note that

                         E[si (em  2
                                i ) ] = 0

                         E[ui (em  2           m 2
                                i ) ] = E[mi (ei ) ] = 0

                       E[si ui (em  2              m 2
                                 i ) ] = E[si mi (ei ) ] = 0,


                                         60
                  Appendix (For Online Publication Only)


and we only need to find the expressions for

                  E[u2i (γθui − γ(1 − θ)mi + ei )2 ]
              = E[u2i {γ 2 θ2 u2i + γ 2 (1 − θ)2 m2i + e2i
                  −2γ 2 θ(1 − θ)ui mi + 2γθui ei − 2γ(1 − θ)mi ei }]
              = γ 2 θ2 E[u4i ] + γ 2 (1 − θ)2 σu2 σm
                                                   2
                                                     + σu2 σe2
              = γ 2 θ2 κu + γ 2 (1 − θ)2 σu2 σm
                                              2
                                                + σu2 σe2 ,


                  E[m2i (γθui − γ(1 − θ)mi + ei )2 ]
              = E[m2i {γ 2 θ2 u2i + γ 2 (1 − θ)2 m2i + e2i
                  −2γ 2 θ(1 − θ)ui mi + 2γθui ei − 2γ(1 − θ)mi ei }]
              = γ 2 θ2 σu2 σm
                            2
                              + γ 2 (1 − θ)2 κm + σm
                                                   2 2
                                                     σe ,

and

      E[ui mi (em  2                                     2
                i ) ] = E[ui mi (γθui − γ(1 − θ)mi + ei ) ]

                      = E[ui mi {γ 2 θ2 u2i + γ 2 (1 − θ)2 m2i + e2i
                          −2γ 2 θ(1 − θ)ui mi + 2γθui ei − 2γ(1 − θ)mi ei }]
                      = −2γ 2 θ(1 − θ)σu2 σm
                                           2
                                             .

Putting these terms together,

           E[(xm  2 m 2       2     m 2      2   2 m 2
               i ) (ei ) ] = δ0 E[(ei ) ] + δ E[si (ei ) ]

                              + E[u2i (γθui − γ(1 − θ)mi + ei )2 ]
                              + E[m2i (γθui − γ(1 − θ)mi + ei )2 ]
                              + 2E[ui mi (em  2
                                           i ) ]

                            = δ02 {γ 2 θ2 σu2 + γ 2 (1 − θ)2 σm
                                                              2
                                                                + σe2 }
                              + δ 2 σs2 (γ 2 θ2 σu2 + γ 2 (1 − θ)2 σm
                                                                    2
                                                                      + σe2 )
                              + {γ 2 θ2 κu + γ 2 (1 − θ)2 σu2 σm
                                                               2
                                                                 + σu2 σe2 }
                              + {γ 2 θ2 σu2 σm
                                             2                      2 2
                                               + γ 2 (1 − θ)2 κm + σm σe }
                              − {4γ 2 θ(1 − θ)σu2 σm
                                                   2
                                                     }.                         (vi)

                                          61
                   Appendix (For Online Publication Only)


Now that we have the expression for both E[Wi Wi0 ] and E[Wi Wi0 (em  2
                                                                   i ) ], we
                                       bm
can compute the asymptotic variance of γ
                                       σe2
                                         
                m   1             2
        V ar (b
              γ )=     (1 − θ) γ θ + 2
                    n                  σu
                                                                 
                                                                 
                                   4 2              4
                                                                
                                                               2 
                                                                 
                          2 (κu − 3σu )θ     (κm − 3σm )(1 − θ)
                       +γ      2 + σ 2 )2
                                          +        2 + σ 2 )2
                                                                   .
                             (σm    u            (σm     u       
                                                                 
                        |                   {z                  }
                                                                 
                                                (a)

Compared to its expression under homoskedasticity (A8), we have an extra
term (a) that accounts for the excess kurtosis of the u and m distributions. It
follows that
         1                           
           Vβ (d; γ) = V ar βbs − βbm
         n                          2 2
                                                      δ 2 σe2
                                                             
                       1             γ σu       2 2
                     =     (1 − θ)          + θδ γ + 2
                       n              σs2              σu
                                         4 2                4
                                                              )(1 − θ)2
                                                                       
                          2 2 (κu − 3σu )θ       (κm − 3σm
                       +γ δ        2 + σ 2 )2
                                              +        2 + σ 2 )2
                                                                           .
                                (σm      u           (σm        u
                                                              4
Note that when ui and mi are normal, κu − 3σu4 = 0 and κm − 3σm = 0,
and the
     variance expression above simplifies to that of equation (A12). Since
       s    m
V ar βb − βb increases in κu and κm and that the balancing test is unaffected
by the heteroskedasticity of em , the power advantage of the balancing test is
larger when ui and mi have thicker tails than a normal distribution.


B      Comparison with Oster (forthcoming)
The Oster (forthcoming) formulation of the causal regression takes the form

                         yi = α + βsi + ρw1i + w2i + ei ,

where w1i is an observed covariate and w2i is an unobserved covariate, uncor-
related with w1i . To map this into our setup, think of the true xi as capturing
both w1i and w2i , i.e. xi = ρw1i + w2i . Furthermore, there is equal selection,
i.e.
                          Cov(si , ρw1i )   Cov(si , w2i )
                                   2
                                          =                ,
                              2
                             ρ σ1               σ22

                                         62
                   Appendix (For Online Publication Only)


where σ12 and σ22 are the variances of w1i and w2i , respectively. Then, Oster’s
(forthcoming) regression can be written as

                               yi = α + βsi + xi + ei ,

which is our regression with γ = 1 (the scaling of xi is arbitrary of course; it
could be xi = w1i + w2i /ρ instead and γ = ρ or anything else).
   Our observed xm
                 i = ρw1i , so measurement error mi = −w2i . Measurement
error here is mean reverting, i.e.

                                   mi = κxi + µi                          (A13)

with κ < 0. Notice that
                                Cov (mi , xi ) = −σ22 ,

and hence
                                            −σ22
                                   κ=                                     (A14)
                                         ρ2 σ12 + σ22
and

                    µi = −w2i − κ (ρw1i + w2i )
                        = −κρw1i − (1 + κ) w2i
                              σ2             ρ2 σ 2
                        = 2 2 2 2 ρw1i − 2 2 1 2 w2i .
                          ρ σ 1 + σ2      ρ σ1 + σ 2

It turns out that µi implicitly defined in (A13) and κ given by (A14) imply
Cov(xi , µi ) = 0 and Cov(si , µi ) = 0. Hence, these two equations represent
mean reverting measurement error as defined in the body of the manuscript.
However, note that Cov(si , µi ) = 0 depends on the equal selection assumption.
With proportional selection, i.e.

                            Cov(si , ρw1i )   Cov(si , w2i )
                        φ            2
                                            =                ,
                                2
                               ρ σ1               σ22

and φ 6= 1 we would have Cov(si , µi ) 6= 0.




                                          63
                   Appendix (For Online Publication Only)


C     Comparison of the LHS and RHS Balancing
      Tests
We compare the LHS and RHS balancing tests introduced in Section 5.3. The
F -statistic of the LHS balancing test is
                                                    1 0
                                 FLHS =                   ar(δ̂)−1 δ̂
                                                      δ̂ vd
                                                    k

and the variance estimator vd
                            ar(δ̂) is

                P                                            −1
                      i    s2i P0            ···      0
                     0          i    s2i    ···      0       
     vd
      ar(δ̂) =                                                     ·
                                                             
                      ..         ..          ..       ..      
                       .          .             .
                                                     P.
                                                             
                                                              2
                    0            0           ···           i si
                   P 2 2                     P 2                          P 2                   
                          s û                      si û(1)i û(2)i · · ·    s û û
                   P i2 i (1)i                    i
                                                  P 2 2                    Pi i2 (1)i (k)i
               
                     i si û(2)i û(1)i              i si û(2)i     ···    i si û(2)i û(k)i
                                                                                                 
                                                                                                 ·
                                                                                                
                           ..                           ..           ..           ..
                                                                         .
                   P 2 .                                  .                 P .2 2
                                                                                                
                                              P 2
                     i si û(k)i û(1)i           i si û(k)i û(2)i · · ·     i si û(k)i
                  P 2                                          −1
                     i si P0                 ···        0
                                     2
                   0             i si       ···        0 
                                                          ..  ,
                                                               
                   ..           ..          ..
                     .            .             .
                                                     P. 2
                                                               
                      0          0           ···           i si

which allows for correlations in the error terms across covariates. Under the
multivariate analog of Assumption A1,
                                      p     1           0     1
                       v ar(δ̂) →
                      nd                     4
                                               E[s2
                                                  i ui ui ] =  2
                                                                 E[ui u0i ]
                                            σs                σs

Hence,
                       k        p
                         FLHS → σs2 δ 0 (E[ui u0i ])−1 δ.              (A15)
                       n
On the other hand, the F -statistic for the RHS balancing test following the
regression
                                            si = π 0 xi + vi



                                                     64
                   Appendix (For Online Publication Only)


is
                                        1 0
                             FRHS =           ar(π̂)−1 π̂
                                          π̂ vd                           (A16)
                                        k
The probability limit of π̂ is

                                     π = Ωx −1 ς                          (A17)

where Ωx = var(xi ) and ς = cov(xi , si ). The probability limit of the variance
estimator is
                            p
                  v ar(π̂) → Ωx −1 E[(xi x0i )(si − π 0 xi )2 ]Ωx −1 .
                 nd                                                       (A18)
Plugging (A17) and (A18) into (A16), the probability of the scaled F -stat of
the RHS balancing test is
                    k      p
                      FRHS → ς 0 E[(xi x0i )(si − π 0 xi )2 ]−1 ς
                    n
                           = σs4 δ 0 E[(xi x0i )(si − π 0 xi )2 ]−1 δ     (A19)

The probability limits (A15) and (A19) are in general different. An analytical
comparison between the two is complicated, as it depends on the higher mo-
ments of s and u. However, we show below that the two scaled F -statistics
have the same probability limits, in the special case where the LHS balancing
regression has a spherical error structure and the RHS balancing regression is
homoskedastic. As mentioned in Section 5.3, we conduct additional investiga-
tions of the relative powers of the two tests via simulation using ACS data.

C.1     Special Case: Spherical LHS Error Structure and
        Homoskedastic RHS Regression
We consider the special case where the RHS regression is homoskedastic and
the LHS balancing regression has a spherical error structure, i.e.

                                  var(ui ) = σu2 I k ,

which is satisfied if s and u are both normally distributed. Substituting this
into (A15), the LHS F -statistic simplifies to
                                            2 0
                                  k      p σ δ δ
                                    FLHS → s 2
                                  n         σu

                                          65
                      Appendix (For Online Publication Only)


     For the RHS F -statistic, homoskedasticity allows us to write

                   E[(xi x0i )(si − π 0 xi )2 ] = E[xi x0i ]E[(si − π 0 xi )2 ]

To find the expression of E[xi x0i ]E[(si − π 0 xi )2 ], first note that

                           σs2 = var(π 0 xi ) + E[(si − π 0 xi )2 ]

so

                           E[(si − π 0 xi )2 ] = σs2 − var(π 0 xi )

with

                                  var(π 0 xi ) = π 0 Ωx π
                                                  = ς 0 Ωx −1 ς
                                                  = σs4 δ 0 Ωx −1 δ.                      (A20)

                                                              σs2 0
Since rank(δδ 0 ) = 1 and tr[(σs2 δδ 0 )( σ12 I k )−1 ] =      2 δ δ,
                                                              σu
                                                                        by Miller (1981) we have
                                              u


                                    1         1       1
                         Ωx −1 =     2
                                       I−     σ 2 0        σ 2 δδ 0
                                                       2 )2 s
                                    σu              (σ
                                          1 + σ2 δ δ u
                                               s
                                                       u

                                    1          σs2         0
                                  = 2I − 2 2       2 2 0 δδ .                             (A21)
                                   σu   (σu ) + σu σs δ δ

Plugging (A21) into (A20):

                                σs4 δ 0 δ           σs6 (δ 0 δ)2
                 var(π 0 xi ) =           −
                                  σu2         (σu2 )2 + σu2 σs2 δ 0 δ
                                σs4 δ 0 δ[(σu2 )2 + σu2 σs2 δ 0 δ] − σs6 (δ 0 δ)2 σu2
                              =
                                               (σu2 )2 [σu2 + σs2 δ 0 δ]
                                     σs4 δ 0 δ(σu2 )2
                              = 2 2 2
                                (σu ) [σu + σs2 δ 0 δ]
                                    σ4δ0δ
                              = 2 s 2 0
                                σu + σ s δ δ




                                                  66
                       Appendix (For Online Publication Only)


It follows that

                       E[(si − π 0 xi )2 ] = σs2 − var(π 0 xi )
                                                      σ4δ0δ
                                           = σs2 − 2 s 2 0
                                                   σu + σ s δ δ
                                             σ [σ + σ 2 δ 0 δ] − σ 4 δ 0 δ
                                               2 2
                                           = s u 2 s 2 0 s
                                                    σu + σs δ δ
                                                  2 2
                                                 σ σ
                                           = 2 s 2u 0
                                             σ u + σs δ δ

As a result, the probability limit of nk FRHS is

                   σs4 δ 0 E[(xi x0i )]−1 E[(si − π 0 xi )2 ]−1 δ
                                  σu2 + σs2 δ 0 δ
                  =σs4 δ 0 Ω−1
                             x                    δ
                                      σs2 σu2
                                                  σs2                  σu + σs2 δ 0 δ
                                                                      2
                     4 0       1                                    0
                  =σs δ           I  −                           δδ                   δ
                              σu2        (σu2 )2 + σu2 σs2 δ 0 δ          σs2 σu2
                        0
                           δ δ(σu2 + σs2 δ 0 δ) (δ 0 δ)2 σs2
                                                                   
                     4
                  =σs                               −
                                    σs2 σu4              σs2 σu4
                        0 2
                           δ δσu
                  =σs4
                            σs2 σu4
                   σ2δ0δ
                  = s2
                     σu

Therefore,
                                     k           k
                                 plim FLHS = plim FRHS .
                                     n           n




                                                  67
