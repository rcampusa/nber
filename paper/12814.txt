                               NBER WORKING PAPER SERIES




                                 PREDICTIVE SYSTEMS:
                          LIVING WITH IMPERFECT PREDICTORS

                                          Lubos Pastor
                                       Robert F. Stambaugh

                                       Working Paper 12814
                               http://www.nber.org/papers/w12814


                     NATIONAL BUREAU OF ECONOMIC RESEARCH
                              1050 Massachusetts Avenue
                                Cambridge, MA 02138
                                    January 2007




Graduate School of Business, University of Chicago, NBER, and CEPR (Pastor) and the Wharton
School, University of Pennsylvania, and NBER (Stambaugh). Helpful comments were received from
seminar participants at the Fall 2006 NBER Asset Pricing Meeting, the Wharton Frontiers of Investing
conference, Boston College, Hong Kong University of Science and Technology, National University
of Singapore, Singapore Management University, University of Chicago, University of Iowa, University
of Michigan, University of Pennsylvania, University of Texas at Austin, and University of Texas at
Dallas. We also thank Ken French, Pietro Veronesi, and especially Jonathan Lewellen (NBER discussant)
and John Cochrane for many helpful suggestions. The views expressed herein are those of the author(s)
and do not necessarily reflect the views of the National Bureau of Economic Research.

¬© 2007 by Lubos Pastor and Robert F. Stambaugh. All rights reserved. Short sections of text, not to
exceed two paragraphs, may be quoted without explicit permission provided that full credit, including
¬© notice, is given to the source.
Predictive Systems: Living with Imperfect Predictors
Lubos Pastor and Robert F. Stambaugh
NBER Working Paper No. 12814
January 2007
JEL No. G1

                                               ABSTRACT

The standard regression approach to modeling return predictability seems too restrictive in one way
but too lax in another. A predictive regression models expected returns as an exact linear function
of a given set of predictors but does not exploit the likely economic property that innovations in expected
returns are negatively correlated with unexpected returns. We develop an alternative framework -
a predictive system - that accommodates imperfect predictors and beliefs about that negative correlation.
 In this framework, the predictive ability of imperfect predictors is supplemented by information in
lagged returns as well as lags of the predictors. Compared to predictive regressions, predictive systems
deliver different and substantially more precise estimates of expected returns as well as different assessments
of a given predictor's usefulness.

Lubos Pastor
Graduate School of Business
University of Chicago
5807 South Woodlawn Ave
Chicago, IL 60637
and NBER
lubos.pastor@chicagogsb.edu

Robert F. Stambaugh
Finance Department
The Wharton School
University of Pennsylvania
Philadelphia, PA 19104-6367
and NBER
stambaugh@wharton.upenn.edu
1.         Introduction

Many studies in finance analyze comovement between the expected return on stocks and various
observable quantities, or ‚Äúpredictors.‚Äù A question of frequent interest is how x t , a vector of pre-
dictors observed at time t , is related to  t , the conditional expected return defined in the equation

                                               r tC1 D  t C u tC1 ;                                              (1)

where r tC1 denotes the stock return from time t to time t C 1 and the unexpected return u tC1
has mean zero conditional on information available at time t . One approach to modeling expected
returns is to use a ‚Äúpredictive regression‚Äù in which r tC1 is regressed on x t and the expected return
is given by  t D a C b 0 x t ; where a and b denote the regression‚Äôs intercept and slope coefficients.1
This approach seems too restrictive in modeling expected return as an exact linear function of
the observed predictors. It seems more likely that the predictors are imperfect, in that they are
correlated with  t but cannot deliver it perfectly.

         At the same time, the predictive regression approach seems too lax in ignoring a likely eco-
nomic property of the unexpected return‚Äîits negative correlation with the innovation in the ex-
pected return. For example, if the expected return obeys the first-order autoregressive process,

                                           tC1 D Àõ C Àá t C w tC1 ;                                              (2)

then it seems likely that the correlation between the unexpected return and the expected-return
innovation is negative, or that uw  .u tC1 ; w tC1 / < 0. That is, an unanticipated increase in
expected future returns (or discount rates) should be accompanied by an unexpected negative return
(or price drop). The likely negative correlation between expected and unexpected returns, which
is not exploited in estimating the predictive regression, emerges as an important consideration in
estimating expected returns when predictors are imperfect.

   Our view that uw is likely to be negative seems reasonable. As observed by Shiller (1981) and
LeRoy and Porter (1981), return volatility appears to be higher than what a constant expected return
can accommodate. If uw were positive, however, return volatility would have to be lower than
when the expected return is constant. In other words, the ‚Äúexcess volatility puzzle‚Äù would be even
     1
     Of the many studies that estimate predictive regressions for stock returns, some early examples include Fama and
Schwert (1977), Rozeff (1984), Keim and Stambaugh (1986), Campbell (1987), and Fama and French (1988). There
is also a substantial literature analyzing econometric issues associated with predictive regressions, including Mankiw
and Shapiro (1986), Stambaugh (1986, 1999), Nelson and Kim (1993), Elliott and Stock (1994), Cavanagh, Elliot, and
Stock (1995), Ferson, Sarkissian, and Simin (2003), Lewellen (2004), Campbell and Yogo (2006), and Jansson and
Moreira (2006).



                                                          1
more puzzling. To see this, first note that the unexpected return can be represented approximately
as
                                      u tC1 D C;tC1      E;tC1 ;                                 (3)
where C;tC1 represents the unanticipated revisions in expected future cash flows and E;tC1 cap-
tures the revisions in expected future returns (Campbell, 1991). If the expected return follows the
process in (2) with 0 < Àá < 1, then E;tC1 D gw tC1 , where g > 0 is a constant, so

                                      uw D .u tC1 ; E;tC1 /:                                    (4)

It follows directly from equations (3) and (4) that uw < 0 if and only if
                                                           .E;tC1/
                                  .C;tC1 ; E;tC1 / <              ;                             (5)
                                                          .C;tC1 /
where the  ‚Äôs denote standard deviations. It is easy to see from equation (3) that a violation
of the condition in (5) would require that .u tC1 / < .C;tC1 /, or that returns be less volatile
than when the expected return is constant. The condition in (5) is violated if cash flow shocks
are more important than discount rate shocks in explaining the variance of stock returns, i.e., if
.C;tC1 / >  .E;tC1/, and if the correlation between those shocks, .C;tC1 ; E;tC1 /, is positive
and sufficiently high. While the latter correlation could well be positive‚ÄîMenzly, Santos, and
Veronesi (2004), Lettau and Ludvigson (2005), and Kothari, Lewellen, and Warner (2005) find a
positive correlation between shocks to expected return and dividend growth‚Äîwe suggest that a
violation of (5) seems less likely, in that it would only deepen the excess volatility puzzle.

     This study develops and applies an approach to estimating expected returns that generalizes the
standard predictive regression approach. The framework we propose, which we term a predictive
system, allows the predictors in x t to be imperfect, in that  t ¬§ a C b 0 x t . The predictive system
also allows us to explore roles for a variety of prior beliefs about the behavior of expected returns.
Chief among these is the belief that unexpected returns are likely to be negatively correlated with
expected returns (uw < 0), but we also include beliefs that the degree of true predictability in
equation (1) is relatively modest and that the expected return  t is fairly persistent. We find that,
compared to predictive regressions, predictive systems deliver different and substantially more
precise estimates of expected return. When predictors are imperfect, their predictive ability can
generally be supplemented by information in lagged returns as well as lags of the predictors, and
the predictive system delivers that information via a parsimonious model. The correlation uw
plays a key role in determining how that additional sample information is used as well as the
relative importance of that information in explaining variation in expected return.

    The additional information in lagged returns is used in an interesting way. Suppose one be-
lieves that the conditional expected return is fairly persistent and then observes that recent returns

                                                  2
have been unusually low. On one hand, since a low mean is more likely to generate low realized
returns, one might think that the expected return has declined. On the other hand, since increases
in expected future returns tend to accompany price drops and thus low returns, one might think that
the expected return has increased. When the correlation between expected and unexpected returns
is sufficiently negative, the latter effect outweighs the former and recent returns enter negatively
when estimating the current expected return. At the same time, more distant past returns enter
positively because they are more informative about the level of the unconditional expected return
than about recent changes in the conditional expected return.

   We illustrate the role of lagged returns in a simplified setting where historical returns are the
only available sample information. Suppose, for example, that an investor in January 2000 is
forming an expectation of the stock market return over the following quarter based on the post-war
history of realized market returns. Does the dramatic rise in stock prices in the 1990s increase or
decrease the investor‚Äôs expectation of future return? The answer depends on the extent to which
the 1990s‚Äô bull market was caused by unexpected declines in expected returns. The conditional
expected stock return in this simplified setting is just a weighted average of all past realized returns,
and the weights depend on the fraction of the variance in unexpected returns that is explained by
changes in expected returns. For example, if this investor believes that fraction is about 72% (the
           2
values of uw implied by the estimates of Campbell (1991) are in that neighborhood), then returns
realized during the most recent decade receive negative weights in the current expected return,
while the returns from the previous four decades receive positive weights. In other words, the
investor in this example would view the 1990s‚Äô bull market as a bearish indicator.

   Imperfection in predictors complicates inference about their relations to expected return. We
show that if predictors are imperfect, the residuals in the predictive regression of r tC1 on x t are
serially correlated. This correlation is often ignored when computing standard errors in predictive
regressions. The serial correlation in residuals joins other features of predictive regressions that are
already well known to complicate inferences, especially in finite samples, such as persistence in the
predictors and correlation between the residuals and innovations in the predictors (e.g., Stambaugh,
1999). Using our alternative framework‚Äîthe predictive system‚Äîwe develop a Bayesian approach
that allows us to conduct clean finite-sample inference about various properties of the expected
return. This approach also allows us to incorporate reasonable prior beliefs, especially beliefs
about uw , the correlation between expected and unexpected returns.

   A striking example of the importance of prior beliefs about uw is provided by regressing post-
war U.S. stock market returns on what we call the ‚Äúbond yield,‚Äù defined as minus the yield on
the 30-year Treasury bond in excess of its most recent 12-month moving average. That variable


                                                   3
receives a highly significant positive slope (with a p-value of 0.001) in the predictive regression,
but its innovations are positively correlated with the residuals in that regression. The latter cor-
relation, opposite in sign to what one would anticipate for the correlation between expected and
unexpected returns, suggests that the bond yield is a rather imperfect predictor of stock returns.
When judged in a predictive system, the bond yield‚Äôs importance as a predictor depends heavily
on prior beliefs about uw . With noninformative beliefs about that correlation, the bond yield ap-
pears to be a very useful predictor; for example, the posterior mode of its conditional correlation
with  t is 0.9. However, with a more informative belief that innovations in expected returns are
negatively correlated with unexpected returns and explain at least half of their variance, the bond
yield‚Äôs conditional correlation with  t drops to 0.2. Prior beliefs also affect the predictive system‚Äôs
advantage in explanatory power over the predictive regression. With noninformative prior beliefs,
the predictive system produces an estimate of  t that is 1.4 times more precise than the estimate
from the predictive regression in terms of its posterior variance, but with the more informative
beliefs, the system‚Äôs estimate is 12.5 times more precise. Moreover, under the more informative
beliefs, the current value of the bond yield explains only 3% of the variance of  t . Adding lagged
unexpected returns allows the system to explain 86% of this variance, and further adding lagged
predictor innovations increases the fraction of explained variance of  t to 95%.

   We also include as predictors two more familiar choices, the market‚Äôs dividend yield and the
consumption-wealth variable ‚ÄúCAY‚Äù proposed by Lettau and Ludvigson (2001). Prior beliefs
about the correlation between expected and unexpected returns play a less dramatic role with these
predictors than with the bond yield, but different prior beliefs can nevertheless produce substantial
differences in estimated expected returns. We assess the economic significance of these expected
return differences by comparing average certainty equivalents for mean-variance investors whose
risk aversion would dictate an all-equity portfolio (i.e., no cash or borrowing) when expected re-
turn and volatility equal their long-run sample values. When all three predictors are included,
an investor with the more informative belief mentioned above would suffer an average quarterly
loss of 1.5% if forced to hold the portfolio selected each quarter by an investor who estimates
expected return by the maximum likelihood procedure (which reflects noninformative views about
all parameters, including the correlation between expected and unexpected returns).

   Ferson, Sarkissian, and Simin (2003) show that persistent predictors may exhibit spurious pre-
dictive power in finite samples even if they have no such power in population (e.g., if they have
been data-mined). Our paper provides tools that can be helpful in avoiding the spurious regression
problem. A spurious predictor is unlikely to produce a substantially negative correlation between
expected and unexpected returns. Therefore, under our informative prior about this correlation, a
predictive system would likely find the spurious predictor to be almost uncorrelated with  t . The

                                                   4
basic intuition holds also outside the predictive system framework: if a predictor does not gen-
erate a negative correlation between expected and unexpected returns, it is unlikely to be highly
correlated with the true conditional expected return.

     This study is clearly related to an extensive literature on return predictability, but it also con-
tributes to a broader agenda of incorporating economically motivated informative prior beliefs in
inference and decision making in finance. Studies in the latter vein include PaÃÅstor and Stambaugh
(1999, 2000, 2001, 2002ab), PaÃÅstor (2000), Baks, Metrick, and Wachter (2001), and Jones and
Shanken (2005). Studies that employ informative priors in the context of return predictability
include Kandel and Stambaugh (1996), Avramov (2002, 2004), Cremers (2002), Avramov and
Wermers (2006), and Wachter and Warusawitharana (2006).

     The remainder of the paper proceeds as follows. In section 2, we first implement the traditional
predictive regression approach to modeling expected stock returns and examine the estimated cor-
relations between expected and unexpected returns. We then present our alternative approach, the
predictive system, and discuss its implications for expected stock returns. Section 3 presents our
empirical results. We first outline our Bayesian approach to estimating the predictive system and
discuss the specifications of prior beliefs. We then compare the explanatory powers of the predic-
tive system and predictive regression, assess the degree to which various predictors are correlated
with expected return, and analyze the behavior of estimated expected returns. Finally, we decom-
pose the variance of expected return into components due to the current predictor values, lagged
unexpected returns, and lagged predictor innovations. Section 4 reviews the paper‚Äôs conclusions.
Many technical aspects of our analysis are presented in the Appendix.



2.     Modeling Expected Returns

2.1. Traditional Approach: Predictive Regression

We begin by estimating predictive regressions on quarterly data for three predictors. The first
predictor is the market-wide dividend yield, which is equal to total dividends paid over the previous
12 months divided by the current total market capitalization. We compute the dividend yield from
the with-dividend and without-dividend monthly returns on the value-weighted portfolio of all
NYSE, Amex, and Nasdaq stocks, which we obtain from the Center for Research in Security Prices
(CRSP) at the University of Chicago. The second predictor is CAY from Lettau and Ludvigson
(2001), whose updated quarterly data we obtain from Martin Lettau‚Äôs website. The third predictor
is the ‚Äúbond yield,‚Äù which we define as minus the yield on the 30-year Treasury bond in excess of

                                                   5
its most recent 12-month moving average. The bond yield data are from the Fixed Term Indices in
the CRSP Monthly Treasury file. The three predictors are used to predict quarterly returns on the
value-weighted portfolio of all NYSE, Amex, and Nasdaq stocks in excess of the quarterly return
on a one-month T-bill, which is also obtained from CRSP.

         Whereas the first two predictors have been used extensively, the third predictor appears to
be new. It seems plausible for the long-term T-bond yield to be related to future stock returns
since expected returns on stocks and T-bonds may comove due to discount-rate-related factors.
Subtracting the 12-month average yield is an adjustment that is commonly applied to the short-
term risk-free rate (e.g., Campbell and Ammer, 1993).

   Table I reports the estimated slope coefficients bO and the R2 ‚Äôs from the predictive regressions,
as well as the estimated correlations between unexpected returns and the innovations in expected
returns. To obtain the innovations in expected return, we make the common assumption that the
vector of predictors follows a first-order autoregressive process,

                                             x t D  C Ax t     1   C vt ;                                      (6)

where v t is distributed independently through time. The correlation between expected and unex-
pected returns is then simply Corr.b 0v t ; e t /, where the predictive regression disturbance is e t D
rt        a   b0xt   1.   Table I also reports the OLS t -statistics and the bootstrapped p-values associated
with these t -statistics as well as with the R2 s. Panel A reports the full-sample results covering
1952 Q1 ‚Äì 2003 Q4. Panels B and C report sub-sample results.2

         The results suggest that all three predictors have some forecasting ability. The dividend yield
produces the weakest evidence (highest p-values, lowest R2 s) in all three sample periods. When
included as the single predictor, the dividend yield is marginally significant in the full sample (p-
value of 5:7%). It is significant in the first subperiod (p D 1:4%) but not in the second subperiod
(p D 40:9%). The significance of the dividend yield weakens further when the other two predictors
are included in the predictive regression.

   In contrast, both the bond yield and CAY are highly significant predictors. When used alone,
both predictors exhibit p-values of 0.1% or less in the full sample, and they are also significant in
both subperiods. If judged by the p-values, CAY is the stronger predictor in the first subperiod but
the bond yield is stronger in the second subperiod. When all three predictors are used together,
both CAY and the bond yield are highly and about equally significant in the full sample.
     2
    Since we use the T-bond and T-bill yields in our analysis, we begin our sample in 1952, after the 1951 Treasury-
Fed accord that made possible the independent conduct of monetary policy. Campbell and Ammer (1993), Campbell
and Yogo (2006), and others also begin their samples in 1952 for this reason.


                                                          6
       In addition to the p-values and R2 s, it is also informative to examine the correlations between
expected and unexpected returns, shown in the fourth column of Table I. When the single predictor
is either the dividend yield or CAY, these correlations are negative and highly significant: -91.9%
for the dividend yield and -53.6% for CAY in the full sample. These negative correlations are
not surprising since both predictors are negatively related to stock prices, by construction. For the
bond yield, however, this correlation is positive and highly significant in all three sample periods,
ranging from 21.7% to 25.1%. This positive correlation makes it unlikely that the bond yield is
perfectly correlated with the true conditional expected return.

       The correlation between expected and unexpected returns is a useful diagnostic that should
be considered when examining the output of a predictive regression. Basic economic principles
suggest that this correlation is likely to be negative, so predictive models in which this correlation
is positive seem less plausible.3 The model in which the bond yield is the single predictor is a
good example. Based on the predictive-regression p-value, the bond yield would appear to be a
highly successful predictor whose forecasting ability is better than that of the dividend yield and
comparable to that of CAY. However, the bond yield produces expected return estimates whose
innovations are positively correlated with unexpected returns, suggesting that this predictor is im-
perfect. We suspect that the same statement can be made about many macroeconomic variables
that the literature has related to expected returns. In the rest of the paper, we develop a predictive
framework that allows us to incorporate the prior belief that the correlation between expected and
unexpected returns is negative.


2.2. Predictive System

In the predictive regression approach, the expected return is modeled as a linear combination of the
predictors in x t . We generalize this approach to recognize that no combination of those predictors
need capture perfectly the true unobserved expected return,  t . Our alternative framework, which
we call a predictive system, combines the three equations in (1), (2), and (6):

                                           r tC1 D              t C u tC1                                           (7)
                                          x tC1 D  C Ax t C v tC1                                                   (8)
                                           tC1 D Àõ C Àá t C w tC1 ;                                                 (9)
   3
     Strictly speaking, the arguments based on equations (3) and (5) apply when r t C1 denotes the total (real) stock
return, but they should hold to a close approximation also when r t C1 denotes the excess stock return, as used here. For
excess returns, Campbell (1991) shows that equation (3) has an additional term representing news about future interest
rates, and he estimates the variance of that term to be typically an order of magnitude smaller than the variances of
C;t C1 and E;t C1.


                                                           7
The residuals in the system are assumed to be distributed identically and independently across t as
                         2     3      02 3 2 2                         31
                           ut              0         u uv uw
                         4 v t 5  N @4 0 5 ; 4 vu Àôvv vw 5A :                              (10)
                                                                   2
                           wt              0         wu wv w
We assume throughout that 0 < Àá < 1 and that the eigenvalues of A lie inside the unit circle.
The predictive system is a version of a state-space model in which there is non-zero correlation
among all of the model‚Äôs disturbances.4 The predictive system nests the predictive regression
model discussed earlier when  t is perfectly correlated with b 0 x t , requiring w t D b 0 v t and A0b D
Àáb.5 In general, though, the predictors in x t are correlated with  t but do not capture it perfectly.
An extreme version of imperfect predictors occurs when there are no predictors, so that equation
(8) is absent from the system and the data include only returns. In fact, we will use that simplified
setting later in this section to illustrate some properties of the predictive system before moving on
to our principal setting in which the predictors are present.

       The value of  t is unobservable, but the predictive system also implies a value for E. t jD t / D
E.r tC1 jD t /, where D t denotes the history of returns and predictors observed through time t . Using
the Kalman filter, we find that this conditional expected return can be written as the unconditional
expected return plus linear combinations of past return forecast errors and innovations in the pre-
dictors. Specifically, if we define the forecast error for the return in each period t as

                                              t D rt      E.r t jD t   1 /;                                       (11)

then the expected return conditional on the history of returns and predictors is given by
                                                            1
                                                            X                                  
                               E.r tC1 jD t / D E.r / C           s  t   s   C s0 v t   s       ;               (12)
                                                            sD0

where, in steady state,
                                                     s D mÀá s ;                                                   (13)
                                                      s D nÀá s ;                                                  (14)
and m and n are functions of the parameters in equations (7) through (10).6 When the predictors
   4
      Harvey (1989) provides a textbook treatment of state-space models, including a brief discussion of the case with
non-zero correlations among all the disturbances. In the Appendix, we provide an independent treatment specific to
the system in (7) through (10). Studies that analyze the predictability of stock and bond returns using state space
models include Conrad and Kaul (1988), Johannes, Polson, and Stroud (2002), Ang and Piazzesi (2003), Brandt and
Kang (2004), Dangl and Halling (2006), Duffee (2006), and Rytchkov (2006).
    5 0
      A b D Àáb means that Àá is an eigenvalue of A0 corresponding to the eigenvector b; one example is A D ÀáI .
    6
      In general, m and n are also functions of time, but as the length of the history in D t grows long, they converge
to steady-state values that do not depend on t. That convergence is reached fairly quickly in the settings we consider.
We first present the steady-state expressions, for simplicity, but later employ the finite-sample Kalman filter as well.
The Appendix derives the functions m and n in finite samples as well as in steady state. The Appendix also shows (in
equation A54) that the finite-sample versions of m and n can be interpreted as the slope coefficients from the regression
of  t on r t and x t , respectively, conditional on the sample information at time t 1.


                                                           8
approach perfection, where w t D b 0 v t and A0b D Àáb, then m approaches zero and n approaches
b. At those limiting values, equation (12) becomes
                                                                     X
                                                                     1
                                                                 0
                              E.r tC1 jD t / D E.r / C b                   As v t   s
                                                                     sD0
                                                D E.r / C b 0 ≈íx t          E.x/¬ç;                   (15)

which is identical to a C b 0 x t , the conditional mean given by the predictive regression. When the
predictors are imperfect, however, their entire history enters the conditional expected return, since
the weighted sum of their past innovations in equation (12) does not then reduce to a function of
just x t . Moreover, when the predictors are imperfect, the expected return depends also on the full
history of returns in addition to the history of the predictors.

   It is also easy to establish that equation (12) implies a recursive representation for returns,

                      r tC1 D .1    Àá/E.r / C Àár t C n0 v t          .Àá       m/ t C  tC1 :        (16)

This representation shows that in the absence of predictors (i.e., without the n0 v t term), stock
returns follow an ARMA(1,1) process. The autocovariance of returns is then equal to
                                                                          
                                Cov.r t ; r t   k/   D Àák   1
                                                                Àá2 C uw ;                         (17)

where 2 D w2 =.1    Àá 2 / is the unconditional variance of  t . As a result, the serial correlation in
stock returns can be positive or negative, depending on the parameter values. The knife-edge case
of zero autocorrelation obtains for uw D Àáw =.u.1 Àá 2 //.


2.3. Expected Return: The Role of uw

    A key feature of the predictive system, in addition to accomodating imperfect predictors, is
the ability to incorporate economically motivated prior beliefs about uw , the correlation between
the unexpected return, u t , and the innovation in the expected return, w t . As discussed earlier, it
seems likely that uw < 0. We find that incorporating such beliefs about uw plays a key role in
computing expected returns and assessing the usefulness of various predictors.

   As mentioned earlier, an extreme version of imperfect predictors occurs when there are no
predictors, so that D t includes only the return history. This special case provides a simplified
setting in which to illustrate the critical role uw can play in the relation between D t and the
conditional expected return. With no predictors, the summation on the right-hand side of equation
(12) includes only the first term, so the conditional expected return is simply a weighted sum of past

                                                       9
forecast errors in returns (thereby giving the Wold representation). We consider here an example
with the predictive R-squared‚Äîthe fraction of the variance in r tC1 explained by  t ‚Äîequal to
0.05, Àá equal to 0.9, and four different values of uw ranging from -0.99 to 0. Panel A of Figure
1 plots the values of s .D mÀá s /, the coefficient in equation (12) that multiplies the forecast error
 t s . Not surprisingly, with Àá D 0:9, the geometric rate of decay in the coefficients makes them
relatively small by lags of around 40 periods. More interesting is the role of uw in determining
m. Differences in m across the values for uw produce strikingly different behaviors for the s ‚Äôs.

   The results in Figure 1 can be understood by noting that there are essentially two effects of
the return history on the current expected return. The first might be termed the ‚Äúlevel‚Äù effect.
Observing recent realized returns that were higher than expected suggests that they were generated
from a distribution with a higher mean. If the expected return is persistent, as it is in this example
with Àá D 0:9, then that recent history suggests that the current mean is higher as well. So the level
effect positively associates past forecast errors in returns with expected future returns. The second
effect, which might be termed the ‚Äúchange‚Äù effect, operates via the correlation between expected
and unexpected returns. In particular, suppose uw is negative, as we suggest is reasonable. Then
observing recent realized returns that were higher than expected suggests that expected returns fell
in those periods. That is, part of the reason that realized returns were higher than expected is that
there were price increases associated with negative shocks to expected future returns and thus to
discount rates applied to expected future cash flows. So the change effect negatively associates
past forecast errors in returns with expected future returns. Overall, the net impact of the return
history on the current return depends on the relative strengths of the level and change effects.

   The level and change effects can be mapped into the return autocovariance in (17). When uw
is sufficiently negative, then Àá2 <    uw , returns are negatively autocorrelated, and the change
effect prevails. Also, m < 0 in that case, so the s ‚Äôs in (13) are negative. When Àá2 >          uw ,
returns are positively autocorrelated, the s ‚Äôs are positive, and the level effect prevails.

   When uw D 0, there is no change effect and only the level effect is present. For that case, the
s ‚Äôs in Figure 1 start at a positive value for the first lag, about 0.04, and then decay toward zero.
The level and change effects offset each other when uw D 0:47 (this is the knife-edge case of
zero autocorrelation in equation (17)), or when the fraction of the variance in unexpected returns
                                      2
explained by expected-return shocks, uw , is about 22%. In that case, the s ‚Äôs plot as a flat line at
zero. This result is worth emphasizing: for uw D 0:47, rational investors do not update their
beliefs about expected return at all, regardless of what realized returns they observe. The change
effect dominates when  D      0:85, where the s ‚Äôs start around -0.04 at the first lag, and it is even
stronger when  D       0:99, where the s ‚Äôs start around -0.08. Clearly, the correlation between


                                                  10
expected and unexpected returns is a critical determinant of the relation between the return history
and the current expected return.

      Since the forecast errors ( t ‚Äôs) in the above analysis are defined relative to conditional expec-
tations that are updated through time based on the available return histories, part of the effects of
past return realizations are impounded in those earlier conditional expectations. To isolate the full
effect of each past period‚Äôs total return, we can subtract the unconditional mean from each return,
defining  U
           t D rt    E.r /, and then rewrite the conditional expected return in equation (12) as
                                                         1
                                                         X                              
                              E.r tC1 jD t / D E.r / C          !s  U      0
                                                                     t s C ƒ±s v t   s       ;                   (18)
                                                         sD0

where, again in steady state,

                                           !s D m.Àá              m/s                                            (19)
                                            ƒ±s D n.Àá             m/s :                                          (20)

It can be verified that Àá       m  0. Panel B of Figure 1 plots the values of !s in the same no-
predictor example discussed above. The patterns are qualitatively similar to those in Panel A, in
that the !s ‚Äôs are again positive and declining for uw D 0, flat at zero for uw D 0:47, and
negative and increasing for  D 0:85 and  D 0:99. In this representation, though, the rates of
geometric decay differ, since they depend on m, and returns at longer lags exert a greater relative
impact as uw takes larger negative values.

   In practice, the true unconditional mean E.r / must be estimated. Consider again the no-
predictor case where, in equation (18), the summation on the right-hand side is truncated at
                                                        Pt
s D t 1 and E.r / is replaced by the sample mean, .1=t / lD1 rl . Then, given Àá and m (essen-
tially second-moment quantities), the estimated conditional expected return becomes a weighted
average of past returns,
                                                           t 1
                                                           X
                                        O tC1 jD t / D
                                        E.r                      s r t s ;                                     (21)
                                                           sD0

where                                                            !
                                                         t
                                                         X
                                            1
                                       s D   1                !l C !s ;                                        (22)
                                            t
                                                         lD1
      Pt   1
and      sD0   s D 1. The weights (s ‚Äôs) are plotted in Panel C of Figure 1 for t D 208, corresponding
to the number of quarters used in our empirical analysis. When uw D 0, all past returns enter
positively but recent returns are weighted more heavily. In the uw D                           0:47 case, where the
level and change effects exactly offset each other, all of the weights equal 1=t , so the conditional

                                                     11
expected return is then just the historical sample average. For the larger negative uw values, where
the change effect is stronger, the weights switch from negative at more recent lags to positive at
more distant lags (as the weights must sum to one). For example, when changes in expected returns
explain about 72% of the variance in unexpected returns (uw D 0:85), the returns from the most
recent 10 years (40 quarters) contribute negatively to the estimated current expected return, while
the returns from the earlier 42 years contribute positively.

       An additional perspective on the role of uw is provided by the time series of conditional
expected returns plotted in Figure 2. In constructing these series, we maintain the no-predictor
setting presented above, with the same parameter values as in Figure 1. The unconditional mean
return E.r / is set equal to the sample average for our 208-quarter sample period, and then, starting
from the first quarter in the sample, the conditional mean is updated through time using the finite-
sample Kalman filter applied to the realized returns data. As before, the level and change effects
exactly offset each other when uw D 0:47, so the conditional expected return in that case is
simply the flat (dashed) line at the sample average for the period. The most striking feature of
the plot is that the expected return series for uw D 0 (solid line) is virtually the mirror image of
the series for uw D      0:85 (dash-dot line). For example, when uw D              0:85, the conditional
expected return plots above the unconditional mean during much of the 1970‚Äôs and early 1980‚Äôs
by amounts that, quarter by quarter, correspond closely to the amounts by which the conditional
expected return plots below the unconditional mean when uw D 0. Moreover, the differences
among the various series of conditional expected returns are large in economic terms, often several
percent per quarter. As before, we see that uw plays a key role in estimating expected returns.

2.4. Predictive System vs. Predictive Regression

Predictive systems have interesting implications for the predictive regression,

                                       r tC1 D a C b 0 x t C e tC1 :                                 (23)

This regressions‚Äôs coefficients and residual variance can be computed from the parameters of the
predictive system in equations (7) through (10), and we exploit that ability in comparing predictive
regressions and predictive systems.7 The predictive-system parameters can also be used to compute
the residual autocovariance,

                   Cov.e t ; e tC1 / D Àá.2      0
                                                 Vx Vxx1 Vx/ C uw       0
                                                                          Vx Vxx1 vu
                                   D ÀáVar. t jx t / C Cov.u t ; w t    b 0 v t /:                   (24)
   For example, b can be computed from the system‚Äôs parameters as b D Vxx1 Vx , where Vxx is given in the
   7

Appendix in equation (A16), Vx D .IK ÀáA/ 1 vw , and IK is a K  K identity matrix.


                                                    12
With imperfect predictors, Var. t jx t / > 0, w t ¬§ b 0 v t , and Cov.e t ; e tC1 / is generally non-zero.8
This serial correlation in the residuals complicates the calculation of standard errors in the predic-
tive regression approach.

       Ferson, Sarkissian, and Simin (2003) make a similar point when the predictor is ‚Äúspurious,‚Äù
or uncorrelated with expected return. Their setting is a special case of (7)‚Äì(9) with one predictor
and a diagonal covariance matrix in (10).9 In specifying a diagonal covariance matrix for the
disturbances, they assume not only that the predictor is spurious but also that the innovations in
expected return are uncorrelated with unexpected returns (i.e., uw D 0). In this special case, we
see from (24) that Cov.e t ; e tC1 / D Àá2 . Ferson et al. do not report this expression but do find,
using simulations, that the positive residual serial correlation can substantially affect inference in
predictive regressions. Duffee (2006) also uses simulations to make a related point in the context
of bond predictability.

    In contrast to a predictive regression, the predictive system allows us to conduct finite-sample
inferences that explicitly incorporate predictor imperfection. The predictive system also produces
more precise inferences about expected returns. To demonstrate this, we compare the explanatory
powers of the system and the regression for a broad range of parameter values. Specifically, we
compare the R2 in the regression of r tC1 on x t for the predictive regression with the R2 in the
regression of r tC1 on E.r tC1 jD t /  E. t jD t / for the predictive system. The ratio of these R2
values when r tC1 is the dependent variable is the same as when  t is the dependent variable,
                                 R2 .r tC1 on x t /           R2 . t on x t /
                                                        D                          ;                           (25)
                             R2 .r tC1 on E. t jD t //   R2 . t on E. t jD t //
since each of the R2 values in the latter ratio is equal to its corresponding value in the first ratio
multiplied by Var.r tC1 /=Var. t /. The parameters in equations (7) through (10) can be used to
obtain the covariance matrix of  t and x t and thereby the R2 in the regression of  t on x t ,
                                                     Var≈íE. t jx t /¬ç
                                             R21 D                     :                                       (26)
                                                       Var. t /
As shown in the Appendix, we can solve analytically for the steady-state value of Var. t jD t /,
which allows us to compute the R2 in the regression of  t on E. t jD t / as
                                         Var≈íE. t jD t /¬ç          Var. t jD t /
                                 R22 D                     D1                      :                           (27)
                                            Var. t /                Var. t /
The ratio on the right-hand side of equation (25) is then computed as R21 =R22 . Note that this R2
ratio cannot exceed one because x t 2 D t . In other words, the predictive system always produces
   8
     If the predictors are perfect, Var. t jx t / D 0 and w t D b 0v t , so Cov.e t ; e t C1/ is then zero.
   9
     The objectives of Ferson et al. differ from ours. For example, they do not use this multiple-equation setting to
estimate expected return or to examine its dependence on lagged returns and predictors.


                                                        13
a more precise estimate of  t than the predictive regression, simply because it uses more informa-
tion. The smaller the R2 ratio, the larger the advantage of using the predictive system.

    Table II reports R21 =R22 under various possible combinations of parameters in the single-predictor
case. We use the same values for the true predictive R2 and uw as before but we now let Àá take
not only the value of 0:9 but also 0:97, and we do the same for A. Note that 0.97 is closer to the
quarterly sample autocorrelations of predictors such as the dividend yield. Finally, we let vw ,
the correlation between v t and w t , range from 0.1 to 0.9. The parameter combinations given do
not uniquely determine R21 =R22 , so for each combination Table II reports that ratio‚Äôs minimum and
maximum values as well as its mean, computed as the equally weighted average across values from
-1 to 1 for the partial correlation of u t and v t given w t .

    Table II shows that the R2 ratio can take essentially any value in its admissible range of .0; 1/,
but some interesting patterns emerge. The degree of imperfection in the predictor is low when vw
is high and when  t and x t have similar autocorrelations (Àá  A). The relative explanatory power
of the predictive regression should be the highest in those cases and, indeed, when vw D 0:9
and Àá D A .D 0:9/, R21 =R22 ranges from 0.81 to 1.0 and is relatively insensitive to uw . With
more imperfection in the predictor, the relative performance of the predictive regression can fall
substantially. Even maintaining vw D 0:9 but letting Àá and A assume different values, 0.9 versus
0.97, results in R21 =R22 dropping below 0.70, sometimes considerably so. In other words, simply
having a predictor whose persistence departs from that of the true expected return by what might
seem a rather modest degree is sufficient to place the predictive regression at a distinctly greater
disadvantage in terms of explanatory power.

    Figure 3 compares the R2 ‚Äôs from predicting r tC1 using the predictive system, the predictive
regression, and the ARMA(1,1) model in equation (16). The four panels correspond to the values
f0; 0:3; 0:6; 0:9g for vw , the conditional correlation between  t and the single predictor x t . In
all four panels, Àá D A D 0:9, and the true predictive R2 (from the regression of r tC1 on  t ) is
0.05. We consider two values of uv , ‚Äúhigh‚Äù and ‚Äúlow‚Äù, which correspond to partial correlations
between u t and v t given w t of uvjw D 0:9 and uvjw D 0:9, respectively.

    Since all three approaches compared in Figure 3 use only information observable at time t ,
they all produce R2 ‚Äôs smaller than 0.05. The R2 from the predictive regression rises from 0 to 0.04
as vw rises from 0 to 0.9 across the four panels. This increase is intuitive: as  t and x t become
more highly correlated, the predictive regression becomes more useful in predicting returns. The
predictive regression R2 is invariant to uw . In contrast, the R2 from the ARMA(1,1) model,
which summarizes the usefulness of past returns in predicting future returns, is heavily influenced
by uw . When uw D          0:47, this R2 is zero: past returns contain no information about future

                                                     14
returns because the level and change effects cancel out, as explained earlier. For uw ¬§         0:47,
stock returns are serially correlated and the ARMA(1,1) R2 is positive; in fact, it can be higher
than the predictive regression R2 . For example, when vw D 0:3 and uw ‚Ä¶ . 0:74; 0:13/, past
returns are more useful than x t in predicting r tC1 . The highest R2 ‚Äôs are invariably achieved by
the predictive system. In all four panels of Figure 3, there exist unique values of uw at which the
system is no more useful than the regression or ARMA(1,1), but for all other values of uw , the
system‚Äôs R2 is higher. This is not surprising since the system uses more information than the other
two approaches.

     In short, Table II and Figure 3 show that when the predictors are imperfect, the conditional
expected returns delivered by the predictive system are often considerably more precise than those
from the predictive regression. The comparison of the explanatory powers of the two approaches
has thus far been conducted with parameter values taken as given. In the next section, we com-
pare the predictive system with the predictive regression in settings in which all parameters are
estimated from the data, with or without economically motivated priors.



3.     Empirical Analysis

In this section we use the predictive system to conduct an empirical analysis of return predictability.
We first discuss identification issues and use the system to estimate expected returns via maximum
likelihood. Then we turn to the main analysis, which takes a Bayesian approach.


3.1. Identification and Maximum Likelihood Estimation

We have discussed how an important feature of the predictive system is its ability to incorporate
economically motivated prior beliefs about parameters such as uw . In the absence of any priors or
parameter restrictions, not all of the parameters in equations (7) through (10) are identified. Of the
parameters in the covariance matrix in (10), denoted by Àô , only Àôvv is identified just by the data.
Identifying the remaining elements requires additional information about at least one of them. We
can nevertheless obtain estimates of conditional expected returns using equations (8) and (16). The
parameters in these equations are identified and can be estimated using, for example, maximum
likelihood. Recall that equation (16) follows directly from the steady-state representation of the
predictive system‚Äôs conditional expected return in equation (12). As the length of the sample
grows, estimates of the parameters in (16) and thus (12) will converge to values that do not depend
on prior beliefs about the parameters in the predictive system (as long as those priors do not strictly

                                                  15
preclude such values). Therefore, after observing a sufficiently long sample, prior beliefs about
uw , for example, will not impact forecasts of future returns. (Our actual sample is evidently not
long in that sense, as prior beliefs about uw exert a substantial effect on estimates of expected
returns.) On the other hand, given the lack of full identification of Àô , prior beliefs about uw will
matter even in large samples when making inferences about the correlation between the predictors
and the true unobservable expected return  t .

    Figure 4 plots the time series of expected returns obtained via maximum-likelihood estimation
of the predictive system (equations (8) and (16)) as well as the expected-return estimates obtained
from OLS estimation of the predictive regression. Panels A and B display results with a single
predictor, either the dividend yield or CAY. In Panel C, those variables are combined with the
bond-yield variable in the three-predictor case. First, observe that the fluctuation of the expected
return estimates seems too large to be plausible. In Panel B, for example, expected returns range
from -5% to 8% per quarter, and the range is even wider in Panel C. Later on, we obtain smoother
time series of  t by specifying informative prior beliefs. Second, observe that although the series
of estimated expected returns exhibit marked differences across the three sets of predictors, the
differences between the predictive-regression estimates and the predictive-system estimates for a
given set of predictors are much smaller.10


3.2. Bayesian Approach

We develop a Bayesian approach for estimating the predictive system. This approach has several
advantages over the frequentist alternatives such as the maximum likelihood approach. First, the
Bayesian approach allows us to specify economically motivated prior distributions for the param-
eters of interest. Second, it produces posterior distributions that deliver finite-sample inferences
about relatively complicated functions of the underlying parameters, such as the correlations be-
tween  t and x t and the R2 s from the regression of r tC1 on  t . Finally, it incorporates parameter
uncertainty as well as uncertainty about the path of the unobservable expected return  t .

    We obtain posterior distributions using Gibbs sampling, a Markov Chain Monte Carlo (MCMC)
technique (e.g., Casella and George, 1992). In each step of the MCMC chain, we first draw the pa-
rameters .; A; Àõ; Àá; Àô / conditional on the current draw of f t g, and then we use the forward filter-
ing, backward sampling algorithm developed by Carter and Kohn (1994) and FruÃàhwirth-Schnatter
(1994) to draw the time series of f t g conditional on the current draw of .; A; Àõ; Àá; Àô /. The
  10
     We also estimate expected returns from the predictive system under diffuse priors (the discussion of prior beliefs
follows later in the text). We find that the resulting estimates (not plotted here) behave similarly to both the OLS
estimates from the predictive regression and the maximum likelihood estimates from the predictive system.


                                                         16
details are in the Appendix.

    We impose informative prior distributions on three quantities:

   1. The correlation uw between unexpected returns and innovations in expected returns,
   2. The persistence Àá of the true expected return  t ,
   3. The predictive R2 from the regression of r tC1 on  t .


These prior distributions are plotted in Figure 5.

    The key prior distribution is the one on uw . We consider three priors on uw , all of which are
plotted in Panel A of Figure 5. The ‚Äúnoninformative‚Äù prior is flat on most of the . 1; 1/ range,
with prior mass tailing off near Àô1 to avoid potential singularity problems. The ‚Äúless informative‚Äù
prior imposes uw < 0 in that 99.9% of the prior mass of uw is below zero. As shown in Panel
                                                            2
B, this prior implies a relatively noninformative prior on uw , with most prior mass between 0 and
                                                                                               2
0.8. Finally, the ‚Äúmore informative‚Äù prior on uw is specified such that the implied prior on uw has
                                                                 2
99.9% of its mass above 0.5, with a mean of about 0.77. Since uw    is the R2 from the regression
of unexpected returns on shocks to expected returns, it represents the fraction of market variance
that is due to news about discount rates (see equation (4)). Therefore, the more informative prior
reflects the belief that at least half of the variance of market returns is due to discount rate news.
This belief is motivated by the evidence of Campbell (1991), Campbell and Ammer (1993), and
others who show that aggregate market returns are driven mostly by discount rate news.

    Campbell (1991) uses a vector-autoregressive approach to decompose unexpected stock market
returns into components due to cash flow shocks, C;tC1 , and discount rate shocks, E;tC1 , as in
our equation (3). He considers two subperiods, 1927‚Äì1951 and 1952‚Äì1988. Since our sample
begins in 1952, we can use Campbell‚Äôs results from the 1927‚Äì1951 period as one source of prior
information. Based on quarterly data (which we use in our empirical work), Campbell estimates
in his Table 2 that .E;tC1/ >  .C;tC1 / in 1927‚Äì1951, meaning that discount rate news is more
important than cash flow news in explaining the variance of stock market returns.11 The same table
also reports estimates of the variance of E;tC1 , 2E , and its covariance with C;tC1 , .C ; E /,
both as fractions of u2 , from which the implied estimates of .u tC1 ; E;tC1 / can be computed as
                                    
 .C ; E /=u2 2E =u2 = E =u . Given equation (4), we interpret these values as implied
estimates of uw . For the 1927‚Äì1951 period, Campbell‚Äôs results imply values of uw ranging from
  11
     Note that this result makes the condition in (5) hold trivially since .C;t C1 ; E;t C1/ < 1. Moreover, Campbell
obtains negative estimates of .C;t C1 ; E;t C1/ for the 1927‚Äì1951 period, which again makes the condition in (5)
hold trivially independent of .E;t C1 /=.C;t C1 /. Campbell‚Äôs empirical results from a sample period that predates
our sample therefore provide further support to the prior belief that uw < 0.


                                                         17
-0.67 to -0.87 across three different specifications. In 1952‚Äì1988, the implied estimates of uw
range from -0.92 to -0.94, and in the full sample, 1927‚Äì1988, they range from -0.71 to -0.86. The
                      2
implied estimates of uw range from 0.50 to 0.74 in 1927‚Äì1988, from 0.44 to 0.76 in 1927‚Äì1951,
                                                      2
and from 0.84 to 0.88 in 1952‚Äì1988. The estimates of uw implied by the evidence of Campbell
and Ammer (1993) in their Table III range from 0.86 to 0.91. All of these estimates seem to be in
line with the more informative prior on uw in Figure 5.

   Note that putting a prior on uw presents a technical challenge. We do not impose the standard
inverted Wishart prior on the covariance matrix Àô because such a prior would be informative about
all elements of Àô , not only about uw , and we see no economic reason to be informative about the
variance of v t or about its covariances with the other error terms. Instead, we build on Stambaugh
(1997) and form the prior on Àô as the posterior from a hypothetical sample that contains more
information about the covariance between u t and w t than about the other covariance elements of
Àô . The details are in the Appendix.

   In addition to putting a prior on uw , we also impose a prior belief that the conditional expected
return  t is stable and persistent. To capture the belief that  t is stable, we impose a prior that the
predictive R2 from the regression of r tC1 on  t is not very large, which is equivalent to the belief
that the total variance of  t is not very large. The prior on the R2 , which is plotted in Panel C of
Figure 5, has a mode close to 1%, most of its mass is below 5%, and there is very little prior mass
above 10%. To capture the belief that  t is persistent, we impose a prior that Àá, the slope of the
AR(1) process for  t , is smaller than one but not by much.12 The prior on Àá, which is plotted in
Panel D of Figure 5, has most of its mass above 0.7 and there is virtually no prior mass below 0.5.
We do not impose a prior belief that  t > 0. Although such a belief is reasonable under a fully
rational view, we do not wish to preclude the possibility that some of the variation in  t is driven
by investor sentiment. The prior distributions on all other parameters (; A; Àõ, and most elements
of Àô ) are noninformative.


3.3. Explanatory Advantage of the Predictive System

In Section 2.4., we show in a theoretical setting that a predictive system produces more precise
estimates of expected return than a predictive regression. In this section, we quantify the advantage
of the predictive system empirically. The sample period is 1952 Q1 ‚Äì 2003 Q4, as before.

   Recall that our theoretical comparison of the explanatory powers of the predictive system and
  12
       Ferson, Sarkissian, and Simin (2003, footnote 2) discuss several reasons to believe expected return is persistent.



                                                            18
the predictive regression is based on the ratio of two R2 ‚Äôs in equation (25) and that, the smaller
the R2 ratio, the larger the advantage of using the predictive system. Table III shows the posterior
means and standard deviations of the R2 ratios for four different priors and four different sets of
predictors. First, observe that the posterior means of the R2 ratios are all comfortably lower than
one, ranging from 0.08 to 0.86 across the 16 cases, and from 0.46 to 0.70 when all three predictors
are used jointly. This result shows that the theoretical explanatory advantage of the predictive
system extends to our empirical setting. Second, the R2 ratios are sensitive to the prior on uw .
For example, with the bond yield as the single predictor, the R2 ratio is estimated to be 0.73 under
the diffuse prior. When we impose the prior belief that uw is negative, the R2 ratio declines to
0.34 under the less informative prior and then further to 0.08 under the more informative prior. In
other words, under the prior that more than half of the market variance is due to discount rate news,
the expected return estimates from the predictive system are about 12.5 times more precise than
those from the predictive regression. For the dividend yield, we observe the opposite pattern‚Äîthe
R2 ratio increases from 0.28 to 0.59 to 0.81 for the same priors. The opposite patterns result from
the opposite effects that the prior on uw has on the adequacy of x t as a predictor in the two cases,
as we will see later.

    The predictive system produces more precise expected return estimates because it uses more
information, not only the most recent predictor values but also their lags and the full history of
asset returns. One way to analyze this additional information is to examine the coefficients s
and s from equation (12), which capture the influence of past unexpected returns and predictor
innovations on the estimate of expected return. Figure 6 plots the first 30 lags of s and s . Both
coefficients decay as the number of lags increases, by construction, but they are mostly nontrivially
different from zero at the first 10-20 quarterly lags. Both coefficients also depend on the prior for
uw , as expected.

    Another way of comparing the predictive system with the predictive regression is to compare
their estimates of the slope coefficient b from the predictive regression. Figure 7 plots the pos-
terior distributions of b computed under three scenarios. The dashed line is the posterior of b
computed from the predictive regression under no prior information. This posterior has a Student t
distribution whose mean is equal to the maximum likelihood estimate (MLE) of b (Zellner, 1971,
pp. 65‚Äì67). The dashed line thus represents ‚Äúconventional inference‚Äù on predictability. The other
two lines in Figure 7 plot the implied posteriors of b computed from the predictive system.13 The
dotted line corresponds to the prior that is noninformative about uw but informative about the
  13
    Although b does not play an explicit role in the predictive system, its value can be computed from the system‚Äôs
parameters, as shown in footnote 7. Posterior draws of b can thus be constructed from the posterior draws of the
underlying parameters. We construct the posteriors of several other quantities such as s in the same manner.



                                                        19
process for  t (i.e., Àá and R2 ). In all three panels of Figure 7, the dotted line is substantially
different from the dashed line, which means that imposing the prior that  t is stable and persistent
significantly affects the inference about predictability. Put differently, in the standard predictive
regression approach, which does not impose such a prior, the estimates of  t are more variable or
less persistent or both (recall Figure 4). In addition, the dotted line is shifted toward zero compared
to the dashed line, which means that the prior belief that  t is stable and persistent weakens the
evidence of predictability. Finally, the solid line corresponds to the prior that is informative not
only about the process for  t but also about uw . The prior on uw clearly affects the inference
about predictability. For example, consider Panel A, in which the single predictor is the bond yield.
Whereas the traditional inference (dashed line) would conclude with almost 100% certainty that
the bond yield is a useful predictor (b > 0), the system-based inference with the more informative
prior on uw (solid line) concludes no such thing because almost half of the posterior mass of b
is below zero. This prior also slightly weakens the predictive power of CAY but it strengthens the
predictive power of the dividend yield.


3.4. How Imperfect Are Predictors?

The predictive system also allows us to learn about the correlation between the expected return
 t and one or more predictors. Since  t is not observed, the manner in which one learns about
such correlations merits some discussion. Consider, for simplicity, the case of a single predictor x t
whose autocorrelation A is equal to Àá. The unconditional correlation between the expected return
and the predictor, x , is then equal to vw , the conditional correlation.14 By virtue of the fact that
the correlation matrix for .u t v t w t / must be non-negative definite, it is readily verified that

                                                     2                               2         2
                         vw D uv uw C  ; where   .1                         uv /.1   uw /:            (28)

In other words, even though correlations are not transitive (two correlations don‚Äôt imply the third),
they become nearly transitive when at least one of them approaches Àô1. Our priors for vw and uv
are noninformative. The data reveal information about uv in that with only modest predictability
in returns, the value of uv is close to that of r v , which can be estimated from the series of r t
and x t .15 Information about uw enters largely through the prior. When the prior is concentrated
  14
       More generally,
                                                                             12
                                                        .1    Àá 2 /.1 A2 /
                                        x D vw                                   ;
                                                             .1 ÀáA/2
         2      2
so that x  vw .
   15
      When the predictive R2 is low, r2u D .1   R2 / is close to one, and equation (28) then implies that uv is well
approximated by r v .


                                                             20
on large negative values, then the likely values of  in (28) are small, so the prior information
about uw and the sample information about uv get combined to provide information about vw .
Alternatively, if the data indicate that uv is close to Àô1 (e.g., in Table I, uv  0:9 when the
predictor is the dividend yield), then again  is likely to be small, so that uv and uw are again
jointly informative about vw .

   Inferences about the degrees to which the various predictors analyzed here can capture the
unobservable true expected return  t are summarized in Figures 8 through 10. We report results
for three predictive systems, in which the predictors are the dividend yield alone (Figure 8), the
bond yield alone (Figure 9), and the dividend yield, bond yield, and CAY together (Figure 10).

   Panel A of each figure plots the posterior distribution of the R2 from the regression of  t on x t .
This R2 is assumed to be one in a predictive regression, but its posterior in the predictive system
has very little mass at values close to one. In all three figures, the R2 s larger than 0.8 receive very
little posterior probability and the values larger than 0.9 are deemed almost impossible, regardless
of the prior. This evidence suggests that none of the three sets of predictors are likely to be perfectly
correlated with  t .16

    The R2 also depends on the prior for uw in an interesting way. In Panel A of Figure 8, becom-
ing increasingly informative about uw shifts the posterior of the R2 to the right, with the mode
shifting from about 0.3 under the noninformative prior to about 0.6 under the more informative
prior. This makes sense ‚Äì since the dividend yield exhibits a highly negative contemporaneous
correlation with stock returns, imposing a prior that  t also possesses such negative correlation
makes the dividend yield more closely related to  t . Exactly the opposite happens in Panel A of
Figure 9, where becoming increasingly informative about uw shifts the posterior of the R2 to the
left so that its mode is close to zero under the more informative prior. This makes sense as well
because the bond yield is positively correlated with stock returns (Table I). In Panel A of Figure
10, becoming increasingly informative about uw shifts the posterior to the right again. The rea-
son is that the set of predictors includes the dividend yield and CAY, both of which are negatively
correlated with contemporaneous returns.

    Panel B of each figure plots the posterior of the predictive R2 from the regression of r tC1 on
 t . Putting a more informative prior on uw increases the R2 in Figures 8 and 9 and decreases it
in Figure 10, but these effects are relatively small. Since we put a fairly informative prior on the
predictive R2 (see Panel C of Figure 5), the posterior is not dramatically different from the prior
  16
    Note that even if x t were perfectly correlated with  t in population, the posterior of their correlation would have
nontrivial mass below one in any finite sample. Since we always observe finite samples, we always perceive imperfect
correlation between x t and  t .


                                                          21
in any of the three figures.

   The remaining panels of each figure plot the posteriors of the partial correlations between
each predictor and  t , both conditional (vw ) and unconditional (x ). (Partial correlations are
correlations that control for the presence of other predictors.) These correlations are all well below
one and they are quite sensitive to the prior on uw . As we become increasingly informative about
uw , we perceive the dividend yield to be more highly correlated with  t (Figure 8) and the bond
yield to be less highly correlated with  t (Figure 9). For CAY in Figure 10, the prior does not
affect vw much but it increases x. Among the three predictors in Figure 10, the bond yield
exhibits the lowest partial correlations with  t under the more informative prior. For example,
almost a third of the posterior distribution for x is below zero and the posterior mode is only
about 0.2. The dividend yield and CAY exhibit substantially higher x ‚Äôs, with posterior modes of
about 0.7, and their conditional correlations vw are even slightly higher.

    Overall, Figures 8 through 10 show that our predictors are imperfectly correlated with  t and
that the inference about this correlation is substantially affected by the prior beliefs about uw .
Prior beliefs informed by economic principles strengthen the predictive power of the dividend
yield and CAY but they weaken the predictive power of the bond yield.


3.5. Estimates of Expected Return

Figure 11 plots the time series of expected returns estimated by three different approaches. The
dashed line plots the fitted values from the predictive regression. These traditional expected return
estimates seem too volatile to be plausible, as we also observed in Figure 4. For example, in Panel
C, which includes all three predictors, expected returns range from -6% to 9% per quarter. Not
surprisingly, imposing the prior that  t is stable and persistent (dotted line) produces smoother
expected return estimates. Adding the more informative prior on uw (solid line) further smoothes
the expected return estimates: in Panel C, they range from -1.5% to 3.5% per quarter. The infor-
mative priors have substantial effects on expected returns not only in Panel C but also in Panel B in
which CAY is the single predictor: while the regression-fitted values range from -5.5% to 7.5% per
quarter, the solid line ranges from -1.5% to 2.5%. Only in Panel A, in which the dividend yield is
the single predictor, the effect of the prior is relatively mild. The reason is that the regression-fitted
values in Panel A are already fairly smooth and negatively correlated with stock returns.

   While eyeballing the expected return estimates seems informative, we also compute measures
summarizing their differences. Table IV compares five different series of expected return estimates.
The first is the series of fitted values from the predictive regression, and the others are produced

                                                   22
by four different approaches to estimating the predictive system. One of the latter approaches
estimates the predictive system by MLE, while the other three impose the prior that  t is stable and
persistent but differ in their prior on uw (noninformative, less informative, more informative). We
compare the five series of expected return estimates in three different ways: pairwise correlations,
mean absolute differences, and average utility losses. The utility losses are computed for a mean-
variance investor allocating between the market and the T-bill who knows the variance of market
returns but must estimate the market‚Äôs expected return. The investor‚Äôs risk aversion is such that the
optimal portfolio is fully invested in the market, on average. We compute the investor‚Äôs certainty
equivalent loss resulting from holding a portfolio that is optimal under a different approach for
estimating expected returns. For example, the 0.15% per quarter average utility loss in the first row
of Panel A is suffered by an investor who wants to estimate expected return in the predictive system
by MLE but is forced to use the fitted values from the predictive regression. Finally, the three
panels consider three different sets of predictors: the dividend yield, CAY, and the two predictors
combined with the bond yield.

   Panel A of Table IV shows that when the dividend yield is the single predictor, the expected
return estimates are fairly similar across the five estimation approaches, confirming the evidence
from Panel A of Figure 11. No average utility loss exceeds 0.15% per quarter, no mean absolute
difference is larger than 0.55% per quarter, and all correlations exceed 84.5%. We also observe
that imposing informative priors makes the system-based estimates closer to the regression-based
estimates. For example, the utility losses fall monotonically from 0.15% to 0.03% as move from
column two to column five in the first row of Panel A.

   The differences across the five approaches are substantially larger in Panel B where we use
CAY to predict returns. For example, compare the system-based estimates obtained by MLE versus
the more informative prior. The mean absolute difference in expected returns is 1.65% per quarter
and the average certainty equivalent loss from using one estimate in place of the other is 1.40% per
quarter. Both quantities are highly economically significant. In Panel C, where we use all three
predictors, the differences across the five approaches are even larger. For the same comparison as
earlier in this paragraph, the mean absolute difference in expected returns is 1.80% per quarter and
the average certainty equivalent loss is 1.49% per quarter.

   In all three panels, the smallest differences are obtained for the noninformative versus the less
informative prior on uw . No average utility loss exceeds 0.06% per quarter, no mean absolute
difference is larger than 0.37% per quarter, and all correlations exceed 95.4%. However, moving
from the less informative to the more informative prior on uw can produce sizeable differences in
expected returns. For example, the mean absolute difference in Panel C is 1.46% per quarter and


                                                 23
the average utility loss is 0.84% per quarter.

    To sum up, when we use the dividend yield as the single predictor, the system-based expected
return estimates are close to the regression-based estimates. In all other cases, the system and the
regression generate substantially different expected returns, and the system-based estimates are
significantly affected by the prior on uw .


3.6. Variance Decomposition of Expected Return

In the predictive regression approach, expected return  t is modeled as an exact linear function
of the predictors in x t . In a predictive system, however, the data provide additional information
about  t because the lagged values of unexpected returns and predictor innovations also enter the
expected return estimates (see Section 2.2.). In this section, we decompose the variance of  t to
assess the relative importance of the various sources of information in a predictive system.

   First, we rewrite the AR(1) processes for x t and  t in equations (8) and (9), respectively, as
moving average processes with an infinite number of lags:
                                                                         1
                                                                         X
                                           x t D Ex C                          Ai v t   i                                              (29)
                                                                      iD0
                                                                      X1
                                           t D Er C                           Àái wt i ;                                               (30)
                                                                         iD0

where Er  E.r t / and Ex  E.x t /. Then we project w t linearly on u t and v t :
                                                               1            
                                          u2 uv                        ut
              w t D ≈íwu wv ¬ç                                                     C t D   u ut      C       v vt   C t :            (31)
                                          vu Àôvv                        vt

Substituting for w t from equation (31) into equation (30), we obtain

                                              X
                                              1                                X
                                                                               1
                                                                                                                    X
                                                                                                                     1
                                                     i                                  i     i
   t D .Er       v Ex /   C   v xt   C   u         Àá ut     i   C        v          Àá IK   A vt          i   C            Àái t i ;   (32)
                                              iD0                              iD0                                   iD0

where K is the number of predictors and IK is a K  K identity matrix. Equation (32) shows
how the lagged values of unexpected returns u t i and predictor innovations v t i affect  t in the
presence of the current predictor values in x t . Based on this equation, we can decompose the
variance of  t into the components due to x t , fus gst , and fvs gst . See the Appendix for details.

    Table V reports the posterior means and standard deviations of the R2 s from the regressions of
 t on x t (column 1),  t on x t and fus gst (column 2), and  t on x t and fus ; vs gst (column 3).

                                                                 24
We consider four sets of predictors x t : the dividend yield, bond yield, CAY, and the combination
of all three predictors. For each set of predictors, we estimate the predictive system under three
different priors. All three priors assume that  t is stable and persistent but they differ in their
degree of informativeness about uw .

     First, note that x t never accounts for more than 63% of the variance of  t and that it can
account for as little as 3% of this variance. In contrast, x t combined with fus ; vs gst can account
for as much as 95% of the variance of  t , and those components account for more than 80% of the
variance in 10 of the 12 cases in Table V. The most striking effect obtains for the bond yield, for
which adding fus ; vs gst to x t increases the R2 from 0.03 to 0.95. It seems clear that a predictive
regression, which uses only x t to predict returns, does not use the data as effectively as a predictive
system, which also uses fus ; vs gst in addition to x t .

     The R2 ‚Äôs in Table V are substantially affected by the prior on uw . For example, consider the
first columns of Panels A and B. Under the noninformative prior on uw , both the dividend yield
and the bond yield explain about a third of the variance of  t . As we become more informative
about uw , this fraction increases from 0.34 to 0.40 to 0.57 for the dividend yield, but it decreases
from 0.33 to 0.24 to 0.03 for the bond yield. These opposite patterns reflect the opposite signs of
the correlations between stock returns and the two predictors, as explained earlier.

     The lagged unexpected returns fus gst contain a significant amount of information about  t
beyond that included in x t . When fus gst is added to x t in estimating  t , the R2 ‚Äôs increase
by anywhere between 7% and 83%. For example, under the more informative prior on uw , the
R2 increases from 0.03 to 0.86 for the bond yield, from 0.53 to 0.87 for CAY, and from 0.63 to
0.85 when fus gst is added to all three predictors. The lagged predictor innovations fvs gst also
contain useful information about  t . When fvs gst is added to x t and fus gst , the R2 ‚Äôs increase
by between 1% and 41%. The smallest increases, of 1% to 5%, obtain for the dividend yield, while
the largest increases, of 9% to 41%, obtain for all three predictors combined.

     To summarize, the past values of unexpected returns and predictor innovations contain use-
ful incremental information about the current expected return. This information is used by the
predictive system but not by the standard predictive regression.


4.     Conclusions

Unlike a predictive regression, a predictive system accommodates imperfect predictors as well as
the prior belief that expected and unexpected returns are negatively correlated. When predictors


                                                  25
are imperfect, expected returns conditional on available data depend not only on the most recent
values of those predictors but also on lagged returns and lags of the predictors. Recent lagged re-
turns receive a negative weight when prior beliefs attribute a significant portion of the variance in
unexpected returns to changes in expected returns. The prior for the correlation between expected
and unexpected returns can also have a substantial effect on inferences about a predictor‚Äôs corre-
lation with expected return. The lags of returns and predictors often account for a large fraction
of the variation in conditional expected returns when the predictors are imperfect. We observe
economically significant differences across estimates of conditional expected returns, not only for
predictive regressions versus predictive systems but across different specifications of priors within
predictive systems as well.

    Our initial exploration of predictive systems could be extended in many directions. We are
intentionally noninformative about the degree of imperfection in a predictor, but one could instead
incorporate an informative prior belief about a predictor‚Äôs correlation with expected return. The
latter approach is likely to be preferable when inference is less the objective than is producing the
best forecast given one‚Äôs own prior judgment. The predictive system is formulated as a one-period-
ahead model, but it can deliver conditional expected returns for longer horizons as well. It could
be interesting to investigate whether, when predictors are imperfect, observations of long-horizon
returns can provide additional insight into the properties of expected returns, such as their persis-
tence. We assume that the conditional mean return follows an AR(1) process, but it would also
make sense to consider more complicated processes. For example, if the mean were allowed to
have not only a slow-moving persistent component but also a higher-frequency transient compo-
nent, the bond yield, which is not very persistent, might be inferred to be more highly correlated
with the conditional mean. We also assume that the return variance is constant, but one could al-
low it to be time-varying, potentially in a manner correlated with expected return (e.g., Brandt and
Kang, 2004). We consider three predictors but it would also be interesting to examine the degrees
of imperfection in various other predictors that have been proposed in the literature.17 We ana-
lyze predictability in U.S. stock market returns, but it would also be interesting to apply predictive
systems to international markets (e.g., Ferson and Harvey, 1993).

    It could also be useful to expand the predictive system to incorporate cash flow news. We
have argued that the innovation in the expected return should be negatively correlated with the
unexpected return, but if one could account for the portion of the latter that is correlated with cash
flow news, the remaining portion would be driven entirely by news about expected return. These
issues are beyond the scope of this paper but they merit more attention. See Cochrane (2006) and
  17
    See, for example, Ferson and Harvey (1991), Lamont (1998), Lewellen (1999), Ang and Bekaert (2006), Santos
and Veronesi (2006), etc.


                                                     26
Rytchkov (2006) for recent analyses of the interaction between return predictability and cash flow
predictability.

    One might ask whether the predictive system produces out-of-sample forecasts with lower
mean squared error (MSE) than a simpler approach such as a predictive regression or just the sam-
ple average.18 A Bayesian investor with a quadratic (MSE) loss function would prefer a forecast
that combines his priors and the available data to estimate the conditional expected return based
on the correct model. The correct model, when estimated using a finite sample, tends to produce
out-of-sample MSEs higher than those from estimates of simpler models when the true degree of
predictability is not sufficiently high, as discussed by Clark and West (2004, 2005) and Hjalmars-
son (2006). Thus, a simple comparison of out-of-sample MSEs would not speak directly to the
question of whether the predictive system is the right model from the investor‚Äôs perspective. That
question, one of model selection, is beyond the scope of this study but could be an interesting area
for future research. Clark and West (2004, 2005) develop frequentist tests based on out-of-sample
statistics, and it could be interesting to pursue model selection issues for predictive systems.




  18
     Goyal and Welch (2003, 2005) and Campbell and Thompson (2005), among others, investigate the abilities of
predictive regressions and sample averages to forecast stock returns out of sample.


                                                     27
Appendix.
    In parts of the Appendix, we work with a generalized version of the predictive system with
more than one asset, so that r t , x t , and  t are all vectors. In those parts, we maintain the usual
convention that matrices are denoted by uppercase letters, so we replace Àá by B, the  ‚Äôs by the
corresponding Àô ‚Äôs, etc.

   We restate the predictive system from equations (7) through (9) here in the multi-asset case:

                                    r tC1 D      t C u tC1                                      (A1)
                                    x tC1 D  C Ax t C v tC1                                     (A2)
                                     tC1 D Àõ C B t C w tC1 ;                                   (A3)

with the disturbances distributed identically and independently across t as
                       2      3       02 3 2                           31
                           ut               0       Àôuu Àôuv Àôuw
                       4 v t 5  N @4 0 5 ; 4 Àôvu Àôvv Àôvw 5A :                                   (A4)
                          wt                0       Àôwu Àôwv Àôww

Let D0 denote the null information set, so that the unconditional moments are given as
                    2      3            02        3 2                  31
                        rt                   Er         Vr r Vr x Vr
                    4 x t 5 jD0  N @4 Ex 5 ; 4 Vxr Vxx Vx 5A :                                 (A5)
                        t                   Er         Vr Vx V

Let z t denote the vector of the observed data at time t ,
                                                       
                                                     rt
                                           zt D            :                                     (A6)
                                                    xt

Denote the data we observe through time t as D t D .z1 ; : : : ; z t /, and note that our complete data
consist of DT . Also define
                                                                             
                            Er               Vr r Vr x                      Vr
                    Ez D         ; Vzz D                    ; Vz D                 :             (A7)
                            Ex               Vxr Vxx                        Vx

From the above we obtain

                                      Er    D    .I B/ 1 Àõ                                       (A8)
                                      Ex    D    .I A/ 1                                        (A9)
                                     Vr r   D    V C Àôuu                                      (A10)
                                     Vxx    D    AVxx A0 C Àôvv                                  (A11)
                                     V    D    BV B 0 C Àôww                                 (A12)
                                     Vr x   D    Vx A0 C Àôuv                                   (A13)
                                     Vx    D    BVx A0 C Àôwv                                  (A14)
                                     Vr    D    BV C Àôwu                                     (A15)

                                                   28
and equations (A11), (A12), and (A14) can be written in explicit form as19

                                 vec .Vxx / D ≈íI          .A Àù A/¬ç 1 vec .Àôvv /           (A16)
                                 vec .V / D ≈íI          .B Àù B/¬ç 1 vec .Àôww /           (A17)
                                 vec .Vx / D ≈íI          .A Àù B/¬ç 1 vec .Àôwv /:          (A18)


                                        Drawing the time series of  t


   To draw the time series of the unobservable values of  t conditional on the current parameter
draws, we apply the forward filtering, backward sampling (FFBS) approach, originally developed
by Carter and Kohn (1994) and FruÃàhwirth-Schnatter (1994). See also West and Harrison (1997,
chapter 15).

Filtering

   The first stage follows the standard methodology of Kalman filtering. Define

                                           at     D   E. t jD t 1 /                      (A19)
                                           bt     D   E. t jD t /                        (A20)
                                           et     D   E.z t j t ; D t 1 /                (A21)
                                           ft     D   E.z t jD t 1 /                      (A22)
                                           Pt     D   Var. t jD t 1 /                    (A23)
                                           Qt     D   Var. t jD t /                      (A24)
                                           Rt     D   Var.z t j t ; D t 1 /              (A25)
                                           St     D   Var.z t jD t 1 /                    (A26)
                                           Gt     D   Cov.z t ; 0t jD t 1 /              (A27)

Conditioning on the (unknown) parameters of the model is assumed throughout but suppressed in
the notation for convenience. First note that

                                                0 jD0  N .b0 ; Q0 /;                    (A28)

where b0 D Er and Q0 D V ,
                                                1 jD0  N .a1 ; P1 /;                    (A29)
where a1 D Er and P1 D V , and

                                                z1 jD0  N .f1 ; S1 /;                    (A30)

where f1 D Ez and S1 D Vzz . Note that

                                                      G1 D Vz                            (A31)
  19
       The solutions employ the well-known identity vec .DF G/ D .G 0 Àù D/vec .F /.


                                                         29
and that
                                           z1 j1 ; D0  N .e1 ; R1 /;                                       (A32)
where
                                      e1 D f1 C G1 P1 1 .1              a1 /                                (A33)
                                      R1 D S1 G1 P1 1 G10 :                                                  (A34)
Combining this density with equation (A29) using Bayes rule gives
                                             1 jD1  N .b1 ; Q1 /;                                          (A35)
where
                         b1 D a1 C P1 .P1 C G10 R1 1 G1 / 1 G10 R1 1 .z1               f1 /                  (A36)
                         Q1 D P1 .P1 C G10 R1 1 G1 / 1 P1 :                                                  (A37)
Continuing in this fashion, we find that all conditional densities are normally distributed, and we
obtain all the required moments for t D 2; : : : ; T :
                          a t D Àõ C Bb t 1                                                                   (A38)
                          P t D BQ t 1 B 0 C Àôww                                                             (A39)
                                             
                                     bt 1
                          ft D                                                                               (A40)
                                   C Ax t 1
                                                   
                                  Q t 1 C Àôuu Àôuv
                          St D                                                                               (A41)
                                      Àôvu       Àôvv
                                         0
                                                 
                                  Q t 1 B C Àôuw
                          Gt D                                                                               (A42)
                                        Àôvw
                         e t D f t C G t P t 1 . t a t /                                                    (A43)
                         R t D S t G t P t 1 G t0                                                            (A44)
                         b t D a t C P t .P t C G t0 R t 1 G t / 1 G t0 R t 1 .z t     ft /                  (A45)
                             D a t C G t0 S t 1 .z t f t /                                                   (A46)
                         Q t D P t .P t C G t0 R t 1 G t / 1 P t :                                           (A47)
The values of fa t ; b t ; Q t ; P t g for t D 1; : : : ; T are retained for the next stage.

Sampling

    Let                                                2  3
                                                       rt
                                                t D 4 xt 5 :                                                (A48)
                                                       t
We wish to draw .0 ; 1 ; : : : ; T / conditional on DT . The backward-sampling approach relies on
the Markov property of the evolution of  t and the resulting identity,
    p.0 ; 1 ; : : : ; T jDT / D p.T jDT /p.T     1 jT ; DT 1 /    p.1 j2 ; D1 /p.0 j1 ; D0 /:   (A49)

                                                        30
We first sample T from p.T jDT /, the normal density obtained in the last step of the filtering.
Then, for t D T 1; T 2; : : : ; 1; 0, we sample  t from the conditional density p. t j tC1 ; D t /.
(Note that the first two subvectors of  t are already observed and thus need not be sampled.) To
obtain that conditional density, first note that
                       02               3 2                                    31
                               bt              Q t C Àôuu Àôuv Q t B 0 C Àôuw
         tC1 jD t  N @4  C Ax t 5 ; 4          Àôvu       Àôvv       Àôvw      5A ;          (A50)
                              a tC1           BQ t C Àôwu Àôwv          P tC1
                                             02   3 2          31
                                              rt        0 0 0
                               t jD t  N @4 x t 5 ; 4 0 0 0 5A ;                                              (A51)
                                              bt        0 0 Qt
and                                                     2            3
                                                         0 0   0
                                          0
                              Cov. t ;  tC1 jD t / D 4 0 0   0 5:                                             (A52)
                                                         Qt 0 Qt B 0
Therefore,
                                         t j tC1 ; D t  N .h t ; H t /;                                      (A53)
where
     2      3 2             32                              3                                 1   2                   3
        rt      0 0   0         Q t C Àôuu Àôuv Q t B 0 C Àôuw                                            r tC1     bt
h t D 4 x t 5C4 0 0   0 54         Àôvu    Àôvv      Àôvw      5                                     4 x tC1       Ax t 5
        bt      Qt 0 Qt B 0    BQ t C Àôwu Àôwv      P tC1                                               tC1     a tC1

and
      2       3 2             32                              3                                        1   2             30
       0 0 0      0 0   0         Q t C Àôuu Àôuv Q t B 0 C Àôuw                                                0 0   0
Ht D 4 0 0 0 5 4 0 0    0 54         Àôvu    Àôvv      Àôvw      5                                            4 0 0   0 5
       0 0 Qt     Qt 0 Qt B 0    BQ t C Àôwu Àôwv      P tC1                                                   Qt 0 Qt B 0

The mean and covariance matrix of  t are taken as the relevant elements of h t and H t .


                                  Expected returns and past values


    In this section, we derive the equations (12), (18), and (21). We still work in the general case
in which r t is a vector of returns rather than a scalar. Therefore, to continue denoting matrices by
uppercase letters, we replace m by M , n by N ,  by ,  by Àö , ƒ± by , ! by Àù, and  by K.

    Below, we express the vector of conditional expected returns, b t D E.r tC1 jD t /, as a function
of past returns and predictors. Denote

                       ≈íM t N t ¬ç  P t .P t C G t0 R t 1 G t / 1 G t0 R t 1 D G t0 S t 1 ;                     (A54)




                                                       31
so that, from equation (A45), for t > 1,

                       b t D a t C ≈íM t N t ¬ç.z t              ft /
                                                                     
                                                             bt 1             rt
                             D Àõ C Bb t 1 C ≈íM t N t ¬ç
                                                       x t  Ax t 1
                             D .I B/Er C .B M t /b t 1 C M t r t C N t v t ;                                                (A55)

or
                        bt    Er D B.b t          1     Er / C M t .r t            bt    1/   C Nt vt :                     (A56)
For t D 1, we obtain

                                   b1     Er D M1 .r1                     b0 / C N1 v1 ;

where v1 denotes x1      Ex . Repeated substitution for the lagged values of .b t                              Er / gives

                                              X
                                              t                                    X
                                                                                   t
                              b t D Er C              s .rs      bs 1 / C               Àös vs ;                            (A57)
                                              sD1                                  sD1

where

                                                  s D B t s Ms                                                             (A58)
                                                  Àös D B t s Ns :                                                           (A59)

That is, the expected return conditional on data observed through period t can be written as the
unconditional mean Er plus a linear combination of past return forecast errors, s D rs bs 1 ,
plus a linear combination of past innovations in the predictors. This is equation (12) in the text.

    The current conditional expected return b t can be rewritten so that past forecast errors are
replaced by returns in excess of the unconditional mean Er . To do so, modify equation (A55)
slightly as
                   b t Er D .B M t /.b t 1 Er / C M t .r t Er / C N t v t                  (A60)
so that repeated substitution for the lagged values of .b t                   Er / then yields

                                                  X
                                                  t                                X
                                                                                   t
                               b t D Er C               Àùs .rs        Er / C             s vs                              (A61)
                                                  sD1                              sD1

where
                          
                              .B        M t /.B       Mt   1 /    .B       MsC1 /Ms             for s < t
                Àùs D                                                                                                        (A62)
                              Ms                                                                   for s D t
                          
                              .B        M t /.B       Mt   1 /    .B       MsC1 /Ns             for s < t
                s D                                                                                                        (A63)
                              Ns                                                                   for s D t

That is, b t is then equal to the unconditional mean return Er plus linear combinations of past
returns in excess of Er and past innovations in the predictors. This is equation (18) in the text.

                                                           32
                                               Pt
   If Er is replaced by the sample mean, .1=t / lD1 rl , in equation (18), then the estimate of b t
becomes
                                       X t         X
                                                   t
                                   O
                                  bt D     Ks rs C     s vs ;                              (A64)
                                             sD1           sD1
where                                                           !
                                                     t
                                                     X
                                           1
                                      Ks D   I             Àùl       C Àùs ;                       (A65)
                                           t
                                                     lD1
      Pt
and     sD1   Ks D I . This is a generalized version of equation (21) in the text.

    In the rest of the Appendix, we discuss the special case (implemented in the paper) in which
we predict the return on a single asset, so that r t is a scalar. This simplification turns  t , Àõ, and
B into scalars as well. Therefore, we now turn back to the notation from the text in which B is
replaced by Àá and the relevant Àô ‚Äôs by  ‚Äôs.

                                        Drawing the parameters

    This section describes how we obtain the posterior draws of all parameters conditional on the
current draw of the time series of  t .

Prior distributions

    First, we discuss the prior on .; A; Àõ; Àá/. We require both x t and  t to be stationary, so that
all eigenvalues of A must lie inside the unit circle and Àá 2 . 1; 1/. Apart from this restriction,
our prior is noninformative about A but informative about Àá, Àá  N .0:99; 0:152 / (see Figure
5). We reparameterize the model to replace the intercepts  and Àõ by the unconditional means
of  t and x t , which we denote by E and Ex , respectively. The equations (8) and (9) then read
x tC1 D Ex C A.x t Ex / C v tC1 and  tC1 D E C Àá. t E / C w tC1 . This reparameterization
allows us to increase the speed of convergence of our MCMC chain by putting a mildly informative
                              2
prior on E , E  N .;  N E  
                                  /, centered at the sample mean return with a large prior standard
                                                                                        2
deviation of 1% per quarter. We use a noninformative prior for Ex , Ex  N .0; E         x
                                                                                            IK / with a
large Ex . All four parameters, A, Àá, E , and Ex , are independent a priori.

     The prior on Àô is informative about the 2  2 submatrix Àô11  ≈íu2 uw I wu w2 ¬ç but noninfor-
mative about the elements of Àô that involve v (i.e., Àôvv ; vu ; vw ). Such a prior on Àô is obtained
as a posterior when a noninformative prior is updated with a hypothetical sample in which there
are T0 observations of .u; w/ but only S0  T0 observations of v. We choose T0 equal to one
fifth of the sample size, which makes the prior on Àô11 quite informative (five times less informa-
tive than the actual sample). We choose S0 D K C 3, where K is the number of predictors used
(K D 1 or 3), which makes the prior on the elements of Àô that involve v virtually noninformative
(as informative as a sample of only K C 3 observations).

    We obtain this latter prior by changing variables from (Àôvv ; vu ; vw ) to the slope C and the
residual covariance matrix Àù from the regression of v t on .u t ; w t /. We put a normal-inverted-
Wishart prior on C and Àù: Àù  I W .S0 ÀùO 0 ; S0 1/ and vec .C /jÀù  N .cO0 ; Àù Àù .X00 X0 / 1 /,

                                                    33
where ÀùO 0 , cO0 , and X00 X0 represent the estimates from a hypothetical sample of S0 observations.
We choose a very small value for S0 , as explained above. In addition, we choose a hypothetical
sample in which the right-hand side variable is much less volatile than the residuals. Both choices
help make the prior on C and Àù noninformative.

    The prior on Àô11 is inverted Wishart, Àô11  IW .T0 ÀôO 11;0 ; T0 K 1/. The prior mean
of u2 is set equal to 95% of the sample variance of market returns, and the prior mean of w2 is
obtained from the total variance of  t under the assumption that Àá D 0:97. These assumptions
lead to a prior for the R2 from the regression of r tC1 on  t that we find plausible (see Figure 5).
To be able to put different priors on uw while keeping the same prior on u2 and w2 , we adopt
a hyperparameter approach in which the off-diagonal element of ÀôO 11;0 is unknown. Specifically,
denoting the .i; j / element of ÀôO 11;0 by Mij , for i D 1; 2 and j D 1; 2, we assume that M11
and M22 are known    pbut M12 is p an unknown hyperparameter with a uniform prior distribution
on the interval . c M11 M22 ; c M11 M22 /. Under this specification, the prior mean of uw is
approximately uniformly distributed as U. c; c/. For all three priors on uw , we specify c D
  0:90 and we vary c as follows: 0.9 for the noninformative prior, -0.35 for the less informative
prior, and -0.87 for the more informative prior. These choices produce the priors on uw plotted in
Figure 5.

Posterior distributions

Drawing .; A; Àõ; Àá/ given Àô

    After changing variables from .; Àõ/ into .Ex ; E /, the equations (8) and (9) can be written as
                                                                                
          x tC1        A 0        xt          IK A         0         Ex              v tC1
                                                                             D               ;
           tC1        0 Àá        t            0       1 Àá          E              w tC1
       ‚Äû ∆í‚Äö ‚Ä¶ ‚Äû ∆í‚Äö ‚Ä¶ ‚Äû ∆í‚Äö ‚Ä¶ ‚Äû                       ∆í‚Äö          ‚Ä¶ ‚Äû ∆í‚Äö ‚Ä¶
           q tC1           L1          qt                   L2             Ex

                                                                             
where the covariance matrix of the residuals is Àô.vw/  Àôvv vw I wv w2 . The prior for Ex is
                                                                
                                    Ex  N Ex0 ; Vx0 ;
                                     h                 i
                      0                 2           2
where Ex0  .0 / N and Vx0  Ex IK 0I 0 E . Since both the prior and the likelihood are
normally distributed, the full conditional posterior distribution of Ex is also normal,
                                                               
                                    Ex j  N EQ x ; VQx ;                              (A66)
                                                                h                       PT                         i
where VQx D .Vx10 CT L02 Àô.vw/
                              1
                                 L2 /       1
                                                and EQ x D VQx Vx10 Ex0 C L02 Àô.vw/
                                                                                     1
                                                                                         tD1 .q tC1        L1 q t / .

     Let x k  .x2k ; : : : ; xTk /0 denote the .T 1/  1 vector of realizations of predictor k in periods
2; : : : ; T , for k D 1; : : : ; K. Also, let x.l/ denote the .T 1/  K vectors of realizations of all K
predictors in periods 1; : : : ; T 1. Similarly, let   .2 ; : : : ; T /0 and .l/  .1 ; : : : ; T 1 /0 ,




                                                       34
and let Ex k be the k-th element of Ex . Denote
        0                   1           0                                                                        1
            x 1 T 1 Ex 1                  x.l/        T      0
                                                            1 Ex    0            0                   0
        B          ::       C           B                          ::                                            C
        B           :       C           B              0              :          0                   0           C
z D B K                     C; Z D B                                                                             C;
        @ x       T 1 Ex K A           @              0            0     x.l/   T      0
                                                                                      1 Ex           0           A
              T 1 E                                 0            0            0           .l/    T   1 E

where T 1 is a .T 1/  1 vector of ones, the dimensions of z are ≈í.T 1/.K C 1/¬ç  1, and the
dimensions of Z are ≈í.T 1/.K C 1/¬ç  .K 2 C 1/. Then we can write the equations (8) and (9) as
                                           z D Zb C errors ;
where b D .vec .A0/0 Àá/0 and the covariance matrix of the error terms is Àô.vw/ Àù IT                 1.   The prior
distribution on b is given by
                                        b  N .b0 ; Vb0 /  1b2S ;
where b0 and Vb0 are chosen as explained earlier and 1b2S is equal to one when x t and  t are
                                          h                      i 1
stationary and zero otherwise. Let VOb D Z 0.Àô.vw/  1
                                                       Àù IT 1 /Z     and bO D VOb Z 0.Àô.vw/
                                                                                         1
                                                                                            ÀùIT 1 /z.
The full conditional posterior distribution of b is then given by
                                                       
                                     bj  N b;    Q VQb  1b2S ;                              (A67)
                                                             
where VQb D .Vb01 C VOb 1 / 1 and bQ D VQb Vb0 1 b0 C VOb 1 bO . We obtain the posterior draws of b
                               
                           Q  Q
by making draws from N b; Vb and retaining only draws that satisfy b 2 S . The posterior draws
of A and Àá are constructed from the posterior draws of b from the definition b D .vec .A0 /0 Àá/0.

Drawing Àô given .; A; Àõ; Àá/
                                                                                   
    Recall that we change variables from Àô D u2 uv uw I vu Àôvv vw I wu wv w2 to the set
of .Àô11 ; C; Àù/, where Àô11  ≈íu2 uw I wu w2 ¬ç, and C and Àù are the slope and the residual
covariance matrix from the regression of v on .u; w/.

    The prior for Àô11 is conditional on the hyperparameter M12 . This hyperparameter can be
drawn from its full conditional posterior density, p.M12 j; D t /, which is given by
                                                                        p           p
                         T0 K 1         T 0       1
p.M12 jÀô11 / / jÀôO 11;0 j 2 exp             tr.Àô11 ÀôO 11;0 / ; M12 2 . c M11 M22 ; c M11 M22 /;
                                        2
                                                                                             (A68)
                                      O
where M12 is the .1; 2/ element of Àô11;0 . Although this is not a density of a well known distri-
bution, we can make posterior draws of M12 easily. We approximate    p this density
                                                                                  p by a piecewise
linear function, using a fine (250-point) grid on the interval . c M11 M22 ; c M11 M22 /. For a
random draw z  U.0; 1/, we find the points on the grid whose cumulative probability densities
are immediately above and below z, and we compute the value of M12 by linear interpolation.

   Conditional on M12 , we have the matrix ÀôO 11;0 in the prior distribution for Àô11 . In addition,
conditional on .; A; Àõ; Àá/, we have the sample of the residuals .u t ; v t ; w t /, t D 1; : : : ; T . Let Y1;T

                                                      35
denote the T  2 matrix of ≈íu t w t ¬ç, let Y2;T denote the T  K matrix of v t , and let X D ≈íT Y1;T ¬ç.
The sample estimates from the regression of Y2;T on Y1;T are given by CO D .X 0X / 1 X 0 Y2;T ,
ÀùO D .Y2;T     X CO /0 .Y2;T   X CO /=T , and ÀôO 11 D Y1;T  0
                                                              Y1;T =T . The posterior of Àô11 has an
inverted Wishart distribution:
                          Àô11 j  I W .T0 ÀôO 11;0 C T ÀôO 11 ; T C T0 K 1/:                          (A69)
                                                             h                       i
In addition, let VC D .X00 X0 C X 0 X / 1 , CQ D VC .X00 X0 /CO0 C .X 0X /CO , cQ D vec .CQ /, and
D D CO00 X00 X0 CO0 C CO 0 X 0 X CO CQ 0 VC 1 CQ . The posterior of Àù has an inverted Wishart distribution:
                           Àùj  IW .S0 ÀùO 0 C T ÀùO C D; T C S0             1/;                      (A70)
and the conditional posterior of c D vec .C / is normal:
                                       cjÀù;   N .c;
                                                   Q Àù Àù VC /:                                       (A71)
Given the posterior draws of .Àô11 ; C; Àù/, we construct the remaining (non-Àô11) elements of Àô as
follows: ≈ívu vw ¬ç D C2 Àô11 and Àôvv D Àù C C2 Àô11 C20 , where C D .C1 C2 /0.

    Our inference is based on 25,000 draws from the posterior distribution. First, we generate a
sequence of 76,000 draws. We discard the first 1,000 draws as a ‚Äúburn-in‚Äù and take every third
draw from the rest to obtain a series of 25,000 draws that exhibit little serial correlation. The
posterior draws of the relevant quantities such as uw , x , R2 . t on x t /, R2 .r tC1 on  t /, etc.
are constructed easily from the posterior draws of the basic parameters in the model.

                                             The R2 ratios.

    The numerator of the R2 ratio in equation (25) is computed as

      2               Var.E. t jx t //   Var.E. t / C Vx Vxx1 .x t     E.x t ///       Vx Vxx1 Vx
                                                                                                    0
    R . t on x t / D                   D                                             D                  ;
                         Var. t /                      Var. t /                             V
                                                                                                     (A72)
where Vxx , V , and Vx are given in equations (A16), (A17), and (A18), respectively.

    The denominator of the R2 ratio in equation (25) is computed as
                                  Var.E. t jD t //   Var. t / Var. t jD t //           Qt
             R2 . t on D t / D                     D                           D1            ;      (A73)
                                     Var. t /                 Var. t /                  V
where Q t is given in equation (A47). We replace Q t by its steady-state value, Q, which can be
shown to be equal to a solution of a quadratic equation:
              q
                 12 42 1
     Q D                       ;                                                          (A74)
                      2
     1 D .1 Àá 2 /.u2 uv Àôvv1 vu/ C 2Àá.uw wv Àôvv1 vu/ .w2 wv Àôvv1 vw /
         D .1 Àá 2 /Var.ujv/ C 2ÀáCov.u; wjv/ Var.wjv/
     2 D .uw wv Àôvv1 vu /2 .u2 uv Àôvv1 vu /.w2 wv Àôvv1 vw /
         D Cov.u; wjv/2 Var.ujv/Var.wjv/ < 0

                                                    36
   The value of Q is also used in computing the steady-state values of M t and N t from equation
(A54), denoted by m t and n t in the scalar case:
                                                                            1
                         m D .ÀáQ C Cov.u; wjv//.Q C Var.ujv//                                  (A75)
                         n D .wv muv /Àôvv1 :                                                 (A76)


                          Variance decomposition of expected return.


    In equation (32), the conditional expected return  t depends on three time-varying variables:
   1. C1 D P x t , the current predictor values
                 1
   2. C2 D PiD0 Àá i u t i , an infinite
                                        sum of current and lagged unexpected returns
                 1      i        i
   3. C3 D iD0 Àá IK A v t i , an infinite sum of current and lagged predictor innovations ,
plus an error term. In the variance decomposition in Table V, we consider regressions of  t on
various subsets of .C 1; C 2; C 3/. Let C denote a given subset of .C 1; C 2; C 3/. The R2 from the
regression of  t on C is equal to
                                                       0
                                                      VC VC 1 VC
                                 R2 . t on C / D                    :                         (A77)
                                                          V

The matrix VC , the covariance matrix of C , is pieced together from

               Var.C1/ D Vxx
               Var.C2/ D u2 .1 Àá 2 / 1
                         
         vec .Var.C3// D .1 Àá 2 / 1 IK 2       .IK ÀáA/ 1 Àù IK               IK Àù .IK     ÀáA/ 1 C
                                                 
                              C .IK 2    A Àù A/ 1 vec .Àôvv /
          Cov.C1, C2/ D .IK ÀáA/ 1 vu
                                                
          Cov.C2, C3/ D .1 Àá 2 / 1 IK .IK ÀáA/ 1 vu
                                                        
    vec .Cov.C1, C30 // D IK Àù .IK ÀáA/ 1 C .IK 2 A Àù A/ 1 vec .Àôvv /;

and VC , the vector of covariances between  t and C , is built from

             Cov. t ; C10 / D     v Var.C1/ C    u Cov.C1, C2/0 C       v Cov.C1,  C30 /0
             Cov. t ; C2/ D       u Var.C2/ C    v Cov.C1, C2/ C        v Cov.C2, C3/
             Cov. t ; C30 / D     v Var.C3/ C
                                                              0
                                                  v Cov.C1, C3 / C
                                                                                        0
                                                                          u Cov.C2, C3/ :




                                                 37
                           Panel A.      Coefficients on lagged forecast errors in E(r | D )
                                                                                          t+1    t


 0.02
    0
                                                                                                      œÅ     =0
                                                                                                       uw
‚àí0.02
                                                                                                      œÅuw = ‚àí0.47
‚àí0.04
                                                                                                      œÅuw = ‚àí0.85
‚àí0.06
                                                                                                      œÅ     = ‚àí0.99
‚àí0.08                                                                                                  uw

        0      20        40         60         80       100       120         140          160       180         200

                                 Panel B.   Coefficients on lagged returns in E(r | D )
                                                                                    t+1    t


 0.02
    0
‚àí0.02
‚àí0.04
‚àí0.06
‚àí0.08
        0      20        40         60         80       100       120         140          160       180         200

                      Panel C.     Weights on lagged returns in E(r | D , E(r) = sample mean)
                                                                    t+1   t
 0.04

 0.02

    0

‚àí0.02

‚àí0.04

        0      20        40         60         80      100       120          140          160       180         200
                                                      Return Lag

Figure 1. The effect of lagged returns on E.r tC1 jD t / when no predictors are used. Panel A plots
s , the coefficients on lagged forecast errors ( t s D r t s E.r t s jD t s 1 /) in E.r tC1 jD t /. Panel B
plots !s , the coefficients on lagged total returns in E.r tC1 jD t /. Panel C plots s , the weights on lagged
total returns in E.r tC1 jD t / when the unconditional mean return is estimated by the sample mean over the
previous 208 quarters (which is the length of the sample used in subsequent analysis). No predictors are
used in the predictive system. The steady-state values of all coefficients are plotted. The different lines
correspond to different values of uw , the correlation between expected and unexpected returns. The mean
reversion coefficient in the AR(1) process for the conditional expected return  t is set equal to Àá D 0:9. The
predictive R2 ‚Äîthe fraction of variation in r tC1 than can be explained by  t ‚Äîis set equal to R2 D 0:05.




                                                        38
                             œÅ     =0
                              uw
                    0.08
                             œÅuw = ‚àí0.47
                             œÅuw = ‚àí0.85
                    0.07     œÅuw = ‚àí0.99


                    0.06



                    0.05



                    0.04
 Expected return




                    0.03



                    0.02



                    0.01



                      0



                   ‚àí0.01



                   ‚àí0.02

                      1952         1960    1969         1977            1986            1994             2003
                                                         Year


Figure 2. The equity premium E.r tC1 jD t / from the predictive system with no predictors. This figure
plots the time series of the quarterly equity premium estimated for four different values of uw , the correla-
tion between expected and unexpected returns. The mean reversion coefficient in the AR(1) process for the
conditional expected return  t is set equal to Àá D 0:9. The predictive R2 ‚Äîthe fraction of variation in r tC1
than can be explained by  t ‚Äîis set equal to R2 D 0:05. The parameters represent quarterly values.




                                                      39
                         Panel A. œÅ       =0                                   Panel B. œÅ        = 0.3
                                     vw                                                     vw
      0.05                                                      0.05
                    System, Low œÅ
                                     uv
     0.045                                                     0.045
                    System, High œÅuv
      0.04          Pred Regression
                                                                0.04
                    All Past Returns
     0.035
                                                               0.035

      0.03
                                                                0.03
R2




     0.025




                                                           2
                                                               0.025




                                                           R
      0.02
                                                                0.02
     0.015
                                                               0.015
      0.01
                                                                0.01
     0.005
                                                               0.005
         0
         ‚àí1       ‚àí0.5          0              0.5   1
                               œÅuw                                0
                                                                  ‚àí1       ‚àí0.5         0            0.5     1
                                                                                       œÅuw




                      Panel C. œÅ       = 0.6                                   Panel D. œÅ        = 0.9
                                  vw                                                      vw
      0.05                                                      0.05

     0.045                                                     0.045

      0.04                                                      0.04

     0.035                                                     0.035

      0.03                                                      0.03
R2




                                                           2




     0.025                                                     0.025
                                                           R




      0.02                                                      0.02

     0.015                                                     0.015

      0.01                                                      0.01

     0.005                                                     0.005

         0                                                        0
         ‚àí1       ‚àí0.5          0              0.5   1            ‚àí1       ‚àí0.5         0            0.5     1
                               œÅuw                                                     œÅuw




Figure 3. Predictive R2 ‚Äôs. Each panel plots the R2 ‚Äôs from three approaches to predicting stock returns r tC1
using information observable at time t . The approaches are: the predictive regression of r tC1 on a single
predictor x t (solid line), the ARMA(1,1) model that uses the full history of past returns but no predictor
data (dotted line), and the predictive system, which uses the full history of returns and predictor realizations
(dashed and dash-dot lines). The dashed (dash-dot) line corresponds to a ‚Äúlow‚Äù (‚Äúhigh‚Äù) value of uv , which
represents the value obtained when the partial correlation between u t and v t given w t equals uvjw D 0:9
(0.9). The conditional correlation between  t and x t , vw , ranges from 0 in Panel A to 0.9 in Panel D. In
all four panels, Àá D A D 0:9, and the true predictive R2 (from the regression of r tC1 on  t ) is 0.05.


                                                      40
                                       Panel A. Predictor: Dividend Yield
  0.1

 0.05

    0

‚àí0.05
                 Regression, fitted values
                 System, maximum likelihood
 ‚àí0.1
    1952                     1965                     1978                     1991                     2004

                                            Panel B. Predictor: CAY
  0.1

 0.05

    0

‚àí0.05

 ‚àí0.1
    1952                     1965                     1978                     1991                     2004

                           Panel C. Predictors: Dividend Yield, CAY, and Bond Yield
  0.1

 0.05

    0

‚àí0.05

 ‚àí0.1
    1952                     1965                     1978                     1991                     2004

Figure 4. The equity premium: Regression vs. system with no prior information. This figure plots the
time series of the quarterly equity premium estimated in two different environments. The dashed line plots
the OLS fitted values from the predictive regression of r tC1 on the given predictor(s). The dotted line plots
the maximum likelihood estimates of E.r tC1 jD t / from the predictive system. In Panel A, the estimation
uses one predictor, dividend yield. In Panel B, the single predictor is CAY. In Panel C, three predictors are
used: dividend yield, CAY, and the bond yield. The sample period is 1952Q1‚Äì2003Q4.




                                                     41
                                                                                                       2
             Panel A. Priors for œÅuw                                Panel B. Implied priors for œÅuw
 9                                                         6
                              More informative                          More informative
 8                            Less informative                          Less informative
                              Noninformative                            Noninformative
                                                           5
 7


 6                                                         4

 5
                                                           3
 4


 3                                                         2

 2
                                                           1
 1


 0                                                         0
 ‚àí1        ‚àí0.5          0         0.5           1             0     0.2       0.4         0.6     0.8     1




               Panel C. Prior for R2                                       Panel D. Prior for Œ≤
60                                                         6



50                                                         5



40                                                         4



30                                                         3



20                                                         2



10                                                         1



 0                                                         0
     0      0.05        0.1       0.15        0.2          0.2         0.4           0.6         0.8       1



Figure 5. Prior distributions. Panel A plots three prior distributions for the correlation between expected
and unexpected returns, uw . The noninformative prior (dotted line) is flat between -0.9 and 0.9, with
tails fading away as uw approaches Àô1. The less informative prior (dashed line) has 99.9% of its mass
below zero (uw < 0). The more informative prior (solid line) has 99.9% of its mass below -0.71, so that
  2
uw   > 0:5 (i.e., unexpected changes in the discount rate explain over half of the variance of unexpected
                                                                         2
market returns). Panel B plots the corresponding implied priors on uw      . Panel C plots the prior on the
              2
predictive R from the regression of returns r tC1 on expected returns  t . Panel D plots the prior on the
slope coefficient Àá in the AR(1) process for  t . All parameters correspond to quarterly data.



                                                     42
                                                                         Panel B.       Coefficients œÜs (Dividend Yield)
        Panel A.       Coefficients Œªs (Dividend Yield)
                                                                  0.08                           Diffuse prior
 0.15                                                                                            Noninformative (œÅuw)
                                                                  0.06
                                                                                                 Less informative (œÅuw)
  0.1                                                                                            More informative (œÅ )
                                                                  0.04                                                uw

 0.05                                                             0.02

    0                                                               0
        0               10                20           30                0               10                20              30

            Panel C.    Coefficients Œªs (Bond Yield)                         Panel D.    Coefficients œÜs (Bond Yield)
 0.01
                                                                 0.015

    0
                                                                  0.01
‚àí0.01
                                                                 0.005
‚àí0.02

                                                                    0
        0               10                20           30                0               10                20              30

              Panel E.       Coefficients Œªs (CAY)                             Panel F.       Coefficients œÜs (CAY)

 0.03                                                            0.025

 0.02                                                             0.02

 0.01                                                            0.015

    0                                                             0.01
‚àí0.01                                                            0.005
‚àí0.02                                                               0
        0               10                20           30                0               10                20              30
                             Return Lag                                                       Return Lag

Figure 6. Coefficients on lagged forecast errors and predictor innovations in E.r tC1 jD t /. Panels A,
C, and E plot steady-state values of s , the coefficients on lagged forecast errors in the expression for the
conditional expected return. Panels B, D, and F plot steady-state values of s , the coefficients on lagged
predictor innovations. Panel headings indicate which predictors are used in the predictive system. The four
lines in each panel represent four different prior distributions on uw , the correlation between expected and
unexpected returns. The solid line represents the ‚Äúmore informative‚Äù prior on uw (uw < 0:71), the
dashed line is the ‚Äúless informative‚Äù prior on uw (uw < 0), the dotted line is the ‚Äúnoninformative‚Äù prior
on uw ; and the dash-dot line is the ‚Äúdiffuse‚Äù prior that is noninformative about all parameters in the model
(not only about uw ). The sample period is 1952Q1‚Äì2003Q4.


                                                            43
                                           Panel A. Slope on Bond Yield
200
                                                                     Regression, diffuse prior
150                                                                  System, noninformative about œÅuw
                                                                     System, more informative about œÅ
                                                                                                        uw
100

 50

  0
 ‚àí0.01     ‚àí0.005         0        0.005        0.01        0.015    0.02       0.025       0.03      0.035

                                       Panel B. Slope on Dividend Yield


100



 50



  0
 ‚àí0.01     ‚àí0.005         0        0.005        0.01        0.015    0.02       0.025       0.03      0.035

                                             Panel C. Slope on CAY

150


100


 50


  0
 ‚àí0.01     ‚àí0.005         0        0.005        0.01        0.015    0.02       0.025       0.03      0.035

Figure 7. Posterior distributions of slope coefficients from predictive regressions. We estimate both the
predictive system and the predictive regression with three predictors: the bond yield, dividend yield, and
CAY. The dashed line plots the posteriors from the standard predictive regression of r tC1 on x t under the
diffuse prior. The dotted line plots the implied posteriors constructed from the results of the predictive sys-
tem under the ‚Äúnoninformative‚Äù prior on uw . The solid line plots the implied posteriors from the predictive
system under the ‚Äúmore informative‚Äù prior on uw (uw < 0:71). To facilitate comparisons across panels,
all predictors are scaled to have unit variance. The sample period is 1952Q1‚Äì2003Q4.




                                                       44
          Panel A. R2 from Regression of ¬µ on x                       Panel B. R2 from Regression of r           on ¬µ
                                          t    t                                                           t+1        t
  3                                                          30
                                                                                          Noninformative (œÅuw)
2.5                                                          25                           Less informative (œÅ )
                                                                                                                 uw
                                                                                          More informative (œÅ )
                                                                                                                 uw
  2                                                          20


1.5                                                          15


  1                                                          10


0.5                                                           5


  0                                                           0
      0        0.2     0.4     0.6     0.8         1              0            0.05       0.1           0.15          0.2



                     Panel C. œÅv,w                                                    Panel D. œÅ
                                                                                                  x,¬µ
  5
                                                              4

  4                                                         3.5

                                                              3
  3                                                         2.5

                                                              2
  2
                                                            1.5

                                                              1
  1
                                                            0.5

  0                                                           0
              0               0.5                  1                       0                    0.5                       1
Figure 8. Posterior distributions for one predictor: Dividend yield. The three lines in each panel
represent three different prior distributions. The solid line represents the ‚Äúmore informative‚Äù prior on uw
(uw < 0:71), the dashed line is the ‚Äúless informative‚Äù prior on uw (uw < 0), and the dotted line is
the ‚Äúnoninformative‚Äù prior on uw . Panel A plots the posterior of the fraction of variation in the expected
return  t that can be explained by the predictors x t . Panel B plots the posterior of the predictive R2 . Panel
C plots the posterior of the conditional correlation vw between the dividend yield and  t . Panel D plots
the posterior of the unconditional correlation x between the dividend yield and  t . The sample period is
1952Q1‚Äì2003Q4.




                                                       45
          Panel A. R2 from Regression of ¬µ on x                        Panel B. R2 from Regression of r        on ¬µ
                                          t     t                                                        t+1        t
                                                             35
10                                                                                       Noninformative (œÅuw)
                                                             30                          Less informative (œÅ )
                                                                                                               uw
  8                                                                                      More informative (œÅ )
                                                             25                                                uw


  6                                                          20

                                                             15
  4
                                                             10
  2
                                                              5

  0                                                           0
      0        0.2     0.4     0.6     0.8          1              0          0.05       0.1          0.15          0.2



                     Panel C. œÅv,w                                                   Panel D. œÅ
                                                                                                x,¬µ
  4

3.5                                                           4

                                                             3.5
  3
                                                              3
2.5
                                                             2.5
  2
                                                              2
1.5
                                                             1.5
  1
                                                              1
0.5                                                          0.5

  0                                                           0
                 0             0.5                  1                          0               0.5                      1
Figure 9. Posterior distributions for one predictor: Bond yield. The three lines in each panel rep-
resent three different prior distributions. The solid line represents the ‚Äúmore informative‚Äù prior on uw
(uw < 0:71), the dashed line is the ‚Äúless informative‚Äù prior on uw (uw < 0), and the dotted line is the
‚Äúnoninformative‚Äù prior on uw . Panel A plots the posterior of the fraction of variation in the expected return
 t that can be explained by the predictors x t . Panel B plots the posterior of the predictive R2 . Panel C plots
the posterior of the conditional correlation vw between the bond yield and  t . Panel D plots the posterior
of the unconditional correlation x between the bond yield and  t . The sample period is 1952Q1‚Äì2003Q4.




                                                        46
         Panel A. R2 from Regression of ¬µ on x                          Panel B. R2 from Regression of r                on ¬µ
                                            t     t                                                               t+1      t
 3
                                                               20
 2
                                                               10
 1

 0                                                              0
     0        0.2        0.4     0.6      0.8         1             0          0.05          0.1               0.15        0.2
          Panel C.    Partial œÅv,w (Bond Yield)                           Panel D.     Partial œÅ         (Bond Yield)
                                                                                                   x,¬µ
 4
                                                                3

                                                                2
 2
                                                                1

 0                                                              0
‚àí0.5                 0              0.5               1        ‚àí0.5                   0                  0.5                   1
         Panel E.    Partial œÅv,w (Dividend Yield)                       Panel F.     Partial œÅx,¬µ (Dividend Yield)
                                                                2
 6

 4
                                                                1
 2

 0                                                              0
‚àí0.5                 0              0.5               1        ‚àí0.5                   0                  0.5                   1
             Panel G.      Partial œÅv,w (CAY)                                Panel H.      Partial œÅ           (CAY)
                                                                                                         x,¬µ
 3
                                                                2
 2

 1                                                              1

 0                                                              0
‚àí0.5                 0              0.5               1        ‚àí0.5                   0                  0.5                   1
Figure 10. Posterior distributions for three predictors: Bond yield, dividend yield, and CAY. The
three lines in each panel represent three different prior distributions. The solid line represents the ‚Äúmore
informative‚Äù prior on uw (uw < 0:71), the dashed line is the ‚Äúless informative‚Äù prior on uw (uw < 0),
and the dotted line is the ‚Äúnoninformative‚Äù prior on uw . Panel A plots the posterior of the fraction of
variation in the expected return  t that can be explained by the predictors x t . Panel B plots the posterior of
the predictive R2 . Panels C, E, and G plot the posteriors of the conditional partial correlation vw between
the given predictor and  t . Panels D, F, and H plot the posteriors of the unconditional partial correlation
x between the given predictor and  t . The sample period is 1952Q1‚Äì2003Q4.




                                                          47
                                       Panel A. Predictor: Dividend Yield

 0.04
 0.02
    0
‚àí0.02            Regression, fitted values
‚àí0.04            System, noninformative about œÅuw
‚àí0.06            System, more informative about œÅ
                                                  uw

    1952                     1965                      1978                    1991                     2004

                                            Panel B. Predictor: CAY

 0.06
 0.04
 0.02
    0
‚àí0.02
‚àí0.04

    1952                     1965                      1978                    1991                     2004

                           Panel C. Predictors: Dividend Yield, CAY, and Bond Yield
  0.1


 0.05


    0


‚àí0.05

    1952                     1965                      1978                    1991                     2004

Figure 11. The equity premium: Regression vs. system with prior information. This figure plots the
time series of the quarterly equity premium estimated in three different environments. The dashed line plots
the OLS fitted values from the predictive regression of r tC1 on the given predictor(s). The dotted line plots
the posterior means of E.r tC1 jD t / from the predictive system under the ‚Äúnoninformative‚Äù prior on uw . The
solid line plots the posterior means of E.r tC1 jD t / from the predictive system under the ‚Äúmore informative‚Äù
prior on uw (uw < 0:71). In Panel A, the estimation uses one predictor, dividend yield. In Panel B, the
single predictor is CAY. In Panel C, three predictors are used: dividend yield, CAY, and bond yield. The
sample period is 1952Q1‚Äì2003Q4.

                                                       48
                                                        Table I
                                                Predictive Regressions
This table summarizes the results from predictive regressions r t D a C b 0 x t 1 C e t , where x t D  C Ax t 1 C v t . r t
denotes quarterly excess stock market returns and x t 1 denotes the predictors (listed in the column headings) lagged
by one quarter. The table reports the estimated slope coefficients b,  O the correlation Corr.e t ; b 0v t / between unexpected
returns and shocks to expected returns, and the R2 from the predictive regression. The OLS t-statistics are given
in parentheses ‚Äú( )‚Äù. The t-statistic of Corr.e t ; b 0v t / is computed as the t-statistic of the slope coefficient from the
regression of the sample residuals eO t on bO vO t . The p-values associated with all t-statistics and R2 s are computed by
bootstrapping and reported in brackets ‚Äú[ ]‚Äù.
                     Bond Yield       Dividend Yield       CAY           Corr.e t ; b 0v t /  100   R2  100
                                                Panel A. 1952 Q1 ‚Äì 2003 Q4
                        2.716                                                   21.735                4.231
                       (3.024)                                                  (3.204)              [0.002]
                       [0.001]                                                  [0.001]
                                           1.153                                -91.887               2.252
                                          (2.184)                              (-33.506)             [0.059]
                                          [0.057]                               [1.000]
                                                           1.704                -53.556               7.292
                                                          (4.035)               (-9.124)             [0.000]
                                                          [0.000]               [1.000]
                        2.573              1.028           1.346                -35.635              11.777
                       (2.902)            (1.966)         (3.139)               (-5.487)             [0.000]
                       [0.003]            [0.058]         [0.003]               [1.000]
                                                Panel B. 1952 Q1 ‚Äì 1977 Q4
                        6.385                                                   25.079                7.080
                       (2.801)                                                  (2.629)              [0.007]
                       [0.004]                                                  [0.008]
                                           2.658                                -96.531               7.003
                                          (2.785)                              (-37.522)             [0.015]
                                          [0.014]                               [1.000]
                                                           3.028                -47.663              15.024
                                                          (4.267)               (-5.503)             [0.000]
                                                          [0.000]               [1.000]
                        3.489              1.345           2.129                -53.153              17.975
                       (1.490)            (1.349)         (2.534)               (-6.369)             [0.000]
                       [0.090]            [0.177]         [0.012]               [1.000]
                                                Panel C. 1978 Q1 ‚Äì 2003 Q4
                        2.073                                                   22.624                3.931
                       (2.053)                                                  (2.357)              [0.047]
                       [0.020]                                                  [0.011]
                                           0.784                                -88.194               1.273
                                          (1.152)                              (-18.989)             [0.423]
                                          [0.409]                               [1.000]
                                                           1.165                -56.949               4.122
                                                          (2.104)               (-7.031)             [0.045]
                                                          [0.037]               [1.000]
                        2.203              0.755           0.968                -18.619               8.828
                       (2.197)            (1.101)         (1.734)               (-1.923)             [0.053]
                       [0.023]            [0.313]         [0.118]               [0.967]

                                                             49
                                                   Table II
               Explanatory Power of the Predictive Regression Relative to the Predictive System:
                                             Theoretical Results
This table shows the ratios of two R-squareds, R21 =R22 . A ratio smaller than one indicates that the predictive system
estimates  t more precisely than the predictive regression does. The smaller the ratio, the larger the advantage of
using the predictive system. R21 , computed as the R-squared from the regression of the true expected return  t
on a given predictor, summarizes the usefulness of the predictive regression in estimating  t . R22 , computed as
1 Var. t jD t /=Var. t / where D t contains all historical returns and predictor realizations, summarizes the usefulness
of the predictive system in estimating  t . The table reports the mean, minimum, and maximum of the possible values
of R21 =R22 under the model parameters specified. uw is the correlation between expected and unexpected returns, vw
is the correlation between the residuals of the AR(1) processes for  t and the predictor, Àá and A are the first-order
autocorrelations of  t and the predictor, respectively. The values are computed under the assumption that  t explains
5% of the variance in realized returns.


                         uw D 0              uw D    0:473             uw D      0:85            uw D     0:99
         vw     Mean      Min     Max     Mean    Min     Max        Mean    Min      Max      Mean    Min      Max

                                                      Panel A. Àá D 0:9; A D 0:97
         0.1     0.034    0.007    0.045   0.524   0.182   0.696      0.026   0.010    0.031    0.009   0.007    0.009
         0.2     0.131    0.028    0.181   0.527   0.183   0.696      0.099   0.033    0.125    0.035   0.028    0.037
         0.3     0.271    0.063    0.407   0.531   0.184   0.696      0.208   0.064    0.281    0.079   0.063    0.084
         0.4     0.404    0.112    0.678   0.538   0.185   0.696      0.329   0.112    0.495    0.139   0.111    0.149
         0.5     0.487    0.174    0.696   0.549   0.191   0.696      0.436   0.175    0.659    0.215   0.174    0.233
         0.6     0.537    0.251    0.696   0.565   0.253   0.696      0.516   0.251    0.696    0.305   0.251    0.335
         0.7     0.574    0.342    0.696   0.588   0.342   0.696      0.573   0.342    0.696    0.406   0.341    0.453
         0.8     0.612    0.446    0.696   0.617   0.446   0.696      0.616   0.446    0.696    0.513   0.446    0.571
         0.9     0.652    0.564    0.696   0.653   0.564   0.696      0.653   0.564    0.696    0.618   0.564    0.668

                                                      Panel B. Àá D 0:9; A D 0:9
         0.1     0.049    0.010    0.065   0.753   0.262   1.000      0.037   0.015    0.045    0.013   0.010    0.013
         0.2     0.188    0.040    0.260   0.757   0.263   1.000      0.142   0.048    0.179    0.050   0.040    0.053
         0.3     0.389    0.091    0.585   0.763   0.264   1.000      0.298   0.092    0.403    0.113   0.090    0.120
         0.4     0.580    0.161    0.974   0.773   0.265   1.000      0.472   0.161    0.711    0.200   0.160    0.214
         0.5     0.700    0.251    1.000   0.788   0.275   1.000      0.626   0.251    0.946    0.309   0.250    0.334
         0.6     0.771    0.361    1.000   0.812   0.363   1.000      0.741   0.361    1.000    0.438   0.360    0.481
         0.7     0.825    0.491    1.000   0.844   0.492   1.000      0.823   0.491    1.000    0.583   0.490    0.651
         0.8     0.878    0.640    1.000   0.886   0.641   1.000      0.885   0.640    1.000    0.736   0.640    0.820
         0.9     0.936    0.810    1.000   0.938   0.810   1.000      0.938   0.810    1.000    0.888   0.810    0.959

                                                      Panel C. Àá D 0:97; A D 0:9
         0.1     0.017    0.007    0.020   0.051   0.022   0.063      0.501   0.273    0.684    0.433   0.371    0.495
         0.2     0.066    0.028    0.080   0.182   0.061   0.251      0.603   0.386    0.696    0.605   0.541    0.659
         0.3     0.145    0.063    0.181   0.332   0.102   0.531      0.625   0.434    0.696    0.652   0.600    0.689
         0.4     0.247    0.112    0.322   0.444   0.139   0.683      0.633   0.460    0.696    0.670   0.628    0.695
         0.5     0.360    0.174    0.503   0.515   0.179   0.696      0.638   0.477    0.696    0.679   0.643    0.696
         0.6     0.464    0.251    0.660   0.561   0.252   0.696      0.641   0.490    0.696    0.684   0.654    0.696
         0.7     0.547    0.341    0.696   0.594   0.342   0.696      0.645   0.499    0.696    0.687   0.661    0.696
         0.8     0.607    0.446    0.696   0.623   0.446   0.696      0.650   0.508    0.696    0.689   0.667    0.696
         0.9     0.651    0.564    0.696   0.655   0.564   0.696      0.662   0.565    0.696    0.690   0.673    0.696




                                                               50
                                              Table III
           Explanatory Power of the Predictive Regression Relative to the Predictive System:
                                         Empirical Results


This table shows the posterior means and standard deviations (the latter in parentheses) of the ratios of two R-squareds,
R21 =R22 . A ratio smaller than one indicates that the predictive system estimates  t more precisely than the predictive
regression does. The smaller the ratio, the larger the advantage of using the predictive system. R21 , computed as the
R-squared from the regression of the true expected return  t on the given predictors, summarizes the usefulness of
the predictive regression in estimating  t . R22 , computed as 1 Var. t jD t /=Var. t / where D t contains all historical
market returns and predictor realizations, summarizes the usefulness of the predictive system in estimating  t . The
resuts are reported for four different prior distributions on uw , the correlation between expected and unexpected re-
turns. Four sets of predictors are considered: dividend yield, bond yield, CAY, and all three predictors combined. The
sample period is 1952Q1‚Äì2003Q4.



                                                                   Predictors

                                 Dividend Yield            Bond Yield            CAY           All 3 Predictors
       Diffuse                          0.28                   0.73              0.86                 0.59
       Prior                           (0.17)                 (0.23)            (0.16)               (0.30)

       Noninformative                   0.50                   0.44              0.61                 0.46
       Prior on uw                    (0.27)                 (0.25)            (0.27)               (0.22)

       Less Informative                 0.59                   0.34              0.73                 0.50
       Prior on uw                    (0.22)                 (0.20)            (0.23)               (0.22)

       More Informative                 0.81                   0.08              0.64                 0.70
       Prior on uw                    (0.19)                 (0.08)            (0.22)               (0.19)




                                                            51
                                                 Table IV
                                  Comparing Estimates of Expected Return.

This table compares the time series of the posterior means of E.r t C1 jD t / obtained in five different environments:
(1) Predictive regression: OLS fitted values
(2) Predictive system: Maximum likelihood estimates
(3) Predictive system: Noninformative prior about uw
(4) Predictive system: Less informative prior about uw
(5) Predictive system: More informative prior about uw
The priors in (3)-(5) are informative about the persistence and volatility of  t . The correlations between the quarterly
series of the posterior means of E.r t C1 jD t / are reported in italics below the main diagonal of each left-panel 5  5
matrix. Above the main diagonal of the same matrix are the mean absolute differences between the posterior means
of E.r t C1 jD t / in percent per quarter. Each right-panel 5  5 matrix reports the average utility losses, in percent per
quarter, of a mean-variance investor who is forced to hold a suboptimal portfolio of the stock market and a risk-free
T-bill: a portfolio that is optimal under the beliefs in the given row when the true beliefs are in the given column.
(For example, the (2,5) cell of the 5  5 matrix reports the certainty equivalent loss of an investor who has the more
informative prior but is forced to hold the portfolio that is optimal under the maximum likelihood estimates.) The risk
aversion is chosen such that there is no borrowing or lending given the sample mean and variance of market returns.
The sample period is 1952Q1-2003Q4.



                     Correlation (%) n Mean Abs Diff (%)                        Average Utility Loss (%)

                       (1)       (2)      (3)       (4)      (5)          (1)      (2)    (3)     (4)      (5)

                                              Panel A. Predictor: Dividend Yield

           (1)                  0.55     0.44      0.41     0.29          0        0.15   0.09    0.07    0.03
           (2)       84.55               0.48      0.45     0.47         0.15        0    0.13    0.11    0.11
           (3)       90.47     87.51               0.06     0.27         0.09      0.13    0      0.00    0.03
           (4)       92.42     89.36     99.78              0.22         0.07      0.11   0.00     0      0.02
           (5)       97.67     90.47     95.81    97.41                  0.03      0.11   0.03    0.02      0

                                                   Panel B. Predictor: CAY

           (1)                  0.59     1.13      1.22     1.60          0        0.16   0.57    0.65    1.13
           (2)       95.15               1.23      1.32     1.65         0.16        0    0.72    0.86    1.40
           (3)       86.28     87.33               0.37     0.96         0.57      0.70    0      0.06    0.39
           (4)       96.88     95.75     95.43              0.63         0.64      0.83   0.06     0      0.16
           (5)       89.71     88.20     59.38    80.11                  1.09      1.34   0.38    0.16      0

                                  Panel C. Predictors: Dividend Yield, CAY, Bond Yield

           (1)                  1.27     1.33      1.27     1.60          0        0.74   0.82    0.74    1.19
           (2)       84.30               1.33      1.33     1.80         0.75        0    0.92    0.88    1.49
           (3)       80.38     80.68               0.14     1.51         0.80      0.88    0      0.01    0.94
           (4)       82.30     81.75     99.79              1.46         0.72      0.84   0.01     0      0.84
           (5)       83.42     79.08     80.75    84.00                  1.19      1.45   0.96    0.87      0



                                                           52
                                                         Table V
                                         Variance Decomposition of Expected Return.

     This table reports the posterior means and standard deviations (the latter in parentheses) of the R2 s from the regressions
     of the market‚Äôs expected excess return  t on its selected components. The first column of each panel, labeled x t , shows
     the fraction of variance of  t that can be explained by the set of predictors listed in the panel heading. Four sets of
     predictors are considered: the dividend yield, bond yield, CAY, and the combination of all three of these predictors.
     The second column of each panel, labeled x t ; fus gst , shows the fraction of variance of  t that can be explained
     jointly by the predictors and by the innovations to stock market returns u t ; u t 1; u t 2; : : :. The third column, labeled
     x t ; fus ; vs gst , shows the fraction of variance of  t that can be explained jointly by the predictors, by the innovations
     to stock market returns u t ; u t 1; u t 2; : : :, and by the innovations to the predictors v t ; v t 1; v t 2 ; : : :. For each set of
     predictors, the predictive system is estimated under three different priors, which are described in the row labels. The
     sample period is 1952Q1-2003Q4.




                                Components of Expected Return                                 Components of Expected Return

                               xt          x t , fus gst    x t , fus ; vs gst             xt         x t , fus gst    x t , fus ; vs gst

                                      Panel A. Dividend Yield                                        Panel B. Bond Yield

Noninformative                0.34             0.43                0.48                     0.33             0.64                0.83
Prior on uw                 (0.20)           (0.21)              (0.21)                   (0.21)           (0.21)              (0.13)
Less Informative              0.40             0.49                0.53                     0.24             0.73                0.86
Prior on uw                 (0.18)           (0.18)              (0.18)                   (0.17)           (0.16)              (0.11)
More Informative              0.57             0.80                0.81                     0.03             0.86                0.95
Prior on uw                 (0.15)           (0.06)              (0.06)                   (0.03)           (0.05)              (0.04)

                                            Panel C. CAY                                        Panel D. All Three Predictors

Noninformative                0.50             0.59                0.81                     0.42             0.49                0.90
Prior on uw                 (0.22)           (0.22)              (0.12)                   (0.20)           (0.22)              (0.07)
Less Informative              0.60             0.70                0.83                     0.46             0.55                0.90
Prior on uw                 (0.20)           (0.17)              (0.10)                   (0.20)           (0.22)              (0.07)
More Informative              0.53             0.87                0.92                     0.63             0.85                0.94
Prior on uw                 (0.18)           (0.07)              (0.05)                   (0.17)           (0.12)              (0.04)




                                                                      53
                                          References
Ang, Andrew, and Geert Bekaert, 2006, Stock return predictability: Is it there?, Review of Finan-
   cial Studies, forthcoming.
Ang, Andrew, and Monika Piazzesi, 2003, A no-arbitrage vector autoregression of term structure
   dynamics with macroeconomic and latent variables, Journal of Monetary Economics 50, 745‚Äì
   787.
Avramov, Doron, 2002, Stock return predictability and model uncertainty, Journal of Financial
   Economics 64, 423‚Äì458.
Avramov, Doron, 2004, Stock return predictability and asset pricing models, The Review of Finan-
   cial Studies 17, 699‚Äì738.
Avramov, Doron, and Russ Wermers, 2006, Investing in mutual funds when returns are predictable
   Journal of Financial Economics 81, 339‚Äì377.
Baks, Klaas P., Andrew Metrick, and Jessica Wachter, 2001, Should investors avoid all actively
   managed mutual funds? A study in Bayesian performance evaluation, Journal of Finance 56,
   45‚Äì85.
Brandt, Michael W., and Qiang Kang, 2004, On the relationship between the conditional mean
   and volatility of stock returns: A latent VAR approach, Journal of Financial Economics 72,
   217‚Äì257.
Campbell, John Y., 1987, Stock returns and the term structure, Journal of Financial Economics 18,
   373‚Äì399.
Campbell, John Y., 1991, A variance decomposition for stock returns, The Economic Journal 101,
   157‚Äì179.
Campbell, John Y., and John Ammer, 1993, What moves the stock and bond markets? A variance
   decomposition for long-term asset returns, Journal of Finance 48, 3‚Äì37.
Campbell, John Y., and Motohiro Yogo, 2006, Efficient tests of stock return predictability, Journal
   of Financial Economics 81, 27‚Äì60.
Campbell, John Y., and Samuel B. Thompson, 2005, Predicting the equity premium out of sample:
   Can anything beat the historical average?, Working paper, Harvard University.
Carter, Chris K., and Robert Kohn, 1994, On Gibbs sampling for state space models, Biometrika
   81, 541‚Äì553.
Casella, G., and E.I. George, 1992, Explaining the Gibbs sampler, The American Statistician 46,
   167‚Äì174.
Cavanagh, Christopher L., Graham Elliott, and James H. Stock, 1995, Inference in models with
   nearly integrated regressors, Econometric Theory 11, 1131‚Äì1147.
Clark, Todd E., and Kenneth D. West, 2004, Using out-of-sample mean squared prediction errors
   to test the martingale difference hypothesis, Journal of Econometrics, forthcoming.



                                                54
Clark, Todd E., and Kenneth D. West, 2005, Approximately normal tests for equal predictive
   accuracy in nested models, Journal of Econometrics, forthcoming.
Cochrane, John H., 2006, The dog that did not bark: A defense of return predictability, Working
   paper, University of Chicago.
Conrad, Jennifer, and Gautam Kaul, 1988, Time-variation in expected returns, Journal of Business
   61, 409-425.
Cremers, K.J. Martijn, 2002, Stock return predictability: A Bayesian model selection perspective,
   Review of Financial Studies 15, 1223‚Äì1249.
Dangl, Thomas, and Michael Halling, 2006, Equity return prediction: Are coefficients time-
   varying?, Working paper, University of Vienna.
Duffee, Gregory R., 2006, Are variations in term premia related to the macroeconomy?, Working
   paper, University of California, Berkeley.
Elliott, Graham, and James H. Stock, 1994, Inference in time series regression when the order of
    integration of a regressor is unknown Econometric Theory 10, 672‚Äì700.
Fama, Eugene F., and Kenneth R. French, 1988, Dividend yields and expected stock returns, Jour-
   nal of Financial Economics 22, 3‚Äì26.
Fama, Eugene F., and G. William Schwert, 1977, Asset returns and inflation, Journal of Financial
   Economics 5, 115‚Äì146.
Ferson, Wayne E., and Campbell R. Harvey, 1991, ‚ÄúThe Variation of Economic Risk Premiums,‚Äù
   Journal of Political Economy 99, 385‚Äì415.
Ferson, Wayne E., and Campbell R. Harvey, 1993, ‚ÄúThe Risk and Predictability of International
   Equity Returns,‚Äù Review of Financial Studies 6, 527‚Äì566.
Ferson, Wayne E., Sergei Sarkissian, and Timothy T. Simin, 2003, Spurious regressions in financial
   economics?, Journal of Finance 58, 1393‚Äì1413.
FruÃàhwirth-Schnatter, Sylvia, 1994, Data augmentation and dynamic linear models, Journal of Time
    Series Analysis 15, 183‚Äì202.
Goyal, Amit and Ivo Welch, 2003, Predicting the equity premium with dividend ratios, Manage-
   mente Science 49, 639‚Äì654.
Goyal, Amit and Ivo Welch, 2005, A Comprehensive look at the empirical performance of equity
   premium prediction, Review of Financial Studies, forthcoming.
Hjalmarsson, Erik, 2006, Should we expect significant out-of-sample results when predicting stock
   returns?, Working paper, Board of Governors of the Federal Reserve System.
Harvey, Andrew C., 1989, Forecasting, structural time series models and the Kalman filter (Cam-
   bridge University Press, Cambridge, UK).
Jansson, Michael, and Marcelo J. Moreira, 2006, Optimal inference in regressions with nearly
   integrated regressors, Econometrica 74, 681‚Äì714.



                                               55
Johannes, Michael, Nicholas Polson, and Jon Stroud, 2002, Sequential optimal portfolio perfor-
   mance: Market and volatility timing, Working paper, Columbia University.
Jones, Christopher S., and Jay Shanken, 2005, Mutual fund performance with learning across
   funds, Journal of Financial Economics 78, 507‚Äì552.
Kandel, Shmuel, and Robert F. Stambaugh, 1996, On the predictability of stock returns: An asset
   allocation perspective, Journal of Finance 51, 385‚Äì424.
Keim, Donald B., and Robert F. Stambaugh, 1986, Predicting returns in the stock and bond mar-
   kets, Journal of Financial Economics 17, 357‚Äì390.
Kothari, S.P., Jonathan Lewellen, and Jerold B. Warner, 2005, Stock returns, aggregate earnings
   surprises, and behavioral finance, Journal of Financial Economics, forthcoming.
Lamont, Owen, 1998, Earnings and expected returns, Journal of Finance 53, 1563‚Äì1587.
Lettau, Martin, and Sydney C. Ludvigson, 2001, Consumption, aggregate wealth, and expected
    stock returns, Journal of Finance 56, 815‚Äì849.
Lettau, Martin, and Sydney C. Ludvigson, 2005, Expected returns and dividend growth, Journal
    of Financial Economics 76, 583‚Äì626.
Leroy, Stephen F. and Richard D. Porter, 1981, The present-value relation: Tests based on implied
   variance bounds, Econometrica 49, 555‚Äì574
Lewellen, Jonathan, 1999, The time-series relations among expected return, risk, and book-to-
   market, Journal of Financial Economics 54, 5‚Äì43.
Lewellen, Jonathan, 2004, Predicting returns with financial ratios, Journal of Financial Economics
   74, 209‚Äì235.
Mankiw, N. Gregory, and Matthew D. Shapiro, 1986, Do we reject too often?: Small sample
  properties of tests of rational expectations models, Economics Letters 20, 139‚Äì145.
Menzly, Lior, Tano Santos, and Pietro Veronesi, 2004, Understanding predictability, Journal of
  Political Economy 112, 1‚Äì47.
Nelson, Charles R., Myung J. Kim, 1993, Predictable stock returns: the role of small sample bias,
   Journal of Finance 48, 641‚Äì661.
PaÃÅstor, LÃåubosÃå, 2000, Portfolio selection and asset pricing models, Journal of Finance 55, 179‚Äì223.
PaÃÅstor, LÃåubosÃå, and Robert F. Stambaugh, 1999, Costs of equity capital and model mispricing,
    Journal of Finance 54, 67‚Äì121.
PaÃÅstor, LÃåubosÃå, and Robert F. Stambaugh, 2000, Comparing asset pricing models: an investment
    perspective, Journal of Financial Economics 56, 335‚Äì381.
PaÃÅstor, LÃåubosÃå, and Robert F. Stambaugh, 2001, The equity premium and structural breaks, Journal
    of Finance 56, 1207‚Äì1239.
PaÃÅstor, LÃåubosÃå, and Robert F. Stambaugh, 2002a, Mutual fund performance and seemingly unre-
    lated assets, Journal of Financial Economics 63, 315‚Äì349.



                                                 56
PaÃÅstor, LÃåubosÃå, and Robert F. Stambaugh, 2002b, Investing in equity mutual funds, Journal of
    Financial Economics 63, 351‚Äì380.
Rozeff, Michael S., 1984. Dividend yields are equity risk premiums, Journal of Portfolio Manage-
   ment 68‚Äì75.
Rytchkov, Oleg, 2006, Filtering out expected dividends and expected returns, Working paper, MIT.
Santos, Tano, and Pietro Veronesi, 2006, Labor income and predictable stock returns, Review of
   Financial Studies, forthcoming.
Shiller, Robert J., 1981, Do stock prices move too much to be justified by subsequent changes in
    dividends?, American Economic Review 71, 421‚Äì436.
Stambaugh, Robert F., 1986, Bias in regressions with lagged stochastic regressors, Working paper,
   University of Chicago.
Stambaugh, Robert F., 1997, Analyzing investments whose histories differ in length, Journal of
   Financial Economics, 45, 285‚Äì331.
Stambaugh, Robert F., 1999, Predictive regressions, Journal of Financial Economics 54, 375‚Äì421.
Wachter, Jessica, and Missaka Warusawitharana, 2006, Predictable returns and asset allocation:
  Should a skeptical investor time the market?, Working paper, University of Pennsylvania.
West, Mike, and Jeff Harrison, 1997, Bayesian Forecasting and Dynamic Models (Springer-Verlag,
  New York, NY).
Zellner, Arnold, 1971, An Introduction to Bayesian Inference in Econometrics (John Wiley and
    Sons, New York, NY).




                                               57
