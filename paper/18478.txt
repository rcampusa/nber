                                NBER WORKING PAPER SERIES




                    ROBUST STANDARD ERRORS IN SMALL SAMPLES:
                             SOME PRACTICAL ADVICE

                                          Guido W. Imbens
                                           Michal Kolesar

                                        Working Paper 18478
                                http://www.nber.org/papers/w18478


                      NATIONAL BUREAU OF ECONOMIC RESEARCH
                               1050 Massachusetts Avenue
                                 Cambridge, MA 02138
                                     October 2012




Financial support for this research was generously provided through NSF grant 0820361. The views
expressed herein are those of the authors and do not necessarily reflect the views of the National Bureau
of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2012 by Guido W. Imbens and Michal Kolesar. All rights reserved. Short sections of text, not to
exceed two paragraphs, may be quoted without explicit permission provided that full credit, including
© notice, is given to the source.
Robust Standard Errors in Small Samples: Some Practical Advice
Guido W. Imbens and Michal Kolesar
NBER Working Paper No. 18478
October 2012
JEL No. C01

                                             ABSTRACT

In this paper we discuss the properties of confidence intervals for regression parameters based on robust
standard errors. We discuss the motivation for a modification suggested by Bell and McCaffrey (2002)
to improve the finite sample properties of the confidence intervals based on the conventional robust
standard errors. We show that the Bell-McCaffrey modification is the natural extension of a principled
approach to the Behrens-Fisher problem, and suggest a further improvement for the case with clustering.
We show that these standard errors can lead to substantial improvements in coverage rates even for
sample sizes of fifty and more. We recommend researchers calculate the Bell-McCaffrey degrees-of-freedom
adjustment to assess potential problems with conventional robust standard errors and use the modification
as a matter of routine.


Guido W. Imbens
Department of Economics
Littauer Center
Harvard University
1805 Cambridge Street
Cambridge, MA 02138
and NBER
Imbens@stanford.edu

Michal Kolesar
Department of Economics
Harvard University
kolesarm@nber.org
1        Introduction
It is currently common practice in empirical work to use standard errors and associated
confidence intervals that are robust to the presence of heteroskedasticity. The most widely
used form of the robust, heteroskedasticity-consistent standard errors is that associated
with the work of White (1980) (see also Eicker, 1967; Huber, 1967), extended to the case
with clustering by Liang and Zeger (1986). The justification for these standard errors
and the associated confidence intervals is asymptotic: they rely on large samples for their
validity. In small samples the properties of these procedures are not always attractive:
the robust (Eicker-Huber-White, or EHW, and Liang-Zeger or LZ, from hereon) variance
estimators are biased downward, and the normal-distribution-based confidence intervals
using these variance estimators can have coverage substantially below nominal coverage
rates.
    There is a large literature documenting and addressing these small sample problems
in the context of linear regression models, some of it reviewed in MacKinnon and White
(1985), Angrist and Pischke (2009), and MacKinnon (2012). A number of alternative
versions of the robust variance estimators and confidence intervals have been proposed
to deal with these problems. Some of these alternatives focus on reducing the bias of the
variance estimators (MacKinnon and White, 1985), some exploit on higher order expan-
sions (Hausman and Palmer, 2011), others attempt to improve their properties by using
resampling methods (MacKinnon and White, 1995; Cameron, Gelbach, and Miller, 2008;
Hausman and Palmer, 2011), and some use t-distribution approximations (Bell and Mc-
Caffrey, 2002; Donald and Lang, 2007). Few of these alternatives are regularly employed
in empirical work. In fact, some researchers argue that for commonly encountered sample
sizes (e.g., fifty or more units) the improvements are not necessary because the EHW
and LZ standard errors perform well. Perhaps it is also the multitude and the ad hoc
nature of many of the proposed alternatives to the EHW and LZ procedures, combined
with the lack of clear guidance among them, that makes empirical researchers wary of
using any of them.
    Here we review some of this work and provide specific recommendations for empirical
practice. The main recommendation of this paper is that empirical researchers should,
as a matter of routine, adopt a particular improvement to the EHW and LZ confidence


                                            [1]
intervals, due to Bell and McCaffrey (2002), BM from hereon. The BM improvement is
simple to implement and in small and moderate-sized samples can provide a considerable
improvement over the EHW and LZ confidence intervals. Here is a brief description of
the BM improvement. Let V̂ehw be the standard EHW variance estimator, and let the
                                                          p
EHW 95% confidence interval for a parameter β be β̂ ± 1.96 V̂ehw . The BM modifi-
cation consists of two components, the first removing some of the bias and the second
changing the approximating distribution from a normal distribution to the best fitting
t-distribution. First, the commonly used variance estimator V̂ehw is replaced by V̂HC2 (a
modification for the general case first proposed by MacKinnon and White, 1985), which
removes some, and in special cases all, of the bias in V̂ehw relative to the true variance V.
                                     p
Second, the distribution of (β̂ − β)/ V̂HC2 is approximated by a t-distribution. When
t-distribution approximations are used in constructing robust confidence intervals, the
degrees of freedom are typically fixed at the number of observations minus the number of
estimated regression parameters. The BM adjustment uses a more sophisticated choice
for the degrees of freedom (dof). The dof of the approximating t-distribution, denoted
by KBM , is choosen so that under homoskedasticity the distribution of KBM · V̂HC2 /V has
the first two moments in common with a chi-squared distribution with dof equal to KBM .
The BM degrees of freedom is a simple analytic function of the matrix of regressors.
   To ease comparisons with other methods we convert this procedure into one that
only adjusts the standard errors. What we then refer to as the BM standard error is
     p         p
then V̂BM = V̂HC2 · (tK                         K
                         0.975/t0.975 ), where tq is the q-th quantile of the t-distribution
                           BM   ∞


with dof equal to K (so that t∞
                              q is the q-th quantile of the normal distribution and thus

t∞
 0.975 = 1.96). A key insight is that the BM dof can differ substantially from the sample

size (minus the number of estimated parameters) if the distribution of the covariates is
skewed.
   We make three specific points in the current paper. One modest novel contribution
is to show that the BM proposal is the principled extension from an approach developed
by Welch (1951) to a simple, much-studied and well-understood problem, known as the
Behrens-Fisher problem (see for a general discussion, Scheffé, 1970). Understanding
how the BM proposals and other procedures perform in the simple Behrens-Fisher case
provides insights into their general performance. Second, and this has been pointed out
in the theoretical literature before (e.g., Chesher and Jewitt, 1987), without having been

                                             [2]
appreciated in the empirical literature, problems with the standard robust EHW and LZ
variances and confidence intervals can be substantial even with moderately large samples
if the distribution of the regressors is skewed. It is the combination of the sample size and
the distribution of the regressors that determines the accuracy of the standard robust
confidence intervals and the potential benefits from small sample adjustments. Third,
we suggest a modification of the BM procedure in the case with clustering that further
improves the performance of confidence intervals in that case.
    This paper is organized as follows. In the next section we study the Behrens-Fisher
problem and the solutions offered by the robust standard error literature specialized to
this case. In Section 3 we generalize the results to the general linear regression case, and
in Section 4 we study the case with clustering. In each of these three sections we provide
some simulation evidence regarding the performance of the various confidence intervals,
using designs previously proposed in the literature. We find that in all these settings the
BM proposals perform well relative to the other procedures. Section 5 concludes.


2     The Behrens-Fisher Problem: the Performance of
      Various Proposed Solutions
In this section we review the Behrens-Fisher problem, which can be viewed as a special
case of linear regression with a single binary regressor. For this special case there is a
large literature and several attractive methods for constructing confidence intervals with
good properties even in very small samples have been proposed. See Behrens (1929),
Fischer (1939), and for a general discussion Scheffé (1970), Wang (1971), Lehman and
Romano (2005), and references therein. We discuss the form of the standard variance
estimators for this case, and discuss when they perform poorly relative to the methods
that are designed especially for this setting.

2.1    The Behrens-Fisher Problem
Consider the following linear model with a single binary regressor, allowing for het-
eroskedasticity:

      Yi = β0 + β1 · Xi + εi ,                                                          (2.1)


                                             [3]
with Xi ∈ {0, 1}, and

      E[εi |Xi = x] = 0,           and Var(εi |Xi = x) = σ 2 (x).

We are interested in β1 = Cov(Yi , Xi )/Var(Xi ) = E[Yi |Xi = 1] − E[Yi |Xi = 0]. Because
the regressor Xi is binary, the least squares estimator for the slope coefficient β1 can be
written as

      β̂1 = Y 1 − Y 0 ,

where, for x = 0, 1,
                                                 N                   N
             1 X                                 X                   X
      Yx=              Yi ,         and N1 =           Xi ,   N0 =         (1 − Xi ).
             Nx i:X =x                           i=1                 i=1
                    i


Over repeated samples, conditional on X = (X1 , . . . , XN )0 , the exact finite sample vari-
ance for the least squares estimator β̂1 is
                              σ 2(0) σ 2 (1)
      V = Var(β̂1|X) =              +        .
                               N0     N1
If in addition we assume normality for εi given Xi , εi |Xi = x ∼ N (0, σ 2 (x)), the exact
distribution for β̂1 conditional on X is
                       σ 2(0) σ 2(1)
                                    
      β̂1 |X ∼ N β1 ,        +         .
                        N0       N1

The problem of how to do inference for β1 in the absence of knowledge of σ 2(x) is old,
and known as the Behrens-Fisher problem.
   Let us first review a number of the standard least squares variance estimators, spe-
cialized to the case with a single binary regressor.

2.2    The Homoskedastic Variance Estimator
Suppose the errors are homoskedastic: σ 2 = σ 2(0) = σ 2(1), so that the exact variance
for β̂1 is V = σ 2/(1/N0 + 1/N1 ). We can estimate the common error variance σ 2 as
                        N
       2     1 X                       2
      σ̂ =           Yi − β̂0 − β̂1 · Xi .
           N − 2 i=1



                                                   [4]
This variance estimator is unbiased for σ 2, and as a result the estimator for the variance
for β̂1,
                     σ̂ 2   σ̂ 2
       V̂homo =           +      ,
                     N0 N1
is unbiased for the true variance V. Moreover, under normality for εi given Xi , the
t-statistic has an exact t-distribution:
                           β̂1 − β1
       thomo = p                         X ∼ t(N − 2).                                                           (2.2)
                       σ̂ 2/N0 + σ̂ 2/N1
This t-distribution with dof equal to N − 2 can be used for the construction of exact
confidence intervals. The exact 95% confidence interval for β̂1 , under homoskedasticity,
is
                                                     p                                 p            
       95% CIhomo =              β̂1 +   tN −2
                                          0.025   ×       V̂homo, β̂1 +   tN −2
                                                                           0.975   ×        V̂homo ,

where tN
       q is the q-th quantile of a t-distribution with dof equal to N. This confidence

interval is exact under these two assumptions, normality and homoskedasticity.

2.3        The Robust EHW Variance Estimator
The familiar form of the robust EHW variance estimator, given the linear model in (2.1),
is
           N
                          !−1     N 
                                                                     !    N
                                                                                         !−1
           X                      X                    2                 X
                 Xi Xi0                   Yi − Xi β̂        Xi Xi0              Xi Xi0           .
           i=1                    i=1                                     i=1

In the Behrens-Fisher case with a single binary regressor the component of this matrix
corresponding to β1 simplifies to
                                                                       N
                σ̃ 2 (0) σ̃ 2(1)                               2 1 X               2
      V̂ehw   =         +        ,                where σ̃ (x) =           Yi − Y x ,                    for x = 0, 1.
                  N0       N1                                    Nx i:X =x
                                                                                   i


The standard, normal-distribution-based, 95% confidence interval based on the robust
variance estimator:
                                                 p                             p            
       95% CIehw =              β̂1 − 1.96 ×          V̂ehw , β̂1 + 1.96 ×             V̂ehw .                   (2.3)

Even if the error term εi has a normal distribution, the justification for this confidence
interval is asymptotic. Sometimes researchers use a t-distribution with N − 2 degrees of

                                                                   [5]
freedome to calculate the confidence limits, replacing 1.96 in (2.3) by the corresponding
quantile of the t-distribution with dof equal to N − 2, tN −2
                                                         0.975. However, there are no

assumptions under which this modification has exact 95% coverage.

2.4    An Unbiased Estimator for the Variance
An alternative to V̂ehw is what MacKinnon and White (1985) call the HC2 variance
estimator, here denoted by V̂HC2 . In general this correction removes only part of the
bias, but in the single binary regressor (Behrens-Fisher) case the MacKinnon-White HC2
correction removes the entire bias. Its form in this case is
                                                             N
                σ̂ 2(0) σ̂ 2(1)              2       1      X            2
      V̂HC2   =        +        ,   where σ̂ (x) =               Yi − Y x ,            (2.4)
                  N0      N1                       Nx − 1 i:X =x
                                                                i


for x = 0, 1. These conditional variance estimators σ̂ 2 (x) differ from σ̃ 2(x) by a factor
Nx /(Nx − 1). In combination with the normal approximation to the distribution of the
t-statistic, this variance estimator leads to the 95% confidence interval
                                  p                    p      
      95% CIHC2 = β̂1 − 1.96 × V̂HC2 , β̂1 + 1.96 × V̂HC2 .

The estimator V̂HC2 is unbiased for V, but the resulting confidence interval is still not
exact. Just as in the homoskedastic case, the sampling distribution of the t-statistic
           p
(β̂1 − β1)/ V̂HC2 is in this case not normally distributed in small samples, even if the
                                                                √
underlying errors are normally distributed (and thus (β̂1 − β1)/ V has an exact standard
normal distribution) . However, whereas in the homoskedastic case the exact distribution
is a t-distribution with degrees of freedom equal to N −2, the exact sampling distribution
               p
of (β̂1 − β1 )/ V̂HC2 does not lend itself to the construction of exact confidence intervals.
   In this single-binary-covariate case it is easy to see that in some cases N − 2 will be
a poor choice for the degrees of freedom for the approximating t-distribution. Suppose
that there are many units with Xi = 0 and few units with Xi = 1 (N0 >> N1). In
that case E[Yi |Xi = 0] is estimated relatively precisely, with variance σ 2(0)/N0 ≈ 0. As
                                                        p
a result the distribution of the t-statistic (β̂1 − β1)/ V̂HC2 is approximately equal to
                               p
that of (Y 1 − E[Yi |Xi = 1])/ σ̂ 2(1)/N1 . The latter has, under normality, an exact t-
distribution with dof equal to N1 − 1, substantially different from the t-distribution with
N − 2 = N0 + N1 − 2 dof if N0 >> N1 .

                                             [6]
2.5    Degrees of Freedom Adjustment: The Welch and Bell-McCaffrey
       Solutions
One of the most attractive proposals for the Behrens-Fisher problem is due to Welch
                                                                                    p
(1951). Welch suggests approximating the distribution of the t-statistic (β̂1 −β1 )/ V̂HC2
by a t-distribution. Rather than using the sample size minus two as the degrees of
freedom for this t-distribution, he suggests using moments of the variance estimator
V̂HC2 to determine the most appropriate value for the degrees of freedom.
   Let us describe the Welch suggestion in more detail. Consider the t-statistic in the
heteroskedastic case:

             β̂1 − β1           β̂1 − β1
      tHC2 = p        =p                         .
                V̂HC2    σ̂ 2(0)/N0 + σ̂ 2(1)/N1

Note that E[V̂HC2 ] = V, and that under normality V̂HC2 is independent of β̂1 − β1. Now
suppose there was a constant K such that the distribution of K · V̂HC2 /V had a chi-
squared distribution with dof equal to K. Then tHC2 would have a t-distribution with
dof equal to K, which could be exploited to construct an exact confidence interval. The
problem is that there exists no such K such that the scaled distribution of the variance
estimator has an exact chi-squared distribution. Welch suggests approximating the scaled
distribution of V̂HC2 by a chi-squared distribution, with the dof choosen to make the
approximation as accurate as possible. Under normality, V̂HC2 is a linear combination of
two chi-squared random variables. To be precise, (N0 − 1)σ̂ 2(0)/σ 2 (0) ∼ X 2 (N0 − 1), and
(N1 − 1)σ̂ 2 (1)/σ 2 (1) ∼ X 2 (N1 − 1), and σ̂ 2 (0) and σ̂ 2 (1) are independent of each other
and of β̂1 − β1. Hence it follows that
                        2σ 4 (0)    2σ 4 (1)
      Var V̂HC2 =                  +            .
                        (N0 − 1)N02 (N1 − 1)N12

Welch’ specific suggestion is to choose the dof parameter K such that K · V̂HC2 /V has
the first two moments in common with a chi-squared distribution with dof equal to K.
Because irrespective of the value for K, E[K · V̂HC2 /V] = K, this amounts to choosing
K such that
                                                                                                     2
                                                                                σ2 (0)       σ2 (1)
                                                          2 · V2               N0
                                                                                         +    N1
                                              ∗
   Var K · V̂HC2 /V = 2K,         leading to Kwelch =             =       σ4 (0)             σ4 (1)
                                                                                                           .
                                                         Var V̂HC2        (N0 −1)N02
                                                                                         +   (N1 −1)N12


                                              [7]
This choice for K is not feasible because Kwelch
                                           ∗
                                                 depends on unknown quantities, namely,
the conditional variances σ 2(x). In the feasible version we approximate the distribution
of tHC2 by a t-distribution with dof equal to
                                       2 
                     σ̂ 2(0) σ̂ 2(1)              σ̂ 4(0)     σ̂ 4(1)
                                                                        
      Kwelch =              +                             +                  ,        (2.5)
                       N0      N1              (N0 − 1)N02 (N1 − 1)N12

where the unknown σ 2 (x) are replaced by the estimates σ̂ 2 (x). Wang (1971) presents
some exact results for the difference between the coverage of confidence intervals based
on the Welch procedures and the nominal levels, showing that the Welch intervals perform
extremely well in very small samples.
   BM (2002) propose a slightly different degrees of freedom adjustment.          For the
Behrens-Fisher problem (regression with a single binary covariate) the BM modification
is minor, but it has considerable attraction in settings with more general distributions of
covariates. The BM adjustment is based on assuming homoskedasticity. In that case the
Welch dof simplifies to
                   2        2
                    σ     σ2
                    N0
                        + N1           (N0 + N1 )2 (N0 − 1)(N1 − 1)
     Kbm =                           =                              .                 (2.6)
                 σ4
             (N0 −1)N 2
                              σ4
                        + (N1 −1)N 2    N12 (N1 − 1) + N02 (N0 − 1)
                         0             1


Because the BM dof does not depend on the conditional variances, it varies less across
repeated samples and as a result tends to be more accurate then the Welch adjustment
which can be conservative in settings with noisy estimates of the conditional error vari-
ances. The associated 95% confidence interval is now
                               p                     p     
                         KBM                   KBM
      95% CIBM = β̂1 − t0.975 × V̂HC2 , β̂1 + t0.975 × V̂HC2 .                        (2.7)

This is the interval we recommend researchers use in practice.
   To gain some intuition for the BM dof adjustment, consider some special cases. First,
if N0 >> N1 , then Kbm ≈ N1 − 1. As we have seen before, as N0 → ∞, using N1 − 1
as the degrees of freedom leads to exact confidence intervals under normally distributed
errors. If the two subsamples are equal size, N0 = N1 = N/2, then Kbm = N − 2. Thus,
if the two subsamples are approximately equal size, the often-used dof adjustment of
N − 2 is appropriate, but if the distribution is very skewed, this adjustment is likely to
be inadequate.


                                                   [8]
2.6    A Small Simulation Study based on a Angrist-Pischke de-
       sign
Now let us see how relevant the small sample adjustments are in practice. We conduct a
small simulation study based on a design previously used by Angrist and Pischke (2009).
The sample size is N = 30, with N1 = 3 and N0 = 27. The parameter values are
β0 = β1 = 0 (note that the results are invariant to the values for β0 and β1). The
distribution of the disturbances is normal,

      εi |Xi = x ∼ N (0, σ 2(x)),    for x = 0, 1,

with σ 2 (1) = 1. Angrist and Pischke report results for three choices for σ(0): σ(0) ∈
{0.5, 0.85, 1}. We add the complementary values σ(0) ∈ {1.18, 2}, where 1.18 ≈ 1/0.85.
Angrist and Pischke report results for a number of variance estimators, including some
where they take the maximum of V̂homo and V̂ehw or V̂HC2 , but they do not the Welch or
BM dof adjustments. For the five designs the Welch dof correction is quite substantial.
Consider the first design with σ(0)/σ(1) = 0.5. Then the value for the infeasible Welch
dof is Kwelch
        ∗
              = 2.1. Given that t20.975 = 4.30, compared to the normal 0.975 quantile
t∞
 0.975 = 1.96, this leads to an adjustment in the standard errors by a factor of 2.2. For

the other four designs the infeasible Welch dof values are equal to 2.3, 2.5, 2.7, and 4.1
respectively, in each case leading to substantial changes in the confidence intervals even
though the overall sample size is substantial.
   When we implement the Welch and BM degrees-of-freedom adjustments the adjusted-
degrees-of-freedom are not necessarily integer. In that case we use the distribution for
the t-distribution defined as the ratio of two random variables, one a random variable
with a standard (mean zero, unit variance) normal distribution and and the second a
random variable with a gamma distribution with parameters α = KBM /2 and β = 2. We
include the following confidence intervals. First, two intervals based on the homoskedastic
variance estimator V̂homo , using either the normal distribution or a t-distribution with
N − 2 dof. Next, two confidence intervals based on V̂ehw , again either using the normal
or the t-distribution with N − 2 dof. Next, six confidence intervals based on V̂HC2 . First
among these is the one with the normal distribution, next the t-distribution with degrees
of freedom equal to N − 2, Kwelch, Kwelch
                                    ∗
                                          , and KBM . Finally, a resampling method,
specifically the wild bootstrap, discussed in more detail in Appendix A. Next we include

                                              [9]
two confidence intervals based on V̂HC3 (see Appendix A for more details), either using
the normal distribution or the wild bootstrap. Finally we include normal distribution
based confidence intervals based on the maximum of V̂homo and V̂ehw , and one based on
the maximum of V̂homo and V̂HC2 . For each of the analytic variance estimators we use
1,000,000 replications. For those based on the wild bootstrap we use 100,000 replications
and 10,000 draws from the wild bootstrap distribution.
   Table 1 presents the simulation results for the Angrist-Pischke design. For each of the
variance estimators we report coverage properties for nominal 95% confidence intervals.
We also report the median of the standard errors over the simulations. In cases where the
confidence intervals are based on t-distributions with K degrees of freedom, we multiply
the standard error by tK
                       0.975/t0.975, to make the standard errors comparable. For the
                              ∞


variance estimators included in the Angrist-Pischke design our simulation results are
consistent with theirs. However, the three confidence intervals based on the (feasible
and infeasible) Welch and BM degrees of freedom adjustments are superior in terms
of coverage to all others. Consider the case with σ(0) = 0.5. The coverage rate for the
normal-distribution confidence interval based on V̂ehw is 0.77. Using the unbiased variance
estimator V̂HC2 raise that to 0.82, but only using the t-distribution approximation with
Welch or BM degrees of freedom gets that close to the nominal level. An interesting
aspect of the Welch dof calculation is that it leads to confidence intervals that are typically
substantially wider, and have substantially more variation in their width. For the two
confidence intervals based on Kwelch and KBM , the median widths of the confidence
intervals are 6.45 and 3.71, but the 0.95 quantile of the widths are 14.89 and 7.59.
The attempt to base the approximating chi-square distribution on the heteroskedasticity
consistent variance estimates leads to a considerable increase in the variability of the
width of the confidence intervals. One of the attractions of the BM intervals is that it
avoids this variation.
   For comparison purposes we report in Table 2 the results for a simulation exercise
with a balanced design where N0 = N1 = N/2 = 15. Here the actual coverage rates
are close to nominal coverage rates for essentially all procedures: for a sample size of
30 with a balanced design asymptotic, normal-distribution-based, approximations are
fairly accurate and refinements are unnecessary. Note that KBM = 28, and t28
                                                                          0.975 = 2.05,

close to the 1.96 for the normal distribution, so the BM dof correction is unlikely to be

                                             [10]
important here.


3     Linear Regression With General Regressors
Now let us look at the general regression case, allowing for multiple regressors, and
regressors with other than binomial distributions.

3.1    The Set Up
We have a L-dimensional vector of regressors Xi , and a linear model

      Yi = Xi0 β + εi ,                with E [εi |Xi ] = 0,                 and Var (εi |Xi ) = σ 2(Xi ).

Let X be the N × L dimensional matrix with ith row equal to Xi0 , and let Y and ε be the
N-vectors with ith elements equal to Yi and εi respectively. The ordinary least squares
estimator is:
                                              N
                                                             !−1     N
                                                                                    !
                        −1
                                              X                      X
                0               0
      β̂ = (X X)             (X Y) =                Xi Xi0                  Xi Yi       .
                                              i=1                     i=1


Without assuming homoskedasticity, the exact variance for β̂ conditional on X is

                                        N
                                                       !−1     N
                                                                                              !   N
                                                                                                                 !−1
                                      X                      X                                  X
      V = Var β̂ X =                          Xi Xi0                  σ 2(Xi ) · Xi Xi0                 Xi Xi0         ,
                                        i=1                    i=1                                i=1

with k-th diagonal element Vk . For the general regression case the EHW robust variance
estimator is
                    N
                                      !−1     N 
                                                                                 !      N
                                                                                                       !−1
                    X                         X                    2                   X
      V̂ehw =                Xi Xi0                  Yi − Xi β̂         Xi Xi0                Xi Xi0         ,
                    i=1                       i=1                                       i=1


with k-th diagonal element V̂ehw,k . Using a normal distribution the associated 95%
confidence interval for βk is
                             q                      q       
      95% CIehw = β̂k − 1.96 × V̂ehw,k , β̂k + 1.96 × V̂ehw,k .

This robust variance estimator and the associated confidence intervals are widely used
in empirical work.


                                                               [11]
3.2    The Bias-adjusted Variance Estimator
In Section 2 we discussed the bias of the robust variance estimator in the case with a
single binary covariate. In that case there was a simple modification of the EHW variance
estimator that removes all bias. In the general regression case the bias-adjustment is more
complicated. Here we focus on a particular adjustment for the bias due to MacKinnon
and White (1985). In the special case with only a single binary covariate this adjustment
is identical to that used in Section 2. It does not, however, remove all bias in general.
   Let P = X(X0 X)−1 X0 be the N × N projection matrix, with i-th column denoted by
Pi = X(X0 X)−1 Xi and (i, i)-th element denoted by Pii = Xi0 (X0 X)−1 Xi . Let Ω be the
N × N diagonal matrix with i-th diagonal element equal to σ 2(Xi ), and let eN,i be the
N-vector with i-th element equal to one and all other elements equal to zero. Let IN be
the N × N identity matrix. The residuals ε̂i = Yi − Xi0 β̂ can be written as

      ε̂i = εi − e0N,iPε = e0N,i(IN − P)ε,      and in vector form ε̂ = (IN − P)ε.

The expected value of the square of the i-th residual is

      E ε̂2i = E (e0N,i(IN − P)ε)2 = (eN,i − Pi )0 Ω(eN,i − Pi ),
                               


which, under homoskedasticity reduces to σ 2 · (1 − Pii ). This in turn implies that ε̂2i /(1 −
Pii ) is unbiased for E [ε2i ] under homoskedasticity. This is the motivation for the variance
estimator MacKinnon and White (1985) introduce as HC2:
                         !−1
                                           2        
                                                                    !−1
              XN              X Yi − Xi β̂
                                N                        XN
     V̂HC2 =      Xi Xi0                        Xi Xi0      Xi Xi0     .                (3.1)
                                                      
                                     1 − P
                             
                                          ii
              i=1              i=1                       i=1


Suppose we want to construct a confidence interval for βk , the k-th element of β. The
variance of β̂k is estimated as V̂HC2,k = e0L,k V̂HC2 eL,k , where eL,k is an L-vector with kth
element equal to one and all other elements equal to zero. The 95% confidence interval,
based on the normal approximation, is then given by
                              q                    q       
     95% CIHC2 = β̂k − 1.96 × V̂HC2,k , β̂k + 1.96 × V̂HC2,k .




                                             [12]
3.3    The Degrees of Freedom Adjustment
BM, building on Satterthwaite (1946), suggest approximating the distribution of
      β̂k − βk
      q         ,
        V̂HC2,k
by a t-distribution instead of a normal distribution. The degrees of freedom K are chosen
so that under homoskedasticity (Ω = σ 2IN ) the first two moments of K · (V̂HC2,k /Vk ) are
equal to those of a chi-squared distribution with degrees of freedom equal to K. Note that
under homoskedasticity, E[V̂HC2 ] = V and thus E[V̂HC2,k ] = Vk , so that the first moment
of K · (V̂HC2,k /Vk ) is always equal to to that of a chi-squared distribution with dof equal
to K, and we choose K to match the second moment. Moreover, under normality V̂HC2,k
is a linear combination of N independent chi-squared one random variables (with some
of the coefficients equal to zero). Let λi be the weight for the i-th chi-squared random
variable, so we can write
                   N
                   X
      V̂HC2,k =          λi · Zi ,            where Zi ∼ X 2 (1), all Zi independent.
                   i=1

Given these weights, the BM dof that match the first two moments of K · (V̂HC2,k /Vk )
to that of a chi-squared K distribution is given by
                                  N
                                        !2 , N
                   2 · V2k       X           X
      KBM =               =        λi           λ2i .                                        (3.2)
              Var V̂HC2,k        i=1          i=1

To characterize the weights, define, the N × N matrix G, with i-th column equal to
                 1
      Gi = √           (eN,i − Pi )Xi0 (X0 X)−1 eL,k .
               1 − Pii
Then the λi are the eigenvalues of the N × N matrix

      σ 2 · G0G.

Note that because of the form of (3.2), the value of KBM does not depend on σ 2 even
though the weights λi do depend on σ 2 . Note also that the dof adjustment may be
different for different elements of parameter β. Formally, the BM 95% confidence interval
is:
                                                 q                               q       
      95% CIBM =             β̂k +   tK BM
                                      0.025   ×       V̂HC2,k , β̂k +   tK BM
                                                                         0.975   × V̂HC2,k .

                                                              [13]
   The BM contribution over the earlier Sattherthwaite (1946) work is to base the dof
calculation on the homoskedastic case with Ω = σ 2 · IN . In general, the weights λi
that set the moments of the chi-squared approximation equal to those of the normalized
variance are the eigenvalues of G0 ΩG. These weights are not feasible, because Ω is not
known in general. The feasible version of the Sattherthwaite dof suggestion replaces Ω
by Ω̂ = diag(ε̂2i /(1 − Pii )). This often leads to substantially conservative confidence
intervals.
   There is a somewhat subtle difference between between the binary and the general
regressor case. Applying the BM solution for the general case, given in (3.2), to data
with a single binary regressor, leads to the same value as applying the BM solution
for the binary case, given in (2.6). Similarly, applying the infeasible Sattherthwaite
solution, based on the eigenvalues of GΩG, to binary regressor data, leads to the same
dof as applying the infeasible Welch solution Kwelch
                                               ∗
                                                     . However, applying the feasible
Sattherthwaite solution to the case with a binary regressor does not lead to the feasible
Welch solution. In the case with a single binary regressor, the Welch proposal for the dof
calculation given in (2.5) is numerically identical to
                   N
                               !2 , N
                 X                 X
      Kwelch =        λwelch,i        λ2welch,i ,
                      i=1          i=1

where the weights λwelch,i are the eigenvalues of

      G0 Ω̂welch G,

with Ω̂welch the diagonal matrix
                    2
                    σ̂ (0)      if Xi = 0, i = j
                       2
      Ω̂welch,ij =   σ̂ (1)      if Xi = 1, i = j
                     0           if i 6= j.
                   

The Welch dof solution is not equal to the dof based on the eigenvalues of G0 Ω̂G, with
Ω̂ = diag(ε̂i /(1 − Pii )), even though the estimated variances are the same:
                                                        
         0    −1    0              0  −1      0  −1                −1
      (X X)        X Ω̂welchX (X X) = (X X)           X Ω̂X (X0 X) .
                                                       0




3.4    A Small Simulation Study (Cragg, 1983)
We carry out a small simulation study based on designs by Cragg (1983). The model is

      Yi = β0 + β1 · Xi + εi

                                            [14]
with β0 = 1, β1 = 1, and

                                 and εi |Xi = x ∼ N 0, γ0 + γ1 · x + γ2 · x2 .
                                                                            
      ln(Xi ) ∼ N (0, 1),

Two designs were used: (γ0 , γ1 , γ2 ) = (0.6, 0.3, 0.0), and (γ0 , γ1 , γ2 ) = (0.3, 0.2, 0.1). The
latter case exhibits considerable heteroskedasticity. The median value of σ(x) is 0.77,
the 0.025 quantile is 0.57, and the 0.975 quantile is 2.60. This particularly impacts the
quality of the confidence intervals based on the feasible Sattherthwaite dof adjustment
with Ω̂. For comparison we also include the standard errors and coverage rates based on
the infeasible Sattherthwaite dof adjustment with the eigenvalue calculations based on
G0 ΩG. We report results for two sample sizes, N = 25 and N = 100. For each design
and each of the analytic variance estimators we use 1,000,000 replications, and 100,000
for the wild bootstrap with 10,000 bootstrap replications. The results are in presented
in Table 3. For comparison, see Table III, panel 2, page 760 in Cragg (1983).
    Qualitatively the results are similar to those for the Angrist-Pischke design. The
robust variance estimators V̂ehw and the bias-adjusted version V̂HC2 do not perform well
unless the confidence intervals are based on t-distributions with the KSatterthwaite or KBM
dof adjustments. The KBM dof adjustment leads to much narrower confidence intervals
with much less variation, so again that is the superior choice in this setting.


4     Robust Variance Estimators in the Presence of
      Clustering
In this section we discuss the extensions of the variance estimators discussed in the
previous sections to the case with clustering. The model is:

      Yi = Xi0 β + εi ,                                                                       (4.1)

where i = 1, . . . , N indexes units. There are S clusters. In cluster s the number of units
is Ns , with the overall sample size N = Ss=1 Ns . Let Si ∈ {1, . . . , S} denote the cluster
                                           P

unit i belongs to. We assume that

      E[ε|X] = 0,           and E[εε0|X] = Ω,

where,
              
                  ωij        if Si = Sj ,
      Ωij =
                  0          otherwise.

                                                [15]
   Let β̂ be the least squares estimator, and let ε̂i = Yi − Xi0 β̂ be the residual. Let ε̂s be
the Ns dimensional vector with the residuals in cluster s, let Xs the Ns × L matrix with
ith row equal to the value of Xi0 for the ith unit in cluster s, and let X be the N ×L matrix
constructed by stacking X1 through XS . Define the N × Ns matrix Ps = X(X0 X)−1 X0s ,
the Ns × Ns matrix Pss = Xs (X0 X)−1 X0s , and define the Ns × N matrix (IN − P)s to
consist of the Ns rows of the N × N matrix (IN − P) corresponding to cluster s.
   The standard robust variance estimator, due to Liang and Zeger (1986), see also
Diggle, Heagerty, Liang, and Zeger (2002), is
              S
                        !−1 S                  S
                                                        !−1
             X             X                   X
     V̂lz =      X0s Xs        X0s ε̂s ε̂0s Xs   X0s Xs     .
                s=1            s=1                  s=1

Often a simple multiplicative adjustment is used, for example in STATA, to reduce the
bias of the LZ variance estimator:
                                         S
                                                        !−1   S                       S
                                                                                                     !−1
                  N −1   S               X                    X                       X
      V̂STATA   =      ·   ·                   X0s Xs               X0s ε̂s ε̂0s Xs         X0s Xs         .
                  N −L S−1               s=1                  s=1                     s=1

The main component of this adjustment is typically the S/(S − 1) factor, because in
many applications (N − 1)/(N − L) is close to one.
   The bias-reduction modification developed by Bell and McCaffrey (2002), analogous
to the HC2 bias reduction of the original Eicker-Huber-White variance estimator, is
            S
                      !−1 S                                                      S
                                                                                          !−1
            X            X                                                       X
    V̂lz2 =    X0s Xs        X0s (INs − Pss )−1/2ε̂s ε̂0s ((INs − Pss )−1/2)0 Xs   X0s Xs     ,
              s=1            s=1                                                                     s=1

where (INs − Pss )−1/2 is the inverse of the symmetric square root of (INs − Pss ). For each
of the variance estimators, let V̂lz,k , V̂STATA,k and V̂lz2,k are the k-th diagonal elements of
V̂lz , V̂STATA and V̂lz2 respectively.
   To formalize the degrees-of-freedom adjustment, define the N × S matrix G with s-th
column equal to the N-vector Gs defined as
                                                          −1
      Gs = (IN − P)0s (INs − Pss )−1/2 Xs (X0 X)               eL,k .

Then the dof adjustment is given by
             P         2
                 N
                 i=1 λi
     KBM = PN 2 .
                  i=1 λi


                                                   [16]
where λi are the eigenvalues of G0 G. The 95% confidence interval is now
                                   q                  q       
             cluster         KBM                KBM
     95% CIBM = β̂k + t0.025 × V̂lz2,k , β̂k + t0.975 × V̂lz2,k .                     (4.2)

   We also consider a slightly different version of the dof adjustment. In principle we
would like to use the eigenvalues of the matrix G0 ΩG, where Ω = E[εε|X]. It is difficult
to estimate ω accurately, which motivated BM to use σ 2 · IN instead. In the clustering
case, however, we have some structure on Ω that may be useful. We estimate a model
for Ω where
             2
             σε + σν2           if i = j,
      Ωij =   σ2                 if i 6= j, Si = Sj ,
             ν
              0                  otherwise.
We estimate σν2 as the average of the product of the residuals for units with Si = Sj , and
i 6= j, and then estimate σε2 as the average of the square of the residuals minus σ̂ν2. We
then calculate the λ̃i as the eigenvalues of G0 Ω̂G, and
             P          2
                 N
                 i=1 λ̃i
     KIK = PN 2 .
                  i=1 λ̃i


4.1    A Small Simulation Study
We carry out a small simulation study following designs first used in Cameron, Gelbach,
and Miller (2008). The model is the same as in (4.1), with a scalar covariate:

      Yi = β0 + β1 · Xi + εi ,

with β0 = β1 = 0. We consider five specific designs. In the first design, Xi = VSi + Wi
and εi = νSi + ηi . The Vs , Wi , νs , ηi are all normally distributed, with mean zero and
unit variance. In the first design there are S = 10 clusters, with Ns = 30 units in each
cluster. In the second design we have S = 5 clusters, again with Ns = 30 in each cluster.
In the third design there there are again S = 10 clusters, half with Ns = 10 and half
with Ns = 50. In the fourth and fifth design we return to the design with S = 10 clusters
and Ns = 30 units per cluster. In the fourth design we introduce heteroskedasticity, with
ηi |X ∼ N(0, 0.9Xi2 ), and in the fifth design, the covariate is fixed within the clusters:
Wi = 0 and Vs ∼ N (0, 2).
   For each design and each of the analytic variance estimators we use 1,000,000 repli-
cations, and 100,000 for the wild bootstrap, with 10,000 bootstrap replications.

                                                [17]
5    Conclusion
Although there is a substantial literature documenting the poor properties of the conven-
tional robust standard errors in small samples, in practice researchers continue to use the
EHW and LZ robust standard errors. Here we discuss one of the proposed modifications,
due to Bell and McCaffrey (2002), and argue that it should be used more widely, even
in moderately sized samples. We discuss the connection to the Behrens-Fisher problem,
and suggest a minor modification for the case with clustering.




                                           [18]
References
Angrist, J., and S. Pischke., (2009), Mostly Harmless Econometrics, Princeton University
    Press, Princeton, NJ.

Behrens, W., (1929). “Ein Beitrag zur Fehlerberechnung weniigen Beobachtungen,” Landswirth,
    Jahrbucher, 68, 807-837.

Bell, R., and D. McCaffrey, (2002), “Bias Reduction in Standard Errors for Linear
    Regression with Multi-Stage Samples,”Survey Methodology , Vol. 28(2), pp. 169-181.

Bester, C., T. Conley, and C. Hansen, (2009), “Inference with Dependent Data Us-
    ing Clustering Covariance Matrix Estimators,” Unpublished Manuscript, University of
    Chicago Business School.

Cameron, C., J. Gelbach, and D. Miller, (2008), “Bootstrap-based Improvements for
    Inference with Clustered Errors,”Review of Economics and Statistics , Vol. 90(3): 414-
    427.

Chesher, A., and G. Austin, (1991). “The finite-sample distributions of heteroskedasticity
    robust Wald statistics,” Journal of Econometrics, 47, 153-173.

Chesher, A., and I. Jewitt, (1987), “The Bias of a Heteroskedasticity Consistent Covari-
    ance Matrix Estimator,” Econometrica, 55(5): 1217-1222.

Cragg, J., (1983), “More Efficient Estimation in the Presence of Heteroscedasticity of Un-
    known Form,”Econometrica , Vol. 51(3): 751-763.

Diggle, P., P. Heagerty, K.-Y. Liang, and S. Zeger, (2002), Analysis of Longitudinal
    Data, Oxford University Press, Oxford.

Donald, S. and K. Lang, (2007), “Inference with Difference in Differences and Other Panel
    Data,”Review of Economics and Statistics, Vol. 89(2): 221-233.

Eicker, F., (1967), “Limit Theorems for Regressions with Unequal and Dependent Errors,”
    Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability,
    1, 59:82. Berkeley, University of California Press.


                                            [19]
Fisher, R., (1939), “The Comparisons of Samples with Possibly Unequal Variances,” Annals
    of Eugenics, 9, 174-180.

Hansen, C., (2009), “Generalized Least Squares Inference in Panel and Multilevel Models
    with Serial Correlation and Fixed Effects,” Journal of Econometrics, 140(2), 670-694.

Hausman, J., and C. Palmer, (2011), “Heteroskedasticity-Robust Inference in Finite Sam-
    ples,” NBER Working Paper 17698, http://www.nber.org/papers/w17698.

Horn, S., and R. Horn, (1975), “Comparisons of Estimators of Heteroscedastic Variances
    in Linear Models,”Journal of the American Statistical Association, 70(352), 872-879.

Horn, S., R. Horn, and D. Duncan, (1975), “Estimating Heteroscedastic Variances in
    Linear Models,”Journal of the American Statistical Association, 70(350), 380-385.

Huber, P. (1967), “The Behavior of Maximum Likelihood Estimates Under Nonstandard
    Conditions,” in Proceedings of the Fifth Berkeley Symposium in Mathematical Statistics,
    Vol. 1, Berkeley, University of California Press, 221-233.

Ibragimov, R., and U. Müller, (2009), “t-statistic based correlation and heterogeneity ro-
    bust inference” Unpublished Manuscript, Department of Economics, Harvard University.

Lehmann, E., and J. Romano, (2005), Testing Statistical Hypotheses, Springer.

Liang, K., and S. Zeger, (1986), “Longitudinal Data Analysis Using Generalized Linear
    Models,” Biometrika, 73(1): 13-22.

Liu, R., (1988), “Bootstrap Procedures under Some Non-iid Models,” Annals of Statistics,
    16, 1696-1708.

MacKinnon, J., (2002), “Bootstrap Inference in Econometrics,” Canadian Journal of Eco-
    nomics, 35 615-645.

MacKinnon, J., (2012), “Thirty Years of Heteroskedasticity-Robust Inference,” in Recent
    Advances and Future Directions in Causality, Prediction, and Specification Analysis, ed.
    Xaiohong Chen and Norman R. Swanson, New York, Springer, p 437–461.




                                            [20]
MacKinnon, J., and H. White, (1985), “Some Heteroskedasticity-consistent Covariance
    Matrix Estimators with Improved Finite Sample Properties,” Journal of Econometrics,
    Vol. 29, 305-325.

Mammen, E., (1993). “Bootstrap and wild bootstrap for high dimensional linear models,”
    Annals of Statistics, 21, 255-285.

Mantel, N., (1967), “The Detection of Disease Clustering and a Generalized Regression
    Approach,” Cancer Research, 27(2):209-220.

Moulton, B., (1986), “Random Group Effects and the Precision of Regression Estimates,”
    Journal of Econometrics, 32, 385-397.

Moulton, B., (1990), “An Illustration of a Pitfall in Estimating the Effects of Aggregate
    Variables on Micro Units,” Review of Economics and Statistics, 334-338.

Moulton, B., and W. Randolph, (1989) “Alternative Tests of the Error Component
    Model,” Econometrica, Vol. 57, No. 3, 685-693.

Pan, W., and M. Wall, (2002) “Small-sample Adjustments in Using the Sandwich Variance
    Estimator in Generalized Estimating Equation,” Statistics in Medicine, Vol. 51, 1429-
    1411.

Sattherthwaite, F., (1946), “An Approximate Distribution of Estimates of Variance Com-
    ponents,” Biometrics Bulletin, 2(6): 110-114.

Scheffé, H., (1970), “Practical Solutions to the Behrens-Fisher Problem,” Journal of the
    American Statistical Association, Vol. 65, No. 332, 1501-1508.

Stock, J., and M. Watson., (2008), “Heteroskedasticity-Robust Standard Errors for Fixed
    Effect Panel Data Regression,” Econometrica, 76(1): 155-174.

Wang, Y. , (1971), “The Probabilities of the Type I Errors of the Welch Tests for the
    Behrens-Fisher Problem,” Journal of the American Statistical Association, Vol. 66 , No.
    335, 605-608.

Welch, B. (1951), “The Generalization of ‘Students’ Problem When Several Different Pop-
    ulation Variances are Involved,” Biometrika, Vol 34: 28-35.

                                            [21]
White, H. (1980), “A Heteroskedasticity-Consistent Covariance Matrix Estimator and a
    Direct Test for Heteroskedasticity,” Econometrica, 48, 817-838.

William S. (1998), Comparison of standard errors for robust, cluster, and standard estima-
    tors, StataCorp http://www.stata.com/support/faqs/stat/cluster.html

Wooldridge, J., (2002), Econometric Analysis of Cross Section and Panel Data, MIT Press,
    Cambridge, MA.

Wu, C. (1986), “Jackknife, Bootstrap and Other Resampling Methods in Regression Analy-
    sis,” (with discussion) Annals of Statistics, 14, 1261-1295.




                                            [22]
Appendix A                   Other methods
A.1     HC3
A second alternative to the EHW variance estimator is V̂HC3 . We use the version dis-
cussed in MacKinnon (2012):
                         !−1
                                              2      
                                                                     !−1
              XN              X Yi − Xi β̂
                                N                         XN
     V̂HC3 =      Xi Xi0                              0
                                                2 Xi Xi      Xi Xi0     .              (A.1)
                                                       
                                        −
                             
              i=1              i=1
                                     (1   Pii )           i=1


Compared to V̂HC2 this variance estimator has the square of 1 − Pii in the denominator.
In the binary regressor case this leads to:
                        N0                  N1
      V̂HC3 = σ 2(0)           + σ 2
                                     (1)           .
                     (N0 − 1)2           (N1 − 1)2
In simple cases this leads to an upwardly biased estimator for the variance.

A.2     The Wild Bootstrap
The simple bootstrap where we resample N units picked with replacement from the
original sample is unlikely to perform well. In particular in cases where either N0 or N1 is
small, the additional noise introduced by variation in the number of Xi = 0 units sampled
is likely to adversely affect the properties of the corresponding confidence intervals. In
this literature researchers have therefore focused on alternative resampling methods. One
that has been proposed as an attractive choice is the wild bootstrap (Liu, 1988; Mammen,
1993; Cameron, Gelbach, and Miller, 2008; MacKinnon, 2011).
   Here we briefly describe the wild bootstrap in the regression setting, and then in the
cluster setting. First consider the regression setting. Let β̂0 and β̂1 be the least squares
estimates in the original sample, and ε̂ = Yi − β̂0 − Xi · β̂1 be the estimated residuals,
and let V̂ be a variance estimator, either V̂ehw , or V̂HC2 , or V̂HC3 . In the wild bootstrap
the regressor values are fixed in the resampling. In the b-th bootstrap sample, the value
for the i-th outcome is

      Yi,b = β̂0 + Xi · β̂1 + Ui,b · ε̂i ,

where Ui,b is a binary random variable with pr(Ui,b = 1) = pr(Ui,b = −1) = 1/2, with Ui,b
independent across i and b. (Other distributions for Ui,b are also possible, here we only
consider this particular choice.)

                                             [23]
   Here we use a symmetric version of the bootstrap. In the b-th bootstrap sample we
calculate the t-statistic

              β̂b,1 − β̂1
      tb =       p        ,
                    V̂b

for some variance estimator V̂b . Over all the bootstrap samples we calculate the 0.95
quantile of the distribution of |tb | (which, because of the symmetry of the design, are
approximately equal to minus the 0.025 and the 0.975 quantile of the distribution of tb .
                      wild
Let this quantile be q0.95 . We use this quantile instead of 1.96 to construct the confidence
interval as
                                                 p                        p 
                                       wild                        wild
      95% CIHC2 =             β̂1 −   q0.95   ×       V̂, β̂1 +   q0.95   × V̂ .                              (A.2)

                                                                                   p
                                                                                               wild
The wild bootstrap standard errors reported in the table are                           V̂HC2 (q0.95 /1.96).
   For the cluster version of the wild bootstrap, the bootstrap variable Us,b is indexed
by the cluster only. Again the distribution of Us,b is binary with values -1 and 1, and
probability pr(Us,b = 1) = pr(Us,b = −1) = 0.5. The bootstrap value for the outcome for
unit i in cluster s is then

      Yis,b = β̂0 + β̂1 · Xis + Us,b · εis ,

with the values for Xis and εis remaining fixed across the bootstrap replications.




                                                             [24]
Table 1: Coverage Rates and Standard Errors, Angrist-Pischke Unbalanced Design, N0 =
27, N1 = 3

                                   Design I     Design II     Design III    Design IV     Design V
                                 σ(0) = 0.50   σ(0) = 0.85   σ(0) = 1.00   σ(0) = 1.18 .00 σ(0) = 2
 variance               dist     cov med       cov med       cov med       cov med. cov med
 estimator              /dof     rate s.e.     rate s.e.     rate s.e.     rate s.e. rate s.e.

                        ∞        0.73   0.33   0.90   0.52   0.94   0.60   0.97   0.70   1.00   1.17
 V̂homo
                        N −2     0.75   0.34   0.92   0.54   0.95   0.63   0.97   0.73   1.00   1.22

                        ∞        0.77   0.40   0.79   0.42   0.81   0.44   0.82   0.45   0.87   0.55
 V̂ehw
                        N −2     0.78   0.42   0.81   0.44   0.82   0.46   0.83   0.47   0.88   0.57

                        ∞        0.82   0.49   0.84   0.51   0.85   0.52   0.86   0.53   0.90   0.62
                        N −2     0.84   0.51   0.86   0.53   0.86   0.54   0.87   0.56   0.91   0.65
                        KWelch   0.93   1.00   0.92   0.93   0.92   0.90   0.93   0.87   0.93   0.80
 V̂HC2                    ∗
                        KWelch   0.96   1.04   0.97   1.02   0.97   1.00   0.97   0.97   0.97   0.87
                        KBM      0.95   0.90   0.96   0.94   0.97   0.95   0.98   0.98   0.99   1.14
                        wild     0.90   0.76   0.90   0.74   0.91   0.73   0.91   0.72   0.92   0.73

                        ∞        0.87   0.60   0.89   0.61   0.89   0.62   0.90   0.63   0.92   0.71
 V̂HC3
                        wild     0.91   0.78   0.91   0.77   0.92   0.77   0.92   0.76   0.93   0.77

 max{V̂homo , V̂ehw}    ∞        0.82   0.41   0.92   0.54   0.95   0.62   0.97   0.71   1.00   1.17

 max{V̂homo , V̂HC2 }   ∞        0.86   0.49   0.93   0.57   0.95   0.64   0.97   0.73   1.00   1.17




                                               [25]
Table 2: Coverage Rates and Standard Errors, Angrist-Pischke Balanced Design, N0 =
15, N1 = 15

                                   Design I     Design II     Design III    Design IV     Design V
                                 σ(0) = 0.50   σ(0) = 0.85   σ(0) = 1.00   σ(0) = 1.18   σ(0) = 2.00
 variance               dist     cov med       cov med       cov med       cov med.      cov med
 estimator              /dof     rate s.e.     rate s.e.     rate s.e.     rate s.e.     rate s.e.

                        ∞        0.94   0.28   0.94   0.33   0.94   0.36   0.94   0.39   0.94   0.57
 V̂homo
                        N −2     0.95   0.30   0.95   0.35   0.95   0.38   0.95   0.41   0.95   0.59

                        ∞        0.93   0.27   0.93   0.32   0.93   0.35   0.93   0.38   0.93   0.55
 V̂ehw
                        N −2     0.94   0.29   0.94   0.34   0.94   0.36   0.94   0.40   0.94   0.57

                        ∞        0.94   0.28   0.94   0.33   0.94   0.36   0.94   0.39   0.94   0.57
                        N −2     0.95   0.30   0.95   0.35   0.95   0.38   0.95   0.41   0.95   0.59
                        KWelch   0.95   0.30   0.95   0.35   0.95   0.38   0.95   0.41   0.95   0.60
 V̂HC2                    ∗
                        KWelch   0.95   0.30   0.95   0.35   0.95   0.38   0.95   0.41   0.95   0.60
                        KBM      0.95   0.30   0.95   0.35   0.95   0.38   0.95   0.41   0.95   0.59
                        wild     0.95   0.30   0.95   0.35   0.95   0.38   0.95   0.41   0.95   0.60

                        ∞        0.94   0.29   0.95   0.35   0.95   0.37   0.95   0.41   0.94   0.59
 V̂HC3
                        wild     0.95   0.30   0.95   0.35   0.95   0.38   0.95   0.41   0.95   0.60

 max{V̂homo , V̂ehw}    ∞        0.94   0.28   0.94   0.33   0.94   0.36   0.94   0.39   0.94   0.57

 max{V̂homo , V̂HC2 }   ∞        0.94   0.28   0.94   0.33   0.94   0.36   0.94   0.39   0.94   0.57




                                               [26]
             Table 3: Coverage Rates and Standard Errors, Cragg Design

                                       (γ0 , γ1, γ2 ) = (0.6, 0.3, 0.0)   (γ0 , γ1, γ2 ) = (0.3, 0.2, 0.1)
                                        Design I          Design II        Design III        Design IV
                                         N = 25           N = 100           N = 25           N = 100
variance              dist/            cov med cov              med.      cov med. cov med.
estimator             dof              rate s.e. rate            s.e.     rate s.e. rate             s.e.

                      ∞                0.81    0.12    0.76     0.05      0.63    0.12     0.51    0.06
V̂homo
                      N −2             0.83    0.13    0.76     0.05      0.65    0.13     0.51    0.06

                      ∞                0.74    0.11    0.83     0.07      0.67    0.14     0.78    0.11
V̂ehw
                      N −2             0.76    0.12    0.83     0.07      0.69    0.15     0.78    0.11

                      ∞                0.82    0.14    0.87     0.07      0.77    0.17     0.84    0.12
                      N −2             0.84    0.15    0.87     0.07      0.79    0.18     0.84    0.12
                      KSatterthwaite   0.96    0.30    0.96     0.12      0.96    0.47     0.97    0.23
V̂HC2                   ∗
                      KSatterthwaite   0.98    0.31    0.97     0.12      0.99    0.51     0.98    0.24
                      KBM              0.96    0.23    0.94     0.09      0.94    0.28     0.93    0.15
                      wild             0.79    0.15    0.87     0.09      0.78    0.21     0.88    0.15

                      ∞                0.89    0.18    0.90     0.08      0.87    0.23     0.90    0.13
V̂HC3
                      wild             0.81    0.17    0.88     0.09      0.81    0.24     0.89    0.16

max{V̂homo, V̂ehw }   ∞                0.83    0.13    0.85     0.07      0.71    0.15     0.78    0.11

max{V̂homo, V̂HC2 }   ∞                0.87    0.15    0.88     0.08      0.79    0.18     0.84    0.12




                                               [27]
Table 4: Coverage Rates and Standard Errors, Cameron-Gelbach-Miller Clustering De-
sign

                               Design I      Design II    Design III    Design IV      Design V
 variance    dist/            cov med       cov med       cov med       cov med       cov med
 estimator   dof              rate s.e.     rate s.e.     rate s.e.     rate s.e.     rate s.e.

             ∞                0.47   0.06   0.52   0.08   0.50   0.06   0.53   0.07   0.36   0.06
 V̂homo
             S−1              0.53   0.07   0.69   0.11   0.56   0.07   0.59   0.08   0.41   0.07

             ∞                0.79   0.12   0.73   0.13   0.84   0.13   0.84   0.14   0.81   0.18
 V̂lz
             S−1              0.85   0.14   0.86   0.18   0.89   0.14   0.89   0.16   0.86   0.21

             ∞                0.81   0.13   0.78   0.15   0.87   0.13   0.86   0.15   0.83   0.19
 V̂STATA
             S−1              0.87   0.15   0.90   0.21   0.91   0.15   0.91   0.17   0.88   0.22

             ∞                0.87   0.15   0.84   0.17   0.89   0.14   0.89   0.16   0.88   0.22
             S−1              0.91   0.17   0.93   0.24   0.93   0.17   0.93   0.18   0.91   0.26
             KBM              0.94   0.19   0.95   0.27   0.94   0.18   0.94   0.20   0.96   0.34
 V̂lz2         ∗
             KSatterthwaite   0.98   0.25   0.98   0.34   0.97   0.21   0.96   0.23   0.96   0.34
             KIK              0.97   0.24   0.97   0.32   0.97   0.20   0.96   0.22   0.96   0.34
             wild             0.91   0.19   0.91   0.29   0.93   0.18   0.93   0.20   0.89   0.27




                                               [28]
