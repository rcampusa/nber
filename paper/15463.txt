                                NBER WORKING PAPER SERIES




              TESTING THE CORRELATED RANDOM COEFFICIENT MODEL

                                          James J. Heckman
                                         Daniel A. Schmierer
                                           Sergio S. Urzua

                                        Working Paper 15463
                                http://www.nber.org/papers/w15463


                      NATIONAL BUREAU OF ECONOMIC RESEARCH
                               1050 Massachusetts Avenue
                                 Cambridge, MA 02138
                                     October 2009




This research was supported by NIH R01-HD043411, NSF SES-024158, the American Bar Foundation
and the Geary Institute, University College Dublin, Ireland. The views expressed in this paper are
those of the authors and not necessarily those of the funders listed here. We have received helpful
comments from Pedro Carneiro, Jeremy Fox, Joel Horowitz, Benjamin Moll, Azeem Shaikh, Christopher
Taber, Edward Vytlacil, the editor, Steve Durlauf, and an anonymous referee and participants in workshops
at the University of Wisconsin and Northwestern University. In the final round of revisions, we received
additional very helpful suggestions from Stephane Bonhomme, Xiaohong Chen, Azeem Shaikh and
Edward Vytlacil. The views expressed herein are those of the author(s) and do not necessarily reflect
the views of the National Bureau of Economic Research. Supplementary material for this paper is
available at the Website http://jenni.uchicago.edu/testing_random/.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2009 by James J. Heckman, Daniel A. Schmierer, and Sergio S. Urzua. All rights reserved. Short
sections of text, not to exceed two paragraphs, may be quoted without explicit permission provided
that full credit, including © notice, is given to the source.
Testing the Correlated Random Coefficient Model
James J. Heckman, Daniel A. Schmierer, and Sergio S. Urzua
NBER Working Paper No. 15463
October 2009
JEL No. C31

                                            ABSTRACT

The recent literature on instrumental variables (IV) features models in which agents sort into treatment
status on the basis of gains from treatment as well as on baseline-pretreatment levels. Components
of the gains known to the agents and acted on by them may not be known by the observing economist.
Such models are called correlated random coefficient models. Sorting on unobserved components
of gains complicates the interpretation of what IV estimates. This paper examines testable implications
of the hypothesis that agents do not sort into treatment based on gains. In it, we develop new tests
to gauge the empirical relevance of the correlated random coefficient model to examine whether the
additional complications associated with it are required. We examine the power of the proposed tests.
We derive a new representation of the variance of the instrumental variable estimator for the correlated
random coefficient model. We apply the methods in this paper to the prototypical empirical problem
of estimating the return to schooling and ˝find evidence of sorting into schooling based on unobserved
components of gains.


James J. Heckman                                   Sergio S. Urzua
Department of Economics                            Northwestern University
The University of Chicago                          Department of Economics
1126 E. 59th Street                                2001 Sheridan Road #3225
Chicago, IL 60637                                  Evanston, IL 60208
and NBER                                           s-urzua@northwestern.edu
jjh@uchicago.edu

Daniel A. Schmierer
Department of Economics
University of Chicago
1126 E. 59th Street
Chicago IL 60637
dschmier@uchicago.edu
1         Introduction

The correlated random coefficient model is the new centerpiece of a large literature in mi-
croeconometrics. For person i, it expresses outcome Yi in terms of choice indicator Di as


                                                Yi = αi + βi Di                                (1)


where Di = 1 if a choice is made; Di = 0 if not and both the intercept, αi , and the slope,
βi , vary among persons. In this expression both the αi and βi may depend on regressors Xi
which we keep implicit.
        βi is the causal effect of Di on Yi holding αi fixed. If agents make their choices to take
treatment based on components of βi that depend on variables not available to the observing
economist, Di is correlated with βi even after conditioning on Xi . Most recent studies focus
on estimating means or quantiles of the distribution of βi .1
        The model that motivated the research of a previous generation (see, e.g., Griliches,
1977) assumes no response heterogeneity (βi = β). The correlated random coefficient model
assumes that βi varies in the population and in addition that


                                               Cov (Di , βi ) 6= 0.                          (C-1)


The model also accounts for selection on intercepts, i.e. selection on pretreatment unobserv-
ables:
                                              Cov (Di , αi ) 6= 0.                           (C-2)

When (C-1) holds, marginal returns to an activity in general differ from average returns.
When assumption (C-2) holds but Di is independent of βi , standard IV identifies the mean
of βi , which we denote by β̄. This configuration of assumptions includes the case when βi is
    1
        Abbring and Heckman (2007) discuss methods for estimating the distribution of βi .




                                                        3
random but independent of Di and the case when βi is the same for everyone.2,3
       As first noted by Heckman and Robb (1985), instrumental variables (IV) applied to (1)
when (C-1) holds produces an instrument-dependent parameter that, in general, is not β̄.4
In general, different instruments identify different parameters. Under conditions specified in
Yitzhaki (1989),5 Imbens and Angrist (1994), Heckman and Vytlacil (1999), and Heckman,
Urzua, and Vytlacil (2006), IV estimates weighted averages of marginal effects. Heckman and
Vytlacil (1999, 2001, 2005, 2007a) generalize the marginal treatment effect (MTE) introduced
by Björklund and Moffitt (1987) and show that the MTE plays the role of a policy-invariant
functional that is invariant to the choice of instrument. The MTE can be used to unify the
literature on treatment effects.6
       Heckman and Vytlacil (2001, 2005, 2007b) derive testable implications of the hypothesis
that βi is statistically independent of Di given Xi :


                                           H0 : βi ⊥
                                                   ⊥ Di | Xi ,


where A ⊥
        ⊥ B | C means A is independent of B given C. In this paper, we develop formal
tests of this hypothesis and analyze their power. We apply our tests to a prototypical problem
of estimating the return to schooling.
       The paper proceeds as follows. Section 2 establishes the equivalence of the correlated
random coefficient model with the Generalized Roy model. We state two testable impli-
cations of it. One test exploits the insight that, in general, in the case when H0 is false,
different instruments identify different parameters. We develop a test based on this principle
in Section 3 after first presenting some new results on the sampling distribution of the instru-
   2
      See Heckman and Vytlacil (1998), Heckman and Vytlacil (2007a,b). The standard “ability bias” problem
(Griliches, 1977) assumes that βi = β, a constant for all i, and that Cov(Di , αi ) 6= 0.
    3
      Evidence from parametric models on the empirical relevance of (C-1) in a variety of areas of economics
is presented in Heckman (2001, Table 3).
    4
      See the discussion of the ensuing literature in Heckman, Urzua, and Vytlacil (2006) or Heckman and
Vytlacil (2007a,b).
    5
      Posted at website for Heckman, Urzua, and Vytlacil (2006), see http://jenni.uchicago.edu/
underiv/.
    6
      See Heckman and Vytlacil (2005).


                                                     4
mental variable estimator and considering the problem of constructing power functions for
tests of hypotheses in the correlated random coefficient model. Section 4 develops tests for a
second implication of the correlated random coefficient model. Section 5 analyzes the power
of the proposed tests. Section 6 applies these tests to a prototypical problem: estimating the
return to schooling using the model of Carneiro, Heckman, and Vytlacil (2006). Section 7
concludes.



2         Equivalence with the Generalized Roy Model and

          Two Testable Implications of H0

An alternative way to represent equation (1) makes the link to economic choice theory
more explicit. Individual i experiences outcome Y1,i if Di = 1 and outcome Y0,i if Di = 0,
i = 1, . . . , I. The observed outcome is Yi = Di Y1,i + (1 − Di )Y0,i .7 Let µj (Xi ) = E(Yj,i | Xi ),
j ∈ {0, 1}. One can write the model for potential outcomes conditional on Xi as Y1,i =
µ1 (Xi ) + U1,i and Y0,i = µ0 (Xi ) + U0,i where E(Uj,i | Xi ) = 0, j ∈ {0, 1}. In this notation,
the observed outcome is


                         Yi = µ0 (Xi ) + [µ1 (Xi ) − µ0 (Xi ) + U1,i − U0,i ] Di + U0,i .


This is the correlated random coefficient model of equation (1) where the baseline outcome
is αi = µ0 (Xi ) + U0,i and the gain is βi = µ1 (Xi ) − µ0 (Xi ) + U1,i − U0,i where, for notational
simplicity, we suppress the dependence of αi and βi on Xi . To simplify the expressions, we
drop the i subscripts throughout the rest of the paper unless their use clarifies the discussion.
We define α = α + Uα and β = β̄ + Uβ where E(Uα | X) = 0 and E(Uβ | X) = 0. Table 1
shows the equivalent parameters for the two models.
        Whether the null hypothesis H0 is true or not depends on the underlying choice model.
    7
        This is in the form of a Quandt (1958) switching regression model.



                                                        5
Table 1: Equivalence of Notation Between the Correlated Random Coefficient Model and the
Generalized Roy Model. All parameters are defined conditional on Xi , which is left implicit.
                                                    Generalized Roy                         Correlated random
                                                        model                               coefficient model

 Baseline outcome                                   Y 0,i = μ 0 + U 0,i                             αi



 Outcome in treated state                           Y 1,i = μ 1 + U 1,i                          β i + αi



 Gain to treatment                       Y 1,i - Y 0,i = μ 1 - μ 0 + U 0,i - U 0,i                 βi

 (Individual causal effect)

 Outcome                           Y i = Y 0,i + D i (Y 1,i - Y 0,i )                       Y i = αi + β i D i
                                   = μ 0,i + (μ 1,i - μ 0,i + U 1,i - U 0,i ) D i + U 0,i



We postulate a threshold crossing model which assumes separability between observables
Z that affect choice and an unobservable V : D = 1(µD (Z) − V ≥ 0), where 1(·) is an
indicator function that takes the value 1 if its argument is true and is 0 otherwise, and
µD is a deterministic function of Z.8 Z can include components of X. Letting FV be the
distribution of V conditional on X, and assuming that Z ⊥
                                                        ⊥ V | X, the choice probability
or “propensity score” is


                              P (z) = Pr(D = 1|Z = z) = FV (µD (z)),


where to simplify the notation, we keep the conditioning on X implicit. The choice equation
can be written in several alternative and equivalent ways:


               D = 1(µD (Z) − V ≥ 0) = 1(FV (µD (Z)) ≥ FV (V )) = 1(P (Z) ≥ UD )
   8
    See, e.g., Thurstone (1927) and McFadden (1974, 1981). We do not strictly require separability, but we
do require that the choice equation has one representation in separable form. See Heckman and Vytlacil
(2007b).




                                                         6
where UD = FV (V ) so UD ∼ Uniform[0, 1].
       We invoke the assumptions of Heckman and Vytlacil (2005, 2007b).9 A fundamental
treatment parameter introduced by Björklund and Moffitt (1987) is the marginal treatment
effect (MTE). The MTE for a given value of X = x is


              M T E(x, uD ) = E(Y1 − Y0 | X = x, UD = uD ) = E(β | X = x, UD = uD ).


It is the mean effect of treatment when the observables X are fixed at a value x and the
unobservable in the choice equation UD is fixed at a value uD . Heckman and Vytlacil (1999,
2001, 2005, 2007b) use the MTE to develop the following implication of H0 .
       In the general case, the conditional expectation of Y given X and Z is


                E(Y |X = x, Z = z) = E(Y |X = x, P (Z) = p)

                                       = E(α|X = x) + E(βD|X = x, P (Z) = p)

                                       = E(α|X = x) + E(β|X = x, D = 1)p
                                                      Z p
                                       = E(α|X = x) +     E(β|X = x, UD = uD )duD ,                   (2)
                                                              0
   9
       Their conditions are:

(A-1) (U0 , U1 , V ) ⊥
                     ⊥ Z | X. Alternatively, (α, β, V ) ⊥⊥ Z | X.

(A-2) The distribution of µD (Z) conditional on X is nondegenerate. Thus the distribution of P (Z) is
nondegenerate.

(A-3) The distribution of V is continuous (i.e., absolutely continuous with respect to Lebesgue measure).
Thus UD = FV (V ) is uniform.

(A-4) E |Y1 | < ∞, and E |Y0 | < ∞, so defining E(β) = β̄, |β̄| < ∞.

(A-5) 1 > Pr (D = 1 | X) > 0.

Vytlacil (2002) shows that under mild regularity conditions, assumptions (A-1)-(A-5) are equivalent to the
IV conditions of Imbens and Angrist (1994) used to define the local average treatment effect (LATE).




                                                       7
where the integrand in the final expression is the M T E(x, uD ).10 Under H0 ,


                               E(β | X = x, UD = uD ) = E(β | X = x),


so
                  E(Y | X = x, P (Z) = p) = E(α | X = x) + E(β | X = x)p.11                                  (3)

Thus the function E(Y |X = x, P (Z) = p) is linear in p, conditional on X = x, which is a
testable hypothesis.
       A second implication of H0 is that any standard instrument identifies β = E(β).12 Thus
under H0 all valid instruments have the same estimand. Under conditions presented in this
paper, comparing the estimates produced by different instruments tests the weaker hypoth-
esis H00 : Cov(β, D | X) = 0, which is an implication of the stronger hypothesis H0 . The
analysis in this paper thus provides an alternative interpretation of standard tests of overi-
dentification. A rejection of the null hypothesis that two instrumental variable estimands
are different is not necessarily a rejection of the validity of one instrument. It could be
interpreted as evidence in support of a correlated random coefficient model.



3        Tests Based on Comparing IV Estimates

Before presenting our test based on comparing the estimates from two IV estimators, we
first discuss some general properties of the IV estimator in the correlated random coefficient
model. We present a new representation of the sampling distribution of the IV estimator.
We consider the problem of constructing the power of tests of several hypotheses using the
sampling distribution of the IV estimator for the correlated random coefficient model before
  10
      The first line follows from (A-1). The rest of the derivation comes from (1) and the law of iterated
expectations.
   11
      To see this, notice that β ⊥⊥ D | X ⇐⇒ β ⊥⊥ 1(P (Z) ≥ UD ) | X ⇐⇒ β ⊥⊥ UD | X.
   12
      In the notation of equation (1), but dropping subscripts i, a standard instrument J has the two properties:
(i) Cov(J, D | X) 6= 0 and (ii) Cov((α, β), J | X) = 0. Note that J is shorthand for J(Z). Note further that
the condition Cov(β, J | X) = 0 only emerges as an interesting condition in a random coefficient model.


                                                       8
developing our test for H0 .


3.1     IV in the Correlated Random Coefficient Model

Consider an instrument J(Z). Denote J(Z) by J and define J˜ = J − J¯ where J¯ is the sample
mean of J(Z). E(J) is assumed to be finite. The IV estimator is

                                                        P ˜
                                                          Yi Ji
                                                βIV,J = P
                                                b                .
                                                          Di J˜i

Define Cov(J, D) = ωJ and let I denote the sample size. Under a weak law of large numbers,
  P ˜ p                p
1
I
    Di Ji → ωJ and J¯ → E(J). As shown in Heckman and Vytlacil (2005, 2007b), under
the conditions (A-1)–(A-5) stated in Section 2,

                                                     Z    1
                              p
                       βbIV,J → βIV,J =                       E(β|UD = uD )hJ (uD )duD   (4)
                                                      0



where
                                   E[(J − E(J)) | P (Z) ≥ uD ] Pr(P (Z) ≥ uD )
                   hJ (uD ) =                                                  ,         (5)
                                                      ωJ

and we keep the conditioning on X implicit. Heckman and Vytlacil (2005) show that
R1
   h (t)dt = 1. Thus we can write
 0 J

                                            Z    1
                        βIV,J = β +                  E(Uβ | UD = uD )hJ (uD )duD .       (6)
                                             0



For later use we break out the component of βIV,J that depends on the instrument J:

                           Z       1
                                       E(Uβ | UD = uD )hJ (uD )duD = ΥJ ,
                               0



so βIV,J = β̄ + ΥJ . By definition, conditional on X, β̄ does not depend on J.




                                                                9
      Under independent sampling,

                                      √               
                                                         d
                                       I βbIV,J − βIV,J → N (0, ΩJ )


where

                             Z1
               2  Var(J)                                 2
                                                                         
        ΩJ = E α           +      2E (αβ | U D = uD ) + E β   | U D = uD    hΩJ (uD )duD          (7)
                      ωJ2
                             0
                 1                              2
                   Z
              −  E(β | UD = uD )hJ (uD )duD 
                       0


and

                                       Z∞                     Z1
                                 1                        2
                     hΩJ (uD ) = 2          (j − E(J))             fP,J (P (z), j)dP (z)dj        (8)
                                ωJ
                                      −∞                      uD
                                                      2
                                    E[(J − E(J)) | P (Z) ≥ uD ] Pr(P (Z) ≥ uD ) 13
                                =                                              .
                                                       ωJ2

The weight hΩJ (uD ) does not necessarily integrate to 1:

                                            1
                                                               Cov(J˜2 , D)
                                       Z
                                                hΩJ (t)dt =                 .
                                        0
                                                                    ˜ D)]2
                                                              [Cov(J,

Appendix A presents the full derivation. The weight hΩj (uD ) plays a role in determining
the variance of the IV estimator that is analogous to the role of hJ (uD ) in generating the
probability limit of the IV estimator. 2E[αβ | UD = uD ] + E[β 2 | UD = uD ] plays a role in
generating the variance of the IV estimator analogous to the role of the MTE in generating
the probability limit of the IV estimator. We use this representation to facilitate comparison
of the power of the tests under alternative data generating processes and to consider the
problem of the optimal choice of instruments.
 13
      fP,J (P (z), j) is the density of P (Z) and J(Z) evaluated at P (Z) = P (z) and J(Z) = j.




                                                          10
       These formulae hold for general functions J(·) of instruments Z that satisfy assumptions
(A-1)-(A-5) given in Section 2. For example, suppose that J(Z) has discrete support on
points j1 , . . . , jK with corresponding values of the propensity score p1 , . . . , pL with L possibly
not equal to K. Let p0 = 0. In this case, for uD ∈ [pl , pl+1 ] both hJ and hΩJ are constant so
we can write

                  L−1                          Z   pl+1
        Var(J) X                                                                                            1
ΩJ = E α2                                                     2E (αβ | UD = uD ) + E β 2 | UD = uD
                                                                                                    
                +     λΩl                                                                                           duD
            ωJ2   l=0                           pl                                                        pl+1 − pl
               L−1
                                                                            !2
                              Z   pl+1
               X                                             1
           −             λl              E(β | UD = uD )           duD           .
                   l=1        pl                         pl+1 − pl

The weights λΩl and λl are defined in the following way. Let ji be the ith smallest value in
the support of J(Z), then

                                           PK           2
                                                          PL
                                             [ji − E(J)]    t>l fP,J (pt , ji )
                                  λΩl = i=1                                     (pl+1 − pl )
                                                      ˜
                                                  Cov(J(Z), D)2
                                        PK               PL
                                         i=1 [ji − E(J)]   t>l fP,J (pt , ji )
                                   λl =                                        (pl+1 − pl ).
                                                      ˜
                                                  Cov(J(Z), D)

The special case of a binary instrument J(Z) has two points of support, j1 and j2 , corre-
sponding to the points p1 and p2 in the propensity score distribution. Let Pr(J(Z) = j1 ) =
Pr(P (Z) = p1 ) = q and Pr(J(Z) = j2 ) = Pr(P (Z) = p2 ) = 1 − q. The λl are λ1 = 1 and
λl = 0, l > 1.14 The weights for the variance simplify to

            [j1 − E(J)]2 q + [j2 − E(J)]2 (1 − q)                 [j2 − E(J)]2 (1 − q)
   λΩ0 =                                          (p1 ) and λΩ1 =                      (p2 − p1 ),
                            ˜
                      Cov(J(Z),   D)2                                    ˜
                                                                     Cov(J(Z),  D)2
  14


                   [j2 − E(J)](1 − q)              (j2 − j1 )(p2 − p1 )q(1 − q)       ˜
                                                                                  Cov(J(Z), P (Z))
            λ1 =                      (p2 − p1 ) =                              =                  = 1.
                          ˜
                      Cov(J(Z), D)                              ˜
                                                         Cov(J(Z),    D)                ˜
                                                                                   Cov(J(Z),  D)




                                                                    11
and

                             (j1 − E(J))2 qp1 + (j2 − E(J))2 (1 − q)p2   Cov(J˜2 , D)
              λΩ0 + λΩ1 =                                              =              .
                                                 ˜ D)2
                                            Cov(J,                            ˜ D)2
                                                                         Cov(J,

      Formula (4) extends the representation of IV as weighted averages of slopes of the un-
derlying function, due to Yitzhaki (1989). It allows the instrument J(Z) be different from
the propensity score P (Z) or a monotonic function of it. It reveals that, in general, different
instruments identify different parameters. Thus, in general, βIV,J 6= βIV,J 0 if J and J 0 apply
different weights (5) to a common MTE.
      As noted by Heckman and Vytlacil (2005, 2007b), while the weight in (5) integrates to
1, it is not necessarily non-negative for all values of uD so the interpretation of the weighted
average produced by IV is obscure. Even though the MTE is positive everywhere, the IV
estimate may be negative.15
      Some applied economists report tests based on IV sampling distributions as if they are
testing the null hypothesis that β̄ = 0. Under H0 , i.e., the absence of a correlated random
coefficient model, the sampling distribution of the standard IV estimator, β̂IV,J , can be used
to consistently test the null hypothesis that β̄ = 0. However, when H0 is false, a test of
β̄ = 0 based on the sampling distribution of the IV estimator is, in general, inconsistent and
biased because by (6), IV does not, in general, converge to β̄.
      Consider the following example based on the normal generalized Roy Model.
                                                                
                           U1      0               σ12
                                                   σ10 σ1V            
                                                                
                           U  ∼ N  0  ,  σ     2                 ,                  (9)
                            0        10 σ0 σ0V                  
                                                                
                            V          0       σ1V σ0V σV2


and assume X = 1. Recalling that uD = FV (v), when V is a normal random variable, the
 15
      See the examples in Heckman, Urzua, and Vytlacil (2006).




                                                    12
marginal treatment effect is

                                                                     
                                                          σ1V − σ0V
                             M T E(UD = uD ) = β̄ +                       Φ−1 (uD )               (10)
                                                              σV

where Φ−1 (·) is the inverse of a standard normal CDF (hence Φ−1 (uD ) = v). Alternatively,
in terms of v,
                                                           σ1V − σ0V
                                    M T E(V = v) = β̄ +              v.
                                                               σV
            σ1V −σ0V
Let τ =        σV
                       . A value of τ 6= 0 produces a correlated random coefficient model. For such
values plim β̂IV,J 6= β̄. The choice equation is assumed to be D = 1(Z ≥ V ) where both
Z (a single instrument) and V are normally distributed and Z ⊥
                                                             ⊥ V . Additionally, assume
that σ12 = σ02 = σU2 , σ10 = 0.5 × σ1 × σ0 and σV2 = 1.
       Figure 1 plots the power of a Wald test of the hypothesis that β̄ = 0 based on β̂IV,J .
We compute the power function for different values of β̄. Recall from (6) that this is the
component of βIV,J that does not depend on J. In Panel A, β̂IV,J is a consistent estimator
for β̄. In the other two panels it is not. Thus in the top panel of the figures, when τ = 0,
and hence H0 is true, the test of the hypothesis β = 0 is unbiased and consistent and the
size of the test is controlled.16 As expected, smaller values of σU2 produce higher power,
and larger values of σZ2 produce higher power. The bottom two panels plot the power of
the test that β = 0 when τ = −1 and τ = 0.6, respectively. In these two latter cases,
plim βbIV,J = βIV,J 6= β̄. Hence the tests are biased and inconsistent. The power and size of
the test for the existence of an “effect” (i.e., whether β = 0) can be badly distorted. Thus
even if β = 0, an “effect” can be detected, and if β 6= 0, no “effect” can be detected.
  16
    Although Figure 1 shows the power function only for one sample size, the consistency of the test is
readily verified.




                                                   13
Figure 1: Power function for a Wald test of β̄ = 0 based on the sampling distribution of
β̂IV,J .
                                                                                                A. τ = 0
                                                         σZ2 = 1, σU2 = 1                                                             σZ2 = 1, σU2 = 0.1

                              1.0




                                                                                                                1.0
                              0.8




                                                                                                                0.8
                              0.6




                                                                                                                0.6
                      Power




                                                                                                        Power
                              0.4




                                                                                                                0.4
                              0.2




                                                                                                                0.2
                              0.0




                                                                                                                0.0
                                    -1.5   -1.0   -0.5         0.0       0.5        1.0   1.5                         -1.5   -1.0   -0.5      0.0         0.5     1.0   1.5




                                                         σZ2   = 0.1,   σU2    =1                                                    σZ2   = 0.1,   σU2   = 0.1
                              1.0




                                                                                                                1.0
                              0.8




                                                                                                                0.8
                              0.6




                                                                                                                0.6
                      Power




                                                                                                        Power
                              0.4




                                                                                                                0.4
                              0.2




                                                                                                                0.2
                              0.0




                                                                                                                0.0
                                    -1.5   -1.0   -0.5         0.0       0.5        1.0   1.5                         -1.5   -1.0   -0.5      0.0         0.5     1.0   1.5




                                                                                                B. τ = -1
                                                         σZ2 = 1, σU2 = 1                                                             σZ2 = 1, σU2 = 0.1
                              1.0




                                                                                                                1.0
                              0.8




                                                                                                                0.8
                              0.6




                                                                                                                0.6
                      Power




                                                                                                        Power
                              0.4




                                                                                                                0.4
                              0.2




                                                                                                                0.2
                              0.0




                                                                                                                0.0




                                    -1.5   -1.0   -0.5         0.0       0.5        1.0   1.5                         -1.5   -1.0   -0.5      0.0         0.5     1.0   1.5




                                                         σZ2   = 0.1,   σU2    =1                                                    σZ2   = 0.1,   σU2   = 0.1
                              1.0




                                                                                                                1.0
                              0.8




                                                                                                                0.8
                              0.6




                                                                                                                0.6
                      Power




                                                                                                        Power
                              0.4




                                                                                                                0.4
                              0.2




                                                                                                                0.2
                              0.0




                                                                                                                0.0




                                    -1.5   -1.0   -0.5         0.0       0.5        1.0   1.5                         -1.5   -1.0   -0.5      0.0         0.5     1.0   1.5




                                                                                                C. τ = 0.6
                                                         σZ2 = 1, σU2 = 1                                                             σZ2 = 1, σU2 = 0.1
                              1.0




                                                                                                                1.0
                              0.8




                                                                                                                0.8
                              0.6




                                                                                                                0.6
                      Power




                                                                                                        Power
                              0.4




                                                                                                                0.4
                              0.2




                                                                                                                0.2
                              0.0




                                                                                                                0.0




                                    -1.5   -1.0   -0.5         0.0       0.5        1.0   1.5                         -1.5   -1.0   -0.5      0.0         0.5     1.0   1.5




                                                         σZ2 = 0.1, σU2 = 1                                                          σZ2 = 0.1, σU2 = 0.1
                              1.0




                                                                                                                1.0
                              0.8




                                                                                                                0.8
                              0.6




                                                                                                                0.6
                      Power




                                                                                                        Power
                              0.4




                                                                                                                0.4
                              0.2




                                                                                                                0.2
                              0.0




                                                                                                                0.0




                                    -1.5   -1.0   -0.5         0.0       0.5        1.0   1.5                         -1.5   -1.0   -0.5      0.0         0.5     1.0   1.5




Note: Each plot shows the power for a hypothetical sample size of 500. The size of the test is 0.05. The model is the normal generalized Roy
                                                           2
model with the unobservables jointly normal with variance σU  and correlation 0.5. The choice equation is D = 1(Z ≥ V ) where V ∼ N (0, 1) and
           2
Z ∼ N (1, σZ ). The power functions plot the power of the Wald test of βIV,J = 0 for alternative values of β̄. The vertical dashed lines denote the
null hypothesis β̄ = 0. Each panel fixes τ = Cov(β, V )/ Var(V ) at a different level. When τ = 0, plim β̂IV,J = β̄0 , which in these figures is zero,
and hence the test is consistent. For all nonzero values of τ , the test is inconsistent.
                                                                                                  14
3.2       Testing Hypotheses About Instrument-Dependent Parame-

          ters

More recently, many applied economists, following Imbens and Angrist (1994), interpret IV
as a weighted average of “LATEs,” or in our framework, a weighted average of MTEs, as in
equation (3). It is understood that β̂IV,J is not, in general, consistent for the true β̄. Within
this framework, economists often report tests of the hypothesis that βIV,J = 0.
      To calculate the power of such tests, we consider alternative values of βIV,J (= β̄ + ΥJ
from equation (6)) obtained by varying β̄ holding ΥJ fixed. Notice that unlike the analysis in
the preceding section, in this section we are not testing the hypothesis that β̄ = 0. Instead
we are testing the hypothesis that βIV,J = 0 (or some other specified value). We vary β̄
to calculate the power of the test for alternative values of βIV,J . This is a sensible way to
proceed because β̄ is instrument invariant. Investigating the power of the test in this fashion
allows us to construct power functions for instrument-invariant alternatives.
      Figure 2 plots the power function for the Wald test of the hypothesis βIV,J = 0 as a
function of βIV,J holding ΥJ fixed at -0.5. Consequently, the β̄ compatible with the null
hypothesis, β̄0 , is 0.5. For the model of unobservables used in the previous subsection,
                                                                 σ1V −σ0V
keeping ΥJ fixed entails, among other things, holding τ =           σV
                                                                            fixed along with the
weighting function hJ (uD ). For a given τ and a fixed IV weighting function hJ (uD ), we vary
the parameters of covariance matrix (9). These parameters affect the sampling distribution
of β̂IV,J and hence the power of the test.
      Neither the IV estimand nor the variance of the IV estimator depends on σ10 . Therefore,
the power of the test of the null hypothesis βIV,J = 0 does not depend on σ10 . The only
remaining parameters that can be changed without changing ΥJ are σ02 , σ12 , σ1V and σ0V .
To keep τ fixed, we can only vary σ1V and σ0V subject to a constraint that σ1V − σ0V is
constant.17 For σV = 1, the four A panels of Figure 2 show the power of the test for different
values of β̄ when we vary σ1V and σ0V such that σ1V − σ0V = −1. The power of the test is
 17
      Variations in σV2 affect the denominator of the weights.


                                                      15
highest when σ1V and σ0V are both close to 0 (ie. straddling 0), and lowest when both are
far from zero (either positive or negative). The panels in B vary βIV,J by varying β̄ holding
ΥJ fixed and hold fixed all of the elements of (9) except for σ12 , while the panels in C vary
β̄ hold fixed all of the parameters of (9) except σ02 . As expected, power decreases as both
variances increase, in general at different rates.
    There are other ways to calculate the power of the test that βIV,J = 0 for alternative
values that are obtained by varying β̄ keeping ΥJ fixed. If the choice equation is


                                            D = 1(Zγ ≥ V )


and Z ∼ N (Z, ΣZ ) and V ∼ N (0, σV2 ), all instruments constructed from linear or affine
transformations of Z have the same weight function (5) and hence have the same instrument-
dependent value, βIV,J . For proof of this claim, see Appendix B.18
    This result implies that one can construct power functions for the hypothesis βIV,J = 0
for different values of βIV,J = β̄ + ΥJ for alternative choices of ΣZ , holding γ 0 ΣZ γ, the
variance of the choice index, constant. The derivation in Appendix B shows that the IV
estimand depends only on the distribution of the index Zγ − V . From assumption (A-1),
Zγ and V are statistically independent. σV2 has to be held constant to keep ΥJ fixed. We
keep this term fixed by varying components of Z while keeping γ 0 Zγ fixed. An instrument
with greater variance that obeys this constraint will produce greater power. Figure 3 plots
power functions of the test of the hypothesis that βIV,J = 0 using each component of a
two-dimensional instrument Z = (Z1 , Z2 ). These plots show that for a given IV estimand
βIV,J , the power of the test is higher when using the instrument that accounts for more of
the variance of the index Zγ. Going from top to bottom, the variance of Z1 is increasing
while the variance of Z2 is decreasing. Accordingly, from top to bottom the power of the
test βIV,J = 0 using Z1 as an instrument is increasing while the power of the test using Z2
  18
     This result is special to the case of J(Z) linear or affine in Z with Z normally distributed, so J(Z) is
normally distributed and the further assumption (A-1) that Z ⊥⊥ V , where V is normally distributed. We
have not analyzed more general conditions on Z and V under which the invariance holds.


                                                     16
Figure 2: Power function for the test of the hypothesis that plim β̂IV,J = 0 when β¯0 = 0.5.
Alternatives are different values of βIV,J obtained by fixing ΥJ and varying β̄.
                                                                                        A.
                                                 σ1V = -0.5, σ0V = 0.5                                                     σ1V = -0.2, σ0V = 0.8
                           1.0




                                                                                                     1.0
                           0.8




                                                                                                     0.8
                           0.6




                                                                                                     0.6
                   Power




                                                                                             Power
                           0.4




                                                                                                     0.4
                           0.2




                                                                                                     0.2
                           0.0




                                                                                                     0.0
                                 -1.5   -1.0   -0.5        0.0        0.5   1.0   1.5                      -1.5   -1.0   -0.5        0.0        0.5   1.0   1.5
                                                            β IV, J                                                                   β IV, J
                                                      σ1V = 0, σ0V = 1                                                     σ1V = 0.5, σ0V = 1.5
                           1.0




                                                                                                     1.0
                           0.8




                                                                                                     0.8
                           0.6




                                                                                                     0.6
                   Power




                                                                                             Power
                           0.4




                                                                                                     0.4
                           0.2




                                                                                                     0.2
                           0.0




                                                                                                     0.0
                                 -1.5   -1.0   -0.5        0.0        0.5   1.0   1.5                      -1.5   -1.0   -0.5        0.0        0.5   1.0   1.5
                                                            β IV, J                                                                   β IV, J
                                                                                        B.
                                                      σ02 = 1, σ12 = 0.1                                                        σ02 = 1, σ12 = 0.5
                           1.0




                                                                                                     1.0
                           0.8




                                                                                                     0.8
                           0.6




                                                                                                     0.6
                   Power




                                                                                             Power
                           0.4




                                                                                                     0.4
                           0.2




                                                                                                     0.2
                           0.0




                                                                                                     0.0




                                 -1.5   -1.0   -0.5        0.0        0.5   1.0   1.5                      -1.5   -1.0   -0.5        0.0        0.5   1.0   1.5
                                                            β IV, J                                                                   β IV, J
                                                       σ02 = 1, σ12 = 1                                                          σ02 = 1, σ12 = 2
                           1.0




                                                                                                     1.0
                           0.8




                                                                                                     0.8
                           0.6




                                                                                                     0.6
                   Power




                                                                                             Power
                           0.4




                                                                                                     0.4
                           0.2




                                                                                                     0.2
                           0.0




                                                                                                     0.0




                                 -1.5   -1.0   -0.5        0.0        0.5   1.0   1.5                      -1.5   -1.0   -0.5        0.0        0.5   1.0   1.5
                                                            β IV, J                                                                   β IV, J
                                                                                        C.
                                                      σ02 = 0.1, σ12 = 1                                                        σ02 = 0.5, σ12 = 1
                           1.0




                                                                                                     1.0
                           0.8




                                                                                                     0.8
                           0.6




                                                                                                     0.6
                   Power




                                                                                             Power
                           0.4




                                                                                                     0.4
                           0.2




                                                                                                     0.2
                           0.0




                                                                                                     0.0




                                 -1.5   -1.0   -0.5        0.0        0.5   1.0   1.5                      -1.5   -1.0   -0.5        0.0        0.5   1.0   1.5
                                                            β IV, J                                                                   β IV, J
                                                       σ02 = 1, σ12 = 1                                                          σ02 = 2, σ12 = 1
                           1.0




                                                                                                     1.0
                           0.8




                                                                                                     0.8
                           0.6




                                                                                                     0.6
                   Power




                                                                                             Power
                           0.4




                                                                                                     0.4
                           0.2




                                                                                                     0.2
                           0.0




                                                                                                     0.0




                                 -1.5   -1.0   -0.5        0.0        0.5   1.0   1.5                      -1.5   -1.0   -0.5        0.0        0.5   1.0   1.5
                                                            β IV, J                                                                   β IV, J



Note: Each plot shows the power for a hypothetical sample size of 500. The size of the test is 0.05. The instrument is normally distributed,
                                                                                                                   2                 2       2
Z ∼ N (1, 1); D = 1(Z ≥ V ). In panel A, the unobservables are generated with covariances given in the figure and σV  = 1, σ10 = 0, σ1 = 1, σ0 = 1.
                                                                                             2
In panels B and C the unobservables are generated with variances given in the figure and σV    = 1, σ10 = 0, σ1V = −0.5, σ0V = 0.5. In all panels,
under the null hypothesis β̄0 = 0.5, and alternative hypotheses are generated by changing β̄. The vertical dashed line shows the value of
plim β̂IV,J = 0 under the null hypothesis and the vertical dotted line shows the value of β̄ under the null hypothesis.
                                                                                        17
as an instrument is decreasing. Each panel shows the fraction of γ 0 Zγ accounted for by the
variance of the instrument used to construct the power function (either Z1 or Z2 ).19 We now
use the tools developed for IV in a correlated random coefficient model to test H0 .




3.3       Testing H0 Using Instrumental Variables

Armed with the preceding results, we now return to the main theme of this paper and study
how to use different IVs to test H0 . Under H0 , the probability limits of any two IV estimators
are identical, because for any choice of J,

                                   Z   1                                   Z   1
           plim βbIV,J = βIV,J =           E(β|UD = uD )hJ (uD )duD = β̄           hJ (uD )duD = β̄.
                                   0                                       0



If H0 is false, in general any two IV estimators will differ. Excluding the case of equal IV
weights for the two instruments, our IV test forms two estimators βbIV,1 and βbIV,2 , based on
J1 (Z) and J2 (Z) respectively, and tests the null hypothesis


                                            H0IV : βIV,1 − βIV,2 = 0


against the alternative hypothesis


                                            HAIV : βIV,1 − βIV,2 6= 0.


This test is identical to a standard test for overidentification. However, within the context of
a correlated random coefficient model, we do not interpret rejections of the null hypothesis
as evidence of the violation of the assumptions required for the validity of an instrument.
Rather, it is interpreted as evidence of selection on heterogeneous gains to treatment.
       Under the null hypothesis, the Wald test statistic is asymptotically distributed as a
  19
    Note that in a given row, the fractions do not sum to 1 because there is a covariance (of 0.1) between
Z1 and Z2 .

                                                       18
Figure 3: Power functions for the test of the hypothesis that βIV,J = plim β̂IV,J = 0 for
β̄0 = 0.5. Alternatives are different values of βIV,J obtained by fixing ΥJ and varying β̄.

                                                           A. σZ12 = 0.5, σZ22 = 2.3, σZ1,Z2 = 0.1
                               Using Z1 as instrument                                                            Using Z2 as instrument
                       (Fraction of γ'ΣΖγ accounted for by Z1 = 1/6)                                   (Fraction of γ'ΣΖγ accounted for by Z2 = 23/30)
          1.0




                                                                                          1.0
          0.8




                                                                                          0.8
          0.6




                                                                                          0.6
  Power




                                                                                  Power
          0.4




                                                                                          0.4
          0.2




                                                                                          0.2
          0.0




                                                                                          0.0
                -1.5    -1.0     -0.5     0.0       0.5    1.0         1.5                      -1.5      -1.0    -0.5     0.0       0.5     1.0        1.5

                                          β IV, J                                                                          β IV, J

                                                            B. σZ12 = 1, σZ22 = 1.8, σZ1,Z2 = 0.1
                               Using Z1 as instrument                                                            Using Z2 as instrument
                       (Fraction of γ'ΣΖγ accounted for by Z1 = 1/3)                                    (Fraction of γ'ΣΖγ accounted for by Z2 = 3/5)
          1.0




                                                                                          1.0
          0.8




                                                                                          0.8
          0.6




                                                                                          0.6
  Power




                                                                                  Power
          0.4




                                                                                          0.4
          0.2




                                                                                          0.2
          0.0




                                                                                          0.0




                -1.5    -1.0     -0.5     0.0       0.5    1.0         1.5                      -1.5      -1.0    -0.5     0.0       0.5     1.0        1.5

                                          β IV, J                                                                          β IV, J

                                                            C. σZ12 = 2, σZ22 = 0.8, σZ1,Z2 = 0.1
                               Using Z1 as instrument                                                            Using Z2 as instrument
                       (Fraction of γ'ΣΖγ accounted for by Z1 = 2/3)                                   (Fraction of γ'ΣΖγ accounted for by Z2 = 4/15)
          1.0




                                                                                          1.0
          0.8




                                                                                          0.8
          0.6




                                                                                          0.6
  Power




                                                                                  Power
          0.4




                                                                                          0.4
          0.2




                                                                                          0.2
          0.0




                                                                                          0.0




                -1.5    -1.0     -0.5     0.0       0.5    1.0         1.5                      -1.5      -1.0    -0.5     0.0       0.5     1.0        1.5

                                          β IV, J                                                                          β IV, J


Note: Each plot shows the power for a hypothetical sample size of 500 varying β̄ keeping ΥJ fixed. The size of the test is 0.05. The instruments
                                      2                 2
are distributed normally, Z1 ∼ N (1, σZ1 ), Z2 ∼ N (1, σZ2 ) and Cov(Z1 , Z2 ) = σZ1,Z2 = 0.1; D = 1(Z1 + Z2 ≥ V ) so γ = (1, 1). The distribution
                                                                                                                     2                  2       1
of the index is held fixed and is distributed N (2, 3). The unobservables are jointly normally distributed with and σV = 1, σ10 = 0.5, σ1 = 1, σ0 =
1, σ1V = −0.5, σ0V = 0.5. In all panels, under the null hypothesis β̄ = 0.5, and alternative hypotheses are generated by changing β̄. The vertical
dashed line shows the value of βIV,J under the null hypothesis and the vertical dotted line shows the value of β̄ = β̄0 under the null hypothesis
being considered, ie. that βIV,J = β̄ + ΥJ .




                                                                             19
χ21 . Under the alternative, in the general case, the Wald statistic converges to a noncen-
tral chi-square distribution. Let h1 (·) and h2 (·) denote the weights (akin to hJ (·) above)
corresponding to J1 (Z) and J2 (Z), respectively. To simplify the notation, we suppress
the Z argument. Define J˜1 = J1 − J 1 and J˜2 = J2 − J 2 as the demeaned values of
the instruments. Let J̃1 = (J˜11 , . . . , J˜1I )0 and J̃2 = (J˜21 , . . . , J˜2I )0 be the matrices of de-
meaned instruments stacked across individuals. Let D = (D1 , . . . , DI )0 be the stacked val-
ues of the choice variable Di . Under random sampling, and the assumptions of Section 2,
J̃01 D   p          J̃02 D   p
   I
         → ω1 and     → ω2 for some finite constants ω1 and ω2 . Under HAIV : βIV,1 − βIV,2 =
                       I
hR                                    i √
   1
  0
     M T E(u D )(h (u
                  1 D  ) − h (u
                            2 D ))duD / I, the noncentrality parameter of the chi-square

distribution of the test statistic is

                                           Z       1                                           2
                                       1
                        λIV,1,2      =                  M T E(uD )(h1 (uD ) − h2 (uD ))duD           Ψ−1
                                                                                                      1,2              (11)
                                       2        0



where

                                                                   
                               Var (J1 ) 2 Cov(J1 , J2 ) Var (J2 )
                                     2
                Ψ1,2   =E α              −               +                                                             (12)
                                  ω12          ω1 ω2         ω22
                          Z1
                              2E(αβ | UD = uD ) + E(β 2 | UD = uD ) hΩ,J1 ,J2 (uD )duD
                                                                   
                        +
                             0
                            1                                  2
                            Z
                         −  M T E(uD )(h1 (uD ) − h2 (uD ))duD  .
                                 0


Defining J1∗ = J1 − E(J1 ) and J2∗ = J2 − E(J2 ), the weight hΩ,J1 ,J2 (·) is given by

                                     Z1 Z∞                   2
                                                J1∗ J2∗
             hΩ,J1 ,J2 (uD ) =                     −               f(J1 −J2 ),P (j1 − j2 , P (z)) d(j1 − j2 ) dP (z)
                                                ω1 ω 2
                                 uD −∞
                                         "                2                  #
                                              J1∗ J2∗
                             =E                  −              | P (Z) ≥ uD Pr(P (Z) ≥ uD ).20
                                              ω1 ω2




                                                                    20
 The derivation follows a logic similar to that used to derive (7).21 Notice that not only
will the difference in the IV estimands depend on the alternative under consideration, but
the variance of the difference between the IV estimators will also depend on the alternative
under consideration.
       We present this characterization of the variance in order to understand the properties of
tests of H0 based on IV estimators. This expression for the variance is not meant as a guide
for how to implement such tests. In practice the analyst would form the test statistic using
a standard estimator of the variance of the vector of IV estimates.
       In general, the weights presented above do not have simple analytical expressions. They
do in the case of a model with normal error terms with normally distributed instruments
and a linear index structure for the choice equation. However, for this case, the proposed IV
test has no power, because, and as previously discussed and as established in Appendix B,
in this case βIV,J1 ≡ βIV,J2 irrespective of the truth or falsity of H0 . For this case, the
noncentrality parameter of the asymptotic chi-square distribution of the test statistic will be
zero so the power of the test equals its size. To have a test with any power, we have to rule
out instruments with equal weights. Since the weights can be constructed from the data on
Z, it is possible to check this condition in any sample.22
       We do not formally analyze conditions that guarantee that the two instruments J1 and
J2 , constructed from Z, optimize the power function of the test. From the expression for
the noncentrality parameter, one can see the ingredients required to construct an asymp-
totically most powerful test. Let Z ∈ Rk be the vector of available instruments and let
     
J = J | J : Rk → R be the space of functions which map the vector of instruments to the
  20
     f(J1 −J2 ),P (j1 − j2 , P (z)) is the joint density of J1 − J2 , and P (Z) evaluated at J1 − J2 = j1 − j2 and
P (Z) = P (z).
  21
     The logic is not, however, identical. Using (J1 − J2 ) as an instrument and testing if βIV,J1 −J2 = 0 is not
equivalent to the test presented in the main text of the paper. The denominators of the IVs differ in the two
approaches.
  22
     It would be desirable to develop a formal test for equality of the two IV weights. The required ingredients
are in the literature. We leave the formal derivation for another occasion.




                                                       21
real line. Then for a given MTE, the optimal choice of J1 and J2 solves the problem

                                       Z   1                                        2
                                   1
                       max                      M T E(uD )(h1 (uD ) − h2 (uD ))duD        Ψ−1
                                                                                           1,2 .
                    J1 ∈J ,J2 ∈J   2    0



The optimal choice of instruments will generally depend on the shape of the MTE(uD ).23
       We present an example with two nonnormal instruments in Figure 4. Specifically, let
D = 1(γ1 Z1 + γ2 Z2 ≥ V ) where the vector Z = (Z1 , Z2 ) is distributed as a multivariate
mixture of normals with the distribution given at the base of the figure. The unobservables
are assumed to be generated by a normal generalized Roy model. The test of equality of the
IV estimators constructed using these two instruments has power to detect deviations from
H0 . Figure 4A plots the weights h1 (·) and h2 (·) which the IV estimator places on the MTE,
using Z1 or Z2 respectively. The weights must differ for the test based on the difference in
IV estimators to have power to detect deviations from H0 . When the mixing proportion in
the mixture of normals is 0.45, the instruments are highly nonnormal and the IV weights
differ substantially. However, when the mixing proportion is 0.75, the instruments become
closer to normal, the weights become very similar, and the test of H0 loses power, as we
demonstrate more systematically in Section 5 below.
       Another example of a test that has power to detect deviations from H0 , even with normal
instruments, constructs IV estimators using nonlinear functions of Z. We consider a normal
generalized Roy model where there is one Z variable in the choice equation that is normally
distributed, D = 1(Z ≥ V ). We plot the weights of the IV estimators based on Z and Z 2 .
Figure 4B plots the weights for these two choices of instruments. The weights differ, and in
addition the amount by which they differ generally depends on the distribution of Z. We
plot the weights for two choices of the mean of Z presented in the figure. These choices
clearly affect the weights and hence will generally affect the power of a test of H0 based on
these IV estimators.
  23
    More generally, one could use multiple instruments and base a test on multiple contrasts of the set of
instruments. We do not develop this test in this paper.



                                                           22
                      Figure 4: IV weights for alternative choices of the instrument.
                                                                          A. Z 1 vs. Z 2, mixtures of normals
                                                p mix = 0.45                                                                                 p mix = 0.75




                                                                                                                1.5
                                                  Z2
          1.5




                            Z1
                                                                                                                            Z1




                                                                                                                1.0
 Weight




                                                                                                       Weight
          1.0




                                                                                                                                                Z2




                                                                                                                0.5
          0.5
          0.0




                                                                                                                0.0
                0.0          0.2          0.4            0.6              0.8          1.0                            0.0        0.2   0.4           0.6       0.8        1.0
                                                  uD                                                                                           uD


                                                                                             B. Z vs. Z 2
                                                 E(Z) = 1                                                                                E(Z) = -0.5




                                                                                                                2.5
                                                                                                                                              Z2
          2.5




                                                                      2
                                                                  Z




                                                                                                                2.0
          2.0




                                                                                                                                                                     Z
 Weight




                                                                                                       Weight
                                                                                                                1.5
          1.5




                                   Z


                                                                                                                1.0
          1.0




                                                                                                                0.5
          0.5




                                                                                                                0.0
          0.0




                0.0          0.2          0.4            0.6              0.8          1.0                            0.0        0.2   0.4           0.6       0.8        1.0
                                                  uD                                                                                           uD


                                                                      C. P(Z) above and below the median
                                                 E(Z) = 0                                                                                     E(Z) = 1
          3.0
          2.5




                                                                                                                8




                                                                                                                                                            P(Z)
          2.0
 Weight




                                                                                                       Weight
                                                                                                                6




                                                                                                                            P(Z)                            above
          1.5




                                 P(Z)                                 P(Z)                                                  below
                                                                                                                4
          1.0




                                 below                                above
                                                                                                                2
          0.5
          0.0




                                                                                                                0




                0.0          0.2          0.4            0.6              0.8          1.0                            0.0        0.2   0.4           0.6       0.8        1.0
                                                  uD                                                                                           uD

                                                                            D. P(Z) separated by quartiles
                                                 E(Z) = 0                                                                                     E(Z) = 1
                                                                                                                30
          6




                                                                                                                                                                     P4
                                                                                                                25
          5




                                                                                                                20
          4
 Weight




                                                                                                       Weight




                                                                                                                                                               P3
                                                                                                                15
          3




                       P1                P2                                     P4                                                                    P2
                                                            P3                                                               P1
                                                                                                                10
          2
          1




                                                                                                                5
          0




                                                                                                                0




                0.0          0.2          0.4            0.6              0.8          1.0                            0.0        0.2   0.4           0.6       0.8        1.0
                                                  uD                                                                                           uD

Note: Panel A plots the weights of IV estimates constructed using either Z 1 or Z2 as an instrument where (Z1, Z2) is distributed as a
multivariate mixture of normals, with D = 1(γ1Z1 + γ2Z2 ≥ V). To construct these results, we assume
                                                ⎛Z ⎞              ⎛ -0.8 ⎛ 1.4 0.5 ⎞ ⎞                      ⎛ -0.8 ⎛ 0.6 -0.3 ⎞ ⎞
                                                ⎜ 1⎟ ∼ p     × N ⎜⎜
                                                ⎜Z ⎟     mix
                                                                        ,⎜         ⎟ ⎟⎟ + (1 − p mix ) × N ⎜⎜     ,⎜          ⎟ ⎟⎟
                                                ⎝ 2⎠              ⎝ 1 ⎝ 0.5 1.4 ⎠ ⎠                         ⎝ 1 ⎝ -0.3 0.6 ⎠ ⎠
and the coefficients in the choice equation are γ1=0.2, γ2=1. In the left plot of Panel A we let p mix = 0.45 and in the right plot p mix = 0.75.
Panel B plots the weights of IV estimates constructed using either Z or Z 2 as an instrument where Z ~ N(µZ,1), µZ = 1 or µZ = -0.5, and D =
1(Z ≥ V). Panel C plots the weights of IV estimates constructed using either P(Z) below the median or P(Z) above the median as instruments.
Panel D plots the weights of IV estimates constructed using P(Z) in different quartiles of its distribution as instruments. In Panels C and D, Z
~ N(µZ,1), µZ = 0 or µZ = 1, and D = 1(Z > V). In all of the plots, we set σV2 = 1.




                                                                                                 23
   Another choice of instruments uses P (Z) on disjoint intervals of the support of P (Z) as
two instruments. Form two disjoint intervals [p1 , p1 ] and [p2 , p2 ], and construct IV estimators
over these intervals as sample analogs to
                                                                          
                                          Cov Y, P (Z) | P (Z) ∈ [p1 , p1 ]
                        βIV,(p1 ,p1 )   =                               
                                           Var P (Z) | P (Z) ∈ [p1 , p1 ]


and
                                                                          
                                          Cov Y, P (Z) | P (Z) ∈ [p2 , p2 ]
                        βIV,(p2 ,p2 )   =                               
                                           Var P (Z) | P (Z) ∈ [p2 , p2 ]


and test


                                    H0IV   : βIV,(p1 ,p1 ) = βIV,(p2 ,p2 )

                                    HAIV   : βIV,(p1 ,p1 ) 6= βIV,(p2 ,p2 ) .


There is no a priori guidance on which intervals to use so we consider two ways to construct
intervals over which to form IV estimates: (1) use the intervals [0, pmed ] and [pmed, 1] where
pmed is the sample median of P (Z), and (2) use the intervals [0, pq1 ], [pq1 , pq2 ], [pq2 , pq3 ]
and [pq3 , 1], where pqj is the jth sample quartile of the distribution of P (Z) and form all
pairwise contrasts between these estimates. Note that even though we split the propensity
score into four intervals, we are still conducting pairwise tests. However, because there is a
multiplicity of pairwise tests, we must control the size of the test. We do this by using the
stepdown procedure of Romano and Wolf (2005). Figures 4C and 4D plot the weights for the
instruments constructed in this manner. These weights are nonoverlapping by construction
and will also depend on the distribution of the instrument Z.
   The power of the test of H0 based on IV estimators also depends on the variance (12),
which determines the denominator of the noncentrality parameter. The important terms


                                                     24
which are affected by the choice of instruments are the variance of the difference in the
            h                                    i
instruments Var(J
               ω2
                  1)
                     − 2 Cov(J1 ,J2 )
                           ω1 ω2
                                      + Var(J2 )
                                          ω2
                                                   and the variance weight hΩ,J1 ,J2 (·). The variance
                  1                       2

of the difference in the instruments is identified from the distribution of Z given X. The
weights hΩ,J1 ,J2 (·), can also be estimated from the data but are less transparent. For each of
the examples presented in Figure 4, we plot the variance weights hΩ,J1 ,J2 (·). In the case of
the normal generalized Roy model, the weights are more intuitive and more easily calculated
when conditioning directly on V = v (rather than UD = uD ), so we plot them as a function
of v. Figure 5 plots the variance weights. Ceteris paribus, the larger the variance weights,
the larger is the variance of the difference in the IV estimators and hence the lower the
power of a test based on this difference. In Panel A of Figure 5 we see that when the
mixing proportion is 0.45 the variance of the difference in the estimators is higher than when
the mixing proportion is 0.75 due to the fact that the IV weights covary highly when the
instruments are closer to normal so the variance of their difference is smaller. In Panel B, the
variance weights are roughly similar for E(Z) = 1 and E(Z) = −0.5. Finally, in Panel C the
variance weights are much larger when E(Z) = 1 than when E(Z) = 0. This demonstrates
that even when the IV weights are nonoverlapping, as is the case in both examples in Panel
C, the variance of the difference in the IV estimators will generally depend on the distribution
of Z.
    We emphasize that the specific comparisons of IV estimators presented in this section are
illustrative examples. Our formal analysis is completely general and allows for any choice of
valid instruments which satisfy (A-1)–(A-5).



4       Testing H0 by Testing for Linearity

We next consider tests of H0 based on linearity in p. Keeping the conditioning on X implicit,
we can write (3) as
                                   E(Y | P (Z) = p) = µ + g(p)                                   (13)



                                                 25
Figure 5: IV variance weights (hΩ,J1 ,J2 (·)) as a function of V = v for alternative choices of
instruments.

                                                           A. Z 1 vs. Z 2, mixtures of normals
                                 p mix = 0.45                                                                               p mix = 0.75
          120
          100




                                                                                                   15
          80
 Weight




                                                                                          Weight
                                                                                                   10
          60
          40




                                                                                                   5
          20
          0




                                                                                                   0
                -4    -2            0                  2                 4                               -4            -2     0            2   4
                                    v                                                                                         v

                                                                             B. Z vs. Z 2
                                   E(Z) = 1                                                                                 E(Z) = -0.5
          6




                                                                                                   6
          5
 Weight




                                                                                          Weight
          4




                                                                                                   4
          3
          2




                                                                                                   2
          1
          0




                                                                                                   0




                -4    -2            0                  2                 4                               -4            -2     0            2   4
                                    v                                                                                         v

                                                        C. P(Z) above and below the median
                                   E(Z) = 0                                                                                  E(Z) = 1
          15




                                                                                                   150
          10




                                                                                                   100
 Weight




                                                                                          Weight
          5




                                                                                                   50
          0




                                                                                                   0




                -4    -2            0                  2                 4                               -4            -2     0            2   4
                                    v                                                                                         v


Note: Panel A plots the variance weights of the difference in the IV estimates constructed using either Z1 or Z2 as an instrument where (Z1, Z2)
is distributed as a multivariate mixture of normals, with D = 1(γ1Z1 + γ2Z2 ≥ V). To construct these results, we assume
                                  ⎛Z ⎞              ⎛ -0.8 ⎛ 1.4 0.5 ⎞ ⎞                      ⎛ -0.8 ⎛ 0.6 -0.3 ⎞ ⎞
                                  ⎜ 1⎟ ∼ p     × N ⎜⎜
                                  ⎜Z ⎟     mix
                                                          ,⎜         ⎟ ⎟⎟ + (1 − p mix ) × N ⎜⎜     ,⎜          ⎟ ⎟⎟
                                  ⎝ 2⎠              ⎝  1   ⎝ 0.5 1.4 ⎠  ⎠                     ⎝ 1 ⎝ -0.3 0.6 ⎠ ⎠
and the coefficients in the choice equation are γ1=0.2, γ2=1. In the left plot of Panel A we let p mix = 0.45 and in the right plot p mix = 0.75.
Panel B plots the variance weights of the difference in the IV estimates constructed using either Z or Z2 as an instrument where Z ~ N(µZ,1),
µZ = 1 or µZ = -0.5, and D = 1(Z ≥ V). Panel C plots the variance weights of the difference in the IV estimates constructed using either P(Z)
below the median or P(Z) above the median as instruments. In Panel C, Z ~ N(µZ,1), µZ = 0 or µZ = 1, and D = 1(Z ≥ V). In all of the plots,
we set σV2 = 1.




                                                                                    26
for some general nonlinear function g(·) where µ and g may depend on X. Our test for the
absence of selection on the gain to treatment is a test of whether the function g(·) belongs
to the linear parametric family F = {a + bp, (a, b) ∈ R2 }. Let P be the support of P (Z),
with typical element p ∈ P. The null hypothesis of linearity can be written as


            H0L : There exists some (a, b) ∈ R2 such that g(p) = a + bp for almost all p ∈ P,


while the alternative is


            HAL : There exists no (a, b) ∈ R2 such that g(p) = a + bp for almost all p ∈ P.


There is a large and still unsettled literature in econometrics and statistics dealing with spec-
ification tests of this type.24 These tests proceed in one of two ways: (i) testing orthogonality
restrictions implied by the parametric model, or (ii) comparing a nonparametric estimate of
g(p) with a parametric estimate, â + b̂p. We implement and explore the properties of both
types of tests and briefly discuss a third test due to Li and Nie (2007).


Linearity Test 1: Wald Test Based on Series

The first test of linearity of E(Y |P (Z) = p) in p determines whether terms in addition to
p are required to fit the data. It is instructive to consider the case of the normal selection
model as a baseline. When the data are generated from the normal generalized Roy model,
we can characterize E(Y |P (Z) = p) by

                                                            Z    p
                         E(Y |P (Z) = p) = ᾱ + β̄p + τ              Φ−1 (uD )duD .
                                                             0



Figure 6 plots E(Y |P (Z) = p) for alternative values of τ . This figure provides, in addition to
the plots of E(Y |P (Z) = p), the R2 of a regression of E(Y |P (Z) = p) on a linear term in p as
  24
   See, e.g., Horowitz and Spokoiny (2001) and the references therein. The properties of particular tests
depend on the specification of alternatives.



                                                   27
well as the R2 after adding a quadratic term in p. The R2 will depend on the distribution of
the propensity score (the regressor). We present the R2 for two different distributions of the
propensity score. In column A of Figure 6 the distribution of the propensity score is uniform,
while in column B the propensity score is concentrated in one half of the unit interval. When
the propensity score is concentrated in one part of the unit interval, a linear function of p
is a very good approximation to E(Y |P (Z) = p). Note, however, that no matter what the
distribution of the P (Z), a quadratic function is able to closely approximate E(Y |P (Z) = p).
This suggests that for a normal alternative one can use a quadratic function in p to estimate
E(Y |P (Z) = p).
       In the general non-normal case, we use polynomials to approximate classes of smooth
alternatives for the function g(·). We estimate E(Y |P (Z) = p) using polynomials of degree
2 or higher. Polynomials approximate well a broad class of functions. Exploring power in
this class gives us an indication of the power of our procedures against such alternatives.25
Our specification of a more general, but still parametric, alternative model is

                                                   L
                                                   X
                                      g(P (Z)) =          φl (P (Z))l ,
                                                    l=0


where L is assumed to be known.26 Our test for linearity is


                             H0 : φl = 0 for l = 2, . . . , L

                            HA : φl 6= 0 for some (or all) l = 2, . . . , L.


       We fit models with many different choices for the degree of the polynomial. We interpret
a rejection of linearity in any of these models as a rejection of the null hypothesis of linearity.
However, we need to use caution in constructing the critical values for our test statistics.
If we conduct tests for linearity in each model separately, as we add more tests (i.e., more
  25
     Ichimura and Todd (2007) discuss the properties of series estimators. Newey (1997) establishes conver-
gence rates and proves asymptotic normality of such estimators.
  26
     Below, we discuss a procedure when L is unknown.


                                                    28
Figure 6: E(Y |P ) (the solid line) and the distribution of propensity scores (the dashed line)
in the normal generalized Roy model.
                                               A.                                                                                             B.
                                           τ = -1.4                                                                                         τ = -1.4
           6




                                                                                                              6
                                                                                    0.6




                                                                                                                                                                                0.6
           5




                                                                                                              5
                               2                                                                                                  2




                                                                                    0.4




                                                                                                                                                                                0.4
                        2 R linear =
                                     0.11098                                                                                 2 R linear =
                                                                                                                                          0.92354
                       R quadratic = 0.99589                                                                                R quadratic = 0.99873
           4




                                                                                                              4
                                                                                    0.2




                                                                                                                                                                                0.2
 Density




                                                                                                    Density
                                                                                           E(Y|P)




                                                                                                                                                                                       E(Y|P)
           3




                                                                                                              3
                                                                                    0.0




                                                                                                                                                                                0.0
           2




                                                                                                              2
                                                                                    -0.2




                                                                                                                                                                                -0.2
           1




                                                                                                              1
                                                                                    -0.4




                                                                                                                                                                                -0.4
           0




                                                                                                              0
               0.0      0.2          0.4              0.6      0.8          1.0                                   0.0       0.2       0.4              0.6   0.8         1.0

                                     Propensity Score                                                                                 Propensity Score



                                           τ = -0.6                                                                                         τ = -0.6
           6




                                                                                                              6
                                                                                    0.6




                                                                                                                                                                                0.6
           5




                                                                                                              5




                               2                                                                                                  2
                                                                                    0.4




                                                                                                                                                                                0.4
                        2 R linear =
                                     0.4133                                                                                  2 R linear =
                                                                                                                                          0.87245
                       R quadratic = 0.99726                                                                                R quadratic = 0.99773
           4




                                                                                                              4
                                                                                    0.2




                                                                                                                                                                                0.2
 Density




                                                                                                    Density
                                                                                           E(Y|P)




                                                                                                                                                                                       E(Y|P)
           3




                                                                                                              3
                                                                                    0.0




                                                                                                                                                                                0.0
           2




                                                                                                              2
                                                                                    -0.2




                                                                                                                                                                                -0.2
           1




                                                                                                              1
                                                                                    -0.4




                                                                                                                                                                                -0.4
           0




                                                                                                              0




               0.0      0.2          0.4              0.6      0.8          1.0                                   0.0       0.2       0.4              0.6   0.8         1.0

                                     Propensity Score                                                                                 Propensity Score



                                               τ=0                                                                                           τ=0
           6




                                                                                                              6
                                                                                    0.6




                                                                                                                                                                                0.6
           5




                                                                                                              5




                               2                                                                                                  2
                                                                                    0.4




                                                                                                                                                                                0.4
                        2 R linear =
                                     1                                                                                       2 R linear =
                                                                                                                                          1
                       R quadratic = 1                                                                                      R quadratic = 1
           4




                                                                                                              4
                                                                                    0.2




                                                                                                                                                                                0.2
 Density




                                                                                                    Density
                                                                                           E(Y|P)




                                                                                                                                                                                       E(Y|P)
           3




                                                                                                              3
                                                                                    0.0




                                                                                                                                                                                0.0
           2




                                                                                                              2
                                                                                    -0.2




                                                                                                                                                                                -0.2
           1




                                                                                                              1
                                                                                    -0.4




                                                                                                                                                                                -0.4
           0




                                                                                                              0




               0.0      0.2          0.4              0.6      0.8          1.0                                   0.0       0.2       0.4              0.6   0.8         1.0

                                     Propensity Score                                                                                 Propensity Score




                2                          2                                                                            2             2
Note: The R linear is calculated as the R of a regression of the true E(Y|P) on a linear term in P(Z). The R quadratic is the R of a regression of the true E(Y|P) on a quadratic
polynomial in P(Z). In column A, the single instrument Z is distributed N(0, 1), the unobservable in the choice equation, V, is distributed N(0, 1) and the choice equation is given by
D = 1(Z ≥ V), which results in uniformly distributed propensity scores. In column B, the single instrument Z is distributed N(1, 0.1), V is distributed N(0, 1) and the choice equation
is D = 1(Z ≥ V), which results in the shifted distributed of propensity scores shown.




                                                                                           29
polynomials for series estimators of ever higher degree), we would, in general, increase the
probability of type I error. However, analogous to the results in the literature on multiple
hypothesis testing, we construct critical values that allow us to control the probability of
type I error.
       Specifically, suppose that we estimate the function g(·) using M (= L−1) different models
corresponding to adding additional polynomial terms. We seek to use the information from
all of these estimators to test for the linearity of g(·). We have statistics for the tests of
linearity for each of those models separately. Call them T1 , . . . , TM . For each model, we
know the asymptotic distribution under the null hypothesis. Because in our case these test
statistics will not in general have the same asymptotic distribution under the null, to make
them comparable, we convert them into p-values, denoted q1 , . . . , qM , which are distributed
unit uniform under the null. Since we are looking for a deviation from linearity in any of
the models, the only p-value that will be relevant for this decision will be the smallest one,
namely
                                           q∗ =      min     {qm }.
                                                  m∈{1,...,M }


This will be the p-value corresponding to the most significant test statistic. To make the size
of the test 0.05, one cannot simply compare q ∗ to 0.05. To obtain the distribution of this
statistic we use a bootstrap procedure developed in Romano and Wolf (2005) and Romano
and Shaikh (2006). Their procedure works in this application because the test used in this
paper can be viewed as the first step in their “stepdown” procedure.27,28 However, in the
results presented below we do not report stepdown p values for the remaining hypotheses
because we are only interested in testing the first round null hypothesis that no higher order
polynomial in p enters (14).
       The Web Appendix, Section 7, presents a detailed description of the testing procedure
  27
     Notice that we are not testing individual coefficients for the M different polynomials, but an entire class
of polynomials of order 2 or higher, up to order k ≤ M + 1.
  28
     Alternatively we could use a χ2 test (or F -test in small samples) of the hypothesis that the coefficients
associated with the polynomials of order two and higher are all zero.



                                                      30
used in this paper and shows how we construct the critical value against which to compare
q ∗ . In our applications of series estimators, we use four polynomials in P (Z) of increasing
degree from degree 2 to degree 5 (M = 4). In simulations available on our website, we
confirm that this procedure is able to control the size of the test.29,30
       In order to examine the power of this test in detecting deviations from H0 , for simplicity
we will consider the test which fits a quadratic polynomial to E(Y |P (Z) = p) and tests
whether the coefficient on the quadratic term is zero. We test the significance of this coeffi-
cient using a Wald test. Again, we use the normal generalized Roy model as a simple base
case. As shown in Figure 6, a quadratic polynomial very closely approximates the function
R P (Z) −1
 0
       Φ (t)dt and therefore we expect this test to have power to detect deviations from H0
  29
     In the Web Appendix, we present simulations summarized in Figures A11 and A12, which are discussed
below. They show that for a variety of configurations of the parameters, under the null hypothesis our
procedure never rejects more than 5% of the time at the 0.05 level. See http://jenni.uchicago.edu/
testing_random/.
  30
     To implement the test, the econometrician would also need to account for conditioning variables X,
which we have thus far kept implicit, and for the estimation of propensity scores, P (Z). Many of the X
variables that we use are categorical or binary. For X variables that are categorical we suggest stratifying the
data on X and perform tests within X cells. However, when the cells are too thin to allow the stratification
on X and still expect to have reasonable power in small samples, we instead suggest incorporating the
conditioning variables X as linear regressors and estimate an alternative specification:
                                                                   L
                                                                   X
                             Yi = Xi δ0 + Xi (δ1 − δ0 )P (Zi ) +         φl P (Zi )l + εi ,                (14)
                                                                   l=1

where it is assumed that E(εi | Xi , Zi ) = 0. The rationale for the interaction between X and P (Z) arises
from noting that

              E(Y |P (Z) = p, X = x)
                = E(α|P (Z) = p, X = x) + E(βD|P (Z) = p, X = x)
                               
                = ᾱ(x) + β̄(x)p + E(Uα |P (Z) = p, X = x) + E(Uβ |D = 1, P (Z) = p, X = x)p
                = xδ0 + x(δ1 − δ0 )p + κ(p)

where κ(p) = E(Uα | P (Z) = p) + E(Uβ | D = 1, P (Z) = p)p. The last equality comes from independence
assumption (A-1) presented in Section 2 and the assumption that α and β are linear functions of X.
  When estimating κ(p) using polynomials in p one can simply regress Y on X, X × P (Z) and polynomials
in P (Z). We do this in our example presented in Section 6. We estimate P (Z) using a probit model. Our
results are robust to alternative specifications of the choice equation.




                                                      31
in this model.31 We estimate the following specification:


                                     Yi = α + βP (Zi ) + η[P (Zi )]2 + εi .


Under the null hypothesis η = 0, the Wald test statistic formed using the least squares
estimator η̂ is asymptotically distributed as a χ21 . Under the alternative, the Wald statistic
will converge to a noncentral chi-square distribution. Let gi = (1, P (Zi ), [P (Zi )]2 ) be the
row vector of regressors for individual i and G = (g1 , . . . , gI )0 denote the matrix of regressors
                                                                                                        G0 G   p
stacked across individuals. Let c = [0 0 1]. Under our conditions,                                       n
                                                                                                               → Γ. In the normal
                                                                    τA
generalized Roy model, under alternative τ =                        √
                                                                      I
                                                                        ,    the noncentrality parameter of the chi-
square distribution of the test statistic is

                                                 1                −1
                                          λquad = η̃ 2 σε2 cΓ−1 c0                                                           (15)
                                                 2

where                                           hR                   i           
                                                       P (Z)    −1              2
                                     τA Cov           0
                                                               Φ (t)dt , [P (Z)]⊥
                                                                                      ⊥
                              η̃ =
                                                          Var([P (Z)]2⊥ )

and the subscript      ⊥   denotes the residuals of a variable after projecting it onto a linear
function of P (Z).32 In Section 5 we examine the power of this test and its determinants,
taking into account that in applications P (Z) is estimated in a first stage.
  31
     In the Web Appendix we derive the power of a direct test of τ = 0 using a correctly specified E(Y |P (Z) =
p) function.
  32
     To justify the expression for η̃, note that we can write
                                            Z   P (Zi )
                   Yi = α + βP (Zi ) + τA                 Φ−1 (t)dt + εi
                                            0
                                                           " Z                                                      #
                                                                        P (Zi )
                                                    2                             −1                       2
                     = α + βP (Zi ) + η[P (Zi )] + τA                             Φ       (t)dt − η[P (Zi )] + εi
                                                                    0


where the term in brackets is the error term. The true η is equal to 0 but plim η̂ = η̃ 6= 0 due to an omitted
variable bias. This bias is given by the formula in the text and is derived from first residualizing on P (Zi )
and then solving for the probability limit of η̂.




                                                               32
Linearity Test 2: Bierens Conditional Moment Test

We also consider a test of the validity of representation (3) which relies on orthogonality
restrictions implied by the parametric model. We use the conditional moment (CM) test of
Bierens (1990).33 This test uses the fact that under the null hypothesis the following moment
condition must be satisfied


                                  E[Y − a0 − b0 P (Z) | P (Z)] = 0


for the true parameter vector (a0 , b0 ) ∈ R2 . This conditional moment restriction implies the
set of unconditional moment restrictions


                             E[(Y − a0 − b0 P (Z)) exp(t0 Λ(P (Z)))] = 0                         (16)


for all t ∈ R, for some bounded one-to-one, mapping Λ from R into R. A test can be
constructed using the sample analog of the left-hand side of (16). Bierens (1990) shows how
one can use sample analogs to construct a test statistic which, under the null hypothesis,
converges in distribution to a χ21 and under the alternative diverges to infinity.
       In analyzing the power of this test, we first consider the power of a test using the sample
analog of (16) for a single t in the context of the normal generalized Roy model and then
discuss how to generalize to the test we actually use. For a single t, let Q̂(t) denote the sample
                                                                        √
analog of the moment condition (16). Bierens (1990) shows that I Q̂(t) is asymptotically
normal and under the null has mean zero. Denote its asymptotic variance under the null by
σ 2 (t), with estimator σ̂ 2 (t). Under H0 , the test statistic, I(Q̂(t))2 /σ̂ 2 (t) is asymptotically
distributed as a central χ21 . Let g(P (Z), θ0 ) = a0 + b0 P (Z) denote the parametric function
we are testing. Note that the gradient of this function with respect to the parameter vector
∂g(P (Z),θ)
    ∂θ
              = [1, P (Z)]0 does not depend on the point of evaluation of θ. Consider a local
  33
    See also Bierens (1982) and Bierens and Ploberger (1997) for related tests. Newey (1985) discusses
conditional moment tests more generally.



                                                 33
                    √
alternative τ = τA / I in a normal generalized Roy model. Under this alternative, the test
statistic is distributed asymptotically as a noncentral chi-square with one degree of freedom
and noncentrality parameter

                                                        1
                                               λCM (t) = (QA (t))2 [σA2 (t)]−1 .34                                       (17)
                                                        2

This expression gives the asymptotic power of a test for a single choice of t.                                  The Bierens
test maximizes the test statistic over t. Under the alternative, the test statistic is not a
simple noncentral chi-square. We explore the power of this test using simulation.35


All of the Preceding Tests are Conditional Moment Tests36

This paper seeks to test if the outcome equation is generated by a model of the form


                                                     E(Y | P (Z)) = a + bP (Z)


which is equivalent to
                                              E [h(P (Z)) [Y − a − bP (Z)]] = 0
  34

             "                                                                         !                 #
                                     Z   P (Z)
                                                         −1                                    0
QA (t) = E       ᾱ + β̄P (Z) + τA                   Φ        (uD )duD + ε − g(P (Z), θ̂) exp(t Λ(P (Z)))
                                     0
                                                                                      !2                                               
                                     Z       P (Z)                                                                                  
                                                                                                                                1
 2
σA (t) = E  ᾱ + β̄P (Z) + τA                       Φ−1 (uD )duD + ε − g(P (Z), θ̂)               0
                                                                                            × exp(t Λ(P (Z))) − ξA (t)Ω−1               
                                         0                                                                                    P (Z)
                                                "       0               #
              ∂                     0                   1          0
ξA (t) = E       g(P (Z), θA ) exp(t Λ(P (Z)) = E             exp(t Λ(P (Z))
             ∂θ0                                     P (Z)
                                                                  
              ∂                ∂                      1     P (Z)
    Ω=E          g(P (Z), θA ) g(P (Z), θA ) = E
             ∂θ0               ∂θ                   P (Z) [P (Z)]2

where θ̂ is the least squares estimate of θ in a regression of Y on a constant and P (Z).
  35
     We modify his test statistic for our applications because we need to account for the fact that the
propensity scores P (Z) are estimated in a first stage. Therefore, when we form the test statistic, we use an
estimate of the variance of the sample analog of (16) using a bootstrap procedure in which we reestimate
P (Z) in each sample, rather than an estimate based on the approximate asymptotic variance of (16) derived
in Bierens (1990).
  36
     We thank Edward Vytlacil for suggesting this unifying approach.



                                                                        34
for any function h. Conditional moment tests would typically use a vector of functions
h(P (z)) to construct tests.
   All of the tests previously discussed are based on different choices of h(P ). For the Bierens
test, we use h(P (Z)) = exp(t0 Λ(P (Z))). The test of linearity based on polynomials takes
                                               
h(P (Z)) = 1, P (Z), (P (Z))2 , . . . , (P (Z))L . The IV test can also be cast in this framework.
   The plim of the IV estimator obtained using Jk (Z), k = 1, . . . , K, as an instrument are
the values of (ak , bk ) that solve


                                      E [Jk (Z) [Y − ak − bk D]] = 0


and
                             E [Y − ak − bk D] = 0,         k = 1, . . . , K.

By the law of iterated expectations, this is equivalent to solving


                           E [Jk (Z) [E(Y | Z) − ak − bk P (Z)]] = 0

                                  E [E(Y | Z) − ak − bk P (Z)] = 0,


which is equivalent to solving


                               E [Jk (Z) [Y − ak − bk P (Z)]] = 0

                                         E [Y − ak − bk P (Z)] = 0.


For one instrument there is no test, but for two or more (K ≥ 2), one can test if a common
pair of (a, b) satisfies all of the moment conditions produced from using different instrumental
variables. This is the classical test of overidentification. Thus, all of the tests previously
discussed can be viewed as conditional moment tests.




                                                   35
Linearity Test 3: A Semiparametric Test Based on Local Linear Regression37

A potential problem with the test based on series estimators (Linearity Test 1) is that it
assumes that the degree of the highest order polynomial in P (Z) is finite and known. A
semiparametric approach that did not rely on strong functional form assumptions about the
generator model would be more desirable.
       Recently, Li and Nie (2007) have used local linear regression methods to develop a test
for linearity of an unknown parametric function in a semiparametric model. They develop a
test of linearity of the unknown nonparametric component (linearity in P (Z) in our setup)
that can be applied to the problem analyzed in this paper if it is adapted to the case of an
                                                                √
estimated P (Z). If P (Z) is parametric and its coefficients are N estimable, their analysis
can be applied directly. The case where P (Z) is estimated nonparametrically is left for
another occasion.
       Li and Nie (2007) conduct a Monte Carlo study of their approach. They show good
size and power properties for their test statistic. Their test can be interpreted as a local
conditional moment test.


Conditioning on X

Throughout, we have conditioned on X. An important practical problem not addressed in
this paper but common to all empirical models is picking the appropriate conditioning set,
and determining how to explicitly model the dependence of Y on X.



5        The Power of the Tests

We use the generalized Roy model as our base case because it allows for a simple param-
eterization of the correlation between β and D. We consider a more general polynomial
alternative in Section 5.4. Our results quantify the sample size needed to produce tests with
  37
   We thank Xiaohong Chen for directing us to this paper and clarifying our thinking about semiparametric
approaches to testing for linearity.


                                                   36
substantial power. We establish that the distribution of the propensity scores, P (Z), is an
important determinant of power.38


5.1     Normal Generalized Roy Model as the Data Generating Pro-

        cess

We simulate data from the generalized Roy model for a range of parameter values given
in Table 2. As a base case, we consider a scalar normal instrument Z, ie. Z ∼ N (0, σZ2 ).
However, we also analyze models with multiple instruments and a variety of distributions
for the instruments.

              Table 2: Initial Specification used to calculate the power of the tests.

            Outcomes                                     Decision Rule:
           Y0 = µ0 + U0                               D = 1(αD + γZ ≥ V )
           Y1 = µ1 + U1
                                     Observed Y = DY1 + (1 − D)Y0

       with parameters:                                  with parameters:
            µ0 = 0                                           αD = 0
           µ1 = 0.2                                           γ=1

                                  Distribution
                                               of Unobservables:
                                                    2
                                                                    
                               U1          0      σU     0      ρ1V
                                                          2
                              U0  ∼ N  0 ,  0       σU     −ρ1V 
                               V           0      ρ1V −ρ1V       1
                                    p
       where ρ1V = Cov(U1 , V )/ Var(U1 ) Var(V ) and we assume ρ0V = −ρ1V . We vary
        2
       σU  between 0.1 and 2. We consider values of ρ1V from −0.7 to 0.7. Values outside
       of this interval result in a covariance matrix that is not positive definite.

                                       Distribution of Observables:
                                                                  2
                                       Normal case: Z ∼ N (µZ , σZ  )
                                                      2
       We calculate the power function for values of σZ between 0.1 and 2 and µZ ∈ {0, 1}.

                                    Mixture of
                                                 Normals case:                            
         Z1                   −0.8      1.4 0.5                             −0.8     0.6 −0.3
               ∼ pmix × N           ,                 + (1 − pmix ) × N          ,
         Z2                     1       0.5 1.4                              1       −0.3 0.6
       We calculate the power function for values of pmix ∈ {0.45, 0.75, 0.95}.


  38
    We do not investigate the power of the Li and Nie (2007) test because they already conduct a Monte
Carlo study of their test for linearity.




                                                    37
   The parameterization in Table 2 makes what seem to be two fairly restrictive assumptions
about the covariance structure of the unobservables in the model. It assumes that ρ10 = 0,
that is, that the covariance between the two potential outcomes is zero, and ρ0V = −ρ1V .
These assumptions are not as restrictive as they may appear to be for calculating the power
of our tests. In the normal generalized Roy model, the nonconstancy of the MTE in uD (or
v) depends only on the term τ = ρ1V σ1 − ρ0V σ0 and so for a given value of that index, the
value of ρ10 does not affect the power of the test. The derivation of the power of the tests
shows that it depends on τ , the distribution of P (Z) and the precision with which we can
form our estimators (which will depend on the variances of the unobservables). We allow all
of these quantities to vary in our simulations.


5.2    The Power of the IV Tests

We investigate the following model without regressors:


                                  Y = µ0 + (µ1 − µ0 )D + ξ


where ξ = (U1 − U0 )D + U0 , and ξ ⊥
                                   ⊥ D. As explained in Section 3.1, we construct a test
                                   
based on the difference in IV estimators. We could potentially use any two IV estimators to
conduct this test. We do not derive the optimal pair or set of instruments that maximize
the power against a given alternative. We consider the three examples from Section 3.1: (i)
Z1 and Z2 , with at least one being non-normal; (ii) Z is scalar and we use Z and Z 2 , a
nonlinear function of the instrument; and (iii) P (Z) over distinct intervals of its support.
   Consider first the power of a test based on the equality of IV estimators using Z1 and Z2
as instruments, where (Z1 , Z2 ) is distributed as a mixture of bivariate normal distributions.
This test was discussed in Section 3.1, where we show the weights that each of the IV
estimators places on the MTE, as well as the variance weights of the difference between the
two estimators. Combining this information, we can, for alternative parameterizations of the


                                              38
normal generalized Roy model, calculate the power of a Wald test based on the difference
between these two IV estimators. Figure 7 plots the power functions for this test. Each of
the plots shows the power as a function of τ , which is a measure of deviation from the null
of a flat MTE in the generalized Roy model. The three plots each fix pmix at alternative
values, from 0.45 to 0.95. Moving from 0.45 to 0.75 the power of the test declines. When
pmix = 0.75, the IV weights are more similar than when pmix = 0.45 but the variance of
the difference in the instruments becomes smaller. As the mixing proportion approaches
1, and the instruments approach normality, the power of the test greatly diminishes. This
is predicted by our analysis that establishes the lack of power of normal instruments in a
normal generalized Roy model.
   We next examine the power of a test based on a nonlinear function of the single instrument
Z – that is using Z vs. Z 2 . In this case D = 1(Z ≥ V ) and Z ∼ N (µZ , σZ2 ). This test
was discussed in Section 3.1, where we plotted the weights of each of the IV estimators as
well as the variance weight for the difference between the two estimators. Figure 8 plots
the power of this test for different distributions of the instrument Z. We consider values of
µZ ∈ {−0.5, 1} and values of σZ2 ∈ {0.5, 1, 2}. The variance of the unobservable in the choice
equation, σV2 , is fixed at 1. The left column of Figure 8 plots the power for µZ = 1. For a
fixed µZ , the power of the test is increasing in σZ2 . The figures in the right column plot the
power for µZ = −0.5. The power is uniformly higher when µZ = −0.5 than when µZ = 1,
which is in accordance with the plots of the weights provided in Section 3.1.
   Finally, we consider a test of equality of IV estimates formed using observations with
P (Z) in different intervals of the support of P (Z). We separate the observations according
to whether they lie above or below the sample median of P (Z). As in the example considered
in Section 3.1, we let D = 1(Z ≥ V ) where Z ∼ N (µZ , σZ2 ). Figure 9 plots the power of this
test for alternative values of µZ ∈ {0, 1} and σZ2 ∈ {0.5, 1, 2}. As with the test based on IV
using Z and Z 2 , the power of this test is increasing in σZ2 . The power of the test is uniformly
lower when µZ = 1 than when µZ = 0 because when µZ = 1, one of the IV estimators places


                                               39
Figure 7: Power of a Wald test of the equality of IV estimators formed using Z1 and Z2 ,
mixtures of normals instruments, as a function of τ .
                                                                       p mix = 0.45




                                    1.0
                                    0.8
                                    0.6
                            Power
                                    0.4
                                    0.2
                                    0.0




                                      -1.5         -1.0         -0.5          0.0           0.5          1.0           1.5
                                                                               τ

                                                                       p mix = 0.75
                                    1.0
                                    0.8
                                    0.6
                            Power
                                    0.4
                                    0.2
                                    0.0




                                      -1.5         -1.0         -0.5          0.0           0.5          1.0           1.5
                                                                               τ

                                                                       p mix = 0.95
                                    1.0
                                    0.8
                                    0.6
                            Power
                                    0.4
                                    0.2
                                    0.0




                                      -1.5         -1.0         -0.5          0.0           0.5          1.0           1.5
                                                                               τ

                              1,000                               4,000                      7,000                       10,000

           Note: Standard errors are computed from the formulae developed in the text. Each line plots the
           power for a different sample size, as indicated by the legend. The data generating process is the normal
           generalized Roy model. The variance of the unobservables in the outcome equations is fixed at 1, as is
                                                           equation The choice equation is D = 1(γ1Z1 + γ2Z2 ≥
           the variance of the unobservable in the choice equation.
                                    ⎛Z ⎞             ⎛ -0.8 ⎛ 1.4 0.5 ⎞ ⎞                     ⎛ -0.8 ⎛ 0.6 -0.3 ⎞ ⎞
                                    ⎜ 1⎟ ∼ p     × N ⎜⎜
                                    ⎜Z ⎟     mix
                                                           ,⎜         ⎟ ⎟⎟ + (1 − p mix ) × N ⎜⎜    ,⎜          ⎟ ⎟⎟
                                    ⎝ 2⎠             ⎝ 1 ⎝ 0.5 1.4 ⎠ ⎠                        ⎝ 1 ⎝ -0.3 0.6 ⎠ ⎠

           and the coefficients in the choice equation are γ1=0.2, γ2=1. The power functions plotted are the power
           of a Wald test of the equality of IV estimates formed using Z1 and Z2 as instruments.




                                                                           40
Figure 8: Power of a Wald test of the equality of IV estimators formed using Z and Z 2 , as
a function of τ .
                                              2                                                                       2
                             μZ = 1, σZ = 0.5                                                          μZ = -0.5, σZ = 0.5
             1.0




                                                                                     1.0
             0.8




                                                                                     0.8
             0.6




                                                                                     0.6
     Power




                                                                             Power
             0.4




                                                                                     0.4
             0.2




                                                                                     0.2
             0.0




                                                                                     0.0
               -1.5   -1.0   -0.5       0.0           0.5   1.0      1.5               -1.5     -1.0    -0.5    0.0           0.5   1.0      1.5
                                        τ                                                                        τ

                             μ Z = 1, σZ2 = 1                                                          μ Z = -0.5, σZ2 = 1
             1.0




                                                                                     1.0
             0.8




                                                                                     0.8
             0.6




                                                                                     0.6
      ower




                                                                              ower
     Po




                                                                             Po
             0.4




                                                                                     0.4
             0.2




                                                                                     0.2
             0.0




                                                                                     0.0




               -1.5   -1.0   -0.5       0.0           0.5   1.0      1.5               -1.5     -1.0    -0.5    0.0           0.5   1.0      1.5
                                        τ                                                                        τ

                                                  2                                                                       2
                              μZ = 1, σZ = 2                                                           μZ = -0.5, σZ = 2
             1.0




                                                                                     1.0
             0.8




                                                                                     0.8
             0.6




                                                                                     0.6
     Power




                                                                             Power
             0.4




                                                                                     0.4
             0.2




                                                                                     0.2
             0.0




                                                                                     0.0




               -1.5   -1.0   -0.5       0.0           0.5   1.0      1.5               -1.5     -1.0    -0.5    0.0           0.5   1.0      1.5
                                        τ                                                                        τ


                                              1,000                4,000                      7,000            10,000
 Note: Standard errors are computed from the formulae developed in the text. Each line plots the power for a different sample size, as indicated
 by the legend. The data generating process is the normal generalized Roy model. The variance of the unobservables in the outcome equations is
 fixed at 1, as is the variance of the unobservable in the choice equation. The choice equation is D = 1(Z ≥ V). The single instrument Z is
 distributed N(μZ,σZ2). The power functions plotted are the power of a test of equality of the IV estimate formed using Z as an instrument and
                                    2
 the IV estimate formed using Z2 as an instrument.




                                                                      41
all of its weight on a relatively small segment of the MTE. In further examples presented
below, we show that, as expected, σZ2 is an important determinant of the power of all of the
tests we consider.
   In the next section, we analyze the power of an IV test which separates P (Z) across
distinct intervals of its support into quartiles of P (Z). This test considers any rejection of
pairwise equality in any of the comparisons as a rejection of the null. In order to control for
the size of the test, we use the stepdown procedure of Romano and Wolf (2005). We analyze
the performance of this test along with other test procedures.


5.3    Comparative Power

We now compare the power of IV-based tests to power for the first two tests of linearity
described in Section 4. The power is calculated for each test using a Monte Carlo procedure
described in the Web Appendix, Section 8. Figure 10 plots the power for all of the tests
analyzed in this paper.
   We start the discussion with the IV test. We use the IV estimators which separate
the observations across distinct intervals of the propensity score. We discuss two IV tests
constructed on this principle. They vary dramatically in their power. One has relatively
high power and the other relatively low power.
   The power of the test of the equality of the estimates using the propensity score above
and below the median as the instruments is given by the dotted line. The power of the
test separating the data by quartiles of the propensity score is given by the dot-dashed line.
Panel A of that figure fixes all of the parameters of the model and varies the sample size.
As expected, the test is consistent. The size of the departures from the null can be gauged
by noticing that with our parameterization, the restriction that the covariance matrix of the
unobservables is positive definite implies |τ | < 1.4. Therefore we can interpret a value of
τ = 0.7 as a large deviation from the null. The IV test based on separating the data into
smaller intervals and conducting more pairwise comparisons tends to have lower power than


                                              42
Figure 9: Power of a Wald test of the equality of IV estimators using P (Z) as an instrument
when the sample is separated by whether P (Z) lies above or below the median.
                                            2                                                                       2
                            μZ = 0, σZ = 0.5                                                           μZ = 1, σZ = 0.5
            1.0




                                                                                    1.0
            0.8




                                                                                    0.8
            0.6




                                                                                    0.6
    Power




                                                                            Power
            0.4




                                                                                    0.4
            0.2




                                                                                    0.2
            0.0




                                                                                    0.0
              -1.5   -1.0   -0.5      0.0           0.5   1.0       1.5               -1.5      -1.0   -0.5     0.0         0.5   1.0       1.5
                                      τ                                                                         τ

                                                2                                                                       2
                             μZ = 0, σZ = 1                                                            μZ = 1, σZ = 1
            1.0




                                                                                    1.0
            0.8




                                                                                    0.8
            0.6




                                                                                    0.6
    Power




                                                                            Power
            0.4




                                                                                    0.4
            0.2




                                                                                    0.2
            0.0




                                                                                    0.0




              -1.5   -1.0   -0.5      0.0           0.5   1.0       1.5               -1.5      -1.0   -0.5     0.0         0.5   1.0       1.5
                                      τ                                                                         τ

                                                2                                                                       2
                             μZ = 0, σZ = 2                                                            μZ = 1, σZ = 2
            1.0




                                                                                    1.0
            0.8




                                                                                    0.8
            0.6




                                                                                    0.6
    Power




                                                                            Power
            0.4




                                                                                    0.4
            0.2




                                                                                    0.2
            0.0




                                                                                    0.0




              -1.5   -1.0   -0.5      0.0           0.5   1.0       1.5               -1.5      -1.0   -0.5     0.0         0.5   1.0       1.5
                                      τ                                                                         τ


                                      1,000                     4,000                        7,000            10,000
Note: Standard errors are computed from the formulae developed in the text. Each line plots the power for a different sample size, as indicated
by the legend. The data generating process is the normal generalized Roy model. The variance of the unobservables in the outcome equations is
fixed at 1, as is the variance of the unobservable in the choice equation. The choice equation is D = 1(Z ≥ V). The single instrument Z is
distributed N(μZ,σZ2). The power functions plotted are the power of a test of equality of the IV estimate formed using P(Z) as an instrument
where the sample is separated according to whether P(Z) is above or below the median of P(Z).




                                                                      43
the test based on the estimates above and below the median. Although one can test more
distinct intervals, the estimates in each interval are less precisely estimated. In addition, the
IV test based on the estimates above and below the median has markedly higher power than
the series test, especially for small sample sizes.
   Panel B of Figure 10 plots the power of the test examining another dimension. These
plots hold the sample size fixed but change the “signal-to-noise ratio” which we define as

                                   Signal   Var(Zγ)  γ 2σ2
                                          =         = 2Z .
                                   Noise    Var(V )   σV

This parameter measures the predictive power of the instrument in determining the treatment
choice. A relatively weak instrument corresponds to a small value of the signal-to-noise ratio.
We can see from these plots that this ratio substantially affects the power of the test. In
particular, when the signal-to-noise ratio is low, the test is unable to detect very large
deviations from the null, even at a sample size of 4,000.
   The signal-to-noise ratio affects the power of the tests through the distribution of propen-
sity scores. A low signal-to-noise ratio results in very little dispersion in the propensity scores.
This implies that estimates of E(Y |P (Z) = p) will be based on a narrow range of values of
P (Z). This makes nonlinearity more difficult to detect. A linear function may be a good
approximation to the true function in a small neighborhood of g(p). Over a large stretch of
values of p, linearity is a good approximation to the normal MTE; see figure 6.
   As a reference, we plot the power of a test of H0 based on whether or not τ = 0 using
maximum likelihood in a normal generalized Roy model. The power of this test is given by
the heavy line. This test outperforms the other tests, as expected, but especially so when
the signal-to-noise ratio is low. This is because with a low signal-to-noise ratio, the range
of values of P (Z) is small. This small range degrades the power of the less parametric tests
but does not significantly hurt the parametric test which completely specifies the form of
the MTE.



                                                44
Figure 10: Power of the tests for selection on the gain to treatment as a function of τ .
                                                  A. Fixed signal/noise ratio; varying sample size a
                              1,000 Observations                                                                4,000 Observations
            1.0




                                                                                         1.0
            0.8




                                                                                         0.8
            0.6




                                                                                         0.6
    Power




                                                                                 Power
            0.4




                                                                                         0.4
            0.2




                                                                                         0.2
            0.0




                                                                                         0.0
              -1.5   -1.0   -0.5         0.0          0.5    1.0      1.5                  -1.5   -1.0        -0.5          0.0           0.5   1.0   1.5
                                          τ                                                                                  τ
                              7,000 Observations                                                               10,000 Observations
            1.0




                                                                                         1.0
            0.8




                                                                                         0.8
            0.6




                                                                                         0.6
    Power




                                                                                 Power
            0.4




                                                                                         0.4
            0.2




                                                                                         0.2
            0.0




                                                                                         0.0

              -1.5   -1.0   -0.5         0.0          0.5    1.0      1.5                  -1.5   -1.0        -0.5          0.0           0.5   1.0   1.5
                                          τ                                                                                  τ
                                                  B. Fixed sample size; varying signal/noise ratio b
                               Signal/noise = 0.1                                                                    Signal/noise = 0.5
            1.0




                                                                                         1.0
            0.8




                                                                                         0.8
            0.6




                                                                                         0.6
    Power




                                                                                 Power
            0.4




                                                                                         0.4
            0.2




                                                                                         0.2
            0.0




                                                                                         0.0




              -1.5   -1.0   -0.5         0.0          0.5    1.0      1.5                  -1.5   -1.0        -0.5          0.0           0.5   1.0   1.5
                                          τ                                                                                  τ
                                   Signal/noise = 1                                                                  Signal/noise = 2
            1.0




                                                                                         1.0
            0.8




                                                                                         0.8
            0.6




                                                                                         0.6
    Power




                                                                                 Power
            0.4




                                                                                         0.4
            0.2




                                                                                         0.2
            0.0




                                                                                         0.0




              -1.5   -1.0   -0.5         0.0          0.5    1.0      1.5                  -1.5   -1.0        -0.5          0.0           0.5   1.0   1.5
                                          τ                                                                                  τ

                             Normal test                    CM test         Series test                  IV test 1*               IV test 2**

   Note: The data are generated according to the normal generalized Roy model given in the text. The choice equation is D = 1(Z ≥ V) where
             2
   Z ~ N(0,σZ ). Standard errors are obtained from the bootstrap.
   a
     Panel A shows the power functions when fixing the signal to noise ratio at 1 (this means Var(Z γ)=1, compared to the variance of the
   unobservable in the choice equation of 1) and changing the sample size.
   b
     Panel B shows the power functions for the fixed sample size of 4,000, but changing the signal to noise ratio from 0.1 to 2. The signal to
   noise ratio is defined as Var(Zγ)/Var(V).
   * IV test 1 separates the observations into those with propensity scores in the interval [0,P median] and those with propensity scores in the
   interval [Pmedian,1] and testing the equality of the IV estimates based on those subsamples.
   ** IV test 2 separates the observations into quartiles of the propensity score. It tests for pairwise equality of the IV estimates calculated on
   each of those four subsamples and rejects the null if equality is rejected in any of the pairwise comparisons. It controls the size of the test
   using the method of Romano and Wolf (2005), as described in the text.

                                                                            45
       To investigate the relationship between the signal-to-noise ratio, the deviation from the
null hypothesis (in the form of τ ), the departure from linearity and the power of the tests,
we define the following R2 measure:39

                                   E [(E(Y |P (Z) = p) − E ∗ (Y |P (Z) = p))2 ]
                       1 − R2 =
                                             Var(E(Y |P (Z) = p))

where E ∗ (Y |P (Z) = p) is the linear projection of Y on P (Z). This quantity is one minus the
R2 from a regression of the true expectation E(Y |P (Z) = p) on a linear function in P (Z).
Under H0 , 1 − R2 = 0. Under the alternative, the magnitude of 1 − R2 is a measure of the
degree to which the alternative differs from the null. We use 1 − R2 as a summary statistic
of deviation from the null and plot the power of all of our tests as a function of it. It is a
useful summary statistic because it does not rely on the normality of the base model. We
use it for other specifications of the model which we discuss below.
       Figure 11 follows the same general format as Figure 10, but graphs power as a function
of 1 − R2 , rather than τ .40 Each plot presents only the right half of the corresponding plot
in figure 10 (the τ > 0 halves) because 1 − R2 is symmetric in τ . Panel A fixes the signal-to-
noise ratio and plots the power for different sample sizes. At small sample sizes, the series
test and the IV test using quartiles of the propensity score have the lowest power, but at
larger sample sizes, the IV test using quartiles has lower power than all of the other tests.41
       Panel B fixes the sample size and shows the power functions for different signal-to-noise
ratios. In these plots, the greater the departure of the null from linearity, the greater the
power of the tests. In addition, changing the signal-to-noise ratio changes 1 − R2 . Therefore,
to the extent that the power is different for different signal-to-noise ratios in Figure 10, those
differences arise due to differences in the distribution of propensity scores. That is, at a
  39
     Measuring deviations from the null in this fashion was suggested to us by Edward Vytlacil.
  40
     Although we define 1 − R2 as a population quantity, we use the sample analog, calculated from a sample
of 10,000. We simply want to choose a large number of observations in order to get a good estimate of the
distribution of the propensity score fP (Z) with which to calculate 1 − R2 .
  41
     At sample size 10,000, the power functions for the conditional moment test, the series test and the test
of the equality of the IV estimates above and below the median lie on top of each other.



                                                     46
given level of 1 − R2 , the signal-to-noise ratio has no effect on the power of the tests (up to
simulation error).42


5.3.1      Tests for Linearity

Test 1: Wald Test Based on Series Estimators

We next calculate the power of the test of the linearity of E(Y |P (Z) = p) in p which compares
the estimate based on a parametric (linear) estimator to a more general polynomial series
estimator.43 First, we calculate analytically the power of the test using only a quadratic
polynomial in p based on expression (15) in Section 4. This allows us to precisely determine
the power of the test across the dimensions we find to be important. Figure 12 plots the
power of this test at different sample sizes for fixed alternatives. Inspection of these power
functions shows that, as in the IV tests, the power is increasing in σZ2 . As expected, it is
decreasing in σ02 = σ12 = σU2 .
    We have shown that in the case of a model with normal errors, a quadratic polynomial
in P (Z) is able to approximate E(Y |P (Z) = p) well, and therefore will have high power
in detecting deviations from H0 . However, in the non-normal case, E(Y |P (Z) = p) is a
general function which may not be well-approximated by a quadratic polynomial. Therefore,
we construct a test which relies on fitting series estimators of E(Y |P (Z) = p) of varying
degrees.
    The test statistic is distributed as the maximum over a set of noncentral chi-square
  42
     In the Web Appendix, we plot additional figures which trace out the power of the specific IV tests more
thoroughly across the three dimensions which we find are important in this model – the deviation from the
null (as measured by τ ), sample size, and the signal to noise ratio. Figures A2 through A4 are plots for
the tests based on the IV estimates above and below the median and Figures A5 through A7 for the tests
based on IV estimates for separate quartiles. We also present the power of a test of whether the IV estimate
using only observations with propensity scores below the 40th percentile and the IV estimate using only
observations with propensity scores above the 60th percentile are equal in Figures A8 through A10. Using
more intervals and fewer observations in general reduces power.
  43
     Here we describe a procedure for a model that does not include conditioning variables X since in our
simulations we do not have such covariates. However, in the empirical examples discussed below, we use
such regressors. The test consists of regressing Y on X, X interacted with P (Z) and a polynomial in P (Z)
and testing for the joint significance of the coefficients on the nonlinear terms in P (Z). When there is no X,
the model simplifies appropriately.


                                                      47
Figure 11: Power of the tests for selection on the gain to treatment as a function of 1 − R2 ,
normal instrument.
                                                          A. Fixed signal/noise ratio; varying sample sizea
                                    1,000 Observations                                                            4,000 Observations
                 1.0




                                                                                              1.0
                 0.8




                                                                                              0.8
                 0.6




                                                                                              0.6
         Power




                                                                                      Power
                 0.4




                                                                                              0.4
                 0.2




                                                                                              0.2
                 0.0




                                                                                              0.0
                       0.0   0.2       0.4          0.6         0.8      1.0                        0.0     0.2         0.4          0.6     0.8   1.0
                                                2                                                                                2
                                             1− R                                                                             1− R
                                    7,000 Observations                                                            10,000 Observations
                 1.0




                                                                                              1.0
                 0.8




                                                                                              0.8
                 0.6




                                                                                              0.6
         Power




                                                                                      Power
                 0.4




                                                                                              0.4
                 0.2




                                                                                              0.2
                 0.0




                                                                                              0.0
                       0.0   0.2       0.4          0.6         0.8      1.0                        0.0     0.2         0.4          0.6     0.8   1.0
                                                2                                                                                2
                                             1− R                                                                             1− R

                                                          B. Fixed sample size; varying signal/noise ratiob
                                     Signal/noise = 0.1                                                            Signal/noise = 0.5
                 1.0




                                                                                              1.0
                 0.8




                                                                                              0.8
                 0.6




                                                                                              0.6
         Power




                                                                                      Power
                 0.4




                                                                                              0.4
                 0.2




                                                                                              0.2
                 0.0




                                                                                              0.0




                       0.0   0.2       0.4          0.6         0.8      1.0                        0.0     0.2         0.4          0.6     0.8   1.0
                                                2                                                                                2
                                             1− R                                                                             1− R
                                      Signal/noise = 1                                                                 Signal/noise = 2
                 1.0




                                                                                              1.0
                 0.8




                                                                                              0.8
                 0.6




                                                                                              0.6
         Power




                                                                                      Power
                 0.4




                                                                                              0.4
                 0.2




                                                                                              0.2
                 0.0




                                                                                              0.0




                       0.0   0.2       0.4          0.6         0.8      1.0                        0.0     0.2         0.4          0.6     0.8   1.0
                                                2                                                                                2
                                             1− R                                                                             1− R

                                   Normal test                CM test          Series test                IV test 1*           IV test 2**

        Note: The data are generated according to the normal generalized Roy model given in the text. The choice equation is D =1(Z ≥ V) where Z
        ~ N(0,σZ2). Standard errors are obtained from the bootstrap.
        a
          Panel A shows the power functions when fixing the signal to noise ratio at 1 (this means Var(Zγ)=1, compared to the variance of the
        unobservable in the choice equation of 1) and changing the sample size. For a given configuration of parameters, R2 is the R2 from a regression
        of the true E(Y|P) on a linear function of P.
        b
          Panel B shows the power functions for the fixed sample size of 4,000, but changing the signal to noise ratio from 0.1 to 2. The signal to noise
        ratio is defined as Var(Zγ)/Var(V).
        * IV test 1 separates the observations into those with propensity scores in the interval [0,Pmedian] and those with propensity scores in the interval
        [Pmedian,1] and testing the equality of the IV estimates based on those subsamples.
        ** IV test 2 separates the observations into quartiles of the propensity score. It tests for pairwise equality of the IV estimates calculated on each
        of those four subsamples and rejects the null if equality is rejected in any of the pairwise comparisons. It controls the size of the test
        using the method of Romano and Wolf (2005), as described in the text.




                                                                                48
Figure 12: Power of a Wald test for the null hypothesis that the coefficient on [P (Z)]2 is
zero in the generalized Roy model.
                                                                            Power across values of τA
                                   σZ2    = 1,   σU2       = 0.5                                                                  σZ2 = 1, σU2 = 1
          1.0




                                                                                                     1.0
          0.8




                                                                                                     0.8
          0.6




                                                                                                     0.6
  Power




                                                                                             Power
          0.4




                                                                                                     0.4
          0.2




                                                                                                     0.2
          0.0




                                                                                                     0.0
            -1.5      -1.0         -0.5          0.0          0.5         1.0     1.5                  -1.5         -1.0         -0.5      0.0       0.5         1.0     1.5
                                                 τ                                                                                         τ


                                                                           Power across values of σU2
                                          σZ2 = 0.5                                                                                     σZ2 = 1
          1.0




                                                                                                     1.0
          0.8
            8




                                                                                                     0.8
                                                                                                       8
  Power




                                                                                             Power
          0.6




                                                                                                     0.6
          0.4




                                                                                                     0.4
          0.2




                                                                                                     0.2
          0.0




                                                                                                     0.0




                0.0          0.5                 1.0                1.5         2.0                        0.0             0.5             1.0             1.5         2.0
                                                       2                                                                                         2
                                                 σU                                                                                        σU


                                                                           Power across values of σZ2
                                          σU2 = 0.5                                                                                     σU2 = 1
          1.0




                                                                                                     1.0
          0.8




                                                                                                     0.8
          0.6




                                                                                                     0.6
  Power




                                                                                             Power
          0.4




                                                                                                     0.4
  P




                                                                                             P
          0




                                                                                                     0
          0.2




                                                                                                     0.2
          0.0




                                                                                                     0.0




                0.0          0.5                 1.0                1.5         2.0                        0.0             0.5             1.0             1.5         2.0
                                                       2                                                                                         2
                                                 σZ                                                                                        σZ


                                                       1,000                     4,000                           7,000                  10,000


Note: Standard errors are obtained analytically. The data are generated according to the normal generalized Roy model given in the text. The choice
                                            2                                                                    2
equation is D = 1(Z ≥ V) where Z ~ N(0,σZ ). These figures plot the power of a test of the significance of [P(Z)] in a regression of Y on P(Z) and
            2
[P(Z)] . The size of the test is fixed at 0.05, and each line plots the power at a different sample size.




                                                                                        49
distributions, each with noncentrality parameter analogous to (15). Below we use simulation
methods to explore the distribution of this test statistic under various alternatives. Note
that although we do not have an exact expression for the distribution of the test statistic for
this general test, we control the size of the test at α = 0.05 using the Romano-Wolf stepdown
procedure. Specifically, we simulate the distribution of the smallest (most significant) p-value
under the null, as discussed above. This method applies the first stage of Romano and Wolf
(2005) and is described in the Web Appendix, Section 7. The Monte Carlo algorithm we use
is described in the Web Appendix, Section 8.
       We limit the degree of the polynomial to 5. We add polynomials in increasing order from
2 to 5, keeping all lower order terms when the order of the polynomial is increased. This
procedure produces a test of size at most 0.05 at τ = 0. Results from these simulations are
also given in Figures 10 and 11. The dashed lines in the figures plot the power of this test.
Figure 10 shows that under the null (τ = 0), we reject the null (up to simulation error) 5%
of the time. Panel A of the figure shows the power function across different sample sizes,
and panel B fixes the sample size and shows the power function across different values of
the signal to noise ratio. The results indicate that sample sizes above 1,000 are necessary in
order for the series test to have power and that with a relatively weak instrument, this test
will fail to have power against most alternatives.44,45,46


Test 2: Bierens Conditional Moment Test

Finally, we examine the power of the conditional moment test of Bierens (1990) in detecting
selection on the gain to treatment. This test uses the fact that under the null hypothesis of
linearity of g(p), moment condition (16) must hold for all t ∈ R and any bounded, one-to-one
  44
     In our web appendix, we report analyses for the series test comparable to those reported in Figures A2
through A4. The results are qualitatively similar to what is obtained for the IV test. (See Figures A11-A13
in the Web Appendix.)
  45
     Apparent inconsistencies of the test under the null in Figures 10 and 11 are due to sampling error.
  46
     An alternative approach to testing the order of the polynomial is to fit a model with L polynomial terms
and to conduct a joint test that the coefficients of the polynomials above order 1 are statistically significantly
different from zero. We have not compared the power and size properties of these two approaches.



                                                       50
mapping Λ(·). Following Bierens (1990), we use Λ(P (Z)) = tan−1 ((P (Z) − P̄ (Z))/sP (Z) )
where P̄ (Z) and sP (Z) are the sample mean and standard deviation of P (Z), recognizing that
this is just one of many possible Λ. To implement the test, we must also choose a region
of values of t over which to calculate the sample analog of (16). We search over a grid of
values of t between -10 and 10 spaced at intervals of 0.1. The method for constructing the
test statistic presented in Bierens (1990) requires the choice of two arbitrary real numbers,
which determine a cutoff value used in a decision rule for how to form the test statistic.
These must be chosen independently of the data generating process. We choose parameter
values that Bierens uses in his Monte Carlo simulations.47 The procedure used to calculate
the power of the test for different values of τ is described in section 8 of the Web Appendix.
The power of the moment test is plotted as the solid line in Figures 10 and 11.48


5.4     Power under Alternative Data Generating Processes

Thus far, our Monte Carlo simulations have only considered the properties of tests under
the specific alternative generated by different parameterizations of a normal generalized Roy
Model. We choose this model because it allows for a simple parameterization of alternatives
and is a useful baseline in empirical work. As a check to the robustness of our analysis, we
have explored alternative data generating processes for the instruments as well as alternative
specifications for the MTE. The results of these simulations are placed in the Web Appendix
to this paper. They suggest that the power of the tests does not depend fundamentally on
the normality assumptions made in the previous section. In particular, when the effective
deviation from the null is measured by 1 − R2 , as defined above, our results indicate that
the power of the tests is relatively invariant to the process generating choices, given 1 − R2 .
  47
     See Bierens (1990), p. 1453. We choose, in Bierens’ notation, γ = 1 and ρ = 0.5. These are not to be
confused with other uses of γ and ρ in this paper. Alternative parameter values produce qualitatively the
same results as we report in this paper. Results are available on request from the authors.
  48
     In the Web Appendix, we plot additional figures which trace out the power of the moment test more
thoroughly across the three dimensions which we find are important in this model – the deviation from the
null (as measured by τ ), the sample size, and the signal to noise ratio. Figures A14 through A16 contain
these plots.



                                                   51
The parametric model always has the highest power, followed by IV in P (Z) partitioned
below or above the median. Generally, the power functions of the series and CM tests are
somewhat below those for IV in P (Z) above and below the median, with no clear ranking
between them. Partitioning IV into quartiles generally produces the lowest power functions.
We next consider a prototypical application of the null the hypothesis of selection on the
gain to treatment.



6        An Analysis of a Prototypical Problem in Microe-

         conometrics

This section draws on the empirical analysis of Carneiro, Heckman, and Vytlacil (2006) to
test for the presence of a correlated random coefficient model in estimating the returns to
college and to explore the power of the main tests considered in this paper in a prototypical
economic problem. The data come from the National Longitudinal Survey of Youth, 1979
(NLSY79), and it contains 1,747 observations for white men from the random sample. The
outcome of interest is the average of deflated (to 1983) hourly wages reported in 1989, 1990,
1991, and 1992.49 The choice variable is D = 1 if the individual has attended some college
or has completed any schooling beyond high school and D = 0 if the individual has a high
school diploma or has completed 12 years of schooling but has never attended college. Our
sample of 1,747 individuals contains 882 observations with D = 0 and 865 observations with
D = 1. See the Web Appendix, Section 9 for further description of the sample.
       Following Carneiro, Heckman, and Vytlacil (2006), in the first stage we run a probit for
D using the following predictors of choice (Z): individual cognitive ability,50 mother’s educa-
tion, number of siblings, an indicator for urban residence at age 14, average unemployment
  49
     Following Carneiro, Heckman, and Vytlacil (2006) we delete all wage observations below one or above
100 dollars.
  50
     We proxy cognitive ability using the Armed Forces Qualification Test (AFQT). The AFQT has been
corrected for the effect of schooling at the time of the test using the method of Hansen, Heckman, and Mullen
(2001).



                                                     52
in the state of residence, average log earnings in the Standard Metropolitan Statistical Area
(SMSA) of residence, local wages and unemployment rates at age 17, an indicator for the
presence of a college in the county of residence at age 14, and cohort dummies.51
       For our estimates of the propensity score, P (Z), we use the fitted values from this probit
and in the second stage we regress the outcome variable on polynomials in the propensity
score in addition to the following control variables (X): years of experience, cognitive ability,
mother’s education, number of siblings, cohort dummies, average unemployment in the state
of residence and average log earnings in the SMSA of residence, local wages in 1991, and
local unemployment rate in 1991.
       Panels A and B of Table 3 give the results from the tests of the equality of IV estimators
described above. We consider two formulations of the test: one based on IV estimates using
P (Z) as an instrument above and below the median, and one based on IV estimates that
separate the data by quartile of P (Z). Panel A shows that the test of H0 based on IV
estimators separated by the median of P (Z) has a p-value of 0.0527. Panel B separates
P (Z) into quartiles. The smallest p-value of the pairwise tests of equality is 0.2407. This
result is consistent with our investigation of the power of these two tests. Recall that the
test based on separating by the median of P (Z) is more powerful than the test based on
separating by quartiles of P (Z).
       In addition, we have carried out tests of linearity described above. When we fit a
quadratic polynomial to E(Y |P (Z) = p), a Wald test for the signficance of the quadratic
term in P (Z) has a p-value of 0.046 – evidence against the null of linearity. However, when
we add higher degrees of the polynomial and use a stepdown method to control for the size of
the test, the critical value to control the size of the test at 0.05 is 0.024 against which we com-
  51
    The complete list of instruments is: mother’s education, number of siblings, urban residence at age 14,
AFQT, mother’s education squared, number of siblings squared, AFQT squared, local average wage, local
average squared, local average unemployment rate, local average unemployment rate squared, year of birth
dummies, presence of a local college at age 14, presence of a local college at age 14 interacted with AFQT,
mother’s education and number of siblings, local wage at age 17, local wage at 17 interacted with AFQT,
mother’s education and number of siblings, local unemployment rate at age 17, and local unemployment rate
at age 17 interacted with AFQT, mother’s education and number of siblings.



                                                    53
pare the p-value of 0.046. Allowing for the possibility of different degrees of the polynomial
beyond the second, we lose the ability to reject linearity. In addition, the more traditional
forward selection procedure for choosing the degree of the polynomial which starts with the
quadratic polynomial and adds higher degree terms as long as they are statistically signifi-
cant leads to choice of the quadratic polynomial as well. That is, if we test the significance
the additional [P (Z)]3 term, we obtain a p-value of 0.374. The conditional moment test
produces a p-value of 0.5525 and fails to reject the null hypothesis. This is consistent with
our evidence on its low power.
   Under the assumptions of the normal selection model, we can test for selection on the
gains to treatment by testing the coefficient on the selection term in the second stage regres-
sion. The test strongly rejects H0 (see Table 3, Panel C). Our evidence for rejection of H0
is consistent with the analysis of Carneiro, Heckman, and Vytlacil (2006), who show that
for the same data a MTE based on a normal model is consistent with a nonparametrically
estimated MTE.
   Figure 13 plots the MTE estimated using different degrees of polynomial approximation,
the MTE estimated assuming normality, and a nonparametric estimate of the MTE based
on local polynomial regression. As shown by Carneiro, Heckman, and Vytlacil (2006), the
normal model does a remarkably good job in describing the marginal treatment effect for
this sample. Figure 13 also shows the weights that the IV estimator places on the MTE as
well as the histogram of estimated propensity scores.


6.1    Monte Carlo Analysis for this Example

The Monte Carlo analyses presented in Section 5 were for general parameter values. To
examine the power issue in the context of the college vs. high school example, it is useful to
start with the benchmark model just presented and conduct a Monte Carlo study for this
prototypical model. It confirms in this setting the general Monte Carlo analysis discussed in
Section 5. All of the tests have relatively low power. This means that tests of the null of H0


                                              54
Table 3: College participation vs. stopping at high school: IV tests for selection on the gain
to treatment. (Standard errors are obtained from the bootstrap.)
                                                                                                a
                                        A. IV estimates above and below the median
                                                           Whole sample                             Below                 Above
Estimate:                                                      0.1266                               0.9112                -0.2445
Standard error:                                                        (0.1500)                     (0.5148)              (0.3553)
p-value of test:                         0.0527

                                    B. IV estimates by quartiles of the propensity scoreb
                                                     1st quartile     2nd quartile    3rd quartile                     4th quartile
Estimate:                                              1.8842           0.5583           0.3076                          -0.9497
Standard error:                                             (4.2662)              (3.4309)          (0.5950)              (5.2114)
Smallest p-value from pairwise tests:                       0.2407

                                    C. Test of heterogeneity in normal selection modelc
Probability value of test:              0.011

a
  These IV estimates do not include interactions between the treatment and X. The test of equality is a Wald test using a covariance
matrix which is constructed using 1,000 bootstrap samples.
b
  These IV estimates do not include interactions between the treatment and X. The size of the test is controlled using a bootstrap
method as described in the text.
c
  The p-value in this panel is calculated using a Wald test for whether the coefficient on the selection term is zero. The standard error
is calculated using 100 bootstrap samples.




                                                                  55
Figure 13: College participation vs. stopping at high school: estimates of marginal treat-
ment effect for different models, IV weights and support of the estimated propensity score.
(Standard errors are obtained from the bootstrap.)

                                        Polynom ial MTE (Degree 2)                                                          Polynom ial MTE (Degree 3)
                     3




                                                                                                          3
                     2




                                                                                             (log wage)
        (log wage)




                                                                                                          2
                     1




                                                                                                          1
    MTE MTE




                                                                                         MTE MTE
                     0




                                                                                                          0
                     -1




                                                                                                          -1
                                                                                                          -2
                     -2




                            0.0   0.2           0.4        0.6          0.8   1.0                               0.0   0.2          0.4        0.6        0.8   1.0
                                                      uD                                                                                 uD


                                                Norm al MTE                                                                    Nonparam etric MTE
                     1.5




                                                                                                          1.0
       (log wage)




                                                                                         (log wage)
                     1.0
                     0.5




                                                                                                          0.5
    MTEMTE




                                                                                     MTE MTE
                     0.0
                     -0.5




                                                                                                          0.0
                     -1.0




                            0.0   0.2           0.4        0.6          0.8   1.0                               0.0   0.2          0.4        0.6        0.8   1.0
                                                      uD                                                                                 uD


                                        IV w eight (P as instrum ent)
                     1.4




                                                                                                                              D=0
                                                                                                                                                    D=1
                     1.2
                     1.0
      IV weight
                     0.8
                     0.6
                     0.4
                     0.2
                     0.0




                            0.0   0.2           0.4        0.6          0.8   1.0
                                            Propensity Score

a
  In the MTE graphs, the dashed line indicates the IV estimate. In the histogram, the dark bars correspond to the D=1 group and the light bars
to the D=0 group. The sample size is 1,747. The confidence intervals are found using 100 bootstrap samples. The covariates in the outcome
equations are: years of experience, corrected AFQT, mother’s education, number of siblings, cohort dummies, average unemployment in the
state of residence and average log earnings in the SMSA of residence, local wages in 1991, and local unemployment rate in 1991. The
instruments are: corrected AFQT, mother's education, number of siblings, an indicator for urban residence at age 14, average unemployment in
the state of residence, average log earnings in the SMSA of residence, local wages and unemployment rates at age 17, an indicator for the
presence of a college in the county of residence at age 14, and cohort dummies. The dependent variable in the probit is 1 if the individual
reports having attended college or completed any schooling past 12 years, and 0 if the individual has a high school diploma or has completed 12
years of school but not attended college (GEDs are excluded).

                                                                                    56
tend to be conservative in the sense that they are likely not to reject when the null is false.
   In these simulations, we take the regressors (X) and instruments (Z) from the NLSY79
data, using the 1,747 observations of the independent variables in the Carneiro et al. model.
Assuming the normal generalized Roy model, we generate outcomes Y and choices D based
on alternative parameterizations of the unobservables of the model. That is, we take X and
Z from the data and we generate Y and D according to


                                  Y1 = α1 + Xβ0 + U1

                                  Y0 = α0 + Xβ1 + U0

                                  D = 1(αD + Zγ ≥ V )

                                   Y   = DY1 + (1 − D)Y0


where α1 , α0 , β0 , β1 , αD and γ are calculated from the NLSY79 data.
   These simulations use the same regressors and instruments as in the Carneiro et al. study.
In order to generate the data under various alternative hypotheses, we specify alternative
parameter values for the distributions of the unobservables U1 , U0 and V . We assume that
the unobservables are jointly normally distributed as in Table 2, but we vary the variances
and covariances of these variables. Therefore, the only way in which these simulated datasets
differ from the actual dataset is the values of Y and D and in the assumption of normal
errors. Carrying out simulations in this fashion lets us examine how the power of the tests
depends, in this specific example, on the departure from the null hypothesis (in the sense of
τ ) and on the sample size. We use the values of σ1 and σ0 calculated from the NLSY79 data.
To gauge the explanatory power of our instruments in the NLSY79 data, we use the pseudo-
R2 of the first stage probit. The pseudo-R2 in this application is 0.2986. For comparison to
the power calculations, we note that this corresponds to a signal to noise ratio (as described
in section 3.1) of about 1.2.
   Figure 14 shows the power of three of our tests for different values of τ and different


                                              57
         Figure 14: Power of the tests from Monte Carlo simulations on NLSY79 data. (Standard
         errors are obtained from the bootstrap.)


                                                                            Power function, across τ
                                                                                                                                                                                                                     2
                                                                                                                                                                                      Power function, across 1 - R
                  1.0




                                                                                                                                                            1.0
                  0.8




                                                                                                                                                            0.8
Power against τ
                  0.6




                                                                                                                                                            0.6
                                                                                                                                                    Power
                  0.4




                                                                                                                                                            0.4
                  0.2




                                                                                                                                                            0.2
                  0.0




                                                                                                                                                            0.0
                          -1.5      -1.0                        -0.5                                    0.0           0.5          1.0     1.5                    0.0           0.2          0.4         0.6   0.8       1.0
                                                                                                                                                                                                     2
                                                                                                         τ                                                                                         1−R
                                                                                                                              Power function, across signal/noise ratio
                                                                                                        1.0
                                                                              Power against τ=-0.2623
                                                                                                        0.8
                                                                                                        0.6
                                                                                                        0.4
                                                                                                        0.2
                                                                                                        0.0




                                                                                                                  0            2           4                  6          8            10
                                                                                                                                          Signal/noise

                                                                                                                                   Power function, across sample size
                                                                      1.0
                                            Power against τ=-0.2623
                                                                      0.8
                                                                      0.6
                                                                      0.4
                                                                      0.2
                                                                      0.0




                                                                                                         2000                 4000               6000                   8000               10000
                                                                                                                                           Sample size


                                                                                                                Normal test              IV test*                 Series test              CM test
                                                                                                                                               Parameterization
                                                                                                                            N= 1747                                     σ10 = 0
                                                                                                                            σ1= 0.4857                                   σV= 1
                                                                                                                                                                          2
                                                                                                                            σ0= 0.3927                            pseudo R = 0.2986

                                                                                                                                          Signal/noise ratio = 1.2
                                                                                                                                                τ = -0.2623

                        Note: In these simulations we use the real data from the NLSY79 on the effect of a college education on wages for regressors (X) and
                        instruments (Z). We parameterize the unobservables to match the estimated values from the data and we generate outcomes Y and choices D
                        for different combinations of parameters. We have 1,747 observations and we fix the std. dev. of U 1 at 0.4857, the std. dev. of U0 at 0.3927. In
                        each of the top three panels, the dot-dashed vertical line indicates the estimates of the parameters calculated in the NLSY79 data. The series test
                        estimates polynomials of degrees 2 through 5 and in each model tests for the joint significance of the nonlinear terms. The size of the test is
                        controlled using the bootstrap procedure described in the text.
                         * The IV test used in these simulations tests for the equality of the IV estimates constructed using the propensity score as the instrument
                         separately for observations below the median of P(Z) and above the median of P(Z).
                                                                                                                                                    58
sample sizes. Note that with our actual sample size of 1,747 and our estimated τ = −0.2623
we are in a range where all of the tests have very low power, although for a fixed τ , the
IV test generally has more power than the series test, which in turn has more power than
the conditional moment (CM) test. In addition, the IV test shows greater increase in power
with the signal/noise ratio than do the other tests. These results are in accordance with the
Monte Carlo simulations conducted in Section 5.52
       For comparison with the power calculations made in the previous sections, we also present
these results in terms of the R2 of a regression of the fitted E(Y |P (Z) = p) on a linear term
in P (Z). This allows us to compare the value of the R2 we calculate in the data with values
of the R2 for other parameterizations as shown in Figure 14. We find that for samples
consistent with those found in practice the estimated 1 − R2 = 0.1375. It is represented by
the vertical dot-dashed lines in Figure 14.
       Overall, the simulations based on the data from this example indicate that for this sample
it would take a large deviation from the null in order for us to reject H0 . Our evidence on
power gives us greater confidence in rejecting the null hypothesis for the Carneiro, Heckman,
and Vytlacil (2006) data. We reject H0 in their sample, even though the power of the tests
is low.



7        Summary and Conclusion

This paper develops and applies tests for the presence of a correlated random coefficient
model. All of the tests we consider can be interpreted as conditional moment tests. We have
developed the sampling distribution of the IV estimator using the marginal treatment effect
and its extensions to higher moments of the distribution of the heterogeneity on which agents
select. We investigate the power of general tests based on the correlated random coefficient
model, where the parameter of interest is determined by an instrument.
       We develop and evaluate instrumental variable tests for the null hypothesis of the absence
  52
       An exception is that the CM test falls appreciably in power in this example.

                                                      59
of a correlated random coefficient model. We examine the power of these tests and the power
of additional tests of the null hypothesis based on linearity in p for E(Y | P (Z) = p, X = x).
   In sample sizes common in empirical economics, the power of the proposed tests is low.
The degradation of power of less parametric tests from the parametric tests is substantial.
Such tests are conservative in the sense that they often do not reject the null of no correlated
random coefficient model (H0 ) when, in fact, a correlated random coefficient model describes
the data. Among the tests we consider, an IV test based on partitioning the propensity score
above and below the median has the best performance in terms of power in samples commonly
encountered in practice. An analysis of the optimal choice of instruments to maximize the
power functions for the tests is left for future work.
   We test if the additional complexity of a correlated random coefficient model is required to
describe data on the returns to college. We find support for the correlated random coefficient
model using our testing procedure. This evidence is strengthened by our study of the power
of these tests. We reject H0 in a situation where the power of the tests we use is low.
   This paper analyzes the case of a binary treatment. Heckman, Urzua, and Vytlacil (2006)
and Heckman and Vytlacil (2007b) analyze the cases of a multiple treatment model generated
by an ordered choice model with stochastic thresholds and a multiple treatment model
generated by an unordered choice model. In all of these cases, IV produces an instrument-
dependent parameter so the IV test for selection on unobserved gains based on comparing
the estimands of two different IVs developed in this paper carries over in general to these
settings. A test of linearity of the conditional expectation of Y given P in (a vector of) P
is developed for the outcome model for multiple treatments generated by the ordered choice
model in Heckman, Urzua, and Vytlacil (2006). It also applies to the unordered multiple
choice model that identifies the treatment effect of a gain option compared to the next best
option which Heckman, Urzua and Vytlacil show is a direct extension of the binary model.




                                              60
A     The Variance of Linear IV in the Correlated Ran-

      dom Coefficient Model

The IV estimator, using instrument J(Z), is

                                                        P ˜
                                                          Yi J i
                                              βbIV,J   =P
                                                          Di J˜i

and hence
                             √                 √1
                                                     P      ¯ i + βi Di )
                                                      (Ji − J)(α
                                                 I
                                 I βbIV,J =                               .
                                                         1
                                                             Di J˜i
                                                           P
                                                         I

Invoking standard central limit theorems,

                                 √               
                                                    d
                                  I βbIV,J − βIV,J → N (0, ΩJ ).


Defining J ∗ = J − E(J), where ΩJ is given by

                    (                2 )
                      Y J∗       Y J∗
                               
            ΩJ = E          −E
                        ωJ        ωJ
                     2 ∗ 2             2
                     Y (J )           Y J∗
               =E            − E
                        ωJ2            ωJ
                                1                          2
                     2 ∗ 2      Z
                     Y (J )
               =E            −  E(β | UD = uD )hJ (uD )duD 
                        ωJ2
                                  0
                                            1                        2
                                            Z
                  1
               = 2 E (α + βD)2 (J ∗ )2 −  E(β | UD = uD )hJ (uD )duD 
                                     
                 ωJ
                                                         0
                 1               2                  1 
               = 2 E α2 (J ∗ )2 + 2 E αβD(J ∗ )2 + 2 E β 2 D(J ∗ )2
                                                                  
                ωJ               ωJ                  ωJ
                     1                             2
                      Z
                  −     E(β | UD = uD )hJ (uD )duD  .
                         0




                                                        61
Using the law of iterated expectations as well as the assumption that α is independent of Z,
this expression can be written as

        Var(J)    1                                 1 
ΩJ = E α2        + 2 2E αβ(J ∗ )2 | D = 1 Pr(D = 1) + 2 E β 2 (J ∗ )2 | D = 1 Pr(D = 1)
                                                                           
             2
            ωJ     ωJ                                ωJ
         1                            2
          Z
      −    E(β | UD = uD )hJ (uD )duD 
              0


where
                                    E[J ∗ | P (Z) ≥ uD ] Pr(P (Z) ≥ uD )
                       hJ (uD ) =                                        .
                                                     ωJ

Under the conditions of Fubini’s Theorem, we can exchange the order of integration and
write

         Var(J)
ΩJ = E α2
               ωJ2
       Z1
                                   2
                                                E((J ∗ )2 | P (Z) ≥ uD ) Pr(P (Z) ≥ uD )
     +      2E (αβ | UD = uD ) + E β | UD = uD                                             duD
                                                                     ωJ2
       0
        1                            2
         Z
     −  E(β | UD = uD )hJ (uD )duD 
          0
                    Z1
      2  Var(J)                               2
                                                             
   =E α           +     2E (αβ | UD = uD ) + E β   | UD = uD    hΩJ (uD )duD
             ωJ2
                    0
        1                            2
          Z
     −  E(β | UD = uD )hJ (uD )duD 
              0


where

                                  Z∞                    Z1
                              1                     2
                  hΩJ (uD ) = 2        (j − E(J))            fP,J (P (z), j)dP (z)dj
                             ωJ
                                  −∞                    uD
                                             2
                              E[(J − E(J)) | P (Z) ≥ uD )] Pr(P (Z) ≥ uD )
                          =
                                                 ωJ2



                                                 62
which is the expression in the text.



B        Proof of Invariance of the IV Estimand to the Choice

         of a Linear Instrument under Normality with a Lin-

         ear Index Choice Equation

Suppose that the choice equation has a linear index structure, so that


                                         D = 1(Zγ ≥ V )


where Z ∼ N (Z̄, ΣZ ), an L-dimensional multivariate normal random variable, γ an L × 1
vector and V ∼ N (0, σV2 ). Consider the instrument J(Z), which is a linear function of Z,
say Z 0 η. In this case, the IV estimand (written as a weighted average over the support of
V ) is

                                     Z   ∞                                   
                                                                          v
                           βIV,J =           M T E(v)hJ (v)φ                      dv
                                       −∞                                σV

where φ(·) is a standard normal pdf and the IV weight is

                                E[J(Z) − E(J(Z))|Zγ > v] Pr(Zγ > v)
                     hJ (v) =                                       .
                                           Cov(J(Z), D)

Under the assumption of multivariate normality for the instruments,
                                                                             
                                         Cov(J(Z),Zγ)
                                             √          φ       √v−Z̄γ
                                              Var(Zγ)               Var(Zγ)
                          hJ (v) =                                               .
                                     Cov(J(Z),Zγ−V )                  −Z̄γ
                                         √              φ       √
                                         Var(Zγ−V )                 Var(Zγ−V )




                                                  63
Under assumption (A-1) in Section 2, Cov(J(Z), Zγ) = Cov(J(Z), Zγ − V ), and we obtain

                                                                       
                                          √ 1
                                                  φ       √v−Z̄γ
                                          Var(Zγ)             Var(Zγ)
                           hJ (v) =                                        .
                                      √   1               √     −Z̄γ
                                                 φ
                                      Var(Zγ−V )              Var(Zγ−V )



That is, the IV weights, and hence the IV estimand, are the same for all J(Z) = Z 0 η for any
η.




                                              64
Acknowledgments

This research was supported by NIH R01-HD043411, NSF SES-024158, the American Bar
Foundation and the Geary Institute, University College Dublin, Ireland. The views expressed
in this paper are those of the authors and not necessarily those of the funders listed here. We
have received helpful comments from Pedro Carneiro, Jeremy Fox, Joel Horowitz, Benjamin
Moll, Azeem Shaikh, Christopher Taber, Edward Vytlacil, the editor, Steve Durlauf, and
an anonymous referee and participants in workshops at the University of Wisconsin and
Northwestern University. In the final round of revisions, we received additional very helpful
suggestions from Stéphane Bonhomme, Xiaohong Chen, Azeem Shaikh and Edward Vytlacil.
Supplementary material for this paper is available at the Website http://jenni.uchicago.edu/
testing_random/.




                                              65
References

Abbring, J. H. and J. J. Heckman (2007). Econometric evaluation of social programs, part
  III: Distributional treatment effects, dynamic treatment effects, dynamic discrete choice,
  and general equilibrium policy evaluation. In J. Heckman and E. Leamer (Eds.), Handbook
  of Econometrics, Volume 6B, pp. 5145–5303. Amsterdam: Elsevier.

Bierens, H. J. (1982). Consistent model specification tests. Journal of Econometrics 20 (1),
  105–134.

Bierens, H. J. (1990, November). A consistent conditional moment test of functional form.
  Econometrica 58 (6), 1443–1458.

Bierens, H. J. and W. Ploberger (1997, September). Asymptotic theory of integrated condi-
  tional moment tests. Econometrica 65 (5), 1129–1151.

Björklund, A. and R. Moffitt (1987, February). The estimation of wage gains and welfare
  gains in self-selection. Review of Economics and Statistics 69 (1), 42–49.

Carneiro, P., J. J. Heckman, and E. J. Vytlacil (2006). Estimating marginal and average
  returns to education. Under revision.

Griliches, Z. (1977, January). Estimating the returns to schooling: Some econometric prob-
  lems. Econometrica 45 (1), 1–22.

Hansen, K. T., J. J. Heckman, and K. J. Mullen (2001). Ordered discrete choice models
  with stochastic shocks. Unpublished manuscript, University of Chicago, Department of
  Economics.

Heckman, J. J. (2001, August). Micro data, heterogeneity, and the evaluation of public
  policy: Nobel lecture. Journal of Political Economy 109 (4), 673–748.




                                             66
Heckman, J. J. and R. Robb (1985). Alternative methods for evaluating the impact of
  interventions. In J. Heckman and B. Singer (Eds.), Longitudinal Analysis of Labor Market
  Data, Volume 10, pp. 156–245. New York: Cambridge University Press.

Heckman, J. J., S. Urzua, and E. J. Vytlacil (2006). Understanding instrumental variables in
  models with essential heterogeneity. Review of Economics and Statistics 88 (3), 389–432.

Heckman, J. J. and E. J. Vytlacil (1998, Fall). Instrumental variables methods for the
  correlated random coefficient model: Estimating the average rate of return to schooling
  when the return is correlated with schooling. Journal of Human Resources 33 (4), 974–987.

Heckman, J. J. and E. J. Vytlacil (1999, April). Local instrumental variables and latent
  variable models for identifying and bounding treatment effects. Proceedings of the National
  Academy of Sciences 96, 4730–4734.

Heckman, J. J. and E. J. Vytlacil (2001). Local instrumental variables. In C. Hsiao,
  K. Morimune, and J. L. Powell (Eds.), Nonlinear Statistical Modeling: Proceedings of
  the Thirteenth International Symposium in Economic Theory and Econometrics: Essays
  in Honor of Takeshi Amemiya, pp. 1–46. New York: Cambridge University Press.

Heckman, J. J. and E. J. Vytlacil (2005, May). Structural equations, treatment effects and
  econometric policy evaluation. Econometrica 73 (3), 669–738.

Heckman, J. J. and E. J. Vytlacil (2007a). Econometric evaluation of social programs, part I:
  Causal models, structural models and econometric policy evaluation. In J. Heckman and
  E. Leamer (Eds.), Handbook of Econometrics, Volume 6B, pp. 4779–4874. Amsterdam:
  Elsevier.

Heckman, J. J. and E. J. Vytlacil (2007b). Econometric evaluation of social programs, part
  II: Using the marginal treatment effect to organize alternative economic estimators to
  evaluate social programs and to forecast their effects in new environments. In J. Heckman


                                             67
  and E. Leamer (Eds.), Handbook of Econometrics, Volume 6B, pp. 4875–5144. Amsterdam:
  Elsevier.

Horowitz, J. L. and V. G. Spokoiny (2001, May). An adaptive, rate-optimal test of a para-
  metric mean-regression model against a nonparametric alternative. Econometrica 69 (3),
  599–631.

Ichimura, H. and P. E. Todd (2007). Implementing nonparametric and semiparametric
  estimators. In J. Heckman and E. Leamer (Eds.), Handbook of Econometrics, Volume 6B.
  Amsterdam: Elsevier.

Imbens, G. W. and J. D. Angrist (1994, March). Identification and estimation of local
  average treatment effects. Econometrica 62 (2), 467–475.

Li, R. and L. Nie (2007, November). Efficient statistical inference procedures for partially
  nonlinear models and their applications. Biometrics 64 (3), 904–911.

McFadden, D. (1974). Conditional logit analysis of qualitative choice behavior. In P. Zarem-
  bka (Ed.), Frontiers in Econometrics. New York: Academic Press.

McFadden, D. (1981). Econometric models of probabilistic choice. In C. Manski and D. Mc-
  Fadden (Eds.), Structural Analysis of Discrete Data with Econometric Applications. Cam-
  bridge, MA: MIT Press.

Newey, W. K. (1985, September). Maximum likelihood specification testing and conditional
  moment tests. Econometrica 53 (5), 1047–1070.

Newey, W. K. (1997, July). Convergence rates and asymptotic normality for series estimators.
  Journal of Econometrics 79 (1), 147–168.

Quandt, R. E. (1958, December). The estimation of the parameters of a linear regres-
  sion system obeying two separate regimes. Journal of the American Statistical Associa-
  tion 53 (284), 873–880.

                                             68
Romano, J. P. and A. M. Shaikh (2006, August). Stepup procedures for control of general-
  izations of the familywise error rate. Annals of Statistics 34 (4), 1850–1873.

Romano, J. P. and M. Wolf (2005, March). Exact and approximate stepdown methods for
  multiple hypothesis testing. Journal of the American Statistical Association 100 (469),
  94–108.

Thurstone, L. L. (1927). A law of comparative judgement. Psychological Review 34, 273–286.

Vytlacil, E. J. (2002, January). Independence, monotonicity, and latent index models: An
  equivalence result. Econometrica 70 (1), 331–341.

Yitzhaki, S. (1989). On using linear regression in welfare economics. Working Paper 217,
  Department of Economics, Hebrew University.




                                             69
