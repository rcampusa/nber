                              NBER WORKING PAPER SERIES




          HETEROGENEOUS BELIEFS AND SCHOOL CHOICE MECHANISMS

                                         Adam Kapor
                                    Christopher A. Neilson
                                     Seth D. Zimmerman

                                      Working Paper 25096
                              http://www.nber.org/papers/w25096


                    NATIONAL BUREAU OF ECONOMIC RESEARCH
                              1050 Massachusetts Avenue
                                Cambridge, MA 02138
                        September 2018, Revised December 2019




We thank Parag Pathak, Maria Marta Ferreyra, Nikhil Agarwal, Eric Budish, Chao Fu, and
numerous seminar participants for their helpful comments. We thank numerous staff and
administrators in the New Haven Public School district for their invaluable assistance. We thank
Fabiola Alba, Alejandra Aponte, Felipe Arteaga Ossa, James Brand, Kevin DeLuca, Manuel
Martinez, Gonzalo Oyanedel, Jordan Rosenthal-Kay, and Julia Wagner for their excellent
research assistance. This material is based upon work supported by the National Science
Foundation under grant SES-1629226. We gratefully acknowledge additional financial support
from the Cowles Foundation, the Yale Program in Applied Economics and Policy, and the
Princeton University Industrial Relations Section. Zimmerman gratefully acknowledges support
from the Richard N. Rosett Faculty Fellowship at the University of Chicago Booth School of
Business. This research received approval from IRBs at NBER and Princeton University. All
errors are our own. The views expressed herein are those of the authors and do not necessarily
reflect the views of the National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.

© 2018 by Adam Kapor, Christopher A. Neilson, and Seth D. Zimmerman. All rights reserved.
Short sections of text, not to exceed two paragraphs, may be quoted without explicit permission
provided that full credit, including © notice, is given to the source.
Heterogeneous Beliefs and School Choice Mechanisms
Adam Kapor, Christopher A. Neilson, and Seth D. Zimmerman
NBER Working Paper No. 25096
September 2018, Revised December 2019
JEL No. D47,I20

                                          ABSTRACT

This paper studies how welfare outcomes in centralized school choice depend on the assignment
mechanism when participants are not fully informed. Using a survey of school choice participants
in a strategic setting, we show that beliefs about admissions chances differ from rational
expectations values and predict choice behavior. To quantify the welfare costs of belief errors, we
estimate a model of school choice that incorporates subjective beliefs. We evaluate the
equilibrium effects of switching to a strategy-proof deferred acceptance algorithm, and of
improving households’ belief accuracy. We find that a switch to truthful reporting in the DA
mechanism offers welfare improvements over the baseline given the belief errors we observe in
the data, but that an analyst who assumed families had accurate beliefs would have reached the
opposite conclusion.

Adam Kapor                                       Seth D. Zimmerman
Department of Economics                          Booth School of Business
Princeton University                             University of Chicago
280 Julis Romo Rabinowitz Building               5807 S. Woodlawn Avenue
Princeton, NJ 08544                              Chicago, IL 60637
and NBER                                         and NBER
akapor@princeton.edu                             seth.zimmerman@chicagobooth.edu

Christopher A. Neilson
Woodrow Wilson School
Princeton University
Firestone Library, Room A2H
Princeton, NJ 08544
and NBER
cneilson@princeton.edu




A data appendix is available at
http://www.nber.org/data-appendix/w25096
Survey materials are available at
http://faculty.chicagobooth.edu/seth.zimmerman/research/papers/KNZ_Survey.
1       Introduction
Many cities in the US and abroad use centralized school choice mechanisms to assign students
to schools. Most centralized assignment mechanisms work by eliciting rank-order lists of schools
from applicants and then making school assignments based on a combination of coarse priorities
and random lotteries. However, districts differ in the extent to which their chosen assignment
algorithms reward informed strategic play by choice participants. Charlotte, Barcelona, and Beijing
use mechanisms that reward strategic play, while Boston, New York, and Denver use mechanisms
which aim to make truthfully reporting one’s preferences a dominant strategy.1 Which type of
mechanism is preferable is a central debate in the literature on school choice mechanism design.
Mechanisms that reward informed strategic play can raise welfare by allowing participants to express
the intensity of their preferences as opposed to just the ordering (Abdulkadiroğlu et al., 2011), but
they can also lead to costly application mistakes and inequitable outcomes if some participants lack
the information or sophistication to strategize effectively (Pathak and Sönmez, 2008).
    Despite the critical role of beliefs and strategic play in the welfare comparison between the two
mechanism types, there is little empirical evidence on what families know about school choice and
how this affects the allocation of students to schools. This paper studies how welfare outcomes
depend on the assignment mechanism when school choice participants are not fully informed. We
combine a new household survey measuring the preferences, sophistication, and beliefs of potential
school choice participants with administrative records of choice and academic outcomes to conduct
two types of analysis.
    First, we present a descriptive analysis of families’ subjective beliefs and strategic behavior, and
how these translate to school placement outcomes. We find that many families engage in strategic
play, but do so on the basis of subjective beliefs that are often wrong. Second, we estimate a model
of school choice in which families make decisions on the basis of subjective beliefs about admissions
chances. The model allows us to quantify the tradeoff between welfare-reducing mistakes and
families’ ability to express cardinal preferences in terms of both aggregate welfare and equity. We
use our model estimates to evaluate the equilibrium effects of improving the information available to
households in a mechanism that rewards strategic play, and of switching from such a mechanism to
a strategy-proof deferred acceptance (DA) algorithm. We find that a switch to truthful reporting in
the DA mechanism offers welfare improvements over the baseline given the belief errors we observe
    1
     Boston, New York, Denver: Abdulkadiroğlu et al. (2005a,b, 2017b). Barcelona: Calsamiglia and Güell (2018);
Charlotte: Hastings et al. (2009); Beijing: He (2012). See Pathak and Sönmez (2013) for a discussion of incentives
to report truthfully in these mechanisms.



                                                        1
in the data, but that an analyst who assumed families had accurate beliefs would have reached the
opposite conclusion.
    We conduct our study in the context of high school choice in the New Haven, Connecticut
school district (henceforth NHPS). NHPS is a low-income, majority-minority school district that
has used a centralized mechanism to assign students to schools since at least 1997. We conducted
home surveys of the families of rising ninth graders in 2015 and 2017. In total, we surveyed 417
households. We link our survey data to administrative records of the school placement process.
    The assignment mechanism NHPS uses (henceforth, the ‘baseline’ mechanism) closely resembles
the ‘Boston’ or immediate acceptance mechanism, which rewards strategic play by giving applicants
higher admissions priority at schools they rank higher on their application forms.2 A theoretical
literature on school choice mechanism design provides conditions under which all students prefer
the Boston mechanism to the student-optimal stable matching mechanism, and others under which
it is (weakly) worse for all students (Ergin and Sonmez, 2006; Abdulkadiroğlu et al., 2011).3 Which
mechanism will perform best in a particular district is therefore an empirical question. The answer
depends on whether applicants’ ability to express cardinal preferences through strategic play in
the Boston mechanism outweighs the welfare costs of strategic mistakes due to misunderstandings
about the mechanism or lack of information about demand conditions. Observations of beliefs and
preferences help us quantify this tradeoff.
    We begin our analysis by using our survey to describe participants’ preferences, subjective beliefs,
and strategic sophistication, as well as the relationship between beliefs and choice behavior. We show
that many families misunderstand the assignment mechanism and make errors in their estimates
of the admissions probabilities associated with different application portfolios. Fewer families can
correctly describe key features of the assignment mechanism than would be expected from random
guessing. When asked about admissions chances for hypothetical application portfolios, respondents
report subjective beliefs that differ from rational expectations admission probabilities by a mean
(absolute) value of 37 percentage points. Consistent with the hypothesis that families do not
understand the assignment mechanism, respondents underestimate how much ranking a school lower
on their application reduces admissions chances.
    Errors in subjective beliefs matter because, together with preference intensity, they are inputs
to strategic behavior. 32% of respondents are ‘revealed strategic’ in the sense that they list a
   2
      In 2017, New Haven used the Boston mechanism. In 2015, it used a mechanism that coincides with the Boston
mechanism when all students are in the same priority group. We discuss the mechanism in detail in Section 2.
    3
      See also Pathak and Sönmez (2008), who provide a model in which sophisticated students benefit, and naive
students suffer, from the Boston mechanism, and Pathak (2011) for a review.



                                                      2
school other than their most-preferred school first on their application. Households reporting weak
relative preferences for their most-preferred school are 58% more likely to be revealed as strategic.
Conditional on rational expectations admissions chances, students with subjective beliefs in the
upper quartile of the belief distribution are 17 percentage points more likely to rank their most-
preferred school first on their application than students with subjective beliefs in the bottom quartile.
In contrast, conditional on subjective beliefs, rational expectations admissions chances do not predict
the rates at which applicants list their most-preferred school first.
   Motivated by these descriptive findings, we use an empirical model of school choice to study
the equilibrium effects of alternative school choice policies. Our approach combines survey evidence
with a revealed preference analysis of students’ application and enrollment choices. Households in
our model maximize expected utility given their subjective beliefs about admissions probabilities,
not rational expectations beliefs. The survey data help us overcome the challenges associated with
separately identifying beliefs and preferences described by Manski (2004) and Agarwal and Somaini
(2018) without imposing strong assumptions on applicants’ equilibrium play.
   Because we cannot ask families about the admissions probabilities associated with each possible
application portfolio, we develop a parsimonious model of belief formation that captures key features
of our survey results. In the model, students’ beliefs about their own admissions rankings relative
to cutoff rankings for admission to each school are equal to the true values plus a shift term. The
shift term depends on a) the student’s priority at a target school, b) the school’s rank on a student’s
submitted application, c) a student level shock that is common across all schools, and d) person-
school components. The first two terms allow us to capture systematic misunderstanding of the
assignment mechanism, while the latter two allow, respectively, for levels of optimism to vary across
students and for errors in belief about school-specific demand.
   We incorporate subjective beliefs into a model of choice in which households choose whether
to participate in choice and, if they participate, what application to submit. The model allows
for correlated heterogeneous preferences across schools. We estimate the model using an MCMC
procedure (McCulloch and Rossi, 1994; Agarwal and Somaini, 2018) that incorporates both survey
and administrative data. For surveyed students, the model fits both administrative records of
submitted applications and survey reports of beliefs and preferences. The model also uses belief
errors to rationalize choices for unsurveyed households.
   With parameter estimates in hand, we study two sets of counterfactual simulations. The first
counterfactual exercise simulates a switch to a DA mechanism. In the DA mechanism, students do
not need to understand assignment probabilities to play an optimal strategy. The second considers a
best-case informational intervention allowing households to play the Bayes Nash equilibrium in the

                                                   3
game induced by the baseline mechanism. To evaluate welfare in these counterfactuals, we consider
each student’s expected utility, according to the utility he or she gets from placement at each school
and the rational expectations chances associated with their lottery application. We measure utility
relative to the outside option of attending a neighborhood school.
    Results from these exercises show that errors in subjective beliefs reverse the welfare comparison
between the baseline and deferred acceptance mechanisms, and that this reversal is economically
large. Given the beliefs we observe in the data, switching from the baseline mechanism to truthful
reporting under a deferred acceptance assignment mechanism would increase mean welfare by the
equivalent of 3.9 fewer miles traveled per trip, or 27% of households’ mean welfare gain relative to
the outside option. This finding does not change across a wide variety of potential deviations from
truthful play in the DA mechanism. Welfare gains are larger for low-SES households.
    To highlight the importance of subjective beliefs data for this welfare comparison, we estimate
an alternate version of the model that does not use information on subjective beliefs. We assume
that observed application portfolios reflect the Bayes Nash equilibrium in the game induced by the
baseline mechanism. Results from this exercise suggest that switching from baseline to DA would
reduce mean welfare by 1.8 miles traveled. The effect of incorporating data on subjective beliefs
is thus to raise our estimate of the benefit of the switch to DA by 5.7 miles traveled, or 39% of
households’ baseline mean welfare. In sum, when the analysis allows for application mistakes, the
costs of mistakes in the baseline mechanism outweigh the benefits of expressiveness.
    The finding that mechanisms rewarding strategic play outperform DA under the assumption
that households have rational expectations beliefs is consistent with a number of previous papers
in the empirical school choice literature. In the absence of data on beliefs, this research assumes
that participants are informed and sophisticated, or deviate from optimal behavior in specific ways.
For example, Agarwal and Somaini (2018) assume, as a baseline specification, that participants
are fully rational and correctly anticipate their chances in the lottery when choosing applications.
Alternatively, Calsamiglia and Güell (2018) consider school choice under a Boston mechanism in
Barcelona. They allow two types of participants: one type is sophisticated and informed while
the other type uses a rule of thumb to determine choices. Calsamiglia et al. (2018), He (2012),
and Abdulkadiroğlu et al. (2017b) take similar approaches. Our findings show that accounting for
application mistakes in an empirically guided way reverses the welfare comparison between deferred
acceptance and a mechanism that rewards strategic play. To the best of our knowledge this is the
first paper to collect belief and preference data from actual and potential school choice participants.4
   4
    Two recent papers incorporate some survey elements to unpack school choice participation decisions and reports.
Dur et al. (2018) make use of data on the frequency with which students access a school choice website to proxy


                                                        4
    Results from our best-case informational intervention suggest that the baseline mechanism could
yield aggregate welfare outcomes similar to DA if the district could help households learn to play
optimally. An intervention that allows all households to make choices using rational expectations
beliefs would raise welfare by the equivalent of 4.5 fewer miles traveled (31%) relative to the observed
baseline, or by 0.55 miles (3%) relative to the DA counterfactual. Descriptive evidence that using
district-provided informational resources does not reduce belief errors suggests that the form of the
best-case intervention may differ from what was available to households during the study period.


2     Empirical Setting
2.1    The school choice process in New Haven
We study the school choice process in New Haven, Connecticut, an urban district composed mostly
of lower-income minority students.5 The school choice system includes district-run schools and
charter schools run by outside operators, such as ‘no excuses’ charter Achievement First.
    The school choice process begins in January, when students and families can learn about schools
and the choice process by visiting schools or attending ‘magnet fairs’ where schools set up informa-
tion booths. The district provides students with descriptions of the rules of choice, data on available
seats, and applicant counts by priority group from the previous year. Students typically submit their
applications in February, and receive notice of their placements in late March or April. School choice
institutions in New Haven resemble those in other districts that offer centralized choice, and have
been around for long enough that they are familiar to students and parents.6
    We focus our analysis on eighth grade students living in New Haven who are making choices
about where to attend high school. We conducted two surveys, one in the school year ending in
2015 and the other in the school year ending in 2017. In the 2015 (2017) school year, there were
1,544 (1,645) potential ninth graders. Of this group, students who do not leave the city or enroll in
for strategic and sincere participants in a school choice mechanism. Students who visit the site multiple times are
assumed to be sophisticated, while those visiting only once are assumed sincere. de Haan et al. (2015) measure
cardinal utility in Amsterdam using a survey that asks students to assign points to each school, with the top choice
receiving 100 points, but do not ask about beliefs. Neither paper incorporates survey data on beliefs into a model of
household behavior or considers counterfactuals that vary the information available to households.
    5
      Over 80% of New Haven students are black or Hispanic, and the majority are eligible for free or reduced price
lunch. See Online Appendix Table A1 for district-level descriptive statistics.
    6
      New Haven has used centralized choice since at least 1997. New York introduced a centralized application in
2003, followed by Denver, New Orleans, Newark, and Washington DC (Abdulkadiroğlu et al., 2017a). Other districts
offer a similar mix of schooling options and choice calendars. See Corcoran et al. (2018) (New York), and Agarwal
and Somaini (2018) (Cambridge).



                                                         5
private school may enter a lottery to enroll in one of 12 high schools. Ten of these are public schools
and two are charter schools. Two schools are K12 institutions that offer spots to already-enrolled
eighth graders outside of the choice process and use the centralized process to fill remaining seats.
Students who do not apply or who are not placed and who are not already enrolled in a K12 school
are assigned to one of two neighborhood schools according to geographic zone boundaries.
   High school choice for rising ninth graders is part of a larger choice system in New Haven. We
focus on grade nine students because the assignment mechanism New Haven used in 2015 more
closely resembles the mechanisms used in other districts for high school choice than for primary
school choice. Some high schools reserve seats for suburban applicants. We exclude these seats
from our sample and focus on the seats reserved for within-city applicants.

2.2   School choice mechanisms in New Haven
The district used different mechanisms to assign students to schools in our two survey years. Be-
ginning in 2016, the district used the Boston mechanism to assign students to schools. This was
the mechanism in place during our 2017 survey. Prior to 2016, the district used an alternative
mechanism that we label the ‘New Haven’ mechanism. The difference between the two mechanisms
is that in the Boston mechanism, the rank in which a school is listed on the application takes
precedence over a student’s priority group when determining placement outcomes, while in the New
Haven mechanism the reverse is true. When all students have the same priority, the Boston and
New Haven mechanisms coincide. This is approximately the case for high school choice in New
Haven. In this section we describe how the two mechanisms work, and show that the New Haven
mechanism closely resembles the Boston mechanism for ninth grade applicants.
   Most school choice mechanisms use some form of coarse priorities to favor certain applicants.
In New Haven, each student is assigned a priority at each school j ∈ J, which is a number between
one and two:                               
                                           1     if i has a sibling at j
                              priorityij =
                                           2     otherwise

Similar priority structures are in place in Boston, Cambridge, New York, Barcelona, Beijing, and
other cities. The priority groups in New Haven do not change over the years we study.
   The New Haven mechanism assigns students to schools using the following algorithm:
  1. Consider each student’s first choice submission. Each school ranks applicants up to its ca-
      pacity, in order of priority group, using random lottery numbers as a tiebreaker. Each school
      provisionally accepts students up to its capacity and rejects the rest of its applicants.

                                                  6
   2. Consider the next listed choice of students who were rejected in the previous step, together with
      the applications provisionally assigned in the previous step. Make provisional assignments at
      each school in order of a) priority group and b) submitted rank, again using lottery numbers
      as a tiebreaker.

   3. Repeat Step 2 until all students are provisionally assigned to schools or have been considered
      and rejected at each listed school.

   4. Following the conclusion of Step 3, permanently assign students to the schools where they are
      provisionally assigned.

The mechanism assigns each student to at most one school. Students may choose to accept or
decline this placement. Students who are unplaced or decline their placement have the option to
enroll in their neighborhood school or leave NHPS.
    Like the familiar student-proposing deferred acceptance algorithm, the New Haven mechanism
employs provisional assignment. It differs from the standard deferred acceptance approach (Roth,
2002) in the use of submitted ranks to break ties within priority groups. The centralized mechanism
in New York also combines provisional assignments with the use of submitted ranks as tiebreakers
(Abdulkadiroğlu et al., 2005b). However, while in the New York mechanism the set of student-
school-rank combinations for which such tiebreakers play a role is relatively small,7 New Haven uses
rank-based tiebreaks for all applications.
    To compare the New Haven mechanism to Boston and deferred acceptance mechanisms, we
employ a cutoff representation of matching algorithms introduced by Azevedo and Leshno (2016)
for stable matchings and extended to a class of ‘report-specific priority plus cutoff’ mechanisms by
Agarwal and Somaini (2018). The cutoff representation of the New Haven mechanism is as follows.
The mechanism assigns student i a ‘report-specific priority’ at school j when i submits rank-order
list a, given by:


                                      rspij (a) = R × priorityij + rankija ,

where R = 4 is the maximum number of schools permitted on an application, and rankija is j’s
rank on application a.8
    Ties are broken with uniform random draws that assign each student a score at each school:
     7
       A subset of New York schools offered automatic admission to students scoring in the top 2% on a standardized
exam who rank a school first on their application list.
     8
       That is, if j is ranked rth on a, then rankija = r. Report-specific priority rspij (a) is undefined when j is not
listed in a, but this does not matter because i cannot be placed in a school he did not apply to.


                                                           7
                              scoreij (a) = rspij (a) + zij , zij ∼ U [0, 1].

   The resulting assignment is characterized by cutoffs πj that fill schools’ capacities when each
student is matched to his earliest-listed school at which scoreij < πj . If a school is undersubscribed,
its cutoff is set above all applicants’ scores. The New Haven mechanism is a mapping from profiles
of applications to distributions over cutoffs π ∈ RJ .
   The New Haven mechanism differs from Boston and student-optimal stable matching (“SOSM”)
mechanisms in the construction of rspij (a). In the New Haven mechanism, report-specific priority
depends lexicographically on the exogenous priority priorityij and the rank that the student assigns
to the school. In the Boston mechanism, this lexicographic order is reversed. Sibling priority plays a
relatively less important role and submitted rank lists a relatively more important role in determining
report-specific priority. In the Boston mechanism in our setting, report-specific priority is given by

                              rspBoston
                                 ij     (a) = priorityij + T × rankija ,

where T = 2 is the number of distinct priority groups.
   In the SOSM mechanism, report-specific priorities depend on the exogenous priority group only,
and not on school j’s position on i’s submitted rank-order list a. That is, for all a,

                                      rspSOSM
                                         ij   (a) = priorityij .

The New Haven Mechanism differs from the SOSM mechanism in that the tiebreaking rule within
priority groups depends on submitted ranks.
   When all students have the same priority, the Boston and New Haven mechanisms produce the
same assignments. In our setting, students are assigned to unconstrained neighborhood schools
outside of the choice process, and few students have sibling preference. The New Haven mechanism
and the Boston mechanism are therefore quite similar. Table 1 describes placement outcomes and
priority groups for ninth grade applicants in 2015 and 2017. As shown in Panel A, 7% of applicants
in 2015 and 8% of applicants in 2017 applied to at least one school where they had sibling priority,
with the remaining students having no priority at any listed school. Simulation indicates that,
because few students have sibling priority, the change in assignment mechanism has little effect
on assignment outcomes. Using the 2015 application data, we simulate random lottery draws 500
times, running both the Boston mechanism and the New Haven mechanism each time. The mean
share of placements that differ across mechanisms is 1.18%.


                                                    8
                    Table 1: Placement outcomes and priority groups by year

                                                           All    2015    2017
                      A. Priorities
                      Any sibling priority                0.08    0.07    0.08
                      None                                0.92    0.93    0.92

                      B. Participation and placement
                      Submits applications                0.68    0.66    0.70
                      Participates in choice              0.66    0.66    0.66
                      Places first                        0.61    0.64    0.59
                      Places second                       0.09    0.11    0.07
                      Places third                        0.02    0.04    0.00
                      Places fourth                       0.01    0.02    0.00
                      Unplaced                            0.26    0.18    0.34
                      N                                  3,189    1,544   1,645
                      Placement outcomes and priority group for in-district
                     eighth graders by year. Students participate in choice
                     when they submit a lottery application containing at least
                     one non-neighborhood school. Placement outcomes and
                     priorities are conditional on participation. ‘Unplaced’ tab-
                     ulates students who do not receive a placement during the
                     main lottery or who are placed into their neighborhood
                     schools (2017 only).


   As shown in Panel B, 66% of applicants in 2015 and 70% of applicants in 2017 submitted
applications to the centralized system. In 2015, the electronic application did not allow students
to list their neighborhood school, while in 2017 students were permitted to list the school, and 4%
of students listed it first. The share of students participating in choice– defined as submitting an
application with a non-neighborhood school listed first– was thus 66% in both years. Conditional
on participation, about 60% of students placed first in each year. A small number of students in
2015 placed in their second- through fourth-listed choices, while in 2017 no student placed lower
than second. The remainder were unplaced.




                                                  9
2.3     Placement chances and cutoff representations
An appealing feature of the cutoff representations of the New Haven and Boston mechanisms is
that placement probabilities for student-school pairs are determined by the cutoff vector π and
the students’ rspij (ai ) under the applications that they submitted. Consider a rank-order list
a : j1  . . .  jk . We say that j  j 0 if school j is listed ahead of school j 0 on application a.
The probability that applicant i will be assigned to school j given that he submits report a to the
mechanism is

      pija = P r zij ≤ πj − rspij (a), zij 0 > πj0 − rspij 0 (a) for all j 0 such that ∃r0 < rankija : rankij 0 a = r0
                                                                                                                         
                                                                                                                              .

In the next section, we use this formulation to simulate rational expectations (or ‘RatEx’) admis-
sions chances for observed and hypothetical application portfolios.


3     Household Survey
3.1     Survey overview
We conducted in-person interviews with the parents or guardians of 417 rising ninth graders begin-
ning in the summers following the 2014-2015 (henceforth ‘2015’) and 2016-2017 (henceforth ‘2017’)
school years. We drew our sample from the universe of New Haven residents enrolled in New Haven
public schools. We interviewed 120 households in 2015 and 297 households in 2017. Our survey
team conducted interviews at parents’/guardians’ residences using a tablet application that gener-
ated questions tailored to each household and recorded respondents’ answers. Both the 2015 and
2017 surveys included questions on preferences and beliefs about admissions probabilities. The 2015
survey included questions on sources of information and consideration sets, while the 2017 survey
included measures of preference intensity.
    We describe survey procedures in Online Appendix E and present question text in Online Ap-
pendix F. Several survey design elements are important to highlight. The first is timing. We
surveyed households in the summers following the student’s eighth grade year. Our interviews thus
took place after households learned of choice placements. An alternative approach is to conduct
surveys prior to the choice process. The post-application approach has two advantages. The first
is that it cannot alter choice behavior. An ex ante survey would likely affect behavior by pushing
respondents to think through the outcomes resulting from different application portfolios. The sec-
ond is that the process of information gathering is complete. A survey conducted in advance of
the submission deadline will not capture ‘finalized’ beliefs and preferences for households that wait
until the deadline to think through the process.


                                                              10
   There are also disadvantages. Respondents may forget the preferences and beliefs they took
as inputs to choice, or may update preferences and beliefs in response to placement outcomes due
to learning or ex post rationalization. We mitigate these disadvantages through survey design
choices, direct measurement, and robustness tests. On the design side, we formulate questions as
hypotheticals set in the past (‘think back to the time you were filling out your own application, or
deciding whether to fill one out,’ and ‘say that you had submitted the following application’) so
as not to highlight respondents’ placement outcomes. We address concerns about forgetfulness or
ex post updating of belief and preference reports by a) testing recall of submitted applications, b)
examining correlations between survey reports and high-stakes application behavior, c) measuring
the effect of placement outcomes on survey reports conditional on applications. Findings from these
exercises suggest that our survey succeeded in capturing inputs to high stakes behavior with limited
ex post updating. In addition, the model-based analysis presented in Sections 4 and 5 explicitly
incorporates measurement error in belief and preference reports.
   72% of respondents who submit an application correctly report the first choice listed on that
application. To assess the sensitivity of our findings to forgetfulness, we examine how belief errors
vary with correct recall (Section 3.7), and how the exclusion of respondents with incorrect recall
from the analysis affects welfare findings (Section 6). The restriction to correct-recall respondents
does not affect our findings in either case. This is consistent with the observations that a) the survey
asks about hypothetical applications, so correct recall of one’s own application is not a direct input
into survey reports, and b) the relationship between belief errors and other measures of engagement
with choice such as submitting an application, stating a preference for a particular school, or using
district-provided information sources is also weak (Section 3.7).
   The second survey design element is the choice of who to talk to. At the high school level,
both parents and students likely have input into the choice process. One concern about surveying
parents/guardians is that the child may have made choice decisions without their knowledge. How-
ever, 74% of parents reported participating in filling out the school choice application, and 92%
report that either they or their child was the ‘most important [person] in deciding which schools
to list.’ Section 3.7 shows that there is little variation in the distribution of belief errors along this
dimension.
   The third survey design element is the survey medium. We conduct surveys in person at students’
homes. We also considered phone surveys and online surveys nested in the choice process. We
ruled out phone surveys due to concerns about takeup, while implementing surveys as part of the
choice process rules out surveying non-participants. The fourth is incentives. We do not incentivize
‘correct’ beliefs, e.g. by paying people to state beliefs that are close to rational expectations chances.

                                                   11
3.2   Coverage
Our survey covers individuals from across the distribution of demographics and participation choices.
Panels A, B, and C of Table 2 compare respondents to the sample universe in terms of student so-
cioeconomic status, race/ethnicity, and English language learner status. We measure socioeconomic
status using poverty rate in the student’s census tract of residence, divided into quintiles. (The
count of students across quintiles is not equal because some tracts are relatively large.) Our survey
population covers each quintile, with some oversampling of lower-income families. In what follows
we define the group of ‘high-SES students’ to be those from the top SES quintile. Black and Latino
students make up 86% of the student population. We undersample black students and oversam-
ple Latinos, but have many students in both groups. Similarly, our sample includes both English
language learners and special education students. The distribution of surveyed students across
neighborhoods closely matches the distribution in the population. See Online Appendix Figure A1.
   Panel D of Table 2 describes school choice participation. Households who participate may list up
to four schools on their application. Our surveyed population somewhat oversamples school choice
participants (76% of respondents vs. 66% in the population), but includes many observations from
both groups. We observe applications of all possible lengths.

3.3   Rational expectations admissions chances
Analyses of effective strategic play and belief errors require estimates of rational expectations beliefs
about admissions chances. We construct a measure that represents the beliefs about admissions
chances that an agent would have if he knew his own report-specific priority, the rules of the
mechanism, schools’ capacities, the number of other applicants, and the underlying distribution of
preference lists and report-specific priorities for other applicants, but did not know which preference
lists and priorities had been drawn from this distribution.




                                                   12
     Table 2: Characteristics of population and survey respondents

                                 Population      Surveys      Pop v.
  Category                         Mean           Mean        Survey
 SES quintile
 Bottom 20%                         0.24           0.27        0.03
 20-40%                             0.18           0.22        0.05
 40-60%                             0.24           0.23       −0.02
 60-80%                             0.15           0.14       −0.01
 Top 20%                            0.20           0.15       −0.05

 Race/Ethnicity
 Black                              0.46           0.36       −0.11
 Hispanic                           0.40           0.53        0.14
 White Non-Hispanic/Other           0.13           0.10       −0.02

 Educational program
 English language learner           0.13           0.20        0.07
 Any special education              0.20           0.20        0.01

 Number of applications
 Participates in choice             0.66           0.76        0.12
 1                                  0.14           0.13        0.01
 2                                  0.21           0.22        0.04
 3                                  0.22           0.24        0.05
 4                                  0.44           0.41        0.01
 N                                  3,189          417
 Means of indicator variables for demographic and socioeconomic char-
acteristics for sample universe and surveyed population. ‘Population’
is universe of New Haven students in 8th grade at time surveyed. ‘Sur-
veys’ describes surveyed households. ‘SES’ represents quintiles of the
distribution of the poverty rate in households’ census tract, using data
from (Bureau, 2016). The count of students across quintiles is not equal
because some tracts are relatively large. ‘Race/Ethnicity’ are observed
in administrative data. ‘Number of applications’ presents counts of
schools listed on choice applications (in 2017, non-neighborhood schools
only), conditional on participation. ‘Pop v. Survey’ column displays
differences between population and survey means, regression adjusted
by adding year fixed effects.



                                  13
    We calculate these probabilities using an approach similar to Agarwal and Somaini (2018).
Within each market (defined here by years) we draw a large number (N = 200) of resampled
markets by sampling from the population iid with replacement. Each resampled market is a list of
individuals with a participation decision, a report if they participated in the lottery, and a priority
at each school. In each resampled market, we solve for market-clearing cutoffs by running the
assignment mechanism.
               n     o
                 (k)
    The cutoffs πj                        allow us to calculate rational-expectations admissions chances. For
                              k=1,...,N
example, if an individual has rspij = 9 in the New Haven mechanism (no sibling priority, first-
ranked school) and lists j first, if the cutoff is π j = 9.4 then the individual has a 0.4 chance of
placing in j. For each individual i, we compute the propensity to place in each school j under the
individual’s observed application and the given cutoff vector, and then average these chances over
the resampled market-clearing cutoffs. Student i’s chance of being placed in school j under report
a is given by


pija = P r zij ≤ πj − rspij (a), zij 0 > πj0 − rspij 0 (a) for all j 0 such that ∃r0 < rankija : rankij 0 a = r0
                                                                                                                   

             N
           1 X                                                                                                           
               P r zij ≤ πj − rspij (a), zij 0 > πj 0 − rspij 0 (a) for all j 0 such that ∃r0 < rankija : rankij 0 a = r0
                          (k)                     (k)
       ≈
           N
             k=1




3.4        Preference data and strategic play
Together with application data, preference reports suggest that many households play strategically.
Our survey asked respondents to list their first- and second-most preferred schools if they could
choose to attend any school with certainty. As shown in the left two bars of Figure 1, 32% of
respondents who submit an application list a school other than their stated most-preferred school
first. We label this set of respondents ‘revealed strategic.’9 Of these, roughly half list their stated
second-most preferred school first, so that overall 81% of respondents list one of their two most-
preferred schools first on their application.
    Rates of revealed strategic play vary with reports of preference intensity. In our 2017 survey,
we measured cardinal preferences in addition to ordinal preferences. We asked respondents whether
   9
    It is possible there is measurement error in preference data such that not all of these households to which we
apply this designation are in fact strategic. Our analysis in Section 4 incorporates survey measurement error.



                                                              14
they would prefer a lottery that assigned them to their most-preferred school with probability X and
to their neighborhood school (no placement) with probability 1 − X to a sure assignment to their
second-most-preferred school, with X equal to 0.25, 0.5, and 0.75. We label the 68% of students who
report a willingness to accept at least one of these lotteries as ‘strong first preference’ students.10
The right three groups of bars in Figure 1 describe application behavior for the 2017 sample overall,
the strong first preference sample, and the weak first preference sample, respectively.


                    Figure 1: Revealed strategic play overall and by preference intensity
          .4
          .3
       Fraction
         .2
          .1
          0




                                                                                   st                              st
                         All                        2017                   Strong 1 pref.                  Weak 1 pref.

                               Revealed Strategic          Mistaken Strategic               Second favorite is first listed



        Share of revealed strategic and mistaken strategic households overall, in 2017 only, and by intensity of
        preference for listed first choice. ‘Revealed strategic’ households are those who list a program other than
        their stated most-preferred school first. ‘Mistaken strategic’ are the subset of revealed strategic households
        whose rational expectations admissions chances are higher (if listed first) at their most-preferred school
        than at their first-listed school. ‘Second favorite is first listed’ gives the rate at which unconstrained
        second choice schools are listed first.


    Households that report preferring their most-preferred school more strongly relative to their
second-most preferred school are more likely to list it first on their application. In the full 2017
sample, 29% of students who submitted an application were revealed strategic. In the strong first
preference group, 24% of students were revealed strategic, compared to 39% of students in the weak
  10
      We cannot reject the null hypothesis that the likelihood of listing the most-preferred program first is equal for
different minimum acceptable values of X at conventional levels.


                                                               15
first preference group, for a gap of 14 percentage points, or 58%. The p-value from a test of equality
across the strong- and weak-first preference groups is 0.050.
   A large share of strategic households appear to be making mistakes. We define ‘mistaken strate-
gic’ as a household that is revealed strategic but for which the first-listed school on a submitted
application offers lower odds of admission than the household’s most-preferred school. This is a
mistake because the student could have obtained a greater chance at attending a more-preferred
school by substituting his or her most-preferred school for the first-listed school on the application.
The unfilled bars in Figure 1 show the share of mistaken strategic individuals. 48% of revealed
strategic applications (16% of applications in the sample) are mistaken strategic. That students
attempt to play strategically but appear to make errors while doing so is consistent with evidence
from beliefs data we discuss in the next section.
   Households form preferences after considering many schools. 20% of surveyed students in 2015
considered each school in the district and two-thirds considered at least half of schools. Online
Appendix Table A2 presents statistics for each school. The school-by-school statistics illustrate
how the use of application data to infer preferences can be misleading in a strategic setting. For
example, Co-op Arts is the most preferred school for 19.2% of students but appears first on 10.9%
of applications, while Engineering & Science is most preferred for 10.5% of students but appears
first on 21.6% of applications.

3.5   Beliefs about admissions chances
We next document respondents’ beliefs about admissions chances and compare them to objective
measures of admissions probabilities. We define optimismija as the difference between i’s reported
subjective belief about his admissions chance at j under application a, p̂ija , and the rational-
expectations chance pija :


                                      optimismija = p̂ija − pija

   The survey asked respondents about their beliefs for schools ranked first and second on two
hypothetical applications, for a total of four elicited beliefs per respondent. Since some respondents
declined to answer some questions or were asked about schools to which they could have been
admitted outside the centralized process, we obtained a total of 1,159 elicited beliefs about admission
to some school j under an application that listed j. We chose hypothetical applications that
contained a mix of nearby schools, high-performing schools, and popular schools at the district level.
The distribution of rational expectations admissions probabilities for the hypothetical applications


                                                    16
is similar to the distribution of rational expectations probabilities for the actual applications that
households in our sample submitted. See Online Appendix Figure A2 for the distribution of rational
expectations probabilities in hypothetical and submitted applications.
    The survey elicited subjective probabilities in bins with widths of 10 percentage points (1 to
10%, 11 to 20%,..., 91-100%). For second-ranked options, the survey elicited beliefs conditional on
non-admission to the first ranked option.11 To facilitate graphical comparison between rational ex-
pecations and subjective probabilities, we place the (conditional) rational expectations probabilities
in the same set of bins as the subjective probabilities. When computing averages of subjective expec-
tations and differences between rational expectations and subjective expectations, we set subjective
expectations to the midpoint of the reported bin.


                                                                 Figure 2: Distribution of beliefs and optimism

                                          Observed and RatEx beliefs                                                                                   Distribution of optimism
                                                                                                                                        .15
        .4




                                                                                                                                                                                    Mean optimism: 7.78
                                                                                                                                                                                     SE optimism: 45.49
                                                                                                                                                                                     Mean |error|: 37.21
        .3




                                                                                                                                        .1
    Fraction




                                                                                                                             Fraction
       .2




                                                                                                               −0.01

                   −0.18                                                                                       (0.02)
                                                    0.12
                   (0.02)
                                                    (0.01)
                                                                                                                                        .05




                                                             −0.03             0.07

                                                             (0.02)            (0.01)
        .1




                                                                      −0.04             0.02
                                          0.04                        (0.01)            (0.01)        0.00
                            0.01
                                          (0.01)                                                      (0.01)
                            (0.01)
        0




                                                                                                                                        0




               0                     20                 40             60                        80                 100                       −100   −50                  0            50              100
                                                    Probability of Acceptance                                                                                         Optimism

                                                   Subjective Belief                       RatEx                                                           Optimism              Mean Optimism


        Notes: N=1,159. Left panel: distribution of subjective and rational expectations assignment probabilities.
        Right panel: distribution of optimism. Bars show shares of population within bins of width 10. Red line
        indicates mean of the distribution. In both panels, beliefs for second-ranked options are conditional
        on non-admission to the first-ranked choice. Text in the left panel indicates the gap between rational
        expectations and observed beliefs with standard errors clustered at the respondent level in parentheses
        below. Red line in the right panel shows the distribution mean.
  11
    The 2017 survey also included separate categories for ‘at most 1%’ and ‘at least 99%.’ For cross-year consistency
we aggregate the 2017 survey to 10 point bins as in the 2015 survey when conducting descriptive analysis.




                                                                                                                        17
   The left panel of Figure 2 plots the distribution of rational expectations and subjective beliefs for
the sample of hypothetical applications. The text above each bar displays the difference between the
share of subjective beliefs observations and the share of rational expectations beliefs observations in
the bin. Many fewer respondents believe they have very low chances of admission than actually do.
33% of all elicited probabilities had rational expectations values in the lowest range, but respondents
reported beliefs in this range in only 15% of cases.




                                                  18
                               Figure 3: Distribution of beliefs and optimism by application rank

                                                                                                                              Rank 1
                                                      Observed and RatEx beliefs                                                                                              Optimism




                                                                                                                                                    .15
                  1



                                                                                                                                                                                              Mean optimism: −12.02
                                                                                                                                                                                                 SE optimism: 37.72
                  .9




                                                                                                                                                                                                 Mean |error|: 31.85
                  .8
                  .7




                                                                                                                                                    .1
                      .6
             Fraction




                                                                                                                                         Fraction
                .5.4




                                                                                                                                                    .05
                  .3




                                                                                                                           −0.12
                  .2




                                0.12                             0.12                                                      (0.02)
                               (0.02)                           (0.02)   −0.16
                                                                         (0.02)             0.03
                                                                                  −0.16    (0.02)    0.05
                                                                                                                   0.04
                  .1




                                         0.05          0.02                                         (0.01)
                                                      (0.01)                      (0.02)                          (0.01)
                                        (0.01)
                  0




                                                                                                                                                    0
                           0                     20                 40             60                        80                 100                       −100   −50                  0            50             100
                                                                Probability of Acceptance                                                                                         Optimism

                                                               Subjective Belief                       RatEx                                                           Optimism              Mean Optimism




                                                                                                                              Rank 2
                                                      Observed and RatEx beliefs                                                                                              Optimism
                                                                                                                                                    .15
                  1




                                                                                                                                                                                               Mean optimism: 29.86
                                                                                                                                                                                                 SE optimism: 43.23
                  .9




                                                                                                                                                                                                 Mean |error|: 43.19
                  .8
                  .7




                                                                                                                                                    .1
                      .6
             Fraction




                                                                                                                                         Fraction
                .5.4




                                                                                                                                                    .05
                  .3
                  .2




                                                                                                                            0.12
                               −0.52                             0.13                                                      (0.02)
                               (0.02)                                                       0.12
                                                                (0.02)    0.10             (0.02)
                                                                         (0.02)    0.09
                                                       0.06                       (0.01)            −0.01         −0.05
                  .1




                                        −0.04         (0.01)                                        (0.02)        (0.02)
                                        (0.02)
                  0




                                                                                                                                                    0




                           0                     20                 40             60                        80                 100                       −100   −50                  0            50             100
                                                                Probability of Acceptance                                                                                         Optimism

                                                               Subjective Belief                       RatEx                                                           Optimism              Mean Optimism


     Notes: N=1,159. Upper panel: beliefs for first-ranked applications. Lower panel: beliefs for second-
     ranked applications. Left panel: distribution of observed and rational expectation chances. Right panel:
     distribution of optimism. Bars show shares of population within bins of width 10. Beliefs for second-ranked
     options are conditional on non-admission to the first-ranked choice. Text in the left panel indicates the
     gap between rational expectations and observed beliefs with standard errors clustered at the respondent
     level in parentheses below. Red line in the right panel shows the distribution mean.


   The right panel of Figure 2 plots the distribution of (conditional) optimism in the sample of
hypothetical applications. Respondents overestimate their conditional admissions chances by 8
percentage points on average, and the spread around this value is wide. The mean absolute error
in conditional beliefs is 37 percentage points.
   Optimism is systematically related to rank. Figure 3 shows the distribution of beliefs and

                                                                                                                                    19
optimism by submitted rank. Households are an average of 42 percentage points more optimistic
about second-ranked options than first ranked options, for which optimism values are centered just
below zero. This reflects a large decline in rational expectations probabilities between the first and
second ranked choices coupled with almost no change in subjective beliefs. The observed distribution
of optimism suggests beliefs to not correspond to rational expectations, and that a realistic model
of belief errors should allow for systematic variation by rank. We return to this point in section 4.

3.6   Validating belief and preference data
Survey results may provide flawed measures of inputs to the application process. One concern
is measurement error. Respondents may report noisy or systematically biased measures of their
true beliefs and preferences. A second concern is ex post changes in beliefs or preferences. Our
survey took place after the realization of lottery outcomes. Students may adjust reported beliefs
to reflect what they have learned from lottery outcomes, or may revise their preferences ex post
in response to placement outcomes. A third concern is private information. If our model of the
assignment process is incomplete and students have information about their application portfolio
or the assignment mechanism that we do not, we may record accurate subjective beliefs as errors
because our rational expectations benchmark is wrong.
   This section presents descriptive evidence on each of these issues. We first consider ex post
updating in response to placement outcomes. Columns 1 and 2 in Panel A of Table 3 show how
placement outcomes relate to reported preferences. The outcome in both columns is an indicator
variable for reporting the first-listed school on the application as the most-preferred school in our
survey. The independent variables in column one are the rational expectations admissions chances
and an indicator for placement in the first choice school. We fail to reject a null of zero placement
effect (p=0.560). The second column adds controls for subjective beliefs. We fail to reject the null
that coefficients on placement and subjective beliefs are jointly zero (p=0.658). This suggests a
limited role for ex-post revision to reported preferences in response to placement.
   We next consider private information and belief updating in response to placement. Columns
three through five in Panel A of Table 3 describe the relationship between rational expectations
beliefs, subjective beliefs, and application outcomes. Let placei1 be an indicator variable equal
to one if a student is placed in his or her first-listed school on the choice application, pi1a∗ be
the measured rational expectations admissions probability at that school for observed application
portfolio a∗ , and p̂i1a∗ be i’s reported subjective belief. If our model of the assignment mechanism




                                                 20
is accurate and students do not update beliefs in response to placement outcomes,

                     E [placei1 |pi1a∗ = p, p̂i1a∗ = s] = E [placei1 |pi1a∗ = p] = p.

We test this restriction using linear probability specifications of the form

                                placei1 = α0 + α1 pi1a∗ + α2 p̂i1a∗ + ei .

Under the null hypothesis of an accurate assignment model and no updating, we expect α0 = 0,
α1 = 1, and α2 = 0. We would expect to reject the null if respondents had private information
about placement probabilities, if respondents updated their beliefs in response to placement, or if
we mis-specified our model of rational expectations chances.
   Column three shows results from a linear probability specification in the full sample of ninth
grade choice participants in which the outcome is first-choice placement and the only covariates
are our rational expectations belief measure and a constant. We estimate a coefficient of 0.990 on
rational expectations beliefs and an intercept of approximately zero. We fail to reject the joint null
of zero constant and slope of one (p=0.183). Column 4 repeats this test in the sample of surveyed
school choice participants for whom beliefs about first choice schools are available (N=186). The
slope is again close to one and the intercept close to zero, and we cannot reject the joint hypothesis
that our rational expectations estimates model is correct (p=0.594). Column 5 adds subjective
beliefs to the regression. We fail to reject the null hypothesis that our rational expectations model
is correct and that conditional on rational expectations beliefs, subjective beliefs have no effect on
placement (p=0.460). We also fail to reject the alternate null that the subjective beliefs coefficient
itself is zero (p=0.203). Our findings suggest that our rational expectations values accurately reflect
the placement process, and that the effect of placement outcomes on reported beliefs is limited.
   Finally, we consider whether subjective beliefs predict high-stakes choices. Figure 4 and Panel
B of Table 3 report evidence indicating that subjective beliefs predict choice but that, conditional
on subjective beliefs, rational expectations beliefs do not. Column one in Panel B reports results
from the linear regression of an indicator for listing the most-preferred school first on the school
choice application on subjective admissions beliefs. The sample is the group of students who submit
a school choice application and for whom we have an elicited belief about admissions probabilities
at the most preferred school when it is ranked first. The intuition is that a student who believes
placement at his most-preferred school is more likely will be more likely to list that school first.




                                                   21
                      Table 3: Subjective vs. RatEx beliefs and application behavior


  A. Testing survey quality
                                (1)                      (2)              (3)           (4)                  (5)
                      State 1st listed as MP   State 1st listed as MP    Placed        Placed               Placed
  Subjective belief                                    0.073                                                 0.127
                                                      (0.104)                                               (0.099)
  RatEx                       −0.072                  −0.084              0.990        0.956                 0.927
                              (0.158)                 (0.160)            (0.027)      (0.094)               (0.095)
  Placed                       0.047                   0.041
                              (0.080)                 (0.081)
  Constant                     0.723                   0.694             −0.005        0.057                 0.007
                              (0.090)                 (0.098)            (0.023)      (0.080)               (0.091)
  Dep. var. mean              0.706                    0.706              0.616        0.634                 0.634
  Model test                  0.560                    0.658              0.183        0.594                 0.460
  N                            186                      186               2, 101        186                   186
  B. Beliefs and application choices
                               (1)                     (2)                 (3)          (4)                 (5)
                          Rank MP 1st             Rank MP 1st           Place MP    Place MP      Place MP | Rank MP 1st
  Subjective belief            0.189                   0.196              0.304        0.206                 0.151
                              (0.095)                 (0.097)            (0.120)      (0.120)               (0.123)
  RatEx                                               −0.056                           0.797                 0.920
                                                      (0.138)                         (0.135)               (0.129)
  Constant                     0.704                   0.734              0.368       −0.060                 0.009
                              (0.063)                 (0.094)            (0.077)      (0.096)               (0.109)
  Dep. var. mean              0.805                    0.805              0.530        0.530                 0.644
  Model test                                           0.687                                                 0.453
  R2                          0.087                    0.079              0.088        0.220                 0.317
  N                            164                      164                164          164                   132

 Robust standard errors in parentheses. Panel A sample is students for whom we observe beliefs about first listed schools,
except (A3), which is the entire universe of first-listed schools for students not applying to their neighborhood school. Panel
B sample is students for whom we observe beliefs about first listed schools and covariates. Regressions in Panel B contain
de-meaned controls for year, SES quintile, race, gender, and whether a student has a continuation option at either Achievement
First or Engineering and Science University Magnet. Subjective belief are observed subjective belief probabilities (on 0-1)
while RatEx reflect rational expectations chances of admission. Placed is an indicator for placement during the initial lottery.
Model test displays p-values for a variety of statisitcal tests: (A1) Placed = 0 (A2) Subjective belief = 0, Placed = 0 (A3-A4)
RatEx=1, constant = 0 (A5) Subjective belief = 0, RatEx=1, constant = 0 (B2) RatEx=0 (B5) Subjective belief = 0, RatEx
= 1. Appendix Table A4 reports alternate versions of the model tests in Panel A that condition on students’ most-preferred
and first-listed schools for columns 1 and 2 and colums 3 through 5, respectively.

   We find an economically large and statistically significant relationship between subjective beliefs
and application behavior. A decrease in subjective beliefs corresponding to one standard deviation
of the first-ranked-school optimism distribution (38 percentage points) raises the probability a re-
spondent lists a non-most-preferred school first by 7 percentage points, or 37% of the sample mean


                                                                22
rate. Effect size is unchanged when we add controls for rational expectations beliefs (column 2).
The effect of rational expectations chances are close to zero. Figure 4 shows binscatter plots for
each bivariate relationship (conditional on the other). Rates at which students rank their most
preferred school first rise by 17 percentage points from the bottom quartile of the subjective belief
distribution to the top quartile.
   We note that this analysis is non-experimental. These patterns could arise through channels
other than a causal effect of beliefs on reports to the mechanism. One possibility is that preference
intensity might be positively correlated with subjective beliefs, but not rational expectations beliefs.
Our findings are strongly suggestive but not definitive evidence (of the type that might come from
a randomized informational intervention) that subjective beliefs affect reports to the mechanism.

           Figure 4: Fraction listing most-preferred first by subjective and RatEx beliefs

                                                                  By belief | RatEx                                                              By RatEx | belief
                                         .9




                                                                                                                        .9
                                                      .85




                                                                                                                                     .85
                         Fraction ranking MP school 1st




                                                                                                        Fraction ranking MP school 1st
                                        .8




                                                                                                                       .8
                       .75




                                                                                                      .75
                                         .7




                                                                                                                        .7




                                                            0   20     40        60       80   100                                         0   20      40       60       80   100
                                                                 Subjective beliefs | RatEx                                                     RatEx | Subjective beliefs

     Notes: N=164. Points are binned means within quartiles of belief type listed in title. Means and fitted lines
     are obtained using regressions of the dummy for listing the most-preferred school first on the listed type,
     controlling for year, other belief type, SES quintile, race, gender, and whether a student has a continuation
     option at either Achievement First or Engineering and Science University Magnet. Covariates are set to
     mean values. Binscatter produced as in Cattaneo et al. (2019).

   Because subjective beliefs influence application behavior, they affect placement. Columns three
and four of Panel B report specifications that parallel those in columns one and two but with place-
ment in the most-preferred school as the outcome. A one-standard deviation increase in subjective
beliefs corresponds to an 8 percentage point increase in the rate students are placed in their most-
preferred school (column 4). The final column of Panel B repeats the model test from Column 3


                                                                                                 23
of Panel A for the set of individuals who rank their most-preferred degree first. We again fail to
reject the joint null that the coefficient on rational expectations is one, the constant is zero, and
the coefficient on subjective beliefs is zero at conventional levels (p=0.453). We interpret findings
from Panel B of Table 3 as evidence both that subjective beliefs are important in choice and that
our survey recovers credible measures of these beliefs.

3.7    Information acquisition and the correlates of belief errors
Though students use the information the district provides, they do not provide accurate reports
about how the mechanism works. 89% of households report using some administrative information
source, defined here to include a visit to a school or choice fair, reading the choice catalog or choice
website, or talking to a counselor.12 Table 4 presents the fraction of students who correctly answer
questions about the ordering of priority groups and the role of rank in the choice mechanism. Only
10.8% of respondents correctly identified the neighborhood priority group as being preferred to the
sibling priority group, and only 20.6% correctly stated that a student rejected from her first choice
school has a (weakly) lower chance of admission at her second choice school than if she had ranked
the second choice school first. There were three possible responses to each question, so correct
answer rates are worse than under random guessing, and we can reject the null that respondents
perform as well as random guessing at the 1% level in both cases. 3.4 percent of respondents
answer both questions about the choice mechanism correctly. Despite not understanding how the
mechanism works, only 5% of respondents describe the choice process as difficult.
   12
      Online Appendix Table A6 displays the fraction of students who reported using different resources to inform
their school choice decision.




                                                       24
                   Table 4: Difficulty of process and understanding of choice rules

                                                     All    High SES      Low SES     p-value
                  Process difficult                 0.053     0.031        0.057       0.347
                  Understand priorities             0.108     0.094        0.110       0.825
                  Understand ranking penalty        0.206     0.203        0.207       0.868
                  Understand both                   0.034     0.031        0.034       0.719
                 Notes: Columns are samples. ‘High SES’ (N = 55), corresponds to respondents
                 in the bottom quintile of census tract poverty rate while ‘low SES’ (N = 362)
                 corresponds to respondents living in the remaining census tracts. Table reports
                 shares of students who responded correctly to questions about priority ordering
                 and the importance of the submitted rank to admissions outcomes, respectively.
                 Respondents who answered ‘I prefer not to answer’ are coded as not under-
                 standing the mechanism; recoding these values as missing or correct does not
                 affect conclusions that results are worse than random guessing. ‘p-value’ tests
                 the regression-adjusted difference between high and low SES samples, control-
                 ling for year, race, and gender.


   Compared to the relationship between optimism and application rank, other correlates of belief
errors are relatively weak. Table 5 presents results from regressions optimism and absolute errors
on student characteristics and descriptors of household interactions with the choice process. All
specifications include controls for hypothetical application rank and an indicator for whether the
student had sibling priority at the hypothetical school, and year fixed effects. Panel A shows that
respondents who answer both mechanism questions correctly are 18 percentage points less optimistic
on average. As in Figure 3, optimism is much greater at second-ranked schools. It is lower for the
small share of respondents with sibling priority.
   Panel B reports results from regressions with controls for preferences and participation in the
school choice process. Participation in choice has a small and statistically insignificant relationship
with both optimism and absolute error. Optimism is higher at most-preferred schools (first column),
but this relationship is due to the negative correlation between preferences and RatEx admissions
chances, not to any correlation between preferences and subjective beliefs. Controlling for RatEx
chances, the relationship between preferences and optimism disappears (second column). Panel C
shows that demographic variables are weakly correlated with belief errors. Poorer students may be
have somewhat higher rates of large absolute error and lower optimism, but we cannot reject the
null of no effect at conventional levels.
   Additional analyses ask how strategic play, the respondent’s relationship to the student, and the
respondent’s use of information sources relate to belief errors. Online Appendix Table A5 shows
that belief errors are weakly related to strategic play, to whether the respondent helped with or



                                                       25
correctly recalled the application, and to whether the respondent is the student’s mother (the most
common relationship) or not. Online Appendix Figures A5 and A6 provide further evidence that the
distributions of subjective beliefs and optimism are similar across splits by respondent involvement
in choice and correct recall of the submitted application. Finally, Online Appendix Table A6 shows
that relationships between belief errors and the use of specific information sources are also weak. Our
findings are consistent with a story of application behavior in which households know they should
strategize on their schooling applications, but have trouble learning how the mechanism works,
even when they are involved in the application process. Applicants may seek out information about
admissions chances on the basis of participation or preferences, but the effects of this search appear
to be second order relative to their misunderstanding of the mechanism.


                                           Table 5: Correlates of belief errors

                            A. Qualitative responses               B. Preference & participation                 C. Demographics
                            Optimism     Abs. Error     Optimism     Optimism     Abs. Error    Abs. Error    Optimism     Abs. Error
 Hypothetical rank 2           41.7          11.4          41.5          6.4         11.5          10.7          42.0         11.4
                               (1.3)         (1.7)         (1.3)        (1.9)        (1.7)         (1.8)         (1.3)        (1.7)
 Have priority                -24.4           5.6          -26.6         -4.5         5.3           5.7          -23.1         6.1
                              (7.3)          (4.8)         (8.1)        (7.6)        (5.2)         (5.2)         (7.8)        (5.1)
 Understand mechanism         -18.2           0.5
                              (6.8)          (2.7)
 Most preferred                                             9.9          2.7          -0.3          -0.4
                                                           (3.1)        (2.3)        (1.8)         (1.7)
 Filed app                                                  -3.0         -2.9         -3.4          -3.4
                                                           (5.2)        (3.9)        (2.5)         (2.4)
 RatEx                                                                   -0.9                       -0.0
                                                                        (0.0)                      (0.0)
 Tract poverty rate                                                                                              -12.3         8.5
                                                                                                                (16.1)        (8.6)
 Black                                                                                                            -3.7         0.8
                                                                                                                 (4.0)        (2.0)
 White                                                                                                            -0.5         -4.9
                                                                                                                 (5.9)        (3.3)
 Female                                                                                                           0.1          -0.5
                                                                                                                 (3.6)        (1.8)
 N                            1,159         1,159         1,126         1,126        1,126         1,126        1,149         1,149
Standard errors in parentheses. Errors clustered at the student level. Sample sizes change across panels due to covariate availability.
All regressions include year fixed effects and exclude neighborhood schools from the sample. See section 3.7 for additional description.




                                                                   26
3.8   How the descriptive analysis informs modeling choices
We use three stylized facts from our descriptive analysis to inform modeling decisions. First, house-
holds behave strategically, trading off preference intensity against admissions chances. Second, the
admissions probability beliefs that students use to inform these tradeoffs are often in error. Third,
belief errors vary with submitted rank and priority group, but have a weaker relationship with par-
ticipation in choice and preferences over schools. These facts suggest a model of optimizing behavior
in which students are misinformed about admissions chances. This contrasts with ‘naive’ behav-
ior in which students simply list preferences in order and suggests that a realistic model of beliefs
should allow for heterogeneity by position in the application portfolio. There is less evidence that
strategic information gathering on more-preferred schools or by students who participate in choice
drives differences in belief errors. This motivates a choice to abstract from a model of information
acquisition.


4     Model
4.1   Student preferences
Our model consists of four stages. First, applicants learn their preferences over schools and the
costs of applying to schools. Second, they choose whether to participate in the school choice process
and, if they participate, what report to submit. Third, the lottery runs and participants receive
placements. Fourth, students who receive placements choose whether to enroll in the placed school
or to decline their placement. Students who decline a placement or do not receive a placement have
the option to either enroll in their zoned neighborhood school or leave the district. Students who
are enrolled in a K12 school may also choose to remain in that school.
    Students i ∈ I have underlying preferences over schools j ∈ J according to:

                                       uij = δj + Xij β + ij ,

    where δj are a full vector of school dummies and Xij are observed school and student character-
istics. The Xij include distance to the school from home distanceij , and a household-level indicator
for low SES. The errors i are distributed according to

                                         i ∼ M V N (0, Σ),

iid across households.

                                                 27
    In practice, each student has exactly one zoned school at which he is guaranteed a position.13
Each student therefore has an outside option ui0 which consists of the choice between attending this
school and leaving the district. We normalize the value of this outside option: ui0 = 0. Students
who wish to attend their zoned school are encouraged not to submit a lottery application, and it
is not possible to select one’s own zoned school in the online version of the application. Therefore
one’s own zoned high school is part of the outside option.14 Because the relative value of placing in
an inside school depends on the identity of the zoned school and the distance to it, we control for
these characteristics.15 The covariance matrix Σ is unrestricted.
    Once a student is placed in school j, he has the option to decline his placement. At the time
of this decision, students receive a shock to preferences for j and for the outside option, giving a
utility
                                                 Uij = uij + eij

where the enrollment-time shock eij has an extreme value distribution with scale parameter λ1 . The
probability of accepting an offer is therefore

                                                                 exp (λuij )
                                    P (uij + eij > ei0 ) =                   .
                                                               1 + exp (λuij )
    The expected value of school j at the time of matriculation is given by

                                                                1
                           vij = E(max{Uij , Ui0 |uij }) =        log (1 + exp (λuij )) .
                                                                λ
    To permit nonparticipation and short application lists, we allow for a cost of receiving a place-
ment. If i receives a placement in any inside school j, he receives a (possibly negative) payment


                                                bi ∼ N (µb , σb2 ).

    We interpret bi as the cost of the actions i must take to accept or decline a placement. It reflects
the real and psychological costs of finding and getting in touch with the school placement office or
assigned school.
    Students make participation and application decisions to maximize their expected utility given
their subjective beliefs about placement chances. Let p̃ija denote i’s subjective estimate of the
  13
      There are two such schools: Wilbur Cross High School, and James Hillhouse High School.
  14
      One may apply to the “opposite” zoned school via the lottery.
   15
      That is, we include in Xij an indicator for i’s zoned school and the distance to the zoned school. Including
zoned-school dummies and distance-to-zoned-school in each inside option is equivalent to parameterizing the outside
option with those terms.


                                                        28
probability that he will be placed in school j if he submits report a to the mechanism.16 Students
for whom a = ∅ are those who do not participate in school choice. i’s decision solves
                                                                     
                                                  XJ
                                             max    p̃ija (vij + bi ) .
                                               a
                                                     j=1

    The use of subjective beliefs for expected utility maximization is our main innovation relative to
Agarwal and Somaini (2018) and Calsamiglia and Güell (2018), or Abdulkadiroğlu et al. (2017b).
These papers impose rational expectations beliefs and/or stipulate that agents follow ‘rule-of-thumb’
approaches to portfolio choice. Our approach is consistent with findings from survey data that
strategic behavior is common but that beliefs are often wrong. To explore the importance of the
analysis of subjective beliefs for policy conclusions, we estimate additional specifications that impose
rational expectations.
    An alternative modeling approach is to consider only the application decision, treating the choice
to accept a placement as exogenous. In this model, bi ≡ 0, and the value of a placement at j is given
by vij = uij . We estimate this alternate model and report details in Online Appendix C. We prefer
our main model because the choice to accept a placement contains information on preferences that
we would like our estimates to incorporate. Descriptive evidence shows that applicants are more
likely to accept placements at more-preferred schools. See Table A7 in the Online Appendix.

4.2     Beliefs
Inaccurate beliefs about pija may arise because students mis-estimate rspij (a) or the distribution
of cutoff values πj . Mistaken beliefs about these two quantities can arise from similar thought
processes. For example, households who do not understand how priority groups and submitted
rankings jointly determine rspij will have inaccurate beliefs about their own values of rspij (a) and
also about πj even given full knowledge of other households’ submitted applications.
    Errors in beliefs about πj and rspij sum to alter beliefs about admissions probabilities. Let
rsp
 ˜ ij (a) and π̃ij = πj + ∆πij be household i’s beliefs about report-specific priority and the cutoff
score for admission, respectively, with ∆πij ∈ R. Then

p̃ija = P r(zij ≤ πj −rspij (a)−shift*ij (a), zij 0 > πj0 −rspij 0 (a)−shift*ij 0 (a) for all j 0 ahead of j under a)
   16
     Subjective belief p̃ija may differ from reported subjective belief p̂ija due to measurement error in p̂ija . We return
to this point below.




                                                            29
where
                               shift*ij (a) = πj − π̃ij − (rspij (a) − rsp
                                                                        ˜ ij (a)).

The shift*ij (a) term incorporates errors in beliefs about both rspij and πj . Rather than trying to
distinguish between these two closely related sources of error, our empirical model takes a parsimo-
nious approach and focuses on the shift*ij term itself. This choice does not restrict the distribution
of deviations of subjective beliefs from rational expectations values.
   Our survey contains observations of beliefs for some application portfolios. Because the number
of possible portfolios is very large, it is not feasible to survey families about each possible submission.
We therefore use our survey data to estimate a flexible model of belief errors. We allow people to
have mistaken beliefs about their priority or, equivalently, about schools’ cutoffs, and about the role
of priority and the rank of applications. For any application a that ranks school j in the rth place,
we let i’s error be given by
                                             shift*ij (a) = shift ijr

for some shift ijr ∈ R. Taking the individual-rank-school triple as given, belief errors do not depend
on other features of the application. This assumption reduces the dimensionality of unknown beliefs
while allowing for relevant misperceptions and mistakes. We let

               shift ijr = ηi0 + ηir (r − rj ) + ηipriority priorityij − priority j + ηij + ηijr
                                                                                   
                                                                                                       (1)

denote i’s error about his own admissions ranking. Here, r is the rank of j on application a for
student i, and rj is the average rank of applications. Similarly, priorityij is i0 s priority at j and
priority j is the average in the data. This functional form nests several relevant cases. For example,
under the New Haven mechanism, ηir = 0 means students understand how priority groups affect
choices, while ηir = −1 if students do not believe score depends on rank, as if a DA mechanism were
used. ηipriority = −2 corresponds to the case where students’ beliefs about admissions probabilities
do not change with changes in their priority group, while ηi0 captures individual-specific optimism
                  0 captures idiosyncratic person-school error.
or pessimism and ηij
   We assume ηij ∼ N (0, ση2school ) iid across j, and ηijr ∼ N (0, ση2school×round ) iid. The remaining
terms are distributed according to

                                        (ηi0 , ηir , ηipriority ) ∼ N (η, Ση ).

We let ση0 , σηpri , and σηround denote the diagonal components of Ση . This specification allows



                                                         30
us to capture many types of errors. For example, people who misunderstand priorities may also
misunderstand the importance of rank. We allow for separate parameters for students from high-
and low-SES backgrounds to facilitate flexible cross-group comparisons. In addition, we estimate
separate models for 2015 and 2017 because the rspij (a) are constructed differently in the New Haven
and Boston mechanisms and units have different interpretations. See section 2.2 for details.
   One limitation of our approach is that it maintains the assumption that beliefs are independent
of preferences. In particular, because we do not model households’ search for information, we cannot
address counterfactuals in which information acquisition behavior may differ endogenously. Though
endogenous information acquisition is surely a first-order issue in many settings, there are several
reasons to think its importance may be more limited here. First, our main counterfactuals focus
on the DA mechanism, in which optimal play does not require knowledge of admissions chances.
Second, survey evidence suggests that the costs of information acquisition on the margin may be
prohibitively large in our setting. See sections 3.7 and 3.8 for a discussion. We leave the challenge
of modeling information acquisition to future research.

4.3   Modeling institutional details
We adapt our model to incorporate several idiosyncratic features of the New Haven setting. These
affect small numbers of students. First, at the two K12 schools (Achievement First and Engineering
and Science), current eighth graders have the option to continue their enrollment without partici-
pating in the choice process. There are 179 such students in 2015 and 189 in 2017. We incorporate
the option to stay in the current school into the outside option, and allow outside option value
to vary with the identity of the current school for these individuals. Second, the school aimed at
students expelled from other schools (Riverside) accepts applications through the centralized system
but makes offers on a different day than other schools and never rejects applicants. We model appli-
cants to this school as having the option to enroll if they want to, so that they are choosing between
their zoned school, their placed school (if they have one and it is not Riverside), and Riverside at
the enrollment stage. We observe 22 students placed at this school in total over both years. Third,
households may apply to specific programs within an arts-themed school (Co-Op Arts). We treat
Co-Op as one school in our analysis.




                                                 31
5     Estimation
We use a Bayesian Markov-Chain Monte Carlo (MCMC) procedure to estimate the model and
sample from the posterior distribution of counterfactual outcomes. Similar methods have been used
successfully in the marketing and industrial organization literatures to model consumers’ demand
for goods (McCulloch and Rossi, 1994) and have been applied successfully to centralized school
choice (Agarwal and Somaini, 2018). Our strategy extends these methods to make use of surveyed
beliefs and preferences as well as data on the decision to accept or decline a placement. We provide
a sketch of our approach here with details in Online Appendix B.
    We use a two-step procedure. In the first step, we estimate the distribution of market-clearing
cutoffs at each school, which determine the rational-expectations chances of admission at each school
conditional on a priority vector and a report. Second, we use the survey and administrative data
together with the distribution of market-clearing cutoffs to estimate the parameters of the model.
To do so, we use data augmentation to pick utility vectors, beliefs, cost terms b, and measurement
error terms for each individual consistent with their choices. If individuals are surveyed, these terms
must be consistent with their survey responses as well. We introduce prior distributions for the
model parameters, and use MCMC in order to sample from the posterior distribution of parameters
conditional on the data. In order to obtain distributions of outcomes under counterfactuals, we
simulate alternative policies at many points drawn from this posterior distribution. This approach
allows us to model belief errors even for non-surveyed individuals.
    In summary, the survey is used in three ways. First, it is used to estimate the parameters of
the belief model. Intuitively, the survey plays the critical role in pinning down the distribution of
belief errors, but belief errors help rationalize observed choices for both surveyed and non-surveyed
students. Second, the survey imposes restrictions directly on beliefs of surveyed households. Sur-
veyed households’ values of shift ijr , together with their belief measurement error terms, must be
such that their reported subjective beliefs lie in the intervals that they declared. Third, the survey
constrains the preferences of surveyed households. The two reported most-preferred schools must
give the highest utility up to measurement error.

5.1   Recovering preference and belief parameters
Before we describe the estimation procedure in detail, we discuss the restrictions implied by house-
holds’ optimal application decisions, accept/decline decisions, and reported first and second choices,
as well as the normalizations we make.



                                                  32
5.1.1    Optimality of applications

Let vi = (vi1 , . . . , viJ ) denote the vector of inclusive values of admission to each of the J schools, and
let pi (a) denote the vector of i’s subjective beliefs about admissions chances under report a. Agarwal
and Somaini (2018) observe that a report a is optimal for agent i if and only if vi · pi (a) ≥ vi · pi (a0 )
for all reports a0 . Hence, given the matrix Γi = (pi (a) − pi (a1 ), . . . , pi (a) − pi (aN )), a report is
optimal if and only if Γ0i (vi + bi ) ≥ 0.
    Optimal applications depend on beliefs, which depend on the distribution of cutoffs. The model
may therefore exhibit multiple equlibria. Conditional on a distribution over cutoff vectors, however,
each household faces a single-agent decision problem. Because we estimate and condition on the
cutoff distribution that occurred in the data, potential multiplicity is not a problem for estimation
of beliefs or preferences.

5.1.2    Reported preferences

In the survey we elicit households’ first and second choices if parents could choose any school,
unconstrained by admissions chances. We allow for measurement error in elicited preferences: If i
says that j1 is the household’s first choice, then

                                         uij1 + survey
                                                 ij1    > uij + survey
                                                                 ij     ∀j.

    Similarly, if j2 is the household’s second choice, then

                                      uij2 + survey
                                              ij2    > uij + survey
                                                              ij     ∀j 6= j1 .

    We assume the measurement error is drawn iid from a normal distribution:

                                           survey
                                            ij
                                                            2
                                                   ∼ N (0, σsurvey ), iid.

5.1.3    Reported beliefs

In addition, we allow for measurement error in reported beliefs. That is, if the household optimizes
according to beliefs shif ti and utility vector ui , the elicited belief about admissions chances at
school j is generated according to:

                                                                0                                               0
   p̃obs
     ija = P r(zij ≤ πj − rspij (a) − shift ijra (j) , zij 0 > πj − rspij 0 (a) − shift ij 0 ra (j 0 ) for all j a j),
                                       g                                           g



                                                           33
where ra (j) is the rank of j on application list a, and

                                         g ijr = shift ijr + η̃ijr .
                                        shift

We observe the interval Iija in which p̃obs                               obs
                                        ija lies. For example, if 0.1 ≤ p̃ija < 0.2 then the household
would report 10 − 20%. In contrast to our descriptive analysis, we do not restrict the p̃obs
                                                                                         ija to take
values equal to the midpoint of the reported interval.
    We assume the measurement error is drawn iid from a normal distribution:

                                          η̃ijr ∼ N (0, ση̃2 ), iid.

Importantly, η̃ijr has the same distributional form as the “true” error ηijr , so functional form is not
being used to distinguish the two.
    We note that we model measurement error in reported beliefs as being independent of mea-
surement error in reported preferences. This is consistent with our maintained assumption that
true beliefs are independent of true preferences, and with evidence from section 3.7 on the weak
relationship between elicited beliefs and preferences. However, it would fail if, for example, sur-
vey respondents who erroneously report particular schools as most-preferred are also more likely
to erroneously report higher or lower beliefs. A feature of our data that facilitates estimation of
measurement error is presence of multiple measures of beliefs and preferences. Survey reports con-
tain noisy measures of beliefs and preferences, while the enrollment decision is noisy measure of
preferences. Reports to the mechanism measure true preferences and beliefs.

5.1.4    Enrollment decision

If i accepts a placement in j, then we require uij + eij > ei0 . If i receives and declines a placement
in j, we require uij + eij < ei0 .

5.1.5    Normalization

We have already imposed the location normalization ui0 = 0, but have not imposed a scale normal-
ization. In the multinomial probit model and its extensions to school choice settings, it is conven-
tional to normalize the scale of a coefficient of known sign, such as the coefficient on distance, βdist .
Without loss, we fix βdist = −1.




                                                     34
5.1.6       Abstract likelihood
Although we do not directly evaluate the likelihood, it is instructive to consider the likelihood of an
individual observation, conditional on the distribution of market-clearing cutoffs that was previously
estimated. This likelihood is given by
              Z
                     P r(enrolli | placementi , ui , θ) P r(surveyi | ui , bi , shift i ) dF (ui , bi , shift i | {Xij }j=1,...,J , disti , θ),
{ui ,bi ,shift i : ai is optimal}


 where P r(surveyi | ui , bi , shift i ) = 1 if household i was not surveyed, and surveyed households have
                                                   Y
      P r(surveyi | ui , bi , shift i , θ) =             P r(shift ijr + η̃ijr ∈ Ii,j,r | shift ijr , ση̃ )
                                         j,r : beliefijr elicited
                                                                                                          
                         × P r uij1 + survey
                                       ij1    > uij2 +  survey
                                                         ij2    > uijk +  survey
                                                                           ijk    ∀k ∈
                                                                                     / {j ,
                                                                                         1 2j } | u , σ 2
                                                                                                   i survey ,


with Ii,j,r the reported interval, and j1 and j2 the reported first- and second-choice schools.
     The application decision enters the likelihood via the region of integration. MCMC methods are
convenient when it is difficult to directly compute this integral, as in our setting, but relatively easy
to sample from conditional distributions of parameters, utilities, beliefs, and measurement error
terms conditional on optimality and survey reports.

5.1.7       Prior distributions

We begin with prior distributions over the preference parameters and belief parameters. We place
priors directly on β, Σ, µb , σb , and σsurvey as well as on the belief parameters separately by SES
category. In order to minimize the priors’ influence on our estimates, we choose diffuse priors, which
we describe in Online Appendix B.

5.1.8       MCMC iteration

We iterate through a sequence of steps which consist of sampling from the conditional posterior
distributions of utilities, utility shocks, beliefs, belief measurement error, application costs, and
model parameters. We describe these steps in detail in Online Appendix B. The steps are standard
Gibbs-sampler steps, with the exception of the updates to belief shift terms shif tijr and belief
measurement error η̃i . To update these parameters in turn we take a sequence of Metropolis-
Hastings steps. Hence our procedure is an example of a “Metropolis-within-Gibbs” procedure.
     To obtain our estimates we use a chain of 300,000 iterations. We estimate separate models by


                                                                      35
year. We discard the first half of the draws in order to allow for burn-in. Trace plots and PSRFs for
parameter estimates are reported in Online Appendix Figures A7 through A19. Online Appendix
Figures A20 through A25 report trace plots and PSRFs for welfare levels and differences. See Online
Appendix B.9 for a discussion.


6     Results
6.1   Estimation results
Table 6 reports estimates and credible intervals for model parameters. For each parameter we
show .025, .5, and .975 quantiles of the posterior distribution. The median may be taken as a
point estimate. Panel A of Table 6 displays estimates of belief model parameters by household
SES. Estimates from 2015 are in the left panel and estimates from 2017 are in the right panel. To
interpret the magnitudes, note that that there is an interval of length 1 for each report-specific
priority type such that if the cutoff lies in this interval, the type is rationed. Further interpretation
depends on the mechanism that was used. In 2015, students were allocated via the New Haven
Mechanism. Under this mechanism, placing a school one rank lower would increase report-specific
priority by 1. Therefore, a value of η round of −1 would mean that, on average, students believe that
the impact of rank on report-specific priority is zero, as if the mechanism were deferred acceptance.
In 2017 when the Boston mechanism was used, placing a school one rank lower would have increased
report-specific priority by 2, so that η round = −2 would indicate that students believe the impact
of rank on report-specific priority is zero.
    Focusing first on idiosyncratic school and school-rank specific errors, we find that σηschool and
σηschool×round converge to values far from zero. The σηschool are between 0.6 and 1.7 depending
on SES category and year, while the σηschool×round are between 0.25 and 0.4 across each year-SES
combination. These values are sufficiently large to lead to mistaken beliefs about the round in
which the capacity constraint binds. Households also make errors that are systematically correlated
with the round in which they apply to a school. Estimates of η round near -2.0 in 2017 and -3.5 in
2015 indicate that, on average, households underestimate the impact of round by the full value of
the round penalty (2017) or more (2015). Estimates of σηround indicate that there is substantial
heterogeneity across households in the effects of round, particularly in 2015, but that most students
substantially underestimate its impacts on placement chances. Round error parameters are similar
across SES groups. We estimate the variance of belief measurement error ση̃ at 0.24 in 2015 and
0.22 in 2017. See Online Appendix Table A8 for estimates of the Ση .


                                                   36
                                     Table 6: Parameter Estimates

                                                                  2015                                  2017
                                                               Quantile                               Quantile
 Variable                                           0.025        0.5          0.975        0.025        0.5          0.975
 A. Belief parameters
 σηindividual (high SES)                           8.207         9.420       11.246        5.952       6.499         7.092
 σηindividual (low SES)                            7.960         8.437       9.036         5.725       6.083        6.540
 σηpriority (high SES)                              0.897        2.065       3.665         3.506       4.226         5.105
 σηpriority (low SES)                               2.429       2.592        2.941         1.417       1.964         2.676
 σηround (high SES)                                2.754         3.207       3.733         0.242        0.346       0.445
 σηround (low SES)                                 2.848         3.033       3.246         0.239       0.297        0.373
 σηschool×round (high SES)                          0.368        0.404        0.449       0.287         0.315       0.352
 σηschool×round (low SES)                          0.236         0.252       0.267        0.307         0.324       0.341
 σηschool (high SES)                                0.734        0.861        1.022       1.024         1.177       1.393
 σηschool (low SES)                                0.554         0.612        0.682       1.553         1.646       1.715
 ση̃                                                0.196        0.240        0.292        0.191       0.223         0.260
 η individual (high SES)                           5.649         6.949        8.356        5.348        6.412       7.030
 η individual (low SES)                            7.738         8.081       8.427         6.723        7.346       8.081
 η priority (high SES)                             -2.729       -1.749       -0.618        3.132       3.714         4.426
 η priority (low SES)                              -2.624       -2.090       -1.686        1.823       2.032         2.268
 η round (high SES)                                -3.828       -3.309       -2.861       -2.178       -2.066       -1.983
 η round (low SES)                                 -3.773       -3.655       -3.526       -2.113       -2.027       -1.944

 B. Preference parameters
 δ Achievement First Amistad HS (1)                -57.495      -22.027       -9.121      -40.442      -17.809       -8.542
 δ Common Ground Charter (2)                       -83.025      -31.758      -15.635      -42.231      -17.899       -8.382
 δ Coop. Arts and Humanities (3)                   -10.454        1.345       15.193       -5.711        1.904      11.046
 δ Engineering & Science Univ. HS (4)              -48.314      -17.024       -5.106      -21.036       -6.850        0.822
 δ High School in the Community (5)                -54.204      -20.490       -8.133      -35.630      -15.267       -7.061
 δ Hill Regional Career (6)                         -7.554        3.457       18.544       -2.167        4.705       15.133
 δ Hillhouse (7)                                  -103.372      -41.025      -21.758      -39.212      -16.923       -7.657
 δ Hyde School (8)                                 -46.031      -11.964        0.423      -22.614       -4.330        3.794
 δ Metropolitan Business Academy (9)               -28.682       -8.737        2.449       -7.152        0.810        9.326
 δ New Haven Academy (10)                          -42.527      -15.330       -3.921      -22.289       -8.227       -0.400
 δ Riverside Education Academy (11)               -141.956      -57.463      -32.398     -236.918     -110.249      -64.335
 δ Wilbur L. Cross High School (12)                -44.486      -15.629       -3.896       -7.431        1.326       10.395
 λ                                                   0.001        0.003        0.005        0.002        0.004        0.006
 µb                                               -563.357     -234.140     -143.674     -316.581     -155.808     -106.071
 σb                                                 14.107       23.199       55.295       32.485       48.429       98.637
 σsurvey                                             9.994       18.079      45.349        6.279        10.157       21.715
 1(default is Cross)                               -25.817       -9.668       -3.727      -24.873      -11.077       -4.655
 Distance to default                                1.705         3.308       7.636        0.700         2.384       5.384
 1(low SES)                                          5.403       12.822       33.038       -0.955        5.013      15.020
 Achievement First                                -112.925      -44.590      -25.937      -46.813      -21.153       -9.607
 Engineering & Science                            -135.556      -52.069      -26.177      -51.152      -19.083       -0.475
Notes: Quantiles of distribution of posterior mean for parameters listed in the rows. Panel A: belief model by student SES.
‘High SES’ is top quintile of SES distribution. Off-diagonal elements of covariance matrices reported in Appendix Table A8.
Panel B: preference parameter estimates by grade. Coefficient on miles traveled is normalized to -1. Appendix Table A9
provides credible intervals for elements of the utility shock covariance matrices Σ. The coefficients on Wilbur Cross and Hill-
house apply only to students who are not zoned into these schools. The coefficient on the own zoned school is set equal to
zero. Achievement First and Eng. & Sci. coefficients are for incumbent students at those K12 schools.




                                                             37
   The main cross-year difference we observe in belief model parameter estimates is for the η priority ,
which are negative in 2015 and positive in 2017. Households underestimate the benefits of sibling
priority in 2015, and overestimate the benefits in 2017. This may reflect the lesser role of sibling
priority in determining report-specific priorities under the Boston mechanism relative to the New
Haven mechanism.
   Panel B of Table 6 presents estimates of preference parameters. To interpret the coefficients,
recall that the coefficient on miles traveled is equal to -1 and that the mean utility of the ‘no
placement’ outcome, which includes the choice to leave the district, is normalized to zero. First
consider preferences for outside relative to inside options. The coefficient on 1(default is Cross) has
a negative sign in both years, meaning that students zoned to Cross find schools of choice relatively
less appealing. Of the two high schools, Cross draws from the higher-SES catchment zone and
scores higher on accountability metrics. Students with the option to continue at Achievement First
or Engineering & Science also find inside options less attractive on average. Low-SES students find
inside options more attractive. Students farther from their default school find the inside option
more attractive. We also observe differences in preferences across schools relative to the outside
option. Mean utility is negative in both years for several schools, including Hillhouse (for out-of-
zone students), Riverside (a school aimed at students with disciplinary issues), and Achievement
First (the no-excuses charter).
   We also find evidence of horizontal differentiation across schools. Credible intervals for six
schools span zero in at least one year. Strong preferences for schools specializing in arts, science, or
business come in large part from high values of the school-student match terms ij . For example,
the 90% credible interval for the standard deviation of the Co-op Arts preference shock is (16, 50) in
2015 and (13, 31) in 2017. The 90% credible interval for the standard deviation of the Metropolitan
Business preference shock is (15, 46) in 2015 and (12, 30) in 2017. See Online Appendix Table A9.
   On average, receiving a placement is costly, with µb equal to -234 in 2015 and -156 in 2017.
Dispersion around this central value is limited, with σb equal to 23 in 2015 and 48 in 2017. Mea-
surement error in reported preferences has a standard deviation of 10 to 18 miles traveled, depending
on year. Finally, scale parameter λ takes values between 0.003 and 0.004 depending on year. Our
scale parameter estimates indicate that distance plays a relatively small role in decisions to accept
or decline placements. As a result, our estimates of costs and mean utilities are large in distance
terms.




                                                  38
6.2      Welfare analysis and counterfactual simulations
We now turn to an analysis of household welfare and test scores under observed and counterfactual
policies. Our procedure estimates the joint distribution of parameters and utilities. Using this
distribution, we are able to compute each household’s expected welfare according to its utility and
the true rational-expectations admissions chances under the application it submitted. We compute
average utility at every 10th iteration along the Markov chain after the burn-in period. Because the
coefficient on distance is normalized to −1, welfare is measured in units of (fewer) miles traveled.
   We consider two sets of policy counterfactuals. The first set considers changing the assignment
mechanism to DA. As a benchmark, we consider the truthful DA mechanism (henceforth ‘DA’),
in which applicants can list each school. The optimal strategy for participating households is to
truthfully report their preferences. Households need not form beliefs about placement chances to
make optimal reports under this policy, provided they trust the recommendation to play truthfully.
   Districts may prefer to keep lists short if they think, e.g., that longer lists make the application
process too challenging for students.17 Truthful reporting need not be optimal under the resulting
‘truncated deferred acceptance’ procedure (Abdulkadiroğlu et al., 2009; Haeringer and Klijn, 2009;
Calsamiglia et al., 2010; Fack et al., 2015). We consider welfare outcomes for ‘naive’ truthful
reporting for lists of lengths one to twelve (DA-N), as well as for equilibrium ‘sophisticated’ play at
the baseline list length under the assumption that households form rational expectations beliefs.
   It is possible that households will not trust or not receive a recommendation to play truthfully.
We augment our baseline analysis with departures from optimal play in which households drop
schools where they think they are unlikely to be admitted, or stop listing schools once they believe
they will be unplaced with low probability. We also consider cases in which some households do not
receive the recommendation to play truthfully and continue to file the same applications as under
the baseline mechanism, and cases in which households play strategically under the DA mechanism
but with belief errors based our beliefs model.
   Our second set of counterfactuals considers the effects of informational interventions by shrinking
the shif tijr error terms by factors ranging from zero to one and then solving for the equilibrium of
the baseline mechanism. A factor of zero corresponds to baseline case. A factor of one corresponds
to a best-case informational intervention, with shif tijr = 0 for all ijr. An alternate interpretation
of the best-case intervention is as the result of providing a strategic and informed ‘proxy’ player with
each applicant’s cardinal utilities and allowing the proxy player to submit the application (Budish
and Cantillon, 2012). By comparing findings from the first and second sets of counterfactuals, we
  17
       Given the relatively small number of schools in New Haven, full lists are feasible in our setting.



                                                            39
assess whether the switch to deferred acceptance offers welfare benefits relative to the observed
mechanisms given the observed distribution of belief errors, and whether this finding would change
if students had access to more accurate information on admissions chances.
   There may be multiple equilibria under rational expectations, under ‘sophisticated’ truncated
deferred acceptance, and under strategic play in either mechanism when households maintain com-
ponents of belief errors. We select an equilibrium as follows. We start with the distribution of
cutoffs π 0 that we recovered from the data in step 1. We then compute optimal applications for
each household. Given the new applications and our resampled draws, we compute a new distri-
bution of cutoffs π 0 . We obtain new cutoffs π 1 = (1 − α)π 0 + απ 0 for α ∈ (0, 1) pointwise in each
resampled market, and compute optimal applications given π 1 . We iterate this procedure until
convergence. We take α = 0.9 as a starting value and decrease this value as we iterate.

6.2.1   Aggregate welfare in policy counterfactuals

Panel A.1 of Table 7 describes the posterior distribution of mean welfare in the market for the
benchmark case, the rational expectations counterfactual and the truthful DA counterfactual, as
measured in miles traveled. For each welfare estimate we report the mean, median, and 95% credible
interval for the posterior distribution of mean welfare. In the first column, labeled ‘Baseline’,
describe the distribution under the mechanism that was used at baseline. The second column,
‘RatEx,’ describes the posterior distribution under optimal reports with rational-expectations beliefs
in the baseline mechanism. The third column, ‘DA,’ describes the posterior distribution under
the truthful DA, while columns four and five present the differences between the RatEx and DA
mechanisms and baseline mechanism. All statistics are averages over the 2015 and 2017 universes
of rising ninth graders.
   Aggregate welfare improves in both counterfactuals. The average household would be made
better off by the equivalent of 4.5 fewer miles traveled under rational expectations. This gain is
equal to 31% of mean utility relative to the outside option of attending a neighborhood school or
leaving the district under the baseline mechanism. Under DA, the average household is better off
by the equivalent of 3.9 fewer miles traveled, or 27% of mean utility relative to the outside option.
95% posterior probability intervals for these differences do not cover zero.




                                                 40
                        Table 7: Distance-Metric Welfare: Benchmark and Counterfactuals


                                         Mean welfare                                                         Welfare differences
                                                                                             RatEx                   DA               No Survey DA
                    Baseline                  RatEx                      DA                 − Baseline            − Baseline           − Baseline
 A1. Posterior distribution of mean distance-metric welfare
 Mean            14.420               18.898              18.346                               4.478                3.926                −1.801
 Median          13.652               17.454              17.215                               4.056                3.557                −1.211
 95% CI      [5.877, 30.995]      [8.863, 38.241]     [8.166, 37.635]                      [2.395, 9.046]       [2.283, 7.607]       [−6.165, −0.542]

 A2. High-SES mean minus low-SES mean
 Mean           −2.887            −3.767                             −3.816                  −0.881                −0.929                 0.644
 Median         −2.686            −3.459                             −3.493                  −0.818                −0.864                 0.410
 95% CI    [−5.842, −1.080]  [−7.716, −1.532]                   [−7.542, −1.582]         [−2.333, 0.163]       [−2.224, 0.027]       [−0.366, 2.228]


                    Truthful                Strategic                  Drops                   Stops
 B. DA-4 - baseline under different strategy types
 Mean             3.455                3.645                          3.443                   3.452
 Median           3.117                3.283                          3.090                   3.114
 95% CI      [1.907, 7.005]       [1.906, 7.397]                  [1.905, 7.004]          [1.906, 7.039]


                       0%                      25%                      50%                     75%                  100%
 C. Share submitting baseline application under DA-4
 Mean            3.455                 2.702             1.946                                1.181                 0.398
 Median          3.117                 2.425             1.768                                1.085                 0.265
 95% CI      [1.907, 7.005]       [1.532, 5.389]     [1.065, 3.688]                       [0.502, 2.469]       [−0.363, 1.506]


                            Switch to DA                              Keep baseline mechanism
             School and priority              School           School and priority             School
 D. Eliminate specific error components under DA-4 and baseline
 Mean            2.308                2.310              1.737                                1.745
 Median          2.059                2.068              1.115                                1.110
 95% CI      [0.913, 5.500]       [0.905, 5.498]     [0.020, 5.733]                       [0.038, 5.716]
Notes: This table describes the posterior distribution of mean welfare in the baseline case and under policy counterfactuals. Welfare is measured us-
ing miles traveled as the numeraire good. See text for details. Panels A1 and A2: ‘Baseline’ is baseline (New Haven or Boston) mechanism given
observed beliefs. ‘RatEx’ is the baseline mechanism under rational expectations beliefs. ‘DA’ is the strategy-proof deferred acceptance mechanism.
‘RatEx-baseline’ and ‘DA-baseline’ columns compare welfare differences under the listed mechanisms. ‘No survey DA-baseline’ column compares wel-
fare under the DA and baseline mechanisms using model estimates based on rational expectations beliefs. Panel A2 displays differences in each of these
objects between high-SES and low-SES households. Panel B: Comparison between truncated DA-4 and baseline under truthful play, strategic play, and
truthful play with ‘drop’ and ‘stop’ rules for listing schools. Panel C: Welfare gain from switch from baseline to truthful DA-4 by share of households
continuing to submit ‘baseline’ applications. Panel D: Welfare change from switch from baseline to strategic truncated DA with school- and school by
priority-specific errors (columns 1+2), and welfare change from switching to only school- and school by priority-specific errors while keeping the baseline
mechanism (columns 3+4).



    Data on subjective beliefs are important for market designers trying to choose the welfare-
maximizing assignment mechanism. The sixth column of panel A.1 of Table 7 compares average

                                                                           41
welfare under the DA and baseline mechanisms using model estimates obtained without survey
data. We impose rational expectations beliefs in estimation and in counterfactual simulations.
These estimates reverse the welfare comparison between the DA and baseline mechanisms, with the
baseline mechanism outperforming DA by 1.8 miles traveled. The welfare comparison we obtain
without using survey data overstates mean welfare of the baseline mechanism by 5.7 fewer miles
traveled relative to the comparison incorporating subjective expecations. This is 40% of mean utility
relative to the outside option in the benchmark case.
   Our finding that the baseline mechanism outperforms DA in no-survey estimates has the same
sign as results from Agarwal and Somaini (2018) and Calsamiglia et al. (2018) but is larger in
magnitude. For example, Agarwal and Somaini (2018) estimate a welfare loss of 0.08 additional
miles traveled when switching from the Cambridge mechanism under rational expectations to DA.
Our findings may reflect stronger preferences across schools, lower travel costs in New Haven relative
to Cambridge, or lower travel costs for high school students than for the early-grade students studied
in previous research. They may also reflect our addition of enrollment choice data to preference
estimation; previous papers have not used enrollment in estimation. When we exclude the enrollment
choice stage of the model, we find a welfare loss of 0.2 miles from the switch to DA in the specification
that excludes survey data, much closer to findings from Agarwal and Somaini. Using the alternative
model does not change the qualitative conclusions we draw about the welfare comparison between
DA and baseline or the importance of using survey data. See section 6.3.
   The welfare comparison between baseline and DA does not depend on list length. Figure 5
presents results from DA counterfactuals in which students truthfully report preferences on appli-
cations of varying length. The vertical axis is the mean of the posterior mean welfare distribution,
and the horizontal axis is the number of schools households are allowed to rank on their application.
Mean welfare from the baseline mechanism case holding list length fixed at four is marked by the
lower horizontal line. Welfare under truthful DA is above benchmark welfare at all counterfactual
application lengths greater than one.
   Panel B of Table 7 compares the DA mechanism at an application length of four (DA-4) to the
baseline mechanism under several assumptions on counterfactual play. The first column assumes
that households would report preferences truthfully under DA-4. The second column assumes
that households play strategically under DA-4 based on rational expectations beliefs. The “Drops”
column considers DA-4 outcomes in which households begin with their truthful applications, but
drop schools if their unconditional chances of placement are below 5%. The “Stops” column assumes
that households stop listing schools once their chance of not receiving a placement falls below 20%.
The point estimates of welfare gains are similar to those for strategy-proof DA in each column.

                                                   42
                                 Figure 5: Welfare under truthful DA by list length


                  20
                  18
             Mean welfare
             14     16




                            Baseline
                  12
                  10




                            1    2     3     4     5    6      7       8       9      10     11     12
                                                 Maximum application length

                                                   Truthful DA              95% CI


     Notes: median of posterior mean welfare distribution (vertical axis) under truthful DA policy counterfac-
     tual by application length (horizontal axis). ‘Baseline’ line is median of posterior mean welfare under the
     baseline mechanism and observed beliefs with an application length of four.



   Panel C of Table 7 describes welfare changes under a ‘surprise’ implementation of deferred
acceptance in which some households are not informed of the mechanism change and keep their
baseline applications, while others report truthfully. An alternative interpretation is that “surprised”
households maintain the same beliefs as under the baseline mechanism. We fix the application length
at four in this exercise. The ‘0% surprised’ column corresponds to the truthful DA-4 counterfactual.
As the share of households who do not change their play rises, welfare falls. A gain of zero falls
outside the 95% credible interval through a 75% ‘surprise’ rate. When no households are informed of
the change (‘100% surprised’), welfare effects are close to zero, with a posterior probability interval
that covers zero. The switch to DA seems likely to be welfare improving at realistic rates of truthful
reporting. The empirical literature studying rates of truthful reporting in the DA context finds
that large majorities of participants play truthfully. For example, Rees-Jones (2018) studies the
medical residency match and reports that between 5% and 17% of participants do not report true


                                                         43
preferences, while Chen and Sönmez (2006) report evidence from a lab setting that between 28%
and 44% of participants misrepresent preferences.
   We next ask how effective an informational intervention would have to be to cause the baseline
mechanism to raise aggregate welfare relative to deferred acceptance. We scale all shift terms by
values ranging from zero to one and simulate counterfactual welfare distribution in each case. Figure
6 presents results from this exercise. The horizontal axis is the fraction reduction in the shift term,
and the vertical axis is the difference in mean welfare between baseline and DA. The gains from
informational interventions of this type are limited until belief errors are completely eliminated, at
which point welfare under the baseline mechanism is similar to welfare under DA.


       Figure 6: Mean welfare under baseline mechanism by reduction in scale of shift term


                              DA
                   0     -2
               Mean welfare
             -4    -6




                              0     .2             .4              .6             .8              1
                                            Fraction reduction in shift terms

                                   Welfare under baseline mechanism                    95% CI


     Notes: median of posterior distribution of differences in mean welfare between baseline and DA (vertical
     axis) by fraction reduction in shift ijr terms (horizontal axis).



   Another way to think about informational interventions is as eliminating certain types of errors.
Information interventions that clarify how the assignment mechanisms work may eliminate belief
errors with respect to the effect of rank on application score, while uncertainty about school-specific

                                                       44
demand, priority groups, and person-specific optimism persist. We consider how eliminating this
type of error affects welfare relative to the baseline in Panel D of Table 7, which shows welfare
changes under sophisticated play for an alternative partial information intervention in which belief
errors about rank are shut down.
   The first column of this panel (“Switch to DA– school and priority”) shows welfare gains relative
to baseline when the mechanism is changed to DA and ηijr , ηir are set to zero for all households,
but the other components of shift ijr are held fixed, including the errors about schools’ cutoffs ηij
and errors about priority ηipriority . We find that welfare would increase under this counterfactual
by the equivalent of 2.3 fewer miles traveled. The “school” column considers welfare changes when,
in addition, errors about priority ηipriority are set to zero for all households, with nearly identical
results. These results suggest that welfare would increase under a switch to deferred acceptance,
even if households attempt to play strategically but misforecast cutoffs, provided that errors about
rank are corrected. The final two columns consider the same changes to shift ijr under the baseline
mechanism. Welfare gains of roughly 1.7 indicate that approximately 40% of the gains from the
perfect informational intervention could be realized by correcting errors about the impact of rank.

6.2.2   Distributional impacts of policy counterfactuals

One argument in favor of deferred acceptance mechanisms is that they may produce a more equitable
distribution of welfare across participants. We consider this point in panel 1.B of Table 7. This
table shows the difference between mean utility for high-SES and low-SES households under different
counterfactuals. Negative numbers correspond to higher welfare for low-SES households than high-
SES households, relative to the outside option for each. As shown in the first three columns, low-
SES households have higher utility from choice under Baseline, RatEx, and DA. Point estimates
in columns four and five show that low-SES students experience larger gains from the switches to
RatEx or DA. 95% credible intervals extend just past zero in both cases.
   We further explore this idea by examining the distribution of welfare across households under
the baseline and DA mechanisms. For each household, we compute mean welfare by averaging
the household’s welfare across MCMC iterations. Figure 7 reports the welfare distribution. The
left panel reports mean welfare for households in each centile of the welfare distribution under the
baseline and deferred acceptance mechanisms. Recall that welfare is normalized to zero for unplaced
households. The middle panel reports the centile-by-centile difference in the welfare distributions
shown on the left panel. The right panel reports centiles of welfare gains or losses under DA relative
to baseline.


                                                  45
                                       Figure 7: Distribution of welfare and welfare changes

                             Percentiles of                                      Percentile−by−percentile                                                        Distribution of
                           welfare distribution                                        differenes                                                               welfare changes
                  60




                                                                    15




                                                                                                                                    20
                                                                                                                   Welfare gain: Naive DA − Baseline
                                                                                                                                                15
                                                                    10
                  40




                                                             DA − Baseline




                                                                                                                                       10
        Welfare




                                                                   5




                                                                                                                             5
                  20




                                                                    0




                                                                                                                   0
                                                                    −5




                                                                                                                                    −5
                  0




                       0   .2    .4      .6     .8       1                   0     .2    .4      .6       .8   1                                       0   .2       .4      .6      .8   1
                                 Percentile                                              Percentile                                                                 Percentile

                                DA            Baseline                                    DA − Baseline                                                              Welfare gain




      Notes: Left panel: posterior mean welfare by centile of welfare distribution under baseline and strategy-
      proof DA. Middle panel: centile-by-centile differences in welfare between DA and baseline policies. Right
      panel: percentiles of welfare gain distribution from switch to strategy-proof DA from baseline.



   The middle panel indicates that the welfare distribution under DA is higher at all percentiles
above the 50th. Quantiles just below the median are somewhat lower under DA than at baseline.
The right panel indicates that about 40% of households would be made better off by a switch to
DA while 30% of households would be unaffected. Intuitively, some households may be made worse
off if they have accurate beliefs at baseline while others are misinformed.

6.3    Robustness and additional analyses
Our model incorporates two features that previous research has generally abstracted from: partici-
pation costs, and enrollment choices. To explore how these features affect our findings, we estimate
an alternate model that excludes them and compute counterfactuals paralleling our main analysis.
We describe this exercise in Online Appendix C. Under the alternate model, mean welfare relative
to the outside option is lower for each of the Baseline, RatEx, and DA mechanisms. Switching to
DA from the baseline mechanism raises welfare by the equivalent of 0.9 fewer miles traveled, a 32%
gain on a base of 2.8 (compare to a 27% gain in our main analysis). When we exclude survey data
and impose rational expectations beliefs, we find that the switch would reduce welfare by 0.2. The


                                                                                         46
1.1 mile-equivalent difference between the evaluations of the switch with and without the survey is
equal to 39% of mean utility at baseline (compare to 40% in our main analysis). While our findings
on the welfare gains from choice in general relative to the outside option as measured in miles trav-
eled are sensitive to including the enrollment/participation decision, findings on differences across
mechanisms and the gains from choice as a share of mean welfare at baseline are not.
    Additional analyses show results a) separately by survey year and b) restricting the survey
data used in estimation to respondents who correctly recall their submitted applications. Online
Appendix Figure A26 and Tables A10 and A11 show that our findings on the distribution of belief
errors and the welfare implications of counterfactual policies and estimation strategies have the
same signs and are similar in percentage terms across years. Welfare levels across all mechanisms
relative to the outside option are higher in 2015 than 2017. Online Appendix Table A12 shows that
our findings are qualitatively unchanged and quantitatively very similar when we limit the survey
data used for model estimation to respondents with correct recall of the submitted application.
    Thus far we have quantified utility changes in terms of fewer miles traveled, and shown that
the welfare gains from a mechanism change are equal to large shares of mean welfare at baseline,
relative to the outside option. A final exercise uses estimated wage rates and summed travel times
at the district level to convert our distance-metric utility into dollars by way of a simple back-of-
the-envelope calculation. We find that the implied dollar values of mechanism changes are large
relative to the costs of other educational interventions, such as reductions in class size (Krueger,
1999; Chetty et al., 2011). See Online Appendix D for details.


7    Conclusions
This paper studies the performance of a centralized school choice mechanism that rewards strategic
behavior when households have heterogeneous beliefs about placement probabilities. We conduct a
household survey asking choice participants about their preferences and beliefs, and link our survey
data to administrative records of the school choice process. We use our linked data to describe
heterogeneity in beliefs and to estimate a model of school choice that allows for belief and preference
heterogeneity. Our survey data allow us to study the effects of counterfactual policies without
making strong assumptions on participants’ equilibrium play. The counterfactuals we consider
highlight the tradeoff between applicants’ ability to express preference intensity in mechanisms that
reward strategic play and the increased likelihood of welfare-reducing application mistakes.
    Our descriptive findings show that while households play strategically and attempt to trade off
preference intensity against admissions chances, they do so using mistaken beliefs about admissions

                                                  47
chances. Counterfactual policy simulations based on model estimates that incorporate survey data
indicate that the ordering of deferred and strategic mechanisms by welfare outcomes depends on the
accuracy of students’ beliefs about admissions chances. Though the strategic mechanism is prefer-
able when students have rational expectations about choice probabilities, the deferred acceptance
mechanism raises aggregate welfare given the distribution of belief errors we observe in our data.
The costs of application mistakes in the strategic mechanism outweigh the benefits of increased
expressiveness. We abstract from other advantages of deferred acceptance, including the reduced
chance of ex-post regret about the submitted application relative to strategic mechanisms.
   Our findings suggest that if market designers choose to use school choice mechanisms that reward
strategic play, offering students some means to learn about admissions probabilities for different
portfolios is likely to be welfare-improving. We leave the discussion of what such an information
intervention might look like for future work.
   More generally, our findings suggest an important role for data on subjective beliefs in preference
estimation and the evaluation of policy counterfactuals. We show that in our setting a market
designer who did not account for application mistakes would reverse the welfare comparison between
the baseline and deferred acceptance mechanisms. The magnitude of belief errors in any particular
setting depends on the experience of and resources available to the economic agent, as well as on the
efficacy with which the market designer or other interested parties impart the information necessary
for informed strategic play. In school choice settings, households submit application portfolios
at most a handful of times in their lives, and districts may vary in their ability to communicate
effectively. We might expect similar challenges in, for example, matching markets for public housing
(Thakral, 2016; Waldinger, 2018). In contrast, we might expect belief errors to be less important in
market settings where sophisticated agents face decision problems repeatedly, such as the matching
markets that dictate kidney exchange across hospitals (Roth et al., 2005; Agarwal et al., 2018) or
food allocation across food banks (Prendergast, 2017). Extensions of subjective beliefs data and
analysis to other matching markets is a topic for future research.


References
Abdulkadiroğlu, Atila, Parag A. Pathak, and Alvin E. Roth, “Strategy-proofness Versus
  Efficiency in Matching with Indifferences: Redesigning the NYC High School Match,” American
  Economic Review, 2009, 99 (5), 1954–78.

Abdulkadiroğlu, Atila, Joshua D. Angrist, Yusuke Narita, and Parag A. Pathak, “Re-


                                                 48
  search Design Meets Market Design: Using Centralized Assignment for Impact Evaluation,”
  Econometrica, 2017, 85 (5), 1373–1432.

  , Nikhil Agarwal, and Parag A. Pathak, “The Welfare Effects of Coordinated Assignment:
  Evidence from the New York City High School Match,” American Economic Review, 2017, 107
  (12), 3635–3689.

  , Parag A. Pathak, Alvin E. Roth, and Tayfun Sönmez, “The Boston Public School
  Match,” American Economic Review, May 2005, 95 (2), 368–371.

  ,   , and    , “The New York City High School Match,” American Economic Review, May 2005,
  95 (2), 364–367.

  , Yeon-Koo Che, and Yosuke Yasuda, “Resolving Conflicting Preferences in School Choice:
  The "Boston Mechanism" Reconsidered,” American Economic Review, February 2011, 101 (1),
  399–410.

Agarwal, Nikhil and Paulo Somaini, “Demand Analysis using Strategic Reports: An application
  to a school choice mechanism,” Econometrica, 2018, 86 (2), 391–444.

  , Itai Ashlagi, Eduardo Azevedo, Clayton R Featherstone, and Ömer Karaduman,
  “Market Failure in Kidney Exchange,” Working Paper 24775, National Bureau of Economic Re-
  search June 2018.

Azevedo, Eduardo and Jacob Leshno, “A Supply and Demand Framework for Two-Sided
  Matching Markets,” Journal of Political Economy, 2016, 124 (5), 1235–1268.

Budish, Eric and Estelle Cantillon, “The Multi-Unit Assignment Problem: Theory and Ev-
  idence from Course Allocation at Harvard,” The American Economic Review, 2012, 102 (5),
  2237–2271.

Bureau, United States Census, “American Community Survey,” https://www.census.gov/
  programs-surveys/acs 2016. (accessed, October 21, 2019).

Calsamiglia, Caterina and Maia Güell, “Priorities in School Choice: The Case of the Boston
  Mechanism in Barcelona,” Journal of Public Economics, 2018, 163, 20–36.

  , Chao Fu, and Maia Güell, “Structural Estimation of a Model of School Choices: the Boston
  Mechanism vs. its Alternatives,” Working Paper 24588, National Bureau of Economic Research
  2018.



                                              49
  , Guillaume Haeringer, and Flip Klijn, “Constrained School Choice: An Experimental
  Study,” American Economic Review, 2010, 100 (4), 1860–74.

Cattaneo, Matias D, Richard K Crump, Max H Farrell, and Yingjie Feng, “On binscat-
  ter,” arXiv preprint arXiv:1902.09608, 2019.

Chen, Yan and Tayfun Sönmez, “School choice: An Experimental Study,” Journal of Economic
  theory, 2006, 127 (1), 202–231.

Chetty, Raj, John Friedman, Nathaniel Hilger, Emmanuel Saez, Diane Whitmore
  Schanzenbach, and Danny Yagan, “How Does Your Kindergarten Classroom Affect Your
  Earnings? Evidence from Project STAR,” The Quarterly Journal of Economics, 2011, 126 (4),
  1593–1660.

Corcoran, Sean P, Jennifer L Jennings, Sarah R Cohodes, and Carolyn Sattin-Bajaj,
  “Leveling the Playing Field for High School Choice: Results from a Field Experiment of Infor-
  mational Interventions,” Working Paper 24471, National Bureau of Economic Research March
  2018.

de Haan, Monique, Pieter A. Gautier, Hessel Oosterbeek, and Bas van der Klaauw,
  “The Performance of School Assignment Mechanisms in Practice,” IZA Discussion Papers 9118,
  Institute for the Study of Labor (IZA) June 2015.

Dur, Umut, Robert G. Hammond, and Thayer Morrill, “Identifying the Harm of Manipu-
  lable School-Choice Mechanisms,” America Economic Journal: Economic Policy, February 2018,
  10 (1), 187–213.

Edsight, “Connecticut Department of Education District Profile and Performance Reports,”
  http://edsight.ct.gov/Output/District/HighSchool/0930011_201415.pdf 2015. (accessed,
  October 21, 2019).

  , “Connecticut Department of Education District Profile and Performance Reports,” http://
  edsight.ct.gov/Output/District/HighSchool/0930011_201617.pdf 2017. (accessed, October
  21, 2019).

Ergin, Haluk and Tayfun Sonmez, “Games of School Choice under the Boston Mechanism,”
  Journal of Public Economics, January 2006, 90 (1-2), 215–237.

Fack, Gabrielle, Julien Grenet, and Yinghua He, “Beyond Truth-Telling: Preference Esti-
  mation with Centralized School Choice,” CEPR Discussion Papers 10907, C.E.P.R. Discussion


                                                 50
  Papers October 2015.

Haeringer, Guillaume and Flip Klijn, “Constrained School Choice,” Journal of Economic The-
  ory, 2009, 144 (5), 1921–1947.

Hastings, Justine, Thomas Kane, and Douglar Staiger, “Heterogeneous Preferences and the
  Efficacy of Public School Choice,” 2009. Unplublished Working Paper.

He, Yinghua, “Gaming the Boston School Choice Mechanism in Beijing,” TSE Working Papers
  12-345, Toulouse School of Economics (TSE) May 2012.

Krueger, Alan B, “Experimental estimates of education production functions,” The Quarterly
  Journal of Economics, 1999, 114 (2), 497–532.

Lohman, Judith, “State School Transportation Requirements and Funding,” https://www.cga.
  ct.gov/2012/rpt/2012-R-0085.htm 2014. Accessed 2017-04-04.

Manski, Charles F., “Measuring expectations,” Econometrica, 2004, 72 (5), 1329–1376.

McCulloch, Robert and Peter Rossi, “An exact likelihood analysis of the multinomial probit
  model,” Journal of Econometrics, 1994, 64 (1-2), 207–240.

NHPS, “Site-Based Budget, FY 2014-2015,” http://nhps.net/sites/default/files/ALL/
  2014-2015_Site_Based_Budget_Volume_I.pdf 2014. Accessed 2017-04-04.

Pathak, Parag, “The Mechanism Design Approach to Student Assignment,” Annual Review of
  Economics, 2011, 3 (1), 513–536.

Pathak, Parag A and Tayfun Sönmez, “School admissions reform in Chicago and England:
  Comparing mechanisms by their vulnerability to manipulation,” The American Economic Review,
  2013, 103 (1), 80–106.

Pathak, Parag A. and Tayfun Sönmez, “Leveling the Playing Field: Sincere and Sophisticated
  Players in the Boston Mechanism,” American Economic Review, September 2008, 98 (4), 1636–52.

Prendergast, Canice, “How Food Banks Use Markets to Feed the Poor,” Journal of Economic
  Perspectives, 2017, 31 (4), 145–62.

Rees-Jones, Alex, “Suboptimal Behavior in Strategy-Proof mechanisms: Evidence from the Res-
  idency Match,” Games and Economic Behavior, 2018, 108, 317–330.

Roth, Alvin E., “The Economist as Engineer: Game Theory, Experimentation, and Computation



                                              51
  as Tools for Design Economics,” Econometrica, 2002, 70 (4), 1341–1378.

Roth, Alvin E, Tayfun Sönmez et al., “A Kidney Exchange Clearinghouse in New England,”
  American Economic Review, 2005, 95 (2), 376–380.

Thakral, Neil, “The Public-Housing Allocation Problem: Theory and Evidence from Pittsburgh,”
  Mimeo, 2016.

Waldinger, Daniel, “Targeting In-Kind Transfers through Market Design: A Revealed Preference
  Analysis of Public Housing Allocation,” Mimeo, 2018.




                                              52
