                             NBER WORKING PAPER SERIES




             OPTIMALLY IMPRECISE MEMORY AND BIASED FORECASTS

                                   Rava Azeredo da Silveira
                                         Yeji Sung
                                     Michael Woodford

                                     Working Paper 28075
                             http://www.nber.org/papers/w28075


                    NATIONAL BUREAU OF ECONOMIC RESEARCH
                             1050 Massachusetts Avenue
                               Cambridge, MA 02138
                                  November 2020




We thank Hassan Afrouzi, Ben Hébert, David Laibson, Yueran Ma, and Andrei Shleifer for
helpful discussions, and the Alfred P. Sloan Foundation, the CNRS through UMR 8023, and the
IOB for research support. The views expressed herein are those of the authors and do not
necessarily reflect the views of the National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.

© 2020 by Rava Azeredo da Silveira, Yeji Sung, and Michael Woodford. All rights reserved.
Short sections of text, not to exceed two paragraphs, may be quoted without explicit permission
provided that full credit, including © notice, is given to the source.
Optimally Imprecise Memory and Biased Forecasts
Rava Azeredo da Silveira, Yeji Sung, and Michael Woodford
NBER Working Paper No. 28075
November 2020
JEL No. D84,E03,G41

                                          ABSTRACT

We propose a model of optimal decision making subject to a memory constraint. The constraint is
a limit on the complexity of memory measured using Shannon's mutual information, as in models
of rational inattention; but our theory differs from that of Sims (2003) in not assuming costless
memory of past cognitive states. We show that the model implies that both forecasts and actions
will exhibit idiosyncratic random variation; that average beliefs will also differ from rational-
expectations beliefs, with a bias that fluctuates forever with a variance that does not fall to zero
even in the long run; and that more recent news will be given disproportionate weight in
forecasts. We solve the model under a variety of assumptions about the degree of persistence of
the variable to be forecasted and the horizon over which it must be forecasted, and examine how
the nature of forecast biases depends on these parameters. The model provides a simple
explanation for a number of features of reported expectations in laboratory and field settings,
notably the evidence of over-reaction in elicited forecasts documented by Afrouzi et al. (2020)
and Bordalo et al. (2020a).

Rava Azeredo da Silveira                         Michael Woodford
Département de Physique                          Department of Economics
Ecole Normale Supérieure                         Columbia University
24 rue Lhomond                                   420 W. 118th Street
75005 Paris                                      New York, NY 10027
France                                           and NBER
rava@ens.fr                                      mw2230@columbia.edu

Yeji Sung
Department of Economics
Columbia University
420 W. 118th Street
New York, NY 10027
ys2992@columbia.edu
    The hypothesis of rational expectations (RE) proposes that decisions are based on ex-
pectations that make use of all available information in an optimal way: that is, those that
would be derived by correct Bayesian inference from an objectively correct prior and the data
that has been observed to that date. Yet both in surveys of individual forecasts of macroe-
conomic and financial variables and in forecasts elicited in experimental settings, beliefs are
more heterogeneous than this hypothesis should allow, and forecast errors are predictable on
the basis of variables observable by the forecasters, contrary to this hypothesis. In particular,
a number of studies have argued that forecasts typically over-react to new realizations of the
variable being forecasted. (See Bordalo et al., 2020a, and Afrouzi et al., 2020, for recent
examples with extensive references to prior literature.)
    A variety of models of expectation formation have been proposed that allow for such
over-reaction. The simplest type of model simply posits that the forecasted future value of
a variable is a particular linear function of the last few observations of the variable; with an
appropriate choice of the coefficients (such as those proposed by Metzler, 1941), a forecasting
heuristic of this kind may imply that a recent increase in the variable will be extrapolated
into the future, so that further increases are anticipated, regardless of whether the degree of
serial correlation of changes in the variable make this an optimal forecast. A classic critique
of such proposals, however, is that of Muth (1961): why should decision makers continue
to forecast in this way, if their forecasts are systematically biased, as repeated observations
should eventually make clear? Moreover, a mechanical heuristic with fixed coefficients is
unable to explain how the biases in observed forecasts change depending on the persistence
of the process that is forecasted (Afrouzi et al., 2020).
    Fuster et al. (2010, 2011) propose a more sophisticated model, in which decision makers
are assumed to forecast a time series by modeling it as a low-order autoregressive process;
the coefficients of their forecasting rule are those implied by the AR(k) model that best fits
the autocorrelation function of the actual series, for some fixed bound on k . The authors
argue that actual time series often involve long-horizon dependencies, and show that in this
case (say, an AR(40) process forecasted by people who consider models with no more than 10
lags), long-horizon forecasts using the best-fitting AR(k) model can significantly over-react
to recent trends in the data.
    This proposal, however, remains subject to several objections. Why should the restriction
to models of the data with a fixed upper bound on k be maintained, even when the available
sequence of observations with which to estimate the model becomes unboundedly long?
Moreover, even if one grants that a constraint on model complexity requires that no more
than some finite number of explanatory variables be stored and used as a basis for forecasts,
why must the explanatory variables correspond to the last k observations of the series? In the
kind of example in which Fuster et al. argue that their proposal predicts over-reaction, more
accurate long-horizon forecasts would be possible if the forecast were conditioned on a long
moving average of observations, rather than only recent observations; yet tracking a small
number of moving averages would seem no more complex than always having access to the
last k observations. And above all, the Fuster et al. explanation implies that over-reaction
should only be observed in the case of variables that are not well-described by an AR(k)
process of low enough order. Yet Afrouzi et al. (2020) find significant over-reaction in an
experiment in which the true data-generating process is an AR(1) process; and in fact, they
find the most severe degree of over-reaction (as discussed further below) when the process

                                               1
to be forecasted is white noise.
    Here we offer a different explanation for the pervasiveness of over-reaction. We consider
a model in which a decision maker's forecasts (or more generally, actions with consequences
that depend on the future realization of some variable) can be based both on currently
observable information and an imperfect memory of past observations. Subject to this con-
straint on the information that the decision rule can use, we assume that the rule is optimal.
Moreover, rather than making an arbitrary assumption about the kind of statistics about
past experience that can be recalled with greater or lesser precision, we allow the memory
structure to be specified in a very flexible way, and assume that it is optimized for the
particular decision problem, subject only to a constraint on the overall complexity of the
information that can be stored in (and retrieved from) memory -- or more generally, subject
to a cost of using a more complex memory structure.
    In the limiting case in which the cost of memory complexity is assumed to be negligible,
the predictions of our model coincide with those of the rational expectations hypothesis.
But when the cost is larger (or the constraint on memory complexity is tighter), our model
predicts that forecasts should be both heterogeneous (even in the case of forecasters who
observe identical data) and systematically biased. Moreover, the predicted biases include
the type of over-reaction to news documented in surveys of forecasts of macroeconomic and
financial time series by Bordalo et al. (2020a) and in laboratory forecasting experiments by
Afrouzi et al. (2020). And unlike the model of Fuster et al. (2010, 2011), our model predicts
that over-reaction to news will be most severe in the case of time series exhibiting little serial
correlation.
    In seeking to endogenize the information content of the noisy cognitive state on the basis
of which people must act, our theory is in the spirit of Sims's (2003) theory of "rational
inattention"; and indeed, we follow Sims in modeling the complexity constraint using infor-
mation theory. There is nonetheless an important difference between our theory and that of
Sims (2003). Sims assumes a constraint on the precision with which new observations of the
world can reflect any current or past conditions outside the head of the decision maker, but
assumes perfectly precise memory of all of the decision maker's own past cognitive states, and
also assumes that past external states can be observed at any time with the same precision as
current conditions. We instead assume (for the sake of simplicity) that the current external
state can be observed with perfect precision, but that memory of past cognitive states is
subject to an information constraint; and we further assume that the decision maker has no
access to external states that occurred in the past, except through (information-constrained)
access to her own memory of those past states. These differences are crucial for the ability
of our model to explain over-reaction to news.
    Other recent papers that explore the consequences of assuming that memory allows only
a noisy recollection of past observations include Neligh (2019) and Afrouzi et al. (2020).
While these authors also assume that some aspects of memory structure are optimized for a
particular decision problem, the classes of memory structures that they consider are different
than the one that we analyze here.
    Both of these papers assume that successive observations of the external state are indi-
vidually encoded (possibly with variable precision) and stored in memory at the time of the
observation; the precision of the record of each observation then evolves over time in a way
that is exogenously specified, rather than optimized; and finally, when memory is accessed

                                                2
later to make a decision, the nature of the signal about the contents of memory may also
be endogenized. (Neligh focuses on endogenizing the precision of the initial encoding of
individual observations; Afrouzi et al. instead focus on endogenizing the precision of what
is retrieved from memory.) Both papers take it as given that memory is a vector of sepa-
rately encoded values of individual observations, and allow no re-encoding of the contents
of memory between the time of an initial observation and its retrieval for use in a decision.
Our concern is instead with the way in which it is optimal for memory to be constantly
re-encoded as time passes, in order to make the most efficient use of a finite limit on the
complexity of the stored representations. We comment further on the differences between
our framework and those of these other papers below.1
    We present the assumptions of our model and state the optimization problem that we
consider in section 1. Section 2 offers a general characterization of the optimal memory
structure in our model, showing in particular that it is optimal under our assumptions
for the memory state at each point in time to be represented by a single real number, a
random variable the mean of which depends on the entire sequence of previous observations.
Section 3 illustrates the model's implications, discussing quantitative aspects of numerical
solutions of the model for particular parameter values. We emphasize the failure of beliefs
ever to converge to those associated with a rational expectations equilibrium, and show that
instead, there are perpetual stationary fluctuations in subjective beliefs similar (though not
identical) to those predicted by models of "constant-gain learning" (Evans and Honkapohja,
2001). Finally, section 4 presents the quantitative predictions of the model for statistics of
the kind reported by Afrouzi et al. (2020) and Bordalo et al. (2020a), showing not only
that the model can produce over-reaction to news, but that it can be parameterized so as to
predict roughly the degree of over-reaction measured by these authors. Section 5 concludes.


1     A Flexible Model of Imprecise Memory
Here we introduce the class of linear-quadratic-Gaussian decision problems that we study,
and specify the nature of a general constraint on the precision of memory. This gives rise to
a dynamic optimization problem, the solution to which we study below.

1.1     The forecasting problem
In the kind of decision problem which we consider, a decision maker [DM] observes the
successive realizations of a univariate stochastic process yt ("the external state"), which we
assume to be a stationary AR(1) process. We write the law of motion of this process in the
form
                                 yt = µ + (yt-1 - µ) + yt ,                               (1.1)
where µ is the mean value of the external state,  is the coefficient of serial correlation (with
|| < 1), and { yt } is an i.i.d. sequence, drawn each period from a Gaussian distribution
   1
     See section 1.2. In addition to considering a different class of possible memory structures, Neligh (2019)
addresses largely distinct questions from those analyzed here. Afrouzi et al. (2020) instead adopt the
explanation that we propose here for their (previously circulated) experimental findings, but show that
similar biases are also predicted by a simpler model of noisy memory than the one that we present here.


                                                      3
N (0,  2 ). The variance of the external state (conditional on the value of µ and the other
                                  2
parameters) will therefore equal y    2 /(1 - 2 ).
   The DM's problem is to produce each period a vector of forecasts zt , so as to minimize
the expected value of a discounted quadratic loss function
                                           
                                       E          t (zt - z
                                                          ~t ) W (zt - z
                                                                       ~t ),                                (1.2)
                                           t=0

where 0 <  < 1, W is a positive definite matrix specifying the relative importance of
accuracy of the different dimensions of the vector of forecasts, and the eventual outcomes
that the DM seeks to forecast are functions of the future evolution of the external state,2
                                                         
                                                 ~t 
                                                 z             Aj yt+j ,
                                                        j =0

where the coefficients {Aj } satisfy j |Aj | < . This formalism allows us to assume that
the DM may produce forecasts about the future state at multiple horizons (as is typically
true in surveys of forecasters, and also in the experiment of Afrouzi et al., 2020). It also
allows us to treat cases in which the DM may choose a vector of actions, the rewards from
which are a quadratic function of the action vector and the external state in various periods;
the problem of action choice to maximize expected reward in such a case is equivalent to a
problem of minimizing a quadratic function of the DM's error in forecasting certain linear
combinations of the value of the external state at various horizons.3
    To simplify our discussion, we assume that the second moments of the stochastic process
for the external state are known (more precisely, that the DM's decision rule can be optimized
for particular values of these parameters, that are assumed to be the correct ones), while the
first moment is not, so that the DM's decision rule must respond adaptively to evidence about
the unknown mean value provided by the DM's observations of the state. Thus the values of
the parameters  and  2 are assumed to be known, while µ is not; the parameter µ is assumed
to be drawn from a non-degenerate prior distribution, µ  N (0, ). Conditional on the
value of µ, the initial lagged state y-1 is assumed to be drawn from the prior distribution
        2
N (µ, y   ), the ergodic distribution for the external state given a value for µ. When we
consider the optimality of a possible decision rule for the DM, we integrate over this prior
distribution of possible values for µ and y-1 , assuming that the decision rule must operate
in the same way regardless of which values happen to be true in a given environment.
    Note that in the absence of any memory limitation -- and given the assumption (main-
tained in this paper) of perfect observability of the realizations of yt -- it should be possible
eventually for the DM to learn the value of µ to arbitrary precision, so that despite our
   2
     Note that the variables denoted z   ~t are not quantities the value of which is determined at time t; the
subscript t is used to identify the time at which the DM must produce a forecast of the quantity, not the
time at which the outcome will be realized. Thus the best possible forecast of z      ~t at time t, even with full
information, would be given by Et z ~t , which will generally not be the same as the realized value z ~t .
   3
     For example, in a standard consumption-smoothing problem with quadratic consumption utility, the
DM's level of expected utility depends on the accuracy with which "permanent income" is estimated at
each point in time. This requires the DM to forecast a single variable z     ~t , for which the coefficient Aj is
proportional to  j for all j  0.


                                                          4
assumption that the value of µ must be learned, the optimal decision rule should coincide
asymptotically with the full-information rational-expectations prediction. We show, how-
ever, that this is not true when the precision of memory is bounded.
   In any problem of this form (regardless of the assumed memory limitations, which have
yet to be specified), the DM's problem can equivalently be formulated as one of simply
choosing an estimate µ  ^t of the unknown mean µ at each date t, based on the information
available at the time that zt must be chosen. It follows from the law of motion (1.1) that
                                                 
                                    Et z
                                       ~t =          Aj [µ + j (yt - µ)],
                                              j =0


where we use the notation Et [·] for the expected value conditional on the true state at time
t, i.e., the value of µ and the history of realizations (y0 , . . . , yt ), even though not all of this
information is available to the DM. Conditioning instead on the coarser information set that
represents the DM's cognitive state at time t (and noting that this includes precise awareness
of the value of yt ), we similarly find that the optimal estimate of z       ~t will be given by
                                              
                                     zt =              µt + j (yt - µ
                                                   Aj [^            ^t )],                       (1.3)
                                            j =0

where µ^t is the expectation of µ conditional on the DM's information set at time t.
    We show in the appendix that the DM's expected loss cannot be reduced by restricting
attention to a class of decision rules of the form (1.3), under different possible assumptions
about how the estimate µ  ^t is formed.4 In the case of any forecasting rule of that kind, the
loss function (1.2) is equal to
                                                     
                                              ·            t M SEt                               (1.4)
                                                    t=0

plus a term that is independent of the DM's forecasts, where

                                                   µt - µ)2 ]
                                         M SEt  E[(^

is the mean squared error in estimating µ, and  > 0 is a constant that depends on the
coefficients {Aj } and W . Thus one can equivalently formulate the DM's problem as one of
optimal choice of an estimate µ
                              ^t each period, so as to minimize M SEt .

1.2       Memory structures and their cost
We assume that the memory carried into each period t  0 can be summarized by a vector
mt of dimension dt ; the action chosen in period t (i.e., the choice of µ ^t ) must be a function of
the cognitive state specified by st = (mt , yt ). The dimension of the memory state is assumed
only to be finite, and is not required to be the same for all t. (The case of perfect memory
can be accommodated by our notation, by assuming that dt = t, and that the elements of
the vector mt correspond to the values (y0 , y1 , . . . , yt-1 ).) We assume that current external
  4
      See Appendix A for details of the argument.


                                                          5
state yt is perfectly observable,5 but that behavior can depend on past states only to the
extent that memory provides information about them.
    We further suppose that the memory state evolves according to a linear law of motion of
the form
                       mt+1 = t st + t+1 ,       t+1  N (0, ,t+1 )                    (1.5)
starting from an initial condition of dimension d0 = 0 (that is, s0 consists only of y0 ).
However, the dimension dt+1 of the memory that is stored, and the elements of the matrices
t , ,t+1 are allowed to be arbitrary; we require only that ,t+1 must be positive semi-
definite (though it need not be of full rank).
    For example, one type of memory structure that this formalism allows us to consider is
an "episodic" memory of the kind assumed by Neligh (2019).6 In this case, dt = t, and there
is an element of mt corresponding to each of the past observations y for 0    t - 1
(generalizing the case of perfect memory just discussed). The memory of y at some later
time t is given by m +1,t = y + u +1,t , where u +1,t is a Gaussian noise term, independent
of the value of y , and with a variance that is necessarily non-decreasing in t. This can be
represented by letting dt = t, t be the identity matrix of dimension t + 1, and ,t+1 a
diagonal matrix of dimension n + 1 (with non-negative elements, but not necessarily of full
rank).
    Another type of memory that we can consider is one in which only the n most recent
past observations of the external state can be recalled, though these are recalled with perfect
precision. The requirement that forecasts be functions of the cognitive state would then
require them to be functions of (yt , yt-1 , . . . , yt-n ) for some finite n, as under the hypothesis
of "natural expectations" proposed by Fuster, H´         ebert, and Laibson (2011). This case would
correspond to a specification in which dt = n for all t; t is an n × (n + 1) matrix, the right
n × n block of which is an identity matrix, and the first column of which consists entirely
of zeroes; and ,t+1 = 0. Our formalism is much more flexible than either of these cases,
however, and neither of those specifications turns out to be optimal.
    We limit the precision of memory by further assuming that there is a cost of storing
and/or accessing the memory state mt+1 , that is an increasing function of the Shannon
mutual information between the memory state mt+1 and the cognitive state st about which
it provides information.7 If It is the mutual information between these two random variables,
we assume a memory cost in period t of c(It ), where c(I ) is an increasing, convex function.
Two extreme possibilities, both of which we consider further below, are the one in which
c(I ) is a linear function, c(I ) =  · I for some  > 0; and the opposite limiting case in which
   5
      We might also assume that the current state is observable only imprecisely, as in the model of Sims
(2003); but in the present treatment, we simplify the analysis, and highlight the consequences of imperfect
memory, by considering the limiting case in which there is no cost of precise observation of the current
external state.
    6
      Note however that Neligh's model is not a special case of ours, because in addition to restricting attention
to a more special class of memory structures, he assumes a different cost function for precision than the one
we propose below.
    7
      Mutual information is a non-negative scalar quantity that can be defined for any joint distribution
for (st , mt+1 ), that measures the degree to which the realized value of either random variable provides
information about the value of the other (Cover and Thomas, 2006). This measure is used to determine the
relative cost of different information structures in the rational inattention theory of Sims (2003); properties
of this measure as an information cost function are discussed in Caplin, Dean and Leahy (2019).


                                                        6
there is a finite upper bound I ¯ on feasible information transmission, with zero cost for any
      ¯
I < I . Either of these cases allows us to consider a one-parameter family of possible cost
functions, ranging from an arbitrarily loose information constraint ( near zero, I  ¯ very large)
to a prohibitively tight one ( extremely high, I  ¯ near zero).
    The cost c(It ) can equivalently be viewed as either a cost of storing a memory record
with information content It (that is then available with perfect precision in period t + 1),
or a cost of retrieving a signal from memory with information content It in period t + 1
(while the memory stored in period t is taken to have been a perfect record of the period t
cognitive state). These two formulations are identical, given that we assume that only the
signal mt+1 that is retrieved in period t + 1 can be stored for future use; thus only the fidelity
with which the retrieved memory mt+1 reproduces the cognitive state st matters. Under the
retrieval-cost interpretation, however, our model remains importantly different from the one
proposed by Afrouzi et al. (2020), in which memory contains a perfect record of all past
observations, but there is a cost of retrieving a precise signal about the contents of memory
for use in a decision. That model assumes that past observations can be stored indefinitely
with perfect precision, with a limit on the precision of recall becoming relevant only when
memory must be consulted; this means that it does not predict "recency bias" as ours does.8
    The memory structure each period, together with the rule for choosing an estimate µ      ^t as
a function of each period's cognitive state, are then assumed to be chosen so as to minimize
total discounted costs             
                                               t [ · M SEt + c(It ), ]                                      (1.6)
                                        t=0

taking into account both the cost of less accurate forecasts (1.4) and the cost of greater
memory precision. Note that no expectation is needed in this objective, since both M SEt and
It are functions of the entire joint probability distribution of possible values for µ, mt , yt , µ
                                                                                                  ^t
and mt+1 .


2      The Optimal Memory Structure
We turn now to a general characterization of the solution to the dynamic optimization
problem just posed.

2.1     Implications of linear-Gaussian dynamics
For any memory structure in the class specified in the previous section, the posterior dis-
tribution over possible values of (µ, y-1 , y0 , . . . , yt-1 ) implied by memory state mt will be a
    8
      See the discussion in section 3.4 below. The model of Afrouzi et al. also assumes that information that
is retrieved from memory (at a cost) for use in a decision at time t has no consequences for the information
that will be available at later times; the perfectly accurate record of all past observations continues to contain
the same information regardless of what is retrieved at time t, while the information retrieved (added to
"working memory") at time t is not available at any later time. This makes the problem of optimal selection
of the information to be retrieved at any time t a (relatively simple) static problem in their model, whereas
it is a dynamic problem in the model proposed here, since in our model, information not remembered at
time t cannot (at any cost) be retrieved in any later period.


                                                        7
multivariate Gaussian distribution. It is thus fully characterized by specifying a finite set of
first and second moments of the posterior associated with the memory state. Moreover, the
particular memory state mt at a given date t can be identified by the associated vector of
first moments. For the second moments of the posterior are the same for all possible memory
states at any time t: they depend on the matrices { , , +1 } for  < t, but not on the
history of the external state, or on the history of realizations of the memory noise {t+1 }.
In what follows, we therefore use the notation mt for the vector of posterior means.9
    Among the state variables about which the memory state may convey information, we
are particularly interested in the vector of variables xt = (µ, yt-1 ) , which are the states
determined prior to period t that are relevant for predicting the external state in periods
  t. Let m  ¯ t  E[xt |mt ] be the two elements of the memory state that identify the posterior
mean of xt , and let t be the 2 × 2 block of second moments of xt under this same posterior,
so that
                                     xt |mt  N (m   ¯ t , t ).
And let us furthermore introduce the vectors

                                    e1  [1 0],              c  [1 -  ]

to select particular elements of this reduced state vector. Then e1 m ¯ t is the posterior mean
and e1 t e1 the posterior variance for µ; while c m  ¯ t is the posterior mean and c t c the
posterior variance for the full-information forecast Et-1 yt .
    The posterior mean and variance for µ after also observing yt will then be given by the
usual Kalman filter formulas,

                               ^t  E[µ |st ] = e1 m
                               µ                  ¯ t + 1t [yt - c m
                                                                   ¯ t ],                                  (2.1)
                              2                          2            2
                             ^t  var[µ |st ] = e1 t e1 - 1 t [c t c +  ],                                  (2.2)
where the Kalman gain is equal to10
                                                       e1 t c
                                            1t =                .                                          (2.3)
                                                     c t c +  2
Since yt is observed precisely, these formulas completely characterize posterior beliefs in
cognitive state st about the states xt+1 that are relevant for forecasting y for all  > t. Note
        2
that  ^t  is necessarily positive (complete certainty about the value of µ cannot be achieved
in finite time, even in the case of perfect memory), and must satisfy the upper bound
                                                            2
                                            2       2
                                                         y
                                           ^t
                                                   ^0         2
                                                                ,                                          (2.4)
                                                          + y
   9
     Here we assume that we only need to distinguish between different memory states to the extent that they
correspond to different posteriors (that is, their information content is different). We could allow for multiple
memory states corresponding to the same posterior, for example by including an arbitrary random signal as
an additional component of the memory state. But in such a case, the notation for the memory state would
be of redundant complexity, since an optimal decision rule will always prescribe the same behavior in the
case of memory states that imply the same posterior.
  10
     We use a 1 subscript in the notation for this variable because it is the first element of a vector of Kalman
gains, defined in the more general formula given in Appendix B.


                                                        8
which corresponds to the degree of uncertainty about µ after observing the external state in
the case of no informative memory whatsoever (the DM's situation in period t = 0).
   Then the average losses from inaccurate forecasting in period t are given by
                                                        2
                                               M SEt = ^t .                                              (2.5)
This determines the value of one of the terms in (1.6) as a function of the posterior uncertainty
associated with the memory state each period. We note that the optimal estimate µ      ^t depends
only on m¯ t (not other components of the memory state), and that the average loss implied
by this estimate depends only on the posterior uncertainty t associated with those same
two components.

2.2     The sufficiency of memory of a reduced cognitive state
We further show in the appendix11 that an optimal memory structure makes the memory
state mt+1 a function only of the "reduced cognitive state"
                                                 µ
                                                 ^t
                                      ¯t 
                                      s                = E[xt+1 |st ].                                   (2.6)
                                                 yt
We first note (using (2.1) and the fact that yt is part of the cognitive state) that the elements
of s
   ¯t are a linear function of st . Thus we can choose a representation of the vector st in which
its elements are made up of two parts, s   ¯t and st , where the elements of st are uncorrelated
with those of s ¯t . We then observe that
                                           m         st |mt+1 ].
                                           ¯ t+1 = E[¯
Hence the only aspect of the memory state that matters for m ¯ t+1 , and hence for determining
both the optimal estimate µ ^t+1 and the reduced cognitive state s
                                                                 ¯t+1 , will be the information
that mt+1 contains about s ¯t .
    To the extent that mt+1 conveys any information about the elements of st , this informa-
tion has no consequences for the DM's estimates µ ^ in any periods   t + 1, but it increases
the mutual information between st and mt+1 , and hence the information cost c(It ). Hence
under an optimal information structure, the reduced memory state m    ¯ t must evolve according
to a law of motion of the form
                                          m
                                          ¯ t+1 = ¯ ts
                                                     ¯t + ¯ t+1 ,                                        (2.7)
where  ¯ t+1  N (0,     ¯ ,t+1 ) is distributed independently of the cognitive state. And in
addition, the complete memory state must convey no more information about st than what
is conveyed by the reduced memory state, so that we can without loss of generality assume
that mt+1 consists solely of m  ¯ t+1 (so that dt+1 = 2 for all t  0).12
  11
    See Appendix C for details of the argument.
  12
    Note that this is not the unique solution to the problem posed at the end of the previous section, since we
can add additional elements to the vector mt+1 , each of which is a linear function of m¯ t+1 plus independent
random noise, without changing either It or the inferences that are drawn from the memory state in period
t + 1. However, adding such additional elements to the memory state simply makes the representation of
the memory state redundant, without changing the implications for observable behavior. Thus we assume
that the memory state consists solely of the reduced memory state m   ¯ t+1 .


                                                      9
   Finally, the 2 × 2 matrices ¯ t and ¯ ,t+1 must satisfy additional restrictions in order for
the reduced memory state defined in (2.7) to be consistent with the normalization

                                            st |m
                                          E[¯   ¯ t+1 ] = m
                                                          ¯ t+1 .                                      (2.8)

We show in the appendix that this relationship will hold if and only if13
                                                      ¯     ¯
                                        ¯ ,t+1 = (I - t )Xt t ,
                                                                                                       (2.9)

where Xt  var[¯
              st ]. Note that (2.6) implies that

                                                  st ] + var[xt+1 |st ],
                                 var[xt+1 ] = var[¯

from which we see that
                                                         2
                                         2             -^t  
                               Xt = X (^
                                       t   )                  2             .                         (2.10)
                                                            + y
                                                          2
Thus the matrix Xt depends only on the value of         ^t  . In addition, (2.4) implies that Xt will
be positive semi-definite (p.s.d.), and non-singular (hence positive definite) except in the
            2      2
case that  ^t =  ^0  (the case of a totally uninformative memory state mt ).
    In order for it to be possible for (2.9) to hold, the matrix   ¯ t must satisfy certain properties:
(a) the matrix   ¯ t Xt = Xt ¯ must be symmetric (so that the right-hand side of (2.9) is also
                               t
symmetric); and (b) the right-hand side of (2.9) must be a p.s.d. matrix. For any symmetric,
positive definite 2 × 2 matrix Xt , we let L(Xt ) be the set of matrices     ¯ t with these properties.
                                        ¯
Then in addition to assuming that t  L(Xt ), the variance matrix                ¯ ,t+1 must be given by
(2.9).
    In the special case in which mt is completely uninformative, µ          ^t is proportional to the
observation yt , so that there exists a vector w >> 0 such that s       ¯t = w · yt . In this case,
                                                   2
                                      Xt = X0  [ + y ] ww ,

and we can show that the requirements stated above are satisfied by a matrix            ¯ t if and only
if  ¯ t w = t w (w is a right eigenvector), with an eigenvalue satisfying 0  t  1. Since the
two elements of s     ¯t are perfectly collinear in this case, the only part of the matrix      ¯ t that
matters for the evolution of the memory state is the implied vector          ¯ t w (which must be a
multiple of w). Thus we can without loss of generality impose the further restriction that if
^t2
     =    2
         ^0 , we will describe the dynamics of the memory state using a matrix       ¯ t of the form

                                              ¯ t = t ww ,
                                                                                                      (2.11)
                                                      ww
for some 0  t  1. We now adopt this more restrictive definition of the set L(X0 ) in this
special case.14
  13
    See the introductory section of Appendix D for details of the argument.
  14
    Restricting the set of transition matrices ¯ t that may be chosen in this way has no consequences for the
evolution of the memory state, but it makes equation (2.12) below also valid in the case that Xt = X0 , and
thus it allows us to state certain conditions below more compactly.


                                                     10
    We have now shown that the memory structure for period t is completely determined by
a specification of a matrix   ¯ t  L(Xt ). In any period t, the value of  ^t2
                                                                              and hence the matrix
Xt will be implied by the choice of memory structure for the periods prior to t. Given a
choice of ¯ t , the variance-covariance matrix   ¯ ,t+1 is uniquely determined by (2.9). As shown
                    15
in the appendix, this then uniquely determines t+1 , indicating the degree of uncertainty
                                                                    2
implied by the memory state mt+1 , which then determines           ^t+1 using (2.2). The degree of
uncertainty about µ in the following period is then given by a function of the form
                                            2          2 ¯
                                           ^t+1 = f (^
                                                     t  , t ),

that is uniquely defined for any non-negative      ^t2
                                                        satisfying the bound (2.4) and any        ¯t 
        2
L(X (^t   )).
                                                                          2
    Then given that we start from an initial degree of uncertainty       ^0 at time t = 0 defined
                                                       ¯
by (2.4), we can define the class of sequences {t } for all t  0 with the property that
¯ t  L(Xt ) for all t  0; let us call this class Lseq . Moreover, for any sequence of transition

matrices in Lseq , we can uniquely define the sequences of values {t , 1t ,   ^t2
                                                                                  , Xt } for all t  0
                                             ¯         seq
implied by it. Thus given any sequence {t }  L , we can calculate the implied sequence
of losses {M SEt } from forecast inaccuracy, using (2.5).
    We can also uniquely identify the information cost implied by such a sequence of transition
matrices, since as shown in the appendix,16 the mutual information between st and mt+1 will
be given by
                              It = I ( ¯ t )  - 1 log det(I -     ¯ t)                          (2.12)
                                                  2
each period. Note that the requirement that      ¯ t  L(Xt ) implies that

                                        0 < det(I - ¯ t )  1,

so that the quantity (2.12) is well-defined, and necessarily non-negative. As the elements of
¯ t are made small, so that memory ceases to be very informative about the prior cognitive

state, I - ¯ t approaches the identity matrix, and It approaches zero. If    ¯ t is varied in such a
                                                          ^
way as to make one of its eigenvalues approach 1, I - t approaches a singular matrix, and
  ^ ,t+1 must approach a singular matrix as well; this means that in the limit, some linear
combination of the elements of s   ¯t is a random variable with positive variance that comes to
be recalled with perfect precision. In this case, det(I -  ^ t ) approaches zero, so that It grows
without bound.
      Thus a given sequence of transition matrices {      ¯ t } uniquely determines sequences
{M SEt , It }, allowing the value of the objective (1.6) to be calculated. The problem of
optimal design of a memory structure can then be reduced to the choice of a sequence
{ ¯ t }  Lseq so as to minimize (1.6). This objective is necessarily well-defined for any such
sequence, since each of the terms is non-negative; the infinite sum will either converge to a
finite value, or will diverge, in which case the sequence in question cannot be optimal.17
  15
      See Appendix D.1 for details of the argument.
  16
      See Appendix D.2 for details of the argument.
   17
      Note that it is clearly possible to choose memory structures for which the infinite sum converges. For
example, if one chooses    ¯ t = 0 for all t  0 (perfectly uninformative memory at all times), M SEt =  2
                                                                                                       ^0 and
                                                                 2
It = 0 each period, and (1.6) will equal the finite quantity    ^0 /(1 -  ).


                                                     11
2.3     A recursive formulation
                                                                      2
We now observe that if for any point in time t, we know the value of ^t (which depends on the
choices made regarding memory structure in periods  < t), the set of admissible transition
matrices {¯  } for   t specifying the memory structure from that time onward will depend
                      2
only on the value of ^t , and not any other aspect of choices made about the earlier periods.
Moreover, any admissible continuation sequence {    ¯  } for   t implies unique continuation
sequences {M SE , I } for   t, so that the value of the continuation objective
                                       
                                              -t [ · M SE + c(I )]                                  (2.13)
                                       =t

will be well-defined.18
    We can then consider the problem of choosing an admissible continuation plan {           ¯  } for
                                                                        2
  t so as to minimize (2.13), given an initial condition for          ^t . (This is simply a more
general form of our original problem choosing memory structures for all t  0 to minimize
                                        2                               2
(1.6), given the initial condition for ^0 specified in (2.4).) Let V (^t  ) be the lowest achievable
                                                           2
value for (2.13), as a function of the initial condition ^t ; this function is defined for any value
     2
of ^t satisfying the bound (2.4), and is independent of the date t from which we consider
the continuation problem. Note that the lower bound necessarily lies in the interval

                                2       2      2                   
                               ^t  V (^
                                      t   )   ^t +                  ^2 .
                                                                                                    (2.14)
                                                                  1- 0
                                                         2
(The lower bound follows from the fact that M SEt =     ^t , and all other terms in (2.13) must
be non-negative; the upper bound is the value of (2.13) if one chooses     ¯  = 0 for all   t,
which is among the admissible continuation plans.)
   This value function also necessarily satisfies a Bellman equation of the form

                        t
                     V (^ 2
                            ) =       min       [  2
                                                  ^t + c( I ( ¯ t )) + V (f (^
                                                                             t 2 ¯
                                                                                , t ))],            (2.15)
                                  ¯ t L(X (^
                                           t 2 ))



where I (   ¯ t ) is the function defined in (2.12). Thus once we know how to compute the value
                                       2
function for arbitrary values of      ^t+1 , the problem of the optimal choice of a memory structure
in any period t can be reduced to the one-period optimization problem stated on the right-
hand side of (2.15). This indicates how the memory structure for period t must be chosen
to trade off the cost c(It ) of retaining a more precise memory against the continuation loss
     2
V (^
   t  +1 ) from having access to a less precise memory in period t + 1.
                                                           2                           2
    Let F be the class of continuous functions V (^       t  ), defined for values of ^t consistent with
(2.4), and consistent with the bounds (2.14) everywhere on this domain. Then (2.15) defines
                                                                      2
a mapping  : F  F : given any conjectured function V (^              t +1 )  F that is used to evaluate
                                             2
the right-hand side for any value of        ^t , the minimized value of the problem on the right-hand
side defines a new function V     ~ (^
                                     t 2
                                         ) that must also belong to F . Condition (2.15) states that
  18
    The infinite sum may diverge, but because all terms are non-negative we can state unambiguously
that the continuation value of the objective is + under such a plan. Moreover, since a finite value for
the continuation objective is always possible (see (2.14) below), it is clear that plans that make (2.13) a
divergent series cannot be optimal, and can be excluded from consideration.


                                                       12
the value function that defines the minimum achievable continuation loss must be a fixed
point of this mapping: a function such that V = (V ).
    We can further show that for any function V  F , the function (V ) defined by the
right-hand side of (2.15) is necessarily a monotonically increasing function.19 It follows that
                      2
the fixed point V (^t   ) must be a monotonically increasing function. Moreover, we can restrict
the domain of the mapping  to the subset F  of increasing functions.
    This then provides us with an approach to computing the optimal memory structure
for a given parameterization of our model. First, we find the value function V (^         2)  F 
that is a fixed point of the mapping , by iterating  to convergence. Then, given the
value function, we can numerically solve the minimization problem on the right-hand side
of (2.15) to determine the optimal transition matrix         ¯ t in any period, once we know the
          2
value of ^t for that period. Solution of this problem also allows us to determine the value of
  2         2 ¯                                                  2
^t+1 = f (^
          t  , t ), so that the entire sequence of values {    ^   } for all   t can be determined
                  2                                                                       2
once we know    ^t . Finally, we recall that for the initial period t = 0, the value of  ^0 is given
                                                           2
by (2.4); we can thus solve for the entire sequence {     ^ } for all t  0 by integrating forward
from this initial condition.
                                                                2
    Once we have determined the sequence of values {          ^t  } implied by an optimal memory
                                                                                             2
structure for each period, we can determine the elements of the matrix Xt = X (^           t   ) each
                                                       20
period, using (2.10). We show in the appendix that the degree of uncertainty at the
beginning of any period given the structure of the memory chosen for the previous period is
given by
                                       t+1 = 0 - Xt        ¯ t.
This in turn allows us to calculate the DM's optimal estimate µ   ^t each period, as a function
of the history of realizations {y } of the external state for all 0    t and the history of
realizations of the DM's memory noise {    ~  +1 } for all 0    t - 1, using (2.1). The DM's
complete vector of forecasts zt each period is then given by (1.3).

2.4        Optimality of a unidimensional memory state
We can show further that the optimal memory state must have a one-dimensional represen-
tation. This simplifies the computational formulation of the optimization problem on the
right-hand side of (2.15), and provides further insight into the nature of an optimally impre-
cise memory. Although the information contained in the cognitive state st that is relevant
for predictiing (at time t) what actions will be desirable for the DM in later periods is two-
dimensional (both elements of s  ¯t matter, if  > 0, and except when memory is completely
uninformative, these are not perfectly correlated with each other), we find that it is optimal
for the DM's memory to include only a noisy record of a single linear combination of the two
variables. Moreover, this is true regardless of how small memory costs may be.
    There is in fact a fairly simple intuition for the result. Note that in any period t, the
Kalman filter (2.1) implies that the optimal estimate of the unknown value of µ will be given
by a linear function of elements of the cognitive state of the form
                                             µ
                                             ^t = t + t m
                                                        ¯ t.                                  (2.16)
  19
       See Appendix E.1 for a proof.
  20
       See Appendix D.1 for details of the argument.


                                                       13
It follows from this that the only information in the memory state mt that matters for the
estimate µ^t is the single quantity t m¯ t.
    We can establish the optimality of a unidimensional memory in the following way. Con-
sider the optimization problem on the right-hand side of (2.15) in any period t, given the
                           2
degree of uncertainty    ^t  determined by the memory structures chosen in earlier periods.
                     2
The fact that V (^ t+1 ) is an increasing function, and that c(It ) is at least weakly increasing,
means that an optimal memory structure must minimize the mutual information It given
                   2                                            21
the uncertainty   ^t+1 that it implies for the following period.    Hence the optimal choice for
¯ t must solve the problem


                                  min           I (¯ t)    s.t. f (^
                                                                   t 2 ¯
                                                                      , t )   2
                                                                             ^t
                              ¯ t L(X (^ 2 ))                                  +1 ,                      (2.17)
                                       t

                       2   2                                     2   2
for given values of (^
                     t   ,^t+1 ). We shall show that whenever (^
                                                               t   ,^t+1 ) are such that the set of
                                                            22
matrices satisfying the constraint in (2.17) is non-empty, the solution         ¯ t to this problem
must be at most of rank one. Thus it must be of the special form
                                                   ¯ t = t Xt v t v ,
                                                                                                         (2.18)
                                                                   t

where t is a scalar satisfying 0    1 and vt is a vector normalized to satisfy vt Xt vt = 1.
It follows that in each period m¯ t+1 = Xt vt m
                                              ~ t+1 , where m
                                                            ~ t+1 is a unidimensional memory
state with a law of motion

                       m
                       ~ t+1 = t vt s
                                    ¯t + ~ t+1 ,               ~ t+1  N (0, t (1 - t )).
                                                                                                         (2.19)
           2   2
       If ^t =^0 , the set L(X0 ) consists only of matrices of the form (2.18), with
                                                               w
                                           vt =             2 )1/2 (w
                                                                             ,                           (2.20)
                                                      ( +   y           w)

because of (2.11). Hence the asserted result is obviously true in that case. Suppose instead
that  ^t2
          <  2
            ^0 , and consider any matrix    ¯ t  L(X (^
                                                      t 2
                                                          )) that satisfies the constraint in (2.17).
If ¯ t is not itself of rank one (or lower), we shall show that we can choose an alternative
transition matrix of the form (2.18), that is also consistent with the constraint in (2.17), but
which achieves a lower value of I (  ¯ t ).
    Let the alternative transition matrix be given by (2.18), with

                               t+1 ¯ t Xt ¯ t+1                             ¯ t t+1
                                           t
                       t =              ¯       ,           vt =         ¯ t Xt ¯ t t+1 )1/2 ,
                               t+1 Xt t t+1                         (t+1 

where t+1  e1 - 1,t+1 c is the vector introduced in (2.16), and let the matrix     ¯ ,t+1 be
correspondingly modified in the way specified by (2.9). We show in the appendix23 that this
  21
      In the case that c(I ) is constant over some interval, reducing It need not reduce c(It ), but it cannot
increase it; thus the solution to the problem (2.17) must be among the solutions to the problem on the
right-hand side of (2.15), even if it is not a unique solution. In such case, showing that the solution to (2.17)
is necessarily a singular matrix suffices to show that we can without any loss impose the further constraint
in (2.15) that the matrix   ¯ t must be at most of rank one.
   22                                       2                               2
      Note that this must be the case if   ^t +1 is chosen optimally given ^t .
   23
      See Appendix E.2 for details of the argument.


                                                          14
specification implies that 0  t  1, so that this alternative matrix also belongs to L(Xt ).
Moreover, the new memory structure implies a conditional distribution

                                   ¯ t+1 |st  N (t+1 
                               t+1 m                 ¯ ts
                                                        ¯t , t+1 ¯ ,t+1 t+1 )

that is the same as under the original memory structure. This implies that the optimal
estimate µ   ^t+1 conditional on the cognitive state st+1 will be the same function of m                      ¯ t+1
and yt+1 in the case of the new memory structure, and that the conditional distribution
                                                             2
^t+1 |st , yt+1 will be the same. It follows that 
µ                                                          ^t +1 will be the same, so that the alternative
transition matrix also satisfies the constraint in (2.17).
     At the same time, we show in the appendix that the reduction in the complexity of
memory cannot increase information costs in any period.24 The new memory structure
consists effectively of a scalar memory state m             ~ t+1 in each period, which is a multiple of
dt+1 m¯ t+1 , a particular linear combination of the elements of the memory state under the
previous memory structure. Hence the information about s                    ¯t that is revealed by mt+1 under
the new memory structure (i.e., that is revealed by m           ~ t+1 ) is also information that was revealed
by m ¯ t+1 under the previous memory structure; thus the value of It under the previous memory
structure must have been at least as large as under the new memory structure. In fact, the
only case in which the mutual information will not be reduced by the proposed modification
of the memory structure is if all elements of m            ¯ t+1 were multiples of dt+1 m      ¯ t+1 ; which is to
               ¯
say, only if t were already of the special form (2.18).
     We conclude, then, that an optimal memory structure must involve a transition matrix
in every period of the special form (2.18), so that the memory state each period can be
represented by a scalar quantity m        ~ t . The choice of memory structure can then be reduced
to a problem of choosing, in each period t  0, a scalar quantity 0  t  1, and the
direction of a vector vt (the length of which will then be chosen each period so as to ensure
that vt Xt vt = 1); the values chosen for these quantities then determine the law of motion
for the unidimensional memory state m            ~ t+1 , specified by (2.19). This in turn determines the
elements of the matrix t+1 , and hence the value of the gain coefficient 1,t+1 in the Kalman
                                                2                                                        2
filter formula (2.1) and the value of         ^t +1 , which determines the matrix Xt+1 = X (^            t+1 ).
                             2    2              2                                                     2
     For any value 0       ^t <  ^0 , let V (^ t ) be the set of vectors vt satisfying vt X (^       t )vt = 1. In
                   2     2                                                         2
the case that    ^t =   ^0 , we add the further stipulation that V (^           0 ) consists only of the single
                                                   2
vector (2.20). Then given a value for            ^t  , determined by the memory structures for periods
 < t, the memory structure for period t is specified by a scalar quantity 0  t  1 and a
                   2                                          2           2
vector vt  V (^  t   ). These determine a value for         ^t +1 = f (^t   , t , vt ), where now the function f
                                                                         2       2                               2
is defined for any values of its arguments satisfying 0                ^t      ^0  , 0  t  1, and vt  V (^      t  ).
     Moreover, it follows from (2.12) that the mutual information associated with the period
t memory structure will be given by
                                                  1
                                            It = - log(1 - t ).                                              (2.21)
                                                  2
The Bellman equation (2.15) can therefore be written in the simpler form
              2                         2                                   2
         V (^
            t   ) =        min     2)
                                     [ ^t + c(-(1/2) log(1 - t )) + V (f (^
                                                                          t   , t , vt ))].                  (2.22)
                      0t 1,vt V (^
                                 t
  24
       See Appendix E.2 for details of the argument.


                                                         15
3     Features of the Model Solution
Here we provide numerical examples of solutions for an optimal memory structure, under
alternative assumptions about both the degree of persistence of the process that must be
forecasted and the nature of the information cost function. In reporting our results, it is
useful to describe the model solution in terms of scale-invariant quantities -- that is, ones
that are independent of the value of y , indicating the amplitude of the transitory fluctuations
in the external state around its mean. Thus we parameterize the degree of prior uncertainty
about µ not in terms a value for  (the variance of the prior distribution for µ), but rather by
                      2
a value for K  /y       (the variance of the prior distribution for µ/y ). We similarly measure
the degree of uncertainty about µ conditional on the cognitive state at a given point in time
                                                                            2
(i.e., after a given amount of experience) not in terms of the value of    ^t , but rather by the
                                    2   2
scaled uncertainty measure t       ^t /y  .
    In terms of this scaled uncertainty measure, an optimal memory structure minimizes the
value of                                
                                               t [t + c
                                                      ~(It ), ]
                                        t=0
                                                                                              2
a scaled version of (1.6), where the scaled cost function is defined as c   ~(I )  c(I )/(y     ).
(Dividing by  further reduces the numbers of parameters that we need to specify in con-
sidering the different possible forms that the optimal memory structure may take, since it
is only the relative weights on the two loss terms in the objective (1.6) that matter for the
optimal memory structure.) Our scale-invariant model is then completely specified by values
for the parameters , , K and the scaled cost function c     ~(I ). In our quantitative analysis,
we assume that each "period" of our discrete-time model corresponds to a quarter of a year
(the variable to be forecasted is a quarterly time series), and hence set  = 0.99 (implying
a discount rate of 4 percent per annum). We consider a variety of values 0   < 1 for the
assumed degree of serial correlation of the external state, and explore the effects of different
assumptions regarding the degree of prior uncertainty and the size of information costs.

3.1    The case of a fixed per-period bound on mutual information
We begin by considering the case in which c    ~(I ) = 0 for all I  I,¯ but the cost is infinite for
                ¯
any value I > I . (That is, there is a fixed upper bound on the possible mutual information
between st and mt+1 in each period; but any memory structure consistent with this bound
is equally feasible and has the same cost.) Here I    ¯ is some finite positive quantity. Solution
                                                                                             2
for the optimal memory structure is particularly simple in this case. Because V (^          t  ) is a
                                                                                       2
monotonically increasing function, it is clear that given a degree of uncertainty    ^t associated
with the period t cognitive state, one wishes to choose a memory structure in period t so as to
minimize the implied value of    2
                                ^t                                        ¯
                                  +1 , consistent with the bound It  I. (There is no trade-off
to consider between economizing on information costs in period t and reducing forecasting
errors later.)
    Because of (2.21), the per-period bound on mutual information can equivalently be writ-
ten as an upper bound t  ,    ¯ where
                                                 ¯
                                       ¯  1 - e-2I
                                                   > 0.

                                                  16
The optimal memory structure in period t is then given by the (t , vt ) that solve the static
problem
                                            2
                                   min f (^
                                          t   , t , vt ),
                                            t ,vt

where the minimization is over values of the arguments satisfying 0  t      ¯ and vt  V (^
                                                                                         t 2
                                                                                             ).
                          25
We show in the appendix that the minimizing value of t is necessarily the largest feasible
value; hence in the solution to this problem, t = ,  ¯ the value determined by the per-period
information bound. We can then characterize the optimal memory structure more simply as
the solution to the problem
                                                  2 ¯
                                        min2 f (^
                                                t  , , vt ).                            (3.1)
                                          vt V (^
                                                t )

   We further show in the appendix that the objective function in (3.1) can alternatively be
written as
                                  2              2 ~
                             f (^
                                t   , t , vt ) = y · f (t , t , v
                                                                ~t ),
where v ~t  y · vt , and the function f~ is independent of the size of y . Moreover, the set of
                        -1         2
vectors v~t such that y    ~t  V (y
                           v         · t ) is independent of the size of y , for a given value of
t . Hence the problem of choosing an optimal vector v      ~t can be written in a scale-invariant
form, and the solution is given by a policy function v ~t = v ~ (t ; ¯ ) that is independent of the
            26
size of y . The implied degree of uncertainty in the next period's cognitive state is then
given by
                                         t+1 = (t ;   ¯ ),                                    (3.2)
where
                                      ( ; ¯)  f
                                              ~(, ,
                                                  ¯ v
                                                    ~ ( ; ¯ ))
is a function that is independent of the scale factor y .
    For any value of ¯ indicating the tightness of the constraint on the complexity of memory,
equation (3.2) indicates how the DM's degree of uncertainty about µ evolves as additional
observations of the external state are made. Starting from the initial condition 0 = K/(K +
1) implied by (2.4), the law of motion (3.2) can be iterated to obtain a unique solution for
the complete sequence of values {t } for all t  0. In the limiting case       ¯ = 1 (unlimited
memory), we show that an analytical solution is possible for (3.2), namely the difference
equation
                                       1       1     1-
                                           =      +        .                              (3.3)
                                     t+1       t     1+
This is simply the standard result for the linear growth in posterior precision under Bayesian
updating as additional observations are made; it has the implication that t declines mono-
tonically, and converges to zero for large t. Thus in the case of perfect memory, the DM
should eventually learn the value of µ with perfect precision, and hence make forecasts of
the kind implied by the hypothesis of rational expectations.
    When   ¯ > 0, instead, the law of motion (3.2) implies that t should decrease initially, as
even imprecise memory of the DM's observations of the external state reduces uncertainty
  25
    See Appendix F.1 for details of the argument.
  26
    The value of this function also depends on the values of the parameters  and K , which we do not
write explicitly as arguments. We write  ¯ as an argument of the function because we are interested, in the
discussion below, in considering how the solution changes with changing values for t .


                                                      17
Section 3.1 The case of a fixed per-period bound on mutual information


                                Figure 1: The evolution of scaled uncertainty about µ


                 0.5                   t                         0.5
                                                                                    ()
                                                 = 0.30

                 0.4                             = 0.60          0.4
                                                 = 0.80
                 0.3                                             0.3
                                                 = 0.90
                 0.2                             = 0.95          0.2
                                                 = 0.99
                 0.1                                             0.1
                                                 = 1.00
                 0.0                                             0.0
                       0        5          10         15               0   0.25    0.5    0.75       1
                                    Time
   Figure 1: The evolution of scaled uncertainty about µ as the number t of previous (im-
   perfectly remembered) observations grows. The right panel shows the long-run value of
Figure 2: Coefficients describing the optimal memory structure in the long run, as a function of the degree of persistence
   scaled uncertainty (to which t converges as t  ) as a function of the constraint on the
   complexity of memory, parameterized by             ¯.
                                                          0.6                                            0.0
   to some degree, but that t remains bounded away from zero, and converges to a value
                                                         0.4                                          -0.2
    (    ¯ ) > 0. This is illustrated in Figure 1,         whichv shows the dynamics         implied    by (3.2) for
                                              ¯  = 0.95                                   27
   each of several different values of , in the case that  = 0 and K = 1. The left panel plots
   the sequence of values {t } implied=by
                                                         0.2 for a given value of 
                                                   0.80(3.2)                          ¯ . (Note that  -0.4the initial
                                                 = 0.30
   value 0 is the same in each case.) The right panel shows the value  to which the sequence
                                                         0.0 ¯                                        -0.6
   converges as 0.0        0.2 this
                    t grows;       0.4    0.6depends
                                      value        0.8 on         0.0 the
                                                               , and     0.2    0.4 0.6
                                                                           functional          0.8
                                                                                         relationship    between     ¯
   and this limiting degree of uncertainty can be described by a function  (), plotted as a       ¯
   smooth curve in the right panel of the figure.
        In the case that   ¯ = 1 (shown as a dashed      0.6curve in the left panel of Figure 1), the sequence
   {t } decreases monotonically to zero at the rate predicted by the difference equation              0.8       (3.3).
   But for any 1
                                                         0.4                                                         ¯
                  number of prior observations t > 0, the        m value of t remains higher the lower is 
   (that is, the tighter the memory constraint),         0.2 and   the long-run degree of uncertainty 0.4      about
                                                               ¯
   µ, measured by  , is a decreasing function of  as well, as shown by the curve in the right
   panel of the figure. Because of the limit on          0.0
                                                           the amount of information that can         0.0
                                                                                                        be retained
                    0.0    0.2     0.4    0.6      0.8            0.0    0.2    0.4    0.6
   in memory, the DM's uncertainty about the value of µ never falls below a certain level,     0.8
   even in the long run, despite our assumption that the value of µ is fixed for all time. We
   further observe that the long-run degree of uncertainty  is larger, the smaller is                         ¯ (that
The  top-right
   is,         panel shows
       the tighter         the direction
                      the constraint     of the
                                        on      vector vIn
                                            memory).      , and
                                                             thethe bottom-right
                                                                  limit  as ¯ approaches      the "intrinsic"
                                                                                 panel showszero              persistence
                                                                                                   (corresponding
derived as m   (e1 v ) (e1 - 1 c) X v .
   to a constraint that memory must be completely uninformative), the long-run degree of
   uncertainty  approaches the prior degree of uncertainty 0 = K/(K + 1).
     27
       The effects of variation in the parameters  and K are illustrated in additional figures shown in Appendix
   F.1. We use the parameterization K = 1 in the figures shown in the text because this value allows a
   reasonably good fit of the predictions shown in Figure 10 below with the experimental evidence reported by
   Afrouzi et al. (2020).                               2

                                                            18
                                    Time

Figure 2: Coefficients describing the optimal memory structure in the long run, as a function of the degree of persistence


                                                          0.6                                         0.0
                                                          0.4                                         -0.2
                                                                    v
                                                 = 0.95
                                                 = 0.80   0.2                                         -0.4
                                                 = 0.30
                                                          0.0                                         -0.6
                        0.0   0.2   0.4    0.6    0.8                   0.0   0.2   0.4   0.6   0.8

                                                          0.6
                                                                                                      0.8
                                                          0.4
                    1                                                m
                                                          0.2                                         0.4

                                                          0.0                                         0.0
                        0.0   0.2   0.4    0.6    0.8                   0.0   0.2   0.4   0.6   0.8

      Figure panel
The top-right 2: Coefficients  describing
                   shows the direction      the
                                       of the    optimal
                                              vector      memory
                                                     v , and        structure
                                                             the bottom-right  in the
                                                                              panel    long
                                                                                    shows   run,
                                                                                          the      as a function
                                                                                              "intrinsic" persistence
      ofas
derived  the
           m degree
                (e1of vpersistence
                        ) (e1 - 1 c) X  vof                                                      ¯
                                           . the external state, for alternative values of . Respective

      panels show the long-run values for  (measuring uncertainty about µ), the direction vector
      v (indicating the content of the memory state), the Kalman gain 1 (for updating the DM's
      estimate of µ), and m (measuring the intrinsic persistence of fluctuations in the memory
      state).
                                                                2

          As t falls along one of these trajectories, the optimal direction vector vt implied by the
      solution to (3.1) shifts as well. As t converges to the long-run value  , the direction vector
      vt similarly converges to a long-run value v , indicating the particular linear combination of
      µ
      ^t and yt that is imprecisely recorded in memory each period. Associated with this stationary
      long-run memory structure there will also be a stationary long-run value for the Kalman gain
      coefficient 1 in equation (2.1), and more generally, stationary values for the coefficients of
      the linear difference equations describing the joint dynamics {yt , m   ~ t } of the external state
      and the memory state.
          These long-run stationary coefficients will depend on the value of     ¯ (indicating the tight-
      ness of the memory constraint) and also on the value of  (indicating the degree of persistence
      of the fluctuations in the external state). Figure 2 indicates how variation in each of these
      parameters affects several of the long-run stationary coefficients. In each panel, a curve
      shows how the coefficient in question varies as a function of  (for values of  between 0.0
      and 0.9), for a given value of ¯ ; curves of this kind are shown for each of three different values
         ¯                    ¯
      of , ranging between  = 0.95 (in which case memory is relatively precise) and          ¯ = 0.30 (in
      which case it is much more constrained). All of the curves shown in Figure 2 are again for
      the case of prior uncertainty K = 1.
          The upper-right panel of the figure shows the long-run direction vector v ; the quantity
      reported on the vertical axis is the long-run value of the ratio v2 /v1 of the vector's two
      components.28 Thus a value of -0.3 for this quantity means that the univariate memory
        28
             This information (together with the value of  given in the upper left panel) suffices to completely


                                                                19
state m~ t+1 is (up to a multiplicative factor that does not affect its information content) equal
to the value of µ ^t - 0.3yt , plus additive Gaussian noise. The figure shows that when  = 0,
the optimal univariate memory state involves v2 = 0; that is, only the current estimate
µ
^t of the unknown mean is remembered with noise, with the current observation yt being
completely forgotten. This is optimal because when  = 0, the current value yt contains no
information that is relevant for improving subsequent forecasts of the external state, except
to the extent that it helps to improve the DM's estimate of µ (which information is already
reflected in the estimate µ   ^t ). Instead, when the external state is serially correlated, it is
optimal to commit to memory a linear combination of µ         ^t and the current state yt ; in the
case that  > 0, the optimal linear combination puts a negative relative weight on yt , to an
extent that is greater the greater the degree of serial correlation, and greater the tighter the
constraint on memory.
    The upper-left panel of the figure shows the long-run degree of uncertainty about µ,
measured by  . As shown in Figure 1, when  = 0,  is a decreasing function of .               ¯ We
see in Figure 2 that this is also true when  > 0. However, for a given memory constraint
¯ the long-run value  is also an increasing function of , with the degree of increase
,
when the external state is highly persistent being particularly notable when memory is more
accurate. The greater the serial correlation of the state, the fewer the effective number
of independent noisy observations of µ that the DM receives over any finite time period;
thus even under perfect Bayesian updating, equation (3.3) indicates that the rate at which
precision is increased by each additional observation is smaller the larger is . In the case
of perfect memory, the long-run degree of uncertainty about µ is nonetheless zero (there
is simply slower convergence to that long-run value when  is large); but with moderately
imperfect memory, the effective amount of experience that can ever be drawn upon remains
bounded, so that the uncertainty about µ remains larger forever when  is larger. When
memory is even more imperfect, not much more than one observation (the most recent one)
can be used in any event, so that the value of  is in this case less sensitive to the value of
.
    In the long run, the dynamics of the cognitive state s    ¯t and the memory state m   ¯ t+1 are
described by linear equations with constant coefficients. The lower-left panel of Figure 2
shows the long-run value for the Kalman gain 1t in (2.1). With imperfect memory, this is
always a quantity between 0 and 1, meaning that a higher value of the current state yt raises
the DM's estimate of the value of µ, though by less than the amount of the increase in yt .
For a given value of , the Kalman gain is larger the tighter the constraint on memory; in the
limit as  ¯  1 (perfect memory), the long-run value of this coefficient approaches zero (as
the true value of µ is eventually learned), while in the limit as  ¯  0 (no memory), the value
approaches a maximum value that is still less than one (because of the DM's finite-variance
prior).
    Finally, the lower-right panel reports the long-run value of m , a measure of the intrinsic
persistence of the memory state. The impulse response function for the effect of a memory-
noise innovation   ~ t on the subsequent path of the univariate memory state m  ~  is proportional
determine the vector vt , since the vector is normalized so that v Xv = 1. The value of  (given by the
constraint ¯ ), the matrix X (determined by the value of  ), and the vector v then completely determine
the long-run stationary elements of the matrix ¯ (using (2.18)) and hence also of the matrix ¯ (using (2.9));
thus the dynamics of the memory state given by (2.7) are completely specified.


                                                     20
to (m ) -t for all   t;29 thus the value of m indicates the rate of exponential decay of
the memory state back to its long-run average value. A smaller value of m means that
the contents of memory decay more rapidly; for any value of , the figure shows that m
is smaller, the tighter the memory constraint. At the same time, while a larger value of
m implies that memory persists for a longer time, it also implies that when memory noise
creates an erroneous impression of prior experience, this bias in what is recalled about is also
corrected more slowly; thus the value of m is an important determinant of the predicted
persistence of forecast bias.

3.2      The case of a linear cost of information
Analysis of the model is more complex when instead the amount of information stored in
memory each period can be increased at some finite cost. As an illustration we consider the
polar opposite case in which c    ~(I ) is a linear function of I , so that the marginal cost of a
further increase in the mutual information is independent of how large it already is. Thus we
assume that c~(I ) = ~ · I, for some coefficient   ~ > 0 which parameterizes the cost of memory.
    Even in this case, the optimal choice for the direction vector vt each period will be given
by the solution to the problem (3.1), except with the quantity        ¯ replaced by whatever value
                                                                                                2
of t is chosen in period t. (This follows from the fact that the continuation value V (^       t +1 )
                                      2
depends only on the value for       ^t+1 implied by the memory structure chosen for period t,
                               ~
while the information cost It depends only on the choice of t .) Thus an optimal memory
structure will involve v~t = v~ (t ; t ) each period.
    However, the optimal choice of t in any period will depend on the value of reducing
                                                                                  2
uncertainty in the following period. We note that the value function V (^       t  +1 ) appearing in
                                                      ~
the Bellman equation (2.22) can be written as y ·V (t+1 ), where t+1 is the scaled uncertainty
measure and the function V    ~ ( ) is independent of the scale factor y (for given values of the
parameters K, ,  and       ~). We can then write the Bellman equation in the scale-invariant
form
                                                ~
                                                
                   ~ (t ) = min
                   V                      t - log(1 - t ) +  V     ~ ((t ; t )) .              (3.4)
                               0t 1             2
The optimal choice of t in any period will be the value that solves the problem on the
right-hand side of (3.4). This problem has a solution t =  (t ) which depends only on
the value of t , the degree of uncertainty in period t determined by the memory structures
chosen for periods prior to t.
    Thus we can solve for the optimal policy function  (t ) once we know the value function
~ (t+1 ), and we can solve numerically for the value function by iterating the Bellman equa-
V
tion (3.4), as discussed further in the appendix.30 Figure 3 provides examples of numerical
solutions for the policy function in the case of a range of different values of ,~ where we
maintain the parameter values K = 1,  = 0 as in Figure 1. When      ~ = 0 (no cost of memory
precision), it is optimal to choose t = 1 (perfect memory) in all cases. But for any value of
  29
     Here we refer to the difference that the realization of ~ t makes for the forecasts of m
                                                                                            ~  at different horizons
  t, by an observer who knows the true value of µ and the DM's cognitive state at time t - 1, in addition
to observing the realization of   ~ t . See Appendix G.1 for details of the calculation.
  30
     See Appendix F.2 for details.


                                                        21
The case of a linear cost of information

                                     Figure 3: The optimal policy function  ( )

                                                               *(   )
                  1.0

                  0.8

                  0.6

                  0.4           = 0.00
                                = 0.10
                                = 0.15
                  0.2           = 0.20
                                = 0.22
                                = 0.24
                  0.0
                    0.0              0.1             0.2                0.3           0.4             0.5

         Figure 3: The optimal policy function  ( ), in the case of progressively larger values for
                                        ~ under the assumption that K = 1,  = 0.
         the information cost parameter ,
                        Figure 4: The dynamics of scaled uncertainty and memory precision

         , the optimal  ( ) < 1 when(      ~         1.0 in this case, perfect memory becomes
                                            ) > 0 (since                                                 1.0
                                                                                             ( ) infinitely
         costly); furthermore it is lower    = 0.8
                                         =(memory     is more imperfect) the higher is ~. We       0.2 that
                                                                                              ), =see
                                                                                           ( also
         for any cost parameter   ~ > 0, the optimal  ( ) is a decreasing function of . This indicates
                                                         

         that the less accurate the information contained in the cognitive state st (as indicated by
         the higher value of t ), the less information about the cognitivet > state
                                                                              0 that it will tbe < optimal
                                                                                                   0
         to store in memory, when    t=1        t=0 cost can be reduced by storing a less informative
                                       the memory
         record.31                                   0.8                                                 0.8
                                          
                         t>0                t t< 0
             The policy function   t =  (      ) together with the law of motion
                                                                                                             t=1
                                                      t+1 = (t ; t )                                               (3.5)

         derived earlier can then be solved for the dynamics of the scaled uncertainty {t } for all
         t  0, starting from the initial condition0.60 = K/(K + 1). The dynamics impliedt=0         0.6
                                                                                             by these
         0.0     0.1 can0.2
         equations             0.3 in
                         be graphed  0.4      0.5 diagram, 0.0
                                         a phase                     0.1 0.2
                                                             as illustrated in Figure    In the 0.5
                                                                                  0.3 4. 0.4     phase
         diagrams shown in each of the two panels, the value of t is indicated on the horizontal
         axis and the value of t on the vertical axis. Equation (3.5), which holds regardless of the
         nature of the information cost function and the degree to which the future is discounted,
         determines a locus  (), indicating for each value of  the unique value of  that will be a
         fixed point of these dynamics if t is held at the value . We can further show that whenever
         t <  (t ), the law of motion (3.5) implies that t+1 > t , so that uncertainty will increase,
         while if t >  (t ), it implies instead that t+1 < t , so that uncertainty will decrease.
           31
              For larger values of ~ than those shown in the figure, the optimal  ( ) falls to zero for large (but still
                                                               3
         finite) values of  ; see Appendix F.2 for numerical examples.


                                                               22
                                 = 0.24
                   0.0
                     0.0             0.1              0.2             0.3           0.4         0.5


                         Figure 4: The dynamics of scaled uncertainty and memory precision

                                                 ()        1.0                                   ()           1.0
                                                = = 0.8                                         ( ), = 0.2


                                                                                  t>0              t<0
                                          t=1       t=0
                                                           0.8                                                0.8
                            t>0                   t<0
                                                                                                      t=1


                                                           0.6                                        t=0     0.6
            0.0    0.1     0.2     0.3      0.4      0.5              0.0   0.1     0.2   0.3   0.4     0.5

Figure 4: The dynamics of scaled uncertainty t and memory precision t graphed in the
phase plane. The left panel gives an alternative graphical presentation of the dynamics
already plotted in Figure 2 for the case of a fixed upper bound ¯ on memory precision.
The right panel shows the corresponding dynamics in the case of a linear cost of precision
parameterized by .~


    The choice of t (and hence the degree to 3   which uncertainty will increase or decrease) is
given by the policy function, that depends on the specification of information costs. When
there is a fixed upper bound on information (the case discussed in the previous subsection),
the policy function is just a horizontal line at the vertical height   ¯ , as shown in the left
                      32
panel of the figure. In this case, the values of (t , t ) in successive periods start at the
point (0 ,  ¯ ), labeled "t = 0" in the figure, and then move left along the graph of the
policy function (since 0 >  (     ¯ ) as shown). They continue to move left along the policy
function, with t converging asymptotically to  (       ¯ ) from above; the stationary long-run
values ( ,  ) correspond to the point at which the policy function  =           ¯ intersects the
locus of fixed points  ().
    The right-hand panel of the figure shows the corresponding phase-plane dynamics in the
less trivial case of a linear cost function for information. In this case, the policy function
is instead a downward-sloping curve, as shown in Figure 3.33 Again the values of (t , t )
in successive periods must always lie on the graph of the policy function; the direction of
motion up or down this curve depends on whether the current position lies to the left or
right of the locus of fixed points  (). The initial point (labeled "t = 0") is determined
  32
      The figure plots the location of this line for the case ¯ = 0.8. The figure is drawn for parameter values
K = 1,  = 0. Thus the dynamics of uncertainty shown in the figure correspond to the curve labeled       ¯ = 0.8
in Figure 1.
   33
      In the figure, the policy function and the implied dynamics are shown for the case in which      ~ = 0.2,
corresponding to one of the intermediate curves shown in Figure 3. Again the figure is for the case K =
1,  = 0, so that the location of the locus of fixed points  () and the law of motion (3.5) remain the same
as in the left panel.


                                                                 23
                                 Figure 5: The evolution of scaled uncertainty about µ


                 0.5                   t                          0.5               ()
                                                   = 0.24
                 0.4                                              0.4
                                                   = 0.22
                 0.3                               = 0.20         0.3

                 0.2                               = 0.15         0.2
                                                   = 0.10
                 0.1                                              0.1
                                                   = 0.00

                 0.0                                              0.0
                       0     5       10       15        20              0   0.25   0.5    0.75       1
                                    Time
     Figure 5: The evolution of scaled uncertainty about µ as the number t of previous (im-
     perfectly remembered) observations grows, now for the case of a linear cost of memory
     complexity. The right panel shows the long-run value of scaled uncertainty for each value
     of                           ~, plotted as a point on the same locus of optimal long-run memory
Figure 6:the cost parameter
          Coefficients describing the optimal memory structure in the long run, as a function of the degree of persistence
     structures as in Figure 1.

                                                   0.2 coordinate given by the initial condition
     as the point on the policy curve with horizontal                                                0.0 0.
     Since this point lies to the right of the locus of fixed points, the points for successive periods
     move up and to the left on the policy curve, meaning that t rises as t falls.
         The scaled uncertainty continues to fall, 0.1 andv the precision   of memory continues to
                                                                       = 0.08                        -0.05
                                                                                                        rise,
     until the values (t , t ) converge to stationary long-run values ( ,  ), again corresponding
                                                                       = 0.05
     to the point at which the policy function  ( ) intersects the locus of fixed points  ().
                                                                       = 0.01
                                                   0.0
     Note that convergence is slower in the right panel of the figure than in the left, because in
                                                                                                     -0.1
     the early      0.2 when
             0.0periods,    0.4uncertainty
                                    0.6 0.8  is high, a less precise 0.2 0.4
                                                             0.0 memory                       0.8linear-cost
                                                                                      0.6 in the
                                                                               is chosen
     case, resulting in slower learning from experience.
         Different values of   ~ correspond to different locations for the policy function  ( ), as
     shown in Figure 3, and hence to different dynamics in the phase plane, converging to different
     long-run levels of scaled uncertainty. The 0.1
                                                                                                     1.000
                                                   dynamics of scaled uncertainty as a function of the
     number of observations t are shown for progressively larger values of         ~ in Figure 5, using the
     same1format as in Figure 1. Once again, we see that while uncertainty about µ eventually        0.995
     falls to zero as a result of when there is no 0.05
                                                     cost of memory complexity, as long as the cost is
     positive, the value of t remains bounded away from zero, and converges asymptotically to
     a value  that is higher the higher the cost of memory complexity.
                                                   0.0 of uncertainty is a particular long-run memory0.990
             0.0 0.2 0.4 0.6 0.8
         Associated with such an asymptotic degree
                                                             0.0 0.2 0.4 0.6 0.8
     structure ( , v ), which will imply a particular long-run value for the Kalman gain 1 . The
     way in which the long-run values of these different quantities vary with different assumptions
     about the values of  and      ~ is illustrated in Figure 6. (We use the same convention as in
     Figure 2 to indicate the direction of the vector v in the upper-right panel of the figure.)
     As we vary  for a given value of ,     ~ the associated value of  changes; hence the fixed-            ~
                                                             4
                                                             24
 Figure 6: Coefficients describing the optimal memory structure in the long run, as a function of the degree of persistence



                                                         0.2                                                  0.0

                                                         0.1        v                                         -0.05
                                                                               = 0.08
                                                                               = 0.05
                                                                               = 0.01
                                                         0.0                                                  -0.1
                 0.0   0.2      0.4     0.6      0.8                    0.0   0.2       0.4   0.6      0.8

                                                                                                              1.000
                                                         0.1
             1                                                                                                0.995
                                                         0.05

                                                         0.0                                                  0.990
                 0.0   0.2      0.4     0.6      0.8                    0.0   0.2       0.4   0.6      0.8

Figure 6: Coefficients describing the optimal memory structure in the long run, as a function
of the degree of persistence  of the external state, in the case of a linear memory cost
                                             4
                                    ~. Respective
function, for alternative values of               panels show the long-run values for  , the
direction vector v , the Kalman gain 1 , and the memory precision coefficient .


curves shown in Figure 6 do not correspond exactly to any of the fixed- curves plotted in
Figure 2, even though each of the long-run memory structures associated with a pair (,         ~)
is identical to the long-run memory structure associated with some pair (,        ¯ ). As shown
in the lower-right panel of the figure, the optimal  rises as  increases, for any value of
the cost parameter    ~ > 0; the more persistent the external state that must be forecasted,
the more it becomes worthwhile to pay a larger information cost in order to retain a more
precise memory of prior experience.
    Not surprisingly, we observe that for any value of , increasing the memory cost     ~ makes
it optimal for the long-run precision of memory  to be smaller, and consequently for the
long-run degree of uncertainty about µ to be larger. In the case of a sufficiently high value
of ,~ it will be optimal for memory to be completely uninformative. In fact, this happens
for a finite value of ,~ and it occurs abruptly, rather than through a gradual increase in
the long-run degree of uncertainty  toward the limiting value of 0 = K/(K + 1) as            ~ is
increased. A graph of the relationship between  and the value of         ~ is shown in Figure 7,
for the case  = 0, and two different possible values of K : K = 1 (as in Figures 3 and 4) and
K = 10. For each value of    ~, the value of  associated with the optimal memory structure
is shown by a large blue dot.
    In each panel of this figure, the continuous black curve is the correspondence consisting of
            ~  ) such that  is a stationary solution of the Euler equation associated with
all points (,
the optimization problem on the right-hand side of (3.4).34 The Euler equation represents a
first-order condition for the optimal choice of the degree of precision of memory; satisfaction
of this condition is necessary but not sufficient for memory precision leading to t+1 =  to
  34
       See Appendix F.3 for derivation of this equation.


                                                               25
                                               1.0                                      1.0

                                               0.8                                      0.8

                                               0.6                                      0.6

                                               0.4                                      0.4

                                               0.2                                      0.2

                                               0.0                                     0.0
             0.0     0.1     0.2     0.3     0.4          0.0   0.1   0.2    0.3     0.4

Figure 7: Long-run value of the scaled uncertainty measure  (blue dots) as a function of
                   ~ in the case of a linear memory cost function. Left panel: K = 1,  = 0.
the cost parameter ,
Right panel: K = 10,  = 0.


be optimal starting from a situation in which t =  . Because the objective function on the
right-hand side of (3.4) is not a convex function, it can have multiple local minima (as well as
a local maximum located between two local minima). Which of the local minima represents
the global minimum (and hence the optimal memory structure) can jump abruptly as a
result of a small change in parameters;35 this is what happens when the value of  changes
abruptly in the right panel of Figure 7, for a value of    ~ slightly above 0.28.
     In the K = 10 case, we see that there need not be a unique value of  for a given value
    ~
of  that represents a stationary solution to the Euler equation. For any value of         ~ greater
than a critical value around 0.15, if one starts from t = 0 (a completely uninformative
memory), the choice of t+1 = 0 again represents a local minimum of the objective; hence
 = 0 is a stationary solution of the Euler equation for all of these values of ,      ~ as shown
in the figure. However, for values of     ~ only moderately larger than the critical value (such
    ~
as  = 0.20), this is not the only local minimum, and the global minimum is instead at an
interior choice for t ; this value results in a path {t } that converges to a different stationary
value for  , on the lower branch of the correspondence (as shown for example by the blue
dot for  ~ = 0.20). Yet for values of  ~ that exceed a second critical value just above 0.28, the
global minimum shifts from the interior minimum to the local minimum at t+1 = 0 . For
all values beyond this point, the optimal memory structure involves t = 0 for all t, so that
 = 0 (as shown by the blue dots on the upper branch of the correspondence).
     Thus while the locus of fixed points  () is the same in Figures 1 and 5, all points on
this locus represent possible long-run memory structures (attainable through an appropriate
choice of  ¯ ) in the case of a fixed upper bound on mutual information, but not all of them
are always attainable in the case of a linear memory cost function. In the case K = 1, the
  35
       See Appendix F.2 for a numerical example.


                                                     26
two sets of long-run solutions are identical; but in the case K = 10, there is a range of values
for  that are associated with particular (relatively low) values of   ¯ but do not correspond
to any possible value of ~.36


3.3     Stationary fluctuations in the long run
Because our model implies that a DM does not learn the true value of µ with certainty even
in the long run, despite an arbitrarily long sequence of observations of the external state, over
which time the coefficients of the data-generating process (1.1) are assumed not to change,
it follows that the DM's forecasts can be quite different from rational-expectations forecasts
-- that is, the forecasts of an ideal statistician who knows the true coefficient values. From
the standpoint of an observer who is able to determine the true process, the forecasts of the
DM with limited memory will appear to be systematically biased. The biases in the DM's
forecasts will furthermore fluctuate over time, in response both to variations in the external
state (to which the DM reacts differently than someone with rational expectations would)
and to noise in the evolution of the memory state.
    We obtain a particularly simple characterization of the systematic pattern of forecast
biases if we consider the long run -- the predictions of the equations in the previous two
sections in the case of very large values of t, so that t has converged to the constant value
 , t has converged to  , and so on. In this case, our model, like the model of "natural
expectations" of Fuster et al. (2010, 2011), predicts a stationary pattern of forecast biases
that do not reflect incomplete adjustment to a new environment.
    In the long run, equations (1.1), (2.1), and (2.7) become a system of linear equations with
constant coefficients and Gaussian innovation terms, describing the evolution of the DM's
cognitive state. This system of equations can be reduced to a VAR(1) system

                          s
                          ~t+1 = f µ + F s
                                         ~t + ut+1 ,             ut+1  N (0, u )                           (3.6)

where
                                           m
                                           ~t                         ~ t+1
                                ~t 
                                s                ,      ut+1                   ,
                                           yt                          y,t+1

and f, F and u are a 2-vector and two 2 × 2 matrices of constant coefficients respectively.
In this vector system, the first equation is obtained by substituting (2.1) into (2.19), while
the second equation is given by (1.1).
    The matrix F furthermore has an upper-triangular form, while u is diagonal. We show
in the appendix that the eigenvalues of the matrix F are  and m .37 In our numerical
solutions, 0 < m < 1, so that both yt and m   ~ t exhibit stationary fluctuations around well-
defined long-run average values which depend linearly on µ. The two independent exogenous
sources of variation in this system are the innovations y,t+1 in the external state and the
memory noise innovations    ~ t+1 .
  36
     We can show analytically that the continuous relationship shown in the left panel of Figure 7 occurs for all
K  1 when  = 0, while the backward-bending correspondence and consequent discontinuous relationship
between  ~ and  occurs for all K > 1. See Appendix F.3 for further explanation.
  37
     See Appendix G.1 for the derivation.



                                                       27
4. Predicted Patterns of Forecast Bias
4.1 Stationary fluctuations in the long run


                              Figure 8: Impulse response of the DM's estimate of µ


                  0.5                                 E[ |mt, yt]
                                                                                            = 0.30
                                                                                            = 0.60
                  0.4                                                                       = 0.80
                                                                                            = 0.90
                                                                                            = 0.95
                                                                                            = 0.99
                  0.3                                                                       = 1.00

                  0.2

                  0.1

                  0.0
                                      0              1              2               3                4
                                                          Time
         Figure 8: Impulse response of the DM's estimate of µ to a unit positive innovation in the
         observed  value of yt at
           Figure 9: Impulse
                                  the time marked as "time = 0" on the horizontal axis, for alternative
                               response of the DM's one-quarter-ahead forecast of the external state
         values of the information bound    ¯ , in the case that K = 1,  = 0.

                  0.7                               E[yt + 1|mt, yt]
             The DM's optimal estimate of µ at each point in time, µ                = 0.30
                                                                   ^t , as well as her optimal forecast
                                                                                    = 0.60
                 0.6
         of the external state at any horizon  > t,                                 = 0.80
                                                                                            = 0.90
                  0.5                          ~ t , yt ] = (1 -  -t )^
                                   ^ |t = E[y |m
                                   y                                  µt +  -t yt ,         = 0.95          (3.7)
                                                                                            = 0.99
                  0.4                                                                       = 1.00
         will then be linear functions of the elements of s     ~t , with coefficients that are also time-
                  0.3
         invariant.  We   thus  obtain  a stationary  multivariate  Gaussian distribution for any number
         of leads and lags of the external state, the DM's memory state, and the DM's estimates
                  0.2
         and forecasts.   This allows us to analyze not only the extent to which the DM's forecasts
         should differ from rational-expectations forecasts, but the correlation that one should observe
                  0.1
         between the bias in the DM's forecasts and other observable variables.
                  0.0
             In particular,  the biases in the DM's forecasts will be correlated with the evolution of the
         external state. An unexpectedly high observed value for yt will be interpreted (because of
                                     0              1             2              3              4
         the DM's uncertainty about µ) as implying Time a higher optimal estimate of µ, and this increase
         in the DM's estimate of µ will furthermore persist, decaying only gradually in subsequent
         periods. This is illustrated in Figure 8, which shows the impulse response function for µ    ^ to a
         unit positive innovation in the value of yt . The6 response  is plotted for a variety of alternative
         values for the information bound ,    ¯ in the case that K = 1 and  = 0 as in Figure 1.38
             In the case that  ¯ = 1 (perfect memory), the value of µ is learned with perfect precision,
         and as a consequence there is no effect (in the long run, depicted here) of fluctuations in
         yt on the DM's estimate of µ. (The Kalman gain 1 has a long-run value of zero in this
         case.) Instead, for values of    ¯ < 1, a higher observed value of yt leads the DM to increase
         her estimate µ ^t (the Kalman gain is positive). The estimate µ    ^ remains higher (on average)
           38
             See Appendix G.1 for illustration of how this figure would change under alternative assumptions about
         the degree of persistence of the fluctuations in the external state.


                                                            28
                             0              1               2              3                4
                                                 Time

  Figure 9: Impulse response of the DM's one-quarter-ahead forecast of the external state


        0.7                                E[yt + 1|mt, yt]
                                                                                   = 0.30
                                                                                   = 0.60
        0.6                                                                        = 0.80
                                                                                   = 0.90
        0.5                                                                        = 0.95
                                                                                   = 0.99
        0.4                                                                        = 1.00

        0.3
        0.2
        0.1
        0.0
                             0              1               2              3                4
                                                 Time
Figure 9: Impulse response of the DM's one-quarter-ahead forecast of the external state to
a unit positive innovation in the observed value of yt , again for alternative values of the
information bound  ¯ , in the case that K = 16
                                             ,  = 0.4.


in subsequent periods as well. The memory state m        ~ t+1 carried into the period following the
innovation is a noisy record of µ   ^t , and hence is higher because of the increase in yt ; this
increases the average value of the estimate µ     ^t+1 , which increases the average value of the
memory state m   ~ t+2 , and so on. The tighter the memory constraint (the lower the value of
¯ ), the greater the effect of the innovation in yt on µ
                                                            ^t , because the DM is more uncertain
about the value of µ before observing yt ; however, the effect on the DM's estimate of µ is
also more transient the lower the value of ,    ¯ because less information is retained from one
period to the next about past cognitive states.
     These effects on the DM's optimal estimate of µ then feed into her optimal forecast of the
external state at any future horizon  , because of (3.7). As an illustration, Figure 9 shows
the impulse response of the one-quarter-ahead forecast y         ^ +1| to a unit positive innovation
in yt , using the same conventions as in Figure 8; in this figure, however, we assume that the
external state is serially correlated, with  = 0.4.39 When  > 0, the rational-expectations
forecast (corresponding to     ¯ = 1 in the figure) is itself increased by a positive innovation
in yt (by an amount equal to fraction  of the innovation), and the increase in the forecast
is furthermore persistent (decaying back to its original level at a rate proportional to  -t ).
But when     ¯ < 1, the forecast is increased by even more, owing to the fact that the higher
observation of yt increases the DM's estimate of µ as well. This additional effect on the
forecast is initially larger the smaller is ¯ ; but a smaller    ¯ (tighter memory constraint) also
causes the additional effect to die out more rapidly, since its propagation can only be through
the DM's memory of her previous judgment about the value of µ.
  39
    In the case that  = 0, the impulse response of y
                                                   ^ +1| will be identical to the impulse response of µ
                                                                                                      ^ ,
already shown in Figure 8. The corresponding impulse responses for additional values of  are shown in
Appendix G.1.



                                                   29
    Thus our model predicts that forecasts of the future value of a variable will over-react to
news about the current value of that variable (assuming, as is often the case with economic
time series, that the variable in question exhibits positive serial correlation). Positive serial
correlation means that a higher current observation should increase somewhat one's forecast
of the variable's future value, even under rational expectations; but imperfect memory results
in a larger increase in the forecast than is consistent with rational expectations. The model
also predicts that biases of this kind will persist for some time. Once a situation occurs
that leads the DM to over-estimate the future level of some time series, the DM will as
a consequence continue (on average) to over-estimate the future level of that variable for
several more quarters.

3.4     "Recency bias" in expectation formation
One type of systematic difference between observed expectations and those of a perfect
Bayesian decision maker that has often been reported is "recency bias" (e.g., Malmendier et
al., 2017) -- a tendency for expectations to be influenced more by more recent observations,
even when in principle, observations of a given time series at earlier dates should be equally
relevant as a basis for inference. Our model predicts that such a bias should exist, as a
consequence of optimal adaptation to limited memory precision (or to the cost of maintaining
a more precise memory). Observations of the external state farther in the past are recalled
with more noise, and as a consequence are given less weight in estimating parameters of the
data generating process than would be optimal in the case of a perfect memory of past data.
     The system (3.6) implies that, in the case that data have been generated in accordance
with this law of motion for a sufficiently long time, we can express the value of the memory
state m~ t+1 as a function of the sequence of external states {y } for   t and the sequence
of memory noise realizations {   ~  +1 } for   t:
                                             
                             ~ t+1 = F12 ·
                             m                      (m )j yt-j +  sum
                                                                 ~t+1 ,                     (3.8)
                                             j =0


where
                                     F12   (1 v1 + v2 )
is the (1, 2) element of the matrix F in (3.6), and
                                               
                                    sum
                                   ~t+1              (m )j ~ t+1-j                          (3.9)
                                              j =0


is a serially correlated Gaussian noise term. Thus one way of describing the optimal memory
structure in the long run (that is, once t has converged to  ) is to say that m
                                                                              ~ t+1 should be a
noisy record of an exponentially-weighted moving average of past observations of the external
state, where the added noise term is a serially correlated Gaussian random variable that
evolves independently of the external state. The moving average puts a weight proportional
to (m )j on observation yt-j ; thus progressively smaller weights are placed on observations
as they recede farther into the past.


                                                30
    Equation (2.1) implies that a DM's estimate of the unknown mean µ of the external state
is given by a linear relation of the form

                                            µ
                                            ^t =  m
                                                  ~ t + 1 yt ,                                          (3.10)

where the coefficient  > 0 is defined in the appendix. Using (3.8) to substitute for the
memory state in this expression, we see that we can write the estimate in the form
                                                
                                                                 sum
                                       µ
                                       ^t =           j yt-j +  ~t   ,                                  (3.11)
                                               j =0

where the weights {j } are all positive, and the weights for j  1 decrease exponentially:
j = 1 (m )j -1 .
    The forecasts specified by (3.7) using this value for µ ^t are similar to those implied by a
model of least-squares learning (Evans and Honkapohja, 2001) in which the DM is assumed
to know that the variable's law of motion is of the form (1.1); the value of the coefficient  is
assumed to be known while µ must be estimated; and the unknown coefficient is estimated
using a "constant-gain" estimator. (The differences between (3.11) and a standard constant-
gain estimate of the mean of a series are the fact that the coefficient 0 is differently specified,
and the presence of the Gaussian error term.) The biases in forecasts predicted by our model
will therefore have important similarities to those of a model of constant-gain learning, of the
kind included in estimated macroeconomic models by authors such as Milani (2007, 2014)
and Slobodyan and Wouters (2012).
    We provide, however, a justification for the declining weight on observations farther
in the past, as a consequence of optimal forecasting based on an imperfect memory, and
furthermore endogenize the nature of that memory.40 Notably, our justification applies even
in the case of an external state process with parameters that do not drift over time (as indeed
we assume in the analysis here). Nor does it depend on the replacement over time of an
earlier population by new individuals with different experiences, as suggested by Malmendier
and Nagel (2016); our model justifies declining weights on earlier observations at the level of
an individual forecaster. Our model also implies that the value of the "gain parameter" m
should depend on the degree of persistence of the series being forecasted, rather than being
a structural feature of the estimation method used by the forecaster.


4      Predictable Forecast Errors
An important consequence of optimal Bayesian inference with perfect memory (as assumed
under the hypothesis of rational expectations) is that the error in a forecast should not itself
be forecastable on the basis of any information available to the forecaster, at or before the
time of the forecast in question. Thus if we let y ^t+h|t denote a DM's forecast at time t of
the value of the external state at time t + h, the forecast error41 F Et  yt+h - y^t+h|t should
  40
     The fact that our model predicts decreasing weights on observations made farther in the past is a notable
difference between our model and the one proposed by Afrouzi et al. (2020).
  41
     Note that we give this variable a time subscript t to indicate that it is the error in the forecast made at
time t; the value of the random variable F Et is not revealed however until date t + h.


                                                       31
be uncorrelated with any variable zt the value of which is known to the DM at time t (or
earlier), either because it has been publicly observable or because it is part of the DM's own
cognitive state. Many econometric investigations of the consistency of observed forecasts with
the hypothesis of rational expectations have accordingly been based on regressions of F Et on
other variables that ought to be known to the forecaster, testing the null hypothesis that all
such regression coefficients should equal zero. Here we discuss the extent to which our model
can account for some widely discussed examples of evidence against this null hypothesis;
we particularly discuss evidence indicating over-reaction of subjective expectations to news
about the series that is to be forecasted.

4.1     Evidence of over-reaction: the response of forecasts to
        fluctuations in the state
As noted in the introduction, Afrouzi et al. (2020) conduct a laboratory experiment in which
forecasts of a stationary AR(1) process are elicited from subjects. They find that subjects'
expectations over-react to innovations in this process, as predicted by our model (as well
as the related model of noisy memory that they discuss). They give particular emphasis
to a measure of over-reaction in which a subject's forecast y      ^t+h|t (where h is the number
of realizations in advance for which the forecast is solicited in trial t) is regressed on the
realization of the variable just before the forecast is solicited:
                                              subj
                                     y
                                     ^t+h|t = h    + subj
                                                     h yt + vt .                                          (4.1)

A separate regression (with coefficients h , subj
                                             h ) can be estimated for each of several hori-
zons h. Afrouzi et al. are interested in the difference between the "subjective degree of
persistence" measured by the estimated coefficient subj
                                                   h    and the corresponding coefficient h
in a regression using actual outcomes:

                                       yt+h = h + h yt + ut+h .                                           (4.2)

The authors measure the degree of over-reaction of expectations to news by the extent to
which subj
        h   is larger than h .42 Note that this is an example of a test of the predictability of
forecast errors, since the coefficient of a regression of F Et on yt will equal h - subj
                                                                                      h .
    We can investigate what our model of expectation formation on the basis of an imperfect
memory implies about the relationship between subj   h  and h in the case of a stationary AR(1)
process. Here we consider the predicted values of the regression coefficients in the long run,
as the length of the time series used to estimate them goes to infinity. The law of motion
(1.1) implies that for any horizon h  1, the joint distribution of yt and yt+h (conditional on
the value of µ) will be bivariate Gaussian, with

                                   E[yt+h |µ, yt ] = (1 - h )µ + h yt .
  42
    The authors also discuss other measures of over-reaction, most notably the predictability of forecast errors
by previous forecast revisions, as discussed in the next subsection, but argue that the difference between
subj
 h    and h is likely to be more robustly estimated, especially in the case of processes with relatively low
persistence (where they argue that over-reaction is greatest).


                                                      32
Hence with a sufficiently long series of observations, the coefficients in a regression of the
form (4.2) should approach the asymptotic values

                                   h = (1 - h )µ,            h =  h .

(Here we assume that the regression uses an arbitrarily long sequence of realizations of a
process for which there is a single, unchanging value of µ.)
   Equation (3.7) implies that subjective forecasts should be given by

                                     ^t+h|t = (1 - h )^
                                     y                µt +  h y t ,

so that the predicted coefficient subj
                                  h    in regression (4.1) will equal

                         subj
                         h    = (1 - h )µ
                                        ^ |y + 
                                                h
                                                  = (1 - h )µ
                                                            ^ |y +  h ,                               (4.3)

where µ
      ^|y is the coefficient in a regression of µ
                                                ^t on yt ,

                                             µt , yt |µ]
                                         cov[^                 µt , yt |µ]
                                                           cov[^
                               µ
                               ^ |y =                    =                 .
                                           var[yt |µ]           y 2


We show in the appendix how to calculate this coefficient as a function of the model param-
eters.43
    Importantly, our numerical solutions indicate that µ  ^t and yt are always positively corre-
lated (conditional on µ). This is because a positive innovation in the external state yt raises
(or at least never lowers) the expected value of y for all   t, and at the same time also
raises the expected value of µ^ for all   t (as illustrated in Figure 8 and similar figures in
the appendix). Since the memory noise has no effect on the evolution of the external state,
there are no shocks that move µ   ^t and yt in opposite directions, while some (at least the
innovation yt ) move both of them in the same direction. But given that µ     ^|y > 0, equation
                     subj
(4.3) implies that h > h ; that is, our model implies over-reaction of the kind exhibited
by the forecasts of the subjects of Afrouzi et al.
    Equation (4.3) also implies that for fixed values of the model parameters other than ,
the over-reaction measure subj
                            h   - h converges to zero as   1, for any forecast horizon h.44
This is also approximately true of the regression coefficients reported by Afrouzi et al. (see
their Figures 2B, 5A, and 5B). Indeed, these authors stress the finding that in their data, the
discrepancy subj
              h   - h is much larger when h is relatively small (either because  is small,
or because  is well below one and h is large). This is also true in numerical solutions of our
model as indicated in Figure 10.
    One of the more striking features of the regressions reported by Afrouzi et al. is that subj
                                                                                            h
is well approximated by an increasing function of h , with approximately the same functional
relationship regardless of whether the variation in h occurs as a result of variation in  or
  43
    See Appendix G.3 for details.
  44
    This prediction depends on µ  ^ |y remaining bounded as  approaches 1. This is the case in our numerical
solutions, both when  ¯ is held constant as  is varied (as in Figure 2) and when   ~ is held constant as  is
varied (as in Figure 6).



                                                    33
                  1.0

                  0.8

                  0.6
           subj
           h
                  0.4

                  0.2                                                               h=1
                                                                                    h=2
                                                                                    h=5
                  0.0
                    0.0         0.2           0.4           0.6           0.8           1.0
                                                       h

Figure 10: Comparison of the values for the regression coefficients h and subj
                                                                           h   for different
values of  and h. (The figure is shown for the case K = 1,      ¯ = 0.3.) The diagonal line
indicates the prediction of the rational-expectations hypothesis.


variation in h.45 The relationship subj () is furthermore an upward-sloping one, with a slope
much less than one, starting well above the diagonal for low values of  and approaching the
diagonal as   1. (See the plot of their regression coefficients in Figure 10.46 ) While our
model does not imply that a functional relationship of that kind should hold precisely, it is
worth noting that to the extent that the value of µ  ^|y remains approximately the same as one
                                          subj
varies , (4.3) implies that the value of h should be nearly the same for all pairs (, h) that
imply the same value of h . Perhaps more to the point, our model can be parameterized so
that it simultaneously fits the experimental evidence for each of the three different horizons
for which forecasts are solicited in the experiment of Afrouzi et al.
    Figure 10 plots the predicted value of subjh   against the value of h , for each of several
different horizons h, each represented by a distinct curve; the curves are shown for the case
in which K = 1 and    ¯ = 0.3. Along each curve, the variation in h is due purely to variation
in . (The fact that    ¯ is fixed despite variation in  means that we assume a fixed upper
bound on the mutual information, as in section 3.1, rather than a convex cost function.)
The horizons used are h = 1, 2 and 5, as these are the horizons for which Afrouzi et al.
elicit forecasts from their subjects; the regression coefficients that they estimate for various
  45
      This was shown in an earlier version of the paper now circulated as Afrouzi et al. (2020), though this
figure is omitted from their most recent draft.
   46
      The data plotted here are based on Figures 2B, 5A, and 5B of Afrouzi et al. (2020).



                                                    34
combinations of  and h are indicated by the circles in the figure (with colors indicating the
horizon h).
    The three curves are not exactly the same, since in our model µ    ^|y is a function of  (but
the same for all values of h), rather than being a function only of h . Nonetheless, for the
parameterization chosen here, µ     ^|y is nearly constant as  is varied; as a consequence, the
                                subj
relationship between h and h predicted by (4.3) is close to a linear one, and is nearly the
same for all values of h. Our model therefore provides quite a good account of the effects
of variation in either  or h on the value of subj  h , as indicated by the fact that none of the
circles in Figure 10 are far from the corresponding curve.

4.2    Evidence of over-reaction: forecast revisions and forecast
       error
A comparison of the coefficient subj h    with h provides a fairly straightforward test of over-
reaction of forecasts to news about the variable being forecasted; but the null hypothesis
that subj
       h   should equal h only follows from rational expectations in the case that one is sure
that forecasters at time t have observed the external state yt , and calculation of the predicted
value h requires knowledge of the true data-generating process. Both of these assumptions
make sense in the case of the laboratory experiment of Afrouzi et al. (2020); but they are
more debatable in the case of forecasts of economic time series outside laboratory settings.
    Bordalo et al. (2020a) use a different approach to document systematic departures from
rational expectations in surveys of professional forecasters. Following a proposal by Coibion
and Gorodnichenko (2015), they regress the error (that eventually becomes known) in a
given forecaster's forecast of a future data release on the revision that the forecast represents,
relative to the same forecaster's forecast of the same future variable at an earlier time. That
is, they test whether the coefficient b is different from zero in a regression specification of
the form
                                    F E t = a + b · F R t + ut ,                             (4.4)
where F Et is the error (as defined above) in the forecast at time t for some horizon h > 0,
and
                                  F Rt  y ^t+h|t - y^t+h|t-1
is the revision of this forecast between time t - 1 and time t.
    If the forecast y
                    ^t+h|t represents the correctly calculated expectation of yt+h conditional
on the forecaster's information set at time t, then the forecast error F Et should not be
forecastable on the basis of any information available to the forecaster at time t, as dis-
cussed above. Bordalo et al. point out that even if one is agnostic about which external
developments are observed by forecasters (or how accurately they observe them), as long as
one supposes that the forecaster's own past cognitive states are known with complete preci-
sion, then the size of the forecast revision F Rt should be part of the information set; hence
the coefficient b in (4.4) should equal zero under a hypothesis of correct Bayesian inference
from the forecaster's information set. A coefficient b = 0 allows one to reject not just the
full-information rational-expectations hypothesis, but also models that assume that people's



                                               35
choices are optimal Bayesian responses conditional upon a noisy cognitive state (but with
perfect memory), such as the models of Sims (2003) or Woodford (2003).47
    Bordalo et al. (2020a) find instead that b is significantly negative in the case of many
macroeconomic and financial time series. (Afrouzi et al., 2020, find that the same is true of
the forecasts elicited in their experiment.48 ) Bordalo et al. interpret this negative sign as
evidence of over-reaction of forecasts to economic news arriving between the dates of the two
successive forecasts: news that implies that one's previous forecast was too low results in an
upward revision that is too large, so that the occurrence of an upward revision is correlated
with the second forecast turning out to be too high.
    As discussed above, our model implies that there will be over-reaction to new realizations
of the external state, and indeed our model predicts that one can easily have a negative
coefficient b in a regression of the form (4.4). The theoretically predicted asymptotic value
for the coefficient b in the case of a long enough series of observations from an environment
with unchanging statistics (including a fixed value of µ) is given by

                                                 cov[F Et , F Rt |µ]
                                          b =                        ,
                                                    var[F Rt |µ]

where the forecast error and forecast revision variables are defined above. Since the de-
nominator is necessarily positive (in any case in which the forecast is not constant at all
times), the coefficient b should be negative if and only the covariance between F Et and F Rt
is negative.
     That this can easily be the case can be illustrated by considering the simple case of an
i.i.d. process ( = 0) for the external state. In this case, (3.7) implies that y
                                                                               ^t+h|t = µ
                                                                                        ^t , for
any forecast horizon h  1. In this case we have

                       var[F Rt |µ] = var[^
                                          µt - µ
                                               ^t-1 |µ] = 2(1 - µ      µt |µ],
                                                                ^ )var[^


where µ
      ^ is the coefficient of serial correlation of the stationary fluctuations in µ
                                                                                   ^t , and

              cov[F Et , F Rt |µ] = cov[yt+h - µ      ^t - µ
                                                 ^t , µ     ^t-1 |µ]
                                  = cov[-µ     ^t - µ
                                          ^t , µ      ^t-1 |µ] = -(1 - µ      µt |µ].
                                                                       ^ )var[^

  47
      Coibion and Gorodnichenko (2015) propose a regression specification of the form (4.4), but where the
forecast y^t+h|t used to define both F Et and F Rt is a consensus forecast (that is, the average of many
forecasters' forecasts) rather than an individual forecast. In this case, a coefficient b = 0 allows one to reject
the hypothesis of full-information rational-expectations forecasts on the part of all forecasters (since in that
case, the forecasters should have a common information set, which should include all information reflected
in the consensus forecast), but could still be consistent with Bayesian rationality under the hypothesis that
different forecasters have different information sets, as Coibion and Gorodnichenko discuss.
   48
      See Figure 2A in their paper. Indeed, the evidence for b < 0 as a general regularity is even stronger
in the laboratory data of Afrouzi et al. than in the field data of Bordalo et al. Afrouzi et al. find that
b < 0 for all of the different time series (with widely varying degrees of persistence) used in their experiment,
whereas this is less consistently true for professional forecasts studied by Bordalo et al.; and Afrouzi et al.
find that b < 0 both when individual forecasters' errors are regressed on their own forecast revisions, and
when consensus forecasts (of a group of experimental subjects who have been shown the same time series)
are regressed on revisions of the consensus forecast, whereas Bordalo et al. often do not find a negative sign
when consensus forecasts are used.


                                                       36
                                                         h



                                         Figure 11: CG regression



               0.0

               0.1

               0.2
          b
               0.3

               0.4

               0.5
                 0.0            0.2           0.4            0.6            0.8   1.0

Figure 11: Predicted value of the coefficient b from a regression of forecast errors on the size
of the revision of the forecast, in the case of external state processes of different degrees of
serial correlation . Model predictions are shown under the assumption that K = 1,       ¯ = 0.3.
                                                     7

(Here the first expression on the second line follows from the fact that yt+h is completely
uncorrelated with any variables observable at time t or earlier, conditional on the value of µ,
when  = 0.) Hence in this case we obtain the prediction b = -1/2, simply as a consequence
of the fact that our model implies stationary fluctuations in µ ^t around some long-run average
estimate, for any parameter values with  < 1.
    Our numerical solutions indicate that b is often negative in the case of positive serial
correlation in the external state as well, as illustrated in Figure 11.49 The figure shows the
predicted value of the coefficient b as the coefficient of serial correlation  varies between 0
and 1, in the case that the second forecast is a one-period-ahead forecast (h = 1, the case for
which Afrouzi et al. provide estimates of this coefficient based on their experimental data).
The figure is computed under the assumption that K = 1 and          ¯ = 0.3, as in Figure 10. We
see that the predicted degree of over-reaction (as measured by the degree to which b < 0) is
greatest when  = 0; the coefficient equals -0.5 when  = 0 (as explained in the previous
paragraph), but is less negative when  > 0, and near zero for large values of . This is also
what Afrouzi et al. find to be true of the forecasts elicited in their laboratory experiment.50
    A similar regularity is observed in the case of professional forecasts of the economic time
series considered by Bordalo et al. (2020a). The estimates that they obtain for b mainly
fall in or near the interval [-0.5, 0]. Moreover, in the case of the highly persistent series that
they consider, the value of b is around zero on average (sometimes slightly negative, but
sometimes slightly positive); in the case of the series that they consider with a coefficient of
serial correlation less than 0.1, the value of b is nearly as negative as -0.5; and for the series
that they consider with intermediate degrees of serial correlation, b is negative but much
 49
      Formulas that can be used to calculate b are given in Appendix G.3.
 50
      Again, see their Figure 2A.



                                                     37
less negative than -0.5.51 Our model is not only able to explain why negative values of b are
often obtained, but also why these are almost always between small positive values and -0.5,
and why the coefficient is more negative for the least persistent time series.

4.3     Idiosyncratic noise in individual forecasts
Another kind of evidence of systematic bias in the forecasts announced by individual fore-
casters is provided by Fuhrer (2018). Fuhrer shows that subsequent revisions of the forecasts
of an individual forecaster are partially forecastable on the basis of information available (at
least to the community of forecasters in general) at the time of the original forecast, a result
inconsistent with the hypothesis of full-information rational expectations (under which not
only should all forecasters be ideal Bayesian statisticians, but all should share a common
information set).
                                 i
     Suppose now that we let y  ^t+h|t be the forecast of yt+h at time t by forecaster i, while
  cons
^t+h|t is the "consensus forecast," the median of the forecasts at time t made by the different
y
forecasters in a given survey. Fuhrer reports regressions of the form
                    i         i                  i           cons           i
                  y
                  ^t +h|t - y
                            ^t +h|t-1 = a +  · (^
                                                yt+h|t-1 - y
                                                           ^t +h|t-1 ) + e Zt + ut ,                   (4.5)

where Zti is a vector of other forecaster-specific control variables (that vary across specifi-
cations). His main finding, obtained for forecasts of several different aggregate variables,
and robust both to different choices for the horizon h and the control variables included, is
that the coefficient  is found to be significantly negative (for example, between -0.5 and
-0.6 in the case of revisions of inflation forecasts by members of the Survey of Professional
Forecasters, where the forecasts are collected at a quarterly frequency). This indicates a
tendency of forecasters to subsequently revise their forecasts so as to partially eliminate the
previous gap between their forecast and the consensus forecast.
    Many models of biased expectation formation proposed in the previous literature predict
systematic departures from rational expectations, and hence allow forecast revisions to be
predictable by information that was publicly available at the earlier date; but they nonethe-
less do not predict that the variable considered by Fuhrer should predict subsequent forecast
revisions, insofar as they do not explain why the forecasts of different forecasters should
respond in different ways to the same publicly available information. Our model instead
requires that there should be idiosyncratic noise in individual forecasts; for it is only because
of the noise term in the law of motion for the memory state (2.19) that the DM is unable to
                                                                                            i
learn the value of µ with perfect precision, and there is no reason for the noise term t     +1 to
                                        52
be correlated across decision makers.
    It is clear that not only the forecasts of different households, but even the forecasts of
professional forecasters exhibit substantial dispersion at a given point in time. A defender
  51
    See Figure 1 of Afrouzi et al., who stress this feature of the results of Bordalo et al.
  52
    The existence of correlation would imply that the memory state m      ~it+1 conveys information about the
DM's environment in addition to the information conveyed about the DM's own prior cognitive state; there
would need to be some way in which m   ~it+1 is also able to convey information about the cognitive states of
other DMs. But even if that were possible, it would be contrary to the spirit of our imposition of a limit on
the informativeness of the memory state not to count the information conveyed about these other cognitive
states as increasing the information cost of such a memory state.


                                                     38
of the hypothesis of Bayesian rationality might argue that this simply reflects the fact that
different forecasters have access to different private sources of information. Yet experiments
in which forecasts are elicited from different subjects who are shown identical sequences of
observations indicate that dispersion of forecasts exists even the experimenter can be certain
that each subject was exposed to precisely the same information; see in particular Khaw
et al. (2017) and Afrouzi et al. (2020). This indicates noisy cognitive processing of the
information presented to the subjects; our model provides at least one possible example of
the nature of such idiosyncratic noise in the process by which individuals' expectations are
formed.
    Our model not only explains why there should exist non-trivial dispersion in the dis-
crepancy between individual forecasts and the consensus forecast, but why this discrepancy
should predict subsequent forecast revisions in the way that it does. In our model, all fore-
casters observe the same external states, but their memories of past states are subject to
idiosyncratic noise. In the case of a large enough sample of forecasters, the mean realizations
                              i
of the memory noise term     ~t+1 across forecasters i should be close to zero each period, so
that (3.8) implies that the mean memory state should be essentially a deterministic function
of the past external states,
                                                       
                                   ~ avg
                                   m t+1    = F12 ·          (m )j yt-j .
                                                      j =0

          ~ dif
If we let m     f,i
            t+1  m  ~i     ~ avg
                     t+1 - m t+1 be the difference between the memory state of DM i and the
average memory state, (3.8) implies that

                                           ~ dif
                                           m t+1
                                                 f,i
                                                     =  sum,i
                                                       ~t+1 .


                        ^dif
    If we similarly let µ t
                             f,i
                                 be the difference between DM i's estimate of µ and the average
estimate, it follows from (3.10) that this difference must entirely be due to the difference
between i's memory state and the average memory state, so that we can write

                                        ^dif
                                        µ t
                                             f,i
                                                   ~ dif
                                                 = m t
                                                         f,i
                                                             .

Finally, it follows from (3.7) that for any forecast horizon h, the difference between i's forecast
and the average forecast will be due entirely to the difference in i's estimate of µ, so that
                                  avg
                      ^t
                      y i
                         +h|t - y
                                ^t             h
                                                 µdif
                                   +h|t = (1 -  )^ t
                                                      f,i
                                                          = (1 - h )  sum,i
                                                                     ~t     .
                                                                                               sum,i
   In the large-sample limit, the population distribution of realizations of the variable     ~t+1
across forecasters i should be essentially be identical to the distribution of the random
           sum                                                                 i
variable ~t +1 . It follows that the distribution of individual forecasts {y  ^t+h|t } should be
                                                                                       avg
approximately a Gaussian distribution, with a median very close to its mean, y        ^t+h|t . Thus
                                                           avg
the consensus forecast should be essentially the same as y^t+h|t , and we obtain the prediction
                                                            sum,i
                            i
                            t  y
                               ^ti
                                  +h|t - y
                                         ^tcons         h
                                            +h|t = (1 -  ) ~t     .                           (4.6)

   In a regression of the form (4.5), but where, for purposes of our theoretical derivation,
we assume there are no control variables Zti included, the asymptotic value of the coefficient

                                                  39
                 0.5
                                                                               h=1
                                                                               h=2
                                                                               h=3
                 0.6

                 0.7

                 0.8

                 0.9

                 1.0
                       0.0            0.2            0.4           0.6   0.8

Figure 12: Predicted value of the coefficient  from a regression of individual forecasters'
forecast revisions on the discrepancy between their forecast and the consensus forecast, in the
case of external state processes of different degrees of serial correlation . Model predictions
are shown under the assumption that K = 1,       ¯ = 0.3, but for three different values of h.


 (with a long enough sample) should equal
                                                          i
                                                   cov[F Rt , it-1 ]
                                             =               i
                                                                     .
                                                      var[t-1 ]

The precise predicted value depends on both the degree of persistence of the variable being
forecasted and the length of the forecast horizon h.53 If we use parameter values K = 1,   ¯=
0.3, as in Figures 10 and 11, then the model predicts a regression coefficient on the order of
-0.8, as shown in Figure 12. And indeed, Fuhrer (2018) finds that the predictable forecast
revision over the next quarter should be more than half of the discrepancy between i's forecast
and the consensus forecast (though less than a complete correction of the discrepancy), for
the most of the variables and alternative regression specifications that he considers.


5       Conclusion
We have shown that it is possible to characterize the optimal structure of memory, for a
class of linear-quadratic-Gaussian forecasting problems, when the cost of a more precise
memory is proptional to Shannon's mutual information, and when we assume that the joint
distribution of past cognitive states and the memory state is of a multivariate Gaussian form,
but with no a priori restriction on the dimension of the memory state or the dimensions of
past experience that may be more or less precisely recalled. Strikingly, we find that for
the class of problems that we consider, the optimal memory structure is necessarily at most
 53
      See Appendix G.4 for an explicit solution for this coefficient.


                                                        40
one-dimensional. This means that what can be recalled at any time about past observations
is simply a noisy recollection of a single summary statistic for past experience. We show how
the model parameters determine the law of motion for that summary statistic, and hence
what single dimension of past experience will be (imprecisely) available as an input to the
DM's forecasts.
     Among the implications of our model, two seem of particularly general interest. First,
while our formalism allows for the possibility of an independent noisy record of each past
observation (as assumed for example in the model of Neligh, 2019), this is not optimal;
instead, the optimal memory structure is one in which only a particular weighted average of
past observations can be recalled with noise. And second, this weighted average places much
larger weights on recent observations than on ones at earlier dates, even though observations
at all dates are equally relevant to inference about the value of the parameter µ, which
matters for the DM's decisions. Thus our model provides an explanation for "recency bias"
in the influence of past observations on current decisions, unlike the model of endogenous
memory precision proposed by Afrouzi et al. (2020).
     We have shown that our model predicts "over-reaction" of forecasts of an autoregres-
sive process to current realizations of the process, of a kind similar to that observed in the
forecasts of experimental subjects (Afrouzi et al., 2020), and in survey forecasts of macroeco-
nomic and financial time series (Bordalo et al., 2020a). Moreover, it predicts that the degree
of over-reaction should be greater in the case of less persistent time series. This prediction
is confirmed both by laboratory experiments and survey forecasts, as discussed by Afrouzi
et al. (2020), but cannot be explained by competing explanations for over-reaction, such
as the theory of "diagnostic expectations" proposed by Bordalo et al. (2018).54 Our model
also predicts that over-reaction should be observed even in the case of time series with a
very simple autocorrelation structure -- even a white noise time series -- unlike the model
of "natural expectations" proposed by Fuster et al. (2010, 2011), and again in conformity
with experimental evidence. And unlike either diagnostic or natural expectations (at least
in their most basic forms), our model provides an explanation for heterogeneity in forecasts
on the part of forecasters with access to the same information; indeed, the noise in people's
memories is at the heart of our explanation for forecast biases.
     In the applications sketched above, we have focused on biases that have been observed
in people's stated expectations. But we suspect that the expectational biases implied by
our model can help to explain puzzling aspects of market outcomes as well. For example,
Bordalo et al. (2020b) argue that a number of well-known puzzles about the behavior of the
aggregate stock market are in fact all consistent with a simple dividend discount model of
stock prices, under the hypothesis that market expectations regarding firms' future earnings
differ systematically from rational expectations in a particular way, that is furthermore
consistent with the biases observed in survey expectations of earnings. They further show
that a particular sort of bias in market expectations is needed in order to explain both the
biases in survey expectations and the asset pricing anomalies, one very much like the kind
of forecast bias predicted by our model.
     Briefly, Bordalo et al. propose a model in which asset prices at time t are based on
  54
    See Afrouzi et al. (2020) for a thorough demonstration of the inability of a variety of familiar models to
explain this pattern.



                                                     41
market expectations of dividend growth gt+h at various future horizons h. Dividend growth
is assumed to be a stationary autoregressive process; market expectations of gt+h differ from
rational expectations by an expectional error term h,t . For any horizon h, h,t is assumed to be
a stationary, mean-zero autoregressive process, with a substantial degree of persistence; and
the innovations in h,t are positively correlated with the innovations in gt , though fluctuations
in h,t also occur that are uncorrelated with fundamentals. Finally, the fluctuations in h,t for
different horizons h are perfectly correlated, and h,t remains different from zero as h  ,
so that innovations in the error process bias expectations about dividend growth in the far
future and not only in the near term.
    These assumptions are all features of subjective forecasts of the future evolution of the
state yt+h in our model (if we identify our yt with dividend growth). We have shown (Figure
9) that in our model, innovations in yt cause subjective expectations of the future state to
rise more than the RE forescast would, and the effect persists for several periods, though
the bias caused by the innovation in any single period t eventually converges to zero. For
each horizon h, (3.7) implies that the bias term is equal to (1 - h )^   µt ; thus the biases for
different forecast horizons are all perfectly correlated. Moreover, as the horizon is increased,
the bias term becomes simply µ  ^t for all large enough h; thus the forecast errors predicted by
the model are above all errors in long-term forecasts.
    Our model also implies that there will be random fluctuations in forecast bias that are
uncorrelated with any underlying fundamentals; these innovations are indicated by the        ~ t+1
shock in (3.6). The most important difference with the reduced-form specification of expec-
tational bias proposed by Bordalo et al. is that in their model, there are arbitrary random
variations in the "market expectations" that determine the value of the stock market; our
model instead implies the existence of idiosyncratic random variation in the beliefs of an
individual forecaster, but one might expect that these idiosyncratic variations should cancel
out in their effects on the market price. It is possible that a satisfactory model of asset
pricing will require us to suppose that some individual traders are large enough for their
idiosyncratic beliefs to have a non-negligible effect on aggregate outcomes, as in the model
of Gabaix et al. (2006). We leave the development of a complete model of asset prices for
future work. But it seems likely that imperfect memory of the kind modeled here will be a
necessary element in such a model.




                                               42
References
 [1] Afrouzi, Hassan, Spencer Youngwook Kwon, Augustin Landier, Yueran Ma, and David
     Thesmar, "Overreaction and Working Memory," NBER Working Paper no. 27947, Oc-
     tober 2020.

 [2] Bordalo, Pedro, Nicola Gennaioli, and Andrei Shleifer, "Diagnostic Expectations and
     Credit Cycles," Journal of Finance 73: 199-227 (2018).

 [3] Bordalo, Pedro, Nicola Gennaioli, Yueran Ma, and Andrei Shleifer, "Over-Reaction in
     Macreconomic Expectations," American Economic Review 110: 2748-2782 (2020a).

 [4] Bordalo, Pedro, Nicola Gennaioli, Rafael LaPorta, and Andrei Shleifer, "Expectations
     of Fundamentals and Stock Market Puzzles," NBER Working Paper no. 27283, May
     2020b.

 [5] Caplin, Andrew, Mark Dean, and John Leahy, "Rationally Inattentive Behavior: Char-
     acterizing and Generalizing Shannon Entropy," working paper, New York University,
     February 2019.

 [6] Coibion, Olivier, and Yuriy Gorodnichenko, "Information Rigidity and the Expectations
     Formation Process: A Simple Framework and New Facts," American Economic Review
     105: 2644-78 (2015).

 [7] Cover, Thomas M., and Joy A. Thomas, Elements of Information Theory, New York:
     Wiley, 2d ed., 2006.

 [8] Evans, George W., and Seppo Honkapohja, Learning and Expectations in Macroeco-
     nomics, Princeton: Princeton University Press, 2001.

 [9] Fuhrer, Jeffrey, "Expectations as a Source of Macroeconomic Persistence: Evidence
     from Survey Expectations in Dynamic Macro Models," Journal of Monetary Economics
     86: 22-35 (2017).

[10] Fuhrer, Jeffrey, "Intrinsic Expectations Persistence: Evidence from Professional and
     Household Survey Expectations," Federal Reserve Bank of Boston Working Paper no.
     18-9, May 2018.

[11] Fuster, Andreas, Ben H´
                           ebert, and David Laibson, "Natural Expectations, Macroeco-
     nomic Dynamics, and Asset Pricing," NBER Macroeconomics Annual 26: 1-48 (2011).

[12] Fuster, Andreas, David I. Laibson, and Brock Mendel, "Natural Expectations and
     Macroeconomic Fluctuations," Journal of Economic Perspectives 24(4): 67-84 (2010).

[13] Gabaix, Xavier, Parameswaran Gopikrishnan, Vasiliki Plerou, and H. Eugene Stanley,
     "Institutional Investors and Stock-Market Volatility," Quarterly Journal of Economics
     121: 461-504 (2006).



                                           43
[14] Khaw, Mel Win, Luminita L. Stevens, and Michael Woodford, "Discrete Adjustment
     to a Changing Environment: Experimental Evidence," Journal of Monetary Economics
     91: 88-103 (2017).

[15] Malmendier, Ulrike, and Stefan Nagel, "Learning from Inflation Experiences," Quarterly
     Journal of Economics 131: 53­87 (2016).

[16] Malmendier, Ulrike, Demian Pouzo, and Victoria Vanasco, "A Theory of Experience
     Effects," working paper, UC Berkeley, January 2017.

[17] Metzler, Lloyd A., "The Nature and Stability of Inventory Cycles," Review of Economics
     and Statistics 23: 113-129 (1941).

[18] Milani, Fabio, "Expectations, Learning and Macroeconomic Persistence," Journal of
     Monetary Economics 54: 2065­2082 (2007).

[19] Milani, Fabio, "Learning and Time-varying Macroeconomic Volatility," Journal of Eco-
     nomic Dynamics and Control 47(C): 94­11 (2014).

[20] Muth, John F., "Rational Expectations and the Theory of Price Movements," Econo-
     metrica 29: 315-335 (1961).

[21] Neligh, Nathaniel, "Rational Memory with Decay," working paper, Chapman Univer-
     sity, November 2019.

[22] Sims, Christopher A., "Implications of Rational Inattention," Journal of Monetary Eco-
     nomics 50: 665-690 (2003).

[23] Slobodyan, Sergey, and Raf Wouters, "Learning in an Estimated Medium-scale DSGE
     Model," Journal of Economic Dynamics and Control 36: 26­46 (2012).




                                            44
                                            APPENDIX

A        Reduction of the General Forecasting Problem to
         Estimation of µ
Consider the problem of choosing the vector of forecasts zt each period so as to minimize
(1.2). The elements of zt must be chosen as a function of the DM's cognitive state at time
t (after observing the external state yt ). As explained in the text, the DM's cognitive state
at time t is assumed to consist of the value of the current external state yt (observed with
perfect precision), along with whatever additional information is reflected in the DM's period
t memory state mt . (In this section, it is not yet necessary to specify the nature of the vector
mt .)
    If we use the notation Et [·] for the expectation of a random variable conditional on a
complete description of the state at date t (including knowledge of the true value of µ), then

                                    E[(zt - Et z       zt - Et z
                                               ~t ) W (~       ~t )] = 0,

      ~t - Et z
since z       ~t is a function of innovations in the external state subsequent to date t, that
must be distributed independently of all of the determinants of both zt and Et z ~t . It follows
                                                                      55
that the term in (1.2) involving zt can be equivalently expressed as

                      E[(zt - z
                              ~t ) W (zt - z
                                           ~t )] = E[(zt - Et z
                                                              ~t ) W (zt - Et z
                                                                              ~t )]
                                                   +E[(~zt - Et z        zt - Et z
                                                                 ~t ) W (~       ~t )]
                                                  L1t + L2t .

Moreover, L2t is independent of the decisions of the DM, and thus irrelevant to a determina-
tion of the optimal decision rule. The loss function (1.2) can thus equivalently be written as
the discounted sum of the L1t terms, which involve squared differences between zt and Et z  ~t .
    It further follows from the law of motion (1.1) that
                                                 
                                      Et z
                                         ~t =          Aj [µ + j (yt - µ)].
                                                j =0

Since the precise value of yt is presumed to be part of the cognitive state on the basis of
which zt can be chosen, one can write any decision rule in the form
                                                          
                                         zt = z
                                              ^t + (             j Aj ) · yt ,
                                                          j =0

where z^t must be some function of the cognitive state at date t. In terms of this notation,
the relevant part of the loss function (1.2) can then be written as

                                              zt - µa) W (^
                                     L1t = E[(^           zt - µa)],
 55
      Here we omit the factor  t that multiplies this term in (1.2).


                                                         45
where we define a                  j
                         j =0 (1 -  )Aj and make use of the fact that Et [µ] = µ.
    The term L1t that we wish to minimize can further be expressed as the expected value
(integrating over all possible realizations of the cognitive state st in period t) of the quantity
                     ~ 1 (st )  E[(^
                     L              zt - µa) W (^    zt - µa) |st ]
                                   zt |st ] W E[^
                               = E[^             zt |st ] + E[       t |st ]
                                                                zt W z
                                 - 2a W E[^  zt |st ] · E[µ|st ] + a W a · E[µ2 |st ],
where we define z   t  z ^t - E[^
                                zt |st ]. (In expanding the right-hand side in this way, we use the
fact that E[ zt |st ] = 0, and that z  t must be independent of the deviation of µ from E[µ|st ],
since the DM has no way to condition her action on µ except through the information about
µ revealed by the cognitive state.) The expression L        ~ 1 (st ) can then be separately minimized
for each possible cognitive state st , by choosing a distribution for z     ^t conditional on that state.
We further note that the random component z             t of the action affects only the second term
on the right-hand side, and so should be chosen to minimize that term; since W is positive
definite, this is achieved by setting z    t = 0 with certainty, so that z   ^t must be a deterministic
function of st .
    We can then simply write E[^     zt |st ] as z
                                                 ^t , and observe that
                 ~ 1 (st ) = (^
                 L            zt - aE[µ|st ]) W (^
                                                 zt - aE[µ|st ]) + a W a · var[µ|st ],            (A.1)
where the final term on the right-hand side is independent of the choice of z    ^t . Thus in each
cognitive state st , z
                     ^t must be chosen to minimize the first term on the right-hand side; since
W is positive definite, this is achieved by setting z^t = a · µ          ^t = E[µ|st ].
                                                              ^t , where µ
    Thus there is no loss of generality in restricting the DM to response rules of the form
^t = a · µ
z        ^t , where µ^t is a scalar choice that depends on the cognitive state in period t, and
that can be interpreted as the DM's estimate of µ given the cognitive state. Substituting
this expression for z ^t into (A.1), we have
                        ~ 1 (st ) = a W a · (^
                        L                     µt - E[µ|st ])2 + var[µ(st )]
                                               µt - µ)2 |st ].
                                  = a W a · E[(^
Then taking the unconditional expectation of this expression, we obtain
                                          L1t =  · M SEt ,
where   a W a > 0 and M SEt is defined as in the text.
    Under any forecasting rule of the kind assumed here, then, the value of the loss function
(1.2) will equal (1.4), plus an additional term
                                                 
                                                       t L2t
                                                t=0

that is independent of the DM's forecasting rule. Hence within this class of forecasting rules,
the rule that minimizes (1.2) must be the one that minimizes (1.4); and since any other kind
of forecasting rule can only lead to a higher value of (1.2), we can replace the problem of
choosing a rule for determining zt that minimizes (1.2) by the problem of choosing a rule for
determining µ ^t that minimizes (1.4).

                                                      46
B     Bayesian Updating After the External State is
      Observed: A Kalman Filter
Let the elements of the memory state be partitioned as

                                                           mt
                                          mt =                   ,                          (B.1)
                                                           m
                                                           ¯t

where the lower block consists of the elements of the reduced memory state

                                                                          µ
                         ¯ t  E[xt |mt ],
                         m                            where xt                 ,
                                                                        yt-1

while the upper block consists of the conditional expectations E[yt-j |mt ] for 2  j  t. (This
simply requires an appropriate ordering of the elements of mt , using the notation for this
vector introduced in the main text.)
   We assume a posterior distribution of the form

                                       xt |mt  N (m
                                                  ¯ t , t )

conditional on the memory state mt , where m  ¯ t is a 2-vector and t is a 2 × 2 symmetric,
p.s.d. matrix. Under our assumption of linear-Gaussian dynamics for the memory state, the
vector m¯ t will also be drawn from a multivariate Gaussian distribution. Since the prior for
the hidden state vector is specified to be

                                                                       
                       xt  N (0, 0 ),                      0             2         ,        (B.2)
                                                                       + y

it follows that the unconditional distribution for the reduced memory state m
                                                                            ¯ t must be of
the form
                                   m¯ t  N (0, 0 - t ).
   The complete set of variables (xt , mt ) also have a multivariate Gaussian distribution.
Moreover, since (by assumption) the expectation of xt conditional on the realization of mt
depends only on the elements of m  ¯ t , it follows that the entire distribution of xt conditional
on mt depends only on m¯ t , so that

                                          xt |mt = xt |m
                                                       ¯ t.

Hence the joint distribution of the variables (xt , mt ) can be factored as

                             p(xt , mt , m              ¯ t ) · p(mt |m
                                         ¯ t ) = p(xt , m             ¯ t ).

    The DM then observes the external state yt , which is assumed to depend on the hidden
state vector xt through an "observation equation" of the form

                             y t = c xt +      yt ,         yt    N (0,  2 )


                                                      47
as a consequence of (1.1), where we further assume that yt is distributed independently of
both mt and xt . It follows that the variables (xt , mt , yt ) will have a joint distribution that is
multivariate Gaussian; and that this distribution can be factored as

                         p(xt , mt , yt ) = p(xt , mt ) · p(yt |xt )
                                          = p(mt |m ¯ t ) · p(xt , m
                                                                   ¯ t ) · p(yt |xt )
                                          = p(mt |m ¯ t ) · p(xt , m
                                                                   ¯ t , yt ).

From this it follows that
                                        xt |mt , yt = xt |m
                                                          ¯ t , yt .
Thus both the expectation of xt conditional on the cognitive state st  (mt , yt ), and the
variance-covariance matrix of the errors in the estimation of xt based on the cognitive state,
will depend only on the joint distribution of the variables (xt , m   ¯ t , yt ). Moreover, the distri-
bution for xt conditional on the realizations of the elements of the cognitive state will be
multivariate Gaussian,
                                      xt |m
                                          ¯ t , yt  N (¯ µt , ¯ t ),                             (B.3)
where µ¯t is a linear function of m
                                  ¯ t and yt , while ¯ t is independent of the realizations of either
m
¯ t or yt .
    We can further decompose the vector of means µ         ¯t as

                          µ
                          ¯t =      E[xt |m
                                          ¯ t , yt ]
                             =      E[xt |m
                                          ¯ t ] + {E[xt |m
                                                         ¯ t , yt ] - E[xt |m
                                                                            ¯ t ]}
                             =      ¯ t + t · (yt - E[yt |m
                                    m                          ¯ t ])
                             =      ¯ t + t · (yt - c E[xt |m
                                    m                            ¯ t ])
                             =      ¯ t + t · (yt - c m
                                    m                    ¯ t ),

where t is the vector of Kalman gains. (The first element of this vector equation is then
just equation (2.1) in the main text.)
    The vector of Kalman gains must be chosen so that the estimation errors xt - µ       ¯t are
orthogonal to the surprise in the observation of the external state, yt - c m
                                                                            ¯ t . This requires
that

                        0 =      cov(xt - µ
                                          ¯ t , yt - c m
                                                       ¯ t)
                          =      cov((xt - m¯ t ) - t (yt - c m
                                                              ¯ t ), y t - c m
                                                                             ¯ t)
                          =      var[xt - m
                                          ¯ t ]c - var[c (xt - m ¯ t ) + yt ] · t
                          =      t c - [c t c +  2 ] · t .

Hence
                                                      t c
                                           t =                 .                                 (B.4)
                                                    c t c +  2
The gain coefficient 1t in equation (2.1) is just the first element of this vector, 1t  e1 t .
This together with (B.4) yields the formula (2.3) given in the main text.


                                                     48
    The variance-covariance matrix in the conditional distribution (B.3) will be given by
                   ¯ t = var[xt - µ
                                   ¯t ] = var[(xt - m ¯ t ) - t (yt - c m
                                                                        ¯ t )]
                       = var[(I - t c )(xt - m¯ t ) - t yt ]
                       = (I - t c )t (I - t c ) +  2 t t
                       = t - 2[c t c +  2 ]t t + [c t c]t t +  2 t t
                       = t - [c t c +  2 ]t t .
                                                                           2
The remaining uncertainty about the value of µ given the cognitive state, ^t , is then equal
   ¯
to 11,t , so that
                       2
                      ^t = e1 ¯ t e1 = e1 t e1 - (c t c +  2 )(1t )2 ,
which is just expression (2.2) in the main text.
   Substituting expression (B.2) for 0 into this solution, we obtain
                                                                              2
                             2                          2      
                            ^0    =  - +                y   ·    2
                                                               + y
                                           2
                                        y
                                  =          2
                                               ,
                                         + y
which is the formula given in (2.4). It remains to be shown that this is an upper bound for
 2
^t . To show this, we observe that
                     2
                    ^t = min var[µ -  m
                                      ¯ t - 1 yt ]
                            ,1
                         min var[µ - 1 yt ]
                             1
                                       2
                         var[µ - (/( + y )) · yt ]
                               2      2             2
                        = var[(y /( + y ))µ - (/( + y ))(yt - µ)]
                                  2      2                               2
                                 y                           
                        =           2
                                             var[µ] +          2
                                                                             var[yt |µ]
                                 + y                         + y
                                  2      2                       2
                                 y                                   2
                        =           2
                                             +         2
                                                                     y
                                 + y                 + y
                               2
                            y        2
                        =        2
                                   = 0 .
                             + y
This establishes the upper bound (2.4) stated in the main text.


C      Demonstration that an Optimal Memory Structure
       Records Information Only about the Reduced
       Cognitive State
Let (1.5) be written in the partitioned form
                        mt+1             a,t b,t            st                 t+1
                                  =                                  +                .   (C.1)
                        m
                        ¯ t+1            c,t d,t            s
                                                            ¯t                ¯ t+1

                                                   49
Here mt+1 is again partitioned as in (B.1). The lower block of st consists of the elements of
the reduced cognitive state
                                               µ
                                               ^t
                                       ¯t 
                                       s           ,
                                               yt
both elements of which are linear functions of st , as a consequence of equation (2.1). We
choose a representation for the vector st such that the lower block consists of the elements
of s¯t , the elements of st are all uncorrelated with the elements of s¯t , and the elements of the
vectors s  ¯t and st together span the same linear space of random variables as the elements of
st . (We can necessarily write any memory structure of the form (1.5) in this way; it amounts
simply to a choice of the basis vectors in terms of which the vectors mt+1 and st are each
decomposed.)
     Let us suppose furthermore that a representation for mt+1 is chosen consistent with the
normalization E[¯    st |mt+1 ] = m¯ t+1 . This holds if and only if both elements of the vector
¯t - m
s       ¯ t+1 are uncorrelated with each of the elements of mt+1 . These consistency conditions
can be reduced to two requirements: (i) the requirement that

                             var[c,t st + ¯ t+1 ] = (I - d,t )Xt d,t ,                       (C.2)

where the matrix Xt  var[¯     st ] is independent of the memory structure chosen for period
t; and (ii) the requirement that s  ¯t - m¯ t+1 be uncorrelated with all elements of mt+1 . (Note
     ¯t - m
that s     ¯ t+1 is uncorrelated with m  ¯ t+1 if and only if (C.2) holds.)

C.1     Forecast accuracy depends only on the matrices {d,t }
Suppose that in any period t, we take the memory structure in periods  < t as given. This
means that the DM's uncertainty about xt given the memory state mt (specified by the
posterior variance-covariance matrix t ) will be given. (If t = 0, 0 is simply given by the
prior.) Hence the value of µ^t as a function of m¯ t and yt will be given, and consequently the
value of M SEt will be given, following the discussion in the main text (and the previous
section of this appendix). The elements of the matrix Xt will similarly be given.
    We next consider how d,t must be chosen, in order for it to be possible to choose matrices
c,t and var[¯t+1 ] such that (C.2) is satisfied. Equation (C.2) requires that (I - d,t )Xt d,t ,
be a symmetric matrix; this will hold if and only if the simpler requirement is satisfied that
d,t Xt = Xt d,t be a symmetric matrix. In addition, it is necessary that (I - d,t )Xt d,t be
a p.s.d. matrix. The set of matrices d,t with these properties is a non-empty set (d,t = 0 is
a trivial example), and depends only on the matrix Xt . Let this set of matrices be denoted
L(Xt ).
    Now let d,t be any matrix that belongs to L(Xt ). Then it is possible to choose the
matrices c,t and var[¯ t+1 ] so that (C.2) is satisfied; and given any such choice of these
two matrices, it is further possible to choose the specification of the equation for mt+1 so
that all elements of mt+1 are uncorrelated with the elements of s    ¯t - m
                                                                          ¯ t+1 . Given any such
specifications, both conditions (i) and (ii) above will be satisfied. Thus the matrix d,t
is admissible as part of the specification of a memory structure; and any possible memory
structure consistent with the matrix d,t will be one of those with the properties just assumed.


                                                50
   Given a matrix d,t of this sort, we next observe that the equations determining m
                                                                                   ¯ t+1
can be written in the form
                                 m
                                 ¯ t+1 = d,t s
                                             ¯t + t+1 ,
where t+1  N (0, d,t Xt ) is distributed independently of s           ¯t . Thus the joint distribution
of (¯st , m
          ¯ t+1 ) will be a multivariate Gaussian distribution, the parameters of which are com-
pletely determined by Xt and d,t . It then follows that the conditional distribution s          ¯t |m
                                                                                                    ¯ t+1
will be a bivariate Gaussian distribution, with a mean m         ¯ t+1 and a variance independent of
the realization of m    ¯ t+1 , which also depends only on Xt and d,t . Moreover, since the elements
of mt+1 are all Gaussian random variables distributed independently of s           ¯t - m¯ t+1 , knowl-
edge of mt+1 cannot further improve one's estimate of s       ¯t , and so the conditional distribution
¯t |mt+1 = s
s             ¯t |m
                  ¯ t+1 . Finally, since we can write
                                                             ut
                                          xt+1 = s
                                                 ¯t +              ,
                                                             0
                  2
where ut  N (0,  ^t ) must be uncorrelated with any of the elements of st (and hence uncor-
related with any of the elements of mt+1 ), we must further have
                                     xt+1 |mt+1  N (m
                                                    ¯ t+1 , t+1 )
where
                                                            2
                                             st |m
                                   t+1 = var[¯   ¯ t+1 ] + ^t e1 e1 .
        2
Since  ^t also depends only on t (see equation (2.2)), it follows that the elements of t+1
depend only on t and d,t .
    This argument can then be used recursively (starting from period t = 0) to show that
given the initial uncertainty matrix 0 implied by the prior (B.2), we can completely de-
termine the entire sequence of matrices {t }, given a sequence of matrices {d,t } for all
t  0 with the property that for each t, d,t  L(Xt ), where Xt is the matrix implied by
t . Moreover, given such a sequence of matrices {d,t }, the value of M SEt for each period t
will be uniquely determined as well. Hence the terms in the loss function (1.6) that depend
on the accuracy of forecasts that are possible using a given memory structure will depend
only on the sequence of matrices {d,t }. (These matrices must be chosen to satisfy a set
of consistency conditions, stated above, but these conditions can also be expressed purely
in terms of the sequence of matrices {d,t }.) Thus the other elements of the specification
(C.1) of the memory structure matter only to the extent that they have consequences for
the information cost terms in (1.6).

C.2      Mutual information: a useful lemma
Information costs in period t are assumed to be an increasing function of It = I (M ; S ),
the Shannon mutual information between random variables M (the realizations of which
are denoted mt+1 ) and S (the realizations of which are denoted st ).56 Each of the random
vectors M and S can further be partitioned as M = (M , M ¯ ), S = (S, S¯).
  56
    Here we adopt the notation used in Cover (2006), with different symbols for the random variables M
and S and their realizations. This is to make it clear that It is not a function of the values taken by mt+1
and st along a particular history, but instead a function of the complete joint distribution of the two random
variables; It is itself not a random variable, but a single number for each date t.


                                                     51
    Now for any random variables X1 , X2 , . . . , let H (X1 , X2 , . . . , Xk ) be the entropy of the
joint distribution for variables (X1 , X2 , . . . , Xk ), and H (X1 , . . . , Xk |Xk+1 , . . . Xk+m ) be the
entropy of the joint distribution of the variables (X1 , . . . , Xk ) conditional on the values of
the variables (Xk+1 , . . . Xk+m ). The chain rule for entropy implies that

         H (X1 , X2 , . . . , Xk ) = H (X1 ) + H (X2 |X1 ) + . . . + H (Xk |X1 , . . . , Xk-1 ).

   We can then define the mutual information between the variables (X1 , . . . , Xk ) and the
variables (Xk+1 , . . . Xk+m ) as

   I (X1 , . . . , Xk ; Xk+1 , . . . , Xk+m )  H (X1 , . . . , Xk ) - H (X1 , . . . , Xk |Xk+1 , . . . Xk+m ).

(The information about the first set of variables that is revealed by learning the values of
the second set of variables is measured by the average amount by which the entropy of the
conditional distribution is smaller than the entropy of the unconditional distribution of the
first set of variables.) Similarly, we can define the mutual information between the first set
of variables and the second set of variables, conditioning on the values of some third set of
variables as

      I (X1 , . . . , Xk ; Xk+1 , . . . , Xk+m |Xk+m+1 , . . . , Xk+m+n )

           H (X1 , X2 , . . . , Xk |Xk+m+1 , . . . , Xk+m+n ) - H (X1 , . . . , Xk |Xk+1 , . . . , Xk+m+n ).
    Thus for any set of four random variables M , M      ¯ , S, S,
                                                                ¯ we must have

      ¯; M , M
I (S, S      ¯) =    H (S, S¯) - H (S, S ¯ |M , M
                                                ¯)
                =         ¯) + H (S |S
                     [ H (S           ¯)] - [H (S ¯ |M , M¯ ) + H (S |S,
                                                                       ¯ M, M ¯ )]
                =         ¯) + H (S |S
                     [ H (S           ¯)] - [H (S,¯ M, M  ¯ ) - H (M |M ¯ ) - H (M ¯ )] - H (S |S,
                                                                                                 ¯ M, M ¯)
                =         ¯) + H (S |S
                     [ H (S           ¯)] - [(H (M  ¯ ) + H (S¯ |M
                                                                 ¯ ) + H (M |M  ¯,S¯)) - H (M |M  ¯ ) - H (M¯ )]
                                                               ¯ M, M
                                                      - H (S |S,      ¯)
                          ¯) + H (S |S
                   = [ H (S           ¯)] - [H (S ¯ |M¯ ) + H (M |M ¯,S ¯) - H (M |M  ¯ )] - H (S |S,
                                                                                                    ¯ M, M ¯)
                          ¯) - H (S
                   = [ H (S       ¯ |M¯ )] + [H (S |S             ¯ M, M
                                                       ¯) - H (S |S,      ¯ )] + [H (M |M  ¯ ) - H (M |M ¯,S¯)]
                        ¯; M
                   = I (S   ¯ ) + I (S ; M , M
                                             ¯ |S¯ ) + I (M ; S¯ |M
                                                                  ¯ ).

Then, since mutual information is necessarily non-negative, we can establish the lower bound
                                              ¯; M , M
                                   It = I (S, S      ¯ )  I (S
                                                             ¯; M
                                                                ¯ ).                                       (C.3)

Furthermore, this lower bound is achieved if and only if
                                            ¯ |S
                                 I (S ; M , M  ¯ ) = I (M ; S
                                                            ¯ |M
                                                               ¯ ) = 0.

     For any three random variables X, Y, Z, the conditional mutual information I (X ; Y |Z ) =
0 if and only if the variables X and Y are distributed independently one another, conditional
on the value of Z . Hence the lower bound (C.3) is achieved if and only if (a) conditional
on the value of m  ¯ t+1 , the variables s¯t and mt+1 are independent of one another; and (b)
conditional on the value of s  ¯t , the variables st and mt+1 are independent of one another.

                                                       52
C.3      Optimality of Setting a,t = b,t = c,t = 0
We return now to the consideration of possible memory structures. Let the sequence of
matrices {d,t } be chosen to satisfy the consistency conditions discussed above, and for a
given such sequence, consider an optimal choice of the remaining elements of the specification
(C.1), from among those specifications that are consistent with the sequence {d,t } (that is,
that will satisfy both conditions (i) and (ii) stated above).
     We have shown above that the sequence of values {M SEt } is completely determined by
the specification of {d,t }. Hence other aspects of the specification of the memory structure
can matter only to the extent that they affect the sequence of values {It }. Moreover, we have
shown that the joint distribution of (¯     st , m
                                                 ¯ t+1 ) each period is completely determined by Xt
and d,t , which means that the lower bound for It given in (C.3) is completely determined
by the choice of {d, } for   t. It thus remains only to consider whether this lower bound
can be achieved, and under what conditions.
     We first observe that the lower bound is achievable. For any sequence of matrices {d,t }
satisfying the specified conditions, a memory structure specification with a,t = b,t =
c,t = 0, together with a stipulation that  t+1 be distributed independently of                 ¯ t+1 and that
var[¯ t+1 ] = d,t Xt , will satisfy both conditions (i) and (ii) stated in the introduction to this
appendix, and thus this represents a feasible memory structure. One can also show that such
a specification satisfies both of conditions (a) and (b) stated at the end of section C.2, so
that the lower bound (C.3) is achieved in each period. Thus such a specification achieves the
lowest possible value for the combined objective function (1.6), and will be optimal, given
our choice of the sequence {d,t }.
     Not only will this specification be sufficient for achieving the lowest possible value of
(1.6), but it will be essentially necessary. We have shown above that achieving the lower
bound for It in period t requires that conditional on the value of s          ¯t , the variables st and mt+1
are independent of one another. This means that the values of the variables in the vector st
cannot help at all in predicting any elements of mt+1 , once one is already using the reduced
cognitive state s¯t to forecast the next period's memory state; thus one must be able to write
law of motion (C.1) for the memory state with a,t = c,t = 0.57 Thus it is necessarily the
case that the elements of mt+1 convey information only about the reduced cognitive state
s
¯t , and not about any other aspects of the cognitive state st .
     In addition, we have shown above that achieving the lower bound for It in period t
requires that conditional on the value of m      ¯ t+1 , the variables s ¯t and mt+1 are independent of
one another. Thus all of the information about s         ¯t that is contained in the memory state mt+1
is contained in the elements m    ¯ t+1 . This means either that b,t = 0 as well, or, to the extent
that some element of mt+1 corresponds to a row of b,t with non-zero elements, that element
of mt+1 must be a linear combination of the elements of m            ¯ t+1 , so that conditioning upon its
value conveys no new information about s         ¯t . Thus any specification of the memory structure
  57
     It might be possible to satisfy the condition required for the lower bound with non-zero elements in one
of these matrices; but this will occur only because of collinearity in the fluctuations in the elements of the
vector st , so that it is possible to have a law of motion in which st has no effect on mt+1 , despite non-zero
matrices a,t and c,t . In such a case, the representation of the cognitive state by the vector st would
involve redundancy; and in any event, there would be no loss of generality in setting a,t = c,t = 0, since
the implied fluctuations in the memory state would be the same.



                                                      53
in which b,t = 0 in any period represents a redundant representation of the contents of
memory available in period t + 1; we can equivalently describe the contents of memory by
eliminating all such rows from mt+1 .
     Thus there is no loss of generality in assuming that the lower bound is achieved by
specifying a,t = b,t = c,t = 0 in each period. Finally, satisfaction of consistency condition
(ii) in this case requires that the elements of  t+1 be distributed independently of the elements
of  ¯ t+1 . We might still allow var[ t+1 ] to be non-zero; this would mean that mt+1 contains
elements that fluctuate randomly, but are completely uncorrelated with the previous period's
cognitive state st . Such an information structure is equally optimal, in the sense that (1.6)
is made no larger by the existence of such components of the memory state, given our
assumption that only mutual information is costly. But the additional components mt+1 of
the memory structure will have no consequences for cognitive processing, and our inclusion
of them as part of the representation of the memory state violates our assumption in the
text that we label memory states by their implied posteriors for the values of µ and the
past realizations of the external state; using labels (mt+1 , m  ¯ t+1 ) in which mt+1 is non-null
will mean having separate labels for memory states that imply the same posterior (since the
value of mt+1 would be completely uninformative about either µ or any past external states).
     Hence in the case of any optimal memory structure, the memory state can be described
more compactly by identifying it with the reduced memory state m           ¯ t+1 , which evolves ac-
cording to
                                       m¯ t+1 =  ¯ ts
                                                    ¯t + ¯ t+1 ,                                (C.4)
where    ¯ t is the matrix called d,t in (C.1). (This corresponds to equation (2.7) in the main
text.) We need only consider (at most) a two-dimensional memory state, and the optimal
memory state conveys information only about the reduced cognitive state s          ¯t , not about any
other aspects of the cognitive state st .

C.4     An alternative representation for the reduced cognitive state
We have shown in the main text (equation (2.10)) that the variance matrix of the reduced
                                                                         2
cognitive state s
                ¯t can be written as a function of the single parameter ^t :
                                                     2
                                       2           -^t  
                             Xt = X (^
                                     t   )                2            .
                                                        + y

There is another way of writing this function that will be useful below.
  We can orthogonalize the reduced cognitive state using the transformation s
                                                                            ¯t =  st ,
where
                                                 
                                            1 +   y2
                                                        .                       (C.5)
                                            0    1
The elements of the orthogonalized cognitive state have the interpretation

                                              ^t - E[µ|yt ]
                                              µ
                                     t 
                                     s                        ,
                                                   yt

from which it is obvious that the first element must be uncorrelated with the second.

                                                 54
    The variance matrix of s
                           t is therefore diagonal:
                                                        2    2
                                       (^ 2            ^0 - ^t  0
                           var[st ] = X t   )                     2          .                   (C.6)
                                                          0     + y

We can then alternatively write

                                       X (^
                                          t 2      (^
                                              ) = X t 2
                                                        ) .                                      (C.7)


D      The Law of Motion for the Memory State and the
       Information Content of Memory
We now consider how the parameterization of the law of motion (C.4) for the memory state
determines the degree of uncertainty about the external state vector that will exist when
beliefs are conditioned on the memory state, and how the same parameters determine the
mutual information between the memory state and the prior cognitive state, and hence the
size of the information cost term c(It ).
    We begin by recapitulating the conditions that the sequence of matrices {  ¯ t } and { ¯ ,t+1 }
must satisfy, in order for (C.4) to represent a memory structure consistent with the normal-
ization according to which E[xt+1 |m  ¯ t+1 ] = m
                                                ¯ t+1 . Condition (C.2) will be satisfied if and
only if
                                                      ¯    ¯
                                      ¯ ,t+1 = (I - t )Xt t .                              (D.1)
In order for there to be a symmetric, p.s.d. matrix         ¯ ,t+1 that satisfies (D.1), it must be
               ¯
the case that t  L(Xt ). As explained above, this means that             ¯ t Xt = Xt  ¯ must be a
                                                                                       t
symmetric matrix, and in addition that (I -  ¯ t ) Xt ¯ t is p.s.d. Note that since

                                Xt ¯ = (I - ¯ t )Xt ¯ +¯ t Xt ¯,
                                    t                t         t

and Xt is necessarily a p.s.d. matrix, it follows from the assumption that (I -          ¯ t )Xt ¯ t is
            ¯           ¯
p.s.d. that t Xt = Xt t will also be a p.s.d. matrix; but this latter condition is weaker
than the one assumed in our definition of the set L(Xt ). This constitutes the complete set of
conditions that must be satisfied for (C.4) to represent a memory structure consistent with
our proposed normalization of the vector mt+1 .
    We can further specialize these conditions in the case that     ¯ t is a singular matrix. (Here
we assume that Xt is of full rank.) If     ¯ t is of rank one (or less), it can be written in the
      ¯
form t = ut vt , where we are furthermore free to normalize the vector vt so that vt Xt vt = 1.
Then the condition that   ¯ t Xt = Xt ¯ will hold only if ut (v Xt ) = (Xt vt )u . This means that
                                        t                      t                 t
ut must be collinear with Xt vt , so that we must be able to write ut = t Xt vt , for some scalar
t . Thus in the singular case, we must be able to write
                                          ¯ t = t Xt vt v ,
                                                                                                 (D.2)
                                                         t

where t is a scalar and vt is a vector such that vt Xt vt = 1. Then

                            (I - ¯ t ) Xt ¯ t = t (1 - t )(Xt vt )(Xt vt )

                                                  55
will be a p.s.d. matrix if and only if in addition 0  t  1. Thus a singular matrix ¯ t is an
element of L(Xt ) if and only if it is of the form (D.2) with 0  t  1 and vt a vector such
that vt Xt vt = 1.
    Consistency with the proposed normalization of mt+1 then further requires that

                                  ¯ ,t+1 = t (1 - t )Xt vt vt Xt .
                                                                                                (D.3)

This implies that       ¯ ,t+1 is a singular matrix; the random vector        ¯ t+1 can be written as
¯ t+1 = Xt vt ·
               ~ t+1 , where   ~ t+1 is a scalar random variable, with distribution N (0, t (1-t ). It
follows that in such a case, the memory state can be given a one-dimensional representation,
writing m¯ t+1 = Xt vt · m  ~ t+1 , where the scalar memory state m  ~ t+1 has a law of motion

                     m
                     ~ t+1 = t vt s
                                  ¯t + ~ t+1 ,        ~ t+1  N (0, t (1 - t )).
                                                                                                (D.4)
                                                                                        2
    In the case that Xt = X0 (the only case in which it is possible for Xt = X (^     t   ) to be
singular), we have defined L(X0 ) to include only matrices of the special form (2.11) with
0  t  1. In this case,     ¯ t is necessarily of the form (D.2), with the vector vt given by
(2.20). Hence our comments above about the case in which       ¯ t is singular apply also in the
case in which Xt is singular, except that in this latter case we have the further restriction
that vt must be given by (2.20). In this special case, (D.3) reduces to
                                                       2
                                ¯ ,t+1 = t (1 - t )[ + y ] ww .
                                

D.1     The degree of uncertainty implied by a given memory
        structure
We turn now to the question of how the posterior uncertainty t+1 in the following period
is determined by the law of motion for the memory state m ¯ t+1 that can be accessed at that
time. Note that the variance of the marginal distribution for xt+1 can be decomposed as

                      var[xt+1 ] = E[var[xt+1 |mt+1 ]] + var[E[xt+1 |mt+1 ]],

where in the first term on the right-hand side, the variance refers to the distribution of values
for xt+1 conditional on the realization of mt+1 , and the expectation is over realizations of
mt+1 , while in the second term the variance refers to the distribution of values for mt+1 ,
and the expectation is over values of xt+1 conditional on the realization of mt+1 . Since the
marginal distribution for xt+1 is the same for all t, and coincides with the prior distribution
for x0 specified in (B.2), the left-hand side must equal the matrix 0 defined there. Hence
the variance decomposition can be written as

                                     0 = t+1 + var[m
                                                   ¯ t+1 ],

which implies that in any period,

                                     t+1 = 0 - var[m
                                                   ¯ t+1 ].

Thus in order to understand how the choice of   ¯ t determines t+1 , it suffices that we deter-
mine the implications for the degree of variation in m¯ t+1 .

                                                 56
   A law of motion of the form (C.4) implies that

                             var[m
                                 ¯ t+1 ] =    ¯ t Xt 
                                                     ¯ +     ¯ ,t+1
                                                       t
                                         =    ¯ t Xt 
                                                     ¯ t + (I -   ¯ t )Xt ¯t
                                         =    Xt  ¯,
                                                   t

where the second line uses (D.1). Hence we obtain the prediction that

                                       t+1 = 0 - Xt ¯.                                            (D.5)
                                                     t

Note that for any     ¯ t  L(Xt ), this must be a symmetric, p.s.d. matrix.
    Hence for any value of      ^t2
                                    satisfying 0       2
                                                      ^t     ^0 2
                                                                  and any transition matrix     ¯t 
       2                                     2
L(X (^t  )), we can substitute Xt = X (^    t  ) and the value of t+1 given by (D.5) into (2.2)
to obtain a solution for     2
                            ^t+1 as a function of   2
                                                   ^t and  ¯ t . This defines the function f (^
                                                                                              t 2 ¯
                                                                                                 , t )
                                                         seq
referred to in the main text. We can then define L as the set of sequences of transition
matrices {  ¯ t } for all t  0 such that

          ¯ 0  L(X0 ),
                             ¯ 1  L(X (f (^
                                          0 2 ¯
                                             , 0 ))),        ¯ 2  L(X (f (f (^
                                                                             0 2 ¯
                                                                                , 0 ), ¯ 1 ))),

and so on.
    Then given any sequence of transition matrices {            ¯ t }  Lseq , there will be uniquely
                         2
defined sequences {    ^t  , Xt } for all t  0. Equation (D.5), together with (B.2), can then be
used to uniquely define the implied sequence of matrices {t } for all t  0. These matrices
can in turn be used in (2.3) to define the Kalman gain 1t for each t  0. Thus for any
sequence of transition matrices {        ¯ t }  Lseq , there will be uniquely determined sequences
           2
{t , 1t , ^t , Xt }, as stated in the text. These in turn will imply a uniquely determined
sequence of losses {M SEt } from forecast inaccuracy, using (2.5).

D.2     The mutual information implied by a given memory structure
Finally, we compute the mutual information It in the case that the memory state consists
only of a reduced memory state m   ¯ t+1 , with law of motion (C.4). We first review the definition
of mutual information in the case of continuously distributed random variables.
    Let X and Y be two random variables, each parameterized using a finite system of
coordinates (so that realizations x and y are each represented by finite-dimensional vectors),
and suppose that at least Y has a continuous distribution, with a density function p(y |x) such
that p(y |x) > 0 for all y in the support of Y and all x in the support of X . Suppose also that
the marginal distribution for Y can be characterized by a density function p(y ) = E[p(Y |x)],
where the expectation is over possible realizations of x, and p(y ) > 0 for all y in the support
of Y . Then we can measure the degree to which knowing the realization of x changes the
distribution that one can expect y to be drawn from by the Kullback-Liebler divergence (or
relative entropy) of the conditional distribution p(y |x) relative to the marginal distribution
p(y ), defined as
                                                          p(y |x)
                            DKL (p(·|x)||p(·))  E log               0,                       (D.6)
                                                           p( y )


                                                  57
where the expectation is over possible realizations of y , and this quantity is a function of
the particular realization x.58 The mutual information I (X ; Y ) can then be defined as the
mean value of this expression,

                                  I (X ; Y )  E[DKL (p(·|x)||p(·))],                                  (D.7)

where the expectation is now over possible realization of x, and the mutual information is
also necessarily non-negative.59
    This definition of the mutual information has the attractive feature of being independent
of the coordinates used to parameterize the realizations of the variable Y . Suppose that
we write y = (z ), where (·) is an invertible smooth coordinate transformation between
two Euclidean spaces of the same dimension. Then corresponding to the conditional density
p(y |x) for any x, there will be a corresponding density function p
                                                                  ~(z |x) for the random variable
Z (which is just the variable Y described using the alternative coordinate system), such that
~(z |x) = p((z )|x) · D(z ) for each z , where D(z ) is the Jacobian matrix of the coordinate
p
transformation, evaluated at z . It follows that for any z in the support of Z and any x in
the support of X ,
                                        p((z )|x)   p~(z |x)
                                                  =          ,
                                         p((z ))      p
                                                      ~(z )
so that
                               DKL (p(·|x)||p(·)) = DKL (~
                                                         p(·|x)||p
                                                                 ~(·))
for all x. We thus find that the mutual information I (X ; Y ) will be the same as I (X ; Z ):
it is unaffected by a change in the coordinates used to parameterize Y .60
     We can similarly define the mutual information in a case in which the support of Y is
not the entire Euclidean space, because of the existence of redundant coordinates in the
parameterization of realizations y . Suppose that all vectors y in the support of Y are of the
form y = (z ), where (·) is a smooth embedding of some lower-dimensional Euclidean space
(the support of Z ) into a higher-dimensional Euclidean space. Then the information about
the possible realizations of y contained in a realization of x is given by the information that
x contains about the possible realizations of z . If the joint distribution of X and Z is such
that we can define conditional density functions p      ~(z |x), with p
                                                                      ~(z |x) > 0 for all z and x, and
a marginal density function p    ~(z ) > 0 for all z , then we can define the mutual information
between X and Z using (D.7) as above. Since mutual information should be independent of
the coordinates used to parameterize the variables, we can use the value of I (X ; Z ) as our
definition of I (X ; Y ) in this case as well (even though expression (D.6) is not defined in this
case).
     In the case of interest in this paper, X and Y are variables with a joint distribution
that is multivariate Gaussian. Let us consider first the generic case in which the conditional
  58
     The value of this quantity is necessarily non-negative because of Jensen's inequality, owing to the con-
cavity of the logarithm.
  59
     Note that this definition -- rather than the one often given in terms of the average reduction in the
entropy of Y from observing X -- has the advantage of remaining well-defined even when the random
variable Y has a continuous distribution. See Cover and Thomas (2006) for further discussion.
  60
     It is equally unaffected by a change in the coordinates used to parameterize X , though we need not show
this here.


                                                     58
variance-covariance matrix var[Y |x] is of full rank. (Note that this matrix will be independent
of the realization of x, and so can be written var[Y |X ], to emphasize that only the parameters
of the joint distribution matter.) In this case var[Y ] is of full rank as well, and for any x and
y , the ratio of the density functions satisfies
      p(y |x)    1    det(var[Y |x])
log           = - log
       p( y )    2     det(var[Y ])
                   1                                        1
                 - (y - E[y |x]) var[Y |x]-1 (y - E[y |x]) + (y - E[y ]) var[Y ]-1 (y - E[y ]).
                   2                                        2
Hence for any x, we have
                                          1    det(var[Y |x])
                               DKL (x) = - log                ,
                                          2     det(var[Y ])
and since this will be independent of the realization of x, we similarly will have
                                            1    det(var[Y |X ])
                              I (X ; Y ) = - log                 .                          (D.8)
                                            2     det(var[Y ])

    One case in which var[Y |x] will not be of full rank is if y = U z for some matrix U , where
z is a random vector of lower dimension than that of y . (In this case, the rank of var[Y |x]
cannot be greater than the rank of var[Z |x], which is at most the dimension of z .) Let us
suppose that the rank of U is equal to the dimension of z , so that any vector y = U z is
associated with exactly one vector z . In such a case we can, as discussed above, define the
mutual information between X and Y to equal the mutual information between X and Z . If
var[Z |x] is of full rank, then we can use the calculations of the previous paragraph to show
that
                                                     1    det(var[Z |X ])
                         I (X ; Y ) = I (X ; Z ) = - log                  .                (D.9)
                                                     2      det(var[Z ])
    We turn now to the calculation of the mutual information between the reduced cognitive
state s
      ¯t and the memory state m ¯ t+1 , in the case of a law of motion of the form (C.4) for the
memory state. We first consider the case in which Xt is of full rank (which, as noted in the
text, will be true except when the memory state mt is completely uninformative). If      ¯ t and
    ¯
I - t are also both matrices of full rank, then

                                ¯ t+1 |s
                            var[m      ¯t ] =               ¯     ¯
                                              ¯ ,t+1 = (I - t )Xt t


will be of full rank, and

                            var[m
                                ¯ t+1 ] = ¯ t Xt ¯ +             ¯
                                                     ¯ ,t+1 = Xt t
                                                  t

will be of full rank as well. We can then apply (D.8) to obtain

                          1    det[(I - ¯ t )Xt ¯]     1
                    It = - log                   t
                                                   = -   log det(I - ¯ t ),                (D.10)
                          2        det[Xt  ¯ t]        2

in conformity with equation (2.12) in the text.

                                               59
     In the case that Xt is of full rank, but    ¯ t is varied so that one of its eigenvalues approaches
                          ¯
1 (meaning that I - t approaches a singular matrix, while the determinant of                    ¯ t remains
bounded away from zero), the value of It implied by (D.10) grows without bound. It thus
makes sense to assign a value of + to the mutual information in the case that                   ¯ t is of full
               ¯
rank but I - t is not. Note that in this case there is a linear combination of the elements of
s
¯t that is revealed with perfect precision by the memory state (since              ¯ ,t+1 will be singular),
while this linear combination is a continuous random variable with positive variance (since
Xt is of full rank). This is not consistent with any finite value for the mutual information
(and so cannot represent a feasible memory structure).
     Suppose instead that while Xt is of full rank,           ¯ t is only of rank one. In this case, we
have shown above that          ¯ t must be of the form (D.2), as a consequence of which                  ¯ ,t+1
must be given by (D.3). In this case, the memory state can be represented in the form
m¯ t+1 = Xt vt · m
                 ~ t+1 , where m  ~ t+1 is a scalar random variable with law of motion (D.4). This
implies that var[m  ~ t+1 |st ] = var[~ t+1 ] = t (1 - t ), while var[m    ~ t+1 ] = t . In the case that
0 < t < 1, we can then apply (D.9) to show that

                                   1    t (1 - t )    1
                             It = - log            = - log(1 - t ),                                    (D.11)
                                   2        t         2

Since in this case, det(I -       ^ t ) = det(I - t vt vt ) = 1 - t , result (D.11) is again just what
(D.10) would imply, so that (D.10) continues to be correct even though               ¯ t is singular.
    If we consider a sequence of matrices of this kind in which t approaches 1, the mutual
information (D.11) grows without bound. Thus we can assign the value + to It in the case
that  ¯ t is a matrix of rank one with t = 1. Indeed, in this case, the memory state reveals
with perfect precision the value of vt s    ¯t , a continuous random variable with positive variance
(under the assumption that Xt is of full rank); but this is not possible in the case of any
finite bound on mutual information. Hence (D.10) can be applied to this case as well.
    Suppose instead that Xt is of full rank, but         ¯ t = 0. In this case, the distribution of m   ¯ t+1
is independent of the value of st+1 , and the mutual information between these two variables
must be zero. This is also what (D.10) would imply, so that (D.10) is correct in this case as
well.
    Finally, consider the case in which Xt = X0 , the only possible case in which Xt is not
of full rank. In this case, we have defined L(X0 ) to consist only of matrices of the form
(D.2), with the vector vt given by (2.20). If t = 0, then the entire matrix               ¯ t = 0, and the
argument in the previous paragraph again applies. Suppose instead that t > 0. Just as in
the discussion above of the case of a singular transition matrix, the memory state can be
represented by a scalar state variable m      ~ t+1 with law of motion (D.4), and we can apply (D.9)
to show that It will be given by (D.11). Again this is just what (D.10) would imply, so that
(D.10) also yields the correct conclusion when Xt is a singular matrix.
    Thus in all cases, (D.10) applies, and the value of It depends only on the choice of the
transition matrix   ¯ t . It follows that for any sequence of transition matrices {      ¯ t }  Lseq , there
will be uniquely defined sequences {M SEt , It }, allowing the objective (1.6) to be evaluated.




                                                      60
E      Recursive Determination of the Optimal Memory
       Structure
We have shown in the text how the optimal memory structure can be characterized if we
                                 2
can find the value function V (^
                               t   ) that satisfies the Bellman equation
                       t
                    V (^ 2
                           ) =       min           [ 2
                                                    ^t + c(I (¯ t )) + V (f (^
                                                                             t 2
                                                                                 , t , vt ))].       (E.1)
                                 ¯ t L(X (^
                                          t 2 ))


Here we establish some properties of the solution to the optimization problem on the right-
hand side of (E.1) for an arbitrary function V  F ., which we can then be used to establish
                                      2
properties of the value function V (^
                                    t   ) that solves this equation, and properties of the optimal
memory structure.

E.1      Monotonicity of the value function
We first show that, for any function V that may be assumed in the problem on the right-
hand side of (E.1), the minimum achievable value of the right-hand side is a monotonically
                        2
increasing function of ^t . This in turn implies that the value function (which must satisfy
(E.1)) must be a monotonically increasing function of its argument.
    Fix any value function V to be used in the problem on the right-hand side of (E.1), and
                                                   2   2
consider any two possible degrees of uncertainty  ^a ,^b , satisfying
                                                2    2  2
                                            0  ^a < ^b  0 .                                          (E.2)
Let  ¯t =    ¯ b be some element of L(X (^ b 2
                                               )), and thus a feasible memory structure when
  2
^t =   2
      ^b , and let us further suppose that I ( ¯ b ) < , as must be true of an optimal memory
structure. We wish to show that we can choose a transition matrix      ¯ a  L(X (^
                                                                                 a 2
                                                                                     )) such that
                                                2 ¯           2 ¯
                                           f (^
                                              a  , a ) = f (^
                                                            b  , b ),                                (E.3)
and in addition
                                                   I (¯ a )  I (¯ b ).                               (E.4)
                                                                2
That is, in the case of the smaller degree of uncertainty      ^a in the cognitive state in period t, it
is possible to choose a memory structure that implies exactly the same degree of uncertainty
                                                       2
in period t + 1, and hence the same value for V (^    t +1 ), at no greater an information cost, and
thus it is possible to achieve a strictly lower value for the right-hand side of (E.1).
    If we can show this for an arbitrary transition matrix        ¯ b  L(X (^   b 2
                                                                                    )), then it is also true
       ¯
when b is the transition matrix associated with the optimal memory structure (the solution
                                                              2      2
to the problem on the right-hand side of (E.1)) when         ^t =  ^b  . This implies that it is possible
                                                                          2      2
to achieve a lower value for the right-hand side of (E.1) when           ^t =   ^a   than it is possible to
                 2     2                                                    2   2
achieve when    ^t =  ^b . Since this must be true for any values of      ^a , ^b consistent with (E.2),
                                                                                           2
the right-hand side of (E.1) defines a monotonically increasing function of               ^t .
    To show that such a construction is always possible, let us first consider the case in which
  2     2
^b  =  ^0 , so that the memory state mt is completely uninformative in case b. In this case,
the assumption that     ¯ b  L(X (^ b 2
                                        )) = L(X0 ) requires that

                                                     ¯ b = b ww
                                                     
                                                             ww
                                                           61
for some 0  b < 1.61 In this case, the memory structure for the following period is
equivalent to one in which there is a univariate memory state
                                   b
                        m
                        ~b =               y +
                                     2 )1/2 t
                                              ~b,            ~ b  N (0, b (1 - b )).
                                                             
                                ( + y

The implied uncertainty in the following period (given the memory state, but before yt+1 is
observed) is then given by
                                                      2
                                      t+1 = 0 - b ( + y )ww .                                   (E.5)

    Now let s
            ¯a be the reduced cognitive state in period t, in the case of a more informative
                                                                   2                    2
memory structure that implies the lower degree of uncertainty    ^a  , and let Xa  X (^
                                                                                      a   ) be
the variance of this random vector. In this case, we can choose a memory structure for the
following period defined by the transition matrix

                                             ¯ a = b Xa e2 e2
                                             
                                                        + y   2


where e2  [0 1] . This is a matrix of the form (D.2), and hence an element of L(Xa ).
Because  ¯ a is singular, the specified memory structure is equivalent to one in which there is
a univariate memory state
                                      e2 s
                                         ¯a
                       m
                       ~ a = b                   +~a,        ~ a  N (0, b (1 - b )).
                                                             
                                  (e2 Xa e2 )1/2
       But this means that
                                   b
                       m
                       ~a =                y +
                                     2 )1/2 t
                                              ~a,            ~ a  N (0, b (1 - b )).
                                                             
                                ( + y

Hence the joint distribution of (m
                                 ~ a , xt+1 ) is identical to the joint distribution of (m
                                                                                         ~ b , xt+1 ),
and the implied uncertainty in the following period given this memory structure is again
                                       2
given by (E.5). Hence the value of    ^t+1 implied by memory structure a is the same as that
implied by memory structure b. This establishes condition (E.3). Moreover, for both memory
structures we have the same mutual information,

                                   I (¯ a ) = I (¯ b ) = - 1 log(1 - b ).
                                                           2
This establishes condition (E.4). Hence the value of the right-hand side of (E.1) must be
              2     2
lower when   ^t =  ^a .
                                                                2       2
   Let us next consider the less trivial case in which 0 <     ^b  <   ^0 . Let s
                                                                                ¯b be the reduced
                                                                   2                     2
cognitive state in period t that implies a degree of uncertainty  ^b , and let Xb  X (^ b  ) be the
variance of this random vector. Let the optimal memory structure for the following period
(the solution to the problem on the right-hand side of (E.1)) in this case be

                                             m
                                             ¯b = ¯ bs
                                                     ¯b + ¯b,                                   (E.6)
  61
       The upper bound is required in order to satisfy the assumption that I (¯ b ) < .


                                                      62
where
                          ¯ b  L(Xb ),
                                              ¯ b  N (0, (I - 
                                                              ¯ b )Xb ¯ ).
                                                                       b

The implied uncertainty in the following period will then be given by

                                       t+1 = 0 - Xb ¯ b.                                         (E.7)

  Let us consider the memory structure for cognitive state a defined by the transition
matrix
                                 ¯a = 
                                        ¯ b -1 ,                                (E.8)
where  is the invertible matrix defined in (C.5), and

                                                    0
                                                             ,
                                                   0 1

where 0 <  < 1 is the quantity
                                                   2   2
                                                  ^0 -^b
                                                   2
                                                         .
                                                  ^0 - 2
                                                      ^a
Note that  is a diagonal matrix, with the property that
                                       a = X
                                       X   a = X
                                               b,

using the notation X i  X     (^i2
                                   ) for i = a, b, where X  (^
                                                             t 2
                                                                 ) is the function defined in (C.6). It
is first necessary to verify that    ¯ a  L(Xa ), so that this matrix defines a possible memory
structure.
     We first show that  ¯ a X a = Xa   ¯ a . Definition (E.8) implies that

                                      ¯ a Xa =
                                                  ¯ b -1 Xa
                                                  
                                             =    ¯ b X
                                                           a
                                             =    ¯ b X
                                                       b
                                             =    ¯ b Xb .
                                                  

The fact that   ¯ b  L(Xb ) implies that ¯ b Xb must be a symmetric matrix; hence ¯ a Xa , which
is the same matrix, must also be symmetric. Thus       ¯ a X a = Xa ¯ a.
                                          ¯      ¯
    Next, we must also show that (I - a )Xa a is a p.s.d. matrix. We first note that I - 
is a diagonal matrix with non-negative elements on the diagonal; it follows that (I - )X     b is
also a diagonal matrix with non-negative elements on the diagonal, and hence p.s.d. From
this it follows that
         ¯ b  · (I - )X
                      b ·  ¯b =         ¯ b (X   b - X    a )     ¯
                                                                    b
                              =         ¯            ¯
                                        b (Xb  )b - (b -1 )(X  ¯             a  )( ¯ b -1 )
                              =         ¯ b Xb 
                                                ¯ -    ¯ a Xa ¯
                                                  b             a
                              =               ¯      ¯      ¯
                                        ( X a a - a Xa a ) - ( Xb       ¯ -   ¯ b Xb ¯ )
                                                                         b             b
                              =                 ¯      ¯
                                        (I - a )Xa a - (I - b )Xb b   ¯   ¯


                                                  63
must be p.s.d. as well. But since the fact that   ¯ b  L(Xb ) implies that (I - ¯ b )Xb ¯ must
                                                                                         b
                                ¯     ¯
be p.s.d., it follows that (I - a )Xa a can be expressed as the sum of two p.s.d. matrices,
and so must also be p.s.d. This verifies the second of the conditions required in order to
show that   ¯ a  L(Xa ).
     Thus if s
             ¯a is a reduced cognitive state for period t that implies a degree of uncertainty
 2
^a , a possible memory structure for the following period is

                                          m
                                          ¯a = ¯ as
                                                  ¯a + ¯a,                                           (E.9)

where the transition matrix ¯ a is defined in (E.8), and

                                     ¯ a  N (0, (I - 
                                                     ¯ a ) Xa ¯ ).
                                                               a

The implied uncertainty in the following period will then be given by

                                         t+1 = 0 - Xa ¯ .
                                                       a

                                                                                                 2
This latter matrix is the same as the one in (E.7); it follows that the implied value of       ^t +1
is also the same as for the memory structure (E.6). Thus we have shown that in the case of
                                      2
the smaller degree of uncertainty   ^a  , it is possible to choose a memory structure that implies
exactly the same degree of uncertainty in period t + 1 as when the degree of uncertainty in
                                                2
period t is given by the larger quantity      ^b  .
    It remains to be shown that memory structure (E.9) involves no greater information
cost than memory structure (E.6). Consider first the case in which the memory state m         ¯ b is
non-degenerate, in the sense that var[m                ¯
                                            ¯ b ] = Xb b is non-singular. It follows that the same
must be true of memory state m  ¯ a . Then for either of the two memory structures i = a, b just
discussed, (D.10) implies that the mutual information will be given by

                                        1    det[(I - ¯ i )Xi ¯ i]
                                  It = - log             ¯]        .
                                        2        det[Xi    i

We have shown above that the value of the denominator in this expression is the same for
i = a, b (and under the assumption that Xb    ¯ is non-singular, it must be positive). Hence
                                               b
the relative size of the two mutual informations depends on the relative size of the numerator
in the two cases. But we have shown above that (I -    ¯ a )Xa ¯ can be expressed as the sum
                                                                a
         ¯      ¯
of (I - b )Xb b plus a p.s.d. matrix. Since both of these matrices are also p.s.d., their
determinants satisfy

                          det[(I - ¯ a )Xa ¯ a ]  det[(I - ¯ b ) Xb ¯ ] > 0,
                                                                     b

where the final inequality is necessary in order for memory structure b to have a finite
information cost. It follows that condition (E.4) must hold in this case.
    Now suppose instead that var[m        ¯ b ] is a singular matrix. In the case that the matrix
                             ¯
is zero in all elements, b = 0, and so (E.8) implies that                 ¯ a = 0 as well. In this case,
det(I -  ¯ a ) = det(I -   ¯ b ) = 1, so that I ( ¯ a ) = I (¯ b ) = 0, and (E.4) is satisfied in this case
as well. Thus we need only consider further the case in which var[m             ¯ b ] is of rank one, which
requires that   ¯ b be of rank one as well.

                                                    64
       In this case, we can write
                                                      ¯ b = b Xb vb v ,
                                                                     b

where 0 < b < 162 and vb is a vector such that vb Xb vb = 1. All columns of  ¯ b are multiples
of the vector Xb vb , and as a consequence the unique non-null right eigenvector of      ¯ b is
given by Xb vb , with the associated eigenvalue b . Alternatively, using the orthogonalized
representation of the cognitive state introduced in section C.4, we can write

                                                   -1 ¯ b  = b X
                                                               bvb v
                                                                   b ,

where we define v b   vb , and note that v    bv
                                            b X  b = 1.
    Then (E.8) implies that the columns of    ¯ a must also all be multiples of the vector Xb vb .
It follows that  ¯ a must also be singular, and that its unique non-null eigenvector must be
Xb vb , with an associated eigenvalue

                                          a =           b vb -1 (Xb vb )
                                            =           b v
                                                          b X bv b
                                                               1/2 
                                            =           b (vb  )Xb (1/2 vb )
                                                        b v  
                                                          b Xb v
                                                               b = b .

Thus we must have

                          det(I - ¯ a ) = (1 - a )  (1 - b ) = det(I - ¯ b ),

from which it follows that (E.4) must hold in this case as well.
                                              2    2
   Thus we have shown that whenever         ^a  ,^b  satisfy (E.2), for any memory structure for
case b with a finite information cost, it is possible to choose a memory stucture for case
a satisfying both (E.3) and (E.4). This means that it must be possible to achieve a lower
                                                 2      2              2
value for the right-hand side of (E.1) when     ^t  = ^a  than when   ^b . This in turn implies that
                                                                                     2
the right-hand side of (E.1) defines a monotonically increasing function of         ^t , regardless of
                                 2
the nature of the function V (^ t+1 ) that is assumed in this optimization problem. Hence the
                     2
value function V (^t   ) defined by (E.1) must be a monotonically increasing function of its
argument.

E.2         Optimality of a unidimensional memory state
Here we establish, as stated in the text, that the matrix ¯ t that solves the problem

                                     min           I (¯ t)   s.t. f (^
                                                                     t 2 ¯
                                                                        , t )   2
                                                                               ^t
                                 ¯ t L(X (^ 2 ))                                 +1 ,          (E.10)
                                          t

                       2   2
for given values of (^
                     t   ,^t+1 ) is necessarily at most of rank one. As explained in the text, we
need only consider the case in which        2
                                           ^t <  ^02
                                                     . Given a matrix ¯ t of rank two that satisfies
the constraint in (E.10), we wish to show that we can choose an alternative transition matrix
of at most rank one, that also satisfies the constraint, but which achieves a lower value of
I (¯ t ).
  62
       Again, the upper bound is required in order for I (¯ b ) to be finite.


                                                             65
    We first note that when     2
                               ^t < ^02
                                        , X (^
                                             t 2
                                                 ) is non-singular. Under the hypothesis that   ¯t
                                    ¯
is non-singular, it follows that Xt t is non-singular as well (where we now simply write Xt
for X (^
       t 2
           )), and hence positive definite. Similarly,   ¯ t Xt ¯ t must be non-singular and hence
positive definite.
    Then let the alternative transition matrix be given by
                                         ¯1
                                          t
                                            D
                                              = t Xt vt vt ,                                   (E.11)

with
                            t+1 ¯ t Xt ¯ t+1                       ¯ t t+1
                                         t
                     t =             ¯ t t+1 ,     vt =         ¯ t Xt ¯ t t+1 )1/2 ,
                            t+1 Xt                         (t+1 
where t+1  e1 - 1,t+1 c is the vector introduced in (2.16), and let the matrix              ¯ ,t+1
be correspondingly modified in the way specified by (2.9). The fact that Xt     ¯ is positive
                                                                                   t
definite implies that the denominator of the expression for t is necessarily positive, so that
this quantity is well-defined. Similarly, the fact that  ¯ t Xt ¯ is positive definite implies
                                                                 t
that the denominator of the expression for vt is necessarily positive, so that this vector is
well-defined as well.
    In addition, the fact that (by assumption)  ¯ t  L(Xt ) implies that (I -   ¯ t ) Xt ¯ must
                                                                                          t
be p.s.d. From this it follows that

                                    t+1 (I - ¯ t ) Xt ¯ t+1  0,
                                                       t

and hence that
                                t+1 Xt ¯ t+1         ¯     ¯
                                        t        t+1 t Xt t t+1 > 0,

where the final inequality follows from the fact that        ¯ t Xt ¯ t is positive definite. Thus the
proposed definition of t satisfies 0 < t  1. One also observes from the definition of vt that
vt Xt vt = 1. These conditions suffice to establish that the alternative transition matrix         ¯ 1D
                                                                                                     t
is also an element of L(Xt ). That is, it represents a feasible memory structure for period t,
                        2
given the value of    ^t  .
    This alternative transition matrix corresponds to a memory structure in which m            ¯ t+1 =
Xt v t m
       ~ t+1 , where m
                     ~ t+1 is the unidimensional memory state with law of motion (2.19). From
this it follows that
                             t+1 m
                                 ¯ t+1 = t t+1 Xt vt vt s
                                                        ¯t + t+1 Xt vt    ~ t+1
will be a normally distributed random variable, with conditional first and second moments
given by

                        ¯ t+1 |st ] = t t+1 Xt vt vt s
                  E[t+1 m                            ¯t
                                          ¯      ¯
                                       t Xt t t+1 t+1 Xt  ¯ t t+1 · t+1   ¯ ts
                                                                             ¯t
                                    = t+1       ¯ t t+1     ¯ t Xt ¯ t t+1
                                       t+1 Xt           t+1 
                                    = t+1 ¯ ts
                                             ¯t




                                                  66
and

                         ¯ t+1 |st ] = t (1 - t )(t+1 Xt vt )2
                 var[t+1 m
                                                 t+1   ¯ t Xt  ¯ t+1 ( Xt    ¯ t+1 )2
                                                                 t      t+1   t
                                     = (1 - t )              ¯             ¯   ¯ t t+1
                                                   t+1 Xt t t+1 t+1 t Xt 
                                     = (1 - t )t+1 Xt      ¯ t t+1
                                     = t+1 Xt  ¯ t+1 -             ¯    ¯
                                                 t             t+1 t Xt t t+1
                                     = t+1 [(I -   ¯ t )Xt  ¯ t ]t+1
                                     = t+1   ¯ t+1 t+1 .


    These are the same conditional mean and variance as in the case of the memory structure
specified by the transition matrix        ¯ t . Since the optimal estimate µ
                                                                           ^t+1 depends on mt+1
only through the value of t+1 m     ¯ t+1 (from equation (2.16)), it follows that the conditional
             ^t+1 |st , yt+1 will be the same under the alternative memory structure. This in
distribution µ
turn implies that the variance of µ    ^t+1 will be the same, and hence that
                                       2
                                      ^t+1 =  - var[^
                                                    µt+1 ]

will be the same. Thus      ¯ 1D also satisfies the constraint in (E.10).
                              t
    Next we show that I (       ¯ 1D ) must be lower than I ( ¯ t ). Let u and u be the two left
                                  t                                       1     2
eigenvectors of ¯ t , with associated eigenvalues µ1 and µ2 respectively, and let the eigenvectors
be normalized so that ui Xt ui = 1 for i = 1, 2. The corresponding right eigenvectors must
then be Xt u1 and Xt u2 respectively. Thus we have
                              ¯ t Xt ui = µi Xt ui ,
                                                         ui ¯ t = µi u ,
                                                                      i

for i = 1, 2, and
                           u1 Xt u1 = u2 Xt u2 = 1,          u1 Xt u2 = 0.
    The vector t+1 introduced in (2.16) can be written as a linear combination of the two
left eigenvectors,
                                  t+1 = 1 u1 + 2 u2 ,
for some coefficients 1 , 2 . This implies that

                                  t+1 Xt ¯ t+1 = 2 µ1 + 2 µ2 ,
                                          t      1      2

                                t+1 ¯ t Xt ¯ t t+1 = 1
                                                     2 2     2 2
                                                       µ1 +  2 µ2 ,
and hence that
                                        2                 2
                                      1   µ1            2   µ2
                            t =    2         2
                                               µ 1 + 2         2
                                                                    µ2 .
                                   1 µ1 + 2 µ2       1 µ1 + 2    µ2
Thus we see that t must be a convex combination of µ1 and µ2 .
   The fact that  ¯ t  L(Xt ) requires that both eigenvalues satisfy 0  µi  1, and the
                ¯
assumption that t is non-singular further requires that µi > 0 for both. Thus we must have

                                    1 - µi > (1 - µ1 )(1 - µ2 )

                                                  67
for both i = 1, 2. Since t is a convex combination of µ1 and µ2 , it follows that

                                   1 - t > (1 - µ1 )(1 - µ2 ).

Thus
                det(I - ¯1D
                         t ) = 1 - t > (1 - µ1 )(1 - µ2 ) = det(I - t ).
                                                                         ¯

Results (D.10) and (D.11) then imply that I (¯ 1D ) < I (¯ t ).
                                               t
         ¯
   Thus t cannot be the solution to the optimization problem (E.10). Since this argument
can be made in the case of any matrix ¯ t  L(Xt ) that is of full rank, we conclude that the
optimal transition matrix can be at most of rank one.


F      Numerical Solution for the Optimal Memory
       Structure
Here we provide further details of the calculations reported in section 3 of the main text.

F.1    Dynamics of uncertainty given the path of {t }
We begin by discussing our approach to numerical solution for the law of motion t+1 =
(t ; t ) for the scaled uncertainty measure {t }, given a path for the memory-sensitivity
coefficient {t }. It is useful to write the equations of the model in terms of scale-invariant
matrices
                                X~ t  y  -2
                                            Xt ,   ~ t  y
                                                           -2
                                                              t .
                                                                ~t  y vt , so that the rescaled
We can corresponding rescale the direction vector vt , defining v
direction vector satisfies the normalization

                                          v  ~tv
                                          ~t X ~t = 1.                                   (F.1)

The memory structure each period is then specified by the values of t and v  ~t .
    Given the memory structure (t , v     ~t ) for each t  0, the evolution of the variables
     ~t, 
{t , X   ~ t+1 } is determined by the difference equations



                               X~t = ~ 0 - t 0
                                             0 0
                             ~ t+1 = 
                                     ~ 0 - t (X
                                              ~tv    ~tv
                                                ~t )(X ~t )
                                                      (e1  ~ t+1 c)2
                             t+1 = e1 ~ t+1 e1 -
                                                    c~ t+1 c + 1 - 2

starting from initial conditions

                              K           ~ 0  1 0 =          K  K
                      0 =         ,                                  .
                             K +1              y2             K K +1


                                               68
    Now let the path {t } be given, satisfying 0  t  1 for each t. we wish to determine the
associated optimal sequence for the direction vectors {v  ~t }, and hence the implied dynamics
for the other variables. To do this, we need to express the law of motion for t+1 as a function
of t , t , and v
               ~t ; we can then minimize over v
                                              ~t for given values of the other two arguments.
    Let us introduce the additional notation e2 for the vector (0 1) . Then from c = (1 -
)e1 + e2 , we get
                 ~ t+1 c = (1 - )2 e  ~                    ~         2    ~
                c                   1 t+1 e1 + 2(1 - )e1 t+1 e2 +  e2 t+1 e2

                         = (1 - )2 ~ 11,t+1 + 2(1 - )~ 12,t+1 + 2 e2 ~ 22,t+1

Similarly, e1 ~ t+1 c = (1 - )~ 11,t+1 + ~ 12,t+1 . Using these two relations, we get

                                                            2
                                               e1 ~ t+1 c
                     t+1 = e1 ~ t+1 e1 -
                                          c  ~ t+1 c + 1 - 2
                            2 ~ 11,t+1 ~ 22,t+1 - 2    ~ 12,t+1 + (1 - 2 )~ 11,t+1
                          =
                                              c  ~ t+1 c + 1 - 2
                            2 det(  ~ t+1 ) + (1 - 2 )e      ~
                                                           1 t+1 e1
                          =
                                     c ~ t+1 c + 1 - 2

    We wish to rearrange the nominator and the denominator as a variant of Rayleigh quo-
tient form. It is straightforward to rewrite the following terms:

                           e1 ~ t+1 e1 = e1 ~ 0 e1 - v    ~ t e1 e1 X
                                                     ~t t X         ~t v
                                                                       ~t

                             c~ t+1 c = c ~ 0c - v    ~ t cc X
                                                 ~t t X      ~t v
                                                                ~t

As for det( ~ t+1 ), we use the identity relation that for any symmetric matrix A and a vector
v , det(A - vv ) = det(A) - v adj (A)v . That is,

                    det(~ t+1 ) = det         ~tv
                                      ~ 0 - t X ~t               ~tv
                                                                 X ~t

                                = det(          ~tv
                                      ~ 0 ) - t X ~t            adj (     ~tv
                                                                     ~ 0) X ~t

                                = det(          ~tv
                                      ~ 0 ) - t X ~t            det(~ 0 )     ~tv
                                                                         ~ -1 X ~t
                                                                           0

                                = det(~ 0) - v
                                             ~t t det(~ 0 )X
                                                           ~t~- 1 ~
                                                              0 Xt v~t

   Thus the law of motion for t+1 can be written in the form

                                                 q1 - v
                                                      ~t Z1,t v
                                                              ~t
                                       t+1 =                                             (F.2)
                                                 q2 - v
                                                      ~t Z2,t v
                                                              ~t




                                                  69
where the coefficients are given by

               q1 = 2 det(~ 0 ) + (1 - 2 )e1 ~ 0 e1
                          ~ t where Z11,t = t X
             Z1,t = Z11,t X                   ~ t 2 det(~ 0 )~ -1 + (1 - 2 )e1 e
                                                               0                1

              q2 = c ~ 0 c + 1 - 2
                          ~ t where Z22,t = t X
             Z2,t = Z22,t X                   ~ t cc

Here the qi are constants and the matrices Zi,t depend only on the values of t and t ; the
form (F.2) thus shows how the objective function (given on the right-hand side) depends on
the vector v
           ~t .
   In any period t, given values for t and t , the vector v
                                                          ~t must be chosen to minimize the
objective (F.2) subject to the constraint (F.1). The Lagrangian problem is then given by

                                             q1 - v
                                                  ~t Z1,t v
                                                          ~t       ~tv
                               min L =                       + (~
                                                                vt X ~t - 1)
                                  v
                                  ~t ,       q2 - v
                                                  ~t Z2,t v
                                                          ~t

The first order conditions for this problem are
                              2
   L              1                                                                               ~tv
      =                                   ~t (q2 - v
                                  [-2Z1,t v        ~t Z2,t v
                                                           ~t ) + 2Z2,t v
                                                                        ~t (q1 - v
                                                                                 ~t Z1,t v
                                                                                         ~t )] + 2X ~t = 0
   v
   ~t      q2 - v~t Z2,t v
                         ~t
   L        ~tv
      =v~t X  ~t - 1 = 0
   
   If we redefine the Lagrange multiplier as ~  (q2 - v
                                                      ~t Z2,t v
                                                              ~t ), we then get

                                             q1 - v
                                                  ~t Z1,t v
                                                          ~t              ~tv
                                   ~t -
                              Z1,t v                           Z2,t v
                                                                    ~t = ~X ~t .                        (F.3)
                                             q2 - v
                                                  ~t Z2,t v
                                                          ~t
                       
The optimal solution (~
                      vt ,~ ) should satisfy

                                         Z1,t v
                                              ~t
                                                 - L Z2,t v
                                                          ~t
                                                             =  ~tv
                                                              ~ X ~t


where                                                                 
                                                    q1 - (~
                                                          vt ) Z1,t v
                                                                    ~t
                                            L                         
                                                    q2 - (~
                                                          vt ) Z2,t v
                                                                    ~t
                                                                               
is the value of the optimized Lagrangian. Multiplying both sides of (F.3) by (~
                                                                              vt ) , we get

                                         ~ = (~
                                               
                                              vt ) (Z1,t - L Z2,t ) v
                                                                    ~t


                                                 
   The solution of the optimization problem is (~
                                                vt ,~ ). Note that the FOCs of the opti-
mization problem can be written as
                                          ~11,t - LZ
                                         (Z               ~tv
                                                   ~22,t )X ~t =  ~tv
                                                                 ~X ~t

where L is the minimum achievable value of t+1 . This relation shows that (,    ~ t v ) must be
                                                                              ~ X
                                                              ~11,t - LZ
an eigenvalue and associated right eigenvector of the matrix (Z        ~22,t ).

                                                        70
                                 t (K = 3)                                    t (K = 100)
               1.0                                              1.0

               0.8                                              0.8
                                              = 0.30
                                              = 0.60
               0.6                                              0.6
                                              = 0.80
               0.4                            = 0.90            0.4
                                              = 0.95
               0.2                            = 0.99            0.2
                                              = 1.00
               0.0                                              0.0
                     0       5           10          15               0       5          10   15
                                  Time                                            Time
Figure 13: The evolution of scaled uncertainty about µ as the number t of previous (imper-
fectly remembered) observations grows. Each panel corresponds to a particular value of K
(maintaining the assumption that  = 0, as in Figure 1). Each panel shows the evolution
for several different possible values of ¯ (color code is the same in both panels).

                                 
    We numerically find a pair (vt ,~ ) that satisfies the FOCs using the following algorithm.
(1) We begin by guessing a value for v ~. (2) Given this value, we evaluate the quantities

                                   ~1 - v
                                   q    ~Z~1,t v
                                               ~
                         L(~
                           v) =                  ,        ~(~
                                                            v) = v ~1,t - LZ
                                                                           ~2,t v
                                                                 ~ Z            ~.
                                   ~2 - v
                                   q      ~
                                        ~ Z2,t v
                                               ~

(3) Then we find a right eigenvector v^ of the matrix Z  ~11,t -L(~  v )Z~22,t , selecting the eigenvector
for which the associated eigenvalue    ^ is closer to (~  v ). (4) Finally, we update our guess v       ~,
replacing it with the normalized vector v such that v     ^=X    ~ t v . If Xt is invertible, there is a
unique such vector, given by v = (X  ~ t )-1 v
                                             ^. If instead Xt is singular, we use the first row of
the matrix equation to solve for v , so that

                                                               1
                                              v =         ^1 -X
                                                          v     ~ 11,t    .
                                                            X~ 12,t

                                                                                       
We then repeat steps (2)-(4), until our estimate of v converges to some value vt         (to an
                                                                              
acceptable tolerance). The corresponding value of  is given by  = (v ).
    Since the FOC allows multiple solutions, it is important to select the one that corresponds
to the global minimum. We therefore use a coarse global search to obtain an approximate
solution for v
             ~ first, and then use this approximate solution to initialize the algorithm de-
scribed above; this ensures that the solution obtained will in fact be the global minimum.
In this way, we obtain a numerical solution for t+1 = (t ; t ) for any values of t and t .
Note that this allows a complete solution for the model dynamics in the case of a fixed upper
bound   ¯ (the results reported in section 3.1).

                                                          71
             0.6          t(   = 0.4)                   0.6        t(   = 0.8)
                                         = 0.30
                                         = 0.60
             0.4                         = 0.80         0.4
                                         = 0.90
                                         = 0.95
             0.2                                        0.2
                                         = 0.99
                                         = 1.00

             0.0                                        0.0
                   0    10          20        30              0   10          20   30
                             Time                                      Time
Figure 14: The evolution of scaled uncertainty about µ as the number t of previous (imper-
fectly remembered) observations grows. Each panel corresponds to a particular value of 
(maintaining the assumption that K = 1, as in Figure 1). Each panel shows the evolution
for several different possible values of ¯ (color code is the same in both panels).


    Figure 1 in the main text shows the dynamics for {t } implied by this solution, for various
                   ¯ in the case that K = 1 and  = 0. Figure 13 shows how this graph would
possible values of ,
be different in the case of two larger values for K (but again assuming  = 0). A higher
value of K (greater prior uncertainty) implies a higher value for the initial value 0 of our
normalized measure of uncertainty (since 0 = K/(K + 1)). This means that the curves all
start higher, the larger the value of K . But the value of K also affects the long-run level of
uncertainty  , even though the initial condition becomes irrelevant in the long run. Except
when   ¯ = 1 (perfect memory), a higher value of K implies greater long-run uncertainty; and
when K is large (as illustrated in the right panel),  is large (not much below the degree
of uncertainty implied by the prior) except in the case of quite high values of . ¯
    Figure 14 similarly shows how Figure 1 would look in the case of two larger values of , but
again assuming K = 1. We see that for a given degree of prior uncertainty and a given bound
on memory precision, the rate at which uncertainty is reduced is slower when the external
state is more serially correlated. This is because there are effectively fewer independent
observations over a given number of periods when the state is serially correlated. In the
case of perfect memory (  ¯ = 1), this affects the speed of learning but not the long-run value
 = 0 that is eventually reached. Instead, when memory is imperfect, the long-run value
 is also higher when the state is more serially correlated; effectively, the limited number of
recent observations of the state that can be retained in memory reveal less about the value
of µ when the state is more serially correlated.




                                                   72
       1.0004                                RHS( ; 0)
                      = 0.2800
                      = 0.2805
                      = 0.2806
       1.0003


       1.0002


       1.0001


       1.0000

             0.0           0.2          0.4            0.6          0.8         1.0

Figure 15: The evolution of scaled uncertainty about µ as the number t of previous (im-
perfectly remembered) observations grows. The right panel shows the long-run value of
scaled uncertainty (to which t converges as t  ) as a function of the constraint on the
complexity of memory, parameterized by  ¯.


F.2    Solving for the value function V~ ( ) and policy function  ( )
       in the case of a linear information cost
In the case of a linear information cost (or any other cost function with a positive marginal
cost of increasing It ), it is necessary to solve the Bellman equation for the value function
~ ( ), in order to determine the optimal dynamics of {t }. Here we explain the methods used
V
to solve this problem in the case of a linear information cost (the results reported in section
3.2).
    Once we have solved for the function (t ; t ), as in the previous subsection, the Bellman
equation for the case of a linear information cost can be written
                                             ~
                                             
                    ~ (t ) = min
                    V                  t -                     ~ ((t ; t )) .
                                               log (1 - t ) +  V                         (F.4)
                             t [0,1]         2
We use the value function iteration algorithm to find the value function that is a fixed point
of this mapping.
     When iterating the mapping to update the value function, we use a grid search method
to find the optimal policy function, because the right-hand side of the Bellman equation is in
general a non-convex function of the policy variable t (as we illustrate in Figure 15 below).
We approximate the value function with Chebyshev polynomials. Once the value function
has converged, we can use our solution for V ~ ( ) to solve numerically for the policy function
 ( ), the solution to the minimization problem on the right-hand side of (F.4). This is the
function graphed (for several values of ~) in Figure 3.
     Figure 15 illustrates our comment about the possible non-convexity of the optimization
problem (F.4). Let RHS (t ; t ) be the function defined on the right-hand side of (F.4),

                                                 73
i.e., the objective of the minimization problem. The figure plots the value of RHS (; 0 ),
normalized by dividing by the positive constant RHS (0; 0 ) (so that a value of 1.0 on the
vertical axis means that RHS (; 0 ) is of exactly the same size as RHS (0; 0 )). This function
is shown for each of three slightly different values of , ~ assuming in each case that K = 10, as
in the right panel of Figure 7 in the text. In the case of each of these curves, a large dot (the
same color as the curve) indicates the global minimum of the function. A horizontal dashed
line (also the same color as the corresponding curve) indicates the minimum of RHS (; 0 )
-- and thus the value of V    ~ (0 ) -- again normalized by dividing by RHS (0 ).
     The figure shows that for values of     ~ in this range, RHS () is not a convex function of
. It is increasing for small enough values of , making the choice t = 0 a local minimum
in this case. (This is true for all values of   ~ greater than a critical value around 0.15, which
explains the existence of the horizontal segment of the connected black curve in the right
panel of Figure 7.) However, the function reaches a local maximum, and then decreases for
larger values of , as the degree to which a larger value of t reduces (0 ; t ) outweighs the
increase in the information cost. (A large enough value of K is required for this to occur.
A larger value of K increases the sensitivity of the value of (0 ; ) to the value of ; see
equation (F.5) below.) For even larger values of  (values approaching 1), further increases
in  increase the information cost term so sharply that RHS (; 0 ) is again decreasing in .
This means that there is a second local minimum of the objective function, at an interior
value of . Which of the two local minima represents the global minimum of the function
depends on parameter values.
     In the case illustrated in the figure, the interior local minimum achieves a lower value
of the objective than the choice t = 0, for all values of        ~ less than a critical value that is
slightly larger than 0.2805. (As shown in the figure, when        ~ = 0.2805, the interior minimum
achieves a value of the objective that is quite close to the value RHS (0; 0 ). However, the
value achieved remains slightly smaller: there is a (barely visible) green dashed line, just
below the blue dashed line at the normalized value 1.0.) But the normalized value of the
objective at the interior minimum increases as          ~ is increased, and for a value of    ~ only
slightly greater than 0.2805, the normalized value becomes greater than 1.0 (which is to
say, the interior local minimum is no longer the global minimum of the objective). When
this critical value of   ~ is passed, the optimal value  (0 ) jumps discontinuously from the
interior local minimum (which is a continuously decreasing function of         ~) to the value zero.
When this happens, the optimal long-run level for the normalized uncertainty measure 
increases discontinuously, from a value on the lower branch of the correspondence shown in
the right panel of Figure 7 to the value 0 = K/K + 1. For all values of          ~ higher than this,
it is optimal to choose a completely uninformative memory for all t, so that t = 0 for all
t, and hence t   = 0 .
     For larger values of  ~ than those considered in Figure 3, the optimal policy function  ( )
is equal to zero for all large enough (though still finite) values of  , as illustrated in Figure
16. Once    ~ is large enough for  (0 ) to equal zero, the optimal dynamics imply t = 0 for
all t, and hence  = 0 = K/K + 1, as shown in Figure 7.




                                                 74
                                                   *(       )
        1.0                                                                             = 0.25
                                                                                        = 0.30
                                                                                        = 0.35
        0.8                                                                             = 0.40
                                                                                        = 0.45
                                                                                        = 0.50
        0.6

        0.4

        0.2

        0.0
          0.0           0.1            0.2                      0.3               0.4     0.5

Figure 16: The optimal policy function  ( ), in the case of progressively larger values for
the information cost parameter ,~ under the assumption that K = 1,  = 0. Here we consider
values of ~ larger than those shown in Figure 3.


F.3     The case  = 0
Additional analytical results are possible in the case that  = 0 (the external state is an i.i.d.
random variable). In this case, the optimal choice for the direction vector is given by
                                                        1
                                        v
                                        ~t =                      e1 ;
                                                      ~ t e1
                                                   e1 X

that is, the optimal memory structure stores only a noisy record of µ    ^t , placing no weight
on the value of yt . This is optimal since information about the current state yt (apart from
the estimate µ^t , which takes into account the observation of yt ) is of no use in improving
estimates or decisions in periods  > t.
    As a consequence, the degree of uncertainty at the beginning of next period given the
memory choice t is given by

                              ~ t+1 = ~ 0 - t      1     ~ t e1          ~ t e1
                                                         X               X
                                                   ~
                                                e1 Xt e1
Hence the law of motion of t+1 = (t ; t ) is given by

                e ~ t+1 e         1                   1
      t+1 =               =1-             =1-                     (t ; t ).                      (F.5)
                ~
              e t+1 e + 1       ~
                              e t+1 e + 1     K + 1 - t (K - t )
   The value function satisfies a Bellman equation of the form
                                        ~
                    ~ (t ) = min  2 t -  log (1 - ) +  V
                    V                                  ~ ((t ; t ))) .
                              t         2


                                                   75
The first order condition with respect to t is
                            ~ 1
                                      ~ (t+1 ) (t ; t ) = 0.
                                    + V                                                   (F.6)
                            2 1 - t            t
And the envelope condition is

                                            ~ (t+1 ) (t ; t ) .
                             ~ (t ) =  2 +  V
                             V
                                                     t
 We can use these two conditions to derive an Euler equation for the dynamics of the scaled
uncertainty measure.
   Substituting the solution (F.5) for (t ; t ) and taking the derivative with respect to t ,
we can rewrite (F.6) as

                                 ~ 1                  -1
                   ~                       (t ; t )
                   V (t+1 ) = -
                                2 1 - t       t
                                 ~                                      -1
                                    1               (K - t )
                            =-            -
                                2 1 - t      (K + 1 - t (K - t ))2
                               ~ (K + 1 - t (K - t ))2
                               
                            =
                              2    (1 - t )(K - t )
                               ~
                                                 1
                            =                                       ,
                              2 (1 - t+1 ) (1 - (1 - t+1 )(1 + t ))

where the last equality is derived by again substituting the law of motion (F.5). It follows
that if t   in the long run, the stationary solution  must satisfy
                                          ~
                                 ~ ( ) = 
                                 V
                                                1
                                                    .                                     (F.7)
                                         2 (1 -  )2


   Next we rewrite (F.3), again taking the derivative of expression (F.5) for ~
                                                                              (t ; t ):

                                ~ (t+1 ) (t ; t )
                 ~ (t ) =  2 +  V
                 V
                                         t
                              ~ (t+1 )           t
                        = 2 + V
                                       (K + 1 - (K - t ))2
                              ~ (t+1 )      t
                        = 2 + V
                                       (1 - t+1 )-2
                              ~ (t+1 )(1 - t+1 )2 (K + 1)(1 - t+1 ) - 1 .
                        = 2 + V
                                                    (K - t )(1 - t+1 )

It follows that the stationary solution  must satisfy

                                ~ ( ) (1 -  ) [(K + 1)(1 -  ) - 1] .
                  ~ ( ) =  2 +  V
                  V                                                                       (F.8)
                                                K - 

                                             76
                                                  ~ ( ) given by (F.7) must also be the value
    Moreover, in a stationary solution, the value V
   ~
of V ( ) in (F.8). Using (F.7) to substitute for V ~ ( ) in (F.8), we obtain a condition that
must be satisfied by  in any stationary solution with an interior optimum (i.e., a stationary
solution in which 0 <  < K/(K + 1)):
                                                     2                      -1
                ~ = 2 3 (1 -  ) 2 1 -  (K + 1)(1 -  ) - (1 -  )
                                                                                 .     (F.9)
                               
                                                 K - 

This is the relationship between   ~ and  that is graphed as a connected black curve in
Figure 7. Note that for any value 0 <  < K/(K + 1), there is a unique       ~ > 0 consistent
with this relationship; but (as shown in the right panel of Figure 7) there may be multiple
solutions for  consistent with a given value of . ~


G     Predicted Values for the Quantitative Measures of
      Forecast Bias
Here we provide further explanation of the numerical results reported in section 4 of the
main text.

G.1     Long-run stationary fluctuations
From the definition of the univariate memory state m ~ t+1 = t vt s
                                                                  ¯t + t+1 , we can derive a
                                              ~ t . Using the subscript  for the long-run
law of motion for the univariate memory state m
stationary coefficients, we get

             m
             ~ t+1 =  v s
                        ¯t + ~ t+1
                             µ
                             ^t
                   =  v         +~ t+1
                             yt
                   =  [e1 v {(e1 - 1 c )mt + 1 yt } + (e2 v )yt ] + ~ t+1
                   =  [e1 v {(e1 - 1 c )X v m~ t + 1 yt } + (e2 v )yt ] + ~ t+1
                   = m m
                       ~ t + my yt + ~ t+1

where m   (e1 v ) (e1 - 1 c ) X v and my   (1 + e2 v ).
    In the long run, we can describe the evolution of the DM's cognitive state using the
following system of equations:

                                m
                                ~ t+1 = m m
                                          ~ t + my yt + ~ t+1
                                yt+1 = (1 - )µ + yt + y,t+1

Therefore, we can write it as a VAR(1) system with constant coefficients and Gaussian
innovation terms:
                    m
                    ~ t+1        0                        m
                                                          ~t        ~ t+1
                            =      µ + m my                     +
                    yt+1        1-     0                  yt        y,t+1



                                             77
    In the case of a fixed per-period bound on mutual information, we can compute the
impulse responses for the DM's estimate of µ and her one-quarter-ahead forecast of the
external state, as explained in section 3.3. Here we present additional figures, showing what
the impulse responses shown in Figures 8 and 9 in the text would be like in the case of
alternative values of  than the ones assumed in those figures. In Figures 17 and 18 shown
here, each panel corresponds to a different value of , and shows the responses for several
different possible values of ¯ . (As with Figures 8 and 9 in the main text, we here assume
that K = 1.)

Figure 17: Impulse responses of the DM's estimate of µ for alternative degrees of persistence
 of theFigure
        external   stateresponse
              2: Impulse process.of the DM's estimate of µ for alternative degree of persistence


                                       = 0.30
                                       = 0.60
                       = 0.00          = 0.80                      = 0.20
     1.0                               = 0.90          1.0
     0.8                               = 0.95          0.8
     0.6                               = 0.99          0.6
     0.4                               = 1.00          0.4
     0.2                                               0.2
     0.0                                               0.0
               0      1       2    3      4                  0    1       2    3      4
                       Time                                        Time
                       = 0.40                                      = 0.60
     1.0                                               1.0
     0.8                                               0.8
     0.6                                               0.6
     0.4                                               0.4
     0.2                                               0.2
     0.0                                               0.0
               0      1       2    3      4                  0    1       2    3      4
                       Time                                        Time
                       = 0.80                                      = 0.99
     1.0                                               1.0
     0.8                                               0.8
     0.6                                               0.6
     0.4                                               0.4
     0.2                                               0.2
     0.0                                               0.0
               0      1       2    3      4                  0    1       2    3      4
                       Time                                        Time


                                                  78


                                              6
   Figure
Figure       18: Impulse
        3: Impulse           responses
                   response of          of the DM's one-quarter-ahead
                               the DM's one-quarter-ahead                        forecast
                                                          forecast of the external state for of  the external
                                                                                             alternative         state
                                                                                                         degree of
persistence
   for alternative degrees of persistence  of the external state process.

                                                 = 0.30
                                                 = 0.60
                              = 0.00             = 0.80                            = 0.20
         1.0                                     = 0.90           1.0
         0.8                                     = 0.95           0.8
         0.6                                     = 0.99           0.6
         0.4                                     = 1.00           0.4
         0.2                                                      0.2
         0.0                                                      0.0
                      0       1       2      3       4                     0      1        2      3       4
                               Time                                                 Time
                              = 0.40                                               = 0.60
         1.0                                                      1.0
         0.8                                                      0.8
         0.6                                                      0.6
         0.4                                                      0.4
         0.2                                                      0.2
         0.0                                                      0.0
                      0       1       2      3       4                     0      1        2      3       4
                               Time                                                 Time
                              = 0.80                                               = 0.99
         1.0                                                      1.0
         0.8                                                      0.8
         0.6                                                      0.6
         0.4                                                      0.4
         0.2                                                      0.2
         0.0                                                      0.0
                      0       1       2      3       4                     0      1        2      3       4
                               Time                                                 Time




                                                         7

                                                             79
G.2      Predicted value of the regression coefficient subj
                                                       h
Given a long enough series of observations from an environment with a fixed µ, our model
yields stationary values for the Kalman gain 1 and for the amplitude of fluctuations in the
memory state var[m ¯ t ]. We can then compute the values of the following long-run conditional
second moments:
         ¯ t |µ] = var[m
     var[m                          ¯ t , µ]var[µ]-1 cov [µ, m
                       ¯ t ] - cov [m                            ¯ t]
                                                          -1
                       ¯ t ] - cov [m
                 = var[m            ¯ t , xt ]e1 var[µ] e1 cov [xt , m¯ t]
                                  1
                       ¯ t] -
                 = var[m                var[m  ¯ t ]e1 e1 var[m
                                                              ¯ t]
                               var[µ]

       µt , yt |µ] = cov [(e1 - 1 c )m
  cov [^                             ¯ t + 1 yt , yt |µ]
                   = (e1 - 1 c )cov [m
                                     ¯ t , yt |µ] + 1 var[yt |µ]
                   = (e1 - 1 c )var[m¯ t |µ]c + 1 var[yt |µ]

          µt |µ] = var[(e1 - 1 c )m
      var[^                       ¯ t + 1 yt |µ]
                                                      2
                 = (e1 - 1 c )var[m
                                  ¯ t |µ](e1 - 1 c) + 1 var[yt |µ] + 21 (e1 - 1 c )cov [m
                                                                                        ¯ t , yt |µ]
                                                      2
                 = (e1 - 1 c )var[m
                                  ¯ t |µ](e1 - 1 c) + 1 var[yt |µ] + 21 (e1 - 1 c )var[m¯ t |µ]c
   In order to write the dynamics of the model in terms of scale-invariant quantities, we
                                          2
divide each second moment by var[yt |µ] = y . Thus we can write
         var[m  ¯ t |µ]   ~m    1 ~           ~¯
                        =   ¯ -    m  ¯ e1 e1 m
         var[yt |µ]             K
            µt , yt |µ]
       cov [^                         var[m ¯ t |µ]
                        = (e1 - 1 c )               c + 1
        var[yt |µ]                    var[yt |µ]
          var[^ µt |µ]                var[m ¯ t |µ]              2                      ¯ t |µ]
                                                                                    var[m
                        = (e1 - 1 c )               (e1 - 1 c) + 1 + 21 (e1 - 1 c )             c,
          var[yt |µ]                  var[yt |µ]                                    var[yt |µ]
using the notation ~m
                    ¯  var[m
                                  2
                            ¯ t ]/y .
   We now wish to calculate the predicted asymptotic value of the regression coefficient
                                                         yt+h|t , yt |µ]
                                                    cov [^
                                         subj
                                         h    
                                                       var[yt |µ]
      ^t+h|t  E [yt+h |m
where y                ¯ t , yt ]. From
                             yt+h|t , yt |µ] = cov [(1 - h )^
                        cov [^                              µt + h yt , yt |µ]
                                          = (1 - h )cov [^
                                                         µt , yt |µ] + h var[yt |µ],
      ^t  E [µ|m
where µ        ¯ t , yt ], we can then compute
                                        µt , yt |µ]
                                   cov [^
               subj
               h    = (1 - h )                      + h
                                    var[yt |µ]
                                            ~m   1 ~         ~ ¯ c + 1
                     = (1 - h ) (e1 - 1 c )  ¯ -   m ¯ e1 e1 m                         + h .
                                                 K

                                                      80
These are the coefficients whose values are plotted against the value of h = h in Figure 10.

G.3      Predicted value of the Coibion-Gorodnichenko regression
         coefficient b
We wish to compute the predicted asymptotic value of the regression coefficient b  cov    [F Et ,F Rt ]
                                                                                         var[F Rt ]
                                                                                                        ,
where the forecast error F Et  yt+1 - y^t+1|t and the forecast revision F Rt  y ^t+1|t - y
                                                                                         ^t+1|t-1 .
(Here we consider only the case of forecasts of a variable realized in period t + 1, as this is
the coefficient plotted in Figure 11.)
   First, from y^t+1|t = (1 - )^
                               µt + yt and y ^t+1|t-1 = (1 - 2 )^
                                                                µt-1 + 2 yt-1 , we get

                        F Et = ((1 - )µ + yt + y,t+1 ) - ((1 - )^
                                                                µt + yt )
                             = (1 - )µ - (1 - )^
                                               µt + y,t+1
                                      µt + yt ) - (1 - 2 )^
                        F Rt = ((1 - )^                   µt-1 + 2 yt-1
                             = (1 - )µ + (1 - )^ µt - (1 - 2 )^
                                                              µt-1 +  y,t

Then

cov [F Et , F Rt |µ] = -(1 - )2 var[^
                                    µt |µ] + (1 - )(1 - 2 )cov [^    ^t-1 |µ]
                                                                µt , µ
      var[F Rt |µ] = (1 - )2 + (1 - 2 )2 var[^
                                             µt |µ] + var[                  y,t |µ]   - 2(1 - )(1 - 2 )cov [^    ^t-1 |µ]
                                                                                                            µt , µ
                              + 2(1 - )cov [^µt , t |µ]
                   = (1 - )2 + (1 - 2 )2 var[^
                                             µt |µ] + 2  2 - 2(1 - )(1 - 2 )cov [^    ^t-1 |µ]
                                                                                 µt , µ
                                            2
                              + 2(1 - )1  ,

                               µt , t |µ] = cov [1
where we have substituted cov [^                          y,t , y,t |µ]   = 1  2 to obtain the last equality.
  It then follows that
cov [F Et , F Rt |µ]                                    µt |µ]                          ^t-1 |µ]
                                   = -(1 - )2 var      [^
                                                  var[yt |µ]
                                                                                    µt ,µ
                                                               + (1 - )(1 - 2 ) covvar
                                                                                   [^
                                                                                       [yt |µ]
                                                                                                 ,                (G.1)
     var[yt |µ]
      var[F Rt |µ]                                 µt |µ]                                                ^t-1 |µ]
                     = {(1 - )2 + (1 - 2 )2 } var [^
                                              var[yt |µ]
                                                           + 2 (1 - 2 ) - 2(1 - )(1 - 2 ) covvar   [^
                                                                                                    µt , µ
                                                                                                       [ y t |µ ]
       var[yt |µ]
                                                               +2(1 - )(1 - 2 )1 .                                (G.2)

Using these expressions, we can compute the Coibion-Gorodnichenko regression coefficient
                                                                                -1
                                  cov [F Et , F Rt |µ] var[F Rt |µ]
                              b =                                                     .                          (G.3)
                                       var[yt |µ]       var[yt |µ]

   Note that when  = 0, we obtain the simple result that b = - 1
                                                               2
                                                                 , since

         cov [F Et , F Rt |µ]         µt |µ] cov [^
                                  var[^                  ^t-1 |µ]
                                                    µt , µ
                              =-             +
              var[yt |µ]          var[yt |µ]      var[yt |µ]
               var[F Rt |µ]          µt |µ]
                                 var[^          cov [^    ^t-1 |µ]
                                                     µt , µ                           cov [F Et , F Rt |µ]
                              =2             -2                    = -2                                      .
                var[yt |µ]       var[yt |µ]       var[yt |µ]                               var[yt |µ]

                                                     81
    Finally, to evaluate the expressions needed in order to compute the predicted value of
b, in general we need to be able to compute cov [^      ^t-1 |µ]. For this, we need to solve for
                                                   µt , µ
the long-run stationary fluctuations in µ
                                        ^t . Using the notation X and v for the long-run
stationary values of Xt and vt , we can derive the long-run dynamics of µ     ^t as follows. We
first note that

          ^t = (e1 - 1 c )m
          µ               ¯ t + 1 yt
                                                  µ
                                                  ^t-1
             = (e1 - 1 c ) ( X v v )                   +¯ t + 1 yt
                                                  yt-1
             = (e1 - 1 c ) ( X v (e1 v µ   ^t-1 + e2 v yt-1 ) + ¯ t ) + 1 (yt-1 + t )
             = µ^µ^t-1 + µ ^,y yt-1 + µ
                                      ^t ,

where µ ^   e1 v (e1 - 1 c )X v , µ       ^,y   e2 v (e1 - 1 c )X v + 1 , and              µ
                                                                                           ^t   =
(e1 - 1 c )¯
           t + 1 y,t . From this, we can derive

                       cov [^    ^t-1 |µ] = cov [µ
                            µt , µ                ^µ^t-1 + µ            ^t-1 |µ]
                                                             ^,y yt-1 , µ
                                          = µ       µt |µ] + µ
                                             ^ var [^                  µt , yt |µ].
                                                              ^,y cov [^

                                                         µ |µ] derived in Appendix G.2, we
   Using this result, together with the solution for var[^
can then evaluate both of the covariances (G.1) and (G.2), and use them to compute the
predicted regression coefficient (G.3). These are the formulas used to compute the values
shown in Figure 11.

G.4     Predicted value of the Fuhrer regression coefficient 
We wish to compute the value of
                                                     i
                                              cov[F Rt , it-1 ]
                                                        i
                                                                ,                         (G.4)
                                                 var[t-1 ]
         i
where F Rt is the forecast revision of an individual forecaster i,
                                        i    i         i
                                     F Rt  y
                                           ^t +h|t - y
                                                     ^t +h|t-1 ,

and i  t-1 is the difference between i's forecast and the consensus forecast in period t - 1.
Individual forecasts are given by
                                      i            h
                                    y
                                    ^t               µi
                                       +h|t = (1 -  )^
                                                           h
                                                       t +  yt ,

      ^i
where µ t is i's estimate of µ at time t, given by

                                         ^i
                                         µ     ~i
                                           t = mt + 1 yt ,

using (3.10).
   We have shown in (4.6) that
                                                   sum,i
                                      i        h
                                      t = (1 -  ) ~t     ,

                                                   82
and correspondingly that
                                                     sum,i
                                  i
                                  t-1 = (1 - 
                                              h+1
                                                  ) ~t-1 ,
       sum,i
where ~t     is defined in (3.9). It follows that the denominator of (G.4) is equal to
                                                          sum,i
                           var[i
                               t-1 ] = (1 - 
                                            h+1 2 2
                                               )  · var[~
                                                        t  - 1 ].

                                                                      i
   Furthermore, the numerator of (3.9) depends only on the part of F Rt that is correlated
       sum,i
with ~ t-1 . It follows from (3.8) that

                                    ~i
                                    m    ~ avg
                                     t = m t   + sum,i
                                                ~t+1 ,

and hence that
                               avg
                   y
                   ^ti
                      +h|t = y
                             ^t             h
                                               µi
                                +h|t + (1 -  )[^   ^avg
                                                 t-µ t ]
                               avg
                             ^t
                           = y              h
                                                ~i
                                +h|t + (1 -  ) [m  ~ avg
                                                 t-m t ]
                               avg          h   sum,i
                           = y
                             ^t +h|t + (1 -  ) ~t
                               avg          h   i              sum,i
                           = y
                             ^t +h|t + (1 -  ) ~t + (1 - h )m ~t-1 .

Similarly,
                                    avg
                      y
                      ^ti
                                  ^t
                         +h|t-1 = y  +h|t-1 + (1 - 
                                                    h+1
                                                          µi
                                                        )[^       ^avg
                                                            t-1 - µ t-1 ]
                                    avg
                                = y
                                  ^t +h|t-1 + (1 - 
                                                    h+1
                                                           ~i
                                                        ) [m       ~ avg
                                                             t-1 - m t-1 ]
                                    avg             h+1    sum,i
                                = y
                                  ^t +h|t-1 + (1 -      ) ~t-1 ,

so that
            i
     cov[F Rt , i            i
                            yt
                t-1 ] = cov[^ +h|t - y
                                     ^ti       i
                                        +h|t-1 t-1 ]
                                       sum,i              sum,i              sum,i
                      = cov[(1 - h )m ~t-1 - (1 - 
                                                   h+1
                                                       ) ~t-1 , (1 - 
                                                                      h+1
                                                                          ) ~t-1 ]
                                                                         sum,i
                      = [m (1 - h )(1 - h+1 ) - (1 - h+1 )2 ]  2 · var[~
                                                                       t  - 1 ].

   Substituting these results into (G.4), we obtain

                                           m (1 - h )
                                     =                - 1.                               (G.5)
                                           1 - h+1
This is the formula that we use to compute the values shown in Figure 12.




                                               83
