                              NBER WORKING PAPER SERIES




 A NOTE ON NONPARAMETRIC IDENTIFICATION OF DISTRIBUTIONS OF RANDOM
             COEFFICIENTS IN MULTINOMIAL CHOICE MODELS

                                          Jeremy T. Fox

                                       Working Paper 23621
                               http://www.nber.org/papers/w23621


                     NATIONAL BUREAU OF ECONOMIC RESEARCH
                              1050 Massachusetts Avenue
                                Cambridge, MA 02138
                                     July 2017




The views expressed herein are those of the author and do not necessarily reflect the views of the
National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.

© 2017 by Jeremy T. Fox. All rights reserved. Short sections of text, not to exceed two
paragraphs, may be quoted without explicit permission provided that full credit, including ©
notice, is given to the source.
A Note on Nonparametric Identification of Distributions of Random Coefficients in Multinomial
Choice Models
Jeremy T. Fox
NBER Working Paper No. 23621
July 2017
JEL No. C25,L0

                                         ABSTRACT

I prove that the joint distribution of random coefficients and additive errors is identified in a
mulltinomial choice model. No restrictions are imposed on the support of the random coefficients
and additive errors. The proof uses large support variation in choice-specific explanatory
variables following Lewbel (2000) but does not rely on an identification at infinity technique
where the payoffs of all but two choices are set to minus infinity.


Jeremy T. Fox
Rice University
Department of Economics - MS22
Baker Hall
P.O. Box 1892
Houston, TX 77251-1892
and NBER
jeremyfox@gmail.com
 A Note on Nonparametric Identification of Distributions of Random
                       Coefficients in Multinomial Choice Models
                                                 Jeremy T. Fox
                                           Rice University and NBER

                                                     July 2017


    Two key papers on identifying the distribution of random coefficients and additive errors in binary (two) choice
have not been extended to multinomial (two or more, importantly three or more) choice (Ichimura and Thompson,
1998; Gautier and Kitamura, 2013). Under large support on choice-specific explanatory variables, one can always
turn a multinomial choice model with three or more goods into a binary choice model by setting the explanatory
variables for all but two goods to be minus infinity. This identification at infinity approach does not identify the
joint distribution of all unobservables in the model if there are unobservables that enter only the utilities of certain
goods. Establishing the identification of the joint distribution of random coefficients and additive unobservables
in the multinomial choice model is the point of this note.
    The argument in this note might be considered an extension to multinomial choice of the brief argument about
identifying a distribution of random coefficients in binary choice in Lewbel (2014, Section 8). Our identification
proof uses the results on identifying distributions of random coefficients in seemingly unrelated regressions by
Masten (forthcoming). A key step of our identification proof is also found in Berry and Haile (2010) and Fox and
Gandhi (2016).
    There are some related papers on multinomial choice. Lewbel (2000) considers multinomial choice in a semi-
parametric setting but does not explicitly identify a distribution of random coefficients. Fox and Gandhi (2016)
study the topic of this note: nonparametric identification in multinomial choice models, where the example of a
linear-in-random-coefficients model is a special case of their analysis. However, Fox and Gandhi assume that the
distribution of random coefficients and additive errors takes on unknown finite support in the appropriate real
space. This note avoids the unknown finite support assumption. Fox, Kim, Ryan and Bajari (2012) nonparamet-
rically identify a distribution of random coefficients on continuous explanatory variables but rely on the additive,
good-specific, unobservables having a known distribution such as the type I extreme value or logit distribution.
In this note, the joint distribution of the good-specific, additive unobservables is identified.
    Consider a multinomial choice model with random coefficients. Let i index a consumer. There are J inside
goods and one outside good, called choice 0. The outside good has a utility normalized to ui,0 = 0. The inside



                                                           1
goods have utilities
                                                             ui,j = βi0 xi,j + i,j ,

where xi,j is a vector of observables for choice j and consumer i, βi is consumer i’s vector of random coefficients
on the explanatory variables, and i,j is an additive unobservable for choice j and consumer i. We do not impose
that i,j has mean zero and we do not allow intercept terms in xi,j . Note that βi is common to all choices.
    We impose the scale normalization that one element of the vector βi has the value ±1 for each i. This rules out
this coefficient can be zero. Let wi,j be the corresponding scalar element of xi,j and let x̃i,j be all other elements
of xi,j , so that xi,j = (wi,j , x̃i,j ). Then the utility to choice j can be rewritten as

                                                      ui,j = ±1wi,j + β̃i0 x̃i,j + i,j ,

where β̃i corresponds to the random coefficients on only the items in x̃i,j . We further impose that the coefficient
on wi,j is either +1 for all consumers i or is −1 for all consumers i.
    We consider i.i.d. observations on (yi , xi ), where yi is the choice that maximizes ui,j over {0, 1, . . . , J}. Given
this, we can nonparametrically identify conditional choice probabilities Pr (yi = j | xi ). We assume that xi is
independent of γi . We can discuss endogeneity with various methods in the literature such as Lewbel (2000). We
leave that for extensions.
    The sign of the coefficient on wi,j is learned in identification; a positive coefficient on wi,j corresponds to higher
values of wi,j increasing the choice probability of good j, other explanatory variables held constant. We make this
formal in the proof of identification.
    Let xi = (xi,1 , . . . , xi,J ), wi = (wi,1 , . . . , wi,J ), x̃i = (x̃i,1 , . . . , x̃i,J ) and i = (i,1 , . . . , 1,J ) all be thought of
as long vectors. Also, define γi = (βi , i ) and think of it as another long vector. Note that γi is a heterogeneous
parameter vector, not a homogeneous parameter. In what follows, we drop the i subscript.
    The only unknown object in this model is F (γ), the joint distribution of the additive unobservables  and
the random coefficients β. Because we will not restrict the support of each j and β, a sufficient condition for
identification of F , as shown in this note, will be that the support of x, reordered as x = (w, x̃) is Rdim(w) × X̃,
where the support of w is the large support Rdim(w) and the support of the vector x̃ is X̃, a closed superset of
an open subset of the real space Rdim(x̃) .1 Having support on the product space Rdim(w) for w and a superset of
an open subset of Rdim(x̃) for x̃ rules out the entire vector x containing polynomial terms of other elements in x,
interactions of two elements also in x, or the same element of x entering the utility of different goods. Continuous
support on all elements in x rules out discrete x. Large support is needed for w but not for x̃.
    Some version of a large support condition, while not particularly attractive, is necessary to achieve identifica-
tion. Consider the special case of our model where there is one inside good (J = 1), one outside good and no x̃.
This is binary choice, which has been extensively studied in the literature. Then the utility of good 1 is

                                                              u1 = ±1w1 + 1
   1 Often   the term mathematical term support is defined to be a closed set.



                                                                        2
and the utility of good 0 is still u0 = 0. We can identify the sign of the common coefficient on w1 by seeing
whether Pr (y = 1 | w1 ) is monotonically increasing or decreasing in the scalar w1 . Let w̄1 = +w1 or w̄1 = −w1 ,
as appropriate. In this example, γ = 1 . We can identify the distribution F (1 ) at the point of evaluation ?1 as
follows
                   F (?1 ) = Pr (1 ≤ ?1 ) = Pr (1 ≤ −w̄1 | −w̄1 = ?1 ) = Pr (y = 0 | −w̄1 = ?1 ) .

If  takes on support on R, then this argument shows that w̄1 must also take on support on all of R to identify
F (1 ) over its entire support. As Pr (y = 0 | w̄1 ) + Pr (y = 1 | w̄1 ) = 1, all available data on conditional choice
probabilities is used in the identification argument. This binary choice example is a special case of our model so
it shows that some large support is necessary for identification.
    We are now ready to state the main identification theorem.

Theorem 1. If the support of x is as stated previously, γ is independent of x, γ has finite absolute moments, and
the distribution of γ is uniquely determined by its moments, then F (γ) is identified.

Proof. The argument works in three steps. First, we identify the sign of the coefficient on the explanatory variables
wj that form the scale normalization. Second, we use w to trace out the CDF of utility values (other than from
w) conditional on the other explanatory variables in x̃. Third, we cite work by Masten (forthcoming, Theorem 1)
on seemingly unrelated regressions with random coefficients to identify the distribution of the random coefficients
and additive unobservables.
    First, we identify the sign of the coefficient on each wj , which has been normalized to ±1. We observe
conditional choice probabilities Pr (y = j | x) and the vector w is a subvector of x. If Pr (y = j | x) is monotonically
increasing in wj for some j, then the coefficient on each wj is +1. If it is decreasing, then the coefficient on each wj
is −1. Let w̄j = +wj or w̄j = −wj , as appropriate based on the sign of the coefficient on wj . Let w̄ = (w̄1 , . . . , w̄J ).
    Second, we use arguments motivated by Lewbel (2000) to trace a CDF of utility values (other than from w)
conditional on the explanatory variables in x̃. Define

                                                         ũj = β̃ 0 x̃j + j

and ũ = (ũ1 , . . . , ũJ ). Let the CDF of the vector ũ conditional on the vector x̃ be Gũ (ũ | x̃). Let ũ? be a point
of evaluation of the CDF. Then, for arbitrary x̃,

                Gũ (ũ? | x̃) = Pr (ũ ≤ ũ? | x̃) = Pr (ũ ≤ −w̄ | −w̄ = ũ? , x̃) = Pr (y = 0 | −w̄ = ũ? , x̃) .

All the lower case letters in the above display equation are vectors, except for y. This argument identifies the
CDF Gũ (ũ? | x̃) at all points of evaluation because w has full support on Rdim(w) . This argument is not new. It
appears in Berry and Haile (2010) and Fox and Gandhi (2016).
   Third, we use Masten (forthcoming, Theorem 1) to identify the distribution of γ itself. From the previous
step we observe Gũ (ũ | x̃) for x̃ varying in an open set, which is equivalent to the distribution that Masten


                                                                 3
(forthcoming, Theorem 1) maintains is observed in a seemingly unrelated regression model.2 Therefore, F (γ) is
identified for any J.

    This identification argument relies heavily on large support of w, meaning w has support Rdim(w) . The
argument does not use identification at infinity. In multinomial choice, we can precisely define identification at
infinity to mean a step of an identification proof that sets wj = −∞ for J − 1 inside goods and uses an analysis
from binary choice on the resulting pair of an inside good and an outside good. One can inspect the identification
proof to see that this type of argument is not used. Indeed, the second step of the proof is explicitly incompatible
with identification at infinity, as the vector w is used over its full support Rdim(w) .


References
Berry, Steven T. and Philip A. Haile, “Nonparametric Identification of Multinomial Choice Demand Models
 with Heterogeneous Consumers,” 2010. Yale University working paper.

Fox, Jeremy T. and Amit Gandhi, “Nonparametric Identification and Estimation of Random Coefficients in
  Multinomial Choice Models,” RAND Journal of Economics, 2016, 47 (1), 118–139.

  , Kyoo il Kim, Stephen Ryan, and Patrick Bajari, “The Random Coefficients Logit Model is Identified,”
  Journal of Econometrics, 2012, 1662 (2), 204–212.

Gautier, E. and Y. Kitamura, “Nonparametric Estimation in Random Coefficients Binary Choice Models,”
 Econometrica, 2013, 81 (2), 581–607.

Ichimura, H. and TS Thompson, “Maximum likelihood estimation of a binary choice model with random
  coefficients of unknown distribution,” Journal of Econometrics, 1998, 86 (2), 269–295.

Lewbel, Arthur, “Semiparametric Qualitative Response Model Estimation with Unknown Heteroscedasticity or
  Instrumental Variables,” Journal of Econometrics, 2000, 97 (1), 145–177.

  , “An Overview of the Special Regressor Method,” in “Oxford Handbook of Applied Nonparametric and Semi-
  parametric Econometrics and Statistics,” Oxford University Press, 2014, pp. 38–62.

Masten, Matthew, “Random coefficients on endogenous variables in simultaneous equations models,” The Review
 of Economic Studies, forthcoming.



    2 Masten (forthcoming, Theorem 1) is stated for a seemingly unrelated regression model with two equations but this was just an

editorial decision on his part and the argument applies to any number of equations. Masten (forthcoming, Theorem 1) directly applies
to a model with a good-specific vector of random coefficients βj for each inside good j, in addition to the good-specific additive error
j . The typical restriction is used in this paper: βj1 = βj2 for all inside goods j1 , j2 . This is a special case of Masten (forthcoming,
Theorem 1).


                                                                    4
