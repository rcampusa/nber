                              NBER WORKING PAPER SERIES




                LEAVE-OUT ESTIMATION OF VARIANCE COMPONENTS

                                         Patrick Kline
                                        Raffaele Saggio
                                        Mikkel Sølvsten

                                      Working Paper 26244
                              http://www.nber.org/papers/w26244


                     NATIONAL BUREAU OF ECONOMIC RESEARCH
                              1050 Massachusetts Avenue
                                Cambridge, MA 02138
                                   September 2019




We thank Isaiah Andrews, Bruce Hansen, Whitney Newey, Anna Mikusheva, Jack Porter, Andres
Santos, Azeem Shaikh and seminar participants at UC Berkeley, CEMFI, Chicago, Harvard,
UCLA, MIT, Northwestern, NYU, Princeton, Queens, UC San Diego, Wisconsin, the NBER
Labor Studies meetings, and the CEME Interactions workshop for helpful comments. The data
used in this study was generously provided by the Fondazione Rodolfo De Benedetti and
originally developed by the Economics Department of the Università Ca Foscari Venezia under
the supervision of Giuseppe Tattara. We thank the Berkeley Institute for Research on Labor and
Employment for funding support and Schmidt Futures, which provided financial assistance for
this project through the Labor Science Initiative at the Berkeley Opportunity Lab. The views
expressed herein are those of the authors and do not necessarily reflect the views of the National
Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.

© 2019 by Patrick Kline, Raffaele Saggio, and Mikkel Sølvsten. All rights reserved. Short
sections of text, not to exceed two paragraphs, may be quoted without explicit permission
provided that full credit, including © notice, is given to the source.
Leave-out Estimation of Variance Components
Patrick Kline, Raffaele Saggio, and Mikkel Sølvsten
NBER Working Paper No. 26244
September 2019
JEL No. C1,J31

                                         ABSTRACT

We propose leave-out estimators of quadratic forms designed for the study of linear models with
unrestricted heteroscedasticity. Applications include analysis of variance and tests of linear
restrictions in models with many regressors. An approximation algorithm is provided that enables
accurate computation of the estimator in very large datasets. We study the large sample properties
of our estimator allowing the number of regressors to grow in proportion to the number of
observations. Consistency is established in a variety of settings where plug-in methods and
estimators predicated on homoscedasticity exhibit first-order biases. For quadratic forms of
increasing rank, the limiting distribution can be represented by a linear combination of normal
and non-central 2 random variables, with normality ensuing under strong identification. Standard
error estimators are proposed that enable tests of linear restrictions and the construction of
uniformly valid confidence intervals for quadratic forms of interest. We find in Italian social
security records that leave-out estimates of a variance decomposition in a two-way fixed effects
model of wage determination yield substantially different conclusions regarding the relative
contribution of workers, firms, and worker-firm sorting to wage inequality than conventional
methods. Monte Carlo exercises corroborate the accuracy of our asymptotic approximations, with
clear evidence of non-normality emerging when worker mobility between blocks of firms is
limited.

Patrick Kline                                    Mikkel Sølvsten
Department of Economics                          University of Wisconsin - Madison
University of California, Berkeley               Office 7430
530 Evans Hall #3880                             Department of Economics
Berkeley, CA 94720                               William H. Sewell Social Science Building
and NBER                                         1180 Observatory Drive
pkline@econ.berkeley.edu                         Madison, WI 53706
                                                 soelvsten@wisc.edu
Raffaele Saggio
University of British Columbia
Vancouver, BC
Canada
rsaggio@mail.ubc.ca
      As economic datasets have grown large, so has the number of parameters employed in econo-
metric models. Typically, researchers are interested in certain low dimensional summaries of these
parameters that communicate the relative influence of the various economic phenomena under
study. An important benchmark comes from Fisher (1925)'s foundational work on analysis of vari-
ance (ANOVA) which he proposed as a means of achieving a "separation of the variance ascribable
to one group of causes, from the variance ascribable to other groups."1
      A large experimental literature (Sacerdote, 2001; Graham, 2008; Chetty et al., 2011; Angrist,
2014) employs variants of Fisher's ANOVA approach to infer the degree of variability attributable
to peer or classroom effects. Related methods are often used to study heterogeneity across firms,
workers, and schools in their responsiveness to exogenous regressors with continuous variation
(Raudenbush and Bryk, 1986, 2002; Arellano and Bonhomme, 2011; Graham and Powell, 2012).
In labor economics, log-additive models of worker and firm fixed effects are increasingly used to
study worker-firm sorting and the dispersion of firm specific pay premia (Abowd et al., 1999; Card
et al., 2013, 2018; Song et al., 2017; Sorkin, 2018) and analogous methods have been applied to
settings in health economics (Finkelstein et al., 2016; Silver, 2016) and the economics of education
(Arcidiacono et al., 2012).
      This paper considers estimation of and inference on variance components, which we define
broadly as quadratic forms in the parameters of a linear model. Notably, this definition yields an
important connection to the recent literature on testing linear restrictions in models with many
regressors (Anatolyev, 2012; Chao et al., 2014; Cattaneo et al., 2018). Traditional variance compo-
nent estimators are predicated on the assumption that the errors in a linear model are identically
distributed draws from a normal distribution. Standard references on this subject (e.g., Searle
et al., 2009) suggest diagnostics for heteroscedasticity and non-normality, but offer little guidance
regarding estimation and inference when these problems are encountered. A closely related litera-
ture on panel data econometrics proposes variance component estimators designed for fixed effects
models that either restrict the dimensionality of the underlying group means (Bonhomme et al.,
2019) or the nature of the heteroscedasticity governing the errors (Andrews et al., 2008; Jochmans
and Weidner, 2016).
      Our first contribution is to propose a new variance component estimator designed for unre-
stricted linear models with heteroscedasticity of unknown form. The estimator is finite sample
unbiased and can be written as a naive "plug-in" variance component estimator plus a bias cor-
rection term that involves "cross-fit" (Newey and Robins, 2018) estimators of observation-specific
error variances. We also develop a representation of the estimator in terms of a covariance be-
tween outcomes and a "leave-one-out" generalized prediction (e.g., as in Powell et al., 1989), which
allows us to apply recent results on the behavior of second order U-statistics. Building on work
  1
      See Cochran (1980) for a discussion of the intellectual development of this early work.



                                                       2
by Achlioptas (2003), we propose a random projection method that enables computation of our
estimator in very large datasets with little loss of accuracy.
   We study the asymptotic behavior of the proposed leave-out estimator in an environment where
the number of regressors may be proportional to the sample size: a framework that has alternately
been termed "many covariates" (Cattaneo et al., 2018) or "moderate dimensional" (Lei et al., 2018)
asymptotics. Verifiable design requirements are provided under which the estimator is consistent
and we show in an Appendix that these conditions are weaker than those required by jackknife bias
correction procedures (Quenouille, 1949; Hahn and Newey, 2004; Dhaene and Jochmans, 2015). A
series of examples is discussed where the leave-out estimator is consistent, while estimators relying
on jackknife or homoscedasticity-based bias corrections are not.
   We present three sets of theoretical results that enable inference based upon our estimator
in a variety of settings. The first result concerns inference on quadratic forms of fixed rank, a
problem which typically arises when testing a few linear restrictions in a model with many covariates
(Cattaneo et al., 2018). Familiar examples of such applications include testing that particular
regressors are significant in a fixed effects model and conducting inference on the coefficients from a
projection of fixed effects onto a low dimensional vector of covariates. Extending classic proposals
by Horn et al. (1975) and MacKinnon and White (1985), we show that our leave-out approach
can be used to construct an Eicker-White style variance estimator that is unbiased in the presence
of unrestricted heteroscedasticity and that enables consistent inference on linear contrasts under
weaker design restrictions than those considered by Cattaneo et al. (2018).
   Next, we derive a result establishing asymptotic normality of quadratic forms of growing rank.
Such quadratic forms typically arise when conducting analysis of variance but also feature in tests
of model specification involving a large number of linear restrictions (Anatolyev, 2012; Chao et al.,
2014). The large sample distribution of the estimator is derived using a variant of the arguments
in Chatterjee (2008) and Sølvsten (2019) and a standard error estimator is proposed that utilizes
sample splitting formulations of the sort considered by Newey and Robins (2018). This standard
error estimator is shown to enable consistent inference on quadratic forms of growing rank in the
presence of unrestricted heteroscedasticity when the regressor design allows for sample splitting
and to provide conservative inference otherwise.
   Finally, we present conditions under which the large sample distribution of our estimator is
non-pivotal and can be represented by a linear combination of normal and non-central 2 random
variables, with the non-centralities of the 2 terms serving as weakly identified nuisance parameters.
This distribution arises in a two-way fixed effects model when there are "bottlenecks" in the mobility
network. Such bottlenecks are shown to emerge, for example, when worker mobility is governed
by a stochastic block model with limited mobility between blocks. To construct asymptotically
valid confidence intervals in the presence of nuisance parameters, we propose inverting a minimum
distance test statistic. Critical values are obtained via an application of the procedure of Andrews

                                                   3
and Mikusheva (2016). The resulting confidence interval is shown to be valid uniformly in the
values of the nuisance parameters and to have a closed form representation in many settings, which
greatly simplifies its computation.
    We illustrate our results with an application of the two-way worker-firm fixed effects model of
Abowd et al. (1999) to Italian social security records. The proposed leave-out estimator finds a
substantially smaller contribution of firms to wage inequality and much more assortativity in the
matching of workers to firms than either the uncorrected plug-in estimator originally considered by
Abowd et al. (1999) or the homoscedasticity-based correction procedure of Andrews et al. (2008).
When studying panels of length greater than two, we allow for serial correlation in the errors by
employing a generalization of our estimator that leaves out all the observations in a worker-firm
match. Failing to account for this dependence is shown to yield over-estimates of the variance of
firm effects.
    Projecting firm effect estimates onto measures of worker age and firm size, we find that older
workers tend to be employed at firms offering higher firm wage effects; however, this phenomenon
is largely explained by the tendency of older workers to sort to bigger firms. Leave-out standard
errors for the coefficients of these linear projections are found to be several times larger than a
naive standard error predicated on the assumption that the estimated fixed effects are independent
of each other. Stratifying our analysis by birth cohort, we formally reject the null hypothesis that
older and younger workers face identical vectors of firm effects. However, the two sets of firm effects
are estimated to have a correlation coefficient of nearly 0.9, while the plug-in estimate of correlation
is only 0.54.
    To assess the accuracy of our asymptotic approximations, we conduct a series of Monte Carlo
exercises utilizing the realized mobility patterns of workers between firms. Clear evidence of non-
normality arises in the sampling distribution of the estimated variance of firm effects in settings
where the worker-firm mobility network is weakly connected. The proposed confidence regions are
shown to provide reliable size control in both strongly and weakly identified settings.



1     Unbiased Estimation of Variance Components
Consider the linear model

                         y i = x i  + i                         (i = 1, . . . , n)

where the regressors xi  Rk are non-random and the design matrix Sxx =               n
                                                                                     i=1 xi xi   has full rank.
The unobserved errors    { i } n
                               i=1   are mutually independent and obey E[i ] = 0, but may possess
observation specific variances E[2     2
                                 i ] = i .
    Our object of interest is a quadratic form  =  A for some known non-random symmetric


                                                   4
matrix A  Rk×k of rank r. Following Searle et al. (2009), when A is positive semi-definite  is a
variance component, while when A is non-definite  may be referred to as a covariance component.
Note that linear restrictions on the parameter vector  can be formulated in terms of variance
components: for a non-random vector v , the null hypothesis v  = 0 is equivalent to the restriction
 = 0 when A = vv . Examples from the economics literature where variance components are of
direct interest are discussed in Section 2.


1.1     Estimator
                                                             ^PI = ^ A^, where      -1
                                                                               ^ = Sxx n
A naive plug-in estimator of  is given by the quadratic form                           i=1 xi yi
                                                                                ^
denotes the Ordinary Least Squares (OLS) estimator of  . Estimation error in  leads the plug-
                                                                                           2 n
in estimator to exhibit a bias involving a linear combination of the unknown variances {i   }i=1 .
Specifically, standard results on quadratic forms imply that E[^] =  + trace(AV[^]), where

                                            n
                                ^] =                  2                     -1   -1
                       trace AV[                  Bii i       and Bii = xi Sxx ASxx xi .
                                            i=1

As discussed in Section 2, this bias can be particularly severe when the dimension of the regressors
k is large relative to the sample size.
   A bias correction can be motivated by observing that an unbiased estimator of the i-th error
variance is

                                            2               ^-i ,
                                           ^i = yi yi - x i 

      ^-i = Sxx - xi xi     -1
where                             =i x    y denotes the leave-i-out OLS estimator of  . This insight
suggests the following bias-corrected estimator of :
                                                                  n
                                           ^= ^ A^-                         2
                                                                       Bii ^i .                      (1)
                                                              i=1

While Newey and Robins (2018) observe that "cross-fit" covariances relying on sample splitting
can be used to remove bias of the sort considered here, we are not aware of existing estimators
                                         2 n
involving the leave-one-out estimators {^i }i=1 .
                            ^                                                 -1
    One can also motivate  via a change of variables argument. Letting x
                                                                       ~i = ASxx xi denote a
vector of "generalized" regressors, we can write
                                                              n                   n
                                          -1
                       =  A =        Sxx Sxx A       =                 xi x
                                                                          ~i  =         E yi x
                                                                                             ~i  .
                                                          i=1                     i=1




                                                          5
This observation suggests using the unbiased leave-out estimator
                                                           n
                                                     ^=
                                                                yi x
                                                                   ~i ^-i .                                          (2)
                                                          i=1


       Note that direct computation of ^-i can be avoided by exploiting the representation

                                                                      yi - xi ^
                                                  yi - x i ^-i =                ,                                    (3)
                                                                       1 - Pii

                -1
where Pii = xi Sxx xi gives the leverage of observation i. Applying the Sherman-Morrison-Woodbury
formula (Woodbury, 1949; Sherman and Morrison, 1950), this representation also reveals that (1)
and (2) are numerically equivalent:

                                                              -1        -1
                       ^-i = yi x   -1                 yi x
                                                          ~i Sxx xi xi Sxx                            ^ - Bii  2
                 yi x
                    ~i          ~i Sxx        xy +                                        x y = yi x
                                                                                                   ~i         ^i .
                                                             1 - Pii
                                         =i                                          =i

                                      ^-Bii yi2                                ^-i
                               =yi x
                                   ~i                               =Bii yi xi 


A similar combination of a change of variables argument and a leave-one-out estimator was used by
Powell et al. (1989) in the context of weighted average derivatives. The JIVE estimators proposed
by Phillips and Hale (1977) and Angrist et al. (1999) also use a leave-one-out estimator, though
without the change of variables.2
                2 n
Remark 1. The {^i }i=1 can also be used to construct an unbiased variance estimator

                                                                n
                                         ^ [      -1
                                            ^] = Sxx                        2     -1
                                         V                           xi xi ^i    Sxx .
                                                               i=1

                     ^ [
Section 3 shows that V  ^] can be used to perform asymptotically valid inference on linear contrasts
                                                                       ^ [
in settings where existing Eicker-White estimators fail. Specifically, V  ^] leads to valid inference
under conditions where the MINQUE estimator of Rao (1970) and the MINQUE-type estimator of
Cattaneo et al. (2018) do not exist (see, e.g., Horn et al., 1975; Verdier, 2017).
                       ^ [
Remark 2. The quantity V  ^] is closely related to the HC2 variance estimator of MacKinnon
and White (1985). While the HC2 estimator employs observation specific variance estimators
          (yi -xi ^)2                                   yi -xi ^)
 2
^i, HC2 =   1-P
                       ^ [
                      ,V  ^] relies instead on  2
                                               ^i = yi (1-P
                 ii                                                    ii

Remark 3. In some cases it may be important to allow dependence in the errors in addition to
heteroscedasticity. A common case arises when the data are organized into mutually exclusive
   2
    The object of interest in JIVE estimation is a ratio of quadratic forms 1 Sxx 2 /2 Sxx 2 in the two-
equation model yij = xi j + ij for j = 1, 2. When no covariates are present, using leave-out estimators of
both the numerator and denominator of this ratio yields the JIVE1 estimator of Angrist et al. (1999).


                                                               6
and independent "clusters" within which the errors may be dependent (Moulton, 1986). The same
change of variables argument implies that an estimator of the form n yi x
                                                                        ~i ^-c(i) will be unbiased
                                                                                            i=1
in such settings, where ^-c(i) is the OLS estimator obtained after leaving out all observations in
the cluster to which observation i belongs.


1.2       Large Scale Computation
From (1) and (3), computation of ^ relies on the values {Bii , Pii }n
                                                                    i=1 . Section 2 provides some
canonical examples where these quantities can be computed in closed form. When closed forms are
unavailable, a number of options exist for accelerating computation. For example, in the empirical
application of Section 8, we make use of a preconditioned conjugate gradient algorithm suggested by
Koutis et al. (2011) to compute exact leave-out variance decompositions in a two-way fixed effects
model involving roughly one million observations and hundreds of thousands of parameters (see
Appendix B.3 for details). However, in very large scale applications involving tens or hundreds of
millions of parameters, exact computation of {Bii , Pii }n
                                                         i=1 is likely to become infeasible. Fortunately,
                                      ^
it is possible to quickly approximate  in such settings using a variant of the random projection
method introduced by Achlioptas (2003). We refer to this method as the Johnson-Lindenstrauss
approximation (JLA) for its connection to the work of Johnson and Lindenstrauss (1984).
       JLA can be described by the following algorithm: fix a p  N and generate the matrices
RB , RP  Rp×n , where (RB , RP ) are composed of mutually independent Rademacher random vari-
ables that are independent of the data, i.e., their entries take the values 1 and -1 with probability
1/2. Next decompose A into A =                 1
                                               2 ( A1 A2   + A2 A1 ) for A1 , A2  Rn×k where A1 = A2 if A is
positive semi-definite.3 Let

                 ^ii = 1 RP XSxx                       ^ii = 1 RB A1 Sxx
                                              2
                              -1                                      -1                            -1
                 P               xi                and B                 xi                  RB A2 Sxx xi
                       p                                     p

where X = (x1 , . . . , xn ) . The Johnson-Lindenstrauss approximation to ^ is

                                                                  n
                                            ^JLA = ^ A^-                ^ii  2
                                                                        B   ^i,JLA ,
                                                                  i=1


       2           yi (yi -xi ^)             3
                                           ^ii     2
                                                 ^ii                         3
                                                                           ^ii     2
                                                                                 ^ii
                                        1 3P   +P                       1 3P   +P
where ^i,JLA =             ^       1-   p 1-P  ^ii     . The term       p 1-P  ^ii     removes a non-linearity bias intro-
                       1-Pii
duced by approximating Pii .
       Section 1.5 establishes asymptotic equivalence between ^JLA and ^. Appendix B.3 discusses
implementation details and numerically illustrates the trade-off between computation time and the
   3
    Interpretable choices of A1 and A2 are typically suggested by the structure of the problem; see, for
instance, the discussion in Example 4 of Section 2.



                                                              7
bias introduced by JLA for different choices of p under a range of sample sizes. Notably, we show
that JLA allows us to accurately compute a variance decomposition in a two-way fixed effects model
with roughly 15 million parameters ­ a scale comparable to the study of Card et al. (2013) ­ in
under an hour. A MATLAB package (Kline et al., 2019) implementing both the exact and JLA
versions of our estimator in the two-way fixed effects model is available online.


1.3     Relation to Existing Approaches
As discussed in Section 2, several literatures make use of bias corrections nominally predicated on
homoscedasticity. A common "homoscedasticity-only" estimator takes the form
                                                                    n
                                          ^HO = ^ A^-                          2
                                                                          Bii ^HO                        (4)
                                                                    i=1

       2        1      n         ^)2 is the degrees-of-freedom corrected variance estimator. A suf-
where ^HO =    n-k          - xi 
                       i=1 (yi
                                       ^HO is that there be no empirical covariance between i2
ficient condition for unbiasedness of                                                          and
                                                                                                 2
(Bii , Pii ). This restriction is in turn implied by the special cases of homoscedasticity where i does
not vary with i or balanced design where (Bii , Pii ) does not vary with i. In general, however, this
estimator will tend to be biased (see, e.g., Scheffe, 1959, chapter 10, or Appendix C.1.3).
    A second estimator, closely related to    ^, relies upon a jackknife bias-correction (Quenouille,
1949) of the plug-in estimator. This estimator can be written

                                                    n
                       ^JK = n^PI - n - 1                 ^PI,-i    where      ^PI,-i = ^-i A^-i .
                                      n
                                                    i=1

In Appendix C.1.3 we illustrate that jackknife bias-correction tends to over-correct and produce a
first order bias in the opposite direction of the bias in the plug-in estimator. This is analogous to
the upward bias in the jackknife estimator of V[  ^] which was derived by Efron and Stein (1981) and
shown by El Karoui and Purdom (2018) to be of first order importance for inference with many
Gaussian regressors.
   There are several proposed adaptations of the jackknife to long panels that can decrease bias
under stationarity restrictions on the regressors. Letting t(i)  {1, ..., T } denote the time period in
which an observation is observed, we can write the panel jackknife of Hahn and Newey (2004) as

                                                      T
                     ^PJK = T ^PI - T - 1                  ^PI,-t    where      ^PI,-t = ^-t A^-t
                                      T
                                                     t=1

    ^-t = (                   -1
and           i:t(i)=t xi xi )     i:t(i)=t xi yi   is the OLS estimator that excludes all observations from



                                                              8
period t. Dhaene and Jochmans (2015) propose a closely related split panel jackknife

                                       ^      ^
                        ^SPJK = 2^PI - PI,1 + PI,2         where   ^PI,j = ^j A^j
                                            2

and ^1 (and ^2 ) are OLS estimators based on the first half (and the last half) of an even number of
time periods. In Appendix C.1.3, we illustrate how short panels can lead these adaptations of the
jackknife to produce first order biases in the opposite direction of the bias in the plug-in estimator.


1.4     Finite Sample Properties
We now study the finite sample properties of the leave-out estimator   ^ and its infeasible analogue
      ^ A  ^-    n       2
 =               i=1 Bii i , which uses knowledge of the individual error variances. First, we note
that ^ is unbiased whenever each of the leave-one-out estimators  ^-i exists, which can equivalently
be expressed as the requirement that maxi Pii < 1. This condition turns out to also be necessary
for the existence of unbiased estimators, which highlights the need for additional restrictions on
the model or sample whenever some leverages equal one.

Lemma 1. 1. If maxi Pii < 1, then E[^] = .

   2. Unbiased estimators of  =  A exist for all A if and only if maxi Pii < 1.

    Next, we show that when the errors are normal, the infeasible estimator  is a weighted sum of
a series of non-central 2 random variables. This second result provides a useful point of departure
for our asymptotic approximations and highlights the important role played by the matrix

                                              -1/2
                                         ~ = Sxx     -1/2
                                         A         ASxx   ,

which encodes features of both the target parameter (which is defined by A) and the design matrix
Sxx .
                                                         ~, where 2
    Let 1 , . . . , r denote the non-zero eigenvalues of A                  2
                                                                  1  · · ·  r and each eigenvalue
appears as many times as its algebraic multiplicity. We use Q to refer to the corresponding matrix
                                    ~ = QDQ where D = diag(1 , . . . , r ). With these definitions
of orthonormal eigenvectors so that A
we have
                                                      r
                                          ^ A
                                             ^=            b2 ,
                                                           ^
                                                      =1

                                   1/2 ^
where ^b = (^
            b1 , . . . , ^
                         br ) = Q Sxx  contains r linear combinations of the elements in ^. The random
vector ^
       b and the eigenvalues 1 , . . . , r are central to both the finite sample distribution provided
below in Lemma 2 and the asymptotic properties of ^ as studied in Sections 3­5. Each eigenvalue


                                                  9
   ~ can be thought of as measuring how strongly  depends on a particular linear combination of
of A
                                                                                              -1
the elements in  relative to the difficulty of estimating that combination (as summarized by Sxx ).
As discussed in Section 5, when a few of these eigenvalues are large relative to the others, a form
of weak identification can arise.
                     2
Lemma 2. If i  N (0, i ), then
                                1/2
   1. ^
      b  N b, V[^
                b] where b = Q Sxx  ,

   2.  =        r
                 =1    b2 - V[^
                       ^      b]

       The distribution of  is a sum of r potentially dependent non-central 2 random variables
                                                                                        2
with non-centralities b = (b1 , . . . , br ) . In the special case of homoscedasticity (i =  2 ) and no
                            b  N 0,  2 Ir , which implies that the distribution of  is a weighted
signal (b = 0) we have that ^
sum of r independent central 2 random variables. The weights are the eigenvalues of A       ~, therefore
consistency of  follows whenever the sum of the squared eigenvalues converges to zero. The
next subsection establishes that the leave-out estimator remains consistent when a signal is present
(b = 0) and the errors exhibit unrestricted heteroscedasticity.


1.5       Consistency
We now drop the normality assumption and provide conditions under which ^ remains consistent.
To accommodate high dimensionality of the regressors we allow all parts of the model to change
with n:

                        yi,n = xi,n n + i,n                          (i = 1, . . . , n)

where xi,n  Rkn , Sxx,n =        n
                                 i=1 xi,n xi,n ,   E[i,n ] = 0, E[2       2
                                                                  i,n ] = i,n and n = n An n for some
sequence of known non-random symmetric matrices An  Rkn ×kn of rank rn . By treating xi,n and
An as sequences of constants, all uncertainty derives from the disturbances i,n : 1  i  n, n  1 .
This conditional perspective is common in the statistics literatures on ANOVA (Scheffe, 1959; Searle
et al., 2009) and allows us to be agnostic about the potential dependency among the {xi,n }n
                                                                                           i=1 and
An .4 Following standard practice we drop the n subscript in what follows. All limits are taken as
n goes to infinity unless otherwise noted.
       Our analysis makes heavy use of the following assumptions.
   4
    An unconditional analysis might additionally impose distributional assumptions on An and consider
¯ =  EA [An ] as the object of interest. The uncertainty in 
                                                            ^- ¯ can always be decomposed into components
         n
                ^             ¯                              ¯
attributable to  -  and  - . Because the behavior of  -  depends entirely on model choices, we leave
such an analysis to future work.


                                                       10
                               -2
Assumption 1. (i) maxi E[4
                         i ] + i  = O(1), (ii) there exist a c < 1 such that maxi Pii  c for
all n, and (iii) maxi (xi  )2 = O(1).

   Part (i) of this condition limits the thickness of the tails in the error distribution, as is typically
required for OLS estimation (see, e.g., Cattaneo et al., 2018, page 10). The bounds on (xi  )2 and
                2
Pii imply that ^i has bounded variance. Part (iii) is a technical condition that can be relaxed to
allow maxi (xi  )2 to increase slowly with sample size as discussed further in Section 7. From (ii) it
               k
follows that   n    c < 1 for all n.
   The following Lemma establishes consistency of ^.

                                                                        ^-  p
Lemma 3. If Assumption 1 and one of the following conditions hold, then       0.

                                                           ~2 ) =
  (i) A is positive semi-definite,  =  A = O(1), and trace(A                 r    2
                                                                              =1      = o(1).
          1
 (ii) A = 2 (A1 A2 + A2 A1 ) where 1 =  A1 A1  and 2 =  A2 A2  satisfy (i).

   The first condition of Lemma 3 establishes consistency of variance components given bounded-
ness of  and a joint condition on the design matrix Sxx and the matrix A. The second condition
shows that consistency of covariance components follows from consistency of variance components
that dominate them via the Cauchy-Schwarz inequality, i.e., 2 = ( A1 A2  )2  1 2 . In several
                                                     ~2 ) is of order r/n2 , which is necessarily small
of the examples discussed in the next section, trace(A
                                                                                       ~2 ) = o(1)
in large samples. A more extensive discussion of primitive conditions that yield trace(A
is provided in Section 7.
    We conclude this section by establishing asymptotic equivalence between the leave-out estimator
^ and its approximation 
                          ^JLA under the condition that p4 is large relative to sample size.

Lemma 4. If Assumption 1 is satisfied, n/p4 = o(1), V[       ^]-1 = O(n), and one of the following
conditions hold, then V[^]-1/2 (^JLA - ^ - Bp ) = op (1) where |Bp |  1 n Pii 2       2
                                                                                |Bii |i .
                                                                         p      i=1

                                        ^ A^] -  =        n       2
  (i) A is positive semi-definite and E[                  i=1 Bii i   = O(1).
                                                                                         V[^1 ]V[^2 ]
 (ii) A = 1
          2 (A1 A2 + A2 A1 ) where 1 =  A1 A1  and 2 =  A2 A2  satisfy (i) and                 ^ 2      = O(1).
                                                                                          nV[]

   Lemma 4 requires that ^ is not super-consistent and that the bias in the plug-in estimator is
asymptotically bounded, assumptions which can be shown to be satisfied in the examples introduced
in the next section. For variance components, the Lemma characterizes an approximation bias Bp
in ^JLA of order 1/p and provides an interpretable bound on Bp : the approximation bias is at
most 1/p times the bias in the plug in estimator ^ A^. For covariance components, asymptotic
equivalence follows when the variance components defined by A1 A1 and A2 A2 do not converge at
substantially slower rates than ^. Under this condition, the approximation bias is at most 1/p times
the average of the biases in the plug in estimators ^ A1 A1 ^ and ^ A2 A2 ^.


                                                   11
    These bounds on the approximation bias suggests that a p of a few hundred should suffice
for point estimation. However, unless n/p2 = o(1), the resulting approximation bias needs to be
accounted for when conducting inference. Specifically, one can lengthen the tails of the confidence
                                     1  n   ^ 2 ^ ^i,JLA
                                                       2
sets proposed in Sections 4 and 6 by p  i=1 Pii |Bii |     when relying on JLA.


2        Examples
We now consider four commonly encountered empirical examples where our proposed estimation
strategy provides an advantage over existing methods.

Example 1 (Coefficient of determination).
    Sewall Wright (1921) proposed measuring the explanatory power of a linear model using the
coefficient of determination. When xi includes an intercept, the object of interest and its corre-
sponding plug-in estimator can be written

                                                       2                                         ^ A^               2
                                  A                    X                                                           ^X, PI
               R2 =                      n   2   =     2
                                                                    ^ PI
                                                                and R 2
                                                                         =                   n                 =
                       A +         1
                                   n     i=1 i         y                               1
                                                                                       n     i=1 (yi     ¯)2
                                                                                                        -y           2
                                                                                                                    ^y
where
                          n                                           n                           n
                      1                                          1                           1
                A=                    ¯)(xi - x
                                (xi - x       ¯) ,          ¯=
                                                            x              xi ,       y
                                                                                      ¯=               yi .
                      n                                          n                           n
                          i=1                                        i=1                         i=1

                                                 2
Theil (1961) noted that the plug-in estimator of X is biased and proposed an adjusted R2 measure
that utilizes the homoscedasticity-only estimator in (4). The above choice of A yields Bii =
1         1                            n             k-1
n (Pii   -n ), which implies           i=1 Bii   =    n .    Hence, Theil's proposal can be written

                                                   2        ^ A^ - k-1  2
                                            2
                                          ^ adj   ^X,  HO            n ^HO
                                          R     =     2   =        2       .
                                                     ^y           ^y

                                                                           2
                                                                        ^ adj
                                                                      1-R              n-1
A rearrangement gives the familiar representation                         2
                                                                        ^ PI
                                                                                  =    n-k   which highlights that the adjusted
                                                                      1-R
estimator of R2 relates to the unadjusted one through a degrees-of-freedom correction.
                               2
    The leave-out estimator of X allows for unrestricted heteroscedasticity and can be found by
                   -1      1
noting that x
            ~i = ASxx xi = n (xi - x
                                   ¯), which yields

                                      2                                         n
                                 ^ 2 ^X                         2       1                         ^-i .
                                 R = 2               where     ^X     =               yi (xi - x
                                                                                               ¯) 
                                      ^y                                n
                                                                               i=1

In general, this estimator does not have an interpretation in terms of degrees-of-freedom correc-
tions. Instead, the explanatory power of the linear model is assessed using the empirical covariance
between leave-one-out predictions (xi - x¯) ^-i and the left out observation yi .


                                                                 12
Example 2 (Analysis of covariance).
    Since the work of Fisher (1925), it has been common to summarize the effects of experimen-
tally assigned treatments on outcomes with estimates of variance components. Consider a dataset
comprised of observations on N groups with Tg observations in the g -th group. The "analysis of
covariance" model posits that outcomes can be written

                  ygt = g + xgt  + gt                                    (g = 1, . . . , N, t = 1, . . . , Tg  2),

where g is a group effect and xgt is a vector of strictly exogenous covariates.
    A prominent example comes from Chetty et al. (2011) who study the adult earnings ygt of
       N
n=     g =1 Tg   students assigned experimentally to one of N different classrooms. Each student also
has a vector of predetermined background characteristics xgt . The variability in student outcomes
attributable to classrooms can be written:
                                                                   N
                                                     2       1                        2
                                                       =                Tg g - ¯
                                                             n
                                                                 g =1

            1     N
where ¯=    n     g =1 Tg g        gives the (enrollment-weighted) mean classroom effect.
    This model and object of interest can written in the notation of the preceding section (yi =
            2
xi  + i and   =  A ) by letting i = i(g, t) where i(·, ·) is bijective with inverse denoted (g (·), t(·)),
yi = ygt , i = gt ,

          xi = (di , xgt ) ,             = ( ,  ) ,       = (1 , . . . , N ) ,            di = (1{g=1} , . . . , 1{g=N } ) ,
and
                                                             n                                         n
                      Add     0                          1               ¯)(di - d
                                                                                 ¯) ,        ¯= 1
          A=                        where     Add =                (di - d                   d              di .
                       0      0                          n                                      n
                                                             i=1                                      i=1

                              2
Chetty et al. (2011) estimate   using a random effects ANOVA estimator (see e.g., Searle et al.,
2009) which is of the homoscedasticity-only type given in (4). As discussed in Section 1 and
Appendix C.1.3, this estimator is in general first order biased when the errors are heteroscedastic
and group sizes are unbalanced.
Special Case: No Common Regressors When there are no common regressors (xgt = 0 for all
                                  2
g, t), the leave-out estimator of   has a particularly simple representation:

                        N                                                                                  Tg
           2       1                             2        Tg             2             2         1
          ^
                 =                Tg    ^g - 
                                             ^
                                             ¯       - 1-               ^g      for   ^g
                                                                                            =                    (ygt - ^ g )2 ,   (5)
                   n                                      n                                   Tg - 1
                        g =1                                                                               t=1


                  1         Tg                       1   N
where ^g =       Tg         t=1 ygt ,   and ^
                                            ¯ =      n   g =1 Tg ^g .        This representation shows that if the model


                                                                       13
consists only of group specific intercepts, then the leave-out estimator relies on group level degrees-
of-freedom corrections. The statistic in (5) was analyzed by Akritas and Papadatos (2004) in the
                                            2
context of testing the null hypothesis that   = 0 while allowing for heteroscedasticity at the group
level.
Covariance Representation Another instructive representation of the leave-out estimator is in
terms of the empirical covariance
                                        n
                                  2              ~i                         ^-i = (^    ^-i ).
                                 ^  =         yi d  ^ -i    where                  -i , 
                                        i=1


The generalized regressor d  ~i can be described as follows: if there are no common regressors then
             ¯), which is analogous to Example 1. If the model includes common regressors then
~i = 1 (di - d
d    n
~i = 1 (di - d ¯) -                                       Tg
d                   ^ (xg(i)t(i) - x             ¯g = 1
                                   ¯g(i) ) where x             xgt and ^ is the coefficient vector from
         n                                                             Tg       t=1
an instrumental variables (IV) regression of di - d¯ on xg(i)t(i) - x
                                                                    ¯g(i) using xg(i)t(i) as an instrument.
                ~
The IV residual di is uncorrelated with xg(i)t(i) and the covariance between di and d       ~i is Add , which
ensures that the empirical covariance between yi = di  + xg(i)t(i)  + i and the generalized prediction
~i                                   2
d  ^ -i is an unbiased estimator of    .

Example 3 (Random coefficients).
    Group memberships are often modeled as influencing slopes in addition to intercepts (Kuh, 1959;
Hildreth and Houck, 1968; Raudenbush and Bryk, 2002; Arellano and Bonhomme, 2011; Graham
and Powell, 2012; Graham et al., 2018). Consider the following "random coefficient" model:

                ygt = g + zgt g + gt                                   (g = 1, . . . , N, t = 1, . . . , Tg  3).

    An influential example comes from Raudenbush and Bryk (1986), who model student mathe-
matics scores as a "hierarchical" linear function of socioeconomic status (SES) with school-specific
                                                                            1     N                      N
intercepts (g  R) and slopes (g  R). Letting ¯=                             n     g =1 Tg g    for n =   g =1 Tg ,     the student-
weighted variance of slopes can be written:

                                                           N
                                              2     1                                 2
                                                  =               Tg g - ¯                .
                                                    n
                                                           g =1

                                                                       2
In the notation of the preceding section we can write yi = xi  + i and   =  A where

                                                                                                         Add       0
             xi = (di , di zgt ) ,       = ( ,  ) ,                     = (1 , . . . , N ) ,       A=
                                                                                                          0        0


for yi , i , di , Add , and  as in the preceding example.
                                                                     2
    Raudenbush and Bryk (1986) use a maximum likelihood estimator of   predicated upon nor-

                                                                  14
                                                                        2
mality and homoscedastic errors. Swamy (1970) considers an estimator of   that relies on group-
level degrees-of-freedom corrections and is unbiased when the error variance is allowed to vary at
the group level, but not with the level of zgt . By contrast, the leave-out estimator is unbiased under
arbitrary patterns of heteroscedasticity.
Covariance Representation The leave-out estimator can be represented in terms of the empir-
ical covariance
                             n
                                                                                              zg(i)t(i) - z
                                                                                                          ¯g(i)
                 ^
                  2
                      =                ~i 
                                    ~i d
                                 yi z     ^-i     where       ~i = 1 (di - d
                                                              d            ¯),    z
                                                                                  ~i =                               ,
                                                                   n                         Tg(i)
                          i=1                                                                               ¯g(i) )2
                                                                                             t=1 (zg (i)t - z

             1        Tg                                             ~i z
and z
    ¯g =    Tg        t=1 zgt .
                      Demeaning zg(i)t(i) at the group level makes d    ~i uncorrelated with di and
scaling by the group variability in zg(i)t ensures that the covariance between d  ~i z
                                                                                     ~i and di zg(i)t(i)
is Add . This implies that the empirical covariance between yi = di  + zg(i)t(i) di  + i and the
                          ~i                                 2
generalized prediction z
                       ~i d  ^-i is an unbiased estimator of   .

Example 4 (Two-way fixed effects).
    Economists often study settings where units possess two or more group memberships, some of
which can change over time. A prominent example comes from Abowd et al. (1999) (henceforth
AKM) who propose a panel model of log wage determination that is additive in worker and firm
fixed effects. This so-called "two-way" fixed effects model takes the form:

                 ygt = g + j (g,t) + xgt  + gt                             (g = 1, . . . , N, t = 1, . . . , Tg  2)         (6)

                                                                                                                         N
where the function j (·, ·) : {1, . . . , N }×{1, . . . , maxg Tg }  {0, . . . , J } allocates each of n =               g =1 Tg
person-year observations to one of J + 1 firms. Here g is a "person effect", j (g,t) is a "firm effect",
xgt is a time-varying covariate, and gt is a time-varying error. In this context, the mean zero
assumption on the errors gt can be thought of as requiring both the common covariates xgt and
the firm assignments j (·, ·) to obey a strict exogeneity condition.
    Interest in such models often centers on understanding how much of the variability in log wages
is attributable to firms (see, e.g., Card et al., 2013; Song et al., 2017). AKM summarize the firm
contribution to wage inequality via the following two parameters:

                                   N   Tg                                              N    Tg
                   2        1                             ¯   2                    1                        ¯ g
                          =                     j (g,t) -         and ,          =                j (g,t) - 
                            n                                                      n
                                  g =1 t=1                                             g =1 t=1


      ¯=     1        N          Tg                                     2
where        n        g =1       t=1 j (g,t) .   The variance component   measures the contribution of firm wage
variability to inequality, while the covariance component , measures the additional contribution
of systematic sorting of high wage workers to high wage firms.


                                                                     15
       To represent this model and the corresponding objects of interest in the notation of the preceding
                        2
section (yi = xi  + i ,   =  A  , and , =  A,  ), let

        xi = (di , fi , xgt ) ,  = ( ,  ,  ) ,  = (1 , . . . , N ) + 1N 0 ,  = (1 . . . , J ) - 1J 0 ,

for yi , i , and di as in the preceding examples, fi = (1{j (g,t)=1} , . . . , 1{j (g,t)=J } ) ,

                                                                     n                                            n
                                       
                        0     0       0
                                                                1                ¯)(fi - f
                                                                                         ¯) ,        ¯= 1
                A =    
                       0     Af f     0
                                       
                                            where      Af f   =            (fi - f                   f                fi ,
                        0     0       0
                                                                n                                       n
                                                                     i=1                                      i=1
and
                                                                           n
                                             
                     1 0             Adf   0
                                                                     1               ¯)(fi - f
                                                                                             ¯) .
               A,   = Adf             0    0
                                             
                                                  where      Adf =             (di - d
                     2 0              0    0
                                                                     n
                                                                         i=1


Computation of the Johnson-Lindenstrauss approximation can be facilitated using the representa-
tions A = Af Af and A, = 1
                         2 (Ad Af + Af Ad ) where

                                                                                           ¯              ¯
                                                                                                           
                                      0      0        0                               d1 - d   ...   dn - d
                    Af =     1
                              n
                                        ¯
                                   f1 - f   ...         ¯
                                                   fn - f     and Ad =           1
                                                                                  n    0
                                                                                      
                                                                                                0      0 
                                                                                                           
                                                                                                              .
                                      0      0        0                                  0      0      0


       Addition and subtraction of 0 in  amounts to the normalization, 0 = 0, which has no effect on
the variance components of interest. As Abowd et al. (1999, 2002) note, least squares estimation
of (6) requires one normalization of the  vector within each set of firms connected by worker
mobility. For simplicity, we assume all firms are connected so that only a single normalization is
required.5
                                                        2
Covariance Representation Abowd et al. (1999) estimated   and , using the naive plug-in
           ^  ^    ^    ^
estimators  A  and  A,  which are, in general, biased. Andrews et al. (2008) proposed the
"homoscedasticity-only" estimators of (4). These estimators are unbiased when the errors i are
independent and have common variance. Bonhomme et al. (2019) propose a two-step estimation
approach that is consistent in the presence of heteroscedasticity when the support of firm wage
effects is restricted to a finite number of values and each firm grows large with the total sample
size n. Our leave-out estimators, which avoid both the homoscedasticity requirement on the errors
and any cardinality restrictions on the support of the firm wage effects, can be written compactly
   5
     Bonhomme et al. (2019) study a closely related model where workers and firms each belong to one of
a finite number of types and each pairing of worker and firm type is allowed a different mean wage. These
mean wage parameters are shown to be identified when each worker type moves between each firm type with
positive probability, enabling estimation even when many firms are not connected.




                                                               16
as covariances taking the form
                              n                                   n
                       2                  -1   ^-i ,                           -1    ^-i .
                      ^  =         yi xi Sxx A            ^, =          yi xi Sxx A, 
                             i=1                                  i=1


Notably, these estimators are unbiased whenever the leave out estimator ^-i can be computed,
regardless of the distribution of firm sizes.
                                                            2
Special Case: Two time periods A simpler representation of ^  is available in the case where
only two time periods are available and no common regressors are present (Tg = 2 and xgt = 0 for
all g, t). Consider this model in first differences

                       yg = fg  + g                                      (g = 1, . . . , N )

                                                                                             2
where yg = yg2 - yg1 , g = g2 - g1 , and fg = fi(g,2) - fi(g,1) . The leave-out estimator of 
applied to this differenced representation of the model is:

                               N
                        2                 ~  ^                 ~          -1
                       ^  =           yg  fg -g        where   fg = Af f Sf f fg .
                              g =1


Note that the quantities Sf f and ^-g correspond respectively to Sxx and ^-i in the first differ-
enced model.
                                                           2
Remark 4. The leave-out representation above reveals that ^  is not only unbiased under arbitrary
heteroscedasticity and design unbalance, but also under arbitrary correlation between g1 and g2 .
                                                                                         2
The same can be shown to hold for ^, . Furthermore, this representation highlights that ^  only
depends upon observations with fg = 0 (i.e., firm "movers").



3     Inference on Quadratic Forms of Fixed Rank
While the previous section emphasized variance components where the rank r of A was increasing
with sample size, we first study the case where r is fixed. Problems of this nature often arise
when testing a few linear restrictions or when conducting inference on linear combinations of the
regression coefficients, say v  . In the case of two-way fixed effects models of wage determination,
the quantity v  might correspond to the difference in mean values of firm effects between male
and female workers (Card et al., 2015) or to the coefficient from a projection of firm effects onto
firm size (Bloom et al., 2018). A third use case, discussed at length by Cattaneo et al. (2018), is
where v  corresponds to a linear combination of a few common coefficients in a linear model with
high dimensional fixed effects that are regarded as nuisance parameters.



                                                        17
   To characterize the limit distribution of ^ when r is small, we rely on a representation of  as
a weighted sum of squared linear combinations of the data:  ^= r  ^        b2 - V
                                                                                ^ [^
                                                                                   b ] where
                                                                    =1

                                        n                                    n
                                  ^                       ^ [^                          2
                                  b=         wi y i   and V  b] =                wi wi ^i
                                       i=1                               i=1

                                   -1/2
for wi = (wi1 , . . . , wir ) = Q Sxx   xi . The following theorem characterizes the asymptotic distribu-
tion of  while providing conditions under which ^
        ^                                               b is asymptotically normal and V^ [^
                                                                                           b] is consistent.

Theorem 1. If Assumption 1 holds, r is fixed, and maxi wi wi = o(1), then
                         d
        b]-1/2 (^
   1. V[^       b - b) -                        1/2
                        N (0, Ir ) where b = Q Sxx  ,
                   p
        b]-1 V
   2. V[^    ^ [^
                b] - Ir ,
              r
   3. ^=
               =1       b2 - V[^
                        ^      b ] + op (V[^]1/2 ),

    The high-level requirement of this theorem that maxi wi wi = o(1) is a Lindeberg condition
ensuring that no observation is too influential. One can think of maxi wi wi as measuring the
inverse effective sample size available for estimating b: when the weights are equal across i, the
            n                              2            1                1       n               r
equality    i=1 wi wi   = Ir implies that wi =          n.       Since   n       i=1 wi wi   =   n,   the requirement that
maxi wi wi = o(1) is implied by a variety of primitive conditions that limit how far a maximum is
from the average (see, e.g., Anatolyev, 2012, Appendix A.1). Note that Theorem 1 does not apply
                                                             r
to settings where r is proportional to n because maxi wi wi  n .
    In the special case where A = vv for some non-random vector v , Theorem 1 establishes that
                        ^ [      -1
                           ^] = Sxx   n          2  -1
the variance estimator V              i=1 xi xi ^i Sxx enables consistent inference on the linear
combination v  using the approximation

                                              v (^ - )       d
                                                             -
                                                              N (0, 1).                                                (7)
                                                  ^ [
                                                 vV  ^]v

To derive this result we assumed that maxi Pii  c for some c < 1, whereas standard Eicker-White
variance estimators generally require that maxi Pii  0 and Cattaneo et al. (2018) establish an
                                                                                 ^ [
asymptotically valid approach to inference in settings where maxi Pii  1/2. Thus V  ^] leads to
valid inference under weaker conditions than existing versions of Eicker-White variance estimators.
Remark 5. Theorem 1 extends classical results on hypothesis testing of a few linear restrictions,
say, H0 : R = 0, to allow for many regressors and heteroscedasticity. A convenient choice of A
                                -1    -1                                r ×k
for testing purposes is 1
                        r R (RSxx R ) R where r , the rank of R  R           , is fixed. Under H0 , the
                             ^                                             2
asymptotic distribution of  is an equally weighted sum of r central  random variables. This
distribution is known up to V[^b] and a critical value can be found through simulation. For a recent

                                                         18
contribution to this literature, see Anatolyev (2012) who allows for many regressors but considers
the special case of homoscedastic errors.



4      Inference on Quadratic Forms of Growing Rank
We now turn to the more challenging problem of conducting inference on  when r increases with n,
as in the examples discussed in Section 2. These results also enable tests of many linear restrictions.
For example, in a model of gender-specific firm effects of the sort considered by Card et al. (2015),
testing the hypothesis that men and women face identical sets of firm fixed effects entails as many
equality restrictions as there are firms.


4.1      Limit Distribution
                                                           n         B                                     -1
In order to describe the result we introduce x
                                             i =            =1 Mi 1-P x where Mi =            1{i= } - xi Sxx x.
                                                              Bii
Note that x
          i gives the residual from a regression          of 1- Pii xi on xi . Therefore,
                                                                                x
                                                                                i = 0 when the
regressor design is balanced. The contribution of x                    ^
                                                  i to the behavior of  is through the estimation
      n       2
of    i=1 Bii i ,   which can be ignored in the case where the rank of A is bounded. When the rank
of A is large, as implied by condition (ii) of Theorem 2 below, this estimation error can resurface
in the asymptotic distribution. One can think of the eigenvalue ratio in (ii) as the inverse effective
                                                  2
rank of A~: when all the eigenvalues are equal r1 2 = 1 .
                                                                r
                                                     =1

                              -1          ^=                 n         ^-i .
Theorem 2. Recall that x
                       ~i = ASxx xi where                    i=1 yi x
                                                                    ~i         If Assumption 1 holds and the
following conditions are satisfied

                           ^]-1 max (~                                         2
                     (i) V[          xi  )2 + (xi  )2 = o(1),        (ii)      1
                                                                               r    2   = o(1),
                                 i
                                                                                =1 


       ^]-1/2 (      d
               ^ - ) -
then V[               N (0, 1).

   The proof of Theorem 2 relies on a variation of Stein's method developed in Sølvsten (2019)
and a representation of ^ as a second order U-statistic, i.e.,

                                                 n
                                            ^=
                                                          Ci yi y                                           (8)
                                                 i=1 =i


where Ci = Bi - 2-1 Mi           -1
                                Mii Bii + M -1 Band Bi = xi Sxx-1    -1
                                                                  ASxx  x . The proof shows that
the "kernel" Ci varies with n in such a way that ^ is asymptotically normal whether or not ^ is a
degenerate U-statistic (i.e., whether or not  is zero).


                                                     19
   One representation of the variance appearing in Theorem 2 is
                                         n                             n
                                                           2    2
                               V[^] =           xi  - x
                                               2~     i         i +2            Ci2 i
                                                                                    2 2
                                                                                       .
                                         i=1                           i=1 =i

                                                      2                 n
Note that this variance is bounded from below by mini i                       xi  )2 +(
                                                                        i=1 (2~        xi  )2 since n
                                                                                                    i=1 x
                                                                                                        ~i  x
                                                                                                            i    =
                                                                      2         2
0. Therefore (i) will be satisfied whenever maxi                (~
                                                                 xi  ) + (xi  ) is not too large compared        to
  n
       xi  )2 + (
  i=1 (~         xi  )2 .   As in Theorem 1, (i) is implied by a variety of primitive conditions that limit
how far a maximum is from the average, but since (i) involves a one dimensional function of xi it
can also be satisfied when r is large. A particularly simple case where (i) is satisfied is when  = 0;
further cases are discussed in Section 7.
Remark 6. Theorem 2 can be used to test a large system of linear restrictions of the form H0 : R =
0 where r   is the rank of R  Rr×k . Under this null hypothesis, choosing A = 1        -1    -1
                                                                                r R (RSxx R ) R
          ^]-1/2  d
                 ^-                                               ~ are equal to 1 . The existing
implies V[         N (0, 1) since all the non-zero eigenvalues of A                             r
literature allows for either heteroscedastic errors and moderately few regressors (Donald et al.,
2003, k 3 /n  0) or homoscedastic errors and many regressors (Anatolyev, 2012, k/n  c < 1).
When coupled with the estimator of V[^] presented in the next subsection, this result enables tests
with heteroscedastic errors and many regressors.
Remark 7. Theorem 2 extends some common results in the literature on many and many weak
instruments (see, e.g., Chao et al., 2012) where the estimators are asymptotically equivalent to
                                                            ~ = Ir /r and r  , in which case
quadratic forms. The structure of that setting is such that A
condition (ii) of Theorem 2 is automatically satisfied.


4.2     Variance Estimation
In order to conduct inference based on the normal approximation in Theorem 2 we now propose
an estimator of V[^]. The U-statistic representation of ^ in (8) implies that the variance of ^ is

                                                           2
                                         n                                 n
                                                              2
                              V[^] = 4                 Ci x   i +2               Ci2 i
                                                                                     2 2
                                                                                        .
                                         i=1      =i                    i=1 =i

                            2 n               2 n
Naively replacing {xi , i    }i=1 with {yi , ^i }i=1 in the above formula to form a plug-in estimator of
  ^] will, in general, lead to invalid inferences as   2 2                          2 2
V[                                                    ^i ^ is a biased estimator of i  . For this reason,
we consider estimators of the error variances that rely on leaving out more than one observation.
Since this approach places additional restrictions on the design, Appendix C.5.1 describes a simple
adjustment which leads to conservative inference in settings where these restrictions do not hold.
Sample Splitting Our specific proposal is an estimator that exploits two independent unbiased


                                                           20
                                                                                                                            n
estimators of xi  that are also independent of yi . We denote these estimators xi  -i,s =                                    =i Pi ,s y
for s = 1, 2, where Pi         ,s   does not (functionally) depend on the {yi }n
                                                                               i=1 . To ensure independence
between xi  -i,1 and xi  -i,2 , we require that Pi ,1 Pi           ,2   = 0 for all . Employing these split sample
                                                           2
estimators, we create a new set of unbiased estimators for i :
                                                                           
                                                                           y (y - x 
              2                                               2             i i    i -i,1 ),              if Pi   ,1   = 0,
             ~i = yi - xi  -i,1          yi - xi  -i,2   and ^i, -       =
                                                                           y (y - x       ),              if Pi        = 0,
                                                                                i     i      i -i,2               ,1

       2                             2
where ^i, - is independent of y and ~i is a cross-fit estimator of the form considered in Newey and
                                                                                   2 2
Robins (2018). These cross-fit estimators can be used to construct an estimator of i  that, under
certain design conditions, will be unbiased. Letting Pim,- = Pim,1 1{Pi ,1 =0} + Pim,2 1{Pi ,1 =0} denote
                                       2        ~      2     n
the weight observation m receives in  ^i, - and Ci = Ci +2       Cmi Cm (Pmi,1 Pm ,2 + Pmi,2 Pm ,1 ),
                                                                             m=1
we define
                                
                                  2
                                 ^i, - ·^ 2,-i ,                   if Pim,- P         m,-i   = 0 for all m,
                                
                                 ~2 · ^2 ,
                                
                                
                                                                   else if Pi         + Pi         = 0,
                        2 2          i     ,- i                                 ,1           ,2
                        i  =
                                  2
                                 ^i, - · ~2,                       else if P    i,1   +P     i,2   = 0,
                                
                                  2
                                              ¯)2 · 1{C
                                
                                     - · (y - y
                                 ^i,                  ~i   <0} ,   otherwise.

                                                                                          2
The first three cases in the above definition correspond respectively to pairs where (i) ^i, - and
^ 2,-i are independent, (ii) xi  -i,1 and xi  -i,2 are independent of y , and (iii) x  -
                                                                                                                  ,1   and x  -         ,2
are independent of yi . When any of these three cases apply, we obtain an unbiased estimator of
2 2
i  . For the remaining set of pairs B = {(i, ) : Pim,- P                     m,-i         = 0 for some m, Pi           ,1   + Pi   ,2   =
0, P   i,1   +P   i,2   = 0} that comprise the fourth case we rely on an unconditional variance estimator
                                     2 2
which leads to a biased estimator of i  and conservative inference.
Design Requirements Constructing the above split sample estimators places additional require-
ments on the design matrix Sxx . We briefly discuss these requirements in the context of Examples
2, 3, and 4. In the ANOVA setup of Example 2, leave-one-out estimation requires a minimum
group size of two, whereas existence of {xi  -i,s }s=1,2 requires groups sizes of at least three. Con-
servative inference can be avoided (i.e., the set B will be empty) when the minimum group size is
at least four. In the random coefficients model of Example 3, minimum group sizes of three and
five are sufficient to ensure feasibility of leave-one-out estimation and existence of {xi  -i,s }s=1,2 ,
respectively. Conservativeness can be avoided with a minimum group size of seven.
   In the first differenced two-way fixed effects model of Example 4, the predictions {xi  -i,s }s=1,2
are associated with particular paths in the worker-firm mobility network and independence requires
that these paths be edge-disjoint. Menger's theorem (Menger, 1927) implies that {xi  -i,s }s=1,2


                                                            21
exists if the design matrix has full rank when any two observations are dropped. Menger's theorem
also implies that conservativeness can be avoided if the design matrix has full rank when any three
observations are dropped. In our application, we use Dijkstra's algorithm to find the paths that
generate {xi  -i,s }s=1,2 (see Appendix B.4 for further details).
                                                   2 2
Consistency The following lemma shows that i         can be utilized to construct an estimator of
V[^] that delivers consistent inference when sufficiently few pairs fall into B and provides conser-
vative inference otherwise.
                                                                                      n
Lemma 5. For s = 1, 2, suppose that xi  -i,s satisfies (unbiasedness)                  =i Pi ,s x    = xi  , (sample
splitting) Pi ,1 Pi   ,2   = 0 for all , and (projection property) max (Ps Ps ) = O(1) where Ps = (Pi ,s )i,
is the hat-matrix corresponding to xi  -i,s . Let
                                                             2
                                            n                           n
                                ^ [^] = 4                        2                  2 2
                                                                                 ~i i
                                V                         Ci y  ~i -2            C     .
                                            i=1      =i                 i=1 =i


                                                                               ^-
                                                                                       d
   1. If the conditions of Theorem 2 hold and |B| = O(1), then               ^  ^  1/2 -
                                                                                           N (0, 1).
                                                                             V [ ]


   2. If the conditions of Theorem 2 hold, then lim inf n P   ^ ± z V
                                                                    ^ [^]1/2                            1 -  where
        2
       z    denotes the (1 - )'th quantile of a central           2
                                                                  1   random variable.

                       ^ [
    In the formula for V  ^], the first term can be seen as a plug-in estimator and standard results
for quartic forms imply that the expectation of this term is V[   ^] + 2 n          ~ 2 2
                                                                           i=1   =i Ci i  . Hence,
the second term is a bias correction which completely removes the bias when B =  and leaves
                                                                                       ^ [
a positive bias otherwise. In Appendix C.5.1 we establish validity of an adjustment to V  ^] that
utilizes an upward biased unconditional variance estimator for observations where it is not possible
to construct {xi  -i,s }s=1,2 .
Remark 8. The purpose of the condition |B| = O(1) in the above lemma is to ensure that the bias
   ^ [
of V  ^] grows small with the sample size. Because the bias of V
                                                               ^ [^] is non-negative, inference based
   ^ [
on V  ^] remains valid even when this condition fails, as stated in the second part of Lemma 5.
In practice, it may be useful for researchers to calculate the fraction of pairs that belong to B to
gauge the extent to which inference might be conservative. Similarly, it may be useful to compute
the share of observations where it is not possible to construct {xi  -i,s }s=1,2 to investigate whether
upward bias in the standard error could lead to power concerns.




                                                             22
5       Weakly Identified Quadratic Forms of Growing Rank
In some settings where r grows with the sample size, condition (ii) of Theorem 2 may not apply.
For example in two-way fixed effects models, it is possible that "bottlenecks" arise in the mobility
network that lead the largest eigenvalues to dominate the others.
    This section provides a theorem which covers the case where some of the squared eigenvalues
2           2
           are large relative to their sum r=1 2 . To motivate this assumption, note that each
1 , . . . , r
              ~ measures how strongly  depends on a particular linear combination of the elements
eigenvalue of A
                                                                                    -1
of  relative to the difficulty of estimating that combination (as summarized by Sxx    ). From
                ~2       r    2                                ^
Lemma 3, trace(A ) =          governs the total variability in . Therefore, Theorem 3 covers the
                                =1
case where  depends strongly on a few linear combinations of  that are imprecisely estimated
relative to the overall sampling uncertainty in ^. The following assumption formalizes this setting.

Assumption 2. There exist a c > 0 and a known and fixed q  {1, . . . , r - 1} such that

                                 2
                                 q +1                          2
                                                               q
                                  r    2   = o(1)   and        r    2   c   for all n.
                                   =1                           =1 

    Assumption 2 defines q as the number of squared eigenvalues that are large relative to their sum.
Equivalently, q indexes the number of nuisance parameters in b that are weakly identified relative
to their influence on  and the uncertainty in ^. The assumption that q is known is motivated by
our discussion of Examples 1­4 in Section 7 and the theoretical literature on weak identification,
which typically makes an ex-ante distinction between strongly and weakly identified parameters
(e.g., Andrews and Cheng, 2012). In Section 6.2 we offer some guidance on choosing q in settings
where it is unknown.


5.1       Limit Distribution
Given knowledge of q , we can split        ^ into a known function of b ^q = (^
                                                                              b1 , . . . , ^
                                                                                           bq ) and ^q where
^
b1 , . . . , ^
             bq are OLS estimators of the weakly identified nuisance parameters:

                         n
                  ^q =
                  b            wiq yi ,                                 wiq = (wi1 , . . . , wiq ) ,
                         i=1
                                 q                                                n
                  ^q = ^-              b2 - V
                                      (^    ^ [^
                                               b ]),                    ^ [^
                                                                        V  b] =         wi wi  2
                                                                                              ^i .
                                =1                                                i=1

     The main difficulty in proving the following Theorem is to show that the joint distribution of
 ^
(bq , ^q ) is normal, which we do using the same variation of Stein's method that was employed for
Theorem 2. The high-level conditions involve x
                                             ~iq and x
                                                     iq which are the parts of x
                                                                               ~i and x
                                                                                      i that


                                                          23
pertain to ^q and are defined in the proof of Theorem 3.

Theorem 3. If maxi wiq wiq = o(1), V[^q ]-1 maxi (~
                                                  xiq  )2 + (xiq  )2 = o(1), and Assumptions 1
and 2 hold, then

         ^q , ^q ) ]-1/2 (b
                          ^q , ^q ) - E[(b
                                         ^q ,        d
                                              ^q ) ] -
   1. V[(b                                            N 0, Iq+1

                q
   2. ^=
                 =1        b2 - V[^
                           ^      b ] +^q + op (V[^]1/2 )

   Theorem 3 provides an approximation to ^ in terms of a quadratic function of q asymptotically
normal random variables and a linear function of one asymptotically normal random variable. Here,
                       ^q ] = (b1 , . . . , bq ) serve as nuisance parameters that influence both  and the
the non-centralities E[b
shape of the limiting distribution of ^ - . The next section proposes an approach to dealing with
these nuisance parameters that provides asymptotically valid inference on  for any value of q .


5.2     Variance Estimation
                                              ^q , 
In Theorem 3 the relevant variance is q := V[(b    ^q ) ],

                                                                                                                     
                   n                        2                                                          2
                                    wiq wiq i                          2wiq              =i Ci q x     i
           q =                                                                           2                           ,
                                                     2                                      2             2   2 2
                   i=1 2wiq          =i Ci q x       i     4         =i Ci q x             i  +2      =i Ci q i 


where Ci   q   is defined in the proof of Theorem 3. Our estimator of this variance reuses the split
sample estimators introduced for Theorem 2:
                                                                                                                     
                       n                      2                                                        2
                                     wiq wiq ^i                         2wiq             = i Ci q y   ~i
               ^q =
                                                                                     2                               
                                                      2                                   2              ~2 2 2
                       i=1   2wiq       = i Ci q y   ~i 4            = i Ci q y          ~i -2        =i Ci q i 


      ~i           2 2
where C    q   and i  are defined in the proof of the next lemma which shows consistency of this
variance estimator.
                                                                          n
Lemma 6. For s = 1, 2, suppose that xi  -i,s satisfies                     =i Pi ,s x        = xi  , Pi ,1 Pi   ,2   = 0 for all ,
                                                                                                                -1 ^ p
and max (Ps Ps ) = O(1). If the conditions of Theorem 3 hold and |B| = O(1), then                               q q  - Iq+1 .

Remark 9. As in the case of variance estimation for Theorem 2, it may be that the design does not
allow for construction of the predictions xi    and xi     used in  ^q . For such cases, Appendix
                                                         -i,1            -i,2
C.5.1 proposes an adjustment to ^q which has a positive definite bias and therefore leads to valid
(but conservative) inference when coupled with the inference method discussed in the next section.




                                                                24
6       Inference with Nuisance Parameters
In this section, we develop a two-sided confidence interval for  that delivers asymptotic size control
                                                                                               ^q and
conditional on a choice of q . Our proposal involves inverting a minimum distance statistic in b
^q , which Theorem 3 implies are jointly normally distributed. To avoid the conservatism associated

with standard projection methods (e.g., Dufour and Jasiak, 2001), we seek to adjust the critical
                                                           ^q , 
value downwards to deliver size control on  rather than E[(b    ^q ) ]. However, unlike in standard
                                                                                              ^q ].
projection problems (e.g., the problem of subvector inference),  is a nonlinear function of E[b
To accommodate this complication, we use a critical value proposed by Andrews and Mikusheva
(2016) that depends on the curvature of the problem.


6.1     Inference With Known q
                                                                                               ^q , 
The confidence interval we consider is based on inversion of a minimum-distance statistic for (b    ^q )
using the critical value proposed in Andrews and Mikusheva (2016). For a specified level of confi-
dence, 1 - , we consider the interval
                                                q                                    q
                   
                 ^,q                                  2 + q ,                              2 + q
                 C   =           min                  b                max                 b
                          (b        q ,
                            1 ,...,b   q ) E
                                           ^ ,q                   1 ,...,b
                                                                (b        q ,q ) E
                                                                                 ^ ,q
                                                =1                                    =1
where
                                                      ^q - bq
                                                      b           -1
                                                                         ^ q - bq
                                                                         b
                 ^ ,q =
                 E        (bq , q )  Rq+1 :                      ^q
                                                                                        2
                                                                                       z, ^q   .
                                                      ^q - q
                                                                         ^q - q
                                                                         

    The critical value function, z, , depends on the maximal curvature, , of a certain manifold
(exact definitions of z, and  are given in Appendix C.6). Heuristically,  can be thought of as
                                                      ^q ] on the shape of 
summarizing the influence of the nuisance parameter E[b                    ^'s limiting distribution.
              2     2                                                   2
Accordingly, z  := z, 0 is equal to the (1 - )'th quantile of a central 1 random variable. As
     2
  , z, approaches the (1 - )'th quantile of a central 2
                                                      q +1 random variable. This upper
limit on z, is used in the projection method in its classical form as popularized in econometrics
by Dufour and Jasiak (2001), while the lower limit z would yield size control if  were linear in
  ^q ].
E[b
                                                      
                                                     ^0                 ^ ± z V
                                                                              ^ [^]1/2 ]. When q = 1,
    When q = 0, the maximal curvature is zero and C      simplifies to [
                                     ^ ^
                                2|1 |V[b1 ]
the maximal curvature is ^1 =    1/2
                                   ^ [   2 1/2 where 
                                      ^1 ]
                                                     ^ is the estimated correlation between ^  b1 and
                                   V         (1-^ )
^1 . This curvature measure is intimately related to eigenvalue ratios previously introduced, as 
                                                                                                 ^2
                                                                                                  1 is
                             2
                           21
approximately equal to     r   2    when the error terms are homoscedastic and  = 0. A closed form
                            =2 
expression for the q = 1 confidence interval is provided in Appendix C.6. When q > 1, inference
relies on solving two quadratic optimization problems that involve q + 1 unknowns, which can be
achieved reliably using standard quadratic programming routines.


                                                         25
   The following lemma shows that a consistent variance estimator as proposed in Lemma 6 suffices
for asymptotic validity under the conditions of Theorem 3 and Appendix C.5.1 establishes validity
when only a conservative variance estimator is available.
            -1 ^                              p
Lemma 7. If q q  - Iq+1 and the conditions of Theorem 3 hold, then

                                                                                
                                                                              ^,q
                                                                  lim inf P   C    1 - .
                                                                   n

                                                                                      ^ ,q and
  The confidence interval studied in Lemma 7 constructs a q + 1 dimensional ellipsoid E
                                         1, . . . , b
                                                    q, q )  q     2  
maps it through the quadratic function (b                    =1  b + q . This approach ensures
uniform coverage over any possible values of the nuisance parameters b1 , . . . , bq which are imprecisely
estimated relative to overall sampling uncertainty in  ^.

Remark 10. An alternative to Lemma 7 is to conduct inference using a first-order Taylor expansion
of q  ^  b2 + 
        =1
              ^q . This so-called "Delta method" approach is asymptotically equivalent to using
the confidence interval [^ ± z V
                               ^ [^]1/2 ] studied in Section 4. However, the Delta method is not
uniformly valid in the presence of nuisance parameters as approximate linearity can fail when
min     q      b2 = O(1). Section 7 introduces a stochastic block model with q = 1 and characterizes
b2
 1 as the squared difference in average firm effects across two blocks multiplied by the number of
between block movers. Thus the Delta method will potentially undercover unless there are strong
systematic differences between the two blocks.


6.2            Choosing q
The preceding discussion of inference considered a setting where the number of weakly identified
parameters was known in advance. In some applications, it may not be clear ex ante what value
q takes. In such situations researchers may wish to report confidence intervals for two consecutive
values of q (or their union). This heuristic serves to minimize the influence of the specific value of
                                                                               
                                                                              ^,q
q picked, and both our simulations and empirical application suggest that C        barely varies with
                  2
                 q+1               1
q when            r        2   <   10 .       Consequently, little power is sacrificed by taking the union.
                   =1   
   This observation also suggests a heuristic threshold for choosing q ; namely, to let q be such that
    2                                     2                                             2
  q                1                 q+1                  1                            1               1
  r        2       10   and           r           2   <   10 ,   with q = 0 when      r        2   <   10 .   A similar threshold rule can be
   =1                                  =1                                              =1   
motivated under a slight strengthening of Assumption 2 which allows one to learn q from the data.

Assumption 2 . There exist a c > 0, an                                      > 0, and a fixed q  {1, . . . , r - 1} such that

                                                      2
                                                      q +1                            2
                                                                                      q
                                                      r    2     = O(r- )    and      r   2        c          for all n.
                                                       =1                              =1 




                                                                                26
    A threshold based choice of q is the unique q
                                                ^ for which

                               2
                               q
                               ^+1                          2
                                                            q
                                                            ^
                               r   2    < cr     and        r    2    cr    for some cr  0,
                                =1                           =1 

                        2
                       1
with q
     ^ = 0 when       r        2   < cr . Under Assumption 2 , q
                                                               ^ = q in sufficiently large samples provided
                       =1   
that cr is chosen so that cr r  . This condition is satisfied when cr shrinks slowly to zero, e.g.,
when cr  1/ log(r).


7     Verifying Conditions
We now revisit the examples of Section 2 and verify the conditions required to apply our theoretical
results. Appendix C.7 provides further details on these calculations.
                                                                        2
Example 1. (Coefficient of determination, continued) Recall that  = X      =  A where A =
1  n                                      - 1
                            ~ = 1 (Ik - nSxx x/2     - 1 /2
n  i=1 (xi - x
             ¯)(xi - x
                     ¯) and A   n                ¯x
                                                  ¯ Sxx ). Suppose Assumption 1 holds.
                                                                             1
Consistency Consistency follows from Lemma 3 since  =                        n   for = 1, . . . , r where r = dim(xi ) -
              ~2 ) = r/n2  1/n = o(1).
1. Thus trace(A
                                                                                 1
Limit Distribution If dim(xi ) is fixed, then wi wi = Pii -                      n   and Theorem 1 applies under the
standard "textbook" condition that maxi Pii = o(1). If dim(xi )  , then Theorem 2 applies if
   ^]-1 maxi (xi  )2 = o(1) which follows if, e.g., maxi 1  n
V[                                                        r  =1 |Mi | = o(1) where Mi = 1{i= } -
    -1
xi Sxx x (this condition holds in the next two examples). Equality among all eigenvalues excludes
the weak identification setting of Theorem 3.
Unbounded Mean Function Inspection of the proofs reveal that Assumption 1(iii), maxi (xi  )2 =
O(1), can be dropped if the above conditions are strengthened to maxi, Pii (x  )2 = o(1) when
                                    n
                           |x  |(1+    |M |)
dim(xi ) is fixed or maxi,j j        =1 i
                                    r
                                             = o(1) when dim(xi )  .
                                                              2                            1   N                 2
Example 2. (Analysis of covariance, continued) Recall that  =   =                          n   g =1 Tg   g - ¯       where
ygt = g + xgt  + gt , g index the N groups, and Tg is group size.
                                                                                          -1
No Common Regressors This is a special case of the previous example with r = N -1, Pii = Tg (i)
                                                          2
    i = 0. Assumption 1(ii),(iii) requires Tg  2 and maxg g
and x                                                       = O(1). Theorem 1 applies if the
number of groups is fixed and ming Tg  , while Theorem 2 applies if the number of groups is
                                                                                 1
large. Theorem 3 cannot apply as all eigenvalues are equal to                    n.
                                                                                                                       2
Common Regressors To accommodate common regressors of fixed dimension, assume                                              +
             2                           1     N       Tg
maxg,t xgt       = O(1) and that         n     g =1    t=1 (xgt   -x
                                                                   ¯g )(xgt - x
                                                                              ¯g ) converges to a positive definite
limit. This is a standard assumption in basic panel data models (see, e.g., Wooldridge, 2010,
Chapter 10). Allowing such common regressors does not alter the previous conclusions: Theorem 1
                                                        -1          -1
applies if N is fixed and ming Tg   since wi wi  Pii = Tg (i) + O (n ), Theorem 2 applies if


                                                             27
                  n
N   since          =1 |Mi    | = O(1), and Theorem 3 cannot apply since n  [c1 , c2 ] for                               = 1, . . . , r
and some c2  c1 > 0 not depending on n.
                                                                   2                                                2
Unbounded Mean Function All conclusions continue to hold if maxg,t g + xgt                                              = O(1) is
                       2        2
                maxg,t g +
                         xgt                      2       1     N       Tg           2
replaced with   max{N,ming Tg }     = o(1) and        +   n     g =1    t=1   xgt        = O(1).

Example 3. (Random coefficients, continued) For simplicity, consider the uncentered second mo-
            1   N       2
ment  =     n   g =1 Tg g where ygt =          g + zgt g + gt . Suppose Assumption 1 holds and assume
                    2    2                                                                      Tg
that   maxg,t g + g   + zgt = O(1) and         ming Szz,g  c > 0 where Szz,g =                  t=1 (zgt    ¯g )2 . Note that
                                                                                                           -z
ming Szz,g > 0 is equivalent to full rank of Sxx and Szz,g indexes how precisely g can be estimated.
Consistency The N eigenvalues of A      ~ are g = Tg Szz,g
                                                       -1
                                                            for g = 1, . . . , N where the group indexes
                                                    n
                                                                                                            Szz,1
are ordered so that 1  · · ·  N . Consistency follows from Lemma 3 if - 1
                                                                      1 =n                                   T1      . This
is automatically satisfied with many groups of bounded size.
                                                                                                              
                                                                                                                N
Limit Distribution If N is fixed and ming Szz,g  , then Theorem 1 applies. If                                  T1 Szz,1       ,
                                                      
                                     N                     N
then Theorem 2 applies. If          T2 Szz,2    ,         T1 Szz,1     = O(1), and Szz,1  , then Theorem 3
applies with q = 1. In this case, 1 is weakly identified relative to its influence on  and the
                                                                 
overall variability of ^. This is expressed through the condition N Szz,1 = O(1) where Szz,1 is the
                                                                  T1    
identification strength of 1 , T1 provides the influence of 1 on  and 1/ N indexes the variability
of ^.

Example 4. (Two-way fixed effects, continued) In this final example, we restrict attention to the
first-differenced setting yg = fg  + g with Tg = 2 and a large number of firms, J  .
                                                        2        Tg           ¯ 2 and we
Our target parameter is the variance of firm effects  =   = 1 N     j (g,t) -    n       g =1   t=1
consider Assumption 1 satisfied; in particular, maxj |j | = O(1).
Leverages The leverage Pgg of observation g is less than one if the origin and destination firms
of worker g are connected by a path not involving g . Letting ng denote the number of edges
                                                                        ng
in the shortest such path, one can show that Pgg                       1+ng .    Therefore, if maxg ng < 100 then
Assumption 1(ii) is satisfied with max Pgg  .99. In our application we find maxg ng = 12, leading
to a somewhat smaller bound on the maximal leverage. The same consideration implies a bound
on the model in levels since Pi(g,t)i(g,t) = 1
                                             2 (1 + Pgg ).
Eigenvalues The eigenvalues of A     ~ satisfy the equality

                                              1
                                        =                     for       = 1, . . . , J
                                             
                                            nJ +1-

where 1  ···   J are the non-zero eigenvalues of the matrix E 1/2 LE 1/2 . L is the normalized
Laplacian of the employer mobility network and connectedness of the network is equivalent to full
rank of Sxx (see Appendix C.7 for definitions). E is a diagonal matrix of employer specific "churn
rates", i.e., the number of moves in and out of a firm divided by the total number of employees


                                                           28
                                                                ~. In Example 3, the quantities
in the firm. E and L interact in determining the eigenvalues of A
{T -1 Szz, }N=1 played a role directly analogous to the churn rates in E , so in this example we focus
on the role of L by assuming that the diagonal entries of E are all equal to one.
                                                                                              
Strongly Connected Network The employer mobility network is strongly connected if             JC  
where C  (0, 1] is Cheeger's constant for the mobility network (see, e.g., Mohar, 1989; Jochmans
and Weidner, 2016). Intuitively, C measures the most severe "bottleneck" in the network, where
a bottleneck is a set of movers that upon removal from the data splits the mobility network into
two disjoint blocks. The severity of the bottleneck is governed by the number of movers removed
divided by the smallest number of movers in either of the two disjoint blocks. The inequalities
                                                                
 J  1 - 1 - C 2 (Chung, 1997, Theorem 2.3) and 2      J    2          -2 imply that a strongly
                                                 1/    =1   4( J J )
connected network yields q = 0, which rules out application of Theorem 3. Furthermore, a strongly
connected network is sufficient (but not necessary) for consistency of ^ as J 2  J (n       J )-2 .
                                                                             =1     n
                                        
Weakly Connected Network When J C is bounded, the network is weakly connected and can
contain a sufficiently severe bottleneck that a linear combination of the elements of  is estimated
imprecisely relative to its influence on  and the total uncertainty in     ^. The weakly identified
linear combination in this case is a difference in average firm effects across the two blocks on
either side of the bottleneck, which contributes a 2 term to the asymptotic distribution. Below
we use a stochastic block model to further illustrate this phenomenon. Our empirical application
demonstrates that weakly connected networks can appear in practically relevant settings.
Stochastic Block Model Consider a stochastic block model of network formation where firms
belong to one of two blocks and a set of workers switch firms, possibly by moving between blocks.
Workers' mobility decisions are independent: with probability pb a worker moves between blocks
and with probability 1 - pb she moves within block. For simplicity, we further assume that the two
blocks contain equally many firms and consider a semi-sparse network where J log( n
                                                                                    J)
                                                                                       + log(J)
                                                                                           npb  0.
                                                                                                   6

In this model the asymptotic behavior of ^ is governed by pb : the most severe bottleneck is between
the two blocks and has a Cheeger's constant proportional to pb . In Appendix C.7, we use this
model to verify the high-level conditions leading to Theorems 2 and 3 and show that Theorem 2
             
applies when Jpb  , while Theorem 3 applies with q = 1 otherwise. The argument extends to
any finite number of blocks, in which case q is the number of blocks minus one. Finally, we show
that  ^ is consistent even when the network is weakly connected. To establish consistency we only
         log(J )
impose     npb      0, which requires that the number of movers across the two blocks is large.
   6
    The semi-sparse stochastic block model is routinely employed in the statistical literature on spectral
clustering, see, e.g, Sarkar and Bickel (2015).




                                                    29
8         Application
Consider again the problem of estimating variance components in two-way fixed effect models of
wage determination. Card et al. (2018) note that plug-in wage decompositions of the sort introduced
by AKM typically attribute 15%­25% of overall wage variance to variability in firm fixed effects.
Given the bias and potential sampling variability associated with plug-in estimates, however, it has
been difficult to infer whether firm effects play a differentially important role in certain markets or
among particular demographic groups.
        In this section, we use Italian social security records to compute leave-out estimates of the AKM
wage decomposition and contrast them with estimates based upon the plug-in estimator of Abowd
et al. (1999) and the homoscedasticity-corrected estimator of Andrews et al. (2008). We then
investigate whether the variance components that comprise the AKM decomposition differ across
age groups. While it is well known that wage inequality increases with age (Mincer et al., 1974;
Lemieux, 2006), less is known about the extent to which firm pay premia mediate this phenomenon.
Standard wage posting models (e.g., Burdett and Mortensen, 1998) suggest older workers have had
more time to climb (and fall off) the job ladder and to receive outside offers (Bagger et al., 2014),
which may result in more dispersed firm wage premia. But older workers have also had more time
to develop professional reputations revealing their relative productivity, which should generate a
large increase in the variance of person effects (Gibbons and Katz, 1992; Gibbons et al., 2005). The
tools developed in this paper allow us to formally study these hypotheses.


8.1         Sample Construction
The data used in our analysis come from the Veneto Worker History (VWH) file, which provides
the annual earnings and days worked associated with each covered employment spell taking place
in the Veneto region of Northeast Italy over the years 1984-2001. The VWH data have been used
in a number of recent studies (Card et al., 2014; Bartolucci et al., 2018; Serafinelli, 2019; Devicienti
et al., 2019) and are well suited to the analysis of age differences because they provide precise
information on dates of birth. These data are also notable for being publicly available, making the
costs of replicating our analysis unusually low.7
        Our baseline sample consists of workers with employment spells taking place in the years 1999
and 2001, which provides us with a three year horizon over which to measure job mobility. In
Section 8.4 we analyze a longer unbalanced sample spanning the years 1996­2001 and find that it
yields similar results. For each worker-year pair, we retain the unique employment spell yielding
the highest earnings in that year. Wages in each year are defined as earnings in the selected spell
    7
   See http://www.frdb.org/page/data/scheda/inps-data-veneto-workers-histories-vwh/doc_
pk/11145 for information on obtaining the VHW.



                                                     30
divided by the spell length in days. Workers are divided into two groups of roughly equal size
according to their year of birth: "younger" workers born in the years 1965-1983 (aged 18-34 in
1999) and "older" workers born in the years 1937-1964 (aged 35-64 in 1999). Further details on
our processing of the VWH records is provided in Appendix A.1.
      Table 1 reports the number of person-year observations available among workers employed by
firms in the region's largest connected set, along with the largest connected set for each age group.
Workers are classified as "movers" if they switch firms between 1999 and 2001. Comparing the
number of movers to half the number of person-year observations reveals that roughly 21% of all
workers are movers. The movers share rises to 26% among younger workers while only 16% of
older workers are movers, reflecting the tendency of mobility rates to decline with age. The average
number of movers per connected firm ranges from nearly 3 in the pooled sample to roughly 2 in the
thinner age-specific samples, suggesting that many firms are associated with only a single mover.
      Our leave-out estimation strategy requires that each firm effect remain estimable after removing
any single observation. The second panel of Table 1 enforces this requirement by restricting to firms
that remain connected when any mover is dropped (see Appendix B.1 for computational details).
Pruning the sample in this way drops roughly half of the firms but less than a third of the movers
and eliminates roughly 30% of all workers regardless of their mobility status. These additional
restrictions raise mean wages by roughly 5% and lower the variance of wages by 5­10% depending
on the sample.
      To assess the potential influence of these sample restrictions on our estimands of interest, we
construct a third sample that further requires the firm effects to remain estimable after removing
any two observations.8 This "leave-two-out connected set" is also of theoretical interest because
it provides a setting where the requirements for consistency of the variance estimator of Lemma
5 appear to be satisfied. On average, the leave-two-out connected sets have roughly half as many
firms and 20% fewer movers than the corresponding leave-one-out sets, and the average number of
movers per firm ranges from approximately 5.6 in the sample of older workers to 4.3 in the sample
of younger workers. Restricting the sample in this way further raises mean wages by 3­4% but
yields negligible changes in variance, except among the sample of older workers, which experiences
a nearly 7% increase in variance. We investigate below the extent to which these changes in
unconditional variances reflect changes in the variance of underlying firm wage effects.


8.2       AKM Model and Design Diagnostics
Consider the following simplified version of the AKM model:

                    ygt = g + j (g,t) + gt .                (g = 1, . . . , N, t = 1, 2)
  8
      We thank an anonymous referee for this suggestion.


                                                    31
We fit models of this sort to the VHW data after having pre-adjusted log wages for year effects in a
first step. This adjustment is obtained by estimating an augmented version of the above model by
OLS that includes a dummy control for the year 2001. Hence, ygt gives the log wage in year t minus
a year 2001 dummy times its estimated coefficient. This two-step approach simplifies computation
                                                                          
without compromising consistency because the year effect is estimated at a N rate.
       The bottom of Table 1 reports for each sample the maximum leverage (maxi Pii ) of any person-
year observation (Appendix B.3 discusses the computation of these leverages). While our pruning
procedure ensures maxi Pii < 1, it is noteworthy that maxi Pii is still quite close to one, indi-
cating that certain person-year observations remain influential on the parameter estimates. This
finding highlights the inadequacy of asymptotic approximations that require the dimensionality of
regressors to grow slower than the sample size, which would lead the maximum leverage to tend to
zero.
       The asymptotic results of Section 5 emphasize the importance of not only the maximal leverage,
but the number and severity of any bottlenecks in the mobility network. Figure 1 illustrates the
leave-two-out connected set for older workers. Each firm is depicted as a dot, with the size of the
dot proportional to the total number of workers employed at the firm over the years 1999 and
2001. Dots are connected when a worker moves between the corresponding pair of firms. The
figure highlights the two most severe bottlenecks in this network, which divide the firms into three
distinct blocks. Each block's firms have been shaded a distinct color. The blue block consists of
only five firms, four of which are quite small, which limits its influence on the asymptotic behavior
of our estimator. However, the green block has 51 firms with a non-negligible employment share
of 9.5%. Theorem 3 and the discussion in Section 7 therefore suggest that the bottleneck between
the green and the larger red block will generate weak identification and asymptotic non-normality,
predictions we explore in detail below.


8.3        Variance Decompositions
Table 2 reports the results of applying to our samples three estimators of the AKM variance decom-
position: the naive plug-in (PI) estimator   ^PI originally proposed by AKM, the homoscedasticity-
only (HO) estimator ^HO of Andrews et al. (2008), and the leave-out (KSS) estimator ^. The PI
estimator finds that the variance of firm effects in the pooled leave-one-out connected set accounts
for roughly 20% of the total variance of wages, while among younger workers firm effect variability
is found to account for 31% of overall wage variance. Among older workers, variability in firm
effects is estimated to account for only 16% of the variance of wages in the leave-one-out connected
set.
       Are these age differences driven by biases attributable to estimation error? Applying the HO
estimator of Andrews et al. (2008) reduces the estimated variances of firm effects by roughly 18% in


                                                  32
the age-pooled sample, 27% in the sample of younger workers, and 16% in the sample of older work-
ers. However, the KSS estimator yields further, comparably sized, reductions in the estimated firm
effect variance relative to the HO estimator, indicating the presence of substantial heteroscedastic-
ity in these samples. For instance, in the pooled leave-one-out sample, the KSS estimator finds a
variance of firm effects that accounts for only 13% of the overall variance of wages, while the HO
estimator finds that firm effects account for 16% of wage variance.
   Moreover, while the plug-in estimates suggested that the firm effect variance was greater among
older than younger workers, the KSS estimator finds the opposite pattern. The KSS estimator also
finds that the pooled variance of firm effects exceeds the corresponding variance in either age-
specific sample, a sign that mean firm effects differ by age. We explore this between age group
component of firm variability in greater depth below.
   A potential concern with analyzing the leave-one-out connected set is that worker and firm
behavior in this sample may be non-representative of the broader (just-)connected set. To assess
this possibility, we also report estimates for the leave-two-out connected set. Remarkably, the KSS
estimator finds negligible differences in the variance of firm effects between the leave-one-out and
leave-two-out samples for both the pooled sample and the sample of younger workers. Among
older workers the estimated firm effect variance falls by about 11% in the leave-two-out sample,
though we show below that this difference may be attributable to sampling variation. The broad
similarity between leave-one-out and leave-two-out KSS estimates is likely attributable to the fact
that trimmed firms tend to be small and therefore contribute little to the person-year weighted
variance of firm effects that has been the focus of the literature.
   PI estimates of person effect variances are much larger than the corresponding estimates of firm
effect variance, accounting for 66%­88% of the total variance of wages depending on the sample.
The PI estimator also finds that person effects are much more dispersed among older than younger
workers, which is in accord with standard models of human capital accumulation and employer
learning. The estimated ratio of older to younger person effect variances in the leave-one-out
sample is roughly 2.6. Applying the HO estimator reduces the magnitude of the person effect
variance among all age groups, but boosts the ratio of older to younger person effect variances to
3.2. The KSS estimator yields further downward corrections to estimated person effect variances,
leading the contribution of person effect variability to range from only 50% to 80% of total wage
variance. Proportionally, however, the variability of older workers remains stable at 3.2 times that
of younger workers.
   PI estimates of the covariance between worker and firm effects are negative in both age-restricted
samples, though not in the pooled sample. When converted to correlations, these figures suggest
there is mild negative assortative matching of workers to firms. Applying the HO estimator leads
the covariances to change sign in both age-specific samples, while generating a mild increase in the
estimated covariance of the pooled sample. In all three samples, however, the HO estimates indicate

                                                  33
very small correlations between worker and firm effects. By contrast, the KSS estimator finds a
rather strong positive correlation of 0.21 among younger workers, 0.27 among older workers, and
0.28 in the pooled leave-one-out sample, indicating the presence of non-trivial positive assortative
matching between workers and firms. While the patterns in the leave-two-out sample are broadly
similar, the KSS correlation estimate among older workers is substantially smaller in the leave-two-
out than the leave-one-out sample (0.18 vs 0.27).
       Finally, we examine the overall fit of the two-way fixed effects model using the coefficient of
determination. The PI estimator of R2 suggests the two-way fixed effects model explains more than
95% of wage variation in the pooled sample, 91% in the sample of younger workers, and 97% in
the sample of older workers. The HO estimator of R2 is equivalent to the adjusted R2 measure of
Theil (1961). The adjusted R2 indicates that the two-way fixed effects model explains roughly 90%
of the variance of wages in the pooled sample, which is quite close to the figures reported in Card
et al. (2013) for the German labor market. Applying the KSS estimator yields very minor changes
in estimated explanatory power relative to the HO estimates. Interestingly, a sample size weighted
average of the age group specific KSS R2 estimates lies slightly below the pooled KSS estimate
of R2 , which suggests allowing firm effects to differ by age group fails to appreciably improve the
model's fit. We examine this hypothesis more carefully in Section 8.5.


8.4        Multiple Time Periods and Serial Correlation
Thus far, our analysis has relied upon panels with only two time periods. Table 3 reports KSS
estimates of the variance of firm effects in an unbalanced panel spanning the years 1996­2001. To
analyze this longer panel, we expand our set of time varying covariates to include unrestricted year
effects and a third order polynomial in age normalized to have slope zero at age 40 as discussed
in Card et al. (2018).9 Allowing up to six wage observations per worker yields a substantially
larger estimation sample with roughly three times more person-year observations in the age-pooled
leave-one-out connected set than was found in Table 1. For older workers, who have especially low
mobility rates, allowing more time periods raises the number of person-year observations in the
leave-one-out connected set by a factor of roughly 5.7 and more than triples the number of firms.
       While these additional observations will tend to reduce the bias in the plug-in estimator, using
longer panels may present two distinct sets of complications. First, the equivalence discussed
in Remark 4 no longer holds, which implies that leaving a single person-year observation out is
unlikely to remove the bias in estimates of the variance of firm effects when the errors are serially
correlated. Second, pooling many years of data may change the target parameter if firm or person
   9
   Pre-adjusting for age has negligible effects on the variance decompositions reported in Table 2 but is
quantitatively more important in this longer panel. Age adjustments are particularly pronounced among
younger workers who generally exhibit greater wage growth and tend to move rapidly to higher paying firms.



                                                   34
effects "drift" with time. The bottom rows of Table 3 probe for the importance of serial correlation
by leaving out "clusters" of observations ­ as described in Remark 3 ­ defined successively as
all observations within the same worker-firm "match" and all observations belonging to the same
worker; see Appendix B.3.1 for computational details. Because worker g 's person effect is not
estimable when leaving that worker's entire wage history out, we estimate a within-transformed
specification that eliminates the person effects in a first step.
   Leaving out the match yields an important reduction in the variance of firm effects relative to
leaving out a single person-year observation, indicating the presence of substantial serial correlation
within match. By contrast, leaving out the worker turns out to have negligible effects on the
estimated variance of firm effects, suggesting that serial correlation across-matches is negligible.
As expected, pooling several years of data reduces the bias of the PI estimator: the magnitude
of the difference between the PI estimates of the variance of firm effects and the leave-worker-out
estimates tends to be smaller than the corresponding difference between the PI and KSS estimates
of the variance of firm effects reported in Table 2.
   Remarkably, the firm effect variance estimates that result from leaving out either the match
or worker are nearly identical to the KSS estimates reported in Table 2 for both the age-pooled
samples and the samples of younger workers, suggesting the firm effects are relatively stable over
this longer horizon. Among older workers, the leave-cluster-out estimates of the variance of firm
effects are higher than those reported in Table 2, which is unsurprising given that the number of
firms under consideration more than tripled in this longer panel. Reassuringly, however, Table 3
reveals that the KSS estimates of the variance of firm effects among older workers in the leave-
one-out and leave-two-out connected sets are very close to one another. The general stability of
the KSS estimates of firm effect variances to alternate panel lengths may be attributable to the
relatively placid macroeconomic conditions present in Veneto over this period, see the discussion
in Devicienti et al. (2019).
   Our leave-cluster-out exercises suggest researchers seeking to analyze longer panels may be able
to avoid biases stemming from serial correlation by simply collapsing the data to match means in a
first step and then analyzing these means using the leave-one-observation-out estimator. This two-
step approach should substantially reduce computational time while generating only mild efficiency
losses due to equal weighting of matches. In what follows, we revert to our baseline sample with
exactly two observations per worker.


8.5     Sorting and Wage Structure
The KSS estimates reported in Table 2 indicate that older workers exhibit somewhat less variable
firm effects and a stronger correlation between person and firm effects than younger workers. These
findings might reflect lifecycle differences in the sorting of workers to firms or differences in the


                                                  35
structure of firm wage effects across the two age groups.
   Table 4 explores the sorting channel by projecting the pooled firm effects from the leave-one-
out sample onto a constant, an indicator for being an older worker, the log of firm size, and the
interaction of the indicator with log firm size. Because these projection coefficients are linear
combinations of the estimated firm effects, we use the KSS standard errors proposed in equation
(7) and analyzed in Theorem 1. For comparison, we also report a naive standard error that
treats the firm effect estimates as independent observations and computes the usual Eicker-White
"robust" standard errors. In all cases, the KSS standard error is at least twice the corresponding
naive standard error and in one case roughly 24 times larger. In light of the consistency results
of Theorem 1, this finding suggests the standard practice of regressing firm effect estimates on
observables in a second step without adjusting the standard errors for correlation across firm effects
can yield highly misleading inferences.
   The first column of Table 4 shows that older workers tend to work at firms with higher average
firm effects. Evidently older workers do occupy the upper rungs of the job ladder. The second
column shows that this sorting relationship is largely mediated by firm size. An older worker at a
firm with a single employee is estimated to have a mean firm wage effect 0.16 log points lower than
a younger worker at a firm of the same size, an economically insignificant difference that is also
revealed to be statistically insignificant when using the KSS standard error. As firm size grows,
older workers begin to enjoy somewhat larger firm wage premia. Evaluated at the median firm
size of 12 workers, the predicted gap between older and younger workers rises to 0.54 log points,
a gap that we can distinguish from zero at the 5% level using the KSS standard error but is still
quite modest. We conclude that the tendency of older workers to be employed at larger firms is a
quantitatively important driver of the firm wage premia they enjoy.
   Figure 2 investigates to what extent the firm wage effects differ between age groups. Using
the age-restricted leave-one-out connected sets, we obtain a pair of age group specific firm effect
             Y ^O
            ^j
estimates {    , j }j J for the set J of 8,578 firms present in both samples (see Appendix B.5 for
                                                               Y
                                                              ^j      O
                                                                     ^j
details). Figure 2 plots the person-year weighted averages of    and    within each centile bin
   ^O                                         ^Y      ^O
of j . A person-year weighted projection of j onto j yields a slope of only 0.501. To correct
this plug-in slope estimate for attenuation bias, we multiply the unadjusted slope by the ratio of
                                                        O
the PI estimate of the person-year weighted variance of j to the corresponding KSS estimate of
this quantity. Remarkably, this exercise yields a projection slope of 0.987, suggesting that, were it
                                 O
                                ^j
not for the estimation error in    , the conditional averages depicted in Figure 2 would be centered
around the dashed 45 degree line. Converting this slope into a correlation using the KSS estimate
                                        Y
of the person-year weighted variance of j yields a person-year weighted correlation between the
                                                                  Y   O
two sets of firm effects of 0.89, which indicates the underlying (j , j ) pairs are tightly clustered
around this 45 degree line.
   Theorem 2 allows us to formally test the joint null hypothesis that the two sets of firm effects

                                                 36
are actually identical, i.e., that both the slope and R2 from a projection of j
                                                                              Y      O
                                                                                onto j are one. We
                                  O   Y
can state this hypothesis as H0 : j = j for all j  J . Using the test suggested in Remark 6 we
obtain a realized test statistic of 3.95 which, when compared to the right tail of a standard normal
distribution, yields a p-value on H0 of less than 0.1%. Hence, we can decisively reject the null
hypothesis that older and younger workers face exactly the same vectors of firm effects. However,
our earlier correlation results suggest that H0 nonetheless provides a fairly accurate approximation
to the structure of firm effects, at least among those firms that employ movers of both age groups.


8.6     Inference
We now study more carefully the problem of inference on the variance of firm effects. For conve-
nience, the top row of Table 5 reprints our earlier KSS estimates of the variance of firm effects in
each sample. Below each estimate of firm effect variance is a corresponding standard error estimate,
computed according to the approach described in Lemma 5. As noted in Remark 8, these standard
errors will be somewhat conservative when there is a large share of observations for which no split
sample predictions can be created. In the leave-one-out samples this share varies between 15% and
22%, indicating that the standard errors are likely upward biased. In the leave-two-out samples,
however, this source of bias is not present as the split sample predictions always exist. The stan-
dard errors will also tend to be conservative when there is a large share of observation pairs in the
set B , for which there is upward bias in the estimator of the error variance product. However, for
both the leave-one-out and leave-two-out samples, this share varies between only 0.03% and 0.46%,
suggesting only a small degree of upward bias stems from this source.
   The next panel of Table 5 reports the 95% confidence intervals that arise from setting q = 0,
q = 1, or q = 2. While the first interval employs a normal approximation, the latter two allow for
weak identification by employing non-standard limiting distributions involving linear combinations
of normal and 2 random variables. We also report estimates of the curvature parameters (1 , 2 )
used to construct the weak identification robust intervals. In the pooled samples both curvature
parameters are estimated to be quite small, indicating that a normal approximation is likely to be
accurate. Accordingly, setting q > 0 has little discernible effect on the resulting confidence intervals
in these samples. However, among older workers, particularly in the leave-two-out sample, we find
stronger curvature coefficients suggesting weak identification may be empirically relevant. Setting
q > 0 in this sample widens the confidence interval somewhat and also changes its shape: mildly
shortening the lower tail of the interval but lengthening the upper tail.
   Treating the samples of younger and older workers as independent, the fact that the confidence
intervals for the two age group samples overlap implies we cannot reject the null hypothesis that
the firm effect variances are identical at the (1 - 0.952 ) × 100 = 9.75% level. The significance of
the 0.23 log point difference between the leave-one-out and leave-two-out estimates of firm effect


                                                  37
variance in the sample of older workers turns out to more difficult to assess. By the Cauchy-
Schwartz inequality, the covariance between the leave-one-out and leave-two-out estimators is at
most (0.0026)2 (0.0014)2 = 3.64 × 10-6 . Hence the standard error on the difference between the two
estimators is at least 0.0012, which implies a maximal t-statistic of 1.92. Therefore, even when using
a normal approximation, we find rather weak evidence against the null that the leave-one-out and
leave-two-out estimands are equal. However, because the leave-one-out standard error estimator is
likely upward biased, this finding is somewhat less conclusive than would typically be the case.
   Theorem 3 suggests two important diagnostics for the asymptotic behavior of our estimator are
                                2
the Lindeberg statistics {maxi wis }s=1,2 and the top eigenvalue shares {2
                                                                         s/
                                                                                         r   2
                                                                                          =1  }s=1,2,3 .   The
bottom panel of Table 5 reports these statistics for each sample. The top eigenvalue shares are fairly
small in the pooled sample and among younger workers. A small top eigenvalue share indicates the
estimator does not depend strongly on any particular linear combination of firm effects and hence
that a normal distribution should provide a suitable approximation to the estimator's asymptotic
behavior (i.e. that q = 0). Accordingly, we find that the confidence intervals are virtually identical
for all values of q in both the pooled samples and the two samples of younger workers.
   Among older workers the top eigenvalue share is 31% in the leave-one-out sample and 58% in
the leave-two-out sample. The next largest eigenvalue share is, in both cases, less than 5%, which
suggests this is a setting where q = 1. In line with this view, confidence intervals based upon
the q = 1 and q = 2 approximations are nearly identical in both samples of older workers. The
accuracy of these weak-identification robust confidence intervals hinges on the Lindeberg condition
                                                                             2
of Theorem 3 being satisfied. One can think of the Lindeberg statistic maxi wis as giving an inverse
measure of effective sample size available for estimating the linear combination of firm effects
associated with the s'th largest eigenvalue. The fact that these statistics are all less than or equal
to 0.05 implies an effective sample size of at least 20. We study in the Monte Carlo exercises below
whether this effective sample size is sufficient to provide accurate coverage. Reassuringly, the sum
of squared eigenvalues is quite small in all six samples considered, indicating that the leave out
estimator is consistent also in our weakly identified settings.


8.7     Monte Carlo Experiments
We turn now to studying the finite sample behavior of the leave-out estimator of firm effect variance
and its associated confidence intervals under a particular data generating process (DGP). Data were
generated from the following first differenced model based upon equation (4):

                    yg = fg ^scale + g ,                          (g = 1, . . . , N ).




                                                  38
Here ^scale gives the J × 1 vector of OLS firm effect estimates found in the pooled leave-one-out
sample, rescaled to match the KSS estimate of firm effect variance for that sample. The errors g
were drawn independently from a normal distribution with variances given by the following model
of heteroscedasticity:

                     V[g ] = exp(a0 + a1 Bgg + a2 Pgg + a3 ln Lg2 + a4 ln Lg1 ),

where Lgt gives the size of the firm employing worker g in period t. To choose the coefficients of
                                                               2
this model, we estimated a nonlinear least squares fit to the ^g in the pooled leave-one-out sample,
which yielded the following estimates:

          ^0 = -3.3441,
          a                a
                           ^1 = 1.3951,   ^2 = -0.0037,
                                          a                ^3 = -0.0012,
                                                           a                 ^4 = -0.0086.
                                                                             a

For each sample, we drew from the above DGP 1,000 times while holding firm assignments fixed
at their sample values.
   Table 6 reports the results of this Monte Carlo experiment. In accord with theory, the KSS
estimator of firm effect variances is unbiased while the PI and HO estimators are biased upwards.
As expected, the KSS standard error estimator exhibits a modest upward bias in the leave-one-
out samples ranging from 15% in the sample of older workers to 44% among younger workers.
In the leave-two-out sample, however, the standard error estimator exhibits biases of only 6% or
less. Unsurprisingly then, the q = 0 confidence interval over-covers in both the pooled leave-one-
out sample and the leave-one-out sample of younger workers. In the corresponding leave-two-out
samples, however, coverage is very near its nominal level, both for the normal based (q = 0) and
the weak identification robust (q = 1) intervals.
   In the samples of older workers, the normal distribution provides a poor approximation to
the shape of the estimator's sampling distribution, which is to be expected given the large top
eigenvalues found in these designs. This non-normality generates substantial under-coverage by
the q = 0 confidence interval in the leave-two-out sample. Applying the weak identification robust
interval in the leave-two-out sample of older workers yields coverage very close to nominal levels
despite the fact that the effective sample size available for the top eigenvector is only about 20.
   In sum, the Monte Carlo experiments demonstrate that confidence intervals predicated on
the assumption that q = 1 can provide accurate size control in leave-two-out samples when the
realized mobility network exhibits a severe bottleneck. We also achieved size control in leave-one-
out samples, albeit at the cost of moderate over-coverage. Hence, in applications where statistical
power is a first-order consideration, it may be attractive to restrict attention to leave-two-out
samples, which tend to yield estimates of variance components very close to those found in leave-
one-out samples but with substantially less biased standard errors.


                                                    39
9     Conclusion
We propose a new estimator of quadratic forms with applications to several areas of economics.
The estimator is finite sample unbiased in the presence of unrestricted heteroscedasticity and can
be accurately approximated in very large datasets via random projection methods. Consistency is
established under verifiable design requirements in an environment where the number of regressors
may grow in proportion to the sample size. The estimator enables tests of linear restrictions of
varying dimension under weaker conditions than have been explored in previous work. A new
distributional theory highlights the potential for the proposed estimator to exhibit deviations from
normality when some linear combinations of coefficients are imprecisely estimated relative to others.
    In an application to Italian worker-firm data, we showed that ignoring heteroscedasticity can
substantially bias conclusions about the relative contribution of workers, firms, and worker-firm
sorting to wage inequality. Accounting for serial correlation within a worker-firm match was found
to be empirically important, while across match correlation appears to be negligible. Consequently,
those studying longer panels may wish to collapse their data down to match level means and then
apply the leave-observation-out estimator. Alternately, researchers can simply extract and analyze
separately balanced panels of length two, which also facilitates analysis of the temporal stability of
the firm and person effect variances.
    Leave-out standard error estimates for the coefficients of a linear projection of firm effects onto
worker and firm observables were found to be several times larger than standard errors that naively
treat the estimated firm effects as independent. These results strongly suggest that researchers
seeking to identify the observable correlates of high-dimensional fixed effects should consider em-
ploying the proposed standard errors, including when studying settings falling outside the tradi-
tional worker-firm setup (e.g., Finkelstein et al., 2016; Chetty and Hendren, 2018). Stratifying our
analysis by birth cohort, we formally rejected the null hypothesis that older and younger workers
face identical vectors of firm effects but found that the two sets of firm effects were highly correlated.
Corresponding techniques can be used to study multivariate models.
    A Monte Carlo analysis demonstrated that bottlenecks in the worker-firm mobility network can
generate quantitatively important deviations from normality. The proposed inference procedure
captured these deviations accurately with a weak identification robust confidence interval. In
cases where the mobility network was strongly connected, accurate inferences were obtained with
a normal approximation. Our results suggest that in typical worker-firm applications, the normal
approximation is likely to suffice. However, when studying small areas, or sub-populations with
limited mobility, accounting for weak identification can be quantitatively important.




                                                   40
References
Abowd, J. M., R. H. Creecy, F. Kramarz, et al. (2002). Computing person and firm effects using
  linked longitudinal employer-employee data. Technical report, Center for Economic Studies, US
  Census Bureau.

Abowd, J. M., F. Kramarz, and D. N. Margolis (1999). High wage workers and high wage firms.
  Econometrica 67 (2), 251­333.

Achlioptas, D. (2003). Database-friendly random projections: Johnson-lindenstrauss with binary
  coins. Journal of computer and System Sciences 66 (4), 671­687.

Akritas, M. G. and N. Papadatos (2004). Heteroscedastic one-way anova and lack-of-fit tests.
  Journal of the American Statistical Association 99 (466), 368­382.

Anatolyev, S. (2012). Inference in regression models with many regressors. Journal of Economet-
  rics 170 (2), 368­382.

Andrews, D. W. K. and X. Cheng (2012). Estimation and inference with weak, semi-strong, and
  strong identification. Econometrica 80 (5), 2153­2211.

Andrews, I. and A. Mikusheva (2016). A geometric approach to nonlinear econometric models.
  Econometrica 84 (3), 1249­1264.

Andrews, M. J., L. Gill, T. Schank, and R. Upward (2008). High wage workers and low wage firms:
  negative assortative matching or limited mobility bias? Journal of the Royal Statistical Society:
  Series A (Statistics in Society) 171 (3), 673­697.

Angrist, J., G. Imbens, and A. Krueger (1999). Jackknife instrumental variables estimation. Journal
  of Applied Econometrics 14 (1), 57­67.

Angrist, J. D. (2014). The perils of peer effects. Labour Economics 30, 98­108.

Arcidiacono, P., G. Foster, N. Goodpaster, and J. Kinsler (2012). Estimating spillovers using panel
  data, with an application to the classroom. Quantitative Economics 3 (3), 421­470.

Arellano, M. and S. Bonhomme (2011). Identifying distributional characteristics in random coeffi-
  cients panel data models. The Review of Economic Studies 79 (3), 987­1020.

Bagger, J., F. Fontaine, F. Postel-Vinay, and J.-M. Robin (2014). Tenure, experience, human
  capital, and wages: A tractable equilibrium search model of wage dynamics. American Economic
  Review 104 (6), 1551­96.


                                                41
Bartolucci, C., F. Devicienti, and I. Monz´
                                          on (2018). Identifying sorting in practice. American
  Economic Journal: Applied Economics 10 (4), 408­38.

Bloom, N., F. Guvenen, B. S. Smith, J. Song, and T. von Wachter (2018). The disappearing
  large-firm wage premium. In AEA Papers and Proceedings, Volume 108, pp. 317­22.

Bonhomme, S. (2017). Econometric analysis of bipartite networks. Econometric Analysis of Net-
  work data edited by B. Graham and A. De Paula .

Bonhomme, S., T. Lamadon, and E. Manresa (2019). A distributional framework for matched
  employer employee data. Econometrica 87 (3), 699­739.

Burdett, K. and D. T. Mortensen (1998). Wage differentials, employer size, and unemployment.
  International Economic Review , 257­273.

Card, D., A. R. Cardoso, J. Heining, and P. Kline (2018). Firms and labor market inequality:
  Evidence and some theory. Journal of Labor Economics 36 (S1), S13­S70.

Card, D., A. R. Cardoso, and P. Kline (2015). Bargaining, sorting, and the gender wage gap:
  Quantifying the impact of firms on the relative pay of women. The Quarterly Journal of Eco-
  nomics 131 (2), 633­686.

Card, D., F. Devicienti, and A. Maida (2014). Rent-sharing, holdup, and wages: Evidence from
  matched panel data. The Review of Economic Studies 81 (1), 84­111.

Card, D., J. Heining, and P. Kline (2013). Workplace heterogeneity and the rise of west german
  wage inequality. The Quarterly journal of economics 128 (3), 967­1015.

Cattaneo, M. D., M. Jansson, and W. K. Newey (2018). Inference in linear regression mod-
  els with many covariates and heteroscedasticity. Journal of the American Statistical Associa-
  tion 113 (523), 1350­1361.

Chao, J. C., J. A. Hausman, W. K. Newey, N. R. Swanson, and T. Woutersen (2014). Testing
  overidentifying restrictions with many instruments and heteroskedasticity. Journal of Economet-
  rics 178, 15­21.

Chao, J. C., N. R. Swanson, J. A. Hausman, W. K. Newey, and T. Woutersen (2012). Asymp-
  totic distribution of jive in a heteroskedastic iv regression with many instruments. Econometric
  Theory 28 (01), 42­86.

Chatterjee, S. (2008). A new method of normal approximation. The Annals of Probability 36 (4),
  1584­1610.

                                               42
Chetty, R., J. N. Friedman, N. Hilger, E. Saez, D. W. Schanzenbach, and D. Yagan (2011). How
  does your kindergarten classroom affect your earnings? evidence from project star. The Quarterly
  Journal of Economics 126 (4), 1593­1660.

Chetty, R. and N. Hendren (2018). The impacts of neighborhoods on intergenerational mobility ii:
  County-level estimates. The Quarterly Journal of Economics 133 (3), 1163­1228.

Chung, F. R. (1997). Spectral graph theory. Number 92. American Mathematical Soc.

Cochran, W. G. (1980). Fisher and the analysis of variance. In RA Fisher: An Appreciation, pp.
  17­34. Springer.

Devicienti, F., B. Fanfani, and A. Maida (2019). Collective bargaining and the evolution of wage
  inequality in italy. British Journal of Industrial Relations 57 (2), 377­407.

Dhaene, G. and K. Jochmans (2015). Split-panel jackknife estimation of fixed-effect models. The
  Review of Economic Studies 82 (3), 991­1030.

Donald, S. G., G. W. Imbens, and W. K. Newey (2003). Empirical likelihood estimation and
  consistent tests with conditional moment restrictions. Journal of Econometrics 117 (1), 55­93.

Dufour, J.-M. and J. Jasiak (2001). Finite sample limited information inference methods for struc-
  tural equations and models with generated regressors. International Economic Review 42 (3),
  815­844.

Efron, B. and C. Stein (1981, 05). The jackknife estimate of variance. Ann. Statist. 9 (3), 586­596.

El Karoui, N. and E. Purdom (2018). Can we trust the bootstrap in high-dimensions? the case of
  linear models. The Journal of Machine Learning Research 19 (1), 170­235.

Finkelstein, A., M. Gentzkow, and H. Williams (2016). Sources of geographic variation in health
  care: Evidence from patient migration. The Quarterly Journal of Economics 131 (4), 1681­1726.

Fisher, R. A. (1925). Statistical methods for research workers. Genesis Publishing Pvt Ltd.

Gibbons, R. and L. Katz (1992). Does unmeasured ability explain inter-industry wage differentials?
  The Review of Economic Studies 59 (3), 515­535.

Gibbons, R., L. F. Katz, T. Lemieux, and D. Parent (2005). Comparative advantage, learning, and
  sectoral wage determination. Journal of labor economics 23 (4), 681­724.

Graham, B. S. (2008). Identifying social interactions through conditional variance restrictions.
  Econometrica 76 (3), 643­660.

                                                 43
Graham, B. S., J. Hahn, A. Poirier, and J. L. Powell (2018). A quantile correlated random coeffi-
  cients panel data model. Journal of Econometrics 206 (2), 305­335.

Graham, B. S. and J. L. Powell (2012). Identification and estimation of average partial effects in
  irregular correlated random coefficient panel data models. Econometrica 80 (5), 2105­2152.

Hahn, J. and W. Newey (2004). Jackknife and analytical bias reduction for nonlinear panel models.
  Econometrica 72 (4), 1295­1319.

Hildreth, C. and J. P. Houck (1968). Some estimators for a linear model with random coefficients.
  Journal of the American Statistical Association 63 (322), 584­595.

Horn, S. D., R. A. Horn, and D. B. Duncan (1975). Estimating heteroscedastic variances in linear
  models. Journal of the American Statistical Association 70 (350), 380­385.

Jochmans, K. and M. Weidner (2016). Fixed-effect regressions on network data. arXiv preprint
  arXiv:1608.01532 .

Johnson, W. B. and J. Lindenstrauss (1984). Extensions of lipschitz mappings into a hilbert space.
  Contemporary mathematics 26 (189-206), 1.

Kline, P., R. Saggio, and M. Sølvsten (2019). LeaveOutTwoWay: A matlab package for leave out
  estimation of variance components in two way fixed effects models. https: // github. com/
  rsaggio87/ LeaveOutTwoWay .

Koutis, I., G. L. Miller, and D. Tolliver (2011). Combinatorial preconditioners and multilevel
  solvers for problems in computer vision and image processing. Computer Vision and Image
  Understanding 115 (12), 1638­1646.

Kuh, E. (1959). The validity of cross-sectionally estimated behavior equations in time series appli-
  cations. Econometrica , 197­214.

Lei, L., P. J. Bickel, and N. El Karoui (2018). Asymptotics for high dimensional regression m-
  estimates: fixed design results. Probability Theory and Related Fields 172 (3-4), 983­1079.

Lemieux, T. (2006). Increasing residual wage inequality: Composition effects, noisy data, or rising
  demand for skill? American Economic Review 96 (3), 461­498.

MacKinnon, J. G. and H. White (1985). Some heteroskedasticity-consistent covariance matrix
  estimators with improved finite sample properties. Journal of econometrics 29 (3), 305­325.

Menger, K. (1927). Zur allgemeinen kurventheorie. Fundamenta Mathematicae 10 (1), 96­115.


                                                44
Mincer, J. A. et al. (1974). Schooling, experience, and earnings. NBER Books .

Mohar, B. (1989). Isoperimetric numbers of graphs. Journal of Combinatorial Theory, Series
  B 47 (3), 274­291.

Moulton, B. R. (1986). Random group effects and the precision of regression estimates. Journal of
  econometrics 32 (3), 385­397.

Newey, W. K. and J. R. Robins (2018). Cross-fitting and fast remainder rates for semiparametric
  estimation. arXiv preprint arXiv:1801.09138 .

Oliveira, R. I. (2009). Concentration of the adjacency matrix and of the laplacian in random graphs
  with independent edges. arXiv preprint arXiv:0911.0600 .

Phillips, G. D. A. and C. Hale (1977). The bias of instrumental variable estimators of simultaneous
  equation systems. International Economic Review , 219­228.

Powell, J. L., J. H. Stock, and T. M. Stoker (1989). Semiparametric estimation of index coefficients.
  Econometrica: Journal of the Econometric Society , 1403­1430.

Quenouille, M. H. (1949). Approximate tests of correlation in time-series. Journal of the Royal
  Statistical Society. Series B (Methodological) 11 (1), 68­84.

Rao, C. R. (1970). Estimation of heteroscedastic variances in linear models. Journal of the American
  Statistical Association 65 (329), 161­172.

Raudenbush, S. and A. S. Bryk (1986). A hierarchical model for studying school effects. Sociology
  of education , 1­17.

Raudenbush, S. W. and A. S. Bryk (2002). Hierarchical linear models: Applications and data
  analysis methods, Volume 1. Sage.

Sacerdote, B. (2001). Peer effects with random assignment: Results for dartmouth roommates.
  The Quarterly journal of economics 116 (2), 681­704.

Sarkar, P. and P. J. Bickel (2015). Role of normalization in spectral clustering for stochastic
  blockmodels. The Annals of Statistics 43 (3), 962­990.

Scheffe, H. (1959). The analysis of variance. John Wiley & Sons.

Searle, S. R., G. Casella, and C. E. McCulloch (2009). Variance components, Volume 391. John
  Wiley & Sons.


                                                 45
Serafinelli, M. (2019). "good" firms, worker flows, and local productivity. Journal of Labor Eco-
  nomics 37 (3), 747­792.

Sherman, J. and W. J. Morrison (1950). Adjustment of an inverse matrix corresponding to a change
  in one element of a given matrix. The Annals of Mathematical Statistics 21 (1), 124­127.

Silver, D. W. (2016). Essays on labor economics and health care.

Sølvsten, M. (2019). Robust estimation with many instruments. Journal of Econometrics .

Song, J., D. J. Price, F. Guvenen, N. Bloom, and T. Von Wachter (2017). Firming up inequality.
  Technical report, National Bureau of Economic Research.

Sorkin, I. (2018).   Ranking firms using revealed preference.      The quarterly journal of eco-
  nomics 133 (3), 1331­1393.

Swamy, P. A. (1970). Efficient inference in a random coefficient regression model. Econometrica ,
  311­323.

Theil, H. (1961). Economic forecasts and policy.

Verdier, V. (2017). Estimation and inference for linear models with two-way fixed effects and
  sparsely matched data. Review of Economics and Statistics (0).

Woodbury, M. A. (1949). The stability of out-input matrices. Chicago, IL 9.

Wooldridge, J. M. (2010). Econometric analysis of cross section and panel data. MIT press.

Wright, S. (1921). Correlation and causation. Journal of agricultural research 20 (7), 557­585.

Yen, J. Y. (1971). Finding the k shortest loopless paths in a network. management Science 17 (11),
  712­716.




                                                46
                       Figure 1: Realized Mobility Network: Older workers



           Firms in second block                 High weight mobility               Firms in main block
           Within-block mobility                                                    Within-block mobility
           Firms in third block
           Within-block mobility




Note: This figure provides a visualization of the design matrix Sxx for the leave-two-out sample of older
workers (see Table 1 for reference). The graph is plotted in the statistical software R using the igraph
package and the large-scale graph layout (DrL) using the option to concentrate firms from the same blocks.
                                                       2       2
High weight mobility refers to observations that have wi 1 or wi2 above 1/500 and these observations form

the bottlenecks between the three blocks.




                                                    47
                      Figure 2: Do Firm Effects Differ Across Age Groups?




                                                                                     Y
                                                                                    ^j
Note: This figure plots the mean of the estimated firm effects for younger workers (   ) by centiles of the
                                           O
                                          ^j
estimated firm effects for older workers (   ) in the sample of 8,578 firms for which both sets of effects
are leave-one-out identified. Both sets of firm effects are demeaned within this estimation sample. "PI
                                                                        Y
                                                                       ^j       O
                                                                               ^j
slope" gives the coefficient from a person-year weighted projection of    onto    . "KSS slope" adjusts for
attenuation bias by multiplying the PI slope by the ratio of the plug-in estimate of the person-year weighted
            O
variance of j to the KSS adjusted estimate of the same quantity. "PI correlation" gives the person-
                                          O
                                         ^j      Y
                                                ^j
year weighted sample correlation between    and    while "KSS correlation" adjusts this correlation for
                        O
                       ^j      Y
                              ^j
sampling error in both    and    using leave out estimates of the relevant variances. "Test statistic" refers
to the realization of ^H /   ^ [
                             V  ^H ] where ^H is the quadratic form associated with the null hypothesis that
                        0         0          0

the firm effects are equal across age groups, see Remark 6 and Appendix B.5 for details. From Theorem 2,
^H /                                                                  O
          ^H ] converges to a N (0, 1) under the null hypothesis that j   Y
  0
       V[   0
                                                                        = j for all 8,578 firms.




                                                     48
                                      Table 1: Summary Statistics
                                          Pooled            Younger Workers          Older Workers
                                             [1]                  [2]                      [3]
Largest Connected Set
Number of Observations                   1,859,459               1,011,111              643,020
Number of Movers                          197,572                 133,627                53,035
Number of Firms                            73,933                  62,848                26,606

Mean Log Daily Wage                        4.7507                 4.6741                4.8925
Variance Log Daily Wage                    0.1985                 0.1321                0.2722

Leave One Out Sample
Number of Observations                   1,319,972                661,528               425,208
Number of Movers                          164,203                 102,746                35,467
Number of Firms                            42,489                  33,151                10,733

Mean Log Daily Wage                        4.8066                 4.7275                4.9455
Variance Log Daily Wage                    0.1843                 0.1200                0.2591

Maximum Leverage (        )                0.9365                 0.9437                0.9513

Leave Two Out Sample
Number of Observations                   1,126,964                514,284               301,948
Number of Movers                          128,705                  70,703                21,066
Number of Firms                            24,424                  16,263                 3,756

Mean Log Daily Wage                        4.8307                 4.7528                4.9834
Variance Log Daily Wage                    0.1898                 0.1232                0.2760

Maximum Leverage (        )                0.8333                 0.8481                0.8604
Note: Data in column 1 corresponds to VHW observations in the years 1999 and 2001. Column 2 restricts
to workers born in the years 1965-1983. Column 3 considers workers born in the years 1937-1964. The
largest connected set gives the largest sample in which all firms are connected by worker mobility. The
leave out sample is the largest connected set such that every firm remains connected even after
removing a worker from the sample, see Appendix B1 for details. The leave two out sample is the largest
connected set such that every firm remains connected even after removing two workers from the
sample, see Appendix B2 for details. Statistics on log daily wages are person-year weighted.
                                                     Table 2: Variance Decomposition
                                                 Pooled                    Younger Workers            Older Workers
                                      Leave one       Leave two       Leave one     Leave two    Leave one     Leave two
                                      out sample      out sample      out sample    out sample   out sample    out sample
Variance of Log Wages                   0.1843          0.1898          0.1200        0.1232       0.2591        0.2760

Variance of Firm Effects
Plug in (PI)                            0.0358          0.0316          0.0368        0.0314       0.0415         0.0304
Homoscedasticity Only (HO)              0.0295          0.0271          0.0270        0.0251       0.0350         0.0243
Leave Out (KSS)                         0.0240          0.0238          0.0218        0.0221       0.0204         0.0180
                                        0.023596
Variance of Person Effects
Plug in (PI)                            0.1321          0.1341          0.0843        0.0827       0.2180         0.2406
Homoscedasticity Only (HO)              0.1173          0.1214          0.0647        0.0663       0.2046         0.2298
Leave Out (KSS)                         0.1119          0.1179          0.0596        0.0634       0.1910         0.2221

Covariance Firm, Person Effects
Plug in (PI)                            0.0039          0.0077         -0.0058        -0.0008     -0.0032        -0.0006
Homoscedasticity Only (HO)              0.0097          0.0117          0.0030        0.0049      0.0040          0.0041
Leave Out (KSS)                         0.0147          0.0149          0.0075        0.0075      0.0171          0.0115

Correlation of Person, Firm Effects
Plug in (PI)                            0.0565          0.1189         -0.1040        -0.0155     -0.0334        -0.0068
Homoscedasticity Only (HO)              0.1649          0.2033          0.0726        0.1205      0.0475          0.0542
Leave Out (KSS)                         0.2830          0.2803          0.2092        0.2016      0.2744          0.1826

Coefficient of Determination
Plug in (PI)                         0.9546           0.9559         0.9183         0.9183         0.9774        0.9781
Homoscedasticity Only (HO)           0.9029           0.9078         0.8184         0.8256         0.9524        0.9551
Leave Out (KSS)                      0.8976           0.9047         0.8091         0.8209         0.9489        0.9536
Note: See Table 1 for sample definitions. All variance components are person-year weighted. Wages have been pre-adjusted for a
year fixed effect.
                                        Table 3: Variance of Firm Effects under Different Leave-Out Strategies
                                                                Pooled                    Younger Workers               Older Workers
                                                      Leave one       Leave two        Leave one Leave two        Leave one      Leave two
                                                     out sample       out sample      out sample out sample       out sample    out sample
Descriptive Statistics
Number of Observations                               5,163,446       4,758,713        2,632,596 2,290,223         2,016,202        1,652,729
Number of Movers                                      440,323         391,313          276,338 229,013             123,777          92,568
Number of Firms                                        86,646          60,896           71,673   46,084            32,809           16,396

Mean Log Daily Wage                                   4.7511          4.7662           4.6644        4.6799         4.8841          4.9158
Variance Log Daily Wage                               0.1839          0.1854           0.1156        0.1162         0.2477          0.2579

Maximum Leverage (        )                           0.9021          0.7713           0.9056        0.7775         0.9174          0.7639

Variance of firm Effects
Variance of Firm Effects (PI)                         0.0304          0.0278           0.0303        0.0273         0.0376          0.0336

Variance of Firm Effects (Leave Person-Year Out)      0.0296          0.0273           0.0302        0.0272         0.0314          0.0322

Variance of Firm Effects (Leave Match Out)            0.0243          0.0231           0.0221        0.0218         0.0265          0.0280

Variance of Firm Effects (Leave Worker Out)              0.0241          0.0233         0.0227       0.0218         0.0270           0.0268
Note: These samples consist of VHW data spanning the years 1996-2001 (T=6). "Pooled" considers all workers, "Younger workers" were born
in the years 1965-1983, "Older workers" were born in the years 1937-1964. Variance of Firm Effects (PI) denotes the uncorrected plug-in
estimate of the variance of firm effects. Variance of Firm Effects (Leave Person-Year Out) computes the leave-out bias correction by leaving a
single person-year observation out. Variance of Firm Effects (Leave Match Out) computes the leave-out bias correction by leaving entire
worker-firm matches out. Variance of Firm Effects (Leave Worker Out) computes the leave-out bias correction by leaving out each worker's
entire employment history. See Appendix B3.1 for computational details. Wages have been pre-adjusted for a cubic in age and year fixed
effects.
                                      Table 4: Projecting Firm Effects on Covariates
                                                                        (1)                                (2)
Older Worker                                                          0.0272                            -0.0016
                                                                     (0.0009)                           (0.0024)
                                                                     [0.0003]                           [0.0001]

Log Firm Size                                                                                            0.0276
                                                                                                        (0.0007)
                                                                                                        [0.0001]


Older Worker x Log Firm Size                                                                             0.0028
                                                                                                        (0.0005)
                                                                                                        [0.0002]

Predicted Gap in Firm Effects Older vs. Younger Worker                0.0272                             0.0054
                                                                     (0.0009)                           (0.0019)
                                                                     [0.0003]                           [0.0008]
Number of Observations                                                 1,319,972                           1,319,972
Note: This table reports projections of the firm effects estimated in the pooled leave one out sample defined in Table 1
onto worker and firm characteristics. In Column 1, the firm effects are projected onto a constant and a dummy for the
workers being born in the years 1937-1964. In Column 2, we regress firm effects on a constant, a dummy for workers born
in the years 1937-1964, log firm size and an interaction between these two covariates. The brackets show the standard
error associated with a given linear projection coefficient. In round brackets we report the standard error based on
Theorem 1. In square brackets we report the naive standard error that one would obtain when regressing firm effects on a
given covariate and using standard Eicker-White heteroskedastic robust standard errors. "Predicted Gap in Firm Effects"
reports the predicted difference in firm effects between older and younger workers according to either Column 1 or
Column 2 evaluated at the median firm size (12 Workers). See text for details.
                                                            Table 5: Inference on the Variance of Firm Effects
                                                                       Pooled                            Younger Workers                            Older Workers
                                                       Leave one out            Leave two out     Leave one out   Leave two out            Leave one out     Leave two out
                                                          sample                   sample            sample          sample                   sample            sample
Variance of Firm Effects
Leave out estimate                                         0.0240                  0.0238             0.0218              0.0221               0.0204               0.0180
                                                          (0.0006)                (0.0006)           (0.0006)            (0.0006)             (0.0025)             (0.0013)

Sum of Squared Eigenvalues                               2.1141E-06              1.6236E-06         3.9720E-06          2.9863E-06             0.0001               0.0001

Percentage of movers with no split sample estimator        14.92%                  0.00%              21.00%               0.00%               22.00%                0.00%
Percentage of mover pairs in                               0.04%                   0.05%               0.03%               0.05%                0.21%                0.46%

Confidence Intervals
95% Confidence Intervals - Strong id (q=0)            [0.0228; 0.0251]      [0.0227; 0.0249]     [0.0207; 0.0230]    [0.0210; 0.0232]     [0.0155; 0.0254]     [0.0154; 0.0205]
95% Confidence Intervals - Weak id (q=1)              [0.0228; 0.0251]      [0.0227; 0.0249]     [0.0207; 0.0230]    [0.0209; 0.0234]     [0.0157; 0.0267]     [0.0158; 0.0221]
95% Confidence Intervals - Weak id (q=2)              [0.0228; 0.0251]      [0.0227; 0.0251]     [0.0207; 0.0231]    [0.0209; 0.0234]     [0.0158; 0.0272]     [0.0161; 0.0231]

Curvature -                                                0.0182                  0.0416             0.0197              0.0689               0.3333               0.7845
Curvature -                                                0.0271                  0.1458             0.0444              0.0678               0.2779               1.1088

Diagnostics
Lindeberg Condition - q=1                                  0.1878                  0.0865             0.2061              0.0371               0.0359               0.0503
Lindeberg Condition - q=2                                  0.0866                  0.1604             0.0237              0.1639               0.0491               0.0479

Eigenvalues Ratios
Eigenvalue Ratio - 1                                       0.0135                0.0233                0.0189              0.0211               0.3132             0.5883
Eigenvalue Ratio - 2                                       0.0131                0.0202                0.0058              0.0135               0.0387             0.0499
Eigenvalue Ratio - 3                                       0.0112                0.0131                0.0050              0.0087               0.0314             0.0187
Note: This table conducts inference on the variance of firm effects using the samples described in Table 1. The round brackets report standard error estimates constructed
according to the procedure described in Section 4.2. Associated confidence intervals are computed under different assumptions on q, as described in Section 6.1. Curvature
reports the maximal curvature, see Appendix C6.1 for further details. "Eigenvalue ratio - 1" gives the ratio of the squared largest eigenvalue of the matrix  to the sum of all its
squared eigenvalues. "Eigenvalue Ratio - 2" and "Eigenvalue Ratio - 3" report the same ratio using the second and third largest eigenvalues respectively. "Percentage of movers
with no split sample estimator" reports the percentage of movers for which it is impossible to find two independent unbiased estimators of their conditional mean. "Percentage
of mover pairs in " gives the fraction of mover pairs where we use an unconditional variance estimate, see Section 4.2 for details.
                                                                         Table 6: Montecarlo Results
                                                                   Pooled                            Younger Workers                              Older Workers
                                                     Leave one out        Leave two out       Leave one out   Leave two out              Leave one out     Leave two out
                                                        sample               sample              sample          sample                     sample            sample
True Variance of Firm effects                           0.0240               0.0208              0.0219           0.0186                    0.0222            0.0199

Mean, Standard deviation across Simulations
Variance of Firm Effects - Leave Out (KSS)              0.0239                 0.0208               0.0219             0.0186                0.0223              0.0198
                                                       (0.00037)             (0.00037)            (0.00040)           (0.00039)            (0.00221)            (0.00276)

Variance of Firm Effects - Homescedatic Only (HO)       0.0336                 0.0260               0.0328             0.0242                0.0345              0.0264
                                                       (0.00036)             (0.00036)            (0.00037)           (0.00038)            (0.00216)            (0.00275)

Variance of Firm Effects - Plug in (PI)                 0.0352                 0.0270               0.0352             0.0256                0.0360              0.0273
                                                       (0.00036)             (0.00036)            (0.00036)           (0.00038)            (0.00216)            (0.00275)

Mean Estimated Standard error:                          0.00050               0.00039              0.00058             0.00042              0.00254              0.00273

Coverage Rate
Leave out - Strong id (q=0)                              0.9890                 0.9610                0.9970               0.9600               0.9720               0.8820
Leave out - Weak id (q=1)                                0.9900                 0.9640                0.9970               0.9630               0.9800               0.9590
Note: See Section 8.7 for description of data generating process (DGP). "True variance of firm effects" gives the variance of the firm effects in the DGP. KSS, PI, and HO report
the average of the estimated variance of firm effects across simulations for the three different estimators. Round brackets report the standard deviation of estimator across
simulations. ``Leave Out - Strong Id" reports the coverage rate of confidence interval for the variance of firms effects based upon KSS standard error and a normal
approximation. ``Leave Out - Weak Id" reports the coverage rate of the confidence interval described in Section 6.1 under q=1. All results based upon 1,000 Monte Carlo draws.
Appendix A                Data
This Appendix describes construction of the data used in the application of Section 8.


A.1     Veneto Workers History
Our data come from the Veneto Workers History (VWH) file, which provides social security based
earnings records on annual job spells for all workers employed in the Italian region of Veneto at
any point between the years 1975 and 2001. Each job-year spell in the VWH lists a start date,
an end date, the number of days worked that year, and the total wage compensation received by
the employee in that year. The earnings records are not top-coded. We also observe the gender of
each worker and several geographic variables indicating the location of each employer. See Card,
Devicienti, and Maida (2014) and Serafinelli (2019) for additional discussion and analysis of the
VWH.
   We consider data from the years 1984­2001 as prior to that information on days worked tend
to be of low quality. To construct the person-year panel used in our analysis, we follow the sample
selection procedures described in Card, Heining, and Kline (2013). First, we drop employment spells
in which the worker's age lies outside the range 18­64. The average worker in this sample has 1.21
jobs per year. To generate unique worker-firm assignments in each year, we restrict attention to
spells associated with "dominant jobs" where the worker earned the most in each corresponding
year. From this person-year file, we then exclude workers that (i) report a daily wage less than
5 real euros or have zero days worked (1.5% of remaining person-year observations) (ii) report a
log daily wage change one year to the next that is greater than 1 in absolute value (6%) (iii) are
employed in the public sector (10%) or (iv) have more than 10 jobs in any year or that have gender
missing (0.1%).




                                                55
Appendix B                  Computation
This Appendix describes the key computational aspects of the leave-out estimator ^, with an
emphasis on the application to two-way fixed effects models with two time periods discussed in
Example 4 and Section 8.


B.1       Leave-One-Out Connected Set
Existence of ^ requires Pii < 1 (see Lemma 1) and the following describes an algorithm which
prunes the data to ensure that Pii < 1. In the two-way fixed effects model of Section 8.2, this
condition requires that the bipartite network formed by worker-firm links remains connected when
any one worker is removed. This boils down to finding workers that constitute cut vertices or
articulation points in the corresponding bipartite network.
      The algorithm below takes as input a connected bipartite network G where workers and firms
are vertices. Edges between two vertices correspond to the realization of a match between a worker
and a firm (see Jochmans and Weidner, 2016; Bonhomme, 2017, for discussion). In practice, one
typically starts with a G corresponding to the largest connected component of a given bipartite
network (see, e.g., Card et al., 2013). The output of the algorithm is a subset of G where removal
of any given worker does not break the connectivity of the associated graph.
      The algorithm relies on existing functions that efficiently finds articulation points and largest
connected components. In MATLAB such functions are available in the Boost Graph Library and
in R they are available in the igraph package.

Algorithm 1 Leave-One-Out Connected Set
 1:   function PruningNetwork(G )             G  Connected bipartite network of firms and
      workers
 2:      Construct G1 from G by deleting all workers that are articulation points in G
 3:      Let G be the largest connected component of G1
 4:      Return G
 5:   end function

      The algorithm typically completes in less than a minute for datasets of the size considered in
our application. Furthermore, the vast majority of firms removed using this algorithm are only
associated with one mover.


B.2       Leave-Two-Out Connected Set
We also introduced a leave-two-out connected set, which is a subset of the original data such that
removal of any two workers does not break the connectedness of the bipartite network formed by

                                                   56
worker-firm links. The following algorithm proceeds by applying the idea in Algorithm 1 to each
of the networks constructed by dropping one worker. A crucial difference from Algorithm 1 is that
two workers who do not break connectedness in the input network may break connectedness when
other workers have been removed. For this reason, the algorithm runs in an iterative fashion until
it fails to remove any additional workers.

Algorithm 2 Leave-Two-Out Connected Set
 1:   function PruningNetwork2(G )               G  Leave-one-out connected bipartite network
      of firms and workers
 2:        a=1
 3:        while a > 0 do
 4:           G del = 
 5:           for g = 1, . . . , N do
 6:                Construct G1 from G by deleting worker g
 7:                Add all workers that are articulation points in G1 to G del
 8:           end for
 9:           a = |G del |
10:           if a > 0 then
11:                Construct G1 from G by deleting all workers in G del
12:                Let G2 be the largest connected component of G1
13:                Let G be the output of applying Algorithm 1 to G2
14:           end if
15:        end while
16:        Return G
17:   end function



B.3      Computing ^
Our proposed leave-out estimator is a function of the 2n quadratic forms

                               -1                 -1   -1
                     Pii = xi Sxx xi    Bii = xi Sxx ASxx xi     for i = 1, ..., n.

The estimates reported in Section 8 of the paper rely on exact computation of these quantities. In
                                                                                              -1
our application, k is on the order of hundreds of thousands, making it infeasible to compute Sxx
                                                                                                -1
directly. To circumvent this obstacle, we instead compute the k -dimensional vector zi,exact = Sxx xi
separately for each i = 1, .., n. That is, we solve separately for each column of Zexact in the system

                                          Sxx Zexact = X .
                                          k×k k×n      k×n




                                                 57
We then form Pii = xi zi,exact and Bii = zi,exact Azi,exact . The solution zi,exact is computed via
MATLAB's preconditioned conjugate gradient routine pcg. In computing this solution, we utilize
the preconditioner developed by Koutis et al. (2011), which is optimized for diagonally dominant
design matrices Sxx . These column-specific calculations are parallelized across different cores using
MATLAB's parfor command.


B.3.1    Leaving a Cluster Out
Table 3 applies the leave-cluster-out estimator introduced in Remark 3 to estimate the variance of
firm effects with more than two time periods and potential serial correlation. The estimator takes
the form   ^cluster = n yi x   ^            ^
                       i=1 ~i -c(i) where -c(i) is the OLS estimator obtained after leaving out
all observations in the cluster to which observation i belongs. A representation of ^cluster that is
useful for computation takes the observations in the c-th cluster and collect their outcomes in yc
and their regressors in Xc . The leave-cluster-out estimator is then

                                             C
                          ^cluster = 
                                     ^ A^-         yc Bc (I - Pc )-1 (yc - Xc ^),
                                             c=1

                                                       -1                   -1   -1
where C denotes the total number of clusters, Pc = Xc Sxx Xc , and Bc = Xc Sxx ASxx Xc . Since
                                                  -1                -1   -1
the entries of Pc and Bc are of the form Pi = xi Sxx x and Bi = xi Sxx ASxx x , computation can
proceed in a similar fashion as described earlier for the leave-one-out estimator.
   When defining the cluster as a worker-firm match, Table 3 applies     ^cluster to the two-way fixed
effects model in (6). When defining the cluster as a worker, the individual effects can not be
estimated after leaving a cluster out. Table 3 therefore applies ^cluster after demeaning at the
individual level. This transformation removes the individual effects so that the resulting model can
be estimated after leaving a cluster out.


B.3.2    Johnson-Lindenstrauss Approximation
When n is on the order of hundreds of millions and k is on the order of tens of millions, the exact
algorithm may no longer be tractable. The JLA simplifies computation of Pii considerably by only
requiring the solution of p systems of k linear equations. That is, one need only solve for the
columns of ZJLA in the system

                                        Sxx ZJLA = (RP X ) ,
                                        k×k k×p           k ×p


which reduces computation time dramatically when p is small relative to n.
   To compute Bii , it is necessary to solve linear systems involving both A1 and A2 , leading to 2p



                                                    58
systems of equations when A1 = A2 . However, for variance decompositions like the ones considered
in Section 8.2, the same 2p systems can be reused for all three variance components, leading to a
total of 3p systems of equations for the full variance decomposition. This is so because the three
                                                                         1
variance components use the matrices A = Af Af , A, =                    2 (Ad Af   + Af Ad ), and A = Ad Ad
where
                                                                              ¯              ¯
                                                                                              
                                   0      0       0                      d1 - d   ...   dn - d
                   Af =   1
                           n
                                     ¯
                                f1 - f   ...        ¯
                                               fn - f    and Ad =   1
                                                                     n    0
                                                                         
                                                                                   0      0 
                                                                                              
                                                                                                 .
                                   0      0       0                         0      0      0


                                                                                            ^ii and
Based on these insights, Algorithm 3 below takes as inputs X , Af , Ad , and p, and returns P
                ^ii 's which are ultimately used to construct the corresponding variance component
three different B
^JLA as defined in Section 1.2.


Algorithm 3 Johnson-Lindenstrauss Approximation for Two-Way Fixed Effects Models
 1:   function JLA(X ,Af ,Ad ,p)
 2:       Generate RB , RP  Rp×n , where (RB , RP ) are composed of mutually independent
      Rademacher entries
 3:       Compute (RP X ) , (RB Af ) , (RB Ad )  Rk×p
 4:       for  = 1, . . . , p do
 5:            Let r,0 , r,1 , r,2  Rk be the -th columns of (RP X ) , (RB Af ) , (RB Ad )
 6:            Let z,  Rk be the solution to Sxx z = r, for = 0, 1, 2
 7:       end for
 8:       Construct Z = (z1, , . . . , zp, )  Rk×p for = 0, 1, 2
 9:       Construct P     ^ii = 1 Z0 xi 2 , B      ^ii, = 1 Z1 xi 2 , B
                                                                      ^ii, = 1 Z2 xi 2 , B
                                                                                         ^ii, =
                                    p                     p                  p
      1
      p
        (Z1 xi ) (Z2 xi ) for i = 1, . . . , n
10:       Return {P   ^ii , B
                            ^ii, , B
                                   ^ii, , B^ii, }n
                                                 i=1
11:   end function


B.3.3      Performance of the JLA
Figure B.1 evaluates the performance of the Johnson-Lindenstrauss approximation across 4 VWH
samples that correspond to different (overlapping) time intervals (2000­2001; 1999­2001; 1998­
2001; 1997­2001). The x-axis in Figure B.1 reports the total number of person and firm effects
associated with a particular sample.
      Figure B.1 shows that the computation time for exact computation of (Bii , Pii ) increases rapidly
as the number of parameters of the underlying AKM model grow; in the largest dataset considered
­ which involves more than a million worker and firm effects ­ exact computation takes about 8
hours. Computation of JLA complete in markedly shorter time: in the largest dataset considered
computation time is less than 5 minutes when p = 500 and slightly over 6 minutes when p = 2500.

                                                         59
Notably, the JLA delivers estimates of the variance of firm effects almost identical to those computed
via the exact method, with the quality of the approximation increasing for larger p. For instance,
in the largest dataset, the exact estimate of variance of firm effects is 0.028883. By comparison,
the JLA estimate equals 0.028765 when p = 500 and 0.0289022 when p = 2500.
   In summary: for a sample with more than a million worker and firm effects, the JLA cuts
computation time by a factor of 100 while introducing an approximation error of roughly 10-4 .


B.3.4      Scaling to Very Large Datasets
We now study how the JLA scales to much larger datasets of the dimension considered by Card
et al. (2013) who fit models involving tens of millions of worker and firm effects to German social
security records. To study the computational burden of a model of this scale, we rely on a synthetic
dataset constructed from our original leave-one-out sample analyzed in Column 1 of Table 2, i.e.,
the pooled Veneto sample comprised of wage observations from the years 1999 and 2001. We scale
the data by creating replicas of this base sample. To connect the replicas, we draw at random
10% of the movers and randomly exchange their period 1 firm assignments across replicas. By
construction, this permutation maintains each (replicated) firm's size while ensuring leave-one-out
connectedness of the resulting network.
   Wage observations are drawn from a variant of the DGP described in Section 8.7 adapted to
the levels formulation of the model. Specifically, each worker's wage is the sum of a rescaled person
effect, a rescaled firm effect, and an error drawn independently in each period from a normal with
           1
variance   2       a0 + a
               exp(^    ^1 Bgg + a
                                 ^2 Pgg + a
                                          ^3 ln Lg2 + a
                                                      ^4 ln Lg1 ). As highlighted by Figure B.1, computing
the exact estimator in these datasets would be extremely costly. Drawing from a stable DGP allows
us to instead benchmark the JLA estimator against the true value of the variance of firm effects.
   Figure B.2 displays the results. When setting p = 250, the JLA delivers a variance of firm
effects remarkably close to the true variance of firm effects defined by our DGP. As expected, the
distance between our approximation and the true variance component decreases with the sample
size for a fixed p. Remarkably, we are able to compute the AKM variance decomposition in a
dataset with approximately 15 million person and year effects in only 35 minutes. Increasing the
number of simulated draws in the JLA to p = 500 delivers estimates of the variance of firm effects
nearly indistinguishable from the true value. This is achieved in approximately one hour in the
largest simulated dataset considered. The results of this exercise strongly suggest the leave-out
estimator can be scaled to extremely large datasets involving the universe of administrative wage
records in large countries such as Germany or the United States.




                                                     60
                                                      Figure B.1: Performance of the JLA Algorithm
                                                                           (a) Computation Time

                                           500
                                                       Computation Time - Exact
                                                       Computation Time - JLA with p=2500
                                           200         Computation Time - JLA with p=500


                                           100
             Minutes (Logarithmic Scale)




                                             5

                                             3



                                             1
                                                  700,000                                       900,000                1,000,000   1,100,000
                                                                                   Number of Person and Firm Effects


                                                                    (b) Quality of the Approximation

                                            .04
                                                       Estimate - JLA with p=500
                                                       Estimate - JLA with p=2500
                                                       Estimate - Exact Computation
                                                       Estimate - Plug-in
                                           .035
             Variance of Firm Effects




                                            .03




                                           .025




                                            .02
                                                  700,000                                       900,000                1,000,000   1,100,000
                                                                                   Number of Person and Firm Effects


Note: Both panels consider 4 different samples of increasing length. The four samples contain data from
the years 2000­2001, 1999­2001, 1998­2001, and 1997­2001, respectively. The x-axis reports the number of
person and firm effects in each sample. Panel (a) shows the time to compute the KSS estimate when relying
on either exact computation of {Bii , Pii }n
                                           i=1 or the Johnson-Lindenstrauss approximation (JLA) of these
numbers using a p of either 500 or 2500. Panel (b) shows the resulting estimates and the plug-in estimate.
Computations performed on a 32 core machine with 256 GB of dedicated memory. Source: VWH dataset.
                                                       Figure B.2: Scaling to Very Large Datasets
                                                                           (a) Computation Time

                                         90
                                                    Computation Time - JLA with p=250
                                                    Computation Time - JLA with p=500




                                         60
              Minutes




                                         30




                                          0
                                               1M            3M                     6M                            10M      14M
                                                                         Number of Person and Firm Effects (in millions)


                                                                   (b) Quality of the Approximation

                                         .04
                                                    Estimate - JLA with p=250
                                                    Estimate - JLA with p=500
                                                    Estimate - Plug-in
                                                    True value
              Variance of Firm Effects




                                         .03




                                         .02
                                               1M            3M                     6M                            10M      14M
                                                                         Number of Person and Firm Effects (in millions)


Note: Both panels consider synthetic datasets created from the pooled Veneto data in column 1 of Table 2
with T = 2. It considers {1, 5, 10, 15, 20} replicas of this sample while generating random links across replicas
such that firm size and T are kept fixed. Outcomes are generated from a DGP of the sort considered in
Table 6. The x-axis reports the number of person and firm effects in each sample. Panel (a) shows the time
to compute the Johnson-Lindenstrauss approximation          ^JLA using a p of either 250 or 500. Panel (b) shows
the resulting estimates, the plug-in estimate, and the true value of the variance of firm effects for the DGP.
Computations performed on a 32 core machine with 256 GB of dedicated memory. Source: VWH dataset.
B.4        Split Sample Estimators
Sections 4.2 and 5.2 proposed standard error estimators predicated on being able to construct
independent split sample estimators xi  -i,1 and xi  -i,2 . This section describes an algorithm for
construction of these split sample estimators in the two-way fixed effects model of Example 4. We
restrict attention to the case with Tg = 2 and consider the model in first differences: yg = fg  +
g for g = 1, . . . , N . When worker g moves from firm j to j , we can estimate fg  = j - j
without bias using OLS on any sub-sample where firms j and j are connected, i.e., on any sample
where there exist a path between firm j and j . To construct two disjoint sub-samples where
firms j and j are connected we therefore use an algorithm to find disjoint paths between these
firms and distribute them into two sub-samples which will be denoted S1 and S2 . Because it can
be computationally prohibitive to characterize all possible paths, we use a version of Dijkstra's
algorithm to find many short paths.10
       Our algorithm is based on a network where firms are vertices and two firms are connected by
an edge if one or more workers moved between them. This view of the network is the same as the
one taken in Section 7, but different from the one used in Sections B.1 and B.2 where both firms
and workers were viewed as vertices. We use the adjacency matrix A to characterize the network
in this section. To build the sub-samples S1 and S2 , the algorithm successively drops workers from
the network, so A-S will denote the adjacency matrix after dropping all workers in the set S .
    Given a network characterized by A and two connected firms j and j in the network, we let
Pjj (A) denote the shortest path between them.11 If j and j are not connected P
                                                                                (A) is empty.
                                                                                jj
Each edge in the path P (A) may have more than one worker associated with it. For each edge in
                           jj
P (A) the first step of the algorithm picks at random a single worker associated with that edge
  jj
and places them in S1 , while later steps place all workers associated with the shortest path in one
of S1 and S2 . This special first step ensures that the algorithm finds two independent unbiased
estimators of fg  whenever the network A is leave-two-out connected.
       For a given worker g with firm assignments j = j (g, 1), j = j (g, 2) and a leave-two-out con-
nected network A the algorithm returns the {Pg ,1 , Pg ,2 }N=1 introduced in Section 4.2. Specifically,
                  N                                N
fg  -g,1 =         =1 Pg ,1 y   and fg  -g,2 =      =1 Pg ,2 y   are independent unbiased estimators of
  10
      The algorithm presented below keeps running until it cannot find any additional paths. In our empirical
implementation we stop the algorithm when it fails to find any new paths or as soon as one of the two
sub-samples reach a size of at least 100 workers. We found that increasing this cap on the sub-sample
size has virtually no effect on the estimated confidence intervals, but tends to increase computation time
substantially.
   11
      Many statistical software packages provide functions that can find shortest paths.              In R
they are available in the igraph package while in MATLAB a package that builds on the
work of Yen (1971) is available at https://www.mathworks.com/matlabcentral/fileexchange/
35397-k-shortest-paths-in-a-graph-represented-by-a-sparse-matrix-yen-s-algorithm?
focused=3779015&tab=function.


                                                     63
fg  that are also independent of yg . If A is only leave-one-out connected then the algorithm
may only find one path connecting j and j . When this happens the algorithm sets Pg      ,2   = 0 for all
 as required in the formulation of the conservative standard errors proposed in Appendix C.5.1.

Algorithm 4 Split Sample Estimator for Inference
 1:   function splitsampleestimator(g, j, j , A) A  Leave-one-out connected network
 2:       Let S1 =  and S2 = 
 3:       For each edge in P   (A-g ), pick at random one worker from A-g who is associated
                                jj
      with that edge and add that worker to S1
 4:       Add to S2 all workers from A-{g,S1 } who are associated with an edge in P (A-{g,S } )
                                                                                    jj     1

 5:       Add to S1 all workers from A-{g,S1 ,S2 } who are associated with an edge in P (A-g )
                                                                                        jj
 6:                        
          Let stop = 1{Pjj (A-{g,S1 ,S2 } ) = } and s = 1
 7:       while stop < 1 do
 8:          Add to Ss all workers from A-{g,S1 ,S2 } who are associated with an edge in
       
      Pjj (A-{g,S1 ,S2 } )
 9:          Let stop = 1{P    (A-{g,S ,S } ) = } and update s to 1 + 1{s = 1}
                               jj       1 2

10:       end while
11:       For s = 1, 2 and = 1, . . . , N , let Pg ,s = 1{  Ss }f ( mSs fm fm ) fg
12:       Return {Pg ,1 , Pg ,2 }N =1
13:   end function

   In line 5, all workers associated with the shortest path in line 3 are added to S1 if they were not
added to S2 in line 4. This step ensures that all workers associated with P   (A-g ) are used in the
                                                                             jj
predictions. In line 11, Pg ,s is constructed as the weight observation receives in the prediction
fg  ^s where  ^s is the OLS estimator of  based on the sub-sample Ss .


B.5       Test of Equal Firm Effects
This section describes computation and interpretation of the test of the hypothesis that firm effects
for "younger" workers are equal to firm effects for the "older" workers which applies Remark 6 of
the main text.
      The hypothesis of interest corresponds to a restricted and unrestricted model which when
written in matrix notation are

                       y = F  +                                                                      (9)
                                    O          Y
                       y = FO  + FY  + F3 3 +  = X +                                                (10)


                                                   64
where y and F collects the first differences yg and fg across g . FO represents F for
"doubly connected" firms present in each age group's leave-one-out connected set interacted with a
dummy for whether the worker is "old"; FY represents F for doubly connected firms interacted
with a dummy for young; F3 represents F for firms that are associated with either younger
movers or older movers but not both. Finally, we let X = (FO , FY , F3 ),  = ( O ,  Y , 3 ) ,
and  = ( O , 3 ) .
   The hypothesis in question is  O -  Y = 0 or equivalently R = 0 for R = [Ir , -Ir , 0] and
r = |J | = dim( O ). Thus we can create the numerator of our test statistic by applying Remark 6
to (10) yielding

                                                      N
                                       ^= ^ A^-                   2
                                                             Bgg ^g                              (11)
                                                      g =1

                 -1    -1           2
where A = 1
          r R (RSxx R ) R; Bgg and ^g are defined as in Section 1.
   Two insights help to simplify computation. First, since FO FY = 0, FO F3 = 0 and
FY F3 = 0, we can estimate equation (10) via two separate regressions, one on the leave-one-
out connected set for younger workers and the other on the leave-one-out connected set for older
workers. We normalize the firm effects so that the same firm is dropped in both leave-one-out
samples.
   Second, we note that ^ A^ = y By where

                                       -1   -1               PX - PF
                                 B = XSxx ASxx X =                   ,                           (12)
                                                                 r
       -1
PX = XSxx X , and PF = F (F F )-1 F . Equation (12) therefore implies that Bii in
(11) is simply a scaled difference between two statistical leverages: the first one obtained in the
unrestricted model (10), say PX,gg , and the other on the restricted model of (9), say PF,gg . Section
B.3 describes how to efficiently compute these statistical leverages. To conduct inference on the
quadratic form in (11) we apply the routine described in Section 4.2.




                                                 65
Appendix C                  Proofs
This Appendix contains all technical details and proofs that where left out of the paper. The
material is primarily presented in the order it appears in the paper and under the same headings.


C.1      Unbiased Estimation of Variance Components
C.1.1     Estimator
Lemma C.1. It follows from the Sherman-Morrison-Woodbury formula that the two representa-
tions of ^ given in (1) and (2) are numerically identical, i.e., that ^ A^- n Bii  2
                                                                                  ^i = n         ^-i
                                                                            i=1        i=1 yi x
                                                                                              ~i 
whenever Sxx has full rank and maxi Pii < 1.

Proof. The Sherman-Morrison-Woodbury formula states that if Sxx has full rank and Pii < 1, then

                                              -1        -1
                                  -1         Sxx xi xi Sxx                       -1
                                 Sxx +                 -1      = Sxx - xi xi          .
                                             1-    xi Sxx xi

                              -1          -1   -1
Furthermore, we have that x
                          ~i Sxx xi = xi Sxx ASxx xi = Bii so

                                                                                            -1        -1
              ^-i = yi x                -1                      -1                   yi x
                                                                                        ~i Sxx xi xi Sxx
        yi x
           ~i          ~i Sxx - xi xi                       ~i Sxx
                                                   x y = yi x                 xy +                -1            xy
                                              =i                         =i
                                                                                          1 - xi Sxx xi    =i
                                                          -1
                                 2
                        ^ - Bii yi                       Sxx                         ^ - Bii yi (yi - xi ^-i )
                     ~i 
                = yi x             + yi Bii xi             -1            x y = yi x
                                                                                  ~i 
                                                   1 - xi Sxx xi    =i

                                                          =xi ^-i


                                       ^ - Bii  2                                 ^ A^=                          n         ^.
                                    ~i 
where the last expression equals yi x          ^i . This finishes the proof since                                       ~i 
                                                                                                                 i=1 yi x
In the above the Sherman-Morrison-Woodbury formula was also used to establish that

                                                                                -1
                        ^-i = xi Sxx - xi xi        -1                         Sxx
                     xi                                       x y = xi            -1              xy,
                                                         =i
                                                                          1 - xi Sxx xi      =i


                                                    yi - xi ^
and from this it follows that yi - xi ^-i =                   as claimed in the paper.
                                                     1 - Pii

C.1.2     Large Scale Computation
All discussions of the computational aspects are collected in Appendix B.

C.1.3     Relation To Existing Approaches
                                                                             2
                                ^HO is a function of the covariation between i
Next we verify that the bias of                                                and (Bii , Pii ).

                                                           66
Lemma C.2. The bias of ^HO is                                            n
                                                                   + SB n-
                               nB
                                                               2
                                                           ii ,i           k P           2
                                                                                     ii ,i
                                                                                             where

                        n                                           n                      n                              n
                                   2                          1                                                       1
       nB       2   =         Bii (i -¯ 2 ), ¯2 =                        2
                                                                         i , SB =               Bii , P       2   =                  2
                                                                                                                                Pii (i -¯ 2 ).
            ii ,i                                             n                                           ii ,i       n
                        i=1                                        i=1                    i=1                             i=1


Proof. Since ^2 =              1
                              n-k
                                     n
                                     i=1 (yi   - xi ^)2 =           1
                                                                   n-k
                                                                              n
                                                                              i=1
                                                                                         n
                                                                                          =1 Mi   i  we get that

                                                   n                         n                     n
                                 ^HO ] -  =                    2                           1                 2
                               E[                          Bii i    -              Bii                   Mii i
                                                                                          n-k
                                                   i=1                       i=1                   i=1
                                                    n                                              n
                                                                2                         1
                                               =           Bii (i -¯ 2 ) - SB                                2
                                                                                                        Mii (i -¯2)
                                                                                         n-k
                                                   i=1                                            i=1
                                                                          n
                                               = nB            2   + SB            2.
                                                           ii ,i        n - k Pii ,i

Comparison to Jackknife Estimators
This subsection compares the leave-out estimator ^ to estimators predicated on jackknife bias
corrections. We start by introducing some of the high-level assumptions that are typically used to
motivate jackknife estimators. We then consider some variants of Examples 2 and 3 where these
high-level conditions fail to hold and establish that the jackknife estimators have first order biases
while the leave-out estimator retains consistency.
High-level Conditions Jackknife bias corrections are typically motivated by the high-level as-
sumption that the bias of a plug-in estimator ^PI shrinks with the sample size in a known way and
that the bias of 1 n    ^PI,-i depends on sample size in an identical way, i.e.,
                         n     i=1

                                                             n
          ^PI ] =  +           D1 D2                   1        ^PI,-i =  + D1 + D2
        E[                       + 2,          E                                                                      for some D1 , D2 .         (13)
                               n  n                    n                   n - 1 (n - 1)2
                                                            i=1

                                    ^JK = n ^PI - n-1 n     ^                        D2
Under (13), the jackknife estimator                 n    i=1 PI,-i has a bias of - n(n-1) .
   For some long panel settings the bias in ^PI is shrinking in the number of time periods T such
that

                                                           
                                         E[^PI ] =  + D1 + D2                                  1, D
                                                                                     for some D    2.
                                                      T    T2
                                                                         1       T   ^                 ^      ^
In such settings, it may be that the biases of                           T       t=1 PI,-t      and 1
                                                                                                    2 (PI,1 + PI,2 ) depend on T in an
identical way, i.e.,

                    T                  2                                                                                    
         1         ^PI,-t =  + D1 +    D                                                     1 ^       ^PI,2 ) =  + 2D1 + 4D2 .
       E                                                                      and E            (PI,1 + 
         T                    T - 1 (T - 1)2                                                 2                       T     T2
               t=1



                                                                             67
From here it follows that the panel jackknife estimator    ^PJK = T ^PI - T -1 T    ^
                                                                           T    t=1 PI,-t has a bias
        2
        D
of - T (T -1) and that the split panel jackknife estimator  ^SPJK = 2^PI - 1 (^     ^
                                                                           2 PI,1 + PI,2 ) has a bias
                                                      1
of - 2D2 , both of which shrink faster to zero than
        2                                             D
                                                      T   if T  . Typical sufficient conditions for
     T
bias-representations of this kind to hold (to second order) are that (i) T  , (ii) the design is
stationary over time, and (iii) that ^PI is asymptotically linear (see, e.g., Hahn and Newey, 2004;
Dhaene and Jochmans, 2015). Below we illustrate that jackknife corrections can be inconsistent in
Examples 2 and 3 when (i) and/or (ii) do not hold. Finally we note that ^PI (a quadratic function)
need not be asymptotically linear as is evident from the non-normal asymptotic distribution of ^
derived in Theorem 3 of this paper.


Examples of Jackknife Failure
Example 2 (Special case). Consider the model

                     ygt = g + gt                (g = 1, . . . , N, t = 1, . . . , T  2),

      2
where gt =  2 and suppose the parameter of interest is  =               1
                                                                        N
                                                                            N    2
                                                                            g =1 g .   For T even, we have the
following bias calculations:

                                                              n
                         ^PI ] =  +   2                   1                      2
                                                                    ^PI,-i =  +  +    2
                       E[               ,             E                                     ,
                                      T                   n                     T  n(T - 1)
                                                              i=1
                 T                      2                                       2
             1         ^PI,-t =  +                    1 ^       ^PI,2 ) =  + 2 .
         E                                 ,    E       (PI,1 + 
             T                        T -1            2                       T
                 t=1

                                                                    2
The jackknife estimator   ^JK has a first order bias of -  , which when T = 2 is as large as
                                                          T (T -1)
that of ^PI but of opposite sign. By contrast, both of the panel jackknife estimators, ^P JK and the
leave-out estimator are exactly unbiased and consistent as n   when T is fixed.

   This example shows that the jackknife estimator can fail when applied to a setting where the
number of regressors is large relative to sample size. Here the number of regressors is N and the
sample size is N T , yielding a ratio of 1/T and we see that 1/T  0 is necessary for consistency of
^JK . While the panel jackknife corrections appear to handle the presence of many regressors, this

property disappears in the next example which adds the "random coefficients" of Example 3.

Example 3 (Special case). Consider the model

                 ygt = g + xgt g + gt                 (g = 1, . . . , N, t = 1, . . . , T  3)

      2
where gt =  2 and  =        1
                            N
                                N    2
                                g =1 g .


                                                 68
   An analytically convenient example arises when the regressor design is "balanced" across groups
as follows:

                                          (xg1 , xg2 , . . . , xgT ) = (x1 , x2 , . . . , xT ),

                                                               T
where x1 , x2 , x3 take distinct values and                    t=1 xt        = 0. The leave-out estimator is unbiased and
consistent for any T  3, whereas for even T  4 we have the following bias calculations:

                                        ^PI ] =  +             2
                                      E[                     T    2
                                                                         ,
                                                             t=1 xt
                               T                            2 T
                           1         ^PI,-t =  +                                     1
                      E                                                                            ,
                           T
                               t=1
                                                 T
                                                               t=1        s=t (xs         ¯-t )2
                                                                                         -x
                                                                         2
                      1 ^     ^PI,2 ) =  +                                                                 2
                E      (     +                                                            +                                ,
                      2 PI,1               2
                                                                T /2
                                                                              ¯ 1 )2
                                                                             -x               2    T
                                                                                                   t=T /2+1 (xt    ¯2 )2
                                                                                                                  -x
                                                                t=1 (xt

                 1                          2       T /2                         2       T
      ¯ -t =
where x        T -1       s=t xs ,   ¯1 =
                                     x      T       t=1 xt ,   and x
                                                                   ¯2 =          T       t=T /2+1 xt .
   The calculations above reveal that non-stationarity in either the level or variability of xt over
time can lead to a negative bias in panel jackknife approaches, e.g.,

                            ^SPJK -                   2 2                    2                         2
                          E                           T    2
                                                                -            T /2 2
                                                                                         -        T         2
                                                                                                                0
                                                      t=1 xt         2       t=1 xt
                                                                                             2    t=T /2+1 xt

                                                                                                  T /2 2        T         2
where the first inequality is strict if x
                                        ¯1 = x
                                             ¯2 and the second if                                 t=1 xt   =    t=T /2+1 xt .      In fact, the
following example

                                          (x1 , x2 , . . . , xT ) = (-1, 2, 0, . . . , 0, -1)

renders the panel jackknife corrections inconsistent for small or large T :

               E[^PJK ] =  - 7/5  2 + O                 1
                                                                   and E[^SPJK ] =  - 8/5  2 + O                      1
                                                                                                                               .
                              6                         T                              6                              T

Inconsistency results here from biases of first order that are negative and larger in magnitude than
                                   2
the original bias of ^PI (which is  ).
                                                6

Computations For this special case of example 2 we have that A = INN
                                                                      and Sxx = T IN so that
~= I             ~2     1                                       ^
A  N T and trace(A ) =
     N
                          2 = o(1) which implies consistency of  . Similarly we have that the
                                     NT




                                                                     69
bias of ~ is

                             N                              N                                                   Tg
                         1                     1                    2 2                             1
                                    Tg V[^
                                         g ] =                      =                    where ^g =                   ygt .
                         n                     n                      T                             Tg
                             g =1                           g =1                                                t=1


The same types of calculations lead to the other biases reported in the paper.
                                                           0 0                 T IN        0
   For this special case of example 3 we have that A =             and Sxx =
                                                           0 NIN
                                                                                 0    IN T      2
                                                                                           t=1 xt
which implies that trace(A~2 ) =      1
                                             = o(1) and therefore consistency of   ^. Similarly we
                                     T   2 2
                                                N       t=1   xt

have that the bias of ~ is

                                    N                                                                     Tg
                             1               ^g ] =               2                     ^g =              t=1 xt ygt
                                        Tg V[                   T    2
                                                                                  where                    T     2
                                                                                                                     .
                             n                                  t=1 xt                                     t=1 xt
                                 g =1


The same types of calculations lead to the other biases reported above. Now for the numerical
                                                                                             T    2             T
example (x1 , x2 , . . . , xT ) = (-1, 2, 0, . . . , 0, -1) we have                          t=1 xt    = 6,     t=T /2+1 (xt     ¯ 2 )2 = 1 - T
                                                                                                                                -x            2
                                                                                                                                                ,
  T /2                       T /2 2
  t=1 (xt    ¯1 )2 = 2
            -x               t=1 xt         ¯2
                                         - Tx 1 =5-
                                                                   2
                                                                   T,   and
                                                                        
                                                                                    4
                                                                        2-
                                                                                  T -1       if t = 2,
                                                                        
                                                     ¯-t )2 =
                                               (xs - x                  5-          1
                                                                                  T -1       if t  {1, T },
                                                                        
                                        s= t                            6                    otherwise,

Thus
                                                                              T
                                        T 2                   (T - 1)                            1
               E[^PJK ] -  =                        - 2
                                         T    2
                                         t=1 xt
                                                                 T
                                                                            t=1      s=t (xs          ¯-t )2
                                                                                                     -x
                                         T     T -1                        2         1      T -3
                              = 2          - 2                                1 +       4 +
                                         6       T                      5 - T -1  2 - T -1    6

                                               2   4   T -1   2       T -1   1                                                  7 2       1
                              = 2                -   -           1  -          4                                          =-       +O
                                               3 6T      T 5 - T -1     T 2 - T- 1
                                                                                                                               30         T

           ^SPJK ] -  =                  2 2                            2                                  2
     and E[                              T    2
                                                    -                                    +           T
                                         t=1 xt         2
                                                                T /2
                                                                t=1 (xt      ¯1 )2
                                                                            -x               2       t=T /2+1 (xt    ¯2 )2
                                                                                                                    -x
                                               1    1                     1                   8 2              1
                              = 2                -            4    -          4     =-           +O                   .
                                               3 10 -         T          2-   T
                                                                                             30                T




                                                                            70
C.1.4      Finite Sample Properties
Here we provide a restatement and proof of Lemmas 1 and 2 together with a characterization of
the finite sample distribution of ^ which was excluded from the main text.

Lemma C.3. Recall that  = ^ A^-                       n       2
                                                      i=1 Bii i .

   1. If maxi Pii < 1, then E[^] = .

   2. Unbiased estimators of  =  A exist for all A if and only if maxi Pii < 1.
                  2
   3. If i  N (0, i ), then  =                 r
                                                =1        b2 - V[^
                                                          ^      b ] and ^
                                                                         b  N b, V[^
                                                                                   b] .

                                   2                                      rC
   4. If maxi Pii < 1 and i  N (0, i ), then ^=
                                                                           =1          ^2 - V
                                                                                  (C ) y                   ^  N (µ, V ),
                                                                                                     where y
                                                                               2           2
        µ = QC X , V = QC QC , C = (Ci )i, ,  =                           diag(1 , . . . , n ),   and C = QC DC QC is a
        spectral decomposition of C such that DC = diag(1 (C ), . . . , rC (C ) and rC is the rank of C .

                       ^ A^=             n       n                   2                           -1
                                                                                     ^-i ) = yi Mii              n
Proof. First note that                   i=1      =1 Bi    yi y and ^i = yi (yi - xi                              =1 Mi   y , so

                     n    n
               ^=                            -1
                              Bi yi y - Bii Mii M i yi y
                     i=1 =1
                      n  n                                                                   n
                 =             Bi - 2-1 Mi                 -1
                                                      Bii Mii + B M -1             yi y =            Ci yi y .
                     i=1 =1                                                                 i=1 =i


The errors are mean zero and uncorrelated across observations, so
                          n                           n     n
                 ^] =                                                           -1
               E[                 Ci xi x  =                    Bi xi x  - Bii Mii Mi xi x  = ,
                         i=1 =i                       i=1 =1

         n      n                              n
since    i=1     =1 Bi   xi x = A and           =1 Mi     x = 0. This shows the first claim of the lemma.
   It suffices to show that no unbiased estimator of  Sxx  exist when maxi Pii = 1. Any potential
unbiased estimator must have the representation y Dy + U where E[U ] = 0 and D = (Di )i,
satisfies (i) Dii = 0 for all i and (ii) X DX = Sxx for X = (x1 , . . . , xn ) . (ii) implies that D must
be D = I + P DM ~ + M DP  ~ + M DM  ~             ~ where P = (Pi )i, and M = (Mi )i, . If the exist
                                         for some D
                              n     2
a i with Pii = 1, then         =1 Pi    = Pii yields Mi = 0 for all               which implies that Dii must equal 1
to satisfy (ii). However, this makes it impossible to satisfy (i). This shows the second claim.
                                        ~ = QDQ and definition of ^           1/2 ^
    Recall the spectral decomposition A                               b = Q Sxx   which satisfies that
^                                2                          r        2
b  N (b, V[^ b]) when i  N (0, i ). We have that  =              ^  b - V[^b ] since
                                                                             =1

                                                                     r
                             1/2 ~ 1/2 ^
                     ^ A^=^ Sxx ASxx  = ^b D^
                                            b=                            b2 ,
                                                                          ^
                                                                     =1


                                                                71
and
                       n                                                                        r
                                 2                        ^]) = trace(DV[^
                             Bii i = trace(B ) = trace(AV[               b]) =                       V[^
                                                                                                       b ].
                       i=1                                                                     =1

where B = (Bi )i, . This shows the third claim.
   The matrix C is is well-defined as maxi Pii < 1. Define y
                                                           ^ = QC (y1 , . . . , yn ) which satisfies that
                           2
^  N (µ, V ) when i  N (0, i
y                            ). Furthermore,

                                                                        rC
                                               ^ = y Cy = y
                                                          ^ DC y
                                                               ^=                 y2,
                                                                              (C )^
                                                                        =1

and Cii = 0 for all i, so that                  (C ) V = trace(C ) = 0. This shows the last claim.


C.1.5           Consistency
The next result provides a restatement and proof of Lemma 3.

                                                                          ^-  p
Lemma C.4. If Assumption 1 and one of the following conditions hold, then       0.

                                                           ~2 ) =
  (i) A is positive semi-definite,  =  A = O(1), and trace(A                                  r    2
                                                                                               =1      = o(1).
          1
 (ii) A = 2 (A1 A2 + A2 A1 ) where 1 =  A1 A1 , 2 =  A2 A2  satisfy (i).

Proof. Suppose that A is positive semi-definite. The difference between ^ and  is

                                           n   n               n                    n
                             ^-  = 2                Bi x i +            B i i  +          Bii (2
                                                                                               i -
                                                                                                   2
                                                                                                  ^i ),
                                          i=1 =1               i=1 =i               i=1


and each term has mean zero so we show that their variances are small in large samples. The
variance of the first term is

          n       n                 2
                                        2        2                       -1
      4                Bi x             i  4 max i  X B 2 X = 4 max i
                                                                    2
                                                                       ASxx          2
                                                                            A  4 max i 1 = o(1)
                                                i                        i                             i
          i=1     =1

where B = (Bi )i, , the last inequality follows from positive semi-definiteness of A, and the last
equality follows from  = O(1) and 1  trace(A    ~2 )1/2 = o(1). The variance of the second term is

                           n                               n    n
                                     2 2 2       4                    2         4       ~2 ) = o(1).
                       2            Bi i   2 max i                   Bi = 2 max i trace(A
                                                      i                         i
                           i=1 =i                         i=1 =1




                                                                72
Finally, the variance of the third term is

                                   n         n                                        2           n
                                                   -1                                     2              -2 2
                                                  Mll B         Mi x                      i +2          Mii Bii Mi2 i
                                                                                                                    2 2
                                                                                                                      
                               i=1           =1                                                  i=1 =i
                                                                          n                             n
                               1           2                                               2
                                   2   max i max(xi  )2                          2
                                                                                Bii +          max i 4        2
                                                                                                             Bii = o(1)
                               c        i             i                                    c    i
                                                                          i=1                          i=1

                                                  n    2              ~2 ) = o(1). This shows the first claim of the lemma.
where mini Mii  c > 0 and                         i=1 Bii       trace(A
                                                                          1
    When A is non-definite, we write A =                                  2     A1 A2 + A2 A1 and note that

       -1               1                      ~1 )
                                 ~2 ) + 2 max (A                                                         ~2 )  trace(A
                                                                                                                     ~2 1/2       ~2  1/2
     ASxx A               1 max (A                                                         and     trace(A            1)    trace(A 2)
                        2

        ~ = Sxx-1/2        -1/2                     ~2 ) is the largest eigenvalue of A ~2 . Thus
where A             Ak Ak Sxx   for = 1, 2 and max (A
consistency of ^ follows from 1 = O(1), 2 = O(1), trace(A ~2                     ~2
                                                           1 ) = o(1), and trace(A2 ) = o(1).

    The next result provides a restatement and proof of Lemma 4.

Lemma C.5. If Assumption 1, n/p4 = o(1), V[        ^]-1 = O(n), and one of the following conditions
hold, then V[^]-1/2 (^JLA - ^ - Bp ) = op (1) where |Bp |  1 n Pii 2       2
                                                                     |Bii |i .                     p   i=1


  (i) A is positive semi-definite and E[^ A^] -  = O(1).

                                                                                                                               V[^1 ]V[^2 ]
 (ii) A = 1
          2 (A1 A2 + A2 A1 ) where 1 =  A1 A1 , 2 =  A2 A2  satisfy (i) and                                                          ^ 2      = O(1).
                                                                                                                                nV[]

                                                               n      4       2            2
                                        n       22                   Pi -Pii (1-Pii )
Proof. Define Bp =             1                                =i                                        ^JLA - ^)2 be a second order ap-
                               p        i=1 Bii i                    (1-Pii )
                                                                                  2            . Letting (
                                                                                                                          V[(^JLA - ^)2 ]
proximation of ^JLA - ^, we first show that E (^JLA - ^)2                                                    = Bp and                              1
                                                                                                                                              = O( p ).
                                                                                                                               V[^]
Then we finish the proof of the first claim by showing that the approximation error is ignor-
                                                                                                        n      2
able. The bias bound follows immediately from the equality                                               = i Pi    = Pii (1 - Pii ) which leads to
      n      4          2                   2
0           
       = i Pi      - Pii ) .
                      Pii (1
    We have ^JLA - ^ = (^JLA - ^)2 + AE2 where

                                                  n                                                                  3     2
                                                           2                                                    1 3Pii + Pii
                      (^JLA - ^)2 =                       ^i          ^ii - B
                                                                Bii - B     ^ii a    ^ii
                                                                                ^i - B                  ^2
                                                                                                        ai -
                                                                                                                p 1 - Pii
                                                  i=1

           ^ii -Pii
           P
for a
    ^i =    1-Pii     and approximation error

                                        n                       2    2         2     2
                                              2^
                                                              ^ii
                                                           1 3P    ^ii
                                                                  +P   - (3Pii   + Pii       ^i )2
                                                                                       )(1 + a        ^3
                                                                                                      a i
                       AE2 =                 ^i Bii                                                -                           .
                                       i=1
                                                           p             ^i )2 (1 - Pii )
                                                                    (1 + a                           1+a  ^i



                                                                                      73
                                         ^JLA -   ^)2 we use independence between B
                                                                                  ^ii , P
                                                                                        ^ii , and  2
For the mean calculation involving (                                                              ^i ,
                ^ii , P
                      ^ii , and  2
unbiasedness of B               ^i , and the variance formula

                                                                        2
                                         2
                                     2 Pii - n=1 Pi4        3
                                                       1 3Pii     2
                                                              + Pii   Pii (1 - Pii )2 - 2                                                 n
                                                                                                                                           =i Pi
                                                                                                                                                4
                            V[^
                              ai ] =                 =              +                                                                               .
                                     p (1 - Pii )2     p 1 - Pii              p(1 - Pii )2

Taken together this implies that

                                                                            n                                   3     2
                                        ^JLA - ^)2 = -                                2                    1 3Pii + Pii
                                     E (                                          Bii i         i ] -
                                                                                              V[^                                     = Bp .
                                                                                                           p 1 - Pii
                                                                        i=1


For the variance calculation we proceed term by term. We have for y = (yi , . . . , yn ) that
                           n                                           n                               n     n                                          ~2 )
                                  2        ^ii ) = E V                           2 ^              2                2    2 2                       trace(A
                   V             ^i (Bii - B                                    ^i Bii | y                        Bi E ^i ^ =O                                 ,
                           i=1                                         i=1
                                                                                                  p   i=1 =1
                                                                                                                                                      p
                                     n                                  n                                         n        n
                                           2 ^                                   2 ^                         2                       E[B     ^ ]E[
                                                                                                                                        ^ ii B     2
                                                                                                                                                  ^i ^ ]
                                                                                                                                                        2

                               V          ^i     ^i = E V
                                             Bii a                              ^i     ^i | y, RB
                                                                                   Bii a                                       Pi2    (1-Pii )(1-P )
                                    i=1                                i=1
                                                                                                             p   i=1 =1
                                                                                    ~2
                                                                         ~2 ) trace(A
                                                                   trace(A            1)
                                                                                        1/2       ~2
                                                                                            trace(A 2)
                                                                                                      1/2
                                                       =O                    +              2
                                                                       p                  p
           -1/2
      ~ = Sxx        -1/2
where A         A A Sxx   for                              = 1, 2,
                   n                                         n     n
                        2 ^
         V             ^i     ^2
                          Bii a i - V[^
                                      ai ]             =                 ^ii B
                                                                             ^
                                                                       E B                E  2 2
                                                                                            ^i       ^2
                                                                                               ^ Cov a  ^2
                                                                                                      i,a
               i=1                                          i=1 =1
                                                                         ~2 )
                                                                   trace(A                     ~2
                                                                                         trace(A 1)
                                                                                                   1/2       ~2
                                                                                                       trace(A 2)
                                                                                                                 1/2
                                                       =O                   2        +
                                                                        p                               p3
                                                             n
                           n
                                  2                    2      =i
                                                                          2
                                                                   Pi4 - Pii (1 - Pii )2                           ~2 )
                                                                                                             trace(A
                   V             ^i ^ii - Bii
                                    B                                                               =O
                           i=1
                                                                   p(1 - Pii )2                                       p3
                                                             n
                             n
                                           2       2   2      =i
                                                                          2
                                                                   Pi4 - Pii (1 - Pii )2                     V[ ^]
                       V           Bii    ^i   -   i                                                =O
                           i=1
                                                                   p(1 - Pii )2                                  p2


From this it follows that V[^]-1/2 (^JLA - ^)2 - Bp                                                                        ~2 ) = O(V[
                                                                                                      = op (1) since trace(A          ^]) and
V [ ^ 1 ]V[ ^2 ]
    4     ^]2      = o(1).
   p V[
   We now treat the approximation error while utilizing that E[^  a3        1
                                                                                  a4
                                                                   i ] = O p2 , E[^
                                                                                             1
                                                                                    i ] = O p2 ,
                            
and maxi |a
          ^i | = op (log(n)/ p) which follows from (Achlioptas, 2003, Theorem 1.1 and its proof).




                                                                                         74
Proceeding term by term, we list the conclusions
                                n                         n                       ^ 1,PI - 1 ] + E[^ 2,PI - 2 ]
                                       2 ^                                      E[
                                      ^i     ^3
                                         Bii ai +              2 ^
                                                              ^i     ^4
                                                                 Bii a i = Op
                                i=1                    i=1
                                                                                                   p2
                                                  n                                      ^ 1,PI - 1 ] + E[^ 2,PI - 2 ]
                                                       2 ^        ^5
                                                                  ai            log(n) E[
                                                      ^i Bii            = Op                           2
                                              i=1
                                                                 1+a ^i             p                 p
       n         ^2   ^2         2      2                                                             ^ 1,PI - 1 ] + E[^ 2,PI - 2 ]
    1                                         ^i )2
           2 ^ 3Pii + Pii - (3Pii + Pii )(1 + a                                           log(n)    E[
          ^i Bii                                    = Op                         1+         
    p i=1                   ^i )2 (1 - Pii )
                       (1 + a                                                                 p                          p2

which finishes the proof.


C.2        Examples
All mathematical discussions of the examples are collected in Appendix C.7.


C.3        Quadratic Forms of Fixed Rank
The next result provides a restatement and proof of Theorem 1.

Theorem C.1. If Assumption 1 holds, r is fixed, and maxi wi wi = o(1), then
                       d
        b]-1/2 (^
   1. V[^       b - b) -                        1/2
                        N (0, Ir ) where b = Q Sxx  ,
                          p
        b]-1 V
   2. V[^    ^ [^
                b] - Ir ,
                r
   3. ^=
                 =1           b2 - V[^
                              ^      b ] + op (V[^]1/2 ),


Proof. The proof has two steps: First, we write ^ as r  ^    b2 - V[^
                                                                    b ] plus an approximation
                                                      =1
error which is of smaller order than V[^]. This argument establishes the last two claims of the
lemma. Second, we use Lyapounov's CLT to show that ^ b  Rr is jointly asymptotically normal.
Decomposition and Approximation From the proof of Lemma 2 it follows that
                                                      r                               n
                                            ^=
                                                                  b2 - V[^
                                                                  ^      b] +                  2
                                                                                          Bii (i - 2
                                                                                                  ^i )
                                                      =1                            i=1

                                                                                            n        2         2            ^]1/2 ).
where we now show that the mean zero random variable                                        i=1 Bii (i       -^i ) is op (V[
   We have
      n                               n           n                             n                             n
                  2       2                                -1                                                                  -1
           Bii (^
                i     -   i )   =           Bii           Mii xi Mi       +           Bii (2
                                                                                           i   -   2
                                                                                                   i )   +         Bii        Mii M i i  .
     i=1                              i=1         =1                            i=1                          i=1         =i




                                                                          75
The variances of these three terms are

   n            n                              2                     n                                                                            n
            2                 -1                        2                     2   -2                            -2
                      Mi Bii Mii xi                 max i                    Bii Mii (xi  )2  max i
                                                                                                  2
                                                                                                    max(xi  )2 Mii ×                                     2
                                                                                                                                                        Bii ,
                                                           i                                               i            i
   =1           i=1                                                 i=1                                                                           i=1
                                                                                     n                                      n
                                                                                            2
                                                                                           Bii V[2          4
                                                                                                 i ]  max E[i ] ×
                                                                                                                                   2
                                                                                                                                  Bii ,
                                                                                                           i
                                                                                     i=1                                    i=1
                       n                                                                                                           n
                                      2   -2                -1                   -1                               -2
                                     Bii Mii   +       Bii Mii B         M                 Mi2 i
                                                                                               2 2
                                                                                                               4
                                                                                                         2 max i Mii         ×             2
                                                                                                                                          Bii .
                                                                                                            i
                       i=1 =i                                                                                                     i=1


Furthermore, we have that
                                 n                                               r
                      V[^]-1           2
                                      Bii    max wi wi V[^]-1                         2  ~                 -4
                                                                                      l (A)  max wi wi max i = o(1),
                                                   i                                                 i              i
                                i=1                                          l=1


so each of the three variances are of smaller order than V[^].
                                                                                                  ^ [v ^
                                                                                                  V    b]-V[v ^
                                                                                                              b]
    For the second claim it suffices to show that  (v ) :=                                                          = op (1) for all nonrandom
                                                                                                       V[v ^
                                                                                                           b]
        r                                              r
v  R with v v = 1. Let v  R be nonrandom with v v = 1. As above we have that  (v ) =
   n              2    2                                                                                   n           4
   i=1 wi (v )(^i   - i  ) is a mean zero                  variable which is op (1) if                     i=1 wi (v )      = o(1) where wi (v ) =
            2
     (v wi )
  n     2      2 . But this follows from
  i=1 i (v wi )


                                               n
                                                                     -4
                                                       wi (v )4  max i  max wi wi = o(1)
                                                                         i                  i
                                              i=1

                                                                                                                n
where the inequality is implied by maxi wi wi = o(1), v v = 1, and                                              i=1 wi wi    = Ir .
Asymptotic Normality Next we show that all linear combinations of ^
                                                                  b are asymptotically normal.
                                                                                               d
        r
                                                                              b]-1/2 v (^
Let v  R be a non-random vector with v v = 1. Lyapunov's CLT implies that V[v ^         b - b) -
                                                                                               
N (0, 1) if

                                       n                                                             n
                               b]-2
                           V[v ^            E[4
                                              i ](v        Q    -1/2
                                                               Sxx   xi )4       = V[v ~]-2              E[4         4
                                                                                                           i ](v wi ) = o(1).                             (14)
                                      i=1                                                         i=1


We have that maxi wi wi = o(1) implies (14) since maxi (v wi )2  maxi wi wi and

                       n
                            (v wi )2 = 1,              V[v ~]-1  max i
                                                                     -2
                                                                        = O(1),                                max E[4
                                                                                                                     i ] = O (1),
                                                                             i                                  i
                      i=1

by definition of wi and Assumption 1.




                                                                                 76
C.4      Quadratic Forms of Growing Rank
This appendix provides restatements and proofs of Theorems 2 and 3. The proofs relies on an
auxiliary lemma which extends a central limit theorem given in Sølvsten (2019).


C.4.1    A Central Limit Theorem
The proofs of Theorem 2 and Theorem 3 is based on the following lemma. Let {vn,i }i,n be a
                                                                                           2
triangular array of row-wise independent random variables with E[vn,i ] = 0 and V[vn,i ] = n,i , let
                                                                                   n     2  2
{w
  n,i }i,n be a triangular array of non-random weights that satisfy                i=1 w
                                                                                        n,i n,i   = 1 for all n, and
                                                                                n×n
let (Wn )n be a sequence of symmetric non-random matrices in R                        with zeroes on the diagonal
                     n         2      2   2                                                               2
that satisfy 2       i=1   =i Wn,i    n,i n,       = 1. For simplicity, we drop the subscript n on vn,i , n,i ,
w
 n,i and Wn . Define

                                           n                        n
                                Sn =             w
                                                  i vi   and Un =            Wi vi v .
                                           i=1                      i=1 =i

                      4     -2
Lemma C.6. If maxi E[vi ] + i  = O(1),

                                         2
                               (i) max w
                                       i   = o(1),            (ii) max (W 2 ) = o(1),
                                       i

                 d
then (Sn , Un ) -
                 N (0, I2 ).

   This lemma extends the main result of Appendix A2 in Sølvsten (2019) to allow for {vi }i to
be an array of non-identically distributed variables and presents the conclusion in a way that is
tailored to the application in this paper. The proof requires no substantially new ideas compared
to Sølvsten (2019), but we give it at the end of the next section for completeness.


C.4.2    Limit Distributions
Theorem C.2. If

                           ^]-1 max (~                                            2
                     (i) V[          xi  )2 + (xi  )2 = o(1),           (ii)      1
                                                                                  r    2   = o(1),
                                  i
                                                                                   =1 


                               ^]-1/2 (      d
                                       ^ - ) -
and Assumption 1 holds, then V[               N (0, 1).

Proof. The proof involves two steps: First, we decompose ^ into a weighted sum of two terms of
the type described in Lemma C.6. Second, we use Lemma C.6 to show joint asymptotic normality
of the two terms. The conclusion that ^ is asymptotically normal is immediate from there.



                                                            77
Decomposition The difference between ^ and  is

                                          n                        n
                             ^-  =
                                                 xi  - x
                                                2~      i  i +              Ci  i  ,
                                          i=1                     i=1 =i


where these two terms are uncorrelated and have variances
                                n                                       n
                     VS =                    i  )2 i
                                      xi  - x
                                    (2~            2
                                                        and VU = 2                      2 2
                                                                                    Ci2 i  .
                             i=1                                       i=1 =i


Thus we write V[^]-1/2 (^ - ) = 1 Sn + 2 Un where

                            n                                                           n
                     -1/2                                                    -1/2
             Sn = VS                 xi  - x
                                    2~      i  i ,                 Un = VU                       C i i  ,
                            i=1                                                         i=1 =i

              1 =     VS /V[^],                                    2 =          VU /V[^].


Asymptotic Normality We will argue along converging subsequences. Move to a subsequence
where 1 converges. If the limit is zero, then V[  ^]-1/2 (^ - ) = 2 Un + op (1) and so it follows from
Result C.2 below and Theorem 2(ii) that     ^ is asymptotically normal. Thus we consider the case
where the limit of 1 is nonzero.
   In the notation of Lemma C.6 we have

                                           xi  - x
                                          2~     i                          Ci
                                    w
                                    i =          1/2
                                                             and Wi =        1/2
                                                                                    .
                                                VS                         VU

   For Lemma C.6(i) we have

                                   2           -1        xi  )2 + (
                                                        (~          xi  )2
                             max w
                                 i            41  max                      = o(1),
                              i                    i          V[ ^]

where the last equality follows from Theorem 2(i) and the nonzero limit of 1 .
    For Lemma C.6(ii) we show instead that trace(W 4 ) = o(1). It can be shown that for all n,
trace(C 4 )  cU · trace(B 4 ) = cU · trace(A
                                           ~4 )  cU 2         ~2                  4         ~
                                                    1 · trace(A ) and VU  cL mini i · trace(A), where
the finite and nonzero constants cU and cL do not depend on n (but depend on mini Mii which is
bounded away from zero). Thus, Assumption 1 implies that

                                        cU 2           ~2
                                            1 · trace(A )                       2
                 trace(W 4 )                   4                 =O             1
                                                                                            = o(1)
                                    (cL mini i   · trace(A~2 ))2              ~2 )
                                                                        trace(A

where the last equality follows from Theorem 2(ii).


                                                        78
Theorem C.3. If maxi wiq wiq = o(1), V[^q ]-1 maxi (~
                                                    xiq  )2 + (xiq  )2 = o(1), and Assumptions
1 and 2 holds, then

            ^q , ^q ) ]-1/2 (b
                             ^q , ^q ) - E[(b
                                            ^q , ^q ) ] -             d
      1. V[(b                                            N 0, Iq+1

                     q
      2. ^=
                      =1     b2 - V[^
                             ^      b ] +^q + op (V[^]1/2 )

for
                                                                                                                                      
                              n                        2                                                              2
                                               wiq wiq i                                     2wiq       = i Ci q x    i
             ^q , 
          V[(b    ^q ) ] =                                                                              2                             ,
                                                             2                                              2              2    2 2
                             i=1    2wiq          =i Ci q x  i 4                          = i Ci q x      i   +2      = i Ci q  i 


Ci    q   = Bi   q   - 2 - 1 Mi      -1
                                    Mii Biiq + M -1 B             q    , Bi        q
                                                                                             -1/2 ~ -1/2
                                                                                       = xi Sxx             ~q =
                                                                                                 Aq Sxx x , A                     r
                                                                                                                                   =q +1    qq,
             n                                  n               -1
x
~iq =         =1 Bi q x     , and x
                                  iq =           =1 Mi     M       B      qx   .

Proof. The proof involves two steps: First, we write     ^ as the sum of (1a) a quadratic function
           ^q , (1b) an approximation error which is of smaller order than V[
applied to b                                                                 ^], and (2) a weighted
sum of two terms, Sn and Un , of the type described in Lemma C.6. Second, we use Lemma C.6 to
           ^q , Sn , Un )  Rq+2 is jointly asymptotically normal.
show that (b
Decomposition and Approximation We have that
                                       q                                                                      n
                             ^=
                                             b2 - V[^
                                            (^      b ]) + ^q + op (V[^]1/2 )                   for ^q =              Ci q yi y
                                     =1                                                                      i=1 =i
since
                                       q           n       n
                         ^ A
                            ^=             b2 +
                                           ^                    B i q yi y
                                     =1           i=1 =1
and
                     n                 n               n                       n
                              2                 2                    2                           2   2
                         Bii ^i =          Bii1 i +            Biiq ^i +               Bii,-q (^
                                                                                               i   - i )
                 i=1                i=1                i=1                   i=1
                                     q                   n
                                                                     2         ^]1/2 )
                               =            V[^
                                              b ]+             Biiq ^i + op (V[
                                     =1                i=1


where Bii,-q = Bii - Biiq and it follows from maxi wiq wiq = o(1) and the calculations in the proof
of Theorem 1 that the mean zero random variable n Bii,-q (^     i2
                                                                   - i2
                                                                        ) is op (V[^]1/2 ).
                                                                                       i=1
      We will further center and rescale ^q by writing


                                              V[^q ]-1/2 ^q - E[^q ] = 1 Sn + 2 Un




                                                                          79
where
                               n                                                      n
                     -1/2                                                    -1/2
             Sn =   VS               xiq  - x
                                    2~      iq  i ,                  Un =   VU                  C i q i  ,
                              i=1                                                    i=1 =i
                     n                                                          n
                                            2   2
             VS =           xiq  - x
                           2~      iq           i ,                  VU = 2               Ci2 q i
                                                                                                2 2
                                                                                                   ,
                    i=1                                                         i=1 =i

             1 =         VS /V[^q ],                                 2 =        VU /V[^q ],

                                        ^q .
and Un is uncorrelated with both Sn and b
Asymptotic Normality As in the proof of Theorem 2, we will argue along converging subse-
quences and therefore move to a subsequence where 1 converges. If the limit is zero, then the
                                                                 ^q ]-1/2 (v b
conclusion of the theorem follows from Lemma C.6 applied to (V[v b           ^q - E[v b
                                                                                      ^q ]), Un ) for
v  Rq with v v = 1. Thus we consider the case where the limit of 1 is nonzero.
    Next we use Lemma C.6 to show that

                                        ^q - E[v b
                                       vb        ^q ] + uSn           d
                                                            , Un     -
                                                                      N (0, I2 )
                                           ^q + uSn ]1/2
                                         V[b

for any non-random (v , u)  Rq+1 with v v + u2 = 1. In the notation of Lemma C.6 we have

                                             -1/2
                                v wiq + uVS      xiq  - x
                                                2~       iq                               Ci
                                                                                          q
                         w
                         i =                        1/ 2
                                                                      and Wi =          1/2
                                                                                            .
                                          ^q + uSn ]
                                        V[b                                            VU

                                                         2
                                        ^q + uSn ]  mini i                               2
    A simple calculation shows that V[v b                                   0, so maxi w
                                                                                       i   = o(1) follows from
Theorem 3(i), Theorem 3(ii), and 1 being bounded away from zero.
    Similarly, we have as in the proof of Theorem 2 that
                                                       r
                   4            4                                         -4
            trace(Cq )  ctrace(Bq )  c2
                                      q +1                    2        2
                                                                  and VU  2      8
                                                                             min i       ~2 )2
                                                                                   trace(A
                                                                                      i
                                                      =q +1


for Cq = (Ci q )i, and Bq = (Bi q )i, , so Assumptions 1 and 2 yield trace(W 4 ) = o(1).


C.4.3     Proof of a Central Limit Theorem
The proof of Lemma C.6 uses the notation and verifies the conditions of Lemmas A2.1 and A2.2 in
Sølvsten (2019) referred to as SS2.1 and SS2.2, respectively. First, we show marginal convergence
in distribution of Sn and Un . Then, we show joint convergence in distribution of Sn and Un . Let
Vn = (v1 , . . . , vn ) where {vi }i are as in the setup of Lemma C.6.
                                      -2                                  n              2 2 2
    Before starting we note that maxi i  = O(1) and 2                     i=1       =i Wi i        = 1 implies that

                                                           80
trace(W 2 ) =          n
                       i=1           =i Wi
                                          2
                                              = O(1) and therefore that

                                                  max (W 2 ) = o(1)  trace(W 4 ) = o(1).


Marginal Distributions
                    4     -2                                      n     2 2                                                              d
Result C.1. maxi E[vi ] + i  = O(1),                              i=1 w
                                                                      i  i                                            
                                                                                = 1, and Lemma C.6(i) implies that Sn -
N (0, 1).

    In the notation of SS2.1 we have,
                                                                                                n
                                     0
                                     i Sn = w
                                             i vi       and E [Tn | Vn ] = 1 +              1
                                                                                            2         w
                                                                                                      i 2 2    2
                                                                                                         (vi - i ),
                                                                                                i=1

                            4     -2                                    n     2 2
and it follows from maxi E[vi ] + i  = O(1),                            i=1 w
                                                                            i  i        = 1, and Lemma C.6(i) that

                                              n                                n                                     4
                             L
                                 1
                                                                                                                  E[vi ]
            E [Tn | Vn ] - 1,                      E[(0     2
                                                      i Sn ) ]   = 1,               E[(0     4
                                                                                       i Sn ) ]  max               2       w
                                                                                                                           i 2
                                                                                                                               = o(1),
                                                                                                              i    i
                                             i=1                              i=1

so Result C.1 follows from SS2.1.
                    4     -2                                      n              2          2   2
Result C.2. maxi E[vi ] + i  = O(1), 2                            i=1        =i Wn,i        n,i n, = 1, and Lemma C.6(ii) implies
            d
that Un -
         N (0, 1).

    In the notation of SS2.1 we have,
                                                                                    n
                 0
                 i Un     = 2vi              Wi v      and E [Tn | Vn ] =                                   2
                                                                                                      (vi + i )Wi Wik v vk ,
                                        =i                                         i=1 =i k=i


and
                n                                       n
                                                                                           -4
                      E[(0     2
                         i Un ) ] = 2 ,                      E[(0     4    5       4 2
                                                                i Un ) ]  2 max E[vi ] max i max                                 Wi2 ,
                                                                                        i                 i           i
                i=1                                    i=1                                                                  =i




                                                                        81
                         2
where maxi          =i Wi          trace(W 4 ) = o(1). Now, split E [Tn | Vn ] - 1 into three terms

                                       n
                                                 2
                           an =                  i Wi2 (v + v 2 -  2 )
                                   i=1 =i
                                     n                                                   n
                                                            2
                           bn = 2                           k W k Wik vi v +                      Wi2 vi (v 2 -  2 )
                                        i=1 =i k=i,                                      i=1 =i
                                       n
                                                                    2   2
                           cn =                            Wi Wik (vi - i )v vk .
                                   i=1 =i k=i,


Interlude: Convergence in L1
an , bn , and cn are a linear sum, a quadratic sum, and a cubic sum. We will need to treat similar sums
                                                                                                                                           n
later, so we record some simple sufficient conditions for their convergence. For brevity, let                                              i=   =
  n                       n                n
  i=1      =i ,   and     i = =k   =       i=1       =i             k=i,   , etc. We use the notation ui = (vi1 , vi2 , vi3 , vi4 )  R4
to denote independent random vectors in order that the result applies to combinations of vi and
 2   2
vi - i as in an , bn , and cn above. For the inferential results we will also treat quartic sums, so we
provide the sufficient conditions here.
                                       n                                   n                              n
Result C.3. Let Sn1 =                  i=1 i vi1 ,     Sn2 =               i=   i vi1 v 2 , Sn3 =         i= =k   i k vi1 v 2 vk3 , and Sn4 =
  n
  i= =k=m i km vi1 v 2 vk3 vm4             where the weights i , i , i k , and i                           km   are non-random. Suppose
that E[ui ] = 0, maxi E[ui ui ] = O(1).
                                                       1
             n   2                                 L
   1. If     i=1 i      = o(1), then Sn1 - 0.
                                                       1
             n     2                               L
   2. If     i=    i = o(1), then Sn2 - 0.
                                                                1
             n          2                                   L
   3. If     i= =k      i k = o(1), then Sn3 - 0.
                                                                       1
             n       2                                                L
   4. If     i= =k=m i km          = o(1), then Sn4 - 0.

    Consider Sn3 , the other results follows from the same line of reasoning. In the notation of SS2.2
we have,

            0
            i Sn3 = vi1                    i k v 2 v k 3 + v i 2                         ik v 1 vk3   + v i3                ki v 1 vk2 .
                              =i k=i,                                       =i k=i,                            =i k=i,




                                                                             82
Focusing on the first term we have,
                                                          2 
              n                                                                              n
                   E vi1              i k v 2 vk3    max E[ui ui ]3                                      2
                                                                                                         i k + i k ik
                                                                         i
             i=1           =i k=i,                                                       i= =k
                                                                                                  n
                                                                                         3               2
                                                                     2 max E[ui ui ]                     i k,
                                                                             i
                                                                                             i= =k

                                       n                 2
so the results follows from SS2.2,     i= =k             i k = o(1), and the observation that the last bound also
applies to the other two terms in     0
                                      i Sn3 .


Marginal Distributions, Continued
                   1        1                       1
                L       L            L                                ¯i =                                         n
To see how an -   0, bn - 0 and cn -   0 follows from Result C.3, let W                                            k=1 Wik Wk   and
note that trace(W 4 ) = n    n
                           i=1 W¯ i2 . We have
                                     =1

                                                                         2
                                                n                                                 n
                                                                2 Wi2   max i
                                                                            4                              2
                                                                                                        ¯ ii
                                                                                                        W    .
                                                                                     i
                                            i=1           =i                                      i=1
                                                                     2
                           n                                                             n         n
                                                2                                    4                  ¯ i2
                                                k W k Wik                        max i                  W
                                                                                  i
                           i=1 =i        k=i,                                            i=1 =1
                                                                n
                                                                             Wi4 = O max Wi2
                                                                                             i,
                                                               i=1 =i
                                                                                                         
                                     n
                                                           Wi2 Wik
                                                                 2
                                                                   = O max                         Wi2  ,
                                                                                     i
                                     i=1 =i k=i,                                          =i


all of which are o(1) as trace(W 4 ) = o(1).


Joint Distribution
Let (u1 , u2 )  R2 be given and non-random with u2    2
                                                 1 + u2 = 1. Define Wn = u1 Sn + u2 Un .
                                                     d
Lemma C.6 follows if we show that Wn -
                                      N (0, 1). In the notation of SS2.1 we have,

              0
              i Wn = u1 w
                         i vi + u2 2vi                   Wi v
                                                    =i




                                                                    83
and
                                                    n                                   n
                  E[Tn | Vn ] = u2
                                 1        1+    1
                                                2         w
                                                          i 2 2    2
                                                             (vi - i )       + u2
                                                                                2
                                                                                                               2
                                                                                                         (vi + i )Wi Wik v vk
                                                    i=1                                i=1 =i k=i
                                            n
                                                          2   2
                             + u1 u2 3                  (vi + i )w
                                                                  i Wi vj .
                                           i=1 =i


The proofs of Result C.1 and Result C.2 showed that
                                     n                                       n
                                           E[(0     2
                                              i Wn ) ] = O (1),                      E[(0     4
                                                                                        i Wn ) ] = o(1)
                                     i=1                                     i=1


and that the first two terms of E[Tn | Vn ] converge to u2    2
                                                         1 + u2 = 1. Thus the lemma follows if we
show that the "conditional covariance"
                                                            n
                                                                       2   2
                                                        3            (vi + i )w
                                                                               i Wi vj
                                                            i=1 =i


converges to 0 in L1 . This conditional covariance involves a linear and a quadratic sum so
                                                    2
                        n                                                                   n
                                      2          4
                                      w Wi   max i max 2 (W )                                     w
                                                                                                  i 2
                                                                                                      = O(max 2 (W ))
                                                                i
                       i=1      =i                                                          i=1
                                 n                              n
                                              2
                                            w
                                            i   Wi2                   Wi2 max w
                                                                              i 2
                                                                                  = O(max w
                                                                                          i 2
                                                                                              )
                                                                                 i                   i
                                i=1 =i                       i=1 =i


ends the proof.


C.5          Asymptotic Variance Estimation
This appendix provides restatements and proofs of Lemmas 5 and 6 which establish consistency
of the proposes standard error estimators that rely on sample splitting. Furthermore, it gives ad-
justments to those standard errors that guarantee existence whenever two independent unbiased
estimators of xi  cannot be formed. However, these adjustments may provide a somewhat con-
servative assessment of the uncertainty in ^ as further investigated in the simulations of Section
8.7.
                                                                                        n                              n
Lemma C.7. For s = 1, 2, suppose that xi  -i,s =                                         =i Pi ,s y      satisfies      =i Pi ,s x    = xi  ,
Pi , 1 Pi   ,2   = 0 for all , and max (Ps Ps ) = O(1).
                                                                                                      ^-
                                                                                                             d
    1. If the conditions of Theorem 2 hold and |B| = O(1), then                                     ^ [^]1/2
                                                                                                             -
                                                                                                                 N (0, 1).
                                                                                                    V



                                                                        84
   2. If the conditions of Theorem 2 hold, then lim inf n P   ^ ± z V
                                                                    ^ [^]1/2                                                 1 - .

                                                             ^ [
Proof. The proof continues in two steps: First, we show that V  ^] has a positive bias which is of
smaller order than V[^] when |B| = O(1). Second, we show that V ^ [^] - E[V
                                                                          ^ [^]] = op (V[^]). When
combined with Theorem 2, these conclusions imply the two claims of the lemma.
        ^ [
Bias of V  ^] For the first term in V
                                    ^ [^], a simple calculation shows that

                                  2                                           2
                n                                   n                                            n
                                     2                                    2
          E 4                 Ci y  ~i =4                          Ci x   i +4                                  2 2
                                                                                                            Ci2 i 
                i=1      =i                        i=1       =i                                  i=1 =i
                                                    n          n
                                                                                                                         2 2
                                              +4                     Cmi Cm (Pmi,1 Pm                ,2   + Pmi,2 Pm ,1 )i 
                                                   i=1 =i m=1
                                                           n
                                                  ^] + 2                     2 2
                                                                          ~i i
                                              = V[                        C     .
                                                             i=1 =i


                       ^ [
For the second term in V  ^], we note that if Pik,- P                    k,-i   = 0 for all k , then independence between
                    2 2        2
error terms yield E[i        i,
                       ] = E[^         2,-i ] = i
                                 - ]E[^
                                                2 2
                                                   . Otherwise if Pi                              ,1   + Pi   ,2   = 0, then
                                                                                                                                    
       2 2
     E i  = E i -                        Pij,1 j  i -              Pik,2 k  x  +   -                                   P   m,-i m
                                                                                                                                    
                                  j =i                       k=i                                                  m=
                                                                                                                           
                  2 2
                = i  + x  E i -                         Pij,1 j  i -                    Pik,2 k               P   m,-i m
                                                                                                                           
                                                j =i                             k =i                  m=


where the second term is zero since P              i,-i   = 0 and Pij,1 Pij,2 = 0 for all j . The same argument
applies with the roles of i and           reversed when P          i,1   +P     i,2   = 0.
    Finally, when (i, )  B we have

                           2 2             2                                                 1
                         E i  =            i  2 + ((x - x
                                                        ¯ )  )2 + O                                    1{C
                                                                                                         ~i   <0}
                                                                                             n

where the remainder is uniform in (i, ) and stems from the use of y
                                                                  ¯ as an estimator of x¯  . Thus
                               2 2
                            ~i i                       2 2
                                                    ~i i                                ^ [^]. This
for sufficiently large n, E[C     ] is smaller than C     leading to a positive bias in V
bias is

                                  2                                                                        1 ^
                               ~i i
                               C     2 1{C
                                         ~i    >0}             ¯)  )2 1{C
                                                       + ((x - x        ~i              <0}      +O          V[]
                                                                                                           n
                      (i, )B


which is ignorable when |B| = O(1).
Variability of V^ [^] Now, V
                           ^ [^]-E[V
                                   ^ [^]] involves a number of terms all of which are linear, quadratic,


                                                              85
cubic, or quartic sums. Result C.3 provides sufficient conditions for their convergence in L1 and
therefore in probability. We have already treated versions of linear, quadratic, and cubic terms
carefully in the proof of Lemma C.6. Thus, we report here the calculations for the quartic terms
(details for the remaining terms can be provided upon request) as they also highlight the role of
the high-level condition max (Ps Ps ) = O(1) for s = 1, 2.
                                                                        2
                                         n                                   2            n
   The quartic term in 4                 i=1           = i Ci   y           ~i is         i= =m=k     i   mk i       m k where

                                                                                                       
                                     n                                                                 1,                   if i = ,
                   i   mk   =            Cji Cj Mjm,1 Mjk,2                   and Mi            ,s   =
                                                                                                       -P
                                 j =1                                                                       i ,s ,          if i = .

Letting       denote Hadamard (element-wise) product and Ms = In - Ps , we have

          n                     n
                2                        2
                i mk                     i mk =              (C 2 )2
                                                                   jj
                                                                      (M1 M1 )jj (M2 M2 )jj
     i= =m=k                i, ,m,k                    j,j
                                               2
                       = trace (C                  C 2 )(M1 M1                M 2 M2 )

                        max M1 M1                       M2 M2 trace C 2                         C 2 = O trace C 4                    = o V[^]2


where max M1 M1             M2 M 2         = O(1) follows from max (Ps Ps ) = O(1) and we established the
                                                                                                                               n            2 2
                                                                                                                                          ~ i
last equality in the proof of Theorem 2. The quartic term involved in 2                                                        i=1     = i Ci  has
                                                   n
variability of the same order as                   i= =m=k          i   mk i          m k where

                                                                                  n
                                i    mk
                                            ~i Mim,1 Mlk,1 +
                                           =C                                             ~ij Mim,1 Mjk,1 Mj
                                                                                          C                          ,2 .
                                                                               j =1


        ~ = (C
Letting C    ~i )i, , we find that

      n                     n                                                               n
              2                 ~i2 (M1 M1 )ii (M2 M2 ) + 2                                     ~ij C
                                                                                                    ~ (M1 M1 )ii (M1 M1 ) (M2 M2 )
              i mk  2           C                                                               C     ij                 jj        jj
  i= =m=k                i,                                                         j,j     i
                                                                                                                     
                                n
                   = O               ~i2 + trace (C
                                     C            ~2                        M1 M1 )(M1 M1              M2 M 2 ) 
                                i,

                             ~2
                   = O trace C                     .

        ~=C
We have C              C + 2(C            P1 ) (C            P2 ) + 2(C               P2 ) (C        P1 ), from which we obtain that

                          ~ 2) = O
                    trace(C                        max Ci2 + max (C 2 ) trace(C 2 )                          = o V[^]2
                                                       i,



                                                                             86
where we established the last equality in the proof of Theorem 2.

   Section 5.2 proposed standard errors for the case of q > 0, but left a few details to the appendix
since the definitions were completely analogous to the previous lemma. Those definitions are
C~i q = Ci2 q + 2 n Cmiq Cm q (Pmi,1 Pm ,2 + Pmi,2 Pm ,1 ) where Ci q was introduced in the proof
                   m=1
of Theorem 3 and is of the form Ci             q   = Bi   q   - 2-1 Mi      -1
                                                                           Mii Biiq + M -1 B                q    for Bi   q   = Bi -
  q
  s=1 s wis w s .
   Furthermore, the proposed standard error estimator relies on
                         
                           2
                          ^i, - ·  ^ 2,-i ,                          if Pik,- P     k,-i     = 0 for all k,
                         
                           2
                               ^ 2,-i ,
                         
                          ~i  ·                                      else if Pi         + Pi         = 0,
                         
                    2 2                                                           ,1           ,2
                    i  =
                           2
                          ^i, - ·  ~2,                               else if P    i,1   +P     i,2   = 0,
                         
                           2
                                            ¯)2 · 1{C
                         
                              - · (y - y
                          ^i,                       ~i q <0} ,       otherwise.

                                                                            n
Lemma C.8. For s = 1, 2, suppose that xi  -i,s satisfies                     =i Pi ,s x       = xi  , Pi , 1 Pi      ,2   = 0 for all
 , and max (Ps Ps ) = O(1) where Ps = (Pi ,s )i, .
                                                               -1 ^                                  p
   1. If the conditions of Theorem 3 hold and |B| = O(1), then q q  - Iq+1 .
                                                                
                                                              ^,q
   2. If the conditions of Theorem 3 hold, then lim inf n P   C    1 - .

   The following provides a proof of the first claim of this lemma, while we postpone a proof of
the second claim to the end of Appendix C.6.

                        ^q ]-1 V  ^q ] 
                               ^ [b    p          ^q ]-1 V
                                                         ^ [^q ] p
Proof. The statements V[b              - Iq and V[               - 1 follow by applying the arguments
in Theorem C.1 and Lemma C.7. Thus we focus on the remaining claim that
                                                                                                                      
                    ^q , 
                ^[v b                ^q , 
                         ^q ] - C [v b    ^q ] p                                         n
                C                                                      ^q , 
                                                                   ^[v b    ^q ] = 2                                      2
        (v ) :=                                - 0
                                                      where        C                          v wiq              Ci q y  ~i
                  V[v b ^q ]1/2 V[ ^q ]1/2
                                                                                        i=1                 =i


for all non-random v  Rq with v v = 1.
                       ^q , 
                   ^[v b    ^q ] Since  2                 2
Unbiasedness of C                      ~i is unbiased for i , it follows that
                                                                                                                
                            n                                         n
          ^q , 
      ^[v b    ^q ] = 2                                 2                                             2          ^q , ^q ]
    E C                          v wiq         Ci q x   i +2               v wiq             Ci q E[ ~i ] = C [v b
                           i=1            =i                         i=1                =i

                                   2
as split sampling ensures that E[ ~i ] for            = i.




                                                              87
Variability of C   ^q , 
               ^[v b    ^q ] Now, C   ^q , 
                                  ^[v b    ^q ] -C [v b
                                                      ^q , ^q ] is composed of the following linear, quadratic,
and quartic sums:
                                                                                                                                              
    n
         v wiq  2   2
                i - i
                                                                2
                                                     Ci q x  +  i              Ci q  +          Ci q  2        Mi ,1 Mik,2 + Mi ,2 Mik,1 k 
   i=1                                          =i                        =i               =i             k=
    n
         v wiq                  Ci q x                     Mim,1 Mik,2 m k +                  Ci q        2   2
                                                                                                          i - i
   i=1                     =i                    m k=m                                   =i


                       +             Ci    q         Mi ,1 Mik,2 + Mi ,2 Mik,1 k 2 -  2
                                =i             k=
    n
         v wiq               Ci      q                   Mim,1 Mik,2  m k
   i=1                 =i                m= k=m,


                              ^q ]1/2 V[
These seven terms are op (V[v b         ^q ]1/2 ) by Result C.3 as outlined in the following.

                                                    2
   n
        (v wiq )2                    Ci q x   = O(max wiq wiq V[^q ]) = o(V[v b
                                                                              ^q ]V[^q ])
                                                                   i
 i=1                            =i
   n          n                           2
                                                         2      ^q ]) = O(2        ^             ^     ^
                   v wiq Ci          q         = O(max (Cq )V[v b         q +1 V[v bq ]) = o(V[v bq ]V[q ])
  =1      i=1
   n          n                                                2
                   v wiq                 Ci q Mi ,1 Mik,2              = O(max wiq wiq trace(Cq M1                              ^q ]V[
                                                                                                               Cq M1 )) = o(V[v b     ^q ])
                                                                               i
 k=1      i=1
                                                                             2                                            2 
   n      n            n                                                                   n
                             v wiq              Ci q x Mim,1 Mik,2  = O                          (v wiq )2          Ci q x   
 m=1 k=1               i=1                =i                                               i=1                 =i
   n
              Ci2 q (v wiq )2 = O(max wiq wiq V[^q ])
                                                     i
 i=1 =i
   n     n             n                                      2
                           v wiq Ci q Mi ,1 Mik,2                          ^q ]max ((Cq
                                                                   = O V[v b                          M1 )(Cq                       ^q ]V[
                                                                                                                     M1 ) ) = o(V[v b     ^q ])
 k=1 =1            i=1
   n     n         n            n                                        2
                                                                                               2
                                                                                     ^q ]max (Cq
                                     v wiq Ci q Mim,1 Mik,2                  = O V[v b           )
  =1 m=1 k=1                 i=1


C.5.1             Conservative Variance Estimation
The standard error estimators considered in the preceding two lemmas relied on existence of the
independent and unbiased estimators xi  -i,1 and xi  -i,2 . This part of the appendix creates an
adjustment for observations where these estimators do not exist. The adjustment ensures that one


                                                                                   88
can obtain valid inference as stated in the lemma at the end of the subsection.
     For observations where it is not possible to create xi  -i,1 and xi  -i,2 , we construct xi  -i,1 to
satisfy the requirements in Lemma 6 and set Pi                          ,2   = 0 for all        so that xi  -i,2 = 0. Then we define
Qi = 1{max           2            as an indicator that xi  -i,2 could not be constructed as an unbiased estimator.
                   Pi    ,2 =0}
     Based on this we let
                                                                       2
                                                    n                                      n
                                      ^ 2 [^] = 4                        2                                2 2
                                                                                                       ~i i
                                      V                           Ci y  ~i, 2-2                        C     2
                                                          
                                                    i=1      =i                           i=1 =i

       2                 2
where ~i, 2 = (1 - Qi )~
                       i              ¯)2 and
                           + Qi (yi - y
                     
                       2
                     
                      ^i, - ·   ^ 2,-i ,                               if Pik,- P     k,-i     = 0 for all k and Qi = Q i = 0
                     
                       2
                            ^ 2,-i ,
                     
                      ~i   ·                                           else if Pi         + Pi         = 0 and Qi = Q i = 0,
                                                                                    ,1           ,2
                     
                       2
                                ~2,
                     
                          - ·                                                                          = 0 and Q = Qi = 0,
                     
                      ^i,                                              else if P          +P
         2 2                                                                        i,1          i,2
         i  2      =
                       2
                     
                      ^i, - · (y - y     ¯)2 · 1{C
                                                 ~i <0} ,              else if Qi = 0,
                                  2      2
                     (yi - y
                              ¯) ·   ^ ,-i · 1{C ~i <0} ,              else if Q i = 0,
                     
                              ¯)2 · (y - y    ¯)2 · 1{C
                     (y - y
                         i                            ~i <0} ,         otherwise

                                                     ^ 2 [
where we let Qi = 1{Pi ,1 =0=Qi } . The defintion of V    ^] is such that V
                                                                          ^ 2 [^] = V
                                                                                    ^ [^] when two
independent unbiased estimators of xi  can be formed for all observations, i.e., when Qi = 0 for
all i.
     Similarly, we let
                                                                                                                                 
                              n                      2                                                             2
                                            wiq wiq ^i, 2                          2wiq              = i Ci q y   ~i, 2
                  ^q,2 =
                                                                                                 2                               
                                                                   2                                  2              ~2 2 2
                             i=1 2wiq          = i Ci q y         ~i, 2 4       = i Ci q y           ~i -2        =i Ci q i  2


       2                 2
where ^i, 2 = (1 - Qi )^
                       i              ¯)2 and i
                                              2 2
                           + Qi (yi - y                          2 2
                                                 2 is defined as i              ~i
                                                                    2 but using C                                         q
                                                                                                                                         ~i .
                                                                                                                              instead of C
     The following lemma shows that these estimators of the asymptotic variance leads to valid
inference when coupled with the confidence intervals proposed in Sections 4 and 6.
                                               n                                           n
Lemma C.9. Suppose that                         =i Pi ,1 x     = xi  , either               = i Pi , 2 x     = xi  or max Pi2 ,2 = 0,
Pi , 1 Pi   ,2   = 0 for all , and max (Ps Ps ) = O(1) where Ps = (Pi ,s )i, .

    1. If the conditions of Theorem 2 hold, then lim inf n P   ^ ± z V
                                                                     ^ 2 [^]1/2                                                 1 - .

                                                                 ^
    2. If the conditions of Theorem 3 hold, then lim inf n P   C (q,2 )  1 - .



                                                                        89
    The following provides a proof of the first claim of this lemma, while we postpone a proof of
the second claim to the end of Appendix C.6.

Proof. As in the proof of Lemma 5 it suffices to show that V   ^ 2 [^] has a positive bias in large samples
         ^ 2 [
and that V    ^] - E[V
                     ^ 2 [^]] is op (V[^]). The second claim involves no new arguments relative to the
                                                                                       ^ 2 [
proof of Lemma 5 and is therefore omitted. Thus we briefly report the positive bias in V    ^].
    We have that
                                                     2
   ^ 2 [
 E V    ^] = V[^] + 4                                      ¯ )  )2
                                             Ci x   ((xi - x
                           i:Qi =1      =i
                              2
            +2             ~i i
                           C     2 1{C
                                     ~i        >0}           ¯)  )2 1{C
                                                     + ((x - x        ~i    <0}
                 (i, )B1

            +2                   2
                           ~i  2 i
                           C       1{C
                                     ~i                       ¯)  )2 1{C
                                                     + ((xi - x        ~i
                                               >0}                          <0}
                 (i, )B2
                                  2 2                   2
            +2             ~i
                           C      i  1{C
                                       ~i      >0}   + 2i       ¯)  )2 + ((xi - x
                                                          ((x - x                        ¯)  )2 1{C
                                                                                ¯)  (x - x        ~i         <0}
                 (i, )B3
                   1 ^
            +O       V[]
                   n

                                             ¯ and B1 , B2 , B3 refers to pairs of observations that
where the remainder stems from estimation of y
                                                          2 2
fall in each of the three last cases in the definition of i  2.


C.6      Inference with Nuisance Parameters
This Appendix starts by defining curvature and accompanying critical value for a given curvature as
                                                                             ~
introduced in Section 6. Then it derives the closed form representation of C (1 ) for any variance
matrix ~1  R2×2 where for general q we have

                                                       q                                      q
               ~                                            2 + q ,                                 2 + q
             C (q )   =                 min                 b                 max                   b
                                       q ,
                               1 ,...,b
                             (b           q ) E (~q )                   1 ,...,b
                                                                      (b        q ,q ) E (~q )
                                                      =1                                       =1
and
                                                           ^q - bq
                                                           b          -1
                                                                            ^q - bq
                                                                            b
             E (~q ) =          (bq , q )  Rq+1 :                    ~q
                                                                                           2
                                                                                          z, (~          .
                                                           ^q - q           ^q - q                  q)
                                                                            

                                  
                                ^,q         ^             ^                               ^q and  ^q,2 ,
Finally, it proofs validity of C       = C (q ) and C     (q,2 ) for any fixed q . As for 
                                  ~ [b
                                  V   ^q ]    ^q , 
                                           ~ [b
                                           C       ^q ]                                          ^
we partition    ~q into  ~q =                                ~ [
                                                        with V  ^q ]  R. In Section 6, C^,q = C  (q ),
                                ~   ^   ^
                               C[bq , q ]     ~  ^
                                             V[q ]
^ ,q = E (
E           ^q ), and ^ q = ( ^q ).



                                                               90
C.6.1     Preliminaries
Critical value function For a given curvature  > 0 and confidence level 1 - , the critical value
function z, is the (1 - )'th quantile of

                                                                                     2
                                                                                1            1
                                  q , 1 ,  =                     2
                                                                 q + 1 +                 -
                                                                                             

where 2     2
      q and 1 are independently distributed variates from the -squared distribution with q and
1 degrees of freedom, respectively.  q , 1 ,  is the Euclidean distance from (q , 1 ) to the circle
                  1                 1
with center (0, -   ) and radius    .   The critical value function at  = 0 is the limit of z, as   0,
which is the (1 - )'th quantile of a central 2
                                             1 random variable. See Andrews and Mikusheva
(2016) for additional details.
                                    ~
Curvature The confidence interval C (q ) inverts hypotheses of the type H0 :  = c versus
H1 :  = c based on the value of the test statistic

                                                          ^ q - bq
                                                          b                -1
                                                                                    ^q - bq
                                                                                    b
                                      min                                 ~q
                                                                          
                               bq ,q :g (bq ,q ,c)=0      ^q - q
                                                                                    ^q - q
                                                                                    

                          q
                          2 + q - c and bq = (b            1, . . . , b
                                                                       q ) . This testing problem depends
where g (bq , q , c) =   b =1 
                         ~q-1/ 2
on the manifold S = {x =         (bq , q ) : g (bq , q , c) = 0} for which we need an upper bound
on the maximal curvature. We derive this upper bound using the parameterization x(y         ) =
 ~ -1/2                     q   2                 q
q       (y           q , c-
         1 , . . . , y    =1  y
                               ) which maps from R to S , is a homeomorphism, and has a Jacobian
of full rank:

                                          -1/2
                                         ~q                         diag(1, . . . , 1)
                                  dx(y
                                     ) = 
                                                                 -21 y
                                                                      1 , . . . , -2q y
                                                                                      q

The maximal curvature of S , (~q ), is then given as (~q ) = max q y
                                                                    where
                                                                 R
                                                                y


                               (I - Py
                                      )V (u          u)                    -1/2
                                                                          ~q                     0
                 y
                  = sup                         2            ,         V =                                   ,
                         uR
                           q        dx(y
                                        )u                                               -21 , . . . , -2q

and Py
      = dx(y
            )(dx(y      ))-1 dx(y
                  ) dx(y         ) . See Andrews and Mikusheva (2016) for additional details.
Curvature when q = 1 In this case the maximization over u drops out and we have

                                                           2
                                                    (v V )
                                        V V -
                                                      vv
                   (~1 ) = max                                         where v = ~ -1/2 (1, -21 y
                                                                                                )
                                                                                  1
                                R
                               y             vv



                                                                  91
                                                                   ~ [
                                                                  ~V  ^q ]                         ~ [^
                                                                                                      b1 ,^q ]
and V = ~ -1/2 (0, -21 ). The value y
                                     = -                                      for   ~=
                                                                                                   C
                                                                                                                  is both a minimizer of
         1                                                      2 1 V~ [^
                                                                        b1 ]                  ~ ^   1/2 ~ ^ 1/2
                                                                                              V[b1 ] V[q ]
                                                                  2|1 |V ~ [^
                                                                            b1 ]
v v and (v V )2 , so we obtain that (~1 ) =
                                                             ~ [^q ]1/2 (1- 2 1/2       .
                                                             V             ~ )
Curvature when q > 1 In this case we first maximize over y
                                                          and then over u. For a fixed u we
want to find

          V u V u - V u Py                                                          q
                          Vu                         -1/2
   max                          ,        where Vu = ~q    (0, -2                             u2 ),    vu,y  ~ -1/2 (u , -2u Dq y
                                                                                                          = q                  ) ,
     q
    R
   y           vu,y
                   vu,y
                                                                                    =1


                                              that solves -2Dq y
and Dq = diag(1 , . . . , q ). The value for y                   ~ [b
                                                                =V  ^q ]-1 C  ^q , 
                                                                           ~ [b    ^q ] sets Py
                                                                                               Vu = 0
and minimizes vu,y
                  vu,y
                      . Thus we obtain

                                              |u Dq u|
                             2 maxuRq                                                            1 (V  ^q ]1/2 Dq V
                                                                                                    ~ [b             ^q ]1/2 )|
                                                                                                                  ~ [b
      ~q ) =                                 uV   ^q ]-1 u
                                               ~ [b                                           2|
     (                                                                  1/2
                                                                              =                                                         1/2
                  ~ [
                  V  ^q ] - C  ^q , 
                            ~ [b    ^q ] V  ^q ]-1 C
                                         ~ [b         ^q , 
                                                   ~ [b    ^q ]                     ~ [
                                                                                    V  ^q ] - C  ^q , 
                                                                                              ~ [b    ^q ] V  ^q ]-1 C
                                                                                                           ~ [b         ^q , 
                                                                                                                     ~ [b    ^q ]

       1 (·) is the eigenvalue of largest magnitude. This formula simplifies to the one derived above
where 
when q = 1.

                                          ~
C.6.2     Closed Form Representation of C (1 )
                                ~
An implicit representation of C (1 ) is

                                        ~
                                      C (1 ) = 1 b2              2
                                                  1,- + 1,- , 1 b1,+ + 1,+


where b1,± and 1,± are solutions to

                                                                                  1/2
                  b1,± = ^
                         b1 ± z,(     ~^
                                 ~1 ) V[b1 ](1 - a
                                                 ~(b1,± ))                              ,                                                     (15)
                                  ~ [
                                  V  ^1 ]1/2                                                                          1/2
                 1,±        ^
                          = 1 - ~            (^
                                         1/2 1
                                              b - b1,± ) ± z,(     ~ ^
                                                              ~1 ) V[1 ](1 - ~2 )~
                                                                                 a(b1,± )                                                     (16)
                                  ~  ^
                                  V[b1 ]

                                                         2     -1
                       sgn(1 )(          1
                                     ~1 )b
for a   1) =
    ~ (b       1+          ~ [^    1/2       +~
                                                     2
                                                                    .
                           V  b1 ]              1-~
    This construction is fairly intuitive. When ^ = 0, the interval has endpoints that combine

                                                                1/2      2                                                        1/2
         1 ^
           b1 ± z,(     ~^
                   ~1 ) V[b1 ](1 - a
                                   ~(b1,± ))                                  and ^q ± z
                                                                                         ,(
                                                                                                ~ ^
                                                                                           ~1 ) V[q ]a(b1,± )


         1 ) estimates the fraction of V[
where a(b                                ^] that stems from ^1 when E[^        1 . When 
                                                                       b1 ] = b         ^ is non-zero,
  ~                                           ^  ^                                 ~
C (1 ) involves an additional rotation of (b1 , 1 ) . This representation of C (1 ) is however not

                                                                    92
unique as (15),(16) can have multiple solutions. Thus we derive the representation above together
with an additional side condition that ensures uniqueness and represents b1,± and 1,± as solutions
to a fourth order polynomial.
                              ~
Derivation The upper end of C (1 ) is found by noting that maximization over a linear function
in 1 implies that the constraint must bind at the maximum. Thus we can reformulate the bivariate
problem as a univariate problem

                                                ^1 ]                      1/2                                                                 1 )2
                    2              2 ^       ~ [                                                                                      (^
                                                                                                                                       b1 -b
    max           1 b    
                     1 + 1 = max 1 b1 + 1 - ~V       (^
                                             ~ ^ 1/2 1
                                                          1) +
                                                      b -b                                         ~ [
                                                                                                   V  ^1 ](1 - ~2 ) z,
                                                                                                                     2
                                                                                                                       (~ )-             ~ [^
  1 ,
(b   1 )E (~1 )                   1
                                  b                              V[b1 ]                                                        1         V  b1 ]




                                                     1 that the term under the square-root is non-
where we are implicitly enforcing the constraint on b
negative. Thus we will find a global maximum in b  1 and note that it satisfies this constraint. The
first order condition for a maximum is

                                        ~ [^1 ]            1/2       ^     1            ~ [^1 ](1- 2
                                   1 +                               b1 -b                        ^ )
                                21 b   ~V
                                        ~ ^ 1/2
                                                +                     ~ [^
                                                                                        V
                                                                                               (^      )2
                                                                                                            =0
                                                  V[b1 ]              V  b1 ]       2           b1 -b  1
                                                                                    z,(~ )-       ~ [^
                                                                                           1      V  b1 ]



                                                                                                   (^
                                                                                                    b1 -b  1 )2
which after a rearrangement and squaring of both sides yields                                                               ))z 2 ~ . This
                                                                                                                  = (1 - a(b
                                                                                                      ~ [^
                                                                                                      V  b1 ]                  ,( )        1
in turn leads to the representation of b1,± given in (15). All solutions to this equation satisfies the
implicit non-negativity constraint since any solution b satisfies

                                       2               (^
                                                        b1 - b  1 )2
                                      z,  ~ )-
                                                                          1 )z 2 ~ > 0.
                                                                     = a(b
                                         (   1            ~ [^                ,(1 )
                                                          V  b1 ]

A slightly different arrangement of the first order condition reveals the equivalent quartic condition

                                                                       2                                                  2
         (^
          b1 -b 1 )2         sgn(1 )(         1
                                          ~1 )b              ~
                                                                                        sgn(1 )(        1
                                                                                                    ~1 )b                      2
            ~ ^        1+                         +                             =                         + ~
                                                                                                                              z, (~            (17)
                                                                                                                                      1)
                                 ~ ^    1/2                      2                                1/ 2         2
            V[b1 ]               V[b1 ]                     1-~                             ~ ^
                                                                                            V[b1 ]         1- ~


which has at most four solutions that are given on closed form. Thus the solution b1,+ can be found
as the maximizer of

                                        ~ [^1 ]        1/2                                                         1/2
                              2
                            1 b    ^
                               1 + 1 - ~V       (^   1) + z
                                                 b -b              ~ ^    
                                                              ~1 ) V[q ]a(b1 )
                                        ~ ^ 1/2 1
                                              V[b1 ]        ,(


among the at most four solutions to (17). More importantly, the maximum is the upper end of




                                                                           93
  ~
C (1 ). Now, for the minimization problem we instead have

                                                 ^1 ]          1/2                                                 1 )2
                     2              2 ^       ~ [                                                           (^
                                                                                                             b1 -b
       min         1 b    
                      1 + 1 = min 1 b1 + 1 - ~V       (^
                                              ~ ^ 1/2 1
                                                           1) -
                                                       b -b                     ~ [
                                                                                V  ^1 ](1 -       2
                                                                                            ~2 ) z, (~ )-      ~ ^
   1 ,
 (b   1 )E (~1 )                1
                                b                     V[b1 ]                                         1         V[b1 ]




which when rearranging and squaring the first order condition again leads to (17) as a necessary
                                                           ~
condition for a minimum. Thus b1,- and the lower end of C  (1 ) can be found by minimizing

                                      ~ [^1 ]   1/2                                       1/2
                            2
                          1 b    ^
                             1 + 1 - ~V       (^   1) - z
                                               b -b              ~ ^    
                                                            ~1 ) V[q ]a(b1 )
                                      ~ ^ 1/2 1
                                       V[b1 ]             ,(


over the at most four solutions to (17).


C.6.3        Asymptotic Validity
               -1 ^             p
Lemma C.10. If q q  - Iq+1 and the conditions of Theorem 3 hold, then

                                                    
                                                  ^,q
                                      lim inf P   C    1 - .
                                       n

Proof. The following two conditions are the inputs to the proof of Theorem 2 in Andrews and
Mikusheva (2016), from which it follows that


                
                                                                     ^ q - bq
                                                                     b           -1
                                                                                      ^ q - bq
                                                                                      b            2
              ^,q
  lim inf P   C   = lim inf P                     min                           ^q
                                                                                                  z, ^q     1-
   n                            n     (bq ,q ) :g (bq ,q ,)=0        ^q - q
                                                                                      ^q - q
                                                                                      

                           q   2 + q -  and bq = (b   1, . . . , b
                                                                 q) .
where g (bq , q , ) =         b
                            =1 
                                   -1/2
                                  ^q     ^q , ^q ) - E[(b^q ,          d
                                                                ^q ) ] -
      Condition (i) requires that       (b                              N 0, Iq+1 , which follows from
              -1 ^          p
Theorem 3 and q q  - Iq+1 .
      Condition (ii) is satisfied if the conditions of Lemma 1 in Andrews and Mikusheva (2016) are
satisfied. To verify this, take the manifold

                                Rq+1 : g
                           ~= x
                           S           ~(x
                                         ) = 0
for
                                 1/2 Dq 0 ^ 1/2
                                ^q                     ^q ] , 1) Dq 0  1/2
                                                                      ^q
                        g
                        ~(x
                          ) = x
                                          q x    + (2E[b                   x.
                                                                           
                                     0 0                         0 1

                 ~ is 
The curvature of S    ^, g
                         ~(0) = 0, and g
                                       ~ is continuously differentiable with a Jacobian of rank 1.
These are the conditions of Lemma 1 in Andrews and Mikusheva (2016).

Proof of the second claims in Lemmas C.8 and C.9. The proof contains two main parts. One part

                                                               94
is to establish that the biases of   ^q and   ^q,2 are positive semidefinite in large samples, and that
       -
   ^q ] 1 ^q - Iq+1 and E[        -
                            ^q,2 ] 1 ^q,2 - Iq+1 are op (1). These arguments are analogues to those
E[
presented in the proofs of Lemmas C.8 and C.9 and are therefore only sketched. The other part is
to show that this positive semidefinite asymptotic bias in the variance estimator does not alter the
validity of the confidence interval based on it. We only cover   ^q,2 as that estimator simplifies to
^q when the design is sufficiently well-behaved.

Validity First, we let QDQ be the spectral decomposition of E[^q,2 ]-1/2 q E[^q,2 ]-1/2 . Here,
QQ = Q Q = Iq+1 and all diagonal entries in the diagonal matrix D belongs to (0, 1] in large
samples. Now,

                                                         ^q - bq
                                                         b                         ^ q - bq
                                                                                   b
 P   C ^
       (q,2 ) = P                       min                          E[^q,2 ]-1                  2
                                                                                                z,    ^              + o(1)
                                                         ^q - q                    ^q - q          (E[      q,2 ])
                              (bq ,q ) :g (bq ,q ,)=0                              

where the minimum distance statistic above satisfies

                                        ^q - bq
                                        b                            ^ q - bq
                                                                     b
                        min                             E[^q,2 ]-1                = min ( - x) ( - x)
              (bq ,q ) :g (bq ,q ,)=0   ^q - q
                                                                     ^q - q
                                                                                    xS2



where S2 = {x : x = Q E[^q,2 ]-1/2 (bq , q ) - E[(b
                                                  ^q , ^q ) ] , g (bq , q , ) = 0} and the random vector
       ^q,2 ]-1/2 (b
                   ^q , ^q ) - E[(b
                                  ^q ,                                     d
                                       ^q ) ] has the property that D-1/2  -
 = Q E[                                                                     N (0, Iq+1 ). From the
geometric consideration in Andrews and Mikusheva (2016) it follows that S2 has curvature of
(E[ ^q,2 ]) since curvature is invariant to rotations. Furthermore,


                   min ( - x) ( - x)  2                   -1 , |1 |, (E[^q,2 ])
                  xS2

                                               2          (D-1/2  )-1 , |(D-1/2  )1 |, (E[^q,2 ])




where  = (1 , -1 ) and D-1/2  = ((D-1/2  )1 , (D-1/2  )-1 ) and the first inequality follows from the
proof of Theorem 1 in Andrews and Mikusheva (2016). Thus

                      ^                                      2
        lim inf P   C (q,2 ) = lim inf P min ( - x) ( - x)  z, (E[^
         n                                    n           xS2                                 q,2 ])


                                         lim inf P 2 q , 1 , (E[^q,2 ])  z 2
                                                                          ,(E[^                             =1-
                                              n                                                    q,2 ])


                    d
since ( -1 , |1 |) -
                    (q , 1 ).
Bias and variability in    ^q,2 We finish by reporting the positive semidefinite bias in ^q,2 . We




                                                              95
have that

          ^q,2 = q +                          2                wiq                           wiq                      0      0            1 ^
        E                                     i                                                                  +               +O         V[]
                                                       2      =i Ci   x            2         =i Ci   x                0 B                 n
                                    i:Qi =1


where

                            2
    B=2                ~i q i
                       C       2 1{C                ¯)  )2 1{C
                                   ~i q >0} + ((x - x        ~i q <0}
             (i, )B1

        +2                     2
                       ~i q  2 i
                       C                             ¯)  )2 1{C
                                   ~i q >0} + ((xi - x
                                 1{C                          ~i q <0}
             (i, )B2
                                    2 2              2
        +2             ~i
                       C    q       i    ~i q >0} + 2i ((x - x
                                       1{C                   ¯)  )2 + ((xi - x        ¯)  )2 1{C
                                                                             ¯)  (x - x        ~i q <0}
             (i, )B3


for B1 , B2 , B3 referring to pairs of observations that fall in each of the three last cases in the
              2 2
definition of i  2.


C.7      Verifying Conditions
Example 1. The only non-immediate conclusions are that:


                 ^]-1 max(~                                   maxi (xi  )2 /n2                        maxi (xi  )2
               V[         xi  )2 = O                               2                         =O
                                i                            mini i  trace(A~2 )                           r
                                                                     -2                1 2                                        2
                 ^]-1 max(                                   maxi,j Mjj Pjj -                (xj  )2 ( n=1 |Mi                   |) /n2
               V[         xi  )2 = O                                                   n
                                                                                          2
                                i                                                  mini i   trace(A~2 )
                                                                                               2
                                                             maxi,j (xj  )2 (       n
                                                                                     =1 |Mi |)
                                                =O                                                          .
                                                                           r

                                                   2
Example 2. We first derive the representations of ^  given in section 2. When there are no
                                                                                                          1                  Tg(i)
common regressors, the representation in (5) follows from Bii =                                         nTg(i)        1-      n       and

                                                                                                
                                                  Tg
                                     2     1                              1                             1                    2
                                    ^g =               ygt ygt -                             ygs  =                         ^i
                                           Tg                          Tg - 1                           Tg
                                                t=1                                    s=t                      i:g (i)=g


which yields that

                                                       n                    N
                                                                  2     1                     Tg      2
                                                             Bii ^i   =                1-            ^g .
                                                                        n                     n
                                                       i=1                  g =1




                                                                            96
With common regressors, it follows from the formula for block inversion of matrices that
                                                                                                                       
                                               ¯1n                                                  -1
                   -1
             ~ = ASxx           D        1  D -d                  I - X X (I - PD )X                     X (I - PD )
             X                         =                                                                               
                                X        n                                        0
                           ¯1n - 
                      1 D -d     ^ X (I - PD )
                  =
                      n          0

                                                                             -1
where D = (d1 , . . . , dn ) , X = (xg(1)t(1) , . . . , xg(n)t(n) ) , PD = DSdd D , 1n = (1, . . . , 1) , and Sdd =
D D. Thus it follows that

                                                  1        ¯- 
                                                      di - d  ^ (xg(i)t(i) - x
                                                                             ¯ g ( i) )
                                           x
                                           ~i =                                                 .
                                                  n                     0

    The no common regressors claims are immediate. With common regressors we have

            -1
      Pi = Tg (i) 1{g (i)=g (   )}   + n-1 (xg(i)t(i) - x
                                                        ¯g(i) ) W -1 (xg(       )t( )   -x          -1
                                                                                         ¯g( ) ) = Tg                  -1
                                                                                                      (i) 1{i= } + O (n )

            1   N      T
where W = n     g =1   t=1 (xgt - x
                                  ¯g )(xgt - x
                                             ¯g ) so Pii  C < 1 in large samples. The eigenvalues of
~ are equal to the eigenvalues of
A

                        1       -1/2 ¯¯ -1/2                            1 1/2          -1/2
                          I - nSdd d  d Sdd                      IN +    S D XW -1 X DSdd
                        n N                                             n dd
                                      c1              c2
which in turn satisfies that          n               n    for   = 1, . . . , N - 1 and c2  c1 > 0 not depending on n.
wi wi = O(Pii ) so Theorem 1 applies when N is fixed and ming Tg  . Finally,

                                                                  2             21       n
                                                           maxg,t g + xgt                       xg(i)t(i) 2 2
                      max V[^]-1 (~
                                  xi  )2 = O                                     n       i=1
                       i                                                         N
                                                                                            2
                            ^]-1 (                         maxi,j (xj  )2 (    n
                                                                                =1 |Mi    |)
                      max V[      xi  )2 = O
                       i                                                N

        n
and      =1 |Mi   | = O(1) so Theorem 2 applies when N  .
    We finish this example with a setup where an unbalanced panel leads to a bias and inconsistency
in ^HO . Consider


                        ygt = g + gt                                   (g = 1, . . . , N, t = 1, . . . , Tg )

where N is even, (Tg = 2, E[2       2                            2      2
                            gt ] = 2 ) for g  N/2 and (Tg = 3, E[gt ] =  ) for g > N/2, and the




                                                                  97
estimand is,

                                                 N                                    N
                                   1                       2                                          5N
                                 =                      Tg g        where n =               Tg =         .
                                   n                                                                   2
                                                 g =1                                g =1


                                      ~2 ) = N/n2 = o(1) as n   so the leave-out estimator
                  ~ = IN /n and trace(A
Here we have that A
is consistent. Furthermore,
                                                                                             
                                  1,             if i  N,                                    2 2 , if i  N,
                                     2                                                   2
                nBii = Pii =                                                             i =
                                  1,             otherwise,                                   2 , otherwise,
                                     3

so
                                         n
                         ~] -  =                     2         2             N            3 2
                       E[                        Bii i =             N+              =        ,
                                                               n             2             5
                                     i=1

                      ^HO ] -  =                                  n         2 2 2 2 2    2
                    E[            nB
                                                     2   + SB        P ,2 =     + ×    =    .
                                                 ii ,i          n - N ii i   50  3  50   15

           ~ is diagonal with N diagonal entries of                                  1 Tg                     1 Tg
Example 3. A                                                                         n Szz,g ,    so g =      n Szz,g   for g = 1, . . . , N .
                                                                                                        2
      ~2 )      1       1   N                                                                (zgt -z
                                                                                                   ¯g )
trace(A      ming Szz,g n   g =1 Tg = O (1 ).                  maxi wi wi = maxg,t             Szz,g         = o(1) when ming Szz,g 
                                   2
                        - 1
                      ^] = O( n ), so
.    Furthermore, V[              N

                                                             2 2
                                  ^]-1 max(~                zgt g
                                V[         xi  )2 = O max                                         = o(1),
                                        i              g,t N Szz,g


and Mi = 0 if g (i) = g ( ) so
                                                                                                                             
                                                                              2                                          2
                                                         n   i:g (i)=g Bii                                    Tg
       V[^]-1 max(xi  )2 = O max                                                   = O max                                    = o(1)
                i                            g                    N                               g          N Sxx,g

                                                                
                                                                   N Sxx,1
both under the condition that N   and                               T1        . Used above:

                          -1                  (zg(i)t(i) - z
                                                           ¯g(i) )(zg(i)t( ) - z
                                                                               ¯g(i) )
                    Pi = Tg (i) 1{g (i)=g (        )}    +                             1{g(i)=g(                        )}
                                                             Szz,g(i)
                          1 zg(i)t(i) - z
                                        ¯g(i) Tg(i)
                    Bii =                             .
                          n     Szz,g(i)     Szz,g(i)




                                                                     98
Finally,

                                                                             ¯1 )2
                                                                      (z1t - z
                                       max wiq wiq = max                           = o(1)
                                         i                    t          Szz,1
                                                                2 2
                                 ^   -1           2            zgt g
                               V[q ] max(~  xiq  ) = O max              = o(1),
                                         i              g 2,t N Szz,g
                                                                             
                                                                           2
                                 ^q ]-1 max(                        Tg
                               V[           xiq  )2 = O max                   = o(1)
                                         i               g 2       N Sxx,g

                                                                                                                         
                                    N                                                                                     N
under the conditions that          T2 Szz,2      and Szz,1  . Thus, Theorem 3 applies when                               T1 Szz,1   =
O(1).

Example 4. Let fi = (1{j (g,t)=0} , fi ) = (1{j (g,t)=0} , 1{j (g,t)=1} , . . . , 1{j (g,t)=J } ) and define the
following partial design matrices with and without dropping 0 from the model:

               n                         n                                  N                               N
     Sf f =         fi fi ,      Sff =         fi fi ,       Sf f =               fg fg ,        Sff =              fg fg ,
              i=1                        i=1                               g =1                            g =1


where fg = fi(g,2) - fi(g,1) . Letting D
                                        be a diagonal matrix that holds the diagonal of S   we
                                                                                         f f
have that

                                          -
                                     E = DS  1                    -1/2 S   D
                                                         and L = D           -1/2 .
                                           ff                           f f


Sff is rank deficient with Sff 1J +1 = 0 from which it follows that the non-zero eigenval-
                                                            -1
ues of E 1/2 LE 1/2 (which are the non-zero eigenvalues of Sff Sff ) are also the eigenvalues of
       -1          1J 1J
Sf f (Sf f +       Sff,11 ).   Finally, from the Woodbury formula we have that Af f is invertible with

                                                                        -1 ¯ ¯ -1
                                                                       Sf f f f Sf f                    1J 1J
              A- 1             ¯ ¯ -1 = n S -1 + n
               f f = n(Sf f - nf f )       ff
                                                                                                -1
                                                                                           = n Sf f +                ,
                                                                      1 - nf ¯ S -1 f¯                  Sff,11
                                                                                  ff

so

                               -1                                 1                              1
                     =  (Af f S f f ) =                                           =                             .
                                                    J +1- (Sf f A-1
                                                                 ff )                  nJ +1- (E 1/2 LE 1/2 )

With Ejj = 1 for all j , we have that

                                               2
                                               1
                                                                -2
                                                                          4
                                                                J
                                                         =         -
                                                                        
                                               J    2         J      2
                                                                       ( J  J )2
                                                =1             =1 




                                                                  99
since   2 (Chung, 1997, Lemma 1.7). An algebraic definition of Cheeger's constant C is

                                                                                -    j X      k/X    Sff,jk
                       C=                          min
                             X {0,...,J }:            jj  1
                                                     D
                                                                   J       jj
                                                                          D
                                                                                                  jj
                                                                                                 D
                                               j X        2        j =0                    j X


and it follows from the Cheeger inequality J  1 -                                   1 - C 2 (Chung, 1997, Theorem 2.3) that
               
  J J   if J C  .
   For the stochastic block model we consider J odd and order the firms so that the first (J + 1)/2
firms belongs to the first block, and the remaining firms belong to the second block. We assume
that fg is generated i.i.d. across g according to

                                                   f = W(1 - D) + BD

where (W, B, D) are mutually independent, P (D = 1) = 1 - P (D = 0) = pb  1
                                                                          2 , W is uniformly dis-
tributed on {v  RJ +1 : v 1J +1 = 0, v v = 2, maxj vj = 1, v c = 0}, and B is uniformly distributed
on {v  RJ +1 : v 1J +1 = 0, v v = 2, maxj vj = 1, (v c)2 = 4} for c = (1(J +1)/2 , -1(J +1)/2 ) . In this
model Ejj = 1 for all j . The following lemma characterizes the large sample behavior of Sff
and L. Based on this lemma it is relatively straightforward (but tedious) to verify the high-level
conditions imposed in the paper.
                                    log(J )        J log(J )
Lemma C.11. Suppose that              npb     +        n        0 as n   and J  . Then

                                      1J +1 1J +1                                                            1J +1 1J +1
        L J +1
            n Sff - IJ +1 +              J +1            = op (1)          and        L L - IJ +1 +             J +1        = op (1)

                    1J +1 1J +1
where L = IJ +1 -      J +1  - (1 - 2pb ) Jcc
                                            +1 and · returns the largest singular value of its argument.
                       -1
Additionally, max            -
                                = op (1) where       ···       are the non-zero eigenvalues of L .
                                                    1          J


Proof. First note that

                    J +1                            2+2pb                   1J +1 1J +1        cc        4pb cc
                      n E[Sff ]       -L=            J -1      IJ +1 -         J +1       -   J +1   +   J -1 J +1 ,


                    1J +1 1J +1
and L = IJ +1 -        J +1       - 1-        1
                                             2pb
                                                      cc
                                                     J +1 ,   so

                                          1J +1 1J +1                                      1J +1 1J +1
        L J +1
            n E[Sff ] - IJ +1 +              J +1             =     2+2pb
                                                                     J -1       IJ +1 -       J +1       -     cc
                                                                                                              J +1    +      2 cc
                                                                                                                           J -1 J +1

                                                                   2+2pb
                                                              =     J -1




                                                                   100
Therefore, we can instead show that S = op (1) for the zero mean random matrix

                                                                                               N
                   S = (L )1/2 J +1
                                 n  Sff - E[Sff ] (L )1/2 =                                          sg sg - E[sg sg ]
                                                                                              g =1

                               
                                2pb -1
where sg =        J +1
                    n fg   -   
                                 2pb n
                                       fg c Jc+1 .         Now since

                                                                  N
                                   J   1                                                                  J   1
                   sg sg = O         +                  and               E[sg sg sg sg ] = O               +
                                   n npb                                                                  n npb
                                                                  g =1

                                                                                                                      2
                                                                                                                     t (J     1
                                                                                                                         n + npb )
                                                                                                                 -
it follows from (Oliveira, 2009, Corollary 7.1) that P( S                                 t)  2(J + 1)e                 c(8+4t)      for some
                                                                      log(J/n )          J log(J/n )
constant c not depending on n. Letting t                                 npb         +        n          for n that approaches zero
slowly enough that log(    J/n )
                          npb    + J log(J/n )
                                         n       0 yields the conclusion that S = op (1).
                 -  1 /2          -1 /2                                                       
     Since L = D         Sff D          the second conclusion follows from the first if J +1
                                                                                          n D - IJ +1 =
                                                            N           2
op (1). We have  J +1
                    n
                           ] = IJ +1 and
                        E[D                J +1  jj =
                                                D
                                                n
                                                      J +1
                                                                (fg ej ) where ej is the j -th basis vector
                                                                  n           g =1
     J +1
in R   and P((fg ej )2 = 1) = 1-P((fg ej )2 = 0) = J +1
                                                     2
                                                        . Thus it follows from V( J +1        J +1
                                                                                    n Djj )  2 n
and standard exponential inequalities that J +1                        J +1 
                                             n D - IJ +1 = maxj | n Djj - 1| = op (1) since
J log(J )
    n        0.
                                                        1J +1 1J +1
    Finally, we note that L L - IJ +1 +                    J +1                implies


                                           v Lv (1 - )  v Lv  v Lv (1 + )

                                                                                                                j
                                                                                                                
which together with the Courant-Fischer min-max principle yields (1 - )                                         
                                                                                                                      (1 + ).
                                                                                                                 j

                                                                                         n
    Next, we will verify the high-level conditions of the paper in a model that uses J +1   L in place
              1                ~ and    n                    
of Sff and n L in place of A          J +1 IJ +1 in place of D . Using an underscore to denote objects
from this model we have

                                                            (1-2pb )
                        max P gg = max J +1           J +1
                                         n fg L fg = 2 n + 2 np      = o(1),
                           g                g                                                        b


                              ~ )= 2       trace((L )2 )          J -1                   1
                        trace(A                     2         =           2   +                = o(1),
                                                n                     n           4(npb )2
                               2
                               1                 1                1
                                       =    2             =
                           J
                            =1 
                                2                     2
                                           J trace((L ) )
                                                            (J - 1)4p2
                                                                     b +1




                                                                101
                                                                                        2
                                                                                         2
                                                                                                     1
which is o(1) if and only if                      Jpb  , and                           J         2   J . Furthermore,
                                                                                        =1   

                                                                           2
                                              c (L )1/2 fg                                     2           2
                                                                                                                    2
               max w2
                    g1   = max                                                 =                               =       = o(1),
                g                 g                  n                                        2pb n                npn
                                                                       2                                                                2
                                              1                                   2                                               1
            xg  )2 = max
        max(~                                    L fg                              2    max(fg  )2 + 1 -                                    (¯cl,1 - ¯cl,2 )2
           g                      g           n                                n             g                                   2pb

                                       1                   1
                         =O               2   +
                                      n            (npb )2

                             
which is o V[^] if                             ~2 ) = O(V[
                                Jpb   as trace(A          ^]) and

                                                                                                     2
                                                       2                   1                                       1
                              max(~
                                  xg1  ) = max                                fg                         =O               = o V[^] .
                                  g                                g       n                                       n2

Finally,
                                                               
                                                  N
                max(xg  )2 = O                              2 
                                                           Bgg                   ~)
                                                               = O max Bgg trace(A
                    g                                                                  g
                                                  g =1
where
                                      J +1          J + 1 1 - 4p2
                    max B gg = max fg 2 (L )2 fg = 2 2 +         b           ~2 )
                                                                   = O trace(A
                      g         g      n             n    (npb )2
                          ~) = J - 1 + 1 = o(1)
                    trace(A
                                 n    2p b n

                   ~) = O(trace(A
so maxg B gg trace(A              ~2 ))o(1).
    Finally, we use the previous lemma to transfer the above results to their relevant sample ana-
logues.

                                                                                  J +1 
            max|Pgg - P gg | = max|fg (S ff
                                            -                                             
                                                                                    n L )fg |
                g                             g
                                                                                                                                        1J +1 1J +1
                                      =   J +1
                                            n      max fg (L )1/2 L1/2 J +1
                                                                         n
                                                                            S
                                                                              ff
                                                                                 L1/2 - IJ +1 +                                            J +1       (L )1/2 fg
                                                       g

                                                                                                     1J +1 1J +1
                                      =O           L J +1
                                                       n Sff - IJ +1 +                                  J +1            max P gg = o max P gg
                                                                                                                         g                    g
                                              J                                                                               
                  ~2 - A
                       ~ ) =  2                    1   1        ~2 )O max                                                    -                    ~ )   2
            trace(A                                  2         -
                                                       = trace(A
                                                       2 2                                                                             = op trace(A
                                           n    n2                                                                           
                                        =1
                                                                     ~2 - A
                                                                                                                             
                                                              trace(A       ~2 )
        2
        1                2
                         1               2 1     | J -  |
                                                        J
                    -                 = J      O           +                                                                  = op (1)
        J
         =1    2
                        J
                         =1   2              2    =1             trace(A~2 )  J




                                                                                       102
                                                           2                2
                                                          2                2
with a similar argument applying to                      J        2   -   J        2   . Furthermore,
                                                          =1               =1   

                                                                                           2
                   2
              max wg           J +1  1/2 (L n S 
                     1 = max fg ( n L )
                                                    1 /2                                             n
                                                                                                (L J +1  
                                                                                                             )1/2 max P gg = op (1)
               g           g               J +1 ff )     q1                                             S ff                       g
                   2       2                        ~2
and maxg |(~        xg  ) | = op (trace(A )) since
           xg  ) - (~
                                                                                                                 2
                                                                                         1J +1 1J +1
            ~ g  )2 =
      xg  - x
  max(~                  J +1
                            2   max fg L LSff D
                                               - IJ +1 +
                                                                                            J +1
                                                                                                         
                                                                                                         J +1
    g                     n       g

                              - IJ +1 +                   1J +1 1J +1                       2
                        LSff D                               J +1         max B gg
                                                                            g             J +1
                       = op (trace(A ))    ~2


                                        xg1  )2 | = op (1) as the previous result does not depend on
                             xg1  )2 - (~
and this also handles maxi |(~
                
the behavior of Jpb . Finally,

                                  J +1                                                                               1J +1 1J +1
         max|Bgg - B gg | =            2       max fg L             n        
                                                                  J +1 LSff DSff L                     - IJ +1 +        J +1           L fg
          g                        n            g

                                     n      J +1  n                                                1J +1 1J +1
                                   J +1 LSff n D J +1 Sff L                            - IJ +1 +      J +1         max B gg
                                                                                                                      g

                               = op (max B gg )
                                           g
                                   J                                         
                 ~-A
           trace(A ~) =                     1
                                              -
                                                 1                ~)O max  - 
                                                          = trace(A                                                   ~)
                                                                                                           = op trace(A
                                           n    n                          
                                   =1




                                                                      103
