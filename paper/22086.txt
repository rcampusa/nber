                              NBER WORKING PAPER SERIES




                  EMPIRICAL TOOLS AND COMPETITION ANALYSIS:
                    PAST PROGRESS AND CURRENT PROBLEMS

                                          Ariel Pakes

                                      Working Paper 22086
                              http://www.nber.org/papers/w22086


                    NATIONAL BUREAU OF ECONOMIC RESEARCH
                             1050 Massachusetts Avenue
                               Cambridge, MA 02138
                                    March 2016




This is a revised version of my Laffont lecture, given at the 2015 CRESSE conference, Crete. I
thank the editor and a referee for extensive helpful comments. The views expressed herein are
those of the author and do not necessarily reflect the views of the National Bureau of Economic
Research.

NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.

¬© 2016 by Ariel Pakes. All rights reserved. Short sections of text, not to exceed two paragraphs,
may be quoted without explicit permission provided that full credit, including ¬© notice, is given
to the source.
Empirical Tools and Competition Analysis: Past Progress and Current Problems
Ariel Pakes
NBER Working Paper No. 22086
March 2016
JEL No. L1,L4

                                          ABSTRACT

I review a subset of the empirical tools available for competition analysis. The tools discussed are
those needed for the empirical analysis of; demand, production efficiency, product repositioning,
and the evolution of market structure. Where relevant I start with a brief review of tools
developed in the 1990‚Äôs that have recently been incorporated into the analysis of actual policy.
The focus is on providing an overview of new developments; both those that are easy to
implement, and those that are not quite at that stage yet show promise.


Ariel Pakes
Department of Economics
Harvard University
Littauer Room 117
Cambridge, MA 02138
and NBER
apakes@fas.harvard.edu
1    Introduction.
This paper reviews a set of tools that have been developed to enable us to empir-
ically analyze market outcomes, focusing on their possible use in formulating
and executing competition policy. I will be more detailed on developments
that seem to be less broadly known by the antitrust community (often because
they have only been recently introduced), and will emphasize problems that are
current roadblocks to expanding the purview of empirical analysis of market
outcomes further.
    The common thread in the recent developments has been a focus on incorpo-
rating the institutional background into our empirical models that is needed to
make sense of the data used in analyzing the issues of interest. In large part this
was a response to prior developments in Industrial Organization theory which
used simplified structures to illustrate how different phenomena could occur.
The empirical literature tries to use data and institutional knowledge to narrow
the set of possible responses to environmental or policy changes (or the inter-
pretations of past responses to such changes). The field was moving from a
description of responses that could occur, to those that were ‚Äúlikely‚Äù to occur
given what the data could tell us about appropriate functional forms, behavioral
assumptions, and environmental conditions.
    In pretty much every setting this required incorporating
    ‚Ä¢ heterogeneity of various forms into our empirical models,
and, when analyzing market responses it required
    ‚Ä¢ use of equilibrium conditions to solve for variables that firms could change
      in response to the environmental change of being analyzed.
    The difficulties encountered in incorporating sufficient heterogeneity and/or
using equilibrium conditions differed between what was traditionally viewed as
‚Äústatic‚Äù and ‚Äúdynamic‚Äù models. The textbook distinction between these two was
that: (i) static models analyze price (or quantity) setting equilibria and solve for
the resultant profits conditional on state variables, and (ii) dynamics analyzes
the evolution of those state variables (and through that the evolution of market
structure). The phrase ‚Äùstate variables‚Äù typically referred to properties of the
market that could only be changed in the medium to long run: the characteristics
of the products marketed, the determinants of costs, the distribution of consumer

                                         3
characteristics, any regulatory or other rules the agents must abide by, and so
on.
   This distinction has been blurred recently by several authors who note that
there are a number of industries in which products of firms already in the market
can be ‚Äùrepositioned‚Äù (can change one or more characteristic) as easily (or al-
most as easily) as they can change prices, and then shown that in these industries
static analysis of an environmental change that does not take into account this
repositioning is likely to be misleading, even in the very short run. The prod-
uct repositioning literature also employs different empirical tools than had been
used to date, and the tools are relatively easy for competition analysis to access
and use. So between the static and dynamic sections I will include a section
on ‚Äùproduct repositioning‚Äù in which; (iii) incumbents are allowed to reposition
characteristics of the products they market.


2        Static Models.
The empirical methodology for the static analysis typically relied on earlier
work by our game theory colleagues for the analytic frameworks we used. The
assumptions we took from our theory colleagues included the following:
    ‚Ä¢ Each agent‚Äôs actions affect all agents‚Äô payoffs, and
    ‚Ä¢ At the ‚Äúequilibrium‚Äù or ‚Äúrest point‚Äù
      (i) agents have ‚Äúconsistent‚Äù perceptions1 , and
      (ii) each agent does the best they can conditional on their perceptions of
      competitors‚Äô and nature‚Äôs behavior.
Our contribution was the development of an ability to incorporate heterogeneity
into the analysis and then adapt the framework to the richness of different real
world institutions.
    The heterogeneity appeared in different forms depending on the issue being
analyzed. When analyzing production efficiency the heterogeneity was needed
to rationalize the persistent differences in plant (or firm) level productivity that
emerged from the growing number of data sets with plant (or firm) level data
on production inputs and outputs. The productivity differences generate both
    1
        Though the form in which the consistency condition was required to hold differed across applications.



                                                           4
a simultaneity problem resulting from input choices being related to the (un-
observed) productivity term and a selection problem as firms who exit are dis-
proportionately those with lower productivity. However once these problems
are accounted for, the new data sets enable an analysis of changes in the envi-
ronment (e.g; a merger) on: (i) changes in the efficiency of the output alloca-
tion among firms, and (ii) the changes in productivities of individual producing
units (Olley and Pakes, 1996). In demand analysis, allowing for observed and
unobserved heterogeneity in both product characteristics and in consumer pref-
erences enabled us to adapt McFadden‚Äôs (1974) seminal random utility model to
problems that arise in the analysis of market demand. In particular allowing for
unobserved product characteristics enabled us to account for price endogene-
ity in a model with rich distributions of unobserved consumer characteristics.
These rich distributions enabled us to both mimic actual choice patterns in the
emerging data sets on individual (or household) choices and to seamlessly com-
bine that data with aggregate data on market shares prices and distributional
information on household attributes (Berry, Levinsohn and Pakes; 1995, 2004).
    A major role of equilibrium conditions was to enable an analysis of counter-
factuals (e.g.; proposed mergers, regulatory or tax changes, ....). Few believed
that a market‚Äôs response to a change in environmental conditions was to imme-
diately ‚Äújump‚Äù to a new equilibrium. On the other hand most also believed that
the agents would keep on changing their choice variables until an equilibrium
condition of some form was reached; i.e. until each agent was doing the best
they could given the actions of the other agents. So the Nash condition was
viewed as a ‚Äúrest point‚Äù to a model of interacting agents. Moreover at least
without a generally accepted model of how firms learn and adapt to changes in
the environment, it was natural to analyze the counterfactual in terms of such a
rest point2 .

2.1     Demand Models and Nash Pricing Behavior.
This material is reasonably well known so I will be brief. The advances made
in demand analysis were focused on models where products were described by
their characteristics. I start with Berry Levinsohn and Pakes (1995; henceforth
   2
     The equilibrium conditions were also used to: (i) compare the fit of various behavioral models for setting
prices of quantities (e.g.; Nevo 2001), and to (ii) generate additional constraints on the relationship between the
primitives and control variables that were often a great help in estimation (e.g. Berry Levinsohn and Pakes, 1995).



                                                        5
BLP) model for the utility of agent i would obtain from product j or
                            X
                    Ui,j =      Œ≤i,k xj,k + Œ±i pj + Œæj + i,j .
                                 k

Here the xj,k and the Œæj are product characteristics. The difference between
them is that the xj,k are observed by the analyst and the Œæj are not. The Œ≤i,k and
Œ±i are household characteristics and the fact that they are indexed by i allows
different households to value the same characteristic differently.
   Each individual (or household) is assumed to chose the product (the j) that
maximized its utility (its Ui,j (¬∑)), and aggregate demand was obtained by sim-
ply summing over the choices of the individuals in the market (a tactic made
possible by the speed of computers and the advent of simulation estimators; see
McFadden, 1989, and Pakes and Pollard, 1989). The main advantages of this
framework are that
   ‚Ä¢ Characteristic space enables us to overcome the ‚Äùtoo-many parameters‚Äù
     problem in models where the demand for a given product depended on
     the prices of all products marketed (their if J is the number of products,
     the number of price coefficients needed grows like J 2 ). In characteristic
     based models the number of parameters estimated grows with the number
     of characteristics.
   ‚Ä¢ The {Œæj } account for unobserved product characteristics and enable an
     analysis which does not need to condition on all characteristics that matter
     to the consumer.
   ‚Ä¢ The {Œ≤i,k } and {Œ±i } enable us to generate own and cross price elasticities
     that depend on the similarity of the characteristics and prices of different
     products. They can be made a function of income and other observed
     characteristics of the consuming unit as well as an unobserved determinant
     of tastes.
   Assume we are studying consumer demand and pricing behavior in a retail
market, and for simplicity consider a constant marginal cost firm which has
only one product to sell in this market. It sets its prices to maximize its profits
from the sale of this product. Profits are given by p1 q1 (¬∑)(p1 , . . . , pJ ) ‚àí mc1 q1 (¬∑)
where q1 (¬∑) is the quantity demanded (which depends on the prices and char-
acteristics of all the competing firms) and mc1 is its marginal cost. As is well

                                             6
known this implies the price setting equation
                                     1                                     1
           p1 = mc1 +                         = x 1 Œ≤ + w 1 Œ≥ + œâ1 +                ,                             (1)
                               [‚àÇq1 /‚àÇp1 ]/q1                        [‚àÇq1 /‚àÇp1 ]/q1
where mc1 = x1 Œ≤ + w1 Œ≥ + œâ1 , x1 is the characteristics of the product, w1
observed market wage rates, and œâ1 is unobserved ‚Äùproductivity‚Äù3 .
   There are a couple of implications of equation (4) that should be kept in
mind. The implication of this equation that has been used extensively in the
relevant literature is that if one believes the pricing model the pricing equation
can be used to back out an estimate of marginal cost4 . The second implication,
and the one I want to focus on here, stems from the fact that the markup above
                                     1
costs in equation (4), that is [‚àÇq1 /‚àÇp1 ]/q1
                                              , can be calculated directly from the de-
mand estimates (we do not need to estimate a pricing equation to get it). The
twin facts that we can get an independent estimate of the markup and that the
pricing theory implies that this markup has a coefficient of one in the pricing
equation enables a test of the pricing model. In studies where reasonably pre-
cise estimates of the demand system are available without invoking the pricing
equation, this test should give the researcher an idea of whether it is appropriate
to use the Nash pricing equation as a model of behavior.
   To illustrate I asked Tom Wollman to use the data underlying his thesis on the
commercial truck market to do the following: (i) use his demand estimates to
construct the markup term above, (ii) regress the estimated markup on ‚Äùinstru-
ments‚Äù to obtain a predicted markup which is not correlated with the product
and cost specific unobservables (Œæ and œâ)5 , and (iii) regress the observed price
on the observed cost determinants and this predicted markup.
   A summary of his results is reported in table one. There are two major
points to note. First, the fit of the equation is extraordinary for a behavioral
model in economics (that is we are fitting a variable which is chosen by the
firm). Second the coefficient of the markup is very close to one (it is within
one standard deviation of one), indicating that the Nash in prices assumption
    3
      The equilibrium pricing equation has an extra term in models with adverse selection, as is frequently the case
in financial and insurance markets. Adverse (or beneficial) selection causes costs to be a function of price and
so will cause a derivative of costs with respect to price to enter the price setting equation; see Einav, Jenkins and
Levin, 2012.
    4
      Of course, if other sources of cost data are available, and the competition authorities can sometimes requisition
internal firm reports on costs, they should be incorporated into the analysis.
    5
      The instruments we used were those used by BLP (1995). For a discussion of the performance of alternative
instruments see Reynaert and Verboven, 2014


                                                          7
                            Table 1: Fit of Pricing Equilibrium.
                                        Price (S.E.) Price (S.E.)
                      Gross Weight        .36 (0.01)       .36 (.003)
                      Cab-over            .13 (0.01)       .13 (0.01)
                      Compact front      -.19 (0.04) 0.21 (0.03)
                      long cab           -.01 (0.04) 0.03 (0.03)
                      Wage                .08 (.003) 0.08 (.003)
                          ÀÜ
                      M arkup             .92 (0.31) 1.12 (0.22)
                      Time dummies? No            n.r.    Yes    n.r.
                        2
                      R                  0.86     n.r.    0.94   n.r.

Note. There are 1,777 observations; 16 firms over the period 1992-2012. S.E.=Standard error.

can not be rejected by the data. The fact that the data seem to not be at odds
with the Nash in prices assumption is not unusual. For example Nevo (2001),
who has independent measures of marginal cost which allow him to compare a
direct measure of markups to those predicted by the model, finds that the Nash
pricing assumption fits the data remarkably well.
    It is also worth noting that level shifts in demand over time that have a com-
mon effect on all firms‚Äô prices account for about 60% of the unexplained vari-
ance in prices in the panel. This latter point is indicative of the fact that our
pricing models do not do as well in fitting shifts in prices over time as they do
in explaining the cross-sectional differences in prices. For example, here the R2
for the cross sectional variance is between .85 and .95 whereas when we used
the same data to look at price changes of a given product over time we got an
R2 of .5 to .6. It is important to point out that even the time series R2 ‚Äôs are
quite high for a behavioral model. Moreover explaining the variance over time
is a very direct test of the power of the pricing equation as all of the explained
variance accounted for in the time series dimension is because of differences in
markups caused by differences in the products competing with the given product
in two adjacent periods (none is due to differences in the characteristics of the
product per se). Still the fact that there is unexplained variance indicates there
is room for more detailed models of how consumers respond to price changes
(e.g. Hendel et. al.,2014) and the way producers respond to changes in their
environment.




                                             8
Horizontal Mergers and the Unilateral Effects Model.   The unilateral effects price
for a single product firm with constant marginal cost is given by the Nash pricing
equilibrium formalized in equation (4) above. The intuition behind that equation
is the following: if the firm increases its price by a dollar it gets an extra dollar
from the consumers who keep purchasing the firm‚Äôs product, but it loses the
markup from the consumers who switch out of purchasing the good because
of the price increase. The firm keeps increasing its price until the two forces‚Äô
contribution to profits negate each other.
    When firm 1 merges with another single product firm, our firm 2, this logic
needs to be modified. Now if the firm increases its price it still gets the extra
dollar from those who stay, but it no longer loses the markup from every con-
sumer who switches out. This because some of the consumers who leave go
to what used to be firm 2‚Äôs product, and the merged firm captures the markup
from those who switch to that product. This causes price to increase beyond
the price charged by firm 1 when it only marketed one product. The extent of
increase depends on the fraction of the customer‚Äôs who switch out of firm 1‚Äôs
product that switch to what was firm 2‚Äôs product, and the markup on the second
product. Formally the post merger price (pm ) for firm 2 is written as
                                              1                      ‚àÇq2 ‚àÇq1
                   pm
                    1 = mc1 +                          + (pm
                                                           2 ‚àí mc2 )    /    .                                  (2)
                                        [‚àÇq1 /‚àÇp1 ]/q1               ‚àÇp1 ‚àÇp1
   The difference between the post merger price in equation (5), our pm , and the
pre merger price in (4), our p, has been labelled the ‚Äùupward pricing pressure‚Äù
or ‚ÄùUPP‚Äù (see DOJ and FTC, 2010; and Farrell and Shapiro, 1990). If we let the
merger create efficiency gains of E1 % in marginal costs we have the following
approximation6
                                                                    ‚àÇq2 ‚àÇq1
                   U P P 1 ‚âà pm
                              1 ‚àí p1 = (p2 ‚àí mc2 )                     /    ‚àí E1 mc1 .
                                                                    ‚àÇp1 ‚àÇp1
      ‚àÇq2 ‚àÇq1
Here ‚àÇp  /
        1 ‚àÇp1
               is the fraction of consumers who leave the good owned by firm 1
that actually switch to firm 2 and is often called the ‚Äùdiversion ration‚Äù.
   The concept of ‚ÄùUPP‚Äù and diversion ratios in merger analysis, like many
other ideas taken from Industrial Organization into antitrust analysis, has been
in graduate class notes for many years. Partly as a result we have had time to
   6
    This is an approximation because the merger will cause a change also in the price of the second good which
should be factored into an equilibrium analysis. This modifies the formulae (see Pakes, 2011), but not the intuition.

                                                         9
consider both when it is appropriate, and the data requirements needed to use
it. A full discussion of these issues is beyond what I wanted to do here7 , but
I did want to stress that the UPP analysis is not always appropriate. This is
rather obvious when co-ordinated (in contrast to unilateral) behavior is a pos-
sibility, so I am going to focus on two more subtle features of the environment
which, when present, are likely to make UPP analysis inaccurate. I focus on
these two because a more telling analysis of both of them is now possible. The
first, horizontal mergers in vertical markets in which there are a small number of
buyers and sellers, is discussed in this section. The second, mergers when prod-
uct repositioning is likely, is discussed in section 3 which is devoted to product
repositioning.
    The basic difference between markets with a small number of buyers and
sellers (a buyer-seller network) and retail markets is that in a retail market we
generally assume that buyers only options are to buy or not buy at the current
price. In a buyer-seller network the buyer can make a counteroffer to any given
price; i.e. the buyer can decline to buy at price p but offer to buy at a lower
price. Negotiations ensue and if a contract is formed it determines how the
profits from the relationship are split8 . There is no general agreement on the
appropriate equilibrium concept for these markets, though there is agreement
that each agents‚Äô ‚Äùoutside option‚Äù (i.e. the profits it would earn were it not to
enter into an agreement) will be a determinant of both which relationships are
established and the split of the profits among those that are. The implication
of this that is not obvious from the UPP formula (and partly as a result is often
mistakenly ignored) is that any analysis of a change in the upstream market
must take account of the structure of the downstream market and visa versa.
    The empirical literature has found it most convenient to use Horn and Wolin-
sky‚Äôs (1988) ‚ÄùNash in Nash‚Äù assumptions as a basis for the analysis of these sit-
uations. In that framework negotiations are bilateral; if two agents contract the
split of the profits between them is determined by Nash bargaining, and a Nash
condition is imposed on who contracts with whom. The Nash condition is that
   7
      My own discussion of these issues can be found at http://scholar.harvard.edu/pakes/presentations/comments-
onupward-pricing-pressure-new-merger-guidelines;.
    8
      There has been a good deal of empirical work recently on other markets where a Nash in price (or quantity)
assumption that underlies the UPP analysis is inappropriate.Two others, are markets with repeated auctions such
as those often used by procurement agencies (see Porter, 2015, for an extended discussion). and markets where
there is a centralized matching mechanism, such as the medical match (see Agarwal, 2015, for a start at analyzing
market structure issues in these markets).



                                                       10
if an agent contracts with another it must earn more profits from the situation
generated by the contract then it would absent the contract, and visa versa for at
least one agent were the agents not to contract. One questionable assumption is
that the deviations are evaluated under the assumption that no other changes in
the contracting situation would occur in response to the deviation.
    The paper that laid the groundwork for empirical work in this area is Craw-
ford and Yorukuglu (2013), who analyzed the likely impact of forced de-bundling
of the tiers set by Cable TV providers. The estimate of the resulting change in
consumer welfare differed dramatically by whether the upstream market (the
market between the content providers and the cable networks) was allowed to
adjust to the new downstream situation (and many content providers would not
survive if no realignment of upstream contracts was allowed). Interestingly a
variant of their model was also used by the FCC to analyze the likely impacts
of the Comcast-NBCU merger9 . Since that time related frameworks have been
used to analyze the likely impacts of hospital mergers (Gowrisankaran, G., A.
Nevo, and R. Town, 2015), whose impacts on insurance premiums would de-
pend on the nature of the downstream premium setting game between insurers,
and health insurer mergers (Ho, K. and R. Lee, 2015) whose impact on hospital
prices depended on the nature of the upstream market between insurers and hos-
pitals10 . The current structural changes in the nature of health care provision in
the United States make this sort of analysis both timely, and economically im-
portant.

2.2     Firm and Industry Productivity.
    Partly due to various public agencies providing access to Census like files,
there has been a marked increase in the availability of data on the inputs and
outputs of production units (firms and/or plants). The advantage over industry
level data on inputs and outputs is that the micro data has opened up the possi-
bility of separating the analysis of the; (i) sources of productivity increases (or
cost decreases) in individual plants, from (ii) the role of inter-firm output allo-
cations, or market structure, in determining industry productivity. So we now
are better able to analyze such traditional IO questions as; the efficiency of the
   9
    See https : //apps.f cc.gov/edocsp ublic/attachmatch/F CC ‚àí 11 ‚àí 4A1.pdf .
  10
    Interestingly, at the time I am writing, the framework set out in this paper being used to analyze mergers in the
U.S. health insurance industry.



                                                        11
output allocation, the presence of economies of scale and/or scope, the role of
market structure in inducing competitive efficiencies11 , spillovers from inven-
tive activity, and the effect of infrastructure on the productivity of individual
firms.
   Productivity is defined as a measure of output divided by an index of inputs.
The input index is typically associated with a production function. Among the
simplest such functions used is the Cobb-Douglas, written as

                            yi,t = Œ≤0 + Œ≤l li,t + Œ≤k ki,t + Œ≤m mi,t + œâi,t ,         (3)

where yi,t is the logarithm of the output measure for firm i in period t, and
ki,t , mi,t and li,t are the logarithms of the labor, capital, and materials inputs
respectively, while œâi,t is the unobserved productivity term. So if we let upper
case letters be antilogs of their lower case counterparts and B = exp[Œ≤0 ], then
productivity would be measured as
                                                             Yi,t
                                         pi,t =                         ,
                                                   BLŒ≤i,tl Ki,t
                                                            Œ≤k   Œ≤M
                                                                Mi,t
or output per unit of the input index.
    To construct the productivity measures one needs estimates of the Œ≤ param-
eters. There are two well known problems in obtaining those estimates from
equation (6). First the input measures are at least partially under the control of
firms, and if input prices are approximately the same across firms (or at least not
negatively correlated with productivity), it is in the interest of more productive
firms to use larger quantities of those inputs. This generates a positive cor-
relation between input quantities and the unobserved productivity term which
generates the estimation problem often referred to as ‚Äùsimultanaeity‚Äù.
    Second, basic economics tells us that higher productivity firms and firms
with more capital should be more highly valued and, at least absent a perfect
market for those inputs, be less likely to exit. As a result highly capitalized
firms who exit tend to have disproportionately low productivity and poorly cap-
italized firms who continue tend to have disproportionately high productivity.
This ‚Äùnatural selection‚Äù process will; (i) cause a correlation between the pro-
ductivities and capital of continuing firms, and (ii) will generate a bias in our
estimates of the productivity effects of any given environmental change. The
 11
      For a review of the empirical findings on this, see Holmes and Schmitz, 2010


                                                        12
bias in our measure of productivity changes results from firms whose produc-
tivity was negatively impacted by the change exit and, as a result, the impact of
the environmental change on their productivity is not measured. The selection
effects are often particularly large when, as is often the case, we are investigat-
ing the effects of a major environmental change (e.g., the impacts of a major
regulatory change).
    Olley and Pakes (1996) show how use of the implications of equilibrium
behavior, both for the choice of inputs and for the decisions of when to exit, can
be used by the researcher to circumvent the resulting estimation problems12 .
The implementation is made significantly easier and less subject to particular
assumptions through use of advances in semi-parameteric estimation techniques
(see Powell, 1994, and the literature cited their). Olley and Pakes (1996) also
propose an intuitive way of decomposing industry wide productivity, defined as
the share weighted average of firm level productivity. Industry productivity (say
Pt ) is equal to the sum of (i) a term measuring the efficiency of the allocation
of output among firms, and (ii) the average of individual firm productivities.
Formally                   X                     X
                      Pt =      pi,t si,t = pt +   ‚àÜpi,t ‚àÜsi,t ,
                                       i                          i
where: pt is the (unweighted) average productivity, ‚àÜpi,t = pi,t ‚àípt , and ‚àÜsi,t is
the firm‚Äôs share minus the average share. So the term measuring the efficiency
of the output allocation is the covariance between share and productivity across
firms in the industry (more precisely the numerator of that covariance)13 .
    The empirical content of the Olley and Pakes (1996) article is also of some
interest to the study of antitrust. They analyze the evolution of productivity
in the telecommunication equipment industry before and after the breakup of
A.T.&T and the divestment of its wholly owned equipment subsidiary, Western
Electric (the consent decree was filed in 1982 and the breakup and divestment
were complete by 1984). They find industry wide productivity growth between
3 to 4% during the period from 1978 to 1987 (this includes a large fall in pro-
ductivity during an adjustment period of 1981-83). Perhaps more important to
regulatory economics, the industry‚Äôs productivity increases, which were accel-
  12
     For a detailed treatment of this estimation procedure, and the improvements that have been made to it, see the
section 2 of Ackerberg, Benkard, Berry, and Pakes (2007).
  13
     For a dynamic version of this decomposition which explicitly considers the contributions of entrants and
exitors in change in Pt over time see Melitz and Polanec, 2015.



                                                       13
erating at the end of the sample period, were: (i) almost entirely caused by the
covariance term in the above formula and, (ii) could largely be attributed to the
reallocation of capital to more productive firms (often as a result of the exit of
unproductive firms). That is the entry and selection effects of the increased com-
petition that resulted from deregulation accounted for most of the productivity
effects that occurred just after deregulation.
    The production data from most data sets contains information from multi-
product producing units, a fact which underlies the widespread use of sales rev-
enue as the output measure in the productivity analysis (indeed this led Olley
and Pakes to relabel the ‚Äùproduction function‚Äù in equation (6) a ‚Äùsales gener-
ating function‚Äù). Though it is of interest to know how environmental changes
effect sales per unit of inputs, it would be of more interest to understand how
those changes effect price and quantity separately. However to do this we would
need to either know or be able to construct separate quantity and price measures
for the different outputs produced by the firm. Jan De Loecker and his coau-
thors have used different methods to separate revenue growth into its quantity
and price components, and then separately analyze the; (i) growth in markups
and, (ii) growth in the productivity of inputs in decreasing costs (or an approxi-
mations thereof). In particular De Loecker and Warzynski (2012) use the input
equilibrium conditions for a perfectly variable factor of production in a price
taking input market to add a step to the estimation procedure that allows them
to separate changes in revenue into: (i) changes in markups, and (ii) productiv-
ity related changes in costs.
    A good example of the usefulness of this type of analysis appears in a sub-
sequent paper by De Loecker, J., P. Goldberg, A. Khandelwal and N. Pavcnik,
(forthcoming). They study the impacts of a trade liberalization in India on pro-
ductivity prices and costs (the liberalization began in 1991, and they study data
from 1989 and 1997). There have been a number of prior analysis of the pro-
ductivity impacts of trade liberalization but most used a framework in which
markups were not allowed to change. The new analysis allowed firm specific
markups, and analyzed how the markups varied after the tariff change. The
tariff declines lowered both input and output prices. The input price declines
resulted in disproportionate increases in markups, rather than to reductions in
consumer prices; i.e. there was limited pass through of the fall in producer costs
to consumer prices (at least during the time period they analyze). The markup


                                       14
increase did induce entry and an increase in the variety of goods available in
the market. An analysis of why this occurred would undoubtedly require more
institutional information on the industries studied and more detailed analysis of
the equilibrium generating the pass through they find. However their study is
clearly an important first step in our understanding of how a major change in
the economic environment impacted a large share of the world‚Äôs population.


3        Product Repositioning.
In some industries incumbents can ‚Äùreposition‚Äù their products as fast, or almost
as fast, as prices can be changed. This section considers two period models that
allow us to analyze product repositioning14 . The most dramatic illustration of
the ease of product repositioning that I am aware of is contained in a series of
figures in Chris Nosko‚Äôs thesis (see Nosko 2014)15 .
   Nosko analyzes the response of the market for CPU‚Äôs in desktop computers
to Intel‚Äôs introduction of the Core 2 Duo generation of chips. Two firms dom-
inate this market: Intel and AMD. Nosko explains that it is relatively easy to
change chip performance provided it is lower than the best performance from
the current generation of chips. Indeed he shows that chips sold at a given price
typically change their characteristics about as often as price changes on a given
set of characteristics.
   The first figure provides benchmark scores and prices for the products of-
fered in June 2006, just prior to the introduction of the Core 2 Duo. The red and
blue dots represent AMD‚Äôs and Intel‚Äôs offerings, respectively. Note that in June
there was intense competition for high performance chips with AMD selling the
highest priced product at just over $1000. Seven chips sold at prices between
$1000 and $600, and another five between $600 and $400. July saw the intro-
duction of the Core 2 Duo and figure 2 shows that by October; (i) AMD no
    14
      I want to stress that two period models for the analysis of product repositioning in situations where it is
relatively easy to reposition products is distinct from two period models for the analysis of firm entry. Two period
entry models, as initially set out in Bresnahan and Reiss, 1988, have been incredibly useful as a reduced form
way to investigate the determinants of the number of active firms in a market (for more details on the analysis and
interpretation of the results from these models see Pakes, 2014). However, at least in my view, the fact that their
is no history before the first period or future after the second in these models makes its predictions for the impact
of an environmental change unreliable for the time horizon relevant for antitrust enforcement. The framework for
product repositioning presented here conditions on history and is designed specifically to analyze how the industry
is likely to respond to such changes.
   15
      I thank him for allowing me to reproduce two of them


                                                        15
longer markets any high performance chips (their highest price chip in October
is just over two hundred dollars), and (ii) there are no chips offered between
$1000 and $600 dollars and only two between $600 and $400 dollars. Shortly
thereafter Intel replaces the non-Core 2 Duo chips with Core 2 Duo‚Äôs.
    Nosko goes on to explain how the returns from the research that went into
the Core 2 Duo came primarily from the markups Intel was able to earn as a
result of emptying out the space of middle priced chips and dominating the high
priced end of the spectrum. He also shows, through a counterfactual merger
simulation, that a similar phenomena would likely occur if AMD were to merge
with Intel.
    Other recent articles that employ an analysis of product repositioning include
Eizenberg (2014) and Wollman (2015). Eizenberg studies the introduction of
the Pentium 4 chip in PC‚Äôs and notebooks. He focuses on the decisions to
stop the production of products with older chips (and lower prices), a decision
relative easy to implement almost immediately, and the implications of those
decisions on consumer welfare. He finds distributional effects to withdrawing
goods; though overall welfare (the sum or producer and consumer surplus) is
likely higher with the withdrawals poorer consumers are better off without them.
Wollman (2015) considers the bailout of GM and Chrysler during the recent
financial crisis, and asks what would have happened had GM and Chrysler not
been bailed out, but rather exited the commercial truck market. He notes that
part of the commercial truck production process is modular (it is possible to
connect different cab types to different trailers), so some product repositioning
would have been almost immediate. In addition to ‚Äùpure exit‚Äù he considers the
possibility of mergers between the ‚Äùexiting‚Äù truck makers and the remaining
incumbents, and he calculates all his counterfactuals twice; once just allowing
the prices of the products offered by the participants who remained after the
counterfactual to adjust, and once allowing their product offerings as well as
their prices to adjust. Allowing for changes in characteristics had policy relevant
differences on the results.
    The analytic framework used in these papers is that of a two period model,
with product offerings set in the first stage and prices set in the second. Detailed
structural models, in the spirit of the models referred to above, are used to esti-
mate demand and cost, and a Nash pricing equilibrium is used to set prices when
needed. This provides the structure used in the second stage which needs to cal-


                                        16
Price/Performance ‚Äì June 2006




Price/Performance ‚Äì July 2006
  Price/Performance ‚Äì Oct 2006




Price/Performance ‚Äì January 2008
culate profits for different sets of products. The analysis of the first stage, i.e.
the calculation of whether to add (or delete) different products, requires an esti-
mate of an additional parameter: the fixed costs of adding or dropping a product
from the market. It is estimated using the profit inequality approach proposed
in Pakes, Porter, Ho and Ishii (2015; henceforth PPHI) and Pakes (2010).
    The profit inequality approach is analogous to the revealed preference ap-
proach to demand analysis when the choice set is discrerte. To understand how
it works assume the fixed costs are the same for adding any product., and let
xj = [1, 0]L where L is the number of products that firm j could offer and
the vector xj has a 1 for products offered and a 0 otherwise. Let ez be an L-
dimensional vector with one in the ‚Äùz‚Äù spot and zero elsewhere, and assume that
the z th product was just added. We use the framework above to compute both
the actual profits and the implied profits had the product not been added16 . Let
(œÄj (xj , x‚àíj ), œÄj (xj ‚àí ez , x‚àíj )), be the profits with and without marketing the
z th product, Ij be the agent‚Äôs information set, and E[¬∑|I| ] deliver expectations
conditional on Ij . The behavioral assumption is that z was added because

                           E[œÄj (xj , x‚àíj ) ‚àí œÄj (xj ‚àí ez , x‚àíj )|Ij ] ‚â• F,

where F is the fixed cost of adding a product. If we average over all the products
introduced and assume expectations are unbiased, we get a consistent lower
bound for F . On the other hand if the z th product is a feasible addition that was
not offered, and (œÄj (xj , x‚àíj ), œÄj (xj + ez , x‚àíj ) were the profits without and with
that product, then

                          E[œÄj (xj + ez , x‚àíj ) ‚àí œÄj (xj , x‚àíj ), |Ij ] ‚â§ F,

which gives us an upper bound to F .
   Notice that it is the power of the structural approach to make out of sample
predictions, the same aspect of that approach used in the analysis of mergers,
that lets us estimate fixed costs. One feature of this bounds approach to esti-
mating fixed costs that makes it attractive in the current context is that it allows
for measurement error in profits, and this provides partial protection from devi-
ations from the assumptions used in constructing the needed counterfactuals17 .
  16
      This would have been a unilateral deviation in the first stage simultaneous move game, and hence not changed
the products marketed by other firms.
   17
      This distinguishes the econometric properties of the current mode of analysis from those of the prior literature
on two period entry models; see Pakes, 2014, for more detail.


                                                         17
Still there are a number of issues that can arise. First we may want to allow for
differential fixed costs across products or locations. A straightforward general-
ization of what I just described can allow fixed costs to differ as a function of
observable variables (like product characteristics)18 . Things do get more com-
plicated when one allows for unobservable factors that generate differences in
fixed costs and were known to the agents when they made their product choice
decision. This because the products that are provided are likely to have been
partially selected on the basis of having unobservable fixed costs that were lower
than average, and those that are not may have been partially selected for having
higher than average fixed costs. Possible ways for correcting for any bias in the
bounds that selection generates are provided in PPHI and in Manski (2003); see
Eizenberg (2014) for an application.
   One might also worry that the two period model misses the dynamic aspects
of marketing decisions that would arise if F represented sunk, rather than fixed,
costs. A more complete analysis of this situation would require the sequential
Markov equilibrium discussed in the next section. However there is a way to
partially circumvent the complexity of Markov equilibria. If it was feasible
to market z but it was not offered and we are willing to assume the firm can
credibly commit to withdrawing it in the next period before competitors next
period decisions are taken, then we can still get an upper bound. The upper
bound is now to the cost of marketing and then withdrawing the product. I.e.
our behavioral assumption now imply that the difference in value between (i)
adding a product not added and then withdrawing it in the next period and (ii)
marketing the products actually marketed, is less than zero. This implies
                     E[œÄj (xj + ez , x‚àíj ) ‚àí œÄj (xj , x‚àíj )|Ij ] ‚â§ F + Œ≤W.
where W ‚â• 0 is the cost of withdrawing and Œ≤ be the discount rate. Lower
bounds require further assumptions, but the upper bound ought to be enough for
examining extremely profitable repositioning moves following environmental
changes (like those discussed in Nosko (2011).
   Given these bounds we turn to considering likely product repositioning de-
cisions that follow an environmental change. Typically if we allow the product
  18
     Though this does require use of estimation procedures from the literature on moment inequality estimators.
For an explanation of how these estimators work see Tamer (2010). When there are many parameters that need
to be estimated in the fixed cost equation these estimators can become computationally demanding, but their is an
active econometric literature developing techniques which reduce this computational burden, see for e.g. Kaido et.
al. 2014.

                                                       18
mix of incumbents to adjust as a result of say a merger or an exit there will
be a number of market structures that are Nash equilibria in the counterfactual
environment (though one should not forget that the number of equilibria will be
limited by profitability implications of the investments in place).
    There are different ways to analyze counterfactuals when there is the pos-
sibility of multiple equilibria. One is to add a model for how firm‚Äôs adjust to
changes in their environment and let the adjustment procedure chose the equi-
libria (for examples see Lee and Pakes, 2009, and Wollman, 2015). Alterna-
tively we could assume that firms‚Äô assume that their competitors do not change
the products they market in the relevant time horizon, and find a best response
to those products given the changed environment19 . Another possibility is enu-
meration of the possible equilibria (or at least those that seem likely on the basis
of some exogenous criteria), and consider properties of all members of that set,
as is done in Eizenberg (2014).


4     Dynamics and the Evolution of State Variables.
Empirical work on dynamic models proceeded in a similar way to the way we
proceeded in static analysis; we took the analytic framework from our theory
colleagues and tried to incorporate the institutions that seemed necessary to
analyze actual markets. We focused on models where
    1. state variables evolve as a Markov process,
    2. and the equilibrium was some form of Markov Perfection (no agent has an
       incentive to deviate at any value of the state variables).
In these models firms chose ‚Äùdynamic controls‚Äù (investments of different types)
and these determine the likely evolution of their state variables. Implicit in
the second condition above is that players‚Äô have perceptions of the controls‚Äô
likely impact on the evolution of the state variables (their own and those of their
competitors) and through that on their current and future profits, and that these
perceptions are consistent with actual behavior (by nature, as well as by their
  19
     We could also iterate on this for all competitors to find a rest point to ‚Äùiterated‚Äù best responses. This is one
way to model the adjustment process. For a theoretical treatment of alternatives see Fudenberg and Levine, 1998,
and for an attempt to distinguish between learning algorithms actually used in an industry, see Doraszelski, Lewis,
and Pakes, 2015. It is worth mentioning that in a world where there is randomness in the environment, adjustment
processes can lead to a probability distribution of possible equilibria.


                                                        19
competitors). The standard references here are Maskin and Tirole (1988a and
b) for the equilibrium notion and Ericson and Pakes (1995) for the framework
used in computational theory and empirical work.
    The Markov assumption is both convenient and, except in situations involv-
ing active experimentation and learning, fits the data well20 , so empirical work
is likely to stick with it. The type of rationality built into Markov Perfection
is more questionable. It has been useful in correcting for dynamic phenomena
in empirical problems which do not require a full specification of the dynamic
equilibrium (as in the Olley and Pakes example above),. The Markov Perfect
framework also enabled applied theorists to explore possible dynamic outcomes
in a structured way. This was part of the goal of the original Maskin and Ti-
role articles and computational advances have enabled Marko Perfection to be
used in more detailed computational analysis of a number of other issues that
were both, integral to antitrust analysis, and had not been possible before21 .
Examples include; the relationship of collusion to consumer welfare when we
endogenize investment entry and exit decisions (as well as price, see Fershtman
and Pakes, 2000), understanding the multiplicity of possible equilibria in mod-
els with learning by doing (Besanko, Doraszelski, Kryukov and Satterthwaite,
2010), and dynamic market responses to merger policy (Mermelstein, Nocke,
Satterthwaite, and Whinston 2014).
    Perhaps not surprisingly, the applied theory indicated just how many differ-
ent outcomes were possible in Markov perfect models, especially if one was
willing to perturb the functional forms or the timing assumptions in those mod-
els. This provided additional incentives for empirical work, but though there
has been some compelling dynamic empirical work using the Markov Perfect
framework (see for e.g.; Benkard, 2004, Collard-Wexler, 2013, and Kaloupt-
sidi, 2014)22 , it has not been as forthcoming as one might have hoped. This is
because the framework becomes unweildly when confronted with the task of
incorporating the institutional background that seemed relevant.
    When we try to incorporate the institutional background that seems essential
  20
      It also does not impose unrealistic data access and retention conditions for decision making agents.
  21
      The computational advances enabled us to compute equilibria quicker and/or with less memory requirements
then in the simple iterative procedure used in the original Pakes and McGuire (1994) article. The advances in-
clude; Judd‚Äôs (1998) use of deterministic approximation techniques, Pakes and McGuire (2001)‚Äôs use of stochastic
approximation, and Doraszelski and Judd (2011)‚Äôs use of continuous time to simplify computation of continuation
values.
   22
      A review by Doraszelski and Pakes, 2000 provides a more complete list of cites to that date.



                                                      20
to understanding the dynamics of the market we often find that both the analyst,
and the agents we are trying to model, are required to: (i) access a large amount
of information (all state variables), and (ii) either compute or learn an unrealistic
number of strategies. To see just how complicated the dynamics can become,
consider a symmetric information Markov Perfect equilibrium where demand
has a forward looking component; as it would if we were studying a durable,
storable, experience, or network good.
    For specificity consider the market for a durable good. Both consumers and
producers would hold in memory at the very least; (i) the cartesian product of
the current distribution of holdings of the good across households crossed with
household characteristics, and (ii) each firm‚Äôs cost functions (one for producing
existing products and one for the development of new products). Consumers
would hold this information in memory, form a perception of the likely prod-
uct characteristics and prices of future offerings, and compute a dynamic pro-
gramme to determine their choices. Firms would use the same state variables,
take consumers decisions as given, and compute their equilibrium pricing and
product development strategies. Since these strategies would not generally be
consistent with the perceptions that determined the consumers‚Äô decisions, the
strategies would then have to be communicated back to consumers who would
then have to recompute their dynamic program using the updated firm strate-
gies. This process would need to be repeated until we found strategies where
consumers do the best they can given correct perceptions of what producers
would do and producers do the best they can given correct perceptions on what
each consumer would do (a doubly nested fixed point calculation). Allowing for
asymmetric information could reduce information requirements, but it would
substantially increase the burden of computing optimal strategies. The addi-
tional burden results from the need to compute posteriors, as well as optimal
policies; and the requirement that they be consistent with one another.
    There have been a number of attempts to circumvent the computational com-
plexity of dynamic problems by choosing judicious functional forms for prim-
itives and/or invoking computational approximations. They can be useful but
often have implications which are at odds with issues that we want to study23 .
  23
    Examples include Gowrisankaran and Rysman 2012, and Nevo and Rossi, 2008. A different approach is
taken in the papers on ‚Äùoblivious equilibrium‚Äù starting with Benkard et. al. (2012). This was introduced as a
computational approximation which leads to accurate predictions when there were a large number of firms in the
market, but I believe could be reinterpreted in a way that would make it consistent with the framework described
below.


                                                      21
I want to consider a different approach; an approach based on restricting what
we believe agents can do. It is true that our static models often endow an agent
with knowledge that they are unlikely to have and then consider the resultant
approximation to behavior to be adequate. However it is hard to believe that
in a dynamic situation like the one considered above, Markov perfection (or
Bayesian Markov Perfection) is as good an approximation to actual behavior
as we can come up with. So I want to consider notions of equilibria which
might both better approximate agents‚Äô behavior and enable empirical work on
a broader set of dynamic issues. This work is just beginning, so I will focus on
the concepts that underlie it and some indication of how far we have gotten.

4.1     Less Demanding Notions of Equilibria.
The framework I will focus on is ‚ÄùRestricted Experience Based Equilibrium‚Äù (or
REBE) as described in Fershtman and Pakes (2012). It is similar in spirit to the
notion of ‚ÄùSelf-confirming Equilibrium‚Äù introduced in Fudenberg and Levine
(1983). The major difference is that REBE uses a different ‚Äùstate space‚Äù, one
appropriate for dynamic games, and as a result has to consider a different set of
issues. There has also been a number of developments of related equilibrium
concepts by economic theorists (see, for example Battigalli, P. et. al, 2015) .
   A REBE equilibrium satisfies two conditions which seem natural for a ‚Äúrest
point‚Äù to a dynamical system. They are
   1. agents perceive that they are doing the best they can conditional on the
      information that they condition their actions on, and that
   2. if the information set that they condition on has been visited repeatedly,
      these perceptions are consistent with what they have observed in the past.
Notice that this does not assume that agents form their perceptions in any par-
ticular way; just that they are consistent with what they have observed in the
past at conditioning sets that are observed repeatedly24 .
   The state space consists of the information sets the agents playing the game
condition their actions on. The information set of firm i in period t is denoted
  24
     It might be reasonable to assume more than this, for example that agents know and/or explore properties of
outcomes of states not visited repeatedly, or to impose restrictions that are consistent with data on the industry of
interest. We come back to this below where we note that this type of information would help mitigate multiplicity
problems.


                                                        22
by Ji,t = {Œæt , œâi,t }, where Œæt is public information observed by all, and œâi,t is
private information. The private information is often information on production
costs or investment activity (and/or its outcomes), and to enhance our ability to
mimic data, is allowed to be serially correlated. The public information varies
with the structure of the market. It can contain publicly observed exogenous
processes (e.g. information on factor price and demand), past publicly observed
choices made by participants (e.g. past prices), and whatever has been revealed
over time on past values of œâ‚àíi,t .
    Firms chose their ‚Äùcontrols‚Äù as a function of Ji,t . Since we have allowed for
private information, these decisions need not be a function of all the variables
that determine the evolution of the market (it will not depend on the private
information of competitors). Relative to a symmetric information Markov equi-
librium, this reduces both what the agent needs to keeps track of, and the num-
ber of distinct policies the agent needs to form. The model also allows agents
to chose to ignore information that they have access to but think is less relevant
(we come back to how the empirical researcher determines Ji,t below).
    Typically potential entrants will chose whether to enter, and incumbents will
chose whether to remain active and if so their prices and investments (in capital,
R&D, advertising, . . .). These choices are made to maximize their perceptions
of the expected discounted value of future net cash flows, but their perceptions
need not be correct.
    The specification for the outcomes of the investment and pricing process,
and for the revelation of information, determines the next period‚Äôs information
set. For example assume costs are serially correlated and are private informa-
tion, and that prices are a function of costs and observed by all participants.
Then past price is a signal on current costs, and we would expect all prices in
period t to be components of Œæt+1 , the public information in period t + 1. If, in
addition, investment is not observed and it generates a probability distribution
for reductions in cost, then the realization of the cost variable is a component of
œâi,t+1 , the private information in period t + 1.
    Since agents choices and states are determined by their information sets, the
‚Äústate‚Äù of the industry, which we label as st , is determined by the collection of
information sets of the firms within it
                           st = {J1,t , . . . , Jnt ,t } ‚àà S.
Assumptions are made which insure that st evolves as a finite state Markov

                                          23
chain. This implies that, no matter the policies, st will wander into a recurrent
subset of the possible states (i.e. of S), and then remain within that subset
forever (Freedman, 1971). Call that subset R ‚äÇ S. These states are determined
by the primitives of the market (its size, feasible outcomes from investment,
....), and they are visited repeatedly. For example a small market may never see
more than x firms simultaneously active, but in a larger market the maximum
firms ever active may be y > x. Firms in the larger market may flounder and
eventually exit, but it is possible that before the number of active firms ever falls
to x there will be entry. Then the recurrent class for the larger market does not
include Ji,t that are a component of an st that has less than x firms active.
     The Ji,t which are the components of the st in R are the information sets at
which the equilibrium conditions require accurate perceptions of the returns to
feasible actions. So in industries that have been active for some time, neither
the agent nor the analyst needs to calculate equilibrium values and policies for
the information sets in all of S, we only need them for those in R, and R can
be much smaller than S.
     The Fershtman and Pakes (2012) article provide a learning algorithm, in the
spirit of reinforcement learning25 that enables the analyst to compute a REBE.
Briefly, the algorithm starts at some initial st and has a formulaic method of
generating initial perceptions of the expected discounted values of alternative
actions at each possible Ji,t . Each agent choses the action which maximizes its
initial perceptions of its values. The actions generate a probability distribution
over outcomes, and a pseudo random draw from those distributions plus the
rules on information access determine both the current profit and the new state
(our Ji,t+1 ). Then the current profit and the new state are used to update the
perceptions of the values for taking actions in the original state (at Ji,t ). More
precisely, the profits are taken as a random draw from the possible profits from
the action taken at the old state, and the new state, or rather the initial percep-
tion of the value of the new state, is treated as a random draw from the possible
continuation values from the action taken at Ji,t . The random draws on profits
and continuation values are averaged with the old perceptions to form new per-
ceptions. This process is then repeated from the new state. So the algorithm is
iterative, but each iteration only updates the values and policies at one point (it is
‚Äùasynchronous‚Äù). The policies at that point are used to simulate the next point,
 25
      For an introduction to reinforcement learning see, Sutton, R. and Barto, A. (1998).



                                                         24
and the simulated output is used to update perceptions at the old point. Notice
that firms could actually follow these steps to learn their optimal policies, but it
is likely to do better as an approximation to how firms react to perturbations in
their environment then to major changes in it26 .
    This process has two computational advantages. First the simulated process
eventually wanders into R and stays their. So the analyst never needs to com-
pute values and policies on all possible states, and the states that are in R are
updated repeatedly. Second the updating of perceptions never requires inte-
gration over all possible future states, it just requires averaging two numbers.
On the negative side there is no guarantee that the algorithm will converge to
a REBE. However Fershtman and Pakes program an easy to compute test for
convergence into the algorithm, and if the test output satisfies the convergence
criteria, the algorithm will have found a REBE.

Multiplicity.  A Bayesian Perfect equilibrium satisfies the conditions of a REBE,
but so do weaker notions of equilibrium. So REBE admits greater multiplicity
than does Bayesian Perfect notions of equilibrium. To explain the major rea-
son for the increase in equilibria partition the points in R into ‚Äúinterior‚Äù and
‚Äúboundary‚Äù points27 . Points in R at which there are feasible (but non-optimal)
strategies which can lead outside of R are boundary points. Interior points are
points that can only transit to other points in R no matter which of the feasible
policies are chosen.
    The REBE conditions only ensure that perceptions of outcomes are consis-
tent with the results from actual play at interior points. Perceptions of outcomes
for feasible (but non-optimal) policies at boundary points need not be tied down
by actual outcomes. As a result differing perceptions of discounted values at
points outside of the recurrent class can support different equilibria. One can
mitigate the multiplicity problem by adding either empirical information or by
  26
      This because the current algorithm does not allow for experimentation, and (ii) can require many visits to
a given point before reaching an equilibrium (especially when initial and equilibrium perceptions differ greatly).
Doraszelski, Lewis and Pakes (2015) study firms learning policies in a new market and find that there is an initial
stage where an analogue of this type of learning does not fit, but the learning algorithm does quite well after an
initial period of experimentation.
   27
      This partitioning is introduced in Pakes and McGuire, 2001. There is another type of multiplicity that may
be encountered; there may be multiple recurrent classes for a given equilibrium policy vector. Sufficient condition
for the policies to generate a process with a unique recurrent class are available (see Freedman, 1971, or Ericson
and Pakes,1995) but there are cases of interest where multiple separate recurrent classes are likely (see Besanko,
Doraszeldi and Kryukov, 2014).



                                                       25
strengthening the behavioral assumptions.
    The empirical information should help identify which equilibria has been
played, but may not be of much use when attempting to analyze counterfactu-
als. The additional restrictions that may be appropriate include the possibility
that prior knowledge or past experimentation will endow agents with realistic
perceptions of the value of states outside, but close to, the recurrent class. In
these cases we will want to impose conditions that insure that the equilibria we
compute are consistent with this knowledge. To accommodate this possibility,
Asker, Fershtman, Jeon, and Pakes (2015) propose an additional condition on
equilibrium play that insures that agents‚Äô perceptions of the outcomes from all
feasible actions from points in the recurrent class are consistent with the out-
comes that those actions would generate. They label the new condition ‚Äùbound-
ary consistency‚Äù and provide a computational simple test to determine whether
the boundary consistency condition is satisfied for a given set of policies.

Empirical Challenges.  In addition to the static profit function (discussed in the
earlier sections), empirical work on dynamics will require: (i) specifying the
variables in Ji and (ii) estimates of the ‚Äúdynamic‚Äù parameters (these usually
include the costs of entry and exit, and parametric models for both the evolution
of exogenous state variables and the response of endogenous state variables to
the agents‚Äô actions).
   There is nothing in our equilibrium conditions that forbids Ji from contain-
ing less variables then the decision maker has at its disposal, and a restricted Ji
may well provide a better approximation to behavior. The empirical researcher‚Äôs
specification of Ji should be designed to approximate how the dynamic controls
are determined (in the simplest model this would include investment, entry and
exit policies). This suggests choosing the variables in Ji through an empiri-
cal analysis of the determinants of those controls. Information from the actual
decision makers or studies of the industry would help guide this process.
   In principle we would like to be able to generate predictions for the dynamic
controls that replicate the actual decisions up to a disturbance which is a sum
of two components; (i) a ‚Äùstructural‚Äù disturbance (a disturbance which is a de-
terminant of the agent‚Äôs choice, but we do not observe) which is independently
distributed over time, and (ii) a measurement error component. The measure-
ment error component should not be correlated with variables which are thought


                                        26
to be correctly measured, and the structural error should not be correlated with
any variable dated prior to the period for which the control is being predicted.
So the joint disturbance should be uncorrelated with past values of correctly
measured variables. This provides one test of whether a particular specification
for Ji is adequate. If the null is rejected computational and estimation proce-
dures which allow for serially correlated errors should be adopted. This need
not pose additional computational problems, but may well complicate the esti-
mation issues we turn to now.
    Estimates of dynamic parameters will typically be obtained from panel data
on firms, and the increased availability of such data bodes well for this part
of the problem. Many of the dynamic parameters can be estimated by careful
analysis of the relationship between observables; i.e. without using any of the
constructs that are defined by the equilibrium to the dynamic model (such as ex-
pected discounted values). For example if investment is observed and directed
at improving a given variable, and (a possibly error prone) measure of that vari-
able is either observed or can be backed out of the profit function analysis, the
parameters governing the impact of the control can be estimated directly.
    However there often are some parameters that can only be estimated through
their relationship to perceived discounted values (sunk and fixed costs often
have this feature). There is a review of the literature on estimating these pa-
rameters in the third section of Ackerberg et. al. (2007). It focuses on two
step semi-parameteric estimators which avoid computing the fixed point that
defines the equilibrium at trial values of the parameter vector28 . In addition
Pakes (2015) describes a ‚Äúperturbation‚Äù estimator, similar to the Euler equation
estimator for single agent dynamic problems proposed by Hansen and Singleton
(1982). This estimator does not require the first step non-parametric estimator,
but is only available for models with asymmetric information. Integrating se-
rially correlated unobservables into these procedures can pose additional prob-
lems; particularly if the choice set is discrete. There has been recent work on
discrete choice models that allow for serially correlated unobservables (see Ar-
cidiano and Miller, 2011), but it has yet to be used in problems that involve
estimating parameters that determine market dynamics.
  28
    The relevant papers here are those of Bajari Benkard and Levin (2007), and Pakes Ostrovsky and Berry,
(2007))




                                                   27
5    Conclusion.
The advantage of using the tools discussed here to evaluate policies is that they
let the data weigh in on the appropriateness of different functional forms and
behavioral assumptions. Of course any actual empirical analysis will have to
maintain some assumptions and omit some aspects of the institutional environ-
ment. The critical issue, however, is not whether the empirical exercise has all
aspects of the environment modeled correctly, but rather whether empirics can
do better at counterfactual policy analysis than the next best alternative avail-
able. Of course in comparing alternatives we must consider only those that;
(i) use the information available at the time the decision is made and (ii) abide
by the resource constraints of the policy maker. By now I think it is clear that
in some cases empirical exercises can do better than the available alternatives,
and that the proportion of such cases is increasing; greatly aided by advances in
both computational power and the resourcefulness of the academic community
(particularly young Industrial Organization scholars).

References.
    ‚Ä¢ Ackerberg D, L. Benkard, S. Berry, A. Pakes (2007) ‚ÄùEconometric Tools for Analyzing Market
      Outcomes‚Äù. In Heckman J, Leamer E, ed., The Handbook of Econometrics Amsterdam: North-
      Holland, Chapter 63.

    ‚Ä¢ Agarwal, N. (2015) ‚ÄùAn Empirical Model of the Medical Match‚Äù, American Economic Review,
      105(7): 1939-1978.

    ‚Ä¢ Arcidiano, P. and R. Miller (2011) ‚ÄùConditional Choice Probability Estimation of Dynamic Dis-
      crete Choice Models with Unobserved Heterogeneity‚Äù, Econometrica, 7(6): 1823-1868.

    ‚Ä¢ Asker, J., C. Fershtman, J. Jeon, and A. Pakes (2014) ‚ÄúThe Competitive Effects of Information
      Sharing‚Äù, Harvard University working paper.

    ‚Ä¢ Bajari, P., L. Benkard, and J. Levin (2007) ‚ÄúEstimating Dynamic Models of Imperfect Competi-
      tion,‚Äù Econometrica, 75(5): 13311370 .

    ‚Ä¢ Battigalli, P., S. Cerreia-Vioglio, F. Maccheroni, and M. Marinacci (2015) ‚ÄùSelf-Confirming Equi-
      librium and Model Uncertainty.‚Äù American Economic Review, 105(2): 646-677.

    ‚Ä¢ Benkard, L., (2004) ‚ÄùA Dynamic Analysis of the Market for Wide-Bodied Commercial Aircraft‚Äù
      Review of Economic Studies, 71(3): 581-611.

    ‚Ä¢ Benkard, L., Van Roy B., and Weintraub G. (2008) ‚ÄùMarkov Perfect Industry Dynamics with
      Many Firms‚Äù Econometrica, 76(6): 1375-1411.



                                                  28
‚Ä¢ Berry S., J. Levinsohn, and A. Pakes (1995) ‚ÄúAutomobile Prices in Market Equilibrium,‚Äù Econo-
  metrica, 63(4): 841-890.
‚Ä¢ Berry S., J. Levinsohn, and A. Pakes (2004), ‚ÄúEstimating Differentiated Product Demand Systems
  from a Combination of Micro and Macro Data: The Market for New Vehicles,‚Äù Journal of Political
  Economy, 112(1): 68-105.
‚Ä¢ Besanko, D., U. Doraszelski, Y. Kryukov, and M. Satterthwaite (2010) ‚ÄùLearning by Doing, Or-
  ganizational Forgetting and Industry Dynamics‚Äù Econometrica, 78 (2): 453-508.
‚Ä¢ Besanko, D., U. Doraszelski, and Y. Kryukov (2014) ‚ÄúThe Economics of Predation: What Drives
  Pricing When There Is Learning-by-Doing?‚Äù, American Economic Review, 104(3): 868897.
‚Ä¢ Bresnahan, T. and P. Reiss (1988) ‚ÄùDo Entry Conditions Vary Across Markets‚Äù, Brookings Papers
  on Economic Activity No.3, 833-882.
‚Ä¢ Collard-Wexler, A. (2013) ‚ÄùDemand Fluctuations in the Ready-Mix Concrete Industry‚Äù, Econo-
  metrica, 81(3): 1003-1037.
‚Ä¢ Crawford, G. and A. Yurukoglu (2013) ‚ÄúThe Welfare Effects of Bundling in Multichannel Televi-
  sion Markets‚Äù American Economic Review, 102(2): 643-85.
‚Ä¢ De Loecker J., and Warzynski (2012) ‚ÄùMarkups and Firm-Level Export Status‚Äù American Eco-
  nomic Review, 102(6): 2437-2471.
‚Ä¢ De Loecker, J., P. Goldberg, A. Khandelwal and N. Pavcnik, (forthcoming), ‚ÄùPrices, Markups and
  Trade Reform‚Äù Econometrica.
‚Ä¢ Doraszelski, U., and K. Judd (2011) ‚ÄùAvoiding the Curse of Dimensionality in Dynamic Stochas-
  tic Games‚Äù, Quantitative Economics, 3(1): 53-93.
‚Ä¢ Doraszelski U., G. Lewis and A. Pakes (2014) ‚ÄúJust starting out: Learning and price competition
  in a new market‚Äù mimeo Harvard University.
‚Ä¢ Doraszelki, U. and A. Pakes (2008), ‚ÄúA Framework for Applied Dynamic Analysis in I.O.,‚Äù in M.
  Armstrong and R. Porter ed.s. The Handbook of Industrial Organization, Chapter 33, 2183-2262.
‚Ä¢ Einav L, M. Jenkins and J. Levin (2012) ‚ÄúContract Pricing in Consumer Credit Markets‚Äù Econo-
  metrica, 80(4): 1387-1432.
‚Ä¢ Eizenberg, A. (2014) ‚ÄùUpstream Innovation and Product Variety in the United States Home PC
  Market,‚Äù the Review of Economic Studies, 81(): 1003-1045
‚Ä¢ Ericson R. and A. Pakes (1995), ‚ÄúMarkov Perfect Industry Dynamics: A Framework for Empirical
  Work,‚Äù Review of Economic Studies, 62(1): 53-82.
‚Ä¢ Fershtman C. and A Pakes (2000), ‚ÄúA Dynamic Game with Collusion and Price Wars,‚Äù RAND
  Journal of Economics, 31(2): 207-36.
‚Ä¢ Fershtman C. and A. Pakes (2012) ‚ÄúDynamic Games with Asymmetric Information: A Framework
  for Applied Work‚Äù The Quarterly Journal of Economics, 127(4): 1611-1662.
‚Ä¢ Fudenberg D. and D. Levine (1983) ‚ÄùSubgame Perfect Equilibrium of Finite and Infinite Horizon
  Games‚Äù Journal of Economic Theory 31, 227-256.

                                             29
‚Ä¢ Fong K. and R. Lee (2013) ‚ÄúMarkov Perfect Network Formation: An Appied Framework for
  Bilateral Oligopoly and Bargaining in Buyer-Seller Networks‚Äù mimeo Havard University.

‚Ä¢ Freedman D. (1971) Markov Chains, Holden Day series in probabilty and statistics.

‚Ä¢ Gowrisankaran, G., A. Nevo, and Town, R. (2015) ‚ÄùMergers When Prices Are Negotiated: Evi-
  dence from the Hospital Industry,‚Äù American Economic Review, 105(1): 172-203.

‚Ä¢ Gowrisankaran, G., and M. Rysman (2012) ‚ÄùDynamics of Consumer Demand for New Durable
  Goods,‚Äù Journal of Political Economy, 120(6) 1173-1219.

‚Ä¢ Hansen, L. and K. Singleton (1982) ‚ÄùGeneralized Instrumental Variables Estimation of Nonlinear
  Rational Expectations Models‚Äù, Econometrica, 50(5): 1269-1286.

‚Ä¢ Hendel I., S. Lach , and Y. Spiegel (2014) ‚ÄùConsumers Activism: the Facebook Boycott on Cot-
  tage Cheese?‚Äù CEPR Discussion Paper 10460.

‚Ä¢ Hendriks, K, and R. Porter (2015), ‚ÄùEmpirical Analysis and Auction Design‚Äù in process North-
  western University.

‚Ä¢ Ho, K, and R. Lee (2015) ‚ÄùInsurer Competition in Health Care Markets‚Äù mimeo Harvard Univer-
  sity

‚Ä¢ Holmes. T. and J. Schmitz (2010) ‚ÄùCompetition and Productivity: A Review of Evidence‚Äù Annual
  Review of Economics, 619-642.

‚Ä¢ Horn H. and A. Wolinsky (1988) ‚ÄùBilateral Monopolies and Incentives for Merger‚Äù RAND Journal
  of Economics 19(3): 408-419.

‚Ä¢ Judd, K. (1998), Numerical Methods in Economics, MIT Press: Cambridge, MA.

‚Ä¢ Kalouptsidi, M. (2014) ‚ÄùTime to Build and Fluctuations in Bulk Shipping‚Äù, American Economic
  Review, 104(2): 564-608.

‚Ä¢ Kaido, H., Molinari, F. and Stoye, J. (2015) ‚ÄùConfidence Intervals for Projections of Partially
  Identified Parameters‚Äù, Discussion paper, Cornell University.

‚Ä¢ Mansk. C. (2003) Partial Identification of Probability Distributions, Berlin, Heidelberg, New
  York: Springer-Verlag.

‚Ä¢ Maskin, E. and J. Tirole (1988) ‚ÄùA Theory of Dynamic Oligopoly, I: Overview and Quantity
  Competition with Large Fixed Costs‚Äù Econometrica, 56(3): 549-569.

‚Ä¢ McFadden D. (1974), ‚ÄúConditional Logit Analysis of Qualitative Choice Behavior‚Äù, in P. Zarem-
  bka (ed.) Frontiers in Econometrics. Academic Press, New York.

‚Ä¢ McFadden D. (1989) ‚ÄúA Method of Simulated Moments for Estimation of Discrete Response
  Models Without Numerical Integration‚Äù, in Econometrica, Volume 57, 995-1026.

‚Ä¢ Melitz, M., and S. Polanec, (2015), ‚ÄùDynamic Olley-Pakes Productivity Decomposition with En-
  try and Exit.‚Äù forthcoming in Rand Journal of Economics .




                                             30
‚Ä¢ B. Mermelstein, V. Nocke, M. Satterthwaite, and M. Whinston (2014), ‚ÄùInternal versus External
  Growth in Industries with Scale Economies: A Computational Model of Optimal Merger Policy,‚Äù
  NBER Working Papers 20051.
‚Ä¢ Nevo A. (2001) ‚ÄùMeasuring Market Power in the Ready-to-Eat Cereal Industry‚Äù, Econometrica,
  69(2): 307-342.
‚Ä¢ Nevo A., and F. Rossi (2008), ‚ÄùAn approach for extending dynamic models to settings with multi-
  product firms‚Äù Economic Letters, 100: 49-52.
‚Ä¢ Nosko C. (2014) ‚ÄùCompetition and Quality Choice in the CPU Market‚Äù Chicago Booth working
  paper.
‚Ä¢ S. Olley and A. Pakes (1996) ‚ÄúThe Dynamics of Productivity in the Telecommunications Equip-
  ment Industry,‚Äù Econometrica, 64(6): 1263-1298.
‚Ä¢ A. Pakes and D. Pollard (1989) ‚ÄúSimulation and the Asymptotics of Optimization Estimators,‚Äù
  Econometrica, 57(5): 1027-1057.
‚Ä¢ A. Pakes and P. McGuire (1994) ‚ÄúComputing Markov Perfect Nash Equilibrium: Numerical Im-
  plications of a Dynamic Differentiated Product Model,‚Äù RAND Journal of Economics, 25(4): 555-
  589.
‚Ä¢ A. Pakes and P. McGuire (2001) ‚ÄúStochastic Algorithms, Symmetric Markov Perfect Equilibria,
  and the ‚ÄôCurse‚Äô of Dimensionality‚Äù, Econometrica, 69(5): 1261-1281.
‚Ä¢ A. Pakes, Ostrovsky M., and Berry S. (2007) ‚ÄúSimple Estimators for the Parameters of Discrete
  Dynamic Games (with Entry-Exit Examples)‚Äù RAND Journal of Economics, 38(2): 373-399.
‚Ä¢ A. Pakes (2010), ‚ÄúAlternative Models for Moment Inequalities‚Äù, Econometrica, 78(6): 1783-1822.
‚Ä¢ Pakes A. (2014). ‚ÄùBehavioral and Descriptive Forms of Choice Models‚Äù International Economic
  Review, 55(3): 603-624.
‚Ä¢ A. Pakes, J. Porter, K. Ho and J. Ishii, (2015) ‚ÄúMoment Inequalities and Their Application‚Äù,
  Econometrica, 83(1): 315-334
‚Ä¢ Pakes A, (forthcoming) ‚ÄùMethodological Issues in Analyzing Market Dynamics‚Äù, in Florian Wa-
  gener ed., Advances in Dynamic and Evolutionary Games: Theory, Applications, and Numerical Methods,
  Springer.
‚Ä¢ Powell J. (1994) ‚ÄúEstimation of Semiparameteric Models‚Äù, in the Handbook of Ecoometrics, Vol-
  ume 4, R. Engle and D. McFadden (ed.s).
‚Ä¢ Reynaert M, Verboven F (2014) ‚ÄúImproving the performance of random coefficients demand mod-
  els: the role of optimal instruments‚Äù Journal of Econometrics, 179(1): 83 - 98.
‚Ä¢ R. Sutton and A. Barto (1998), Reinforcement Learning: An Introduction, MIT Press, Cambridge
  MA.
‚Ä¢ Tamer E. (2010) ‚ÄùPartial Identification in Econometrics,‚Äù Annual Review of Economics, 2: 167-
  195.
‚Ä¢ Wollman, T. (2014): ‚ÄúTrucks without Bailouts: Equilibrium Product Characteristics for Commer-
  cial Vehicles,‚Äù Chicago-Booth working paper.


                                             31
