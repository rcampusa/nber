                             NBER WORKING PAPER SERIES




           SOCIAL MEDIA NETWORKS, FAKE NEWS, AND POLARIZATION

                                      Marina Azzimonti
                                      Marcos Fernandes

                                     Working Paper 24462
                             http://www.nber.org/papers/w24462


                    NATIONAL BUREAU OF ECONOMIC RESEARCH
                             1050 Massachusetts Avenue
                               Cambridge, MA 02138
                                   March 2018




We would like to thank the participants of the 2017 NBER Summer Institute on Political
Economy (Boston), 22nd Coalition Theory NetworkWorkshop (Glasgow), 27th International
Conference on Game Theory (Stony Brook), 43rd Eastern Economic Association Conference
(New York) and seminar series at London Business School/Government (London), Warwick,
Harris Public School (Chicago). In particular we are grateful for the valuable comments from
Jesse Shapiro, Daron Acemoglu, Ernesto Dal Bo, Matthew Jackson, Yair Tauman and Helios
Herrera. The views expressed herein are those of the authors and do not necessarily reflect the
views of the National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.

© 2018 by Marina Azzimonti and Marcos Fernandes. All rights reserved. Short sections of text,
not to exceed two paragraphs, may be quoted without explicit permission provided that full
credit, including © notice, is given to the source.
Social Media Networks, Fake News, and Polarization
Marina Azzimonti and Marcos Fernandes
NBER Working Paper No. 24462
March 2018
JEL No. C45,C63,D72,D8,D83,D85,D91

                                         ABSTRACT

We study how the structure of social media networks and the presence of fake news might affect
the degree of misinformation and polarization in a society. For that, we analyze a dynamic model
of opinion exchange in which individuals have imperfect information about the true state of the
world and are partially bounded rational. Key to the analysis is the presence of internet bots:
agents in the network that do not follow other agents and are seeded with a constant flow of
biased information. We characterize how the flow of opinions evolves over time and evaluate the
determinants of long-run disagreement among individuals in the network. To that end, we create
a large set of heterogeneous random graphs and simulate a long information exchange process to
quantify how the bots’ ability to spread fake news and the number and degree of centrality of
agents susceptible to them affect misinformation and polarization in the long-run.


Marina Azzimonti
Economics Department
Stony Brook University
100 Nicolls Road
Stony Brook, NY 11794
and NBER
marina.azzimonti@gmail.com

Marcos Fernandes
Economics Department
Stony Brook University
100 Nicolls Road
Stony Brook, NY 11794
marcos.fernandes@stonybrook.edu
1    Introduction
In the last few years, the United States has become more polarized than ever. A recent survey
conducted by The Pew Research Center indicates that Republicans and Democrats are further
apart ideologically than at any point since 1994 (see Figure (1)).




      Figure 1: Political Polarization in the American Public (2014, Pew Research Center)

    Traditional theories in economics and political science typically model disagreement as aris-
ing from one of two sources: (i) differences in preferences and (ii) informational frictions. In the
first case, agents may disagree on the optimal level of a given policy because they benefit differ-
ently from it. This happens when their income or wealth levels are different (such as in the case of
redistributive policies) or when they have different preferences over public goods (e.g. defense vs
education or health-care, etc.). In the case of informational frictions, there may exist an optimal
action, but society may not know exactly what it is. Examples are the need for environmen-
tal policy, mandatory vaccination, restrictions on certain groups of immigrants, unconventional
monetary policy, or simply choosing one political candidate over another. Individuals may learn
about the desirability of the policy by acquiring information, but to the extent that they are ex-
posed to biased sources of information, their beliefs may differ at the time in which decisions
must be taken.
    There is a large literature trying to explain how slanted news and media bias may affect vot-
ers’ opinions by generating misinformation and exacerbating polarization (see Della Vigna and
Kaplan, 2007 or Martin and Yurukoglu, 2015). While this literature has been mostly focused on
traditional media, such as newspapers, radio, and cable TV—broadly covered under the umbrella
of ‘broadcasting’—recent interest has shifted towards social media. There are several reasons for
this shift. First, because individuals are increasingly obtaining information from social media


                                                 2
networks. According to a 2016 study by the Pew Research Center and the John S. and James L.
Knight Foundation, 62% of adults get their news from social media (a sharp increase from the 49%
observed in 2012).1 Among these, two-thirds of Facebook users (66%) get news on the site, nearly
six-in-ten Twitter users (59%) get news on Twitter, and seven-in-ten Reddit users get news on
that platform.
    Second, the technology of communication in social media is significantly different. In the
world of broadcasting, agents are mostly consumers of information. There is a small number
of news outlets that reach a large (and relatively passive) audience. In the world of Web 2.0, or
‘social media,’ individuals are not only consuming information, but they are also producing it.
This technological change is less well understood. A key aspect of social media communication
is that one given message can reach a large audience almost immediately. Another important
change is that it is much more difficult for individuals to back out the reliability of a piece of
information, as they observe a distilled signal from a friend in their network without necessarily
knowing its source.
    This is relevant when coupled with another phenomena that became prevalent particularly
around 2016 presidential election: the massive spread of fake news (also referred to as disinfor-
mation campaigns, cyber propaganda, cognitive hacking, and information warfare) through the
internet. As defined by Gu, Kropotov, and Yarochkin (2016), ‘Fake news is the promotion and
propagation of news articles via social media. These articles are promoted in such a way that
they appear to be spread by other users, as opposed to being paid-for advertising. The news sto-
ries distributed are designed to influence or manipulate users’ opinions on a certain topic towards
certain objectives.’ While the concept of propaganda is not new, social media apparently has made
the spreading of ideas faster and more scalable, making it potentially easier for propaganda ma-
terial to reach a wider set of people. Relative to more traditional ways of spreading propaganda,
fake news are extremely difficult to detect posing a challenge for social media users, moderators,
and governmental agencies trying control their dissemination. A December 2016 Pew Research
Center study found that ‘about two-in-three U.S. adults (64%) say fabricated news stories cause a
great deal of confusion about the basic facts of current issues and events.’ Moreover, 23% admit
to having shared a made-up news story (knowingly or not) on social media. Understanding how
fake news spread and affect opinions in a networked environment is at the core of our work.
    With the dispersion of news through social media, and more generally the internet, and given
   1
     The distribution of social media users is similar across education levels, race, party affiliation and age. About
22% of 18-29 year olds are social media users, 34% are aged 30-49, 26% are aged 50-64, and 19% 65 and older.


                                                          3
that a growing proportion of individuals, politicians, and media outlets are relying more inten-
sively on this networked environment to get information and to spread their world-views, it is
natural to ask whether and to what extent misinformation and polarization might be exacerbated
by social media communication.
   In this context, we study a dynamic model of opinion formation in which individuals who
are connected through a social network have imperfect information about the true state of the
world, denoted by θ. For instance, the true state of the world can be interpreted as the relative
quality of two candidates competing for office, the optimality of a specific government policy or
regulation, the degree of government intervention in specific markets, etc.
   Individuals can obtain information about the true state of the world from unbiased sources
external to the network, like scientific studies, unbiased news media, reports from non-partisan
research centers such as the Congressional Budget Office, etc. This is modeled as an informative
and unbiased private signal received by each agent. Due to limited observability of the structure
of the network and the probability distribution of signals observed by others, individuals are as-
sumed to be incapable of learning in a fully Bayesian way. Moreover, we assume that individuals
are unable to process all the available information and for that they can also rely on the informa-
tion from their social neighbors (i.e. individuals connected to them through the network) who are
potentially exposed to other sources. In this sense, individuals in our network update their beliefs
as a convex combination of the Bayesian posterior belief conditioned on their private signals and
the opinion of their neighbors, as per the update rule proposed by Jadbabaie, Molavi, Sandroni,
and Tahbaz-Salehi (2012) (JMST (2012) henceforth).
   There are three types of agents in this society: Sophisticated agents, unsophisticated agents
and Internet bots. Their characterization is to some extent interrelated because it depends not
only on signals observed, but also on their network connectivities. In terms of signals received,
both sophisticated and unsophisticated agents receive informative private signals every period
of time. Internet bots, on the other hand, rely only on biased information and produce a stream
of fake news. In terms of connectivities, Internet bots do not relay in the information of others
(they are sinks in a Markov chain sense), but have a positive mass of followers. More specifically,
bots’ followers are exclusively composed by unsophisticated agents. These agents are unable to
identify the bot as a source of misinformation, implying that they cannot detect and disregard
fake news, which are incorporated when updating beliefs (therefore the adjective unsophisticated).
The opinions generated from the exchange of information forms an inhomogeneous Markov
process which may never lead to consensus among sophisticated agents since they are exposed

                                                 4
to unsophisticated agents. In such environment, it can be shown that society’s beliefs fail to
converge. Moreover, under some conditions, the belief profile can fluctuate in an ergodic fashion
leading to misinformation and polarization cycles.
   The structure of the graph representing the social media network and the degree of influ-
ence of unsophisticated agents shape the dynamics of opinion and the degree of misinformation
and polarization in the long-run. More specifically, long-run misinformation and polarization
depends on three factors: behavioral assumptions (e.g. the updating rule), communication tech-
nology (e.g. the speed at which network connections are active and creation of fake news), and
the network topology (e.g. the degree of clustering, the share of unsophisticated agents on the
population, how central they are, and the ability of bots to flood the network with fake news).
Because a theoretical characterization of the relationship between the topology of the network
and the degrees of misinformation and polarization is not trivial, we create a large set of random
graphs with different behavioral assumptions, communication technologies and topologies. We
then quantify how fake news, the degrees of centrality, and influence affect misinformation (e.g.
how far agents beliefs are from the true state of the world) and long-run polarization, defined as
in Esteban and Ray (1994).
   We find that misinformation and polarization have an inverted u-shape relationship. This
is to be expected: when individuals are able to effectively aggregate information and learn the
true state of the world, polarization vanishes. At the other extreme, there are situations where
there is no polarization because most individuals in the network converge to the wrong value
of θ. This involves maximal misinformation with no polarization. Finally, there are cases in
which individuals are on average correct but distributed symmetrically around the true state of
the world, with large mass at the extremes of the belief distribution. Here, there are intermediate
levels of misinformation and extreme polarization. Even though this implies somewhat better
aggregation of information, it may lead to inefficient gridlock due to inaction. We find that when
unsophisticated agents have a large number of followers in social media, misinformation rises
but polarization is hardly affected. On the other hand, the clustering coefficient (i.e. a network
statistics that says the extent through which friends of friends are also direct friends) is important
for polarization (it actually reduces it) but irrelevant for misinformation. When unsophisticated
agents are relatively more influential (because they manage to affect the opinions of influential
followers), information is more efficiently aggregated. However, to the extent that agents do not
fully learn the true state of the world, there is a significant amount of networks in which opinions
become extreme. These are networks in which a bot with views at one extreme targets relatively

                                                  5
more influential unsophisticated agents than the bot with opposing views. This makes the bot
more more efficient at spreading fake-news, since the speed at which each given piece of fake-
news travels through the network rises, pulling opinions towards an extreme. We show that,
for specific network topologies, significant levels of misinformation and polarization are possible
in network in which as little as 10% of agents believe fake news. This is relevant, because it
shows that the network externality effects are quantitatively important. In other words, only a
relevant small number of unsophisticated agents is able to generate significant misinformation
and polarization in our simulated networks.


Related Literature     Our paper is related to a growing number of articles studying social learn-
ing with bounded rational agents and the spread of misinformation in networks.
   The strand of literature focusing on social learning with bounded rational agents assumes
that individuals use simple heuristic rules to update beliefs, like taking repeated averages of ob-
served opinions. Examples are DeGroot (1974), Ellison and Fundenberg (1993, 1995), Bala and
Goyal (1998,2001), De Marzo, Vayanos and Zwiebel (2003) and Golub and Jackson (2010). In most
of these environments, under standard assumptions about the connectivity of the network and
the bounded prominence of groups in growing societies, the dynamics of the system reaches an
equilibrium and consensus emerges. In this sense, long-run polarization or misinformation would
only arise in such models if those assumptions are relaxed. Common to most of these models is
the fact that there is no new flow of information entering into the network. Agents are typically
assumed to be bounded rational (naive) and do not observe private signals from external sources
(and hence do not use standard Bayesian update rules). JMST (2012) extends these environments
to allow for a constant arrival of new information over time in an environment in which agents
also learn from their neighbors in a naive way. This feature allows agents to efficiently aggre-
gate information even when some standard assumptions that ensure consensus are relaxed. Our
paper uses an update rule based on JMST (2012) for gents, but introduces internet bots which
break the connectivity of the network by basing their information exclusively on biased sources
and disregard the information provided by others. The latter is a feature that we borrow from
the literature on misinformation. More particularly, from the work by Acemoglu, Ozdaglar and
ParandehGheibi (2010) (AOP henceforth) and Acemoglu, Como, Fagnani, and Ozdaglar (2013)
(ACFO henceforth)
   AOP (2010) focuses on understanding the conditions under which agents fail to reach consen-
sus or reach wrong consensus. In their model, agents exchange opinion in a naive way conditional

                                                6
on being pair-wise matched. Crucial to the emergence of misinformation in is the presence of
forceful agents whose roles are to exert disproportional influence over regular agents and force
them to conform with their opinions. ACFO (2013) consider the same naive learning model with
random meetings dictated by a Poisson process, but allow for the existence of stubborn agents
instead. These agents never update their opinions (they are sinks in a Markov chain sense) but
influence other agents. Therefore, the information exchange dynamics never reaches a steady
state and opinions fluctuate in a stochastic fashion. Both papers abstract from Bayesian learning.
In our paper, we consider simultaneously the possibility that regular agents learn from unbiased
sources while being exposed to fake news spread by Internet bots. Our learning rule follows
JMST (2012) in the sense that agents learn from private signals in a a fully Bayesian fashion but
also incorporate friends’ opinions naively. The final belief is basically a convex combination of
the Bayesian posterior and friends’ posteriors. Moreover, we add the feature that agents meet
randomly in the spirit of of AOP (2010) and ACFO (2013). Therefore, the main extensions with
respect to JMST (2012) are i) the presence of Internet bots (sinks) seeded with biased information
that spread fake news, which becomes the main source of misinformation in the system and ii)
the fact that we allow for random meetings (inhomogeneous Markov chain). On the other hand,
the main extension relative to ACFO (2013) is that we introduce Bayesian learning features. Our
Internet bots can be understood as stubborn agents endowed with the capacity to countervail the
flow of informative private signals that reaches regular agents every period of time. We call this
feature flooding capacity and it basically consists in allowing these bots to spread a larger stream
of fake news (signals) as other agents in the network.2 Hence, our paper contributes to the social
learning and spread of misinformation literatures by studying misinformation in an environment
with informative signals.
    Our main contribution relative to the existing literature, however, is that we simulate a large
set of complex social networks and quantify the relative importance of behavioral assumptions,
technological characteristics, and network topology on long-run polarization and misinforma-
tion. To the best of our knowledge, this is the first paper to quantify the relative importance of
network characteristics on long-run misinformation and polarization.
    Finally, there is a growing empirical literature analyzing the effects of social media in opinion
formation and voting behavior (Halberstam and Knight, 2016). Because individual opinions are
unobservable from real network data, these papers typically use indirect measures of ideology to
    2
      Our model considers a Bernoulli rather than a Poisson process and restrict attention to a particular class of
beliefs (Beta distributions) though.


                                                        7
back-out characteristics of the network structure (such as homophily) potentially biasing their
impact. By creating a large number artificial networks, we can directly measure how homophily
and other network characteristics affect opinion. Finally, our paper complements the literature
on the role of biased media such as Campante and Hojman (2013), Gentzkow and Shapiro (2006,
2010, and 2011), and Flaxman et al. (2013) and the effects of social media on political polarization,
such as Boxell et al (2017), Barbera (2016), and Weber at al (2013).


Basic notation: The notation and terminology introduced here is mostly employed in the ap-
pendix of this work but also serves as an important guide for the next sections. All vectors are
viewed as column vectors, unless stated to the contrarily. Given a vector v ∈ Rn , we denote by vi
its i-th entry. When vi ≥ 0 for all entries, we write v ≥ 0. To avoid potential burden of notation,
                 P
the summation      v without index represents the sum of all entries of vector v. Moreover, we
define v > as the transpose of the vector v and for that, the inner-product of two vectors x, y ∈ Rn
is denoted by x> y. Similarly for the product of conforming vectors and matrices. We denote by
1 the vector with all entries equal to 1. Finally, a vector v is said to be a stochastic vector when
           P
v ≥ 0 and i vi = 1. In terms of matrices, a square matrix M is said to be row stochastic when
each row of M is a stochastic vector. A matrix M is said to be a square matrix of size n when the
number of rows and columns is n. The identity matrix of size n is denoted by In . For any matrix
M , we write Mij or [M ]ij to denote the matrix entry in the i-th row and j-th column. The sym-
bols Mi∗ and [M ]i∗ are used to denote the i-th row of matrix M , while M∗j and [M ]∗j denote the
j-th column of the matrix. Finally, the transpose of a matrix M is denoted by M > and represents
a new matrix whose rows are the columns of the original matrix M , i.e. M > ij = [M ]ji .
                                                                             



2    Baseline Model
Agents, social bots and information structure           The economy is composed by a finite number
of agents i ∈ N = {1, 2, . . . , n} who interact in a social network over time for a large number
T of periods (which need not be finite), date at which a one-dimensional policy needs to be
determined. Individuals have imperfect information about θ ∈ Θ = [0, 1], the optimal value of
the policy. This parameter can be interpreted as the degree of government intervention in private
markets (e.g. environmental control, enforcement of property rights, restrictions on the use of
public land, gun control, etc.), as optimal fiscal or monetary policy (e.g. the inflation rate, tax rates
on capital or labor income, tariffs, etc.), or as the best response to an unexpected shock (e.g. the

                                                   8
size of a bailout during a financial crisis, the response to a national security threat, the amount
of aid given to a region that suffered a natural disaster, etc.). In period 0 individuals have a prior
about θ and update their beliefs from private signals obtained up to period T , where the policy
needs to be implemented. 3
       Agents obtain information from: (i) an unbiased source, (ii) other agents connected to them
in a social network, and (iii) a bot spreading fake news. There are two types of bots, L − bot and
R − bot with opposing agendas. Their objective is to manipulate opinions by sending extremely
biased signals (e.g. close to 0 or 1). We assume that a majority of the population is sophisticated,
meaning that they can identify bots and disregard fake news in their update process. There is
small proportion µu of individuals, on the other hand, that can be influenced by fake news. We
refer to them as unsophisticated. A key assumption is that individuals cannot back out the sources
of information of other agents. As a result, sophisticated agents may be influenced by fake news
indirectly through their social media friends.
       To the extent that policy is chosen democratically (via direct voting or through representa-
tives), the implemented policy may differ from the optimal one when bots are present. If a large
number of voters have homogeneous beliefs but are misinformed (that is, have beliefs far away
from θ), implemented policies will be inefficient. If, in addition, voters are polarized, then sub-
optimal delays in the response to shocks may arise. These result from gridlock or stalemate among
policymakers representing individuals with opposing views. The welfare losses arising from in-
formational frictions can be captured by a social welfare function S(M I, P ), which is decreasing
in the aggregate degree of misinformation M I and the societal level of polarization in beliefs,
denoted by P . Both M I and P depend critically on the distribution of voters’ opinions. We will
first describe how opinions evolve over time and then define how statistics obtained from this
distribution can be used to compute misinformation and polarization, and hence quantify welfare
losses associated to them.
       Each agent starts with a prior belief θi,0 assumed to follow a Beta distribution,
                                                                  
                                              θi,0 ∼ Be αi,0 , βi,0 .

       This distribution or world-view is characterized by initial parameters αi,0 > 0 and βi,0 > 0.
Note that individuals agree upon the parameter space Θ and the functional form of the probability
distribution, but have different world-views as they disagree on αi,0 and βi,0 . Given prior beliefs,
   3
       In most of the analysis we will focus on the limiting case T → ∞ to allow for convergence.



                                                          9
we define the initial opinion of agent i yi,0 about the true state of the world as her best guess of θ
given the available information,4
                                                                          αi,0
                                               yi,0 = E[θ|Σ0 ] =
                                                                      αi,0 + βi,0

where Σ0 = {αi,0 , βi,0 } denotes the information set available at time 0.

Example 1. In the Figure below, we depict the world-views of two individuals (distributions) and
their associated opinions (vertical lines). The world-view that is skewed to the right is represented
by the distribution Be(α = 2, β = 8). The one skewed to the left is represented by the distribution
Be(α = 8, β = 2). The opinions are, respectively, 0.2 and 0.8.
                                        4
                                        3
                                     f (θ)
                                       21
                                        0




                                             0.0    0.2   0.4        0.6    0.8     1.0
                                                                 θ




       We formalize the information obtained from unbiased sources as a draw si,t from a Bernoulli
distribution centered around the true state of the world θ,

                                                   si,t ∼ Bernoulli(θ).

       Through this channel, a majority of the population may learn θ in the limit. However, agents
update their world-views and opinions based not only on si,t , but also through the influence of
individuals connected to them in a social network, which may introduce misinformation. Social
media thus generates an externality on the information aggregation process. To the extent that
the social media externality is important, the true state of the world may not be uncovered by
enough individuals and inefficient policies may be enacted or gridlock may arise. The network
structure, and in particular the location of unsophisticated agents in it, will be important to de-
   4
       Note that E[θ|Σ0 ] is the Bayesian estimator of θ that minimizes the mean squared error given a Beta distribution.


                                                                10
termine the quality of information and the degree of polarization in society. We formalize the
social network structure next.


Social Network           The connectivity among agents in the network at each point in time t is de-
scribed by a directed graph Gt = (N, gt ), where gt is a real-valued n × n adjacency matrix. Each
regular element [gt ]ij in the directed-graph represents the connection between agents i and j at
time t. More precisely, [gt ]ij = 1 if i is paying attention to j (i.e. receiving information from) at
time t, and 0 otherwise. Since the graph is directed, it is possible that some agents pay attention to
others who are not necessarily paying attention to them, i.e. [gt ]ij 6= [gt ]ji . The out-neighborhood
of any agent i at any time t represents the set of agents that i is receiving information from, and
               out
is denoted by Ni,t = {j | [gt ]ij = 1}. Similarly, the in-neighborhood of any agent i at any time
               in
t, denoted by Ni,t = {j | [gt ]ji = 1}, represents the set of agents that are receiving information
from i (e.g. i’s audience or followers). We define a directed path in Gt from agent i to agent j
as a sequence of agents starting with i and ending with j such that each agent is a neighbour of
the next agent in the sequence. We say that a social network is strongly connected if there exists
a directed path from each agent to any other agent.
       In the spirit of AOP (2010) and ACFO (2012), we allow the connectivity of the network to
change stochastically over time. This structure captures rational inattention, incapacity of pro-
cessing all information, or impossibility to pay attention to all individuals in the agent’s social
clique. More specifically, for all t ≥ 1, we associate a clock to every directed link of the form (i,j)
in the initial adjacency matrix g0 to determine whether the link is activated or not at time t. The
ticking of all clocks at any time is dictated by i.i.d. samples from a Bernoulli Distribution with
fixed and common parameter ρ ∈ (0, 1], meaning that if the (i,j)-clock ticks at time t (realization
1 in the Bernoulli draw), then agent i receives information from agent j. Hence, the parameter ρ
measures the speed of communication in the network. The Bernoulli draws are represented by the
n × n matrix ct , with regular element [ct ]ij ∈ {0, 1}. Thus, the adjacency matrix of the network
evolves stochastically across time according to the equation

                                                    gt = g0 ◦ ct ,                                               (1)

where the initial structure of the network, represented by the initial adjacency matrix g0 , remains
unchanged.5
   5
       The notation ◦ denotes the Hadamard Product, or equivalently, the element-wise multiplication of the matrices.



                                                          11
Example 2 (Bernoulli Clock). Panel (2a) represents the original network and its adjacency matrix,
whereas Panel (2b) depicts a realization such that agent 1 does not pay attention to agents 2 and 4
in period 1. Agents 2 and 3, on the other hand, pay attention to agent 1 in both periods.

                        1                                                  1

                 2             3                                    2            3

                        4                                                  4
                                                                                  
                       0 1 0 1                                           0 0 0 0
                                                                            
                   1 0 0 0                                          1 0 0 0 
             g0 =                                              g1 = 
                                                                            
                                                                              
                   1 0 0 0                                          1 0 0 0 
                                                                            
                    0 0 0 0                                            0 0 0 0

           (a) Original Network at t = 0                     (b) Potential Network at t = 1

                        Figure 2: Bernoulli Clock and Network Dynamics


Evolution of Beliefs Before the beginning of each period, both sophisticated and unsophisti-
cated agents receive information from individuals in their out-neighbourhood, a set determined
by the realization of the clock in period t and the initial network. All agents share their opinions
and precisions, summarized by the shape parameters αi,t and βi,t . This representation aims at
capturing communication exchanges through social media feeds. At the beginning of every pe-
riod t, a signal profile is realized and an unbiased signal is privately observed by every regular
agent, whereas bots observe a biased signal (see details below). In this way, part of the information
obtained by unsophisticated agents from biased sources will be transmitted through the network
in the following period, as it is incorporated during the belief updating process and shared with
other agents in the network that naively internalize them.
   We now explain the update rule of regular agents (sophisticated and unsophisticated) and
bots. The full characterization of the update rules can be found in Appendix A.
   Internet bots (Fake News source)

   We assume that there are two types of Internet bots, a left wing bot (or L-bot) and right wing
bot (or R-bot), both with extreme views. Internet bot i produces a stream of fake news κspi,t , for
p ∈ {L, R}, where sLi,t = 0 for type L-bot and sR
                                                i,t = 1 for type R-bot for every t. The parameter


                                                 12
κ ∈ N+ measures the ability of bots to spread more than one fake-news article per period, which
can be interpreted as their flooding capacity (i.e. how fast they can produce fake news compared
to the regular flow of informative signals received by agents). Bots transmit the whole stream of
information to agents paying attention to them. Hence, a value of κ > 1 gives them more de-facto
weight in the updating rule of unsophisticated agents, emphasizing their degree of influence on
the network. We can model the bot update as

                                                p        p
                                               αi,t+1 = αi,t + κspi,t                                (2)
                                                p        p
                                               βi,t+1 = βi,t + κ − κspi,t .                          (3)

    Regular agents: sophisticated and unsophisticated

    Sophisticated and unsophisticated agents share the same update rule. The only thing that
distinguishes these agents is the composition of their neighborhood: while sophisticated agents
only pay attention to regular agents, unsophisticated agents devote some share of their atten-
tion to bots and for that will be exposed to fake news. After observing the signal from unbiased
sources, agents compute their Bayesian posteriors conditional on the observed signals. We as-
sume that parameters αi,t+1 and βi,t+1 are convex combinations between their Bayesian posterior
parameters and the weighted average of the neighbors’ parameters. In mathematical terms we
have that
                                                                           X
                             αi,t+1 = (1 − ωi,t )[αi,t + si,t+1 ] + ωi,t       [ĝt ]ij αj,t         (4)
                                                                           j

                                                                               X
                             βi,t+1 = (1 − ωi,t )[βi,t + 1 − si,t+1 ] + ωi,t         [ĝ]ij βj,t ,   (5)
                                                                                 j
                              P
where ωi,t = ω when             j   [gt ]ij > 0, and ωi,t = 0 otherwise.
    Note that this rule assumes that agents exchange information (i.e. αj,t and βj,t ) before pro-
cessing new signals si,t+1 .
    A regular agent’s full attention span is split between processing information from unbiased
sources, (1 − ωi,t ), and that provided by their friends in the network, ωi,t (e.g. reading a Facebook
                                                                                   P
or Twitter feed). If no friends are found in the neighborhood of agent i, j [ĝt ]ij = 0, then
the agent attaches weight 1 to the unbiased signal, behaving like a standard Bayesian agent.
Conversely, if at least one friend is found, this agent uses a common weight ω ∈ (0, 1). The term
              [gt ]ij
[ĝt ]ij =      out |
             |Ni,t
                        represents the weight given to the information received from her out-neighbor

                                                           13
j. As ωi,t approaches 1, the agent only incorporates information from social media, making her
update process closer to a DeGrootian in which individuals are purely conformists. In general, ω
can be interpreted as the degree of influence of social media friends.
        Finally, note we are assuming that the posterior distribution determining world-views of
agents will also be a Beta distribution with parameters αi,t+1 and βi,t+1 . Hence, an agent’s opinion
regarding the true state of the world at t can be computed as
                                                             αi,t
                                                yi,t =               .
                                                         αi,t + βi,t

        Our heuristic rules resembles the one in JMST (2012), but there are three important distinc-
tions. First, their adjacency matrix is fixed over time (homogeneous Markov chain), whereas
ours is stochastic (inhomogeneous Markov chain), an element we borrowed mainly from ACFO
(2013). Second, we restrict attention to a specific conjugated family (Beta-Bernoulli) and assume
that individuals exchange shape parameters αi,t and βi,t that characterize this distribution. So
the heuristic rule involves updating two real valued parameters, whereas JMST (2012)’s heuris-
tic rule involves a convex combination of the whole distribution function. Given their rule, the
posterior distribution may not belong to the same family as the prior distribution, as the con-
vex combination of two Beta distributions is not a Beta distribution. That is not the case in our
environment, as the posterior will also belong to the Beta distribution family. Finally, we are
considering the influence of fake news spread by bots and this feature is the main source of mis-
information. Therefore, to the extent that bots reach unsophisticated agents who are influential,
their presence will affect the existence and persistence of misinformation and polarization over
time. This is due to the fact that they will consistently communicate fake news (biased signals)
to some unsophisticated agents pushing them to extremes of the belief spectrum.6



3        Misinformation, Polarization, and Network Structure
An agent is misinformed when her beliefs are not close enough to the true state of the world θ.
We can define the degree of ‘misinformation’ in society as the average distance between opinions
    6
     We believe, even though we have not proved it, that the choice of modeling bots as agents in the network instead
of simply biased signals reaching a subset of agents comes without any costs to our findings. Moreover, the decision
of modeling bots as agents is similar to the idea of fanatics or stubborn agents in the spread of misinformation
literature. Thus, the potential benefit of modeling in this way is the possibility of making direct comparisons to the
current results in the literature. Finally, as pointed out by Gu, Kropotov, and Yarochkin (2016), fake news articles
sometimes are promoted in such a way that they appear to be spread by other users. In this sense, modeling bots as
agents seems to be a fair natural starting point. We get back to the resulting technical challenges later.


                                                          14
and the true state of the world θ.

Definition 1 (Misinformation). The degree of misinformation is given by
                                                   n
                                            1 X          2
                                     M It =       yi,t − θ .                                   (6)
                                            n i=1

Given an arbitrarily small value  > 0, the proportion of misinformed agents in the population is
given by
                                                     n
                                                 1X
                                     #M It =           1{|yi,t −θ|≥} .                        (7)
                                                 n i=1
   The degree of misinformation grows when a large number of agents are far from the true
state of the world. It is important to note that this is not measuring polarization. Moreover, it
is not even capturing the variance of opinions, as the average opinion in a given society may
be different from θ. For example, consider a network in which all agents believe that yi,t =
1 in period t. Then the variance of opinions is null (there is no polarization either), yet the
degree of misinformation M It will be large (at its maximum theoretical value). This variable
measures the ‘intensive margin’ of misinformation (e.g. how far society is from the truth). The
‘extensive margin’ is represented by #M It which considers the percentage of individuals who
are misinformed. In eq. (7), 1{·} represents an indicator function that returns 1 whenever the
condition within braces is met and 0 otherwise. These two definitions are related, as #M It = 0
implies no misinformation.
   We define a ‘wise society’ as one where there is no misinformation in the limit. Equivalently,
when the maximum distance between the limiting opinion of agents and θ is arbitrarily small, as
stated below.

Definition 2 (Wise society). We say that a society is wise, for a fixed  > 0, if
                                                          
                              lim P max |yi,t − θ| ≥  = 0.
                                t→∞        i≤n


   We base our notion of polarization on the seminal work by Esteban and Ray (1994), adapted
to the context of this environment. At each point in time, we partition the [0, 1] interval into
K ≤ n segments. Each segment represents significantly-sized groups of individuals with similar
opinions. We let the share of agents in each group k ∈ {1, ..., K} be denoted by πk,t , with
P
  k πk,t = 1.

   Esteban and Ray (1994)’s polarization measure aggregates both ‘identification’ and ‘alien-
ation’ across agents in the network. Identification between agents captures a sense of ideological

                                                    15
alignment: an individual feels a greater sense of identification if a large number of agents in so-
ciety shares his or her opinion about the true state of the world. In this sense, identification of
a citizen at any point in time is an increasing function of the share of individuals with a similar
opinion. The concept of identification captures the fact that intra-group opinion homogeneity
accentuates polarization. On the other hand, an individual feels alienated from other citizens
if their opinions diverge. The concept of alienation captures the fact that inter-group opinion
heterogeneity |ỹk,t − ỹl,t | amplifies polarization. Mathematically, we have the following repre-
sentation.

Definition 3 (Polarization). Polarization Pt aggregates the degrees of ‘identification’ and ‘alien-
ation’ across groups at each point in time.
                                        K X
                                        X K
                                                   1+ς
                                 Pt =             πk,t πl,t |ỹk,t − ỹl,t |                      (8)
                                        k=1 l=1

   where ς ∈ [0, 1.6] and ỹk,t is the average opinion of agents in group k and πk,t is the share of
agents in group k at time t.

   Clearly, a society with no polarization may be very misinformed, as described above. On the
other hand, we may observe a society in which there is a high degree of polarization but where
opinions are centered around θ, so their degree of misinformation may be relatively small. In the
latter case, individuals may be deadlocked on a policy choice despite relatively small differences in
opinion. In terms of welfare, both variables capture different dimensions of inefficiency. Because
of that, we are interested in characterizing both, misinformation and polarization in the limit,

                               M I = plim M It       and      P̄ = plim Pt .
                                        t→T                          t→T

   We can think of long-run misinformation and polarization as functions of: (i) the updating
process (clock speed ρ and influence of friends ω), (ii) the initial network structure g0 , and (iii)
the degree of influence of bots (flooding parameter κ, share µu and location of unsophisticated
agents on the network). More formally,


                    M I = MI(ρ, ω, µu , κ, g0 ) and          P̄ = P(ρ, ω, µu , κ, g0 ).

We aim at characterizing the properties of the functions P and MI. We will first show theoretical
results for the limiting case T → ∞ and then those obtained via computer simulations.


                                                    16
Non-influential Bots         The following two results show conditions under which misinformation
and polarization vanish in the limit. The first one is analogous to Sandroni et al (2012), whereas
the second one extends it to a network with dynamic link formation as in Acemoglu et al (2010).

Proposition 1. If the network G0 is strongly connected, the directed links are activated every period
(e.g., ρ = 1) and bots exert no influence, then the society is wise (i.e., all agents eventually learn the
true θ). As a consequence, both polarization and misinformation converge in probability to zero.

   When the network is strongly connected all opinions and signals eventually travel through the
network allowing agents to perfectly aggregate information. Since bots exert no influence (either
because there are no bots or because all agents are sophisticated), individuals share their private
signals who are jointly informative and eventually reach consensus (e.g. there is no polarization)
uncovering the true state of the world, θ.
   The result in Proposition (1) is in line with the findings in JMST (2012) despite the difference
in heuristic rules being used. Proposition (2) shows that the assumption of a fixed listening matrix
can be relaxed. In other words, even when Gt is not constant, the society is wise and polarization
vanishes in the long run in strongly connected networks.

Proposition 2. If the network G0 is strongly connected, bots exert no influence, then even when
the edges are not activated every period (i.e. ρ ∈ (0, 1)) society is wise. As a consequence, both
polarization and misinformation converge in probability to zero.


Influential Bots     Influential bots cause misinformation by spreading fake news. This does not,
however, necessarily imply that the society will exhibit polarization. The following example de-
picts two networks with three agents each: a sophisticated one (node 3) and two unsophisticated
ones (nodes 1 and 2) who are influenced by only one bot—L-bot in panel (3a) and R-bot in the
panel (3b)—.

                         L                                                    R

                 1               2                                    1               2

                         3                                                    3
          (a) Society influenced by L-bot                      (b) Society influenced by R-bot

                              Figure 3: Two societies with internet bots

                                                   17
   Polarization in both societies converges to zero in the long-run. However, neither society is
wise. This illustrates that the influence of bots may generate misinformation in the long run,
preventing agents from uncovering θ, but does not necessarily create polarization. This insight
is formalized in Proposition (3).

Proposition 3. If a society is wise, then it experiences no social polarization in the long run. The
converse is not true.

   More generally, when the relative influence of one type of bot is significantly larger than the
other, it is possible for a society to reach consensus (i.e. experience no polarization of opinions) to
a value of θ that is incorrect. This can happen when there is a sufficient amount of unsophisticated
agents or when these unsophisticated agents, even if few, reach a large part of the network (i.e.
when they are themselves influential). It is also necessary that one of the bots can reach a larger
number of unsophisticated agents than the other; the example presented in Figure (3) is extreme
in that one bot is influential whereas the other one is not. But, as we will see in Section (5), a
society may converge to the wrong θ under less extreme assumptions. In order for a society to
be polarized, individuals need to be sufficiently exposed to bots with opposing views.

                                                                         0.40




                                                                         0.35
                                                          Polarization




        L          1         2          R                                0.30




                                                                         0.25




                                                                         0.20

                                                                                0   100      200          300   400   500

                                                                                                   Time


       (a) Society with both L-bot and R-bot                                              (b) Cycles

                             Figure 4: Two societies with internet bots


   Consider the social network of two agents depicted in Figure (4a), in which both bot-types
are present: agent 1 is influenced by the L-bot whereas agent 2 is influenced by the R-bot. Even
though unsophisticated agents 1 and 2 receive unbiased signals and communicate with each other,
this society exhibits polarization in the long run. This happens because bots subject to different
biases are influential. The degree of misinformation may be lower than in the previous example


                                                  18
(as opinions end up being averaged out and potentially closer to the true state of the world), but to
the extent policy is chosen by majority voting may still lead to inaction and hence inefficiencies.
    A noticeable characteristic of the evolution of Pt over time, depicted in Panel (4b) of Figure
(4), is that rather than settling at a constant positive value, it fluctuates in the interval [0.2,0.4].
The example illustrates that polarization cycles are possible in this environment.

                                                    0.30




                                                    0.25
                              Social Polarization




                                                    0.20




                                                    0.15




                                                    0.10


                                                           0   100   200           300   400   500
                                                                            Time




                                                     Figure 5: Different polarization levels


    Finally, we want to point out that whether misinformation and polarization increase, decrease,
or fluctuate over time depend importantly on the topology of the network, the number and degree
of influence of bots, the frequency of meetings between individuals (e.g. the clock) and the degree
of rationality of agents. Figure (5) depicts the behavior of Pt over time for a series of larger random
networks (e.g. there are 100 nodes, an arbitrary number of bots, and different rationality levels).
The next section is devoted to uncovering what drives these different dynamics.



4    Numerical Simulation
One of the biggest challenges when using network analysis is to ascertain analytical closed forms
and tractability for our objects of interest, namely MI and P. The combinatorial nature of social
networks that exhibit a high degree of heterogeneity makes them very complex objects, imposing
a natural challenge for theoretical analysis. In our work, limiting properties can be characterized
only when we assume strong connectivity and absence of internet bots’ influence. As we drop
these assumptions, we observe that different networks might experience different limiting mis-
information and polarization levels, even if departing from the very same belief distribution.


                                                                           19
   To understand such differences, we resort to numerical methods where a large number M
of random networks is generated and limiting properties of the distribution of beliefs, namely
long run average misinformation and polarization, are computed through simulations. In Section
(4.1) we describe the algorithm used to generate the initial network g0 from a combination of
the Barabasi-Albert and the Erdos-Renyi random graph models. In Section (4.2) we describe the
simulation exercise in which key parameters determining the initial network, the location of
unsophisticated agents in it, the communication technology, the influence of bots, and update
process are drawn to generate a synthetic sample of social media networks. In each network
j ∈ {1, ..., M }, we simulate the evolution of beliefs and compute the limiting misinformation
and polarization measures, M I j and P j . In Section (5) we characterize how MI and P depend
on key statistics of the social network topology and parameters determining the learning process
by estimating the conditional expectation of M I j and P j conditional on the parameters.


4.1    Network Algorithm
Here we describe the algorithm used to create and populate each initial network G0j in our sample,
and define a series of statistics which characterize its topology.


Generating g0 :    Social networks have two important characteristics. First, there is reciprocity
in the sense that the exchange of information among individuals is bi-directional. Second, some
agents are more influential than others (that is, they have a larger in-neighborhood). Barabasi
and Albert’s (1999) random graph model has the preferential attachment property, generating a
few nodes in the network which are very popular relative to others. This is illustrated in Figure
(6a) for a network with n = 21 nodes. Unfortunately, it exhibits no reciprocity. Their algorithm
better captures characteristics of broadcasting, where newspapers, tv, or radio stations (e.g. nodes
1, 3, and 8) send a signal received by a large—and passive—audience. The Erdos-Renyi’s random
graph model, on the other hand, allows for reciprocity but presumes that all agents have similar
degrees of influence, as can be seen in Figure (6b).
   Because we want our exercise to incorporate both characteristics, the initial network is con-
structed as the union of these two random graph models. More specifically, we first create a
random graph with n nodes using the Barabasi-Albert algorithm. We then create another one
(also with n nodes) following Erdos-Renyi’s algorithm. Finally, we combine them into a “BAUER
network,” which is simply the union of these two graphs. The resulting network is illustrated in


                                                 20
Figure (6c). We can see that information flows in both directions (e.g. there is reciprocity) and
that some agents are more influential than others (e.g. there is preferential attachment).

                  ●
                  18
                                                                                              ●18
                                                                                                                                                                        ●
                                                                                                                                                                        18




           ●
           21
                                            ●   5
                                                    ●    16                              ●
                                                                                         21
                                                                                                                         ●   5
                                                                                                                                 ●    16                           ●
                                                                                                                                                                   21
                                                                                                                                                                                                  ●   5
                                                                                                                                                                                                          ●    16

                                ●   2
                                                                        ●
                                                                        15                                   ●   2
                                                                                                                                                     ●
                                                                                                                                                     15                               ●   2
                                                                                                                                                                                                                              ●
                                                                                                                                                                                                                              15




                                ●
                                1                             ●4
                                                                                                             ●
                                                                                                             1                             ●4
                                                                                                                                                                                      ●
                                                                                                                                                                                      1                             ●4


                                                                             ●
                                                                             17
                                                                                                                                                          ●
                                                                                                                                                          17
                                                                                                                                                                                                                                   ●
                                                                                                                                                                                                                                   17


                   ●   6
                                                     ●    3
                                                                                               ●    6
                                                                                                                                  ●    3
                                                                                                                                                                         ●   6
                                                                                                                                                                                                           ●    3



       ●
       7

                  ●
                  12                    ●
                                        9
                                                                   ●
                                                                   14                ●
                                                                                     7

                                                                                              ●
                                                                                              12                     ●
                                                                                                                     9
                                                                                                                                                ●
                                                                                                                                                14             ●
                                                                                                                                                               7

                                                                                                                                                                        ●
                                                                                                                                                                        12                    ●
                                                                                                                                                                                              9
                                                                                                                                                                                                                         ●
                                                                                                                                                                                                                         14

                           ●
                           20
                                                                                                        ●
                                                                                                        20
                                                                                                                                                                                 ●
                                                                                                                                                                                 20



                                            ●
                                            8
                                                                   ●
                                                                   13
                                                                                                                         ●
                                                                                                                         8
                                                                                                                                                ●
                                                                                                                                                13
                                                                                                                                                                                                  ●
                                                                                                                                                                                                  8
                                                                                                                                                                                                                         ●
                                                                                                                                                                                                                         13




                                ●
                                19
                                                                                                             ●
                                                                                                             19
                                                                                                                                                                                      ●
                                                                                                                                                                                      19




                                                    ●
                                                    11
                                                                                                                                 ●
                                                                                                                                 11
                                                                                                                                                                                                          ●
                                                                                                                                                                                                          11




                                                              ●
                                                              10
                                                                                                                                           ●
                                                                                                                                           10
                                                                                                                                                                                                                    ●
                                                                                                                                                                                                                    10




                (a) Barabasi-Albert                                                           (b) Erdos-Renyi                                                                    (c) BAUER

                                                                                  Figure 6: Random graph models


       There are five key parameters affecting the topology of g0 in the BAUER model, defined by
the set Ω = {m, α, a, p, n}. Three of them are associated to the construction of the Barabasi-
Albert network, namely, the number of meetings of incoming nodes m, the power of preferential
attachment α, and the attractiveness of nodes with no adjacent edges a. The last two define
P [i] = k[i]α + a, the probability that an old node i is chosen to be linked to a new node at each
iteration of the network formation algorithm of Barabasi and Albert. The parameter p represents
the probability of drawing an edge between two arbitrary nodes in the Erdos-Renyi algorithm.
Finally, n denotes the number of nodes which determines the size of the network. When creating
our dataset, we fix n and vary the remaining four parameters.


Populating g0 :                                 We populate each network with two types of agents, sophisticated and unso-
phisticated, and define which unsophisticated agent is influenced by the L-bot and the R-bot. We
do this in two steps. First, using a uniform distribution we randomly select a number u = µu n
of agents. This defines the location of unsophisticated agents in g0 . Note that from an ex-ante
perspective, every node in the network has the same probability of being populated by an unso-
phisticated agent. Second, we assign a probability 0.5 to each unsophisticated agent to receive
signals from L-bot (exclusively). The remaining unsophisticated agents are assumed to receive
signals from the R-bot. This ensures that, on average, bots’ messages reach the same number of
unsophisticated agents.7 This does not imply, however, that bots will have the same degree of
   7
     We experimented allowing some unsophisticated agents to follow both bots at the same time but it proved to be
too complicated compared to the little gain of resulting insights.

                                                                                                                     21
influence, as the audience of unsophisticated agents may differ depending on their location in the
network. The remaining individuals, n − u, are assigned the update rule in eq. ((5)).


4.2         Generating the dataset
We fix the number of agents (or nodes) to n = 37 and the true state of the world at θ = 0.5.
We also fix the initial distribution of beliefs so that the same mass of the total population lies
in the middle point of each one of 7 groups. This rule basically distributes our agents evenly
                                                                                                            1
over the political spectrum [0, 1] such that each of the 7 groups contains exactly                          7
                                                                                                                of the total
mass of agents. Moreover, we set the same variance for each agent world-view to be σ 2 = 0.03.
With both opinion and variance, we are able to compute the initial parameter vector (α0 , β0 ).
8
    Given these parameters, we draw and populate a large number M = 8, 248 of initial random
networks g0 following the BAUER model described in Section (4.1). To generate heterogeneity,
we consider values for m (number of meetings) in the set {1, .., 5}, the preferential attachment α
in [0.5, 1.5], the attractiveness of nodes with no adjacent edges a ∈ {1, ...4}, and the probability
p ∈ [0.01, 0.1].


Characteristics of g0 :            Since the process of generating g0 and populating it with unsophisti-
cated agents involves randomness, a given set of parameters Ω may lead to significantly different
network graphs g0 . These graphs can be characterized by a series of standard network statistics,
such as diameter, average clustering, and reciprocity. Average values across the M networks are
reported in Table (1), which contrasts them to those observed on real-life social media networks
such as Twitter, Facebook and Google +.

                                                Table 1: Network Topology


                                                    Simulation    Twitter   Facebook   Google +
                                  Diameter            6.5              7       6         8
                                  Reciprocity        0.04         0.03
                                  Avg Clustering     0.36         0.57       0.49       0.6




         Diameter captures the shortest distance between the two most distant nodes in the network.
The average diameter in our network is 6.5, very much in line with the values observed in real-life
                                                                        α         2        αβ
     8
         In this case, we only need to use the relationships µ =       α+β and σ = (α+β)2 (α+β+1)   to fully determine α and
                                                  2    2                   2  2
                                                         −µ)
β. Algebraic manipulation yields α =         − µ(σ +µ
                                                    σ2         and β   = (σ +µ σ−µ)(µ−1)
                                                                                 2       .

                                                                  22
networks. Reciprocity is the proportion of all possible pairs (i, j) which are reciprocal (e.g. have
edges between them in both directions), provided there is at least one edge between i and j,
                                                           >
                                          P P                
                                             i  j  g0 ◦  g 0 ij
                                R(g0 ) =       P P
                                                 i   j gij

Our synthetic sample mean of 0.04 is similar to that observed in Twitter (reciprocity cannot be
computed in Facebook and Google + because it is impossible to back out who is following whom).
Average clustering captures the tendency to form circles in which one’s friends are friends with
each other. We use an extension to directed graphs of the clustering coefficient proposed by
Fagiolo (2007), defined as the average, over all nodes i, of the nodes-specific clustering coefficients
                                                              3
                                      1X             g0 + g0> ii
                            cl(g0 ) =                                         ,
                                      n i 2 (Ditot (Ditot − 1) − 2 (g02 )ii )

   where Ditot is the total degree, i.e. in-degree plus out-degree, of agent i. Average clustering
across networks is 0.36, somewhat smaller than values observed in real-life networks as seen in
Table (1). Finally, we can also compute the initial homophily of opinions of agents, measured by
an assortativity coefficient, as in Newman (2003), which takes positive values (maximum 1) if
nodes with similar opinion tend to connect to each other, and negative (minimum −1) otherwise.
In the sample, initial homophily ranges from −0.4 and 0.5, with an average value of −0.03.
Notice though, that the degree of homophily in the long-run is endogenously determined. In an
environment with no bots, for example, all agents converge to the same opinion (in which case
limiting homophily is 1).



Influence of unsophisticated agents:           The amount and location of unsophisticated agents in
g0 are important determinants of limiting M I and P , as it is through them that fake news spread
in the network. If the bot manages to manipulate an unsophisticated agent who is very influ-
ential (e.g. central), it may be able to effectively affect the opinion of others. We vary the share
of unsophisticated agents by drawing µu from a uniform distribution in the interval [0.1, 0.4].
The average share on the sample is 0.25 as shown in Table (2) with a maximum percentage of
unsophisticated agents of 40% and a minimum of 10%.




                                                    23
                                 Table 2: Centrality of unsophisticated agents



                                                         Mean     Std Dev.   Min    Max
                              Share of unsophisticated    0.250     0.100    0.1    0.4
                              in-Degree                   0.12      0.06     0      0.54
                              out-Degree                  0.15      0.05     0.03   0.3
                              PageRank                    0.03      0.01     0.01   0.11
                              in-Closeness                0.18      0.06     0.01   0.29
                              out-Closeness               0.22      0.14     0.01   0.56
                              Betweenness                 0.05      0.02     0      0.24




       There are several statistics in the literature that can proxy for the degree of influence or cen-
trality of an unsophisticated agent. Degree is the simplest centrality measure, which consists
on counting the number of neighbors an agent has. The in-degree is defined as the number of
incoming links to a given unsophisticated agent,
                                                          1 X
                                                Diin =         [g0 ]ji .
                                                         n−1 j

The out-degree is defined analogously. Table (2) reports the average in-Degree and out-Degree
of all unsophisticated agents in the network (regardless of which type of bot influences them).
The average in-degree is 0.12. There is a large dispersion across networks, with cases in which
unsophisticated agents are followed by about 54% of agents in the network. Out-degree of these
agents is on average 0.15. A larger value of this measure increases the influence of friends in the
network for each given unsophisticated agent, reducing the influence of bots.
       While this measure of influence is intuitive, it is not necessarily the only way in which a bot
can be efficient at manipulating opinion, and hence affecting misinformation and polarization.
There are networks in which unsophisticated agents have very few followers (and hence a low
in-degree) but each of their followers is very influential. An alternative measure of centrality
that incorporates these indirect effects is Google’s PageRank centrality.9 PageRank tries to ac-
count not only by quantity (e.g. a node with more incoming links is more influential) but also by
quality (a node with a link from a node which is known to be very influential is also influential).
   9
       This measure is a variant of eigenvector centrality, also commonly used in network analysis.




                                                            24
Mathematically, the PageRank centrality P Ri of a node i is represented by
                                                         X [g0 ]ji                1−ν
                                           P Ri = α                    P Rj +         ,
                                                          j
                                                              Djout                n

       where Djout is the out-degree of node j if such degree is positive and ν is the damping factor.10
Note that the PageRank of agent i depends on the PageRank of its followers in the recursion above.
Summary statistics for the average PageRank of unsophisticated agents are shown in Table (2).
       An alternative measure of centrality is given by closeness centrality. This measure keeps track
of how close a given agent is to each other node in the network. High proximity of the unsophis-
ticated agent to all other agents in the network makes the bot more efficient in spreading fake
news, as they reach their targeted audience more quickly. To compute closeness, we first mea-
sure the mean distance between the unsophisticated agent and every other agent in the network.
Define dji as the length of the shortest path from agent j to unsophisticated i in the network
G0 .11 In-closeness centrality is defined as the inverse of the mean distance dji across agents to
reach unsophisticated agent i,
                                                                       n
                                                         Ciin = P            .
                                                                       j dji

Out-closeness is similarly defined. Finally, betweenness centrality measures the frequency at
which a node acts as a bridge along the shortest path between two other nodes. Statistics for
these measures can be found in the last two rows of Table (2).

                                Table 3: Correlation across centrality measures


                               in-Degree    out-Degree    in-Closeness     out-Closeness   PageRank   Betweenness


               in-Degree          1
               out-Degree         0.54         1
               in-Closeness       0.63         0.65           1
               out-Closeness      0.43         0.66           0.78               1
               PageRank           0.64        −0.005          0.33               0.06        1
               Betweenness        0.18         0.004          0.33               0.09        0.41         1




  10
      The damping factor tries to mitigate two natural limitations of this centrality measure. First, an agent can get
“stuck” at the nodes that have no outgoing links (bots) and, second, nodes with no incoming links are never visited.
The value of ν = 0.85 is standard in the literature and it is the one we will use in the simulations.
   11
      In many networks sometimes one agent may find more than one path to reach the unsophisticated agent). In
such case, the shortest path is the minimum distance among all possible distances.




                                                                  25
   It is worth noticing that even though all of these are alternative measures of centrality, they
capture slightly different concepts. An unsophisticated agent is central according to in-degree
when it has a large number of followers, whereas she is central according betweenness if she
is in the information path of many agents. The correlation between these two variables is just
0.18, as seen in Table (3) which reports the correlation coefficient across centrality measures in
our sample. Moreover, the correlation of betweeness and almost all other centrality measures is
relatively low. Note that even though the relationship between in-degree and PageRank is 64%
and the correlation between in-degree and in-closeness is 63%, PageRank and in-closeness have
a relatively low correlation of 0.33. This illustrates the challenges in finding a minimum set of
measures to characterize influence in a random graph model.


Belief heterogeneity:      The variability in the behavioral dimension is given by changes in the
parameter ω, capturing the degree to which agents rely more or less heavily on the opinion of
others. We draw ω from a Uniform distribution on the interval (0.1, 1] with band-with 0.05. The
average value og ω is 0.54, with a standard deviation of 0.29 in our sample. These are reported
in Table (4).

                          Table 4: Belief heterogeneity and bot influence



                                                    Mean      Std Dev.   Min     Max


                         Influence of friends ω      0.55       0.29     0.10    1
                         Speed of communication ρ    0.54       0.26     0.10    1
                         Flooding capacity κ        92        109        1      300




   The two parameters capturing communication technology are ρ, which controls the speed at
which links are activated and κ, which determines the ability of bots to flood the network with
fake news. We draw ρ from a uniform distribution in (0, 1]. Its average value is 0.54. A standard
deviation of 0.29 ensures that there is significant variability in the artificial dataset. Note that we
are excluding cases in which nodes are never activated, ρ = 0, as the network would exhibit no
dynamics.
   We consider alternative values for the flooding parameter, κ ∈ {1, 10, 50, 100, 300}. The av-
erage number of signals sent by each bot in our sample is 92, with a minimum of one (which is
the number of signals sent by unsophisticated agents per encounter) and a maximum of 300, indi-

                                                         26
cating that bots can place fake news 300 times faster than an unbiased source of news per period.
Note that increasing κ is analogous to increasing the number of bots in the model (assuming the
number and location of unsophisticated agents does not change).


4.3    Simulation
We simulate communication in each social media network for a large number of periods (T =
2000) and use the resulting opinions to compute misinformation and polarization. For each net-
work j, we draw a signal sji,t for individual i ∈ N at time t ≤ T from a Bernoulli distribution with
parameter θ = 0.5. We also draw the n×n matrix ct at each period t from a Bernoulli distribution
with parameter ρj , which determines the evolution of the network structure according to eq. (1).
Together, the signals and the clock determine the evolution of world-views according to eqs. (4)
and (5). With these, for each network j, we compute a time series for opinions yi,t,j for individual
i at period t. Figure (7) displays the distribution of average opinions y j across networks. We
define y j as
                                                                      n
                                                         1 1 X X
                                                yj =                     yi,t,j ,
                                                        500 n t≥1500 i=1

We choose this threshold because simulations converge after about 500 periods to an ergodic set
(most statistics and results are unchanged when using the last 200 periods instead).

                                                            Average opinion
                                      1.5
                                      1
                            Density
                                      .5
                                      0




                                            0      .2         .4                 .6   .8   1
                                                                   Avg opinion




                                                Figure 7: Average opinions


    There is a large number of networks in which individuals are on average correct (e.g. close
to θ = 0.5), but there exists dispersion around this value (the standard deviation of y j across
networks is reported in Table (5)). Moreover, there is a non-trivial amount of networks in which
agents’ opinions become extreme implying that bots are successful at manipulating options to-

                                                                   27
wards their own.
   Our variables of interest are the long-run degree of misinformation M I j , the share of misin-
formed agents #M I j , and limiting polarization P̄j in each network.
   The degree of misinformation is simply,
                                                    1X
                                           M Ij ≡       (yi,j,T − θ)2 .
                                                    n i

where yi,j,T is the opinion of agent i at the last period T = 2000. The average degree of long-run
misinformation across all networks, reported in Table (5), is 0.09, with a standard deviation of 0.07.
The distribution of misinformation across networks is depicted in Figure (8). It is skewed to the
left, indicating that there is a significant amount of networks with low degrees of misinformation.
However, there is some mass around 0.25, which constitutes the theoretical upper bound for this
measure. There are 147 networks in which M I j ∼ 0.25, so individuals are fully misinformed.

                                           Table 5: Simulation results



                                                 Mean          Std Dev.   Min      Max


                        Average opinion y j         0.50         0.28     0        1
                        Misinformation M I j        0.09         0.07     0.0006   0.25
                        % Misinformed #M I j        0.97         0.045    0.26     1
                        Polarization P j            0.11         0.09     0        0.64




   Computing the proportion of misinformed individuals is challenging in this environment. The
reason being that there are very few informed individuals at any given point in time. Moreover,
small shocks (e.g. signals from either biased or unbiased sources) can easily move opinions 
away from the true θ. Instead of simply computing the average percentage of individuals in the
last period (as we did with misinformation), we compute the smallest proportion of misinformed
individuals over an interval [1500, 2000]. Mathematically,

                                           #M I j ≡ max{#M Ij,t },
                                                           t


where #Mj,t is calculated from eq. ((6)) assuming  = 0.00025 (we do not assume  = 0 to al-
low for computational rounding error). In other words, we restrict attention to an interval in the
long-run and look for the lowest number of individuals who are misinformed in that period (or

                                                           28
equivalently, the largest percentage of wise agents). Even under this very generous definition,
the proportion of misinformed agents is on average 0.97 across simulations, implying that most
individuals in the network have opinions which are  away from θ = 0.5. In 30% of our networks
all individuals are misinformed, whereas in about 40% of the cases only one or two agents learn
the true θ. The lowest value for #M I j in our sample is 0.26 (i.e. only 26% of agents are misin-
formed). The result is not driven by high values of the flood parameter or a high proportion of
unsophisticated agents in the network, as #M I ∼ 0.96 when κ = 1 and µu = 0.1. These values
are likely to result from the fact that we are considering small networks (there are only 37 nodes
in them), in which at least 4 agents are unsophisticated, so misinformation is bound to be large
by construction. In addition, we are ignoring cases in which agents are fairly Bayesian, as the
smallest value of ω is 0.1 in the simulations.

                                  Misinformation                                             Average polarization


                                                                            8
              10




                                                                            6
           Density




                                                                         Density
                                                                           4
           5




                                                                            2
              0




                                                                            0




                     0      .05    .1            .15   .2     .25                  0         .2                      .4   .6
                                    Misinformation                                                Avg polarization




                         Figure 8: Misinformation                                      Figure 9: Polarization


       For each network j, and given ς = 0.5, we compute
                                                                1 X
                                                       Pj ≡                Pj,t ,
                                                              500p̂ t≥1500

which is normalized by p̂ to belong to the interval [0, 1]. 12 About 98% of our sample exhibits pos-
itive polarization in the long run. Figure (9) depicts the distribution of polarization resulting from
our simulation exercise. There is a significant degree of variability in our sample, even though
the polarization levels are relatively small, with most P̄j observations lying below 0.5 (recall that
maximum polarization has been normalized to 1, yet the maximum polarization level observed
in our sample is just 0.64). The average value of P̄j across networks is 0.11, with a standard
  12
     With ς = 0.5 the maximum possible level of polarization (theoretically) is p̂ = 0.707. We divide all values of
polarization by this number to normalize the upper bound to 1. This is without loss of generality and aims at easing
interpretation.



                                                                    29
deviation of 0.09. Interestingly, we also observe some mass near 0, indicating that agents reach
quasi-consensus. Unfortunately, most of these cases involves consensus around extreme values
of θ rather than efficient aggregation of information to the true θ. Figure (10) shows a scatterplot
of misinformation and polarization in networks with a small number of unsophisticated agents
µu = 0.1. We see that there is an inverted U-shape relationship between these variables. Polar-
ization can be low either because individuals learned the true (e.g low misinformation) or because
they converged to the wrong value of θ. There are, however, situations where misinformation is
relatively low but polarization is extremely high.

                                                  Misinformation and polarization
                                   .6  .4
                            Polarization
                                   .2
                                   0




                                            0   .05        .1            .15    .2   .25
                                                            Misinformation




Figure 10: Relationship between misinformation and polarization (with 4 unsophisticated agents)



5     Regression Analysis
We are interested in estimating the effect of network characteristics on limiting misinformation
and polarization. To assess the quantitative importance of each explanatory variable, we estimate
the coefficients of an OLS model,
                                                      Yj = Xj β + j .                           (9)

    where the M × 1 vector Yj denotes either long-run misinformation M I j or polarization Pj
obtained from simulation j ∈ {1 . . . M }, Xj denotes the matrix of network characteristics per
simulation j, and j is the error term.


5.1   Benchmark case
The set of explanatory variables includes BAUER parameters in Ω, parameters of the heuristic
update rule and the technology of communication (see Table (4)), as well as those characterizing


                                                             30
the network topology (see Table (1)) and the average centrality of unsophisticated agents (see
Table (2)). In addition, we also consider the relative influence of bots. An R-bot may create more
misinformation if it is more effective at manipulating influential individuals than the L-bot. We
compute ‘relative centrality’ as the absolute value of the difference in the centrality measure of
individuals who are influenced by bots of different types. For example, in the case of PageRank,
we compute


                     Relative PageRank = |P ageRank(L) − P ageRank(R)|.

Other relative measures of centrality are computed analogously.
   The results are summarized in Table (6), where we omit the estimated coefficients on reci-
procity and homophily as they are statistically insignificant across regressions models. In the
first two columns, variables have been normalized by their sample standard deviation in order
to simplify the interpretation of coefficients and ease comparison across covariates. Hence, each
estimated coefficient represents by how many standard deviations (st devs) misinformation or
polarization change when the respective independent variable increases by one standard devia-
tion.
   The positive coefficient on ω implies that as agents place more weight on the opinions of
their social media friends (and less on the unbiased signal), both misinformation (column 1) and
polarization (column 2) rise. This is expected as higher values of this variable increases (indirectly)
the influence of bots. The effect on polarization is larger, as it rises by 0.14 st devs, whereas
misinformation only rises by 0.08 standard deviations.
   The overall effect of a higher clock parameter ρ is a priori ambiguous: on the one hand, it is
more likely that a sophisticated agent will incorporate fake news from those paying attention to
the extreme views of bots as the speed of communication rises; on the other hand, a faster flow of
information allows agents to incorporate unbiased private signals obtained by friends at a faster
rate. Under the current specification, we find that misinformation declines with ρ, suggesting
that the effect of internalizing a larger number of opinions outweighs the effect of higher fake
news exposure. Clearly, this result would change in larger networks or those with a smaller share
of unsophisticated agents in them. The effect of the clock on polarization is much stronger, as P̄
declines by 0.15 st devs when ρ rises by one st deviation.
   In terms of the network topology, we find that larger networks (measured by their diameter)
are associated with higher misinformation and polarization, whereas high clustering is important


                                                  31
                                Table 6: Regression results: Benchmark case
                                                       Misinformation     Polarization   Average opinion
                                                             (1)              (2)              (3)

                    Communication technology

                    Influence of friends ω                 0.08***          0.14***           0.002
                                                            (0.009)          (0.007)         (0.007)
                    Speed of communication ρ               -0.05***         -0.15***         -0.004
                                                            (0.008)          (0.007)         (0.007)

                    Network Topology

                    Diameter                               0.05***           0.006           -0.003
                                                            (0.01)           (0.01)          (0.01)
                    Clustering                               0.01           -0.16***         -0.006
                                                            (0.02)           (0.02)          (0.02)
                    Ω                                        yes              yes             yes

                    Bot influence

                    Share of unsophisticated µ                0                +               0

                    Flooding κ                                +                +               0

                    in-Degree                               0.13***          -0.003
                                                            (0.023)          (0.018)
                    PageRank                               -0.44***          0.26***
                                                             (0.02)           (0.02)
                    in-Closeness                           -0.10***         -0.70***
                                                             (0.02)           (0.02)
                    Betweeness                              0.05***           0.02*
                                                            (0.016)          (0.013)
                    out-Degree                             -0.08***           -0.03
                                                            (0.024)          (0.019)
                    Out-Closeness                           -0.002          0.06***
                                                            (0.019)          (0.015)
                    Relative PageRank                       0.62***         -0.21***          31***
                                                            (0.017)          (0.014)          (0.61)
                    Relative in-Degree                     -0.05***          0.09***          0.22*
                                                            (0.016)          (0.013)          (0.12)
                    Relative Betweeness                    -0.10***           0.02*          -6.4***
                                                             (0.01)           (0.01)          (0.24)
                    Relative in-Closeness                   0.14***         -0.24***         3.8***
                                                            (0.013)           (0.01)          (0.19)


                    Observations                            8,248             8,248           8,248
                    R-squared                                0.39              0.60            0.57
                                             Robust standard errors in parentheses
                                                *** p<0.01, ** p<0.05, * p<0.1



for polarization (it actually reduces it) but irrelevant for misinformation. The negative coefficient
on clustering suggests that the implied higher connectivity reached with higher clustering coun-
tervails the bias reinforcement associated with echo-chambers (Sunstein 2002, 2009). The effects
of initial homophily on misinformation and polarization vanish over time, as their coefficient is
statistically insignificant (result omitted from the table).
   The coefficients associated with the parameters Ω are not reported to make the exposition



                                                              32
of the regression results clearer, but are available upon request. It is worth mentioning that
once centrality measures and network topology statistics are included, most of these parameters
become statistically insignificant.13
       The most interesting results are those related to measures of centrality. When unsophisticated
agents have a large number of followers in social media, misinformation rises: the coefficient of
in-Degree, reported on Table (6), is 0.13 and statistically different from zero. The effect on polar-
ization is statistically insignificant. Keeping the number of followers constant, we can consider
how unsophisticated agents who are followed by influential agents affect our variables of inter-
est through the coefficient of PageRank (see definition on Section (4.2)). Interestingly, a higher
PageRank is associated with less misinformation but more polarization. A one st dev increase
in PageRank is associated with a reduction of 0.44 st devs in misinformation and an increase of
0.26 st devs in polarization, both quantitatively more important than in-Degree. This suggests
that when unsophisticated agents are relatively more influential (because they manage to affect
the opinions of influential followers), information is more efficiently aggregated. However, to
the extent that agents do not fully learn the true θ, there is a significant amount of networks in
which opinions become extreme. This is illustrated in the left panel of Figure (11), which depicts
the distribution of average opinions across networks for PageRank below average. We see that
this is basically a three point distribution (e.g., y j = 0, y j = 0.5, and y j = 1) with small mass in
intermediate values. The right panel of that figure shows y j for PageRank above average. The
distribution is much more uniform in that case. There is a significantly smaller mass at θ (e.g.
more misinformation) but also at the extremes (indicating less polarization).

                                  PageRank below mean                                           PageRank above mean
                                                                                 1.5
                     3
                     2




                                                                                 1
           Density




                                                                       Density
                                                                                 .5
                     1
                     0




                                                                                 0




                         0   .2        .4           .6   .8   1                        0   .2       .4                 .6   .8   1
                                        Avg opinion                                                      Avg opinion




                                     Figure 11: Average opinion for different PageRank
  13
     Only a and α, determining the preferential attachment (and hence influence of any given node), are significant
for polarization. This would suggest that our set of centrality measures may not be sufficient to capture influence in
a network.



                                                                  33
   It is useful to analyze this coefficient together with that of Relative PageRank, which is positive
for misinformation and negative for polarization. As unsophisticated agents following the R-bot
become relatively more influential (e.g. P ageRank(R) rises) they tend to pull opinions towards
1. This is clearly seen in first panel in Figure (12) which depicts limiting values of average opinion
for networks in which P ageRank(R) > P ageRank(L). The distribution is clearly skewed to
the right (analogously, when P ageRank(R) < P ageRank(L) the distribution is skewed to the
left). In column 3 of Table (6) we present the results of an estimated regression equation similar
to eq. ((9)), in which the dependent variable is average opinions, ȳj , and we consider relative
centrality as the difference between unsophisticated agents following R vs those following L (nor
in absolute value neither normalized by its standard deviation, so that a higher centrality of R
would be associated with ȳj closer to 1). Interestingly, the only coefficients which are statistically
significant are those related to relative centrality.

                              Average opinion, PageRank advantage                                              Average opinion, in-Closeness advantage
           2




                                                                                             2
           1.5




                                                                                             1.5
        Density




                                                                                          Density
          1




                                                                                            1
           .5




                                                                                             .5
           0




                                                                                             0




                  0               .2               .4                  .6   .8   1                   0                .2                .4                 .6    .8   1
                                                        Avg opinion                                                                          Avg opinion
                  Note: Computed for PageRank(R-bot)>PageRank(L-bot)                                Note: Computed for in-Closeness(R-bot)>in-Closeness(L-bot)




                      Figure 12: Average opinion for alternative measures of relative centrality


   This phenomenon is also observed when considering relative in-Closeness instead, as evi-
dent from analyzing the right panel of Figure (12). As an R-bot gets relatively closer to the rest of
the network through their influence on unsophisticated agents (with high in-Closeness scores),
it becomes more efficient at spreading fake-news, since the speed at which each given piece of
fake-news travels through the network rises, pulling opinions towards 1. This increases misin-
formation and decreases polarization. The effects of relative in-Closeness are much smaller than
those of relative PageRank, as they increase misinformation by 0.64 and 0.14 st devs, respectively.
Interestingly, when in-Closeness increases on average both misinformation and polarization de-
cline. The effect on polarization is opposite to that of PageRank (a coefficient of −0.7 st devs in
the former vs 0.26 st devs in the latter), indicating that higher in-closeness allows for better ag-
gregation of information without having to incur the cost of greater disagreement. This happens


                                                                                     34
because higher in-Closeness is associated with smaller distance between each node in the net-
work and that of the unsophisticated agent. A larger average value is then proxying for a more
connected network in which information travels faster. As we have seen from the analysis of ρ, as
speed of communication rises, it is easier for agents to learn the true θ (which implies both lower
misinformation and polarization). The effects of Betweenness, out-Degree, and out-Closeness are
quantitatively smaller and in some cases statistically insignificant.

                                  0.16
                                         Polarization   Misinformation
                                  0.14

                                  0.12
                    Coefficient




                                   0.1

                                  0.08

                                  0.06

                                  0.04

                                  0.02

                                    0
                                          1                10                 50   100   300

                                                                     

                           Figure 13: Estimated coefficient on the indicator I(κ).


   Through our simulations, we consider networks in which bots can send multiple fake-news
articles each period (e.g. by allowing κ > 1). We control for this greater ability to spread fake-
news by introducing a set of dummy variables I(κ), one for each κ in the regression equation.
To ease readability, we plot the resulting coefficients in Figure ((13)). The solid bars represent
the effects of κ on misinformation, whereas the dashed pattern their effect on polarization (these
variables are not normalized by the standard deviations). As evident from the graph, all coef-
ficients are positive (and significant with p-values lower than 1%. ), indicating that the greater
ability to spread fake news by each bot, keeping everything else constant, results in less infor-
mation aggregation and more variability of opinions in the long run. In addition, note that the
larger effects on misinformation are observed for relatively small values of κ, with the effects
remaining more or less stable for κ ≥ 50. This suggests decreasing marginal returns to the intro-
duction of fake-news on misinformation levels. In contrast to the flooding parameter, increases
in the proportion of unsophisticated agents µu do not significantly affect misinformation in our
estimation. They do, on the other hand, increase polarization (results omitted).




                                                                         35
5.2    No flooding: κ = 1
We restrict the flooding technology of bots such that they can only send one signal each pe-
riod. Because this significantly reduces their ability to spread fake news, average misinformation
declines by 17% (from 0.09 to 0.075) and average polarization is 22% lower (from 0.11 to about
0.08). We re-estimated eq. ((9)) by restricting attention to networks with κ = 1, which lowers
the number of observations from 8,248 to 1,651. The resulting coefficients for misinformation are
presented in Specification (2) and for polarization in Specification (5) of Table (7). Columns (1)
and (4) replicate estimated coefficient under the benchmark (e.g. unrestricted) case for reference.
All variables are normalized by their standard deviations, as in the previous section. Note, how-
ever, that the normalizing st devs in the benchmark and restricted cases need not be the same, as
the volatility of some variables (particularly the dependent variables) may change when bots are
constrained.
   We have omitted coefficients on Betweeness, Out-Closeness, and Out-Degree as they are sta-
tistically insignificant for the restricted sample. The influence of friends, ω is now much more
important in increasing polarization. This is intuitive: if a bot can only spread one fake news per
period, the only way in which this would influence opinions is if individuals pay more attention
to it and relatively less attention to unbiased signals. A similar pattern is observed for polariza-
tion. The effects of ρ are also larger, on both M I and P , when bots’ technology of spreading fake
news is slower. Information aggregation becomes more efficient when the speed of communica-
tion rises, as bots are less likely to clutter signals received by unsophisticated agents.
   Relative PageRank, average in-Closeness and relative in-Closeness are all significantly less
important in reducing polarization (their coefficients are more than halved) when compared to the
benchmark case. This is probably happening because bots are less effective into pulling opinions
towards their preferred value when the volume of fake news they are able to broadcast declines.


5.3    Small share of uninformed: µu < 0.1
Figure (14) depicts the distribution of misinformation (top) polarization (bottom) across networks
for two cases: a small share of unsophisticated agents µu < 0.1 (left) and a high share of unso-
phisticated agents µu > 0.1. As expected, there is a larger number of networks in which agents
aggregate information better when µu is relatively low. This can be seen by the fact that the mass
near zero M I and P is higher in the left panels. Despite of this, misinformation is 22% higher
and polarization 35% smaller than in the benchmark case.

                                                 36
                           Table 7: Regression results: κ = 1 and µu < 0.1 cases
                                                       Misinformation                          Polarization


                                          Benchmark        κ=1          µu = 0.1   Benchmark      κ=1         µu = 0.1

                                               (1)           (2)           (3)        (4)           (5)         (6)

           Communication technology

           Influence of friends ω            0.08***       0.14***         0.04      0.14***       0.23***      0.04
                                              -0.01         -0.02         -0.02       -0.01         -0.01      -0.02
           Speed of communication ρ         -0.05***       -0.06**        -0.03     -0.15***      -0.21***    -0.08***
                                              -0.01         -0.02         -0.02       -0.01         -0.01      -0.02

           Network Topology

           Diameter                          0.05***        0.07*          0.05       0.01        0.09***        0.05
                                              -0.01         -0.03         -0.03      -0.01         -0.02        -0.03
           Clustering                           0           -0.04         -0.06     -0.16***      -0.18***     -0.13*
                                              -0.02         -0.05         -0.05      -0.02         -0.04        -0.05
           Ω                                   yes           yes           yes        yes           yes          yes

           Bot influence

           in-Degree                         0.13***         0.09         0.14*         0           -0.03        0.06
                                              -0.02          -0.05        -0.06       -0.02         -0.04       -0.05
           PageRank                         -0.44***       -0.32***     -0.51***     0.26***       0.32***     0.35***
                                              -0.02          -0.05        -0.06       -0.02         -0.03       -0.05
           in-Closeness                     -0.10***        0.22***       0.15**    -0.70***      -0.21***    -0.83***
                                              -0.02          -0.05        -0.05       -0.02         -0.03       -0.05
           Relative PageRank                 0.61***        0.59***      0.58***    -0.21***      -0.11***    -0.24***
                                              -0.02          -0.04        -0.05       -0.01         -0.03       -0.04
           Relative in-Degree                -0.04**        -0.08*        -0.06      0.09***           0        0.08*
                                              -0.02          -0.04        -0.04       -0.01         -0.03       -0.04
           Relative Betweeness              -0.10***         -0.03       -0.11**       0.02          0.01       -0.02
                                              -0.01          -0.03        -0.04       -0.01         -0.02       -0.03
           Relative in-Closeness             0.15***        0.20***      0.40***    -0.23***      -0.09***    -0.44***
                                              -0.01          -0.03        -0.04       -0.01         -0.02       -0.03

           Observations                       8248         1651        1147          8248          1651        1147
           R-squared                          0.392        0.338       0.444         0.596         0.677       0.528
                                      Robust standard errors in parentheses
                                         *** p<0.01, ** p<0.05, * p<0.1



   This is potentially driven by the non-trivial number of networks in which misinformation
becomes extreme with opinions converging to either 0 or 1. This is surprising given the smaller
share of uninformed agents. A potential explanation is that when a large proportion of agents
are targeted by bots in a symmetric way (in a relatively small network such as ours), agents av-
erage out opposing views from their friends muting the impact of extreme signals. Alternatively,
an unsophisticated agent is very likely to receive both types of extreme signals (directly or indi-
rectly), and hence not be susceptive to modifying their own views to a large extent. When the
share of unsophisticated agents is small, each one of them is more likely to receive only one type
of extreme signal: the one sent by the bot targeting them. Because of this, their views become
more extreme (as there is no counterbalancing information received), particularly if the bot can

                                                               37
                      Low share of unsophisticated                                   High share of unsophisticated

         15




                                                                        10
         10




                                                                     Density
   Density




                                                                     5
         5
         0




                                                                        0
              0     .05        .1            .15     .2        .25             0   .05        .1            .15        .2        .25
                                Misinformation                                                 Misinformation


                      Low share of unsophisticated                                   High share of unsophisticated
         15




                                                                        8
                                                                        6
         10
   Density




                                                                     Density
                                                                      4
         5




                                                                        2
         0




                                                                        0




              0           .2                   .4         .6                   0         .2                       .4        .6
                                Polarization                                                    Polarization




              Figure 14: Misinformation and polarization for µu ≤ 0.1 (left) and µu ≥ 0.1 (right)


flood them. To the extent that one bot manages to manipulate a relatively more influential set
of agents, it will be significantly more likely to succeed in modifying average opinions. Hence, a
bot may be more effective by targeting a small set of influential agents, rather than by targeting
the whole population.
     In Specifications (3) and (6) we re-estimate our regression restricting analysis to networks
with a small share of uninformed agents µu < 0.1. Interestingly, both ρ and ω do not affect
misinformation in this case, whereas polarization is only marginally reduced when the speed of
communication rises (a considerable difference relative to the benchmark case). As suggested
by the previous analysis, a one standard deviation increase in relative Page-Rank, relative in-
Closeness, and even average in-Closeness create significantly more misinformation than in the
benchmark case.


5.4           Welfare analysis
Misinformation and polarization may introduce inefficiencies in the decision-making process
through different channels. A highly misinformed society will agree on selecting the wrong
policy. A polarized society—which is on average correct—may not react fast enough to shocks


                                                                     38
due to inefficient gridlock. A micro-founded political-economy model would dictate how much
individuals would be willing to trade-off polarization for misinformation when opinions do not
converge to the true θ. Because elaborating such model is beyond the scope of our paper, we now
take a reduced-form approach and consider instead the following social welfare function

                             SW (M Ij , Pj ) = − [λM Ij + (1 − λ)Pj ] ,

which is decreasing in M I and P , with λ capturing the relative importance of misinformation
in society j. Given this ad-hoc function, we can analyze how our explanatory variables affect
societal welfare for alternative values of λ. The results are presented in Table (8), with variables
normalized by their standard deviations (similarly to the procedure followed in Table (6)).
   Regardless of the values of λ considered, a higher degree of influence of bots resulting from
higher weight on opinions of social media friends (ω), the ability of bots to flood the network (κ),
or the fact that they manage to influence agents with a large number of followers (in-Degree)
decreases societal welfare. By comparing the size of different coefficients in Table (8), we can
observe that the largest effects on welfare are caused by PageRank scores. To the extent that bots
are symmetric, higher average PageRank results in higher welfare as it allows more information
aggregation. However, when one bot is relatively more influential (through their targeting of
agents with high PageRank scores), the negative effects on welfare are extremely large. This has
policy implications: a society that is successful in eliminating a source of fake news promoting
one extreme of the political spectrum may end up worse off due to the unintended consequences
of making the other extreme relatively more powerful. This, in the end, would generate greater
misinformation and lower welfare, despite effectively reducing polarization.


5.5    Robustness
There were a few statistics measuring network topology and centrality of unsophisticated agents
who were either statistically insignificant in the analysis above, or have very small effects on po-
larization and misinformation. These are Diameter, out-Degree, out-Closeness, Initial homophily,
and Reciprocity. In Specifications (2) and (5) of Table (9) we re-estimate the model excluding these
variables. It is evident that the results are basically unchanged: the goodness of fit is identical to
the second decimal and the size of the coefficients are basically unchanged relative to the Bench-
mark case (replicated in columns (1) and (4)).
   It may also be of interest to consider how the results change when we do not include the pa-


                                                 39
                                Table 8: Regression results: Welfare

                                                            λ=1       λ = 0.8    λ = 0.5

                          Communication technology

                          Influence of friends ω           -0.08***   -0.12***   -0.17***
                                                            (0.009)    (0.008)    (0.007)
                          Speed of communication ρ          0.05***    0.10***    0.16***
                                                            (0.009)    (0.008)    (0.007)

                          Network Topology

                          Diameter                         -0.05***   -0.05***   -0.04***
                                                            (0.01)     (0.01)      (0.01)
                          Clustering                        -0.01      0.04*      0.12***
                                                            (0.02)     (0.02)      (0.02)
                          Ω                                  yes        yes         yes

                          Bot influence

                          Share of unsophisticated µ          0          -          -

                          Flooding κ                          -          -          -

                          in-Degree                        -0.13***   -0.13***   -0.08***
                                                             (0.02)     (0.02)     (0.02)
                          PageRank                          0.44***    0.35***    0.08***
                                                             (0.02)     (0.02)     (0.02)
                          in-Closeness                     0.10***     0.30***    0.63***
                                                             (0.02)     (0.02)     (0.02)
                          Betweeness                       -0.05***   -0.05***   -0.05***
                                                             (0.02)     (0.02)     (0.01)
                          out-Degree                        0.08***    0.08***   0.07***
                                                             (0.02)     (0.02)     (0.02)
                          Out-Closeness                      0.002      -0.02    -0.05***
                                                             (0.02)     (0.02)     (0.02)
                          Relative PageRank                -0.62***   -0.54***   -0.24***
                                                             (0.02)     (0.02)     (0.01)
                          Relative in-Degree                0.04***      0.02    -0.05***
                                                             (0.02)     (0.02)     (0.01)
                          Relative Betweeness               0.10***    0.10***   0.05***
                                                             (0.01)     (0.01)     (0.01)
                          Relative in-Closeness            -0.14***   -0.07***    0.10***
                                                             (0.01)     (0.01)     (0.01)


                          Observations                        8,248    8,248      8,248
                          R-squared                           0.392    0.436      0.572
                                 Robust standard errors in parentheses
                                     *** p<0.01, ** p<0.05, * p<0.1



rameters in Ω as regressors. This is useful to know because when considering real-life networks, it
is typically impossible to back out the parameters in Ω (which determine how the initial network
is created). A statistician may only be able to compute statistics from observable variables, such
links determining centrality and clustering. As in the previous case, estimated coefficients and
the R-squared are unchanged when Ω is excluded from the regression model. This suggests that
the set of network statistics in columns (3) and (6) are sufficient to describe how misinformation



                                                       40
                                     Table 9: Robustness Exercises
                                              Misinformation                              Polarization


                                  Benchmark     Less regressors     No Ω      Benchmark   Less regressors    No Ω

                                      (1)              (2)            (3)        (4)            (5)           (6)

       Communication technology

       Influence of friends ω       0.08***          0.08***        0.08***     0.14***        0.14***       0.14***
                                     (0.01)           (0.01)         (0.01)      (0.01)         (0.01)        (0.01)
       Speed of communication ρ    -0.05***         -0.05***       -0.05***    -0.15***       -0.15***      -0.15***
                                     (0.01)           (0.01)         (0.01)      (0.01)         (0.01)        (0.01)

       Network Topology

       Diameter                     0.05***                                      0.01
                                     (0.01)                                     (0.01)
       Clustering                     0.01           -0.00           0.01      -0.16***       -0.17***      -0.16***
                                     (0.02)          (0.02)         (0.01)      (0.02)         (0.02)        (0.01)
       Ω                              yes             yes             no         yes            yes            no

       Bot influence

       in-Degree                    0.13***          0.14***       0.13***       -0.00          -0.00        -0.04*
                                     (0.02)           (0.02)        (0.02)       (0.02)         (0.02)        (0.02)
       PageRank                    -0.44***         -0.44***       -0.43***     0.26***        0.25***       0.28***
                                     (0.02)           (0.02)        (0.02)       (0.02)         (0.02)        (0.01)
       in-Closeness                -0.10***         -0.12***       -0.18***    -0.70***       -0.68***      -0.69***
                                     (0.02)           (0.02)        (0.01)       (0.02)         (0.02)        (0.01)
       Betweeness                   0.05**          0.06***        0.07***        0.02           0.02         0.03*
                                     (0.02)           (0.01)        (0.01)       (0.01)         (0.01)        (0.01)
       Out-Degree                   -0.08**                                      -0.03
                                     (0.02)                                      (0.02)
       Out-Closeness                 -0.00                                      0.06***
                                     (0.02)                                      (0.02)
       Relative PageRank            0.62***         0.62***        0.62***     -0.21***       -0.20***      -0.21***
                                     (0.02)           (0.02)         (0.02)      (0.01)         (0.01)        (0.01)
       Relative in-Degree           -0.04**          -0.05**        -0.04**     0.09***        0.10***       0.11***
                                     (0.02)           (0.02)         (0.02)      (0.01)         (0.01)        (0.01)
       Relative Betweeness         -0.10***         -0.10***       -0.10***       0.02           0.02          0.02
                                     (0.01)           (0.01)         (0.01)      (0.01)         (0.01)        (0.01)
       Relative in-Closeness        0.14***         0.16***        0.15***     -0.24***       -0.24***      -0.25***
                                     (0.01)           (0.01)         (0.01)      (0.01)         (0.01)        (0.01)

       Observations                  8248              8248          8248       8248           8248          8248
       R-squared                    0.392              0.390         0.389      0.597          0.596         0.593
                                   Robust standard errors in parentheses
                                       *** p<0.01, ** p<0.05, * p<0.1



and polarization react to changes in the environment.



6    Conclusions
We created a large sample of synthetic social media networks by varying their characteristics
through simulations in order to understand what the most important drivers of misinformation
and polarization are. A premise in all of them is the ability of bots (with opposite extreme views)


                                                             41
who purposely spread fake news in order to manipulate the opinion of a small share of agents in
the network. To the extent that agents can be partially influenced by these signals—directly by
not filtering out fake news, or indirectly by following friends who are themselves influenced by
bots—, this can generate misinformation and polarization in the long run. In other words, fake
news prevent information aggregation and consensus in the population. We find that when bots
at one extreme are relatively more efficient at manipulating news (by targeting a small number
of influential agents), the may be able to generate full misinformation in the long run, where
beliefs are at one end of the political spectrum. There would be no polarization in that case, but
at the expense of agents converging to the wrong value of θ, the parameter of interest. There are
other situations where agents are on average correct, but have nonetheless very heterogeneous
opinions. These cases would still be sub-optimal, as they may result in inefficient gridlock and
inaction.
   An important assumption is that the links in the network evolve stochastically. It would be
interesting to extend the model to consider a case in which links are endogenously determined.
This could achieved by allowing agents to place a higher weight on individuals who share similar
priors and choose to ‘unfollow’ (e.g. break links) agents who have views which are relatively far
from their own.
   Having identified the main determinants of polarization, it would be interesting to parame-
terize a real-life social media network (e.g. calibrate it) in order to back out the amount of fake
news necessary to produce the observed increase in polarization between two periods of time. It
would also be possible to carry forward a key-player analysis on the location of internet bots to
better understand what is the most efficient way to reduce polarization.
   Finally, we do observe polarization cycles in some of our networks. Analyzing their determi-
nants could be a fruitful avenue for future research.




                                                42
References
Acemoglu, D., V. Chernozhukov, and M. Yildiz (2008): “Fragility of Asymptotic Agreement
  Under Bayesian Learning,” SSRN eLibrary.

Acemoglu, D., G. Como, F. Fagnani, and A. Ozdaglar (2013): “Opinion Fluctuations and Dis-
  agreement in Social Networks,” Mathematics of Operations Research, 38, 1–27.

Acemoglu, D., M. A. Dahleh, I. Lobel, and A. Ozdaglar (2011): “Bayesian learning in social
  networks,” The Review of Economic Studies, 78, 1201–1236.

Acemoglu, D. and A. Ozdaglar (2011): “Opinion Dynamics and Learning in Social Networks,”
  Dynamic Games and Applications, 1, 3–49.

Acemoglu, D., A. Ozdaglar, and A. ParandehGheibi (2010): “Spread of (mis)information in
  social networks,” Games and Economic Behavior, 70, 194 – 227.

Alesina, A., A. Devleeschauwer, W. Easterly, S. Kurlat, and R. Wacziarg (2003): “Fraction-
  alization,” Tech. rep., National Bureau of Economic Research.

Andreoni, J. and T. Mylovanov (2012): “Diverging opinions,” American Economic Journal: Mi-
  croeconomics, 4, 209–232.

Aumann, R. J. . (1976): “Agreeing to Disagree,” The Annals of Statistics, 4, 1236–1239.

Azzimonti, M. (2015): “Partisan Conflict and Private Investment,” NBER Working Paper, 21273.

Bala, V. and S. Goyal (1998): “Learning from neighbours,” The review of economic studies, 65,
  595–621.

Baldassarri, D. and P. Bearman (2007): “Dynamics of political polarization,” American socio-
  logical review.

Banerjee, A. and D. Fudenberg (2004): “Word-of-mouth learning,” Games and Economic Behav-
  ior, 46, 1–22.

Banerjee, A. V. (1992): “A simple model of herd behavior,” The Quarterly Journal of Economics,
  797–817.



                                                43
Barabási, A.-L. and R. Albert (1999): “Emergence of scaling in random networks,” science, 286,
  509–512.

Barber, M. and N. McCarty (2015): “Causes and consequences of polarization,” Solutions to
  Political Polarization in America, 15.

Barberá, P. (2014): “How Social Media Reduces Mass Political Polarization. Evidence from Ger-
  many, Spain, and the US,” Working Paper, New York University, 46.

Boxell, L., M. Gentzkow, and J. M. Shapiro (2017): “Is the internet causing political polariza-
  tion? Evidence from demographics,” Tech. rep., National Bureau of Economic Research.

Chandrasekhar, A. G., H. Larreguy, and J. P. Xandri (2012): “Testing Models of Social Learn-
  ing on Networks,” Working paper, 1–54.

Chatterjee, S. and E. Seneta (1977): “Towards consensus: some convergence theorems on re-
  peated averaging,” Journal of Applied Probability, 89–97.

Conover, M., J. Ratkiewicz, and M. Francisco (2011): “Political Polarization on Twitter,”
  ICWSM.

DeGroot, M. H. (1974): “Reaching a Consensus,” Journal of the American Statistical Association,
  69, 118–121.

DeMarzo, P. M., D. Vayanos, and J. Zwiebel (2003): “Persuasion Bias, Social Influence, and
  Unidimensional Opinions,” The Quarterly journal of economics, 118, 909–968.

Dixit, A. K. and J. W. Weibull (2007): “Political polarization,” Proceedings of the National
  Academy of Sciences of the United States of America, 104, 7351–7356.

Duclos, J.-Y., J. Esteban, and D. Ray (2004): “Polarization: Concepts, Measurement, Estimation,”
  Econometrica, 72, 1737–1772.

Ellison, G. and D. Fudenberg (1993): “Rules of thumb for social learning,” Journal of political
  Economy, 612–643.

——— (1995): “Word-of-mouth communication and social learning,” The Quarterly Journal of Eco-
  nomics, 93–125.



                                               44
Epstein, L. G., J. Noor, and A. Sandroni (2010): “Non-Bayesian Learning,” The B.E. Journal of
  Theoretical Economics, 10.

Erdös, P. and A. Rényi (1959): “On random graphs, I,” Publicationes Mathematicae (Debrecen), 6,
  290–297.

Esteban, J., C. Gradín, and D. Ray (2007): “An extension of a measure of polarization, with
  an application to the income distribution of five OECD countries,” The Journal of Economic
  Inequality, 5, 1–19.

Esteban, J. and D. Ray (1994): “On the Measurement of Polarization,” Econometrica, 62, 819–851.

——— (2010): “Comparing Polarization Measures,” Journal Of Peace Research, 0–29.

Fiorina, M. and S. Abrams (2008): “Political Polarization in the American Publlic,” The Annual
  Review of Political Science, 49–59.

Gentzkow, M. and J. M. Shapiro (2006): “Media bias and reputation,” Journal of political Econ-
  omy, 114, 280–316.

——— (2010): “What drives media slant? Evidence from US daily newspapers,” Econometrica, 78,
  35–71.

——— (2011): “Ideological segregation online and offline,” The Quarterly Journal of Economics, 126,
  1799–1839.

Golub, B. and M. Jackson (2010): “Naive Learning in Social Networks and the Wisdom of
  Crowds,” American Economic Journal: Microeconomics, 2, 112–149.

Goyal, S. (2005): “Learning in networks,” Group formation in economics: networks, clubs and coali-
  tions, 122–70.

Groseclose, T. and J. Milyo (2005): “A Measure of Media Bias,” The Quarterly Journal of Eco-
  nomics, CXX.

Gruzd, A. and J. Roy (2014): “Investigating political polarization on Twitter: A Canadian per-
  spective,” Policy & Internet.




                                               45
Guerra, P. H. C., W. M. Jr, C. Cardie, and R. Kleinberg (2013): “A Measure of Polarization on
  Social Media Networks Based on Community Boundaries,” Association for the Advancement of
  Artificial Intelligence, 1–10.

Jackson, M. (2010): Social and Economic Networks, vol. 21, Princeton University Press.

Jackson, M. and B. Golub (2012): “How homophily affects the speed of learning and best-
  response dynamics,” The Quarterly Journal of Economics, 1287–1338.

Jadbabaie, A., P. Molavi, A. Sandroni, and A. Tahbaz-Salehi (2012): “Non-Bayesian social
  learning,” Games and Economic Behavior, 76, 210–225.

Kelly, J., D. Fisher, and M. Smith (2005): “Debate, division, and diversity: Political discourse
  networks in USENET newsgroups,” Online Deliberation Conference.

Lee, J. K., J. Choi, C. Kim, and Y. Kim (2014): “Social Media, Network Heterogeneity, and Opinion
  Polarization,” Journal of Communication, 64, 702–722.

Messing, S. and S. J. Westwood (2012): “Selective Exposure in the Age of Social Media: En-
  dorsements Trump Partisan Source Affiliation When Selecting News Online,” Communication
  Research, 41, 1042–1063.

Meyer, C. D., ed. (2000): Matrix Analysis and Applied Linear Algebra, Philadelphia, PA, USA:
  Society for Industrial and Applied Mathematics.

Mohammad, S. M., X. Zhu, S. Kiritchenko, and J. Martin (2015): “Sentiment, emotion, purpose,
  and style in electoral tweets,” Information Processing & Management, 51, 480–499.

Mossel, E., A. Sly, and O. Tamuz (2012): “On Agreement and Learning,” Arxiv preprint
  arXiv:1207.5895, 1–20.

Roux, N. and J. Sobel (2012): “Group Polarization in a Model of Information Aggregation,” .

Seneta, E. (1979): “Coefficients of ergodicity: structure and applications,” Advances in applied
  probability, 576–590.

——— (2006): Non-negative matrices and Markov chains, Springer Science & Business Media.

Sethi, R. and M. Yildiz (2013): “Perspectives, Opinions, and Information Flows,” SSRN Electronic
  Journal.

                                               46
Shapiro, J. M. and N. M. Taddy (2015): “Measuring Polarization in High-Dimensional Data:
  Method and Application to Congressional Speech,” .

Smith, L. and P. Sørensen (2000): “Pathological outcomes of observational learning,” Economet-
  rica, 68, 371–398.

Sobkowicz, P., M. Kaschesky, and G. Bouchard (2012): “Opinion mining in social media: Mod-
  eling, simulating, and forecasting political opinions in the web,” Government Information Quar-
  terly, 29, 470–479.

Sunstein, C. R. (2002): Republic.com, Princeton University Press.

——— (2009): Republic.com 2.0, Princeton University Press.

Tahbaz-Salehi, A. and A. Jadbabaie (2008): “A Necessary and Sufficient Condition for Consen-
  sus Over Random Networks,” IEEE Transactions on Automatic Control, 53, 791–795.

Watts, D. J. and P. S. Dodds (2007): “Influentials, Networks and Public Opinion Formation,”
  Journal of Consumer Research, 34, 441–458.

Webster, J. G. and T. B. Ksiazek (2012): “The Dynamics of Audience Fragmentation: Public
  Attention in an Age of Digital Media,” Journal of Communication, 62, 39–56.

Yardi, S. and D. Boyd (2010): “Dynamic Debates: An Analysis of Group Polarization Over Time
  on Twitter,” Bulletin of Science, Technology & Society, 30, 316–327.




                                                47
Appendix A: Beta-Bernoulli model and the update rule
At any time t, the belief of agent i is represented by the Beta probability distribution with param-
eters αi,t and βi,t
                             
                               Γ (αi,t + βi,t ) αi,t −1
                             
                                                    θ  (1 − θ)βi,t −1            , for 0 < θ < 1
                  µi,t (θ) =   Γ (α i,t ) Γ (β i,t )
                             
                             0                                                   , otherwise,

where Γ(·) is a Gamma function and the ratio of Gamma functions in the expression above is a
normalization constant that ensures that the total probability integrates to 1. In this sense,

                                      µi,t (θ) ∝ θαi,t −1 (1 − θ)βi,t −1 .

    The idiosyncratic likelihood induced by the vector of length K of observed signals si,t+1 is
                                                    P                      P
                                                        si,t+1
                                `(si,t+1 |θ) = θ                 (1 − θ)K−     si,t+1
                                                                                        ,

and therefore the standard Bayesian posterior is computed as

                                                             `(si,t+1 |θ) µi,t (θ)
                              µi,t+1 (θ|si,t+1 ) = Z                                        .
                                                             `(si,t+1 |θ) µi,t (θ) dθ
                                                         Θ

Since the denominator of the expression above is just a normalizing constant, the posterior dis-
tribution is said to be proportional to the product of the prior distribution and the likelihood
function as

                      µi,t+1 (θ|si,t+1 ) ∝ `(si,t+1 |θ) µi,t (θ)
                                                                                     P
                                                                                            si,t+1 −1
                                                                   (1 − θ)βi,t +K−
                                                 P
                                                     si,t+1 −1
                                        ∝ θαi,t +                                                       .

    Therefore, the posterior distribution is
                            
                              Γ (αi,t+1 + βi,t+1 ) αi,t+1 −1
                            
                                                       θ    (1 − θ)βi,t+1 −1                    , for 0 < θ < 1
       µi,t+1 (θ|si,t+1 ) =   Γ (α i,t+1 ) Γ (β i,t+1 )
                            
                            0                                                                   , otherwise,




                                                          48
where
                                                     X
                                   αi,t+1 = αi,t +      si,t+1                               (10)
                                                         X
                                   βi,t+1   = βi,t + K −      si,t+1 .                       (11)

   Equations (10) and (11) are used to update the shape parameters of both regular agents (so-
phisticated and unsophisticated, by setting K = 1) and bots (left and right, by setting K = κ) as
per Equations (4), (5), (2) and (3) in subsection Evolution of beliefs in section 2.




                                                    49
Appendix B: Auxiliary lemmas and propositions regarding
the properties of the sequence {Wt}∞
                                   t=1 and the average weight

matrix W̄
Before proceeding, we implement here a slight change of notation: we let 1 − ωi,t = bi,t . Then, as
explained in section (2), the weight given by agent i to the bayesian update at any time t depends
on whether agent i finds any other agent j in his neighborhood. In algebraic terms
                                                                       
                            bi,t = 1{P [ĝt ] =0} 1 + 1 − 1{P [ĝt ] =0} b.                         (12)
                                      j      ij              j      ij



Lemma 1. The adjacency matrix g0 is an irreducible matrix if and only if G0 is strongly connected.

Proof. By assumption, g0 is strongly connected, for the completeness of the argument see ? (Ch.
8, page 671).

Lemma 2. For all t ≥ 0, the matrix Wt is row-stochastic.

Proof. It is sufficient to show that Wt1 = Bt1 + (In − Bt ) ĝt1 = 1 . For that we can show that for
every period t the vector Wt1 has all entries equal to
                                                                          
                                bi,t = 1 P          1 +   1 − 1              b = 1
                                                                                         P
                                                                                          j [ĝt ]ij = 0
                                
                   X                     { j t ij }
                                            [ĝ ] =0            { j t ij }
                                                                 P
                                                                   [ĝ ] =0         , if
bi,t + (1 − bi,t )   [ĝt ]ij =                                                          P
                   j            1
                                
                                                                                    , if j [ĝt ]ij = 1,

as per the equation (12).
                                                                        out
Lemma 3. The matrix W̄ has diagonal entries W̄ ii = b + (1 − b)(1 − ρ)|Ni,0 | for all i and
                                            

off-diagonal entries
       
       W̄ ij = 0 when [g0 ]ij = 0 and
                                      out
                                            
       W̄ ij = 1 − b − (1 − b)(1 − ρ)|Ni,0 | [ĝ0 ]ij when [g0 ]ij 6= 0.
       



Proof. For any agent i ∈ N , the number of neighbors met at time t is a binomial random variable
                  out
with parameters |Ni,0 | and ρ. Therefore, the probability that agent i finds no other agent in his
neighborhood at time t (denoted as p0it ) is
                               out 
                               |Ni,0 | 0              out              out
                         0
                        pit =              ρ (1 − ρ)|Ni,0 | = (1 − ρ)|Ni,0 | .
                                  0

                                                   50
Notice that the right hand side of the expression above does not depend on time t, thus, we can
establish that p0it = p0i .
    Therefore, according to equation (12), we conclude that the elements in the main diagonal of
the matrix W̄ are
                                      
                                  W̄   ii
                                            = E(bi,t )
                                            = 1p0i + b(1 − p0i )
                                                           out |
                                                                                            out |
                                                                                                     
                                                         |Ni,0                             |Ni,0
                                            = (1 − ρ)              + b 1 − (1 − ρ)
                                                                              out |
                                            = b + (1 − b)(1 − ρ)|Ni,0

    In contrast, the elements off-diagonal can be written as
               
               W̄ ij = E ((1 − bi,t )[ĝt ]ij )

                       = E (1 − bi,t ) E ([ĝt ]ij )
                                                                    !
                                                     [g ]
                       = (1 − E (bi,t )) E          P t ij
                                                      j [gt ]ij
                                                                                                              !
                                                                   out |
                                                                                     [g ] [c ]
                       = 1 − b + (1 − b)(1 − ρ)                    |Ni,0
                                                                                  E   P 0 ij t ij
                                                                                       j ([g0 ]ij [ct ]ij )
                         
                         0                                                        , if [g0 ]ij = 0
                         
                         
                       =                            out |
                                                                 [g ] ρ
                          1 − b − (1 − b)(1 − ρ)
                                                 |Ni,0
                                                                P0 ij  , if [g0 ]ij 6= 0
                                                             ρ       j [g0 ]ij
                         
                         0
                                                                   , if [g0 ]ij = 0
                       =                            
                         (1 − b) 1 − (1 − ρ)|Ni,0 |
                                                out
                                                            1
                                                                   , if [g0 ]ij 6= 0
                                                        |Ni,0
                                                            out
                                                                |



    The next Lemma shows that the matrix W̄ is primitive, i.e. there is a positive integer m such
that W̄ m > 0.

Lemma 4. The average weight matrix W̄ is irreducible and primitive.

Proof. The irreducibility of W̄ comes from the fact that g0 is irreducible by the Assumption 1
(strongly connectedness and aperiodicity). By the Perron-Frobenius theorem, the largest eigen-
value of W̄ in absolute terms is 1 and it has algebraic multiplicity of 1 (i.e. it is the only eigenvalue
in the spectral circle of W̄ ). By the Frobenius’ test for primitivity (see Meyer (2000), ch. 8, page

                                                                   51
678) it can be shown that any nonnegative irreducible matrix having only one unity eigenvalue
on its spectral circle is said to be a primitive matrix. The converse is always true.

    Before introducing the next lemma, we will introduce two definitions. First, the distance
between any two agents i and j is defined as the number of connections in the shortest path
connecting them, i.e. the minimum number of “steps” that agent i should take to reach agent j.
The diameter of the network is the largest distance between any two agents in the network, i.e.
the maximum shortest path length in the network.
    The next lemma provides a positive uniform lower bound on the entries of the matrix W̄ d as
a function of the diameter of the network d induced by W̄ and the minimum (non-zero) expected
share of attention received by an agent i from any other agent j ∈ N (i.e. including i himself),
ω, defined as
        +    
   ω = min W̄ ij
        i,j
            (                                                                                   !)
                                                                                      1
                                                  , min (1 − b) 1 − (1 − ρ)|Ni,0 |
                                          out                                out
      = min min b + (1 − b)(1 − ρ)|Ni,0     |
                                                                                          out
                                                                                                     .
                i∈N                                 i∈N                                  Ni,0

Lemma 5. Let d denote the diameter of the network induced by the social interaction matrix W̄
and ω > 0 be the scalar defined above. Then the entries of the matrix W̄ d are bounded below by the
scalar ω d .

    We will omit the proof of Lemma (5). For further details, the reader can refer to Theorems 1.3
and 1.4 in Seneta (2006, pgs. 18 and 21, respectively) and its related Lemmas.
    Consider the update process described in the equation (4) of Section (2) in its matrix form

                            αt+1 = Bt (αt + st+1 ) + (In − Bt )gˆt αt
                                   = [Bt + (In − Bt )ĝt ] αt + Bt st+1 .

    Notice that Bt is not fixed over time as it depends on the realization of encounters in every
period t. The stochastic matrix (see Lemma (2)) inside the squared bracket is denoted by Wt from
now on and we re-write the previous update process as

                                      αt+1 = Wt αt + Bt st+1 .

    By forward iteration, we have that when t = 0,

                                      α1 = W0 α0 + B0 s1 .

                                                   52
       When t = 1,

                                      α2 = W1 α1 + B1 s2
                                           = W1 (W0 α0 + B0 s1 ) + B1 s2
                                           = W1 W0 α0 + W1 B0 s1 + B1 s2 .

       When t = 2,

                         α3 = W2 α2 + B2 s3
                               = W2 (W1 W0 α0 + W1 B0 s1 + B1 s2 ) + B2 s3
                               = W2 W1 W0 α0 + W2 W1 B0 s1 + W2 B1 s2 + B2 s3 ,

so on and so forth and similarly for the shape parameter vector β.
       Following Chaterjee and Seneta (1977), Seneta (2006) and Tahbaz-Salehi and Jadbabaie (2008),
we let {Wt }, for t ≥ 0, be a fixed sequence of stochastic matrices, and let Ur,k be the stochastic
matrix defined by the backward product of matrices

                                   Ur,k = Wr+k · Wr+(k−1) . . . Wr+2 Wr+1 Wr .14                                 (13)

       With this definition in hand, we show some important properties of the expected backward
product that will help us to prove convergence of opinions in probability to θ.

Proposition 4. Let d be the diameter of the network induced by the matrix W̄ and ω be the min-
imum expected share of attention received by some agent i from any other agent j ∈ N . Then, for
all r ≥ 1, i and j, and given d

                                                           ωd     ωd
                                                             
                                       pij = P [Ur,d ]ij ≥      ≥    > 0.
                                                           2      2



Proof.

                                              ωd                               ωd
                                                                               
                          P       [Ur,d ]ij ≥          = P 1 − [Ur,d ]ij ≤ 1 −
                                              2                                 2
                                                                                   ωd
                                                                                     
                                                       = 1 − P 1 − [Ur,d ]ij ≥ 1 −                               (14)
                                                                                    2
  14
    Our backward product has last term equals to Wr , rather than Wr+1 . This is because our first period is 0, rather
than 1. This notation comes without costs or loss of generality.


                                                            53
   By the Markov inequality, the probability in the right hand side of the equation (14) can be
written as                                                                         
                             
                                                       ωd
                                                                   E 1 − [Ur,d ]ij
                         P       1 − [Ur,d ]ij ≥ 1 −            ≤                       ,
                                                       2                   ωd
                                                                        1−
                                                                           2
and therefore,                                                                         
                             
                                                       ωd
                                                                       E 1 − [Ur,d ]ij
                    1−P          1 − [Ur,d ]ij ≥ 1 −            ≥1−                         .    (15)
                                                       2                        ωd
                                                                             1−
                                                                                2
   Using equation (15), I rewrite equation (14) as
                                                                      
                              
                                           ω d
                                                    E   1 − [U    ]
                                                                r,d ij
                             P [Ur,d ]ij ≥       ≥1−                     .                       (16)
                                           2                  ωd
                                                          1−
                                                               2
   From the functional form of the backward product (see eq. (13)) and given that {Wt }∞
                                                                                       t=1 is a

sequence of independent matrices over all t (see eq. (1)), the expectation above can be written as

                          E (Ur,d ) = E (Wr+d−1 Wr+d−2 · · · Wr ) = W̄ d ,

thus, from Lemma (5), this implies that for all i and j
                                                           
                                            E [Ur,d ]ij ≥ ω d .

   Therefore, eq. (16) becomes

                                     ωd       1 − ωd      ωd     ωd
                                       
                       P [Ur,d ]ij ≥      ≥1−       d =        ≥    ,
                                     2        1 − ω2    2 − ωd   2

proving that the (i, j) entry of the matrix represented by the backward product Ur,d is positive
with non-zero probability, i.e pij > 0.

   Therefore, Proposition (4), together with the assumption that the sequence of matrices {Wt }∞
                                                                                               t=1

are i.i.d. and have positive diagonals (see Lemma (3)), ensures that the matrix represented by the
backward product U1,n2 d+1 of length n2 d is positive with at least probability Πi,j pij > 0. The
choice n2 d is a conservative one as in AOP (2010) and Tahbaz-Salehi and Jadbabaie (2008).

Lemma 6. Consider ρ = 1, i.e. Wt = W for every t. The iteration of the row-stochastic matrix
W is convergent and therefore there exists a threshold τ̄ ∈ N such that |Wijτ +1 − Wijτ | <  for any
τ ≥ τ̄ and  > 0

                                                       54
Proof. In order to see how W τ behaves as τ grows large, it is convenient to rewrite W using
its diagonal decomposition. In particular, let v be the squared matrix of left-hand eigenvectors
of W and D = (d1 , d2 , . . . , dn )> the eigenvector of size n associated to the unity eigenvalue
λ1 = 1.15 Without loss of generality, we assume the following normalization 1 > D = 1. Therefore,
W = v −1 Λv, where Λ = diag(λ1 , λ2 , . . . , λn ) is the squared matrix with eigenvalues on its
diagonal, ranked in terms of absolute values. More generally, for any time τ we write

                                               W τ = v −1 Λτ v.

       Noting that v −1 has ones in all entries of its first column, it follows that
                                                         X
                                                                    −1
                                       [W τ ]ij = dj +         λτr vir vrj ,
                                                           r


for each r, where λr is the r-th largest eigenvalue of W . Therefore, limτ →∞ [W τ ]ij = D11> , i.e.
each row of W τ for all τ ≥ τ̄ converge to D, which coincides with the stationary distribution.
Moreover, if the eigenvalues are ordered the way we have assumed, then kW τ −D11> k = o(|λ2 |τ ),
i.e. the convergence rate will be dictated by the second largest eigenvalue, as the others converge
to zero more quickly as τ grows.

  15
     This is a feature shared by all stochastic matrices because having row sums equal to 1 means that kW k∞ = 1
or, equivalently, W11 = 1 , where 1 is the unity n-vector.




                                                      55
Appendix C: Backward product ergodicity
Our main concern in order to prove that agents’ opinions converge in probability to θ is the
behavior of Ur,k when k → ∞ for each r ≥ 0. For that, we need to define two concepts of
ergodicity. The sequence {Wt }∞
                              t=1 is said to be weakly ergodic if for all i, j, s = 1, 2, . . . , n and

r ≥ 0,
                                          [Ur,k ]is − [Ur,k ]js → 0

as k → ∞.
   On the other hand, we say that this very same sequence is strongly ergodic for all r ≥ 0, and
element-wise if
                                             lim Ur,k = 1Pr> ,
                                            k→∞

where 1 is a vector of ones of size n and Pr is a probability vector in which Pr ≥ 0 and Pr> 1 = 1
for all r ≥ 0.
   Both weak and strong ergodicity (in the backward direction) describe a tendency to consensus.
In the strong ergodicity case, all rows of the stochastic matrix Ur,k are becoming the same as k
grows large and reaching a stable limiting vector, whereas in the weak ergodicity case, every row
is converging to the same vector, but each entry not necessarily converges to a limit. In our case,
we can show that there is an equivalence between both concepts since each row of Ur,k+1 is a
weighted average of the rows of Ur,k .

Lemma 7. For the backward product (13), weak and strong ergodicity are equivalent.

Proof. Following Theorem 1 in Chatterjee and Seneta (1977), it suffices to show that weak ergod-
icity implies strong ergodicity. For that, we fix an arbitrary r ≥ 0 and a small  > 0 and assume
that there is a k = k̄ such that Wk has the form 1P > , where P is a probability vector. Then by
the definition of weak ergodicity we have that

                 − ≤ [Ur,k ]is − [Ur,k ]js ≤  ⇐⇒ [Ur,k ]is −  ≤ [Ur,k ]js ≤ [Ur,k ]is + 

for k ≥ k̄ and for all i, h, s = 1, . . . , n. Since each row of Ur,k+1 is a weighted average of the rows




                                                     56
of Ur,k , we have
                      n
                      X                                 n
                                                      X
                            [Wr+k+1 ]hj [Ur,k ]is −  ≤   [Wr+k+1 ]hj [Ur,k ]js
                      j=1                                     j=1
                                                              Xn
                                                                                              
                                                          ≤          [Wr+k+1 ]hj [Ur,k ]is +  .
                                                               j=1


       The inequality above shows that for any h and k ≥ k̄

                                     [Ur,k ]is −  ≤ [Ur,k+1 ]hs ≤ [Ur,k ]is + .

       Thus, by induction, for any i, h, s = 1, 2, . . . , n, for any k ≥ k̄ and for any integer q ≥ 1

                                             [Ur,k+q ]js − [Ur,k ]is ≤ .

By setting i = j in the expression above and taking k ≥ k̄, we see that [Uk,r ]i,s converges to a
limit as k → ∞ for all s.

Definition 4. The scalar function τ (·) continuous on the set of n × n stochastic matrices and satis-
fying 0 ≤ τ (·) ≤ 1 is called coefficient of ergodicity. It is said to be proper if τ (W ) = 0 if and only
if W = 1 v > , where v > is any probability vector (i.e. whenever W is a row-stochastic matrix with
unit rank), and improper otherwise.

       Two examples of coefficients of ergodicity, in terms of W , drawn from Seneta (2006) p. 137
are
                                                                         
                                     a(W ) = max max    [W ]ij − [W ]i0 j
                                               j   i,i0
                                                               
                                     c(W ) = 1 − max min [W ]ij ,
                                                      j        i


where the first coefficient is proper and the second improper. Moreover, it can be shown that, i)
for any stochastic matrix W , a(W ) ≤ c(W ) and ii) if τ (·) is a proper coefficient of ergodicity, the
inequality
                                                                        m
                                                                        Y
                                     τ (Wm Wm−1 · · · W2 W1 ) ≤               τ (Wt )                              (17)
                                                                        t=1

is satisfied for any m ≥ 1. In this sense, for a proper coefficient of ergodicity τ (·), weak ergod-
                                16


icity is equivalent to τ (Ur,k ) → 0 as k → ∞ and r ≥ 0.

                                         Qmfor any two proper coefficients of ergodicity τi (·) ≤ τj (·), the inequality
  16
    More specifically, it can be shown that
holds with τi (Wm Wm−1 · · · W2 W1 ) ≤ t=1 τj (Wt ).

                                                          57
Lemma 8. The sequence {Wt }∞
                           t=1 is weakly ergodic if there exists a strictly increasing subsequence

of the positive integers {ix }, x = 1, 2, . . . such that
                                      ∞
                                      X                                       
                                            1 − τ (Wix+1 · · · Wix +1 )                       (18)
                                      x=1

diverges.

Proof. Let θx = τ (Wix+1 · · · Wix +1 ). The standard inequality z − 1 ≥ log z (or equivalently
1 − z ≤ − log z) implies that 1 − θx ≤ − log θx , for any x. Summing up across index x in both
sides yields
                                    ∞
                                    X                       ∞
                                                            X
                                          (1 − θx ) ≤ −           log θx
                                    x=1                     x=1
                                                                   ∞
                                                                              !
                                                                   Y
                                                   ≤ − log               θx       .           (19)
                                                                   x=1

   Since equation (17) holds, the sum in the left hand side of equation (19) diverges, implying
that log ( ∞
          Q                                                   Q∞
            x=1 θx ) = −∞. For that, it must be the case that  x=1 θx → 0 as x → ∞. Because

τ (·) is a proper coefficient of ergodicity, equation (17) ensures weak ergodicity of the sequence
{Wt }∞
     t=1




                                                     58
Appendix D: Proofs of propositions

Proof of proposition (1)
Proof. We start by considering the parameter update process described in eq. (5) of section (2).
Since the network’s edges are activated every single period (i.e. ρ = 1), ĝt = ĝ and Bt =
B = diag(b, b, . . . , b), where b ∈ (0, 1). Moreover, since we are assuming strong connectivity,
P
  j [g]ij 6= 0 for any i. Thus, the update process for the parameter vector α (of size n) in its matrix

form is

                              αt+1 = B(αt + st+1 ) + (In − B)ĝαt
                                     = [B + (In − B)ĝ] αt + Bst+1 .

   We define the matrix inside the squared bracket as W for any t. We re-write the update
process above as follows
                                       αt+1 = W αt + Bst+1

   When t = 0,

                                        α1 = W α0 + Bs1

   When t = 1,

                                  α2 = W α1 + Bs2
                                      = W (W α0 + Bs1 ) + Bs2
                                      = W 2 α0 + W Bs1 + Bs2

   When t = 3,

                            α3 = W α2 + Bs3
                                 = W W 2 α0 + W Bs1 + Bs2 + Bs3
                                                         

                                 = W 3 α0 + W 2 Bs1 + W Bs2 + Bs3

   So on and so forth, resulting in the following expression for any particular period τ
                                                    τ −1
                                                    X
                                             τ
                                    ατ = W α0 +            W t Bsτ −t                             (20)
                                                    t=0



                                                  59
   Similarly for the parameter β, we have
                                                     τ −1
                                                     X
                                         τ
                               βτ = W β0 +                  W t B(1 − sτ −t ).                     (21)
                                                     t=0

   where 1 is the vector of ones of size n. From Equations (20) and (21), the sum of this two
parameter-vectors is given by the following expression
                                                                           τ −1
                                                                           X
                                                 τ
                            ατ + βτ = W (α0 + β0 ) +                              W t B1
                                                                           t=0
                                                                           τ −1
                                                                           X
                                        = W τ (α0 + β0 ) +                        W tb
                                                                           t=0
                                        = W τ (α0 + β0 ) + τ b.                                    (22)
                                                                                               αi,τ
   Therefore, at any point in time τ , the opinion of any agent i is given by yi,τ =                   .
                                                                                           αi,τ + βi,τ
From equation (20), we write
                                                            τ −1
                                                            X
                               αi,τ =        Wi∗τ α0   +           Wi∗t bsτ −t
                                                            t=0
                                                                    τ −1
                                                            1X t
                                     =       Wi∗τ α0   + τb      W sτ −t
                                                            τ t=0 i∗
                                     = Wi∗τ α0 + τ bθ̃i (τ ),                                      (23)

   where the symbol Wi∗τ is used to denote the i-th row of matrix W τ and W 0 = In . From
equations (23) and (22), we write yi,τ as

                                            Wi∗τ α0 + τ bθ̃i (τ )
                               yi,τ =
                                            Wi∗τ (α0 + β0 ) + τ b
                                                                                  !
                                                   1
                                      τ            τ
                                                     Wi∗τ α0       + bθ̃i (τ )
                                    =            1                                    ,            (24)
                                      τ          τ
                                                   Wi∗τ (α0        + β0 ) + b

   From Equation (24), we have that the limiting opinion (in probability) of any agent i, at any




                                                       60
point in time τ , is described as

                    plim yi,τ = plim θ̃i (τ )
                    τ →∞               τ →∞
                                                τ −1
                                              1X t
                                 = plim            W sτ −t
                                       τ →∞   τ t=0 i∗
                                                 τ̄                      τ
                                              1X t                   1 X
                                 = plim             Wi∗ sτ −t + plim           Wi∗t sτ −t .     (25)
                                       τ →∞   τ t=0             τ →∞ τ t=τ̄ +1

     From Lemma (6), we can split the series in Equation (26) into two parts. The first term de-
scribes a series of τ̄ terms that represent the “most recent” signals coming in to the network.
Notice that every weight-matrix W t in the interval from t = 0 to t = τ̄ is different from one
another, since the matrix W t does not converge to a row-stochastic matrix with unity rank for
low t. It is straight-forward to see that this term converges to zero as τ → ∞. The second term
represents describes a series of τ − τ̄ terms that represent the“older signals” that entered in the
network and fully reached all agents. As τ → ∞, this term becomes a series with infinite terms.
From the i.i.d. property of the Bernoulli signals, we can conclude that
                                            τ
                                        1 X
                     plim yi,τ   = plim           Wi∗t sτ −t
                     τ →∞          τ →∞ τ t=τ̄ +1
                                            τ
                                        1 X
                                 = plim           W i∗ sτ −t
                                   τ →∞ τ
                                          t=τ̄ +1
                                                 τ
                                             1 X
                                 = plim W i∗           sτ −t
                                   τ →∞      τ t=τ̄ +1
                                                      τ
                                 asy           1 X
                                 = plim W i∗                sτ −t
                                   τ →∞      τ − τ̄ t=τ̄ +1
                                 asy
                                 = W i∗θ ∗ = θ∗ ,             (i.i.d. Bernoulli signals)        (26)

     where W = D11> . From equation (26), we conclude that society is wise and because of that,
plimt→∞ |ỹk,t − ỹl,t | = 0, i.e. the K groups reach consensus, impliying plimt→∞ Pt = |θ∗ − θ∗ | =
0.




                                                       61
Proof of proposition (2)
Proof. The update process of both shape parameters can be represented in their matrix form for
any period τ as
                                                   τ −1
                                                                                    !
                                                   X
                 ατ = U0,τ −1 α0 +                           Ur,τ −1−r Br−1 sr           + Bτ −1 sτ ,                                         (27)
                                                   r=1
                                                     −1
                                                   τ
                                                                                                !
                                                   X
                 βτ = U0,τ −1 β0 +                           Ur,τ −1−r Br−1 (11 − sr )              + Bτ −1 (11 − sτ ) .                      (28)
                                                       r=1

                                                                                          (r,k)
   To reduce the burden of notation, consider [Ur,k ]ij = uij                                     for any r and k. Therefore, from
equation (27), we write its entries as
                                                         τ −1
                                                                                                    !
                               (0,τ −1)                               (r,(τ −1−r))
                      X                                 XX
           αi,τ =             uij       αj,0   +                     uij           bj,r−1 sj,r          + bi,τ −1 si,τ
                          j                              j     r=1
                                                         "      τ −1
                                                                                                              !                    #
                      X        (0,τ −1)          1             XX               (r,(τ −1−r))
                 =            uij       αj,0 + τ                            uij                bj,r−1 sj,r        + bi,τ −1 si,τ
                          j
                                                 τ               j    r=1
                               (0,τ −1)
                      X
                 =            uij       αj,0   + τ θ̃i,1 (τ ).                                                                                (29)
                          j


Each entry of the parameter vector β is written in a similar way
                                                 τ −1
                                                                                                          !
                        (0,τ −1)                                (r,(τ −1−r))
                 X                              XX
        βi,τ =         uij       βj,0   +                      uij           bj,r−1 (1         − sj,r )       + bi,τ −1 (1 − si,τ ).
                  j                                j     r=1


   The sum of both parameters αi,τ and βi,τ yields
                                                                            τ −1
                                                                                                                   !
                                    (0,τ −1)                                              (r,(τ −1−r))
                              X                                            XX
     αi,τ + βi,τ =                uij          (αj,0 + βj,0 ) +                          uij              bj,r−1       + bi,τ −1
                              j                                             j     r=1
                                                                            "       τ −1
                                                                                                                          !               #
                              X    (0,τ −1)                       1                XX              (r,(τ −1−r))
                      =           uij          (αj,0 + βj,0 ) + τ                                 uij           bj,r−1        + bi,τ −1
                              j
                                                                  τ                  j    r=1
                                    (0,τ −1)
                              X
                      =           uij          (αj,0 + βj,0 ) + τ θ̃i,2 (τ ).                                                                 (30)
                              j

              P        (r,(τ −1))
   In which       j   uij           = 1, for all r ≥ 0 since Ur,k is a stochastic matrix. Therefore, the
                                                                                                                         αi,τ
opinion of each agent i in this society, at some particular time τ , is yi,τ =                                        αi,τ +βi,τ
                                                                                                                                 ,   where each




                                                                      62
entry of the parameter vectors can be written as follows:
                                             P    (0,τ −1)
                                               j uij       αj,0 + τ θ̃i,1 (τ )
                               yi,τ =   P (0,τ −1)
                                         j uij       (αj,0 + βj,0 ) + τ θ̃i,2 (τ )

    Asymptotically we have:
                                                                 (0,τ −1)
                                                        P                                         !
                                                             j uij          αj,0 + τ θ̃i,1 (τ )
                       plim yi,τ = plim        P         (0,τ −1)
                       τ →∞         τ →∞
                                                   j   uij
                                                     (αj,0 + βj,0 ) + τ θ̃i,2 (τ )
                                                  (0,τ −1)
                                                        P                       
                                               j uij       αj,0
                                        τ                       + θ̃i,1 (τ )
                                 = plim  P (0,τ −1)τ                            
                                   τ →∞ τ  j uij      (αj,0 +βj,0 )
                                                     τ
                                                                    + θ̃i,2 (τ )
                                            θ̃i,1 (τ )
                                 = plim                                                                 (31)
                                    τ →∞    θ̃i,2 (τ )

    With the results of Lemmas in Appendices B and C, weak law of large numbers and the
assumption of independence between bt and st we can can show that
                                                    1
                                                         P Pτ −τ̄      (r,(τ −1−r))
                             θ̃i,1 (τ )             τ        jr=1 uij               bj,r−1 sj,r
                        plim            = plim                   τ −τ̄   (r,(τ −1−r))
                                                   1
                                                      P     P
                        τ →∞ θ̃i,2 (τ )
                                                                 r=1 uij              bj,r−1
                                          τ →∞
                                                   τ      j
                                                  P         1
                                                              P    τ −τ̄
                                                    j ūij         r=1 bj,r−1 sj,r
                                        = plim P τ 1 Pτ −τ̄
                                            τ →∞       j ūij τ      r=1 bj,r−1
                                                  P           1
                                                                   Pτ −τ̄
                                        asy          j ūij             r=1 bj,r−1 sj,r
                                        ≡ plim P τ −τ̄1 Pτ −τ̄
                                             τ →∞       j ūij τ −τ̄      r=1 bj,r−1
                                          P
                                            j ūij E(bj sj )
                                        = P
                                               ūij E(bj )
                                          P j
                                            j ūij E(bj )E(sj )
                                        =   P
                                                j ūij E(bj )
                                          θ∗ j ūij E(bj )
                                            P
                                        = P                   = θ∗                                      (32)
                                                ū
                                              j ij  E(b  j )




Proof of proposition (3)
Proof. If perfect information aggregation is reached at any particular time t̄, then we know that
yi,t̄ = θ for all i ∈ G, thus all alienation terms in the polarization function are zero because
|yi,t̄ − yj,t̄ | = |θ − θ| = 0, for all i and j in N . Therefore, Polarization Pt̄ is zero for any particular


                                                             63
choice of parameter a. Conversely, if polarization at time t̄ is zero, then all alienation terms are
necessarily zero, since the measure of groups is non-negative. This means that |yi,t̄ − yj,t̄ | = 0
implies yi,t̄ = yj,t̄ and, therefore, any opinion consensus of the form yi,t̄ = yj,t̄ = θ̃, such that
θ̃ ∈ Θ = [0, 1] and θ̃ 6= θ, meets this requirement.




                                                 64
