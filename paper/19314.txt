                                           NBER WORKING PAPER SERIES




                                         ECONOMETRIC MEDIATION ANALYSES:
,'(17,)<,1*7+(6285&(62)75($70(17())(&76)520(;3(5,0(17$//<(67,0$7('
PRODUCTION TECHNOLOGIES WITH UNMEASURED AND MISMEASURED INPUTS

                                                     James J. Heckman
                                                       Rodrigo Pinto

                                                   Working Paper 19314
                                           http://www.nber.org/papers/w19314


                                 NATIONAL BUREAU OF ECONOMIC RESEARCH
                                          1050 Massachusetts Avenue
                                            Cambridge, MA 02138
                                                August 2013




           This research was supported in part by the American Bar Foundation, the JB & MK Pritzker Family
           Foundation, Susan Thompson Buffett Foundation, NICHD R37HD065072, R01HD54702, a grant
           to the Becker Friedman Institute for Research and Economics from the Institute for New Economic
           Thinking (INET), and an anonymous funder. We acknowledge the support of a European Research
           Council grant hosted by University College Dublin, DEVHEALTH 269874. The views expressed
           in this paper are those of the authors and not necessarily those of the funders, the persons named here,
           or the National Bureau of Economic Research. We thank the editor and an anonymous referee for
           helpful comments.

           NBER working papers are circulated for discussion and comment purposes. They have not been peer-
           reviewed or been subject to the review by the NBER Board of Directors that accompanies official
           NBER publications.

           © 2013 by James J. Heckman and Rodrigo Pinto. All rights reserved. Short sections of text, not to
           exceed two paragraphs, may be quoted without explicit permission provided that full credit, including
           © notice, is given to the source.
Econometric Mediation Analyses: Identifying the Sources of Treatment Effects from Experimentally
Estimated Production Technologies with Unmeasured and Mismeasured Inputs
James J. Heckman and Rodrigo Pinto
NBER Working Paper No. 19314
August 2013
JEL No. C21,C38,C43,D24

                                             ABSTRACT

This paper presents an econometric mediation analysis. It considers identification of production functions
and the sources of output effects (treatment effects) from experimental interventions when some inputs
are mismeasured and others are entirely omitted.


James J. Heckman
Department of Economics
The University of Chicago
1126 E. 59th Street
Chicago, IL 60637
and University College Dublin and IZA
and also NBER
jjh@uchicago.edu

Rodrigo Pinto
Department of Economics
The University of Chicago
1126 E. 59th Street
Chicago, IL 60637
rodrig@uchicago.edu
1     Introduction

William Barnett is a pioneer in the development and application of index theory and productivity

accounting analyses. He has also pioneered the estimation of production functions. This paper

follows in the tradition of Barnett’s work. It develops an econometric mediation analysis to explain

the sources of experimental treatment effects. It considers how to use experiments to identify

production functions in the presence of unmeasured and mismeasured inputs. The goal of the

analysis is to determine the causes of effects: sources of the treatment effects properly attributable

to experimental variation in measured inputs.

    Social experiments usually proceed by giving a vector of inputs to the treatment group and

withholding it from the control group. Analysts of social experiments report a variety of treatment

effects. Our goal is to go beyond the estimation of treatment effects and examine the mechanisms

through which experiments generate these effects. Thus we seek to use experiments to estimate the

production functions producing treatment effects. This exercise is called mediation analysis in the

statistics literature (Imai et al., 2011, 2010; Pearl, 2011). Such analyses have been used for decades

in economics (Klein and Goldberger, 1955; Theil, 1958) and trace back to the work on path analysis

by Sewall Wright (1921; 1934).

    We provide an economically motivated interpretation of treatment effects. Treatment may

affect outcomes through changing inputs. Treatment may also affect outcomes through shifting the

map between inputs and outputs for treatment group members. When there are unmeasured (by

the analyst) inputs, empirically distinguishing these two cases becomes problematic. We present a

framework for making this distinction in the presence of unmeasured inputs and when the measured

inputs are measured with error.

    A fundamental problem of mediation analysis is that even though we might observe experimen-

tal variation in some inputs and outputs, the relationship between inputs and outputs might be

confounded by unobserved variables. There may exist relevant unmeasured inputs changed by the

experiment that impact outputs. If unmeasured inputs are not statistically independent of mea-

sured ones, then the observed empirical relation between measured inputs and outputs might be due



                                                  1
to the confounding effect of experimentally induced changes in unmeasured inputs. In this case,

treatment effects on outputs can be wrongly attributed to the enhancement of measured inputs

instead of experimentally induced increase in unmeasured inputs.

    Randomized Controlled Trials (RCTs) generate independent variation of treatment which allows

the analyst to identify the causal effect of treatment on measured inputs and outputs. Nevertheless,

RCTs unaided by additional assumptions do not allow the analyst to identify the causal effect

of increases in measured inputs on outputs nor do they allow the analyst to distinguish between

treatment effects arising from changes in production functions induced by the experiment or changes

in unmeasured inputs when there is a common production function for treatments and controls.

    This paper examines these confounding effects in mediation analysis. We demonstrate how

econometric methods can be used to address them. We show how experimental variation can be used

to increase the degree of confidence in the validity of the exogeneity assumptions needed to make

valid causal statements. In particular, we show that we can test some of the strong assumptions

implicitly invoked to infer causal effects in statistical mediation analyses. We analyze the invariance

of our estimates of the sources of treatment effects to changes in measurement schemes.

    The paper is organized in the following fashion. Section 2 discusses the previous literature and

defines the mediation problem as currently framed in the statistics literature. Section 3 presents a

mediation analysis within a linear framework with both omitted and mismeasured inputs. Section 4

discusses identification. Section 5 presents an estimation method. Section 6.1 discusses an invari-

ance property when input measures are subject to affine transformations. Section 6.2 discusses

further invariance results for general monotonic transformations of measures and for nonlinear

technologies. Section 7 concludes.



2     Assumptions in Statistical Mediation Analysis

The goal of mediation analysis as framed in the literature in statistics is to disentangle the average

treatment effect on outputs that operates through two channels: (1) Indirect output effects arising

from the effect of treatment on measured inputs and (2) Direct output effects that operate through



                                                  2
channels other than changes in the measured inputs. The mediation literature often ignores the

point that Direct Effects are subject to some ambiguity: they can arise from inputs changed by

the experiment that are not observed by the analyst, but can also arise from changes in the map

between inputs and the outputs.

   To clarify ideas it is useful to introduce some general notation. Let D denote treatment assign-

ment. D = 1, if an agent is treated and D = 0 otherwise. Let Y1 and Y0 be counterfactual outputs

when D is fixed at “1” and “0” respectively. By fixing, we mean an independent manipulation

where treatment status is set at d. The distinction between fixing and conditioning traces back to

Haavelmo (1943). For recent discussions see Pearl (2001, 2011) and Heckman and Pinto (2012).

We use the subscript d ∈ {0, 1} to represent variables when treatment is fixed at d. In this notation

Yd represents output Y when treatment status is fixed at d and the realized output is given by


                                      Y = DY1 + (1 − D)Y0 .                                      (1)


In our notation, the average treatment effect between treatment and control groups is given by


                                        AT E = E(Y1 − Y0 ).                                      (2)


   We define a vector of inputs when treatment is fixed at d by θd = (θdj : j ∈ J ), where J is

an index set for inputs. We define the vector of realized inputs by θ in a fashion analogous to Y :

θ = Dθ1 + (1 − D)θ0 . While output Y is assumed to be observed, we allow for some inputs to

be unobserved. Notationally, let Jp ⊆ J be the index set of proxied inputs—inputs for which we

have observed measurements. We represent the vector of proxied inputs by θdp = (θdj : j ∈ Jp ). We

allow for the possibility that observed measurements may be imperfect proxies of measured inputs

so that measured inputs may not be observed directly. We denote the remaining inputs indexed by

J \ Jp as unmeasured inputs, which are represented by θdu = (θdj : j ∈ J \ Jp ).

   We postulate that the output Y is generated through a production function whose arguments

are both measured and unmeasured inputs in addition to an auxiliary set of baseline variables



                                                 3
X. Variables in X are assumed not to be caused by treatment D that affects output Y in either

treatment state. The production function for each treatment regime is


                                       Yd = fd (θdp , θdu , X), d ∈ {0, 1}.                          (3)


Equation (3) states that output Yd under treatment regime D = d is generated by (θdp , θdu , X)

according to function fd such that d ∈ {0, 1}. If f1 = f0 , functions (f1 , f0 ) are said to be invariant

across treatment regimes. Invariance means that the relationship between inputs and output is

determined by a stable mechanism defined by a deterministic function unaffected by treatment.

    From Equation (2), the average treatment effect or AT E is given by:


                               AT E = E(f1 (θ1p , θ1u , X) − f0 (θ0p , θ0u , X)).


Expectations are computed with respect to all inputs. Treatment effects operate through the

impact of treatment D on inputs (θdp , θdu ), d{0, 1} and also by changing the map between inputs
                                                                                   P
and the outcome, namely, fd (·); d ∈ {0, 1}. Observed output is given by Y =         d∈{0,1} 1[D =

d] · fd (θdp , θdu , X).

    We are now equipped to define mediation effects. Let Yd,θ̄dp represent the counterfactual output

when treatment status D is fixed at d and proxied inputs are fixed at the some value θ̄dp ∈ supp θdp .

From production function (3),


                                    Yd,θ̄p = fd (θ̄dp , θdu , X), d ∈ {0, 1}.                        (4)


Note that the subscript d of Yd,θ̄p arises both from the selection of the production function fd (·),

from the choice of d, and from changes in unmeasured inputs θdu . Moreover, conditional on X and

fixing θdp = θ̄dp , the source of variation of Yd,θ̄p is attributable to unmeasured inputs θdu . Keeping

X implicit, use Yd,θ̄p0 to represent the value output would take fixing D to d and simultaneously
                           d

fixing measured inputs to be θ̄dp0 .

    In the mediation literature, ATE is called the total treatment effect. It is often decomposed


                                                        4
into direct and indirect treatment effects. The indirect effect (IE) is the effect of changes in the

distribution of proxied inputs (from θ0p to θ1p ) on mean outcomes while holding the technology fd

and the distribution of unmeasured inputs θdu fixed at treatment status d. Formally, the indirect

effect is


                                IE(d) = E(Yd (θ1p ) − Yd (θ0p ))

                                        = E(fd (θ1p , θdu , X) − fd (θ0p , θdu , X)).


Here expectations are taken with respect to θdu and X. One definition of the direct effect (DE) is

the average effect of treatment holding measured inputs fixed at the level appropriate to treatment

status d but allowing technologies and associated distributions of unobservables to change with

treatment regime:


                                DE(d) = E(Y1,θdp − Y0,θdp )                                      (5)

                                         = E(f1 (θdp , θ1u , X) − f0 (θdp , θ0u , X)).


Robin (2003) terms these effects as the pure, direct, and indirect effects, while Pearl (2001) calls

them the natural direct and indirect effects.

    We can further decompose the direct effect of Equation (5) into portions associated with the

change in the distribution of θdu ; d ∈ {0, 1} and the change in the map between inputs and outputs

fd (·); d ∈ {0, 1}. Define


                             DE 0 (d, d0 ) = E(f1 (θdp , θdu0 , X) − f0 (θdp , θdu0 , X)).       (6)


DE 0 (d, d0 ) is the treatment effect mediated by changes in the map between inputs and outputs when

fixing the distribution of measured inputs at θdp , and unmeasured inputs at θdu0 for d, d0 ∈ {0, 1}.




                                                          5
Define

                             00
                         DE (d, d0 ) = E(fd0 (θdp , θ1u , X) − fd0 (θdp , θ0u , X)).               (7)


   00
DE (d, d0 ) is the treatment effect mediated by changes unmeasured inputs from θ0u to θ1u while

setting the production function at fd0 (·) where measured inputs are fixed at θdp for d, d0 ∈ {0, 1}.

   The definition of direct effect in Equation (5) is implicit in the mediation literature. Defini-

tions (6) – (7) are logically coherent. Direct effects (5) can be written alternatively as:

                                                   0             00
                                  DE(d) = DE (d, 1) + DE (d, 0);
                                                   0             00
                                  DE(d) = DE (d, 0) + DE (d, 1).


   The source of the direct treatment effect is often ignored in the statistical literature. It can

arise from changes in unobserved inputs induced by the experiment (from θ0u to θ1u ). It could also

arise from an empowerment effect e.g. that treatment modifies the technology that maps inputs

into outputs (from f0 to f1 ). The change in technology may arise from new inputs never previously

available such as parenting information as studied by Cunha (2012) and Heckman et al. (2012).

If both measured and unmeasured inputs were known (including any new inputs never previously

available to the agent), then the causal relationship between inputs and outputs could be estimated.

Using the production function for each treatment state, one could decompose treatment effects into

components associated with changes in either measured or unmeasured inputs. Since unmeasured

inputs are not observed, the estimated relationship between measured inputs and outputs may be

confounded with changes in unmeasured inputs induced by the experiment.

   In this framework, under the definition of a direct effect (Equation 5), we can decompose the




                                                       6
total treatment effect into the direct and indirect effect as follows:


                          AT E = E(Y1 (θ1p ) − Y0 (θ0p ))

                                = E(Y1 (θ1p ) − Y0 (θ1p )) + E(Y0 (θ1p ) − Y0 (θ0p ))
                                  |         {z           } |           {z           }
                                           DE(1)                      IE(0)

                                = E(Y1 (θ0p ) − Y0 (θ0p )) + E(Y1 (θ1p ) − Y1 (θ0p )) .
                                  |         {z           } |           {z           }
                                           DE(0)                      IE(1)


   The literature of mediation analysis deals with the problem of confounding effects of unobserved

inputs and the potential technology changes by invoking different assumptions. We now examine

those assumptions.

   The standard literature on mediation analysis in psychology regresses outputs on mediator

inputs (Baron and Kenny, 1986). The assumptions required to give these regressions a causal

interpretation are usually not explicitly stated. This approach often adopts the strong assumption of

no variation in unmeasured inputs conditional on the treatment. Under this assumption, measured

and unmeasured inputs are statistically independent. Moreover, the effect of unmeasured inputs

θdu are fully summarized by a dummy variable for treatment status. In addition, this literature

assumes full invariance of the production function, that is, f1 (·) = f0 (·). Under these assumptions,

function (3) reduces to


                                          Yd = f (θdp , d, X)                                     (8)


which can readily be identified and estimated. A similar framework is also used by in Pearl (2001).

   Imai et al. (2011, 2010) present a different analysis and invoke two conditions in their Sequential

Ignorability Assumption. Their approach does not explicitly account for unobserved inputs. They

invoke statistical relationships that can be interpreted as a double randomization, i.e., they assume

that both treatment status and measured inputs are randomized. More specifically, their approach

assumes independence of both treatment status D and measured inputs θdp with respect to Yd,θ̄dp

conditional on covariates X.



                                                       7
Assumption A-1. Sequential Ignorability (Imai et al., 2011, 2010):


                 (Yd,θ̄dp , θdp0 ) ⊥⊥ D|X; d, d0 ∈ {0, 1}                                         (i)

                        Yd,θ̄dp ⊥⊥ θdp0 |D, X; d, d0 ∈ {0, 1}                                    (ii)

  0 < P r(D = d | X) < 1 and 0 < P r(θdp = θ | D = d, X) < 1; d ∈ {0, 1}, ∀θdp , θ ∈ supp(θdp ) (iii)


   Condition (i) of Assumption (A-1) states that both counterfactual outputs and counterfactual

measured inputs are independent of D conditional on pre-program variables. These statistical

relationships are generated by a RCT that randomly assigns treatment status D given X. Indeed,

if treatment status D were randomly assigned by a randomization protocol that conditions on pre-

program variables X, then Yd ⊥⊥ D|X (see e.g. Heckman et al. (2010) for a discussion). But proxied

and unmeasured inputs are also outcomes in a RCT, and therefore (θdp , θdu ) ⊥⊥ D|X. Condition (i)

of Assumption (A-1) is invoked to eliminate the dependence arising from the fact that for fixed X

the source of variation of Yd,θ̄dp is θdu .

   Condition (ii) declares that counterfactual outcomes given d and θ̄dp are independent of unmea-

sured inputs given the observed treatment status and the pre-program variables X. In other words,

input θdp0 is statistically independent of potential outputs when treatment is fixed at D = d and

measured inputs are fixed at θ̄dp0 conditional on treatment assignment D and same pre-program

characteristics X. The same randomization rationale used to interpret Condition (i) can be applied

to Condition (ii). Thus Condition (ii) can be understood as if a second RCT were implemented for

each treatment group such that measured inputs are randomized through a randomization protocol

conditional on pre-program variables X and treatment status D. This randomization is equivalent

to assuming that θdp ⊥
                     ⊥ θd00 for all d and d0 . Condition (iii) is a support condition that allows the

estimation of treatment effects conditioned on the values X takes. Even though the Imai et al.

(2010) and Imai et al. (2011) approach is weaker than the Pearl (2001) solution which is based on

lack of variation of unobserved inputs, their assumptions are nonetheless still quite strong.




                                                            8
   Imai et al. (2010) show that under Assumption A-1, the direct and indirect effects are given by:

                     Z
                         E(Y |θ p = t, D = d, X) dF(θp |D=1,X) (t) − dF(θp |D=0,X) (t)
                                                                                      
      E(IE(d)|X) =                                                                                (9)
                     Z
                          E(Y |θ p = t, D = 1, X) − E(Y |θ p = t, D = 0, X) dF(θp |D=1,X) (t).
                                                                           
      E(DE(d)|X) =                                                                               (10)


Pearl (2011) uses the term Mediation Formulas for Equations (9)–(10). Like Imai et al. (2010),

Pearl (2011) invokes the assumption of exogeneity on mediators conditioned on variables X to

generate these equations.

   Identification of the direct and indirect effects under the strong implicit assumption A-1, trans-

lates to an assumption of no-confounding effects on both treatment and measured inputs. This

assumption does not follow from a randomized assignment of treatment. Randomized trials ensure

independence between treatment status and counterfactual inputs/outputs, namely Yd ⊥⊥ D|X and

θdp ⊥⊥ D|X. Thus RCTs identify treatment effects for proxied inputs and for outputs. However,

random treatment assignment does not imply independent variation between proxied inputs θdp and

unmeasured inputs θdu . In particular, it does not guarantee independence between counterfactual

outputs Yd,θ̄dp , which is generated in part by θdu , and measured inputs θdp0 as assumed in Condition

(ii) of Assumption A-1.


2.1     Mediation Analysis under RCT

It is useful to clarify the strong causal relationships implied by Condition (ii) of Assumption A-1

in light of a mediation model based on a RCT. To this end, we first start by defining a standard

confounding model arising from uncontrolled pre-program unobserved variables. We then introduce

a general RCT model and establish the benefits of RCTs in comparison with models that rely on

standard matching assumptions. We then define a general mediation model with explicitly formu-

lated measured and unmeasured inputs. We then examine the causal relationships of the mediation

model that are implied by Condition (ii) of Assumption A-1. We show that the assumptions made

in Assumption A-1 are stronger than standard assumptions invoked in matching.

   A standard confounding model can be represented by three variables: (1) An output of interest


                                                   9
Y ; (2) A treatment indicator D that causes the output of interest. As before, we use D = 1 for

treated and D = 0 for untreated; (3) An unobserved variable V that causes both D and Y. A

major difference between unobserved variable V and unobserved input θdu is that V is not caused
                                                                                             dist              dist
by treatment D while we allow θdu to be determined by treatment. Thus, V1 = V0 , where =

means equal in distribution. We discuss the relationship between unobserved variables θdu and V

in presenting our mediation model.

   Model (a) of Figure 1 represents the standard confounding model as a Directed Acyclic Graph
           1
(DAG).         In this model, (Y1 , Y0 ) ⊥⊥ D does not hold due to confounding effects of unobserved

variables V . As a consequence, the observed empirical relationship between output Y and treatment

D is not causal and ATE cannot be evaluated by the conditional difference in means between treated

and untreated subjects, i.e., E(Y |D = 1) − E(Y |D = 0). Nevertheless, if V were observed, ATE
                         R
could be identified from E(Y |D = 1, V = v) − E(Y |D = 0, V = v)dFV (v) as (Y1 , Y0 ) ⊥⊥ D|V

holds.

   The literature on matching (Rosenbaum and Rubin, 1983) solves the problem of confounders

by assumption. It postulates that a set of observed pre-program variables, say X, spans the space

generated by unobserved variables V although it offers no guidance on how to select this set. Thus

it assumes that observed pre-program variables X can be found such that (Y1 , Y0 ) ⊥⊥ D|X holds.

In this case, ATE can be computed by

                                   Z
                   E(Y1 − Y0 ) =       E(Y |D = 1, X = x) − E(Y |D = 0, X = x)dFX (x).


For a review of matching assumptions and their limitations see Heckman and Navarro (2004) and

Heckman and Vytlacil (2007).

   Randomized controlled trials solve the problem of confounders by design. A standard RCT

model for confounders can be represented by five variables: (1) An output of interest Y ; (2) A

treatment indicator D that causes the output of interest and is generated by a random device R

and variables X used in the randomization protocol; (3) Pre-program variables X used in the
  1 See   Pearl (2009) and Heckman and Pinto (2012)for discussions of causality and Directed Acyclic Graphs.




                                                        10
randomization protocol; (4) A random device R that assigns treatment status. (5) An unobserved

variable V that causes both X and Y. Model (b) in Figure 1 represents the RCT model as a DAG.

   In the RCT model, potential confounding effects of unobserved variables V are eliminated by

observed variables X. ATE can be identified by

                                Z
                E(Y1 − Y0 ) =       E(Y |D = 1, X = x) − E(Y |D = 0, X = x)dFX (x).


While (Y1 , Y0 ) ⊥⊥ D|X holds in both matching and RCT models, it holds by assumption in matching

models and by design in RCT models.

   We now examine mediation analysis under the assumption that treatment status is generated

by a RCT. To this end, we explicitly include measured and unmeasured inputs (θ p , θ u ) to our RCT

framework depicted in Model (b) of Figure 1. Inputs mediate treatment effects, i.e. inputs are

caused by D and cause Y. Moreover, we also allow pre-program variables X to cause mediators

θ p , θ u . The most general mediation model is described by the following relationships: (R1) media-

tors θ p , θ u are caused by unobserved variable V ; and (R2) measured inputs can cause unmeasured

ones and vice-versa. Model (c) of Figure 1 represents this mediation model for RCT as a Directed

Acyclic Graph (DAG).

   A production function representation that rationalizes the mediation model is


                                    Yd = fd (θdp , θdu , V , X), d ∈ {0, 1}.                    (11)


Equation (11) differs from Equation (3) by explicitly introducing pre-program unobserved variables

V . Yd,θ̄p is now defined as:


                                        Yd,θ̄p = fd (θ̄dp , θdu , V , X).                       (12)


It is the variation in θdu , V , and X that generate randomness in outcome Yd , fixing θ̄ p .

   We gain further insight into Assumption A-1 by examining it in light of the mediation model.

The mediation model is constructed under the assumption that treatment status is generated by a


                                                      11
RCT. Therefore Condition (i) of Assumption A-1 holds. However randomization does not generate

Condition (ii) of Assumption A-1. If either R1 or R2 occurs, measured and unmeasured inputs will

                                                                                       ⊥ θdp0 |D, X.
not be independent conditioned on observed variables (D, X). As a consequence, Yd,θ̄dp ⊥
                                                                                       

Model (d) of Figure 1 represents a mediation model in which Assumption A-1 holds, but neither

R1 nor R2 occurs.
   Condition (ii) is stronger than the conditions invoked in conventional matching analyses. Indeed,
if V is assumed to be observed (a matching assumption), then relationship R1 reduces to a causal
relationship among observed variables. Nevertheless, the matching assumption does not rule out R2.
Relationship R2 would not apply if we adopt the strong assumption that unmeasured inputs have
no variation conditional on the treatment.2 The no-variation assumption assures that measured and
unmeasured mediators are statistically independent conditional on D. This model is represented
as a DAG in Model (e) of Figure 1. Pearl (2001) shows why Condition (ii) will not hold for
Model (e) of Figure 1. However the direct effect (Equation 5) can be computed by Condition (ii):


 DE(d) =
 Z Z                                                                    Z
     (E(Y |D = 1, θ p = t, V = v) − E(Y |D = 0, θ p = t, V = v)) dFV (v) dFθp |D=d,X=x,V =v (t)dFX (x).




   A general solution to the mediation problem is outside the scope of this paper. Instead we

use a linear model to investigate how experimental variation coupled with additional econometric

exogeneity assumptions can produce a credible mediation analysis for the case where some inputs

are unobserved (but may be changed by the experiment) and proxied variables θ p are measured

with error. Our analysis is based on the production function defined in Equation (3). We assume

that the map between inputs θdp , θdu and output Yd is given by a linear function. We then show how

multiple measures on inputs and certain assumptions about the exogeneity of inputs allow us to

test for invariance, i.e. whether f1 (·) is equal to f0 (·). Alternatively, invoking invariance we show

how to test the hypothesis that increments in θdp are statistically independent of θdu .
  2 Pearl   (2001) invokes this assumption.




                                                  12
3     A Linear Model for Meditation Analysis

We focus on examining a linear model for the production function of output in sector d. The benefit

of the linear model stems from its parsimony in parameters, which facilitates reliable estimation in

small samples. Non-linear or non-parametric procedures require large samples often not available

in RCTs. We write:

                                Yd = κd + αd θd + βd X + ˜d , d ∈ {0, 1},                     (13)


where κd is an intercept, αd and βd are, respectively, |J |-dimensional and |X|-dimensional vectors

of parameters where |Q| denotes the number of elements in Q. Pre-program variables X are

assumed not to be affected by the treatment, their effect on Y can be affected by the treatment. ˜d

is a zero-mean error term assumed to be independent of regressors θd and X.

    Technology (13) is compatible with a Cobb-Douglas model using linearity in logs. Thus an

alternative to (13) is


                         log(Yd ) = κd + αd log(θd ) + βd log(X) + ˜d , or                    (14)

                             Yd = κd + αd log(θd ) + βd log(X) + ˜d , d ∈ {0, 1}.             (15)


We discuss the estimation of θd in Section 3.1. There, we also adopt a linear specification for the

measurement system that links unobserved inputs θ with measurements M. The Cobb-Douglas

specification can be applied to the linear measurement system by adopting a linear-in-logs specifi-

cation in the same fashion as used in outcome equations (14)–(15).

    Analysts of experiments often collect an array of measures of the inputs. However, it is very

likely that there are relevant inputs not measured. We decompose the term αd θd in equation (13)




                                                    13
into components due to inputs that are measured and inputs that are not:

                                 X
                     Yd = κd +          αdj θdj + βd X + ˜d                                       (16)
                                 j∈J
                                       X                         X
                       = κd +               αdj θdj   +                    αdj θdj   +βd X + ˜d
                                    j∈Jp                     j∈J \Jp
                                    |      {z     }          |        {z        }
                                 inputs on which we        inputs on which we
                                 have measurements        have no measurements
                                 X
                       = τd +           αdj θdj + βd X + d ,
                                 j∈Jp



where d ∈ {0, 1}, τd = κd + j∈J \Jp αdj E(θdj ), and d is a zero-mean error term defined by d =
                                P

˜d + j∈J \Jp αdj (θdj − E(θdj )). Any differences in the error terms between treatment and control
     P

groups can be attributed to differences in the inputs on which we have no measurements. Without
                                           dist                dist
loss of generality we assume that ˜1 = ˜0 , where = means equality in distribution. Note that

the error term d is correlated with the measured inputs if measured inputs are correlated with

unmeasured inputs.

   We seek to decompose treatment effects into components attributable to changes in the inputs

that we can measure. Assuming that changes in unmeasured inputs attributable to the experiment

are independent of X, treatment effects can be decomposed into components due to changes in

inputs E(∆θj ) and components due to changes in parameters ∆αj (= α1j − α0j ):


                E(∆Yd |X) =E(Y1 − Y0 |X)                                                           (17)
                                                               
                                              X j j           
                            =(τ1 − τ0 ) + E    α1 θ1 − α0j θ0j  + (β1 − β0 ) X
                                                      j∈Jp


                            = (τ1 − τ0 )
                                 X                           
                                         ∆αj + α0j E ∆θj + ∆αj E θ0j
                                                        
                              +
                                   j∈Jp

                                 + (β1 − β0 )X.3




                                                        14
    Equation (17) can be simplified if treatment affects inputs, but not the impact of inputs and

background variables on outcomes, i.e. α1j = α0j ; j ∈ Jp and β1 = β0 .4 This says that all treatment

effects are due to changes in inputs. Under this assumption, the term associated with X drops from

the decomposition. Note that under this assumption there still may be a direct effect (Equation 5)

but it arises from experimentally induced shifts in unmeasured inputs.

    If measured and unmeasured inputs are independent in the no-treatment outcome equation, α0

can be consistently estimated by standard methods. Under this assumption, we can test if the

experimentally-induced increments in unmeasured inputs are independent of the experimentally

induced increments in measured inputs. This allows us to test a portion of Condition (ii) of

Assumption A-1. The intuition for this test is as follows. The inputs for treated participants

are the sum of the inputs they would have had if they were assigned to the control group plus

the increment due to treatment. If measured and unmeasured input increments are independent,

α1 is consistently estimated by standard methods and we can test H0 : plim α̂1 = plim α̂0

where (α̂1 , α̂0 ) are least squares estimators of (α1 , α0 ). Notice that even if α̂0 is not consistently

estimated, the test of the independence of the increments from the base is generally valid. Assuming

the exogeneity of X, we can also test if plim β̂1 = plim β̂0 .

    Note further that if we maintain that measured inputs are independent of unmeasured inputs

for both treatment and control groups, we can test the hypothesis of autonomy H0 : α1 = α0 .

Thus there are two different ways to use the data from an experiment (a) to test the independence

of the increments given that unmeasured inputs are independent of measured inputs or (b) to test

H0 : α1 = α0 maintaining full independence.

    Imposing autonomy simplifies the notation. Below we show conditions under which we can test
   3 Alternativedecompositions are discussed below in section 6.1.
   4 Theseare called structural invariance or autonomy assumptions in the econometric literature. See, e.g., Hurwicz
(1962). These assumptions do not rule out heterogenous responses to treatment because θ1 and θ0 may vary in the
population.




                                                        15
for autonomy. Equation (16) can be expressed as

                                           X
                              Yd = τd +           αj θj + βX + d , d ∈ {0, 1}.                             (18)
                                           j J


In this notation, the observed outcome can be written as:

                            X                                                    X
             Y = D (τ1 +           αj θ1j + βX + 1 ) +(1 − D) (τ0 +                   αj θ0j + βX + 0 )   (19)
                            j∈Jp                                                j∈Jp
                     |               {z               }                   |              {z            }
                                     Y1                                                  Y0
                               X
                = τ0 + τ D +          αj θj + βX + ,
                               j∈Jp


where τ = τ1 − τ0 is the contribution of unmeasured variables to mean treatment effects,  =

D1 + (1 − D)0 is a zero-mean error term, and θj = Dθ1j + (1 − D)θ0j , j ∈ Jp denotes the inputs

that we can measure.

   If the θdj , j ∈ Jp are measured without error and are independent of the error term , least

squares estimators of the parameters of equation (19) are unbiased for αj , j ∈ Jp . If, on the other

hand, the unmeasured inputs are correlated with both measured inputs and outputs, least squares

estimators of αj , j ∈ Jp , are biased and capture the effect of changes in the unmeasured inputs as

they are projected onto the measured components of θ, in addition to the direct effects of changes

in measured components of θ on Y .

   The average treatment effect is

                                                                      X
                         E(Y1 − Y0 ) =         (τ1 − τ0 )         +           αj E(θ1j − θ0j ) .            (20)
                                               | {z }
                                                                      j∈Jp
                                           treatment effect due
                                          to unmeasured inputs        |           {z          }
                                                                      treatment effect due
                                                                       to measured inputs



Input j can explain treatment effects only if it affects outcomes (αj 6= 0) and, on average, is affected

by the experiment (E(θ1j − θ0j ) 6= 0). Using experimental data it is possible to test both conditions.

   Decomposition (20) would be straightforward to identify if the measured variables are indepen-

dent of the unmeasured variables, and the measurements are accurate. The input term of Equation


                                                          16
(20) is easily constructed by using consistent estimates of the αj and the effects of treatment on in-

puts. However, measurements of inputs are often riddled with measurement error. We next address

this problem.


3.1       Addressing the Problem of Measurement Error

We assume access to multiple measures on each input. This arises often in many studies related to

the technology of human skill formation. For example, there are multiple psychological measures

of the same underlying development trait. (See e.g., Cunha and Heckman (2008) and Cunha et al.

(2010)). More formally, let the index set for measures associated with factor j ∈ Jp be Mj . Denote
                              j                j
the measures for factor j by Mm j ,d , where m   ∈ Mj , d ∈ {0, 1}. θd denotes the vector of factors

associated with the inputs that can be measured in treatment state d, i.e., θd = (θdj : j ∈ Jp ), d ∈

{0, 1}.

   We assume that each input measure is associated with at most one factor. The following equation

describes the relationship between the measures associated with factor j and the factor:


                                j         j     j   j    j              j   j
                    Measures : Mm j ,d = νmj + ϕmj θd + ηmj , j ∈ Jp , m ∈ M .                   (21)


                                                                         j
To simplify the notation, we keep the covariates X implicit. Parameters νm j are measure-specific


intercepts. Parameters ϕjmj are factor loadings. The d in (18) and ηm
                                                                     j
                                                                       j are mean-zero error terms


assumed to be independent of θd , d ∈ {0, 1}, and of each other. The factor structure is characterized

by the following equations:


                               Factor Means : E[θdj ] = µjd , j ∈ Jp                             (22)

                           Factor Covariance : Var[θd ] = Σθd , d ∈ {0, 1}.                      (23)


                                       j      j         j       j   j
   The assumption that the parameters νm j , ϕmj , Var(ηmj ) : m ∈ M , j ∈ Jp , do not depend on d


simplifies the notation, as well as the interpretation of the estimates obtained from our procedure.

It implies that the effect of treatment on the measured inputs operates only through the latent



                                                 17
inputs and not the measurement system for those inputs. However, these assumptions are not

strictly required. They can be tested by estimating these parameters separately for treatment and

control groups and checking if measurement equation factor loadings and measurement equation

intercepts differ between treatment and control groups.



4     Identification

Identification of factor models requires normalizations that set the location and scale of the factors

(e.g., Anderson and Rubin, 1956). We set the location of each factor by fixing the intercepts of one

measure—designated “the first”—to zero, i.e. ν1j = 0, j ∈ Jp . This defines the location of factor j

for each counterfactual condition. We set the scale of the factor by fixing the factor loadings of the

first measure of each skill to one, i.e. ϕj1 = 1, j ∈ Jp . For all measures that are related to a factor (i.e.

have a non-zero loading on the factor, ϕjmj ), the decomposition of treatment effects presented in this

paper is invariant to the choice of which measure is designated as the “first measure” for each factor

provided that the normalizing measure has a non-zero loading on the input. The decompositions

are also invariant to any affine transformations of the measures. Our procedure can be generalized

to monotonic nonlinear transformations of the measures.

    Identification is established in four steps. First, we identify the means of the factors, µjd . Second,
                                                                      j
we identify the measurement factor loadings ϕjmj , the variances Var(ηm j ) of the measurement


system, and the factor covariance structure Σθd . Third, we use the parameters identified from the
                                                                               j
first and second steps to secure identification of the measurement intercepts νm j . Finally, we use


the parameters identified in the first three steps to identify the factor loadings α and intercept τd

of the outcome equations. We discuss each of these steps.


1. Factor Means We identify µj1 and µj0 from the mean of the designated first measure for
                                 j
treatment and control groups: E(M1,d ) = µjd , j ∈ Jp , d ∈ {0, 1}.


2. Measurement Loadings From the covariance structure of the measurement system, we

can identify: (a) the factor loadings of the measurement system ϕjmj ; (b) the variances of the


                                                     18
                              j
measurement error terms, Var(ηm j ); and (c) the factor covariance matrix, Σθd . Factors are allowed


to be freely correlated. We need at least three measures for each input j ∈ Jp , all with non-zero

factor loadings. The ϕjmj can depend on d ∈ {0, 1}, and we can identify ϕjmj ,d . Thus we can test if

H0 : ϕjmj ,1 = ϕjmj ,0 , j ∈ Jp , and do not have to impose autonomy on the measurement system.

                                                                                 j           j
3. Measurement Intercepts            From the means of the measurements, i.e. E(Mm j ,d ) = νmj +


ϕjmj µjd , we identify νm
                        j      j   j                                                j
                          j , m ∈ M \{1}, j ∈ Jp . Recall that the factor loadings ϕmj and factor means


µjd are identified. Assuming equality of the intercepts (νm
                                                          j
                                                            j ) between treatment and control groups

                                                       j             j
guarantees that treatment effects on measures, i.e. E(Mm j ,1 ) − E(Mmj ,0 ), operate solely through


treatment effects on factor means, i.e. µj1 −µj0 . However, identification of our decomposition requires
                                                                                           j         j
intercept equality only for the designated first measure of each factor. We can test H0 : νm j ,1 = νmj ,0


for all mj ∈ Mj \ {1}, j ∈ Jp , and hence do not have to impose autonomy on the full measurement

system.


4. Outcome Equation Outcome factor loadings in equation (18) can be identified using the

covariances between outcomes and the designated first measure of each input. We form the co-

variances of each outcome Yd with the designated first measure of each input j ∈ Jp to obtain

Cov(Yd , M1,d ) = Σθd α where α = (αj ; j ∈ Jp ). By the previous argument, Σθd is identified.

Thus α is identified whenever det(Σθd ) 6= 0. We do not have to impose autonomy or structural

invariance. Outcome factor loadings α can depend on d ∈ {0, 1}, as they can be identified through

Cov(Yd , M1,d ) = Σθd αd which can be identified separately for treatments and controls. We can

test H0 : α1j = α0j , j ∈ Jp . Using E(Yd ), we can identify τd because all of the other parameters of

each outcome equation are identified.



5    Estimation Procedure

We can estimate the model using a simple three stage procedure. First, we estimate the measure-

ment system. Second, from these equations we can estimate the skills for each participant. Third,

we estimate the relationship between participant skills and outcomes. Proceeding in this fashion


                                                   19
makes identification and estimation transparent.


Step 1: For a given set of dedicated measurements, and choice of the number of factors, we

estimate the factor model using measurement system (21)–(23). There are several widely used

procedures to determine the number of factors. Examples of these procedures are the scree test

(Cattell, 1966), Onatski’s criterion (2009), and Horn’s (1965) parallel analysis test. In addition, the

Guttman-Kaiser rule (Guttman, 1954, and Kaiser, 1960, 1961) is well known to overestimate the

number of factors (see Zwick and Velicer, 1986, Gorsuch, 2003, and Thompson, 2004). We refer to

Heckman et al. (2013) for a detailed discussion of the selection of number of factors.


Step 2: We use the measures and factor loadings estimated in the first step to compute a vector

of factor scores for each participant i. We form unbiased estimates of the true vector of skills

θi = (θij ; j ∈ Jp ) for agent i. The factor measure equations contain X which we suppress to

simplify the expressions. Notationally, we represent the measurement system for agent i as


                                        M =          ϕ        θi + ηi ,                                        (24)
                                        |{z}i       |{z}     |{z}  |{z}
                                       |M|×1     |M|×|Jp | |Jp |×1   |M|×1


where ϕ represents a matrix of the factor loadings estimated in first step and Mi is the vector of
                                                               j
stacked measures for participant i subtracting the intercepts νm j of equation (21). The dimension of


each element in equation (24) is shown beneath it, where M = ∪j∈Jp Mj is the union of all the index

sets of the measures. The error term for agent i, ηi , has zero mean and is independent of the vector

of skills θi . Cov(ηi , ηi ) = Ω. The most commonly used estimator of factor scores is based on a linear

function of measures: θS,i = L0 Mi . Unbiasedness requires that L0 ϕ = I|J | , where I|J | is a |J |-

dimensional identity matrix.5 To achieve unbiasedness, L must satisfy L0 = (ϕ0 Ω −1 ϕ)−1 ϕ0 Ω −1 .

The unbiased estimator of the factor is:


                                  θS,i = L0 Mi = (ϕ0 Ω −1 ϕ)−1 ϕ0 Ω −1 Mi .
   5 The method is due to Bartlett (1937) and is based on the restricted minimization of mean squared error, subject
to L0 ϕ = I|J | .




                                                        20
Factor score estimates can be interpreted as the output of a GLS estimation procedure where

measures are taken as dependent variables and factor loadings are treated as regressors. By the

Gauss-Markov theorem, for a known ϕ the proposed estimator is the best linear unbiased estimator

of the vector of inputs θi .6


Step 3: The use of factor scores instead of the true factors to estimate equation (18) generates

biased estimates of outcome coefficients α. Even though estimates of θi are unbiased, there is still

a discrepancy between the true and measured θi due to estimation error. To correct for the bias,

we propose a bias-correction procedure. Because we estimate the variance of θ and the variance of

the measurement errors in the first step of our procedure, we can eliminate the bias created by the

measurement error.

   Consider the outcome model for agent i :


                                            Yi = αθi + γZi + i ,                                           (25)


where (θi , Zi ) ⊥⊥ i and E(i ) = 0. For brevity of notation, we use Zi to denote pre-program

variables, treatment status indicators, and the intercept term of equation (18). From equation (24),

the factor scores θS,i can be written as the inputs θi plus a measurement error Vi , that is,


                         θS,i = θi + Vi such that (Zi , θi ) ⊥⊥ Vi and E(Vi ) = 0.                          (26)


Replacing θi with θS,i yields Yi = αθS,i + γZi + i − αVi . The linear regression estimator of α and
  6 Note that the assumption that ϕ is known can be replaced with the assumption that ϕ is consistently estimated
and we can use an asymptotic version of the Gauss-Markov theorem replacing “unbiased” with “unbiased in large
samples”. Standard GMM methods can be applied.




                                                       21
γ is inconsistent:

                        
                      α̂
               plim     =                                                                        (27)
                        
                      γ̂
                                                    −1                                   
                     Cov(θS , θS )      Cov(θS , Z)   Cov(θ, θ)            Cov(θ, Z)   α 
                                                                                         .
                      Cov(Z, θS )        Cov(Z, Z)         Cov(Z, θ)          Cov(Z, Z)     γ
                    |                                 {z                                }
                                                         A


This is the multivariate version of the standard one-variable attenuation bias formula. All covari-

ances in A can be computed directly except for the terms that involve θ. Cov(θ, θ) is estimated in

step (1). Using equation (26), we can compute Cov(Z, θS ) = Cov(Z, θ). Thus, A is identified. Our

bias-correction procedure consists of pre-multiplying the least squares estimators (α̂, γ̂) by A−1 ,

thus providing consistent estimates of (α, γ).7 A one step maximum likelihood procedure, while

less intuitive, directly estimates the parameters without constructing the factors and accounts for

measurement error. It is justified in large samples under standard regularity conditions.



6       Invariance to Transformation of Measures

We present some invariance results regarding the decomposition of treatment effects under transfor-

mations of the measures used to proxy the inputs. Our analysis is divided into two parts. Section 6.1

examines the invariance of the decomposition for affine transformation of measures under the linear

model discussed in the previous section. Section 6.2 relaxes the linearity assumption of Section 6.1

and discusses some generalized results for the case of non-linear monotonic transformations using

the analysis of Cunha et al. (2010).


6.1         Invariance to Affine Transformations of Measures

We first establish conditions under which outcome decomposition (20), relating treatment effects to

experimentally induced changes in inputs, is invariant to affine transforms of any measure of input
    7 See   Croon (2002) for more details on this bias correction approach.


                                                             22
for any factor. Decomposition (20) assumes α1 = α0 . We also consider forming decompositions for

the more general nonautonomous case where α1 6= α0 . We establish the invariance of the treatment

effect due to measured inputs (see Equation (20)) but not of other terms in the decompositions

that arise in the more general case. Throughout we assume autonomy of the measurement system

so that intercepts and factor loadings are the same for treatments and controls for all measurement

equations. Our analysis can be generalized to deal with that case but at the cost of greater notational

complexity.
                                                                                                j
    Before presenting a formal analysis, it is useful to present an intuitive motivation. Let M̃m j ,d be

                                         j                             j   j
an affine transformation of the measure Mm j ,d , for some j ∈ Jp and m ∈ M . Specifically, define

  j
M̃m j ,d by:




            j          j
          M̃m j ,d = aMmj ,d + b such that a ∈ R \ {0}, b ∈ R, and d ∈ {0, 1}, for all j ∈ Jp .     (28)


Let ϕ̃jmj , η̃m
              j       j
                j , ν̃mj be the factor loading, error term and intercept associated with the transformed

           j
measure M̃m  j ,d , d ∈ {0, 1}. The key condition for the invariance of decomposition (20) to linear
                                                    X
transformations of the different measures is that       αj E(θ1j − θ0j ) be invariant.
                                                          j∈Jp
    We apply the same normalization to the transformed system as we do to the original system.

Suppose that the measure transformed is a “first measure” so mj = 1. Recall that in the original

system, ν1j = 0 and ϕj1 = 1. Transformation (28) can be expressed as


                                            j
                                          M̃1,d = b + aθdj + aη1j .


    Applying the normalization rule to this equation defines factor θ̃j = b + aθj , i.e. the scale and

the location of the factor are changed, so that in the transformed system the intercept is 0 and the

factor loading 1:
                                                   j
                                                 M̃1,d = θ̃dj + η̃1j


where η̃1j = aη1j is a rescaled mean zero error term. This transformation propagates through the

entire system, where θdj is replaced by θ̃dj .


                                                         23
   Notice that in decomposition (20), the induced shift in the mean of the factor is irrelevant. It

differences out in the decomposition. The scale of θj is affected. The covariance matrix Σθd is

transformed to Σθ̃d where

                                            Σθ˜d = Ia Σθd Ia


where Ia is a square diagonal matrix of the same dimension as the number of measured factors and

the j th diagonal is a and the other elements are unity. The factor loading for the outcome function

for the set of transformed first measures, M̃1,d = M1,d Ia , is the solution to the system of equations


                                       Cov(Yd , M̃1,d ) = Σθ̃d α̃d .


Thus


                                   α̃d = Σθ̃−1 Cov(Yd , M̃1,d )
                                             d


                                       =   I−1 −1 −1
                                            a Σθd Ia     Cov(Yd , M̃1,d )

                                       = I−1 −1
                                          a Σθd Cov(Yd , M1,d )

                                       = I−1
                                          a αd .



Since θ̃d = Ia θd , it follows trivially that decomposition (20), α0D (θ1 − θ0 ), is invariant to transfor-

mations.

   Suppose next that the transformation is applied to any measure other than a first measure. In-

voking the same kind of reasoning, it is evident that θ̃d = θd and α̃d = αd . Thus the decomposition

is invariant. Clearly, however, the intercept of the transformed measure becomes


                                               j           j
                                             ν̃m j
                                                   = b + aνm j




and the factor loading becomes

                                              ϕ̃jmj = ϕjmj a.




                                                    24
   The preceding analysis assumes that the outcome system is autonomous: α0 = α1 , and β0 = β1 .

Suppose that α1 6= α0 . To simplify the argument, we continue to assume that β0 = β1 . In this

case

                                  E(Y1 − Y0 ) = E(α01 θ1 − α00 θ0 ).


   In the general case, the decomposition is not unique due to a standard index number problem.

Using the notation ∆α = α1 − α0 ,


                         E(Y1 − Y0 ) = α00 E(θ1 − θ0 ) +         (∆α)0 E(θ1 )
                                       |     {z     }            |   {z    }
                                        invariant to affine   non invariant to affine
                                        transformations of      transformations of
                                             measures                measures

                                     = α01 E(θ1 − θ0 ) −          (∆α)E(θ0 )            .
                                       |     {z     }             |  {z   }
                                        invariant to affine   non invariant to affine
                                        transformations of      transformations of
                                             measures                measures


For any α∗ that is an affine transformation of (α0 , α1 ):


               E(Y1 − Y0 ) = (α∗ )E(θ1 − θ0 ) + (α1 − α∗ )E(θ1 ) − (α0 − α∗ )E(θ0 ).


For all three decompositions, the first set of terms associated with the mean change in skills due to

treatment is invariant to affine transformations. The proof follows from the preceding reasoning.

Any scaling of the factors is offset by the revised scaling of the factor loadings.

   Notice, however that when α1 6= α0 , in constructing decompositions of treatment effects we

acquire terms in the level of the factors. For transformations to the first measure, the change in

the location is shifted. Even though the scales of (∆α) and E(θd ) offset, there is no compensating

shift in the location of the factor. Thus the terms associated with the levels of the factor are not,

in general invariant to affine transformations of first measures although the decompositions are

invariant to monotonic transformations of any non-normalization measures. Obviously the point of

evaluation against E(θ1 − θ0 ) is evaluated depends on the choice of α0 , α1 , and α∗ if they differ.

   We now formally establish these results. It is enough to consider the transformation of one

measure within group j for treatment category d. First, suppose that the transformation (28) is not


                                                    25
applied to the first measure, that is, mj 6= 1. In this case, E(θ1j −θ0j ); j ∈ Jp are invariant as they are

identified through the first measure of each factor which is not changed. We can also show that the

αj , j ∈ Jp , are invariant. We identify α = [αj ; j ∈ Jp ] through Cov(Yd , M1,d ) = Σθd α. Therefore

it suffices to show that covariance matrix Σθd is invariant under the linear transformation (28).

But the covariance between the factors is identified through the first measure of each factor. The

variance of factor j under transformation (28) is identified by:

            j       j          j      j                       j       j          j      j
       Cov(M1,d , M̃m,d ) Cov(M1,d , Mm 0 ,d )           Cov(M1,d , aMm,d ) Cov(M1,d , Mm 0 ,d )

                      j      j
                                                 =                      j      j
                                                                                                    by (28)
                Cov(M̃m,d , Mm 0 ,d )                             Cov(aMm,d , Mm 0 ,d )

                                                                j      j          j      j
                                                         a Cov(M1,d , Mm,d ) Cov(M1,d , Mm 0 ,d )
                                                 =                       j      j
                                                                  a Cov(Mm,d , Mm 0 ,d )

                                                              j      j          j      j
                                                         Cov(M1,d , Mm,d ) Cov(M1,d , Mm 0 ,d )
                                                 =                     j      j
                                                                  Cov(Mm,d , Mm 0 ,d )



                                                 = Var(θdj ),


so that the variance is unchanged. Hence αd is unchanged.

   Now suppose that transformation (28) is applied to the first measure, mj = 1. In this case, the

new variance of factor j is given by:

                  j      j           j      j                        j      j           j      j
            Cov(M̃1,d , Mm,d ) Cov(M̃1,d , Mm 0 ,d )          a Cov(M1,d , Mm,d )a Cov(M1,d , Mm 0 ,d )

                          j      j
                                                          =                   j      j
                     Cov(Mm,d , Mm 0 ,d )                                Cov(Mm,d , Mm 0 ,d )



                                                          = a2 Var(θdj ).                                     (29)


The new covariance between factors j and j 0 is given by:

                                                     0                           0
                                        j      j              j      j
                                  Cov(M̃1,d , M1,d ) = a Cov(M1,d , M1,d )
                                                                            0
                                                           = a Cov(θdj , θdj )                                (30)


Let Σ̃θd be the new factor covariance matrix obtained under transformation (28). According to

equations (29)–(30), Σ̃θd = Ia Σθd Ia , where, as before, Ia is a square diagonal matrix whose j-



                                                            26
th diagonal element is a and has ones for the remaining diagonal elements. By the same type

of reasoning, we have that the covariance matrix Cov(Yd , M1,d ) computed under the transforma-

tion is given by: Cov(Yd , M̃1,d ) = Ia Cov(Yd , M1,d ). Let α̃ be the outcome factor loadings under

transformation (28). Thus,


                       Ia Cov(Yd , M1,d ) = Cov(Yd , M̃1,d ) = Σ̃θd α̃ = Ia Σθd Ia α̃               (31)


and therefore α̃ = I−1
                    a α. In other words, transformation (28) only modifies the j-th factor loading
                          αj
which is given by α̃j =   a .
                                                                            0           0   0
      Let the difference in factor means between treatment groups be ∆j = E(θ1j − θ0j ), j 0 ∈ Jp , and
    ˜j 0 be the difference under transformation (28). Transformation (28) only modifies the j-th
let ∆
                                                                                      0   0    0  0
difference in means which is given by ∆˜j = a∆j and thereby α̃j ∆˜j = αj ∆j . Thus α̃j ∆˜j = αj ∆j =
  0       0     0
αj E(θ1j − θ0j ) for all j 0 ∈ Jp , as claimed. It is straightforward to establish that if α1 6= α0 , the

decomposition is, in general, not invariant to affine transformations, although the term associated

with E(θ1 − θ0 ) is.


6.2      A Sketch of More General Invariance Results

We next briefly consider a more general framework. We draw on the analysis of Cunha et al.

(2010) to extend the discussion of the preceding subsection to a nonlinear nonparametric setting.

We present two basic results: (1) outcome decomposition terms that are locally linear in θ are

invariant to monotonic transformations of θ; and (2) terms associated with shifts in the technology

due to the experimental manipulation are not. In this section we allow inputs to be measured with

error but assume that unmeasured inputs are independent of the proxied ones. We focus only on

invariance results and only sketch the main ideas.

      Here we sketch the main results. Following the previous notation, we use D for the binary

treatment status indicator, D = 1 for treated and D = 0 for control. We denote Yd ; d ∈ {0, 1} to

denote the output Y when treatment D is fixed at value d. In the fashion, θd ; d ∈ {0, 1} denotes

the input θ when treatment D is fixed at value d. For sake of simplicity, let the production function


                                                    27
be given by f : supp(θ) → supp(Y ) where supp means support. Thus Yd = f (θd ); d ∈ {0, 1}.

   We analyze both the invariant and noninvariant case. We relax the the invariance assumption

for the production function by indexing it by treatment status. We use fd : supp(θ) → supp(Y ) to

denote the production function that governs the data generating process associated with treatment

status D = d.

   In this notation, the average treatment effect is given by:


                                  E(Y1 − Y0 ) = E(f1 (θ1 ) − f0 (θ0 )).                           (32)


Equation (32) repeats the discussion in Section 2 that there are two sources of treatment effects

exist: (1) treatment might shift the map between θ and the outcomes from f0 to f1 (i.e. it might

violate invariance); and (2) treatment might also change the inputs from θ0 to θ1 .

   Assume the existence of multiple measures of θ that are generated through an unknown function

M : supp(θ) → supp(M ) that is monotonic in θ. Then, under conditions specified in Cunha,

Heckman, and Schennach (2010), the marginal distributions of θ1 and θ0 can be non-parametrically

identified (although not necessarily the joint distribution of θ1 and θ0 ). We develop the scalar case.

Theorem T-1. The scalar case: Let the production function be a uniformly differentiable scalar

function fd : supp(θ) → supp(Y ); d ∈ {0, 1, }. If the production function is autonomous, i.e.

f1 (t) = f0 (t) ∀ t ∈ supp(θ), then the effect attributable to changes in θ is invariant to monotonic

transformations M of θ.

Proof. Without loss of generality, write the input for treated in terms of the input for untreated

plus the difference across inputs. Thus θ1 = θ0 + ∆. Now, under structural invariance:


                   Y1 − Y0 =f (M (θ1 )) − f (M (θ0 )) = f (M (θ0 + ∆)) − f (M (θ0 )).


From uniform differentiability of M and f we have that:

                                          Y1 − Y0   ∂f ∂M
                                       lim        =       .
                                      ∆→0    ∆      ∂M ∂θ


                                                   28
Thus the infinitesimal contribution of a change in input to output can be decomposed as:

                                                           ∂f ∂M
                                         d(Y1 − Y0 ) =           dθ.
                                                           ∂M ∂θ

If we use θ as the argument of the function, under conditions specified in Cunha et al. (2010)
                                        ∂f ∂M
nonparametric regression identifies     ∂M ∂θ .   If we use M (θ) as the argument, nonparametric regres-
                  ∂f                                          ∂M
sion identifies   ∂M   but the increment to input is now      ∂θ dθ.   The combined terms for the output

decomposition remain the same in either case. Thus the decomposition is invariant to monotonic

transformations M of inputs θ. Extension to the vector case is straightforward.

     Suppose that we relax autonomy. For sake of simplicity, take the scalar case and let the input

for the treated input be written as θ1 = θ0 + ∆. In this case, we can write the total change in output

induced by treatment as:


              Y1 − Y0 = f1 (M (θ1 )) − f0 (M (θ0 ))

                         = f1 (M (θ0 + ∆)) − f0 (M (θ0 ))
                                                                                    
                         = f1 (M (θ0 + ∆)) − f1 (M (θ0 )) + f1 (M (θ0 )) − f0 (M (θ0 )) .


If we rework the rationale of the proof for Theorem T-1 and apply the intermediate value theorem,

we obtain the following expression:

                                      ∂f1 ∂M
                         Y1 − Y0 =               ∆θ + f1 (M (θ0 )) − f0 (M (θ0 )) .                 (33)
                                      ∂M ∂θ θ=θ∗
                                               0
                                     |      {z    } |              {z           }
                                          Invariant              Non-Invariant


where θ0∗ is an intermediate value in the interval (θ0 , θ0 + ∆). The first term is invariant for the

same reasons stated in Theorem T-1 which concerns the autonomous case. Namely, the change in
∂f                           ∂M
∂M   offsets the change in   ∂θ .

     The source of non-invariance of the second term in Equation (33) is attributed to the shift

in production function f0 to f1 due to treatment. This shift implies that the output evaluation



                                                      29
will differ when evaluated at the same input points θ0 . Under structural invariance or autonomy,

f1 (·) = f0 (·) and regardless of the transformation M, we have that f1 (M (θ0 )) = f0 (M (θ0 )) and,

therefore, the second term of Equation (33) vanishes.



7     Summary and Conclusions

Randomization identifies treatment effects for outputs and measured inputs. If there are unmea-

sured inputs that are statistically dependent on measured inputs, unaided experiments do not

identify the causal effects of measured inputs on outputs.

    This paper reviews the recent statistical mediation literature that attempts to identify the causal

effect of measured changes in inputs on treatment effects. We relate it to conventional approaches

in the econometric literature. We show that the statistical mediation literature achieves its goals

under implausibly strong assumptions. For a linear model, we relax these assumptions maintaining

exogeneity assumptions that can be partially relaxed if the analyst has access to experimental data.

Linearity gives major simplifying benefits even in the case where θdp is independent of θdu , where

the point of evaluation of mean effects does not depends on the distribution of θdu . Extension of

this analysis to the nonlinear case is a task left for future work.

    We also present results for the case where there is measurement error in the proxied inputs,

a case not considered in the statistical literature. When the analyst has multiple measurements

on the mismeasured variables, it is sometimes possible to circumvent this problem. We establish

invariance to the choice of monotonic transformations of the input measures for both linear and

nonlinear technologies.




                                                  30
References

Anderson, T. W. and H. Rubin (1956). Statistical inference in factor analysis. In J. Neyman (Ed.),

  Proceedings of the Third Berkeley Symposium on Mathematical Statistics and Probability, 5, pp.

  111–150. Berkeley: University of California Press.

Baron, R. M. and D. A. Kenny (1986). The moderator-mediator variable distinction in social psy-

  chological research: Conceptual, strategic, and statistical considerations. Journal of Personality

  and Social Psychology 51 (6), 1173–1182.

Bartlett, M. S. (1937, July). The statistical conception of mental factors. British Journal of

  Psychology 28 (1), 97–104.

Cattell, R. B. (1966). The scree test for the number of factors. Multivariate Behavioral Re-

  search 1 (2), 245–276.

Croon, M. A. (2002). Using predicted latent scores in general latent structure models. In G. A.

  Marcoulides and I. Moustaki (Eds.), Latent Variable and Latent Structure Models, pp. 195–223.

  NJ: Lawrence Erlbaum Associates, Inc.

Cunha, F. (2012). Eliciting maternal beliefs about the technology of skill formation. Presented at,

  Family Inequality Network: Family Economics and Human Capital in the Family, November 16,

  2012.

Cunha, F. and J. J. Heckman (2008, Fall). Formulating, identifying and estimating the technology

  of cognitive and noncognitive skill formation. Journal of Human Resources 43 (4), 738–782.

Cunha, F., J. J. Heckman, and S. M. Schennach (2010, May). Estimating the technology of cognitive

  and noncognitive skill formation. Econometrica 78 (3), 883–931.

Gorsuch, R. L. (2003). Factor analysis. In I. B. Weiner, D. K. Freedheim, J. A. Schinka, and W. F.

  Velicer (Eds.), Handbook of psychology: Research methods in psychology, Volume 2, Chapter 6,

  pp. 143–164. Hoboken, NJ: John Wiely & Sons, Inc.



                                                31
Guttman, L. (1954). Some necessary conditions for common-factor analysis. Psychometrika 19 (2),

  149–161.

Haavelmo, T. (1943, January). The statistical implications of a system of simultaneous equations.

  Econometrica 11 (1), 1–12.

Heckman, J., M. Holland, T. Oey, D. Olds, R. Pinto, and M. Rosales (2012). A reanalysis of

  the nurse family partnership program: The memphis randomized control trial. Unpublished

  manuscript, University of Chicago.

Heckman, J. J., S. H. Moon, R. Pinto, P. A. Savelyev, and A. Q. Yavitz (2010, August). Analyzing

  social experiments as implemented: A reexamination of the evidence from the HighScope Perry

  Preschool Program. Quantitative Economics 1 (1), 1–46.

Heckman, J. J. and S. Navarro (2004, February). Using matching, instrumental variables, and

  control functions to estimate economic choice models. Review of Economics and Statistics 86 (1),

  30–57.

Heckman, J. J. and R. Pinto (2012). Causal analysis after Haavelmo: Definitions and a unified

  analysis of identification. Unpublished manuscript, University of Chicago.

Heckman, J. J., R. Pinto, and P. A. Savelyev (2013). Understanding the mechanisms through

  which an influential early childhood program boosted adult outcomes. Unpublished manuscript,

  University of Chicago, Department of Economics (first draft, 2008). Forthcoming, American

  Economic Review.

Heckman, J. J. and E. J. Vytlacil (2007). Econometric evaluation of social programs, part II:

  Using the marginal treatment effect to organize alternative economic estimators to evaluate social

  programs and to forecast their effects in new environments. In J. Heckman and E. Leamer (Eds.),

  Handbook of Econometrics, Volume 6B, Chapter 71, pp. 4875–5143. Amsterdam: Elsevier.

Horn, J. L. (1965). A rationale and test for the number of factors in factor analysis. Psychome-

  trika 30 (2), 179–185.


                                                32
Hurwicz, L. (1962). On the structural form of interdependent systems. In E. Nagel, P. Suppes, and

  A. Tarski (Eds.), Logic, Methodology and Philosophy of Science, pp. 232–239. Stanford University

  Press.

Imai, K., L. Keele, D. Tingley, and T. Yamamoto (2011). Unpacking the black box of causal-

  ity: Learning about causal mechanisms from experimental and observational studies. American

  Political Science Review 105 (4), 765–789.

Imai, K., L. Keele, and T. Yamamoto (2010). Identification, inference and sensitivity analysis for

  causal mediation effects. Statistical Science 25 (1), 51–71.

Kaiser, H. F. (1960). The application of electronic computers to factor analysis. Educational and

  Psychological Measurement 20 (1), 141–151.

Kaiser, H. F. (1961). A note on Guttman’s lower bound for the number of common factors. British

  Journal of Statistical Psychology 14 (1), 1–2.

Klein, L. R. and A. S. Goldberger (1955). An Econometric Model of the United States, 1929–1952.

  Amsterdam: North-Holland Publishing Company.

Onatski, A. (2009). Testing hypotheses about the number of factors in large factor models. Econo-

  metrica 77 (5), 1447–1479.

Pearl, J. (2001). Direct and indirect effects. In Proceedings of the Seventeenth Conference on Uncer-

  tainty in Artificial Intelligence, pp. 411–420. San Francisco, CA: Morgan Kaufmann Publishers

  Inc.

Pearl, J. (2009). Causality: Models, Reasoning, and Inference (2nd ed.). New York: Cambridge

  University Press.

Pearl, J. (2011). The mediation formula: A guide to the assessment of causal pathways in nonlinear

  models. Forthcoming in Causality: Statistical Perspectives and Applications.




                                                   33
Robin, J.-M. (2003, December 23). Comments on ”structural equations, treatment effects and

  econometric policy evaluation” by james j. heckman and edward vytlacil. Presented at the

  Sorbonne, Paris.

Rosenbaum, P. R. and D. B. Rubin (1983, April). The central role of the propensity score in

  observational studies for causal effects. Biometrika 70 (1), 41–55.

Theil, H. (1958). Economic Forecasts and Policy. Amsterdam: North-Holland Publishing Company.

Thompson, B. (2004). Exploratory and confirmatory factor analysis : understanding concepts and

  applications. Washington, DC: American Psychological Association.

Wright, S. (1921). Correlation and causation. Journal of Agricultural Research 20, 557–585.

Wright, S. (1934). The method of path coefficients. Annals of Mathematical Statistics 5 (3), 161–215.

Zwick, W. R. and W. F. Velicer (1986). Comparison of five rules for determining the number of

  components to retain. Psychological Bulletin 99 (3), 432–442.




                                                 34
    V                                               V                                          V                                            V

    X                                               X                                          X                                            X


R   D       M        Y                   R          D               M            Y
                                                                                 R             D           M             Y              R   D   M   Y


            U                                                       U                                      U                                    U
                                 Figure 1:       Mechanisms of Causality for Treatment Effects

                                         (a) Confounding Model                                 (b) Randomization Model




            X       V                                               V                                      X            V                       V

    R       D       Y                R             D                Y                          R           D            Y           R       D   Y

                                          (c) Mediation Model                            (d) Mediation Under Assumption A-1

                X                        V
                                                                                                       V
                                       XX                           V VX
                             p                                                                     X X                      V V
        R       D                        Y                                                 p
                                                        p       R           D                          Y
                            R          D                    p       Y                                          p
                             R          D                               Y            R             D                p       Y
                                                                                          R            D                        Y
                             u
                                                        u
                                                                                           u
                                                            u                                                  u
                                                                                                                   u

                                      (e) Observed Confounders




                                             X                          V

                                                                p
                             R               D                          Y




            Notes: This chart represents five causal models as directed acyclic graphs. Arrows represent causal
            relationships. Circles represent unobserved variables. Squares represent observed variables. Y is an output
            of interest. V are unobserved variables. D is the treatment variable. X are pre-program variables. R is the
            random device used in RCT models to assign treatment status. θ p are measured inputs. θ u are unmeasured
            inputs. Both θ p and θ u play the role of mediation variables. Figure (a) shows a standard confounding
            model. Figure (b) shows a general randomized trial model. Figure (c) shows a general mediation model
            where unobserved variables V cause mediation variable (θ p , θ u ). Figure (d) shows the causal relationships
            of a mediation model that are allowed to exist for Assumption A-1 to hold. Figure (e) shows the mediation
            model presented in Pearl (2001).




                                                                                35
