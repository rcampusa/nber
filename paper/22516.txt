                              NBER WORKING PAPER SERIES




   ASSESSING POINT FORECAST ACCURACY BY STOCHASTIC ERROR DISTANCE

                                      Francis X. Diebold
                                        Minchul Shin

                                      Working Paper 22516
                              http://www.nber.org/papers/w22516


                    NATIONAL BUREAU OF ECONOMIC RESEARCH
                             1050 Massachusetts Avenue
                               Cambridge, MA 02138
                                   August 2016




We are especially grateful to the Editors (Peter C.B. Phillips and Aman Ullah) and two
anonymous referees for helpful guidance and comments. We are also grateful to Ross Askanazi,
Alex Belloni, Lorenzo Braccini, Xu Cheng, Peter Christoffersen, Valentina Corradi, Ed George,
Roger Koenker, Mai Li, Oliver Linton, Laura Liu, Essie Maasoumi, Andrew Patton, Ehsan Soofi,
Norm Swanson, Mike Steele, Allan Timmermann, Aman Ullah, Mark Watson, and Tiemen
Woutersen. We also thank seminar participants at Federal Reserve Bank of San Francisco, the
Emory University Conference in Honor of Essie Maasoumi, the European University Institute,
and the University of Pennsylvania. The usual disclaimer applies. The views expressed herein are
those of the authors and do not necessarily reflect the views of the National Bureau of Economic
Research.

NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.

© 2016 by Francis X. Diebold and Minchul Shin. All rights reserved. Short sections of text, not
to exceed two paragraphs, may be quoted without explicit permission provided that full credit,
including © notice, is given to the source.
Assessing Point Forecast Accuracy by Stochastic Error Distance
Francis X. Diebold and Minchul Shin
NBER Working Paper No. 22516
August 2016
JEL No. C52,C53

                                          ABSTRACT

We propose point forecast accuracy measures based directly on distance of the forecast-error
c.d.f. from the unit step function at 0 ("stochastic error distance," or SED). We provide a precise
characterization of the relationship between SED and standard predictive loss functions, and we
show that all such loss functions can be written as weighted SED's. The leading case is absolute-
error loss. Among other things, this suggests shifting attention away from conditional-mean
forecasts and toward conditional-median forecasts.


Francis X. Diebold
Department of Economics
University of Pennsylvania
3718 Locust Walk
Philadelphia, PA 19104-6297
and NBER
fdiebold@sas.upenn.edu

Minchul Shin
Department of Economics
University of Illinois
214 David Kinley Hall
1407 W. Gregory
Urbana, IL 61801
mincshin@illinois.edu
1       Introduction
One often wants to rank competing point forecasts by accuracy. Invariably one proceeds
by ranking by expected loss, E(L(e)), where e is forecast error and the loss function L(e)
satisfies L(0) = 0 and L(e) ≥ 0, ∀e.1 Typically, however, little thought is given to the loss
function L(e). Instead, Gauss’ centuries-old quadratic loss, L(e) = e2 , remains routinely
invoked, primarily for mathematical convenience.
    Against this background, in this paper we approach the accuracy ranking problem di-
rectly, basing rankings on the entire distribution of e. In particular, recognizing that any
reasonable loss function must satisfy L(0) = 0, we study accuracy measures based directly
on the distance between F (e), the c.d.f. of e, and F ∗ (e), the unit step function at 0,2
                                                     (
                                            ∗            0,   e<0
                                          F (e) =
                                                         1,   e ≥ 0.

F ∗ (e) is the error cdf that corresponds to perfect forecasts; hence we compare F (e) to F ∗ (e),
and we favor forecasts that minimize the integrated absolute distance between the two, or
“stochastic error distance” (SED). This approach turns out to yield useful insights with
important practical implications. We proceed as follows. In section 2 we introduce SED
loss and relate it to traditional loss functions, and in section 3 we assess the likely empirical
relevance of our basic result. In section 4 we introduce a weighted version SED and again
relate it to traditional loss functions. In section 5 we generalize SED in a way that facilitates
relating it to other divergence and distance measures, such as Cramér-von-Mises divergence
and Kolmogorov-Smirnov distance. In section 6, building on the results of section 5 for
our generalized generalized SED, we provide a complete characterization of the relationship
between generalized SED minimization and traditional expected loss minimization. We
conclude in section 7. All proofs appear in an appendix.
    1
      More general representations are possible, which recognize that the actual and forecasted values (y and
ŷ, say) need not enter loss only through their difference, which is the forecast error, e = y − ŷ. See, for
example, Patton (2015) and the references therein. We could instead rank by E(L(y, ŷ)), where the loss
function L(y, ŷ) satisfies L(y, y) = 0 and L(y, ŷ) ≥ 0, ∀y, ŷ. In the vast majority of the literature, however,
the simple L(e) form is used, and we shall follow suit here.
    2
      In an abuse of notation, throughout this paper we use “F (·)” to denote any cumulative density function.
The meaning will be clear from context.
2         Ranking Forecasts by Stochastic Error Distance
We propose simply using the distribution of e directly, ranking forecasts by stochastic dis-
tance of F (e) from F ∗ (·), the unit step function at 0. That is, we rank forecasts by
                                                       Z   ∞
                                              ∗
                                  SED(F, F ) =                 |F (e) − F ∗ (e)| de,
                                                       −∞


where smaller is better. We call SED(F, F ∗ ) the stochastic error distance. It will prove
useful for what follows to split the SED(F, F ∗ ) integral at the origin, yielding

                            SED(F, F ∗ ) = SED− (F, F ∗ ) + SED+ (F, F ∗ )
                                           Z 0            Z ∞                              (1)
                                         =     F (e) de +     (1 − F (e)) de.
                                                  −∞                    0


Hence SED(F, F ∗ ) has both “integrated c.d.f.” and “integrated survival function” compo-
nents.3 In Figure 1a we show SED(F, F ∗ ) and its components, and in Figure 1b we provide
an example of two error distributions such that one would prefer F1 to F2 under SED(F, F ∗ ).


2.1         The Relation Between SED(F, F ∗ ) and Other Loss Functions
We motivated SED(F, F ∗ ) as directly appealing and intuitive. It turns out, moreover, that
SED(F, F ∗ ) is intimately connected to one, and only one, traditionally-invoked loss function,
and it is not quadratic. We now state a key result.

Proposition 2.1 (Equivalence of SED and Expected Absolute Error Loss)
For any forecast error e, with cumulative distribution function F (e) such that E(|e|) < ∞,
we have
                                  SED(F, F ∗ ) = E(|e|).                                 (2)

That is, SED(F, F ∗ ) equals expected absolute loss for any error distribution.

Hence if one is comfortable with SED(F, F ∗ ) and wants to use it to evaluate forecast accu-
racy, then one must also be comfortable with expected absolute-error loss and want to use
it to evaluate forecast accuracy. The two criteria are identical.
                                                           R0
    3
        Note that in the symmetric case SED(F, F ∗ ) = 2       −∞
                                                                    F (e) de.




                                                           2
(a) c.d.f. of e. Under the SED(F, F ∗ ) criterion, we prefer smaller SED(F, F ∗ ) = SED− (F, F ∗ ) + SED+ (F, F ∗ ).




        (b) Two forecast error distributions. Under the SED(F, F ∗ ) criterion, we prefer F1 (e) to F2 (e).


                      Figure 1: Stochastic Error Distance (SED(F, F ∗ ))




                                                         3
Figure 2: Absolute-Error Loss vs. Squared-Error Loss, e1 ∼ N (0, 1), e2 ∼ N (µ2 , σ22 ). We
show the disagreement region in black.


3    On M AE vs. M SE Accuracy Rankings
Squared-error loss (mean-squared error, M SE, or its square root, RM SE) and absolute-error
loss (mean absolute error, M AE) are the two great workhorses of point forecast accuracy
evaluation. Primary focus, however, is generally on M SE rankings, with M AE rankings
something of a sideshow. Our results argue for a reversal of emphasis, with primary focus
on M AE. (Recall Proposition 2.1, which says that SED is M AE.) But does it matter?
That is, do M AE and M SE rankings agree, in which case it doesn’t matter which is used?
     Empirically, M SE rankings and M AE rankings agree often, but not always. Theoret-
ically, M SE rankings and M AE rankings certainly don’t have to agree – they’re simply
different loss functions – which is why both are typically calculated and examined. However,
little is known theoretically about whether and when M SE rankings and M AE rankings do




                                             4
or don’t agree; the question is largely unexplored.4
    Provision of a full answer turns out to be quite difficult, but we can obtain some results
for extremely-restrictive cases. If, for example, forecast errors are Gaussian, e ∼ N (µ, σ 2 ),
then |e| follows the folded normal distribution with mean

                                                µ2
                                                   h       µ i
                                     p
                           E(|e|) = σ 2/π exp − 2 + µ 1 − 2Φ −     .
                                               2σ              σ

Hence for unbiased forecasts (µ = 0) we have E(|e|) ∝ σ, so that M AE rankings and M SE
rankings must be identical.
    Even in the restrictive Gaussian case, however, the rankings can diverge if one (or both)
of the forecasts are biased. Consider, for example, two forecast errors, e1 ∼ N (0, 1) and
e2 ∼ N (µ2 , σ22 ), with µ2 ∈ [−1.3, 1.3] and σ2 ∈ (0, 1.3]. By numerical computation we
identify situations M AE and M SE rankings diverge, which we show in Figure 2. The
regions are not large, but they are certainly not negligible. We conjecture, moreover, that
divergences may be much more pronounced in non-Gaussian situations involving asymmetry
and/or fat tails.


4         Weighted Stochastic Error Distance
In some circumstances one may feel that the basic idea behind SED(F, F ∗ ) is appropriate,
but that divergence of F (·) from F ∗ (·) on one side of the origin is more harmful than on the
other. This leads to the idea of a weighted SED (W SED) criterion, given by a weighted
sum of SED− (F, F ∗ ) and SED+ (F, F ∗ ).
    In particular, let,

                   W SED(F, F ∗ ; τ ) = 2(1 − τ )SED(F, F ∗ )− + 2τ SED(F, F ∗ )+
                                                  Z 0               Z ∞
                                      = 2(1 − τ )     F (e) de + 2τ     (1 − F (e)) de,
                                                       −∞                   0


where τ ∈ (0, 1).5 The following result is immediate.

Proposition 4.1 (Equivalence of W SED and Expected Lin-Lin Loss)
    4
        Patton (2015) performs some related, but nevertheless different, explorations.
    5
        Note that when τ = 0.5, W SED(F, F ∗ ; τ ) is just SED(F, F ∗ ).




                                                         5
For any forecast error e, with cumulative distribution function F (e) such that E(|e|) < ∞,
we have
                              W SED(F, F ∗ ; τ ) = 2E(Lτ (e)),                           (3)

where Lτ (e) is the loss function
                                                
                                                (1 − τ )|e|,   e≤0
                                     Lτ (e) =
                                                τ |e|,         e > 0,

and τ ∈ (0, 1).

The loss function Lτ (e) appears in the forecasting literature as a convenient and simple
potentially asymmetric loss function.6 It is often called “lin-lin” loss (i.e., linear on each side
of the origin), or “check function” loss, again in reference to its shape. Importantly, it is the
loss function underlying quantile regression; see Koenker (2005).
    Because W SED(F, F ∗ ; τ ) is proportional to expected lin-lin loss as established by Propo-
sition 4.1, we are led inescapably to the insight that point forecast accuracy evaluation by
W SED(F, F ∗ ; τ ) is actually point forecast accuracy evaluation by expected lin-lin loss. The
primacy of lin-lin loss in the W SED(F, F ∗ ; τ ) case, like the primacy of absolute error loss
in the leading special case of W SED(F, F ∗ ; 1/2) (SED(F, F ∗ )), emerges clearly.
    Patton and Timmermann (2007) suggest a different route that also leads directly and
exclusively to lin-lin loss. Building on the work of Christoffersen and Diebold (1997) on
optimal prediction under asymmetric loss, they show that if loss L(e) is homogeneous and
the target variable y has no conditional moment dependence beyond the conditional variance,
then the L-optimal forecast is always a conditional quantile of y. Hence under their conditions
W SED(F, F ∗ ; τ ) loss is the only asymmetric loss function of relevance.
    Our results and those of Patton and Timmermann are highly complementary but very
different, not only in the perspective from which they are derived, but also in the results
themselves. If, for example, y displays conditional moment dynamics beyond second-order,
then the L-optimal forecast is generally not a conditional quantile (and characterizations
in such cases remain elusive), whereas the W SED(F, F ∗ ; τ )-optimal forecast is always a
conditional quantile.
    In closing this section, we also note that W SED(F, F ∗ ; τ ) can be used as a forecast
model estimation criterion. By Proposition 4.1, this amounts to estimation using quantile
  6
      See Christoffersen and Diebold (1997).



                                                      6
regression, with the relevant quantile governed by τ . When τ = 1/2, the quantile regression
estimator collapses to the least absolute deviations (LAD) estimator. Similarly, because
the forecast combination problem is a regression problem (Granger and Ramanathan, 1984),
forecast combination under W SED(F, F ∗ ; τ ) simply amounts to estimation of the combining
equation using quantile regression, with the relevant quantile governed by τ .


5     Generalized Weighted Stochastic Error Distance
Here we generalize and represent SED in a way that facilitates relating it to other divergence
and distance measures.


5.1    A Natural Extension
As always let F (e) be the forecast error c.d.f., and let F ∗ (e) be the unit step function at
zero. Now consider the following generalized weighted stochastic error distance (GW SED)
measure:                                     Z
                    GW SED(F, F ; p, w) = |F (e) − F ∗ (e)|p w(e) de,
                                   ∗
                                                                                           (4)

where p > 0. All of our stochastic error distance measures are of this form. When p = 1 and
w(e) = 1 ∀ e, we have SED(F, F ∗ ), and when p = 1 and
                                        
                                        2(1 − τ ), e < 0
                                 w(e) =
                                        2τ,        e ≥ 0,

we have W SED(F, F ∗ ; τ ). The GW SED(F, F ∗ ; p, w) representation facilitates comparisons
of W SED(F, F ∗ ; τ ) to other possibilities that emerge for alternative choices of p and/or
w(·).
   Interesting connections emerge between GW SED(F, F ∗ ; p, w) and various other impor-
tant distance and divergence measures. We now discuss several.


5.2    GW SED and Cramér-von Mises Divergence
When p = 2 and w(e) = f (e), the density corresponding to F (e), GW SED(F, F ∗ ; p, w) is
Cramér-von Mises divergence,
                                           Z
                                  ∗
                         CV M (F , F ) =       |F ∗ (e) − F (e)|2 f (e) de.               (5)

                                                7
Note that the weighting function w(e) in Cramér-von Mises divergence CV M (F ∗ , F ) is
distribution-specific, w(e) = f (e). We can decompose Cramér-von-Mises divergence as
                    Z
          ∗
CV M (F , F ) =          |F ∗ (e) − F (e)|2 f (e) de
                    Z
                             F (e)(1 − F ∗ (e)) + (1 − F (e))F ∗ (e)
                         
                =

                                − F (e)(1 − F (e)) − F ∗ (e)(1 − F ∗ (e)) f (e) de
                                                                         
                    Z                       Z                         Z
                =            F (e)f (e)de +    (1 − F (e))f (e)de −      F (e)(1 − F (e))f (e) de
                     R−                              R+                                   R
                    Z F (0)             Z   1                       Z   1
                =              p dp +               (1 − p) dp −            p(1 − p) dp (by change of variable, e = F −1 (p))
                     0                  F (0)                       0
                                                1
                = F (0)2 − F (0) +
                                                3
                    1
                ≥      .
                    12

CV M (F ∗ , F ) achieves its lower bound of 1/12 if and only if F (0) = 1/2, which implies that
CV M (F ∗ , F ) ranks by “closeness to median unbiasedness,” just as does SED(F, F ∗ ).
Remark 5.1 (Directional properties of CV M ).
Although CV M (F ∗ , F ) is well-defined, CV M (F, F ∗ ) is not, because
                                                          Z
                                 CV M (F, F ) =      ∗
                                                              |F (e) − F ∗ (e)|2 f ∗ (e) de,

where f ∗ (e) is Dirac’s delta.

Remark 5.2 (Comparative directional properties of CV M and Kullback-Leibler divergence).7
The Kullback-Leibler divergence KL(F ∗ , F ) between F ∗ (e) and F (e) is

                                                                        f ∗ (e)
                                                          Z                      
                                                ∗
                                    KL(F , F ) =              log                     f ∗ (e) de,
                                                                        f (e)

where f ∗ (x) and f (x) are densities associated with distributions F ∗ and F . Unlike CV M (F ∗ , F ),
KL(F ∗ , F ) does not fit in our GW SED(F, F ∗ ; p, w) framework as it is ill-defined in both di-
rections.
   7
    There are of course many other distance/divergence measures, exploration of which is beyond the scope
of this paper. On Hellinger distance, for example, see Maasoumi (1993).



                                                                8
Remark 5.3 (Kolmogorov-Smirnov distance, CV M , and SED).
Kolmogorov-Smirnov distance is

                        KS(F, F ∗ ) = sup |F (e) − F ∗ (e)| = max (F (0), 1 − F (0)) .
                                         e


Like CV M (F ∗ , F ), KS(F, F ∗ ) achieves its lower bound at F (0) = 1/2, and KS(F, F ∗ )
therefore ranks by “closeness to median unbiasedness,” just as does SED(F, F ∗ ).

Remark 5.4 (Preferences for low-variance errors among unbiased forecasts).
    Note that although CV M divergence and KS distance value median unbiasedness, as
emphasized earlier in this section, they don’t invoke a notion of variance to rank unbiased
forecasts. In contrast, as emphasized in section 3, SED ranks unbiased forecasts by variance,
preferring forecast errors with smaller variance.

Remark 5.5 (Bias-variance tradeoffs).
   Although CV M and KS don’t consider variance among unbiased forecasts, they do
consider it among biased forecasts. But they do it in a counter-intuitive way, due to the
choice of weighting function.


5.3         GW SED and Cramér Distance
When p = 2 and w(e) = 1, GW SED(F, F ∗ ; p, w) is Cramér distance, also known as Mallows
distance, or Monge-Kantorovich distance, or earth-movers distance; see Levina and Bickel
(2001). Closely related, moreover, are the “energy distance” used in higher dimensions (e.g.,
Székely and Rizzo, 2013) and the “continuous ranked probability score” of Gneiting and
Raftery (2007).8
    We can decompose Cramér distance as
  Z    ∞                     2
                                     Z
                        ∗
                                             F (e)(1 − F ∗ (e)) + (1 − F (e))F ∗ (e)
                                        
                F (e) − F (e) de =
       −∞

                                          − F (e)(1 − F (e)) − F ∗ (e)(1 − F ∗ (e)) de
                                                                                   
                                   Z 0           Z ∞
                                                                
                                                                        Z ∞                           (6)
                                 =     F (e)de +       1 − F (e) de −        F (e)(1 − F (e)) de
                                    −∞             0                      −∞
                                                  Z ∞
                                             ∗
                                 = SED(F, F ) −         F (e)(1 − F (e)) de.
                                                           −∞
   8
   The continuous ranked probability score, however, is not used to compare point forecasts, but rather to
compare density forecasts.


                                                           9
Equation (6) is particularly interesting insofar as it shows that Cramér distance is closely
related to SED(F, F ∗ ), yet not exactly equal to it, due to the adjustment term, F (e)(1 −
                                                                                 R

F (e)) de. One can show that9
                                    Z
                                                             1
                                        F (e)(1 − F (e)) de = E(|e − e0 |),
                                                             2

where e0 is a stochastic copy of e, revealing that the adjustment term, like the leading term,
is a measure of forecast error variability.
    One can also rewrite Cramér distance solely in terms of two SED’s. We state this result
as a proposition.

Proposition 5.6 (SED and Cramér distance)
For any forecast error e, with cumulative distribution function F (e) such that E(|e|) < ∞,
we have          Z ∞
                                      2                    1
                       F (e) − F ∗ (e) de = SED(F, F ∗ ) − SED(G, F ∗ ),
                      
                   −∞                                       2
where G is the distribution function of e − e0 and e0 is a stochastic copy of e.


6         GW SED(F (e), F ∗(e); p, w(e)) vs. E(L(e)):
          A Complete Characterization
Equivalence of GW SED(F (e), F ∗ (e); p, w(e)) minimization and E(L(e)) minimization can
actually be obtained for a wide class of loss functions L(e). In particular, we have the
following proposition.

Proposition 6.1 (Equivalence of GW SED minimization and E(L) minimization)
Suppose that L(e) is piecewise differentiable with dL(e)/de > 0 for e > 0 and dL(e)/de < 0
for e < 0, and suppose also that F (e) and L(e) satisfy F (e)L(e) → 0 as e → −∞ and
(1 − F (e))L(e) → 0 as e → ∞. Then
                                Z   ∞
                                                            dL(e)
                                        |F (e) − F ∗ (e)|         de = E(L(e)).           (7)
                                 −∞                          de

That is, minimization of GW SED(F (e), F ∗ (e); p, w(e)) when p = 1 and w(e) = |dL(e)/de|
corresponds to minimization of expected loss E(L(e)).
    9
        See Gneiting and Raftery (2007).


                                                            10
Several remarks are in order.
    The importance of the proposition is its highlighting the fact that for any L there is a
corresponding and easily-calculated GW SED, and similarly, for any GW SED there is a
corresponding and easily-calculated L. Hence the result clarifies what it means to chose a
loss function: choosing a loss function amounts to choosing a GW SED weighting function.
This may be helpful not only in helping with introspection as to what loss function might
be “reasonable” in a given situation, but also in obtaining new results by switching from
“L-representations” to “GW SED-representations.”
Remark 6.2 (GW SED weightings other than those corresponding to W SED and SED).
Note that the E(L) minimizers that match various GW SED(F, F ∗ ; p, w) minimizers gener-
ally correspond to non-standard and intractable loss functions L(e) in all cases but the ones
we have emphasized, namely W SED(F, F ∗ ; τ ) and its leading case SED(F, F ∗ ).

Remark 6.3 (The GW SED weighting that produces quadratic loss).
The weighting function in (7) that produces expected squared-error loss (E(L(e)) = E(e2 ))
is immediately |dL(e)/de| = |2e|. It is not obvious why one would generally want to adopt
such a weighting, other than for mathematical convenience.

Remark 6.4 (Relationship between GW SED and Elliott et al. (2005) loss).
GW SED(F, F ∗ ; p, w) somewhat resembles the Elliott et al. (2005) (ETK) loss function,

                        LET K (e; p, α) = |e|p (α + (1 − 2α)I(e < 0)) .

It differs fundamentally, however, in that GW SED(F, F ∗ ; p, w) is based on distributional
distance, |F − F ∗ |, whereas ETK loss is based on the usual forecast error distance, (y − ŷ).
Ultimately, ETK loss is a special case of GW SED(F, F ∗ ; p, w), corresponding to a particular
choice of exponent p and weighting function w(e), as per Proposition 6.1, as are all L(e) loss
functions that satisfy the regularity conditions of the proposition.


7    Concluding Remarks
Starting from first principles, we have proposed and explored several “stochastic error dis-
tance” (SED) measures of point forecast accuracy, based directly on the distance between
the forecast-error c.d.f. and the unit step function at 0. SED-type criteria sharply focus


                                              11
attention on a particular loss function, absolute loss (and its lin-lin generalization), as op-
posed to the ubiquitous quadratic loss, or anything else. Our results elevate the status of
absolute and lin-lin loss for both point forecast evaluation and for estimation.
    SED is related to important recent work on tests of stochastic dominance (SD) of loss
distributions, including Corradi and Swanson (2013), Lee et al. (2014), and Jin et al. (2015).
The SED and SD approaches are related in at least two ways. First, and obviously, both
are based on comparative properties of certain c.d.f.’s. Second, the SD literature focuses on
achieving robustness of accuracy rankings to loss function choices, and SED initially appears
that way too, insofar as it is motivated from first principles without reference to an L(e)-type
loss function.
    Yet there is also a clear difference between SD and SED. If SD holds (whether first- or
higher-order), it really does imply robustness to certain classes of loss functions. SED, in
contrast, leads one inextricably to absolute-error loss. Indeed we have shown in Proposition
2.1 that the SED criterion is the absolute-error loss criterion. Hence, in contrast to SD, which
focuses attention on whether one forecast dominates another regardless of loss function, SED
ultimately embraces a particular loss function, absolute error loss, which until now has been
something of a sideshow relative to the ubiquitous quadratic loss.
    Several interesting directions arise for future research. One direction concerns multivari-
ate extensions, in which case it’s not clear how to define the unit step function at zero, F ∗ (e).
Consider, for example, the bivariate case, in which the forecast error is e = (e1 , e2 )0 . It seems
clear that we want F ∗ (e) = 0 when both errors are negative and F ∗ (e) = 1 when both are
positive, but it’s not clear what to do when the signs diverge.
    This difficulty is not surprising insofar as it parallels difficulties with multivariate ex-
tensions of the median. Various notions of the multivariate median are available, but it
seems that no single measure dominates others (Small, 1990; Gneiting, 2011). It would be
interesting to investigate how various multivariate versions of SED (for various versions of
the unit step function at zero) relate to various versions of multivariate median.
    Another direction is further study of M AE vs. M SE rankings as introduced in section
3. In work building on the research reported here, Ardakani et al. (2015) take interesting
steps in that direction, obtaining analytic results under a “convex ordering” assumption
weaker than a normality. Necessary and sufficient conditions remain elusive, however, for
the general case of non-Gaussian, non-zero-mean forecast errors.




                                                12
Appendices
A        A Useful Lemma
We begin with a lemma.

Lemma A.1
  (i) Let y be a positive random variable such that E(|y|)) < ∞. Then
                                                  Z     ∞
                                         E(y) =             [1 − F (y)]dy,
                                                    0


where F (y) is the cumulative distribution function of y.
   (ii) Let y be a negative random variable such that E(|y|)) < ∞.10 Then
                                                        Z    0
                                          E(y) = −               F (y)dy,
                                                            −∞


where F (y) is the cumulative distribution function of y.

Lemma A.1 (i) is well known in the mathematical statistics literature (e.g., Block and Fang,
1988; Rao, 2009), and it also features prominently in the hazard/survival literature (e.g.,
Neumann, 1999).


B        Proofs of Propositions
We now prove the Propositions stated in the paper.

Proposition 2.1 (Equivalence of SED and Expected Absolute Error Loss)
For any forecast error e, with cumulative distribution function F (e) such that E(|e|) < ∞,
we have                          Z         0    Z                    ∞
                      SED(F, F ∗ ) =           F (e) de +                [1 − F (e)] d e = E(|e|).
                                          −∞                     0

That is, SED(F, F ∗ ) equals expected absolute loss for any error distribution.
 10
      In another abuse of notation, we use “y” to denote either a generic random variable or its realization.




                                                        13
Proof of Proposition 2.1

SED(F, F ∗ ) = SED− (F, F ∗ ) + SED+ (F, F ∗ )
               Z 0              Z ∞
             =      F (e) de +       (1 − F (e)) de
                −∞               0
                  Z 0              Z ∞
             =−        ef (e) de +       ef (e) de (by Lemma A.1 (i) for SED− and (ii) for SED+ )
                   −∞                0
               Z ∞                 Z ∞
             =      ef (−e) de +        ef (e) de
               Z0 ∞                 0

             =      e(f (−e) + f (e)) de
                0
               Z ∞
             =      |e|f (e) de
                  −∞

             = E(|e|).


Proposition 4.1 (Equivalence of W SED and Expected Lin-Lin Loss)
For any forecast error e, with cumulative distribution function F (e) such that E(|e|) < ∞,
we have
                                 Z 0                Z ∞
                  ∗
    W SED(F, F ; τ ) = 2(1 − τ )      F (e) de + 2τ     [1 − F (e)] de = 2E(Lτ (e)),   (A.1)
                                    −∞               0


where Lτ (e) is the loss function
                                         
                                         (1 − τ )|e|, e ≤ 0
                                Lτ (e) =
                                         τ |e|,       e > 0,

and τ ∈ (0, 1).




                                             14
Proof of Proposition 4.1
                                             Z   0                   Z       ∞
                   ∗
    W SED(F, F ; τ ) = 2(1 − τ )                     F (e) de + 2τ (1 − F (e)) de
                                              −∞                         0
                                             Z 0                   Z ∞
                               = 2(1 − τ )      (−e)fe (e) de + 2τ      efe (e) de (by Lemma A.1)
                                            −∞                       0
                                           Z                            Z
                               = 2(1 − τ ) |e|1{e ≤ 0}fe (e) de + 2τ |e|1{e > 0}fe (e) de
                                  Z
                                                                            
                               =2      (1 − τ )|e|1{e ≤ 0} + τ |e|1{e > 0} fe (e) de

                               = 2E(Lτ (e)).


Proposition 6.1 (Equivalence of GW SED minimization and E(L(e)) minimization)
Suppose that L(e) is piecewise differentiable with dL(e)/de > 0 for e > 0 and dL(e)/de < 0
for e < 0, and suppose also that F (e) and L(e) satisfy F (e)L(e) → 0 as e → −∞ and
(1 − F (e))L(e) → 0 as e → ∞. Then
                                 Z   ∞
                                                             dL(e)
                                         |F (e) − F ∗ (e)|         de = E(L(e)).
                                   −∞                         de

That is, minimization of GW SED(F, F ∗ ; p, w) when p = 1 and w(e) = |dL(e)/de| corre-
sponds to minimization of expected loss E(L(e)).

Proof of Proposition 6.1
     Z   ∞                              Z 0                    Z ∞
                           dL(e)
                           ∗                        dL(e)                       dL(e)
             F (e) − F (e)       de = −       F (e)       de +      (1 − F (e))       de
       −∞                   de            −∞         de         0                de
                                      Z 0                  Z ∞
                                    =      f (e)L(e) de +      f (e)L(e) de
                                       −∞                   0
                                      Z ∞
                                    =      f (e)L(e) de
                                                        −∞

                                                     = E[L(e)],

where we obtain the second line by integrating by parts and exploiting the the assumptions
on L(e) and F (e). In particular,
                       Z   0                                                     Z   0
                                dL(e)                                0
                          F (e)       de = F (e)L(e)                         −           f (e)L(e) de,
                       −∞        de                                  −∞          −∞


                                                             15
by integration by parts, but the first term is zero because F (e)L(e) → 0 as e → −∞, and
similarly,
             Z    ∞                                            ∞
                                                                       Z   ∞
                                  dL(e)
                      (1 − F (e))       de = (1 − F (e))L(e)       +           f (e)L(e) de,
              0                    de                          0       0


again by integration by parts, and again the first term is zero because (1 − F (e))L(e) → 0
as e → ∞.




                                                16
References
Ardakani, O.M., N. Ebrahimi, and E.S. Soofi (2015), “Ranking Forecast Models by Stochastic
  Error Distance and Survival Information Risk,” Manuscript, Armstrong State University,
  Northern Illinois University, and University of Wisconsin-Milwaukee.

Block, H.W. and Z. Fang (1988), “A Multivariate Extension of Hoeffding’s Lemma,” Annals
  of Probability., 16, 1803–1820.

Christoffersen, P.F. and F.X. Diebold (1997), “Optimal Prediction Under Asymmetric Loss,”
 Econometric Theory, 13, 808–817.

Corradi, V. and N.R. Swanson (2013), “A Survey of Recent Advances in Forecast Accuracy
  Comparison Testing, with an Extension to Stochastic Dominance,” In X. Chen and N.
  Swanson (eds.), Causality, Prediction, and Specification Analysis: Recent Advances and
  Future Directions, Essays in honor of Halbert L. White, Jr., Springer, 121-143.

Elliott, G., I. Komunjer, and A. Timmermann (2005), “Estimation and Testing of Forecast
  Rationality under Flexible Loss,” Review of Economic Studies, 72, 1107–1125.

Gneiting, T. (2011), “Quantiles as Optimal Point Forecasts,” International Journal of Fore-
 casting, 27, 197–207.

Gneiting, T. and A.E. Raftery (2007), “Strictly Proper Scoring Rules, Prediction, and Esti-
 mation,” Journal of the American Statistical Association, 102, 359–378.

Granger, C.W.J. and R. Ramanathan (1984), “Improved Methods of Forecasting,” Journal
  of Forecasting, 3, 197–204.

Jin, S., V. Corradi, and N.R. Swanson (2015), “Robust Forecast Comparison,” Manuscript,
  Singapore Management University, University of Surrey, and Rutgers University. Available
  at SSRN: http://ssrn.com/abstract=2605927.

Koenker, R. (2005), Quantile Regression, Econometric Society Monograph Series, Cambridge
 University Press, 2005.

Lee, T.H., Y. Tu, and A. Ullah (2014), “Nonparametric and Semiparametric Regressions
  Subject to Monotonicity Constraints: Estimation and Forecasting,” Journal of Economet-
  rics, 182, 196–210.


                                            17
Levina, E. and P. Bickel (2001), “The Earth Mover’s Distance is the Mallows Distance: Some
  Insights from Statistics,” Proceedings Eighth IEEE International Conference on Computer
  Vision, 251–256.

Maasoumi, E. (1993), “A Compendium to Information Theory in Economics and Economet-
 rics,” Econometric Reviews, 12, 137–181.

Neumann, G.R. (1999), “Search Models and Duration Data,” In M.H. Pesaran and P.
  Schmidt (eds.), Handbook of Applied Econometrics, Volume 2, Blackwell, 300-351.

Patton, A.J. (2015), “Evaluating and Comparing Possibly Misspecified Forecasts,”
  Manuscript, Duke University.

Patton, A.J. and A. Timmermann (2007), “Testing Forecast Optimality Under Unknown
  Loss,” Journal of the American Statistical Association, 102, 1172–1184.

Rao, C.R. (2009), Linear Statistical Inference and Its Applications, John Wiley and Sons.

Small, C.G. (1990), “A Survey of Multidimensional Medians,” International Statistical Re-
  view , 58, 263–277.

Székely, G.J. and M.L. Rizzo (2013), “Energy Statistics: A Class of Statistics Based on
  Distances,” Journal of Statistical Planning and Inference, 143, 1249–1272.




                                            18
