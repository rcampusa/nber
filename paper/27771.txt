                                NBER WORKING PAPER SERIES




                      HOW TO ESTIMATE A VAR AFTER MARCH 2020

                                          Michele Lenza
                                        Giorgio E. Primiceri

                                        Working Paper 27771
                                http://www.nber.org/papers/w27771


                      NATIONAL BUREAU OF ECONOMIC RESEARCH
                               1050 Massachusetts Avenue
                                 Cambridge, MA 02138
                                    September 2020




We thank Domenico Giannone for numerous conversations on the topic, and Todd Clark for
comments. The views expressed in this paper are those of the authors and do not necessarily
represent those of the European Central Bank or the Eurosystem. The views expressed herein are
those of the authors and do not necessarily reflect the views of the National Bureau of Economic
Research.

At least one co-author has disclosed a financial relationship of potential relevance for this research.
Further information is available online at http://www.nber.org/papers/w27771.ack

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2020 by Michele Lenza and Giorgio E. Primiceri. All rights reserved. Short sections of text, not
to exceed two paragraphs, may be quoted without explicit permission provided that full credit, including
© notice, is given to the source.
How to Estimate a VAR after March 2020
Michele Lenza and Giorgio E. Primiceri
NBER Working Paper No. 27771
September 2020
JEL No. C11,C32,E32,E37

                                         ABSTRACT

This paper illustrates how to handle a sequence of extreme observations--such as those recorded
during the COVID-19 pandemic--when estimating a Vector Autoregression, which is the most
popular time-series model in macroeconomics. Our results show that the ad-hoc strategy of
dropping these observations may be acceptable for the purpose of parameter estimation.
However, disregarding these recent data is inappropriate for forecasting the future evolution of
the economy, because it vastly underestimates uncertainty.


Michele Lenza
European Central Bank
Sonnemannstrasse 20
60314 Frankfurt An Main
Germany
michele.lenza@ecb.int

Giorgio E. Primiceri
Department of Economics
Northwestern University
2211 Campus Drive
Evanston, IL 60208
and NBER
g-primiceri@northwestern.edu
                  How to Estimate a VAR after March 2020*

                               Michele Lenza                 Giorgio E. Primiceri



                             First version: June 2020. This version: July 2020




                                                     Abstract

          This paper illustrates how to handle a sequence of extreme observations--such as those
      recorded during the COVID-19 pandemic--when estimating a Vector Autoregression, which
      is the most popular time-series model in macroeconomics. Our results show that the ad-hoc
      strategy of dropping these observations may be acceptable for the purpose of parameter es-
      timation. However, disregarding these recent data is inappropriate for forecasting the future
      evolution of the economy, because it vastly underestimates uncertainty.




1    Introduction

The COVID-19 pandemic is devastating the world economy, producing unprecedented variation
in many key macroeconomic variables. For example, in March 2020, U.S. unemployment in-
creased by 0.7 percentage points, which is approximately 7 times as much as its typical monthly
change. Things got much worse in April, when unemployment reached a record-high level of
14.7 percent, rising by 10 percentage points in a single month. This change was two orders of
magnitude larger than its typical month-to-month variation. Most other macroeconomic indi-
cators experienced changes of similar proportion, including employment, consumption expen-
ditures, retail sales and industrial production, to name just a few examples. It is too early to tell
whether these extremely large shocks will propagate through the economy in a standard way, or
   * We thank Domenico Giannone for numerous conversations on the topic, and Todd Clark for comments. The

views expressed in this paper are those of the authors and do not necessarily represent those of the European Central
Bank or the Eurosystem.
   
     European Central Bank and ECARES
   
     Northwestern University, CEPR and NBER
                             HOW TO ESTIMATE A VAR AFTER MARCH 2020



whether their transmission mechanism will be altered. Unfortunately, answering this question
requires observing a much longer time span of data. But even with an unchanged transmis-
sion mechanism, the massive data variation of the last few months constitutes a challenge for
the estimation of standard time-series models. When it comes to inference, should we treat the
data from the pandemic period as conventional observations? Or will these observations distort
the parameter estimates of our models? Should we instead discard these recent data? These
questions are not only essential for the estimation of time-series models during the outbreak of
COVID-19, but they will remain crucial for many years to come, since data from the pandemic
period will "contaminate" any future sample of time-series observations.

   This paper provides a simple solution to this inference problem in the context of Vector Au-
toregressions (VARs), the most popular time-series model in macroeconomics. Our solution
consists of explicitly modeling the change in shock volatility, to account for the exceptionally
large macroeconomic innovations during the period of the epidemic. What makes our recipe
different, and simpler than standard models of time-varying volatility, is the fact that we know
the exact timing of the increase in the variance of macroeconomic innovations due to COVID-19.
As a result, we can both flexibly model and easily estimate these volatility changes. For exam-
ple, suppose to work with a monthly VAR based on U.S. macroeconomic data. We know that
March 2020 was the first month of abnormal data variation. We can then simply re-scale the
standard deviation of the March shocks by an unknown parameter s
                                                               ¯0 , and do the same for April
and May with two other parameters s
                                  ¯1 and s
                                         ¯2 . As we show in section 2, the parameters s
                                                                                      ¯0 , s
                                                                                           ¯1
and s
    ¯2 can be easily estimated using the approach of Giannone et al. (2015), provided that this
re-scaling is common to all shocks. This commonality assumption is the same assumption un-
derlying the stochastic volatility model of Carriero et al. (2016), and it should of course only be
interpreted as an approximation. But this approximation seems reasonable in a period in which
all variables experienced record variation. This strategy is also surely preferable to assuming
s
¯0 = s    ¯2 = 1, which would be implicit in a treatment of the data in March, April and May
     ¯1 = s
2020 as conventional observations.

   The final step of our simple procedure consists of modeling the likely future evolution of the
residual variance, beyond May 2020. This is considerably more challenging, since the data to
inform these estimates are not yet available. To tackle this task, we set up a prior centered on the
assumption that the residual variance after May will decay at a 20 percent monthly rate. As more
data will become available, the researcher will be able to update such prior with the likelihood
information. It is important to stress that this step is irrelevant for the estimation of the model



                                                 2
                            HOW TO ESTIMATE A VAR AFTER MARCH 2020



on current data, but it plays an important role for using the VAR to derive predictive densities,
since the dispersion of these densities depends on the future value of the residual variances.

   To illustrate the properties of our procedure, we estimate a monthly VAR including data on
employment, unemployment, consumption and prices, and we use the estimated model to per-
form two exercises. First, we compute the impulse responses to the forecast error in unemploy-
ment. To be clear, these responses do not have any structural interpretation, but we use them
only as summary statistics of the estimated dynamics. When we attempt to compute these im-
pulse responses using a standard estimation strategy that does not downweight the data from
the pandemic period, this experiment produces nonsensical results, as the error bands of the
impulse responses explode. This finding suggests that the March, April and May 2020 observa-
tions, despite being a tiny fraction of the whole sample, are so wild that they can influence pa-
rameter estimates substantially (and not necessarily in a good way). When the VAR is estimated
using our proposed procedure, the impulse response are instead very similar to those that we
would obtain by estimating the VAR with data until February 2020. This finding suggests that
the ad-hoc procedure of dropping the extreme observations from the pandemic is acceptable
for the purpose of parameter estimation, at least given the data available at the time of writing
of this paper.

   This approach based on disregarding the recent data, however, would be inappropriate for
generating any type of prediction for the future evolution of the economy, because it vastly un-
derestimates uncertainty. We demonstrate this point in a second application, in which we use
the estimated VAR to produce density forecasts of various macroeconomic variables, conditional
on the consensus unemployment projection from the latest release of the Blue Chip Forecasts.
When we perform this exercise by estimating the model without the data from the pandemic,
the predictions of employment, consumption expenditures and prices appear to be excessively
sharp. Instead, our proposed estimation strategy captures the idea that economic fluctuations
may be quite volatile for many months to come. As a consequence, our predictions are consis-
tent with a much broader range of possible recovery paths from the COVID-19 crisis.

   The literature on time variation in macroeconomic shock volatility is vast, and its compre-
hensive review is beyond the scope of this paper. However, it is important to contrast the simple
volatility modeling strategy we propose here with the more typical time-varying volatility mod-
els that have recently been adopted in the context of VARs, unobserved component or DSGE
models (e.g. Cogley and Sargent, 2005, Primiceri, 2005, Sims and Zha, 2006, Carriero et al., 2016,
Stock and Watson, 2007, Fernandez-Villaverde and Rubio-Ramirez, 2007, Justiniano and Prim-


                                                3
                              HOW TO ESTIMATE A VAR AFTER MARCH 2020



iceri, 2008, Curdia et al., 2014). A potential issue with these approaches is that the degree of time
variation in volatility is always informed by past data. For example, if historically shock volatili-
ties have varied by at most a factor of two or three from month to month, any estimated ARCH,
GARCH, Markov-switching or stochastic volatility model would have a hard time capturing the
massive increase in volatility associated with the outbreak of the COVID-19 pandemic. The sec-
ond important difference between our approach and the more typical models in the literature
is that the time of the volatility change is known in the case of COVID-19, which simplifies the
estimation of our model. This feature distinguishes our strategy from the one adopted by Stock
and Watson (2016) to deal with outliers in their univariate model of inflation.

    The rest of the paper is organized as follows. Section 2 describes the methodology we propose
to handle the extreme observations recorded during the COVID-19 era. Section 3 presents the
results of our two empirical applications, and section 4 concludes.



2    The methodology

To account for the large variance of macroeconomic shocks associated with the outbreak of
COVID-19, we modify a standard VAR as follows:


                               yt = C + B1 yt-1 + ... + Bp yt-p + st t                              (1)


                                            t  N (0, ) ,

where yt is an n × 1 vector of variables, modeled as a function of a constant term, their own past
values, and an n × 1 vector of forecast errors t . In expression (1), the factor st is used to scale up
the residual covariance matrix during the period of the pandemic. More precisely, st is equal to
1 before the time period in which the epidemic begins, which we denote by t . We then assume
that st = s
          ¯0 , st +1 = s                                  s2 - 1) j -2 , where   [¯
                                    ¯2 , and st +j = 1 + (¯
                       ¯1 , st +2 = s                                             s0 , s    ¯2 , ] is a
                                                                                       ¯1 , s
vector of unknown coefficients. This flexible parameterization allows for this scaling factor to
take three (possibly) different values in the first three periods after the outbreak of the disease,
and to decay at a rate 1 -  after that. This modeling strategy is particularly suitable for monthly
and quarterly time series, given that the amount of data variation was substantially different in
the months of March, April and May 2020, and will likely be different when comparing 2020:Q1,
Q2 and Q3. We note, however, that alternative parameterizations are possible, even though they




                                                  4
                                 HOW TO ESTIMATE A VAR AFTER MARCH 2020



would not affect the results and their interpretation.1

       How can we estimate equation (1)? This task is actually relatively easy. To see why, begin by
assuming that st is known, and rewrite (1) as


                                                yt = Xt  + st t ,


where Xt  In  xt , xt  1, yt-1 , ..., yt-p and   vec [C, B1 , ..., Bp ] . Dividing both sides of
this equation by st , we obtain
                                                 y    ~ t  + t ,
                                                 ~t = X

         ~t  yt /st and X
in which y               ~ t  Xt /st . For given st , y      ~ t are simple transformations of our
                                                      ~t and X
data. Therefore, the parameters  and  can be estimated using the transformed data y             ~t,
                                                                                         ~t and X
and the researcher's preferred approach to inference, such as ordinary least squares, maximum
likelihood, or Bayesian estimation.

       While the previous insight applies to all estimation procedures (and we will later show how
to estimate the model by maximum likelihood), it is now useful to specialize our discussion to
the case of Bayesian inference, given the well-known advantages of this approach in the con-
text of heavily parameterized models like VARs. As in Giannone et al. (2015), we focus on prior
distributions for VAR coefficients belonging to the conjugate Normal-Inverse Wishart family


                                                   IW (, d)


                                                 N (b,   ) ,

where the elements , d, b and  are typically functions of a lower dimensional vector of hyper-
parameters  . This class of densities includes the flat prior, the popular Minnesota, Single-Unit-
Root and Sum-of-Coefficients priors, as well as the Prior for the Long Run of our earlier work
(Litterman, 1980, Doan et al., 1984, Sims and Zha, 1998, Giannone et al., 2019). Giannone et al.
(2015) propose a simple method to evaluate the posterior of of  ,  and  in a model without
st . But if we assume that st is known and replace yt and Xt with y      ~ t , we can use the exact
                                                                  ~t and X
same methodology to estimate (1).

       Of course, in practice, st is unknown and must be estimated as well. Fortunately, the poste-
rior of the parameter vector  that governs the evolution of st can be evaluated like the posterior
   1
     In a few months, it may be necessary to extend the model to accommodate a large degree of variation in relation
to a second wave of the epidemic.



                                                         5
                             HOW TO ESTIMATE A VAR AFTER MARCH 2020



of  . More precisely,
                                   p (, |y )  p (y |, ) · p (, ) ,                                (2)

where y = [yp+1 , ..., yT ] . In this expression, the first element of the product corresponds to the
so-called marginal likelihood, and it can be computed as


                             p (y |, ) =   p (y |, ) p (, | ) d (, ) ,


which has an analytical expression. The second density on the right-hand side of (2) is the hy-
perprior, i.e. the prior on the hyperparameters. Our prior on the elements of  is the same as in
Giannone et al. (2015). As a prior for s
                                       ¯0 , s
                                            ¯1 and s
                                                   ¯2 , we use a Pareto distribution with scale and
shape equal to one, which has a very fat right tail, and is thus consistent with possible large in-
creases in the variance of the VAR innovations. For , instead, we impose a Beta prior with mode
and standard deviation equal to 0.8 and 0.2, respectively. We stress that the hyperpriors on s
                                                                                             ¯0 ,
¯1 and s
s      ¯2 are not particularly important for any of the results presented below, because the data
are very informative about these parameters. Instead, the hyperprior on  determines entirely
the shape of its posterior, given that we use data up to May 2020. As more data get released, the
researcher will be able to update this prior with the likelihood information.

    Appendix A presents some additional technical details of the posterior evaluation procedure.
In addition, if a researcher wishes to pursue a frequentist approach, appendix B describes how
to easily estimate the full set of unknown parameters in 1,  ,  and , by maximum likelihood.
Matlab codes to implement these procedures are also available.



3    Two applications

To illustrate the working and advantages of our modeling approach, we estimate a VAR with
some key U.S. macroeconomic indicators. We use the estimated model (i) to track the effects
of a surprise change in unemployment; and (ii) to forecast the evolution of the U.S. economy,
conditional on the consensus unemployment prediction from the June 2020 release of the Blue
Chip Forecasts.

    More precisely, the VAR includes six variables available at the monthly frequency: (i) un-
employment, measured by the civilian unemployment rate; (ii) employment, measured by the
logarithm of the total number of nonfarm employees; (iii) PCE, measured by the logarithm of
real personal consumption expenditures; (iv) PCE: services, measured by the logarithm of real

                                                 6
                             HOW TO ESTIMATE A VAR AFTER MARCH 2020



personal consumption expenditures in services; (v) PCE (price), measured by the logarithm of
the price index of personal consumption expenditures; (vi) PCE: services (price), measured by
the logarithm of the price index of personal consumption expenditures in services; and (vii) core
PCE, measured by the logarithm of the price index of personal consumption expenditures ex-
cluding food and energy. The VAR has 13 lags and it is estimated on the sample from 1988:12 to
2020:5 using a standard Minnesota prior, whose tightness is chosen as in Giannone et al. (2015).
We do not extend the sample before 1988:12 because Del Negro et al. (2020) document a reduced
reaction of inflation to fluctuations in real activity since the 1990s, compared to the pre-1990 pe-
riod.

   Figure 1 plots the posterior distribution of four (hyper)parameters of the model. The left
panel presents the posterior of the overall standard deviation of the Minnesota prior (denoted
by ), which provides information on the appropriate degree of shrinkage on the  coefficients.
The other panels of the figure, present instead the posterior distribution of the volatility scaling
factors in March, April and May 2020, i.e. s
                                           ¯0 , s
                                                ¯1 and s
                                                       ¯2 . These posteriors peak around 17, 70 and
20, suggesting that the innovation standard deviation in these three months were one and two
orders of magnitude larger than in the pre-COVID-19 period. For comparison, figure 1 also re-
ports the posterior of  when the VAR is estimated in a standard way, without st . When the latest
observations are excluded from the estimation sample (essentially assuming s
                                                                           ¯0 = s    ¯2 = ),
                                                                                ¯1 = s
the posterior of  is similar to the one implied by our baseline model, consistent with the fact
that our inferential procedure assigns comparatively less weight to these most recent observa-
tions. When instead these observations are included in the estimation sample and treated as
conventional data (essentially assuming s
                                        ¯0 = s    ¯2 = 1), the posterior of  exhibits a large
                                             ¯1 = s
shift to the right, implying much less shrinkage for the  coefficients. This reduction in shrink-
age is the necessary cost to pay to fit the large variability of the latest data with a change in the
estimated  .

   We now illustrate the implications of these estimation results in two empirical applications.
In our first application, we study the dynamic response of the variables in the VAR to a positive
shock to unemployment, when it is ordered first in a Cholesky identification scheme. There-
fore, this shock corresponds to the typical linear combination of structural disturbances that
drives the one-step-ahead forecast error of unemployment. We do not assign a structural inter-
pretation to these impulse responses, but just use them as summary statistics of the estimated
dynamics. Figure 2 presents their posterior when the VAR is estimated using the procedure out-
lined in section 2. The real economy (employment, unemployment and consumption) initially



                                                 7
                                   HOW TO ESTIMATE A VAR AFTER MARCH 2020




Figure 1: Posterior distribution of the overall standard deviation of the Minnesota prior, and the March, April and
May 2020 volatility scaling factors.




                                                        8
                                 HOW TO ESTIMATE A VAR AFTER MARCH 2020




Figure 2: Impulse responses to a one standard deviation shock to the unemployment equation. The shock is iden-
tified using a Cholesky strategy, with unemployment ordered first. The solid lines are posterior medians, while the
shaded areas correspond to 68- and 95-percent posterior credible regions.


slows down and then partially recovers. Prices also experience some downward pressure, con-
sistent with a "demand" interpretation of this shock.

    As we did earlier, it is useful to compare these impulse responses to those obtained when
estimating the model in a more conventional way. Unfortunately, if we include the latest obser-
vations in the estimation sample without any special treatment, the implied impulse responses
become explosive and unreasonable (for this reason, we do not report them). This finding illus-
trates the importance of explicitly modeling the change in shock volatility during the COVID-19
era. Instead, the responses in figure 2 are very similar to those implied by a standard VAR esti-
mation that excludes the March, April and May 2020 data (given this similarity, we do not report
them either). We summarize the lesson from this first application as follows: For the purpose of


                                                        9
                                  HOW TO ESTIMATE A VAR AFTER MARCH 2020



estimating the parameters  and , we recommend to adopt the procedure we described in sec-
tion 2, which involves a minimal deviation from the conventional VAR estimation. However, if a
researcher still wishes to estimate a VAR "as usual," it is much better to exclude the data from the
pandemic rather than including them and treating them as any other observation in the sample.
It is likely that the latter approach will produce meaningless results, as it did in our application.

       We now illustrate a second empirical application in which the gains of adopting our ap-
proach to inference are even more evident. In this application, we conduct a scenario analysis to
highlight the impact of the current change in shock volatility and its expected future evolution
on the U.S. economic outlook and the uncertainty surrounding it. More precisely, we compute
the most likely evolution of the macroeconomic indicators included in our VAR under the con-
sensus unemployment projection from the latest release of the Blue Chip Forecasts. This con-
sensus unemployment projection is plotted in the first panel of figure 3.2 According to the Blue
Chip projections, unemployment will likely decline steadily over the next few years, reaching 9
percent by the end of 2020, and 7 percent by the end of 2021. The other panels of the figure,
show a slow and gradual recovery for employment and consumption of services, and a more
sudden rebound of total consumption The three measures of prices seem to be affected much
less from the deep recession and, after a short-lived downturn and a rebound, they return to
their historical trend.

       How would these conditional forecasts look like if we estimated the model in a more standard
way? Once again, the answer to this question depends on whether the last few observations from
the COVID-19 period are included or excluded from the estimation sample. If they are included,
the estimates of  and  are unduly affected, and these conditional forecasts become unreason-
able, as in the case of the impulse responses above. If instead the estimation sample ends in
February 2020, the implied conditional forecasts are more in line with those produced by our
model with time-varying volatility. But despite a broad similarity in the qualitative features, the
conditional forecasts show now important quantitative differences, as we can see by comparing
figures 3 and 4. In particular, the VAR estimated with our proposed procedure incorporates a
higher innovation variance not only in March, April and May 2020, but also in the subsequent
few months.3 As a consequence, the estimated conditional forecasts in figure 3 exhibit a higher
degree of uncertainty about the future prospects of the U.S. economy, relative to the forecasts
in figure 4 obtained by excluding the latest data. For example, the predictions of consumption
   2
     The Blue Chip Forecasts report quarterly projections. To translate them into monthly projections, we simply
interpolated them using a smooth exponential curve.
   3
     As mentioned earlier, the expected future evolution of the shock volatility depends only on the prior at the mo-
ment, but the availability of a longer sample will soon allow to update this prior with more likelihood information.


                                                         10
                                 HOW TO ESTIMATE A VAR AFTER MARCH 2020




Figure 3: Forecasts of the variables in the VAR conditional on unemployment following the path in the first subplot.
The solid lines are posterior medians, while the shaded areas correspond to 68- and 95-percent posterior credible
regions.




                                                        11
                                 HOW TO ESTIMATE A VAR AFTER MARCH 2020




Figure 4: Forecasts of the variables in the VAR conditional on unemployment following the path in the first subplot.
The solid lines are posterior medians, while the shaded areas correspond to 68- and 95-percent posterior credible
regions.


produced by the standard VAR at short horizons are quite sharp, presumably too much. The
density forecast in figure 3, instead, not only entails more uncertainty, but it also implies a faster
rebound, since it is consistent with the idea that the size of fluctuations may be larger for some
time. This implication is reasonable, given that consumption has declined considerably more
relative to other variables at the onset of the COVID-19 recession.



4    Concluding Remarks

The sequence of wild macroeconomic variation experienced during the COVID-19 pandemic
constitutes a challenge for the estimation of macro-econometric models in general, and VARs


                                                        12
                             HOW TO ESTIMATE A VAR AFTER MARCH 2020



in particular. In this paper, we propose a simple solution to this problem, which consists of
explicitly modeling the large change in shock volatility during the outbreak of the disease. We
also show that estimating such a model is quite straightforward, because the time of the volatility
change is known. Our empirical results show that the ad-hoc procedure of dropping the extreme
observations from the pandemic era is acceptable for the purpose of parameter estimation--at
least given the data available at the time of writing of this paper--but it is inappropriate for
forecasting the future evolution of the economy, because it vastly underestimates uncertainty.



A     Posterior Evaluation

This appendix describes the technical details of the MCMC algorithm that we use to evaluate the
posterior of the model parameters. This algorithm is a standard Metropolis algorithm, almost
identical to that in Giannone et al. (2015), consisting of the following steps:


    1. Initialize the hyperparameters  and  at their posterior mode, which requires a prelimi-
      nary numerical maximization of their marginal posterior (whose analytical expression is
      derived below).

    2. Draw a candidate value [  ,  ] of the hyperparameters from a Gaussian proposal distribu-
      tion, with mean equal to  (j -1) , (j -1) and variance equal to c · W , where  (j -1) , (j -1)
      is the previous draw of [, ], W is the inverse Hessian of the negative of the log-posterior
      of the hyperparameters at the peak, and c is a scaling constant chosen to obtain an accep-
      tance rate of approximately 25 percent.

    3. Set                                    
                                                      [  ,  ]     with pr. (j )
                              (j ) , (j ) =
                                                (j -1) , (j -1) with pr. 1 - (j ) ,

      where
                                                         p (  ,  |y )
                                    (j ) = min 1,
                                                       p  (j -1) , (j -1) |y

    4. Draw  (j ) , (j ) from p , |y,  (j ) , (j ) , which is a Normal-Inverse-Wishart density (see
      details below).

    5. Increment j to j + 1 and go to 2.




                                                     13
                              HOW TO ESTIMATE A VAR AFTER MARCH 2020



In step 3, the density p (, |y ) is given by


                                     p (, |y )  p (y |, ) · p (, ) ,


where the second term of the product corresponds to the hyperprior. The first term is instead
the marginal likelihood, and it can be computed analytically as in Giannone et al. (2015). If we
condition on the initial p observations of the sample, which is a standard assumption, we obtain

                                      T                             T            ~ t , , 
                                                                             ~t |X
                                                                           p y
                       p (y |, ) =           p (yt |Xt , , ) =                                      ,
                                                                                   sn
                                                                                    t
                                     t=p+1                       t=p+1


where the denominator on the right-hand side captures the Jacobian of the transformation y
                                                                                         ~t =
yt /st . From the results in Giannone et al. (2015), it follows immediately that
                                                       
                                             T                   n(T -p)         T -p+d
                                                            1       2      n        2
                         p (y |, ) =              s-
                                                   t
                                                     n
                                                                                    d
                                                                                                ·
                                                                                 n 2
                                          t=p+1


                                             n     d                    -n
                                     ||- 2 · || 2 · x
                                                    ~x~ + -1             2
                                                                             ·
                                                                           - T -2
                                                                                p+d

                              +^
                               ~ ^  ^  b -1 B
                                    ~ -^
                                 ~+ B       ^
                                            ~ -^
                                               b                                        ,


      ~t  1, yt-1 , ..., yt-p /st , x
where x                             ~  [~
                                        xp+1 , ..., x      ~  [~
                                                    ~T ] , y   yp+1 , ..., y      ^
                                                                                  ~ x
                                                                           ~T ] , B ~x~ + -1
                                                                                             -1
                                                                                                ~y
                                                                                                x ~ + -1^
                                                                                                        b ,
^
~ y~-x   ^
         ~ , and ^
        ~B       b is a matrix obtained by reshaping the vector b in such a way that each column
corresponds to the prior mean of the coefficients of each equation (i.e. b  vec ^
                                                                                b ).

   The posterior of (, ) in step 4 is given by


                     |Y  IW       +^
                                   ~ ^  ^  b -1 B
                                        ~ -^
                                     ~+ B       ^
                                                ~ -^
                                                   b ,T - p + d


                                         ^
                                         ~ ,  x
                             |, Y  N vec B    ~x~ + -1
                                                                                   -1
                                                                                            .




                                                       14
                                    HOW TO ESTIMATE A VAR AFTER MARCH 2020



B      Maximum Likelihood Estimation

This appendix describes how to estimate our model using a frequentist, maximum likelihood
method. The likelihood function of model (1) is given by
                                                                                                         
                                T                     1       T
                                           -1
                                                                                                         
                                                                                       -1
            p (y |, , )               s2
                                       t
                                            2
                                                · exp -            yt - Xt      s2
                                                                                 t            yt - Xt 
                                                      2                                                  
                            t=p+1                          t=p+1

                             
                     T
                                        T -p             1 ~
                          s-
                           t
                             n
                               · ||-      2    · exp -         ~
                                                           Y - X         (  IT -p )-1 Y   ~
                                                                                      ~ - X              ,   (3)
                                                         2
                  t=p+1

      ~  vec (~
where Y               ~  In  x
              y ) and X      ~, and we are conditioning on the initial p observations, as
usual.

     The maximum likelihood estimators of  and , as functions of , are given by

                                                ^mle () = x       -1
                                                B         ~x~          x
                                                                       ~y~                                   (4)

                                               ^mle () = vec B
                                                             ^mle ()                                         (5)

                                                           ^    ^
                                                ^ mle () = 
                                                
                                                           ~mle ~mle
                                                                     ,                                       (6)
                                                             T -p

where ^
      ~mle  y
            ~-x
              ~B^mle (). Recal that all variables with a "
                                                         ~" depend on , although we do not
make this dependence explicit to streamline the notation. Substituting (4)-(6) into (3) yields the
concentrated likelihood, which, after some algebraic manipulations, can be written as

                                                              T                       -T- p
                                                                          ^
                                                                          ~mle ^
                                                                               ~mle     2

                         p y |^mle () , ^ mle () ,                 s-
                                                                    t
                                                                      n
                                                                        ·                     .              (7)
                                                                            T -p
                                                           t=p+1



     The parameters  can then be estimated by numerically maximizing (7), and the estimates of
 and  can be obtained using (4)-(6).



References
C ARRIERO, A., T. E. C LARK ,   AND   M. M ARCELLINO (2016): "Common Drifting Volatility in Large Bayesian
    VARs," Journal of Business & Economic Statistics, 34, 375­390.

C OGLEY, T. AND T. J. S ARGENT (2005): "Drift and Volatilities: Monetary Policies and Outcomes in the Post


                                                         15
                                HOW TO ESTIMATE A VAR AFTER MARCH 2020



  WWII U.S," Review of Economic Dynamics, 8, 262­302.

C URDIA , V., M. D. N EGRO,    AND   D. L. G REENWALD (2014): "Rare Shocks, Great Recessions," Journal of
  Applied Econometrics, 29, 1031­1052.

D EL N EGRO, M., M. L ENZA , G. E. P RIMICERI ,   AND   A. TAMBALOTTI (2020): "What's up with the Phillips
  Curve?" Brookings Papers on Economic Activity, 51, forthcoming.

D OAN , T., R. L ITTERMAN , AND C. A. S IMS (1984): "Forecasting and Conditional Projection Using Realistic
  Prior Distributions," Econometric Reviews, 3, 1­100.

F ERNANDEZ -V ILLAVERDE , J.   AND   J. F. RUBIO -R AMIREZ (2007): "Estimating Macroeconomic Models: A
  Likelihood Approach," Review of Economic Studies, 74, 1059­1087.

G IANNONE , D., M. L ENZA , AND G. E. P RIMICERI (2015): "Prior Selection for Vector Autoregressions," The
  Review of Economics and Statistics, 97, 436­451.

------ (2019): "Priors for the Long Run," Journal of the American Statistical Association, 114, 565­580.

J USTINIANO, A.   AND   G. E. P RIMICERI (2008): "The Time-Varying Volatility of Macroeconomic Fluctua-
  tions," American Economic Review, 98, 604­641.

L ITTERMAN , R. B. (1980): "A Bayesian Procedure for Forecasting with Vector Autoregression," Working
  paper, Massachusetts Institute of Technology, Department of Economics.

P RIMICERI , G. E. (2005): "Time Varying Structural Vector Autoregressions and Monetary Policy," Review
  of Economic Studies, 72, 821­852.

S IMS , C. A. AND T. Z HA (1998): "Bayesian Methods for Dynamic Multivariate Models," International Eco-
  nomic Review, 39, 949­68.

------ (2006): "Were There Regime Switches in U.S. Monetary Policy?" American Economic Review, 96,
  54­81.

S TOCK , J. H. AND M. W. WATSON (2007): "Why Has U.S. Inflation Become Harder to Forecast?" Journal of
  Money, Credit and Banking, 39, 3­33.

------ (2016): "Core Inflation and Trend Inflation," The Review of Economics and Statistics, 98, 770­784.




                                                     16
