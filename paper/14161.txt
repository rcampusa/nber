                                NBER WORKING PAPER SERIES




      ESTIMATING DERIVATIVES IN NONSEPARABLE MODELS WITH LIMITED
                         DEPENDENT VARIABLES

                                          Joseph G. Altonji
                                          Hidehiko Ichimura
                                            Taisuke Otsu

                                        Working Paper 14161
                                http://www.nber.org/papers/w14161


                      NATIONAL BUREAU OF ECONOMIC RESEARCH
                               1050 Massachusetts Avenue
                                 Cambridge, MA 02138
                                      July 2008




Our research was supported by the National Science Foundation under SBR-9512009 and the Institute
for Policy Research, Northwestern University (Altonji) , the Economic Growth Center and the Cowles
Foundation, Yale University (Altonji and Otsu), JSPS Basic Research (B) 18330040 (Ichimura), and
the National Science Foundation under SES-0720961 (Otsu). We thank Eugene Canjels, Paul McGuire,
and Ernesto Villanueva for excellent research assistance and seminar participants at several universities
for helpful comments. The views expressed herein are those of the author(s) and do not necessarily
reflect the views of the National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2008 by Joseph G. Altonji, Hidehiko Ichimura, and Taisuke Otsu. All rights reserved. Short sections
of text, not to exceed two paragraphs, may be quoted without explicit permission provided that full
credit, including © notice, is given to the source.
Estimating Derivatives in Nonseparable Models with Limited Dependent Variables
Joseph G. Altonji, Hidehiko Ichimura, and Taisuke Otsu
NBER Working Paper No. 14161
July 2008
JEL No. C1,C14,C23,C24

                                             ABSTRACT

We present a simple way to estimate the effects of changes in a vector of observable variables X on
a limited dependent variable Y when Y is a general nonseparable function of X and unobservables.
We treat models in which Y is censored from above or below or potentially from both. The basic idea
is to first estimate the derivative of the conditional mean of Y given X at x with respect to x on the
uncensored sample without correcting for the effect of changes in x induced on the censored population.
We then correct the derivative for the effects of the selection bias. We propose nonparametric and
semiparametric estimators for the derivative. As extensions, we discuss the cases of discrete regressors,
measurement error in dependent variables, and endogenous regressors in a cross section and panel
data context.


Joseph G. Altonji                                   Taisuke Otsu
Department of Economics                             Department of Economics
Yale University                                     Yale University
Box 208264                                          Box 208281
New Haven, CT 06520-8269                            New Haven, CT 06520-8281
and NBER                                            taisuke.otsu@yale.edu
joseph.altonji@yale.edu

Hidehiko Ichimura
Graduate School of Economics
University of Tokyo
Hongo 7-3-1
Tokyo 113-0033
Japan
ichimura@e.u-tokyo.ac.jp
1        Introduction
Many problems in economics involve dependent variables that are censored in some way. For
example, hundreds of empirical studies have used the Tobit and generalized Tobit models
to study the effects of a set of independent variables X on a dependent variable Y that
is censored at some constant. While great theoretical progress has been made in relaxing
assumptions on distribution forms of unobservables and functional forms to relate X and
Y , almost all of the approaches in the literature rely on the assumption that unobservables
in the model for the latent variable that determines Y are additively separable from the
observables.1
    In many applications in economics, however, nonseparability is likely to be the rule rather
than exception. For example, Altonji, Hayashi and Kotlikoff (1997) consider the problem
of money transfers from parents to children. Assume that parents’ utility depends on their
own consumption Cp , the consumption of their child Ck , and a preference heterogeneity
vector U . That is, the parents’ utility function is

                                           Vp (Cp , U ) + Vk (Ck , U ).

In their model the condition for positive transfers from the parents to the child is

                                           Vp0 (Xp , U ) < Vk0 (Xk , U ),

where Xk and Xp are the endowments of the child and parents. Otherwise the transfer
amount is zero. Let M (Xp , Xk , U ) be a value that solves

                                     Vp0 (Xp − M, U ) = Vk0 (Xk + M, U ).

The observed transfer amount Y equals M (Xk , Xp , U ) if M (Xk , Xp , U ) > 0 and 0 otherwise.
Altonji, Hayashi and Kotlikoff (1997) point out that the function M (Xp , Xk , U ) is nonsep-
arable in the endowments and preferences, and this nonseparability is a generic property
of transfer equations that are based on a consumer choice framework with interdependent
preferences.
    In consumer demand and factor demand analysis, the utility function or production
function is often chosen so that there is a transformation of the demand functions, expen-
diture functions, or cost functions that lead to additive error terms. Usually this requires
that the unobservables enter the problem in a particular way. If the unobservables and the
regressors are independent and the dependent variable is not censored or truncated, the
inability to specify separable conditional mean functions does not lead to a serious problem
in terms of estimating the mean of the derivatives of the regression functions. In this setting
one can apply the average derivative methods of Stoker (1986), Härdle and Stoker (1989),
and Powell, Stock and Stoker (1989) among others. However, nonseparability and nonmono-
tonicity with respect to U are not innocuous when one wishes to make inferences about a
selected sample, such as parents who make monetary transfers, because nonseparability will
invalidate the existing methods of correcting for sample selection.
    In this paper, we present a simple way to estimate the effects of changes in a vector of
observable regressors X on a limited dependent variable Y when Y is a general nonseparable
    1
        See, e.g., Powell (1994), Horowitz (1998), and Pagan and Ullah (1999) for a survey.


                                                         1
function of X and unobservables U . The general model we consider includes models of the
form Y = M (X, U ) if L(X) < M (X, U ) and Y = CL otherwise, where M (X, U ) is a
differentiable function with respect to X indexed by U , L(X) is an unknown function of X,
and Y = CL indicates that Y is censored from below. In this special case, the parameter
of interest in Altonji, Hayashi and Kotlikoff (1997) reduces to β(x) = E[∇M (X, U )|X =
x, L(X) < M (X, U )], where ∇M (X, U ) is the partial derivative of M (X, U ) with respect to
X. Note that in the ordinary linear censored regression model with an additive error (i.e.,
the Tobit model), β(x) is constant and coincides with the slope coefficients of the regressors.
Altonji, Hayashi and Kotlikoff (1997) use the semi-parametric version of our estimator with
L(X) = 0 to estimate the effects of the endowments Xk and Xp on the transfer amount Y
among parents who are making transfers.2
    Our estimation strategy is very simple. The basic idea is: (i) estimate the derivative
of Ψ(x) = E[M (X, U )|X = x, L(X) < M (X, U )] with respect to x without correcting for
the effect of changes in x induced on the censored population through changes in L(x) (i.e.,
selection bias) and then (ii) correct the partial derivative for the effects of the selection
bias. It turns out that the correction has a simple structure which only depends on L(x),
the conditional minimum of Y given X = x and on Pr{L(X) < M (X, U )|X = x}, the
probability that Y is uncensored given X = x. We consider models in which Y is censored
from both above and below.
    The paper continues in Section 2, where we provide a brief literature review. In Section
3 we present a canonical nonseparable limited dependent variable model. We then show
that β(x) is identified from knowledge of certain estimable functions of x. Starting from the
expression for β(x) that underlies our identification result, Section 4 discusses a nonpara-
metric estimator of β(x) as well as an average derivative estimator, which provides a way
to circumvent the curse of dimensionality. Section 5 discusses semiparametric estimation of
β(x). As extensions, Section 6 discusses the case of discrete regressors, measurement error
in the dependent variable, and the case of endogenous regressors in a cross section or panel
data context when a control function can be estimated from X and an external variable.
We also consider the panel data case in which the distribution of U conditional on the val-
ues of X for members in a group is exchangeable in those values. Section 7 presents some
Monte Carlo evidence on the performance of our estimators relative to the Tobit maximum
likelihood estimator. Section 8 concludes.


2       Previous Literature
Some early efforts on estimation of parameters in nonseparable models are found in Han
(1987), Matzkin (1991), and Powell (1991). One of the difficulties in nonseparable models
is to define an estimable parameter of interest. Suppose one assumes Y = M (X, U ). Han
(1987) considered estimation of β in models where Y = M (X 0 β, U ), Matzkin (1991) con-
sidered estimation of m in models where Y = M (m(X), U ), and Powell (1991) considered
estimation of β in models where Y = M (X, β, U ). All models assume that U is a scalar
and that M is nondecreasing in U . Han (1987) and Matzkin (1991) allow the function M
    2
    Other applications of the estimator include Kazianga (2006), Rauta and Tranb (2005) and Villanueva
(2002).



                                                  2
to be unknown and Powell (1991) assumes it to be known. As the above authors discuss,
these models generalize many limited dependent variable models, some hazard models, and
transformation models of Box and Cox (1964).
    Since the early drafts of our paper and Altonji, Hayashi and Kotlikoff (1997) were
circulated, a few papers on nonparametrically estimating features of limited dependent
variable models have appeared. Lewbel and Linton (2002) consider the model in which the
error term is additive and

                         M (X, U ) = m(X) + U,          L(X) = c, H(x) = ∞,

where the constant c is known. Note that under their model β(x) = ∇m(x). Under the
additive error model they show that ∇m(x) is the derivative of E[1(Y > c)(Y − c)|X = x]
divided by the probability that Y is uncensored given X = x. We show that this result
holds much more generally when we replace ∇m(x) with β(x). We do not require additive
error structure and allow both censoring from above and below and the censoring points to
depend generally on X.3
    Chen, Dahl and Kahn (2005) provide an estimator for m(x) based on conditional quan-
tiles in a model similar to Lewbel and Linton’s. They assume M (X, U ) = m(X) + σ(X)U
were U is a scalar and independent of X and σ(X) is strictly positive. Their approach
breaks down if monotonicity in U is dropped and/or if a second additive error term appears
in the model. They do not consider estimation of β(x) or the case in which L(X) depends
on X and must be estimated. In contrast, we place almost no restrictions M (X, U ) and
allow for endogeneity of X, but only consider estimation of β(X), L(X) and H(X).
    Over the past decade there has been an explosion of research on nonseparable models
with particular attention to models with endogenous regressors.4 This literature is concerned
with estimation of the partial effects of X on Y holding the error distribution fixed as well
as with estimation of the structural function M (X, U ) and the distribution function of U
given X, which we do not address. Monotonicity in a scalar valued U plays a key role in the
identification of M (x, u) for a point x and a conditional quantile points of u in the support
of X, but is not a reasonable assumption for models of family transfers in particular or for
consumer expenditure problems in general. We discuss the “control function” version of our
estimator in Section 6.3.


3    Model and Identification
In Section 3.1 we present the general model we treat and define the parameter of interest,
β(x). In Section 3.2 we derive an expression for β(x) and formally establish that β(x) is
identified from knowledge of certain estimable functions of x. We begin with a relatively
simple model that includes the regular Tobit model as a special case and then state an
identification result for the general case. We also make a brief comparison to the control
function approach of Heckman (1976) and many subsequent studies.
    3
      Lewbel and Linton (2002) discuss identification of m(x)+k for some constant k as well, but once ∇m(x)
is identified, clearly m(x) + k for some constant k can be identified.
    4
      See for example Blundell and Powell (2003), Chesher (2003, 2005), Imbens and Newey (2002) and
Matzkin (2007).


                                                    3
3.1   Model and Parameter of Interest
Let X ∈ Rk be a k × 1 random vector, and M (X, U ) be a random function of X, where
the random object U indexes a class of differentiable functions from Rk to R. The random
object U does not need to be a scalar random variable nor a finite dimensional random
vector. All that is required for our purpose is that it is a well defined random object. In
our model, M (X, U ) is a latent variable. Instead we observe Y :
                          
                           M (X, U ) if L(X) < M (X, U ) < H(X),
(1)                  Y =     C    if M (X, U ) ≤ L(X),
                           L
                             CH if H(X) ≤ M (X, U ),

where L(X) and H(X) are scalar valued functions of X, and CL and CH are constants that
indicate whether Y is censored from below or above, respectively. Our notation allows for
the possibility that the functions M (X, U ), L(X), and H(X) do not depend on all of the
elements of X. The linear censored regression model (i.e., the Tobit model) is a special
case of (1) in which U is a scalar random variable, M (X, U ) = X 0 β + U , L(X) = 0, and
H(X) = ∞. For notational convenience we introduce three indicator random variables:
IM (X) = I{L(X) < M (X, U ) < H(X)}, IL (X) = I{M (X, U ) ≤ L(X)}, and IH (X) =
I{H(X) ≤ M (X, U )}, where I{A} = 1 if the event A occurs and 0 otherwise, and the
argument U is suppressed to simplify the notation.
    The parameter of interest, β(x), is the average derivative of Y with respect to X given
that X = x and Y is not censored. That is

(2)                     β(x) = E[∇M (X, U )|X = x, IM (X) = 1],

where ∇M (X, U ) is the partial derivative of M (X, U ) with respect to X. Note that in the
standard Tobit model mentioned above, β(x) corresponds to the constant slope parameter β.

3.2   Identification of β(x)
We now discuss identification of the parameter of interest β(x). For the sake of exposition
only we momentarily assume that U is a scalar with the Lebesgue density dµ and that
M (X, U ) is continuous and monotonic with respect to U for each X. If U and X are
independent, the parameter of interest β(x) is written as
                                                   Z    uH (x)
(3)    β(x) = E[∇M (X, U )|X = x, IM (X) = 1] =                  ∇M (x, u)dµ(u)/GM (x),
                                                    uL (x)

where uL (x) and uH (x) solve M (x, u) = L(x) and M (x, u) = H(x), respectively, µ is the
probability measure of U , and GM (x) = Pr{IM (X) = 1|X = x}. Denote
                                                   Z    uH (x)
         Ψ(x) = E[M (X, U )|X = x, IM (X) = 1] =                 M (x, u)dµ(u)/GM (x).
                                                       uL (x)




                                            4
Let us examine the relationship between the derivatives of Ψ(x) and β(x). Denoting the
partial derivative with respect to x by ∇, the Leibniz integral rule implies
                                        Z   uH (x)
                   ∇[Ψ(x)GM (x)] =                   ∇M (x, u)dµ(u)
                                          uL (x)
                                        +M (x, uH (x))dµ(uH (x))∇uH (x)
(4)                                     −M (x, uL (x))dµ(uL (x))∇uL (x).

Note that M (x, uH (x)) = H(x) and M (x, uL (x)) = L(x). Let GH (x) = Pr{IH (X) =
1|X = x} and GL (x) = Pr{IL (X) = 1|X = x}. Then ∇GH (x) = −dµ(uH (x))∇uH (x) and
∇GL (x) = dµ(uL (x))∇uL (x). Therefore, β(x) can be written as

(5)     β(x) = ∇Ψ(x) + {Ψ(x)∇GM (x) + H(x)∇GH (x) + L(x)∇GL (x)}/GM (x).

The second term in (5) corrects for the fact that x affects selection of the population for
which Y is observed. Our estimator is based on the observation that the correction term can
be identified from knowledge of (i) Ψ(x)∇GM (x), the product of the conditional mean of Y
given that Y is uncensored and the derivative of the probability that Y is uncensored, (ii)
H(x)∇GH (x), the product of the upper bound H(x) and the derivative of the probability
that M (x, U ) exceeds H(x), and (iii) L(x)∇GL (x), the product of the lower bound L(x) and
the derivative of the probability that M (x, U ) is below L(x). All components are normalized
by GM (x), the probability that Y is uncensored.
    An important special case of the above model is fixed censoring from below, i.e., L(x) = 0
and H(x) = ∞. In this case,

                          β(x) = ∇Ψ(x) + Ψ(x)∇GM (x)/GM (x).

The case of L(X) = c with a known constant c can be reduced to the case of L(X) = 0 by
simply subtracting off c from the values of Y when Y > c.
    We now consider the general case where U need not be a scalar and not be continuous
and M (X, U ) need not be monotonic and not be continuous in U . In particular, we impose
the following assumptions.

Assumption 3.1 Assume that

(i) U and X are independent,

(ii) L(x) and H(x) are continuous at x and satisfy L(x0 ) < H(x0 ) for all x0 in a neighbor-
      hood of x,

(iii) GL (x), GM (x) > 0, and GH (x) are differentiable at x,

(iv) M (x0 , U ) is continuously differentiable a.s. at each x0 in a neighborhood of x, and
     there exists a real-valued function    B such that for any x0 in a neighborhood of x,
              0
                                       R
     |∇M (x , U )| ≤ B(U ) a.s., and B(u)dµ(u) < ∞,

(v) Pr{M (X, U ) = L(X)|X = x} = Pr{M (X, U ) = H(X)|X = x} = 0.


                                               5
    The first assumption is stronger than the usual conditional mean independence assump-
tion E[U |X] = 0 in a regression framework. However, the maximum likelihood estimator
for the Tobit model requires U to be a normal random variable and it is independent of
X. In Section 6.3, we discuss a generalization to the case of endogenous regressors. The
condition L(x0 ) < H(x0 ) for all x0 in a neighborhood of x in the second assumption reflects
the definition of L and H as the lower and upper bounds and is a regularity condition that
simplifies our analysis. The fourth assumption is standard and guarantees that one may
change the order of differentiation and integration. The rest of the assumptions are natural
given that we wish to estimate some aspects of derivatives. Here we implicitly assume that
all elements of X are continuous. In Section 6.3, we discuss the case where some elements
of X are discrete.
    The derivation of (5) is based on the derivative formula in (4). We prove the following
lemma which extends the formula in (4) to the general case.

Lemma 3.1 Under Assumption 3.1,
        Z
      ∇ M (x, u)I{L(x) < M (x, u) < H(x)}dµ(u)
      Z
   =    ∇M (x, u)I{L(x) < M (x, u) < H(x)}dµ(u) − H(x)∇GH (x) − L(x)∇GL (x).

    The proof is contained in the appendix. We emphasize that this lemma applies to any
random object U with the probability measure µ and the region of integration need not
be rectangular. In particular, U may be a vector and M (X, U ) need not be monotone in
U . When L(x) = −∞, the last term on the right hand side does not appear and when
H(x) = ∞, the second term on the right hand side does not appear. From the definition of
β(x) in (3), this lemma directly implies the identification result in (5).

Theorem 3.1 Under Assumption 3.1, the expression for β(x) in (5) holds true.

    Based on this theorem, we derive nonparametric and semiparametric estimators of β(x)
in the next two sections. We close this section by a brief comparison with the control function
approach, such as Heckman (1976). Consider the standard Tobit model for simplicity.
The conventional control function approach is: (i) obtain the conditional mean function
E[Y |X = x, Y > 0] = xβ + Q(x) parametrically or semiparametrically, where Q(x) =
E[U |X = x, Y > 0], and then (ii) estimate β and Q(x) jointly. In contrast, our approach
is: (i) estimate
                            ∇E[Y |X = x, Y > 0] = β + ∇Q(x),
and then (ii) estimate the correction term ∇Q(x) to estimate β. More generally, β(x) is
given by (5), where the last three terms on the right hand side correspond to the correction
terms for sample selection. We emphasize that our approach can handle general random
object including a random function, and nonadditive error terms.




                                              6
4     Nonparametric Estimation
Based on Theorem 3.1, we estimate the parameter of interest β(x) by replacing the unknown
functions of x on the right hand side of (5) with either parametric or nonparametric estima-
tors. This section considers the nonparametric case. We first propose a fully nonparametric
estimator of β(x). Since the fully nonparametric approach may not be useful in multivariate
applications because of the curse of dimensionality, we also propose an estimator for the
average of β(x) over a range of X.

4.1    Estimation of β(x), L(x), and H(x)
To estimate β(x) from (5), we need to estimate the unknown functions Ψ(x), ∇Ψ(x), GM (x),
∇GL (x), ∇GH (x), L(x), and H(x). Observe that Ψ(x), ∇Ψ(x), GM (x), ∇GL (x), and
∇GH (x) are written as conditional mean functions or their derivatives. Thus, we can apply
any standard nonparametric estimator, such as the kernel or series estimator. Here we
employ the local polynomial estimator (see, e.g., Fan and Gijbels (1996)). In addition to
the statistical benefits discussed by Fan (1992), an additional benefit of the local polynomial
estimator is that we can estimate the conditional mean function and its derivatives at
the same time. Let x = (x1 , . . . , xk )0 be a vector ofQreal numbers and       q = (q1 , . . . , qk )0
                                                                     q
be a vector of non-negative integers. Define xq = kj=1 xj j , [q] =             k
                                                                               P
                                                                                j=1 qj , and q! =
Qk
Pj=1 qj ! with the convention
                        q
                                    that 0! = 1. Consider the p-th order polynomial Pp (α, x̄, x) =
           α
   0≤[q]≤p q   (x̄ − x)   /q!.  For  example, when k = 2 and p = 2, q 0 takes on the values (0, 0),
(1, 0), (0, 1), (1, 1), (2, 0), and (0, 2) and Pp (α, x̄, x) may be written as

        Pp (α, x̄, x) = α00 + α10 (x̄1 − x1 ) + α01 (x̄2 − x2 ) + α11 (x̄1 − x1 )(x̄2 − x2 )
                                α20                  α02
                              +       (x̄1 − x1 )2 +     (x̄2 − x2 )2 .
                                   2                   2
For a random sample {Zi , Xi }ni=1 , let α̂(x) denote the weighted least square estimator for
the regression of Zi on the terms of Pp (α, Xi , x), that is
                                       n                                           
                                       X                         2         Xi − x
(6)                  α̂(x) = arg min         {Zi − Pp (α, Xi , x)} K                    ,
                                   α                                        hn
                                       i=1

where K(·) is a kernel function and hn is a bandwidth parameter. The local polynomial
estimator for the conditional mean E[Z|X = x] corresponds to the constant term α̂(x)0,...,0 .
The estimator for the partial derivative ∇j E[Z|X = x] is α̂(x)0,...0,1,0,...0 , where 1 in the
subscript is at the jth term, which corresponds to the coefficient on the term Xij − xj in
the polynomial. By setting Z to Y , IM , IL , or IH as is appropriate, we can obtain the local
polynomial estimators of Ψ(x), ∇Ψ(x), GM (x), ∇GL (x), and ∇GH (x).
    We now consider estimation of the boundary functions L(x) and H(x). To estimate the
lower bound function L(x) (or the upper bound function H(x)), we apply a nonparametric
extreme quantile regression approach in which the quantile goes to 0 (or 1) as n gets large
(see, Chernozhukov (1998), Knight (2001), and Ichimura, Otsu and Altonji (2008)). Let
{bn }n∈N be a sequence of positive numbers satisfying bn → 0 as n → ∞, and consider the
estimated parameters of the (the r-th order) local polynomial quantile regression at the


                                                    7
τ n -th quantile:
                                     n                                                        
                                     X                                                Xi − x
(7)          α̂(x; τ n ) = arg min         ρτ n (Yi − Pr (α, Xi , x)) IM (Xi )K                    ,
                               α                                                        bn
                                     i=1

where ρτ n (v) = (τ n − I{v ≤ 0})v is Koenker and Bassett’s (1978) check function. The
kernel function does not have to be the same as that used in the estimation of functions
Ψ(x), GM (x) and derivatives of Ψ(x), GM (x), GL (x), and GH (x).
    For the sequence {τ n }n∈N satisfying τ n → 0, our estimator of L(x) is the constant
term α̂(x; τ n )0,...,0 . Similarly, our estimator of H(x) is the constant term α̂(x; τ n )0,...,0 when
τ n → 1.
    From (5), the parameter of interest β(x) is rewritten as

                                        β(x) = c(x)0 ⊗ Ik D(x),
                                                         
(8)

where Ik is the k-dimensional identity matrix, ⊗ is the Kronecker product,

                     c(x) = (1, cG (x)0 )0 ,
                    cG (x) = (Ψ(x)/GM (x), H(x)/GM (x), L(x)/GM (x))0 ,
                    D(x) = (∇Ψ(x)0 , ∇GM (x)0 , ∇GH (x)0 , ∇GL (x)0 )0 .

Our estimator of β(x) consists of replacing the functions c(x) and D(x) with the nonpara-
metric estimators defined above. Denoting the estimators of c(x) and D(x) by ĉ(x) and
D̂(x), respectively, we define our estimator as β̂(x) = [ĉ(x)0 ⊗ Ik ] D̂(x).
    To clarify the structure of the asymptotic theory we first state a lemma based on higher
level assumptions about the asymptotics of the nonparametric estimators ĉ(x) and D̂(x).

Lemma 4.1 Suppose that for some sequence {rn }n∈N satisfying rn → ∞ as n → ∞,

   1. rn (D̂(x) − D(x)) converges in distribution to a normal random vector with mean 0
      and variance-covariance matrix V (x),

   2. rn (ĉ(x) − c(x)) converges in probability to 0.
      Then
                                          d
                        rn (β̂(x) − β(x)) → N (0, (c(x)0 ⊗ Ik )V (x)(c(x) ⊗ Ik )).

     The proof follows from the continuous mapping theorem. This lemma says that the
first order asymptotics of β̂ (x) are driven by those of D̂ (x), the vector of nonparametric
estimators of derivatives of conditional mean functions. The estimators ĉ(x) of c (x) can
be treated as constants to the first order. Condition 1 is satisfied by most nonparametric
estimators by adequately choosing the convergence rate rn . Note that the rate rn typically
depends on the number of regressors k (i.e., the curse of dimensionality). For Condition 2,
we need to guarantee faster convergence rates for Ψ̂(x), ĜM (x), L̂(x), and Ĥ(x) than rn−1 .
For the estimators of the conditional mean functions Ψ̂(x) and ĜM (x), it is known that
the optimal convergence rate of nonparametric estimators of the conditional mean is faster
than that of its derivatives (see, Stone (1980)). For the nonparametric quantile regression


                                                      8
estimators L̂(x) and Ĥ(x) with drifting quantiles τ n , we can apply the asymptotic theory of
extreme quantile regression by Chernozhukov (1998) or Ichimura, Otsu and Altonji (2008).
    We now describe a concrete version of the above lemma. We first consider the compo-
nents D(x), Ψ(x), and GM (x) estimated by the local polynomial least square regression in
(6). Denote the conditional distribution of Y given X = x and IM (X) = 1 by FY (y|x)
and the marginal Lebesgue density of X by f (x). Based on Masry (1996a, Theorem 5), we
impose the following assumptions.

Assumption 4.1 Assume that

(i) {Yi , Xi }ni=1 is iid,

(ii) GM (x) > 0 and 0 < f (x) < ∞,

(iii) Ψ, GM , GH , and GL are continuously differentiable at x up to total order of p + 1,
      FY (y|x) is continuous at x, and E |Yi IM (Xi )|2 < ∞,

(iv) K : RRk → R is a uniformly bounded symmetric function, supported on a compact set
     and ss0 K(s)ds is positive definite.

(v) as n → ∞, hn → 0, nhk+2
                        n   → ∞, and nhk+2p+2
                                       n      → 0.

    To present the asymptotic distribution of D̂ (x), we need additional notation from Masry
(1996a). Let q be a k×1 vector with non-negative integer arguments and Nt = i+k−1              k−1     be the
number of distinct k×1 vectors with [q] = t for a given non-negative interger t. Arrange these
Nt vectors in a lexicographical order from qt,1 = (t, 0, . . . , 0)0 to qt,Nt = (0, .R. . , 0, t)0 . Let Mi,t
                                                                                           qt,l qt̄,m K(a)da
and ΓR i,t̄q be qNt × Nt̄ 2matrices for t, t̄ = 0, . . . , p whose (l, m) elements are a a
andP a t,l a t̄,m K(a) da and let M = [Mt,t̄ ] and Γ = [Γt,t̄ ]. Let A = [0k , Ik , 0k , . . . , 0k ]0 be
a ( p0=1 Ni ) × k matrix, where 0k is a k × 1 vector of zeros. The asymptotic properties of
the local polynomial estimators are obtained as follows.

Lemma 4.2 Suppose that Assumption 4.1 holds. Then
                        q
                                              p
                          nhk+2
                            n (Ψ̂(x) − Ψ(x)) → 0,
                        q
                                                  p
                          nhk+2
                            n (ĜM (x) − GM (x)) → 0,
                        q
                                               d
                          nhk+2
                            n (D̂(x) − D(x)) → N (0, V (x)),

where
                        σ 2M (x)/GM (x)
                                                 
                                         0
                                                      ⊗ f (x)−1 A0 M−1 ΓM−1 A ,
                                                                             
        V (x) =
                                0       Ω(x)
                                                                         
                 GM (x)(1 − GM (x)) −GM (x)GH (x)      −GM (x)GL (x)
        Ω(x) =  −GM (x)GH (x)      GH (x)(1 − GH (x)) −GH (x)GL (x)      .
                 −GM (x)GL (x)      −GH (x)GL (x)      GL (x)(1 − GL (x))



                                                      9
    Since this theorem is a special case of Masry (1996a, Theorem 5), the proof is omitted.
Assumption 4.1 (i) is on the sampling of data. It is possible to extend the setup to allow
weakly dependent data (see, Masry (1996a)). Assumption 4.1 (ii) says that the probability
that Y is uncensored conditional on X = x is positive and that the Lebesgue density of X
is positive at x. These are natural conditions given that we are trying to estimate β(x).
Assumption 4.1 (iii) is on the smoothness of the estimand functions Ψ, GM , GH , and GL to
be estimated. Assumption 4.1 (iv) restricts the shape of the kernel function. For example,
the triangle kernel and Epanechnikov kernel satisfy this condition. Assumption 4.1 (v)
contains standard conditions for the bandwidth hn . The condition nhk+2p+2  n       → 0 is used
to eliminate the asymptotic bias term in the local polynomial estimator. Note that if the
dimension of the regressors k is higher, then the convergence rate of the estimator becomes
slower (i.e., the curse
                     p of dimensionality). The     presults on Ψ̂(x) and ĜM (x) are obtained
from the fact that nhkn (Ψ̂(x) − Ψ(x)) and nhkn (ĜM (x) − GM (x)) are asymptotically
normal (i.e., Op (1)). Since (1, 1, 1)Ω(x) = (0, 0, 0), Ω(x) is a singular matrix. However, the
vector cG (x) is not proportional to (1, 1, 1)0 . If it is, then L(x) = H(x) and it contradicts
with GM (x) > 0 in Assumption 4.1 (ii). In order to conduct inference on β(x), we need to
estimate the asymptotic variance V (x). The matrices M and Γ can be evaluated analytically,
typically, and always by numerical integration, and the functions σ 2M (x), GM (x), Ω(x), and
f (x) can be consistently estimated by some nonparametric estimators. Note that GM (x)
and Ω(x) may be estimated from the constant terms of the local polynomial estimators for
GM (x), GH (x), and GL (x).
    We next consider the local polynomial quantile regression estimators L̂(x) and Ĥ(x).
For simplicity we focus on the lower bound estimator L̂(x). Similar results hold for the
upper bound estimator Ĥ(x). Recall that the sample size is denoted n and bn denotes the
bandwidth used in the local polynomial quantile regression. The asymptotic properties of
L̂(x) crucially depends on the convergence rate of the drifting quantile τ n to 0, and can be
analyzed by splitting into three cases: (i) extreme case (nbkn τ n → 0), (ii) intermediate case
(nbkn τ n → ∞), and (iii) edge case (nbkn τ n → c > 0). For the extreme case, the quantile
regression estimator L̂(x) is asymptotically equivalent to a linear programming estimator,
and its asymptotic properties are investigated by Chernozhukov (1998). The intermediate
case is considered by Ichimura, Otsu and Altonji (2008). Although the asymptotics of the
edge case are an open area for research in this nonparametric setup, we conjecture that
the results analogous to Chernozhukov (2005) hold. Here we present the convergence rate
of L̂(x) and Ĥ(x) for the intermediate case.5 Let Bx be some fixed closed ball around x.
Based on Ichimura, Otsu and Altonji (2008), the convergence rates of L̂(x) and Ĥ(x) are
obtained as follows. The relation f1 (a) ∼ f2 (a) means that f1 (a)/f2 (a) → 1 as a specified
limit for a.

Assumption 4.2 Assume that

(i) X is absolutely continuous on Bx , and f is continuous at x and is positive on Bx ,

(ii) for all n large enough, the τ n -th conditional quantile function Lτ n (x) and Hτ n (x) are
  5
   For the extreme case (nbkn τ n → 0), the convergence rates of L̂(x) and Ĥ(x) can be obtained from
Chernozhukov (1998, Theorem 3).


                                                 10
        continuously differentiable up to total order of r at each x ∈ Bx , and its rth derivative
          (r)         (r)
        Lτ n (x) and Hτ n (x) are uniformly Lipschitz on Bx ,
(iii) the conditional distribution function FV (v|x) of V = Y − L(X) ≥ 0 and FU (u|x) of
      U = H(X) − Y ≥ 0 given X = x and IM (X) = 1 have Lebesgue density fV (v|x) and
      fU (u|x) are continuous and are positive on u, v ∈ [0, δ) for some δ > 0 uniformly on
      x ∈ Bx .
(iv) as n → ∞, it holds bn → 0 and for the case of estimating L(x), τ n → 0 and nbkn τ n → ∞
     and for the case of estimating H(x), τ n → 1 and nbkn (1 − τ n ) → ∞. Also assume that
      nbk+2r+2   τn
        log log n → 0.
        n



     Assumption 4.2 (i) and (ii) are standard (see, Chaudhuri (1991a)). Assumption 4.2
(iii), which plays a key role, is on the tail behavior of the error terms V and U . As we
consider censored data, this assumption seems reasonable. This assumption also implies
that |Lτ (x) − L(x)| ≤ Cτ for some C > 0 uniformly on x ∈ Bx , for example, and helps
control the bias due to using the drifting quantile. Although the convergence rates become
more complicated, it is possible to consider more general situations for the tail behavior of
V .6 Assumption 4.2 (iv) is on the drifting quantile τ n and the bandwidth bn .

Lemma 4.3 Suppose that Assumptions 4.1 (i) and 4.2 hold. Then
                                       q                  
                                                          k
        |L̂(x) − L(x)| = Oas max τ n , τ n log log n/(nbn )    ,
                                           q                       
                                                                  k
       |Ĥ(x) − H(x)| = Oas max 1 − τ n , (1 − τ n ) log log n/(nbn )   .

    Thus the boundary functions can be estimated with rate log log n/(nbkn ).
    Combining Lemmas 4.1-4.3, the asymptotic distribution of the nonparametric estimator
β̂(x) is obtained as follows.

Theorem 4.1 Suppose that Assumptions 4.1 and 4.2 hold. Furthermore, assume that
hk+2          2    2k           k+2 2
 n (log log n) /(nbn ) → 0 and hn τ n → 0 as n → ∞. Then
                q
                                     d            0
                 nhk+2
                    n (β̂(x) − β(x)) → N (0, (c(x) ⊗ Ik )V (x)(c(x) ⊗ Ik )).

    The bandwidths are required to satisfy the conditions to eliminate the bias.
    Theorem 4.1 shows that our estimator β̂(x) converges to β(x) at a rate that depends on
the number of the regressors k. Since the bandwidth hn converges to 0, higher k implies
a slower rate of convergence. Thus, if we insist on estimating β(x) for each x without
restricting the class of potential functions for β(x), the curse of dimensionality results. One
way to circumvent the curse of dimensionality is to focus on the averages of β(x) over a
subset of the support of X. In other contexts, Ahmad (1976), Hall and Marron (1987),
and Powell, Stock and Stoker (1989) showed that the average of certain nonparametric
                                                                             √
estimators converges to the limiting distribution at the parametric (or n) rate. In the
next subsection we show that an analogous result holds in our case.
  6
      See Ichimura, Otsu and Altonji (2008)


                                                11
4.2      Estimation of Averages of β(Xi ), L(Xi ), and H(Xi )
Let X̄ be a compact subset of the support of X (denoted as X) and Ii = I{Xi ∈ X̄} be
a trimming term. Our parameter of interest is defined as β(X̄) = E[β(Xi )|Ii = 1]. For
simplicity we will usually suppress the X̄ argument and write β(X̄) as β. We estimate β by
                                                n
                                                X                       n
                                                                        X
                                   β̂ = n−1           Ii β̂(Xi )/(n−1         Ii ).
                                                i=1                     i=1

For the asymptotic distribution of β̂, we add the following assumptions.

Assumption 4.3 Assume that

(i) X̄ is compact, the (p + 1)-th total order derivatives of Ψ are uniformly bounded and
      Lipschitz continuous on X, the (p + 1)-th total order derivatives of GM , GH , and GL
      are Lipschitz continuous on X, f is uniformly bounded and uniformly continuous on X
      and is continuously differentiable on X̄ up to total order of 2, inf x∈X̄ GM (x) ≥ c1 for
      some c1 > 0, inf x∈X̄ f (x) ≥ c2 for some c2 > 0, and E |Yi IM (Xi )|4 < ∞, E[Yi2 |Xi =
      x, IM (Xi ) = 1] is continuous on x ∈ X̄,

(ii) K is non-negative and satisfies K(a)da = 1, aK(a)da = 0, aa0 K(a)da = c3 Ik for
                                      R              R                R

      some c3 > 0, and aq K(a) is Lipschitz continuous on the support of K for all q with
      0 ≤ [q] ≤ 2p + 1,
                                       2p
(iii) as n → ∞, nhk+2
                  n / log(n) → ∞ and nhn → 0,

(iv) supx∈X̄ L̂(x) − L(x) = op (n−1/2 ) and supx∈X̄ Ĥ(x) − H(x) = op (n−1/2 ).

    Assumption 4.3 (i)-(iii) are similar to those used in Masry (1996b, Theorem 6) and Li,
Lu and Ullah (2003, Theorem 2.1). Assumption 4.3 (i), which is an extension of Assumption
4.1 ((ii) and (iii)), contains smoothness and boundedness conditions over the sets X and X̄
as well as the requirement that Y has a positive probability of being uncensored at the
values of X in X̄. Assumption 4.3 (ii) and (iii) are additional conditions on the kernel
function K and the bandwidth hn , respectively. The condition nh2p       n → 0 is required to
make the asymptotic bias termP   negligible. Assumption 4.3 (iv) is required  to guarantee that
the average components n−1/2 ni=1 Ii (L̂(Xi ) − L(Xi )) and n−1/2 ni=1 Ii (Ĥ(Xi ) − H(Xi ))
                                                                      P
                                                                   √
are asymptotically negligible in the first-order asymptotics of n(β̂ − β). Although it is
technically challenging, these higher level assumptions may be replaced with more primitive
ones by establishing the uniform Bahadur representation of the boundary estimators. Under
the intermediate quantile case (nbkn τ n → ∞), the conditions on L̂(x) and Ĥ(x) are satisfied
if Assumption 4.2 (i)-(iii) hold over X̄ instead of holding on the set Bx and the bandwidth
bn satisfies nb2k                     2r
               n / log n → ∞ and nbn → 0 when τ n = log n/(nbn ).
                                                                   k 7

    There are two reasons to introduce the trimming term Ii over the subset X̄. First, in
practice, empirical researchers are typically interested in the behavior of β(x) over some fixed
subset of the support of X. Second, as a technical matter, averaging over the trimming set
  7
      See, Ichimura, Otsu and Altonji (2008).


                                                         12
X̄ allows us to apply the uniform convergence results on L̂(x) and Ĥ(x) of Chernozhukov
(1998) for the extreme case or Ichimura, Otsu and Altonji (2008) for the intermediate
case. Although it is beyond the scope of P     this paper, it may be possible to consider the
sample average without trimming (i.e., n−1 ni=1 β̂(Xi )) as the average derivative estimator
of E[β(Xi )]. The main technical difficulty is to derive a uniform convergence rate of the
boundary estimators for L(x) and H(x) over the whole support X or a growing subset that
converges to X as n grows to infinity (see, e.g., Ai (1997)).
    Denote Qs,t,t̄ be an Nt × Nt̄ matrix for s = 1, . . . , k and t, t̄ = 0, . . . , p whose (l, m)
element is Qs,t,t̄ = as aqt,l aqt̄,m K(a)da, Qs = [Qs,t,t̄ ], and Q(x)= ks=1 ∂f (x)/∂xs Qs . The
                    R                                                  P

asymptotic distribution of the average derivative estimator β̂ is obtained as follows.

Theorem 4.2 Suppose that Assumptions 4.1 (i), (iii), and (iv) and 4.2 hold. Then
                                     √             d
                                         n(β̂ − β) → N (0, Σ),

where P = Pr(Ii = 1) and Σ is the variance covariance matrix of

             Ii h
    ri =         β(Xi ) − β
             P
                                                         M−1 Q(Xi ) 2:k+1,1
                                                                                           !
                                                                                ∇GM (Xi )
            +(Yi − E[Yi |IM (Xi ) = 1, Xi ])IM (Xi )                          −
                                                           GM (Xi )f (Xi )      GM (Xi )2
           +(IM (Xi ) − E[IM (Xi )|Xi ])
                 Ψ(Xi ) M−1 Q(Xi )
                                                                                          !
                                         2:k+1,1   H(X   i )∇G H  (Xi )   L(Xi )∇G L (Xi )
             ×                                   −                      −
                        GM (Xi )f (Xi )               GM (Xi )2             GM (Xi )2
                                          H(Xi ) M−1 Q(Xi ) 2:k+1,1
                                                                 
           +(IH (Xi ) − E[IH (Xi )|Xi ])
                                                  GM (Xi )f (Xi )
                                         L(Xi ) M−1 Q(Xi ) 2:k+1,1 i
                                                              
           +(IL (Xi ) − E[IL (Xi )|Xi ])
                                                 GM (Xi )f (Xi )
           +β(Ii − P ),

where M−1 Q(Xi ) 2:k+1,1 is a k × 1 column vector defined from (2, 1) element to (k + 1, 1)
                  

element of M−1 Q(Xi ).

    Note that the convergence rate of β̃ no longer depends on the dimension of the regressors
k. This phenomenon is common in average derivative estimation, where the data used in
the local estimate β̃(Xi ) overlap with the data used in β̃(Xj ) if Xi and Xj are sufficiently
close. The theorem can be modified to obtain the asymptotic distribution of estimators of
other weighted averages of β(x), i.e., E[Ii w(Xi )β(Xi )] for some weight function w.


5    Semiparametric Estimation
Imposing a priori information about potential functional forms is the most obvious way to
circumvent the curse of dimensionality. One way to impose a parametric specification is to

                                                 13
specify M (x, u), L(x), H(x), and the distribution of U parametrically. This approach always
leads to a parametric model which is consistent with the model (1). However, we may not
wish to specify the distribution of U explicitly, particularly if U is a vector. An alternative
is to specify Ψ(x), GH (x), GL (x), L(x), and H(x) parametrically without specifying the
distribution form of U . We consider the following semiparametric model:
                                  
                                   Ψ(X; θ) + V if IM (X) = 1,
(9)                          Y =     C     if IL (X) = 1,
                                   L
                                     CH if IL (X) = 1,
                          IL (x) = 1 with probability GL (x; θ),
                         IH (x) = 1 with probability GH (x; θ),
where E[V |IM (X) = 1, X = x] = 0 and θ is a finite dimensional parameter vector. From
the definitions, IM (x) = 1 with probability 1 − GH (x; θ) − GL (x; θ). To aid the search for
functional forms, the following lemma identifies the conditions that the parametric spec-
ification must satisfy to be consistent with a member of the class of models specified in
(1).
Lemma 5.1 Under the model (9), suppose that
  1. there exists ε > 0 such that L(x; θ) + ε < Ψ(x; θ) < H(x; θ) − ε for all x,
  2. there exist p1 and p2 such that 0 < GL (x; θ) < p1 < p2 < 1 − GH (x; θ) < 1 for all x.
     Then (1) holds with
                  M (X, U ) = M0 (X) + M1 (X)U1 + M2 (X)U2 ,
                      L(X) = L(X; θ),        GL (x; θ) = Pr{IL (X) = 1|X = x},
                     H(X) = H(X; θ),          GH (x; θ) = Pr{IH (X) = 1|X = x},
      where M1 (x), M2 (x) > 0 for all x, U1 and U2 are independent scalar random variables,
      and U = (U1 , U2 ) has the joint density fU such that
                                       Z
                             Ψ(x; θ) =               M (x, u)fU (u)du.
                                         u∈{u:IM (x)=1}

    It is remarkable that we do not need to consider more general forms of M (x, u) than
the one specified in this lemma. The reason is that the parameter of interest in our analysis
is the conditional mean of the derivative of M (X, U ) rather than the whole function of
M (X, U ).
    There is a simple way to impose the conditions of Lemma 5.1. First, specify some
parametric functional forms on L(x; θ), a(x; θ), ∆1 (x; θ) > 0, ∆2 (x; θ) > 0, 0 < P (x; θ) < 1,
and a distribution function F (·). Then the model that satisfies the conditions in Lemma
5.1 is obtained as H(x; θ) = L(x; θ) + ∆1 (x; θ) and
                    
                     L(X; θ)P (X; θ) + H(X; θ)(1 − P (X; θ)) + V if IM (X) = 1,
            Y =        C     if IL (X) = 1,
                     L
                       CH if IL (X) = 1,
        IL (x) = 1 with probability F (L(x; θ) + a(x; θ)),
       IH (x) = 1 with probability 1 − F (H(x; θ) + a(x; θ) + ∆2 (x; θ)).

                                              14
    We now discuss the estimation problem of the parameter of interest by the semipara-
metric model (9). This estimation problem is not standard because of the presence of
the lower and upper bound functions L(x; θ) and H(x; θ). For simplicity we assume that
θ = (θ0L , θ0H , θ0R )0 ∈ Θ = ΘL × ΘH × ΘR , L(x; θ) = L(x; θL ), and H(x; θ) = H(x; θH ). The
parameter vector θR appears in GL (x; θ), GH (x; θ), and Ψ(x; θ). To estimate the parame-
ters θL and θH in the boundary functions L(x; θL ) and H(x; θH ), we apply extreme quantile
regression:
                                       n
                                       X
                θ̂L = arg min                ρτ n (Yi − L(Xi ; θL ))IM (Xi ) for τ n → 0,
                              θL ∈ΘL
                                       i=1
                                        Xn
                θ̂H   = arg min              ρτ n (Yi − H(Xi ; θH ))IM (Xi ) for τ n → 1.
                              θH ∈ΘH
                                       i=1

By combining the discrete choice likelihood for GL (x; θ) and GH (x; θ) and the least square
objective function for Ψ(x; θ), the remaining parameter θR can be estimated as

                                  θ̂R = arg max `(θ̂L , θ̂H , θR ),
                                                θR ∈ΘR

where

            `(θL , θH , θR )
            Xn
          =    [IL (Xi ) log GL (Xi ; θL , θH , θR ) + IM (Xi ) log GM (Xi ; θL , θH , θR )
              i=1
                                                          n
                                                          X
              +IH (Xi ) log GH (Xi ; θL , θH , θR )] −          (Yi − Ψ(Xi ; θL , θH , θR ))2 IM (Xi ).
                                                          i=1

Note that if there is no overlapping parameter of θR in Ψ(x; θ) and (GL (x; θ), GH (x; θ)).
Then we can separately maximize the two terms in `(θL , θH , θR ). There may be an efficiency
gain in accounting for heteroskedasticity in V but we do not consider this problem here.
The asymptotic properties of the extreme quantile regression estimators θ̂L and θ̂H can
be derived from, e.g., Knight (2001) or Chernozhukov (2005) when the model is linear
in parameters. The asymptotic property of θ̂R depends on the convergence rates of θ̂L
             √                                √
and θ̂H . If n(θ̂L − θL ) = op (1) and n(θ̂H − θH ) = op (1), then we can apply the
standard asymptotic theory on extremum estimators for θ̂R (see, e.g., Newey and McFadden
(1994)). In particular, the asymptotic distribution of θ̂R is equivalent to that of θ̂R =
arg maxθR ∈ΘR `(θL , θH , θR ) in which θL and θH are known. The semiparametric estimator
for the parameter of interest β(x) is obtained by replacing the unknown functions with their
parametric estimators.
    We close this section by noting that an alternative or complementary strategy is to
make use of linear index restrictions in the spirit of Ichimura and Lee (1991). One could
specify the model as M (x0 θM , U ), L(x0 θL ), H(x0 θH ), GL (x0 θL , x0 θM ), and GH (x0 θH , x0 θM ),
where M , L, H, GL , and GH are nonparametric functions. These restrictions imply that
Ψ(x) = Ψ(x0 θM , x0 θL , x0 θH ), GL (x) = GL (x0 θL , x0 θM ), and GH (x) = GH (x0 θH , x0 θM ).
Using the methods of Ichimura and Lee (1991), it would be relatively straightforward to

                                                     15
implement the estimator with the index restrictions imposed. It might also be possible to
work with partially linear specifications of the M, GL , and GH functions using a two-step
approach in the spirit of Chen and Kahn (2001) to estimate Ψ, GL , and GH functions.
Finally, Lemma 5.1 may provide a way to further restrict the specification.


6     Extensions
6.1     Estimating the Effects of Discrete Regressors
Thus far we have discussed estimation of the average derivatives of Y with respect to X,
and our assumptions rule out discrete regressors. This section considers the case where X
contains both continuous and discrete elements. We assume we can partition X into X =
(XC , XD ), where XC and XD are vectors of continuous and discrete regressors, respectively.
Let β C (xC , xD ) denote the vector of average derivatives of Y with respect to XC given
IM (X) = 1, XC = xC , and XD = xD . It would be straightforward to extend our methods
above to allow estimation of β C (xC , xD ). However, estimation of the effect of XD raises
issues of identification. For notational simplicity, assume XD is a scalar binary random
variable that takes on the values 0 and 1. There are a number of ways we can define
parameters of interest.8 Here we consider identification of

      β 01
        D (xC , xD ) = E(IM (xC , 1)M (xC , 1, U ) − M (xC , 0, U )|IM (xC , 0) = 1, XC = xC );

the effect of a shift in XD from 0 to 1 on the average value of Y chosen by those for
whom IM (xC , 0) = 1 (initially uncensored). Assume that L(X) = 0 and H(X) = ∞. The
parameter of interest can be rewritten as

      β 01
        D (xC , xD ) = E(IM (xC , 1)M (xC , 1, U )|IM (xC , 0) = 1, XC = xC )
                          −E(M (xC , 0, U )|IM (xC , 0) = 1, XC = xC )
                      = E(IM (xC , 1)IM (xC , 0)M (xC , 1, U )|XC = xC )/GM (xC , 0)
                          −E(M (xC , 0, U )|IM (xC , 0) = 1, XC = xC )
                      = E(IM (xC , 1)M (xC , 1, U )|XC = xC )/GM (xC , 0)
                          −E(M (xC , 0, U )|IM (xC , 0) = 1, XC = xC )
                          −E(IM (xC , 1)(1 − IM (xC , 0))M (xC , 1, U )|XC = xC )/GM (xC , 0)
                      = E(M (xC , 1, U )|IM (xC , 1) = 1XC = xC )GM (xC , 1)/GM (xC , 0)
                          −E(M (xC , 0, U )|IM (xC , 0) = 1, XC = xC )
                          −E(IM (xC , 1)(1 − IM (xC , 0))M (xC , 1, U )|XC = xC )/GM (xC , 0)


Note that although the first and second terms of β 01D (xC , xD ) are estimable from the sample
analogs, the third term is not in general. If IM (xC , 1) ≤ IM (xC , 0), then the last term is
zero and thus we can identify β 01
                                D (xC , xD ). If IM (xC , 1) ≤ IM (xC , 0) does not hold always,
then we need to analyze the bounds of the third term.
   8
     For example E(M (xC , 1, U ) − M (xC , 0, U )|IM (xC , 1) = 1, IM (xC , 0) = 1, XC = xC ). This parameter
can be analyzed analogously.


                                                     16
         One way to find the bound is to impose the following assumption.9
Assumption 6.1 M (xC , 0, u0 ) < M (xC , 0, u00 ) if and only if M (xC , 1, u0 ) < M (xC , 1, u00 ).
    The assumption presumes the ordering of individuals do not change between the two
cases. For any u0 ∈ {u0 : IM (xC , 0) = 0, IM (xC , 1) = 1} and u00 ∈ {u00 : IM (xC , 0) =
1, IM (xC , 1) = 1}, we have M (xC , 0, u0 ) ≤ 0 < M (xC , 0, u00 ) and 0 < M (xC , 1, u0 ) <
M (xC , 1, u00 ) by the above assumption.
    Let B(xC ) = E(IM (xC , 1)(1 − IM (xC , 0))M (xC , 1, U )|XC = xC )/GM (xC , 0). Note that
B(xC ) > 0 from M (xC , 1, u0 ) > 0 for any u0 ∈ {u0 : IM (xC , 0) = 0, IM (xC , 1) = 1}. For the
upper bound of B(xC ), observe that
                                  E[M (xC , 1, U )|IM (xC , 0) = 0, IM (xC , 1) = 1]
                              ≤                  min                      M (xC , 1, u00 )
                                  u00 ∈{u00 :IM (xC ,0)=1,IM (xC ,1)=1}
                              ≤ E[M (xC , 1, U )|IM (xC , 0) = 1, IM (xC , 1) = 1],
and
             Ψ(xC , 1)
         = E[M (xC , 1, U )|IM (xC , 0) = 1, IM (xC , 1) = 1] Pr {IM (xC , 0) = 1|IM (xC , 1) = 1}
             +E[M (xC , 1, U )|IM (xC , 0) = 0, IM (xC , 1) = 1] Pr {IM (xC , 0) = 0|IM (xC , 1) = 1}
         ≥ E[M (xC , 1, U )|IM (xC , 0) = 0, IM (xC , 1) = 1].
Thus, the upper bound of B(xC ) is obtained as
                             B(xC )
                         = E[M (xC , 1, U )|IM (xC , 0) = 0, IM (xC , 1) = 1]
                             × Pr{IM (xC , 0) = 0, IM (xC , 1) = 1}/GM (xC , 0)
                         ≤ Ψ(xC , 1) min{1 − GM (xC , 0), GM (xC , 1)}/GM (xC , 0).
If one assumes that M is nondecreasing in XD , with M (xC , 0, u) ≤ M (xC , 1, u) for all u,
then the bound becomes
                      0 < B(xC ) < [GM (xC , 1) − GM (xC , 0)]Ψ(xC , 1)/GM (xC , 0).
By a similar argument, we can analyze β 10
                                        D (xC , xD ), the effect of a shift in xD from 1 to 0
on the mean of Y chosen by those for whom IM (xC , 1) = 1, i.e.,
                                            GM (xC , 0)
                         β 10
                           D (xC , xD ) =               Ψ(xC , 0) − Ψ(xC , 1) + B 0 (xC ),
                                            GM (xC , 1)
where the bound for the bias term
                     Z
             0
           B (xC ) =                                              M (xC , 0, u)dµ(u)/GM (xC , 0)
                                u∈{u:IM (xC ,0)=1,IM (xC ,1)=0}
is
                 0 ≤ B 0 (xC ) ≤ min{GM (xC , 0), 1 − GM (xC , 0)}Ψ(xC , 0)/GM (xC , 1).
We leave the analysis of the effect of XD in the case in which Y is censored by the general
functions L(XC , XD ) and/or H(XC , XD ) to further research.
     9
         This assumption is used by Heckman, Smith, Clements (1997).


                                                          17
6.2    Measurement Error in the Dependent Variable
This section considers the effect of measurement error in the dependent variable Y . Consider
a special case, where H(x) = ∞ and L(x) = 0 (or some known constant). In this special
case, GH (x) = 0 and GL (x) = 1 − GM (x). Instead of Y and IM (X), we observe

(10)                              Y ∗ = IR IM (X)(e1 Y + e2 ),
                                   ∗
                                  IM    = IR IM (X),

respectively, where IR is a Bernoulli random variable (IR is 1 with probability p and is 0
with probability 1 − p) that is independent of (X, U, e1 , e2 ), e1 is a positive random variable
with the mean µ that is independent of (X, U, IR ), and e2 is a random variable with the
mean 0 that is independent of (X, U, IR ). IR can be interpreted as random variation in
whether Y is reported or not. e1 and e2 are multiplicative and additive measurement errors
for Y , respectively. The definition of Y ∗ implies10

                               E[Y ∗ |X = x, IM
                                              ∗
                                                = 1] = µΨ(x),
                                     ∗
                                 Pr{IM = 1|X = x} = pGM (x).

It follows immediately from the derivation of (5) that if one uses Y ∗ instead of Y to estimate
the components of β(x) in (5), then the probability limit of the estimator of β(x) is obtained
as
                                           µβ(x).
Hence, random variation IR in whether the value of Y is reported when IM (X) = 1 does
not affect the probability limit of the estimator provided that the report of Y is unbiased
(i.e., E[Y ∗ |Y = y, IM (X) = 1] = y), even if a fraction of respondents with Y > L(X) report
 ∗ = 0. The same conclusions go through if Y ∗ = I ∗ f (Y, e) under the assumptions that
IM                                                        M
the measurement error component e is distributed independently of (X, U, IR ) and that the
function f and the distribution of e satisfy E[f (Y, e)|Y = y, IM = 1] = y. Thus the form of
the measurement error can be generalized a bit.
     Unfortunately, measurement error in Y in the form of (10) is a serious problem if L(x)
has to be estimated, because the conditional quantiles of Y ∗ and Y will not coincide. The
estimators of L(x) and β(x) are consistent even if p is less than 1 provided e1 = 1 and
e2 = 0. When both L(x) and H(x) must be estimated, then both forms of measurement
error lead to inconsistency.

6.3    Endogenous Regressors in a Cross Section
Our estimator can be modified to handle the case where X is correlated with U using
a control function approach. Assume that the distribution of X depends on a vector of
observable variables W . One can write X as X = ϕ(W ) + V , where ϕ(W ) is defined so that
E[V |W = w] = 0 a.s. We assume
                                        U ⊥ W |V.
  10
    Note that there may be cases in which Y ∗ is negative even though IM
                                                                       ∗                           ∗
                                                                         = 1. The researcher uses IM as
the indicator for whether Y > 0.



                                                  18
This assumption is strong, but will be hard to avoid unless one is willing to impose addi-
tional restrictions on M (X, U ), such as monotonicity in scalar valued function of U . This
assumption implies that dµ(u|ϕ(w), v) = dµ(u|v) for all v, where dµ(u|ϕ(w), v) and dµ(u|v)
are the conditional densities of U given (ϕ(W ), V ) = (ϕ(w), v) and V = v, respectively.
Let dµV (v|x) be the conditional density of V given X = x. Since X and W are observable,
one can consistently estimate ϕ(w) and dµV (v|x) under some regularity conditions. Given
ϕ(w), one can estimate the regression function Ψ(x, v) = E[Y |X = x, V = v, IM (X) = 1],
which can be written as
                             Z
                 Ψ(x, v) =                    M (x, u)dµ(u|ϕ(w), v)/GM (x, v)
                               u∈{u:IM (x)=1}
                             Z
                          =                   M (x, u)dµ(u|v)/GM (x, v),
                                    u∈{u:IM (x)=1}

where GM (x, v) = Pr{IM (X) = 1|X = x, V = v} = Pr{IM (X) = 1|ϕ(W ) = ϕ(w), V = v}.
The parameter of interest is
                    Z
         β(x) =                      ∇M (x, u)dµ(u|x)/GM (x)
                      u∈{u:IM (x)=1}
                    Z Z
(11)             =                    {∇M (x, u)dµ(u|x, v)/GM (x, v)}dµV (v|x).
                           v   u∈{u:IM (x)=1}

Differentiating Ψ(x, v) with respect to x holding v fixed leads to
                  Z
   ∇Ψ(x, v) =                    ∇M (x, u)dµ(u|v)/GM (x, v)
                    u∈{u:IM (x)=1}
                   +{H(x)∇GH (x, v) + L(x)∇GL (x, v) + Ψ(x, v)∇GM (x, v)}/GM (x, v).

The second, third, and fourth terms on the right hand side and ∇Ψ(x, v) can also be
estimated using the approaches above. Rearranging the above equation leads to
        Z
(12)                  ∇M (x, u)dµ(u|v)/GM (x, v)
          u∈{u:IM (x)=1}
(13) = ∇Ψ(x, v) − {H(x)∇GH (x, v) + L(x)∇GL (x, v) + Ψ(x, v)∇GM (x, v)}/GM (x, v).

Taking v as known, the functions ∇Ψ(x, v), Ψ(x, v), H(x), L(x), ∇GH (x, v), ∇GL (x, v), and
GM (x, v) can be estimated using the parametric or nonparametric approaches discussed
above subject to similar regularity conditions, with x in the previous sections redefined
as (x, v). Multiplying the right hand side (13) by dµV (v|x) (which we can estimate) and
integrating over v yields the parameter of interest β(x) in (11).
    Our treatment of endogeneity is closely related to a number of estimation procedures
in the literature in which a residual is introduced as a control variable in the second step,
particularly Smith and Blundell (1986) and Rivers and Vuong (1988) in the context of the
Tobit and probit models. Because of nonseparability between X and U , one must use
(11) to “undo” the effects of conditioning on V when estimating the response of X to Y
on the uncensored sample. Blundell and Powell (2004) and Altonji and Matzkin (2001)
use a similar idea in settings that differ from ours. Imbens and Newey (2002,2007) and

                                                     19
Chesher (2003) consider the case in which X = g(Z, V ) and g is monotone in the scalar
unobservable V and M takes the form M (X, U ), where U = {U1 , V } and M is monotone
in scalar U1 . See also Matzkin (2003). Following their approach, one can recover V from
the cumulative distribution function of X given Z and proceed as outlined above if Z and
(V, U ) are independent.11 We suspect that the specification of M (X, U ) and estimation
method used in Florens et al (2008) could also be used here as well.
    A number of papers in the literature discuss estimation in nonseparable models with
endogenous variables when a control variable Z that is excluded from X is observed directly
and has the property dµ(u|X = x, Z = z) = dµ(u|Z = z). If one has such a variable, then
one can estimate β(x) using the estimator defined above by replacing v, V , and dµV (v|x)
with z, Z, and the conditional density dµZ (z|x) of Z given X = x. The problem with this
strategy, of course, is that it is hard to think of applications in which an appropriate Z
variable is directly available.

6.4       Endogenous Regressors in a Panel
When panel data are available, there are other possibilities. Suppose that one has panel
data observations Yit , Xit , and IM it , where i is a group indicator and t is a time indicator
(t = 1, . . . , T ). Assume that Xit and Uit are independent given Zi . In this case one can
show that
                         Z
               β(x) =      ∇Ψ(x, z) − {H(x)∇GH (x, z) + L(x)∇GL (x, z)
                                z
                                       +Ψ(x, z)∇GM (x, z)/GM (x, z)}dµz (z|xit = x)dz.

We can estimate β(x) by substituting suitable parametric or nonparametric estimators for
the functions on the right hand side of this equation.12 Following Altonji and Matzkin (2001,
2005), if one is willing to assume that the conditional distribution of Uit is exchangeable in
(Xi1 , Xi2 , . . . XiT ), then symmetric functions (Xi1 , Xi2 , . . . XiT ), such as the group mean of
Xit for each i, might be a suitable choice for Zi .13
  11
     These papers and others discussed by Blundell and Powell (2003), Matzkin (2007), and Chesher (2007)
focus on estimation of M (x, U ) and ∇M (x, U ) at various quantiles of U as well as dµ(U ). Identifying the
structural function dµ(U |X = x) is much more demanding than identifying an average derivative such as
β(x) so it is not surprising that stronger assumptions are required. Note that β(x) is what Altonji and
Matzkin (2005) call a local average response. It is the average partial effect of an exogenous change in x
evaluated using the actual conditional distribution of U given X = x and IM (X) = 1. It corresponds to
how the population of agents with X = x and IM (x) = 1 would respond to an exogenous change in x.
Blundell and Powell (2004) focus
                             R     on what Woodridge (2007) calls the average partial effect. In our context
the average partial effect is u∈{u:I (x)=1} ∇M (x, u)dµ(u)/GM (x).
                                    M
  12
     In some applications this assumption may not be appropriate. Following along the lines of Altonji and
Matzkin (2001), one could proceed as follows. Write Xit = ϕ(Wit , Zi ) + Vi , where ϕ(Wit , Zi ) is defined so
that E[V |W = w, Zi = zi ] = 0 a.s. Assume U ⊥ W, Zi |V . Then one can construct an estimator based on
                        Z
            β(x) =          ∇Ψ(x, z, v) − {H(x)∇GH (x, z, v) + L(x)∇GL (x, z, v)
                           zv
                                       +Ψ(x, z)∇GM (x, z, v)/GM (x, z, v)}dµz,v (z, v|xit = x)dzdv.

  13
       In the case T = 2, the condition is


                                                      20
    The panel data version of our estimator complements Honoré’s (1992) trimmed LAD
estimator, which permits one to estimate θ in censored and truncated regression models
when M (Xit , Uit ) = Xit θ +Uit . His estimator is based on differencing the panel observations
in clever ways and is quite distinct from our approach.


7     A Monte Carlo Investigation
In this section we compare the performance of nonparametric and semiparametric versions
of our average derivative estimator to maximum likelihood Tobit. In Table 1, we report the
results of a series of Monte Carlo experiments based on the model

                         Model 1: M (X, U ) = α0 + α1 X + α2 XU + U,
                                                 Y    = max{0, M (X, U )},

where U has a normal distribution with mean 0 and variance 1 (written N (0, 1)) and X
has a uniform distribution between 0 and 4 (written U (0, 4)). The column headings report
the values of X at which β(x) is evaluated. The column labelled “Avg. β” reports results
for β̄, the average value of β(X) over the distribution of X for the uncensored observations.
The rows labeled “True Value” reports the true value of β̄ and the true values of β(x)
when x is 0, .4, .8, 1.2, 2, 2.8, 3.2, 3.6 and 4. The rows labelled “AIO-SP” report the
results for a semiparametric version, the rows labelled “AIO-NP” report the results for
a nonparametric version, and the rows labelled “Tobit” report the results for the Tobit
maximum likelihood estimator. For Model 1 as well as Models 2 and 3 below, in the
semiparametric case we specify Ψ(x; θ1 ) to be a fourth order polynomial in x plus a constant
term and estimate θ1 by OLS. We do not impose the restriction that the estimated values
of Ψ(x; θ1 ) is greater than 0 for all x. For the conditional probability GM (x; θ2 ), we specify
GM (x; θ2 ) = Φ(P (x; θ2 )) where Φ(·) is the standard normal CDF and P (x; θ2 ) is a fourth
order polynomial in x plus a constant and estimate θ2 by the maximum likelihood. In
the nonparametric   case we estimate both Ψ and GM by local linear regression with the
                     X−x
kernel weight K hn = I{−.5 ≤ X − x ≤ .5} (i.e., the uniform density kernel with the
bandwidth hn = 1). The kernel is symmetric around x if it is away from the boundary. When
x is 0, .4, 3.6, or 4, we extend the kernel in the direction away from the boundary to keep
the width of the window at 1.14 The Tobit estimation is conducted under the assumption

                       dµit (uit |Xi1 = xi1 , Xi2 = xi2 ) = dµit (uit |Xi1 = xi2 , Xi2 = xi1 ).
  Altonji and Matzkin note that under the exchangeability, dµit (uit |Xi1 = xi1 , Xi2 = xi2 ) may be written
as dµit (uit |zi ) where zi = Z(xi1 , xi2 ) is a vector of known symmetric functions of xi1 and xi2 . In the case in
which T = 2 and xit is a scalar, any continuous symmetric function can be approximated arbitrarily closely
by a function of the first 2 elementary symmetric functions zi1 = (xi1 +xi2 ) and zi2 = xi1 xi2 . The idea extends
to higher values of T using the first T elementary symmetric functions. However, exchangeability alone does
not restrict the z functions sufficiently to permit one to identify ∇Ψ(x, z), ∇GH (x, z), ∇GL (x, z), Ψ(x, z),
∇GM (x, z) and GM (x, z) nonparametrically. Consequently, some restrictions on these functions (e.g. linear
index restrictions) would be needed.
  14
     We also performed simulations for the cases in Tables 1 and 2 using an Epanechnikov (or quadratic)
kernel with an automatic choice for the bandwidth. Our bandwidth choice rule is: (i) compute the rule of
thumb bandwidth bm of Fan and Gijbels (1996, pp. 110-113) to estimate the conditional mean function,


                                                         21
that the analyst does not know the functional form of M (X, U ) and approximates it with
a fourth order polynomial with an additively separable normal error term.
    The rows labelled with “sd” report the standard deviations of the estimators across Monte
Carlo replications. For the semiparametric version of our estimator, the rows labelled “se”
report the mean of the asymptotic standard error estimates, and the rows labelled “90%” are
the coverages rates of the 90% confidence interval estimates. The sample size is 2,000 and
each row of the table is based on 4,000 Monte Carlo replications.15 For the semiparametric
version of our estimator, we compute asymptotic standard error estimates by applying the
delta method with the Huber-White heteroskedasticity consistent variance estimators for
the OLS and probit maximum likelihood estimators. For the nonparametric version of our
estimator and the Tobit maximum likelihood, we do not report estimated standard errors
or coverage rates.
    The results are quite striking. For all cases AIO-SP is less biased than Tobit to estimate
β̄ and β(x). Consider, for example, the first panel of Table 1, where M (X, U ) = 1.0 −
0.5X + XU + U . For this specification β(x) ranges from -0.212 when x is 0 to 0.429 when
x is 4. The Monte Carlo mean of Ave. β by AIO-SP is 0.229, while the true value is
0.210. For β(x), the Monte Carlo means of AIO-SP are -0.029 for β(.4) = −0.027, 0.056
for β(.8) = 0.098, 0.335 for β(2) = 0.298, 0.325 for β(2.8) = .366, 0.336 for β(3.2) =
.391, and 0.448 for β(3.6) = 0.412. Note, however, that at the boundaries, the Monte
Carlo means of AIO-SP are 0.002 for β(0) = −0.212 and 0.607 for β(4) = 0.429. The
discrepancies at 0 and 4 illustrate the fact that for most specifications we tried there is
substantial bias near the boundaries of the support of X.16 The standard deviations are
also large near the boundaries of the support of X in almost all of the experiments. Based on
our preliminary simulation study (not reported here), this reflects the large sampling errors
in ∇Ψ(x; θ̂1 ) and ∇GM (x; θ̂2 ) near the boundaries of the support of X, and these sampling
errors are magnified in Ψ(x; θ̂1 )/GM (x; θ̂2 ). The relative importance of these two sources of
the sampling error varies to some extent with the design. Overall, however, AIO-SP does a
good job of fitting β̄ and tracking β(x), particularly between x = 0.4 and x = 3.6.
    The results for AIO-NP are also encouraging. In many instances it is even closer to β(x)
than AIO-SP in terms of the Monte Carlo means. Interestingly, in all but two instances,
the nonparametric version has a smaller sampling variance at the boundaries x = 0 and 4.
and then (ii) compute the adjusted bandwidth as bm0 = bm (n1/5 /n1/7 ). We use this adjustment because the
dominant components that drive the asymptotics of the estimator β̂(x) are the estimators for the derivatives
D̂(x) (see, Lemma 4.1), and because the optimal bandwidths to estimate the conditional mean function and
its first-order derivative take the form of cm n−1/5 and cm0 n−1/7 , respectively. The simulation results are in
Table 4. Overall, the performance is similar to that of the uniform kernel particularly in the range between
x = .4 and x = 3.6.
   15
    √One can obtain standard errors of the Monte Carlo means by dividing the reported standard deviations
by 4000 or 63.2.
   16
      We obtained similar results for n = 500, so small sample bias does not appear to be the main problem.
The bias appears to be a consequence of minor misspecification of Ψ(x; θ1 ) and GM (x; θ2 ) for the behavior
of the estimator of OΨ(x; θ1 ) and OGM (x; θ2 ) near the boundaries of the support of X. To isolate the role of
functional form, we performed the following experiment. For one set of parameter values, we computed the
true values of Ψ(x) and GM (x) implied by Model 1. We then estimated Ψ(x) by regressing the uncensored
values of Y on a third order polynomial in the true Ψ(x). We estimated GM (x) using a probit model with
the probit index specified to be a cubic function in the true value of Φ−1 (GM (x)), where Φ−1 (·) is the inverse
of the standard normal CDF. The estimator was essentially unbiased for values of x between 0.01 and 4.



                                                       22
This superior performance near the boundaries may reflect the effects of heteroskedasticity
on the efficiency of OLS in the semiparametric case. (Ignoring the effects of censoring, the
error variance rises with the square of x when α2 6= 0.)17 In contrast, Tobit is severely
biased to estimate β̄ and β(x) and is also very noisy near the boundary values of X.
    The results in Panel 3, where M (X, U ) = −1 + XU + U , are also quite interesting. In
this case, the Monte Carlo mean of Ave. β of Tobit is 0.918 which is reasonably close to the
true value of β̄ = 1.046. However, the Monte Carlo mean of Ave. β of AIO-SP is 1.034 and
is better than Tobit. Both AIO-SP and AIO-NP do a good job of tracking the variation
in β(x) in this experiment at least between x = .4 and x = 3.6, while the Tobit does very
poorly.
    In Panel 6 we report the results for an experiment in which α2 = 0. Note that β̄ =
β(x) = α1 in this case and Tobit is the maximum likelihood estimator for the problem. All
estimators are essentially unbiased, and perhaps surprisingly, AIO-SP is almost as efficient
as Tobit. However, we find that if one uses Tobit with α0 + α1 X, the true form, imposed
as the Tobit index, then Tobit sd of the estimates of α1 is about half of that of AIO-SP for
β̄ and is between 1/9th and 2/5ths as large for β(x) between x = 0.4 and x = 3.6.
    What about inference? The asymptotic standard error estimates of AIO-SP closely track
the standard deviations of the estimators and the coverage rates are close to 0.9 in all cases,
even at the boundary values. Both se and sd of AIO-SP depend on V ar(α2 XU + U ) and
on how far the value of x is from the boundaries of the support. They tend to be negatively
related to the number of uncensored values in the neighborhood of x, although we do not
provide enough information to infer this from the tables. The Monte Carlo simulations
indicate that the standard errors based on the delta method perform well.
    In Table 2 we report the Monte Carlo results for AIO-SP, AIO-NP, and Tobit under
Model 2, which is:

                       Model 2: M (X, U ) = α0 + α1 X + α2 XU1 + U2 ,
                                             Y    = max{0, M (X, U )},

where U1 follows N (0, 1), U2 follows N (0, 1), and X follows U (0, 4). Since U2 does not
interact with X, Model 2 is closer to a Tobit model than Model 1. Consequently, one would
expect that the presence of U2 would lead to improvement in the Tobit estimator from Table
1 relative to AIO-SP and AIO-NP for the same parameter values. The results for Panels
1-4 in Tables 1 and 2 support this conjecture. Although the results are not reported here,
increasing the variance of U2 reduces the amount of bias in Tobit. However, in a number of
cases Tobit is still substantially biased. Our AIO-SP and AIO-NP are essentially unbiased
for β̄ and also tracks the true values of β(x) reasonably closely when x is between 0.4 and
3.6 in all cases.
  17
    It also may reflect the possibility that for the window width we have chosen, the functional forms implicit
in the local linear regression estimators are more restrictive than regression and probit with global fourth
order polynomials.




                                                      23
    The third model that we examine is of the form

                   Model 3 :      M (X, U ) = α0 + α1 X + α2 XU + U,
                                              
                                                 M (X, U ) if M (X, U ) > L(X)
                                         Y =                                   ,
                                                 CL otherwise
                                      L(X) = a0 + a1 X,

where U follows N (0, 1) and X follows U [0, 4]. The specification of M (X, U ) is the same
as Model 1, but the lower bound for Y is L(X) rather than 0. We did not impose the
restriction that the sample estimate of Ψ(x; θ1 ) be greater than L(x) for all values of x. We
performed the simulations under the assumption that the econometrician knows the form
of L(x) up to the parameter values a1 and a2 . We used quantile regression with the third
centile to estimate a0 and a1 as discussed in Section 5.18 We did not experiment much with
whether choosing a lower or higher quantile improves the performance of the estimator,
although in a few experiments not reported here we found that in large samples choosing a
very low quantile, (say τ n = .01) reduces the bias in the estimates of a0 and a1 but did not
alter the estimates of β(x) by very much. The rows labeled “Tobit” report the results for
the maximum likelihood estimator of the censored regression model under the assumption
that M (X, U ) = α0 + α1 X + U . This estimator requires Y > â1 + â2 X for all observations
in which Y = M (X, U ).
    The results are in Table 3. In Panel 1 we consider the case in which M (X, U ) = X + U
and L(X) = .5X. For this specification, β(x) = 1 for all x. The Monte Carlo mean of
AIO-SP is very close to 1 for all values of x, and the coverage rates are close to 0.9. The
Monte Carlo mean of â0 and â1 are 0.001 and 0.565, respectively. Thus there is a small
positive bias in the estimation of L(x), which is not surprising given our use of the third
centile. In Panel 1 the censored regression model is correctly specified and the censored
Tobit is the maximum likelihood estimator for the problem. Not surprisingly, it does very
well.
    In the remaining panels we consider several specifications in which α2 differs from 0,
so β(x) varies and the censored regression model is misspecified. In all of the cases, the
Monte Carlo mean of AIO-SP tracks β(x) closely between x = .4 and x = 3.6. For all of
the specifications in Table 3, se tracks sd well and coverages of 90% confidence intervals are
close to 0.9. The censored regression model is biased for β̄ and fails to track β(x).
    We repeated the experiments in Tables 1 and 2 for sample sizes of 500 (not reported).
The behaviors of AIO-SP for β̄ and β(x) are quite similar. Although AIO-SP se and sd
typically double, coverage rates remain close to 0.9. It is likely, however, that in small
samples the mean squared error can be improved if one is more parsimonious in specifying
Ψ(x; θ1 ) and P (x; θ2 ) than we have been.
    Overall, the Monte Carlo results are very encouraging.
  18
     We did not bother to estimate standard errors for â0 and â1 . Recall that under our assumptions the
asymptotic distribution of β̂(x) is not influenced by sampling errors in â0 and â1 . The asymptotic standard
error estimates of β̂(x) are calculated by the delta method, as in Models 1 and 2.




                                                     24
8    Conclusions
We provide an estimator for partial derivatives in nonseparable limited dependent variables
models, with and without endogenous variables. We place almost no restrictions on the
function that determines the dependent variable. The basic idea is to first estimate the
derivatives of the regression function relating the dependent variable to the explanatory
variables on the uncensored sample, and then correct for the effects of sample selection. The
correction term for the derivative has a simple structure and can be estimated quite easily.
For example, if the censoring point is known and one chooses to use flexible parametric forms,
the estimator can be computed using a regression program and a probit or logit program.
If the censoring points are not known, then in addition one requires a quantile regression
program such as qreg in STATA. The nonparametric version can be implemented using a
local polynomial regression routine and a quantile regression that allows weights, such as
the lpoly and qreg packages in STATA. We provide encouraging Monte Carlo evidence for
cases in which the outcome is censored at 0 and cases in which the censoring point is an
unknown linear function of an explanatory variable. The estimator has been successfully
applied in a few empirical studies.
    When the censoring point is known the estimator is robust to random misclassification
of some uncensored observations as censored and to measurement error in the dependent
variable, provided that the mean of the report conditional on the explanatory variables is
unbiased. A version of the estimator that can be used in the case of endogenous explanatory
variables in some circumstances, such as when an exogenous determinant of X is available
or in panel data that satisfy the exchangeability conditions used in Altonji and Matzkin
(2001, 2005) is available.
    In future research, it would be valuable to examine whether the estimator can be ex-
tended to the case of stochastic censoring functions L(x; η) and H(x; η), where η is a random
vector, which would cover a very broad class of models.




                                             25
References
 [1] Ahmad, I. A. (1976) “On asymptotic properties of an estimate of a functional of a
     probability density,” Scandinavian Actuarial Journal, 3, 176-181.

 [2] Ai, C. (1997) “A semiparametric maximum likelihood estimator,” Econometrica, 65,
     933-963.

 [3] Altonji, J. G., Hayashi, F. and L. J. Kotlikoff (1997) “Parental altruism and inter vivos
     transfers: theory and evidence,” Journal of Political Economy, 105, 1121-1166.

 [4] Altonji, J. G. and R. L. Matzkin (2001) “Panel data estimators for nonseparable models
     with endogenous regressors,” Working paper.

 [5] Altonji, J. G. and R. L. Matzkin (2005) “Cross section and panel data estimators for
     nonseparable models with endogenous regressors,” Econometrica, 73, 1053-1102.

 [6] Barro, R. J. (1974) “Are government bonds net wealth?,” Journal of Political Economy,
     82, 1095-1117.

 [7] Becker, G. S. (1974) “A theory of social interactions,” Journal of Political Economy,
     82, 1063-1093.

 [8] Blundell, R. and J. L. Powell (2003) “Endogeneity in nonparametric and semiparametric
     regression models,” in Dewatripont, M., Hansen, L. P. and S. J. Turnovsky (eds.)
     Advances in Economics and Econometrics: Theory and Applications: Eighth World
     Congress, vol. II, Cambridge University Press.

 [9] Blundell, R. and J. L. Powell (2004) “Endogeneity in semiparametric binary response
     models,” Review of Economic Studies, 71, 655-679.

[10] Box, G. E. P. and D. R. Cox (1964 ) “An analysis of transformations,” Journal of the
     Royal Statistical Society, B, 211-264.

[11] Chaudhuri, P. (1991a) “Nonparametric estimates of regression quantiles and their local
     Bahadur representation,” Annals of Statistics, 19, 760-777.

[12] Chaudhuri, P. (1991b) “Global nonparametric estimation of conditional quantile func-
     tions and their derivatives,” Journal of Multivariate Analysis, 39, 246-269.

[13] Chen, S., Dahl, G. B. and S. Khan (2005) “Nonparametric identification and estima-
     tion of a censored location-scale regression model,” Journal of the American Statistical
     Association, 100, 212-221.

[14] Chen, S. and S. Khan (2001) Semiparametric estimation of a partially linear censored
     regression model, Econometric Theory, 17, 567-590.

[15] Chernozhukov, V. (1998) “Nonparametric extreme regression quantiles,” Working pa-
     per.



                                             26
[16] Chernozhukov, V. (2005) “Extremal quantile regression,” Annals of Statistics, 33, 806-
     839.

[17] Chesher, A. (2003) “Local identification in nonseparable models,” Econometrica, 71,
     1405-1441.

[18] Fan, J. and I. Gijbels (1996) “Local Polynomial Modelling and Its Applications,” Chap-
     man & Hall/CRC.

[19] Hall, P. and J. S. Marron (1987) “Estimation of integrated squared density derivatives,”
     Statistics & Probability Letters, 6, 109-115.

[20] Han, A. K. (1987) “Non-parametric analysis of a generalized regression model: the
     maximum rank correlation estimator,” Journal of Econometrics, 35, 303-316.

[21] Härdle, W. and T. M. Stoker (1989) “Investigating smooth multiple regression by the
     method of average derivatives,” Journal of the American Statistical Association, 84,
     986-995.

[22] Heckman, J. J. (1976) “The common structure of statistical models of truncation, sam-
     ple selection and limited dependent variables and a simple estimator for such models,”
     Annals of Economic and Social Measurement, 5, 475-492.

[23] Heckman, J. J., Smith, J, and Clements, N. (1997) “Making the Most Out of Programme
     Evaluations and Social Experiments: Accounting for Heterogeneity in Programme Im-
     pacts,” Review of Economic Studies, 64, 487-535.

[24] Honoré, B. E. (1992) “Trimmed LAD and least squares estimation of truncated and
     censored regression models with fixed effects,” Econometrica, 60, 533-565.

[25] Horowitz, J. L. (1998) “Semiparametric Methods in Econometrics,” Springer.

[26] Imbens, G. W., and W. K. Newey (2002): \Identication and Estimation of Triangular
     Simultaneous Equations Models Without Additivity," Technical Working Paper 285,
     NationalBureau of Economic Researh.

[27] Ichimura, H. and L. Lee (1991) “Semiparametric least squares estimation of multi-
     ple index models: single equation estimation,” in Barnett, W. A., Powell, J. L. and
     G. Tauchen (eds.), Nonparametric and Semiparametric Methods in Econometrics and
     Statistics, Cambridge University Press.

[28] Ichimura, H., Otsu, T. and J. G. Altonji (2008) “Nonparametric intermediate order
     regression quantiles,” Working paper.

[29] Kazianga, H. (2006) “Motives for household private transfers in Burkina Faso,” Journal
     of Development Economics, 79, 73-117.

[30] Knight, K. (2001) “Limiting distributions of linear programming estimators,” Extremes,
     4, 87-103.

[31] Koenker, R. and G. Bassett (1978) “Regression quantiles,” Econometrica, 46, 33-50.

                                             27
[32] Lewbel, A. and O. Linton (2002) “Nonparametric censored and truncated regression,”
     Econometrica, 70, 765-779.

[33] Li, Q., Lu, X. and A. Ullah (2003) “Multivariate local polynomial regression for esti-
     mating average derivatives,” Journal of Nonparametric Statistics, 15, 607-624.

[34] Masry, E. (1996a) “Multivariate regression estimation: local polynomial fitting for time
     series,” Stochastic Processes and Their Applications, 65, 81-101.

[35] Masry, E. (1996b) “Multivariate local polynomial regression for time series: uniform
     strong consistency and rates,” Journal of Time Series Analysis, 17, 571-599.

[36] Matzkin, R. L. (1991) “A nonparametric maximum rank correlation estimator,” in Bar-
     nett, W. A., Powell, J. L. and G. Tauchen (eds.), Nonparametric and Semiparametric
     Methods in Econometrics and Statistics, Cambridge University Press.

[37] Pagan, A. and A. Ullah (1999) “Nonparametric Econometrics,” Cambridge University
     Press.

[38] Powell, J. L. (1991) “Estimation of monotonic regression models under quantile restric-
     tions,” in Barnett, W. A., Powell, J. L. and G. Tauchen (eds.), Nonparametric and
     Semiparametric Methods in Econometrics and Statistics, Cambridge University Press.

[39] Powell, J. L. (1994) “Estimation of semiparametric models,” in Engle, R. F. and D. L.
     McFadden (eds.), Handbook of Econometrics, vol. IV, 2443-2521, Elsevier, Amsterdam.

[40] Powell, J. L., Stock, J. H. and T. M. Stoker (1989) “Semiparametric estimation of index
     coefficients,” Econometrica, 57, 1403-1430.

[41] Newey, W. K. and D. L. McFadden (1994) “Large sample estimation and hypothesis
     testing,” in Engle, R. F. and D. L. McFadden, eds., Handbook of Econometrics, Vol.
     IV, ch. 36, Elsevier, Amsterdam.

[42] Rauta, L. K. and L. H. Tran (2005) “Parental human capital investment and old-age
     transfers from children: Is it a loan contract?” Journal of Development Economics, 77,
     2, 389-414.

[43] Rivers, D. and Q. Vuong (1988) “Limited information estimators and exogeneity tests
     for simultaneous probit models,” Journal of Econometrics, 39, 347-366.

[44] Smith, R. L. (1994) “Nonregular regression,” Biometrika, 81, 173-183.

[45] Smith, R. J. and R. W. Blundell (1986) “An exogeneity test for a simultaneous equation
     Tobit model with an application to labor supply,” Econometrica, 54, 679-686.

[46] Stoker, T. M. (1986) “Consistent estimation of scaled coefficients,” Econometrica, 54,
     1461-1481.

[47] Stone, C. J. (1982) “Optimal rates of convergence for non-parametric regression,” An-
     nals of Statistics, 10,1040-1053.


                                             28
[48] Villanueva E. (2002) “Parental altruism under imperfect information: theory and evi-
     dence,” Working paper.




                                           29
A     Appendix
A.1    Proof of Lemma 3.1 and Theorem 3.1
Lemma 3.1 directly implies Theorem 3.1. We prove Lemma 3.1. Clearly it is sufficient to
prove Lemma 3.1 for ∇1 , the partial derivative with respect to the first element of x, i.e.,
(14) Z                       Z
∇1     M (x, u)IM (x)dµ(u) =     ∇1 M (x, u)IM (x)dµ(u) − H(x)∇1 GH (x) − L(x)∇1 GL (x).

The left hand side of (14) is written as
                  Z                                     Z                   
              lim     M (x + εe1 , u)IM (x + εe1 )dµ(u) − M (x, u)IM (x)dµ(u) /ε
             ε→0
                  Z
          = lim [M (x + εe1 , u) − M (x, u)] IM (x + εe1 )dµ(u)/ε
             ε→0
                    Z
             + lim M (x, u) [IM (x + εe1 ) − IM (x)] dµ(u)/ε
                 ε→0
          = T1 + T2 ,
where e1 = (1, 0, . . . , 0). Assumption 3.1 (ii), (iv), and (v) imply limε→0 IM (x + εe1 ) =
IM (x) a.s. Thus,R Assumption 3.1 (iv) and the Lebesgue dominated convergence theorem
imply that T1 = ∇1 M (x, u)IM (x)dµ(u). We now consider T2 . From the definition of IM
and Assumption 3.1 (ii),
              IM (x + εe1 ) − IM (x)
          = [I{L(x + εe1 ) < M (x + εe1 , U )} + I{M (x + εe1 , U ) < H(x + εe1 )}]
              − [I{L(x) < M (x, U )} + I{M (x, U ) < H(x)}] ,
a.s. for all ε sufficiently close to zero. So, T2 can be written as
               Z
T2 = lim M (x, u) [I{L(x + εe1 ) < M (x + εe1 , u)} − I{L(x) < M (x, u)}] dµ(u)/ε
          ε→0
                 Z
          + lim M (x, u) [I{M (x + εe1 , u) < H(x + εe1 )} − I{M (x, u) < H(x)}] dµ(u)/ε.
           ε→0

Since I{L(x + εe1 ) < M (x + εe1 , u)} = 1 − I{M (x + εe1 , u) ≤ L(x + εe1 )} for all ε
sufficiently close to zero, the following lemma completes the proof.
Lemma A.1 Under Assumption 3.1,
        Z
(15) lim M (x, u) [I{M (x + εe1 , u) < H(x + εe1 )} − I{M (x, u) < H(x)}] dµ(u)/ε
        ε→0
        = −H(x)∇1 GH (x).
    It is sufficient to show that both an upper bound and a lower bound of the left hand
side of (15) converge to the right hand side as ε → 0. The left hand side of (15) equals
             Z
         lim M (x, u)I{M (x + εe1 , u) < H(x + εe1 )}I{M (x, u) ≥ H(x)}dµ(u)/ε
         ε→0
             Z
      − lim M (x, u)I{M (x + εe1 , u) ≥ H(x + εe1 )}I{M (x, u) < H(x)}dµ(u)/ε.
       ε→0


                                             30
Since the argument is analogous, we only show the result for an upper bound. To do so,
note that if M (x + εe1 , u) < H(x + εe1 ), then Assumption 3.1 (iv) implies M (x, u) <
H(x + εe1 ) + εB(u) for all ε sufficiently close to zero, where B(u) is defined in Assumption
3.1 (iv). Similarly, if M (x + εe1 , u) ≥ H(x + εe1 ), then M (x, u) ≥ H(x + εe1 ) − εB(u) for
all ε sufficiently close to zero. Hence the left hand side of (15) can be bounded from above
by
              Z
         lim H(x + εe1 )I{M (x + εe1 , u) < H(x + εe1 )}I{M (x, u) ≥ H(x)}dµ(u)/ε
         ε→0
                Z
         + lim B(u)I{M (x + εe1 , u) < H(x + εe1 )}I{M (x, u) ≥ H(x)}dµ(u)
            ε→0
                Z
         − lim H(x + εe1 )I{M (x + εe1 , u) ≥ H(x + εe1 )}I{M (x, u) < H(x)}dµ(u)/ε
            ε→0
                Z
         + lim B(u)I{M (x + εe1 , u) ≥ H(x + εe1 )}I{M (x, u) < H(x)}dµ(u).
          ε→0

By Assumption 3.1 (ii), (iv), and (v), the Lebesgue dominated convergence theorem implies
that the second term and the fourth term converge to zero. The first term and the third
term can be rewritten as
                   Z
    lim H(x + εe1 ) [I{M (x + εe1 , u) < H(x + εe1 )} − I{M (x, u) < H(x)}] dµ(u)/ε
    ε→0

which equals the right hand side of (15) under Assumption 3.1 (ii) and (iii). Therefore, the
conclusion is obtained.

A.2    Proof of Theorem 4.2
Observe that
                    √
                       n(β̃ − β)
                           n
                      1 X
                =    √       {Ii β(Xi ) − β}
                       n
                         i=1
                            n
                      1    X       n                 o
                    +√           Ii ∇Ψ(X
                                     d i ) − ∇Ψ(Xi )
                       n
                           i=1
                            n
                                  (                                         )
                      1    X       Ψ̂(Xi ) \              Ψ(Xi )
                    +√       Ii              ∇GM (Xi ) −          ∇GM (Xi )
                       n          Ĝ M (Xi )             GM (Xi )
                         i=1
                          n
                                (                                           )
                      1  X         H̃(Xi ) \              H(Xi )
                    +√       Ii              ∇GH (Xi ) −          ∇GH (Xi )
                       n          ĜM (Xi )              GM (Xi )
                         i=1
                          n
                                (                                          )
                      1 X          L̃(Xi ) [              L(Xi )
                    +√       Ii              ∇GL (Xi ) −          ∇GL (Xi )
                       n          ĜM (Xi )              GM (Xi )
                           i=1
                = T0 + T1 + T2 + T3 + T4 .



                                               31
We analyze the j-th component of Tm for each j = 1, . . . , k and m = 1, . . . , 4. For T1 , an
application of Li, Lu and Ullah (2003, Theorem 2.1) implies that

                                                         IM (Xi ) M−1 Q(Xi ) j+1,1
                  n
                                                                              
              1 X
     T1,j = √        Ii (Yi − E [Yi |IM (Xi ) = 1, Xi ])                           + op (1).
               n                                              GM (Xi )f (Xi )
                      i=1

From Masry (1996b, Theorem 6) and Assumption 4.3 (iii), we have
                                                      
                    sup Ψ̂(x) − Ψ(x) ĜM (x) − GM (x)        = op (n−1/2 ),
                                 x∈X̄
                                                                 
                      sup        Ψ̂(x) − Ψ(x) ∇G\M,j (x) − ∇GM,j (x)                = op (n−1/2 ),
                      x∈X̄
                                                               
               sup        ĜM (x) − GM (x) ∇G\M,j (x) − ∇GM,j (x)                   = op (n−1/2 ),
               x∈X̄
                                                                                2
(16)                                                    sup ĜM (x) − GM (x)        = op (n−1/2 ).
                                                        x∈X̄

Thus, for T2,j , adapted versions of Li, Lu and Ullah (2003, Theorem 2.1) yield
                                  n
                           1 X Ψ(Xi )  \                           
        T2,j     =        √   Ii          ∇GM,j (Xi ) − ∇GM,j (Xi )
                            n    GM (Xi )
                                 i=1
                                    n
                            1      X          ∇GM,j (Xi )                  
                          +√             Ii                 Ψ̂(Xi ) − Ψ(Xi )
                             n                 GM (Xi )
                                   i=1
                          n
                      1 X Ψ(Xi )∇GM,j (Xi )                                 
                   −√        Ii                      Ĝ M (X i ) − G M (Xi )   + op (1)
                       n            GM (Xi )2
                         i=1
                                                            Ψ(Xi ) M−1 Q(Xi ) j+1,1
                        n
                                                                                  
                    1 X
                 = √       Ii (IM (Xi ) − E [IM (Xi )|Xi ])
                     n                                             GM (Xi )f (Xi )
                                 i=1
                                    n
                            1      X                                                    ∇GM,j (Xi )
                          +√             Ii (Yi − E [Yi |IM (Xi ) = 1, Xi ]) IM (Xi )
                             n                                                           GM (Xi )2
                                   i=1
                               n
                            1 X                                   Ψ(Xi )∇GM,j (Xi )
                          −√     Ii (IM (Xi ) − E [IM (Xi )|Xi ])                   + op (1).
                             n                                       GM (Xi )2
                                   i=1

>From Masry (1996b, Theorem 6) and Assumption 4.3 (iii), we have
                                                      
           sup ĜM (x) − GM (x) ∇G \  H,j (x) − ∇GH,j (x)   = op (n−1/2 ).
                 x∈X̄




                                                               32
Thus, from Assumption 4.3 (iv), (16), and adapted versions of Li, Lu and Ullah (2003,
Theorem 2.1), T3,j satisfies
                          n
                      1 X H(Xi )  \                           
          T3,j   =   √   Ii          ∇GH,j (Xi ) − ∇GH,j (Xi )
                       n    GM (Xi )
                         i=1
                            n
                       1   X     H(Xi )∇GH,j (Xi )                           
                     −√             Ii                 Ĝ M (Xi ) − G M (Xi )   + op (1)
                        n            GM (Xi )2
                          i=1
                                                             H(Xi ) M−1 Q(Xi ) j+1,1
                         n
                                                                                   
                      1 X
                 =   √      Ii (IH (Xi ) − E [IH (Xi )|Xi ])
                       n                                           GM (Xi )f (Xi )
                         i=1
                            n
                       1   X                                           H(Xi )∇GH,j (Xi )
                     −√             Ii (IM (Xi ) − E [IM (Xi )|Xi ])                     + op (1).
                        n                                                 GM (Xi )2
                            i=1

Similarly, we have
                                                            L(Xi ) M−1 Q(Xi ) j+1,1
                         n
                                                                               
                      1 X
          T4,j   =   √     Ii (IL (Xi ) − E [IL (Xi )|Xi ])
                       n                                         GM (Xi )f (Xi )
                          i=1
                             n
                       1    X                                          L(Xi )∇GL,j (Xi )
                     −√             Ii (IM (Xi ) − E [IM (Xi )|Xi ])                     + op (1).
                        n                                                 GM (Xi )2
                              i=1

Combining these results, the conclusion is obtained.

A.3       Proof of Lemma 5.1
Denote the marginal distribution functions of U1 and U2 by F1 and F2 , respectively. Suppose
F1 and F2 have zero mean, strictly increasing, and have smooth marginal densities f1 and
f2 , respectively. For these distributions we construct a function
                              M (x, u) = M0 (x) + M1 (x)u1 + M2 (x)u2
so that
                     GL (x; θ) = Pr {M (X, U ) ≤ L(X; θ)|X = x} ,
                     GH (x; θ) = Pr {M (X, U ) ≥ H(X; θ)|X = x} ,                   and
                      Ψ(x; θ) = E [M (X, U )|IM (X) = 1, X = x] .
For notational convenience from now on we drop the arguments x and θ from functions.
First, note that
                               Z ∞                            
                                                L − M0 − M1 u1
(17)                   GL =        f1 (u1 )F2                    du1
                                −∞                   M2
                               Z ∞                            
                                                L − M0 − M2 u2
(18)                        =      f2 (u2 )F1                    du2 ,
                                −∞                   M1
                               Z ∞                            
                                                H − M0 − M1 u1
(19)              1 − GH =         f1 (u1 )F2                    du1
                                −∞                   M2
                               Z ∞                            
                                                H − M0 − M2 u2
(20)                        =      f2 (u2 )F1                    du2 .
                                −∞                   M1

                                                       33
If L = −∞ (or H = +∞), then GL = 0 (or GH = 0) and these equalities are trivially
satisfied. Similarly,

  ΨGM     = M 0 GM
                 Z ∞                                                   
                                    H − M0 − M1 u1          L − M0 − M1 u1
            +M1      u1 f1 (u1 ) F2                  − F2                     du1
                  −∞                     M2                      M2
                 Z ∞                                                   
                                    H − M0 − M2 u2          L − M0 − M2 u2
(21)        +M2      u2 f2 (u2 ) F1                  − F1                     du2 .
                  −∞                     M1                      M2

Reparametrize so that λ = M1 /M2 . By holding λ constant, we can find M0∗ (λ) and M2∗ (λ)
that solve (18) andR(20) with respect to M0 and M2 , respectively.
                                                                R ∞ Let lλ and hλ denote the
                     ∞
solutions to GL = −∞ f1 (u1 )F2 (lλ − λu1 )du1 and 1 − GH = −∞ f1 (u1 )F2 (hλ − λu1 )du1 ,
respectively. Then by the definitions, M0∗ (λ) and M2∗ (λ) are written as

                                      hλ L − lλ H                    H −L
                          M0∗ (λ) =               ,    M2∗ (λ) =             .
                                       hλ − lλ                       hλ − lλ

By substituting these solutions, the right hand side of (21) can be regarded as a function
of λ (denote the function by m(λ)). Thus, for the conclusion it is sufficient to check the
existence of λ∗ > 0 that solves ΨGM = m(λ). From the mean value theorem and Condition
1, the existence of λ∗ can be verified by showing that

(22)                lim m(λ) < (L + ε)GM ,            lim m(λ) > (H − ε)GM ,
                    λ→0                               λ→∞

for some ε > 0 satisfying Condition 1.
    We now show (22). Choose F1 and F2 so that F1−1 (p1 ) < 0 < F1−1 (p2 ) and F2−1 (p1 ) <
0 < F2−1 (p2 ) for some p1 and p2 satisfying Condition 2. Note that hλ → h0 and lλ → l0 as
λ → 0, where h0 and l0 solve F2 (h0 ) = 1 − GH and F2 (l0 ) = GL , respectively. Similarly,
hλ /λ → h∞ and lλ /λ → l∞ as λ → ∞, where h∞ and l∞ solve F1 (h∞ ) = 1 − GH and
F1 (l∞ ) = GL , respectively. From Condition 2, we have l0 < 0 < h0 and l∞ < 0 < h∞ . As
λ → 0, we have
                                          H − L h0
                                                 Z
                         m(λ) → LGM +                (u − l0 )f2 (u)du,
                                          h0 − l0 l0
and as λ → ∞, we have
                                                           h∞
                                           H −L
                                                       Z
                     m(λ) → HGM −                               (h∞ − u)f1 (u)du.
                                          h∞ − l∞      l∞

Therefore, by choosing F1 and F2 , we can obtain l0 , h0 , l∞ , and h∞ that satisfy (22). This
completes the proof.




                                                 34
             Table 1: Models with a single error term and a known censoring point


       Model 1: M (X, U ) = 1.0 − 0.5X + 1.0X · U + U ;    percentage uncensored: 53.8%
            Avg. β                             Evaluation Point of β(x)
                        0.0       0.4     0.8      1.2    2.0      2.8     3.2     3.6      4.0
True Value   0.210    -0.212 -0.027 0.098 0.186 0.298 0.366 0.391 0.412                    0.429
 AIO-SP       0.229    0.002    -0.029   0.056    0.178  0.335    0.325   0.336   0.448    0.607
    sd        0.126    0.747     0.245   0.167    0.204  0.181    0.312   0.326   0.844    2.166
    se        0.126    0.702     0.247   0.162    0.206  0.179    0.313   0.322   0.845    2.129
   90%        0.901    0.862     0.904   0.880    0.905  0.889    0.902   0.891   0.894    0.891
 AIO-NP       0.215    0.020    -0.006   0.080    0.182  0.294    0.338   0.390   0.376    0.294
    sd        0.118    0.572     0.277   0.306    0.382  0.542    0.703   0.793   0.869    0.939
  Tobit      -0.130    -0.967   -0.639 -0.390 -0.208 0.002        0.085   0.109   0.137    0.180
    sd        0.113    0.988     0.332   0.178    0.222  0.158    0.252   0.242   0.620    1.479
       Model 2: M (X, U ) = 1.0 + 0.0X + 1.0X · U + U ;    percentage uncensored: 65.3%
True Value   0.556     0.288    0.405 0.481 0.533 0.598 0.638 0.653 0.655                  0.675
 AIO-SP       0.567    0.415     0.397   0.455    0.530  0.619    0.610   0.622   0.703    0.764
    sd        0.131    0.757     0.245   0.162    0.202  0.169    0.293   0.303   0.783    1.988
    se        0.128    0.731     0.250   0.157    0.202  0.168    0.294   0.298   0.779    1.964
   90%        0.890    0.888     0.908   0.887    0.902  0.894    0.902   0.891   0.894    0.896
 AIO-NP       0.558    0.437     0.416   0.474    0.524  0.601    0.654   0.648   0.645    0.602
    sd        0.119    0.560     0.280   0.293    0.372  0.522    0.644   0.730   0.803    0.843
  Tobit       0.252    -0.265   -0.048   0.110    0.219  0.330    0.366   0.381   0.407    0.454
    sd        0.117    0.973     0.325   0.178    0.220  0.157    0.251   0.245   0.627    1.488
       Model 3: M (X, U ) = −1.0 + 0.0X + 1.0X · U + U ;    percentage uncensored: 34.7%
True Value   1.046     1.525    1.301 1.182 1.108 1.021 0.973 0.955 0.941                  0.929
 AIO-SP       1.034    1.401     1.316   1.212    1.113  1.002    0.999   0.983   0.900    0.539
    sd        0.194    0.890     0.407   0.187    0.257  0.193    0.345   0.332   0.873    2.287
    se        0.194    1.061     0.391   0.188    0.255  0.192    0.347   0.334   0.867    2.241
   90%        0.896    0.955     0.886   0.895    0.898  0.895    0.907   0.900   0.901    0.887
 AIO-NP       1.050    1.614     1.301   1.207    1.128  1.018    0.973   0.973   0.939    0.902
    sd        0.169    4.311     0.400   0.405    0.462  0.605    0.742   0.814   0.879    0.928
  Tobit       0.918    2.416     1.735   1.258    0.950  0.686    0.645   0.615   0.529    0.350
    sd        0.155    1.555     0.609   0.263    0.291  0.193    0.300   0.275   0.679    1.686




                                               35
       Table 1: (Continued) Models with a single error term and a known censoring point


       Model 4: M (X, U ) = 0.0 + 1.0X   + 0.5X · U + U ;    percentage uncensored: 80.2%
            Avg. β                              Evaluation Point of β(x)
                        0.0      0.4       0.8      1.2    2.0      2.8     3.2     3.6      4.0
True Value   1.162    1.399 1.299        1.237 1.195 1.144 1.115 1.105 1.097                1.090
 AIO-SP       1.156   1.388     1.300     1.242    1.196  1.143    1.118   1.110   1.092    0.940
    sd        0.088   0.644     0.244     0.115    0.149  0.105    0.167   0.166   0.428    1.047
    se        0.089   0.684     0.236     0.117    0.149  0.104    0.171   0.165   0.430    1.065
   90%        0.890   0.916     0.887     0.909    0.901  0.898    0.906   0.895   0.904    0.910
 AIO-NP       1.167   1.357     1.303     1.245    1.206  1.151    1.111   1.113   1.111    1.116
    sd        0.081   0.558     0.254     0.241    0.270  0.318    0.377   0.420   0.439    0.451
  Tobit       1.147   1.699     1.449     1.273    1.159  1.061    1.046   1.035   1.005    0.942
    sd        0.083   0.772     0.285     0.127    0.152  0.106    0.166   0.161   0.413    0.990
       Model 5: M (X, U ) = 0.0 − 0.0X   + 1.0X · U + U ;    percentage uncensored: 50.0%
True Value   0.798    0.798 0.798        0.798 0.798 0.798 0.798 0.798 0.798                0.798
 AIO-SP       0.802   0.826     0.802     0.802    0.800  0.791    0.789   0.800   0.822    0.731
    sd        0.149   0.831     0.287     0.168    0.223  0.172    0.312   0.309   0.804    2.067
    se        0.150   0.831     0.286     0.168    0.220  0.176    0.312   0.310   0.814    2.059
   90%        0.889   0.893     0.899     0.900    0.894  0.909    0.899   0.898   0.903    0.895
 AIO-NP       0.797   0.822     0.789     0.811    0.806  0.801    0.787   0.799   0.789    0.748
    sd        0.135   0.604     0.308     0.331    0.407  0.534    0.684   0.769   0.829    0.875
  Tobit       0.493   0.523     0.517     0.509    0.499  0.481    0.475   0.480   0.491    0.511
    sd        0.125   1.175     0.425     0.202    0.243  0.166    0.266   0.251   0.630    1.527
       Model 6: M (X, U ) = 0.0 + 0.0X   + 0.0X · U + U ;    percentage uncensored: 50.0%
True Value   0.000    0.000 0.000        0.000 0.000 0.000 0.000 0.000 0.000                0.000
 AIO-SP      -0.000   0.030     0.001     0.001    0.000 -0.001 -0.001 0.000       0.001    -0.026
    sd        0.041   0.450     0.177     0.075    0.085  0.055    0.085   0.076   0.175    0.448
    se        0.041   0.452     0.179     0.076    0.085  0.057    0.085   0.076   0.179    0.451
   90%        0.905   0.892     0.899     0.903    0.904  0.911    0.904   0.906   0.911    0.899
 AIO-NP      -0.001   0.031 -0.003        0.002 -0.000 0.001       0.001   0.004 -0.003     -0.010
    sd        0.043   0.536     0.217     0.181    0.181  0.183    0.183   0.184   0.183    0.184
  Tobit       0.000   0.007     0.004     0.001    0.000 -0.001 -0.001 -0.000 -0.001        -0.002
    sd        0.039   0.426     0.167     0.071    0.080  0.052    0.080   0.072   0.166    0.421




                                                 36
                 Table 2: Models with two error terms and a known censoring point


       Model 1: M (X, U ) = 1.0 − 0.5X + 1.0X · U1 + U2 ;    percentage uncensored: 54.7%
            Avg. β                              Evaluation Point of β(x)
                        0.0       0.4     0.8       1.2     2.0      2.8     3.2      3.6     4.0
True Value   0.050    -0.500 -0.354 -0.172 -0.007 0.214 0.334 0.374 0.405                    0.430
 AIO-SP      0.042     -0.597   -0.351   -0.155   -0.011   0.199    0.350   0.388    0.382   0.190
    sd       0.099     0.574     0.191   0.126     0.147   0.138    0.244   0.258    0.663   1.763
    se       0.100     0.564     0.194   0.121     0.151   0.139    0.247   0.260    0.689   1.774
   90%       0.896     0.886     0.905   0.883     0.913   0.900    0.905   0.900    0.907   0.890
 AIO-NP      0.049     -0.394   -0.328   -0.187   -0.017   0.205    0.324   0.366    0.358   0.280
    sd       0.098     0.498     0.216   0.216     0.269   0.409    0.545   0.638    0.709   0.781
  Tobit      -0.181    -0.836   -0.671   -0.507   -0.350 -0.081 0.093       0.129    0.125   0.074
    sd       0.088     0.740     0.251   0.137     0.167   0.118    0.193   0.184    0.470   1.121
       Model 2: M (X, U ) = 1.0 + 0.0X + 1.0X · U1 + U2 ;    percentage uncensored: 69.4%
True Value   0.400     0.000    0.117    0.235    0.338 0.480 0.562 0.590 0.612              0.631
 AIO-SP      0.393     -0.063    0.113   0.253     0.344   0.463    0.574   0.612    0.606   0.384
    sd       0.102     0.595     0.103   0.117     0.144   0.126    0.228   0.235    0.625   1.653
    se       0.101     0.596     0.196   0.117     0.147   0.128    0.229   0.236    0.624   1.616
   90%       0.894     0.900     0.905   0.891     0.910   0.907    0.901   0.901    0.898   0.891
 AIO-NP      0.398     0.101     0.137   0.225     0.331   0.483    0.555   0.574    0.577   0.533
    sd       0.093     0.472     0.215   0.213     0.253   0.368    0.505   0.572    0.628   0.664
  Tobit      0.196     -0.115   -0.046   0.026     0.098   0.231    0.330   0.361    0.374   0.369
    sd       0.092     0.756     0.250   0.134     0.166   0.117    0.194   0.190    0.502   1.191
       Model 3: M (X, U ) = −1.0 + 0.0X + 1.0X · U1 + U2 ;    percentage uncensored: 30.6%
True Value   0.908     0.000    0.545    0.845    0.957 0.986 0.964 0.952 0.940              0.930
 AIO-SP      0.931     0.281     0.563   0.798     0.953   1.019    0.918   0.907    0.999   1.141
    sd       0.177     0.880     0.336   0.147     0.208   0.152    0.281   0.273    0.714   1.877
    se       0.175     0.934     0.324   0.151     0.205   0.153    0.282   0.274    0.720   1.877
   90%       0.893     0.899     0.893   0.892     0.891   0.900    0.896   0.893    0.903   0.901
 AIO-NP      0.922     0.691     0.577   0.834     0.963   0.982    0.958   0.969    0.946   0.921
    sd       0.153     6.050     0.344   0.324     0.360   0.467    0.591   0.649    0.724   0.765
  Tobit      0.755     0.673     0.870   0.951     0.945   0.778    0.589   0.556    0.600   0.749
    sd       0.129     1.325     0.526   0.216     0.238   0.161    0.243   0.224    0.549   1.365
       Model 4: M (X, U ) = 0.0 + 1.0X + 0.5X · U1 + U2 ;    percentage uncensored: 86.3%
True Value   1.052     1.000    1.056    1.073    1.071 1.056 1.046 1.042 1.039              1.037
 AIO-SP      1.052     1.083     1.058   1.070     1.071   1.060    1.041   1.041    1.048   0.979
    sd       0.065     0.595     0.202   0.095     0.112   0.076    0.120   0.118    0.317   0.771
    se       0.065     0.599     0.197   0.093     0.111   0.075    0.121   0.119    0.313   0.769
   90%       0.893     0.895     0.897   0.897     0.895   0.897    0.902   0.905    0.897   0.905
 AIO-NP      1.060     1.110     1.066   1.081     1.081   1.067    1.046   1.040    1.042   1.044
    sd       0.060     0.511     0.209   0.185     0.193   0.230    0.272   0.300    0.320   0.326
  Tobit      1.076     1.374     1.244   1.150     1.086   1.024    1.013   1.011    1.005   0.988
    sd       0.064     0.579     0.217   0.097     0.111   0.077    0.121   0.117    0.312   0.750




                                                  37
                   Table 3: Models with a single error term and an unknown censoring function


 Model 1: M (X, U ) = 0.0 + 1.0X + 0.0X · U + U , L(x) = 0.0 + 0.5x; percentage uncensored: 80.5%    L(x) parameters
             Avg. β                            Evaluation Point of β(x)                                a0      a1
                         0.0      0.4     0.8      1.2     2.0      2.8      3.2     3.6      4.0
True Value    1.000     1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000                        0.000    0.500
 AIO-SP       0.985     1.030   0.992    0.988    0.983   0.980    0.986    0.991   0.991    0.964   0.001    0.565
    sd        0.037     0.497   0.237    0.077    0.082   0.053    0.072    0.067   0.152    0.373   0.001    0.014
    se        0.036     0.484   0.234    0.075    0.082   0.052    0.071    0.065   0.150    0.371
   90%        0.868     0.893   0.899    0.885    0.894   0.868    0.892    0.887   0.893    0.900
  Tobit       1.010     1.010   1.010    1.010    1.010   1.010    1.010    1.010   1.010    1.010   -0.084   0.500
    sd        0.021     0.021   0.021    0.021    0.021   0.021    0.021    0.021   0.021    0.021   0.008    0.005
Model 2: M (X, U ) = −1.0 + 0.0X + 1.0X · U + U , L(x) = 0.0 + 0.5x; percentage uncensored: 24.3%    L(x) parameters
True Value    1.283     1.525 1.412 1.350 1.311 1.265 1.239 1.229 1.222 1.215                        0.000    0.500
 AIO-SP       1.272     1.493   1.417    1.356    1.302   1.248    1.253    1.241   1.181    0.885   0.016    0.551
    sd        0.198     0.984   0.401    0.202    0.274   0.210    0.381    0.367   0.959    2.507   0.027    0.019
    se        0.199     1.076    .389    0.202    0.271   0.210    0.378    0.368   0.955    2.444
   90%        0.895     0.929    .887    0.899    0.899   0.903    0.896    0.900   0.898    0.882
  Tobit       0.991     0.991   0.991    0.991    0.991   0.991    0.991    0.991   0.991    0.991   -0.105   0.510
    sd        0.090     0.090   0.090    0.090    0.090   0.090    0.090    0.090   0.090    0.090   0.015    0.009
Model 3: M (X, U ) = −1.0 + 1.0X + 1.0X · U + U , L(x) = 0.0 + 0.5x; percentage uncensored: 46.2%    L(x) parameters
True Value    1.826     2.525 2.194 2.021 1.917 1.798 1.732 1.709 1.691 1.675                        0.000    0.500
 AIO-SP       1.791     2.250   2.169    2.035    1.906   1.758    1.744    1.727   1.640    1.269   0.016    0.551
    sd        0.179     0.821   0.423    0.174    0.240   0.172    0.313    0.300   0.774    1.997   0.027    0.019
    se        0.844     1.061   0.388    0.173    0.239   0.174    0.312    0.298   0.778    2.006
   90%        0.891     0.958   0.862    0.893    0.902   0.895    0.907    0.900   0.900    0.888
  Tobit       1.671     1.671   1.671    1.671    1.671   1.671    1.671    1.671   1.671    1.671   -0.106    0.509
    sd        0.082     0.082   0.082    0.082    0.082   0.082    0.082    0.082   0.082    0.082   0.013     0.008
Model 4: M (X, U ) = −1.0 + 0.0X + 1.0X · U + U , L(x) = 0.0 − 0.5x; percentage uncensored: 46.2%    L(x) parameters
True Value    0.826     1.525 1.194 1.021 0.917 0.798 0.732 0.709 0.691 0.675                        0.000    -0.500
 AIO-SP       0.796     1.272   1.189    1.038    0.898   0.748    0.749    0.741   0.660    0.289   0.016     -0.448
    sd        0.180     0.815   0.420    0.176    0.250   0.176    0.323    0.301   0.786    2.061   0.026     0.019
    se        0.180     1.061   0.389    0.173    0.239   0.174    0.313    0.298   0.779    2.007
   90%        0.891     0.961   0.873    0.896    0.888   0.887    0.891    0.901   0.897    0.890
  Tobit       0.670     0.670   0.670    0.670    0.670   0.670    0.670    0.670   0.670    0.670   -0.106    -0.490
    sd        0.084     0.084   0.084    0.084    0.084   0.084    0.084    0.084   0.084    0.084   0.014     0.008
Model 5: M (X, U ) = −1.0 + 1.0X + 1.0X · U + U , L(x) = 0.0 − 0.5x; percentage uncensored: 67.7%    L(x) parameters
True Value    1.469     2.525 1.988 1.729 1.582 1.427 1.350 1.324 1.304 1.288                        0.000    -0.500
 AIO-SP       1.424     2.156   1.990    1.738    1.540   1.373    1.365    1.344   1.246    0.835   -0.016    -0.379
    sd        0.160     0.728   0.441    0.172    0.233   0.164    0.285    0.266   0.705    1.788   0.0379    0.030
    se        0.163     1.118   0.407    0.163    0.231   0.159    0.282    0.267   0.707    1.811
   90%        0.890     0.971   0.872    0.884    0.892   0.872    0.895    0.901   0.902    0.897
  Tobit        1.45      1.45    1.45     1.45    1.45     1.45     1.45    1.45    1.45     1.45    -0.107   -0.48
    sd        0.079     0.079   0.079    0.079    0.079   0.079    0.079    0.079   0.079    0.079   0.014    0.009




                                                          38
                           Table 4: Results using Epanechnikov kernel


         Model 1: M (X, U ) = 1.0 − 0.5X + 1.0X · U + U ;     percentage uncensored: 53.8%
           Avg. β                               Evaluation Point of β(x)
                       0.0      0.4       0.8     1.2       2.0      2.8       3.2      3.6     4.0
True Value  0.210    -0.212 -0.027 0.098         0.186     0.298    0.366    0.391    0.412    0.429
 AIO-NP      0.264    0.137    0.112    0.123    0.190     0.310    0.388     0.399    0.430   0.370
    sd       0.214    0.324    0.250    0.230    0.255     0.307    0.371     0.422    0.630   1.194
         Model 2: M (X, U ) = 1.0 + 0.0X + 1.0X · U + U ;     percentage uncensored: 65.3%
True Value  0.556     0.288    0.405 0.481       0.533     0.598    0.638    0.653    0.655    0.675
 AIO-NP      0.595    0.516    0.497    0.506    0.548     0.613    0.656     0.669    0.690   0.652
    sd       0.275    0.360    0.303    0.288    0.301     0.336    0.383     0.426    0.620   1.106
         Model 3: M (X, U ) = −1.0 + 0.0X + 1.0X · U + U ;     percentage uncensored: 34.7%
True Value  1.046     1.525    1.301 1.182       1.108     1.021    0.973    0.955    0.941    0.929
 AIO-NP      1.070    1.339    1.267    1.230    1.160     1.059    1.005     0.986    0.955   0.871
    sd       0.304    0.515    0.343    0.328    0.349     0.378    0.420     0.465    0.667   1.211
         Model 4: M (X, U ) = 0.0 + 1.0X + 0.5X · U + U ;     percentage uncensored: 80.2%
True Value  1.162     1.399    1.299 1.237       1.195     1.144    1.115    1.105    1.097    1.090
 AIO-NP      1.208    1.287    1.293    1.276    1.245     1.200    1.172     1.162    1.161   1.161
    sd       0.333    0.397    0.353    0.343    0.343     0.353    0.373     0.390    0.453   0.666
         Model 5: M (X, U ) = 0.0 − 0.0X + 1.0X · U + U ;     percentage uncensored: 50.0%
True Value  0.798     0.798    0.798 0.798       0.798     0.798    0.798    0.798    0.798    0.798
 AIO-NP      0.827    0.838    0.829    0.823    0.824     0.823    0.825     0.822    0.826   0.777
    sd       0.288    0.372    0.323    0.311    0.315     0.356    0.403     0.449    0.648   1.151
         Model 6: M (X, U ) = 0.0 + 0.0X + 0.0X · U + U ;     percentage uncensored: 50.0%
True Value  0.000     0.000    0.000 0.000       0.000     0.000    0.000    0.000    0.000     0.000
 AIO-NP     0.0007   0.0157 0.0033 0.0006 -0.0008 0.0015 -0.0005 -0.0005 -0.0003               -0.0053
    sd      0.0345    0.224    0.126    0.0872 0.0780 0.0782 0.0791          0.0846    0.127    0.227




                                               39
