                                  NBER WORKING PAPER SERIES




                       HOW OFTEN TO SAMPLE A CONTINUOUS-TIME
                             PROCESS IN THE PRESENCE OF
                           MARKET MICROSTRUCTURE NOISE

                                           Yacine Aït-Sahalia
                                            Per A. Mykland

                                          Working Paper 9611
                                  http://www.nber.org/papers/w9611


                       NATIONAL BUREAU OF ECONOMIC RESEARCH
                                1050 Massachusetts Avenue
                                  Cambridge, MA 02138
                                       April 2003




Financial support from the NSF under grants SBR-0111140 (Aït-Sahalia) and DMS-0204639 (Mykland) is
gratefully acknowledged. The views expressed herein are those of the authors and not necessarily those of the
National Bureau of Economic Research.

©2003 by Yacine Aït-Sahalia and Per A. Mykland. All rights reserved. Short sections of text not to exceed
two paragraphs, may be quoted without explicit permission provided that full credit including ©notice, is
given to the source.
How Often to Sample a Continuous-Time Process in the Presence of
Market Microstructure Noise
Yacine Aït-Sahalia and Per A. Mykland
NBER Working Paper No. 9611
April 2003
JEL No. G12, C22

                                           ABSTRACT


Classical statistics suggest that for inference purposes one should always use as much data as is
available. We study how the presence of market microstructure noise in high-frequency financial

data can change that result. We show that the optimal sampling frequency at which to estimate the
parameters of a discretely sampled continuous-time model can be finite when the observations are
contaminated by market microstructure effects. We then address the question of what to do about

the presence of the noise. We show that modelling the noise term explicitly restores the first order
statistical effect that sampling as often as possible is optimal. But, more surprisingly, we also
demonstrate that this is true even if one misspecifies the assumed distribution of the noise term. Not
only is it still optimal to sample as often as possible, but the estimator has the same variance as if

the noise distribution had been correctly specified, implying that attempts to incorporate the noise
into the analysis cannot do more harm than good. Finally, we study the same questions when the
observations are sampled at random time intervals, which are an essential feature of transaction-level

data.

Yacine Aït-Sahalia                                    Per A. Mykland
Bendheim Center for Finance                           Department of Statistics
Princeton University                                  University of Chicago
Princeton, NJ 08544-1021                              Chicago, IL 60637-1514
and NBER                                              Mykland@galton.uchicago.edu
yacine@princeton.edu
   The notion that the observed transaction price in high frequency financial data is the unobserv-
able eﬃcient price plus some noise component due to the imperfections of the trading process is
a well established concept in the market microstructure literature (see for instance Black (1986)).
In this paper, we study the implications of such a data generating process for the estimation of
the parameters of the continuous-time eﬃcient price process, using discretely sampled data on the
transaction price process. In particular, we focus on the eﬀects of the presence of the noise for the
estimation of the variance of asset returns, σ2 . In the absence of noise, it is well known that the
quadratic variation of the process (i.e., the average sum of squares of log-returns measured at high
frequency) estimates σ2 . In theory, sampling as often as possible will produce in the limit a perfect
estimate of σ2 . We show, however, that the situation changes radically in the presence of market
microstructure noise that is not taken into account in the analysis.
   We start by asking whether it remains optimal to sample the price process as often as possible
in the presence of market microstructure noise, consistently with the basic statistical principle that,
ceteris paribus, more data is preferred to less. We show that, if noise is present but unaccounted
for, then the optimal sampling frequency is finite. The intuition for this result is as follows. The
volatility of the underlying eﬃcient price process and the market microstructure noise tend to behave
diﬀerently at diﬀerent frequencies. Thinking in terms of signal-to-noise ratio, a log-return observed
from transaction prices over a tiny time interval is mostly composed of market microstructure noise
and brings little information regarding the volatility of the price process since the latter is (at least in
the Brownian case) proportional to the time interval separating successive observations. As the time
interval separating the two prices in the log-return increases, the amount of market microstructure
noise remains constant, since each price is measured with error, while the informational content of
volatility increases. Hence very high frequency data are mostly composed of market microstructure
noise, while the volatility of the price process is more apparent in longer horizon returns. Running
counter to this eﬀect is the basic statistical principle mentioned above: in an idealized setting where
the data are observed without error, sampling more frequently cannot hurt. What we show is that
these two eﬀects compensate each other and result in a finite optimal sampling frequency (in the
root mean squared error sense).
   We then address the question of what to do about the presence of the noise. If, convinced by
either the empirical evidence and/or the theoretical market microstructure models, one decides to
account for the presence of the noise, how should one go about doing it? We show that modelling



                                                     1
the noise term explicitly restores the first order statistical eﬀect that sampling as often as possible
is optimal. But, more surprisingly, we also demonstrate that this is true even if one misspecifies
the assumed distribution of the noise term. If the econometrician assumes that the noise terms are
normally distributed when in fact they are not, not only is it still optimal to sample as often as
possible (unlike the result when no allowance is made for the presence of noise), but the estimator
has the same variance as if the noise distribution had been correctly specified. Put diﬀerently,
attempts to include a noise term in the econometric analysis cannot do more harm than good. This
robustness result, we think, is a major argument in favor of incorporating the presence of the noise
when estimating continuous time models with high frequency financial data, even if one is unsure
about what is the true distribution of the noise term. Finally, we study the same questions when the
observations are sampled at random time intervals, which are an essential feature of transaction-level
data.
   Our results also have implications for the two parallel tracks that have developed in the recent
financial econometrics literature dealing with discretely observed continuous-time processes. One
strand of the literature has argued that estimation methods should be robust to the potential issues
arising in the presence of high frequency data and, consequently, be asymptotically valid without
requiring that the sampling interval ∆ separating successive observations tend to zero (see, e.g.,
Hansen and Scheinkman (1995), Aït-Sahalia (1996) and Aït-Sahalia (2002)). Another strand of the
literature has dispensed with that constraint, and the asymptotic validity of these methods requires
that ∆ tend to zero instead of or in addition to, an increasing length of time T over which these
observations are recorded (see, e.g., Andersen, Bollerslev, Diebold, and Labys (2003), Bandi and
Phillips (2003) and Barndorﬀ-Nielsen and Shephard (2002)).
   The first strand of literature has been informally warning about the potential dangers of using
high frequency financial data without accounting for their inherent noise (see e.g., page 529 of Aït-
Sahalia (1996)), and we propose a formal modelization of that phenomenon. The implications of our
analysis are most salient for the second strand of the literature, which is predicated on the use of high
frequency data but does not account for the presence of market microstructure noise. Our results
show that the properties of estimators based on the local sample path properties of the process (such
as the quadratic variation to estimate σ2 ) change dramatically in the presence of noise, while at
the same time we suggest a robust approach to correcting for the presence of market microstructure
noise.



                                                   2
     The paper is organized as follows. We start by describing in Section 1 our reduced form setup
and the underlying structural models that support it. We then review in Section 2 the base case
where no noise is present, before analyzing in Section 3 the situation where the presence of the
noise is ignored. Next, we show in Section 4 that accounting for the presence of the noise restores
the optimality of high frequency sampling. Our robustness results are presented in Section 5 and
interpreted in Section 6. We incorporate random sampling intervals into the analysis in Section
7, and a drift term in 8. Sections 9 and 10 present two further relaxation of our assumptions, to
serially correlated and cross-correlated noise respectively. Section 11 concludes. All proofs are in the
Appendix.


1.     Setup

     Our basic setup is as follows. We assume that the underlying process of interest, typically the
log-price of a security, is a time-homogenous diﬀusion on the real line

                                      dXt = µ(Xt ; θ)dt + σdWt                                     (1.1)

where X0 = 0, Wt is a Brownian motion, µ(., .) is the drift function, σ2 the diﬀusion coeﬃcient and
θ the drift parameters, θ ∈ Θ and σ > 0. The parameter space is an open and bounded set. As
discussed in Aït-Sahalia and Mykland (2003), the properties of parametric estimators in this model
are quite diﬀerent depending upon we estimate θ alone, σ 2 alone, or both parameters together. When
the data are noisy, the main eﬀects that we describe are already present in the simpler of these three
cases, where σ2 alone is estimated, and so we focus on that case. Moreover, in the high frequency
context we have in mind, the diﬀusive component of (1.1) is of order (dt)1/2 while the drift component
is of order dt only, so the drift component is mathematically negligible at high frequencies. This is
validated empirically: including a drift actually deteriorates the performance of variance estimates
from high frequency data since the drift is estimated with a large standard error. Not centering
the log returns for the purpose of variance estimation produces more accurate results (see Merton
(1980)). So we simplify the analysis one step further by setting µ = 0, which we do until Section 8,
where we then show that adding a drift term does not alter our results.
     In that case,
                                              Xt = σWt .                                           (1.2)

Until Section 7, we treat the case where we observations occur at equidistant time intervals ∆, in

                                                   3
which case he parameter σ2 is therefore estimated at time T on the basis of N +1 discrete observations
recorded at times τ 0 = 0, τ 1 = ∆,..., τ N = N∆ = T . In Section 7, we let the sampling intervals
be themselves random variables, since this feature is an essential characteristic of high frequency
transaction data.
   Where we depart from the inference setup previously studied in Aït-Sahalia and Mykland (2003)
is that we now assume that, instead of observing the process X at dates τ i , we observe X with error:

                                           X̃τ i = Xτ i + Uτ i                                     (1.3)

where the Uτ0 i s are iid noise with mean zero and variance a2 and are independent of the W process.
In that context, we view X as the eﬃcient log-price, while the observed X̃ is the transaction log-price.
In an eﬃcient market, Xt is the log of the expectation of the final value of the security conditional
on all publicly available information at time t. It corresponds to the log-price that would be in eﬀect
in a perfect market with no trading imperfections, frictions, or informational eﬀects. The Brownian
motion W is the process representing the arrival of new information, which in this idealized setting
is immediately impounded in X.
   By contrast, Ut summarizes the noise generated by the mechanics of the trading process. What
we have in mind as the source of noise is a diverse array of market microstructure eﬀects, either
information or non-information related, such as the presence of a bid-ask spread and the correspond-
ing bounces, the diﬀerences in trade sizes and the corresponding diﬀerences in representativeness
of the prices, the diﬀerent informational content of price changes due to informational asymmetries
of traders, the gradual response of prices to a block trade, the strategic component of the order
flow, inventory control eﬀects, the discreteness of price changes in markets that are not decimalized,
etc. That these phenomena are real are important is an accepted fact in the market microstructure
literature, both theoretical and empirical. One can in fact argue that these phenomena justify this
literature.
   We view (1.3) as the simplest possible reduced form of structural market microstructure models.
Our specification coincides with that of Hasbrouck (1993), who discusses the theoretical market
microstructure underpinnings of such a model, estimates the value of the parameter a to be 0.33%
(since the model is set in terms of log prices, a is a percentage of the asset price) and argues that
the parameter a is a summary measure of market quality. Simple structural market microstructure
models will indeed generate (1.3). For instance, Roll (1984) proposes a model where U is due entirely
to the bid-ask spread, and takes the form U = ± spread/2. A disturbance U can also be generated

                                                   4
by adverse selection eﬀects as in Glosten (1987), where the spread has two components: one that
is due to monopoly power, clearing costs, inventory carrying costs, etc., as previously, and a second
one that arises because of adverse selection whereby the specialist is concerned that the investor
on the other side of the transaction has superior information. In that situation, the disturbance U
would no longer be uncorrelated with the W process and would exhibit autocorrelation at the first
order, which would complicate our analysis without fundamentally altering it: see Sections 9 and
10 where we relax the assumptions that the U 0 s are serially uncorrelated and independent of the W
process, respectively. The situation where the measurement error is primarily due to the fact that
transaction prices are multiples of a tick size (i.e., X̃τ i = mi κ where κ is the tick size and mi is the
integer closest to Xτ i /κ) can be modeled as a rounding oﬀ problem (see Jacod (1996) and Delattre
and Jacod (1997)). Finally, more complex structural models, such as that of Madhavan, Richardson,
and Roomans (1997), also give rise to reduced forms where the observed transaction price X̃ takes
the form of an unobserved fundamental value plus error.
     With (1.3) as our basic data generating process, we now turn to the questions we address in this
paper: how often should one sample a continuous-time process when the data are subject to market
microstructure noise, what are the implications of the noise for the estimation of the parameters of
the X process, and how should one correct for the presence of the noise, allowing for the possibility
that the econometrician misspecifies the assumed distribution of the noise term, and finally allowing
for the sampling to occur at random points in time? We proceed from the simplest to the most
complex situation by adding one extra layer of complexity at a time: Figure 1 shows the three
sampling schemes we consider, starting with fixed sampling without market microstructure noise,
then moving to fixed sampling with noise and concluding with an analysis of the situation where
transaction prices are not only subject to microstructure noise but are also recorded at random time
intervals.


2.     The Baseline Case: No Microstructure Noise

     We start by briefly reviewing what would happen in the absence of market microstructure noise,
that is when a = 0. With X denoting the log-price, the first diﬀerences of the observations are
                                                                             ¡             ¢
the log-returns Yi = X̃τ i − X̃τ i−1 , i = 1, ..., N. The observations Yi = σ Wτ i+1 − Wτ i are then iid
N(0, σ2 ∆) so the likelihood function is

                              l(σ 2 ) = −N ln(2πσ2 ∆)/2 − (2σ2 ∆)−1 Y 0 Y,                          (2.1)

                                                    5
where Y = (Y1 , ..., YN )0 .. The maximum-likelihood estimator of σ2 coincides with the discrete ap-
proximation to the quadratic variation of the process
                                                    N
                                              2   1X 2
                                             σ̂ =     Yi                                            (2.2)
                                                  T
                                                        i=1

which has the following exact small sample moments:
                                         N         ¡   ¢
                                £ 2¤  1 X £ 2 ¤ N σ2 ∆
                               E σ̂ =       E Yi =       = σ2 ,
                                      T i=1          T
                               "N       #   ÃN           !
                  £ 2¤  1       X          1 X       £ 2¤   N ¡      ¢ 2σ4 ∆
                                      2
              V ar σ̂ = 2 V ar      Yi = 2       V ar Yi   = 2 2σ4 ∆2 =
                       T        i=1
                                          T  i=1
                                                            T            T
and the following asymptotic distribution

                                          ¡         ¢
                                     T 1/2 σ̂2 − σ 2 −→ N(0, ω)                                     (2.3)
                                                     T −→∞

where
                                                 h        i−1
                             ω = AV AR(σ̂2 ) = ∆E −¨l(σ2 )    = 2σ4 ∆.                              (2.4)

Thus selecting ∆ as small as possible is optimal for the purpose of estimating σ 2 .


3.      When the Observations Are Noisy But the Noise Is Ignored

     Suppose now that market microstructure noise is present but the presence of the U 0 s is ignored
when estimating σ2 . In other words, we use the log-likelihood (2.1) even though the true structure
of the observed log-returns Yi0 s is given by an MA(1) process since

                                Yi = X̃τ i − X̃τ i−1 + Uτ i − Uτ i−1
                                       ¡               ¢
                                   = σ Wτ i − Wτ i−1 + Uτ i − Uτ i−1                                (3.1)

                                    ≡ εi + ηεi−1

where the ε0i s are iid with mean zero and variance γ 2 . The relationship to the original parametrization
(σ 2 , a2 ) is given by

                                 γ 2 (1 + η2 ) = V ar[Yi ] = σ2 ∆ + 2a2                             (3.2)

                                        γ 2 η = cov(Yi , Yi−1 ) = −a2                               (3.3)



                                                    6
or equivalently

                                    1n 2             p                  o
                           γ2 =         2a + σ2 ∆ + σ2 ∆ (4a2 + σ2 ∆)                               (3.4)
                                    2
                                     1 n     2     2
                                                         p
                                                            2 ∆ (4a2 + σ 2 ∆)
                                                                             o
                            η =           −2a  − σ   ∆ +  σ                                         (3.5)
                                    2a2

which implies, as required, that −1 < η < 0.
     The estimator σ̂2 obtained from maximizing the misspecified log-likelihood has the following
properties:


Proposition 1. In small samples (finite T ), the RMSE of the estimator σ̂ 2 is given by

                       £ ¤     ³¡ £ ¤      ¢2      £ ¤´1/2
                   RMSE σ̂ 2 =   E σ̂2 − σ2 + V ar σ̂ 2
                               Ã       ¡                     ¢      !1/2
                                 4a4 2 σ4 ∆2 + 4σ2 ∆a2 + 6a4    4a4
                             =      +                          − 2                                  (3.6)
                                 ∆2            T∆               T

and has a minimum (provided that T is greater than 21/2 a2 /σ2 ) which is reached at
                                                                                           
                1/3
               2 a  24/3  ³     ¡                 ´
                                               ¢1/2 1/3                1/3
                                                                     2 a    4/3
                                                                                            
         ∆∗ =       2      σ T + σ4 T 2 − 2a4          +³                             ´1/3 .      (3.7)
                  σ
                                                           σ 2 T + (σ 4 T 2 − 2a4 )1/2

As T grows, we have                                         µ           ¶
                                      ∗ 22/3 a4/3 1/3            1
                                    ∆ =          T    +O                    .
                                          σ4/3                  T 1/3


     Figure 2 displays the RMSE of the estimator as a function of ∆ and T, with parameter values
σ = 0.5 and a = 0.05. Complementary to this are the results of Gloter and Jacod (2000) which show
that the presence of even increasingly negligible noise is suﬃcient to adversely aﬀect the identification
of σ2 . They study the asymptotic distribution of σ̂2 when the standard deviation aN of the noise
term goes to zero as the sample size increases, showing that σ̂2 is consistent if and only if NaN goes
to zero, and characterizing the asymptotic distribution of σ̂2 as a function of whether N 3/2 aN goes
to zero, a finite constant, or infinity.


4.     Incorporating Market Microstructure Noise Explicitly

     Now we show that if we explicitly incorporate the U 0 s into the likelihood function, then we are
back into the standard case where the optimal sampling scheme consists in sampling as often as

                                                    7
possible. Suppose that the microstructure noise is normally distributed, an assumption we will relax
below in Section 5. The likelihood function for the Y 0 s is then given by

                      l(η, γ 2 ) = − ln det(V )/2 − N ln(2πγ 2 )/2 − (2γ 2 )−1 Y 0 V −1 Y,          (4.1)

where the covariance matrix for the vector Y = (Y1 , ..., YN )0 is given by γ 2 V , where
                                                                                    
                                            1 +   η 2   η        0     ·  · ·  0
                                                                                    
                                                                       ..      ..   
                                             η       1+η   2    η          .    .   
                                                                                    
                                                                       .            
                  V = [vij ]i,j=1,...,N = 
                                             0         η     1 + η2 . .       0     
                                                                                                   (4.2)
                                              .                                     
                                              ..      . ..     . ..    . ..         
                                                                              η     
                                                                                    
                                              0        ···       0       η 1 + η2

Further,
                                                        1 − η 2N+2
                                            det(V ) =                                               (4.3)
                                                          1 − η2
and, neglecting the end eﬀects, an approximate inverse of V is the matrix Ω = [ω ij ]i,j=1,...,N where

                                               ¡        ¢−1
                                         ω ij = 1 − η 2     (−η)|i−j|

(see Durbin (1959)). The product V Ω diﬀers from the identity matrix only on the first and last rows.
                           £ ¤
The exact inverse is V −1 = vij i,j=1,...,N where

                        ¡       ¢−1 ¡           ¢−1 n
              vij =      1 − η2      1 − η2N +2      (−η)|i−j| − (−η)i+j − (−η)2N−i−j+2             (4.4)
                                                                                  o
                                − (−η)2N+|i−j|+2 + (−η)2N+i−j+2 + (−η)2N−i+j+2 .

(see Shaman (1969) and Haddad (1995)).
   We then obtain the following for the MLE estimators of σ2 and a2 :


Proposition 2. The MLE (σ̂2 , â2 ) is consistent and its asymptotic variance is given by
                                p                                                             
                                 4 σ 6 ∆ (4a2 + σ 2 ∆) + 2σ 4 ∆     −σ 2 ∆h(∆, σ 2 , a2 )
    AV ARnormal (σ̂2 , â2 ) =                                   ¡ 2         ¢                .
                                                                ∆          2 ∆ h(∆, σ 2 , a2 )
                                               •                2  2a  + σ

with
                                                     p
                            h(∆, σ2 , a2 ) ≡ 2a2 +    σ2 ∆ (4a2 + σ2 ∆) + σ2 ∆.                     (4.5)




                                                        8
     Since AV ARnormal (σ̂2 ) is increasing in ∆, it is optimal to sample as often as possible. Further,
since
                             AV ARnormal (σ̂ 2 ) = 8σ3 a∆1/2 + 2σ2 ∆ + o(∆),                           (4.6)

the loss of eﬃciency relative to the case where no market mircrostructure noise is present (and
AV AR(σ̂2 ) = 2σ2 ∆ as given in (2.4)) is at order ∆1/2 . Figure 3 plots the asymptotic variances of
σ̂2 as functions of ∆ with and without noise (the parameter values are again σ = 0.5 and a = 0.05).


5.      The Eﬀect of Misspecifying the Distribution of the Microstruc-
        ture Noise

     We now study the situation where one attempts to incorporate the presence of the U 0 s into the
analysis, as in Section 4, but assumes a misspecified model for them. Specifically, we consider the
case where the U 0 s are assumed to be normally distributed when in reality they have a diﬀerent
distribution. We still suppose that the U 0 s are iid with mean zero and variance a2 .
     Since the econometrician assumes the U 0 s to have a normal distribution, inference is still done with
the log-likelihood l(σ2 , a2 ), or equivalently l(η, γ 2 ) given in (4.1), using (3.2)-(3.3). This means that
the scores l̇σ2 and l˙a2 , or equivalently (B.1) and (B.2), are used as moment functions (or “estimating
equations”). Since the first order moments of the moment functions only depend on the second order
moment structure of the log-returns (Y1 , ..., YN ), which is unchanged by the absence of normality,
the moment functions are unbiased

                                         Etrue [l˙η ] = Etrue [l̇γ 2 ] = 0                             (5.1)

and similarly for l˙σ2 and l̇a2 . Hence the estimator (σ̂2 , â2 ) based on these moment functions is
consistent and asymptotically unbiased (even though the likelihood function is misspecified.)
     The eﬀect of misspecification lies in the asymptotic variance matrix. We use a technical trick to
simplify calculations that would otherwise be daunting. By using the cumulants of the distribution
of U, we express the asymptotic variance of these estimators in terms of deviations from normality.




                                                        9
We have that
       h         i
  Etrue l˙η l˙γ 2 = Covtrue (l̇η , l˙γ 2 )
                                                                         
                                           N
                                           X          ij     N
                                                             X
                                       1           ∂v      1
                   = Covtrue − 2            Yi Yj       ,     Yk Yl v kl 
                                     2γ             ∂η 2γ 4
                                              i,j=1                 k,l=1
                                  N
                                  X
                          1                 ∂vij kl
                  = −                           v Covtrue (Yi Yj , Yk Yl )                                    (5.2)
                         4γ 6                ∂η
                                i,j,k,l=1
                                  N
                                  X
                      1             ∂vij kl
                  = − 6                 v [Cumtrue (Yi , Yj , Yk , Yl ) + 2Covtrue (Yi , Yj )Covtrue (Yk , Yl )].
                     4γ              ∂η
                                i,j,k,l=1

where “true” denotes the true distribution of the Y 0 s, not the incorrectly specified one, and Cum
denotes the cumulants. The last transition is because

           Covtrue (Yi Yj , Yk Yl ) = Etrue [Yi Yj Yk Yl ] − Etrue [Yi Yj ] Etrue [Yk Yl ]

                                      = κijkl − κij κkl

                                      = κi,j,k,l + κi,j κk,l [3] − κi,j κk,l

                                      = κi,j,k,l + κi,k κj,l + κi,l κj,k

                                      = Cumtrue (Yi , Yj , Yk , Yl ) + Covtrue (Yi , Yk )Covtrue (Yj , Yk )

                                             +Covtrue (Yi , Yl )Covtrue (Yj , Yk )

since Y has mean zero (see e.g., Section 2.3 of McCullagh (1987)). The need for permutation goes
away due to the summing over all indices (i, j, k, l), and since V −1 = [vij ] is symmetric.
   When looking at (5.2), note that Cumnormal (Yi , Yj , Yk , Yl ) = 0, where “normal” denotes a Normal
distribution with the same first and second order moments as the true distribution. That is, if the
Y 0 s were normal we would have
                    h         i    1
                                                N
                                                X ∂v ij kl
             Enormal l˙η l˙γ 2 = − 6                   v [2Covnormal (Yi , Yj )Covnormal (Yk , Yl )].
                                  4γ               ∂η
                                              i,j,k,l=1

Also, since the covariance structure does not depend on Gaussianity, Covtrue (Yi , Yj ) = Covnormal (Yi , Yj ).
Next, we have
                                       h         i          h      i        h       i
                                Enormal l̇η l̇γ 2 = −Enormal ¨lηγ 2 = −Etrue ¨lηγ 2                           (5.3)

with the last equality following from the fact that ¨lηγ 2 depends only on the second moments of the
                                  h         i         h      i
Y 0 s. (Note that in general Etrue l̇η l̇γ 2 6= −Etrue ¨lηγ 2 because the likelihood may be misspecified.)


                                                             10
Thus, it follows from (5.2) that

                   h         i         h         i  1
                                                             N
                                                             X ∂vij kl
              Etrue l˙η l˙γ 2 = Enormal l̇η l˙γ 2 − 6              v Cumtrue (Yi , Yj , Yk , Yl )
                                                   4γ           ∂η
                                                           i,j,k,l=1
                                    h      i  1
                                                           N
                                                           X ∂vij kl
                            = −Etrue ¨lηγ 2 − 6                  v Cumtrue (Yi , Yj , Yk , Yl )                  (5.4)
                                             4γ               ∂η
                                                        i,j,k,l=1

   It follows similarly that
                 ·³ ´ ¸
                        2
            Etrue l̇η       = V artrue (l̇η )

                                     h i     1
                                                           N
                                                           X ∂vij ∂v kl
                             = −Etrue ¨lηη + 4                          Cumtrue (Yi , Yj , Yk , Yl )             (5.5)
                                            4γ                ∂η ∂η
                                                        i,j,k,l=1

and
                   ·³ ´ ¸
                          2
              Etrue l̇γ 2   = V artrue (l̇γ 2 )

                                      h         i  1
                                                               N
                                                               X
                              = −Etrue ¨lγ 2 γ 2 + 8                     vij vkl Cumtrue (Yi , Yj , Yk , Yl ).   (5.6)
                                                  4γ
                                                             i,j,k,l=1

   To calculate the fourth cumulant Cumtrue (Yi , Yj , Yk , Yl ), recall from (3.1) that the observed log-
returns are
                                         ¡             ¢
                                   Yi = σ Wτ i − Wτ i−1 + Uτ i − Uτ i−1 .

First, note that the τ i are nonrandom, and since W is independent of the U 0 s, and has Gaussian
increments. Second, the cumulants are multilinear, so

                                      ¡ ¡             ¢                   ¡             ¢
Cumtrue (Yi , Yj , Yk , Yl ) = Cumtrue σ Wτ i − Wτ i−1 + Uτ i − Uτ i−1 , σ Wτ j − Wτ j−1 + Uτ j − Uτ j−1 ,
                                    ¡            ¢                   ¡             ¢                ¢
                                  σ Wτ k − Wτ k−1 + Uτ k − Uτ k−1 , σ Wτ l − Wτ l−1 + Uτ l − Uτ l−1

                          = σ4 Cumtrue (Wτ i − Wτ i−1 , Wτ j − Wτ j−1 , Wτ k − Wτ k−1 , Wτ l − Wτ l−1 )

                              +σ3 Cumtrue (Wτ i − Wτ i−1 , Wτ j − Wτ j−1 , Wτ k − Wτ k−1 , Uτ l − Uτ l−1 )[4]

                              +σ2 Cumtrue (Wτ i − Wτ i−1 , Wτ j − Wτ j−1 , Uτ k − Uτ k−1 , Uτ l − Uτ l−1 )[6]

                              +σCumtrue (Wτ i − Wτ i−1 , Uτ j − Uτ j−1 , Uτ k − Uτ k−1 , Uτ l − Uτ l−1 )[4]

                              +Cumtrue (Uτ i − Uτ i−1 , Uτ j − Uτ j−1 , Uτ k − Uτ k−1 , Uτ l − Uτ l−1 )

Out of these terms, only the last is nonzero because W has Gaussian increments (so all cumulants
of its increments of order greater than two are zero), and is independent of the U 0 s (so all cumulants


                                                      11
involving increments of both W and U are also zero.) Therefore,

        Cumtrue (Yi , Yj , Yk , Yl ) = Cumtrue (Uτ i − Uτ i−1 , Uτ j − Uτ j−1 , Uτ k − Uτ k−1 , Uτ l − Uτ l−1 )

where U is a generic random variable with distribution Uτ i .
   If i = j = k = l, we have:

   Cumtrue (Uτ i − Uτ i−1 , Uτ i − Uτ i−1 , Uτ i − Uτ i−1 , Uτ i − Uτ i−1 ) = Cum4 (Uτ i − Uτ i−1 )

                                                                            = Cum4 (Uτ i ) + Cum4 (−Uτ i−1 )

                                                                            = 2 Cum4 (U)

with the second equality following from the independence of Uτ i and Uτ i−1 , and the third from the
fact that the cumulant is of even order. Cum4 (U) denotes the fourth cumulant of the random variable
U, which has mean zero, so, in terms of the moments of U we have
                                                  £ ¤     ¡ £ ¤¢2
                                      Cum4 (U) = E U 4 − 3 E U 2 .                                                (5.7)

   If max(i, j, k, l) = min(i, j, k, l) + 1, two situations arise. Set m = min(i, j, k, l) and M =
max(i, j, k, l). Also set s = s(i, j, k, l) = #{i, j, k, l = m}. If s is odd, say s = 1 with i = m, and
j, k, l = M = m + 1, we get a term of the form

         Cumtrue (Uτ m − Uτ m−1 , Uτ m+1 − Uτ m , Uτ m+1 − Uτ m , Uτ m+1 − Uτ m ) = −Cum4 (Uτ m ).

By permutation, the same situation arises if s = 3. If instead s is even, i.e., s = 2, then we have
terms of the form

          Cumtrue (Uτ m − Uτ m−1 , Uτ m − Uτ m−1 , Uτ m+1 − Uτ m , Uτ m+1 − Uτ m ) = Cum4 (Uτ m ).

Finally, if at least one pair of indices in the quadruple (i, j, k, l) is more than one integer apart, then

                    Cumtrue (Uτ i − Uτ i−1 , Uτ j − Uτ j−1 , Uτ k − Uτ k−1 , Uτ l − Uτ l−1 ) = 0

by independence of the U 0 s.
   Putting it all together, we have
                                    
                                    
                                     2 Cum4 (U) if i = j = k = l
                                    
                                    
     Cumtrue (Yi , Yj , Yk , Yl ) =   (−1)s Cum4 (U) if max(i, j, k, l) = min(i, j, k, l) + 1                     (5.8)
                                    
                                    
                                    
                                     0 otherwise

   We now need to evaluate the sums that appear on the right hand sides of (5.4), (5.5) and (5.6).
We obtain:


                                                          12
Theorem 1. The estimators (σ̂2 , â2 ) obtained by maximizing the log-likelihood (4.1) are consistent
and their asymptotic variance is given by
                                                                                             
                                                                                      0   0
                     AV ARtrue (σ̂2 , â2 ) = AV ARnormal (σ̂2 , â2 ) + Cum4 (U)                        (5.9)
                                                                                      0 ∆

where AV ARnormal (σ̂2 , â2 ) is the asymptotic variance in the case where the distribution of U is
Normal, that is, the expression given in Proposition 2.


6.     Robustness to Misspecification of the Noise Distribution

     The above Theorem 1 has implications for the use of the Gaussian likelihood l that go beyond
consistency, namely that this likelihood can also be used to estimate the distribution of σ̂2 under
misspecification. With l denoting the log-likelihood assuming that the U 0 s are Gaussian, given in
(4.1), −¨l(σ̂2 , â2 ) denote the observed information matrix in the original parameters σ 2 and a2 . Then
                                                        µ              ¶−1
                                                            1
                                          \
                                     V̂ = AV ARnormal = − ¨l(σ̂ , â )
                                                                  2  2
                                                            T
is the usual estimate of asymptotic variance when the distribution is correctly specified as Gaussian.
Also note, however, that otherwise, so long as (σ̂ 2 , â2 ) is consistent, V̂ is also a consistent estimate
of the matrix AV ARnormal (σ̂2 , â2 ). Since this matrix coincides with AV ARtrue (σ̂2 , â2 ) for all but the
(a2 , a2 ) term (see (5.9)), the asymptotic variance of T 1/2 (σ̂2 − σ 2 ) is consistently estimated by V̂σ2 σ2 .
The similar statement is true for the covariances, but not, obviously, for the asymptotic variance of
T 1/2 (â2 − a2 ).
     In the likelihood context, the possibility of estimating the asymptotic variance by the observed
information is due to the second Bartlett identity. For a general log likelihood l, if S ≡ Etrue [l˙l˙0 ]/N
and D ≡ −Etrue [¨l]/N (diﬀerentiation refers to the original parameters (σ2 , a2 ), not the transformed
parameters (γ 2 , η)) this identity says that

                                                  S − D = 0.                                               (6.1)

It implies that the asymptotic variance takes the form

                                      AV AR = ∆(DS −1 D)−1 = ∆D−1 .                                        (6.2)

It is clear that (6.2) remains valid if the second Bartlett identity holds only to first order, i.e.,

                                                 S − D = o(1)                                              (6.3)

                                                       13
as N → ∞, for a general criterion function l which satisfies Etrue [l̇] = o(N).
   However, in view of Theorem 1, equation (6.3) cannot be satisfied. In fact, we show in Appendix
D that
                                      S − D = Cum4 (U)gg 0 + o(1),                                  (6.4)

where                                                                                      
                                                                   ∆1/2
                                   gσ2                          σ(4a2 +σ2 ∆)3/2
                             g=          =
                                                          µ                          ¶     
                                                                                            .      (6.5)
                                                      1            ∆1/2 σ (6a2 +σ2 ∆)
                                   ga2               2a4
                                                            1−                        3/2
                                                                     (4a2 +σ 2 ∆)

From (6.5), we see that g 6= 0 whenever σ2 > 0. This is consistent with the result in Theorem 1 that
the true asymptotic variance matrix, AV ARtrue (σ̂2 , â2 ), does not coincide with the one for Gaussian
noise, AV ARnormal (σ̂2 , â2 ). On the other hand, the 2 × 2 matrix gg 0 is of rank 1, signaling that
there exist linear combinations that will cancel out the first column of S − D. From what we already
know of the form of the correction matrix, D−1 gives such a combination, so that the asymptotic
variance of the original parameters (σ2 , a2 ) will have the property that its first column is not subject
to correction in the absence of normality.
   A curious consequence of (6.4) is that while the observed information can be used to estimate
the asymptotic variance of σ̂ 2 when a2 is not known, this is not the case when a2 is known. This
is because the second Bartlett identity also fails to first order when considering a2 to be known,
i.e., when diﬀerentiating with respect to σ2 only. Indeed, in that case we have from the upper left
component in the matrix equation (6.4)
                                          h                   i            h                  i
              Sσ2 σ2 − Dσ2 σ2 = N −1 Etrue l˙σ2 σ2 (σ2 , a2 )2 + N −1 Etrue ¨lσ2 σ2 (σ2 , a2 )

                                = Cum4 (U) (gσ2 )2 + o(1)

which is not o(1) unless Cum4 (U) = 0.
   To make the connection between Theorem 1 and the second Bartlett identity, one needs to go to
the log profile likelihood
                                           λ(σ2 ) ≡        sup l(σ2 , a2 ).                         (6.6)
                                                            a2

Obviously, maximizing the likelihood l(σ 2 , a2 ) is the same as maximizing λ(σ2 ). Thus one can
think of σ2 as being estimated (when α2 is unknown) by maximizing the criterion function λ(σ2 ),
or by solving λ̇(σ̂2 ) = 0. Also, the observed profile information is related to the original observed
information by
                                                     h                  i
                                         λ̈(σ̂2 )−1 = ¨l(σ̂ 2 , â2 )−1           ,                 (6.7)
                                                                         σ2 σ 2


                                                           14
i.e., the first (upper left hand corner) component of the inverse observed information in the original
problem. We recall the rationale for equation (6.7) in Appendix D, where we also show that Etrue [λ̇] =
o(N). In view of Theorem 1, λ̈(σ̂2 ) can be used to estimate the asymptotic variance of σ̂2 under the
true (possibly non-Gaussian) distribution of the U 0 s, and so it must be that the criterion function λ
satisfies (6.3), that is
                             N −1 Etrue [λ̇(σ2 )2 ] + N −1 Etrue [λ̈(σ2 )] = o(1).                (6.8)

This is indeed the case, as shown in Appendix D.
     This phenomenon is related, although not identical, to what occurs in the context of quasi-
likelihood (for comprehensive treatments of quasi-likelihood theory, see the books by McCullagh and
Nelder (1989) and Heyde (1997), and the references therein, and for early econometrics examples see
Macurdy (1982) and White (1982)). In quasi-likelihood situations, one uses a possibly incorrectly
specified score vector which is nevertheless required to satisfy the second Bartlett identity. What
makes our situation unusual relative to quasi-likelihood is that the interest parameter σ2 and the
nuisance parameter a2 are entangled in the same estimating equations (l̇σ2 and l̇a2 from the Gaussian
likelihood) in such a way that the estimate of σ2 depends, to first order, on whether a2 is known
or not. This is unlike the typical development of quasi-likelihood, where the nuisance parameter
separates out (see, e.g., Table 9.1 (p. 326) of McCullagh and Nelder (1989)). Thus only by going to
the profile likelihood λ can one make the usual comparison to quasi-likelihood.


7.     Randomly Spaced Sampling Intervals

     One could of course argue that we have made many simplifying or special assumptions. We now
show that none of these assumptions drive the results, and that the eﬀects we describe are all present
in more complex (and realistic) setups. We start by relaxing the assumption that ∆ is constant.
     Indeed, one essential feature of transaction data in finance is that the time that separates suc-
cessive observations is random, or at least time-varying. So, as in Aït-Sahalia and Mykland (2003),
we are led to consider the case where ∆i = τ i − τ i−1 are either deterministic and time-varying, or
random in which case we assume for simplicity that they are iid, independent of the W process. We
denote by NT the number of observations recorded by time T . NT is random if the ∆0 s are. We also
suppose that Uτ i can be written Ui , where the Ui are iid and independent of the W process and the
∆0i s. Thus, the observation noise is the same at all observation times, whether random or nonran-
dom. If we define the Yi s as before, in the first two lines of (3.1), though the MA(1) representation

                                                      15
is not valid in the same form.
   We can do inference conditionally on the observed sampling times, in light of the fact that the
likelihood function using all the available information is

            L (YN , ∆N , ..., Y1 , ∆1 ; β, ψ) = L (YN , ..., Y1 |∆N , ..., ∆1 ; β) × L (∆N , ..., ∆1 ; ψ)

where β are the parameters of the state process, that is (σ 2 , a2 ), and ψ are the parameters of the
sampling process, if any (the density of the sampling intervals density L (∆NT , ..., ∆1 ; ψ) may have
its own nuisance parameters ψ, such as an unknown arrival rate, but we assume that it does not
depend on the parameters β of the state process.) The corresponding log-likelihood function is
                      N
                      X                                               N−1
                                                                      X
                            ln L (YN , ..., Y1 |∆N , ..., ∆1 ; β) +         ln L (∆N , ..., ∆1 ; ψ)         (7.1)
                      n=1                                             n=1

and since we only care about β, we only need to maximize the first term in that sum.
   We operate on the covariance matrix Σ of the log-returns Y 0 s, now given by
                                                                                                     
                       2        2     −a2
                   σ ∆1 + 2a                         0        ···        0                           
                                                               . ..       ..                         
                        −a2      σ2 ∆2 + 2a2       −a2                     .                         
                                                                                                     
                                                               ..                                    
              Σ=        0           −a2       σ2 ∆3 + 2a2        .      0                           
                                                                                                           (7.2)
                          ..                                                                         
                                      ..            ..         ..                                    
                           .             .             .          .     −a2                          
                                                                                                     
                          0           ···             0       −a2 σ2 ∆n + 2a2

Note that in the equally spaced case, Σ = γ 2 V . The log-likelihood function is given by

            ln L (YN , ..., Y1 |∆N , ..., ∆1 ; β) ≡ l(σ2 , a2 )                                             (7.3)

                                                   = − ln det(Σ)/2 − N ln(2π)/2 − Y 0 Σ−1 Y /2,

Suppose in the following that β 1 and β 2 can represent either σ2 or a2 . We start with:


Lemma 1. Fisher’s Conditional Information is given by
                                h           ¯ i     1 ∂ 2 ln det Σ
                                            ¯
                              E − ¨lβ 2 β 1 ¯ ∆ = −                .                                        (7.4)
                                                    2 ∂β 2 β 1

7.1   Expansion around a fixed value of ∆

   To continue further with the calculations, we now expand around a fixed value of ∆, namely
∆0 = E [∆] . Specifically, suppose now that

                                                 ∆i = ∆0 (1 + ξ i )                                         (7.5)

                                                          16
where    and ∆0 are nonrandom, the ξ 0i s are iid random variables with mean zero. We will Taylor-
expand the expressions above around = 0, i.e., around the non-random sampling case. For simplic-
ity, we take the ξ 0i s to be bounded. Denote by Σ0 the value of Σ when ∆ is replaced by ∆0 , and let
Ξ denote the matrix whose diagonal elements are the terms ∆0 ξ i , and whose oﬀ-diagonal elements
are zero. We obtain:


Theorem 2. The MLE (σ̂ 2 , â2 ) is again consistent, this time with asymptotic variance

                                AV AR(σ̂2 , â2 ) = A(0) +   2
                                                                 A(2) + O( 3 )                     (7.6)

where                p                                                                        
                      4 σ 6 ∆0 (4a2 + σ2 ∆0 ) + 2σ4 ∆0              −σ 2 ∆0 h(∆0 , σ2 , a2 )
            A(0)   =                                             ¡ 2          ¢               
                                                             ∆0
                                     •                        2    2a + σ2 ∆0 h(∆0 , σ2 , a2 )
and                                                                   
                                                          (2)    (2)
                                           V ar[ξ]       Aσ2 σ2 Aσ2 a2
                               A(2)   =                               
                                        (4a2 + ∆0 σ2 )     •
                                                                 (2)
                                                                Aa2 a2
with
                           ³              p            ´
                (2)                   3/2
               Aσ2 σ2 = −4 ∆20 σ6 + ∆0 σ5 4a2 + ∆0 σ 2
                (2)      3/2
                               p           ¡            ¢       ¡            ¢
               Aσ2 a2 = ∆0 σ3 4a2 + ∆0 σ2 2a2 + 3∆0 σ2 + ∆20 σ4 8a2 + 3∆0 σ2
                               ³       p p                     ´2
                (2)
               Aa2 a2 = −∆20 σ2 2a2 + σ ∆0 4a2 + ∆0 σ2 + ∆0 σ2


   Note that A(0) is the asymptotic variance matrix already present in Proposition 2, except that
it is evaluated at ∆0 = E[∆]. Note also that the second order correction term is proportional to
V ar[ξ], and is therefore zero in the absence of sampling randomness. When that happens, ∆ = ∆0
with probability one and the asymptotic variance of the estimator reduces to the leading term A(0) ,
i.e., to the result in the fixed sampling case given in Proposition 2..



7.2     Randomly Spaced Sampling Intervals and Misspecified Microstructure Noise

   Suppose now, as in Section 5, that the U 0 s are iid, have mean zero and variance a2 , but are
otherwise not necessarily Gaussian. We adopt the same approach as in Section 5, namely to express
the estimator’s properties in terms of deviations from the deterministic and Gaussian case. The
additional correction terms in the asymptotic variance are given in the following result.


                                                   17
Theorem 3. The asymptotic variance is given by
                                ³                    ´                      ³                    ´
        AV ARtrue (σ̂2 , â2 ) = A(0) + Cum4 (U)B (0) +                 2
                                                                             A(2) + Cum4 (U)B (2) + O( 3 )   (7.7)

where A(0) and A(2) are given in the statement of Theorem 2 and
                                                       
                                                  0  0
                                        B (0) =        
                                                  0 ∆0

while                                                                               
                                                                  (2)          (2)
                                                               Bσ2 σ2 Bσ2 a2
                                          B (2) = V ar[ξ]                    
                                                                        (2)
                                                                  •    Ba2 a2
                                           3/2                     ¡                           ¢
                          (2)         10∆0 σ5              4∆20 σ6 16a4 + 11a2 ∆0 σ2 + 2∆20 σ4
                      Bσ2 σ2 =                         +
                                   (4a2 + ∆0 σ2 )5/2           (2a2 + ∆0 σ2 )3 (4a2 + ∆0 σ2 )2

                                −∆20 σ4             ³p           ¡                                           ¢
  (2)
Bσ2 a2 =                                              4a2 + ∆0 σ2 32a6 + 64a4 ∆0 σ 2 + 35a2 ∆20 σ4 + 6∆30 σ6
             (2a2 + ∆0 σ2 )3 (4a2 + ∆0 σ2 )5/2
                 1/2 ¡                                          ¢´
             +∆0 σ 116a6 + 126a4 ∆0 σ2 + 47a2 ∆20 σ 4 + 6∆30 σ6

                                          5/2  ¡                           ¢
              (2)                   16a8 ∆0 σ3 13a4 + 10a2 ∆0 σ2 + 2∆20 σ4
             Ba2 a2   =                               ³           p              ´2 .
                           2      2 3   2      2  5/2    2   2       2       2 2
                        (2a + ∆0 σ ) (4a + ∆0 σ )      2a + σ ∆ − σ ∆ (4a + σ ∆)


     The term A(0) is the base asymptotic variance of the estimator, already present with fixed sam-
pling and Gaussian noise. The term Cum4 (U)B (0) is the correction due to the misspecification of
the error distribution. These two terms are identical to those present in Theorem 1. The terms
proportional to       2   are the further correction terms introduced by the randomness of the sampling.
A(2) is the base correction term present even with Gaussian noise in Theorem 2, and Cum4 (U)B (2)
is the further correction due to the sampling randomness. Both A(2) and B (2) are proportional to
V ar[ξ] and hence vanish in the absence of sampling randomness.


8.      Presence of a Drift Coeﬃcient

     What happens to our conclusions when the underlying X process has a drift? We shall see in
this case that the presence of the drift does not alter our earlier conclusions. As a simple example,
consider linear drift, i.e., replace (1.2) with

                                                   Xt = µt + σWt .                                           (8.1)

                                                             18
The contamination by market microstructure noise is as before: the observed process is given by
(1.3).
   As before, we first-diﬀerence to get the log-returns Yi = X̃τ i − X̃τ i−1 + Uτ i − Uτ i−1 . The likelihood
function is now

    ln L (YN , ..., Y1 |∆N , ..., ∆1 ; β) ≡ l(σ 2 , a2 , µ)

                                         = − ln det(Σ)/2 − N ln(2π)/2 − (Y − µ∆)0 Σ−1 (Y − µ∆)/2,

where the covariance matrix is given in (7.2), and where ∆ = (∆1 , ..., ∆N )0 . If β denotes either σ 2
or a2 , one obtains
                                                           −1
                                            ¨lµβ = ∆0 ∂Σ (Y − µ∆),
                                                       ∂β
so that E[¨lµβ |∆] = 0 no matter whether the U 0 s are normally distributed or have another distribution
with mean 0 and variance a2 . In particular,

                                                     E[¨lµβ ] = 0.                                     (8.2)

   Now let E[¨
             l] be the 3×3 matrix of expected second likelihood derivatives. Let E[¨l] = −T E[∆]D+
                                ˙ = T E[∆]S + o(T ). As before, when the U 0 s have a normal distribu-
o(T ). Similarly define Cov(l̇, l)
tion, S = D, and otherwise that is not the case. The asymptotic variance matrix of the estimators
is of the form AVAR = E[∆]D−1 SD−1 .
   Let Dσ2 ,a2 be the corresponding 2 × 2 matrix when estimation is carried out on σ2 and a2 for
known µ, and Dµ is the asymptotic information on µ for known σ2 and a2 . Similarly define Sσ2 ,a2
and AVARσ2 ,a2 . Since D is block diagonal by (8.2),
                                                                            
                                                        Dσ2 ,a2      0
                                             D=                             ,
                                                          00       Dµ

it follows that                                                              
                                                        Dσ−1
                                                           2 ,a2         0
                                           D−1 =                             .
                                                           00      Dµ−1
Hence
                                  AV AR(σ̂2 , â2 ) = E[∆]Dσ−1               −1
                                                             2 ,a2 Sσ 2 ,a2 Dσ 2 ,a2 .                 (8.3)

The asymptotic variance of (σ̂2 , â2 ) is thus the same as if µ were known, in other words, as if µ = 0,
which is the case that we focused on in all the previous sections.


                                                          19
9.     Serially Correlated Noise

     We now examine what happens if we relax the assumption that the market microstructure noise
is serially independent. Suppose that, instead of being iid with mean 0 and variance a2 , the market
microstructure noise follows
                                           dUt = −bUt dt + cdZt

where b > 0, c > 0 and Z is a Brownian motion independent of W. U∆ |U0 has a Gaussian distribution
                                  2 ¡         ¢
with mean e−b∆ U0 and variance c2b 1 − e−2b∆ . The unconditional mean and variance of U are 0
            c2
and a2 =    2b .   The main consequence of this model is that the variance contributed by the noise to
a log-return observed over an interval of time ∆ is now of order O(∆), that is of the same order as
the variance of the eﬃcient price process σ2 ∆, instead of being of order O(1) as previously. In other
words, log-prices observed close together have very highly correlated noise terms. Because of this
feature, this model for the microstructure noise would not be appropriate if the primary source of
the noise consists of bid-ask bounces. In such a situation, the fact that a transaction is on the bid or
ask side has little predictive power for the next transaction, or at least not enough to predict that
two successive transactions are on the same side with very high probability. On the other hand, this
model can better capture eﬀects such as the gradual adjustment of prices in response to a shock such
as a large trade. In practice, the noise term probably encompasses both of these examples, resulting
in a situation where the variance contributed by the noise has both types of components, some of
order O(1), some of lower orders in ∆.
     The observed log-returns take the form

                                   Yi = X̃τ i − X̃τ i−1 + Uτ i − Uτ i−1
                                          ¡               ¢
                                      = σ Wτ i − Wτ i−1 + Uτ i − Uτ i−1

                                       ≡ wi + ui

where the wi0 s are iid N(0, σ2 ∆) and the u0i s are independent of the wi0 s, are Gaussian with mean
zero and variance
                                 h¡                    ¡         ¢
                          £ 2¤                  ¢2 i c2 1 − e−b∆
                         E ui = E Uτ i − Uτ i−1     =              = c2 ∆ + o(∆)                   (9.1)
                                                           b
instead of 2a2 .
     In addition, the u0i s are now serially correlated. In particular, we have
                                                         ¡              ¢
                                                       c2 1 − e−b∆(i−k)
                                      E [Uτ i Uτ k ] =
                                                              2b

                                                    20
for i ≥ k. If one ignores the presence of this type of serially correlated noise when estimating σ2 ,
then:


Proposition 3. In small samples (finite T ), the RMSE of the estimator σ̂ 2 is given by
                           Ã ¡             ¢2       ¡          ¢2 ¡ T −2b∆                ¢
                  £ 2¤        c4 1 − e−b∆         c4 1 − e−b∆       ∆e       − 1 + e−2T b
          RMSE σ̂       =                      +                               2
                                   b2 ∆2                    T 2 b2 (1 + e−b∆ )
                                                  Ã
                                                            2
                                                              ¡       −b∆
                                                                          ¢ !2 1/2
                                              2            c 1−e
                                         +          σ2∆ +                                          (9.2)
                                             T∆                    b
                                          ¡ 2       ¢2                   µ ¶
                            2    bc2       σ + c2 ∆              2          1
                        = c −        ∆+         2
                                                        + O(∆ ) + O
                                  2            c T                          T2

so that for large T, starting from a value of c2 in the limit where ∆ → 0, increasing ∆ first reduces
        £ ¤
RMSE σ̂2 . Hence the optimal sampling frequency is finite.


   Figure 4 displays the RMSE of the estimator as a function of ∆ and T, with parameter values
σ = 0.15, b = 1 and c = 0.1. With realistic parameter values, T must be quite large before
      £ ¤
RMSE σ̂2 exhibits a minimum in ∆: asymptotically in small ∆, this will only occur if
                                                ¡ 2     ¢2
                                          bc2    σ + c2
                                        −     +            <0
                                           2       c2 T

that is,                                        ¡        ¢2
                                               2 σ2 + c2
                                           T >
                                                   bc4
or T > 21.125 years with these parameter values. This is due to the fact that the existence of a
minimum in ∆ comes from the bias component, which becomes predominant as T gets large. This
is another way of seeing that this type of noise is not nearly as bad as iid noise for the purpose of
inferring σ 2 from high frequency data. Recall from (9.1) that the variance of the noise is of the same
order O(∆) as the variance of the eﬃcient price process. Thus log returns computed from transaction
prices sampled close together are not subject to a lot of noise (O(∆) vs. O(1)). Figure 4 shows the
shape of the curve for two values of T , one too small for a minimum to occur, one large enough.
   As for the rest of the analysis of the paper, the covariance matrix of the log-returns, γ 2 V in (4.2),
should be replaced by the matrix whose diagonal elements are
                                                         ¡
                                                        2 1 − e−b∆
                                                                   ¢
                            £ 2¤   £ 2¤   £ 2¤         c
                        V ar Yi = E wi + E ui = σ2 ∆ +
                                                             b

                                                   21
and oﬀ-diagonal elements i > j are:

           cov (Yi , Yj ) = E [Yi Yj ] = E [(wi + ui ) (wj + uj )]
                                           £¡             ¢¡           ¢¤
                          = E [ui uj ] = E Uτ i − Uτ i−1 Uτ j − Uτ j−1
                              £         ¤      £         ¤      £       ¤  £              ¤
                          = E Uτ i Uτ j − E Uτ i Uτ j−1 − E Uτ i−1 Uτ j + E Uτ i−1 Uτ j−1
                                ¡            ¢2
                              c2 1 − e−b∆ e−b∆(i−j−1)
                          = −
                                            2b
Note that the theorems in the previous sections do not apply to this new situation because, hav-
ing modified the matrix γ 2 V, the artificial “normal” distribution that assumes iid U 0 s that are
N(0, α2 ) would no longer use the correct second moment structure of the data. Thus the analysis
would have to be repeated for this new scenario.


10.     Noise Correlated with the Price Process

   We have assumed so far that the U process was uncorrelated with the W process. Microstructure
noise attributable to informational eﬀects is likely to be correlated with the eﬃcient price process,
since it is generated by the response of market participants to information signals (i.e., to the eﬃcient
price process). This would be the case for instance in the bid-ask model with adverse selection of
Glosten (1987). When the U process is no longer uncorrelated from the W process, the form of the
variance matrix of the observed log-returns Y must be altered, replacing γ 2 vij in (4.2) with
                              ¡           ¢                   ¡              ¢
       cov(Yi , Yj ) = cov(σ Wτ i − Wτ i−1 + Uτ i − Uτ i−1 , σ Wτ j − Wτ j−1 + Uτ j − Uτ j−1 )
                                       ¡             ¢
                     = σ2 ∆δ ij + cov(σ Wτ i − Wτ i−1 , Uτ j − Uτ j−1 )
                                ¡            ¢
                       +cov(σ Wτ j − Wτ j−1 , Uτ i − Uτ i−1 ) + cov(Uτ i − Uτ i−1 , Uτ j − Uτ j−1 )

where δ ij is the Kronecker symbol.
   The small sample properties of the misspecified MLE for σ 2 , including its RMSE, can be obtained
from
                                        N
                         £ ¤          1 X £ 2¤
                        E σ̂2 =           E Yi
                                      T
                                        i=1
                                         XN                  N i−1
                         £ ¤          1              £ 2¤  2 XX       ¡         ¢
                     V ar σ̂2 =                  V ar Yi + 2       cov Yi2 , Yj2 .
                                      T2                  T
                                           i=1                 i=1 j=1

Specific expressions for all these quantities depend upon the assumptions of the particular structural
model under consideration: for instance, in the Glosten (1987) model (see his Proposition 6), the U 0 s

                                                      22
remain stationary, the transaction noise Uτ i is uncorrelated with the return noise during the previous
                                                                    ¡             ¢
observation period, i.e., Uτ i−1 − Uτ i−2 , and the eﬃcient return σ Wτ i − Wτ i−1 is also uncorrelated
with the transaction noises Uτ i+1 and Uτ i−2 .
   With these in hand, the analysis can then proceed as above. The same caveat as in serially
correlated U case applies: having modified the matrix γ 2 V, the artificial “normal” distribution would
no longer use the correct second moment structure of the data. Thus the theorems should be modified
accordingly.


11.     Conclusions

   Our first finding in the paper is that there are situations where the presence of market microstruc-
ture noise makes it optimal to sample less often than would otherwise be the case in the absence
of noise. We then addressed the issue of what to do about it, and showed that modelling the noise
term explicitly restores the first order statistical eﬀect that sampling as often as possible is optimal.
We also demonstrated that this is true even if one misspecifies the assumed distribution of the noise
term. If the econometrician assumes that the noise terms are normally distributed when in fact they
are not, not only is it still optimal to sample as often as possible (unlike the result when no allowance
is made for the presence of noise), but the estimator has the same asymptotic variance as if the noise
distribution had been correctly specified.
   We purposefully adopted the simplest possible setup to demonstrate that our results are not driven
by the complexity of the model, but rather are likely to be genuine features facing the econometrics
of high frequency data. Our robustness results suggest that attempts to incorporate the market
microstructure noise when estimating continuous-time models based on high frequency data should
have beneficial eﬀects.




                                                   23
Appendix A:                  Proof of Proposition 1
      The estimator (2.2) has the following mean
                                         N         ¡         ¢
                                £ 2¤  1 X £ 2 ¤ N σ2 ∆ + 2a2          2a2
                               E σ̂ =       E Yi =             = σ2 +     .                                (A.1)
                                      T i=1          T                 ∆

Given the form of the bias, one would in fact want to select the largest ∆ possible to minimize the bias (as
opposed to the smallest one as in the no-noise case of Section 2).
   The estimator’s variance is
                                    "N        #      ÃN                 N
                                                                                        !
                     £ 2¤     1       X           1 X          £ 2¤    X      ¡ 2 2 ¢
                                            2
                V ar σ̂ = 2 V ar          Yi = 2           V ar Yi + 2     cov Yi , Yi−1 .
                             T        i=1
                                                 T     i=1             i=2
                                               £ ¤
Since the Yi0 s are normal with mean zero, V ar Yi2 = 2V ar[Yi ]2 = 2γ 4 (1 + η2 )2 and
                      ¡           ¢
                   cov Yi2 , Yi−1
                               2
                                    = cov(ε2i + 2ηεi εi−1 + η2 ε2i−1 , ε2i−1 + 2ηεi−1 εi−2 + η 2 ε2i−2 )
                                              £     ¤
                                    = η2 V ar ε2i−1
                                     = 2η2 γ 4

so that
                                 £ ¤           1 ©                                    ª
                             V ar σ̂ 2 =        2
                                                   2N γ 4 (1 + η2 )2 + 4(N − 1)η2 γ 4
                                              T
                                               1 ©        ¡              ¢ª    1 ©      ª
                                         =      2
                                                   2N γ 4 1 + 4η2 + η4 − 2 4η2 γ 4
                                              T ¡                 ¢           T
                                              2γ 4 1 + 4η2 + η4       4η2 γ 4
                                         =                          −
                                                ¡ 4 2T ∆ 2 2            T¢2
                                              2 σ ∆ + 4σ ∆a + 6a4             4a4
                                         =                                  − 2                            (A.2)
                                                           T∆                 T
                                                                                                   £ ¤
Note that these are exact small sample expressions, valid for all (T, ∆). Asymptotically in T, V ar σ̂ 2 → 0,
and hence the RMSE of the estimator is dominated by the bias term.
   In finite samples, the expression for the RMSE given in (3.6) follows from those for the expected value
and variance(A.1) and (A.2). The optimal value ∆∗ of the sampling interval given in (3.7) is obtained by
minimizing the RMSE (3.6) over ∆.


Appendix B:                  Proof of Proposition 2
      The partial derivatives of the log-likelihood function (4.1) have the form

                                                  1 ∂ ln det(V )   1   ∂V −1
                                        l̇η = −                  − 2Y0       Y,                            (B.1)
                                                  2      ∂η       2γ    ∂η
and
                                                           N    1 0 −1
                                              l̇γ 2 = −       +     Y V Y.                                 (B.2)
                                                          2γ 2 2γ 4
so that the MLE for γ 2 is
                                                              1 0 −1
                                                     γ̂ 2 =     Y V Y.                                     (B.3)
                                                              N


                                                               24
                                                                                 h i     h i
     At the true parameters, the expected value of the score vector is zero: E l˙η = E l˙γ 2 = 0. Hence it
follows from (B.1) that
                    ·          ¸                             ¡                            ¢
                        ∂V −1          ∂ ln det(V )        2η 1 − (1 + N) η2N + N η2(1+N)
                  E Y0        Y = −γ 2              = −γ 2               ¡             ¢
                         ∂η                 ∂η                  (1 − η2 ) 1 − η 2(1+N)
thus as N → ∞                            ·           ¸
                                                 −1
                                            0 ∂V           2ηγ 2
                                        E Y         Y =−            + o(1).
                                               ∂η        (1 − η 2 )
Similarly, it follows from (B.2) that
                                                 £          ¤
                                                E Y 0 V −1 Y = N γ 2 .
    Turning now to Fisher’s information, we have
                                 h         i    N   1 £           ¤  N
                               E −¨lγ 2 γ 2 = − 4 + 6 E Y 0 V −1 Y = 4 ,                                    (B.4)
                                               2γ  γ                2γ
whence the asymptotic variance of T 1/2 (γ̂ 2 − γ 2 ) is 2γ 4 ∆. We also have that
                           h       i            ·              ¸
                                          1           ∂V −1               η
                         E −¨lγ 2 η = 4 E Y 0                 Y =− 2             + o(1),                    (B.5)
                                        2γ              ∂η           γ (1 − η2 )
whence the asymptotic
                h     covariance
                      i          of T 1/2 (γ̂ 2 − γ 2 ) and T 1/2 (η̂ − η) is zero.
   To evaluate E −¨lηη , we compute
                              h     i 1 ∂ 2 ln det(V )                 ·     2 −1
                                                                                      ¸
                                                                 1        0∂ V
                            E −¨lηη =                       +        E  Y           Y                       (B.6)
                                          2       ∂η2          2γ 2           ∂η2
and evaluate both terms. For the first term in (B.6), we have from (4.3):
                                                   ( ¡                ¡       ¢¢ ¡        ¢
                 ∂ 2 ln det(V )            1         2 1 + η2 + η2+2N 1 − 3η2     1 − η2N
                                =                2
                       ∂η2          (1 − η2+2N )                   (1 − η2 )2
                                                            ¡          ¢           ª
                                                   − 2N η2N 3 + η2+2N − 4N 2 η2N
                                      ¡       ¢
                                    2 1 + η2
                                =               + o(1)                                                      (B.7)
                                     (1 − η2 )2
For the second term, we have for any non-random N × N matrix Q:

                        E [Y 0 QY ] = E [T r [Y 0 QY ]] = E [T r [QY Y 0 ]] = T r [E [QY Y 0 ]]
                                                             £       ¤
                                    = T r [QE [Y Y 0 ]] = T r Qγ 2 V = γ 2 T r [QV ]

where T r denotes the matrix trace, which satisfies T r[AB] = T r[BA]. Therefore
                                                                                  
               ·    2 −1
                           ¸             · 2 −1 ¸             XN X  N
                  0∂ V               2      ∂ V            2          ∂ 2 vij 
             E Y         Y     = γ Tr              V =γ                        vij
                     ∂η2                     ∂η2              i=1 j=1
                                                                        ∂η2
                                       ÃN                                                             !
                                     2
                                        X ∂ 2 vii ¡
                                                         2
                                                           ¢ N−1
                                                               X ∂ 2 vi,i+1         XN
                                                                                        ∂ 2 v i,i−1
                               = γ                  1+η +                      η+                   η
                                        i=1
                                              ∂η2              i=1
                                                                      ∂η2           i=2
                                                                                           ∂η2
                                                   (    ¡                     ¡         ¢¢ ¡           ¢
                                         γ2            4 1 + 2η 2 + η 2+2N 1 − 4η2           1 − η 2N
                               =                 2  −                              2
                                    (1 − η 2+2N )                       (1 − η2 )
                                           ¡        ¡                                 ¢¢                )
                                       2N 1 + η2N 6 − 6η 2 + 2η2+2N − 3η4+2N                      2 2N
                                    +                                                     + 8N η
                                                          (1 − η2 )
                                         2γ 2 N
                                   =               + o(N )                                                  (B.8)
                                        (1 − η 2 )

                                                             25
   Combining (B.7) and (B.8) into (B.6), it follows that
                 h    i 1 ∂ 2 ln det(V )              ·     2 −1
                                                                   ¸
                                       N        1        0 ∂ VN              N
               E −¨lηη =                    +       E  Y         Y     ∼             + o(N)                             (B.9)
                           2      ∂η2          2γ 2         ∂η2      N−→∞ (1 − η 2 )

In light of that and (B.5), the asymptotic variance of T 1/2 (η̂ − η) is the same as in the γ 2 known case, that
is, (1 − η2 )∆ (which of course confirms the result of Durbin (1959) for this parameter).
     We can now retrieve the asymptotic covariance matrix for the original parameters (σ2 , a2 ) from that of
the parameters (γ 2 , η). This follows from the delta method applied to the change of variable (3.2)-(3.3):
                                    Ã     !               Ã                  !
                                      σ2          2         ∆−1 γ 2 (1 + η)2
                                            = f (γ , η) =                      .                         (B.10)
                                      a2                         −γ 2 η
Hence                                  ÃÃ          !       Ã         !!
                                            σ̂ 2               σ2                    ¡                    ¢
                             T   1/2
                                                       −                      →     N 0, AV AR(σ̂2 , â2 )
                                            â2                a2          T −→∞

where
          AV AR(σ̂ 2 , â2 ) = ∇f (γ 2 , η).AV AR(γ̂ 2 , η̂).∇f (γ 2 , η)0
                               Ã                        !Ã                          !Ã                       !
                                  (1+η)2     2γ 2 (1+η)          4                        (1+η)2
                                    ∆             ∆
                                                              2γ   ∆         0                ∆       −η
                             =                                                           2γ 2 (1+η)
                                    −η         −γ 2              0       (1 − η2 )∆           ∆       −γ 2
                               Ã p                                                                     !
                                 4 σ6 ∆ (4a2 + σ2 ∆) + 2σ4 ∆                  −σ 2 ∆h(∆, σ 2 , a2 )
                             =                                            ∆
                                                                            ¡ 2     2
                                                                                       ¢          2 2
                                                                                                           .
                                                    •                      2 2a + σ ∆ h(∆, σ , a )


Appendix C:                Proof of Theorem 1
                                                 £ ¤       £     ¤
    Consider two generic symmetric N × N matrices ν i,j and ω i,j We are interested in expressions of the
form
                  X                                        N−1
                                                           X                  X
                           (−1)s ν i,j ω k,l       =                                      (−1)s ν i,j ω k,l             (C.1)
             i,j,k,l:M=m+1                                  h=1 i,j,k,l:m=h,M =h+1
                                                           N−1
                                                           XX  3                    X
                                                   =                                                (−1)r ν i,j ω k,l
                                                            h=1 r=1 i,j,k,l:m=h,M =h+1,s=r
                                                           N−1
                                                           X       ©
                                                   =                −2ν h,h+1 ωh+1,h+1 − 2ν h+1,h+1 ω h,h+1
                                                            h=1
                                                               +ν h,h ω h+1,h+1 + ν h+1,h+1 ω h,h + 4ν h,h+1 ω h,h+1
                                                                                               ª
                                                               −2ν h+1,h ωh,h − 2ν h,h ω h+1,h

   It follows that if we set
                                                         N
                                                         X
                                       Υ(ν, ω) =                   ν i,j ω k,l Cumtrue (Yi , Yj , Yk , Yl )             (C.2)
                                                       i,j,k,l=1

then Υ(ν, ω) = Cum4 (U ) Ψ(ν, ω) where
                                        N
                                        X                      N−1
                                                               X       ©
                  ψ(ν, ω) = 2                 ν h,h ω h,h +             −2ν h,h+1 ω h+1,h+1 − 2ν h+1,h+1 ωh,h+1
                                        h=1                    h=1
                                                   +ν h,h ωh+1,h+1 + ν h+1,h+1 ωh,h + 4ν h,h+1 ωh,h+1                   (C.3)
                                                                                    ª
                                                   −2ν h+1,h ω h,h − 2ν h,h ω h+1,h

                                                                         26
                     £ ¤        £   ¤
If the two matrices ν i,j and ωi,j satisfy the following reversibility property: ν N+1−i,N+1−j = ν i,j and
ω N+1−i,N+1−j = ω i,j (so long as one is within the index set), then (C.3) simplifies to:
                                           N
                                           X                     N−1
                                                                 X     ©
                         ψ(ν, ω) = 2             ν h,h ω h,h +          −4ν h,h+1 ω h+1,h+1 − 4ν h+1,h+1 ωh,h+1
                                           h=1                   h=1
                                                                                                  ª
                                                           +2ν h,h ωh+1,h+1 + 4ν h,h+1 ω h,h+1

This is the case for V −1 and its derivative ∂V −1 /∂η, as can be seen from the expression for v i,j given in (4.4),
and consequently for ∂vi,j /∂η.
    If we wish to compute the sums in equations (5.4), (5.5), and (5.6), therefore, we need, respectively, to
find the three quantities ψ(∂v/∂η, v), ψ(∂v/∂η, ∂v/∂η), and ψ(v, v) respectively. All are of order O(N), and
only the first term is needed. Replacing the terms v i,j and ∂vi,j /∂η by their expressions from (4.4), we obtain:
                                            2                      n         ¡         ¢³                               ´
                                                                                    2N
     ψ(v, v) =                              3¡                   ¢2 − (1 + η) 1 − η      1 + 2η 2 + 2η2(1+N) + η 2(2+N)
                         (1 + η2 ) (1 − η) 1 − η 2(1+N)
                                                   ¡     ¢³                                     ´o
                                       + N (1 − η) 1 + η2 2 + η 2N + η2(1+N) + 6η 1+2N + 2η2+4N
                             4N
                 =                    + o(N )                                                                               (C.4)
                         (1 − η)2

                     µ           ¶          ¡                ¡       ¢ ¡                    ¢          ¢
                         ∂v               2 O(1) + 2N (1 − η) 1 + η2 η 1 + η2 + O(η 2N ) + N 2 O(η 2N )
                 ψ          ,v        =                                     ¡            ¢3
                         ∂η                             η(1 − η)4 (1 + η2 )2 1 − η2(1+N)
                                             4N
                                      =            + o(N)                                                                   (C.5)
                                          (1 − η)3

                                       ³           ¡       ¢ ³¡         ¢2             ´               ´
           µ             ¶           4 O(1) + 3N 1 − η4 η2 1 + η2 + O(η2N ) + N 2 O(η2N ) + N 3 O(η2N )
               ∂v ∂v
       ψ         ,           =                                                       ¡            ¢4
               ∂η ∂η                                  3η2 (1 + η) (1 + η2 )3 (1 − η)5 1 − η2(1+N)
                                        4N
                             =                + o(N )                                                    (C.6)
                                     (1 − η)4

     The asymptotic variance of the estimator (γ̂ 2 , η̂) obtained by maximizing the (incorrectly-specified) log-
likelihood (4.1) that assumes Gaussianity of the U 0 s is given by
                                                                           ¡         ¢−1
                                                 AV ARtrue (γ̂ 2 , η̂) = ∆ D0 S −1 D

where, from (B.4), (B.5) and (B.9) we have
                                                1         hi       1      hi  1       h i
                                 D    = D0 = − Etrue ¨     l = − Enormal ¨ l = Enormal l˙l˙0
                                        Ã       N                 N           N
                                            1            η
                                                                    ¡1¢ !
                                           2γ 4  − Nγ 2 (1−η2 ) + o  N
                                      =                  1
                                                                                                                            (C.7)
                                            •       (1−η ) 2 +  o(1)

and, in light of (5.4), (5.5), and (5.6),
                                     1      h i       1     hi
                             S=        Etrue l˙l̇0 = − Etrue ¨l + Cum4 (U ) Ψ = D + Cum4 (U ) Ψ                             (C.8)
                                     N                N




                                                                        27
where
                                                                      ³        ´ 
                                                   1             1       ∂v
                                          1  γ 8 ψ (v, v) − γ 6 ψ          ,
                                                                     ³ ∂η ´ 
                                                                              v
                                     Ψ =
                                         4N            •       1
                                                                 ψ     ∂v ∂v
                                                              γ4       ∂η , ∂η
                                         Ã       1                  −1
                                                                                    !
                                           γ 8 (1−η)2
                                                       + o(1) γ 6 (1−η)  3 + o(1)
                                       =                             1
                                                                                                                                          (C.9)
                                                     •         γ 4 (1−η)4
                                                                            + o(1)

from the expressions just computed. It follows that
                                                   ³                        ´−1
                                                                        −1
                      AV ARtrue (γ̂ 2 , η̂) = ∆      D (D + Cum4 (U ) Ψ) D
                                                   ³ ¡                     ¢−1 ´−1
                                               = ∆ D Id + Cum4 (U ) D−1 Ψ
                                                   ¡                    ¢
                                               = ∆ Id + Cum4 (U ) D−1 Ψ D−1
                                                   ¡                    ¢
                                               = ∆ Id + Cum4 (U ) D−1 Ψ D−1
                                               = AV ARnormal (γ̂ 2 , η̂) + ∆ Cum4 (U ) D−1 ΨD−1

where Id denotes the identity matrix and
                                   Ã                   !                                           Ã          4      −2(1+η)          !
                          2          2γ 4 ∆     0                               −1        −1               (1−η)2    γ 2 (1−η)2
           AV ARnormal (γ̂ , η̂) =                       ,                  D        ΨD        =                      (1+η)2
                                       0    (1 − η2 )∆                                                       •       γ 4 (1−η)2

so that                                    Ã                       !                       Ã                                  !
                                                                                                      4          −2(1+η)
                             2                 2γ 4       0                                        (1−η)2        γ 2 (1−η)2
               AV ARtrue (γ̂ , η̂) = ∆                                  + ∆Cum4 (U )                              (1+η)2          .
                                                0      (1 − η2 )                                       •         γ 4 (1−η)2

By applying the delta method as in the previous section, we now recover the asymptotic variance of the
estimates of the original parameters

   AV ARtrue (σ̂2 , â2 ) = ∇f(γ 2 , η).AV ARtrue (γ̂ 2 , η̂).∇f (γ 2 , η)0
                            Ã p                                                                            !
                              4 σ 6 ∆ (4a2 + σ 2 ∆) + 2σ4 ∆                     −σ2 ∆h(∆, σ2 , a2 )
                          =                                           ∆
                                                                         ¡ 2   2
                                                                                 ¢      2 2
                                                                                                             .
                                             •                         2 2a + σ ∆ h(∆, σ , a ) + ∆Cum4 (U)


Appendix D:                 Derivations for Section 6
    To see (6.4), let “orig” (D.7) denote parametrization in (and diﬀerentiation with respect to) the original
parameters σ2 and a2 , while “transf” denotes parametrization and diﬀerentiation in γ 2 and η, and finv denotes
the inverse of the change of variable function defined in (B.10), namely
                  Ã      !                           n              p                     o 
                                                    1    2     2         2 ∆ (4a2 + σ 2 ∆)
                      γ2                            2  2a  + σ   ∆ +   σ
                            = finv (σ2 , α2 ) =  1 n                 p                     o .          (D.1)
                      η                                    2     2
                                                  2a2 −2a − σ ∆ +        σ2 ∆ (4a2 + σ 2 ∆)

and ∇finv its Jacobian matrix. Then, from l̇orig = ∇finv (σ2 , α2 )0 .l̇transf , we have

                                 ¨lorig = ∇finv (σ2 , α2 )0 .¨ltransf .∇finv (σ 2 , α2 ) + H[l̇transf ]




                                                                   28
where H[l˙transf ] is a 2 × 2 matrix whose terms are linear in l̇transf and the second partial derivatives of finv .
Now Etrue [l̇orig ] = Etrue [l̇transf ] = 0, and so Etrue [H[l̇transf ]] = 0 from which it follows that

                         Dorig      = N −1 Etrue [−¨
                                                   lorig ]
                                    = ∇finv (σ 2 , α2 )0 .Dtransf .∇finv (σ 2 , α2 )
                                       1/2 2 2                                                    
                                         ∆     (2a +σ ∆)                        ∆1/2
                                       2σ3 (4a2 +σ2 ∆)3/2           µ σ(4a21/2 +σ2 ∆)3/2        ¶  + o(1)
                                    =                                       ∆     σ(6a2 +σ 2 ∆)                                 (D.2)
                                                                   1
                                                   •             2a4   1 −      (4a2 +σ 2 ∆)3/2


with Dtransf = N −1 Etrue [−¨ltransf ] given in (C.7). Similarly, l̇orig l̇orig
                                                                           0
                                                                                = ∇finv (σ2 , α2 )0 .l̇transf l˙transf
                                                                                                                0
                                                                                                                       .∇finv (σ 2 , α2 )
and so

                            Sorig     = ∇finv (σ2 , α2 )0 .Stransf .∇finv (σ2 , α2 )
                                      = ∇finv (σ2 , α2 )0 .(Dtransf + Cum4 (U ) Ψ).∇finv (σ 2 , α2 )
                                      = Dorig + Cum4 (U )∇finv (σ 2 , α2 )0 .Ψ.∇finv (σ 2 , α2 )                                  (D.3)

with the second equality following from the expression for Stransf given in (C.8).
    To complete the calculation, note from (C.9) that
                                                                    0
                                                      Ψ = gtransf .gtransf + o(1),

where                                                         Ã                          !
                                                                  γ −4 (1 − η)−1
                                                 gtransf =                                   .
                                                                  −γ −2 (1 − η)−2
Thus
                                      ∇finv (σ2 , α2 )0 .Ψ.∇finv (σ2 , α2 ) = gorig .gorig
                                                                                      0
                                                                                           + o(1),                                (D.4)

where                                                                                                          
                                                                                             ∆1/2
                                                                                  µ  σ(4a2 +σ 2 ∆)3/2         ¶ 
                          g = gorig = ∇finv (σ2 , α2 )0 .gtransf =           1           ∆1/2 σ(6a2 +σ2 ∆)                      (D.5)
                                                                             2a4    1−       (4a2 +σ 2 ∆)3/2

which is the result (6.5). Inserting (D.4) into (D.3) yields the result (6.4).
   For the profile likelihood λ, let â2σ2 denote the maximizer of l(σ2 , a2 ) for given σ 2 . Thus by definition
λ(σ ) = l(σ 2 , â2σ2 ). From now on, all diﬀerentiation takes place with respect to the original parameters, and
   2

we will omit the subscript “orig” in what follows. Since 0 = l˙a2 (σ2 , â2σ2 ), it follows that

                                                     ∂ ˙
                                          0 =            la2 (σ2 , â2σ2 )
                                                    ∂σ 2
                                                                                                     2
                                                                                              ∂â 2
                                              = ¨lσ2 a2 (σ 2 , â2σ2 ) + ¨la2 a2 (σ2 , â2σ2 ) σ2 ,
                                                                                              ∂σ
so that
                                                       ∂â2σ2     ¨
                                                                  lσ2 a2 (σ2 , â2σ2 )
                                                              = −                                                                 (D.6)
                                                       ∂σ2        ¨
                                                                  la2 a2 (σ2 , â2σ2 )
    The profile score then follows
                                                                                                 2
                                                                                             ∂â 2
                                            λ̇(σ2 ) = l˙σ2 (σ2 , â2σ2 ) + l̇a2 (σ2 , â2σ2 ) σ2                                  (D.7)
                                                                                             ∂σ


                                                                     29
so that at the true value of (σ2 , a2 ),

                                                                 Etrue [¨lσ2 a2 ] ˙
                                   λ̇(σ 2 ) = l˙σ2 (σ 2 , a2 ) −                  la2 (σ2 , a2 ) + Op (1),                  (D.8)
                                                                 Etrue [¨la2 a2 ]

since â2 = a2 + Op (N −1/2 ) and

                          ∆¨lσ2 a2         ≡ N −1 ¨lσ2 a2 (σ 2 , â2σ2 ) − N −1 Etrue [¨lσ2 a2 ] = Op (N −1/2 )
                          ∆¨la2 a2         ≡ N −1 ¨la2 a2 (σ2 , â2 2 ) − N −1 Etrue [¨
                                                                    σ                  la2 a2 ] = Op (N −1/2 )

as sums of random variables with expected value zero, so that

                                  ∂â2σ2           N −1¨lσ2 a2 (σ 2 , â2σ2 )
                              −              =
                                  ∂σ2              N −1¨la2 a2 (σ2 , â2 2 )
                                                                        σ
                                                   N −1 Etrue [¨  lσ2 a2 ] + ∆¨
                                                                              lσ2 a2
                                             =
                                                   N Etrue [la2 a2 ] + ∆¨
                                                     −1           ¨           la2 a2
                                                           ¨
                                                   Etrue [lσ2 a2 ]     ³                 ´
                                             =                      + ∆¨lσ2 a2 − ∆¨la2 a2 + op (N −1/2 )
                                                   Etrue [¨la2 a2 ]
                                                   Etrue [l¨σ2 a2 ]
                                             =                      + Op (N −1/2 )
                                                   Etrue [¨la2 a2 ]

while
                                                         l̇a2 (σ 2 , a2 ) = Op (N 1/2 )

also as a sum of random variables with expected value zero.
    Therefore
                                                                            Etrue [¨
                                                                                   lσ2 a2 ]       h               i
                    Etrue [λ̇(σ2 )] = Etrue [l˙σ2 (σ 2 , a2 )] −                             Etrue l˙a2 (σ2 , a2 ) + O(1)
                                                                            Etrue [¨la2 a2 ]
                                      = O(1)
                                     h               i
since Etrue [l̇σ2 (σ2 , a2 )] = Etrue l˙a2 (σ2 , a2 ) = 0. In particular, Etrue [λ̇(σ 2 )] = o(N) as claimed.
    Further diﬀerentiating (D.7), one obtains
                                                                                        µ        ¶2
                                                                                          ∂â2σ2
                          λ̈(σ 2 ) = ¨lσ2 σ2 (σ2 , â2σ2 ) + ¨
                                                             la2 a2 (σ 2 , â2σ2 )
                                                                                          ∂σ 2
                                                                   ∂â2 2                       ∂ 2 â2 2
                                          +2¨lσ2 a2 (σ 2 , â2σ2 ) σ2 + l˙a2 (σ2 , â2σ2 ) 2 σ2
                                                                    ∂σ                           ∂ σ
                                                                  ¨lσ2 a2 (σ2 , â2 2 )2                        2 2
                                       = ¨lσ2 σ2 (σ2 , â2σ2 ) −                   σ
                                                                                          + l˙a2 (σ2 , â2 2 ) ∂ âσ2
                                                                    ¨la2 a2 (σ2 , â2 2 )                 σ
                                                                                    σ
                                                                                                               ∂ 2σ2

from (D.6). Evaluated at σ2 = σ̂ 2 , one gets â2σ2 = â2 and l̇a2 (σ̂ 2 , â2 ) = 0, and so

                                                                              ¨l 2 2 (σ̂2 , â2 )2
                                                        lσ2 σ2 (σ̂2 , â2 ) − σ a
                                             λ̈(σ̂2 ) = ¨
                                                                                ¨la2 a2 (σ̂2 , â2 )
                                                                       1
                                                      = h                   i                                               (D.9)
                                                          ¨l(σ̂ 2 , â2 )−1
                                                                                σ2 σ2
     h              i
      ¨ 2 , â2 )−1
where l(σ̂                    is the upper left element of the matrix ¨
                                                                      l(σ̂ 2 , â2 )−1 . Thus (6.7) is valid.
                      σ2 σ2


                                                                        30
   Alternatively, we can see that the profile likelihood λ satisfies the Bartlett identity to first order, i.e., (6.8).
Note that by (D.8),
                                                Ã                                                              !2 
                                                                        E     [¨
                                                                               l  2
                                                                          true σ a ˙ 2 ]
           N −1 Etrue [λ̇(σ2 )2 ] = N −1 Etrue  l˙σ2 (σ 2 , a2 ) −                      la2 (σ2 , a2 ) + Op (1) 
                                                                        Etrue [¨la2 a2 ]
                                                Ã                                                     !2 
                                                                        E     [¨
                                                                               l  2
                                                                          true σ a ˙ 2 ]
                                  = N −1 Etrue  l˙σ2 (σ 2 , a2 ) −                      la2 (σ2 , a2 )  + o(1)
                                                                        Etrue [¨la2 a2 ]
                                                                      Ã                                !2
                                      −1        ˙       2 2 2           Etrue [¨lσ2 a2 ]        2 2
                                  = N Etrue lσ2 (σ , a ) +                                l̇a2 (σ , a )
                                                                         Etrue [¨la2 a2 ]
                                                                                              #
                                                Etrue [¨lσ2 a2 ] ˙     2 2           2 2
                                           −2                    la2 (σ , a )l̇σ2 (σ , a ) + o(1)
                                                Etrue [¨la2 a2 ]

so that
                                                  µ      ¶2
                                                  Dσ2 a2              D 2 2
             N   −1            2 2
                      Etrue [λ̇(σ ) ] = Sσ2 σ2 +            Sa2 a2 − 2 σ a Sa2 σ2 + op (1)
                                                  Da a
                                                    2  2              Da2 a2
                                        Ã         µ        ¶2                          !
                                                    Dσ2 a2               Dσ2 a2
                                      =  Dσ2 σ2 +             Da2 a2 − 2        D 2 2
                                                    Da2 a2               Da2 a2 a σ
                                                   Ã        µ        ¶2                       !
                                                       2      Dσ2 a2     2      Dσ2 a2
                                        +Cum4 (U) gσ2 +                 ga2 − 2        gσ2 ga2 + op (1)
                                                              Da2 a2            Da2 a2

by invoking (6.4).
    Continuing the calculation,
                                         µ                  ¶            µ                ¶2
                  −1            2 2                Dσ2 2 a2                     Dσ2 a2
              N        Etrue [λ̇(σ ) ] =  Dσ2 σ2 −            + Cum4 (U ) gσ2 −        g 2 + o(1)
                                                   Da2 a2                       Da2 a2 a
                                           £ −1 ¤
                                       = 1/ D σ2 σ2 + o(1)                                                     (D.10)

since from the expressions for Dorig and gorig in (D.2) and (D.5) we have
                                                         Dσ2 a2
                                                 gσ2 −          g 2 = 0.                                       (D.11)
                                                         Da2 a2 a
Then by (D.9) and the law of large numbers, we have
                                                                 £   ¤
                                      N −1 Etrue [λ̈(σ 2 )] = −1/ D−1 σ2 σ2 + o(1),                            (D.12)

and (6.8) follows from combining (D.10) with (D.12).


Appendix E:                   Proof of Lemma 1
    ΣΣ−1 ≡ Id implies that
                                                ∂Σ−1         ∂Σ −1
                                                      = −Σ−1      Σ                                              (E.1)
                                                 ∂β 1        ∂β 1
and, since Σ is linear in the parameters σ 2 and a2 (see (7.2)) we have

                                                       ∂ 2Σ
                                                                =0                                               (E.2)
                                                      ∂β 2 ∂β 1

                                                           31
so that
                                          ¶ µ
                ∂ 2 Σ−1             ∂Σ−1
                                      ∂
                            =
                ∂β 2 ∂β 1            ∂β 2
                                     ∂β 1
                                  ∂Σ −1 ∂Σ −1          ∂Σ −1 ∂Σ −1          ∂2Σ
                            = Σ−1      Σ       Σ + Σ−1      Σ      Σ − Σ−1           Σ−1
                                  ∂β 2    ∂β 1         ∂β 1   ∂β 2         ∂β 1 ∂β 2
                                  ∂Σ −1 ∂Σ −1          ∂Σ −1 ∂Σ −1
                            = Σ−1      Σ       Σ + Σ−1      Σ      Σ                                   (E.3)
                                  ∂β 2    ∂β 1         ∂β 1   ∂β 2

    In the rest of this lemma, let expectations be conditional on the ∆0 s. We use the notation E[ ·| ∆] as a
shortcut for E[ ·| ∆N , ..., ∆1 ]. At the true value of the parameter vector, we have,
                                                   ¯
                                                   ¯
                                     0 = E[ l̇β 1 ¯ ∆]
                                                                 ·           ¯ ¸
                                               1 ∂ ln det Σ 1        ∂Σ−1 ¯¯
                                         = −               − E Y0          Y¯∆ .                        (E.4)
                                               2 ∂β 1         2       ∂β 1

with the second equality following from (7.3). Then, for any nonrandom Q, we have

                                            E [Y 0 QY ] = T r [QE [Y Y 0 ]] = T r [QΣ] .               (E.5)

This can be applied to Q that depends on the ∆0 s, even when they are random, because the expected value
is conditional on the ∆0 s. Therefore it follows from (E.4) that
                                      ·             ¯ ¸        · −1 ¸         ·         ¸
                     ∂ ln det Σ             ∂Σ−1 ¯¯              ∂Σ                ∂Σ
                                = −E Y 0          Y ¯ ∆ = −T r         Σ = T r Σ−1        ,        (E.6)
                        ∂β 1                 ∂β 1                 ∂β 1             ∂β 1

with the last equality following from (E.1) and so
                                                          ·         ¸
                            ∂ 2 ln det Σ           ∂            ∂Σ
                                                =      T r Σ−1
                              ∂β 2 ∂β 1           ∂β 2         ∂β 1
                                                     ·      µ          ¶¸
                                                        ∂          ∂Σ
                                                = Tr         Σ−1
                                                       ∂β 2       ∂β 1
                                                       ·                                    ¸
                                                              ∂Σ −1 ∂Σ             ∂ 2Σ
                                                = −T r Σ−1        Σ         + Σ−1
                                                             ∂β 2      ∂β 1       ∂β 2 ∂β 1
                                                       ·                    ¸
                                                              ∂Σ −1 ∂Σ
                                                = −T r Σ−1        Σ           ,                        (E.7)
                                                             ∂β 2      ∂β 1

again because of (E.2).
    In light of (7.3), the expected information (conditional on the ∆0 s) is given by
                               h          ¯ i 1 ∂ 2 ln det Σ 1 · ∂ 2 Σ−1 ¯¯ ¸
                                          ¯
                                  ¨
                             E − lβ 2 β 1 ¯ ∆ =             + E Y0            Y ¯∆ .
                                                2 ∂β 2 β 1    2      ∂β 2 β 1 ¯

Then,
                        ·         ¯ ¸      · 2 −1 ¸
                           Σ−1 ¯¯
                            0∂
                                 2
                                            ∂ Σ
                     E Y         Y ∆  = Tr         Σ
                         ∂β 2 β 1 ¯          ∂β β
                                           · 2 1                               ¸
                                              −1 ∂Σ −1 ∂Σ        −1 ∂Σ −1 ∂Σ
                                      = Tr Σ         Σ        +Σ        Σ
                                                ∂β 2     ∂β 1      ∂β 1   ∂β 2
                                            ·                  ¸
                                                  ∂Σ −1 ∂Σ
                                      = 2T r Σ−1       Σ
                                                  ∂β 2    ∂β 1


                                                                32
with the first equality following from (E.5) applied to Q = ∂ 2 Σ−1 /∂β 2 β 1 , the second from (E.3) and the third
from the fact that T r[AB] = T r[BA]. It follows that
                      h          ¯ i            ·                   ¸      ·                     ¸
                                 ¯         1          ∂Σ −1 ∂Σ                     ∂Σ −1 ∂Σ
                   E − ¨lβ 2 β 1 ¯ ∆ = − T r Σ−1           Σ          + T r Σ−1         Σ
                                           2          ∂β 2    ∂β 1                 ∂β 2     ∂β 1
                                              ·                   ¸
                                         1          ∂Σ −1 ∂Σ
                                     =     T r Σ−1       Σ
                                         2          ∂β 2     ∂β 1
                                              2
                                           1 ∂ ln det Σ
                                     = −                 .
                                           2 ∂β 2 β 1

Appendix F:                Proof of Theorem 2
    In light of (7.2) and (7.5),
                                                       Σ = Σ0 + σ2 Ξ                                           (F.1)
from which it follows that
                                       ¡ ¡            ¢¢−1
                          Σ−1      =   Σ0 Id + σ2 Σ−1
                                                   0 Ξ
                                     ¡             ¢−1 −1
                                   = Id + σ2 Σ−1
                                               0 Ξ    Σ0
                                                                          ¡ −1 ¢2 −1
                                   = Σ−1   2 −1  −1
                                      0 − σ Σ0 ΞΣ0 +
                                                                 2 4
                                                                  σ        Σ0 Ξ Σ0 + O( 3 )                    (F.2)

since
                                                  −1                  2
                                       (Id + A)        = Id − A +         A2 + O( 3 ).
    Also,
                                               ∂Σ     ∂Σ0    ∂σ2
                                                    =      +      Ξ.
                                               ∂β 1   ∂β 1   ∂β 1
Therefore, recalling (E.6), we have
                             ·           ¸
        ∂ ln det Σ              −1 ∂Σ
                      = Tr Σ
            ∂β 1                    ∂β 1
                             ·³                                                          ´ µ ∂Σ           ¶¸
                                                               ¡ −1 ¢2 −1                       0   ∂σ2
                      = T r Σ−1   0  −    σ 2 −1
                                             Σ 0 ΞΣ −1
                                                    0  +   2 4
                                                            σ    Σ0   Ξ   Σ0   +  O(  3
                                                                                        )         +      Ξ
                                                                                             ∂β 1   ∂β 1
                             ·           ¸        ·                            2
                                                                                       ¸
                                    ∂Σ 0                           ∂Σ  0   ∂σ
                      = T r Σ−1 0           + T r −σ2 Σ−1 0 ΞΣ0
                                                                −1
                                                                         +       Σ−1 Ξ
                                    ∂β 1                           ∂β 1    ∂β 1 0
                                 ·                                                 ¸
                                      ¡       ¢2 −1 ∂Σ0        2 ∂σ
                                                                    2
                         + 2 T r σ 4 Σ−1  0  Ξ   Σ0        − σ        Σ−1
                                                                          ΞΣ −1
                                                                                 Ξ
                                                      ∂β 1       ∂β 1 0      0

                         +Op ( 3 )                                                                             (F.3)

Two things can be determined from this expansion. Since the ξ 0i s are iid with mean 0, E[Ξ] = 0, and so,
taking unconditional expectations with respect to the law of the ∆0i s, we obtain that the coeﬃcient of order
is
                                     · ·                                    ¸¸
                                              2 −1     −1 ∂Σ0    ∂σ2 −1
                                   E T r −σ Σ0 ΞΣ0             +       Σ Ξ
                                                          ∂β 1   ∂β 1 0
                                      · ·                                   ¸¸
                                              2 −1     −1 ∂Σ0    ∂σ2 −1
                               = T r E −σ Σ0 ΞΣ0               +       Σ Ξ
                                                          ∂β 1   ∂β 1 0
                                      ·                                         ¸
                                          2 −1         −1 ∂Σ0    ∂σ2 −1
                               = T r −σ Σ0 E [Ξ] Σ0            +       Σ E [Ξ]
                                                          ∂β 1   ∂β 1 0
                               = 0.

                                                            33
   Similarly, the coeﬃcient of order 2 is
                                 · ·                                                 ¸¸
                                          ¡      ¢2 −1 ∂Σ0        2 ∂σ
                                                                       2 ¡        ¢2
                               E T r σ 4 Σ−1 0 Ξ    Σ0        − σ          Σ−1
                                                                                Ξ
                                                        ∂β 1        ∂β 1 0
                                  ·    h¡                                                  ¸
                                                ¢2 i −1 ∂Σ0        2 ∂σ
                                                                        2   h¡        ¢2 i
                          = T r σ4 E Σ−1    0  Ξ    Σ 0       −  σ        E    Σ −1
                                                                                 0  Ξ
                                                         ∂β 1        ∂β 1
                                  ·    h¡           µ                          ¶¸
                                                ¢2 i 2 −1 ∂Σ0 ∂σ2
                          = T r σ2 E Σ−1    0 Ξ       σ Σ0         −       Id
                                                              ∂β 1    ∂β 1
                                  ·                    µ                         ¶¸
                                    2 −1
                                            £ −1 ¤ 2 −1 ∂Σ0 ∂σ 2
                          = T r σ Σ0 E ΞΣ0 Ξ σ Σ0                     −        Id .
                                                                ∂β 1      ∂β 1
              £       ¤
The matrix E ΞΣ−1 0 Ξ has the following terms
                                           N X
                                           X N
                              £ −1 ¤              £    ¤                 £    ¤
                               ΞΣ0 Ξ i,j =     Ξik Σ−1
                                                    0     Ξ = ∆20 ξ i ξ j Σ−1
                                                        kl lj              0   ij
                                                 k=1 l=1
            £      ¤
and since E ξ i ξ j = δ ij V ar[ξ] (where δ ij denotes the Kronecker symbol), it follows that
                                            £          ¤                       £ −1 ¤
                                          E ΞΣ−1               2
                                                  0 Ξ = ∆0 V ar[ξ] diag Σ0                                            (F.4)
           £ −1 ¤
where diag Σ0 is the diagonal matrix formed with the diagonal elements of Σ−1                 0 . From this, we obtain that
            ·             ¸          ·            ¸
              ∂ ln det Σ                    ∂Σ0
         E                    = T r Σ−1  0
                 ∂β 1                       ∂β 1
                                          ·                         µ                        ¶¸
                                                       £ −1 ¤ 2 −1 ∂Σ0 ∂σ2
                                  + 2 T r σ 2 Σ−10  E   ΞΣ  0   Ξ    σ  Σ  0       −       Id    + O( 3 )
                                                                              ∂β 1    ∂β 1
                                     ·            ¸
                                            ∂Σ0
                              = T r Σ−1  0                                                                            (F.5)
                                            ∂β 1
                                                      ·                        µ                       ¶¸
                                                                      £ −1 ¤ 2 −1 ∂Σ0 ∂σ2
                                  + 2 ∆20 V ar[ξ]T r σ 2 Σ−1 0   diag  Σ  0     σ  Σ0        −       Id     + O( 3 ).
                                                                                        ∂β 1    ∂β 1
                      h      i
    To calculate E ¨lβ 2 β 1 , in light of (7.4), we need to diﬀerentiate E [∂ ln det Σ/∂β 1 ] with respect to β 2 .
Indeed          h         i     h h           ¯ ii               · 2           ¸             µ ·              ¸¶
                    ¨                  ¨      ¯            1       ∂ ln det Σ         1 ∂          ∂ ln det Σ
             E −lβ2 β1 = E E −lβ 2 β 1 ¯ ∆ = − E                                 =−            E
                                                           2        ∂β 2 ∂β 1         2 ∂β 2          ∂β 1
where we can interchange the unconditional expectation and the diﬀerentiation with respect to β 2 because
the unconditional expectation is taken with respect to the law of the ∆0i s, which is independent of the β
parameters (i.e., σ2 and a2 ). Therefore, diﬀerentiating (F.5) with respect to β 2 will produce the result we
need. (The reader may wonder why we take the expected value before diﬀerentiating, rather than the other
way around. As just discussed, the results are identical. However, it turns out that taking expectations first
reduces the computational burden quite substantially.)
    Combining with (F.5), we therefore have
             h          i               µ ·                ¸¶
                 ¨               1 ∂            ∂ ln det Σ
          E −lβ2 β1         = −            E
                                 2 ∂β 2            ∂β 1
                                            ·            ¸
                                 1 ∂            −1 ∂Σ0
                            = −         T r Σ0
                                 2 ∂β 2             ∂β 1
                                                        µ ·                          µ                        ¶¸¶
                                 1 2 2             ∂              2 −1
                                                                              £ −1 ¤ 2 −1 ∂Σ0 ∂σ2
                               −     ∆0 V ar[ξ]          T r σ Σ0 diag Σ0              σ Σ0         −       Id
                                 2                ∂β 2                                        ∂β 1     ∂β 1
                               +O( 3 )
                          ≡ φ(0) +       2 (2)
                                          φ      + O( 3 )                                                            (F.6)

                                                            34
    It is useful now to introduce the same transformed parameters (γ 2 , η) as in previous sections and write
Σ0 = γ 2 V with the parameters and V defined as in (3.2)-(3.3) and (4.2), except that ∆ is replaced by ∆0 in
these expressions. To compute φ(0) , we start with
                              ·         ¸         "          ¡ 2 ¢#
                                −1 ∂Σ0              −2 −1 ∂ γ V
                           T r Σ0         = Tr γ V
                                   ∂β 1                      ∂β 1
                                                  ·        ¸       ·                 ¸
                                                       ∂V                       ∂γ 2
                                          = T r V −1         + T r γ −2 V −1 V
                                                      ∂β 1                      ∂β 1
                                                  ·       ¸                   2
                                                      ∂V ∂η                ∂γ
                                          = T r V −1              + N γ −2                              (F.7)
                                                       ∂η ∂β 1             ∂β 1
with ∂γ 2 /∂β 1 and ∂η/∂β 1 to be computed from (3.4)-(3.5). If Id denotes the identity matrix and J the
matrix with 1 on the infra and supra-diagonal lines and 0 everywhere else, we have V = η2 Id + ηJ, so that
∂V/∂η = 2ηId + J. Therefore
                       ·        ¸
                             ∂V            £     ¤      £     ¤
                    T r V −1       = 2ηT r V −1 + T r V −1 J
                             ∂η
                                              N
                                              X              N−1
                                                             X     © i,i−1         ª
                                       = 2η         vi,i +          v      + vi,i+1 + v1,2 + vN,N−1
                                              i=1            i=2
                                              ¡          ¡ ¡       ¢    ¢¢
                                           2η 1 − η2N N 1 − η2 + 1
                                       =                 ¡            ¢
                                                (1 − η2 ) 1 − η2(1+N)
                                              2η
                                       =             + o(1)                                                       (F.8)
                                           (1 − η2 )
    Therefore the first term in (F.7) is O(1) while the second term is O(N ) and hence
                                         ·          ¸
                                            −1 ∂Σ0             ∂γ 2
                                      T r Σ0          = N γ −2      + O(1).
                                               ∂β 1            ∂β 1
This holds also for the partial derivative of (F.7) with respect to β 2 . Indeed, given the form of (F.8), we have
that                             µ ·            ¸¶         µ         ¶
                             ∂              ∂V         ∂       2η
                                  T r V −1         =                     + o(1) = O(1)
                           ∂β 2             ∂η        ∂β 2 (1 − η2 )
since the remainder term in (F.8) is of the form p(N )ηq(N) , where p and q are polynomials in N or order
greater than or equal to 0 and 1 respectively, whose diﬀerentiation with respect to η will produce terms that
are of order o(N ). Thus it follows that
                           µ ·             ¸¶            µ         2
                                                                     ¶
                       ∂           −1 ∂Σ0            ∂       −2 ∂γ
                             T r Σ0           = N          γ           + o(N )
                      ∂β 2            ∂β 1          ∂β 2        ∂β
                                                    ½ −2 2 1                2 2
                                                                                   ¾
                                                      ∂γ ∂γ            −2 ∂ γ
                                              = N                 +γ                 + o(N)             (F.9)
                                                       ∂β 2 ∂β 1         ∂β 2 ∂β 1
    Writing the result in matrix form, where the (1, 1) element corresponds to (β 1 , β 2 ) = (σ2 , σ 2 ), the (1, 2)
and (2, 1) elements to (β 1 , β 2 ) = (σ 2 , a2 ) and the (2, 2) element to (β 1 , β 2 ) = (a2 , a2 ), and computing the
partial derivatives in (F.9), we have
                                         µ ·               ¸¶
                     (0)          1 ∂              −1 ∂Σ0
                   φ      = −              T r Σ0
                                  2 ∂β 2              ∂β 1
                                    1/2 2 2                                               
                                      ∆0 (2a +σ ∆0 )                      1/2
                                                                         ∆0
                                    2σ3 (4a2 +σ2 ∆0 )3/2               +σ 2 ∆0 )3/2
                                                                 µ σ(4a21/2              ¶ 
                          = N                                                   2   2      + o(N ).             (F.10)
                                                             1        ∆ 0 σ (6a +σ ∆0 )
                                                •           2a 4  1 −       2   2
                                                                        (4a +σ ∆ )   3/2
                                                                                  0




                                                             35
      As for the coeﬃcient of order     , that is φ(2) in (F.6), define
                                            2

                                         ·                   µ                       ¶¸
                                            2 −1
                                                       £ −1 ¤ 2 −1 ∂Σ0 ∂σ2
                                 α ≡ T r σ Σ0 diag Σ0          σ Σ0         −      Id                              (F.11)
                                                                       ∂β 1   ∂β 1
so that
                                                               1            ∂α
                                                       φ(2) = − ∆20 V ar[ξ]      .
                                                               2            ∂β 2
We have
                  ·                              ¸
                              £ −1 ¤ −1 ∂Σ0            2 ∂σ
                                                            2    £            £ −1 ¤¤
       α = σ4 T r Σ−1 0  diag   Σ0   Σ 0           − σ        T r Σ−1
                                                                    0 diag Σ0
                                            ∂β 1         ∂β 1
                       "                           ¡ 2 ¢#
                                   £     ¤       ∂ γ V             ∂σ 2 −4 £ −1             £     ¤¤
         = σ4 γ −6 T r V −1 diag V −1 V −1                   − σ2       γ T r V diag V −1
                                                    ∂β 1           ∂β 1
                       ·                                  ¸
                                   £     ¤       ∂V ∂η
         = σ4 γ −4 T r V −1 diag V −1 V −1
                                                 ∂η ∂β 1
                     ∂γ  2    £           £      ¤¤       ∂σ 2 −4 £ −1            £      ¤¤
           +σ 4 γ −6       T r V −1 diag V −1 − σ 2            γ T r V diag V −1
                     ∂β 1                                 ∂β 1
                            ·                             ¸ µ                                 ¶
            4 −4 ∂η             −1
                                       £ −1 ¤ −1 ∂V                4 −6 ∂γ
                                                                           2
                                                                                  2 ∂σ
                                                                                       2
                                                                                           −4
                                                                                                   £         £    ¤¤
         = σ γ          T r V diag V             V          + σ γ              −σ        γ      T r V −1 diag V −1 .
                   ∂β 1                               ∂η                 ∂β 1       ∂β 1
Next, we compute separately
            ·                       ¸      ·                     ¸
                      £    ¤     ∂V             £    ¤     ∂V −1
         T r V −1 diag V −1 V −1      = T r diag V −1 V −1     V
                                 ∂η                        ∂η
                                             ·                 ¸
                                                  £ −1 ¤ ∂V −1
                                      = −T r diag V
                                                          ∂η
                                                              N
                                                              X        ∂v i,i
                                                       = −        vi,i
                                                              i=1
                                                                        ∂η
                                                                           ¡                           ¢
                                                            O(1) − 2N η 1 + η2 − η4 − η6 + O(η2N ) + N 2 O(η2N )
                                                       =                                     ¡            ¢3
                                                                        (1 + η2 )2 (1 − η2 )4 1 − η2(1+N)
                                                             −2Nη
                                                       =                + o(N)
                                                            (1 − η2 )3
and
                                                    N
                                                    X
                               £         £     ¤¤     ¡ i,i ¢2
                            T r V −1 diag V −1    =    v
                                                                   i=1
                                                                                 ¡                ¢
                                                                     O(1) + N 1 − η4 + O(η 2N )
                                                              =                        ¡            ¢2
                                                                   (1 + η2 ) (1 − η2 )3 1 − η2(1+N)
                                                                       N
                                                              =              2 + o(N )
                                                                   (1 − η2 )
      Therefore
                                     Ã                 !    µ                          ¶Ã            !
                                                                      2         2
                       4 −4   ∂η          −2N η               4 −6 ∂γ      2 ∂σ     −4        N
                  α=σ γ                                    + σ γ        −σ        γ                    + o(N ),
                              ∂β 1       (1 − η 2 )3               ∂β 1      ∂β 1         (1 − η2 )2

which can be diﬀerentiated with respect to β 2 to produce ∂α/∂β 2 . As above, diﬀerentiation of the remainder
term o(N ) still produces a o(N ) term because of the structure of the terms there (they are again of the form
p(N )ηq(N) .)

                                                                     36
    Note that an alternative expression for α can be obtained as follows. Going back to the definition (F.11),
                                 ·                        ¸
                                           £ −1 ¤ −1 ∂Σ0         ∂σ2     £       £ −1 ¤¤
                      α = σ 4 T r Σ−1
                                   0  diag  Σ0   Σ0         − σ2      T r Σ−1
                                                                           0 diag Σ0                    (F.12)
                                                     ∂β 1        ∂β 1

the first trace becomes
                         ·                      ¸      ·                     ¸
                           −1
                                 £ −1 ¤ −1 ∂Σ0              £ −1 ¤ −1 ∂Σ0 −1
                      T r Σ0 diag Σ0 Σ0           = T r diag Σ0 Σ0         Σ
                                           ∂β 1                       ∂β 1 0
                                                         ·                ¸
                                                             £    ¤ ∂Σ−1
                                                  = −T r diag Σ−1
                                                               0
                                                                       0
                                                                     ∂β 1
                                                               XN
                                                                             ∂Σ−1
                                                        = −        (Σ−1
                                                                     0 )ii (
                                                                                0
                                                                                   )ii
                                                               i=1
                                                                              ∂β 1
                                                                       N
                                                               1 ∂ X −1 2
                                                        = −              (Σ )
                                                               2 ∂β 1 i=1 0 ii
                                                               1 ∂       £       £ −1 ¤¤
                                                        = −           T r Σ−1
                                                                           0 diag Σ0     .
                                                               2 ∂β 1

so that we have
                                1 ∂        £           £ −1 ¤¤        ∂σ2    £         £ −1 ¤¤
                   α = −σ4             T r Σ−1 0 diag Σ0         − σ2     T r Σ−10 diag Σ0
                                2 ∂β 1                                ∂β
                                                                     µ 14¶
                                1 ∂        £           £      ¤¤   1 ∂σ           £       £ −1 ¤¤
                       =   −σ4         T r Σ−1 0 diag Σ0
                                                           −1
                                                                 −            T r Σ−1
                                                                                    0 diag Σ0
                                2 ∂β 1                             2 ∂β 1
                             1 ∂ ¡ 4 £ −1                £ −1 ¤¤¢
                       =   −         σ T r Σ0 diag Σ0
                             2 ∂β 1
                             1 ∂ ¡ 4 −4 £ −1                  £     ¤¤¢
                       =   −         σ γ T r V diag V −1
                             2 ∂β 1
                                    Ã           Ã                     !!
                             1 ∂        4 −4          N
                       =   −           σ γ                    + o(N )
                             2 ∂β 1               (1 − η 2 )2
                                     Ã              !
                             N ∂          σ 4 γ −4
                       =   −                          + o(N ),
                             2 ∂β 1 (1 − η2 )2
                            £         £    ¤¤
where the calculation of T r V −1 diag V −1 is as before, and where the o(N ) term is a sum of terms of the
form p(N )ηq(N) as discussed above. From this one can interchange diﬀerentiation and the o(N ) term, yielding
the final equality above.
    Therefore
                                                    Ã          Ã                    !!
                            ∂α          1 ∂2                         N
                                   = −               σ 4 γ −4                + o(N )
                            ∂β 2        2 ∂β 1 ∂β 2              (1 − η 2 )2
                                                     Ã             !
                                        N ∂2             σ 4 γ −4
                                   = −                               + o(N ).                          (F.13)
                                         2 ∂β 1 ∂β 2 (1 − η2 )2

Writing the result in matrix form and calculating the partial derivatives, we obtain
                                                            Ã                          !
                         1           ∂α      N  ∆ 2
                                                    V ar[ξ]       2     (8a2 −2σ2 ∆0 )
                                                              −2a    −
                φ(2) = − ∆20 V ar[ξ]      =       0                          2∆0         + o(N )       (F.14)
                                            (4a2 + σ2 ∆0 )3
                                                                               2
                         2           ∂β 2                       •         − 8σ   ∆0




                                                        37
    Putting it all together, we have obtained
                                1 h ¨       i   1 ³ (0)                     ´
                                  E −lβ2 β1   =      φ + 2 φ(2) + O( 3 )
                                N               N
                                              ≡ F (0) + 2 F (2) + O( 3 ) + o(1)                                           (F.15)

where                                          1/2                                                                   
                                               ∆02a2 +σ 2 ∆0
                                                      (            )                        1/2
                                                                                           ∆0
                                          2σ (4a2 +σ2 ∆0 )3/2
                                             3
                                                                                  µ  σ(4a +σ2 ∆0 )3/2
                                                                                         2
                                                                                                                  ¶ 
                          F (0) =                                                        1/2
                                                                                        ∆0 σ 6a2 +σ 2 ∆0
                                                                                                (             )          (F.16)
                                                                             1
                                                          •                 2a4    1−     (4a2 +σ 2 ∆0 )3/2
                                                                        Ã                                   !
                                                    ∆20 V ar[ξ]                            (8a2 −2σ2 ∆0 )
                                     (2)                                    −2a2       −
                                 F         ≡                                                   2∆0                .       (F.17)
                                               (4a2 + σ 2 ∆0 )3
                                                                                                    2
                                                                             •               − 8σ
                                                                                               ∆0

    The asymptotic variance of the maximum-likelihood estimators AV AR(σ̂2 , â2 ) is therefore given by
                                             ³                         ´−1
                   AV AR(σ̂ 2 , â2 ) = E [∆] F (0) + 2 F (2) + O( 3 )
                                           µ     µ         h     i−1               ¶¶−1
                                      = ∆0 F (0) Id + 2 F (0)        F (2) + O( 3 )
                                               µ                      h      i−1         ¶−1 h      i−1
                                                                    2    (0)      (2)  3
                                           = ∆0 Id +                   F         F + O( )     F (0)
                                               µ                      h      i−1         ¶h     i−1
                                                                    2    (0)
                                           = ∆0 Id −                   F         F + O( ) F (0)
                                                                                  (2)  3

                                               h      i−1                          h      i−1      h      i−1
                                           = ∆0 F (0)     −                  2
                                                                                 ∆0 F (0)     F (2) F (0)     + O( 3 )
                                           ≡ A(0) +           2
                                                                  A(2) + O( 3 )
                                        £    ¤−1                   £    ¤−1 (2) £ (0) ¤−1
where the final results for A(0) = ∆0 F (0)      and A(2) = −∆0 F (0)      F     F        , obtained by replacing
F (0) and F (2) by their expressions in (F.15), are given in the statement of the Theorem.


Appendix G:                 Proof of Theorem 3
    It follows as in (5.4), (5.5) and (5.6) that
            h             i
      Etrue l̇β 1 l˙β 2 |∆ = Covtrue (l̇β1 , l̇β 2 |∆)
                                                                                             
                                                 X N         µ −1 ¶         N
                                                                            X       µ −1 ¶
                                            1                 ∂Σ          1          ∂Σ
                            = Covtrue −               Yi Yj           ,−     Yk Yl           
                                            2 i,j=1            ∂β 1 ij    2           ∂β 2 kl
                                                                                              k,l=1
                                     N
                                     X         µ         ¶ µ −1 ¶
                             1                      ∂Σ−1     ∂Σ
                        =                                            Covtrue (Yi Yj , Yk Yl |∆)
                             4                       ∂β 1 ij ∂β 2 kl
                                 i,j,k,l=1
                                h            i 1   XN     µ −1 ¶ µ −1 ¶
                                                           ∂Σ         ∂Σ
                        = −Etrue ¨lβ 1 β 2 |∆ +                              Cumtrue (Yi , Yj , Yk , Yl |∆)
                                                4           ∂β 1 ij ∂β 2 kl
                                                i,j,k,l=1
                                h           i 1              µ −1          ¶
                                 ¨                             ∂Σ     ∂Σ−1
                        = −Etrue lβ 1 β 2 |∆ + Cum4 (U )ψ           ,                                      (G.1)
                                              4                 ∂β 1 ∂β 2




                                                                        38
since Cumtrue (Yi , Yj , Yk , Yl |∆) = 2, ±1, or 0, ×Cumtrue (U ), as in (5.8), and with ψ defined in (C.3). Taking
now unconditional expectations, we have
                       h          i
               Etrue l˙β 1 l˙β 2     = Covtrue (l˙β 1 , l˙β2 )
                                           h                        i
                                     = E Covtrue (l˙β 1 , l˙β 2 |∆) + Covtrue (Etrue [l˙β1 |∆], Etrue [l̇β2 |∆])
                                           h                        i
                                     = E Covtrue (l˙β 1 , l˙β 2 |∆)
                                                h         i 1              · µ −1                ¶¸
                                                                                ∂Σ          ∂Σ−1
                                     = −Etrue ¨lβ1 β2 + Cum4 (U )E ψ                     ,            .          (G.2)
                                                               4                  ∂β 1 ∂β 2

with the first and third equalities following from the fact that Etrue [l˙β i |∆] = 0.
    Since                                   h            i         h          i
                                       Etrue ¨lβ 1 β 2 |∆ = Enormal ¨lβ1 β2 |∆

and consequently                               h         i         h        i
                                          Etrue l¨β 1 β 2 = Enormal ¨lβ1 β2
                                                                                           h         i
have been found in the previous subsection (see (F.15)), what we need to do to obtain Etrue l̇β1 l̇β2 is to
calculate                                  · µ −1           ¶¸
                                                ∂Σ     ∂Σ−1
                                         E ψ         ,         .
                                                 ∂β 1 ∂β 2
    With Σ−1 given by (F.2), we have for i = 1, 2

                   ∂Σ−1    ∂Σ−1
                              0     ∂ ¡ 2 −1 −1 ¢               2    ∂ ³ 4 ¡ −1 ¢2 −1 ´
                         =       −      σ Σ0 ΞΣ0 +                       σ Σ0 Ξ Σ0 + O( 3 )
                    ∂β i    ∂β i   ∂β i                             ∂β i

and therefore by bilinearity of ψ we have
           · µ −1             ¶¸         µ −1           ¶     · µ −1                     ¶¸
                ∂Σ       ∂Σ−1               ∂Σ0 ∂Σ−1  0           ∂Σ0       ∂ ¡ 2 −1 −1 ¢
         E ψ           ,           = ψ           ,        − E ψ          ,     σ Σ 0 ΞΣ0    [2]
                 ∂β 1 ∂β 2                   ∂β 1 ∂β 2             ∂β 1 ∂β 2
                                              · µ −1                          ¶¸
                                                   ∂Σ0    ∂ ³ 4 ¡ −1 ¢2 −1 ´
                                       + 2E ψ           ,     σ Σ0 Ξ Σ0          [2]
                                                    ∂β 1 ∂β 2
                                              · µ                                     ¶¸
                                          2         ∂ ¡ 2 −1 −1 ¢ ∂ ¡ 2 −1 −1 ¢
                                       + E ψ            σ Σ0 ΞΣ0 ,         σ Σ0 ΞΣ0
                                                   ∂β 1             ∂β 2
                                          +O( 3 ),                                                              (G.3)

where the “[2]” refers to the sum over the two terms where β 1 and β 2 are permuted.
   The first (and leading) term in (G.3),
              µ −1            ¶        Ã ¡          ¢ ¡ −2 −1 ¢ !
                ∂Σ0 ∂Σ−1   0             ∂  γ −2 −1
                                                V       ∂ γ V
           ψ          ,          = ψ                  ,
                 ∂β 1 ∂β 2                    ∂β 1          ∂β 2
                                       µ −2                                              ¶
                                         ∂γ                ∂V −1 ∂γ −2 −1          ∂V −1
                                 = ψ           V −1 + γ −2       ,     V    + γ −2
                                          ∂β 1              ∂β 1 ∂β 2               ∂β 2
                                       µ −2                   −1         −2               −1
                                                                                                 ¶
                                         ∂γ      −1     −2 ∂V      ∂η ∂γ      −1    −2 ∂V    ∂η
                                 = ψ           V +γ                  ,      V    +γ
                                          ∂β 1              ∂η ∂β 1 ∂β 1                 ∂η ∂β 1

corresponds to the equally spaced, misspecified noise distribution, situation studied in Section 5.




                                                          39
      The second term, linear in , is zero since
                   · µ −1                        ¶¸     µ −1      ·                   ¸¶
                        ∂Σ0     ∂ ¡ 2 −1 −1 ¢            ∂Σ0         ∂ ¡ 2 −1 −1 ¢
                E ψ           ,      σ Σ0 ΞΣ0       = ψ        ,E        σ Σ0 ΞΣ0
                         ∂β 1 ∂β 2                        ∂β 1      ∂β 2
                                                        µ −1                          ¶
                                                         ∂Σ0     ∂ ¡ 2 −1         −1
                                                                                     ¢
                                                    = ψ        ,      σ Σ0 E [Ξ] Σ0
                                                          ∂β 1 ∂β 2
                                                    = 0

with the first equality following from the bilinearity of ψ, the second from the fact that the unconditional
expectation over the ∆0i s does not depend on the β parameters, so expectation and diﬀerentiation with respect
to β 2 can be interchanged, and the third equality from the fact that E [Ξ] = 0.
    To calculate the third term in (G.3), the first of two that are quadratic in , note that
                                · µ −1                                ¶¸
                                     ∂Σ0       ∂ ¡ 4 −1 −1 −1 ¢
                    α1 ≡ E ψ                ,       σ Σ0 ΞΣ0 ΞΣ0
                                       ∂β 1 ∂β 2
                                µ −1                                   ¶
                                  ∂Σ0      ∂ ¡ 4 −1 £ −1 ¤ −1 ¢
                          = ψ           ,       σ Σ0 E ΞΣ0 Ξ Σ0
                                   ∂β 1 ∂β 2
                                           µ −1                                ¶
                                            ∂Σ0       ∂ ¡ 4 −1                ¢
                          = ∆20 V ar[ξ]ψ          ,        σ Σ0 diag(Σ−1
                                                                      0  )Σ−1
                                                                           0
                                              ∂β    ∂β 2
                                           Ã ¡1          ¢                               !
                                2            ∂ γ −2 V −1      ∂ ¡ 4 −6 −1        −1  −1
                                                                                        ¢
                          = ∆0 V ar[ξ]ψ                    ,      σ γ V diag(V )V          ,             (G.4)
                                                 ∂β 1        ∂β 2
                                                      £        ¤
with the second equality obtained by replacing E ΞΣ−1      0 Ξ with its value given in (F.4), and the third by
recalling that Σ0 = γ 2 V . The elements (i, j) of the two arguments of ψ in (G.4) are
                                         ¡         ¢
                                   i,j ∂ γ −2 vi,j      ∂γ −2 i,j      ∂vi,j ∂η
                                 ν =                 =        v + γ −2
                                            ∂β 1         ∂β 1           ∂η ∂β 1

and
                                   Ã         N
                                                             !
                               ∂            X
                    k,l                4 −6      k,m m,m m,l
                ω         =          σ γ        v v     v
                              ∂β 2          m=1
                               ¡        ¢ N                          Ã N                 !
                              ∂ σ 4 γ −6 X k,m m,m m,l            ∂   X                    ∂η
                          =                   v v     v + σ4 γ −6         vk,m vm,m vm,l
                                 ∂β 2    m=1
                                                                  ∂η  m=1
                                                                                           ∂β 2


from which ψ in (G.4) can be evaluated through the sum given in (C.3).
    Summing these terms, we obtain
                      Ã ¡           ¢                                  !
                        ∂ γ −2 V −1      ∂ ¡ 4 −6 −1            −1 −1
                                                                      ¢
                    ψ                 ,      σ γ V diag(V )V
                            ∂β 1        ∂β 2
                                                   ¡      ¡      ¢             ¢
                    4N (C1V (1 − η) + C2V C3V ) C1W 1 − η 2 + 2C2W C3W (1 + 3η)
                =                                                                + o(N )
                                               (1 − η)7 (1 + η)3

where
                                           ∂γ −2                         ∂η
                                 C1V   =         , C2V = γ −2 , C3V =
                                            ∂β 1                         ∂β 1
                                             ¡       ¢
                                           ∂ σ4 γ −6                           ∂η
                                 C1W   =               , C2W = σ4 γ −6 , C3W =
                                               ∂β 2                            ∂β 2


                                                        40
    The fourth and last term in (G.3), also quadratic in ,
                                  · µ                                    ¶¸
                                         ∂ ¡ 2 −1 −1 ¢ ∂ ¡ 2 −1 −1 ¢
                           α2 ≡ E ψ          σ Σ0 ΞΣ0 ,         σ Σ0 ΞΣ0
                                        ∂β 1               ∂β 2

is obtained by first expressing
                                         µ                                 ¶
                                              ∂ ¡ 2 −1 −1 ¢ ∂ ¡ 2 −1 −1 ¢
                                     ψ            σ Σ0 ΞΣ0 ,      σ Σ0 ΞΣ0
                                             ∂β 1            ∂β 2

in its sum form and then taking expectations term by term. Letting now
                            µ                   ¶           µ                ¶
                       i,j     ∂ ¡ 2 −1 −1 ¢            k,l     ∂ ¡ 2 −1 −1 ¢
                      ν =          σ Σ0 ΞΣ0         , ω =           σ Σ0 ΞΣ0
                              ∂β 1               ij           ∂β 2            kl

we recall our definition of ψ(ν, ω) given in (C.3) whose unconditional expected value (over the ∆0i s, i.e., over
Ξ) we now need to evaluate in order to obtain α2 .
    We are thus led to consider four-index tensors λijkl and to define
                                     N
                                     X                    Xn
                                                          N−1
                       ψ̃(λ) ≡ 2             λh,h,h,h +        −2λh,h+1,h+1,h+1 − 2λh+1,h+1,h,h+1
                                     h=1                  h=1
                                                      h,h,h+1,h+1
                                                 +λ            + λh+1,h+1,h,h + 4λh,h+1,h,h+1              (G.5)
                                                                            o
                                                 −2λh+1,h,h,h − 2λh,h,h+1,h

where λijkl is symmetric in the two first and the two last indices, respectively, i.e., λijkl = λjikl and λijkl =
λijlk . In terms of our definition of ψ in (C.3), it should be noted that ψ(ν, ω) = ψ̃(λ) when one takes
λijkl = ν i,j ω k,l . The expression we seek is therefore
                                    · µ                                     ¶¸
                                          ∂ ¡ 2 −1 −1 ¢ ∂ ¡ 2 −1 −1 ¢
                            α2 = E ψ           σ Σ0 ΞΣ0 ,      σ Σ0 ΞΣ0         = ψ̃(λ)                     (G.6)
                                         ∂β 1             ∂β 2

where λijkl is taken to be the following expected value
                           £           ¤
               λijkl   ≡ E ν i,j ω k,l
                           "µ                       ¶ µ                      ¶ #
                                  ∂ ¡ 2 −1 −1 ¢              ∂ ¡ 2 −1 −1 ¢
                       = E              σ Σ0 ΞΣ0                σ Σ0 ΞΣ0
                                ∂β 1                  ij  ∂β 2                kl
                           " N                                                                   #
                                X         ∂ ¡ 2 −1                  ¢ ∂ ¡ 2 −1                  ¢
                                                              −1                          −1
                       = E                    σ (Σ0 )ir Ξrs (Σ0 )sj       σ (Σ0 )kt Ξtu (Σ0 )ul
                             r,s,t,u=1
                                         ∂β 1                        ∂β 2
                             N
                             X        ∂ ¡ 2 −1              ¢ ∂ ¡ 2 −1             ¢
                       =                  σ (Σ0 )ir (Σ−1
                                                      0 )sj       σ (Σ0 )kt (Σ−1
                                                                              0 )ul E [Ξrs Ξtu ]
                           r,s,t,u=1
                                     ∂β 1                    ∂β 2
                                         XN
                                              ∂ ¡ 2 −1              ¢ ∂ ¡ 2 −1              ¢
                       =   ∆20 V   ar[ξ]          σ (Σ0 )ir (Σ−1
                                                              0 )rj       σ (Σ0 )kr (Σ−1
                                                                                      0 )rl
                                         r=1
                                             ∂β 1                    ∂β 2

with the third equality following from the interchangeability of unconditional expectations and diﬀerentiation
with respect to β, and the fourth from the fact that E [Ξrs Ξtu ] 6= 0 only when r = s = t = u, and

                                                   E [Ξrr Ξrr ] = ∆20 V ar[ξ].




                                                                41
      Thus we have
                                              XN
                                                   ∂ ¡ 2 −4 −1              ¢ ∂ ¡ 2 −4 −1              ¢
                λijkl   = ∆20 V ar[ξ]                  σ γ (V )ir (V −1 )rj       σ γ (V )kr (V −1 )rl
                                              r=1
                                                  ∂β 1                       ∂β 2

                                              XN
                                                   ∂ ¡ 2 −4 i,r r,j ¢ ∂ ¡ 2 −4 k,r r,l ¢
                        =       ∆20 V ar[ξ]            σ γ v v            σ γ v v        .                                         (G.7)
                                              r=1
                                                  ∂β 1               ∂β 2

and
                                                                           Ã ¡         ¢                 ¡ i,r r,j ¢      !
          ∂ ¡ 2 −4 i,r r,j ¢ ∂ ¡ 2 −4 k,r r,l ¢                              ∂ σ2 γ −4 i,r r,j         ∂  v   v      ∂η
              σ γ v v            σ γ v v        =                                        v v + σ2 γ −4
         ∂β 1               ∂β 2                                                ∂β 1                        ∂η       ∂β 1
                                                                             Ã ¡          ¢                 ¡         ¢      !
                                                                               ∂ σ2 γ −4 k,r r,l   2 −4 ∂ v
                                                                                                              k,r r,l
                                                                                                                 v      ∂η
                                                                           ×                v v +σ γ                           .
                                                                                  ∂β 2                        ∂η        ∂β 2

      Summing these terms, we obtain
                         ∆20 V ar[ξ] 2N                      ¡   ¡      ¢¡       ¡                 ¢     ¡       ¢¢
       ψ̃(λ) =              7           3                     C1λ 1 − η4 2C5λ C6λ 1 + η + η 2 + 2η3 + C4λ 1 − η4
                   (1 − η) (1 + η) (1           + η2 )3
                                                                         ¡         ¡                                    ¢
                                                                +2C2λ C3λ 2C5λ C6λ 1 + 2η + 4η2 + 6η3 + 5η4 + 4η5 + 4η6
                                                                     ¡                                      ¢¢¢
                                                                +C4λ 1 + η + η 2 + 2η3 − η4 − η5 − η6 − 2η7
                   +o(N )

where
                                                             ¡        ¢
                                                            ∂ σ2 γ −4                           ∂η
                                       C1λ          =                   , C2λ = σ2 γ −4 , C3λ =
                                                               ∂β                               ∂β 1
                                                             ¡ 1 ¢
                                                            ∂ σ2 γ −4                           ∂η
                                       C4λ          =                   , C5λ = σ2 γ −4 , C6λ =
                                                               ∂β 2                             ∂β 2
      Putting it all together, we have
                         · µ −1            ¶¸    µ −1        ¶
                              ∂Σ      ∂Σ−1        ∂Σ0 ∂Σ−10                               2
                       E ψ          ,         =ψ       ,       +                              (α1 [2] + α2 ) + O( 3 )
                               ∂β 1 ∂β 2           ∂β 1 ∂β 2
      Finally, the asymptotic variance of the estimator (σ̂ 2 , â2 ) is given by
                                                                      ¡         ¢−1
                                    AV ARtrue (σ̂ 2 , â2 ) = E [∆] D0 S −1 D                                                      (G.8)

where
                                             1        hi      1     hi  1       h i
                            D      = D0 = −     Etrue ¨l = − Enormal ¨
                                                                     l = Enormal l˙l˙0
                                             N                N         N
                                   ≡ F (0) + 2 F (2) + O( 3 )

is given by the expression in the correctly specified case (F.15), with F (0) and F (2) given in (F.16) and (F.17)
respectively. Also, in light of (G.1), we have
                            1      h i       1        hi
                      S = Etrue l˙l̇0 = − Etrue ¨l + Cum4 (U ) Ψ = D + Cum4 (U ) Ψ
                           N                 N
where
                                        h ³ −1               ´i   h ³ −1           ´i 
                                                ∂Σ      ∂Σ−1            ∂Σ     ∂Σ−1
                                   1    E  ψ    ∂σ 2 ,  ∂σ 2    E  ψ      2 ,    2
                          Ψ =                                      h ³ ∂σ−1 ∂a−1 ´i 
                                  4N               •             E ψ    ∂Σ
                                                                           2 ,
                                                                               ∂Σ
                                                                                  2           ∂a     ∂a
                                              (0)       2    (2)       3
                                    ≡ Ψ             +       Ψ      + O( ).

                                                                           42
Since, from (G.3), we have
                       · µ −1            ¶¸      µ −1         ¶
                            ∂Σ      ∂Σ−1           ∂Σ0 ∂Σ−1
                     E ψ          ,          =ψ          ,  0
                                                                 + 2 α1 [2] +                                                      2
                                                                                                                                       α2 + O( 3 ),
                             ∂β 1 ∂β 2              ∂β 1 ∂β 2
                                                     ³ −1       ´
                                                 1    ∂Σ   ∂Σ−1
it follows that Ψ(0) is the matrix with entries 4N ψ ∂β0 , ∂β0 , i.e.,
                                                                                       1         2

                                                                    µ                                            1/2
                                                                                                                                          ¶       
                                   ∆0                    ∆0
                                                               1/2
                                                                                 1                            ∆0 (6a2 +∆0 σ2 )
                           σ2 (4a2 +∆0 σ2 )3            2a4              σ(4a2 +∆0 σ2 )3/2
                                                                                                         −     (4a2 +∆0 σ2 )3                     
                 Ψ(0) = 
                        
                                                           µ                                                                                    ¶  + o(1),
                                                      1
                                                                           1/2
                                                                          ∆0 σ 6a2 +∆0 σ2
                                                                                   (                      )       2a    4     2
                                                                                                                         (16a +3∆0 σ       2
                                                                                                                                               ) 
                                    •                2a8        1−         (4a2 +∆0 σ2 )3/2
                                                                                                              −         (4a2 +∆0 σ2 )3


and
                                                                               1
                                                        Ψ(2) =                   (α1 [2] + α2 ) .
                                                                              4N
with
                                                                         ³                                                 3/2     1/2
                                                                                                                                                                       ´   
                                2∆0
                                    3/2
                                          (−4a2 +∆0 σ2 )             ∆0       (−4a2 +∆0 σ2 )(4a2 +∆0 σ2 )                         −∆0     σ (−40a4 +2a2 ∆0 σ2 +∆20 σ4 )
  1                             σ(4a2 +∆0 σ 2 )9/2                                                  ³            2a4 (4a2 +∆0 σ 2 )9/2                   ´                
    α1 [2] = V ar[ξ]                                                                                                                                                      
 4N                                                                                        8∆0 σ 2       (4a2 +∆0 σ2 )3/2 −∆1/2    2     2
                                                                                                                            0 σ (6a +∆0 σ )
                                              •                                                                   a4 (4a2 +∆0 σ2 )9/2
                   +o(1),

                                                                                                                                                                 
                                    ∆0
                                        3/2
                                              (40a8 −12a4 ∆0 2 σ4 +∆0 4 σ8 )                   ∆0
                                                                                                 3/2                                               2
                                                                                                         σ(−44a6 −18a4 ∆0 σ2 +7a2 ∆0 σ 6 +3∆0 3 σ6 )
          1                          2σ(2a2 +∆0 σ2 )3 (4a2 +∆0 σ 2 )9/2                                       (2a2 +∆0 σ2 )3 (4a2 +∆0 σ2 )9/2                    
            α2    = V ar[ξ]                                                                                  3/2        2
                                                                                                                 σ3 (50a2 +42a2 ∆0 σ2 +9∆0 2 σ 4 )
                                                                                                                                                                  
         4N                                                •
                                                                                                         2∆0
                                                                                                               (2a2 +∆0 σ2 )3 (4a2 +∆0 σ2 )9/2
                      +o(1).

      It follows from (G.8) that
                                                    ³                      ´−1
                     AV ARtrue (σ̂2 , â2 ) = E [∆] D (D + Cum4 (U) Ψ)−1 D
                                                  ³ ¡                  ¢−1 ´−1
                                            = ∆0 D Id + Cum4 (U ) D−1 Ψ
                                                  ¡                  ¢
                                            = ∆0 Id + Cum4 (U ) D−1 Ψ D−1
                                                  ¡                  ¢
                                            = ∆0 Id + Cum4 (U ) D−1 Ψ D−1
                                                    = AV ARnormal (σ̂2 , â2 ) + ∆0 Cum4 (U ) D−1 ΨD−1

where
                                AV ARnormal (σ̂2 , â2 ) = ∆0 D−1 = A(0) +                                          2
                                                                                                                        A(2) + O( 3 )

is the result given in Theorem 2, namely (7.6).




                                                                                  43
    The correction term due to the misspecification of the error distribution is determined by Cum4 (U) times
                             ³                         ´−1 ³                      ´
         ∆0 D−1 ΨD−1     = ∆0 F (0) + 2 F (2) + O( 3 )      Ψ(0) + 2 Ψ(2) + O( 3 )
                            ³                         ´−1
                           × F (0) + 2 F (2) + O( 3 )
                             µ         h     i−1               ¶h     i−1 ³                      ´
                         = ∆0 Id − 2 F (0)       F (2) + O( 3 ) F (0)      Ψ(0) + 2 Ψ(2) + O( 3 )
                            µ        h      i−1               ¶h     i−1
                           × Id − 2 F (0)       F (2) + O( 3 ) F (0)
                              h     i−1     h      i−1
                         = ∆0 F (0)     Ψ(0) F (0)     +
                                  µh     i−1       h    i−1 h         i−1      h      i−1     h      i−1
                           + 2 ∆0 F (0)      Ψ(2) F (0)      − F (0)      F (2) F (0)     Ψ(0) F (0)
                                h     i−1     h      i−1      h      i−1 ¶
                              − F (0)     Ψ(0) F (0)     F (2) F (0)

                             +O( 3 )
                         ≡ B (0) +     2
                                           B (2) + O( 3 ).

where the matrices The asymptotic variance is then given by
                                    ³                     ´   ³                     ´
            AV ARtrue (σ̂2 , â2 ) = A(0) + Cum4 (U )B (0) + 2 A(2) + Cum4 (U )B (2) + O( 3 ).

with the terms A(0) , A(2) , B (0) and B (2) given in the statement of the Theorem.


Appendix H:               Proof of Proposition 3
    From                                                             ¡         ¢
                                 £ 2¤      £ 2¤      £ 2¤    2     c2 1 − e−b∆
                               E Yi = E wi + E ui = σ ∆ +
                                                                         b
it follows that the estimator (2.2) has the following expected value
                                                      N
                                    £ ¤           1 X £ 2¤
                                   E σ̂2 =               E Yi
                                                  T i=1
                                                     Ã            ¡          ¢!
                                                               2         −b∆
                                                  N           c    1 −  e
                                                =      σ2∆ +
                                                  T                   b
                                                          ¡          ¢
                                                        c2 1 − e−b∆
                                                = σ2 +
                                                             b∆
                                                  ¡ 2      ¢  bc2
                                                = σ + c2 −          ∆ + O(∆2 ).                            (H.1)
                                                                2
    The estimator’s variance is
                                                      "N       #
                              £ ¤             1        X
                          V ar σ̂2 =             V ar      Yi2
                                              T2       i=1
                                                  N                 N i−1
                                               1 X       £ 2¤    2 XX        ¡          ¢
                                       =        2
                                                    V ar  Yi  +   2
                                                                          cov Yi2 , Yj2
                                              T i=1             T i=1 j=1



                                                             44
Since the Yi0 s are normal with mean zero,
                                            £ ¤                    £ ¤2
                                        V ar Yi2 = 2V ar[Yi ]2 = 2E Yi2

and for i > j
                                      ¡         ¢                  2              2
                                   cov Yi2 , Yj2 = 2 cov (Yi , Yj ) = 2 E [ui uj ]

since
                          cov (Yi , Yj ) = E [Yi Yj ] = E [(wi + ui ) (wj + uj )] = E [ui uj ] .

   Now we have
                                £¡              ¢¡            ¢¤
                  E [ui uj ] = E  Uτ i − Uτ i−1 Uτ j − Uτ j−1
                                £        ¤      £        ¤      £        ¤   £              ¤
                            = E Uτ i Uτ j − E Uτ i Uτ j−1 − E Uτ i−1 Uτ j + E Uτ i−1 Uτ j−1
                                   ¡          ¢2
                                c2 1 − e−b∆ e−b∆(i−j−1)
                            = −
                                             2b
so that
                                                     Ã      ¡        ¢2           !2
                               ¡         ¢                c2 1 − e−b∆ e−b∆(i−j−1)
                            cov Yi2 , Yj2 = 2           −
                                                                     2b
                                                                   ¡        ¢4
                                                   c4 e−2b∆(i−j−1) 1 − e−b∆
                                             =
                                                                2b2
and consequently
                     ¡          ¢ ¡ −2b∆                    ¢      Ã           ¡          ¢ !2 
            £ 2¤  1  c4
                         1 − e−b∆ 2
                                     Ne      −    1 + e−2Nb∆
                                                                             c2
                                                                                 1 −  e−b∆      
                                                                       2
        V ar σ̂ = 2                             2              + 2N  σ   ∆ +                        (H.2)
                 T              b2 (1 + e−b∆ )                                     b           

with N = T/∆. The RMSE expression follows from (H.1) and (H.2). As in Proposition 1, these are exact
small sample expressions, valid for all (T, ∆).




                                                           45
References
Aït-Sahalia, Y., 1996, “Nonparametric Pricing of Interest Rate Derivative Securities,” Econometrica,
 64, 527—560.
      , 2002, “Maximum-Likelihood Estimation of Discretely-Sampled Diﬀusions: A Closed-Form
 Approximation Approach,” Econometrica, 70, 223—262.

Aït-Sahalia, Y., and P. A. Mykland, 2003, “The Eﬀects of Random and Discrete Sampling When
 Estimating Continuous-Time Diﬀusions,” Econometrica, 71, 483—549.
Andersen, T. G., T. Bollerslev, F. X. Diebold, and P. Labys, 2003, “Modeling and Forecasting
 Realized Volatility,” Econometrica, 71, 579—625.
Bandi, F. M., and P. C. B. Phillips, 2003, “Fully Nonparametric Estimation of Scalar Diﬀusion
 Models,” Econometrica, 71, 241—283.
Barndorﬀ-Nielsen, O. E., and N. Shephard, 2002, “Econometric Analysis of Realized Volatility and
 Its Use in Estimating Stochastic Volatility Models,” Journal of Royal Statistical Society, B, 64,
 253—280.
Black, F., 1986, “Noise,” Journal of Finance, 41, 529—543.

Delattre, S., and J. Jacod, 1997, “A Central Limit Theorem for Normalized Functions of the Incre-
 ments of a Diﬀusion Process, in the Presence of Round-Oﬀ Errors,” Bernoulli, 3, 1—28.

Durbin, J., 1959, “Eﬃcient Estimation of Parameters in Moving-Average Models,” Biometrika, 46,
 306—316.
Glosten, L. R., 1987, “Components of the Bid-Ask Spread and the Statistical Properties of Transac-
 tion Prices,” Journal of Finance, 42, 1293—1307.

Gloter, A., and J. Jacod, 2000, “Diﬀusions with Measurement Errors: I - Local Asymptotic Normality
 and II - Optimal Estimators,” Working paper, Université de Paris VI.
Haddad, J., 1995, “On the Closed Form of the Likelihood Function of the First Order Moving Average
 Model,” Biometrika, 82, 232—234.

Hansen, L. P., and J. A. Scheinkman, 1995, “Back to the Future: Generating Moment Implications
 for Continuous-Time Markov Processes,” Econometrica, 63, 767—804.
Hasbrouck, J., 1993, “Assessing the Quality of a Security Market: A New Approach to Transaction-
 Cost Measurement,” Review of Financial Studies, 6, 191—212.
Heyde, C. C., 1997, Quasi-Likelihood and Its Application. Springer-Verlag, New York.
Jacod, J., 1996, “La Variation Quadratique du Brownien en Présence d’Erreurs d’Arrondi,”
 Astérisque, 236, 155—162.

Macurdy, T. E., 1982, “The Use of Time Series Processes to Model the Error Structure of Earnings
 in a Longitudinal Data Analysis,” Journal of Econometrics, 18, 83—114.

Madhavan, A., M. Richardson, and M. Roomans, 1997, “Why Do Security Prices Change?,” Review
 of Financial Studies, 10, 1035—1064.

                                                46
McCullagh, P., 1987, Tensor Methods in Statistics. Chapman and Hall, London, U.K.

McCullagh, P., and J. Nelder, 1989, Generalized Linear Models. Chapman and Hall, London, U.K.,
 second edn.

Merton, R. C., 1980, “On Estimating the Expected Return on the Market: An Exploratory Investi-
 gation,” Journal of Financial Economics, 8, 323—361.

Roll, R., 1984, “A Simple Model of the Implicit Bid-Ask Spread in an Eﬃcient Market,” Journal of
 Finance, 39, 1127—1139.

Shaman, P., 1969, “On the Inverse of the Covariance Matrix of a First Order Moving Average,”
 Biometrika, 56, 595—600.

White, H., 1982, “Maximum Likelihood Estimation of Misspecified Models,” Econometrica, 50, 1—25.




                                              47
                                   Figure 1
Various Discrete Sampling Modes — No Noise (Section 2), With Noise (Sections 3-6)
                   and Randomly Spaced with Noise (Section 7).

                                       48
                           Figure 2
                        2
RMSE of the Estimator σ̂ When the Presence of the Noise is Ignored




                                49
                                  Figure 3
Comparison of the Asymptotic Variances of the MLE σ̂2 Without and With Noise
                             Taken into Account




                                     50
                                   Figure 4
                        2
RMSE of the Estimator σ̂ When the Presence of Serially Correlated Noise is Ignored




                                        51
