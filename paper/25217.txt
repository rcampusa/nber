                                NBER WORKING PAPER SERIES




   ON THE INFORMATIVENESS OF DESCRIPTIVE STATISTICS FOR STRUCTURAL
                             ESTIMATES

                                           Isaiah Andrews
                                         Matthew Gentzkow
                                          Jesse M. Shapiro

                                        Working Paper 25217
                                http://www.nber.org/papers/w25217


                     NATIONAL BUREAU OF ECONOMIC RESEARCH
                                   1050 Massachusetts Avenue
                                     Cambridge, MA 02138
                                         November 2018
We acknowledge funding from the National Science Foundation (DGE-1654234), the Brown
University Population Studies and Training Center, the Stanford Institute for Economic Policy
Research (SIEPR), the Alfred P. Sloan Foundation, and the Silverman (1968) Family Career
Development Chair at MIT. We thank Tim Armstrong, Matias Cattaneo, Gary Chamberlain,
Liran Einav, Nathan Hendren, Yuichi Kitamura, Adam McCloskey, Costas Meghir, Ariel Pakes,
Ashesh Rambachan, Eric Renault, Jon Roth, Susanne Schennach, and participants at the Radcliffe
Institute Conference on Statistics When the Model is Wrong, the Fisher-Schultz Lecture, the HBS
Conference on Economic Models of Competition and Collusion, the University of Chicago
Becker Applied Economics Workshop, the UCL Advances in Econometrics Conference, the
Harvard-MIT IO Workshop, the BFI Conference on Robustness in Economics and Econometrics
(especially discussant Jinyong Hahn), the Cornell Econometrics-IO Workshop, and the Johns
Hopkins Applied Micro Workshop, for their comments and suggestions. We thank Nathan
Hendren for assistance in working with his code and data. The views expressed herein are those
of the authors and do not necessarily reflect the views of the National Bureau of Economic
Research.

At least one co-author has disclosed a financial relationship of potential relevance for this research.
Further information is available online at http://www.nber.org/papers/w25217.ack

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2018 by Isaiah Andrews, Matthew Gentzkow, and Jesse M. Shapiro. All rights reserved. Short sections
of text, not to exceed two paragraphs, may be quoted without explicit permission provided that full
credit, including © notice, is given to the source.
On the Informativeness of Descriptive Statistics for Structural Estimates
Isaiah Andrews, Matthew Gentzkow, and Jesse M. Shapiro
NBER Working Paper No. 25217
November 2018, Revised May 2020
JEL No. C18,D12,I13,I25

                                            ABSTRACT

We propose a way to formalize the relationship between descriptive analysis and structural
estimation. A researcher reports an estimate ĉ of a structural quantity of interest c that is exactly
or asymptotically unbiased under some base model. The researcher also reports descriptive
statistics γ̂ that estimate features γ of the distribution of the data that are related to c under the
base model. A reader entertains a less restrictive model that is local to the base model, under
which the estimate ĉ may be biased. We study the reduction in worst-case bias from a restriction
that requires the reader's model to respect the relationship between c and γ specified by the base
model. Our main result shows that the proportional reduction in worst-case bias depends only on
a quantity we call the informativeness of γ̂ for ĉ. Informativeness can be easily estimated even
for complex models. We recommend that researchers report estimated informativeness alongside
their descriptive analyses, and we illustrate with applications to three recent papers.


Isaiah Andrews                                     Jesse M. Shapiro
Department of Economics                            Economics Department
Harvard University                                 Box B
Littauer M18                                       Brown University
Cambridge, MA 02138                                Providence, RI 02912
and NBER                                           and NBER
iandrews@fas.harvard.edu                           jesse_shapiro_1@brown.edu

Matthew Gentzkow
Department of Economics
Stanford University
579 Jane Stanford Way
Stanford, CA 94305
and NBER
gentzkow@stanford.edu
1       Introduction

Empirical researchers often present descriptive statistics alongside structural estimates that answer
policy or counterfactual questions of interest. One leading case is where the structural model is
estimated on data from a randomized experiment, and the descriptive statistics are treatment-
control differences (e.g., Attanasio et al. 2012a; Duflo et al. 2012; Alatas et al. 2016). Another
is where the structural model is estimated on observational data, and the descriptive statistics are
regression coefficients or correlations that capture important relationships (e.g., Gentzkow 2007a;
Einav et al. 2013; Gentzkow et al. 2014; Morten 2019). Researchers often provide a heuristic
argument that links the descriptive statistics to key structural estimates, sometimes framing this
as an informal analysis of identification.1
    Such descriptive analysis has the potential to make structural estimates more interpretable.
Structural models are often criticized for lacking transparency, with large numbers of assumptions
and a high level of complexity making it difficult for readers to evaluate how the results might
change under plausible forms of misspecification (Heckman 2010; Angrist and Pischke 2010). If
a particular result were mainly driven by some intuitive descriptive features of the data, a reader
could focus on evaluating the assumptions that link those features to the result.
    In this paper, we propose a way to make this logic precise. A researcher is interested in a scalar
quantity of interest c (say, the effect of a counterfactual policy). The researcher specifies a base
model that relates the value of c to the distribution F of some data (say, the joint distribution of
the data in a randomized experiment). The researcher reports an estimate ĉ of c that is unbiased
(either exactly or asymptotically) under the base model. A reader of the research may not accept
all of the assumptions of the base model, and may therefore be concerned that ĉ is biased.
    The researcher also reports a vector γ̂ of descriptive statistics (say, sample mean outcomes
in different arms of the experiment). These statistics uncontroversially estimate some features
γ = γ (F ) of the distribution F (say, population mean outcomes in different arms). Because the
base model specifies the relationship between c and F , it also implicitly specifies the relationship
between c and γ, which may or may not be correct.
    Suppose the researcher is able to convince the reader of the relationship between c and γ specified
by the base model (say, by arguing that the counterfactual policy is similar to one of the arms of
the experiment). Should this lessen the reader’s concern about bias in ĉ, even if the reader does
not accept the base model in its entirety?
    We answer this question focusing on the worst-case bias when the alternative model contem-

    1
    See, for example, Fetter and Lockwood (2018, pp. 2200-2201), Spenkuch et al. (2018, pp. 1992-1993), and the
examples discussed in Andrews et al. (2017, 2020).


                                                      2
plated by the reader is local to the base model in an appropriate sense. To outline our approach,
it will be useful to define the base model as a correspondence F 0 (·), where F 0 (c) is the set of
distributions F consistent with a given value of c under the model. The identified set for c given
some F under the base model is found by taking the preimage of F under F 0 (·). We assume that
c is point identified under the base model, so that for any F consistent with the base model, the
identified set given F is a singleton.
      The reader contemplates a model that is less restrictive than the base model. We describe the
reader’s model by a correspondence F N (·), where F N (c) ⊇ F 0 (c) is the set of distributions F
consistent with a given value of c under the reader’s model. Because F N (c) ⊇ F 0 (c) for all c, the
identified set for c given some F is larger under the reader’s model than under the base model, and
may not be a singleton. Moreover, ĉ may be biased under the reader’s model. Let bN denote the
largest possible absolute bias in ĉ that can arise under F N (·), where this bound may be infinite.
      To formalize the idea that the reader’s model is local to the base model, we suppose that each
F̃ ∈ F N (c) lies in a neighborhood N (F ) of an F consistent with the base model, so that

                                                        n           o
(1)                                F N (c) = ∪F ∈F 0 (c) F̃ ∈ N (F ) .


We take the neighborhood N (F ) to contain distributions F̃ within a given statistical distance of
F.
      To formalize the possibility that the researcher convinces the reader of the relationship between
c and γ prescribed by the base model, we consider restricting attention to the elements F̃ ∈ N (F )
             
such that γ F̃ = γ (F ). This results in the restricted correspondence F RN (·)

                                                n                         o
(2)                       F RN (c) = ∪F ∈F 0 (c) F̃ ∈ N (F ) : γ F̃ = γ (F ) .


If F 0 (·) implies that only certain values of γ are consistent with a given value of c, then F RN (·)
preserves that implication whereas F N (·) may not. For this reason F N (c) ⊇ F RN (c) ⊇ F 0 (c) for
all c, i.e., the correspondence F RN (·) is less restrictive than the base model, but more restrictive
than the reader’s model. Let bRN denote the largest possible absolute bias in ĉ that can arise under
F RN (·). Because F RN (·) is more restrictive than F N (·), we know that bRN ≤ bN .
      We focus on characterizing the ratio bRN /bN , which lies between zero and one. Section 2 shows
how to derive the correspondences F N (·), F RN (·) , and F 0 (·), and the worst-case biases bN and
bRN , from explicitly parameterized economic models. Section 3 provides an exact characterization
of bRN /bN in a linear model with normal errors. Section 4 provides an approximate characterization
of bRN /bN in more general nonlinear models, obtained via a local asymptotic analysis. Sections 3


                                                    3
and 4 show that under given conditions the ratio bRN /bN (or its asymptotic analogue) is equal to
√
  1 − ∆, where ∆ is a scalar which we call the informativeness of the descriptive statistics γ̂ for the
structural estimate ĉ. Informativeness is the R2 from a regression of the structural estimate on the
descriptive statistics when both are drawn from their joint (asymptotic) distribution. Intuitively,
when informativeness is high, γ̂ captures most of the information in the data that determines ĉ.
We propose informativeness as a way to formalize the colloquial notion of the extent to which γ̂
“drives” ĉ.
    Informativeness can be estimated at low cost even for computationally challenging models.
Section 5 shows that a consistent estimator of ∆ can be obtained from manipulation of the estimated
influence functions of ĉ and γ̂. In the large range of settings in which estimated influence functions
are available from the calculations used to obtain ĉ and γ̂, the additional computation required
to estimate ∆ is trivial. We recommend that researchers report an estimate of informativeness
whenever they present descriptive evidence as support for structural estimates.
    Section 6 implements our proposal for three recent papers in economics, each of which reports or
discusses descriptive statistics alongside structural estimates. In the first application, to Attanasio
et al. (2012a), the quantity c of interest is the effect of a counterfactual redesign of the PROGRESA
cash transfer program, and the descriptive statistics γ̂ are sample treatment-control differences for
different groups of children. In the second application, to Gentzkow (2007a), the quantity c of
interest is the effect of removing the online edition of the Washington Post on readership of the print
edition, and the descriptive statistics γ̂ are linear regression coefficients. In the third application,
to Hendren (2013a), the quantity c of interest is a parameter governing the existence of insurance
markets, and the descriptive statistics γ̂ summarize the joint distribution of self-reported beliefs
about the likelihood of loss events and the realizations of these events. In each case, we report an
estimate of ∆ for various definitions of γ̂, and we discuss the implications for the interpretation
of ĉ. These applications illustrate how estimates of ∆ can be presented and discussed in applied
research.
    Important limitations of our analysis include the use of asymptotic approximations to describe
the behavior of estimators, and the use of a purely statistical notion of distance to define sets of
alternative models. Ideally one would like to use exact finite-sample properties to characterize the
bias of estimators, and economic knowledge to define sets of alternative models. We are not aware
of convenient procedures that achieve this ideal in the generality that we consider. We therefore
propose the use of informativeness as a practical option to improve the precision of discussions of
the connection between descriptive statistics and structural estimates in applied research.
    Our results are related to Andrews et al. (2017). In that paper, we propose a measure Λ of the


                                                   4
sensitivity of a parameter estimate ĉ to a vector of statistics γ̂, focusing on the case where γ̂ are
estimation moments that fully determine the estimator ĉ (and so ∆ = 1).2 In Online Appendix A,
we generalize our main result to accommodate the setting of Andrews et al. (2017) and so provide
a unified treatment of sensitivity and informativeness.
   In a related paper, Mukhin (2018) derives informativeness and sensitivity from a statistical-
geometric perspective, and notes strong connections to semiparameteric efficiency theory. Mukhin
also shows how to derive sensitivity and informativeness measures based on alternative metrics for
the distance between distributions, and discusses the use of these measures for local counterfactual
analysis.
   Our work is also closely related to the large literature on local misspecification (e.g., Newey 1985;
Conley et al. 2012; Andrews et al. 2017). Much of this literature focuses on testing and confidence
set construction (e.g. Berkowitz et al. 2008; Guggenberger 2012; Armstrong and Kolesár, 2019) or
robust estimation (e.g., Rieder 1994; Kitamura et al. 2013; Bonhomme and Weidner 2018). Rieder
(1994) studies the choice of target parameters and proposes optimal robust testing and estimation
procedures under forms of local misspecification including the one that we consider here. Bonhomme
and Weidner (2018) derive minimax robust estimators and accompanying confidence intervals for
economic parameters of interest under a form of local misspecification closely related to the one we
study. Armstrong and Kolesár (2019) consider a class of ways in which the model may be locally
misspecified that nests the one we consider, derive minimax optimal confidence sets, and show that
there is limited scope to improve on their procedures by “estimating” the degree of misspecification,
motivating a sensitivity analysis. In contrast to this literature, we focus on characterizing the
relationship between a set of descriptive statistics and a given structural estimator, with the goal of
allowing readers of applied research to sharpen their opinions about the reliability of the researcher’s
conclusions, thus improving transparency in the sense of Andrews et al. (2020).
   Our use of statistical distance to characterize the degree of misspecification relates to a number
of recent papers. Our results cover the Cressie-Read (1984) family, which nests widely studied
measures including the Kullback-Leibler divergence, Hellinger divergence, and many others, up to
a monotone transformation. Kullback-Leibler divergence has been used to measure the degree of
misspecification by, for example, Hansen and Sargent (2001), Hansen and Sargent (2005), Hansen et
al. (2006), Hansen and Sargent (2016), and Bonhomme and Weidner (2018). Hellinger divergence
has been used by, for example, Kitamura et al. (2013).
   Finally, our work relates to discussions about the appropriate role of descriptive statistics in
structural econometric analysis (e.g., Pakes 2014).3 It is common in applied research to describe
   2
       The present paper draws on the analysis of “sensitivity to descriptive statistics” in Gentzkow and Shapiro (2015).
   3
       See also Dridi et al. (2007) and Nakamura and Steinsson (2018) for discussion of the appropriate choice of


                                                            5
the data features that “primarily identify” structural parameters or “drive” estimates of those pa-
rameters.4 As Keane (2010) and others have noted, such statements are not directly related to
the formal notion of identification in econometrics (see also Andrews et al. 2020). Their intended
meaning is therefore up for grabs. If researchers are prepared to reinterpret these as statements
about informativeness, then our approach provides a way to sharpen and quantify these statements
at low cost to researchers.


2     Setup and Key Definitions

The introduction describes our approach in terms of correspondences between the quantity of
interest c and the distribution F of the data. In this section we first show how to derive these cor-
respondences from explicitly parameterized economic models, and then use these correspondences
to define the worst-case biases that we characterize in our analysis. Section 4 defines analogous
objects in a local asymptotic framework.
     Suppose that, under the base model considered by the researcher, both the distribution of the
data F and the quantity of interest c are determined by a structural parameter η ∈ H. Formally,
under the base model, we have that F = F (η) and c = c (η) so the correspondence F 0 (·) is given
by
                                       F 0 (c) = {F (η) : η ∈ H, c (η) = c} .

Because the structural parameter η determines the distribution F , it also determines γ = γ (η) =
γ (F (η)).
     Suppose further that, under the reader’s model, the distribution of the data F is determined
by η and by a misspecification parameter ζ ∈ Z (say, indexing economic forces omitted from the
researcher’s model) that is normalized to zero under the base model. Formally, under the reader’s
model we have that F = F (η, ζ), with F (η, 0) = F (η) for all η ∈ H, and correspondingly that
γ = γ (η, ζ) = γ (F (η, ζ)), with γ (η, 0) = γ (η) for all η ∈ H. We focus on settings where forms of
misspecification indexed by ζ are rich, in the sense that the range of F (η, ζ) under ζ ∈ Z does not
depend on η. For simplicity we continue to write the quantity of interest as a function of η alone,
c = c (η). Forms of misspecification that affect the mapping from η to c but preserve its range are
equivalent to those we study.5

moments to match when fitting macroeconomic models.
    4
      Andrews et al. (2017, footnotes 2 and 3) provide examples.
    5
      Specifically, consider a model where the distribution of the data is F = F (η, ζ) as above, while the quantity of
interest is c = c̃ (η, ζ), and the range of c̃ (η, ζ) under η ∈ H for any ζ ∈ Z is the same as that of c (η) under η ∈ H. In
this case, the sets {(c (η) , F (η, ζ)) : η ∈ H, ζ ∈ Z} and {(c̃ (η, ζ) , F (η, ζ)) : η ∈ H, ζ ∈ Z} are both Cartesian prod-
ucts, equal to {c (η) : η ∈ H} × {F (η, ζ) : η ∈ H, ζ ∈ Z}. Consequently, the correspondences F (·) that we consider


                                                             6
    We formalize the idea that the reader’s model is local to the base model as follows. Let r (η, ζ) ≥
0 denote some Cressie-Read (1984) divergence between the distribution F (η) and the distribution
F (η, ζ), so that r (η, 0) = 0 for all η ∈ H. For any distribution F = F (η) consistent with the
base model, we define the neighborhood N (F ) to consist of all distributions F (η, ζ) such that the
divergence r (η, ζ) is less than some scalar bound µ ≥ 0:


                          N (F ) = {F (η, ζ) : η ∈ H, F (η) = F, ζ ∈ Z, r (η, ζ) ≤ µ} .


We then define the reader’s model F N (·) as in (1).6 The neighborhood N (F ) is increasing in µ.
Hence, larger values of µ imply imply a greater relaxation of assumptions as we move from the base
model F 0 (·) to the reader’s model F N (·) . We suppress the dependence of N (F ) and F N (c) on µ
for brevity.
    The base model specifies a relationship between c and γ in the sense that if the quantity of inter-
est takes value c, then the feature γ must take a value γ (η) for some η ∈ H such that c = c (η). The
reader’s model F N (·) need not respect the base model’s specification of the relationship between
c and γ. By contrast, the model F RN (·), defined in (2), respects the base model’s specification of
the relationship between c and γ in the sense that, for any F ∈ F RN (c), there is some η ∈ H such
that c = c (η), γ (F ) = γ (η), and F ∈ N (F (η)) .7 Hence, a given (c, γ) pair is compatible with
F RN (·) if and only if it is compatible with F 0 (·) .
    The researcher chooses an estimator ĉ that is unbiased under the base model in the sense
that EF [ĉ − c] = 0 for any F ∈ F 0 (c), where EF [·] denotes the expectation when the data are
distributed according to F . The estimator ĉ may be biased under the reader’s model F N (·), and
indeed if we take µ to infinity the parameter c is completely unidentified under F N (·). The largest
absolute bias in ĉ that is possible under F N (·) is


                                             bN = sup       sup        |EF [ĉ − c]| .
                                                      c   F ∈F N (c)


    Considering F RN (·) rather than F N (·) can reduce the worst-case bias in ĉ. The largest absolute
bias in ĉ that is possible under F RN (·) is


                                           bRN = sup         sup        |EF [ĉ − c]| .
                                                      c   F ∈F RN (c)


The proportional reduction in worst-case bias from limiting attention to F RN (·) is measured by
are the same whether constructed from the model with c = c (η) or that with c = c̃ (η, ζ).
    6
      Specifically, F N (c) = {F (η, ζ) : η ∈ H, c (η) = c, ζ ∈ Z, r (η, ζ) ≤ µ} .
    7
      To see that this is the case, note that F RN (c) = {F (η, ζ) : η ∈ H, c (η) = c, ζ ∈ Z, r (η, ζ) ≤ µ, γ (F (η, ζ)) = γ (η)} .


                                                                7
the ratio bRN /bN , which is the primary focus of our analysis.


3          Informativeness in a Linear Normal Setting

To build intuition for our approach, we next specialize to a linear normal setting and provide an
exact characterization of the ratio bRN /bN . We illustrate with a stylized example, and conclude
the section with some further discussion of our approach and its limitations.


3.1         Characterization of Worst-Case Bias

Now suppose that H = Rp , Z = Rk , and that under F (η, ζ) the data Y ∈ Rk follow


(3)                                              Y ∼ N (Xη + ζ, Ω)


for X and Ω known, nonrandom matrices with full column rank.
      The quantity of interest is some linear function c (η) = L0 η of the parameters, with L ∈ Rp×1 a
known, non-random vector. The researcher chooses a linear estimator ĉ = C 0 Y for C a vector. The
researcher ensures that ĉ is unbiased for c under F 0 (·) by choosing C 0 = L0 M for some matrix M
with M X = Ip .
      The researcher computes the vector γ̂ = Γ0 Y of descriptive statistics, with Γ ∈ Rk×pγ a known,
non-random matrix. The vector γ̂ is trivially unbiased for γ (η, ζ) = Γ0 (Xη + ζ).
      Absent any restriction on ζ the quantity of interest c is entirely unidentified. Intuitively, without
any restriction on ζ, the mean of the data Y is entirely unrestricted for any fixed η, making it
impossible to learn c = L0 η. The reader’s model F N (·) limits the size of ζ. In particular, given (3),
the assumption that r (·, ·) is in the Cressie-Read family implies that r (η, ζ) is a strictly increasing
                                         √
transformation of kζkΩ−1 , for kV kA = V 0 AV . Thus, for this section we define N (·) based on the
restriction kζkΩ−1 ≤ µ.8
      Under the base model F 0 (·),
                                                                                               
                    ĉ              L0 η                          σc2   Σcγ           C 0 ΩC   C 0 ΩΓ
                         ∼ N             , Σ for Σ =                   =                       .
                    γ̂              Γ0 Xη                         Σγc Σγγ             Γ0 ΩC    Γ0 ΩΓ

We assume that σc2 > 0 and that Σγγ has full rank.


      8
          That is, we let              
                               N (F ) = F (η, ζ) : η ∈ H, F (η) = F, ζ ∈ Z, kζkΩ−1 ≤ µ .




                                                          8
Definition. The informativeness of γ̂ for ĉ is

                                              Σcγ Σ−1
                                                   γγ Σγc
                                        ∆=                ∈ [0, 1] .
                                                  σc2

Informativeness is the R2 from the population regression of ĉ on γ̂ under their joint distribution.
Informativeness determines the ratio of worst-case biases bRN /bN .

Proposition 1. The set of possible biases under F N (·) is


                                  EF [ĉ − c] : F ∈ F N (c) = [−µσc , µσc ]
                              


for any c, while the set of possible biases under F RN (·) is

                                                  h    √          √      i
                      EF [ĉ − c] : F ∈ F RN (c) = −µσc 1 − ∆, µσc 1 − ∆
                     


                                      √
for any c. Hence, bN = µσc , bRN = µσc 1 − ∆, and

                                             bRN  √
                                                 = 1 − ∆.
                                              bN

All proofs are collected at the end of the paper.
   Importantly, the value of ∆, and hence the proportional reduction in worst-case bias from
restricting from F N (·) to F RN (·), does not depend on µ. In addition to characterizing the worst-
case biases bRN and bN , Proposition 1 characterizes the set of possible biases under F N (·) and
F RN (·), showing in particular that any absolute bias smaller than the worst case is achievable.
Imposing additional restrictions on ζ, beyond those captured by F N (·) or F RN (·), could further
reduce the worst-case bias.


3.2   Example

To fix ideas, suppose that a researcher observes i.i.d. data from a randomized evaluation of a
conditional cash transfer program. The program gives each household a payment of size s if their
children attend school regularly. Households are uniformly randomized among subsidy levels s ∈
{0, 1, 2}. We can think of those receiving s = 0 as the control group.
   The data consist of the average school attendance Ys of children assigned subsidy s ∈ {0, 1, 2}.
The quantity of interest c is the expected attendance at a counterfactual subsidy level s∗ > 2.




                                                     9
Under the base model the mean of Ys is given by


(4)                                                            η1 + η2 s


for s ∈ {0, 1, 2, s∗ }. Average attendance Ys is independent and homoskedastic across arms of the
experiment, with standard deviation ω.9
       Under the base model F 0 (·), c can be estimated by linear extrapolation of average attendance
from two or more of the observed subsidy levels s ∈ {0, 1, 2} to subsidy level s∗ . We continue to
assume that the researcher chooses a linear estimator ĉ that is unbiased for c under F 0 (·).10
       Under the reader’s model F N (·), the estimator ĉ may be biased. Intuitively, if ζ 6= 0 then
the mean of Ys may be nonlinear in s, so that linear extrapolation to s∗ may produce a biased
estimate of c. The restriction to F RN (·) can lessen the scope for bias. The economic content of the
restriction depends on the choice of Γ, which in turn determines the descriptive statistic γ̂ = Γ0 Y
and the informativeness ∆.
       As a concrete example, suppose that a reader entertains that the effect of incentivizing school
attendance may be discontinuous at zero, with the mean of Ys for s ∈ {0, 1, 2, s∗ } given by


(5)                                                    η̃0 + 1 {s > 0} η̃1 + sη̃2


for η̃ a composite of η and ζ with η̃1 6= 0.11 The model F N (·) allows that the mean of Ys may follow
(5), as long as η̃1 is sufficiently small.12 The bound bN thus reflects a worst case over scenarios that
include (5).
   Whether the set F RN (·) allows that the mean of Ys may follow (5) depends on the choice of
                                                                                         
Γ. If Γ = e1 e2 for es the basis vector corresponding to subsidy s, so that γ̂ = Y1 Y2 ,
then under F RN (·) the mean of Ys is linear in s for s > 0, which is consistent with (5). If instead

      9
           To cast this example into the notation of Section 3.1, take
                                                         
                                                   1 0                        
                                                                            0
                                           X =  1 1  , Ω = ω 2 I3 , L =        .
                                                                            s∗
                                                   1 2

                                                  −1
      For example, if we take M = (X 0 X) X 0 , then ĉ is the the ordinary least squares extrapolation to s∗ , and is
      10

also the maximum likelihood estimator of c under F 0 (·).
   11
      Specifically, choose η̃ so that
                                                  X̃ η̃ = ζ + Xη
for                                                                         
                                                                1      0   0
                                                         X̃ =  1      1   1 .
                                                                1      1   2
      12
           In particular, to ensure that a given η̃ is consistent with kζkΩ−1 ≤ µ, it suffices that |η̃1 | ≤ ωµ.


                                                                  10
                                                 
Γ =        e0 e1       , so that γ̂ =       Y0 Y1       , then under F RN (·) the mean of Ys is linear in s for
s 6= 2, which is not consistent with (5). The bound bRN thus reflects a worst case over scenarios
that may or may not include (5), depending on the choice of Γ.
    The informativeness ∆ measures the extent to which the restriction to F RN (·) lessens the scope
                   √
for bias, bRN /bN = 1 − ∆. Again imagine a reader who entertains that the mean of Ys may follow
                                                    
(5). Learning that ∆ is close to one for γ̂ = Y1 Y2 might be reassuring to this reader because
the restriction from F N (·) to F RN (·) greatly lessens the scope for bias in ĉ while still allowing for
                                                        
(5). Learning that ∆ is close to one for γ̂ = Y0 Y1 might not be as reassuring, because in this
case the restriction from F N (·) to F RN (·) rules out (5).


3.3     Discussion

3.3.1      Relationship to Analysis of Identification

Our analysis is distinct from an analysis of identification. We focus on the behavior of a particular
estimator ĉ under misspecification, taking as given that c is identified under the base model. This
is distinct from asking whether the identification of c is parametric or nonparametric, and from
asking how the identified set changes under misspecification. To see the latter point, consider a case
where γ̂ is an unbiased estimator of c under F 0 (·), but differs from ĉ.13 An analysis of identification
would conclude that c is point-identified under F RN (·), whereas our analysis would conclude that
the estimator ĉ may be biased under F RN (·).
    We can connect our analysis to an analysis of identification if we consider identification from
the distribution of ĉ alone. In particular, Proposition 1 implies that the identified set for c based on
                                                                           √               √
the distribution of ĉ is [ĉ − µσc , ĉ + µσc ] under F N (·) and ĉ − µσc 1 − ∆, ĉ + µσc 1 − ∆ under
                                                                                                 

F RN (·). Under this interpretation, the ratio bRN /bN measures how much the identified set shrinks
when we restrict from F N (·) to F RN (·).


3.3.2      Interpretation and Limitations

We pause here to discuss some other aspects and limitations of our approach.
    First, our analysis focuses on bounding the absolute bias of the estimator ĉ. Since the variance
of ĉ is unaffected by misspecification, there is a one-to-one relationship between absolute bias and
MSE. So, for fixed µ, ∆ governs the extent to which restricting from F N (·) to F RN (·) reduces the
maximal MSE for ĉ. Unlike for absolute bias, however, the ratio of worst-case MSEs under F N (·)
and F RN (·) depends in general on µ.
  13
    For instance, γ̂ might be an estimator based on matching a statistically non-sufficient set of moments, while ĉ
might be the maximum likelihood estimator.


                                                             11
      Second, the correspondence F RN (·) requires that the relationship between c and γ specified
by the base model be correct local to each point in the base model. This is more restrictive
than requiring that the pair (c, γ) be globally consistent with the base model, which yields the
correspondence F GN (·) with

                                                       n                  o
(6)                    F GN (c) = F N (c) ∩ ∪F ∗ ∈F 0 (c) F̃ : γ F̃ = γ (F ∗ )


for all c. If any (c, γ) pair is possible under F 0 (·), then F GN (·) is equivalent to F N (·), but F RN (·)
need not be. More generally F N (c) ⊇ F GN (c) ⊇ F RN (c) ⊇ F 0 (c), and the ratio of worst case
                                                                        √
bias under F GN (·) to worst-case bias under F N (·) is bounded below by 1 − ∆.
      Third, we see the use of statistical distance to define the neighborhoods N (F ) as a key potential
limitation of our analysis. While defining neighborhoods in this way provides a practical default for
many situations, it also means that the informativeness ∆ depends on the sampling process that
generates the data. To illustrate, suppose we are interested in estimating the average treatment
effect c of some policy, that ĉ is a treatment-control difference from an RCT, and that γ̂ is the control
group mean from the same RCT. If the control group is much larger than the treatment group,
variability in ĉ will primarily be driven by the treatment group mean, and the informativeness of γ̂
for ĉ will be low. If, on the other hand, the control group is much smaller than the treatment group,
variability in ĉ will primarily be driven by the control group mean, and the informativeness of γ̂
for ĉ will be high. Thus, the informativeness of the control group mean for the average treatment
effect estimate in this setting depends on features of the experimental design, and not solely on
economic objects such as the distribution of potential outcomes.


4      Informativeness Under Local Misspecification

This section translates our results on finite-sample bias in the linear normal model to results on
asymptotic bias in nonlinear models with local misspecification. We first introduce our asymptotic
setting and state regularity conditions. We then prove our main result under local misspecification,
develop intuition for the local misspecification neighborhoods we consider, and discuss a version of
our analysis based on probability limits.
      We assume that a researcher observes an i.i.d. sample Di ∈ D for i = 1, ..., n. The researcher
considers a base model which implies that Di ∼ F (η), for η ∈ H a potentially infinite-dimensional
parameter. The implied joint distribution for the sample is ×ni=1 F (η). The parameter of interest
remains c (η) . The researcher computes (i) a scalar estimate ĉ of c and (ii) a pγ × 1 vector of
descriptive statistics γ̂.


                                                     12
    As in Section 2, to allow the possibility of misspecification we suppose that under the reader’s
model Di ∼ F (η, ζ) for some (η, ζ) ∈ H × Z, where F (η, 0) = F (η) for all η ∈ H. The joint
distribution for the sample under the reader’s model is ×ni=1 F (η, ζ). Defining the correspondences
F N (·) and F RN (·) as in Section 2, we are interested in the ratio of worst-case biases bRN /bN .
    While Section 3 exactly characterizes bRN /bN in the linear normal model, we are not aware of
similarly tractable expressions in general nonlinear settings. In this section, we therefore instead
approximate bRN /bN by characterizing the first-order asymptotic bias of the estimator ĉ under
sequences of data generating processes in which (η, ζ) approaches a base value (η0 , 0) ∈ H × Z at
a root-n rate.
    Formally, define H and Z as sets of values such that for any h ∈ H and z ∈ Z, we have
η0 + th ∈ H and tz ∈ Z for t ∈ R sufficiently close to zero.14 For Fh,z (th , tz ) = F (η0 + th h, tz z),
we consider behavior under sequences of data generating processes
                                                                          ∞
                                                                     1  1
                                  S (h, z) =       ×ni=1 Fh,z       √ ,√           .
                                                                      n n    n=1


    The statement that (η, ζ) approaches (η0 , 0) at a root-n rate should not be taken literally to
imply that the data generating process depends on the sample size, but is instead a device to approx-
imate the finite-sample behavior of estimators in situations where the influence of misspecification
is on the same order as sampling uncertainty.15 Section 4.4 instead considers fixed misspecification
and develops results based on probability limits.
    Throughout our analysis, we state assumptions in terms of the base distribution F0 = F (η0 ) .
If these assumptions hold for all η0 ∈ H then our local asymptotic approximations are valid local
to any point in the base model, though many of the asymptotic quantities we consider will depend
on the value of η0 . Section 5 discusses consistent estimators of these quantities that do not require
a priori knowledge of η0 .


4.1    Regularity Conditions

We next discuss a set of regularity conditions used in our asymptotic results. Our first assumption
requires that ĉ and γ̂ behave, asymptotically, like sample averages.




  14
    For η0 + th 6∈ H or tz 6∈ Z we may define distributions arbitrarily.
  15
    The order √1n perturbation to the base-model parameter η is a common asymptotic tool to analyze the local
behavior of estimators (see for example Chapters 7-9 of van der Vaart, 1998). Setting the degree of misspecification
proportional to √1n is likewise a common technique for modeling local misspecification (see e.g. Newey (1985),
Andrews et al. (2017), and Armstrong and Kolesár (2019)).


                                                          13
Assumption 1. Under S (0, 0) ,

                                                                  n                     n
                                                                                                          !
                  √                                      1        X                     X
(7)                   n (ĉ − c (η0 ) , γ̂ − γ (η0 )) = √                 φc (Di ) ,           φγ (Di )       + op (1) ,
                                                          n
                                                                  i=1                   i=1


for functions φc (Di ) and φγ (Di ), where EF0 [φc (Di )] = 0, EF0 [φγ (Di )] = 0. For
                                                          h                i            
                                                                         2             0
                          σc2   Σcγ         EF0 φc (Di )          EF0 φc (Di ) φγ (Di )
               Σ=                    =                                                  ,
                                                                                       0
                          Σγc Σγγ         EF0 [φγ (Di ) φc (Di )] EF0 φγ (Di ) φγ (Di )

Σ is finite, σc2 > 0, and Σγγ is positive-definite.

      The functions φc (Di ) and φγ (Di ) are called the influence functions for the estimators ĉ and
γ̂, respectively. Asymptotic linearity of the form in (7) holds for a wide range of estimators (see
e.g. Ichimura and Newey 2015), though it can fail for James-Stein, LASSO, and other shrinkage
estimators (e.g. Hansen 2016). Asymptotic linearity immediately implies that ĉ and γ̂ are jointly
asymptotically normal under S (0, 0).
      We next strengthen asymptotic normality of (ĉ, γ̂) to hold local to η0 under the base model.
We impose the following.

Assumption 2. Let γ (η) denote the probability limit of γ̂ under ×ni=1 F (η), and assume that for
                                                                                                   
all h ∈ H, γ (η0 + th) exists for t sufficiently close to zero. For any h ∈ H, cn (h) = c η0 + √1n h ,
                          
and γn (h) = γ η0 + √1n h , under S (h, 0) we have

                                                                                             
                                     √        cn (h) − c (η0 )                        c? (h)
                                         n                           →                       ,
                                              γn (h) − γ (η0 )                        γ ? (h)

and moreover
                                                                                               
                                 √        ĉ − c (η0 )                            c? (h)
                                     n                   →d N                            , Σ .
                                          γ̂ − γ (η0 )                            γ ? (h)

      The first part of Assumption 2 requires that cn (h) and γn (h) be asymptotically well-behaved, in
the sense that with appropriate recentering and scaling they converge to limits that can be written
as functions of h. Under this assumption, we can interpret c? (h) as the local parameter of interest,
playing the same role in our local asymptotic analysis as the parameter c does in the normal model.
      The second part of Assumption 2 requires that (ĉ, γ̂) be a regular estimator of (c (η) , γ (η)) at η0
under the base model (see e.g., Newey 1994), and is again satisfied under mild primitive conditions


                                                                 14
in a wide range of settings. In particular, this assumption implies that ĉ is asymptotically unbiased
for our local parameter of interest c? (h) under S (h, 0).
    We next assume the distributions F (η, ζ) have densities f (d; η, ζ) with respect to a common
dominating measure ν. For (th , tz ) ∈ R2 , if we consider the perturbed distributions Fh,z (th , tz )
with densities fh,z (d; th , tz ) then the information matrix for (th , tz ), treating (h, z) as known, is

                                                    ∂
                                                                            2         ∂
                                                                                                                                 
                                                         f (Di ;th ,tz )
                                                     ∂th h,z
                                                                                          f (Di ;th ,tz ) ∂ fh,z (Di ;th ,tz )
                                                                                      ∂th h,z              ∂tz
                                                      fh,z (Di ;th ,tz )               fh,z (Di ;th ,tz )  fh,z (Di ;th ,tz )   
     Ih,z (th , tz ) = EFh,z (th ,tz )     ∂
                                                                                                                      2        .
                                              f (Di ;th ,tz ) ∂ fh,z (Di ;th ,tz )
                                           ∂th h,z
                                                                                                 ∂
                                                                                                    f (Di ;th ,tz )              
                                                                ∂tz                             ∂tz h,z
                                             fh,z (Di ;th ,tz )  fh,z (Di ;th ,tz )               fh,z (Di ;th ,tz )



We consider the two-dimensional submodels obtained by fixing (h, z), Fh,z (th , tz ) : (th , tz ) ∈ R2 ,
                                                                    

and impose a sufficient condition for these models to be differentiable in quadratic mean at zero.

Assumption 3. For all h ∈ H, z ∈ Z, there exists an open neighborhood of zero such that for
                                    p
(th , tz ) in this neighborhood, (i) fh,z (d; th , tz ) is continuously differentiable with respect to (th , tz )
for all d ∈ D and (ii) Ih,z (th , tz ) is finite and continuous in (th , tz ).

    Assumption 3 imposes standard conditions used in deriving asymptotic results, and holds in a
wide variety of settings; see Chapter 7.2 of van der Vaart (1998) for further discussion.
    Finally, we require that the forms of misspecification we consider be sufficiently rich. To state
                                                         ∂                                          ∂
this assumption, let us define sh (d) =                 ∂th   log (fh,z (d; 0, 0)), sz (d) =       ∂tz   log (fh,z (d; 0, 0)) as the
score functions corresponding to h and z, respectively.

Assumption 4. The set of score functions sz (·) includes all those consistent with Assumption 3,
                                                              h        i
in the sense that for any s (·) with EF0 [s (Di )] = 0 and EF0 s (Di )2 < ∞ there exists z ∈ Z with
     h                   i
EF0 (s (Di ) − sz (Di ))2 = 0.

    Assumption 4 requires that the set of score functions sz (Di ) implied by z ∈ Z include all those
consistent with Assumption 3.16 Intuitively, this means that the set of nesting model distributions
holding η fixed at η0 , {F (η0 , ζ) : ζ ∈ Z}, looks (locally) like the set of all distributions, and so is
the local analogue of the richness condition discussed in Section 2. If this assumption fails, the
local asymptotic bias bounds we derive below continue to hold, but need not be sharp.
    Under Assumption 4, the nesting model allows forms of misspecification against which all spec-
ification tests that control size have trivial local asymptotic power.17 This highlights an important
   16
      That the score function sz (Di ) has mean zero and finite variance under Assumption 3 follows from Lemma 7.6
and Theorem 7.2 in van der Vaart (1998).
   17
      In particular, for h ∈ H, Assumption 4 implies that there exists z ∈ Z such that EF0 (sh (Di ) − sz (Di ))2 = 0.
                                                                                                                

Arguments along the same lines as e.g. Chen and Santos (2018) then imply that S (h, 0) and S (0, z) are asymptotically
indistinguishable, and thus that no specification test which controls the rejection probability under S (h, 0) has
nontrivial power against S (0, z).


                                                                   15
aspect of our local analysis. A possible justification for bounding the degree of misspecification
(see, e.g., Huber and Ronchetti 2009, p. 294, as quoted in Bonhomme and Weidner 2018) is that
specification tests eventually detect unbounded misspecification with arbitrarily high probability, so
conditional on non-rejection it is reasonable to focus on bounded, and in particular local, misspec-
ification. By contrast, we allow some forms of misspecification that are statistically undetectable
absent knowledge of the true parameters. Hence, restrictions on the magnitude of misspecification
in our setting should be understood as a-priori restrictions on the set of models considered, rather
than a-posteriori restrictions based on which models survive specification tests.


4.2     Main Result Under Local Misspecification

We can now derive the analogue of Proposition 1 in our local asymptotic framework. As a first
                                         √
step, we note that under our assumptions, n (ĉ − c (η0 ) , γ̂ − γ (η0 )) is asymptotically normal with
variance Σ. Moreover, we obtain a simple expression for its asymptotic mean.

Lemma 1. If Assumptions 1-3 hold, then under S (h, z) for any (h, z) ∈ H × Z,
                                                                                    
                          √        ĉ − c (η0 )                     c̄ (S (h, z))
                              n                   →d N                           , Σ ,
                                   γ̂ − γ (η0 )                    γ̄ (S (h, z))

where                                                                                       
                          c̄ (S (h, z))               EF0 [φc (Di ) (sh (Di ) + sz (Di ))]
                                         =                                                  .
                          γ̄ (S (h, z))               EF0 [φγ (Di ) (sh (Di ) + sz (Di ))]

Moreover, c? (h) = EF0 [φc (Di ) sh (Di )], and γ ? (h) = EF0 [φγ (Di ) sh (Di )] .

   Recall that c? (h) is the parameter of interest in our local asymptotic analysis. We can thus
interpret c̄ (S (h, z)) − c? (h) = EF0 [φc (Di ) sz (Di )] as the first-order asymptotic bias of ĉ under
S (h, z), analogous to EF [ĉ − c] under the normal model.
   As in the normal model we restrict the degree of misspecification. We first consider the case of
correct specification. Let
                                   S 0 (c? ) = {S (h, 0) : h ∈ H, c? (h) = c? }

denote the set of sequences in the base model such that the local parameter of interest takes value
c? . Limiting attention to sequences S ∈ S 0 (c? ) imposes correct specification, and is analogous to
limiting attention to F 0 (c).
   To relax the assumption of correct specification, next suppose we bound the degree of local




                                                           16
misspecification by µ ≥ 0. For S ∈ S 0 (·) =                        S 0 (c? ) , let us define the neighborhood
                                                           S
                                                               c?

                                                                          h          i1   
                                                                                     2 2
                  N (S) =        S (h, z) : h ∈ H, S (h, 0) = S, z ∈ Z, EF0 sz (Di )     ≤µ .


For reasons elaborated in Section 4.3 below, N (S) is a sequence-space analogue of the neighborhood
N (F ) defined in Section 2. Taking a union over N (S) for S ∈ S 0 (c? ) yields

                                                               [          n          o
                                         S N (c? ) =                       S̃ ∈ N (S) ,
                                                       S∈S 0 (c? )


which we can interpret as the sequence-space analogue of F N (c).
      Finally, let us define a restricted set of sequences as

                                                [          n                         o
                              S RN (c? ) =                  S̃ ∈ N (S) : γ̄ S̃ = γ̄ (S) .
                                             S∈S 0 (c? )


Limiting attention to sequences S ∈ S RN (c? ) is analogous to limiting attention to the set F RN (c).
      Let b?N and b?RN denote the worst-case first–order asymptotic bias under S N (·) and S RN (·),
respectively:


(8)                                        b?N = sup            sup        |c̄ (S) − c? |
                                                     c? S∈S N (c? )




(9)                                      b?RN = sup             sup         |c̄ (S) − c? | .
                                                    c? S∈S RN (c? )


Our main result under local misspecification is analogous to Proposition 1 under the normal model.

Proposition 2. Under Assumptions 1-4, the set of first-order asymptotic biases for ĉ under S ∈
S N (·) is
                                     c̄ (S) − c? : S ∈ S N (c? ) = [−µσc , µσc ] ,
                                    


for any c? such that S N (c? ) is nonempty, while the set of first-order asymptotic biases under
S ∈ S RN (·) is
                                    ?          RN         ?
                                                                      h        √               √   i
                           c̄ (S) − c : S ∈ S        (c ) = −µσc 1 − ∆, µσc 1 − ∆ ,

for any c? such that S RN (c? ) is nonempty. Hence,

                                                     b?RN  √
                                                        ? = 1 − ∆.
                                                      bN


                                                                    17
4.3    Scaling of Perturbations
                                           h        i
Under regularity conditions, the bound EF0 sz (Di )2 ≤ µ in the definition of N (S) can be inter-
                                                                                            
preted as a bound on the asymptotic Cressie-Read divergence of Fh,z √1n , √1n from Fh,z √1n , 0 .
   Specifically, we consider divergences of the form
                                                                                  
                                                               fh,z (Di ; th , tz )
(10)                        rh,z (th , tz ) = EFh,z (th ,0) ψ
                                                               fh,z (Di ; th , 0)

for ψ (·) a twice continuously differentiable function with ψ (1) = 0 and ψ 00 (1) = 2. A leading class
of such divergences is the Cressie-Read (1984) family, which takes

                                                     2             
                                      ψ (x) =                x−λ − 1 .
                                                 λ (λ + 1)

Many well-known measures for the difference between distributions, including Kullback-Leibler
divergence and Hellinger distance, can be expressed as monotone transformations of Cressie-Read
(1984) divergences for appropriate λ.
   Online Appendix B shows that under regularity conditions

                                                               h         i
(11)                              lim n · rh,z (th , tz ) = EF0 sz (Di )2 .
                                 n→∞


Hence, Cressie-Read (1984) divergences yield the same asymptotic ranking over values of z, and
                                                         h         i
therefore over sequences S (h, z), as that implied by EF0 sz (Di )2 .18 Online Appendix C shows
                     h        i
that bounds on EF0 sz (Di )2 also correspond to bounds on the asymptotic power of tests to
distinguish elements of N (S) from S.


4.4    Non-Local Misspecification

To clarify the role of local misspecification in our results it is helpful to consider the analogue of ∆
under non-local misspecification. Suppose now that the reader believes the data follow ×ni=1 F (η, ζ),
where (η, ζ) do not change with the sample size. Let us denote the probability limits of ĉ and γ̂
under F by c̃ (F ) and γ (F ) , respectively. We assume for ease of exposition that these probability
limits exist.
   To simplify the analysis, let us further fix a value η0 of the base model parameter, so the true
value of the parameter of interest is c (η0 ). Suppose that for a divergence r of the form considered

                                                                                                              
  18                                                                                                  √1 , 0
    In equation (11) we scale by n to obtain a nontrivial limit, as the divergence between Fh,z         n
                                                                                                                   and
             
Fh,z √1n , √1n tends to zero as n → ∞.


                                                       18
in Section 4.3, r (η, ζ) = EF (η,0) [ψ (f (Di ; η, ζ) /f (Di ; η, 0))], the reader believes that the degree of
misspecification is bounded in the sense that r (η0 , ζ) ≤ µ. Given this bound, the probability limit
of |ĉ − c (η0 )| is no larger than


                        b̃N (µ) = sup {|c̃ (F (η0 , ζ)) − c (η0 )| : ζ ∈ Z, r (η0 , ζ) ≤ µ} ,


where we now make the dependence on µ explicit. This is a non-local analogue of the bias bound
b?N , fixing η = η0 . We can likewise bound the probability limit of |ĉ − c (η0 )| under an analogue of
F RN (·),


         b̃RN (µ) = sup {|c̃ (F (η0 , ζ)) − c (η0 )| : ζ ∈ Z, r (η0 , ζ) ≤ µ, γ (F (η0 , ζ)) − γ (F0 ) = 0} .


This is a non-local analogue of bias bound b?RN , again fixing η = η0 .
   Provided that b̃N (µ) and b̃RN (µ) are both finite and non-zero, we can define a non-local ana-
      ˜ (µ) of informativeness ∆ by
logue ∆


                                                   ˜ (µ) = b̃RN (µ) .
                                             q
                                                 1−∆
                                                            b̃N (µ)

                                                                          ˜ (µ) based on finite
Online Appendix D shows that, under regularity conditions, an analogue of ∆
collections of ζ values converges to ∆ as µ → 0. This provides a sense in which ∆ approximates
˜ (µ) when the degree of non-local misspecification is small.
∆


5      Implementation

In a wide range of applications, convenient estimates Σ̂ of Σ are available from standard asymptotic
results (e.g., Newey and McFadden 1994) or via a bootstrap (e.g., Hall 1992). Given such an
estimate one can construct a plug-in estimate

                                                       Σ̂cγ Σ̂−1
                                                              γγ Σ̂γc
(12)                                             ˆ =
                                                 ∆                    .
                                                             σ̂c2

                                                                ˆ under the sequences we study
Provided Σ̂ is consistent under S (0, 0), consistency of Σ̂ and ∆
follows immediately under our maintained assumptions that σc2 > 0 and Σγγ has full rank.
                         p
Assumption 5. Σ̂ → Σ under S (0, 0).
                                             p         p
                                                     ˆ →
Proposition 3. Under Assumptions 3 and 5, Σ̂ → Σ and ∆   ∆ under S (h, z) for any h ∈ H,
z ∈ Z.

                                                         19
Mukhin (2018) provides alternative sufficient conditions for consistent estimation of informativeness,
and also derives results applicable to GMM models with non-local misspecification.


5.1    Implementation with Minimum Distance Estimators

We have so far imposed only high-level assumptions (specifically Assumptions 1 and 5) on ĉ, γ̂,
and Σ̂. While these high-level assumptions hold in a wide range of settings, minimum distance
estimation is an important special case that encompasses a large number of applications. To
facilitate application of our results, in this section we discuss estimation of Σ in cases where c (η)
can be written as a function of a finite-dimensional vector of parameters that are estimated by GMM
or another minimum distance approach (Newey and McFadden 1994), and γ̂ is likewise estimated
via minimum distance.
   Formally, suppose that we can decompose η = (θ, ω) where θ is finite-dimensional and c (η)
depends on η only through θ, so we can write it as c (θ). We assume that c (θ) is continuously
differentiable in θ.
                                           
   The researcher forms an estimate ĉ = c θ̂ where θ̂ solves


(13)                                                    min ĝ (θ)0 Ŵ ĝ (θ)
                                                            θ


for ĝ (θ) a kg -dimensional vector of moments and Ŵ a kg × kg -dimensional weighting matrix. The
researcher likewise computes γ̂ by solving


(14)                                                   min m̂ (γ)0 Û m̂ (γ) ,
                                                        γ


for m̂ (γ) a km -dimensional vector of moments and Û a km × km -dimensional weighting matrix.
                                                                        √                 √
    Provided Ŵ and Û converge in probability to limits W and U , while nĝ (θ (η0 )) and nm̂ (γ (η0 ))
are jointly asymptotically normal under S (0, 0) ,
                                                                                        
                               √        ĝ (θ (η0 ))                            Σgg   Σgm
                                   n                    →d N 0,                          ,
                                        m̂ (γ (η0 ))                            Σmg Σmm

existing results (see for example Theorem 3.2 in Newey and McFadden, 1994) imply that under
S (0, 0) and standard regularity conditions,
                                                                                                             0
   √        ĉ − c (θ (η0 ))                                        Λcg    0          Σgg   Σgm         Λcg    0
       n                       →d N (0, Σ) , Σ =                                                              .
             γ̂ − γ (η0 )                                            0    Λγm         Σmg Σmm            0    Λγm



                                                                    20
Here Λcg = −C (G0 W G)−1 G0 W and Λγm = − (M 0 U M )−1 M 0 U are the sensitivities of ĉ with
respect to ĝ (θ (η0 )) and of γ̂ with respect to m̂ (γ (η0 )) as defined in Andrews et al. (2017), and
        ∂
C=      ∂θ c (θ (η0 )).                                          
                                                         ∂
      We can consistently estimate C by Ĉ =             ∂θ c    θ̂ . If ĝ (θ) and m̂ (γ) are continuously differ-
entiable then under regularity conditions (see Theorem 4.3 in Newey and McFadden, 1994) we
                                                    
                                               ∂                     ∂
can likewise consistently estimate G by Ĝ = ∂θ  ĝ θ̂ and M by M̂ = ∂γ m̂ (γ̂).19 Hence, given
consistent estimators Σ̂gg , Σ̂gm , and Σ̂mm we can estimate Σ by
                                                                                         0
                                     Λ̂cg       0        Σ̂gg     Σ̂gm          Λ̂cg    0
                            Σ̂ =                                                         
                                      0      Λ̂γm        Σ̂mg Σ̂mm               0     Λ̂γm

                       −1                               −1
for Λ̂cg = −Ĉ Ĝ0 Ŵ Ĝ    Ĝ0 Ŵ and Λ̂γm = − M̂ 0 Û M̂     M̂ 0 Û .
                                                                   
    What remains is to construct estimators Σ̂gg , Σ̂gm , Σ̂mm . When θ̂ and γ̂ are GMM or ML
estimators, we can write

                                            n                              n
                                         1X                        1X
                              ĝ (θ) =      φg (Di ; θ) , m̂ (γ) =    φm (Di ; γ) ,
                                         n                         n
                                            i=1                           i=1


for (φg (Di ; θ) , φm (Di ; γ)) the moment functions for GMM or the score functions for ML. We can
then estimate Σ by
                                                                                    
                                       n
                                    1 X         φ̂c (Di )2     φ̂c (Di ) φ̂γ (Di )0
(15)                           Σ̂ =                                                  ,
                                    n
                                      i=1   φ̂γ (Di ) φ̂c (Di ) φ̂γ (Di ) φ̂γ (Di )0

for
                                                                  −1                  
                          φ̂c (Di ) = Λ̂cg φg Di ; θ̂ = −Ĉ Ĝ0 Ŵ Ĝ    Ĝ0 Ŵ φg Di ; θ̂

and
                                                                      −1
                          φ̂γ (Di ) = Λ̂γm φm (Di ; γ̂) = − M̂ 0 Û M̂     M̂ 0 Û φm (Di ; γ̂) .
                                
      In the GMM case, φg Di ; θ̂ and φm (Di ; γ̂) are available immediately from the computation
of the final objective of the solver for (13) and (14), respectively. In the case of MLE, the score is
likewise often computed as part of the numerical gradient for the likelihood. The elements of Λ̂cg
and Λ̂γm are likewise commonly precomputed. The weights Ŵ and Û are directly involved in the

   19
       If ĝ (θ) and m̂ (γ) are not continuously differentiable, as sometimes occurs for simulation-based estimators, we
can estimate G and M in other ways. For example, we can estimate the jth column of G by the finite difference
                                                                                                  √
(ĝ (θ + ej εn ) − ĝ (θ − ej εn )) /2εn for ej the jth standard basis vector, where εn → 0 and εn n → ∞ as n → ∞. See
Section 7.3 of Newey and McFadden (1994) for details on this approach and sufficient conditions for its validity.


                                                            21
calculation of the objectives in (13) and (14), respectively. When ĝ (θ) and m̂ (γ) are differentiable,
Ĝ and M̂ are used in standard formulae for asymptotic inference on θ and γ, and the gradient Ĉ
is used in delta-method calculations for asymptotic inference on c.20
      In this sense, in many applications estimation of Σ will involve only manipulation of vectors
and matrices already computed as part of estimation of, and inference on, the parameters θ, γ, and
c.
Recipe. (GMM/MLE With Differentiable Moments)
                                                                                      
      1. Estimate θ̂ and γ̂ following (13) and (14), respectively, and compute ĉ = c θ̂ .
                n          on
      2. Collect φg Di ; θ̂           and {φm (Di ; γ̂)}ni=1 from the calculation of the objective functions
                                i=1
          in (13) and (14), respectively.
                                                                                                         
                                                     ∂                    ∂                        ∂
      3. Collect the numerical gradients Ĝ =        ∂θ ĝ    θ̂ , M̂ =   ∂γ m̂ (γ̂),   and Ĉ =   ∂θ c    θ̂ from the
         calculation of asymptotic standard errors for θ̂, γ̂, and ĉ.
                                     −1                               −1
      4. Compute Λ̂cg = −Ĉ Ĝ0 Ŵ Ĝ     Ĝ0 Ŵ and Λ̂γm = − M̂ 0 Û M̂     M̂ 0 Û using the weights Ŵ
         and Û from the objective functions in (13) and (14), respectively.
                                           
      5. Compute φ̂c (Di ) = Λ̂cg φg Di ; θ̂ and φ̂γ (Di ) = Λ̂γm φm (Di ; γ̂) for each i.

      6. Compute Σ̂ as in (15).

                 ˆ as in (12).
      7. Compute ∆


6         Applications

In this section we present and interpret estimates of ∆ for three structural articles in economics,
each of which estimates the parameters η of a base model via maximum likelihood. In each case,
we estimate the informativeness of each vector γ̂ for the estimator ĉ following the recipe in Section
5.1. Because in each case model estimation is via maximum likelihood and γ̂ can be represented as
GMM, the recipe applies directly.


6.1       The Effects of PROGRESA

Attanasio et al. (2012a) use survey data from Mexico to study the effect of PROGRESA, a random-
ized social experiment involving a conditional cash transfer aimed in part at increasing persistence
     20
     Note that in cases where the function c (θ) depends on features of the data beyond θ, for example on the
distribution of covariates, our formulation implicitly treats those features as fixed at their sample values for the
purposes of estimating ∆. Online Appendix E discusses how to account for such additional dependence on the data,
and presents corresponding calculations for some of our applications.


                                                         22
in school. The paper uses the estimated base model to predict the effect of a counterfactual inter-
vention in which total school enrollment is increased via a budget-neutral reallocation of program
funds.
   The estimate of interest ĉ is the partial-equilibrium effect of the counterfactual rebudgeting on
the school enrollment of eligible children, accumulated across age groups (Attanasio et al. 2012a,
sum of ordinates for the line labeled “fixed wages” in Figure 2, minus sum of ordinates for the line
labeled “fixed wages” in the left-hand panel of Figure 1).
   Attanasio et al. (2012a) discuss the “exogenous variability in [their] data that drives [their]
results” as follows (p. 53):

          The comparison between treatment and control villages and between eligible and
      ineligible households within these villages can only identify the effect of the existence
      of the grant. However, the amount of the grant varies by the grade of the child. The
      fact that children of different ages attend the same grade offers a source of variation
      of the amount that can be used to identify the effect of the size of the grant. Given
      the demographic variables included in our model and given our treatment for initial
      conditions, this variation can be taken as exogenous. Moreover, the way that the grant
      amount changes with grade varies in a non-linear way, which also helps identify the
      effect.
          Thus, the effect of the grant is identified by comparing across treatment and control
      villages, by comparing across eligible and ineligible households (having controlled for
      being “non-poor”), and by comparing across different ages within and between grades.
      (p. 53)

Motivated by this discussion, we define three vectors γ̂ of descriptive statistics, which correspond
to sample treatment-control differences from the experimental data. The first vector (“impact
on eligibles”) consists of the age-grade-specific treatment-control differences for eligible children
(interacting elements of Attanasio et al. 2012a, Table 2, single-age rows of the column labeled
“Impact on Poor 97,” with the child’s grade). The second vector (“impact on ineligibles”) consists
of the age-grade-specific treatment-control differences for ineligible children (interacting elements
of Attanasio et al. 2012a, Table 2, single-age rows of the column labeled “Impact on non-eligible,”
with the child’s grade). The third vector consists of both of these groups of statistics.
   Table I reports the estimated informativeness of each vector of descriptive statistics. The
estimated informativeness for the combined vector is 0.28. This is largely accounted for by the
age-grade-specific treatment-control differences for eligible children.


                                                  23
Table I. Estimated informativeness of descriptive statistics for the effect of a counterfactual rebud-
geting of PROGRESA (Attanasio et al. 2012a)

                       Descriptive statistics γ̂                             ˆ
                                                   Estimated informativeness ∆
                       All                                    0.283
                       Impact on eligibles                    0.227
                       Impact on ineligibles                  0.056

Notes: The table shows the estimated informativeness ∆     ˆ of three vectors γ̂ of descriptive statistics
for the estimated partial-equilibrium effect ĉ of the counterfactual rebudgeting on the school enroll-
ment of eligible children, accumulated across age groups (Attanasio et al. 2012a, sum of ordinates
for the line labeled “fixed wages” in Figure 2, minus sum of ordinates for the line labeled “fixed
wages” in the left-hand panel of Figure 1). Vector γ̂ “impact on eligibles” consists of the age-grade-
specific treatment-control differences for eligible children (interacting elements of Attanasio et al.
2012a, Table 2, single-age rows of the column labeled “Impact on Poor 97,” with the child’s grade).
Vector γ̂ “impact on ineligibles” consists of the age-grade-specific treatment-control differences for
ineligible children (interacting elements of Attanasio et al. 2012a Table 2, single-age rows of the
column labeled “Impact on non-eligible,” with the child’s grade). Vector γ̂ “all” consists of both
of these groups of statistics. Estimated informativeness ∆   ˆ is calculated according to the recipe in
Section 5.1 using the replication code and data posted by Attanasio et al. (2012b).


     Restricting from F N (·) to F RN (·) reduces the worst-case bias by an estimated factor of 1 −
√
    1 − 0.28 ≈ 0.15 in the sense of Proposition 2. Further reduction in the worst-case bias would
require including in γ̂ descriptive statistics that are orthogonal to the treatment-control differences
we consider, thus imposing that F RN (·) respects the relationship specified by the base model F 0 (·)
between c and the features of the distribution of the data estimated by these orthogonal statistics.
     To illustrate the distinction between informativeness and identification highlighted in Section
3.3.1, now let c be the partial-equilibrium effect of the actual program on the school enrollment of
eligible children, accumulated across age groups. The parameter c is nonparametrically identified,
and can be nonparametrically estimated by comparing the school enrollment of eligible children in
treatment and control villages (as in Attanasio et al. 2012a, Table 2, column labeled “Impact on
Poor 97”). The parameter c can also be estimated parametrically using the researcher’s estimated
model (as in Attanasio et al. 2012a, sum of ordinates for the line labeled “fixed wages” in the
left-hand panel of Figure 1). The descriptive statistics γ̂ have an informativeness of 1 for a natural
nonparametric estimator, and an estimated informativeness of 0.31 for the parametric estimator,
indicating that assumptions beyond those required for nonparametric identification are necessary
to guarantee that the parametric estimator is unbiased in the sense of Proposition 2.




                                                   24
6.2   Newspaper Demand

Gentzkow (2007a) uses survey data from a cross-section of individuals to estimate demand for print
and online newspapers in Washington DC. A central goal of Gentzkow’s (2007a) paper is to estimate
the extent to which online editions of papers crowd out readership of the associated print editions,
which in turn depends on a key parameter governing the extent of print-online substitutability.
   The estimate of interest ĉ is the change in readership of the Washington Post print edition that
would occur if the Post online edition were removed from the choice set (Gentzkow 2007a, Table
10, row labeled “Change in Post readership”).
   Gentzkow (2007a) discusses two features of the data that can help to distinguish correlated
tastes from true substitutability: (i) a set of instruments—such as a measure of Internet access
at work—that plausibly shift the utility of online papers but do not otherwise affect the utility of
print papers; and (ii) a coarse form of panel data—separate measures of consumption in the last
day and last five weekdays—that make it possible to relate changes in consumption of the print
edition to changes in consumption of the online edition over time for the same individual (p. 730).
   Motivated by Gentzkow’s (2007a) discussion, we define three vectors γ̂ of descriptive statistics.
The first vector (“IV coefficient”) is the coefficient from a 2SLS regression of last-five-weekday
print readership on last-five-weekday online readership, instrumenting for the latter with the set of
instruments (Gentzkow 2007a, Table 4, Column 2, first row). The second vector (“panel coefficient”)
is the coefficient from an OLS regression of last-one-day print readership on last-one-day online
readership controlling for the full set of interactions between indicators for print readership and
indicators for online readership in the last five weekdays. Each of these regressions includes the
standard set of demographic controls from Gentzkow (2007a, Table 5). The third vector γ̂ consists
of both the IV coefficient and the panel coefficient. Thus, the first two vectors have dimension 1,
and the third has dimension 2.
   Table II reports the estimated informativeness of each vector of descriptive statistics. The
estimated informativeness of the combined vector is 0.51. This is accounted for almost entirely by
the panel coefficient, which alone has an estimated informativeness of 0.50. The IV coefficient, by
contrast, has an estimated informativeness of only 0.01.
   Gentzkow’s (2007a) discussion of identification highlights both the exclusion restrictions under-
lying the IV coefficient and the panel variation underlying the panel coefficient as potential sources
of identification, and if anything places more emphasis on the former. Based on Gentzkow’s (2007a)
discussion, and the large literature showing that exclusion restrictions can be used to establish non-
parametric identification in closely related models (Matzkin 2007), it is tempting to conclude that
accepting the relationship specified by the base model between the counterfactual c and the pop-


                                                 25
Table II. Estimated informativeness of descriptive statistics for the effect of eliminating the Post
online edition (Gentzkow 2007a)

                        Descriptive statistics γ̂                             ˆ
                                                    Estimated informativeness ∆
                        All                                    0.514
                        IV coefficient                         0.009
                        Panel coefficient                      0.503

Notes: The table shows the estimated informativeness ∆     ˆ of three vectors γ̂ of descriptive statistics
for the estimated effect ĉ on the readership of the Post print edition if the Post online edition were
removed from the choice set (Gentzkow 2007a, table 10, row labeled “Change in Post readership”).
Vector γ̂ “IV coefficient” is the coefficient from a 2SLS regression of last-five-weekday print read-
ership on last-five-weekday online readership, instrumenting for the latter with the set of excluded
variables such as Internet access at work (Gentzkow 2007a, Table 4, Column 2, first row). Vector
γ̂ “panel coefficient” is the coefficient from an OLS regression of last-one-day print readership on
last-one-day online readership controlling for the full set of interactions between indicators for print
readership and for online readership in the last five weekdays. Each of these regressions includes
the standard set of demographic controls from Gentzkow (2007a, Table 5). Vector γ̂ “all” consists
of both the IV coefficient and the panel coefficient. Estimated informativeness ∆        ˆ is calculated
according to the recipe in Section 5.1 using the replication code and data posted by Gentzkow
(2007b).


ulation value of the IV coefficient would greatly limit the scope for bias in Gentzkow’s (2007a)
estimator ĉ.
     Our findings suggest otherwise. When γ̂ consists only of the IV coefficient, restricting from
                                                                                        √
FN   (·) to F RN (·) reduces the worst-case bias in ĉ by an estimated factor of only 1− 1 − 0.01 < 0.01
in the sense of Proposition 2. By contrast, when γ̂ consists only of the panel coefficient, restricting
                                                                                         √
from F N (·) to F RN (·) reduces the worst-case bias in ĉ by an estimated factor of 1 − 1 − 0.50 ≈
0.29. Intuitively, a reader interested in evaluating the scope for bias in ĉ may wish to focus more
attention on the assumptions of the base model that relate c to the population value of the panel
coefficient (e.g., restrictions on the time structure of preference shocks), than on assumptions that
relate c to the population value of the IV coefficient (e.g., exclusion restrictions).


6.3     Long-term Care Insurance

Hendren (2013a) uses data on insurance eligibility and self-reported beliefs about the likelihood of
different types of “loss” events (e.g., becoming disabled) to recover the distribution of underlying
beliefs and rationalize why some groups are routinely denied insurance coverage. We focus here on
Hendren’s (2013a) model of the market for long-term care (LTC) insurance.
     The estimate of interest ĉ is the minimum pooled price ratio among rejectees (Hendren 2013a,
Table V, row labeled “Reject,” column labeled “LTC”). The minimum pooled price ratio determines


                                                    26
Table III. Estimated informativeness of descriptive statistics for the minimum pooled price ratio
(Hendren 2013a)

                 Descriptive statistics γ̂                                             ˆ
                                                             Estimated informativeness ∆
                 All                                                    0.700
                 Fractions in focal point groups                        0.005
                 Fractions in non-focal point groups                    0.018
                 Fraction in each group needing LTC                     0.676

Notes: The table shows the estimated informativeness ∆        ˆ of four vectors γ̂ of descriptive statistics
for the “minimum pooled price ratio” ĉ (Hendren 2013a, Table V, row labeled “Reject,” column
labeled “LTC”). Vector γ̂ “fractions in focal point groups” consists of the fraction of respondents
who report exactly 0, the fraction who report exactly 0.5, and the fraction who report exactly 1.
Vector γ̂ “fractions in non-focal point groups” consists of the fractions of respondents whose reports
are in each of the intervals (0.1, 0.2], (0.2, 0.3], (0.3, 0.4], (0.4, 0.5), (0.5, 0.6], (0.6, 0.7], (0.7, 0.8],
(0.8, 0.9], and (0.9, 1). Vector γ̂ “fraction in each group needing LTC” consists of the fractions of
respondents giving each of the preceding reports who eventually need long-term care. Vector γ̂ “all”
consists of all three of the other vectors. Estimated informativeness ∆     ˆ is calculated according to the
recipe in Section 5.1 using the replication code and data posted by Hendren (2013b), supplemented
with additional calculations provided by the author.


the range of preferences for which insurance markets cannot exist (Hendren 2013a, Corollary 2 to
Theorem 1). This ratio is a key output of the analysis, as it provides an economic rationale for the
insurance denials that are the paper’s focus.
    Hendren (2013a) explains that the parameters that determine the minimum pooled price ratio
are identified from the relationship between elicited beliefs and the eventual realization of loss events
such as long term care (pp. 1751-2).
    Motivated by Hendren’s (2013a) discussion, we define four vectors γ̂ of descriptive statistics.
The first vector (“fractions in focal-point groups”) consists of the fraction of respondents who report
exactly 0, the fraction who report exactly 0.5, and the fraction who report exactly 1. The second
vector (“fractions in non-focal-point groups”) consists of the fractions of respondents whose reports
are in each of the intervals (0.1, 0.2], (0.2, 0.3], (0.3, 0.4], (0.4, 0.5), (0.5, 0.6], (0.6, 0.7], (0.7, 0.8],
(0.8, 0.9], and (0.9, 1). The third vector (“fraction in each group needing LTC”) consists of the
fraction of respondents giving each of the preceding reports who eventually need long-term care.
The fourth vector γ̂ consists of all three of the other vectors.
    Hendren’s (2013a) discussion suggests that the third vector will be especially informative for
the minimum pooled price ratio.
    Table III reports the estimated informativeness of each vector of descriptive statistics. The
estimated informativeness of the combined vector is 0.70. The estimated informativeness is 0.01
with respect to the fractions in focal point groups, 0.02 with respect to the fractions in non-focal-

                                                      27
point groups, and 0.68 with respect to the fraction in each group needing LTC. When γ̂ consists
only of the fraction in each group needing LTC, restricting from F N (·) to F RN (·) reduces the
                                                   √
worst-case bias in ĉ by an estimated factor of 1 − 1 − 0.68 ≈ 0.43. This finding seems consistent
with the author’s discussion.


7    Conclusions

Descriptive analysis has become an important complement to structural estimation. It is common
for a researcher to report descriptive statistics γ̂ that estimate features γ of the distribution of the
data that are in turn related to the quantity c of interest under the researcher’s model. A reader
who accepts the relationship between the features γ and the structural quantity c specified by the
researcher’s model, and who believes that the statistics γ̂ play an important role in “driving” the
structural estimate ĉ, may then be more confident in the researcher’s conclusions.
    We propose one way to formalize this logic. We define a measure ∆ of the informativeness of
descriptive statistics γ̂ for a structural estimate ĉ. Informativeness captures the share of variation
in ĉ that is explained by γ̂ under their joint asymptotic distribution. We show that, under some
conditions, informativeness also governs the reduction in worst-case bias from accepting the rela-
tionship between γ and c specified by the researcher’s model. In this sense, descriptive analysis
based on statistics with high informativeness can indeed increase confidence in structural estimates.
    Informativeness can be computed at negligible cost even for complex models, and we provide
a convenient recipe for computing it. We show in the context of our applications that reporting
informativeness can sharpen the interpretation of structural estimates in important economic set-
tings. We recommend that researchers report estimated informativeness alongside their descriptive
analyses.




Proofs
Proof of Proposition 1 First consider F ∈ F N (c) . By the definition of F N (·) there exist
η ∈ Rp , ζ ∈ Rk such that F = F (η, ζ) and c = c (η) . Note, moreover, that since c (η) = L0 η while
EF [ĉ] = L0 η + C 0 ζ, EF [ĉ − c] = C 0 ζ. Thus, our task reduces to showing that
                             n                           o
                              C 0 ζ : ζ ∈ Rk , kζkΩ−1 ≤ µ = [−µσc , µσc ] .

                                   1   1
Note, however, that C 0 ζ = C 0 Ω 2 Ω− 2 ζ, so by the Cauchy-Schwarz inequality, |C 0 ζ| ≤ σc kζkΩ−1 .
Hence, to prove the result we need only show that any bias c̄ with |c̄| ≤ µσc can be achieved. To


                                                   28
this end, pick such a |c̄| ≤ µσc . Consider ζ =                  c̄
                                                                σc2
                                                                    ΩC     and note that C 0 ζ = c̄ and kζkΩ−1 =                c̄
                                                                                                                               σc    ≤ µ,
as desired.
    Next consider F ∈ F RN (c) . By the definition of F RN (·) there exist η ∈ Rp , ζ ∈ Rk such that
F = F (η, ζ), c = c (η) , and Γ0 (Xη + ζ) = Γ0 Xη. Thus, our task reduces to showing that
                     n                                     o h    √          √     i
                      C 0 ζ : ζ ∈ Rk , kζkΩ−1 ≤ µ, Γ0 ζ = 0 = −µσc 1 − ∆, µσc 1 − ∆ .

Let us first show that for any ζ with kζkΩ−1 ≤ µ and Γ0 ζ = 0, C 0 ζ satisfies these bounds. To this
end, define C̃ = C − ΓΛ0 for Λ = Σcγ Σ−1                                   0         0     0
                                        γγ , and note that for any ζ with Γ ζ = 0, C̃ ζ = C ζ. Note,
                   p
next, that C̃ 0 ζ ≤ C̃ 0 ΩC̃ kζkΩ−1 by the Cauchy-Schwarz inequality, and that

                                       C̃ 0 ΩC̃ = σc2 − Σcγ Σ−1        2
                                                             γγ Σγc = σc (1 − ∆) ,

                                                                                   √
from which the result follows. We next want to show that for any c̄ with |c̄| ≤ µσc 1 − ∆ there
exists ζ with kζkΩ−1 ≤ µ and Γ0 ζ = 0 such that C 0 ζ = c̄. This result is trivial if ∆ = 1, so let us
                                                    √                          c̄
suppose that ∆ < 1, and pick some c̄ with |c̄| ≤ µσc 1 − ∆. Define ζ = σ2 (1−∆)     ΩC̃ and note that
                                                                                                          c
Γ0 ζ = 0 and C 0 ζ = C̃ 0 ζ = c̄, while
                                                                          c̄2
                                                     kζk2Ω−1 =                   ,
                                                                     σc2 (1 − ∆)
which is bounded above by µ2 .

                                                                                                                         p
Proof of Lemma 1 By Lemma 7.6 of van der Vaart (1998), Assumption 3 implies that                                             fh,z (Di ; th , tz )
is differentiable in quadratic mean in the sense that for all (h, z) ∈ H × Z,
Z q                                                                                           2
                                q                     1                       q                           
                                                                                                                                               2
                                                                                                                                                    
       fh,z (Di ; th , tz ) −       fh,z (Di ; 0, 0) − (th sh (d) + tz sz (d)) fh,z (Di ; 0, 0) dν (d) = o (th , tz )0
                                                      2

as (th , tz ) → 0. Hence, Theorem 7.2 of van der Vaart (1998) implies that under S (0, 0), defining
F n = ×ni=1 F ,
                                                                                             !0
                   th √
           n          , tzn                   n
                                                                                                                         !
         dFh,z    √
                                                                                           th                       th
                    n
                               = √1                                  1
                                      X
 log                                   (th sh (Di ) + tz sz (Di )) −                                 Ih,z (0, 0)            + op (1)
                 dF0n               n                                 2                    tz                       tz
                                          i=1

                                                       h         i       h         i
and that EF0 [sh (Di )] = EF0 [sz (Di )] = 0. Since EF0 sh (Di )2 and EF0 sz (Di )2 are finite,
Assumption 1, the Central Limit Theorem, and Slutsky’s Lemma imply that under S (0, 0) , for
g (Di ; h, z) = sh (Di ) + sz (Di ) ,
                                                         !                                                  !0
                                        n
                                      dFh,z    √1 , √1
                                                 n    n
                                                               √1
                                                                     P
                                                                                     √1
                                                                                           P              0
                               log            dF0n               n
                                                                          φc (Di )     n
                                                                                               φγ (Di )




                                                                     29
                                                      h               i    
                                                − 12 EF0 g (Di ; h, z)2
                                                                         ∗
                                        →d N 
                                                         0              ,Σ ,
                                                                             
                                                           0
for
                        h               i
                                             EF0 [g (Di ; h, z) φc (Di )] EF0 g (Di ; h, z) φγ (Di )0
                                                                                                                                 
                     EF0 g (Di ; h, z)2
                                                                                                     
                                                       h             i
       Σ∗ = 
                                                                                                                                 
                                                                   2
                                                                           EF0 φc (Di ) φγ (Di )0
                                                                                                                                .
              E
             F0  [g (D i ; h, z) φ c (Di )]     E  F0   φ c (Di )                                                                
                                                                           EF0 φγ (Di ) φγ (Di )0
                                                                                                 
              EF0 [g (Di ; h, z) φγ (Di )]    EF0 [φγ (Di ) φc (Di )]

By LeCam’sfirst lemma
                       (see  Example 6.5 of van der Vaart 1998) the convergence in distribution
          n     1
of log dFh,z √n , √n /dF0 to a normal with mean equal to − 12 of its variance implies that the
                    1      n
                                            
sequences ×ni=1 F0 and ×ni=1 Fh,z
                              n     √1 , √1    are mutually contiguous. Le Cam’s third lemma (see
                                      n    n
Example 6.7 of van der Vaart 1998) then implies that under S (h, z) ,
                                                            !                                                        !0
                                           n
                                         dFh,z    √1 , √1
                                                    n    n
                                                                     √1
                                                                           P
                                                                                              √1
                                                                                                    P              0
                                 log             dF0n                  n
                                                                                φc (Di )        n
                                                                                                        φγ (Di )

                                                                   h                    i              
                                                             1
                                                             2 EF0       g (Di ; h, z)2
                                              E [φ (D ) g (D ; h, z)]  , Σ∗  .
                                                                           
                                       →d N 
                                             F0 c i            i           
                                               EF0 [φγ (Di ) g (Di ; h, z)]
Together with contiguity, Assumption 1 implies that

                 √                                       1 X                       
                     n ĉ − c (η0 ) , γ̂ 0 − γ (η0 )0 − √                  φγ (Di )0 = op (1) ,
                                                                        X
                                                              φc (Di ) ,
                                                          n

under S (h, z) , from which the result is immediate for
                         !                                                      !                                                 !
         c̄ (S (h, z))                 EF0 [φc (Di ) g (Di ; h, z)]                        EF0 [φc (Di ) (sh (Di ) + sz (Di ))]
                             =                                                      =                                                  .
         γ̄ (S (h, z))                 EF0 [φγ (Di ) g (Di ; h, z)]                        EF0 [φγ (Di ) (sh (Di ) + sz (Di ))]

      Finally, note that Assumption 2 implies c? (h) = c̄ (S (h, 0)) and γ ? (h) = γ̄ (S (h, 0)) . Conse-
quently, by the results above we have c? (h) = EF0 [φc (Di ) sh (Di )], and γ ? (h) = EF0 [φγ (Di ) sh (Di )] .

Proof of Proposition 2 Let us first consider the case with S ∈ S N (c? ) , with S N (c? ) nonempty.
By the definition of S N (c? ) and Lemma 1, for any S ∈ S N (c? ) there exist (h, z) ∈ H × Z with
S = S (h, z) and c? (h) = c? . For this (h, z) we can write

      c̄ (S) − c? = EF0 [φc (Di ) (sh (Di ) + sz (Di ))] − EF0 [φc (Di ) sh (Di )] = EF0 [φc (Di ) sz (Di )] .




                                                                           30
Writing c̄z = EF0 [φc (Di ) sz (Di )] for brevity, our task thus reduces to showing
                                 n                h         i    o
                                  c̄z : z ∈ Z, EF0 sz (Di )2 ≤ µ2 = [−µσc , µσc ] .

Note, however, that by the Cauchy-Schwarz inequality
                                             r         h            ir       h          i
                                                                2                   2
                                  |c̄z | ≤       EF0 φc (Di )            EF0 sz (Di )       ≤ µσc .

                             h         i
Hence, for any z ∈ Z with EF0 sz (Di )2 ≤ µ2 , c̄z necessarily satisfies the bounds. Going the other
direction, for any c̄ with |c̄| ≤ µσc , if we take s∗ (Di ) = σc̄2 φc (Di ) , we have EF0 [s∗ (Di ) φc (Di )] =
               h         i                                      c
                 ∗     2        2  2        2
c̄, while EF0 s (Di ) = c̄ /σc ≤ µ . By Assumption 4, however, there exists z ∈ Z with
     h                     i                           h          i
EF0 (s∗ (Di ) − sz (Di ))2 = 0, so c̄z = c̄ and EF0 sz (Di )2 ≤ µ2 , as desired.
    For the case with S ∈ S RN (c? ) , note that by the definition of S RN (c? ) and Lemma 1, for any
S ∈ S RN (c? ) there exist (h, z) ∈ H × Z with S = S (h, z), c? (h) = c? , and

         EF0 [φγ (Di ) (sh (Di ) + sz (Di ))] − EF0 [φγ (Di ) sh (Di )] = EF0 [φγ (Di ) sz (Di )] = 0.

Thus, writing γ̄z = EF0 [φγ (Di ) sz (Di )] for brevity, our task reduces to showing that
              n                         h
                                                 2
                                                   i
                                                     2
                                                       o h  √          √     i
               c̄z : z ∈ Z, γ̄z = 0, EF0 sz (Di ) ≤ µ = −µσc 1 − ∆, µσc 1 − ∆ .

Let Λ = Σ−1                                   0
         γγ Σγc . For φ̃c (Di ) = φc (Di ) − Λ φγ (Di ) , note that if γ̄z = 0 then

                                                                h                  i
                                   EF0 [φc (Di ) sz (Di )] = EF0 φ̃c (Di ) sz (Di ) .

The Cauchy-Schwarz inequality then implies that
                          h                  i r   h            ir    h          i
                                                              2
                       EF0 φ̃c (Di ) sz (Di ) ≤ EF0 φ̃c (Di )      EF0 sz (Di )2

                                                                       √
                       q                          r      h          i        r   h         i
                   =       σc2   − ΛΣγγ      Λ0       EF0 sz (Di ) = σc 1 − ∆ EF0 sz (Di )2 .
                                                                  2

                                      h         i
Hence, we see that for z such that EF0 sz (Di )2 ≤ µ2 ,
                                                  h    √           √     i
                                             c̄z ∈ −µσc 1 − ∆µ, µσc 1 − ∆ ,

which are the bounds stated in the proposition.
    To complete the proof it remains to show that these bounds are tight, so that for any (c̄, µ)
with                                              h    √          √      i
                                              c̄ ∈ −µσc 1 − ∆, µσc 1 − ∆



                                                                    31
                                                  h         i
there exists z ∈ Z with c̄z = c̄, γ̄z = 0, and EF0 sz (Di )2 ≤ µ2 .This result is trivial if ∆ = 1, so
                                                            √
let us suppose that ∆ < 1 and pick some c̄ with |c̄| ≤ µσc 1 − ∆. Now define

                                                                      c̄
                                     s∗ (Di ; c̄) = φ̃c (Di )               .
                                                                σc2 (1 − ∆)

Note that EF0 [φγ (Di ) s∗ (Di ; c̄)] = 0, while
                                                        h           i          c̄
                       EF0 [φc (Di ) s∗ (Di ; c̄)] = EF0 φ̃c (Di )2                  = c̄.
                                                                         σc2 (1 − ∆)

Moreover,
                                        h             i  c̄2
                                     EF0 s∗ (Di ; c̄)2 =        .
                                                    σc2 (1 − ∆)
                                                          √           h             i
However, by the definition of c̄ we know that |c̄| ≤ µσc 1 − ∆, so EF0 s∗ (Di ; c̄)2 ≤ µ2 . By
Assumption 4, however, there exists z ∈ Z with
                                       h                          i
                                    EF0 (sz (Di ) − s∗ (Di ; c̄))2 = 0,
                                            h         i
and thus z yields c̄z = c̄, γ̄z = 0, and EF0 sz (Di )2 ≤ µ2 as desired.

Proof of Proposition
                    3 As shown in the proof
                                             of Lemma 1, under Assumption 3 the log likeli-
                 n    1    1    n     1   1
hood ratio log dFh,z √n , √n /dFh,z √n , √n     converges under S (0, 0) to a normal distribution
with mean equal to − 21 times its variance. Le Cam’s First Lemma thus implies that the distribu-
tion of the data under S (h, z) is mutually contiguous with that under S (0, 0). Hence, to establish
convergence in probability under S (h, z), it suffices to establish convergence in probability under
                          ˆ under S (0, 0) is implied by Assumption 5, the Continuous Mapping The-
S (0, 0) . Consistency of ∆
orem (see e.g. Theorem 2.3 of van der Vaart 1998), and the maintained assumptions that σc2 > 0
and Σγγ has full rank.


References
Alatas, Vivi, Abhijit Banerjee, Rema Hanna, Benjamin A. Olken, Ririn Purnamasari, and Matthew
     Wai-Poi. 2016. Self-targeting: Evidence from a field experiment in Indonesia. Journal of
     Political Economy 124(2): 371-427.
Andrews, Isaiah, Matthew Gentzkow, and Jesse M. Shapiro. 2017. Measuring the sensitivity of pa-
     rameter estimates to estimation moments. Quarterly Journal of Economics 132(4): 1553–1592.
Andrews, Isaiah, Matthew Gentzkow, and Jesse M. Shapiro. 2020. Transparency in structural
     research. NBER Working Paper No. 26631.
Angrist, Joshua D. and Jörn-Steffen Pischke. 2010. The credibility revolution in empirical eco-
     nomics: How better research design is taking the con out of econometrics.” Journal of Eco-
     nomic Perspectives 24(2): 3-30.

                                                       32
Armstrong, Timothy and Michal Kolesár. 2019. Sensitivity analysis using approximate moment
    condition models. Cowles Foundation Discussion Paper No. 2158R.
    SSRN: https://ssrn.com/abstract=3337748.
Attanasio, Orazio P., Costas Meghir, and Ana Santiago. 2012a. Education choices in Mexico:
    Using a structural model and a randomized experiment to evaluate PROGRESA. Review of
    Economic Studies 79(1): 37-66.
Attanasio, Orazio P., Costas Meghir, and Ana Santiago. 2012b. Supplementary data for Education
    Choices in Mexico: Using a Structural Model and a Randomized Experiment to Evaluate PRO-
    GRESA. Accessed at <https://academic.oup.com/restud/article/79/1/37/1562110#supple mentary-
    data> in October 2017.
Berkowitz, Daniel, Mehmet Caner, and Ying Fang. 2008. Are “nearly exogenous instruments”
    reliable? Economic Letters 101(1): 20–23.
Bonhomme, Stéphane and Martin Weidner. 2018. Minimizing sensitivity to model misspecification.
    arXiv:1807.02161v2 [econ.EM].
Chen, Xiaohong and Andres Santos. 2018. Overidentification in regular models. Econometrica
    86(5): 1771-1817.
Conley, Timothy G., Christian B. Hansen, and Peter E. Rossi. 2012. Plausibly exogenous. Review
    of Economics and Statistics 94(1): 260–272.
Cressie, Noel and Timothy RC Read. 1984. Multinomial goodness-of-fit tests. Journal of the Royal
    Statistical Society Series B 46(3): 440–464.
Dridi, Ramdan, Alain Guay, and Eric Renault. 2007. Indirect inference and calibration of dynamic
    stochastic general equilibrium models. Journal of Econometrics 136(2): 397-430.
Duflo, Esther, Rema Hanna, and Stephen P. Ryan. 2012. Incentives work: Getting teachers to
    come to school. American Economic Review 102(4): 1241–1278.
Einav, Liran, Amy Finkelstein, Stephen P. Ryan, Paul Schrimpf, and Mark R. Cullen. 2013.
    Selection on moral hazard in health insurance. American Economic Review 103(1): 178–219.
Fetter, Daniel K. and Lee M. Lockwood. 2018. Government old-age support and labor supply:
    Evidence from the Old Age Assistance Program. American Economic Review 108(8): 2174-
    2211.
Gentzkow, Matthew. 2007a. Valuing new goods in a model with complementarity: Online news-
    papers. American Economic Review 97(3): 713-744.
Gentzkow, Matthew. 2007b. Supplementary data for Valuing new goods in a model with comple-
    mentarity: Online newspapers. Accessed at <https://www.openicpsr.org/openicpsr/project/
    116273/version/V1/view> in May 2020.
Gentzkow, Matthew and Jesse M. Shapiro. 2015. Measuring the sensitivity of parameter estimates
    to sample statistics. NBER Working Paper No. 20673.
Gentzkow, Matthew, Jesse M. Shapiro, and Michael Sinkinson. 2014. Competition and ideologi-
    cal diversity: Historical evidence from US newspapers. American Economic Review 104(10):
    3073–3114.


                                                   33
Guggenberger, Patrik. 2012. On the asymptotic size distortion of tests when instruments locally
    violate the exogeneity assumption. Econometric Theory 28(2): 387–421.
Hall, Peter. 1992. The Bootstrap and Edgeworth Expansion. Springer Series in Statistics. New
    York: Springer-Verlag.
Hansen, Bruce E. 2016. Efficient shrinkage in parametric models. Journal of Econometrics 190(1):
    115-132.
Hansen, Lars P. and Thomas J. Sargent. 2001. Acknowledging misspecification in macroeconomic
    theory. Review of Economic Dynamics 4(3): 519-35.
Hansen, Lars P. and Thomas J. Sargent. 2005. Robust estimation and control under commitment.
    Journal of Economic Theory 124(2): 258-301.
Hansen, Lars P. and Thomas J. Sargent. 2016. Sets of models and prices of uncertainty. NBER
    Working Paper No. 22000.
Hansen, Lars P., Thomas J. Sargent, Gauhar Turmuhambetova, and Noah Williams. 2006. Robust
    control and model misspecification. Journal of Economic Theory 128(1): 45-90.
Heckman, James J. 2010. Building bridges between structural and program evaluation approaches
    to evaluating policy. Journal of Economic Literature 48(2): 356-98.
Hendren, Nathaniel. 2013a. Private information and insurance rejections. Econometrica 81(5):
    1713–1762.
Hendren, Nathaniel. 2013b. Supplementary data for Private information and insurance rejections.
    Accessed at <https://www.econometricsociety.org/content/supplement-private-information-a
    nd-insurance-rejections-0> in March 2014.
Huber, Peter J. and Elvezio M. Ronchetti. 2009. Robust Statistics (2nd ed). Hoboken, NJ: John
    Wiley & Sons.
Ichimura, Hidehiko and Whitney K. Newey. 2015. The influence function of semiparametric esti-
    mators. arXiv:1508.01378v1 [stat.ME].
Keane, Michael P. 2010. Structural vs. atheoretic approaches to econometrics. Journal of Econo-
    metrics 156(1): 3–20.
Kitamura, Yuichi, Taisuke Otsu, and Kirill Evdokimov. 2013. Robustness, infinitesimal neighbor-
    hoods, and moment restrictions. Econometrica 81(3): 1185-1201.
Lehmann, Erich L. and Joseph P. Romano. 2005. Testing Statistical Hypotheses. New York:
    Springer.
Morten, Melanie. 2019. Temporary migration and endogenous risk sharing in village India. Journal
    of Political Economy 127(1): 1-46.
Matzkin, Rosa L. 2007. Nonparametric identification. In James J. Heckman and Edward E. Leamer,
    Eds., Handbook of Econometrics, Vol. 6(B), Ch. 73: 5307-5368. Amsterdam: North-Holland.
Mukhin, Yaroslav. 2018. Sensitivity of regular estimators. arXiv:1805.08883v1 [econ.EM].
Nakamura, Emi and Jón Steinsson. 2018. Identification in macroeconomics. Journal of Economic
    Perspectives 32(3): 59-86.




                                                34
Newey, Whitney K. 1985. Generalized method of moments specification testing. Journal of Econo-
    metrics 29(3): 229–256.
Newey, Whitney K. 1994. The asymptotic variance of semiparametric estimators. Econometrica
    62(6): 1349-1382.
Newey, Whitney K. and Daniel McFadden. 1994. Large sample estimation and hypothesis testing.
    In Robert F. Engle and Daniel L. McFadden, Eds., Handbook of Econometrics, Vol. 4, Ch. 36:
    2111-2245. Amsterdam: North-Holland.
Pakes, Ariel. 2014. Behavioral and descriptive forms of choice models. International Economic
    Review 55(3): 603-624.
Rieder, Helmut. 1994. Robust Asymptotic Statistics. New York: Springer.
Spenkuch, Jörg L., B. Pablo Montagnes, and Daniel B. Magleby. 2018. Backward induction in the
    wild? Evidence from sequential voting in the US Senate. American Economics Review 108(7):
    1971-2013.
van der Vaart, Aad W. 1998. Asymptotic Statistics. Cambridge, UK: Cambridge University Press.




                                             35
                                               Online Appendix for

                On the Informativeness of Descriptive Statistics
                                       for Structural Estimates

                         Isaiah Andrews, Harvard University and NBER
                        Matthew Gentzkow, Stanford University and NBER
                         Jesse M. Shapiro, Brown University and NBER


                                                      May 2020




A     Sensitivity and Informativeness
Proposition 2 considers the effect of limiting attention to forms of misspecification that do not
affect γ̂. In some cases, however, researchers may be interested in forms of misspecification with a
non-zero, but known, effect on γ̂. In such cases, our assumptions again imply a relationship between
the biases in ĉ and γ̂.
     This relationship depends on the sensitivity of ĉ to γ̂. This is the natural extension of the
sensitivity measure proposed in Andrews et al. (2017) to the current setting.

Definition. The sensitivity of ĉ with respect to γ̂ is

                                                    Λ = Σcγ Σ−1
                                                             γγ .


     To build intuition, note that sensitivity characterizes the relationship between ĉ and γ̂ in
the asymptotic distribution under the base model. If we assume, as in Section 3, that ĉ and
γ̂ are normally distributed in finite samples, then Λ is simply the vector of coefficients from the
population regression of ĉ on γ̂. In this case, element Λj of Λ is the effect of changing the realization
of a particular γ̂j on the expected value of ĉ, holding the other elements of γ̂ constant.
     Andrews et al. (2017) show that for ĉ = c (η̂), η̂ a minimum distance estimator based on
moments ĝ (η), and γ̂ = ĝ (η0 ) the estimation moments evaluated at the true parameter value,
under regularity conditions sensitivity translates the effect of misspecification on γ̂ to the effect on
ĉ, in the sense that

                           c̄ (S (h, z)) − c̄ (S (h, 0)) = Λ (γ̄ (S (h, z)) − γ̄ (S (h, 0))) .

Our next proposition extends this result.


                                                           36
Proposition 4. Suppose that Assumptions 1-4 hold, and let
                                                    n                              o
                       S RN (c? , γ̄) = ∪S∈S 0 (c? ) S̃ ∈ N (S) : γ̄ S̃ − γ̄ (S) = γ̄ .

Provided µ (γ̄)2 = µ2 − γ̄ 0 Σ−1
                              γγ γ̄ ≥ 0, the set of possible biases under S ∈ S
                                                                                RN (·, γ̄) is

                                                h               √                      √     i
              c̄ (S) − c? : S ∈ S RN (c? , γ̄) = Λγ̄ − µ (γ̄) σc 1 − ∆, Λγ̄ + µ (γ̄) σc 1 − ∆ ,
          


for any c? such that S RN (c? , γ̄) is nonempty.

     Proposition 4 extends the results of Andrews et al. (2017) to the case where γ̂ need not be
a vector of estimation moments, and thus we may have ∆ < 1. It likewise extends Proposition 2.
The resulting set of first-order asymptotic biases for ĉ is centered at Λγ̄ with width proportional to
√
  1 − ∆.
q Unlike in Proposition 2, the degree of misspecification now enters the width through    q µ (γ̄) =
  µ − γ̄ Σγγ γ̄. Intuitively, µ (γ̄) measures the degree of excess misspecification beyond γ̄ 0 Σ−1
   2     0 −1
                                                                                                 γγ γ̄,
                                               
which is the minimum necessary to allow γ̄ S̃ − γ̄ (S) = γ̄. If the degree of excess misspecification
is small then the first-order asymptotic bias of ĉ is close to Λγ̄, while if the degree of excess
misspecification is large then a wider range of biases is possible.

Proof of Proposition 4 The proof is similar to that for Proposition 2 in the main text. By
Lemma 1 we again have
                                      c? (h) = EF0 [φc (Di ) sh (Di )] .

Note, next, that by the definition of S RN (c? , γ̄) and Lemma 1, for any S ∈ S RN (c? , γ̄) there exist
(h, z) ∈ H × Z with S = S (h, z), c? (h) = c? , and

        EF0 [φγ (Di ) (sh (Di ) + sz (Di ))] − EF0 [φγ (Di ) sh (Di )] = EF0 [φγ (Di ) sz (Di )] = γ̄.

Thus, writing γ̄z = EF0 [φγ (Di ) sz (Di )] and c̄z = EF0 [φc (Di ) sz (Di )] for brevity, our task reduces
to showing that
    n                          h         i    o h               √                      √     i
     c̄z : z ∈ Z, γ̄z = γ̄, EF0 sz (Di )2 ≤ µ2 = Λγ̄ − µ (γ̄) σc 1 − ∆, Λγ̄ + µ (γ̄) σc 1 − ∆ .

     Define s (Di ; γ̄) = φγ (Di )0 Σ−1
                                     γγ γ, and


                                      εz (Di ) = sz (Di ) − s (Di ; γ̄z ) .

Note that EF0 [φγ (Di ) εz (Di )] = 0 and EF0 [s (Di ; γ̄z ) εz (Di )] = 0 by construction. We can write

           cz = EF0 [φc (Di ) sz (Di )] = EF0 φc (Di ) φγ (Di )0 Σ−1
                                                               
                                                                  γγ γ̄z + EF0 [φc (Di ) εz (Di )]




                                                       37
                                                = Λγ̄z + EF0 [φc (Di ) εz (Di )] .

Next, define
                                                 φ̃c (Di ) = φc (Di ) − Λφγ (Di )

and note that                                                      h                  i
                                      EF0 [φc (Di ) εz (Di )] = EF0 φ̃c (Di ) εz (Di ) .

The Cauchy-Schwarz inequality then implies that
                                  h                     i r   h            ir    h          i
                                                                         2
                            EF0       φ̃c (Di ) εz (Di ) ≤ EF0 φ̃c (Di )      EF0 εz (Di )2

                                                                             √
                  q                      r       h                 i                 r      h         i
                                                               2
              =       σc2 − ΛΣγγ Λ0          EF0 εz (Di )              = σc 1 − ∆        EF0 sz (Di )2 − γ̄z0 Σ−1
                                                                                                               γγ γ̄z .

                                                                    h         i
Combining these results we see that for z such that γ̄z = γ̄ and EF0 sz (Di )2 ≤ µ2 ,

                                        √                                                √
                                                 q                                               q                        
                  c̄z ∈ Λγ̄ − σc 1 − ∆                µ2 −     γ̄ 0 Σ−1
                                                                     γγ γ̄, Λγ̄   + σc 1 − ∆          µ2 −   γ̄ 0 Σ−1
                                                                                                                   γγ γ̄       ,

which are the bounds stated in the proposition. In particular,
                                                    h         i
                                             0 ≤ EF0 εz (Di )2 ≤ µ2 − γ̄z0 Σ−1
                                                                            γγ γ̄z ,

                                                                h           i
                                                                          2
so if γ̄z = γ̄ we must have γ̄ 0 Σ−1
                                  γγ γ̄ ≤ µ 2 in order that E
                                                             F0  sz (Di )     ≤ µ2 . Hence, if µ2 − γ̄ 0 Σ−1
                                                                                                          γγ γ̄ < 0
                                             h         i
there exists no z with γ̄z = γ̄ and EF0 sz (Di )2 ≤ µ2 .
       To complete the proof it remains to show that these bounds are tight, so that for any (c̄, γ̄, µ)
with
                                        √                                                √
                                                 q                                               q              
(16)              c̄ ∈ Λγ̄ − σc 1 − ∆                 µ2   −   γ̄ 0 Σ−1
                                                                     γγ γ̄, Λγ̄   + σc              2    0 −1
                                                                                             1 − ∆ µ − γ̄ Σγγ γ̄

                                                   h         i
there exists z ∈ Z with c̄z = c̄, γ̄z = γ̄, and EF0 sz (Di )2 ≤ µ2 . If ∆ < 1, define

                                                                                            c̄ − Λγ̄
                                       s∗ (Di ; c̄, γ̄) = s (Di ; γ̄) + φ̃c (Di )                    .
                                                                                         σc2 (1 − ∆)

Note that
                                                 EF0 [φγ (Di ) s∗ (Di ; c̄, γ̄)] = γ̄

while                                                             h           i                   c̄ − Λγ̄
                       EF0 [φc (Di ) s∗ (Di ; c̄, γ̄)] = Λγ̄ + EF0 φ̃c (Di )2                              = c̄.
                                                                                                σc2 (1− ∆)




                                                                        38
Moreover,
                        h                 i     h            i     h           i (c̄ − Λγ̄)2
                     EF0 s∗ (Di ; c̄, γ̄)2 = EF0 s (Di ; γ̄)2 + EF0 φ̃c (Di )2
                                                                                σc4 (1 − ∆)2
                                                                    (c̄ − Λγ̄)2
                                                  = γ̄ 0 Σ−1
                                                          γγ γ̄ +               .
                                                                    σc2 (1 − ∆)
However, by (16) we know that

                                                           √          q
                                         |c̄ − Λγ̄| ≤ σc 1 − ∆            µ2 − γ̄ 0 Σ−1
                                                                                     γγ γ̄


and thus that
                                              (c̄ − Λγ̄)2    2      0 −1
                                                                             
                                                          ≤ µ  − γ̄  Σ γγ γ̄
                                              σc2 (1 − ∆)
      h                 i
so EF0 s∗ (Di ; c̄, γ̄)2 ≤ µ2 . By Assumption 4, however, there exists z ∈ Z with
                                            h                              i
                                         EF0 (sz (Di ) − s∗ (Di ; c̄, γ̄))2 = 0,
                                             h         i
and thus z yields c̄z = c̄, γ̄z = γ̄, and EF0 sz (Di )2 ≤ µ2 as desired. In cases with ∆ = 1, on the
other hand, we can use s∗ (Di ; c̄, γ̄) = s (Di ; γ̄) .


B          Asymptotic Divergence
This section studies the asymptotic behavior of the divergence
                                                                                         
                                  
                                       1  1
                                                                      fh,z Di ; √1n , √1n
(17)                       rh,z       √ ,√        = EFh,z (th ,0) ψ                    
                                        n n                                        1
                                                                        fh,z Di ; n , 0
                                                                                  √


as n → ∞, where as in the main text we assume that ψ (1) = 0 and ψ 00 (1) = 2. To derive our
results we impose the following assumption.

Assumption 6. For t = (th , tz ) ∈ R2 and fh,z (Di ; t) = fh,z (Di ; th , tz ) , fh,z (Di ; t) is twice contin-
uously differentiable in t at 0, and there exists an open neighborhood B of zero such that
       "                                                                                                           ∂
                                                                                                                                        !#
                                     ∂2                                                                           ∂tz fh,z   (Di ; t)
                                                                                                            
                  ∂                                      fh,z (Di ; th , 0) 0                fh,z (Di ; t)
EF0 sup              fh,z (Di ; t) +   2
                                         fh,z (Di ; t) +                   ψ                                                                 ,
           t∈B   ∂tz                 ∂tz                  fh,z (Di ; 0)                      fh,z (Di ; t)         fh,z (Di ; t)
                                                                                                            
                                                                                !   ∂2
                                         fh,z (Di ; th , 0) 0       fh,z Di ; t̃         f
                                                                                     ∂t2z h,z
                                                                                                  Di ; t̃
                       EF0  sup                           ψ                                                 ,
                            (t,t̃)∈B2     fh,z (Di ; 0)             fh,z (Di ; t)      fh,z (Di ; t)

and                                                                        !         ∂
                                                                                                      !2 
                                       fh,z (Di ; th , 0) 00    fh,z Di ; t̃          ∂tz fh,zDi ; t̃
                     EF0  sup                           ψ                                                
                          (t,t̃)∈B2     fh,z (Di ; 0)           fh,z (Di ; t)          fh,z (Di ; t)


                                                                39
are finite.

        Under this assumption, we obtain the asymptotic approximation to divergence discussed in
the main text.

Proposition 5. Under Assumptions 3 and 6,
                                                                                          
                                                                             1  1                    h         i
                                                     lim n · rh,z           √ ,√                = EF0 sz (Di )2 .
                                                  n→∞                         n n
                                                                                            
Proof of Proposition 5 Recall that rh,z                                         √1 , √1          can be written as in (17). Assumption 6 and
                                                                                  n    n
Leibniz’s rule implies for n sufficiently large we can exchange integration and differentiation twice,
so by Taylor’s Theorem with a mean-value residual,21
                                                                                                  
                                                                                      1  1
                                                                    n · rh,z         √ ,√                =
                                                                                       n n
                                                                                                       ∂                                                
                                                                                   0 fh,z (Di ;tn ) ∂tz fh,z (Di ;tn ) √1 +
                                                                                    
                                      fh,z (Di ;tn )         fh,z (Di ;tn )
                                     fh,z (Di ;0)       ψ fh,z   (Di ;tn )    + ψ     fh,z (Di ;tn )        fh,z (Di ;tn )        n                        
          n · EF0 
                                                         ∂2                                                                           2 ! !              
                                                             2 fh,z (Di ;t̃n )
                                                                                                           ∂
                                            (          )                                       (          )         f     ( D  ; t̃   )
                                                                                                                                                            
                                      f       D  ; t̃n                                   f       D  ; t̃n             h,z    i      n
                           1
                                 ψ 0 fh,z (Di ;tn )                                 00                                                      1
                                        h,z    i           ∂t                              h,z    i             ∂tz
                                                             fh,z (Di ;tn ) + ψ
                                                            z                                                                                              
                           2                                                             fh,z (Di ;tn )            fh,z (Di ;tn )           n

                                                                     h      i
for tn =          √1 , 0       , t̃n =       √1 , t̃z,n       and t̃z,n ∈ 0, √1n . Thus, since ψ (1) = 0 by assumption,
                    n                          n

                                                                                                  
                                                                                      1  1
                                                                    n · rh,z         √ ,√                =
                                                                                       n n

                                                                    √                   ∂
                                                                                                                                                           
                                                                                            fh,z (Di ;tn )
                                                                         nψ 0 (1)      ∂tz
                                                                                           fh,z (Di ;0) +
                                                                                                                                                       2 ! 
        EF0                                                            ∂2                                                                                   .
                                                                                   (Di ;t̃n )
                                                                                                                            
                                                  fh,z (Di ;t̃n )           fh,z                             fh,z (Di ;t̃n )         ∂
                                                                                                                                      fh,z (Di ;t̃n )
                   1 fh,z (Di ;tn )                                     ∂t2
                                         ψ0                               z
                                                                                                + ψ 00                              ∂tz
                                                                                                                                                            
                   2 fh,z (Di ;0)                 fh,z (Di ;tn )         fh,z (Di ;tn )                      fh,z (Di ;tn )          fh,z (Di ;tn )


        Assumption 6 and Leibniz’s rule imply that for n sufficiently large,
              "    ∂
                                             #
                  ∂tz fh,z     (Di ; tn )
                                                     Z                                 Z             
                                                           ∂           1              ∂             1
        EF0                                      =            fh,z d; √ , 0 dν (d) =       fh,z d; √ , 0 dν (d) = 0.
                    fh,z (Di ; 0)                         ∂tz           n            ∂tz             n

Hence, we see that                                                                                
                                                                                      1  1
                                                                    n · rh,z         √ ,√                =
                                                                                       n n



   21
        Specifically, note that for q(th , tz ) = rh,z (th , tz ) we can write

                                                                                 ∂                1 ∂2
                                                 q(th , tz ) = q(th , 0) +          q(th , 0)tz +        q(th , t̃z )t2z
                                                                                ∂tz               2 ∂t2z

with t̃z ∈ [0, tz ].


                                                                                      40
                                                                                                                                                !2  
                                                           ∂2
                                                  !                                                          !
                                                               f        Di ; t̃n                                            ∂
      1 fh,z (D  ; t )
                i n  0             fh,z Di ; t̃n          ∂t2z h,z                              fh,z Di ; t̃n             ∂tz fh,z   Di ; t̃n
EF0                     ψ                                                             + ψ 00                                                          .
      2 fh,z (Di ; 0)               fh,z (Di ; tn )             fh,z (Di ; tn )                  fh,z (Di ; tn )            fh,z (Di ; tn )

     Since ψ 00 (1) = 2, the Dominated Convergence Theorem and Assumption 6 imply that
                                                                                                                                                !2  
                                                           ∂2
                                                  !                                                          !
                                                               f        Di ; t̃n                                            ∂
         1 fh,z (Di ; tn )  0      fh,z Di ; t̃n          ∂t2z h,z                              fh,z Di ; t̃n             ∂tz fh,z   Di ; t̃n
EF0                        ψ                                                          + ψ 00                                                         
         2 fh,z (Di ; 0)            fh,z (Di ; tn )             fh,z (Di ; tn )                  fh,z (Di ; tn )            fh,z (Di ; tn )

                                                                                                                   !2 
                                                     ∂2                                        ∂
                          1                              f
                                                     ∂t2z h,z
                                                                (Di ; 0)                      ∂tz fh,z   (Di ; 0)
                         → EF0 ψ 0 (1)                                    + ψ 00 (1)                                 
                          2             fh,z (Di ; 0)                                           fh,z (Di ; 0)
                                                                                                        
                                                                 ∂2
                                             1                       f
                                                                 ∂t2z h,z
                                                                            (Di ; 0)
                                      = EF0  ψ 0 (1)                                   + sz (Di )2  .
                                             2        fh,z (Di ; 0)

However, Assumption 6 and Leibniz’s rule imply that
                                            
                       ∂2
                         2 fh,z   (Di ; 0)              ∂2                        ∂2
                                                  Z                                               Z
               EF0    ∂tz                   =              fh,z (d; 0) dν (d) =                     fh,z (d; 0) dν (d) = 0,
                         fh,z (Di ; 0)                  ∂t2z                      ∂t2z

so                                                                         
                                                               1  1                  h         i
                                       lim n · rh,z           √ ,√              = EF0 sz (Di )2 ,
                                      n→∞                       n n
as we wanted to show.


C        Asymptotic Distinguishability
In Section 4.3 of the paper, and Section B above, we discuss that the neighborhoods studied in our
localasymptotic
                analysis
                       correspond
                                  to bounds on the asymptotic Cressie-Read divergence between
       1                 1 1
Fh,z √n , 0 and Fh,z √n , √n . In the section, we show that they also correspond to bounds on
the asymptotic power of tests to distinguish S (h, z) and S (h, 0).

Proposition 6. Under Assumption 3, the most powerful level α test of the null hypothesis
                                                                                                 
                                                                                               1
                                          H0 : (D1 , ..., Dn ) ∼           ×ni=1 Fh,z         √ ,0
                                                                                                n

against                                                                                                 
                                                                                             1  1
                                       H1 : (D1 , ..., Dn ) ∼          ×ni=1 Fh,z           √ ,√
                                                                                              n n
                                                          r        h           i
                                                                              2
has power converging to 1−FN (0,1) vα −                          EF0 sz (Di )      for vα the 1−α quantile of the standard
normal distribution.

     The proof of Proposition 6 shows that the most powerful test corresponds asymptotically to a

                                                                       41
                                                        r      h         i
z-test, where the z-statistic has mean                      EF0 sz (Di )2 under H1 .


Proof of Proposition 6 By the Neyman-Pearson Lemma (see Theorem 3.2.1 in           Lehmann
                                                                                           and
                                                                                n   1
Romano 2005), the most powerful level-α test of H0 : (D1 , ..., Dn ) ∼ ×i=1 Fh,z √n , 0 against
                                         
                        n           1 √1
H1 : (D1 , ..., Dn ) ∼ ×i=1 Fh,z   √
                                     n
                                       , n rejects when the log likelihood ratio
                                                                 
                                            n    1   1      n    1
                                      log dFh,z √ , √    /dFh,z √ , 0
                                                  n n             n

exceeds a critical value vα,n chosen to ensure rejection probability α under H0 (and may randomize
when the log likelihood ratio exactly equals the critical value). Here we again abbreviate ×ni=1 F =
F n.
       By Assumption 3 and the quadratic expansion of the likelihood in the proof of Lemma 1,
however, we see that under S (0, 0) , for g (Di ; h, z) = sh (Di ) + sz (Di ) ,


                                                                         !0                 h               i  
                                                                                      − 21 EF0 g (Di ; h, 0)2
                               !                                 !
                  n
                dFh,z √1 ,0                    n
                                             dFh,z       √1 , √1
                         n                                 n    n
          log       dF0n              log               dF0n                  →d N          h               i  , Σ̃
                                                                                      − 12 EF0 g (Di ; h, z)2

for                                    h               i                                    
                                     EF0 g (Di ; h, 0)2    EF0 [g (Di ; h, 0) g (Di ; h, z)]
                  Σ̃ =                                             h                i       .
                         EF0 [g (Di ; h, 0) g (Di ; h, z)]     EF0 g (Di ; h, z)2

Le Cam’s third lemma thus implies that under S (h, 0) ,
                                                            !                                  !   !0
                                              n
                                            dFh,z √1 ,0                         n
                                                                              dFh,z    √1 , √1
                                                     n                                   n    n
                                     log        dF0n                  log             dF0n                  →d

                                                               h                    i                              
                                                        1
                                                        2 EF0i       g (Di ; h, 0)2
                  N                   h
                                                           2
                                                                                                                  , Σ̃ ,
                                − 12 EF0 g (Di ; h, z)           + EF0 [g (Di ; h, 0) g (Di ; h, z)]

while under S (h, z) ,
                                                            !                                  !   !0
                                              n
                                            dFh,z √1 ,0                         n
                                                                              dFh,z    √1 , √1
                                                     n                                   n    n
                                     log        dF0n                  log             dF0n                  →d

                             h              i                                     
                       − 12 EF0 g (Di ; h, 0)2 + EF0 [g (Di ; h, 0) g (Di ; h, z)]
                  N                           h
                                                                 2
                                                                   i                , Σ̃ .
                                         1
                                           E
                                         2 F0     g (D i ; h, z)




                                                                        42
       Since
                                                                                                     
                              n
                            dFh,z       √1 , √1                   n
                                                                dFh,z    √1 , √1                  n
                                                                                                dFh,z √1 , 0
                                          n    n                           n    n                       n
                 log                               = log                         − log                   ,
                                                                           n                           n
                                    
                              n
                            dFh,z        √1 , 0                         dF0                         dF0
                                           n


and sz (d) = sh (d) = 0 when h = z = 0, g (Di ; h, 0) − g (Di ; h, z) = −g (Di ; 0, z) we see that
                                  h             i      h              i
              n
            dFh,z       √1 , √1
                            N − 1 EF g (Di ; 0, z)2 , EF g (Di ; 0, z)2                                    Under S (h, 0)
                          n    n
                               2 h
                                      0                    0
  log                →d                         i     h               i
          n    1
        dFh,z √n , 0        N 12 EF0 g (Di ; 0, z)2 , EF0 g (Di ; 0, z)2                                   Under S (h, z) .

                h              i     h         i
Hence, since EF0 g (Di ; 0, z)2 = EF0 sz (Di )2 and vα,n corresponds to the 1 − α quantile of the
log likelihood ratio under the null, we have that
                                         !
                          n
                        dFh,z  √1 , √1
                                n n          − vα,n
                                                           
             log                                           N (−vα , 1)
                           n
                         dFh,z  √1 ,0
                                  n
                                                                                        under S (h, 0)
                    r                                   →d    r        h     i        
                                                           N    EF0 sz (Di )2 − vα , 1  under S (h, z)
                        h          i
                     EF0 sz (Di )2
                                                           


for vα the 1 − α quantile of a standard normal distribution, from which the result follows.


D       Non-Local Misspecification
This section develops our informativeness measure based on probability limits, rather than first-
order asymptotic bias.
       Under Assumptions 1, 3, and 4, provided the estimators ĉ and γ̂ are regular in the sense
discussed in Newey (1994), Theorem 2.1 of Newey (1994) implies that the probability limits c̃ (·)
and γ (·) are asymptotically linear functionals, in the sense that

                limtz →0 kc̃ (F0,z (0, tz )) − c (η0 ) − tz EF0 [sz (Di ) φc (Di )]k /tz = 0 for all z ∈ Z
(18)
               limtz →0 kγ (F0,z (0, tz )) − γ (F0 ) − tz EF0 [sz (Di ) φγ (Di )]k /tz = 0 for all z ∈ Z.

Assumption 2 would be implied by an assumption that (ĉ, γ̂) are regular in the base model, so
the assumption of regularity of (ĉ, γ̂) in the nesting model can be understood as a strengthening
of Assumption 2. See Newey (1994) and Rieder (1994) for discussion. Since (18) only restricts
                                                     ˜ (r̄) as defined in the main text let us instead
behavior as tz → 0 for fixed z, rather than studying ∆
consider an analogue defined   h  using finite      i collections of paths. Specifically, continuing to define
                                  fh,z (Di ;th ,tz )
rh,z (th , tz ) = EFh,z (th ,0) ψ fh,z (Di ;th ,0) , for each z ∈ Z let

                                               t̄ (z, µ) = inf {tz ∈ R+ : r0,z (0, tz ) ≥ µ}

denote the largest value of t such that r0,z (0, tz ) < µ for all tz < t̄ (z, µ) . Let Z+ ⊂ Z denote the


                                                                    43
                     h         i
set of z ∈ Z with EF0 sz (Di )2 > 0.
     Let Q ⊂ Z+ denote a finite subset of Z+ , and let Q denote the set of all such finite subsets.
Finally, let
                       b̃N (µ, Q) = sup {|c̃ (F0,z (0, tz )) − c (η0 )| : z ∈ Q, tz < t̄ (z, µ)}

denote the analogue of b̃N (µ) based on the finite set of paths Q, and for ε > 0 let

                                                                                                               √
b̃RN,ε (µ, Q) = sup {|c̃ (F0,z (0, tz )) − c (η0 )| : z ∈ Q, tz < t̄ (z, µ) , kγ (F0,z (0, tz )) − γ (F0 )k ≤ ε µ}

denote the analogue of b̃RN (µ, Q) based on Q which allows the probability limit of γ̂ to change by
         √
at most ε µ. Because b̃RN,0 (µ, Q) may equal 0 even for large µ due to the approximation error in
(18), we consider limits as ε ↓ 0 (i.e., as ε → 0 from above). Based on these objects, we define the
            ˜ (µ) as
analogue of ∆
                                    ˜ (µ, Q) = sup inf lim b̃RN,ε (µ, Q1 ) ,
                                    ∆
                                              Q1 ∈Q Q2 ∈Q ε↓0 b̃N (µ, Q2 )

provided the limit exists.

Proposition 7. Suppose Assumptions 1, 3, and 4 hold, that the estimators ĉ and γ̂ are regular,
and that Assumption 6 holds for h = 0 and all z ∈ Z+ . For ψ (·) twice continuously differentiable
and ψ (1) = 0, ψ 00 (1) = 2,

                                                        b̃RN,ε (µ, Q1 )        √
                                   sup inf lim lim                         =       1 − ∆.
                                  Q1 ∈Q Q2 ∈Q ε↓0 µ↓0       b̃N (µ, Q2 )

     It is important that we take the limit as µ ↓ 0 inside the limit as ε ↓ 0 and the sup and inf,
since this order of limits allows us to take advantage of the approximation result (18).

Proof of Proposition 7 Note, first, that our Assumptions 1, 3, and 4 imply the conditions of
Theorem 2.1 of Newey (1994) other than regularity of (ĉ, γ̂). Specifically, conditions (i) and (ii) of
Theorem 2.1 in Newey (1994) follow from our Assumptions 3 and 4. Condition (iii) is implied by
our Assumption 1. Regularity of (ĉ, γ̂) is assumed, so Theorem 2.1 of Newey (1994) implies (18).
     Note, next, that for any z ∈ Z+ , the proof of Proposition 5 implies that
                                                                   h         i
                                       lim r0,z (0, tz ) /t2z = EF0 sz (Di )2 .
                                       tz ↓0

                                             i−         1
                            √     h
Hence, as µ ↓ 0, t̄ (z, µ) / µ → E sz (Di )2
                                                2
                                                  . For all z ∈ Z+ , (18) implies that


                limµ↓0 suptz ≤t̄(z,µ) kc̃ (F0,z (0, tz )) − c (η0 ) − tz EF0 [sz (Di ) φc (Di )]k /tz = 0
               limµ↓0 suptz ≤t̄(z,µ) kγ (F0,z (0, tz )) − γ (F0 ) − tz EF0 [sz (Di ) φγ (Di )]k /tz = 0,




                                                            44
and thus that
                                                                                                          
                          1
                         √ (c̃ (F0,z (0, tz )) − c (η0 ) , γ (F0,z (0, tz )) − γ (F0 )) : tz ≤ t̄ (z, µ)
                           µ
                                                                                          h           i− 1 
                                                                                                     2    2
              →         t̃z (EF0 [sz (Di ) φc (Di )] , EF0 [sz (Di ) φγ (Di )]) : t̃z ≤ EF0 sz (Di )

in the Hausdorff sense as µ ↓ 0. Correspondingly, for any Q ∈ Q,
                                                                                                        
                    1
                   √ (c̃ (F0,z (0, tz )) − c (η0 ) , γ (F0,z (0, tz )) − γ (F0 )) : z ∈ Q, tz ≤ t̄ (z, µ)
                     µ
                                                                                           h          i− 1 
                  t̃z (EF0 [sz (Di ) φc (Di )] , EF0 [sz (Di ) φγ (Di )]) : z ∈ Q, t̃z ≤ EF0 sz (Di )2
                                                                                                          2
        →                                                                                                     .

Hence, for any nonempty Q ∈ Q
                                                                                        
                                                   
                                                    |E                                  
                          1                               F0 [sz (Di ) φc (Di )]|
                                                                                         
                         √ b̃N (µ, Q) → max                                i1     : z ∈ Q as µ ↓0.
                           µ                                   h
                                                          EF0 sz (Di )2
                                            
                                                                           2            
                                                                                         


     Matters are somewhat more delicate for b̃RN,ε (µ, Q) . Note, in particular, that for ε > 0, as
µ ↓ 0 we have
                                                      1
                                                     √ b̃RN,ε (µ, Q) →
                                                       µ
                                                               h             i− 1                                         
                                                                          2      2
     sup t̃z EF0 [sz (Di ) φc (Di )] : z ∈ Q, t̃z ≤ EF0 sz (Di )                     , t̃z kEF0 [sz (Di ) φγ (Di )]k ≤ ε
                                                         h         i− 1                               ε
                                                                                                                         
  = sup t̃z EF0 [sz (Di ) φc (Di )] : z ∈ Q, t̃z ≤ min EF0 sz (Di )2
                                                                        2
                                                                          ,                                                    ,
                                                                                             kEF0 [sz (Di ) φγ (Di )]k
where we define ε/0 = ∞ for ε > 0. Consequently,

                                                      1
                                                     √ b̃RN,ε (µ, Q) →
                                                       µ
                                                         h         i− 1      ε
                                                                                              
  sup t̃z |EF0 [sz (Di ) φc (Di )]| : z ∈ Q, t̃z ≤ min EF0 sz (Di )2
                                                                        2
                                                                          ,                      .
                                                                    kEF0 [sz (Di ) φγ (Di )]k
                                                            h         i
Note, however, that by the Cauchy-Schwarz inequality and EF0 sz (Di )2 < ∞, EF0 [sz (Di ) φc (Di )]
is finite for all z ∈ Z, so for any z with EF0 [sz (Di ) φγ (Di )] 6= 0,

                                              ε
                                                            EF [sz (Di ) φc (Di )] → 0
                                   kEF0 [sz (Di ) φγ (Di )]k 0




                                                               45
as ε ↓ 0. Hence, as ε ↓ 0,
                                                          h          i− 1                               ε
                                                                                                                           
    sup t̃z |EF0 [sz (Di ) φc (Di )]| : z ∈ Q, t̃z ≤ min EF0 sz (Di )2
                                                                          2
                                                                            ,
                                                                                               kEF0 [sz (Di ) φγ (Di )]k
                                                                                            
                                                    
                                                     |E                                     
                                                            [sz (Di ) φc (Di )]|
                                                           F0
                                                                                             
                                       → max                  h           i1     : z ∈ Q0
                                                                       2 2               
                                                          EF0 sz (Di )                   

for Q0 = {z ∈ Q : EF0 [sz (Di ) φγ (Di )] = 0}, where we define this max to be zero if Q0 is empty.
         This immediately implies that
                                                                         h          i1             
                                                                                    2 2
                                        max |EF0 [sz (Di ) φc (Di )]| /EF0 sz (Di )      : z ∈ Q1,0
                      b̃RN,ε (µ, Q1 )
              lim lim                 =                                              i1           
              ε↓0 µ↓0 b̃N (µ, Q2 )
                                                                           h
                                                                                     2 2
                                        max |EF0 [sz (Di ) φc (Di )]| /EF0 sz (Di )       : z ∈ Q2


for Q1,0 = {z ∈ Q1 : EF0 [sz (Di ) φγ (Di )] = 0} , provided the denominator on the right hand side is
non-zero.22
         To complete the proof, note that for Q0 the set of possible Q0 ,
                                                                                h         i1        
                                     supQ0 ∈Q0 max |EF0 [sz (Di ) φc (Di )]| /EF0 sz (Di )2 : z ∈ Q0
                                                                                            2

                   b̃RN,ε (µ, Q1 )
 sup inf lim lim                   =                                                      i1       .
Q1 ∈Q Q2 ∈Q ε↓0 µ↓0 b̃N (µ, Q2 )
                                                                                 h
                                                                                          2 2
                                      supQ∈Q max |EF0 [sz (Di ) φc (Di )]| /EF0 sz (Di )      :z∈Q

The proof of Proposition 2 shows, however, that
                                                                     h         i1
                                   max |EF0 [sz (Di ) φc (Di )]| /EF0 sz (Di )2 = σc
                                                                                2

                                   z∈Z+


and                                                                                   h
                                                                                               2
                                                                                                   i1
                                                                                                    2
                                                                                                            √
                              max                  |EF0 [sz (Di ) φc (Di )]| /EF0 sz (Di )              = σc 1 − ∆.
                  z∈Z+ :EF0 [sz (Di )φγ (Di )]=0

Hence,
                                                                b̃RN,ε (µ, Q1 )       √
                                        sup inf lim lim                           =       1 − ∆,
                                       Q1 ∈Q Q2 ∈Q ε↓0 µ↓0        b̃N (µ, Q2 )
as we wanted to show.


E         Accounting for Richer Dependence of ĉ on the Data
In Section 5, for cases where the function c (θ) depends on the distribution of the data other than
through θ, we effectively fix the distribution of the data at the empirical distribution for the purposes

    22
         If the denominator on the right hand side is zero, we define the limit as +∞.


                                                                  46
of estimating ∆ and Λ. Here we discuss how to allow for uncertainty about the distribution of data
in a special case, and present corresponding calculations for our applications.
         Suppose in particular that

                                                      1X        
(19)                                           ĉ =     c θ̂; Di
                                                      n
                                                        i

for some function c (·). In contrast to the setup in Section 5, here we allow that ĉ depends on the
data directly, and not only through the dependence of ĉ on θ̂.
         In this case, one can show that the recipe in Section 5 applies, with the modification that
                                                                          
(20)                                  φ̂c (Di ) = c θ̂; Di + Λ̂cg φg Di ; θ̂
               
where φg Di ; θ̂ and Λ̂cg are as defined in Section 5, and Ĉ in the definition of Λ̂cg is now given
by the gradient of n1 i c (θ; Di ) with respect to θ at θ̂.
                     P

         The proof of this result, which we omit, proceeds by noting that we can augment the GMM pa-
rameter vector as (c, θ), and correspondingly augment the moment equation as (c (θ; Di ) − c, φg (Di ; θ)),
following which we can derive the estimated influence function for ĉ as we would for any element
of θ̂.
      In the cases of Attanasio et al. (2012a) and Gentzkow (2007a), we can represent the calculation
of ĉ in the form given in (19) and thus calculate ∆ˆ using the modified estimated influence function
in (20). In the case of Attanasio et al. (2012a), the estimates in Table I change from 0.283, 0.227,
and 0.056, respectively, to 0.277, 0.221, and 0.055. In the case of Gentzkow (2007a), the estimates
in Table II change from 0.514, 0.009, and 0.503, respectively, to 0.517, 0.008, and 0.507.




                                                            47
