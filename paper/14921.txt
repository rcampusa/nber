                                NBER WORKING PAPER SERIES




                                EXPERTS AND THEIR RECORDS

                                          Alexander Frankel
                                          Michael Schwarz

                                        Working Paper 14921
                                http://www.nber.org/papers/w14921


                      NATIONAL BUREAU OF ECONOMIC RESEARCH
                               1050 Massachusetts Avenue
                                 Cambridge, MA 02138
                                      April 2009




We would like to thank Heski Bar-Isaac, Martin Cripps, Drew Fudenberg, Andrzej Skrzypacz, and
Steve Tadelis for their helpful comments, as well as seminar audiences at Stanford and Stanford GSB.
 Alexander Frankel primarily worked on this during an internship at Yahoo! Research. The views
expressed herein are those of the author(s) and do not necessarily reflect the views of the National
Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2009 by Alexander Frankel and Michael Schwarz. All rights reserved. Short sections of text, not
to exceed two paragraphs, may be quoted without explicit permission provided that full credit, including
© notice, is given to the source.
Experts and Their Records
Alexander Frankel and Michael Schwarz
NBER Working Paper No. 14921
April 2009
JEL No. C7,C73,D82,J01

                                              ABSTRACT

Consider an environment where long-lived experts repeatedly interact with short-lived customers.
In periods when an expert is hired, she chooses between providing a profitable major treatment or
a less profitable minor treatment. The expert has private information about which treatment best serves
the customer, but has no direct incentive to act in the customer's interest. Customers can observe the
past record of each expert's actions, but never learn which actions would have been appropriate. We
find that there exists an equilibrium in which experts always play truthfully and choose the customer's
preferred treatment. The expert is rewarded for choosing the less profitable action with future business:
customers return to an expert with high probability if the previous treatment was minor, and low probability
if it was major.

If experts have private information regarding their own payoffs as well as what treatments are appropriate,
then there is no equilibrium with truthful play in every period. But we construct equilibria where
experts are truthful arbitrarily often as their discount factor converges to one.


Alexander Frankel
Stanford Graduate School of Business
518 Memorial Way
Stanford, CA 94305
afrankel@stanford.edu

Michael Schwarz
Yahoo! Research
2397 Shattuck Ave
Berkeley, CA 94704
and NBER
mschwarz@yahoo-inc.com
1    Introduction
In many economic environments, uninformed customers must rely on experts to both diag-
nose and treat their problems. Doctors, dentists, mechanics, and management consultants
all help to determine what services their clients need in addition to providing those services.
There is a misalignment of incentives when experts earn higher profits on certain treatments
than on others.
    This paper considers a repeated environment in which experts are long-lived and cus-
tomers can use experts’ records of past actions to determine whom to hire. We take treat-
ments to be pure credence goods: customers observe past treatments, but they never receive
signals about what treatments would have been appropriate. This makes it difficult for cus-
tomers to punish experts for past dishonesty, as dishonesty is never revealed. Moreover, we
assume that an expert’s payoffs from each action are completely independent of the cus-
tomer’s underlying need, and that customers are short-lived players for whom long-term
contracts are impossible.
    Since experts’ payoffs do not depend on the problem a customer faces, an expert will
play truthfully in a period only if her expected discounted profit is equal across actions. An
honest expert is likely to have a balanced record over time, with the proportion of major
and minor treatments close to the probability that each is needed, but it will not be an
equilibrium for customers to choose the expert whose record is most balanced. If customers
chose in this way, then experts would just take actions to keep their records balanced.
    Experts may have the proper incentives to be truthful if customers give more business to
experts who have chosen the less profitable treatments in the past. The logic is illustrated by
an anecdote about McKinsey & Co.’s former managing director Marvin Bower, relayed in a
Business Week obituary. “In the 1950s, Bower was summoned to Los Angeles by billionaire
Howard Hughes, who wanted him to study Paramount Pictures.... But Bower sensed that
nothing good could come of working for Hughes. He found the entrepreneur’s approach
to business ‘so unorthodox and so unusual’ that he felt he would never be able to help
Paramount. Instead of taking the assignment and reaping a big fee, he walked away. The
move was classic Bower. He built McKinsey into a global consulting powerhouse by insisting
that values mattered more than money” (Byrne (2003)). In other words, by publicly rejecting
a profitable action, McKinsey increased its future business.
    We study a repeated game between customers and experts modeled after the above
interactions and look for conditions under which the experts may be truthful in equilibrium.
    In each period, a new customer arrives on the market and chooses an expert. There are

                                              1
two possible states of the world: the customer might need a major treatment, or a minor one.
The customer prefers that the appropriate action be taken but has no information about
the state of the world. His only action is to choose an expert. Once a choice is made, the
customer must defer to the expert’s judgment.
    The chosen expert observes the state and then decides whether to provide a major or
minor treatment, and the expert’s payoffs depend on what she does but not on what the
customer needs. In other words, her payoff is a function of the action but not the state.
This can be thought of as an environment where prices are exogenously fixed at industry
standard levels. Experts always prefer some work to no work, and when chosen they will
earn a higher profit on major treatments than on minor ones.
    While true states are never revealed to future customers, we assume the full history of
experts chosen and actions taken to be observable. Experts are infinitely long lived, but
customers disappear from the market after they receive a treatment.
    In Section 3 we show that if experts are homogeneous then a truthful equilibrium of
the repeated game can be found; the promise of future business removes the incentive to
play major treatments over minor ones. Customers only need to look at the most recent
action taken. If it was a minor treatment, they return to the last period’s expert with
high probability. If it was a major treatment, they return with a low probability. By
setting appropriate probabilities, they can make experts exactly indifferent between major
treatments with a high short term payoff and minor ones with a high continuation value.
This explains the intuition behind the McKinsey story: it can be an equilibrium for experts
to report their private information truthfully when their likelihood of future work rises with
less profitable actions and falls with more profitable ones.
    In Section 4 we consider a more general model with heterogeneous experts who have
private information along two dimensions. In addition to observing each period’s hidden
state, experts privately observe their own relative payoffs from providing a minor versus
major treatment. Now customers will not know what probability would make an expert
indifferent across actions, and so we cannot enforce the truthful equilibria above.
    Customers are short-lived and so long-term contracts are impossible. If experts could
commit to long-term contracts over their actions, however, and if they did not discount
future payoffs, then a quota system could allow for truthfulness in some periods. Say that
contracts were written over two-period blocks, and an expert was required to play one major
and one minor treatment in each block. Whatever she played in the first period, she would
play the opposite in the second period and get one major payoff and one minor payoff over


                                              2
the block. Regardless of her relative payoffs across actions, then, the expert would agree
to be truthful in the first period of any block. In a similar contract with blocks of length
K > 2, the expert would be truthful until one of the actions reached its quota and would
then play the other action deterministically to the end of the block. If the quota for action
a were set to be close to K times the probability of a being appropriate, then the share of
truthful actions would approach 100% as K grew large.
    Even if long-term contracts were possible, the discounting of payoffs would prevent us
from exploiting this idea directly. Under a quota like the one described above, experts would
shift all of the profitable major treatments into the early periods. One way to return experts
to truthful play in early periods would be to allow their payoffs to depend slightly on the
state of the world, and in particular to suppose that experts receive a utility benefit from
taking the appropriate action in a period. Consider an expert who observes that a minor
treatment is appropriate in an early period of a block. Playing the minor action over the
major one would impose a cost from delaying the major payoff, but would yield a benefit
from aligning the action with the state in this period. If she were sufficiently patient then
the cost would be small, so she would prefer to be truthful and would play the minor action.
    In Section 4 we show that the logic of a quota can be recovered to induce heterogenous
experts to act truthfully in almost every period, even in an environment in which there is no
commitment over time and experts have no preference for aligning actions with states. In a
standard quota, the number of plays on an action is constant over prospective equilibrium
paths; once one action hits its quota, the underplayed action must be played until the end of
the block. Instead of a literal quota where the number of times an action is played is constant
over prospective equilibrium paths, we use a “discounted quota” in which the number of
expected discounted plays is constant over paths. In each block, experts are truthful in early
periods, and in later periods they deterministically choose the underplayed action until a
new block begins.
    Here is a basic example to illustrate the idea of such a discounted quota (see Example
1). Strategies repeat every three periods. In the first period the expert acts truthfully,
performing a major or minor treatment. In the second and third periods any expert who is
chosen plays the opposite action of the first period, regardless of the state. After the first
(truthful) period, the new customer keeps the old expert with some probability q and moves
to a new one with probability 1 − q. After the second and third periods, the customer retains
the expert if the suggested action was played and otherwise goes to a new one.
    Say that experts have discount factor β in this example. Over the course of the three


                                              3
periods, an expert chosen in the first period gets an expected discounted weight of 1 play
towards whichever action is taken first, and a weight of q(β +β 2 ) towards the opposite action.
                                   1
If the retention probability q is β+β 2 then the weight is 1 on both actions. So over the three

periods an expert gets one major payoff and one minor payoff in expectation along either
path, and is willing to condition her first period action on the state of the world.
    The customer facing a truthful expert is happy, and has no incentive to deviate. But
customers facing deterministic experts are stuck – whenever a customer switches experts,
the new expert plays exactly like the old one would have.
    The example allows for truthfulness in every third period. Taking the experts’ discount
factor to 1 and enforcing the discounted quotas over longer and longer blocks, we can con-
struct equilibria with an arbitrarily high share of truthful periods.
    In this paper, we examine whether experts can be induced to act truthfully under very
strict assumptions: experts have no intrinsic motivation to aid customers, and customers
never learn whether past experts had been truthful. Even after stacking the deck in this
manner, we find that truthful play is still possible so long as records are available. In
more realistic settings experts may be somewhat altruistic, or customers (one-time or repeat
visitors) may observe signals about the quality of past play. In either of these cases, our
results provide a lower bound on what is achievable; the equilibria we construct continue to
hold.


2    Literature Review
Darby and Karni (1973) introduced the concept of credence goods, goods whose value is
known by a seller but never fully revealed to the consumer. Dulleck and Kerschbamer (2006)
provide a recent review of the literature on when and how credence goods can be provided
efficiently. In the credence goods literature, most work focusses on inducing truthfulness in
one-shot settings in which expert payoff levels are common knowledge.
    In the terminology of Dulleck and Kerschbamer (2006), we impose the Verifiability rather
than Liability assumption: customers can confirm that the announced treatment has been
performed, but the success or failure of the treatment is not publicly observed and is non-
contractible. When the Liability assumption holds instead, experts will correctly treat the
problems but may attempt to overcharge customers, performing a cheap treatment but re-
porting an expensive one.
    We also impose what that paper calls the Commitment assumption, that a customer who


                                               4
goes to an expert must be treated by that expert. When this is relaxed, truthfulness can
be induced by having one expert diagnose the problem and another perform the treatment.
This is explored in Wolinsky (1993), Dulleck and Kerschbamer (2006, 2008), and Alger and
Salanie (2006). In these models, agents may incur inefficient search and diagnosis costs. An
alternative way of relaxing the Commitment assumption is to prevent the customer from
seeking other experts, but allowing him to refuse treatment after observing a diagnosis.
Pitchik and Schotter (1987) take this approach and find a mixed strategy equilibrium in
which customers sometime reject expensive treatments and experts are sometimes truthful.
    If the prices for treatments are set so that profits are equal across all actions, then experts
will play truthfully. Emons (1997) and Dulleck and Kerschbamer (2006) build models of
credence goods which exploit this solution. In this sense, when the parties can bargain over
prices they may achieve truthful play without resorting to a repeated game. But such an
approach only works when, as in our model of homogeneous experts in Section 3, experts
have no private information about their costs for each action. If costs are privately observed,
as in Section 4, then customers have no way to know what prices would induce truthful play,
and experts will have no incentive to report their costs honestly. In the Web Appendix,
we elaborate on the impossibility of endogenous prices to induce truthful play in a one-shot
setting.
    There is a large body of work on repeated games outside of the context of credence goods.
It is common for players to have private information on their own payoffs, as experts do in
Section 4. Our model diverges from a standard set-up in that some players – the customers
– do not know their own payoffs over others’ actions.
    Bar-Isaac and Tadelis (2008) provide a recent survey of results on repeated games with
“reputations” in which some players have hidden types and other players have beliefs about
these types. The uninformed agents observe past actions (or signals thereof) to generate
inferences about others’ types, and therefore about the future actions they may play. When
we consider the case where experts have private information on their own payoffs in Section 4,
we will construct equilibria where reputational dynamics are trivial: all experts play identical
strategies, and customers make no inferences about types or strategies based on histories.
    Bar-Isaac and Tadelis (2008) also discuss the problem of firms’ trying to develop a rep-
utation for having expertise, which – although orthogonal to our analysis – is more in the
spirit of credence goods. To demonstrate expertise, firms may may have incentives to skew
their reports to match or go against what customers expect to hear.
    There is a small literature involving repeated markets for credence goods.


                                                5
    Fudenberg and Levine (1994) present a number of general results about payoff frontiers
in repeated games with long-run and short-run players who do not have private payoff types.
Their Investment Game example shares a number of features with our model of homoge-
neous experts in Section 3. As in our paper, short-lived players offer business to long-lived
players who may secretly take advantage of them. In equilibrium, the long-run players’
temptation to cheat can largely be overcome by the threat that future short-run players will
withdraw their business after suspect outcomes. Our model allows us to explicitly construct
efficient equilibria; Fudenberg and Levine (1994) focus on conditions under which there exist
equilibria approaching efficiency.
    Schneider (2007) studies repeated interactions in the market for car repair, an example
of a credence good. He considers a 2-period model with multiple experts, and runs a field
experiment to test the predictions. As in our paper, Schneider (2007) takes prices to be fixed
exogenously and shows that there is an equilibrium in which customers return to an expert
with lower probability after an expensive repair. In this equilibrium, profit-maximizing me-
chanics are honest in the first period and do unnecessary major repairs in the second period.
Our equilibrium in Section 3 demonstrates a similar intuition for inducing truthfulness in
every period in a fully repeated setting.
    Wolinsky (1993) also considers a setting where customers return to a market twice and
the choice of expert in the second period depends on the expert’s first period action. In
this model experts may reject customers with expensive problems, and customers return to
experts who had been willing to treat them in the past.
    Park (2005) studies an infinitely repeated game in which there are finitely many expert
mechanics, and a diagnosis reveals to the mechanics which of them can best perform the
repair. After a diagnosis, mechanics report their diagnoses (possibly falsely) and the customer
chooses an expert based on the reports. Experts have no opportunities to lie about the actual
repair they perform; once an expert is chosen, she fixes the car and receives a deterministic
payoff. Park (2005) finds that equilibria with many or even 100% truthful reports are possible
when payoffs do not vary too much across periods. One crucial feature of the model is that
customers learn the true state at the end of a period, which lets them punish liars. This
means that experts do not have to be made precisely indifferent over reports in order to be
truthful, and so the equilibria are robust against some uncertainty in the experts’ payoffs.
    Ely and Valimaki (2003) study a model where short-lived customers play a repeated
game with long-lived mechanics who privately observe the state and determine the proper
repair. In their model altruistic mechanics strictly prefer to act truthfully and perform


                                              6
repairs which are appropriate to the state, while bad mechanics always want to do engine
repairs. But instead of being truthful at first, the good mechanics will do a tune-up in
order to separate themselves from bad mechanics and prove their goodness to all future
customers. No consumer wants to be the first to go to an expert, so the market breaks
down. Ely, Fudenberg, and Levine (2008) extend this work and find sufficient conditions for
when observable histories lead to market collapse.
    While we set up the problem similarly to Ely and Valimaki (2003), two key differences
make our strategic environment vastly different. First, there is no altruism in our model
– the utility an expert receives from each repair is independent of the state of the world.
Second, in their model customers will exercise an outside option rather than receive a repair
that is independent of the state; we do not allow customers to opt out of the market. This
prevents the market from breaking down.
    Ely and Valimaki (2003) and Schneider (2007) consider the possibility that some propor-
tion of experts are altruistic. One of the key innovations of our paper is to treat a different
form of unobserved heterogeneity, in which experts are profit maximizers but are privately
informed about their payoffs across actions. Park (2005) does allow for private information
on instantaneous profits, but in that paper customers can discover and punish deviations.
The punishments give experts a strict incentive to prefer truthful reports over a range of
profit levels.
    Finally, there is another set of related papers which bears mentioning. Recall that in our
model an expert can never be given a strict incentive to act truthfully. We can achieve truth-
ful play via indifference by enforcing “discounted quotas” which fix the number of expected
discounted times that each action can be played by an expert. Past work has explored the
use of standard quotas, which fix the absolute number of times that an action is played, to
induce truthful revelation of private information. Townsend (1982) shows how quotas can
be applied to the context of repeated bilateral trade, and Jackson and Sonnenschein (2007)
extends this to a general environment with independent and ex ante identical allocation de-
cisions. Agents are asked to report types jointly over many decisions, and the distribution of
reported types is restricted to match the theoretical distribution. Jackson and Sonnenschein
(2007) call this the “linking” of separate decisions through “budgets” or “rations.” As more
decisions are linked, the mechanisms approach efficiency.
    As mentioned in the introduction, when agents discount future payoffs a standard quota
will only work if agents receive some benefit from telling the truth. In these papers there
is such a benefit because the agents’ private information regards their own preferences, and


                                              7
conditional on their reported types efficient outcomes are realized. If a trade is more likely
when a buyer reports a high value, and if a buyer can only report that he has a high value
a limited number of times, he prefers to report this when his value truly is high. In both
papers, the mechanisms fall apart if agents cannot commit in advance to participate over
long time horizons.
    Although less directly related to our own work, Pesendorfer (2000) employs a similar
intuition. He studies a bidding cartel for procurement contracts in which firms link separate
auctions by reporting to each other a desirability ranking of the available contracts, and
determine bidders from the cartel based on these rankings. As the number of linked auctions
increases, the mechanism gives rise to nearly optimal bids. A quota is analogous to a ranking
when information is binary rather than continuous.


3       Homogeneous Experts
There is a set of experts E = {e1 , e2 , ...} and a set of customers C = {c1 , c2 , ...}. Customers
are short-term players, while experts are infinitely long-lived and have discount factor β ∈
(0, 1).1
     In period t ∈ {1, 2, ...}, customer ct arrives on the market and observes the past history
of experts chosen and actions taken. The customer then chooses a single expert et from E.
(Superscripts denote elements of the set E, while subscripts represent time periods). The
expert observes the state θt and then chooses an action at .
     In each period the set of possible states is Θ = {θm , θM }, and the set of actions for the
chosen expert is A = {m, M }, where m refers to a “minor” treatment and M a “major”
one. The customers always want the expert to be “truthful” and choose a when the state is
θa . But in the short term, every expert prefers action M . Formally, write stage payoffs in
period t as

                        Customer ct : 
                                      U t (at |θt )
                                      
                                      
                                       0           if et 6= ei
                                      
                          Expert ei :   R(m) = r if et = ei & at = m
                                      
                                      
                                      R(M ) = 1 if e = ei & a = M
                                      
                                                        t       t

    1
    The set of experts is modeled as countably finite, but our arguments will not hinge on this assumption;
we discuss the case of finitely many experts in the Web Appendix.




                                                    8
where these stage payoffs satisfy

                                 U t (a|θa ) > U t (a0 |θa ) for a0 6= a
                                 0 < r < 1.

For each expert only the relative payoffs of the different actions matter, so we have normalized
R(M ) to 1 and the payoff when not selected to 0. R(m) = r is in between these two. Expert
ei ’s lifetime utility is {t|et =ei } β t−1 R(at ).
                         P

     A customer only receives a payoff in the period in which she chooses an expert, and this
payoff is a function of the treatment received along with the current state of the world. The
payoff does not depend on the identity of the expert.
     Although we do not analyze efficiency explicitly, efficiency is synonymous with truthful-
ness if the benefit to customers from an appropriate treatment always outweighs the costs
to the experts.
     In each period, the state is θm with probability 0 < p < 1 and θM with probability 1 − p.
     The identities of experts chosen in past periods and the actions taken by these experts
are publicly observable to all players. We write this list of experts and actions observed prior
to period t as a “public history” Ht = (e1 , a1 , ...et−1 , at−1 ), with H1 ≡ (∅). Let Ht be the set
of all possible public histories at time t, and let H be the set of possible public histories at
any time: H ≡ ∪t Ht .
     Let ⊕ be the concatenation operator, so that for histories H and H 0 , the notation H ⊕ H 0
means history H followed by history H 0 . Say that a history G begins with history H if
G = H ⊕ H 0 for some H 0 .
     Customers observe the list of past experts and actions, but they have no way of discerning
whether past actions were appropriate. A customer’s only decision is to use the observable
public history to choose an expert to treat his problem. The customer may play a mixed
strategy and choose experts probabilistically. We write customer ct ’s strategy in period t as

                                          ρt : Ht → ∆(E)

where, for any countable set S, ∆(S) denotes the space of probability distributions over S.
In order to avoid awkward descriptions of pure strategies, we will slightly abuse notation
and use s to denote the element of ∆(S) which places probability 1 on s ∈ S. Define the
collective strategies of all customers as ρ ≡ (ρ1 , ρ2 , ...).
    Each expert also sees the public history, and once chosen, she also observes the current


                                                   9
state. The expert then chooses a treatment based on all of this information. We write expert
ei ’s strategy conditional on being chosen as

                                           σ i : H × Θ → ∆(A).

More generally, an expert could also condition her strategy on privately observed values of
the state in previous periods in which she was chosen. Past states are payoff irrelevant to all
players, so allowing this would complicate notation without affecting our results.2
   In this model, the expert’s utility is independent of the true state, and the customers
cannot confirm whether an expert has acted honestly or dishonestly in the past. Moreover,
each customer is a short term player who is unable to reward or punish an expert after
choosing her.
   There exists an inefficient equilibrium in which every expert always performs the more
profitable major treatment M . We will show that when experts are patient enough a truthful
equilibrium will also exist, in which experts always play the action corresponding to the
true state. Here and in the rest of the paper, the term equilibrium refers to a sequential
equilibrium.

Definition. The expert ei with strategy σ i is said to be truthful at history Ht if σ i (Ht , θa ) = a
for each a ∈ A. A truthful equilibrium is an equilibrium in which, at every equilibrium history,
every expert who may be chosen with positive probability is truthful.

Proposition 1. A truthful equilibrium exists if and only if β ≥ 1 − r.

    Before proving the proposition, it is useful to state a lemma. Recall that an expert’s
payoffs are independent of the state, so an expert is only willing to play truthfully if she is
indifferent across possible actions. The following lemma states that if truthful play on some
set of histories is a best response, then the strategy remains a best response if the expert
switches to arbirtrary play at those histories.

Lemma 1. Fix the customers’ strategy ρ and the strategies σ −i of experts aside from ei , and
suppose that expert ei has a sequentially rational best response σ i . Take Ĥ ⊆ H to be a set of
histories for which σ i is truthful. Now construct a new strategy σ̂ i , where σ̂ i : H × θ → ∆(A)
   2
    Any strategy that is optimal in the class of those which do not depend on past states is also optimal in the
larger class of strategies which do. So the equilibria we construct will remain equilibria in the more general
strategy space. The results that no truthful equilibria can exist under various conditions (see Proposition 1
and Remark 1) also hold in the general strategy space.



                                                      10
is identical to σ i on all H\ Ĥ, and is arbitrary on Ĥ. The strategy σ̂ i gives the same expected
utility as σ i starting at every history and is also a sequentially rational best response.

Proof. See Appendix.

Proof of Proposition 1. First, we will show the “If” direction. Suppose that β ≥ 1 − r; we
will now construct a truthful equilibrium.
     Experts play truthfully: for each a ∈ A, for each ei ∈ E, and for each Ht ∈ H, let
σ i (Ht , θa ) = a.
     The customer’s strategy is the following. c1 chooses ρ1 (H1 ) = e1 . For t > 1, ct chooses
                                                  
                                                ei with Prob q(at−1 )
                ρt Ht−1 ⊕ (et−1   = ei , at−1 ) =
                                                  ei+1 with Prob 1 − q(a
                                                                           t−1 )


with q(m) = 1, q(M ) = r+β−1
                          rβ
                              . Notice that 0 ≤ q(M ) < q(m) = 1.
   Because all experts play truthfully at every history, customers are indifferent across
experts and any strategy is a best response.
   To check that truthful play is a best response for the experts, consider the expected
payoff V for an expert who follows the strategy, conditional on being chosen in a given
period but unconditional on the realization of θ. By the one-shot deviation principle, the
expert’s strategy is optimal if she always plays a maximizer of R(a) + βq(a)V , and if V =
maxa∈A {R(a) + βq(a)V }. This holds if we can find a V such that

                                   r+β−1
                           V =1+β           V = r + βV
                                       rβ
                                         
                                 r+β−1
                        ⇐⇒ V 1 −            = 1 & V (1 − β) = r
                                     r
                                r
                        ⇐⇒ V =     .
                               1−β
                                   r
So the continuation value V is 1−β   , and there are no profitable deviations.
    This shows that the above strategies are an equilibrium when β ≥ 1 − r – the experts
and customers are indifferent with respect to all actions at all histories.
    Now, in the “Only If” direction, suppose that a truthful equilibrium exists. By Lemma
1, if a given expert is willing to play truthfully at every period in which she is chosen then
she must be indifferent to switching to the strategy of playing m at every period, or to
the strategy of playing M in every period. An expert selected in the current period whose


                                                11
strategy is to always play m will get stage payoffs of at most r in each period (0 in any
                                                                                           r
period in which she is not chosen) and so her present value of future payoffs is at most 1−β .
An expert who plans to play M in every period gets a stage payoff of 1 today, and some
nonnegative payoff in the future. The expert can only be indifferent over these two strategies
    r
if 1−β ≥ 1, or rather β ≥ 1 − r.

    To implement this equilibrium customers only need to observe the previous period’s
action, and experts can ignore the histories entirely. The customer utility functions never
appear in the construction. Moreover, the equilibrium is completely independent of p, the
parameter controlling the probabilities of the various states of the world. The expert’s
continuation probabilities from different actions are set such that every expert is exactly
indifferent between the higher payoff today versus the lower payoff in the future from playing
M , and p plays no role in this.
    In the above equilibrium, customers return to an expert with probability 1 if the minor
action m was played last period, and probability less than 1 if the major action M was
played. In the Web Appendix we show that similar truthful equilibria can be constructed if
expert utility functions differ but are observed by customers, or if experts have more than
two possible actions.


4    Heterogeneous Experts
In Section 3 customers always return to an expert who has just performed a minor treatment,
and rehire an expert who has just performed a major treatment with a probability less than
1. This probability is chosen so that each expert will be exactly indifferent between a major
and minor treatment in every period. But this equilibrium falls apart if customers are no
longer certain about an expert’s relative payoff from the two treatments. An expert with
a slightly lower payoff from the major treatment will find it profitable to play the minor
treatment in each period she is chosen, and vice versa.
    We will proceed to consider whether any truthful play is possible when an expert’s payoffs
are private information. We find that experts can often be incentivized to play truthfully
when strategies depend on more than just the last period’s play.




                                             12
   In this section, we take expert stage payoffs to be
                                     
                                     
                                     
                                      0                         if et 6= ei
                                     
                          Expert ei : Ri (m) = ri                if et = ei & at = m .
                                     
                                     
                                     Ri (M ) = 1
                                     
                                                                 if et = ei & at = M

    We maintain the normalization of the instantaneous payoff of the major treatment to 1,
and we now let the relative payoff of the minor to major treatment be an expert’s private
information. Each expert ei realizes a relative payoff ri drawn from a distribution over
R+ , the set of nonnegative real numbers; some (or all) experts may prefer m to M . These
distributions need not be identical or independent. The realization of ri is privately observed
by expert ei at the start of the game, and is fixed over time.
    Experts have discount factor β as before, and β is common knowledge.
    Customer payoffs are as in Section 3. Customers prefer to receive appropriate treatments,
but conditional on the action and state have no preferences over the identity of the expert.
Each customer still observes only the public history of past experts and actions when choosing
a new expert, so customer strategies are also as above; the collective strategy of the customers
is ρ : H → ∆(E).
    We now have to generalize the strategy space of each expert to depend not only on the
history and state but also her realized type ri . In this context, an expert ei ’s strategy is a
function φi : H × Θ × R+ → ∆(A). For ease of notation, we will use the term conditional
strategy to refer to maps σ : H × Θ → ∆(A). An expert’s strategy can be thought of as a
map from types to conditional strategies. The conditional strategy for an expert ei of type
r ∈ R+ will be denoted by σri (H i , θ) ≡ φi (Ht , θ, r).
    At any history Ht with state θt at which the expert et = ei is chosen, a conditional
strategy σ for the expert induces weights on the number of expected discounted times that
m and M will be played in the current and future periods. For a ∈ A, we can define the
weight on action a by
                                       ∞
                                       X
       Wai (Ht , σ|θt , φ−i , ρ)   ≡          β τ −t Prob[eτ = ei and aτ = a|θt , σ, Ht , φ−i , ρ; et = ei ]
                                       τ =t


where the probability is taken with respect to the (Bayesian) beliefs of ei . The expected
present value of conditional strategy σ at some history Ht (conditional on φ−i , ρ, θt , and
conditional on et = ei , but suppressing these from the notation) in the current and future


                                                            13
periods is

                                                        i
                                   ri · Wmi (Ht , σ) + WM (Ht , σ).

Sequential rationality requires that σri i be a maximizer of this expression for each ri ∈ R+ .
Notice that this value depends on θτ in periods where eτ = ei only through the effect of θτ
on σ.
   It can be natural to think of an expert ei as choosing a bundle (Wmi , WM i
                                                                               ) at each action
node rather than a strategy. Let the set of available bundles be denoted

                                               i
                  W i (Ht ) ≡ {(Wmi (Ht , σ), WM (Ht , σ))|σ : H × Θ → ∆(A)},

suppressing the dependence on φ−i and ρ. W i (Ht ) is independent of θt because no matter
what the state is, a strategy exists that plays “as if” the state were the opposite.
    Points in W i (Ht ) fully determine an expert’s utility going forward, so we can consider
indirect preferences over this set. Expert ei ’s indifference curves over W i (Ht ) are straight
lines with slope −ri . The equilibrium in Proposition 1 holds up because the customers
construct W i (Ht ) such that all points lie on a straight line with slope −r; every expert is
indifferent between every action at every history.
    No experts with different values of ri can be indifferent over distinct pairs of (Wmi , WMi
                                                                                                )
because the indifference curves have a unique intersection point. However, if two distinct
strategies yield the same pair of weights, then experts of any ri ∈ R+ will be indifferent. We
formalize this in the following lemma.
Lemma 2. Take two conditional strategies σ 0 , σ 00 : H × Θ → ∆(A), and take r0 6= r00 ∈ R+ .
An expert ei chosen at history Ht is indifferent between σ 0 and σ 00 for both possible types r0
and r00 if and only if

                  Wmi (Ht , σ 0 ) = Wmi (Ht , σ 00 ) & WM
                                                        i
                                                          (Ht , σ 0 ) = WM
                                                                         i
                                                                           (Ht , σ 00 ).

Proof. See Appendix.

    Lemma 1 of Section 3 continues to hold: an expert whose best response includes truthful
actions is indifferent to switching her strategy arbitrarily at truthful histories.
Remark 1. If each expert can realize at least two possible types then no truthful equilibrium
exists. More formally, there is no truthful equilibrium if for no expert ei does there exist
r ∈ R+ such that ri = r with probability 1.

                                                   14
Proof. Suppose there is a truthful equilibrium. Take ei to be some expert who is selected
with positive probability at H1 . In a truthful equilibrium, there is a probability one that ri
is such that σri i is truthful at every equilibrium history in which ei is chosen with positive
probability. Take some such ri . (An equilibrium history is an element of H which occurs
with positive probability in equilibrium).
    For each a ∈ A, consider the conditional strategy σa of playing action a at every history.
The payoff to the expert of playing σa is exactly the same as the payoff of a strategy which
plays a at all equilibrium histories in which she is selected with positive probability, and
mimics σri i at other histories. And by Lemma 1, such a strategy is optimal; the expert is
truthful at all equilibrium histories in which she may be chosen, and so is indifferent to
modifying her strategy arbitrarily at these periods. So the expert ei of type ri is indifferent
between σm and σM at H1 .
    But the two conditional strategies σm and σM induce different points in W i (Ht ): if expert
ei is chosen at H1 , then WM  i                       i
                                (H1 , σM ) ≥ 1 while WM (H1 , σm ) = 0. So by Lemma 2, there is
                                           i
at most a single type in R+ for which e would be indifferent between σm and σM at H1 . This
type must be realized almost surely in a truthful equilibrium, and so there is no truthful
equilibrium if no single type is realized with probability 1.

   While an equilibrium with truthful play at every history cannot exist, we can still find
equilibria in which experts play truthfully at some histories. Here is an example with truthful
play once every third period.

Example 1. We will show that for any β & .80, the following strategy profile constitutes
an equilibrium in which experts are truthful at periods t = 1, 4, 7, 10, and so on.
   First we define T0 be the set of time periods of the form 3n + 1, and t0 (t) ∈ T0 to be the
most recent period in T0 up to period t:

                               T0 ≡ {τ |τ = 3n + 1 for some n ≥ 0}
                            t0 (t) ≡ max{τ ∈ T0 |τ ≤ t}.

So t0 (t) is 1 for t = 1, 2, 3, and t0 (t) = 4 for t = 4, 5, 6. It will be convenient to use t0 for
both the function t0 (t) and as a representative element of the set T0 .
    The experts’ strategies are as follows. All experts of all types play identical conditional
strategies. They will be truthful at the T0 periods, experts will play the action opposite of
that played at t0 ∈ T0 in the periods t0 + 1 and t0 + 2. Strategies repeat every three periods.


                                                15
So for all ei ∈ E, for all r ∈ R+ , and for each a ∈ A,
                                          
                                          
                                          
                                           a      if t0 (t) = t
                                          
                          σri (Ht , θa ) = M       if t0 (t) > t & at0 (t) = m .
                                          
                                          
                                          
                                          m       if t0 (t) > t & at0 (t) = M

See Figure 1 for an illustration of the experts’ strategies.
    We move now to the customers’ strategies. After each period, the customer either returns
to the previous expert or “fires” her and moves to an entirely new one. After the first period
in each repeating block, the truthful period t0 , the customer arriving at t0 + 1 has some fixed
positive probability of firing the old expert. After the two deterministic periods, the next
customer returns to the expert if she played the suggested deterministic action and fires her
otherwise. Formally, at period 1 c1 chooses ρ1 (H1 ) = e1 . At period t > 1, if the previous
expert chosen was ei (so that Ht = Ht−1 ⊕ (ei , at−1 )), the customer chooses
                                           
                                           ei      with Prob q(Ht )
                              ρt (Ht ) =
                                           ei+1    with Prob 1 − q(Ht )

where the function q : H \ H1 → [0, 1] determines the probability of continuing with an
expert rather than moving to the next one. q(Ht ) satisfies
                               
                                  1
                                           if t0 (t) = t − 1
                                β+β 2
                               
                               
                       q(Ht ) = 1           if t0 (t) 6= t − 1 & at0 (t−1) 6= at−1 .
                               
                               
                                            if t0 (t) 6= t − 1 & at0 (t−1) = at−1
                               
                               0

                                                                                            √
                                                                   1                         5−1 ∼
     Such a strategy profile can be constructed so long as β+β         2 ≤ 1, that is, β ≥   2   = .62.
The customers are necessarily best responding because at every history, each expert plays
identically to every other expert. So the customer is indifferent over who he chooses. We will
show that the experts’ strategies also constitute a best response when their discount factor
is large enough, and therefore that this strategy profile is indeed an equilibrium.
                                     i
     Consider the contribution to WM   (Ht0 , σri ) and Wmi (Ht0 , σri ) from the three periods t0 , t0 +
1, t0 + 2, for t0 ∈ T0 . If θt0 = m then the expert is instructed to play m in t0 , and M in
t0 + 1 and t0 + 2 if chosen. This gives a weight of 1 towards the minor action m and a
              1         2
weight of β+β   2 (β + β ) = 1 towards the major action M over the block. The same holds




                                                    16
                                                        t0
                                         m                           M




                              Figure 1: Expert Strategies in Example 1.

This picture illustrates the equilibrium paths of play in a single 3-period block of the strategies in Example
1. At the end of the block, the strategy repeats. The open circle represents a truthful period; the closed
circles represent deterministic periods. The style of the lines is varied in order to show which histories lead
to which actions at deterministic periods.


if θt0 = M and the expert is instructed to play M, m, m; the expert gets one minor payoff
and one major payoff in expected discounted terms over the three periods. She is therefore
indifferent between m and M at the t0 periods and is willing to follow the suggested strategy
of truthful play.
    We still have to show that there are no profitable deviations at the t0 + 1 or t0 + 2 stages.
At a period with t equal to t0 + 1 or t0 + 2, suppose that a is the suggested action and the
selected expert considers a deviation to a 6= a. Deviating gives a weight of 1 towards a and
0 towards a. So a sufficient condition for deviations to be unprofitable is that the weight on
a from following the equilibrium is at least 1.
    An expert chosen at t0 receives a weight of 1 towards both m and M over the three periods
of the block. Iterating this out, the weight over all periods from following the strategy is
                                 1+β
Wai (Ht0 , σri ) = 1− 11 β 3 = 1+β−β 2 for a = m, M . So following the equilibrium starting at
                      β+β 2
                              1+β                                                   1+β
t0 +1 gives a weight of β 2 1+β−β 2 towards a; starting at t0 +2, the weight is β 1+β−β 2 . Deviating
                                  1+β
is unprofitable as long as β 2 1+β−β                     3      2
                                      2 ≥ 1, or rather β + 2β − β − 1 ≥ 0.
                                                                                3


   3
     This is a sufficient but not necessary condition. We may have an equilibrium for values of β which don’t
satisfy this, if the support of possible
                                      √
                                         values of ri is limited to some subset of R+ bounded away from 0 and
infinity. In particular, for any β ≥ 5−12   there are no profitable deviations if all values of ri are known to be
                          1
in the interval [1 − β, 1−β ].


                                                       17
    This gives us a condition under which the proposed strategy will be a best response at all
histories for all experts, and we already determined that the customers are best responding.
The proposed strategy is an equilibrium as long as β 3 + 2β 2 − β − 1 ≥ 0, which holds for
β & .80.                                                                                    

    In the above example, we have blocks of length three in which there is a truthful period
followed by two deterministic periods in each block. The weight on each action is constant
across all prospective equilibrium action paths. At truthful periods different equilibrium
paths allow for different actions in the current period, and so the expert is willing to condition
her choice of path on the payoff-irrelevant state of the world. At deterministic periods, the
continuation payoff from following the suggested strategy and receiving future work is greater
than the benefit from deviating and never being chosen again.
    All action paths consistent with equilibrium play give an expert the same weights towards
each action; for each action, she faces a “quota” or a “budget” on the number of expected
discounted plays. When the discount factor is large enough any off-equilibrium strategy
provides a weakly lower weight on both actions, and hence has a weakly lower payoff for an
expert of any type.
    As the discount factor increases, we can find similar equilibria in which truthful periods
occur more frequently. We take strategies that repeat in blocks of longer than three periods,
and have experts play truthfully until some number of either m or M actions are played
within the current block. Once one of these actions reaches enough plays, the opposite
action is played until the end of the block. Taking the length of blocks to be large and
taking the discount factor to 1, we can get the long-term proportion of truthful periods to
approach 1.

Proposition 2. Take  > 0. For β large enough, there is an equilibrium in which the
long-run proportion of truthful periods is greater than 1 −  with probability 1.

Proof. We will use the notation bxc to denote the “floor” of a number x, the greatest integer
less than or equal to x. First, a technical lemma:
Lemma 3. For any k ∈ N, there exists K ≥ k such that

   i. 1 < pK < K − 1,

  ii. pK is not an integer,

  iii. 2p − 1 + ζ < pK − bpKc < 2p − ζ for ζ ≡ min{p/2, (1 − p)/2} > 0, and

                                               18
            n                         o                  n               o
                (1−p)bpKc pb(1−p)Kc                           p
  iv. min                ,
                pb(1−p)Kc (1−p)bpKc
                                          > 1 − 14 min       1−p
                                                                 , 1−p
                                                                    p
                                                                          .

Proof. See Appendix.

    Take some K satisfying conditions (i)-(iv) of the above lemma; we will construct a strat-
egy profile for which strategies repeat every K periods. Conditions (i) and (ii) are necessary
for constructing the strategies of the experts. Conditions (iii) and (iv) will guarantee that
the proposed probabilities chosen by the customers are valid for a large enough discount
factor, and also that the experts’ responses are optimal. We will show that the strategy
is well-defined and is an equilibrium for β large enough, and that as K is taken to ∞ the
long-term proportion of truthful periods will converge to 1. Because we can find K arbitrar-
ily large that satisfies the above conditions, this means that we can find equilibria in which
truthful periods occur arbitrarily often.
    Strategies will be defined on blocks of K periods, and will reset at periods of the form
nK + 1. T0 will denote the set of these periods at which new blocks begin, and t0 (t) will be
the most recent period in T0 starting at period t:

                                 T0 ≡ {τ |τ = nK + 1 for some n ≥ 0}
                               t0 (t) ≡ max{τ ∈ T0 |τ ≤ t}.

The term t0 will express this function as well as a representative element of the set T0 .
    Each block begins with a segment of truthful periods. Once m or M is played a certain
number of times within the block, the players move into a segment where experts take
deterministic actions. After K periods in the block, t0 (t) increments up by K and strategies
repeat. We now construct these strategies.
    Partition H into “deterministic histories” H(D) and “truthful histories” H(T ). A history
is truthful if m has been played less than bpKc times in the current block and M has been
played less than b(1 − p)Kc times in the current block. Once either action has been played
this many times, histories are deterministic for the rest of the block:
                    
                    
                    
                     H(T )    if #{τ |t0 (t) ≤ τ ≤ t − 1, aτ = m} < bpKc
                    
            Ht ∈             and #{τ |t0 (t) ≤ τ ≤ t − 1, aτ = M } < b(1 − p)Kc .
                    
                    
                    H(D) otherwise
                    


    For Ht ∈ H(D), let X(Ht ) be the first deterministic period in the block containing period
t, and let N a be the number of times that action a had been played over the truthful periods

                                                         19
in the block:

                           X(Ht ) ≡ max{τ ≤ t|Hτ −1 ∈ H(T )}
                          N a (Ht ) ≡ #{τ |t0 (t) ≤ τ < X(Ht ), aτ = a}.

    On H(D), if N M = b(1 − p)Kc then let a(Ht ) = m and a(Ht ) = M ; otherwise, if
N m = bpKc, then let a(Ht ) = M and a(Ht ) = m. Either N M = b(1 − p)Kc or N m = bpKc
at a deterministic period, because periods only become deterministic once one of these holds.
In words, a is the action that has been played “enough” over the truthful periods while a is
the action which “needs more plays”.
    Now, let all experts of all types share the following conditional strategy. At any his-
tory in H(T ) the expert plays truthfully, and at any history in H(D) the expert plays the
underplayed action a(Ht ): for all ei , in state is θa ,
                                                 
                                                 a        if Ht ∈ H(T )
                              σri (Ht , θa ) =
                                                 a(H ) if H ∈ H(D)
                                                     t      t


See Figure 2 for an illustration of the experts’ strategy.
   Now we construct the customers’ strategy ρ. At period 1, c1 chooses ρ1 (H1 ) = e1 . At
period t > 1, if the previous expert chosen was ei (so that Ht = Ht−1 ⊕ (ei , at−1 )), the
customer chooses                      
                                      ei     with Prob q(Ht )
                             t
                            ρ (Ht ) =
                                      ei+1 with Prob 1 − q(H )
                                                                       t

where the function q : H \ H1 → [0, 1] determines the probability of continuing with an
expert rather than firing her and moving to the next one. q(Ht ) satisfies
                      
                      
                      
                       q StartT (Ht )   if t = t0 (t) & at−1 = a(Ht−1 )
                      
                      
                                         if t = t0 (t) & at−1 6= a(Ht−1 )
                      
                      
                      
                       0
                      
                      
                                         if Ht ∈ H(T ) & t > t0 (t)
                      
                      1
             q(Ht ) =
                      
                      
                       q StartD (Ht )   if Ht ∈ H(D) & t = X(Ht )
                      
                      
                                         if Ht ∈ H(D) & t > X(Ht ) & at−1 = a(Ht )
                      
                      
                      
                       1
                      
                      
                                         if Ht ∈ H(D) & t > X(Ht ) & at−1 6= a(Ht )
                      
                      0


with q StartD and q StartT defined below. At t0 , the start of a new block – and therefore the end

                                                      20
                                                                 t0
                                                       m                 M




                                                                                            t0+ (1-p)K




  t0+ pK




               Figure 2: Expert Strategies in Proposition 2, for K = 10 and p = 2/3.
This picture illustrates the strategy of the experts in Proposition 2 in a single K-period block, for K = 10
and p = 2/3. At the end of the block, the strategy repeats. The open circles represent truthful periods; the
closed circles represent deterministic periods. The style of the lines is varied in order to show which histories
lead to which actions at deterministic periods.
On the equilibrium path, the customer chooses the continuation probability as q = 1 at every history except
for the first deterministic history in a block, at which q StartD is chosen; and the truthful history at t0 , at
which q StartT is chosen. These q’s depend on the full history of actions at all truthful periods in the most
recent block.

In this example, bpKc = 6 and b(1 − p)Kc = 3. The expected number of truthful periods in a block is about
6.60, so the long-term proportion of truthful actions is 66%.


of an old block – there is probability q StartT of keeping the previous expert if she played the
suggested action in the previous period, and probability 0 otherwise. At every other truthful
period, the customer returns to the previous expert with probability 1 no matter what.


                                                       21
At the start of the first deterministic period, customers again move to a new expert with
probability 1 − q StartD . For the remaining deterministic periods in the block, the customer
keeps the previous expert with probability 1 if the expert plays the suggested action a and
fires her otherwise.
    It will be useful to define a few other terms on the way to constructing q StartD and q StartT .
For a deterministic history Ht , let Z a (Ht ) be the weight that would accumulate towards
action a (relative to t0 (t)) for an expert chosen at t0 intending to play actions consistent
with Ht over the truthful periods in the block, t0 through X(Ht ) − 1. Let W a (Ht ) be the
weight that would accumulate over all periods in the block, from t0 through t0 + K − 1, for
an expert chosen at t0 intending to play actions consistent with Ht over the truthful periods
and the equilibrium action a(Ht ) for the rest of the block. That is, on Ht ∈ H(D),
                                 X
             Z a (Ht ) ≡                        β τ −t0 (t)
                              τ s.t. aτ =a
                           & t0 (t)≤τ <X(Ht )
                           
                           Z a (H )                                                             if a = a(Ht )
                                     t
             W a (Ht ) ≡                                          Pt0 (t)+K−1                                    .
                           Z a (H ) + q StartD (H )
                                     t                        t        τ =X(Ht )   β τ −t0 (t)   if a = a(Ht )

    Adjusting q StartD lets us adjust the weight W a that accumulates towards a within a single
block without affecting the weight W a that accumulates towards a. We want to choose q StartD
                                   Wm
so that the ratio of weights W       M is equal to the respective ratio of the probabilities of the
                                 p
actions being appropriate, 1−p      , across all equilibrium paths. For Ht ∈ H(D), let q StartD (Ht )
be defined by                             p Z M (H )−Z m (H )
                                                      t         t
                                          1−p
                                           Pt0 (t)+K−1 τ −t (t)
                                                        β    0
                                                                  if a(Ht ) = m
                         StartD               τ =X(Ht )
                       q        (Ht ) ≡ 1−p Z m (Ht )−Z M (Ht )                 .
                                          Pp t0 (t)+K−1 τ −t (t) if a(Ht ) = M
                                         
                                                    τ =X(Ht )
                                                             0     β

Rearranging, we see that q StartD has been chosen so that

                                pW M (Ht ) = (1 − p)W m (Ht ) if a = m
                                (1 − p)W m (Ht ) = pW M (Ht ) if a = M

                       W    m p
and so in either case, W M = 1−p as desired.

   Now, for Ht ∈ H(D), define Y a (Ht ) as

                                                             W a (Ht )
                                         Y a (Ht ) ≡
                                                        1 − β K q StartD (Ht )

                                                              22
and define Y a as
                                          Ya ≡      min Y a (Ht ).
                                                 Ht ∈H(D)

The minimum is well-defined because blocks are identical, and there are only finitely many
action paths along the truthful periods of a block. If two deterministic histories share the
same action path over the truthful periods in their respective blocks then the histories have
                                         p                        p
identical Y a values. Y m is equal to 1−p   Y M and so Y m = 1−p    Y M.
    Y a (Ht ) would be the lifetime weight on a, relative to t0 , that an expert chosen at t0 would
receive if she planned to repeat the actions played in the truthful periods of the current block
of Ht in the truthful periods of every future block and to play the suggested actions in all
deterministic periods, if the continuation probability across blocks were q StartD . But in fact
the continuation probability is not q StartD but q StartD · q StartT , because the expert may be
fired at both the first deterministic period in a block and also at the start of the next block.
Just as adjusting q StartD allowed us to manipulate the relative weights on m and M along a
path, adjusting q StartT will let us affect the level of the weights along a repeating path while
holding the relative weights fixed. We want to set q StartT (Ht ) so that the lifetime weight on
a is equal to Y a along any repeating path.
    For t0 ∈ T0 with t0 > 1, let

                                                 W m (Ht0 −1 )                W M (Ht0 −1 )
                                           1−        Ym
                                                                        1−        YM
                     q StartT (Ht0 ) ≡                            =                          .
                                         β K q StartD (Ht0 −1 )       β K q StartD (Ht0 −1 )

It holds that
                                                  W a (Ht0 −1 )
                              Ya =
                                     1 − β K q StartD (Ht0 −1 )q StartT (Ht0 )
where the right-hand side is the actual lifetime weight on a for an expert picked at t0 who
plans to repeat the path consistent with Ht . This is constant across all Ht . It will turn out
that if these levels are constant across repeating paths, they will also be constant across all
prospective equilibrium paths.
    This completes the descriptions of the strategies. Before we check whether these strategies
imply an equilibrium for high discount factors, we need to show that q StartD and q StartT are
valid probabilities for β large enough, i.e., that they are numbers in [0,1].

                                        p
   • limβ→1 q StartD (Ht ) ∈ ( 41 min{ 1−p , 1−p
                                              p
                                                 }, 1):




                                                      23
As β → 1,
                                         
                                         (X − t ) − b(1 − p)Kc if a = m
                                                0
                             Zm   → Nm =
                                         bpKc                     if a = M
                                         
                                         b(1 − p)Kc         if a = m
                            ZM       M
                                  →N =                                  .
                                         (X − t ) − bpKc if a = M
                                                            0


So
                        p                          p                                    1
                          Z M −Z m                    b(1−p)Kc−(X−t0 )+b(1−p)Kc            b(1−p)Kc−(X−t0 )
                 
                      1−p                 β→1     1−p                                  1−p
                  t0 (t)+K−1 β τ −t0 (t) −→
                  P                                         K−(X−t0 )
                                                                                   =        K−(X−t0 )
                                                                                                              if a = m
    StartD            τ =X(Ht )
q            =           1−p m
                             Z −Z M         β→1
                                                  1−p
                                                      bpKc−(X−t0 )+bpKc       1
                                                                                bpKc−(X−t0 )
                                                                                                                         .
                           p                       p                          p
                 
                    Pt0 (t)+K−1 τ −t (t)   −→         K−(X−t0 )
                                                                          =    K−(X−t0 )
                                                                                                              if a = M
                      τ =X(Ht )
                                β    0



Because pK is not an integer, in either case the numerator is strictly smaller than the
(positive) denominator; the limit of q StartD is strictly less than 1.
Now we wish to show that the minimum value of lim q StartD over all deterministic
                                   p
histories is greater than 14 min{ 1−p , 1−p
                                         p
                                            }.
The maximum value that (X − t0 ) can take is bpKc + b(1 − p)Kc − 1 = K − 2. And
for any fixed a, the above expression for lim q StartD is decreasing in (X − t0 ) as long as
(X − t0 ) < K. This implies that for any Ht ∈ H(D),
                                              (    1                               1
                                                                                                     )
                                                  1−p
                                                      b(1   − p)Kc − K + 2         p
                                                                                     bpKc   −K +2
             lim q StartD (Ht ) ≥ min                                          ,                         .
                                                                2                           2

Noting that b(1 − p)Kc = K − bpKc − 1, the first fraction can be reduced to

             b(1 − p)Kc − (1 − p)K + 2(1 − p)   pK − bpKc − 2p + 1       ζ
                                              =                    >
                         2(1 − p)                    2(1 − p)        2(1 − p)

and the second can be reduced to

                                              bpKc − pK + 2p   ζ
                                                             >
                                                    2p         2p

where the inequalities come from condition (iii) of Lemma 3, with ζ = min{p/2, (1 −
p)/2}.
                                                     ζ     ζ               p
Therefore lim q StartD (Ht ) is greater than min{ 2(1−p) , 2p } = 41 min{ 1−p , 1−p
                                                                                 p
                                                                                    }.

                                                        24
   • limβ→1 q StartT (Ht ) ∈ (0, 1]:
      For t = t0 − 1,

                                                                                                    W m (Ht )
                                            W m (Ht )                    m                 1−        W m (Ht )
                StartT
                                        1−    Ym                 1 − W Y (Hm
                                                                             t)
                                                                                                1−β K q StartD (Ht )
            q            (Ht+1 ) =     K  StartD
                                                           ≤                           =                               =1
                                      β q        (Ht )         β K q StartD (H    t)         β K q StartD (Ht )

      and so q StartT ≤ 1. Now we will show that the limit of q StartT (Ht+1 ) as β goes to 1 is
      strictly positive.

                                                                                            lim W m (Ht )
                                                                    W m (Ht )          1−
                                     StartT
                                                           1 − limlim Ym
                                                                                                Ŵ m
                                                                                             1−q̂ StartD
                             lim q            (Ht+1 ) =         StartD
                                                                                =
                                                          lim q        (Ht )           lim q StartD (Ht )

      where q̂ StartD = lim q StartD (Ĥ) and Ŵ m = lim W m (Ĥ) for some Ĥ ∈ H(D) with
      lim Y m (Ĥ) = lim Y m . This is positive if and only if

                                                     Ŵ m
                                                               > 1 − q̂ StartD .
                                                 lim W m (Ht )

                                        p
      We know that q̂ StartD > 14 min{ 1−p , 1−p
                                              p
                                                 } so it suffices to show that

                                          Ŵ m            1       p 1−p
                                                    > 1 −   min{    ,   }.                                                  (1)
                                      lim W m (Ht )       4      1−p p

      Notice that lim W m (Ht ) can be expressed as
                                                
                                                N m (H ) = bpKc                                 if a = M
                                                           t
                          lim W m (Ht ) =
                                                     p                   p
                                                
                                                    1−p
                                                        N M (Ht )   =   1−p
                                                                            b(1   − p)Kc if a = m

      and the same holds for Ŵ m . Therefore (1) follows from condition (iv) of Lemma 3.

    So for β sufficiently large, we have defined a valid strategy profile. It remains to be shown
that this strategy profile is an equilibrium when β is close to 1, and that the proportion of
truthfulness in this equilibrium goes to 1 as K increases.
    Because all experts act identically at every history, on or off the equilibrium path, any
customer strategy will be a best response. So to show that the strategy is an equilibrium,
it will suffice to show that the experts play best responses at every history when β is large
enough.

                                                               25
   By the following lemma, to show that the strategy is a best response for experts at
truthful periods, we only need to look at how deviations at later periods would affect the
weights relative to t0 . If an expert chosen at t0 would get the same set of t0 -weights from
planning to deviate to m as she would deviating to M at any later truthful period in the
block, then there will be no profitable deviation once any such period is reached.
Lemma 4. Let σHτ ,a : H × Θ → ∆(A) for a ∈ A, Hτ ∈ H be identical to the conditional
strategy for σri , defined above, except at the history Hτ . At Hτ , σHτ ,a plays a for either θ.
Take some t0 ∈ T0 to be the start of a block, and suppose that for all Hτ ∈ H(T ) satisfying
t0 (τ ) = t0 (that is, for all truthful histories in that block) it holds that Wai (Ht0 , σHτ ,m ) =
Wai (Ht0 , σHτ ,M ) for a = m, M . Then ei has no profitable deviation if selected at any Hτ ∈
H(T ) satisfying t0 (τ ) = t0 .

Proof. See Appendix.

    We will show that the conditions of Lemma 4 hold, implying that our strategy is in fact
a best response at truthful periods.
    If expert ei is picked at time t0 ∈ T0 , she receives weights Wmi (Ht0 , σ i ), WM   i
                                                                                           (Ht0 , σ i ) from
following the proposed strategy. Consider a path of actions from t0 to t0 + K − 1, going
from the first period in the block to the last period before the block repeats. Relative to t0 ,
a weight of W a (Ht0 +K−1 ) accumulates towards Wai (Ht0 , σ i ). Because strategies repeat anew
every K periods, Wai (Ht0 , σ i ) = Wai (Ht0 +K , σ i ), and so Wai (Ht0 , σ i ) satisfies the recursive
formula
                          h                                                                         i
    Wai (Ht0 , σ i )        a               K StartD
                       = E W (Ht0 +K−1 ) + β q       (Ht0 +K−1 )q StartT           i         i
                                                                         (Ht0 +K )Wa (Ht0 , σ ) Ht0

                                                                                                             a
                                                                                        StartT         1− W
                                                                                                          Ya
where the expectation is taken over Ht0 +K , given Ht0 . On each path, q                         =   q StartD β K
                                                                                                                    .
Plugging this in to the recursive formula gives

                                                         W a (Ht0 +K−1 )
                              h                                                                     i
          Wai (Ht0 , σ i ) = E W a (Ht0 +K−1 ) + 1 −           Ya
                                                                           W  i
                                                                                (Ht , σ i
                                                                                          ) Ht0 , σ i
                                                h                          ia 0
                                Wai (Ht0 ,σ i )
       =⇒               0 = 1−       Ya
                                                 E W a (Ht0 +K−1 )|Ht0 , σ i
       =⇒ Wai (Ht0 , σ i ) = Y a .

                                                                           i
Now consider a deviation of the form discussed in Lemma 4 to the strategy σH τ ,a
                                                                                  . Following




                                                      26
the same substitutions, with Wai (Ht0 , σ i ) = Y a , this gives weights

             i
  Wai (Ht0 ,σH τ ,a
                    )
                   h                                                                                       i
            = E W a (Ht0 +K−1 ) + β K q StartD (Ht0 +K−1 )q StartT (Ht0 +K )Wai (Ht0 , σ i ) Ht0 , σH
                                                                                                    i
                                                                                                      τ ,a

                                    W i (Ht , σ i )
                                                     h                               i
            = Wai (Ht0 , σ i ) + 1 − a a0             E W a (Ht0 +K−1 )|Ht0 , σHi
                                                                                  τ ,a
                                         Y
                = Wai (Ht0 , σ i )

(In equilibrium, if expert ei is selected at t0 then the same expert will be selected at every
truthful period in a block, and W a , q StartD , and q StartT are determined only by the play at
truthful periods. So when the expectation is taken, only ei ’s strategy need be considered.).
So all such deviations yield the same point in (Wm , WM )-space, and Lemma 4 applies; there
are no profitable deviations by experts at any H(T ) periods.
     To finish the proof that the strategies are an equilibrium for β and K large enough, we
now only need to show that the expert prefers not to deviate at H(D) periods. Starting at
any H(D) period Ht , the expert is at most K periods away from reaching the next block
at t0 (t + K) with probability q StartT . So from following the equilibrium path, Wai (Ht , σ i ) ≥
β K q StartT Wai (Ht0 (t+K) , σ i ) = β K q StartT Y a . Fixing K and taking β → 1, β K goes to 1 and
                                                                   ζ    ζ
q StartT approaches some value at least equal to min{ 2(1−p)          , 2p }. The limit of Y m is bounded
below by an expression which goes to infinity:
                                                                                                            
        m                        m                         p                                       p
lim Y       ≥    min     lim W (Hτ ) ≥ min pK − 1,            ((1 − p)K − 1)          = pK−max 1,
β→1             Hτ ∈H(D) β→1                              1−p                                     1−p

and a similar argument shows that Y M also diverges. Therefore, for β and K large enough,
Wai (Ht0 , σ i ) becomes arbitrarily large and in particular is greater than 1. Deviating from a
to a at Ht ∈ H(D) gives a weight of 1 on a and 0 on a, and so for K and β large enough
this is strictly dominated by not deviating because ri ≥ 0.
    So the strategies we have constructed do form an equilibrium. Now we show that as K
increases, these strategies give an arbitrarily high long-term proportion of truthful play.
    Consider the probability of having less than or equal to n truthful periods in a block of
length K. Writing pa as the probability of state θa in a period, so pm = p and pM = 1 − p,




                                                     27
and letting xna denote a binomial random variable of n draws from probability pa ,

P rob At most n truthful periods in a block = P rob At least bpa Kc θa ’s after n periods, for some a
                                                                                                   
                                               X
                                                  P rob At least bpa Kc θa ’s after n periods
                                                                                             
                                             ≤
                                                         a
                                                         X
                                                             P rob xna ≥ bpa Kc
                                                                               
                                                    =
                                                         a
                                                         X            xna           K − n − 1
                                                    ≤        P rob         ≥ pa + pa            .      (2)
                                                         a
                                                                       n                n
                                                                   q
    The random variable xna /n has mean pa and standard deviation pa (1−p
                                                                        n
                                                                          a)
                                                                             . So if we take
n ' K − K s for some s ∈ (1/2, 1) then pa K−n−1
                                            n
                                                 divided by the standard deviation goes to
infinity as K goes to infinity:
                       K −1     s
          pa K−n−1  pa K−K                           Ks − 1                 pa K s − 1
                                          r                           r
               n           s                    pa
          q        'q        =                      √        ≥                   √     −→ ∞.
             pa (1−pa )      pa (1−pa )       1 − pa K − K s              1 − pa   K K→∞
                  n           K−K s


Therefore each of the probability terms in (2) goes to 0 as K increases (for instance, by
Chebyshev’s theorem), and the probability of more than n ' K − K s out of K truthful
                                              s
periods goes to 1. Moreover, since Kn ' K−KK
                                                goes to 1 for large enough K, the expected
proportion of truthful periods in a block must approach 1 for large enough K.4
   Because blocks are independent, the Law of Large Numbers tells us that the long-term
proportion of truthful periods approaches the expected proportion in a given block, and we
can get this arbitrarily close to 1.


5       Extensions
We consider an extreme environment in which customers are short term players who never
receive signals about the true state in past periods, and in which expert preferences are
completely independent of the state of the world. Any combination of these assumptions
can be relaxed without fundamentally altering our conclusions.
    The stream of customers can be thought of as a single long-term player, or some combina-
tion of short- and long-term players, without affecting any of the equilibria. The customers’
    4
                                                                                        √
    This implies that the number of deterministic periods in a block is of order at most K; taking s < 1/2,
the difference
     √         divided by the standard deviation goes to 0, which can be used to show that the order is at
least K.


                                                    28
actions have no effect on the current or future play of experts, and so any strategy is both
myopically and dynamically optimal. We focussed on short-lived customers to highlight the
fact that long-term relationships between individuals are unnecessary so long as histories are
observable.
    If experts receive a small amount of disutility from mismatching the action and the state
– due to guilt from lying, a fear of God or audits, or because their underlying cost structure
depends on the state – then all equilibrium strategy profiles in the paper remain equilibria.
Indeed, a slight preference for truth makes truthfulness a strict rather than a weak best
response at the appropriate periods.
    Moreover, because all of the equilibria we construct are fully pooling, signals about the
true state in past periods reveal no new information about an expert’s type or about how
an expert will play in the future. Signals may alter the set of equilibria, but do not disrupt
the ones we lay out. (It is easy to imagine that long-lived customers in particular might
observe signals of an expert’s past truthfulness – for instance, a customer brought his car
into a mechanic to have it repaired, and the car still had problems when he got it back).
    The assumption that the outside option of an expert is 0 may not make sense if we
interpret experts as having no business outside of this market. How could we possibly
support a large (infinite) number of experts, when almost all of them get no business? But
if we think that the experts have nonbinding capacity constraints and otherwise linear costs,
then this constructed market can be thought of as being on top of whatever other business
they get – possibly from identical markets running in parallel.
    In this model, the infinity of available experts stands in for the ability of a customer to
go to a new expert in each period. We can implement all of the equilibria considered so
long as the previously chosen expert and a single new one are always available. In the Web
Appendix we show how such an equilibrium can work in an environment akin to Section 3 in
which customers must always return to a fixed, finite set of experts. We also extend Section
3 to a setting with a larger action space in which there may be observable heterogeneity.
    Finally, in the Web Appendix we discuss the possibility of truthful play in a one-shot
setting where prices may not be exogenously fixed. When experts are homogeneous or when
their costs are commonly known, prices for each treatment can be found at which experts
earn the same profit for each action. At these prices, experts will be truthful even when
no future business depends on the action chosen today. But it is impossible to find such
prices when an expert’s costs are drawn from a nondegenerate distribution and are privately
observed. There is no way to condition prices on reported costs appropriately without giving


                                              29
experts an incentive to lie about these costs.


6     Conclusion
We looked at a model where short-lived customers successively choose long-lived experts
who decide on and then perform actions for the customers. Customer utilities depend on
the state of the world along with the chosen action, but customers cannot observe the state.
Customers only observe the history of past actions taken by experts. The experts see the
state, but it doesn’t factor into their utility function. We considered how experts could be
induced to take the state-dependent action preferred by the customers.
    In the game where experts do not have private information about their own preferences,
a truthful equilibrium can be implemented in the following manner. If the previous period’s
expert just performed a minor treatment, then the next customer returns to that expert; if
the expert just performed a major one then the customer moves to a new expert with some
probability. The expert is indifferent between the actions because she gets more money
today but less future business from a major treatment, and the customer is indifferent across
experts because all would be truthful.
    When experts do have private information about their own preferences, fully truthful
equilibria are no longer possible. But customers can play a strategy in which all types of
experts will be indifferent over actions, and will therefore play truthfully, in certain periods.
In other periods, the experts are told to ignore the state and perform some predetermined
action. At the truthful periods, experts are indifferent because either action will lead to the
expert’s performing the same number of expected, discounted lifetime minor treatments and
the same number of expected, discounted lifetime major treatments. As the discount factor
approaches 1, we can achieve truthful actions in nearly all periods.


Appendix: Proofs of Lemmas

Proof of Lemma 1. The strategy σ i is sequentially rational and gives an optimal payoff start-
ing from every history, so it is sufficient to show that σ̂ i gives an optimal payoff starting from
any history in Ĥ.
    We divide this into three cases.

    • Case 1: There is a single history Ĥt ∈ Ĥ. The strategy of playing m at Ĥt and
      continuing with σ i in the future gives the same payoff as the strategy of playing M

                                                 30
       at Ĥt followed by σ i ; otherwise, it would not be optimal for ei to be truthful. So for
       any realization of θt , any mixture of these two strategies also gives this same optimal
       payoff.

    • Case 2: Ĥ is finite. We can apply the argument of Case 1 inductively, changing the
      strategies at each element of Ĥ in any order. After each of these changes, the strategy
      remains sequentially rational and payoffs remain the same.

    • Case 3: Ĥ is countably infinite. Suppose that σ̂ i gives δ > 0 less utility than σ i to ei if
      she is chosen at some history Ht . For any positive N , we can change the strategy from
      σ i to σ̂ i at the finitely many histories Ĥτ ∈ Ĥ which satisfy t ≤ τ ≤ t + N , and utilities
                                                                                                i
      at all periods will remain constant as in Case 2. Call this intermediate strategy σN        .
       The highest stage payoff that the player can receive is 1 and the lowestPis 0, and so
       starting at Ht the utilities from strategies σ̂ i and σN
                                                              i
                                                                can differ by at most ∞        τ
                                                                                      τ =N +1 β =
       β N +1
        1−β
              . For N large enough, this difference must be less than any fixed δ > 0. Contra-
       diction.5

Proof of Lemma 2. The “If” part is immediate from the fact that, given on an expert’s type,
the utility of a conditional strategy is determined entirely by the weights it induces on M
and m.
   To show the “Only If” part, let ei be indifferent between σ 0 and σ 00 at history Ht for both
r and r00 . Then
 0

                  (
                    r0 Wmi (Ht , σ 0 ) + WMi
                                              (Ht , σ 0 ) = r0 Wmi (Ht , σ 00 ) + WM  i
                                                                                          (Ht , σ 00 )
                    r00 Wmi (Ht , σ 0 ) + WMi
                                               (Ht , σ 0 ) = r00 Wmi (Ht , σ 00 ) + WM  i
                                                                                            (Ht , σ 00 )
                  (
                    r0 Wmi (Ht , σ 0 ) − Wmi (Ht , σ 00 ) = WM        (Ht , σ 00 ) − WM     (Ht , σ 0 )
                                                                  i                    i
             =⇒
                    r00 Wmi (Ht , σ 0 ) − Wmi (Ht , σ 00 ) = WM        (Ht , σ 00 ) − WM     (Ht , σ 0 )
                                                                   i                     i


              =⇒(r0 − r00 ) Wmi (Ht , σ 0 ) − Wmi (Ht , σ 00 ) = 0.
                                                                 


Because r0 6= r00 , it must hold that Wmi (Ht , σ 0 ) = Wmi (Ht , σ 00 ). Plugging this back into the
original indifference r0 Wmi (Ht , σ 0 ) + WM
                                            i
                                              (Ht , σ 0 ) = r0 Wmi (Ht , σ 00 ) + WM
                                                                                   i
                                                                                     (Ht , σ 00 ) implies that
WM i
     (Ht , σ 0 ) = WM
                    i
                      (Ht , σ 00 ) as well.
Proof of Lemma 3. Conditions (i) and (iv) hold for any K large enough. Condition (ii) holds
if pK − bpKc 6= 0.
      Condition (iii) is equivalent to pK − bpKc ∈ ( 5p−22
                                                           , 3p
                                                              2
                                                                ) for p ≤ 12 , or pK − bpKc ∈
( 3p−1
    2
       , 5p−1
           2
              ) for p ≥ 12 . In either case, both conditions (ii) and (iii) will be satisfied if
pK − bpKc is in some small neighborhood Np ⊆ (0, 1) about p. (In fact, even for K small,
condition (iv) is satisfied if pK − bpKc is close to p).
   5
   When this lemma is applied in Section 4, the highest stage payoff is max{ri , 1} rather than 1, and so the
                               N +1                           N +1
maximum payoff difference is β1−β · max{ri , 1} rather than β1−β . The argument is otherwise unchanged.



                                                     31
   For p ∈ (0, 1) irrational, {pn−bpnc|n ∈ N} is dense on (0, 1). For p rational with reduced
denominator d, any K of the form K = nd + 1 will have pK − bpKc = p ∈ Np . In each case
an arbitrarily large K can be found with pK − bpKc in Np .
Proof of Lemma 4. Let Gτ = {Hτ ∈ H(T )} be the set of possible truthful histories at time τ .
Say that Hτ0 and Hτ00 in Gτ are equivalent if the actions from periods t0 (τ ) through τ − 1 are
the same in both histories. For τ = t0 (τ ), all Hτ are equivalent. The experts’ and customers’
strategies are such that picking an expert ei at two equivalent histories yields identical play
by all agents going forward.
    Take t0 ∈ T0 , and fix some τ such that t0 (τ ) = t0 and Gτ is nonempty. Suppose that ei is
selected at Ht . Define Giτ (Hτ |Ht0 ) to be the single history following Ht0 that is equivalent
to Hτ , in which ei is selected at period t0 and is selected with positive probability at period
τ according to the customers’ equilibrium strategy. It is the element of the equivalence class
following Ht0 in which ei is chosen in every period from t through τ − 1 and the actions
corresponding to Hτ are played. For τ = t0 , define Giτ (Hτ |Ht0 ) to be Hτ even if ei is not
selected by the customers with positive probability at Ht0 .
    Conditional on customer ct0 selecting expert ei at history Ht0 , given some arbitrary
conditional strategy σ 0 of ei , denote the probability of a history Hτ occurring and ei being
selected at Hτ by π i (Hτ , Ht0 |σ 0 ). For any history Hτ 6= Giτ (Hτ |Ht0 ), the customers’ strategy
is such that either Hτ occurs with probability 0 or ei is selected with probability 0 at Hτ ;
in either case, π i (Hτ , Ht0 |σ 0 ) = 0 for any σ 0 . Under the equilibrium conditional strategy σri ,
the probability π i (Giτ (Hτ |Ht0 ), Ht0 |σri ) is in fact positive. An expert chosen at t0 is never
fired before the start of deterministic periods in a block, so π i (Giτ (Hτ |Ht0 ), Ht0 |σri ) is just
the probability that the states θt0 through θτ −1 are such that truthful play gives the correct
sequence of actions.
    Now, consider some conditional strategy σ 0 which differs from the equilibrium strategy
σri only at Hτ . We can see that Wai (Ht0 , σ 0 ) is equal to some constant (the weight added
along all histories which do not follow Hτ ) plus β τ −t0 π(Giτ (Hτ |Ht0 ), σri ) times a convex com-
bination of Wai (Hτ , σHτ ,m ) and Wai (Hτ , σHτ ,M ). The convex combination places a weight on
Wai (Hτ , σHτ ,a0 ) equal to the probability (unconditional on θτ ) of ei playing a0 at Hτ if chosen.
    In particular, if σ 0 = σHτ ,a0 then the convex combination places a weight of 1 on WHi τ (a, σHτ ,a0 )
and 0 on Wai (Hτ , σHτ ,a00 ), for a00 6= a0 .
    Suppose that the condition of the lemma holds: Wai (Ht0 , σHτ ,m ) = Wai (Ht0 , σHτ ,M ). Then
it must be the case that either π(Hτ , Ht0 |σri ) = 0, or Wai (Hτ , σHτ ,m ) = Wai (Hτ , σHτ ,M ). These
weights are equal for equivalent histories, and each history is equivalent to one for which π
is positive. So in fact Wai (Hτ , σHτ ,m ) = Wai (Hτ , σHτ ,M ) for all Hτ ∈ H(T ).
    Therefore at the history Hτ , if ei is selected, she has no profitable deviations. She can
either deviate to m, M , or some mixture of the two; and any such deviation yields the same
weights on each action, that is, the same number of expected discounted lifetime plays. Any
such mixture of actions is optimal at a truthful period Hτ .




                                                  32
References
Ingela Alger and Francois Salanie. A theory of fraud and overtreatment in experts markets.
  Journal of Economics & Management Strategy, 15(4):853–881, Winter 2006.

Heski Bar-Isaac and Steve Tadelis. Seller reputation. Foundations and Trends in Microeco-
  nomics, 4(4):273–351, 2008.

John A. Byrne. Goodbye to an ethicist. Business Week, page 38, February 10, 2003.

Michael R. Darby and Edi Karni. Free competition and the optimal amount of fraud. Journal
 of Law and Economics, 16(1), 1973.

Uwe Dulleck and Rudolf Kerschbamer. On doctors, mechanics and computer specialists:
 The economics of credence goods. Journal of Economic Literature, 44:5–42, 2006.

Uwe Dulleck and Rudolf Kerschbamer. Experts vs. discounters: Consumer free-riding and
 experts withholding advice in markets for credence goods. International Journal of Indus-
 trial Organization, 2008.

Jeffey C. Ely and Juuso Valimaki. Bad reputation. Quarterly Journal of Economics, 118(3):
  785–814, 2003.

Jeffrey C. Ely, Drew Fudenberg, and David K. Levine. When is reputation bad. Games and
  Economic Behavior, 63:498–526, 2008.

Winand Emons. Credence goods and fraudulent experts. RAND Journal of Economics, 28
 (1):107–119, Spring 1997.

Drew Fudenberg and David K. Levine. Efficiency and observability in games with long-run
  and short-run players. Journal of Economic Theory, 62:103–135, 1994.

Matthew O. Jackson and Hugo Sonnenschein. Overcoming incentive constraints. Economet-
 rica, 75(1):241–257, January 2007.

In-Uck Park. Cheap talk referrals of differentiated experts in repeated relationships. RAND
  Journal of Economics, 36(2), Summer 2005.

Martin Pesendorfer. A study of collusion in first price auctions. Review of Economic Studies,
 67:381–411, 2000.

Carolyn Pitchik and Andrew Schotter. Honesty in a model of strategic information trans-
  mission. American Economic Review, 77(5):1032–1036, December 1987.

Henry Schneider. Agency problems and reputation in expert services: evidence from auto
  repair. Johnson School Research Paper Series #15-07, October 2007.



                                             33
Robert M. Townsend. Optimal multiperiod contracts and the gain from enduring rela-
  tionships under private information. The Journal of Political Economy, 90(6):1166–1186,
  December 1982.

Asher Wolinsky. Competition in a market for informed experts’ services. RAND Journal of
  Economics, 24:380–398, 1993.




                                           34
