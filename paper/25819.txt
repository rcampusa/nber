                             NBER WORKING PAPER SERIES




   ERRORS IN THE DEPENDENT VARIABLE OF QUANTILE REGRESSION MODELS

                                      Jerry A. Hausman
                                         Haoyang Liu
                                            Ye Luo
                                      Christopher Palmer

                                     Working Paper 25819
                             http://www.nber.org/papers/w25819


                    NATIONAL BUREAU OF ECONOMIC RESEARCH
                             1050 Massachusetts Avenue
                               Cambridge, MA 02138
                                    May 2019




We thank Isaiah Andrews, Colin Cameron, Victor Chernozhukov, Denis Chetverikov, Kirill
Evdokimov, Hank Farber, Brigham Frandsen, Larry Katz, Brad Larsen, Rosa Matzkin, James
McDonald, Ulrich Muller, Shu Shen, and Steven A. Snell for helpful feedback and discussions,
as well as seminar participants at Cornell, Harvard, MIT, Princeton, UC Davis, UCL, and
UCLA. Lei Ma, Yuqi Song, and Jacob Ornelas provided outstanding research assistance. The
views expressed herein are those of the authors and do not necessarily reflect the views of the
National Bureau of Economic Research, the Federal Reserve Bank of New York or the Federal
Reserve System.

NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.

© 2019 by Jerry A. Hausman, Haoyang Liu, Ye Luo, and Christopher Palmer. All rights reserved.
Short sections of text, not to exceed two paragraphs, may be quoted without explicit permission
provided that full credit, including © notice, is given to the source.
Errors in the Dependent Variable of Quantile Regression Models
Jerry A. Hausman, Haoyang Liu, Ye Luo, and Christopher Palmer
NBER Working Paper No. 25819
May 2019
JEL No. C19,C21,C31,I24,I26,J30

                                       ABSTRACT

The popular quantile regression estimator of Koenker and Bassett (1978) is biased if there is
an additive error term. Approaching this problem as an errors-in-variables problem where the
dependent variable suffers from classical measurement error, we present a sieve maximum-
likelihood approach that is robust to left-hand side measurement error. After providing
sufficient conditions for identification, we demonstrate that when the number of knots in the
quantile grid is chosen to grow at an adequate speed, the sieve maximum-likelihood
estimator is consistent and asymptotically normal, permitting inference via bootstrapping.
We verify our theoretical results with Monte Carlo simulations and illustrate our
estimator with an application to the returns to education highlighting changes over time
in the returns to education that have previously been masked by measurement-error bias.

Jerry A. Hausman                               Ye Luo
Department of Economics, E52-518               The University of Hong Kong
MIT                                            Faculty of Business and Economics
50 Memorial Drive                              kurtluo@hku.hk
Cambridge, MA 02142
and NBER                                       Christopher Palmer
jhausman@mit.edu                               MIT Sloan School of Management
                                               100 Main Street, E62-639
Haoyang Liu                                    Cambridge, MA 02142
Federal Reserve Bank of New York               and NBER
liuhy@berkeley.edu                             cjpalmer@mit.edu
2                                  HAUSMAN, LIU, LUO, AND PALMER

                                           1. Introduction

   Economists are aware of problems arising from errors in variables (EIV) in regressors but
generally ignore measurement error in the dependent variable. The EIV problem has re-
ceived its most significant attention in the linear model, including the well-known results
that classical measurement error causes attenuation bias if present in the regressors and
has no effect on unbiasedness if present in the dependent variable (see Hausman, 2001 for
an overview). In general, however, the linear model results do not hold in nonlinear mod-
els.1 In this paper, we study left-hand-side EIV in random-coefficients models, where even
an additive disturbance uncorrelated with the regressors can create bias in estimating the
outcome’s conditional distribution. We focus on the consequences of measurement error in
the dependent variable of linear conditional quantile models, a setting where we can achieve
nonparametric identification even with some discrete covariates (in contrast to the generic
random-coefficients model).2 We propose a maximum-likelihood approach to consistently
estimate the distributional effects of covariates in such a setting. While EIV in regressors
usually require instrumental variables, we provide sufficient conditions for our estimator to
identify the conditional distribution of the outcome without instrumenting. Our estima-
tor has fractional polynomial of n convergence speed and asymptotic normality, permitting
inference by bootstrapping.
   Quantile regression (Koenker and Bassett, 1978) is the most widely used special case of
heterogenous-effects random-coefficients models and has become a popular tool for applied
microeconomists to consider the impact of covariates on the distribution of the dependent
variable. As noted, a key benefit of the restrictions imposed by quantile regression on
the general linear random-coefficients model is to accommodate non-continuous covariates,
which cause the general random-coefficients model to become unidentified. However, in part
because left-hand side variables in microeconometrics often come from self-reported survey
data, the sensitivity of traditional quantile regression to dependent variable measurement
error poses a serious problem to its validity.3 Put another way, while omitted variables are
problematic in the linear model insofar as they are correlated with the regressors, in quantile
regression even unobserved heterogeneity independent of included covariates causes bias.
In this sense, our results are applicable to settings with many covariates and unobserved

    1
      Schennach (2008) establishes identification and a consistent nonparametric estimator when EIV exists
in an explanatory variable. Wei and Carroll (2009) proposed an iterative estimator for the quantile regression
when one of the regressors has EIV. Studies focusing on nonlinear models in which the left-hand side variable
is measured imperfectly include Hausman, Abrevaya, and Scott-Morton (1998) and Cosslett (2004), who
study probit and tobit models, respectively.
    2
      Hausman (2001) observes that EIV in the dependent variable in quantile regression models generally
leads to significant bias in contrast to the linear model intuition.
    3For overviews of the econometric issues associated with measurement error in survey data, see Bound
et al. (2001) and Meyer et al. (2015).
             ERRORS IN THE DEPENDENT VARIABLE OF QUANTILE REGRESSION MODELS                                   3

heterogeneity, such as the nonparametric estimation of a panel-data models with unobserved
heterogeneity studied by Evdokimov (2010).
   Intuitively, the estimated quantile regression line xTi β(τb ) for quantile τ may be far from
the observed yi because of LHS measurement error or because the unobserved conditional
quantile ui of observation i is far from τ . Our ML framework estimates the likelihood that
a given quantile-specific residual (εij := yi − xTi β(τj )) is large because of measurement error
rather than observation i’s unobserved conditional quantile ui being far away from τj . The
estimate of the joint distribution of the conditional quantile and the measurement error
allows us to weight the log-likelihood contribution of observation i more in the estimation
of β(τj ) where it is more likely that ui ≈ τj . We show in simulations that a mixture of
normals can accommodate a wide set of EIV distributions.4 In the case of Gaussian errors
in variables, this estimator reduces to weighted least squares, with weights equal to the
probability of observing the quantile-specific residual for a given observation as a fraction of
the total probability of that observation’s residuals across all quantiles.
   An empirical example (extending Angrist et al., 2006) studies heterogeneity in the returns
to education across conditional quantiles of the wage distribution. Correcting for likely mea-
surement error in the self-reported wage data, we estimate considerably more heterogeneity
across the wage distribution in the education-wage gradient than implied by traditional
methods. In particular, the returns to education for latently high-wage individuals have
been increasing over time and are much higher than previously estimated. By 2000, the
return to education for individuals at the top of the conditional wage distribution was over
three times larger than returns for any other segment of the distribution, whereas tradi-
tional methods find only a two-fold increase. We also document that increases in the returns
to education between 2000–2010, while still skewed towards top earners, were shared more
broadly across the wage distribution.
   The rest of the paper proceeds as follows. In Section 2, we introduce our model specifica-
tion and identification conditions. In Section 3, we introduce our estimator and characterize
its properties. We present Monte Carlo simulation results in Section 4, and Section 5 contains
our empirical application. Section 6 concludes.
   We adopt the following notation. Define x to have dimension dx and support X . Let xk
denote the k th dimension of x, and let x−k denote the subvector of x corresponding to all
                                                                       p
but the k th dimension of x. Define the space of y as Y. Let →        − stand for convergence in
probability. Let f (ε|σ) be the p.d.f. of the EIV ε parametrized by σ where σ has dimension
dσ and domain Σ. We denote the true coefficient and measurement     qR            error distributional
                                                                         1
parameters as β0 (·) and σ0 , respectively. Define ||(β, σ)|| :=        0
                                                                           ||β(τ )||22 dτ + ||σ||22 as the

   4See    Burda et al. (2008, 2012) for other applications demonstrating the flexibility of a finite mixture of
normals.
4                                   HAUSMAN, LIU, LUO, AND PALMER

L2 norm of (β0, σ0 ), where || · ||2 is the usual Euclidean norm. Finally, we use the notation
x - y for x = O(y) and x -p y for x = Op (y).

                                   2. Model and Identification

  Consider the general linear random-coefficients model as a framework to characterize un-
observed heterogeneity in marginal effects

                                               yi = xTi βi + εi ,                                          (2.1)

where the covariates vector xi is independent of the random coefficient vector βi . This model
is nonparametrically identified even in the presence of additive unobserved heterogeneity εi
such that additional measurement error is isomorphic to any other form of independent
unobserved heterogeneity and poses no immediate problem for bias. However, identification
requires xi to be continuously distributed and practical computation requires the dimension
of xi to be low to avoid the curse of dimensionality.
   When at least some covariates are discrete (the most common situation when estimat-
ing treatment effects), a special case of (2.1) that permits nonparametric identification of
heterogenous treatment effects is linear conditional quantile regression, which takes the form

                                               yi∗ = xTi β0 (ui ),                                         (2.2)

where the unobserved heterogeneity in treatment effects enters as the scalar ui ∼ U [0, 1]
representing the unobserved quantile of yi in the conditional distribution of yi |xi .5 In this
model, the τ th conditional quantile of the dependent variable y ∗ is a linear function of x

                                            Qy∗ |x (τ ) = xT β0 (τ ).

However, we are interested in the situation where y ∗ is not directly observed, and we instead
observe y where
                                          y = y∗ + ε
and ε is a mean-zero, i.i.d error term independent from y ∗ and x.
   Unlike the linear-regression case where EIV in the left-hand side variable does not matter
for consistency and asymptotic normality, EIV in the dependent variable can lead to severe
bias in quantile regression. More specifically, with ρτ (z) denoting the check function

                                         ρτ (z) = z(τ − 1(z < 0)),



    5Here  we study the linear conditional quantile model, as is ubiquitous in practice. While the conditional
quantile model is identified for linear and many nonlinear specifications, it is not nonparametrically identified
(Horowitz and Lee, 2005). Note that our results will allow for polynomials in xi , somewhat relaxing the
linearity assumption.
           ERRORS IN THE DEPENDENT VARIABLE OF QUANTILE REGRESSION MODELS                                       5

the minimization problem in the usual quantile regression

                                      β(τ ) ∈ arg min E[ρτ (y − xT b)],                                     (2.3)
                                                     b

is generally no longer minimized at the true β0 (τ ) when EIV exists in the dependent variable.
When there exists no EIV in the left-hand side variable, y ∗ is observed and the FOC is

                                      E[x(τ − 1(y ∗ < xT β(τ )))] = 0,                                      (2.4)

where the true β(τ ) is the solution to the above system of first-order conditions as shown
by Koenker and Bassett (1978). However, with left-hand side EIV, the first-order condition
determining β(τ
            b ) becomes

                                   E[x(τ − 1(y ∗ + ε < xT β(τ )))] = 0.                                     (2.5)

In Appendix A, we demonstrate the bias of bivariate quantile regression, showing that coef-
ficient estimates are biased inwards from their minimum and maximum levels over τ , which
we refer to as compression bias.6 For intuition, note that for τ 6= 0.5, the presence of mea-
surement error ε will result in the FOC being satisfied at a different estimate of β than in
equation (2.4) even in the case where ε is symmetrically distributed because of the asym-
metry of the check function. Observations for which y ∗ ≥ xT β(τ ) and should therefore be
weighted by τ in the minimization problem may end up on the left-hand side of the check
function, receiving a weight of (1 − τ ) such that equal-sized differences on either side of zero
do not cancel each other out. Note that for median regression, ρ.5 (·) is symmetric around
zero. This means that if ε is symmetrically distributed and β(τ ) symmetrically distributed
around τ = .5 (as would be the case, for example, if β(τ ) were linear in τ ), the expectation
in equation (2.5) holds for the true β0 (0.5).
   A Monte-Carlo simulation shows the degree of bias in a two-factor model with random
disturbances in the dependent variable y to illustrate the direction and magnitude of mea-
surement error bias.

Example 1. We consider a data-generating process

                                 yi = β1 (ui ) + x2i β2 (ui ) + x3i β3 (ui ) + εi

with the measurement error εi again distributed as N (0, σ 2 ) and the unobserved conditional
quantile ui of observation i following ui ∼ U [0, 1]. The coefficient function β(τ ) has compo-
                                                    √
nents β1 (τ ) = τ , β2 (τ ) = exp(τ ), and β3 (τ ) = τ . The variables x2 and x3 are drawn from
independent lognormal distributions LN (0, 1). The number of observations is 100,000.

    6See also Arellano and Weidner (2016),  who find that estimation error in the fixed effects can create bias in
the quantile-regression estimate of the slope coefficients, essentially understating the degree of heterogeneity
by smoothing across quantiles. However, their setup does not permit them to characterize the direction of
the bias.
6                                 HAUSMAN, LIU, LUO, AND PALMER

                           Table 1. Monte-Carlo Results: Mean Bias
                                EIV                             Quantile (τ )
             Parameter      Distribution           0.1     0.25    0.5      0.75        0.9
                               ε=0                0.000   0.000 -0.000 -0.000          0.000
                            ε ∼ N (0, 4)          0.156   0.126 0.027 -0.117          -0.215
             β2 (τ ) = eτ
                           ε ∼ N (0, 16)          0.262   0.214 0.042 -0.200          -0.353
                          True parameter:         1.105   1.284 1.649 2.117            2.460
                               ε=0                0.000   -0.000 0.000 0.000           0.000
                      √     ε ∼ N (0, 4)          0.125   0.053 -0.021 -0.069         -0.086
             β3 (τ ) = τ
                           ε ∼ N (0, 16)          0.196   0.091 -0.030 -0.112         -0.141
                          True parameter:         0.316     0.5   0.707 0.866          0.949
Notes: Table reports mean bias (across 500 simulations) of slope coefficients estimated for each
quantile τ from standard quantile regression of y on a constant, x2 , and x3 where y = β1 (τ ) +
x2 β2 (τ ) + x3 β3 (τ ) + ε and ε is either zero (no measurement error case, i.e. y ∗ is observed) or ε is
distributed normally with variance 4 or 16. The covariates x2 and x3 are i.i.d. draws from LN (0, 1).
N = 100, 000.


   Table 1 presents Monte-Carlo results for three cases: when there is no measurement error
and when the variance of ε equals 4 and 16. The simulation results show that under the
presence of measurement error, the quantile regression estimator is severely biased. Further-
more, we find evidence of the attenuation-towards-the-median behavior posited by Hausman
(2001), with quantiles above the median biased down and quantiles below the median up-
wardly biased, understating the distributional heterogeneity in the β(·) function. For sym-
metrically distributed EIV and uniformly distributed β(τ ), the median regression results
appear unbiased. Comparing the mean bias when the variance of the measurement error
increases from 4 to 16 shows that the bias is increasing in the variance of the measurement
error. Intuitively, the information of the functional parameter β(·) is decaying as the variance
of the EIV becomes larger.

2.1. Identification and Regularity Conditions. To establish the nonparametric identi-
fication of our model, we require the following two assumptions.

Assumption 1 (Properties of β(·)). We assume the following properties on the coefficient
vectors β(τ ).
    (1) β(τ ) is in the space M [B1 × B2 × B3 × ... × Bdx ] where the functional space M is
        defined as the collection of all functions b = (b1 , ..., bdx ) : [0, 1] → [B1 × ... × Bdx ]
        with Bk ⊂ R being a closed interval ∀ k ∈ {1, ..., dx } such that xT b(τ ) : [0, 1] → R is
        monotonically increasing in τ for all x ∈ X .
    (2) The true parameter β0 is a vector of C 2 functions with first-order derivatives bounded
        from above by a positive constant.
           ERRORS IN THE DEPENDENT VARIABLE OF QUANTILE REGRESSION MODELS                             7

   Monotonicity of xT β(·) is a key assumption in quantile regression and important for iden-
                                                              R1
tification because in the log-likelihood function, f (y|x) = 0 f (y − xT β(u))du is invariant
to a rearrangement of the function β(u).7 The function β(·) is therefore unidentified if we
do not impose further restrictions. However, given the distribution of the random variable
{β(u) | u ∈ [0, 1]}, the vector of functions β : [0, 1] → B1 × B2 × ... × Bdx is unique under
rearrangement if xT β(·) is monotonic in τ .

Assumption 2 (Properties of x). We assume the following properties of the vectors x that
comprise the design matrix X.
   (1) E[x0 x] is non-singular.
   (2) There is at least one dimension x1 of x such that x1 |x−1 is continuously distributed,
       and the element of β0 (·) corresponding to x1 , denoted as β0,1 (τ ), does not have any
       point mass in its probability distribution.

  The above conditions on the parameters and covariate matrix allow us to state our main
nonparametric identification result.

Theorem 1 (Nonparametric Global Identification). Assume that Assumptions 1 and 2 hold
and that the PDFs of ε, f (·) and f0 (·), are continuously differentiable functions such that
       R∞               R∞
   (1) −∞ εf (ε)dε = 0, −∞ εf0 (ε)dε = 0, and
       R∞                  R∞
   (2) −∞ ε2 f (ε)dε < C, −∞ ε2 f0 (ε)dε < C for some constant C.
Then, for any β(·) and f (·) which generate the same density of y|x almost everywhere as the
true function β0 (·) and f0 (·), it must be that β(τ ) = β0 (τ ) almost everywhere for all τ ∈ [0, 1]
and f (ε) = f0 (ε) almost everywhere for all ε ∈ R.

Proof. See Appendix G.1.                                                                             
   Although the above identification result allows x−1 to enter into xT β(·) in an unrestricted
fashion, Theorem 1 holds under the presence of a continuously distributed x1 that enters x
linearly. To illustrate that nonlinear functions of x1 are admissible, the following lemma es-
tablishes nonparametric identification when polynomials of arbitrarily high degree of x1 are
also included in x. Whenever the contribution of x1 can be approximated by finite polyno-
mials, nonparametric identification holds. Before stating the lemma, we restate Assumption
2 to allow for polynomials of x1 .

Assumption 3 (Properties of x allowing for polynomials of x1 ). We assume the following
properties of the vectors x that comprise the design matrix X.
   (1) E[x0 x] is non-singular.

   7Note   that the monotonicity assumption in Assumption 1 also requires that if x ∈ X then −x ∈
                                                                                                / X . In
practice, many quantile models assume that x ≥ 0.
8                                 HAUSMAN, LIU, LUO, AND PALMER

    (2) We can partition x = (W (x1 ), x−w )T where x1 is one dimensional, W (x1 ) = (x1 , x21 , · · · , xp1 )T
        for some p, x1 |x−w is continuously distributed, and the element of β0 (·) corresponding
        to xp1 , denoted as β0,xp1 (τ ), does not have any point mass in its probability distribution.
    (3) β0,xp1 (τ ) has continuous and bounded derivatives with respect to τ for all τ ∈ (0, 1).
Lemma 1 (Nonparametric Identification with Higher-order Polynomials). Assume that As-
sumptions 1 and 3 hold and that the PDFs of ε, f (·) and f0 (·), are continuously differentiable
functions such that
       R∞               R∞
   (1) −∞ εf (ε)dε = 0, −∞ εf0 (ε)dε = 0, and
       R∞                 R∞
   (2) −∞ ε2 f (ε)dε < C, −∞ ε2 f0 (ε)dε < C for some constant C.
Then, for any β(·) and f (·) which generate the same density of y|x almost everywhere as the
true function β0 (·) and f0 (·), it must be that β(τ ) = β0 (τ ) almost everywhere for all τ ∈ [0, 1]
and f (ε) = f0 (ε) almost everywhere for all ε ∈ R.
Proof. See Appendix G.1.                                                                              

                                           3. Estimation

   In this section, we first demonstrate the consistency of the ML estimator, which we then
operationalize with a sieve-ML estimator, establishing its consistency and asymptotic nor-
mality. In addition, we extend Chen and Pouzo (2013) to provide sufficient conditions for
inference by bootstrapping in our setting. While Theorem 1 and Lemma 1 establish iden-
tification even when the distribution of ε is nonparametric, for estimation, we require the
following assumptions on the properties of the measurement error ε.8
Assumption 4 (Properties of EIV). The probability density function of the EIV is parametrized
as f (ε|σ), and the true density is abbreviated f0 (ε) := f (ε|σ0 ).
    (1) The domain of the parameter σ is a compact space Σ, and the true value σ0 is in the
        interior of Σ.
    (2) f (ε|σ) is twice differentiable in ε and σ with bounded derivatives up to the second
        order.
    (3) For all σ ∈ Σ, there exists a uniform constant C̄ > 0 such that E[| log f (ε|σ)|] < C̄.
        Moreover, f (·|σ) is non-zero all over the entire space R and bounded from above
        uniformly.
                 R∞
    (4) E[ε] = −∞ εf (ε|σ) = 0.
                                                         Rl
    (5) For any σ ∈ Σ, l > 0, and some constant Cl > 0, −l |φε (s)−φε0 (s)|2 ds ≥ Cl ||σ−σ0 ||22 ,
                         R∞
        where φε (s) := −∞ exp(isε)f (ε|σ)dε is the characteristic function of ε given PDF
        f (ε|σ).
    8While    Assumption 4 requires knowing the distribution of the EIV up to a finite set of parameters,
we show in simulations below that when the distribution of the EIV is unknown, a mixture of normals is
sufficiently flexible to approximate a wide range of potential distributions.
          ERRORS IN THE DEPENDENT VARIABLE OF QUANTILE REGRESSION MODELS                         9

  Note that Assumption 4 holds for all mean-zero distributions in the exponential family.
  Given this parameterization of f (·|σ), we define our log likelihood function as follows.
Denote θ := (β(·), σ) ∈ Θ. For any θ, define the expected log-likelihood function L(θ) as

                                    L(θ) = E[log g(y|x, θ)],                                (3.1)

with the empirical log likelihood being denoted

                                  Ln (θ) = En [log g(y|x, θ)],                     (3.2)

where En is the empirical average operator En h(x) := n1 ni=1 h(xi ).
                                                           P

  Using the fact that the unobserved conditional quantile is the CDF of y|x and CDFs are
distributed uniformly, the conditional density function g(y|x, θ) is given by
                                         Z 1
                             g(y|x, θ) =     f (y − xT β(u)|σ)du.                  (3.3)
                                            0

Then the ML estimator is defined as

                       θb = (β(·),
                             b σ   b) ∈ arg max En [log g(y|x, β(·), σ)],                   (3.4)
                                           (β(·),σ)∈Θ

where g(·|·, ·, ·) is the conditional density of y given x and parameters, as defined in equation
(3.3). The following theorem states the consistency property of the ML estimator.

Lemma 2 (MLE Consistency). Under Assumptions 1, 3, and 4, the maximum-likelihood
estimator defined by (3.4) exists and converges in probability to the true parameter (β0 (·), σ0 )
under the L2 norm in the functional space M and Euclidean norm in Σ.

Proof. See Appendix G.2.                                                                        

  The consistency theorem is a special version of a general MLE consistency theorem (Van
der Vaart, 2000). Two conditions play critical roles here: the monotonicity of xT β(·) for all
x ∈ X and the local continuity of at least one right-hand side variable. If monotonicity fails,
we lose compactness of the parameter space Θ and the consistency argument will fail.

3.1. Sieve Maximum Likelihood Estimation. While we have demonstrated that the
maximum likelihood estimator restricted to parameter space Θ converges to the true param-
eter with probability approaching 1, the estimator still lives in a large space with β(·) being
dx -dimensional functions such that xT β(·) is monotonic and σ being a finite dimensional
parameter. Although theoretically such an estimator does exist, in practice it is computa-
tionally infeasible to search for the likelihood maximizer within this large space. Here, we
consider a spline estimator of β(·) for their computational advantages in calculating the sieve
estimator. The estimator below is easily adapted to the reader’s preferred estimator. We
use a piecewise-spline sieve space, which we define as follows.
10                                   HAUSMAN, LIU, LUO, AND PALMER

Definition 1 (Sieve Space). Define ΘrJ := ΩrJ × Σ to denote the sieve-ML parameter space,
where ΩrJ stands for the space of rth -order spline functions with J knots on [0, 1] such that
xT β(τ ) is monotonically increasing in x ∈ X for all β(·) ∈ ΩrJ and elements in ΩrJ are
bounded above as in Assumption 1.

   For example, for any β(·) ∈ Ω1J , βk (·) is a piecewise linear function on a set of intervals
covering [0, 1] and k = 1, . . . , dx . Such a definition allows ΩrJ to cover a dense set in M [B1 ×
B2 × B3 × ... × Bdx ] as J grows to infinity with sample size.
   The space ΩrJ can therefore be written as the collection of functions β(τ ) such that β(τ ) :=
Pr         l
              PJ                             r    Pr+J                                th
   l=1 bl τ +   j=1 bj+r (max {τ − tj , 0}) =        l=1 bl Sl (τ ) where tj is the j    knot, Sl (τ ) and
bl with l = 1, 2, ..., r + J are the spline functions and their coefficients.. In general, the L2
distance of the space ΘrJ to the true parameter θ0 satisfies d2 (θ0 , ΘrJ ) ≤ CJn−r−1 for some
generic constant C (Chen, 2007). It is easy to see that ΘrJ ⊂ Θ.
   The sieve estimator is defined as follows.

Definition 2 (Sieve Estimator).

                             θbJ = (βbJ (·), σ
                                             b) = arg max
                                                        r
                                                          En [log g(y|x, β, σ)]                              (3.5)
                                                           θ∈ΘJn

where Jn → ∞ as n → ∞.

     The following lemma establishes the consistency of the sieve estimator.

Lemma 3 (Sieve Estimator Consistency). If Assumptions 1, 3, and 4 hold, Jn → ∞, and
Jn /n → 0, then the sieve estimator defined in (3.5) is consistent.

Proof. See Appendix G.2.                                                                                        
   Our objective is to show that βbJ will converge to β0 with certain speed. Doing so requires a
definition of the parametric score evaluated at a functional β(·). Let the Hadamard derivative
of g with respect to β in the directions of S1 (τ ), ..., SJ+r (τ ) and evaluated at β̃ and σ̃ be
defined as
                   Z 1                                  Z 1                             
        ∂g                0     T                             0      T
                :=      f (y − x β(τ )|σ)S1 (τ )dτ, ...,     f (y − x β(τ )|σ)SJ+r (τ )dτ .
        ∂β            0                                   0
             β̃,σ̃
                                     ∂g
                                                     ∂g
                                                     , . . . , ∂b∂g
                                                                     
Note that for a (βJ , σ) ∈ ΘrJ ,     ∂β βJ ,σ
                                                =    ∂b1         J+r
                                                                       , where b1 . . . , bJ+r are the coefficients
for S1 (τ ), ..., SJ+r (τ ) in βJ (τ ). We also define the information matrix evaluated at (β,            eσ e) as
                                                                                    0 
                                      ∂ log(g) ∂ log(g)           ∂ log(g) ∂ log(g)
                     Iβ̃,σ̃ := E              ,                            ,
                                          ∂β      ∂σ                 ∂β        ∂σ
                                                                                              β̃,σ̃
                                 " ∂g ∂g ! ∂g ∂g !0 #
                                         ,
                                      ∂β ∂σ
                                                   ,
                                                ∂β ∂σ
                            =E
                                        g          g
                                                                   β̃,σ̃
           ERRORS IN THE DEPENDENT VARIABLE OF QUANTILE REGRESSION MODELS                                  11

When J goes to infinity, the smallest eigenvalue of I(β0 , σ0 ) goes to 0, leading to an ill-
posedness problem. Intuitively, as we are trying to estimate β(·) and σ via MLE from the
mixture distribution of y = xT β(τ ) + ε, where τ ∼ U [0, 1] and ε ∼ f (·|σ0 ), the estimation of
β(·) is ill-posed. However, the curse of dimensionality in β is not at play because xT β(·) is
a monotone function of a single random variable τ . We will adopt the following measure of
ill-posedness.

Assumption 5 (Ill-posed Measure). Define mineigen(I) as the minimum eigenvalue for a
given matrix I. Let one of the following two assumptions on the degree of ill-posedness hold
    (1) Mild ill-posedness: mineigen(Iβ,σ ) ≥ C/J λ for some λ > 0 and constant C > 0, for
        all (β0 , σ0 ) ∈ Θ.
    (2) Severe ill-posedness: mineigen(Iβ,σ ) ≥ C exp(−λJ) for some λ > 0 and constant
        C > 0, and all (β0 , σ0 ) ∈ Θ.

    These ill-posed measures are closely related to the smoothness of the PDF of the EIV
(Fan, 1991). The normal distribution is severely ill-posed with λ = 2, and the Laplace
distribution is mildly ill-posed with λ = 1. Unlike the usual sieve estimation problem, our
problem is ill-posed with minimum eigenvalue decaying at speed J λ under mild ill-posedness
of degree λ. When the PDF of the EIV is super smooth, the problem becomes severely
ill-posed. While convergence to normality will be too slow for our bootstrap results to hold,
consistency still holds under super smoothness. However, we note that mild ill-posedness
will be satisfied under even minor perturbations from super smoothness. In such a case, we
could use a sieve mixture of non-smooth PDFs to approximate a smooth PDF and reduce the
ill-posedness of the problem, a point we leave to future research. We establish consistency
and the convergence rate under severe ill-posedness in Theorem 3 below.
    A sufficient condition for mild ill-posedness is the following discontinuity assumption on
f —see also An and Hu (2012).9

Assumption 6 (Discontinuity of f ). There exists a positive integer λ such that f ∈ C λ−1 (R),
and the λth order derivative of f equals

                                      f (λ) (x) = h(x) + cδ δ(x − a),                                   (3.6)

with h(x) being a bounded function and L1 Lipschitz except at a, cδ a non-zero constant, and
δ(x − a) a Dirac δ-function at a.



   9See  Lemma 8 in Appendix G.2 for a formal statement and proof of this result for the special case of
a piecewise-constant sieve, showing that if a function is of the class C λ , the minimum eigenvalue of I is of
order O(J −λ ) as J → ∞ for λ ∈ Z+ . In general, for smooth functions f (·), the minimum eigenvalue of I
will decay with speed O(exp(−J −a )) for some a > 0.
12                            HAUSMAN, LIU, LUO, AND PALMER

  The following final assumption on the characteristic function significantly simplifies our
proof of the convergence rate of the distributional parameters. It holds whenever there exists
enough variation in x such that the characteristic function is non-constant around x.

Assumption 7 (Variation on Characteristic Function). Let φxβ (s|x) denote the characteris-
tic function of xT β conditional on x. Suppose there exists a local neighborhood N ⊂ X such
that there exists a constant c > 0 and for any (β, σ) ∈ Θ and any s ∈ [−l, l],
                                                  "                          #
                                                                         2
                             φxβ (s|x)              φxβ (s|x) − φxβ0 (s|x)
                   V arx∈N                ≥ cEx∈N
                             φxβ0 (s|x)                   φxβ0 (s|x)
where V arx∈N and Ex∈N denote the variance and expectation operators evaluated over all x
in a neighborhood N .

  In the lemma below, we use the stochastic equicontinuity of the log likelihood function to
establish key facts about the convergence rate of σ
                                                  b, including that it converges to σ0 at rate
 − 14
n .

Lemma 4 (Convergence Rate of σ        b). If Assumptions 1, 3, and 7 hold and Jn2r+2 /n → ∞,
the sieve estimator (βbJ (·), σ
                              b) has the following property:
                                                      1
                                     b − σ0 = op (n− 4 ).
                                     σ                                                   (3.7)

Moreover, defining δ := ||βbJ − βJ∗ ||, then
                                                     √        
                                   2            log n δ − log δ
                       ||b
                         σ − σ0 || = Op max          ,   √                               (3.8)
                                                  n       n

Proof. See Appendix G.2.                                                                    

  For EIV distributions that are mildly ill-posed, we require that the sieve grid Jn grow
quickly enough to overcome the bias but slowly enough to overcome the ill-posed problem,
as we formalize in the following theorem.

Theorem 2 (Sieve Estimator Asymptotic Normality). Let Assumptions 1, 3, 5.1 (the mild
                                                                               2
ill-posed case), and 7 hold. Further, let the number of knots Jn satisfy Jn4λ +6λ log(n)/n → 0
and Jn2r+2 /n → ∞ as n → ∞. Then
                                                                          
                                                 1         λ        1     1
                     βbJ − β0 , σ
                                b − σ0 = O p           = J n Op         ,√ .
                                               Jnr+1              Jnr+1    n
Moreover, there exists a sequence κJ ≥ JCλ for some generic constant C > 0 such that for
                                           n
any fixed τ
                            √                       d
                             nκJ βbJ (τ ) − β0 (τ ) →− N (0, ΩJ,τ ),
         ERRORS IN THE DEPENDENT VARIABLE OF QUANTILE REGRESSION MODELS                        13

where ΩJ,τ is a sequence of positive definite matrices with the largest eigenvalue bounded by
a constant, and
                               √               d
                                 nκJ σbJ − σ0 → − N (0, ΩJ,σ ),
where ΩJ,σ is a sequence of positive definite matrices with the largest eigenvalue bounded by
a constant.

Proof. See Appendix G.2.                                                                       

  The smoothness of the mapping from the data to the estimator β(·) helps with robustness
to mild forms of misspecification. Following same proof as for Theorem 2 above, misspecifi-
cation would produce a second residual term in addition to the stochastic term. Using this
smoothness along with the capacity of our estimator to accommodate additional polynomial
terms, the approximation provided by the sieve estimator would still approach the truth
asymptotically.
  As discussed above, while asymptotic normality need not hold under the severe ill-posed
case, the following theorem establishes the convergence rate of the sieve estimator under
severe ill-posedness.

Theorem 3 (Severe Ill-posedness Sieve Estimator Convergence Rate). Let Jn be a sequence
of positive numbers such that exp(λJ  )
                                  √ n = 1 . Then under Assumptions 1, 3, 4, 5.2 (the severe
                                   n      Jn
ill-posed case), and 7, the sieve estimator βJn satisfies
                                                            1
                                     ||βbJn − β0 || -p          .
                                                         log(n)

Proof. See Appendix G.2.                                                                       

3.2. Inference via Bootstrap. In the last section we proved asymptotic normality for
the sieve-ML estimator θ = (β(τ ), σ). However, computing the convergence speed µkjJ for
βk,J (τj ) by explicit formula can be difficult in general. To conduct inference, we recommend
using nonparametric pairs bootstrap. Define (xbi , yib ) as a resampling of data (xi , yi ) with
replacement for bootstrap iteration b = 1, . . . B, and define the estimator

                               θb = arg max Enb [log g b (yib |xbi , θ)],                   (3.9)
                                          θ∈ΘJ

where Enb denotes the operator of empirical average over resampled data for bootstrap iter-
ation b. Then our preferred form of the nonparametric bootstrap is to construct the 95%
confidence interval pointwise for each covariate k and quantile τ from the variance of each
                      B
coefficients βkb (τj ) b=1 as βbk (τj ) ± z1−α/2 · σ
                                                   bjk where the critical value z1−α/2 ≈ 1.96 for
significance level of α = .05 and σ   bjk is the standard deviation of the bootstrapped estimates
of βk (τj ).
14                                 HAUSMAN, LIU, LUO, AND PALMER

   The following lemma establishes the asymptotic normality of the bootstrap estimates and
allows us, for example, to use the empirical variance of the bootstrapped parameter estimates
to construct bootstrapped confidence intervals.

Lemma 5 (Validity of the Bootstrap). As in Theorem 2, let Assumptions 1, 3, 5.1 (the
                                                                             2
mild ill-posed case), and 7 hold, and let the number of knots Jn satisfy Jn4λ +6λ log(n)/n → 0
                                           √                     √
and Jn2r+2 /n → ∞ as n → ∞. Then nκJ (βbJb − βbJ ) and nκJ (b          σb − σb) have the same
                √                    √
distribution as nκJ (βJ − β0 ) and nκJ (b
                       b                    σ − σ0 ), respectively.

Proof. See Appendix G.2.                                                                                   

                                  4. Monte-Carlo Simulations

  We examine the properties of our estimator empirically in Monte-Carlo simulations. Let
the data-generating process be

                               yi = β1 (ui ) + x2i β2 (ui ) + x3i β3 (ui ) + εi

where N = 100, 000, the conditional quantile ui of each individual is u ∼ U [0, 1], and
the covariates are distributed as independent lognormal random variables, i.e. x2i , x3i ∼
LN (0, 1). The coefficient vector is a function of the conditional quantile ui of individual i
                                                            
                                β1 (u)            1 + 3u − u2
                              β2 (u)  =  exp(u)  .
                                                            
                                                       √
                                β3 (u)                   u
In our baseline scenario, we draw mean-zero measurement error ε from a mixed normal
distribution                 
                             N (−3, 1) with probability 0.5
                             
                             
                         εi ∼ N (2, 1)   with probability 0.25                 (4.1)
                             
                             
                               N (4, 1)  with probability 0.25.
                             

To simulate robustness to real-world settings in which the econometrician does not know
the true distribution of the residuals, we also present results simulating measurement error
from alternative distributions and test how well quasi-MLE modeling the error distribution
as a Gaussian mixture accommodates misspecification in Fε .10 We use a genetic-algorithm
optimizer to find the maximizer of the log-likelihood function defined in Section 3 with

     10InAppendix B, we examine the performance of a weighted least squares EM algorithm when the
measurement error is normally distributed. While estimating a mixture model allows for an arbitrary amount
of measurement-error distributional flexibility by increasing the number of mixture components, we are also
interested in more parsimonious specifications that may be computationally attractive to applied researchers
willing to make parametric assumptions on the data-generating process. As discussed in Appendix B, if
the measurement error is normally distributed, the estimand reduces to a weighted-least squares objective
function, similar to how linear MLE is equivalent to OLS in the case of normally distributed stochastic terms.
           ERRORS IN THE DEPENDENT VARIABLE OF QUANTILE REGRESSION MODELS                                 15

start values provided by a gradient-based constrained optimizer. For the start values of the
distributional parameters, we place equal 1/3 weights on each mixture component, with unit
variance and means -1, 0, and 1.
   In Figures 1-3, we plot the true coefficient function defined above, average coefficients
from quantile regression, and our MLE estimator and associated 95% confidence intervals
using a sieve for β(·) consisting of 15 knots. The plotted confidence intervals are calculated
pointwise as β(τ
               b j )±1.96b σj , where σ
                                      bj is the standard deviation across simulations of parameter
                  11
estimates β(τj ). To test the validity of our bootstrapped confidence intervals, we further
             b
calculated bootstrap confidence intervals for each simulation using the procedure described
in section 3.2 and calculated the fraction of simulations for which the true parameter lied
within the bootstrapped confidence interval. We found that our confidence intervals had
a coverage of 98%, suggesting them to be slightly conservative on average.12 Focusing on
Figures 2 and 3 that plot estimates of the slope coefficients β2 (·) and β3 (·), quantile regression
estimates are badly biased, with lower quantiles biased upwards and upper quantiles biased
downwards. In contrast, the ML estimates fall almost directly on top of the true parameter
functional, and the bias of the ML estimator is statistically indistinguishable from zero at
all quantiles. The average absolute bias for the ML estimates is 0.6% and 1.5% of the true
coefficients for β2 (·) and β3 (·) respectively, and always less than 4% of the true magnitude.
By contrast, the mean bias of the quantile regression coefficients is 12% and 22% for the two
slope coefficients and exceeds 100% for some quantiles. Appendix Table C1 confirms that
the quantile-regression average absolute bias is 26 and 16 times larger than the MLE bias
for β2 (·) and β3 (·), respectively. Appendix Table C1 further reports MSE results, showing
that the average MSE is an order of magnitude smaller for the ML estimates than the
quantile-regression estimates.
   Figure 1 plots estimates of the intercept term, showing that quantile-regression estimates
of β1 (·) are badly biased. Given that quantile-regression estimated intercepts ensure that the
τ th conditional quantile of the residuals Qεb(τ ) = 0, when the slope coefficients are biased,
this exacerbates the bias in the constant function. Whereas the mean absolute bias of the
ML estimates of β1 (·) is 2% of the true magnitude, quantile regression has a mean absolute
bias of 116% of the true β1 (·) functional.
   Figure 4 shows the true mixed-normal distribution of the measurement error ε as defined
above (dashed blue line) plotted with the estimated distribution of the measurement error

  11We   estimate the critical value for simultaneous confidence intervals to be 2.92, roughly 50% wider than
pointwise confidence intervals.
   12To further demonstrate the performance of the bootstrapped confidence intervals (and in particular
the asymptotic results in Theorem 2), we varied the sample size in the simulations, holding the number of
knots fixed, and calculated how the width of the pointwise 95% √ confidence intervals changed. Decreasing the
sample size from 50,000 to 10,000 observations—a decrease in n by a factor of 2.24—increased the width
of the confidence intervals for both β1 and β2 (averaged across quantiles) by a factor of 2.25.
16                                 HAUSMAN, LIU, LUO, AND PALMER



                      Figure 1. Monte Carlo Simulation Results: βb1 (τ )

          8
          6
          4
          2
          0
          -2
          -4




               0     .1       .2      .3     .4     .5       .6    .7     .8      .9     1
                                                  Quantile

                     True β           Quantile Regression         MLE          MLE 95% CI



Notes: Figure plots the true β1 (τ ) = 1 + 3τ − τ 2 (blue line) against quantile-regression estimates
(green circles), bias-corrected MLE (red xs), and 95% confidence intervals for the MLE estimates
(dashed red lines) from 100 MC simulations using the data-generating process described in the text
with the measurement error generated as a mixture of three normals.


from the average estimated distributional parameters across all MC simulations (solid red
line). The 95% confidence interval of the estimated density (dotted green line) are estimated
pointwise as the 2.5th and 97.5th percentiles of EIV densities across all simulations. Despite
the bimodal nature of the true measurement error distribution, our algorithm captures the
overall features of true distribution well, with the true density always within the confidence
interval for the estimated density.
   In practice, the econometrician seldom has information on the distribution family to which
the measurement error belongs. To probe robustness on this dimension, we demonstrate the
flexibility of the Gaussian mixture-of-three specification by showing that it accommodates
alternative errors-in-variables data-generating processes well. Table 2 shows that when the
errors are distributed as a t distribution with three degrees of freedom (normalized to have
the same variance as in (4.1)) in panel I or as a Laplace (with λ = 2.29 to again have the
same variance across ε DGPs) in panel II, the ML estimates that model the EIV distribution
as a mixture of three normals still significantly outperform quantile regression. As expected,
          ERRORS IN THE DEPENDENT VARIABLE OF QUANTILE REGRESSION MODELS                            17



                      Figure 2. Monte Carlo Simulation Results: βb2 (τ )

          3
          2.5
          2
          1.5
          1
          .5




                0     .1       .2    .3      .4     .5       .6    .7      .8      .9      1
                                                  Quantile

                      True β          Quantile Regression         MLE           MLE 95% CI



Notes: Figure plots the true β2 (τ ) = exp(τ ) (blue line) against quantile-regression estimates (green
circles), bias-corrected MLE (red xs), and 95% confidence intervals for the MLE estimates (dashed
red lines) from 100 MC simulations using the data-generating process described in the text with the
measurement error generated as a mixture of three normals.


quantile regression is again biased towards the median under both distributions and for both
slope coefficients (visible as positive mean bias for quantiles below the median and negative
bias for quantiles above the median). By comparison, ML estimates are generally much
less biased than quantile regression for both data-generating processes. Our ML framework
easily accommodates mixtures of more than three normal components for additional distri-
butional flexibility in a quasi-MLE approach. Appendix C provides additional simulation
results—including both mean bias and MSE—for alternative measurement error distribu-
tions, when x2 is binary, and when β(·) is estimated using a finer sieve space (99 knots).




                                    5. Empirical Application

  To illustrate the use of our estimator in practice, we examine distributional heterogeneity
in the wage returns to education. First, we estimate the quantile-regression analog of a
18                                 HAUSMAN, LIU, LUO, AND PALMER



                     Figure 3. Monte Carlo Simulation Results: βb3 (τ )

          1.2
          1
          .8
          .6
          .4
          .2
          0




                0    .1       .2      .3     .4     .5       .6    .7    .8      .9       1
                                                  Quantile

                     True β           Quantile Regression         MLE         MLE 95% CI


                                        √
Notes: Figure plots the true β3 (τ ) = τ (blue line) against quantile-regression estimates (green
circles), bias-corrected MLE (red xs), and 95% confidence intervals for the MLE estimates (dashed
red lines) from 100 MC simulations using the data-generating process described in the text with the
measurement error generated as a mixture of three normals.




Mincer regression, replicating and extending results from Angrist et al. (2006)

       Qy|x (τ ) = β1 (τ ) + β2 (τ )educationi + β3 (τ )experiencei + β(τ )experience2i       (5.1)

where Qy|x (τ ) is the τ th quantile of the conditional (on the covariates x) log-wage distribu-
tion, and the education and experience variables are measured in years. In contrast to the
linear Mincer equation, quantile regression assumes that all unobserved heterogeneity enters
through the unobserved rank of person i in the conditional wage distribution. The presence
of an additive error term, which could include both measurement error and wage factors
unobserved by the econometrician, would bias the estimation of the coefficient function β(·).
   Appendix Figure E1 plots quantile-regression estimates of equation (5.1) using census
microdata samples from four decennial census years: 1980, 1990, 2000, and 2010, along with
               ERRORS IN THE DEPENDENT VARIABLE OF QUANTILE REGRESSION MODELS                             19


      Figure 4. Monte Carlo Simulation Results: Distribution of Measurement Error

               .25
               .2.15
          Density
               .1
               .05
               0




                       −8   −6      −4        −2       0          2           4         6         8
                                                Measurement Error

                             True Density            Estimated Density               MLE 95% CI


Notes: Figure reports the true measurement error (dashed blue line), a mean-zero mixture of three
normals ( N (−3, 1), N (2, 1), and N (4, 1) with weights 0.5, 0.25, and 0.25, respectively) against
the average density estimated from the 100 Monte Carlo simulations (solid red line). For each grid
point, the dotted green line plots the 2.5th and 97.5th percentile of the EIV density function across
all MC simulations.

pointwise confidence intervals.13 Consistent with Angrist et al. (2006), we find quantile-
regression evidence that heterogeneity in the returns to education across the conditional
wage distribution has increased over time. Adding data from 2010 shows a large jump in the
returns to education for the entire distribution, with top conditional incomes increasing much
less from 2000 to 2010 than bottom conditional incomes. Still, the post-1980 convexity of the
education-wage gradient is readily visible in the 2010 results, with wages in the top quartile
of the conditional distribution being much more sensitive to years of schooling than the rest




  13For  further details on the data including summary statistics, see Appendix D. For comparability with
Angrist et al. (2006) and to have a sufficient number of observations to run our estimator, we focus on prime
age white males (aged 40-49). In Appendix D, we provide evidence that other demographic groups have
markedly different patterns of heterogeneity in the education-wage gradient across the conditional income
distribution, motivating further study on treatment effect heterogeneity.
20                                 HAUSMAN, LIU, LUO, AND PALMER

         Table 2. MC Simulation Mean Bias: Robustness to Alternative Data-
         Generating Processes

                                            I. ε ∼ t                   II. ε ∼ Laplace
                                     β2                β3             β2             β3
                 Quantile       QR        MLE     QR        MLE     QR MLE QR MLE
                   0.1          0.14       0.04   0.10       0.03   0.17     0.01    0.13   -0.01
                   0.2          0.11       0.00   0.05      -0.01   0.12     0.01    0.05   -0.01
                   0.3          0.09      -0.04    0.02      0.00   0.09     0.00    0.02    0.05
                   0.4          0.06       0.03   0.00       0.01   0.06     0.02    0.00    0.00
                   0.5          0.03       0.02   -0.02     -0.02   0.03    -0.03   -0.02   -0.03
                   0.6          0.00      -0.02   -0.03      0.00   -0.01    0.00   -0.03   -0.01
                   0.7         -0.05      -0.04   -0.05     -0.01   -0.05   -0.04   -0.05    0.00
                   0.8         -0.11      -0.01   -0.06     -0.01   -0.13   -0.01   -0.07    0.00
                   0.9         -0.20      -0.02   -0.08     -0.02   -0.24    0.00   -0.10   -0.03
                  |Bias|        0.09       0.02   0.05       0.01   0.10     0.01    0.05    0.02
Notes: Table reports mean bias of slope coefficients for estimates from classical quantile regression
and bias-corrected MLE modeling the error term as a mixture of three normals across 100 MC
simulations of N = 100, 000 observations each. The data are simulated from the data-generating
process described in the text and the measurement error generated
                                                                √     by either a Student’s t distri-
bution (panel I) with three degrees of freedom (normalized by 3.5 or a Laplace distribution with
λ = 2.29 such that both data-generating processes result in measurement errors with the same
variance (10.5) as in the original data-generating process in (4.1). The last row reports the mean
absolute bias over the nine quantiles listed above.

of the distribution.14 In 2010, the education coefficient for the 95th percentile percentile was
six log points higher than the education coefficient for the 5th percentile. Note, too, that
traditional quantile regression estimates become quite unstable at the highest wage quantiles,
characterized as the extremal quantiles problem by Chernozhukov (2005).
   We observe a different pattern when we correct for measurement-error bias in the self-
reported wages in the census data. Figure 5 plots the education coefficient βb2 (τ ) from
estimating equation (5.1) by MLE and quantile regression. We approximate β(·) with a
piecewise linear function consisting of 15 knots using our sieve-ML estimator developed in
Section 3. We construct 95% bootstrapped confidence intervals pointwise as βb2 (τj ) ± 1.96b  σj
where σbj is the empirical standard deviation of bootstrapped estimates of β2 (τj ).
                                                                              b
   In each year, quantile regression estimates understate the returns to education at the top
of the conditional wage distribution relative to ML estimates. A formal test of the joint
equality across the grid of 15 knots of QR and ML coefficients rejects equality of the ed-
ucation coefficient function for each year except 1990. For 1980, the quantile-regression


     14That
          the wage-education gradient varies significantly with the quantile of the wage distribution suggests
that average or local average treatment effects estimated from linear estimators fail to represent the returns
to education for a sizable portion of the population.
              ERRORS IN THE DEPENDENT VARIABLE OF QUANTILE REGRESSION MODELS                                                21


              Figure 5. Returns to Education Correcting for LHS Measurement Error

                                   1980                                                       1990




                                                                 .4
    .3




                                                                 .3
    .2




                                                                 .2
    .1




                                                                 .1
    0




                                                                 0
    −.1




          0   .1   .2   .3    .4     .5 .6    .7   .8   .9   1        0   .1   .2   .3   .4     .5 .6    .7   .8   .9   1
                                   Quantile                                                   Quantile


                                   2000                                                       2010



                                                                 .6
    .6
    .4




                                                                 .4
    .2




                                                                 .2
    0
    −.2




                                                                 0




          0   .1   .2   .3    .4     .5 .6    .7   .8   .9   1        0   .1   .2   .3   .4     .5 .6    .7   .8   .9   1
                                   Quantile                                                   Quantile


                             MLE                    Quantile Regression                          MLE 95% CI


Notes: Figure reports quantile regression (red lines) and maximum likelihood estimates (dotted blue
lines) of (self-reported) log weekly wages on education and a quadratic in experience. Dashed blue
lines plot 95% pointwise confidence intervals from 500 bootstrap iterations. The data comes from
the indicated decennial census year and consist of 40-49 year old white men with positive wages
born in America. The number of observations in each sample is 60,051, 80,115, 90,201, and 98,292
in 1980, 1990, 2000, and 2010, respectively.


estimates show relatively constant returns to education across the conditional wage distri-
bution, with a sharp decline at the very top characteristic of quantile-regression estimates at
extremal quantiles. The ML estimates feature more convexity, with the pattern of increasing
returns to education for higher quantiles seen in quantile-regression estimates in later years
visible in the ML estimates for 1980. In 1990, the quantile-regression estimates are less af-
fected by measurement error in the sense that the classical quantile-regression estimates and
bias-corrected ML estimates are nearly indistinguishable given the typically wide confidence
intervals for extremal quantiles, and we fail to reject equality of QR and ML estimates.
   In the 2000 sample, the quantile-regression and ML estimates of the returns to education
again diverge for top incomes, with the point estimate suggesting that after correcting for
22                                             HAUSMAN, LIU, LUO, AND PALMER


                                  Figure 6. ML Estimated Returns to Education Across Years


                     .45
                     .4
           .15 .2 .25 .3 .35
          Education Coefficient
                     .1
                     .05
                     0




                                  0     .1    .2    .3    .4       .5       .6     .7   .8   .9     1
                                                                 Quantile

                                             1980              1990              2000        2010



Notes: Figure overlays ML estimates of the returns to education across the conditional wage distri-
bution from Figure 5. See notes to Figure 5 for details.



measurement error in self-reported wages, the true returns to an additional year of education
for 98th percentile of the conditional wage distribution is 15 log points (17 percentage points)
higher than estimated by classical quantile regression. This bias correction affects the amount
of inequality estimated in the education-wage gradient, with the ML estimates implying
that top wage earners gained 27 log points (31 percentage points) more from a year of
education than workers in the bottom three quartiles of wage earners. For 2010, both ML and
classical quantile-regression estimates agree that the returns to education increased across all
quantiles, but again disagree about the marginal returns to schooling for top wage earners.
The quantile regression estimates at the very top of the conditional wage distribution are
again outside the 95% confidence intervals for the ML estimates.
   For each year besides 1990, the quantile regression lines understate the returns to education
in the top decile of the wage distribution. Correcting for measurement error in self-reported
wages generally increases the estimated returns to education for the top quintile of the
conditional wage distribution, a distinction that is missed because of the compression bias in
the quantile regression coefficients. Figure 6 overlays each year’s ML estimates to facilitate
easier comparisons across years. The returns to education have varied significantly over time.
           ERRORS IN THE DEPENDENT VARIABLE OF QUANTILE REGRESSION MODELS                                    23

Each decade—with the exception of 1990-2000—we see an increase in the returns to education
broadly enjoyed across the wage distribution. However, the increase in the education-wage
gradient is relatively constant across the bottom nine deciles and very different for the top
decile.
   These two trends—constant, moderate increases for the bottom three quartiles and acute
increases in the schooling coefficient for top earners—are consistent with the observations of
Angrist et al. (2006) and other work on inequality (e.g., Autor et al., 2008) that finds sig-
nificant increases in income inequality post-1980. Nevertheless, the distributional story that
emerges from correcting for measurement error suggests that the concentration of education-
linked wage gains for top earners is even more substantial than is apparent in previous work.
This finding is particularly relevant for recent discussions of the role of education in income
inequality (Goldin and Katz, 2009), the rise in top-income inequality (see, for example,
Piketty and Saez, 2006), and the increasing returns to cognitive performance (Lin et al.,
2016).15
   Our methodology also permits a characterization of the distribution of dependent-variable
measurement error. Figure 7 plots the estimated distribution of the measurement error
by census year. Despite the flexibility afforded by the mixture specification, the estimated
density is unimodal but somewhat skewed with negative excess kurtosis (thinner tails) than
the density of a single normal. Over time, the variance in the measurement error is increasing,
consistent with recent concerns about declining response rates and a potential deterioration
in the reliability of large-scale survey data (see, e.g., Bound et al., 2001; Brick and Williams,
2013; Meyer et al., 2015).

                                             6. Conclusion

   In this paper, we develop a methodology for estimating the functional parameter β(·)
in quantile regression models when there is measurement error in the dependent variable.
Assuming that the measurement error follows a distribution that is known up to a finite-
dimensional parameter, we establish general convergence-speed results for the MLE-based
approach. Under an assumption about the degree of ill-posedness of the problem (Assump-
tion 5), we establish the convergence speed of the sieve-ML estimator. We prove the validity
of bootstrapping based on asymptotic normality of our estimator and suggest using a boot-
strap procedure for inference. Monte Carlo results demonstrate substantial improvements
in mean bias and MSE relative to classical quantile regression when there are modest errors


   15Our  results here are not causal given that we are using observational variation in education as in Angrist
et al. (2006). IV QR techniques (e.g., Chernozhukov and Hansen, 2005) could be adapted to our setting.
We note that the IV literature on the returns to education has found larger effects after addressing the
endogeneity of education (e.g., Griliches, 1977; Angrist and Krueger, 1991; Card, 2001).
24                                 HAUSMAN, LIU, LUO, AND PALMER


                   Figure 7. Estimated Distribution of Wage Measurement Error


            .8
            .6
         Density
           .4
            .2
            0




                    -2    -1.5      -1     -.5      0         .5          1   1.5      2
                                             Measurement Error

                                 1980          1990                2000         2010


Note: Graph plots the estimated probability density function of the measurement error each year
when specified as a mixture of three normal distributions.



in the dependent variable, highlighted by the ability of our estimator to estimate the simu-
lated underlying measurement error distribution (a bimodal mixture of three normals) with
a high-degree of accuracy.
   Finally, we revisited the Angrist et al. (2006) question of whether the returns to education
across the wage distribution have been changing over time. We find a somewhat different
pattern than prior work, highlighting the importance of correcting for errors in the dependent
variable of conditional quantile models. When we correct for likely measurement error in
self-reported wage data, we find that top wages have grown more sensitive to education
than wages in the rest of the conditional wage distribution, an important potential source of
secular trends in income inequality.



                                           References

An, Y., and Y. Hu (2012): “Well-posedness of measurement error models for self-reported
 data,” Journal of Econometrics, 168(2), 259–269.
         ERRORS IN THE DEPENDENT VARIABLE OF QUANTILE REGRESSION MODELS                25

Angrist, J., V. Chernozhukov, and I. Fernández-Val (2006): “Quantile regression
  under misspecification, with an application to the US wage structure,” Econometrica,
  74(2), 539–563.
Angrist, J., and A. Keueger (1991): “Does compulsory school attendance affect school-
  ing and earnings?,” The Quarterly Journal of Economics, 106(4), 979–1014.
Arellano, M., and M. Weidner (2016): “Instrumental Variable Quantile Regressions in
  Large Panels with Fixed Effects,” Working Paper.
Autor, D. H., L. F. Katz, and M. S. Kearney (2008): “Trends in US wage inequality:
  Revising the revisionists,” The Review of Economics and Statistics, 90(2), 300–323.
Bound, J., C. Brown, and N. Mathiowetz (2001): “Measurement error in survey data,”
  Handbook of Econometrics, 5, 3705–3843.
Brick, J. M., and D. Williams (2013): “Explaining rising nonresponse rates in cross-
  sectional surveys,” The ANNALS of the American academy of political and social science,
  645(1), 36–59.
Burda, M., M. Harding, and J. Hausman (2008): “A Bayesian mixed logit–probit model
  for multinomial choice,” Journal of Econometrics, 147(2), 232–246.
          (2012): “A Poisson mixture model of discrete choice,” Journal of Econometrics,
  166(2), 184–203.
Card, D. (2001): “Estimating the return to schooling: Progress on some persistent econo-
  metric problems,” Econometrica, 69(5), 1127–1160.
Chen, X. (2007): “Large sample sieve estimation of semi-nonparametric models,” Handbook
  of Econometrics, 6, 5549–5632.
Chen, X., and D. Pouzo (2013): “Sieve Quasi Likelihood Ratio Inference on
  Semi/nonparametric Conditional Moment Models,” Cowles Foundation Discussion Paper
  #1897.
Chernozhukov, V. (2005): “Extremal quantile regression,” Annals of Statistics, pp. 806–
  839.
Chernozhukov, V., and C. Hansen (2005): “An IV model of quantile treatment effects,”
  Econometrica, 73(1), 245–261.
Cosslett, S. R. (2004): “Efficient Semiparametric Estimation of Censored and Truncated
  Regressions via a Smoothed Self-Consistency Equation,” Econometrica, 72(4), 1277–1293.
Dempster, A. P., N. M. Laird, D. B. Rubin, et al. (1977): “Maximum likelihood from
  incomplete data via the EM algorithm,” Journal of the Royal Statistical Society, 39(1),
  1–38.
DiNardo, J., N. M. Fortin, and T. Lemieux (1996): “Labor Market Institutions and
  the Distribution of Wages, 1973-1992: A Semiparametric Approach,” Econometrica, 64(5),
  1001–1044.
26                           HAUSMAN, LIU, LUO, AND PALMER

Evdokimov, K. (2010): “Identification and Estimation of a Nonparametric Panel Data
  Model with Unobserved Heterogeneity,” Princeton University Working Paper.
Fan, J. (1991): “On the optimal rates of convergence for nonparametric deconvolution
  problems,” The Annals of Statistics, pp. 1257–1272.
Goldin, C. D., and L. F. Katz (2009): The Race Between Education and Technology.
  Harvard University Press.
Griliches, Z. (1977): “Estimating the returns to schooling: Some econometric problems,”
  Econometrica, pp. 1–22.
Hausman, J. (2001): “Mismeasured variables in econometric analysis: problems from the
  right and problems from the left,” Journal of Economic Perspectives, 15(4), 57–68.
Hausman, J. A., J. Abrevaya, and F. M. Scott-Morton (1998): “Misclassification
  of the dependent variable in a discrete-response setting,” Journal of Econometrics, 87(2),
  239–269.
Horowitz, J. L., and S. Lee (2005): “Nonparametric estimation of an additive quantile
  regression model,” Journal of the American Statistical Association, 100(472), 1238–1249.
Koenker, R., and G. Bassett Jr (1978): “Regression quantiles,” Econometrica, pp.
  33–50.
Lin, D., R. Lutter, and C. J. Ruhm (2016): “Cognitive Performance and Labor Market
  Outcomes,” NBER Working Paper #22470.
Meyer, B. D., W. K. Mok, and J. X. Sullivan (2015): “Household surveys in crisis,”
  Journal of Economic Perspectives, 29(4), 199–226.
Newey, W. K., and D. McFadden (1994): “Large sample estimation and hypothesis
  testing,” Handbook of Econometrics, 4, 2111–2245.
Piketty, T., and E. Saez (2006): “The Evolution of Top Incomes: A Historical and
  International Perspective,” The American Economic Review, pp. 200–205.
Powell, D. (2013): “A new framework for estimation of quantile treatment effects: Nonsep-
  arable disturbance in the presence of covariates,” RAND Working Paper Series WR-824-1.
Ruggles, S., K. Genadek, R. Goeken, J. Grover, and M. Sobek (Minneapolis: Uni-
  versity of Minnesota, 2015): “Integrated Public Use Microdata Series Version 6.0 [Machine-
  readable database],” .
Schennach, S. M. (2008): “Quantile regression with mismeasured covariates,” Econometric
  Theory, 24(04), 1010–1043.
Van der Vaart, A. W. (2000): Asymptotic Statistics, vol. 3. Cambridge University Press.
Van Der Vaart, A. W., and J. A. Wellner (1996): “Weak convergence,” in Weak
  Convergence and Empirical Processes, pp. 16–28. Springer.
Wei, Y., and R. J. Carroll (2009): “Quantile regression with measurement error,” Jour-
  nal of the American Statistical Association, 104(487).
          ERRORS IN THE DEPENDENT VARIABLE OF QUANTILE REGRESSION MODELS                                    27

                               Appendix A. Bias Characterization

 In this section, we prove compression bias for the quantile regression slope coefficient. We
make the following assumptions:

    (1) Besides the constant, there is one covariate x, which is nonnegative and strictly
        positive with a positive probability.
    (2) Let β1 (τ ) and β2 (τ ) denote the true constant and slope coefficient functions. We
        assume that β2 (τ ) is not a constant, i.e. minτ β2 (τ ) < maxτ β2 (τ ). We also assume
        that with a positive probability, β2 (τ ) is strictly greater than minτ β2 (τ ) and strictly
        smaller than maxτ β2 (τ ).
    (3) We assume that the true data generating process is y = β1 (τ ) + β2 (τ )x + ε, where the
        EIV ε has a positive probability density everywhere between −∞ and ∞.

Let βb1 (τ0 ) and βb2 (τ0 ) denote the estimated constant and slope coefficients at τ0 . In the fol-
lowing, we will show that with left-hand side measurement error ε, minτ β2 (τ ) < βb2 (τ0 ) <
maxτ β2 (τ ) holds for every τ0 . In other words, the quantile-regression estimated slope coeffi-
cient is always strictly bounded by the lower and upper bounds of the true slope coefficient
function. We first write out the first-order conditions for βb1 (τ0 ) and βb2 (τ0 ) respectively:
                                    h                                i
                             Ex,τ,ε 1(y − βb1 (τ0 ) − βb2 (τ0 )x < 0) = τ0
                                  h                                  i
                            Ex,τ,ε x1(y − βb1 (τ0 ) − βb2 (τ0 )x < 0) = τ0 E[x]

where Ex,τ,ε [·] denotes an expectation taken over the domains of x, τ , and ε. Using iterated
expectations, the first-order conditions can be written as

                                              Ex [ατ0 (x)] = τ0                                          (A.1)
                                          Ex [xατ0 (x)] = τ0 E[x],                                       (A.2)
where
                h                                i
  ατ0 (x) = Eτ,ε 1 y − βb1 (τ0 ) − βb2 (τ0 )x < 0
                h                                                 i
          = Eτ,ε 1 ε < βb1 (τ0 ) − β1 (τ ) + (βb2 (τ0 ) − β2 (τ ))x
                h                                                                                  i
          = Eτ,ε 1 ε < βb1 (τ0 ) − β1 (τ ) + (βb2 (τ0 ) − min β2 (τ ))x + ((min β2 (τ )) − β2 (τ ))x     (A.3)
                                                          τ                  τ

   We prove that βb2 (τ0 ) > minτ β2 (τ ) by contradiction. Suppose that β(τ
                                                                           b 0 ) ≤ minτ β2 (τ ).
Then the slope for x inside (A.3) is nonpositive for every τ and negative for some τ by
the assumption that β2 (τ ) is not everywhere equal to its minimum. This together with the
assumption that ε has a positive probability density everywhere implies that ατ0 (x) is a
strictly decreasing function of x. However, the monotonicity of ατ0 (x) causes a contradiction
28                             HAUSMAN, LIU, LUO, AND PALMER

to (A.1) and (A.2). (A.1) claims that the mean of ατ0 (x) over the range of x is τ0 . The left-
hand side of (A.2) is a weighted average of ατ0 (x) over the range of x, where the average
weight is E[x], and the weight increases as x increases. Since ατ0 (x) is strictly decreasing, the
weighted average in (A.2) must be smaller than the average weight times the mean of ατ0 (x).
In other words, the left-hand side of (A.2) must be smaller than τ0 E[x] and cannot be equal
to τ0 E[x]. This causes a contradiction to (A.2). By a similar argument, βb2 (τ0 ) < maxτ β2 (τ ).
Therefore,
                                min β2 (τ ) < βb2 (τ0 ) < max β2 (τ ),
                                  τ                        τ
which we refer to as compression bias because the estimated parameters strictly lie in the
interior of their true maximum and minimum values over τ ∈ [0, 1].
            ERRORS IN THE DEPENDENT VARIABLE OF QUANTILE REGRESSION MODELS                                       29

                                     Appendix B. Weighted Least Squares

   Iterative weighted-least squares is a computationally attractive estimator that is theoret-
ically equivalent to MLE when the true DGP is for ε to be distributed as a single normal
random variable. WLS estimates provide useful alternative start values.
   Under a normality assumption of the EIV term ε, the maximization of Q(·|θ) reduces to the
minimization of a weighted least squares problem.16 Suppose the disturbance ε ∼ N (0, σ 2 ).
Then the maximization problem (3.5) becomes the following, with the parameter vector
θ = [β(·), σ]
                                        0                         0         0
                                                                                           
                                max
                                 0
                                    Q(θ   |θ) := E  log(f (y − xβ   (τ ))|θ   )w(x, y, θ)|θ                   (B.1)
                                 θ
                     "Z                                                                                  #
                            1
                                     f (y − xT β(τ )|σ)                         (y − xβ 0 (τ ))2
                                                                                                  
                                                                1
                =E                                             − log(2πσ 02 ) −                        dτ .
                                                                                    2σ 02
                                R1
                        0            f (y − xT β(u)|σ)du        2
                                 0
   It is apparent from the above equation that the maximization problem of β 0 (·)|θ is to
minimize the sum of weighted least squares. As in standard normal MLE, the FOC for β 0 (·)
does not depend on σ 02 . The σ 02 is solved after all the β 0 (τ ) are solved from equation (B.1).
Therefore, the estimand can be implemented with an EM algorithm that reduces to iteration
on weighted least squares, which is both computationally tractable and easy to implement
in practice.
   Given an initial estimate W
                             c of a weighting matrix W , the weighted least squares estimates
of β and σ are
                                             b j ) = (X 0 W
                                             β(τ          cj X)−1 X 0 W
                                                                      cj y
                                                     s
                                                          1 XX
                                                 σ
                                                 b =                wbij εb2ij
                                                       NJ
                                                                     j    i

where W cj is the diagonal matrix formed from the j th column of W c , which has elements w bij .
   Given estimates εbj = y − X β(τ
                               b j ) and σ
                                         b, the weights w
                                                        bij for observation i in the estimation
of β(τj ) are
                                                               φ (εbij /σ
                                                                        b)
                                                  w
                                                  bij =    1   P                                              (B.2)
                                                           J     j φ (εbij /σ
                                                                            b)
where φ(·) is the PDF of a standard normal distribution J is the number of τ s in the sieve,
e.g. J = 9 if the quantile grid is {τj } = {0.1, 0.2, ..., 0.9}.
   Figures B1 and B2 show results from estimating the coefficient vectors β2 (·) and β3 (·)
from iterated WLS when the DGP is taken to be ε ∼ N (0, 10.5) to match the variance
                                                                                  q       in
the other simulation designs, along with confidence intervals constructed as ±1.96 Vdar(β)
                                                                                         b
where Vd    b is the empirical variance of the coefficient estimates across MC simulations.
        ar(β)
The results confirm that WLS is a successful alternative to MLE when the data is normal.

  16See,   also, the related method in Section 4.6 of Dempster et al. (1977).
30                               HAUSMAN, LIU, LUO, AND PALMER



     3
     2.5
     2
     1.5
     1     Figure B1. Weighted Least Squares Monte Carlo Simulation Results: βb2 (τ )




           0      .1       .2     .3       .4      .5        .6       .7       .8       .9        1
                                                 Quantile

                  True β           Quantile Regression             WLS              WLS 95% CI


Notes: Figure plots the true β2 (τ ) = exp(τ ) (blue line) against quantile-regression estimates (green
circles), weighted least squares estimates (red xs), and 95% confidence intervals for the WLS esti-
mates (dashed red lines) from 100 MC simulations with 40 WLS iterations each. The data-generating
process is described in the text with the measurement error generated as a normal random variable
distributed N (0, 10.5).
            ERRORS IN THE DEPENDENT VARIABLE OF QUANTILE REGRESSION MODELS                          31



   1
   .8
   .6
   .4
   .2
   0    Figure B2. Weighted Least Squares Monte Carlo Simulation Results: βb3 (τ )




        0       .1        .2      .3       .4     .5        .6       .7       .8       .9       1
                                                Quantile

                 True β           Quantile Regression             WLS              WLS 95% CI


                                          √
Notes: Figure plots the true β3 (τ ) = τ (blue line) against quantile-regression estimates (green
circles), weighted least squares (red xs), and 95% confidence intervals for the WLS estimates (dashed
red lines) from 100 MC simulations with 40 WLS iterations each. The data-generating process is
described in the text with the measurement error generated as a normal random variable distributed
N (0, 10.5).
32                             HAUSMAN, LIU, LUO, AND PALMER

                       Appendix C. Additional Simulation Results

   In this appendix, we present Monte Carlo simulation results (mean bias and MSE) under
alternative data generating processes. For each design, quasi-ML estimation continues to
treat the measurement error as a mixture of three normals. After simulating measurement
error under alternative measurement error distributions (all normalized such that ε has equal
variance across designs), Appendix Tables C5 and C6, respectively, present results when x1
is binary (normalized to have equal variance across simulation designs) and when a 99-knot
sieve is used to approximate β(·).




                 Table C1. Mean Bias and Mean Squared Error: ε ∼ 3N
                                 β1           β2           β3
                  Quantile QR       MLE   QR     MLE   QR     MLE
                                         I. Mean Bias
                    0.1    -2.924 -0.020 0.145 0.003 0.134 0.010
                    0.2    -2.486 0.022 0.223 -0.003 0.144 -0.014
                    0.3    -2.074 -0.005 0.265 0.011 0.132 0.009
                    0.4    -1.510 -0.013 0.248 0.009 0.089 0.002
                    0.5    -0.402 -0.075 0.101 0.007 -0.012 0.011
                    0.6    1.055 -0.023 -0.123 0.012 -0.120 0.001
                    0.7    1.939 0.002 -0.238 0.011 -0.141 -0.005
                    0.8    2.601 0.047 -0.285 -0.001 -0.125 -0.010
                    0.9    3.355 0.078 -0.284 -0.019 -0.097 -0.002
                   |Bias|  2.038 0.032 0.213 0.008 0.110 0.007

                                         II. Mean Squared Error
                    0.1       8.548   0.042 0.021 0.005 0.018           0.006
                    0.2       6.179   0.020 0.050 0.002 0.021           0.003
                    0.3       4.302   0.037 0.070 0.007 0.018           0.008
                    0.4       2.280   0.019 0.062 0.004 0.008           0.003
                    0.5       0.164   0.040 0.011 0.010 0.000           0.004
                    0.6       1.113   0.018 0.015 0.004 0.014           0.003
                    0.7       3.761   0.028 0.057 0.008 0.020           0.004
                    0.8       6.767   0.019 0.082 0.003 0.016           0.001
                    0.9      11.259   0.034 0.081 0.005 0.010           0.002
                    MSE       4.930   0.029 0.050 0.005 0.014           0.004
Notes: Table reports mean bias (panel I) and MSE (panel II) for estimates from classical quantile
regression (QR) and MLE across 100 MC simulations of N = 100, 000 observations using data
simulated from the data-generating process described in Section 4. The last row reports the mean
absolute bias (panel I) and the mean MSE (panel II) over the nine quantiles listed above.
          ERRORS IN THE DEPENDENT VARIABLE OF QUANTILE REGRESSION MODELS                        33


                 Table C2. Mean Bias and Mean Squared Error: ε ∼ 2N
                                 β1           β2           β3
                  Quantile QR       MLE   QR     MLE   QR     MLE
                                         I. Mean Bias
                    0.1    -3.894 0.049 0.237 0.001 0.205 0.014
                    0.2    -3.082 0.010 0.342 -0.001 0.206 -0.008
                    0.3    -1.663 -0.027 0.259 0.022 0.096 0.006
                    0.4    0.537 0.004 -0.049 0.003 -0.105 -0.002
                    0.5    1.184 0.012 -0.141 -0.004 -0.125 -0.007
                    0.6    1.536 0.007 -0.182 0.009 -0.111 0.001
                    0.7    1.807 -0.080 -0.196 0.022 -0.090 0.000
                    0.8    2.077 -0.025 -0.192 0.010 -0.070 0.002
                    0.9    2.457 -0.069 -0.174 0.020 -0.055 0.002
                   |Bias|  2.026 0.031 0.197 0.010 0.118 0.005

                                       II. Mean Squared Error
                    0.1      15.163 0.036 0.057 0.005 0.042              0.007
                    0.2       9.500 0.020 0.117 0.002 0.043              0.002
                    0.3       2.768 0.073 0.067 0.016 0.009              0.008
                    0.4       0.289 0.026 0.003 0.005 0.011              0.002
                    0.5       1.403 0.079 0.020 0.017 0.016              0.006
                    0.6       2.360 0.021 0.033 0.004 0.012              0.002
                    0.7       3.266 0.093 0.039 0.019 0.008              0.005
                    0.8       4.315 0.014 0.037 0.002 0.005              0.001
                    0.9       6.037 0.024 0.031 0.003 0.003              0.002
                    MSE       5.011 0.043 0.045 0.008 0.017              0.004
Notes: Table reports mean bias (panel I) and MSE (panel II) for estimates from classical quantile
regression (QR) and quasi-MLE modeling the error term as a mixture of three normals across 100
MC simulations of N = 100, 000 observations each. The data are simulated from the data-generating
process described in Section 4 and the measurement error generated as a mixture of two normals
N (−4.36, 1) and N (2.18, 1) with weights 1/3 and 2/3, respectively, such that the variance of the
measurement error is equal across simulation designs. The last row reports the mean absolute bias
(panel I) and the mean MSE (panel II) over the nine quantiles listed above.
34                             HAUSMAN, LIU, LUO, AND PALMER


                  Table C3. Mean Bias and Mean Squared Error: ε ∼ t
                                 β1           β2           β3
                  Quantile QR       MLE   QR     MLE  QR      MLE
                                         I. Mean Bias
                    0.1    -1.962 0.039 0.142 0.039 0.103 0.028
                    0.2    -1.087 0.084 0.112 0.000 0.048 -0.015
                    0.3    -0.649 0.018 0.088 -0.038 0.021 -0.004
                    0.4    -0.336 -0.099 0.062 0.028 0.000 0.007
                    0.5    -0.062 0.001 0.032 0.015 -0.016 -0.015
                    0.6    0.227 0.053 -0.004 -0.020 -0.031 -0.005
                    0.7    0.581 0.109 -0.052 -0.041 -0.045 -0.011
                    0.8    1.098 0.074 -0.115 -0.007 -0.062 -0.013
                    0.9    2.082 0.049 -0.200 -0.022 -0.082 -0.017
                   |Bias|   0.898 0.058 0.090 0.023 0.045 0.013

                                         II. Mean Squared Error
                    0.1       3.850   0.216 0.020 0.021 0.011           0.036
                    0.2       1.183   0.104 0.013 0.009 0.002           0.009
                    0.3       0.422   0.164 0.008 0.028 0.001           0.022
                    0.4       0.113   0.120 0.004 0.017 0.000           0.010
                    0.5       0.004   0.191 0.001 0.044 0.000           0.017
                    0.6       0.052   0.064 0.000 0.017 0.001           0.007
                    0.7       0.338   0.108 0.003 0.033 0.002           0.011
                    0.8       1.205   0.039 0.013 0.006 0.004           0.003
                    0.9       4.337   0.018 0.040 0.011 0.007           0.007
                    MSE       1.278   0.114 0.011 0.021 0.003           0.013
Notes: Table reports mean bias (panel I) and MSE (panel II) for estimates from classical quantile
regression (QR) and quasi-MLE modeling the error term as a mixture of three normals across 100
MC simulations of N = 100, 000 observations each. The data are simulated from the data-generating
                                             √ error generated as a Student’s t random variable
process described in Section 4 but measurement
with three degrees of freedom, multiplied by 3.5 to ensure the variance of the measurement error
is equal across simulation designs. The last row reports the mean absolute bias (panel I) and the
mean MSE (panel II) over the nine quantiles listed above.
          ERRORS IN THE DEPENDENT VARIABLE OF QUANTILE REGRESSION MODELS                           35


               Table C4. Mean Bias and Mean Squared Error: ε ∼ Laplace
                                β1           β2             β3
                 Quantile QR       MLE   QR     MLE    QR      MLE
                                        I. Mean Bias
                   0.1    -2.499 0.082 0.173 0.008 0.126 -0.009
                   0.2    -1.325 0.026 0.124 0.009 0.052 -0.009
                   0.3    -0.773 -0.098 0.092 -0.001 0.019 0.049
                   0.4    -0.391 -0.047 0.062 0.020 -0.001 0.002
                   0.5    -0.058 0.099 0.031 -0.031 -0.018 -0.028
                   0.6    0.286 0.019 -0.007 0.000 -0.032 -0.014
                   0.7    0.697 0.044 -0.054 -0.037 -0.046 -0.004
                   0.8    1.327 0.041 -0.127 -0.012 -0.068 -0.005
                   0.9    2.631 0.047 -0.245 0.000 -0.100 -0.030
                  |Bias|   1.110 0.056 0.102 0.013 0.051 0.017

                                           II. Mean Squared Error
                     0.1       6.247    0.117 0.030 0.014 0.016            0.021
                     0.2       1.756    0.064 0.016 0.008 0.003            0.008
                     0.3       0.598    0.155 0.009 0.031 0.000            0.026
                     0.4       0.154    0.072 0.004 0.013 0.000            0.008
                     0.5       0.004    0.178 0.001 0.040 0.000            0.017
                     0.6       0.082    0.047 0.000 0.011 0.001            0.006
                     0.7       0.486    0.096 0.003 0.033 0.002            0.009
                     0.8       1.762    0.019 0.016 0.005 0.005            0.003
                     0.9       6.922    0.014 0.060 0.009 0.010            0.006
                     MSE       2.001    0.085 0.015 0.018 0.004            0.011
Notes: Table reports mean bias (panel I) and MSE (panel II) for estimates from classical quantile
regression (QR) and quasi-MLE modeling the error term as a mixture of three normals across 100
MC simulations of N = 100, 000 observations each. The data are simulated from the data-generating
process described in Section 4 but measurement error generated as a Laplace random variable with
λ = 2.29 to ensure the variance of the measurement error is equal across simulation designs. The last
row reports the mean absolute bias (panel I) and the mean MSE (panel II) over the nine quantiles
listed above.
36                             HAUSMAN, LIU, LUO, AND PALMER


                Table C5. Mean Bias and Mean Squared           Error: Binary x2
                                β1           β2                       β3
                 Quantile QR       MLE   QR     MLE              QR      MLE
                                        I. Mean Bias
                   0.1    -3.019 0.205 0.177 -0.026              0.139   0.025
                   0.2    -2.608 0.154 0.296 -0.022              0.138   -0.019
                   0.3    -2.242 0.028 0.386 0.015               0.112   -0.014
                   0.4    -1.759 0.054 0.377 -0.021              0.074   0.014
                   0.5    -0.512 0.031 0.162 -0.006             -0.018   -0.004
                   0.6    1.238 -0.026 -0.197 0.008             -0.100   -0.002
                   0.7    2.035 -0.054 -0.348 -0.016            -0.108    0.007
                   0.8    2.731 -0.120 -0.394 0.036             -0.118   -0.014
                   0.9    3.535 -0.115 -0.356 -0.001            -0.102   -0.002
                  |Bias|  2.186 0.087 0.299 0.017                0.101    0.011

                                       II. Mean Squared Error
                    0.1       9.113 0.160 0.031 0.009 0.019              0.012
                    0.2       6.800 0.081 0.088 0.007 0.019              0.005
                    0.3       5.027 0.161 0.149 0.034 0.013              0.014
                    0.4       3.093 0.085 0.143 0.012 0.006              0.003
                    0.5       0.264 0.189 0.026 0.041 0.001              0.014
                    0.6       1.534 0.076 0.039 0.012 0.010              0.004
                    0.7       4.141 0.156 0.121 0.035 0.012              0.008
                    0.8       7.456 0.062 0.155 0.007 0.014              0.002
                    0.9      12.495 0.075 0.127 0.011 0.010              0.003
                    MSE       5.547 0.116 0.098 0.019 0.012              0.007
Notes: Table reports mean bias (panel I) and MSE (panel II) for estimates from classical quantile
regression (QR) and MLE across 100 MC simulations of N = 100, 000 observations using data
simulated from the data-generating process described in Section 4 but where x2 ∈ {0, 4.32} with
equal probability (to ensure that the variance of x2 is equal across simulation designs). The last
row reports the mean absolute bias (panel I) and the mean MSE (panel II) over the nine quantiles
listed above.
          ERRORS IN THE DEPENDENT VARIABLE OF QUANTILE REGRESSION MODELS                        37


                 Table C6. Mean Bias and Mean Squared          Error: 99 Knots
                                 β1           β2                     β3
                  Quantile QR       MLE   QR     MLE            QR      MLE
                                         I. Mean Bias
                    0.1    -2.924 -0.220 0.145 0.007             0.134   -0.018
                    0.2    -2.486 -0.237 0.223 -0.001            0.144    0.021
                    0.3    -2.074 -0.243 0.265 -0.002            0.132    0.009
                    0.4    -1.510 -0.196 0.248 -0.006            0.089   -0.008
                    0.5    -0.402 -0.184 0.101 0.007            -0.012   -0.018
                    0.6    1.055 -0.220 -0.123 0.008            -0.120    0.003
                    0.7    1.939 -0.281 -0.238 0.034            -0.141    0.006
                    0.8    2.601 -0.156 -0.285 -0.002           -0.125   -0.005
                    0.9    3.355 -0.108 -0.284 -0.015           -0.097   -0.009
                   |Bias|  2.038 0.205 0.213 0.009               0.110    0.011

                                       II. Mean Squared Error
                    0.1       8.548 0.343 0.021 0.007 0.018              0.012
                    0.2       6.179 0.315 0.050 0.013 0.021              0.020
                    0.3       4.302 0.315 0.070 0.019 0.018              0.014
                    0.4       2.280 0.256 0.062 0.024 0.008              0.012
                    0.5       0.164 0.254 0.011 0.025 0.000              0.014
                    0.6       1.113 0.313 0.015 0.025 0.014              0.010
                    0.7       3.761 0.359 0.057 0.027 0.020              0.005
                    0.8       6.767 0.212 0.082 0.023 0.016              0.004
                    0.9      11.259 0.070 0.081 0.015 0.010              0.004
                    MSE       4.930 0.271 0.050 0.020 0.014              0.011
Notes: Table reports mean bias (panel I) and MSE (panel II) for estimates from classical quantile
regression (QR) and MLE across 100 MC simulations of N = 100, 000 observations using data
simulated from the data-generating process described in Section 4 and when a sieve of J = 99 knots
is used in estimation. The last row reports the mean absolute bias (panel I) and the mean MSE
(panel II) over the nine quantiles listed above.
38                              HAUSMAN, LIU, LUO, AND PALMER

                                 Appendix D. Data Appendix

   Following the sample selection criteria of Angrist et al. (2006), our data comes from
1% samples of decennial census data available via IPUMS.org (Ruggles et al., 2015) from
1980–2010. From each database, we select annual wage income, education, age, and race data
for prime-age (age 40-49) black and white males who have at least five years of education,
were born in the United States, had positive earnings and hours worked in the reference
year, and whose responses for age, education, and earnings were not imputed (which would
have been an additional source of measurement error). Our dependent variable is log weekly
wage, obtained as annual wage income divided by weeks worked. For 1980, we take the
number of years of education to be the highest grade completed and follow the methodology
of Angrist et al. (2006) to convert the categorical education variable in 1990, 2000, and 2010
into a measure of the number of years of schooling. Experience is defined as age minus years
of education minus five. For 1980, 1990, and 2000, we use the exact extract of Angrist et
al. (2006), and draw our own data to extend the data to include the 2010 census. Table
D1 reports summary statistics for the variables used in the regressions in the text. Wages
for 1980–2000 were expressed in 1989 dollars after deflating using the Personal Consumption
Expenditures Index. As slope coefficients in a log-linear quantile regression specification are
unaffected by scaling the dependent variable, we do not deflate our 2010 data.

                   Table D1. Education and Wages Summary Statistics
                   Year                    1980 1990 2000 2010
                   Log weekly wage         6.43   6.48   6.50   8.37
                                          (0.66) (0.69) (0.74) (0.76)
                   Education              12.99 13.97 13.90 14.12
                                          (3.08) (2.66) (2.41) (2.39)
                   Experience             25.38 24.45 24.45 24.55
                                          (4.32) (4.01) (3.60) (3.83)
                   Number of Observations 60,051 80,115 90,201 98,292
Notes: Table reports summary statistics for the Census data used in the quantile wage regressions in
the text. The 1980, 1990, and 2000 datasets come from Angrist et al. (2006). Following their sample
selection, we extended the sample to include 2010 Census microdata from IPUMS.org (Ruggles et
al., 2015).

  Although quantile regression recovers effects on the conditional distribution of the out-
come, it is worth noting that given the substantial variation in wages left unexplained by the
Mincer model, the empirical difference between effects on the unconditional and conditional
distributions of the dependent variable is likely small. See DiNardo et al. (1996) and Powell
(2013) for further discussion and methods that recover effects on the unconditional distri-
bution. Appendix Figure D1 illustrates this point for the 0.9 quantile estimates, showing
that because of the relatively low goodness of fit of equation (5.1) (as is the case in many
                        ERRORS IN THE DEPENDENT VARIABLE OF QUANTILE REGRESSION MODELS                      39

cross-sectional applied microeconomics settings), over 63% of the observations in the top
unconditional decile are also in the top conditional decile.




           Figure D1. Overlap between Unconditional and Conditional Wage Distribution
            12
            10      8
   log(Weekly Wage)
   2     4  0
            −2 6




                        8    9     10    11     12       13    14     15     16   17    18    19       20
                                                     # of Years of Education

                              0.9 Quantile Regression Line        Top 10% Unconditional Distribution


Notes: Figure plots log weekly wages against years of education from the 1990 decennial Census
microdata extract used by Angrist et al. (2006). The regression line plots the average predicted
values by year of education from estimating equation (5.1) by classic quantile regression. Lighter
colored dots indicate observations in the top 10% of the unconditional wage distribution (individuals
with over $1,326 in weekly wages in 1989 dollars).
40                                 HAUSMAN, LIU, LUO, AND PALMER

       Appendix E. Quantile Regression Results across Demographic Subgroups

   In this appendix, we plot quantile-regression estimates of the return to education for
several demographic subgroups: white, black, Asian, and hispanic men and women. For
comparability, we again follow Angrist et al. (2006) and restrict the sample to 40-49 year olds.
Our main results focus on white males for two reasons. First, only the white-male subgroup
had enough observations in the census data to have sufficient power for our semiparametric
estimator to have enough precision to be useful. Second, the results below show that the
level and quantile dependency of the education-wage gradient varies substantially across
subgroups. Accordingly, pooling demographic subgroups to estimate a common returns to
education quantile function results in misspecification that likely significantly misstates the
relationship between education and wages for many demographics.
   Over time, the returns to education have been increasing for all ethnic subgroups at al-
most all quantiles. The education coefficients are increasing across quantiles only for white
men.17 While most ethnic subgroups have similar quantile patterns for men and women,
white women (Figure E5) have a hump-shaped relationship between quantile and the Mince-
rian education coefficient, with education consistently less valuable lower in the conditional
distribution of income. For black men and women (Figures E2 and E6), the relationship is
generally downward sloping such that low-income blacks appear to have the highest returns
to education relative to blacks elsewhere in the conditional wage distribution. The return on
education for the bottom of the conditional wage distribution for low-income blacks is higher
in 2010 than for any other demographic in any year at any quantile, potentially motivating
future research into the social return on increasing educational attainment among low-income
black men. Hispanic men and women (Figures E3 and E7) also exhibit a downward trend in
the education-wage gradient across quantiles, while the education-wage gradient is relatively
constant across the conditional wage distribution for Asian men and women (Figures E4 and
E8).
   We leave an explanation of this heterogeneity—as well as an exploration of whether the
education-wage gradient differs for other age groups—to future research.




     17We report extremal quantile estimates of the return to education only for white men to highlight their
instability and extremal quantile problem discussed in Chernozhukov (2005).
                                        ERRORS IN THE DEPENDENT VARIABLE OF QUANTILE REGRESSION MODELS           41


                                        Figure E1. QR Estimates of the Returns to Education for White Men
   4 6 8 10 12 14 16 18 20 22
   Schooling Coefficient (log points)
                     2
                     0




                                        0     .1     .2     .3     .4       .5       .6     .7   .8   .9     1
                                                                          Quantile

                                                     1980               1990              2000        2010


Notes: Figure reports quantile regression estimates of log weekly wages (self-reported) on education
and a quadratic in experience for a grid of 99 quantiles from 0.01 to 0.99. Robust 95% pointwise
confidence intervals are plotted for the quantile regression estimates for each year. The data comes
from the indicated decennial census year and consist of 40-49 year-old U.S. born white men with
positive reported wages. The number of observations in each sample is 60,051, 80,115, 90,201, and
98,292 in 1980, 1990, 2000, and 2010, respectively.
42                                                            HAUSMAN, LIU, LUO, AND PALMER


                                          Figure E2. QR Estimates of the Returns to Education for Black Men
                       30
     Schooling Coefficient (log points)
      10       15      520       25




                                          0     .1     .2       .3     .4       .5       .6     .7   .8   .9     1
                                                                              Quantile

                                                       1980                 1990              2000        2010


Notes: Figure reports quantile regression estimates of log weekly wages (self-reported) on education
and a quadratic in experience for a grid of 99 quantiles from 0.02 to 0.98. Horizontal lines plot
OLS estimates for each year, and robust 95% pointwise confidence intervals are plotted for the
quantile regression estimates for each year. The data comes from the indicated decennial census
year and consist of 40-49 year-old U.S. born black men with positive reported wages. The number
of observations in each sample is 4,991, 6,855, 6,733, and 7,365 in 1980, 1990, 2000, and 2010,
respectively.
                                        ERRORS IN THE DEPENDENT VARIABLE OF QUANTILE REGRESSION MODELS             43


                                        Figure E3. QR Estimates of the Returns to Education for Hispanic Men
                     25
   Schooling Coefficient (log points)
    5        10      0 15        20




                                         0      .1     .2     .3     .4       .5       .6     .7   .8   .9     1
                                                                            Quantile

                                                      1980                1990              2000        2010


Notes: Figure reports quantile regression estimates of log weekly wages (self-reported) on education
and a quadratic in experience for a grid of 99 quantiles from 0.03 to 0.97. Horizontal lines plot
OLS estimates for each year, and robust 95% pointwise confidence intervals are plotted for the
quantile regression estimates for each year. The data comes from the indicated decennial census
year and consist of 40-49 year-old U.S. born Hispanic men with positive reported wages. The
number of observations in each sample is 1,948, 3,005, 3,666, and 5,501 in 1980, 1990, 2000, and
2010, respectively.
44                                                            HAUSMAN, LIU, LUO, AND PALMER


                                          Figure E4. QR Estimates of the Returns to Education for Asian Men
                       30
     Schooling Coefficient (log points)
        0          10  -10       20




                                          0     .1     .2       .3     .4       .5       .6     .7   .8   .9     1
                                                                              Quantile

                                                       1980                 1990              2000        2010


Notes: Figure reports quantile regression estimates of log weekly wages (self-reported) on education
and a quadratic in experience for a grid of 99 quantiles from 0.06 to 0.94. Horizontal lines plot
OLS estimates for each year, and robust 95% pointwise confidence intervals are plotted for the
quantile regression estimates for each year. The data comes from the indicated decennial census
year and consist of 40-49 year-old U.S. born men of Asian ethnicity with positive reported wages.
The number of observations in each sample is 348, 532, 544, and 786 in 1980, 1990, 2000, and 2010,
respectively.
                                         ERRORS IN THE DEPENDENT VARIABLE OF QUANTILE REGRESSION MODELS           45


                                        Figure E5. QR Estimates of the Returns to Education for White Women
                     20
   Schooling Coefficient (log points)
      5         10   0       15




                                         0      .1     .2    .3     .4       .5       .6     .7   .8   .9     1
                                                                           Quantile

                                                      1980               1990              2000        2010


Notes: Figure reports quantile regression estimates of log weekly wages (self-reported) on education
and a quadratic in experience for a grid of 99 quantiles from 0.02 to 0.98. Horizontal lines plot OLS
estimates for each year, and robust 95% pointwise confidence intervals are plotted for the quantile
regression estimates for each year. The data comes from the indicated decennial census year and
consist of 40-49 year-old U.S. born white women with positive reported wages. The number of
observations in each sample is 43,965, 69,903, 74,878, and 76,985 in 1980, 1990, 2000, and 2010,
respectively.
46                                                             HAUSMAN, LIU, LUO, AND PALMER


                                          Figure E6. QR Estimates of the Returns to Education for Black Women
                       25
     Schooling Coefficient (log points)
         10          155         20




                                           0      .1    .2       .3     .4       .5       .6     .7   .8   .9     1
                                                                               Quantile

                                                        1980                 1990              2000        2010


Notes: Figure reports quantile regression estimates of log weekly wages (self-reported) on education
and a quadratic in experience for a grid of 99 quantiles from 0.02 to 0.98. Horizontal lines plot
OLS estimates for each year, and robust 95% pointwise confidence intervals are plotted for the
quantile regression estimates for each year. The data comes from the indicated decennial census
year and consist of 40-49 year-old U.S. born black women with positive reported wages. The
number of observations in each sample is 5,295, 8,215, 8,479, and 9,351 in 1980, 1990, 2000, and
2010, respectively.
                                        ERRORS IN THE DEPENDENT VARIABLE OF QUANTILE REGRESSION MODELS          47


                               Figure E7. QR Estimates of the Returns to Education for Hispanic Women
                     25
   Schooling Coefficient (log points)
    5        10      0 15        20




                                        0     .1     .2    .3     .4       .5       .6     .7   .8   .9     1
                                                                         Quantile

                                                    1980               1990              2000        2010


Notes: Figure reports quantile regression estimates of log weekly wages (self-reported) on education
and a quadratic in experience for a grid of 99 quantiles from 0.03 to 0.97. Horizontal lines plot
OLS estimates for each year, and robust 95% pointwise confidence intervals are plotted for the
quantile regression estimates for each year. The data comes from the indicated decennial census
year and consist of 40-49 year-old U.S. born Hispanic women with positive reported wages. The
number of observations in each sample is 1,326, 2,711, 3,645, and 5,616 in 1980, 1990, 2000, and
2010, respectively.
48                                                             HAUSMAN, LIU, LUO, AND PALMER


                                          Figure E8. QR Estimates of the Returns to Education for Asian Women
                       40
     Schooling Coefficient (log points)
      0        10        20
                       -10         30




                                           0      .1     .2      .3     .4       .5       .6     .7   .8   .9     1
                                                                               Quantile

                                                        1980                 1990              2000        2010


Notes: Figure reports quantile regression estimates of log weekly wages (self-reported) on education
and a quadratic in experience for a grid of 99 quantiles from 0.06 to 0.94. Horizontal lines plot OLS
estimates for each year, and robust 95% pointwise confidence intervals are plotted for the quantile
regression estimates for each year. The data comes from the indicated decennial census year and
consist of 40-49 year-old U.S. born women of Asian ethnicity with positive reported wages. The
number of observations in each sample is 272, 451, 463, and 720 in 1980, 1990, 2000, and 2010,
respectively.
             ERRORS IN THE DEPENDENT VARIABLE OF QUANTILE REGRESSION MODELS                                                49

    Appendix F. Additional Maximum Likelihood Estimates of Mincer Equation

   In this appendix, we plot ML estimates of equation (5.1) for the non-education terms
in the model, consisting of the constant term β1 (·), the coefficient on experience β3 (·), and
the coefficient on experience squared β4 (·). For each graph, we plot the quantile-regression
estimate, the ML estimate, and the bootstrapped 95% confidence intervals associated with
the ML estimates.




              Figure F1. ML Estimates of Quantile Intercept in Log Wage Model

                                  1980                                                       1990
    15




                                                                8
                                                                6
    10




                                                                4
    5




                                                                2
    0




                                                                0
    −5




                                                                −2




         0   .1   .2   .3    .4     .5 .6    .7   .8   .9   1        0   .1   .2   .3   .4     .5 .6    .7   .8   .9   1
                                  Quantile                                                   Quantile


                                  2000                                                       2010
    10




                                                                10
                                                                8
    8




                                                                6
    6




                                                                4
    4




                                                                2
    2




                                                                0




         0   .1   .2   .3    .4     .5 .6    .7   .8   .9   1        0   .1   .2   .3   .4     .5 .6    .7   .8   .9   1
                                  Quantile                                                   Quantile


                            MLE                    Quantile Regression                          MLE 95% CI


Notes: Graphs plot estimated intercept term using quantile regression (red lines) and the ML
estimator described in the text (blue line). Dashed blue lines plot 95% pointwise confidence intervals.
See notes to Figure E1 for further details.
50                                                 HAUSMAN, LIU, LUO, AND PALMER


                   Figure F2. ML Estimates of Experience Coefficient in Log Wage Model

                                        1980                                                          1990
     .6




                                                                        .4
     .4




                                                                        .2
     .2
     0




                                                                        0
     −.4 −.2




                                                                        −.2
               0   .1   .2   .3    .4     .5 .6      .7   .8   .9   1         0   .1   .2   .3   .4     .5 .6    .7   .8   .9   1
                                        Quantile                                                      Quantile


                                        2000                                                          2010
     .5




                                                                        .5
     0




                                                                        0
     −.5




                                                                        −.5
     −1




               0   .1   .2   .3    .4     .5 .6      .7   .8   .9   1         0   .1   .2   .3   .4     .5 .6    .7   .8   .9   1
                                        Quantile                                                      Quantile


                                  MLE                      Quantile Regression                           MLE 95% CI


Notes: Graphs plot experience coefficients estimated using quantile regression (red lines) and the
ML estimator described in the text (blue line). Dashed blue lines plot 95% pointwise confidence
intervals. See notes to Figure E1 for further details.
                                    ERRORS IN THE DEPENDENT VARIABLE OF QUANTILE REGRESSION MODELS                                                                       51


                                Figure F3. ML Estimates of Experience Quadratic Term in Log Wage Model

                                                         1980                                                                              1990




                                                                                       .005
    .005 .01




                                                                                       0
    0




                                                                                       −.005
    −.01 −.005




                                                                                       −.01
                                0   .1   .2   .3    .4     .5 .6    .7   .8   .9   1                               0   .1   .2   .3   .4     .5 .6    .7   .8   .9   1
                                                         Quantile                                                                          Quantile


                                                         2000                                                                              2010



                                                                                       −.01−.005 0 .005 .01 .015
    −.005 0 .005 .01 .015 .02




                                0   .1   .2   .3    .4     .5 .6    .7   .8   .9   1                               0   .1   .2   .3   .4     .5 .6    .7   .8   .9   1
                                                         Quantile                                                                          Quantile


                                                   MLE                    Quantile Regression                                                 MLE 95% CI


Notes: Graphs plot experience2 coefficients estimated using quantile regression (red lines) and the
ML estimator described in the text (blue line). Dashed blue lines plot 95% pointwise confidence
intervals. See notes to Figure E1 for further details.
52                                             HAUSMAN, LIU, LUO, AND PALMER

                              Appendix G. Proofs of Lemmas and Theorems
G.1. Lemmas and Theorems in Section 2.




Proof of Theorem 1. If there exist β(·) and f (·) which generate the same density g(y|x, β(·), f (·))
as the true parameters β0 (·) and f0 (·) then by applying a Fourier transformation and conditional
on x,
                                     Z   1                                           Z       1
                                                      T
                            φε (s)           exp(isx β(τ ))dτ = φε0 (s)                          exp(isxT β0 (τ ))dτ.
                                     0                                                   0

   Denote m(s) = φφε0ε (s)
                        (s)
                            . Without loss of generality, by Assumption 2, we can assume that x1 is
the continuous variable and the support of x1 |x−1 contains an open neighborhood of 0. A Taylor
expansion on both sides around x1 = 0 gives us
  Z 1                      ∞                                Z 1                       ∞
             T
                          X    (is)k xk1 β1 (τ )k                      T
                                                                                      X (is)k xk1 β0,1 (τ )k
      exp(isx−1 β−1 (τ ))                         dτ = m(s)     exp(isx−1 β0,−1 (τ ))                        dτ.
   0                                  k!                     0                                   k!
                               i=0                                                                               i=0

Since x1 is continuous, then it must be that any corresponding polynomials of x1 are the same on
both sides. Namely, for any k ≥ 1 and any s,

                  1                                                                      1
      (is)k                                                      (is)k
              Z                                                                  Z
                      exp(isxT−1 β−1 (τ ))β1 (τ )k dτ = m(s)                                 exp(isxT−1 β0,−1 (τ ))β0,1 (τ )k dτ.   (G.1)
        k!    0                                                    k!                0

Dividing both sides of the above equation by (is)k /k! when s 6= 0 and letting s approach 0,
                                  Z 1               Z 1
                                      β1 (τ )k dτ =     β0,1 (τ )k dτ.                       (G.2)
                                                  0                  0

We now show that (G.2) implies that random variables β1 and β0,1 share the same distribution.
The characteristic function of β1 (τ ) can be written as
                                                 ∞
                                                   (is)k 1
                                                 X      Z
                                  φβ1 (τ ) (s) =           β1 (τ )k dτ                 (G.3)
                                                     k! 0
                                                           k=0
                                 R1
Since β1 (τ ) is bounded, 0 β1 (τ )k dτ ≤ M k for some constant M > 0. Therefore, φβ1 (τ ) (s) ≤
P∞ |s|k k
  k=0 k! M = exp(M |s|) < ∞ for any s, and the right-hand side of (G.3) is well defined. Com-
bining (G.2) and (G.3), we have
                                        φβ1 (τ ) (s) = φβ0,1 (τ ) (s),
and thus β1 and β0,1 share the same distribution almost everywhere as two random variables. Thus
there exists a measurableR one-to-one Rreordering mapping π : [0, 1] 7→ [0, 1]. Then β1 (π(τ )) = β0,1 (τ )
almost everywhere, and h(τ )dτ = h(π(τ ))dτ for all integrable functions h(·) defined on [0,1].
  Now consider (G.1) again. Dividing both sides by (is)k /k!, we have, for all k ≥ 0
        Z 1                                       Z 1
                   T                        k
            exp(isx−1 β−1 (π(τ )))β1 (π(τ )) dτ =     exp(isxT−1 β−1 (τ ))β1 (τ )k dτ
         0                                                       0
                                                                         Z       1
                                                           = m(s)                    exp(isxT−1 β0,−1 (τ ))β0,1 (τ )k dτ.           (G.4)
                                                                             0
                ERRORS IN THE DEPENDENT VARIABLE OF QUANTILE REGRESSION MODELS                                                                53
                                                                                    R∞
  Consider the first-order terms of s in (G.4). Since both f and f0 satisfy −∞ εf (ε) = 0, we have
m0 (0) = 0, and hence the coefficients for the first-order terms of s in (G.4) can be written as
                    Z 1                                 Z 1
                         T                       k
                        x−1 β−1 (π(τ ))β1 (π(τ )) dτ =      xT−1 β0,−1 (τ )β0,1 (τ )k dτ.
                               0                                                      0

By Assumption 2.2, β0,1 (τ )k , k ≥ 0 is a functional basis of L2 [0, 1], therefore

                                                      xT−1 β−1 (π(τ )) = xT−1 β0,−1 (τ )
almost everywhere and everywhere for τ ∈ [0, 1] by invoking the continuity of β−1 (·) and β0,−1 (·).
Hence E[x−1 xT−1 ]β−1 (π(τ )) = E[x−1 xT−1 ]β0,−1 (τ ) almost everywhere for τ ∈ [0, 1]. By Assump-
tion 2.2, E[xxT ] is non-singular. Ergo E[x−1 xT−1 ] is also non-singular. Multiplying both sides of
E[x−1 xT−1 ]β−1 (π(τ )) = E[x−1 xT−1 ]β0,−1 (τ ) by E[x−1 xT−1 ]−1 ,we get β−1 (π(τ )) = β0,−1 (τ ) for almost
all τ ∈ [0, 1].
   For any x, xT β(π(τ )) = xT β0 (τ ). Since conditional on x, xT β(τ ) has the same distribution
as xT β(π(τ )), xT β(τ ) has the same distribution as xT β0 (τ ). By the monotonicity of xT β(τ ) and
xT β0 (τ ), they must equal each other at almost all τ . Since E[xxT ] is non-singular, β(τ ) = β0 (τ )
almost everywhere. Consequently, φε (s) = φε0 (s), and f (ε) = f0 (ε) almost everywhere.                    




Proof of Lemma 1. We first prove the case when W (x1 ) = [x1 , x21 ]T and then describe how the proof
can be generalized to the case where W (x1 ) is a pth -order polynomial. If there exist β(·) and f (·)
which generate the same density as the true parameters β0 (·) and f0 (·) then by applying a Fourier
transformation and conditional on x,

                                                    φxβ (s|x)φε (s) = φxβ0 (s|x)φε0 (s).                                                  (G.5)
Then
                                                       φxβ (s|x) = m(s)φxβ0 (s|x),                                                        (G.6)
                        φ (s)
where m(s) = φεε0(s) is a function depending only on s. Let βw (τ ) = [βx1 , βx21 ]T and β0,w =
[β0,x1 , β0,x21 ]T denote the subvectors of β and β0 associated with W (x1 ) = [x1 , x21 ]T . Expanding
(G.6) around s = 0, we get
                            ∞ Z 1
                           X        (is)k
                                          [(x1 , x21 )βw (τ ) + xT−w β−w (τ )]k dτ
                                0     k!
                           k=0
                            ∞         ∞ Z 1
                           X         X        (is)k
                        =      ak sk                  [(x1 , x21 )β0,w (τ ) + xT−w β0,−w (τ )]k dτ,
                                           0     k!
                                   k=0         k=0
                                   ∞           k           Z   1 l
                                   X          hX                i                                                      i
                             =           sk         ak−l             [(x1 , x21 )β0,w (τ ) + xT−w β0,−w (τ )]l dτ                         (G.7)
                                                           0    l!
                                   k=0        l=0
         P∞
where k=0 ak sk is a Taylor expansion of m(s) around s = 0. Since both ε and ε0 have zero mean,
we have a0 = 1, and a1 = 0. Comparing the coefficients on both sides of (G.7) for sk , we have
 Z     1 k                                                           k            Z   1 l
        i                                                            X                 i
             [(x1 , x21 )βw (τ )   +   xT−w β−w (τ )]k dτ      =           ak−l                [(x1 , x21 )β0,w (τ ) + xT−w β0,−w (τ )]l dτ (G.8)
   0    k!                                                                        0       l!
                                                                     l=0
54                                    HAUSMAN, LIU, LUO, AND PALMER

holding for any k, x1 and x−w . Comparing both sides of (G.8) for any k and the coefficients for
x2k
 1 , we have                       Z 1 k                      Z 1 k
                                       i          k               i              k
                                          βx1 (τ ) dτ =
                                             2                         β0,x21 (τ ) dτ.
                                    0 k!                       0 k!
Using the same argument as in the proof for Theorem 1 through the characteristic functions, the
two random variables βx21 (τ ) and β0,x21 (τ ) share the same distribution, and there exists a measurable
reordering mapping π : [0, 1] 7→ [0, 1] such that βx21 (π(τ )) = β0,x21 (τ ) almost everywhere. Comparing
both sides of (G.8) for any k and the coefficients for x2k−1        1     , we have
  Z 1 k                               Z 1 k                                       Z 1 k
      i             k−1                  i                k−1                      i              k−1
          βx21 (τ )      βx1 (τ )dτ =          β0,x21 (τ )      β0,x1 (τ )dτ =          βx21 (π(τ ))     β0,x1 (τ )dτ,
    0 k!                                0 k!                                       0 k!

where we used the fact that βx21 (π(τ )) = β0,x21 (τ ). As argued above in the proof of the previous
                                                   k−1
lemma, by Assumption 3, we know that βx21 (τ )           for k ≥ 1 forms a functional basis of L2 [0, 1],
implying that
                                        βx1 (π(τ )) = β0,x1 (τ )
almost everywhere. Comparing both sides of (G.8) for any k and the coefficients for x2k−2
                                                                                       1  , we have
                   ik 1
                     Z
                                 k−2           2           k−1 T
                        βx21 (τ )       βx1 (τ ) + βx21 (τ )      x−w β−w (τ )dτ
                   k! 0
                   ik 1
                     Z
                                    k−2           2            k−1 T
                =       β0,x21 (τ )      β0,x1 (τ ) + β0,x21 (τ )     x−w β0,−w (τ )dτ        (G.9)
                   k! 0
where we used the fact that a1 = 0 and thus for a fixed k, the only l on the right-hand side of
(G.8) that can generate a nonzero coefficient for x2k−2     1       is l = 0. Since we already proved that
βx1 (π(τ )) = β0,x1 (τ ) and βx1 (π(τ )) = β0,x1 (τ ) almost everywhere, (G.9) can be rewritten as
                                    2            2


                  Z 1                                   Z 1
                                   k−1 T                               k−1 T
                         βx21 (τ )     x−w β−w (τ )dτ =      β0,x21 (τ )    x−w β0,−w (τ )dτ
                    0                                      0
                                                          Z    1               k−1 T
                                                      =            βx21 (π(τ ))    x−w β0,−w (τ )dτ.
                                                           0
                                         k−1
Again, using the fact that βx21 (τ )     , k ≥ 1 forms a functional basis of L2 [0, 1], we have for any
x−w
                                    xT−w β−w (π(τ )) = xT−w β0,−w (τ )
almost everywhere in τ ∈ [0, 1]. Following the same argument as in Theorem 1, we know that there
is sufficient variation in x−w such that xT−w β−w (π(τ )) = xT−w β0,−w (τ ) implies β−w (π(τ )) = β0,−w (τ )
almost everywhere. By monotonicity of xT β(τ ) and xβ0T (τ ), we have π(τ ) = τ almost everywhere,
and thus β(τ ) = β0 (τ ) and f (ε) = f0 (ε) almost everywhere.
   The argument for the case of W (x1 ) being a pth order polynomial is very similar to the argument
above. We start from a Taylor expansion similar to (G.7). Then we compare the coefficients for
each term sk and get          Z 1 k
                                   i
                                      [(x1 , · · · , xp1 )βw (τ ) + xT−w β−w (τ )]k dτ
                                0  k!
                            k      Z 1 l
                           X            i
                        =     ak−l        [(x1 , · · · , xp1 )β0,w (τ ) + xT−w β0,−w (τ )]l dτ.     (G.10)
                            l=0      0 l!
          ERRORS IN THE DEPENDENT VARIABLE OF QUANTILE REGRESSION MODELS                             55

Using the fact that for each k ≥ 1, the coefficients for xkp
                                                           1 on both sides of (G.10) must equal each
other, we can show that there exists a reordering mapping π(τ ) such that βxp1 (π(τ )) = β0,xp1 (τ ).
Using the fact that for each k, the coefficients for xkp−l
                                                        1   , 1 ≤ l ≤ k − 1 on both sides of (G.10)
must equal each other, we can show that βxp−l   1
                                                   (π(τ )) =  β0,xp−l
                                                                  1
                                                                      (τ ) almost everywhere. Because
                     k(p−1)
the coefficients for x1    must equal each other on both sides of (G.10), we can also show that
 T                   T
x−w β−w (π(τ )) = x−w β0,−w (τ ) almost everywhere. The rest follows the same argument as in the
proof for Theorem 1, and we have β(τ ) = β0 (τ ) almost everywhere for all τ ∈ [0, 1] and f (ε) = f0 (ε)
almost everywhere for all ε ∈ R.                                                                      

G.2. Lemmas and Theorems in Section 3. The following lemmas are used in the proofs of
Lemmas 2 and 3.
Lemma 6. The space M [B1 × B2 × B3 ... × Bdx ] is a compact and complete space under Lp for any
p ≥ 1.
Proof of Lemma 6. For bounded monotonic functions, pointwise convergence is equivalent to uni-
form convergence, making a space of bounded monotonic functions compact under any Lp norm
for p ≥ 1. Hence the product space B1 × B2 × ... × Bdx is compact. It is complete since the Lp
functional space is complete and the limit of monotonic functions is still monotonic.       
Lemma 7 (Donskerness of Θ). The set of functions
                      G = {h(y, x, β(·), σ) := log(g(y|x, β(·), σ))|(β(·), σ) ∈ Θ}
is µ-Donsker, where µ is the joint PDF of (y, x).
Proof. By Theorem 2.7.5 of Van der Vaart and Wellner (1996), the bracketing number N[] of the
space of uniformly bounded monotone functions F satisfies
                                                              1
                                    log N[] (ε, F, Lr (Q)) ≤ K ,
                                                              ε
for every probability measure Q and every r ≥ 1 and a constant K which depends only on r.
Consider a collection of functions F := q(y, x, θ)|θ ∈ Θ such that

                              |q(y, x, θ1 ) − q(y, x, θ2 )| ≤ ||θ1 − θ2 ||2 w(y, x).             (G.11)

                                            EQ [|w(y, x)|2 ] < ∞,                                (G.12)
where Q is some probability measure on (y, x). Since Θ is a product space of bounded monotone
functions M and a finite-dimensional bounded compact set Σ, the bracketing number of F given
measure Q is also bounded by
                                                                   1
                                       log N[] (ε, F, L2 (Q)) ≤ Kdx ,
                                                                   ε
                                                                Rδq
where K is a constant only depend on Θ and w(y, x). Therefore, 0 log N[] (ε, F, Q) < ∞, i.e., F
is Donsker.
   In particular, let q = log g and Q = µ, where µ is the joint PDF of (x, y). By Assumption
                                           R1
4.5, equation (G.11) holds with w(y|x) := 0 |y − xT β(τ )|γ dτ . Equation (G.12) is satisfied by
Assumption 4.3. Hence, G is µ-Donsker.                                                         
Proof of Lemma 2. To show the consistency of the ML estimator, it is sufficient to prove the satis-
faction of the following regularity conditions (Newey and McFadden, 1994).
56                                     HAUSMAN, LIU, LUO, AND PALMER

     (1) The parameter space Θ = M × Σ is compact.
                                                                                                         R1
     (2) Global identification holds, i.e., there exists no other θ0 = (β 0 , σ 0 ) ∈ Θ such that E[log 0 f (y−
                                    R1
         xβ 0 (τ )|σ 0 )dτ ] = E[log 0 f (y − xβ0 (τ )|σ0 )dτ ].
                                            R1
     (3) The objective function E[log 0 f (y − xβ 0 (τ )|σ 0 )dτ ] is continuous for all θ0 = (β 0 , σ 0 ) ∈ Θ.
                                                   R1
     (4) Stochastic equicontinuity of En [log 0 f (y − xT β(τ )|σ)], with θ ∈ Θ.

Condition 1 is established by Lemma 6. Condition 2 is provided by Lemma 1. Condition 3 holds
under Assumption 4. For the proof of point 4, see Lemma 7 above. Therefore, the ML estimator
defined herein is consistent.                                                             


  The following lemma establishes mild ill-posedness (Assumption 5.1) under the special
case of piecewise-constant sieves.

Lemma 8 (Sufficient Condition for Mild Ill-posedness). If the function f satisfies Assumptions 1,
3, 4, and 6 with degree λ > 0 then

     (1) The minimum eigenvalue of I, denoted as v(I), satisfies J1λ - v(I).
     (2) For any θ, J1λ - supp∈ΘJ −θ,p6=0 ||p||
                                                                       1/2
                                                                   0
                                           ||p|| where ||p||d := |p Ip| .
                                                d




Proof. Suppose f satisfies the discontinuity condition in Assumption 6 with degree λ > 0, and
without loss of generality, assume cδ = 1. Denote lJ = (fτ1 , fτ2 , ..., fτJ , gσ ). For any pJ ∈ ΘJ − θ,
                     0 2
pJ Ip0J = E[ R (lJ pgJ ) dy] ≥ CE[(lJ p0J )2 ] for some constant C > 0 since g is bounded from above.
            R
                                                                 Pλ  i 2
   Define c := inf x∈X ,τ ∈[0,1] (xβ00 (τ )) > 0. Let S(λ) :=                 , where ab stands for the
                                                                                          
                                                                   i=0 λ λ
combinatorial number choosing b elements from a set with size a. Then

                                                                                    
                Z                     λ  2 Z                          2
                                       X   j               j              j
        S(λ)E       (lJ p0J )2 dy = E            lJ y +        p0J y +           dy  .
                  R                        λ    R        2λuJc          2λuJc
                                         j=0

where u > 0 is a constant that will be specified later. By the Cauchy-Schwarz inequality,
                                                                            
                        λ  2 Z                                  2
                       X    j                     j                j
                   E                 lJ y +            p0J y +           dy 
                            λ      R           2λuJc            2λuJc
                           j=0

                                                                                 2
                                                                                    
                      Z      λ      2                    
                           X         j                j                     j
               ≥ E   (−1)j              lJ y +              p0J y +              dy  .
                        R            λ              2λuJc                2λuJc
                            j=0
                              h                                                    i
Defining the interval QiJ := a + xT β( i+1/2
                                         J   ) −   1
                                                 2λuJ , a +  x T β( i+1/2 ) + 1
                                                                      J       2λuJ ,
                                                                                    
                          λ  2 Z                                       2
                         X     j                   j                    j
                    E                lJ y +              p0J y +                 dy 
                               λ  R             2λuJc                2λuJc
                           j=0

                                                                        2
                                                                          
                   Z             λ      2             
                                X       j           j              j
                ≥E             (−1)j     lJ y +        p0J y +         dy  .
                         QiJ            λ         2λuJc          2λuJc
                                 j=0
            ERRORS IN THE DEPENDENT VARIABLE OF QUANTILE REGRESSION MODELS                                                           57

By the discontinuity assumption,
                            λ       2                        
                                  j j           j             j
                            X
                                                      0
                              (−1)     lJ y +        pJ y +
                                    λ         2λuJc         2λuJc
                            j=0
                                                                                        
                                                                           J
                                     1       c             ci              X
                             =                    xi p 0 +                          xj pj  ,
                                  (uJ)λ−1 (λ − 1)! i uJ
                                         
                                                                         j=1,j6=i

with cj uniformly
          i      bounded from above since         is     f (λ)     L1
                                                         Lipschitz except at a. Noting that the
intervals QJ do not intersect each other as J → ∞ and u → 0,
                                                      J
                            Z                           "Z                 #
                                                     X
                      S(λ)E     (lJ p0J )2 dy ≥ S(λ)    E      (lJ p0J )2 dy
                                         R                         i=1       QiJ

                                                                                               2 
                                   J                                                J
                             1     X            1          c              ci        X
                  ≥E                    
                                                  λ−1
                                                                xi p0i +                     xj pj  
                           λuJc              (uJ)       (λ − 1)!          uJ
                                   i=1                                             j=1,j6=i

Finally, when the u is chosen to be large enough (and only depending on λ, sup ci and c),

                                                                2 
     J                                              J                                           J
    X           1          c              ci        X                                   1       X                      1
  E   
                  λ−1
                                xi p0i +                     xj pj )  ≥ c(λ)                      E[x2j p2j ]         ||p||22
             (uJ)       (λ − 1)!          uJ                                         J 2λ−1                           J 2λ
      i=1                                          j=1,j6=i                                     j=1

with the constant c(λ) > 0 only depending on λ and u. Therefore p0 Ip % J12λ ||p||22 . Hence, the
smallest eigenvalue of I is bounded by Jcλ from below with some generic constant c depending on
λ, X , and the L1 Lipschitz coefficient of f (λ) at set R − [a − η, a + η].                    

Proof of Lemma 3. By Lemma 7, the set of log likelihood functions indexed by θbn ∈ Θ is Donsker
such that the sample-average log likelihood converges uniformly to its population counterpart:
                            E[− log g(y|x, θbn )] ≤ En [− log g(y|x, θbn )] + op (1).
By Chen (2007), there exists a θn∗ → θ0 as Jn → ∞ where θn∗ ∈ ΘrJn given that d2 (θ0 , ΘrJn ) =
   − min(p,r)
O(Jn          ), denoting the degree of smoothness of β0 (·) as p. Because θbn is the minimizer of the
negative log likelihood,
                                  En [− log g(y|x, θbn )] ≤ En [− log g(y|x, θn∗ )].
Again, by uniform convergence,
                            En [− log g(y|x, θn∗ )] ≤ E[− log g(y|x, θn∗ )] + op (1)
                                                       ≤ E[− log g(y|x, θ0 )] + op (1)
where the last step used the continuity of the population log-likelihood function around θ0 . Since
                                                               p
Θ is compact, by identification (i.e., Theorem 1), we have θbn →
                                                               − θ0 as Jn → ∞.                   

Proof of Lemma 4. By Lemma 3, we know that sieve-ML estimators for β0 and σ0 are consistent,
                  p                      p
i.e., ||σ
        b − σ0 || →
                  − 0 and ||βbJ − β0 ||2 →
                                         − 0. MLE by definition implies that

                             En [log(g(y|x, βbJ , σ))] ≥ En [log(g(y|x, βJ∗ , σ0 ))]
58                                                  HAUSMAN, LIU, LUO, AND PALMER

where by construction of the sieve, there exists a βJ∗ such that ||βJ∗ − β0 ||2 ≤ CJn−r−1 for some
                                                                p
generic constant C > 0. Therefore, ||(βbJ , σb) − (βJ∗ , σ0 )|| →
                                                                − 0 as Jn → ∞.
  By Lemma 7, G = {h(y, x, β(·), σ) := log(g(y|x, β(·), σ))|(β(·), σ) ∈ Θ} is Donsker. Thus by sto-
chastic equicontinuity,
                                                 b))] − E[log(g(y|x, βbJ , σ
                            En [log(g(y|x, βbJ , σ                         b))]
                                     ∗                             ∗               √
                   = En [log(g(y|x, βJ , σ0 ))] − E[log(g(y|x, βJ , σ0 ))] + op (1/ n),
implying that
                                                                  b))] − E[log(g(y|x, βJ∗ , σ0 ))]
                                               E[log(g(y|x, βbJ , σ

                                                                               √            √
                                    b))] − En [log(g(y|x, βJ∗ , σ0 ))] − op (1/ n) ≥ −op (1/ n).
             = En [log(g(y|x, βbJ , σ
                 √
     Define Gn := n(En − E). By the maximal inequality, for any δ > 0,

         "                                                                           #                                            #
                                                                                             Z    δ
                                                    b) − Gn log g(y|x, βJ∗ , σ0 )| ≤ K
                                                                                                      p
     E           max            |Gn log g(y|x, β
                                               cJ , σ                                                     log N (r, M, || · ||2 )dr ,
             ||βJ −βJ∗ ||2 <δ                                                                 0

where N (r, M, || · ||2 ) is the covering number of r balls on M , the space of β, and K is a generic
constant. Since M is a bounded and co-monotone space (xβ(τ ) is monotone in τ for all x ∈ X ),
N (r, M, || · ||2 ) < δ dx . Therefore,
                    "                                                               #
                                                      b) − Gn log g(y|x, β ∗ , σ0 )| ≤ δ − log δ,
                                                                                        p
                 E       max      |Gn log g(y|x, β
                                                 cJ , σ
                                                                                         J
                            ||βJ −βJ∗ ||2 <δ

                           cJ − β ∗ ||). Consequently,
                b − σ||, ||β
where δ = max(||σ                J

                                                 b))] − E[log(g(y|x, βJ∗ , σ0 ))]
                              E[log(g(y|x, βbJ , σ
                                                                     √                      √
                                                     ∗              δ − log δ              δ − log δ
                            b))] − En [log(g(y|x, βJ , σ0 ))] − Op ( √
      = En [log(g(y|x, βJ , σ
                        b                                                         ) ≥ −Op ( √        ).
                                                                           n                   n
By consistency of (β    b), δ →p 0.
                   cJ , σ
  Since E[log g(y|x, β, σ)] is maximized at (β0 , σ0 ), the Hadamard derivative of E[log g(y|x, β0 , σ0 )]
with respect to β ∈ Θ is 0. By Assumption 4.2, the log g(·|·, ·, ·) function is twice differentiable with
bounded derivatives up to the second order. Therefore, for some generic constant C1 > 0,
                                                                                                        1
         E[log g(y|x, βJ∗ , σ0 )] − E[log g(y|x, β0 , σ0 )] ≥ −C1 ||βJ∗ − β0 ||22 ≥ −C1 C 2 Jn−2r−2 = O( ).
                                                                                                        n
Then
                                                                  b)] − E[log g(y|x, β0 , σ0 )]
                                               E[log g(y|x, βbJ , σ
                                                              b)] − E[log g(y|x, βJ∗ , σ0 )]
                                          =E[log g(y|x, βbJ , σ
                                         + E[log g(y|x, βJ∗ , σ0 )] − E[log g(y|x, β0 , σ0 )]
                                             √                                      √
                                            δ − log δ           2 −2r−2            δ − log δ
                                     ≥ −Op ( √        ) − C1 C Jn         = −Op ( √           )
                                                n                                         n
                                                                    Jn2r+2
where the last step used the assumption that                           n     → ∞.
          ERRORS IN THE DEPENDENT VARIABLE OF QUANTILE REGRESSION MODELS                                           59
                                                                                       R∞
                            b) − g(y|x, β0 , σ0 ) and define ||z(y|x)||1 :=
  Let z(y|x) = g(y|x, βbJ , σ                                                            −∞ |z(y|x)|dy.   Then by the
Scheffe Theorem and Pinsker’s Inequality,
                               Ex [||z(y|x)||21 ] ≤ D(g(·|β0 , σ0 )||g(·|βbJ , σ
                                                                               b))                             (G.13)
                                                                                              √
                                                                               δ − log δ
               ≤ 2(E[log(g(y|x, β0 , σ0 ))] − E[log(g(y|x, βbJ , σ b))]) = Op ( √        ),
                                                                                   n
where D(P |Q) is the K-L divergence between two probability distribution P and Q.
  Now consider the characteristic functions of xβbJ (τ ) and xβ0 (τ ) conditional on x and given that
τ ∼ U [0, 1],
                                               R∞
                                                                 b)eisy dy
                                                    g(y|x, βbJ , σ
                                 φxβbJ (s|x) = −∞
                                                        φε (s|σ)
                                               R∞
                                                    g(y|x, β0 , σ0 )eisy dy
                                 φxβ0 (s|x) = −∞
                                                        φε (s|σ0 )
                                                                           R∞
Then for any x and s, |φxβbJ (s|x)φε (s|σ  b) − φxβ0 (s|x)φε (s|σ0 )| = | −∞ z(y|x)eisy dy| ≤ ||z(y|x)||1 .
                                  b) and dividing both sides by φε (s|σ)φxβ0 (s|x),
Defining m(s) := φε (s|σ0 )/φε (s|σ

                                         φxβbJ (s|x)              ||z(y|x)||1
                               m(s) −                    ≤                          .                          (G.14)
                                         φxβ0 (s|x)          |φxβ0 (s|x)φε (s|σ0 )|
Plugging in (G.14) back into (G.13), we have
                                               
                                              2
                                  φxβbJ (s|x)                  ||z(y|x)||21
                                                                                
                     Ex m(s) −
                                                ≤ Ex                                                         (G.15)
                                  φxβ0 (s|x)             |φxβ0 (s|x)φε (s|σ0 )|2

                                           s2
                                                                                       
                                                                                 1
                   ≤ Ex ||z(y|x)||21                                     2
                                    
                                                     = op Ex [||z(y|x)||1 ]               ,
                                     C 2 φε (s|σ0 )2                        φε (s|σ0 )2
where in the last step we require that s ∈ [−l, l] for some l > 0 such that |φxβ0 (s|x)| is bounded away
from zero. Using the fact that for any random variable a and any number b, V ar(a) ≤ E[(a − b)2 ],
we have that                                          
                                                     2                         !
                                        φxβbJ (s|x)               φxβbJ (s|x)
                          Ex  m(s) −                   ≥ V arx                 .
                                        φxβ0 (s|x)                φxβ0 (s|x)

Inequality (G.15) then implies that
                                                   !
                                     φxβbJ (s|x)                                   1
                            V arx                      -p Ex [||z(y|x)||21 ]              .                    (G.16)
                                     φxβ0 (s|x)                                φε (s|σ0 )
Applying Assumption 7, inequality (G.16) implies that
                                              
                                             2
                    φxβbJ (s|x) − φxβ0 (s|x)
                                                                                   
                                                                    2        1
              Ex                               = Op Ex [||z(y|x)||1 ]               .                        (G.17)
                            φxβ0 (s|x)                                  φε (s|σ0 )2
60                                                     HAUSMAN, LIU, LUO, AND PALMER

                                       φ       (s|x)                    φ       (s|x)−φxβ (s|x)
We can rewrite m(s) − φxxββJ (s|x) as (m(s) − 1) − xβJ φxβ (s|x)0         . Using that 12 a2 − b2 ≤ (a − b)2
                                           b                                b

                           0                                     0
for any a, b ∈ R, we can bound inequality (G.17) from below such that
                                                                                            
                                                            2                                2
    1 h               i        φ xβ J
                                      (s|x) −  φ xβ   (s|x)                    φ     (s|x)
      Ex |m(s) − 1|2 − E 
                                                    0
                                                               ≤ Ex  m(s) − xβJ
                                  b                                               b
                                                                                                      (G.18)
    2                                   φxβ0 (s|x)                             φxβ0 (s|x)
                                                                                                  
                                                                                   2        1
                                                                = Op Ex [||z(y|x)||1 ]               . (G.19)
                                                                                       φε (s|σ0 )2
Combining (G.18) with (G.17),
                                                                                                           
                         h
                                   2
                                     i                                                            1
                      Ex |m(s) − 1| = Op Ex [||z(y|x)||21 ]                                                     ,                         (G.20)
                                                                                             φε (s|σ0 )2
or, equivalently, for any s ∈ [−l, l] where l is some fixed constant,
                                                                   b)|2 = Op Ex [||z(y|x)||21 ] .
                                                                                               
                                               |φε (s|σ0 ) − φε (s|σ                                                                      (G.21)
                                                             b − σ0 ||2 = Op                                                 Ex [||z(y|x)||21 ] =
                                                                                                                                               
Applying Assumption 4.5 along with (G.21), it follows that ||σ
         √
             −
Op ( δ       √ log δ ).
               n
     If ||σ           cJ − β ∗ ||, then δ = ||σ
          b − σ|| > ||β       J               b − σ||, and it follows that ||σ          b−√σ0 ||2 = Op ( logn n ).
     If ||σ           cJ − β ∗ ||, then δ = ||βbJ − β ∗ ||, and ||σ
          b − σ|| ≤ ||β       J                        J             b − σ0 ||2 = Op ( δ −  √ log δ ).
                                                                                               n
                                                              √                     
                            2                log n ||βbJ −βJ∗ || √− log ||βbJ −βJ∗ ||
     Therefore, ||σb − σ0 || = Op max           n ,                n
                                                                                         .                                                     


     The following lemma will be instrumental in proving asymptotic normality.
                                                                                                                    Jn2r+2
Lemma 9. Under Assumptions 1, 3, 4, 5.1 (the mild ill-posed case), 7, and                                              n     → ∞, ||βbJ −βJ∗ || =
             1
                   1 !
                   2λ+1
     Jn2λ log 1 n
              2
Op                      .
                     n2



Proof. Our argument follows the proof of Lemma 4. Let z(y|x) := g(y|x, βbJ , σ0 ) − g(y|x, βJ∗ , σ0 ).
By the Scheffe Theorem and Pinsker’s Inequality,
  Ex [||z(y|x)||21 ] ≤ D(g(·|βbJ , σ0 )||g(·|βJ∗ , σ0 )) ≤ 2(E[log(g(y|x, βJ∗ , σ0 ))] − E[log(g(y|x, βbJ , σ0 ))]),
                                                                                                               (G.22)
where D(P |Q) is the K-L divergence between two probability distribution P and Q.
  By the maximal inequality, for any δ > 0,

         "                                                                               #                                            #
                                                                                                   Z    δ
                                |Gn log g(y|x, βJ∗ , σ0 ) − Gn log g(y|x, βJ , σ0 )| ≤ K
                                                                                                            p
     E           max                                                                                         log N (r, M, || · ||2 )dr ,
             ||βJ −βJ∗ ||2 <δ                                                                       0

where N (r, M, || · ||2 ) is the covering number of r balls on M , the space of β. K is a generic
constant. Since M is a bounded and co-monotone space (xβ(τ ) is monotone in τ for all x ∈ X ),
N (r, M, || · ||2 ) < δ dx . Therefore,
                   "                                                                 #
                                 |Gn log g(y|x, βJ∗ , σ0 ) − Gn log g(y|x, βJ , σ0 )| ≤ δ − log δ.
                                                                                         p
                E        max ∗
                            ||βJ −βJ ||2 <δ
            ERRORS IN THE DEPENDENT VARIABLE OF QUANTILE REGRESSION MODELS                                       61

Hence,                                                                            q
                     |Gn log g(y|x, βJ∗ , σ0 )
                                                                                           
                                                 − Gn log g(y|x, βJ , σ0 )| = Op δ − log δb ,
                                                                 b               b

where δb := ||βJ∗ − βbJq
                       ||. Using a similar argument as in the proof of Lemma 4, we can show that
  b − σ0 ||2 = Op ( √1n δb − log δ).
||σ                              b Thus,

                           E[log(g(y|x, βJ∗ , σ0 ))] − E[log(g(y|x, βbJ , σ0 ))]
                            1                               1
                         = √ Gn [log(g(y|x, βJ∗ , σ0 ))] − √ Gn [log(g(y|x, βbJ , σ0 ))]
                             n                               n
                                             ∗
                           + En [log(g(y|x, β , σ0 ))] − En [log(g(y|x, βbJ , σ0 ))]
                                                  J                                                          (G.23)

   The first term on the right-hand side of (G.23), √1n Gn [log(g(y|x, βJ∗ , σ0 ))]− √1n Gn [log(g(y|x, βbJ , σ0 ))],
              q
is Op ( √1n δb − log δ).
                     b For the second term on the right-hand side of (G.23), we have

                              En [log(g(y|x, βJ∗ , σ0 ))] − En [log(g(y|x, βbJ , σ0 ))]
                            =En [log(g(y|x, βJ∗ , σ0 ))] − En [log(g(y|x, βbJ , σ
                                                                                b))]
                                                      b))] − En [log(g(y|x, βbJ , σ0 ))].
                               + En [log(g(y|x, βbJ , σ

We know that En [log(g(y|x, βJ∗ , σ0 ))] − En [log(g(y|x, βbJ , σ b))] ≤ 0 by the first-order condition. We
also have that
                                                                                               q
                                                                                            1
       En [log(g(y|x, βbJ , σ                                          b − σ0 ||2 ) = Op ( √ δb − log δ).
                            b))] − En [log(g(y|x, βbJ , σ0 ))] = Op (||σ                              b
                                                                                             n
Combining the results on different terms in (G.23), we have
                                                                                   q
                                                                                1 b
                      E[log(g(y|x, βJ∗ , σ0 ))]   − E[log(g(y|x, βJ , σ0 ))]-p √ δ − log δ.
                                                                 b                       b
                                                                                 n
It follows that
                                                                                                q
                                                                                             1
         Ex [||z(y|x)||21 ] ≤ 2E[log(g(y|x, βJ∗ , σ0 ))] − E[log(g(y|x, βbJ , σ0 ))] = Op ( √ δb − log δ).
                                                                                                       b
                                                                                              n

  Now consider the characteristic functions of xβbJ (τ ) and xβJ∗ (τ ) conditional on x, τ ∼ U [0, 1]

                                                      R∞                      isy dy
                                                       −∞ g(y|x, βJ , σ0 )e
                                                                   b
                                     φxβbJ (s|x) =
                                                              φε (s|σ0 )
                                                      R∞          ∗        isy
                                                       −∞ g(y|x, βJ , σ0 )e dy
                                     φxβ (s|x) =
                                         ∗
                                         J
                                                              φε (s|σ0 )
It follows that
                                                                   Z   ∞
               |φxβbJ (s|x)φε (s|σ0 ) − φxβJ∗ (s|x)φε (s|σ0 )| =            z(y|x)eisy dy ≤ ||z(y|x)||1 .
                                                                       −∞

Then
                                                                       ||z(y|x)||1
                                    |φxβbJ (s|x) − φxβJ∗ (s|x)| ≤p                 .
                                                                        φε (s|σ0 )
62                                                  HAUSMAN, LIU, LUO, AND PALMER

Using the relationship between the CDF and characteristic function of a random variable x (Fx (w) =
1
    R ∞ exp(iws)φx (s)
2 − −∞        2πis     ds), we have that
                                              Z q
                                                  exp(iws)
                  FxβbJ (w) − FxβJ∗ (w) = lim              (φxβbJ (s|x) − φxβJ∗ (s|x))ds.
                                          q→∞ −q    2πis

Then since in our sieve setting βbJ and     ∗      th
                                          βJ are r -order spline functions with grid interval size
of order O(1/Jn ), we know that max φxβbJ (s|x) , φxβJ∗ (s|x) ≤ Jn sc for some constant c > 0.
Therefore,                                h                    i
                                      Ex FxβbJ (w) − FxβJ∗ (w)
           Z q                                 Z ∞                                 
                 exp(iws) ||z(y|x)||1                    1
     ≤ Ex                               ds + Ex 2            φ b (s|x) − φxβJ (s|x) ds .
                                                                            ∗                (G.24)
             −q    2πis    |φε (s|σ0 )|             q  2πs xβJ
The first term of the right-hand side of equation (G.24) is weakly bounded from above by
                        Z q                                     λ p              
                     1         1                                 q              1
                                      dsEx [||z(y|x)||1 ] = op       δ(− log δ)
                                                                     b       b  4
                    2π −q sφε (s|σ0 )                           n1/4
where λ is the degree of mild ill-posedness. The second term of (G.24) is weakly bounded by Jn 4c
                                                                                                q -
                                                                                      λ p                                                      
Jn /q. Putting these together, the right-hand side of (G.24) has an upper bound of Op nq1/4 δ(−    b 41 +
                                                                                            b log δ)                                       Jn
                                                                                                                                            q
for an arbitrary q.             h                        i
         b      ∗
   Since δ = ||β − βJ ||2 = O Ex F b (w) − Fxβ (w)
                     b                               ∗        , we have
                       J                                 xβJ                 J



                                                                   q λ pb
                                                                                                   
                                                    δb = Op                     b 41 + Jn
                                                                        δ(− log δ)                      .
                                                                  n 1/4                 q
             1                                                                                                      1
                 
If δb = Op   n       ,then the conclusion holds. If δb converges to 0 slower than                                   n ,we   have
                                                  λ p                      
                                                     q            1     J n
                                         δb = Op           δb log 4 n +       ,
                                                    n1/4                 q
which implies
                                                                       q 2λ
                                                                                           
                                                                                 1     Jn
                                                       δb = Op           1/2
                                                                             log 2 n +          .
                                                                       n                q
                                             1
                                    1        2λ+1
                                Jn n 2
The optimal q is                   1                . Then we have
                               log 2 n

                                                                                        1
                                                                                                            !
                                                                                   2λ log n
                                                                                         2          1
                                               δb = ||βbJ − βJ∗ || = Op          (Jn    1     )   2λ+1          .
                                                                                       n2
                                                                                                                                       




                                            b) ∈ ΘrJ is the rth -order sieve estimator. By the consis-
Proof of Theorem 2. Suppose θbJ = (βbJ (·), σ
                                                                    p
                                                                    − 0. It is easy to see that Θr ⊂ Θ.
tency of the sieve estimator established by Lemma 3, ||θbJ − θ0 ||2 →                                                              J
                                                                                                        1
By Lemma 4, σ   b will always converge to σ0 at rate of at least n− 4 . By construction     of the sieve,
there exists a set of parameters (βJ∗ , σ0 ) in ΘrJ such that ||βJ∗ − β0 ||2 = Op J r+1
                                                                                    1
                                                                                        
                                                                                          .
                                                                                                                        n
           ERRORS IN THE DEPENDENT VARIABLE OF QUANTILE REGRESSION MODELS                                                                                             63
                                          √
  Let Gn denote the operator               n(En − E). Then we have


                                                                                                                                             
    1           ∂ log g ∂ log g                                 ∂ log g ∂ log g                                          ∂ log g ∂ log g
   √ Gn                ,                            = En               ,                                    −E                  ,
     n            ∂β      ∂σ                                      ∂β      ∂σ                                               ∂β      ∂σ
                                        (βbJ ,σb)                                               (βbJ ,σb)                                                 (βbJ ,σb)
                                                                                           
                                                                    ∂ log g ∂ log g
                                                    = −E                   ,                                 ,                                               (G.25)
                                                                      ∂β      ∂σ
                                                                                                 (βbJ ,σb)
                                                                 
where we used the first-order condition En ∂ log        g ∂ log g
                                                          ,                   = 0. For the left-hand side of
                                                                    (βJ ,σb)
                                                     ∂β       ∂σ      b
                                           
                             ∂ log g ∂ log g
(G.25), by Donskerness of      ∂β , ∂σ         (β̃,σ̃) (β̃, σ̃) ∈ Θ , we have
                                                                                                                                   
             1           ∂ log g ∂ log g                           1                                    ∂ log g ∂ log g
            √ Gn                ,                               = √ (1 + op (1))Gn                             ,                                      ,
              n            ∂β      ∂σ                               n                                     ∂β      ∂σ
                                                    (βbJ ,σb)                                                                             (β0 ,σ0 )

which is asymptotically Gaussian. Next, we work on the right-hand side of (G.25). It can be
expanded as
                           
          ∂ log g ∂ log g
    −E            ,
            ∂β        ∂σ
                               (βbJ ,σb)
                                                                             
                                                                                                 
            ∂ log g   ∂ log g                   ∂ log g   ∂ log g                   ∂ log g   ∂ log g
  = − E            ,                      −E           ,                     −E           ,                     ,
              ∂β        ∂σ                        ∂β        ∂σ                        ∂β        ∂σ
                                  (βJ ,σb)
                                   b                                  ∗
                                                                    (βJ ,σ0 )                             ∗
                                                                                                        (βJ ,σ0 )
                                                                                                          (G.26)
and then a Taylor expansion of the term inside brackets of (G.26) gives
                                                                               
                    IβJ ,σ0 bbJ − b∗J , σ
                                        b − σ0 + Op ||bbJ − b∗J ||2 + ||σ
                                                                        b − σ0 ||2 ,                                                                         (G.27)

where bbJ and b∗J denote the coefficient vectors
                                             for the spline
                                                                  functions in βbJ and βJ∗ . The second
term on the right-hand side of (G.26), −E ∂ log  g ∂ log g
                                               ∂β , ∂σ       (β ∗ ,σ0 )
                                                                        , equals
                                                                                                J

                                                                                                              
                                  ∂ log g ∂ log g                                       ∂ log g ∂ log g
                          E              ,                                −E                   ,                                  ,
                                    ∂β      ∂σ                                            ∂β      ∂σ
                                                            (β0 ,σ0 )                                                (βJ∗ ,σ0 )
                                                               
because (β0, σ0 ) is the truth and therefore E ∂ log
                                                   ∂β
                                                      g ∂ log g
                                                       ,  ∂σ                                                 = 0.
                                                                                                (β0 ,σ0 )
  Since ||βJ∗ − β0 || = Op J r+1
                              1
                                 
                                   , by continuity
                                    n

                                                                                                              
                                   ∂ log g ∂ log g                                       ∂ log g ∂ log g
                           E              ,                                 −E                  ,
                                     ∂β      ∂σ                                            ∂β      ∂σ
                                                                (β0 ,σ0 )                                                (βJ∗ ,σ0 )
                                                                           
                                                                      1
                         =Op ||βJ∗ − β0 || = Op
                                          
                                                                                .
                                                                    Jnr+1
64                                             HAUSMAN, LIU, LUO, AND PALMER

Combining for both sides of (G.25), we have
                                            
          1                  ∂ log g ∂ log g
         √ (1 + op (1))Gn           ,
           n                   ∂β      ∂σ
                                                                  (β0 ,σ0 )
                                                                                               
                     
                              ∗
                                                 
                                                            ∗  2              2
                                                                                           1
         = − IβJ∗ ,σ0 bJ − bJ , σ
                      b           b − σ0 + Op ||bJ − bJ || + ||σ
                                                     b               b − σ0 || + Op               . (G.28)
                                                                                          Jnr+1
        b −σ0 ||2 = op √1n , it is dominated by the Gaussian term on the left-hand side of (G.28). By
                            
Since ||σ
                                        4λ2 +6λ
Lemma 9 and the condition that Jn n log(n) → 0, we know that ||βbJ − βJ∗ ||2 = Jn−λ op ||βbJ − βJ∗ ||
                                                                                                         

and ||bbJ − b∗J ||2 = Jn−λ op ||bbJ − b∗J || , for Jn satisfying the growth rate conditions stated in the
                                            

theorem. Therefore, (G.28) becomes
                                                                  
                         − (1 + op (1))IβJ∗ ,σ0 bbJ − b∗J , σ
                                                            b − σ0
                                                                                       
                         1                       ∂ log g ∂ log g                      1
                      = √ (1 + op (1))Gn                ,                 + Op              .       (G.29)
                          n                        ∂β       ∂σ                      Jnr+1
                                                                                       (β0 ,σ0 )

By continuity of the information matrix as a function of β, we know that the smallest eigenvalue of
IβJ∗ ,σ0 is on the same order as the smallest eigenvalue of Iβ0 ,σ0 , i.e. bounded by Jcλ from below with
                                                                                               n
c as a constant. Hence (G.29) implies
                                                                               
                                            ∗                λ         1     1
                                     bbJ − b , σ
                                            J b − σ0 = Jn Op               ,√ ,
                                                                     Jnr+1    n
or                                                                                                 
                                               1          bJ − β ∗ , σ              λ       1      1
                βbJ − β0 , σ
                           b − σ0 = Op               +   β       J     − σ 0   =  Jn pO         , √     ,
                                             Jnr+1                                        Jnr+1
                                                                     b
                                                                                                    n
establishing the convergence rate of the Sieve estimator. For asymptotic normality, note that if
         √
Jnr+1 / n → ∞, then the first term on the right-hand side of (G.29) dominates the second term on
the right-hand side of (G.29), so we have
                                                                                        
                          
                                 ∗
                                                  1                     ∂ log g ∂ log g
                   IβJ ,σ0 bJ − bJ , σ
                           b         b − σ0 =     √   (1 + op (1))Gn            ,                   .
                                                    n                      ∂β        ∂σ
                                                                                                                         (β0 ,σ0 )

Therefore,
                                                                              
                
                 bbJ − b∗ , σ
                                   1             −1           ∂ log g ∂ log g
                        J b − σ0 = √ (1 + op (1))IβJ∗ ,σ0 Gn          ,                                                              .
                                     n                           ∂β      ∂σ
                                                                                                                         (β0 ,σ0 )

Since Gn   ∂ log g
             ∂β ,
                     ∂ log g 
                      ∂σ         (β0 ,σ0 )
                                             is asymptotically normal, Iβ−1
                                                                          ∗
                                                                            ,σ0 Gn
                                                                                                   ∂ log g
                                                                                                     ∂β ,
                                                                                                             ∂ log g 
                                                                                                              ∂σ         (β0 ,σ0 )
                                                                                                                                         is also asymp-
                                                                                   J

totically normal, with distribution               N (0, Iβ−1
                                                           ∗     ΩGn Iβ−1
                                                                        ∗     ),   where
                                                           J ,σ0        J ,σ0


                                                          ∂ log g ∂ log g                     
                                  ΩGn := Var Gn                  ,                 (β0 ,σ0 )
                                                                                                   →p Iβ0 ,σ0 .
                                                            ∂β      ∂σ
Let κJ denote the smallest eigenvalue for Iβ0 ,σ0 . By Assumption, κJ ≥ C/J λ for some generic
positive constant C > 0. Since ||βJ∗ − β0 || = O( J r+1
                                                    1
                                                        ),
                                                                                                       1
                                    ||Iβ0 ,σ0 − IβJ∗ ,σ0 ||∞ = O(||βJ∗ − β0 ||) = O(                       ).
                                                                                                     J r+1
           ERRORS IN THE DEPENDENT VARIABLE OF QUANTILE REGRESSION MODELS                                         65

By the growth conditions, we have r + 1 > λ, so ||Iβ0 ,σ0 − IβJ∗ ,σ0 ||2 = O( J1r ) = o(κJ ). Hence,
Iβ−1
   ∗     = (Iβ0 ,σ0 − Iβ0 ,σ0 + IβJ∗ ,σ0 )−1 = (Iβ0 ,σ0 (1 + o(1)))−1 → Iβ−1     .
   J ,σ0                                                                   0 ,σ0
                            −1           −1           −1
    Let ΩJ denote κJ Iβ ∗ ,σ0 ΩGn Iβ ∗ ,σ0 → κj Iβ0 ,σ0 . We have
                             J            J

                                  √                          
                                                                d
                                    nκJ bbJ − b∗J , σ b − σ0 −  → N (0, ΩJ ) = Op (1).
         √              
Then nκJ σ − σ0 converges to N (0, ΩJ,σ ) in distribution, where ΩJ,σ denotes the submatrix of
ΩJ for σ. Since the largest eigenvalue of ΩJ is bounded by a constant, the largest eigenvalue of ΩJ,σ
                                                         √
is bounded by the same constant. Similarly nκJ bbJ − b∗J ) converges to N (0, ΩJ,b ) in distribution,
where ΩJ,b is the submatrix of ΩJ for b. It follows that for each τ ,
                               √                          d                0
                                 nκJ βbJ (τ ) − βJ∗ (τ ) − → S J (τ ) ⊗ Ddx N (0, ΩJ,b ),
where Ddx denotes a dx × dx identity matrix, and S J (τ ) denotes S1 (τ ), · · · , SJ+r (τ ) . By assump-
                                                                                                 
                                                                                                   0
tionthat J+r                2                                                            J           ΩJ,b S J (τ )⊗
            P
              l=1 |Sk (τ )| is bounded by a constant, the largest eigenvalue of S (τ )⊗Dd       x
                                                                        ∗                      1
Ddx is also bounded by a constant. Note that because ||βJ (τ ) − β0 (τ )|| = Op J r+1 , the limiting
                   √                                                                        √ n
distribution of nκJ βbJ (τ )−β ∗ (τ ) is the same as the limiting distribution of nκJ βbJ (τ )−β0 (τ ) .
                                                                                                                
                                   J
Thus we have
                   √                    d               0
                                         → S J (τ ) ⊗ Ddx N (0, ΩJ,b ) S J (τ ) ⊗ Ddx ,
                                                                                     
                 nκJ βbJ (τ ) − β0 (τ ) −
                  0
where S J (τ )⊗Ddx N (0, ΩJ,b ) S J (τ )⊗Ddx is a positive definite matrix with the largest eigenvalue
                                            

bounded by a constant.                                                                              


Proof of Theorem 3. Following the same argument as those in the proof of Theorem 2, we have:
                                                                                    −1
                       I(βbJ − βJ∗ , σ
                                     b − σ0 ) + Op (||(βbJ − βJ∗ , σ
                                                                   b − σ0 )||22 ) = √ Gn,Jn .                 (G.30)
                                                                                     n

By setting Jn such that exp(λJn
                                 )
                            √ n = 1 , we have ( 1 − η) log(n) < Jn < 1 log(n), for any small
                                      Jn                2λ                        2λ
η > 0 and n large enough.
  By Assumption 5.2, the minimum eigenvalue of I is bounded by C exp(−λJn ) for some λ > 0
and C > 0. It follows that ||I(βbJ − βJ∗ , σ
                                           b − σ0 )|| ≥ C exp(−λJn )||(βbJ − βJ∗ , σ
                                                                                   b − σ0 )||2 .

    (a) If ||(βbJ − βJ∗ , σ
                          b − σ0 )||2 ≥ C1 /C exp(−λJn ), for some constant C1 large enough, then with
        probability approaching 1, we have ||I(βbJ − βJ∗ , σ     b − σ0 )||2 > 2Op (||(βbJ − βJ∗ , σ
                                                                                                   b − σ0 )||22 ),
                              ∗
        where Op (||(β − βJ , σ            2
                                 b − σ0 )||2 ) is the higher order residual term in the equation (G.30). It
        follows that ||(βJ − βJ∗ , σ
                            b         b − σ0 )||2 -p exp(λJ )
                                                        √ n .
                                                         n
                                  b − σ0 )||2 ≤ C1 /C exp(−λJn ) ≤ C1 /Cn (1/2 − ηλ) = o( exp(λJ
    (b) Else we have ||(β − βJ∗ , σ                                                              )
                                                                                             √ n ).
                                                                                              n

                                                   b − σ0 )||2 = Op ( exp(λJ
Combining the two situations, we have ||(β − βJ∗ , σ                         )
                                                                         √ n ).
                                                                          n
  By construction of the sieve, ||(β − βJ∗ , σ
                                             b − σ0 )||2 = O( J1n ). Hence, ||(β − βJ∗ , σ
                                                                                         b − σ0 )||2 =
O(max( exp(λJ
           n
              )
          √ n , 1 )). By assumption, we set Jn such that
                Jn
                                                                       exp(λJn )
                                                                          √
                                                                            n
                                                                                   =    1
                                                                                       Jn
                                                                                                    1
                                                                                            = O( log(n) ). Therefore,
the sieve estimator satisfies: ||(β − βJ∗ , σ                     1
                                            b − σ0 )||2 = Op ( log(n) ).                                           


Proof of Lemma 5. A bootstrap process can be considered as putting non-negative weights wi,n
on the ith observation. We require E[wi,n ] = 1, and E[wi,n ]2 = σw,n
                                                                  2   < ∞. One example is to
66                                          HAUSMAN, LIU, LUO, AND PALMER

let (wi,1 , ..., wi,n ) ∼ M ultinomial(n, n1 , ..., n1 ), which is the nonparametric pairs bootstrap recom-
mended in the text and used in the simulation and empirical results. The bootstrapped estimator
(βbJb , σ
        bb ) should satisfy the first-order condition
                                        "                                   #
                                                  b          ∂ log g b
                                       b ∂ log g
                                     En                    ,                  = 0.
                                            ∂β βbb ,σbb         ∂σ βbb ,σbb
                                                           J                 J

   By Assumption 4.3, E[supβ,σ |wi,n log g(yi |xi , β, σ)|] < ∞. Moreover, by Assumption 4.2, wi,n log g(yi |xi , β, σ)
satisfies that

              E[|w log g(yi |xi , β, σ) − w0 log g(yi |xi , β 0 , σ 0 )|] ≤ C1 (|w − w0 | + ||(β, σ) − (β 0 , σ 0 )||)
for some generic constant C1 > 0. By the ULLN for any (β, σ) ∈ M × Σ, Enb [log g(β, σ)] →p
En [log g(β, σ)], which converges to E[log g(β, σ)] with probability approaching 1. Since M × Σ is
compact and identification holds, it must be that (βbJb , σ
                                                          bb ) →p (β0 , σ0 ). Therefore,

                                                 ||(βbJb , σ
                                                           bb ) − (βbJ , σ
                                                                         b)|| →p 0.
   Denote G(β, σ) = ( ∂ log g ∂ log g
                         ∂β , ∂σ ). By stochastic equicontinuity,
                                                                          
           b      b   b
                                       b    b
                                                     b
                                                                                      1 
          En G βJ , σ
                b   b       − En G βJ , σ
                                       b   b               b              b  b + op √ .
                                                               b − En G βJ , σ
                                                   = En G βJ , σ
                                                                                         n
                                                          
In the above equation, Enb G βbJb , σ bb = En G βbJ , σ
                                                          
                                                          b = 0 by the first-order condition. Thus we
have
                                                                          
                                           1 
       Enb                                            b     b
                                                                             
              G βJ , σ
                b             b    b + op √ = − En G βJ , σ
                     b − En G βJ , σ                 b    b       − En G βJ , σ
                                                                         b    b     .                                    (G.31)
                                            n
   Next we will show that the left-hand side of (G.31) is asymptotically normal and the right-hand
side of (G.31) can be written as a matrix multiplied by bbbJ , σ
                                                               bb − (bbJ , σ
                                                                           b), where bbJ and bbbJ are the
                                                                 
                                                  b
coefficients for the sieve functions in βbJ and βbJ .
   For the left-hand side of (G.31),
                                                                              
                        b                                 b
                                                                              
                      En G βJ , σ
                              b  b − En G βJ , σ
                                               b     b = En wi,n − 1 G βJ , σ
                                                                            b  b

     Note that
                      √
                                                                                         
                             n                                a                          0
                               Eb
                                                                               
                          n               wi,n − 1 G βbJ , σ
                                                           b ∼ N 0, E G βbJ , σ
                                                                              b G βbJ , σ      .
                            n−1 n
                                                                                        b
                                                                                         
                                               Jnλ                                    0                   Jnλ
By Theorem 2, ||(βJ , σ
                  b   bJ )−(β0 , σ0 )|| = Op ( √n ), therefore, E G βJ , σ
                                                                      b   b G βJ , σ
                                                                               b    b       = Iβ0 ,σ0 +Op ( √ n
                                                                                                                ),
                                          
                                       0                      Jnλ
where Iβ0 ,σ0 := E G β0 , σ0 G β0 , σ0 . By assumption, √         n
                                                                    /mineigen(Iβ0 ,σ0 ) → 0 as n → ∞, thus
                               λ
                       Jn
Iβ0 ,σ0 dominates Op ( √ n
                           ).
   For the right-hand side of (G.31), by Donskerness of M × Σ and stochastic equicontinuity, we
have
                                                                        
                    b     b
                                                   b     b
                                                                                   1
            En G βJ , σ
                   b    b       − En G βJ , σ
                                       b    b = E G βJ , σ
                                                    b    b            b    b + op ( √ ),
                                                                − E G βJ , σ
                                                                                      n
                  ERRORS IN THE DEPENDENT VARIABLE OF QUANTILE REGRESSION MODELS                                                                                              67

where the remainder term op ( √1n ) does not affect the derivation further and is dropped.
  By Taylor expansion,
                                                                                      
         b    b
                                              b           b           b           b    2
  E G βbJ , σ
            b                   b = IβbJ ,σb (bbJ − bbJ , σ
                    − E G βbJ , σ                         b −σb) + O (bbJ − bbJ , σ
                                                                                  b −σb)

                                                                                                                                  (bbbJ − bbJ , σ
                                                                                                                                                bb − σ
                                                                                                                             
                                                           = Iβ0 ,σ0 + O (bbJ − b0 , σ
                                                                                     b − σ0 )                                                        b)
                                                                                      
                                                                                     2
                                                           + O (bbbJ − bbJ , σ
                                                                             bb − σ
                                                                                  b)
                                                                                                                                                                         
                                                                                                                                                                      2
                                                                              + op (1) (bbbJ − bbJ , σ
                                                                                                     bb − σ                                 (bbbJ               b
                                                                                      
                                                           = Iβ0 ,σ0                                      b) + O                                    − bbJ , σ
                                                                                                                                                            b −σ
                                                                                                                                                               b)

     Combining different terms in (G.31), we have

                                                                                                                                         Jλ
                                                                                                          
                                                                                                       2          1
 (1 + op (1))Iβ0 ,σ0 (bbbJ − bbJ , σ
                                   bb − σ
                                        b) + O                        (bbbJ − bbJ , σ
                                                                                    bb − σ
                                                                                         b)                    = √ N (0, (Iβ0 ,σ0 + Op ( √n ))). (G.32)
                                                                                                                   n                      n
                                                                                                               2
 Similar to Theorem 2, we need to show that (bbbJ −bbJ , σ   b) is dominated by (1+op (1))Iβ0 ,σ0 (bbbJ −
                                                         bb −σ
      bb − σ
bbJ , σ    b). Our strategy is to use stochastic equicontinuity, which implies that

                                                                                                                                                    
                                                                                                                                                                     1 
      Enb       log g βbJb , σ
                             bb                        log g βbJb , σ
                                                                    bb                Enb
                                                                                                                                                    
                                            − En                                  =             log g βbJ , σ
                                                                                                            b              − En       log g βbJ , σ
                                                                                                                                                  b            + op √
                                                                                                                                                                      n
or
                                                                                                                                                    
                                                                                                                                                                     1 
                                                   log g βbJb , σ
                                                                bb                Enb                                      Enb       log g βbJb , σ
                                                                                                                                                  bb
                                                                                                                                                    
      En        log g βbJ , σ
                            b           − En                                  =             log g βbJ , σ
                                                                                                        b              −                                       + op √ ,
                                                                                                                                                                      n
                                            
where Enb log g βbJ , σ
                      b − Enb log g βbJb , σ
                                           bb ≤ 0 by optimality of βbJb , σ
                                                                          bb . Thus we have
                                                                          

                                                                                                                
                                                                                                                             1 
                                                                                           log g βbJb , σ
                                                                                                        bb
                                                                                                              
                                             En       log g βbJ , σ
                                                                  b           − En                                     ≤ op √ .
                                                                                                                              n
On the other hand, we know that
                                                              
                                         b − En log g βbJb , σ
                                                             bb ≥ 0
                                                              
                          En log g βbJ , σ
                         
by optimality of βbJ , σ
                       b . Hence,
                                                                                             1 
                                             En log g(βbJb , σ
                                                             bb ) − En log g(βbJ , σ
                                                                                   
                                                                                   b ) = op √ .
                                                                                              n
With this, we can apply similar arguments as in Lemma 4 and Lemma 9 to show that
                                                   1
                                       b|| = op n− 4
                                  bb − σ
                                ||σ
                                                          1   1 !
                                                     2λ log n
                                                           2   2λ+1
                                  b
                              ||βJ − βJ || = Op
                                b     b             Jn     1
                                                          n2
                                                                           √
  By an argument similar to the one in Theorem 2, (G.32) implies that nκJ (βbJb − βbJ ) and
√                                            √                   √
       bb − σ
  nκJ (σ    b) have the same distributions as nκJ (βbJ − β0 ) and nκJ (σ
                                                                       b − σ0 ).          
