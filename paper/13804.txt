                                 NBER WORKING PAPER SERIES




                                  PREDICTIVE SYSTEMS:
                           LIVING WITH IMPERFECT PREDICTORS

                                            Lubos Pastor
                                         Robert F. Stambaugh

                                         Working Paper 13804
                                 http://www.nber.org/papers/w13804


                      NATIONAL BUREAU OF ECONOMIC RESEARCH
                               1050 Massachusetts Avenue
                                 Cambridge, MA 02138
                                     February 2008




Helpful comments were received from the audiences at the Fall 2006 NBER Asset Pricing Meeting,
2006 Wharton Frontiers of Investing conference, 2007 Western Finance Association conference, 2007
European Finance Association conference, 2007 ESSFM at Gerzensee, 2007 Vienna Symposia in
Asset Management, Tel Aviv University Conference in Honor of Shmuel Kandel, Boston College,
Goldman Sachs, Hong Kong University of Science and Technology, National University of Singapore,
New York University, Norwegian School of Economics and Business Administration (Bergen), Norwegian
School of Management (Oslo), Singapore Management University, University of California at San
Diego, University of Chicago, University of Iowa, University of Michigan, University of Pennsylvania,
University of Texas at Austin, and University of Texas at Dallas. We also thank Jacob Boudoukh,
Ken French, Cam Harvey, Jesper Rangvid, Cesare Robotti, Ross Valkanov, Pietro Veronesi, and especially
Jonathan Lewellen, John Cochrane, and two anonymous referees for many helpful suggestions. The
views expressed herein are those of the author(s) and do not necessarily reflect the views of the National
Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2008 by Lubos Pastor and Robert F. Stambaugh. All rights reserved. Short sections of text, not to
exceed two paragraphs, may be quoted without explicit permission provided that full credit, including
© notice, is given to the source.
Predictive Systems: Living with Imperfect Predictors
Lubos Pastor and Robert F. Stambaugh
NBER Working Paper No. 13804
February 2008
JEL No. G1,G11,G12

                                              ABSTRACT

We develop a framework for estimating expected returns---a predictive system---that allows predictors
to be imperfectly correlated with the conditional expected return. When predictors are imperfect, the
estimated expected return depends on past returns in a manner that hinges on the correlation between
unexpected returns and innovations in expected returns. We find empirically that prior beliefs about
this correlation, which is most likely negative, substantially affect estimates of expected returns as
well as various inferences about predictability, including assessments of a predictor's usefulness. Compared
to standard predictive regressions, predictive systems deliver different and more precise estimates
of expected returns.


Lubos Pastor
Graduate School of Business
University of Chicago
5807 South Woodlawn Ave
Chicago, IL 60637
and NBER
lubos.pastor@chicagogsb.edu

Robert F. Stambaugh
Finance Department
The Wharton School
University of Pennsylvania
Philadelphia, PA 19104-6367
and NBER
stambaugh@wharton.upenn.edu
1.         Introduction

Many studies in finance analyze comovement between expected asset returns and various observ-
able quantities, or “predictors.” A question of frequent interest is how x t , a vector of predictors
observed at time t , is related to  t , the conditional expected return defined in the equation

                                               r tC1 D  t C u tC1 ;                                              (1)

where r tC1 denotes the stock return from time t to time t C 1, and the unexpected return u tC1
has mean zero conditional on information available at time t . One approach to modeling expected
returns is to use a “predictive regression” in which r tC1 is regressed on x t and the expected return
is given by  t D a C b 0 x t ; where a and b denote the regression’s intercept and slope coefficients.1
This approach seems too restrictive in modeling expected return as an exact linear function of
the observed predictors. It seems more likely that the predictors are imperfect, in that they are
correlated with  t but cannot deliver it perfectly.

   At the same time, the predictive regression approach seems too lax in ignoring a likely eco-
nomic property of the unexpected return—its negative correlation with the innovation in the ex-
pected return. For example, if the expected return obeys the first-order autoregressive process,

                                           tC1 D ˛ C ˇ t C w tC1 ;                                              (2)

then it seems likely that the correlation between the unexpected return and the innovation in the
expected return is negative, or that uw  .u tC1 ; w tC1 / < 0. That is, unanticipated increases in
expected future returns (or discount rates) should be accompanied by unexpected negative returns.
While it is possible for uw < 0 to be violated, we argue that such violations are unlikely. The
likely negativity of uw , which is not exploited in estimating the predictive regression, emerges as
an important consideration in estimating expected returns when predictors are imperfect.

         We develop an approach to estimating expected returns that generalizes the standard predictive
regression approach. The framework we propose, which we term a predictive system, allows the
predictors in x t to be imperfect, in that  t ¤ a C b 0 x t . The predictive system also allows us
to explore roles for a variety of prior beliefs about the behavior of expected returns, chief among
which is the belief that unexpected returns are negatively correlated with innovations in expected
     1
     Of the many studies that estimate predictive regressions for stock returns, some early examples include Fama and
Schwert (1977), Rozeff (1984), Keim and Stambaugh (1986), Campbell (1987), and Fama and French (1988). There
is also a substantial literature analyzing econometric issues associated with predictive regressions, including Mankiw
and Shapiro (1986), Stambaugh (1986, 1999), Nelson and Kim (1993), Elliott and Stock (1994), Cavanagh, Elliot,
and Stock (1995), Ferson, Sarkissian, and Simin (2003), Lewellen (2004), Campbell and Yogo (2006), Jansson and
Moreira (2006), and Lettau and van Nieuwerburgh (2007).


                                                          1
returns (uw < 0). We find that, compared to predictive regressions, predictive systems deliver
different and more precise estimates of expected returns. When predictors are imperfect, their
predictive ability is supplemented by information in lagged returns as well as lags of the predictors,
and the predictive system delivers that information via a parsimonious model. The correlation uw
plays a key role in determining how that additional sample information is used as well as how
important that information is in explaining variation in expected returns.

   The additional information in lagged returns is used in an interesting way. Suppose that recent
returns have been unusually low. On one hand, one might think that the expected return has
declined, since a low mean is more likely to generate low realized returns, and the conditional mean
is likely to be persistent. On the other hand, one might think that the expected return has increased,
since increases in expected future returns tend to produce low realized returns. When uw is
sufficiently negative, the latter effect outweighs the former and recent returns enter negatively
when estimating the current expected return. At the same time, more distant past returns enter
positively because they are more informative about the level of the unconditional expected return
than about recent changes in the conditional expected return.

   We illustrate the role of lagged returns in a simplified setting where historical returns are the
only available sample information (D t ). Suppose, for example, that an investor in January 2000 is
forming an expectation of the stock market return over the following quarter based on the post-war
history of realized market returns. Does the dramatic rise in stock prices in the 1990s increase or
decrease the investor’s expectation of future return? The answer depends on the extent to which
the 1990s’ bull market was caused by unexpected declines in expected returns. The conditional
expected stock return in this simplified setting is just a weighted average of all past realized returns,
                                                         t 1
                                                         X
                                      E.r tC1 jD t / D         s r t s ;
                                                         sD0

and the weights s depend on uw . For example, if this investor believes that uw D 0:85, so
that 72% of the variance in unexpected returns is due to changes in expected returns (the estimates
of Campbell (1991) are in that neighborhood), then returns realized during the most recent decade
receive negative weights, while the returns from the previous four decades receive positive weights.
The investor in this example views the 1990s’ bull market as a bearish indicator.

   Imperfection in predictors complicates inference about their relations to expected return. We
show that if predictors are imperfect, the residuals in the predictive regression of r tC1 on x t are
serially correlated. This correlation is often ignored when computing standard errors in predictive
regressions. The serial correlation in residuals joins other features of predictive regressions that are
already well known to complicate inferences, especially in finite samples, such as persistence in the

                                                    2
predictors and correlation between the residuals and innovations in the predictors (e.g., Stambaugh,
1999). Using our alternative framework—the predictive system—we develop a Bayesian approach
that allows us to conduct clean finite-sample inference about various properties of the expected
return. This approach also allows us to incorporate prior beliefs about uw .

   A striking example of the importance of such prior beliefs is provided by regressing post-war
U.S. stock market returns on the “bond yield,” defined as minus the yield on the 30-year Treasury
bond in excess of its most recent 12-month moving average. That variable receives a highly signifi-
cant positive slope (with a p-value of 0.001) in the predictive regression, but its AR(1) innovations
are positively correlated with the residuals in that regression. The latter correlation, opposite in sign
to what one would anticipate for uw , suggests that the bond yield is a rather imperfect predictor
of stock returns. When judged in a predictive system, the bond yield’s importance as a predictor
depends heavily on prior beliefs about uw . With noninformative beliefs about uw , the bond yield
appears to be a very useful predictor; for example, the posterior mode of its conditional correlation
with  t is 0.9. However, with a more informative belief that innovations in expected returns are
negatively correlated with unexpected returns and explain at least half of their variance, the bond
yield’s conditional correlation with  t drops to 0.2. With the more informative belief, the current
value of the bond yield explains only 3% of the variance of  t . Adding lagged unexpected returns
allows the system to explain 86% of this variance, and further adding lagged predictor innovations
increases the fraction of explained variance of  t to 95%.

    Prior beliefs also affect the predictive system’s advantage in explanatory power over the predic-
tive regression. In the same bond yield example, with noninformative prior beliefs, the predictive
system produces an estimate of  t that is 1.4 times more precise than the estimate from the pre-
dictive regression. With the more informative beliefs, though, the system’s estimate is 12.5 times
more precise. We measure improvements in precision by improvements in explanatory power.
Specifically, we compute the posterior mean of the ratio of the R2 from the predictive regression
to the R2 from the regression of r tC1 on the return forecast from the predictive system.

   We also include as predictors two more familiar choices, the market’s dividend yield and the
consumption-wealth variable “CAY” proposed by Lettau and Ludvigson (2001). Prior beliefs
about uw play a less dramatic role with these predictors than with the bond yield, but differ-
ent prior beliefs can nevertheless produce substantial differences in estimated expected returns.
We assess the economic significance of these expected return differences by comparing average
certainty equivalents for mean-variance investors whose risk aversion would dictate an all-equity
portfolio (i.e., no cash or borrowing) when expected return and volatility equal their long-run sam-
ple values. When all three predictors are included, an investor with the more informative belief


                                                   3
mentioned above would suffer an average quarterly loss of 1.5% if forced to hold the portfolio
selected each quarter by an investor who estimates expected return by the maximum likelihood
procedure (which reflects noninformative views about all parameters, including uw ).

   The predictive system, along with our Bayesian approach, also allows us to compute multi-
period return variances that incorporate imperfect predictors and parameter uncertainty. We find
that prior beliefs about uw play an important role here as well. In our example, using the div-
idend yield as the predictor, the five-year return variance is about 33 percent higher with a non-
informative prior for uw than with the more informative prior discussed above. More negative
values of uw imply stronger mean reversion in stock returns, resulting in lower long-horizon vari-
ances. We also find that the prior for uw has a much larger effect at longer investment horizons.

   Ferson, Sarkissian, and Simin (2003) show that persistent predictors may exhibit spurious pre-
dictive power in finite samples even if they have no such power in population (e.g., if they have
been data-mined). We provide tools that can be helpful in avoiding the spurious regression prob-
lem. A spurious predictor is unlikely to produce expected return estimates whose innovations are
substantially negatively correlated with unexpected returns. Therefore, under an informative prior
about this correlation, a predictive system would likely find the spurious predictor to be almost
uncorrelated with  t . The basic intuition holds also outside the predictive system framework: if a
predictor does not generate a negative correlation between expected and unexpected returns, it is
unlikely to be highly correlated with the true conditional expected return.

    This study is clearly related to an extensive literature on return predictability, but it also con-
tributes to a broader agenda of incorporating economically motivated informative prior beliefs in
inference and decision making in finance. Studies in the latter vein include Pástor and Stambaugh
(1999, 2000, 2001, 2002ab), Pástor (2000), Baks, Metrick, and Wachter (2001), and Jones and
Shanken (2005). Studies that employ informative priors in the context of return predictability
include Kandel and Stambaugh (1996), Avramov (2002, 2004), Cremers (2002), Avramov and
Wermers (2006), and Wachter and Warusawitharana (2006).

    The paper is organized as follows. Section 2 introduces the predictive system and discusses
its properties. Section 3 explains why uw is likely to be negative and how it affects expected
returns. Section 4 presents our empirical work, in which we estimate the predictive system with a
Bayesian approach. We compare the explanatory powers of the predictive system and predictive
regression and quantify the differences in expected return estimates. We also assess the degree to
which various predictors are correlated with the expected return, decompose the variation in the
expected return into three components, and analyze the variance of the multiperiod returns. Section
5 reviews our conclusions. Many technical aspects of our analysis are presented in the Appendix.

                                                  4
2.         Predictive System
In the predictive regression approach, the expected return is modeled as a linear combination of the
predictors in x t . This modeling assumption is unlikely to be exact, in that no linear combination of
the predictors is likely to capture perfectly the true unobserved expected return,  t . We relax this
assumption and develop an alternative predictive framework, which we call a predictive system.
The predictive regression is a special case of the predictive system, as we show in Section 2.3.

         We define the predictive system in its most general form as a vector autoregression for . t ; x t /,
with an arbitrary number of lags. We do not analyze this general form here. Given our objective
to provide an initial exploration of predictive systems, simplicity is a virtue. We examine a simple
version of the predictive system, in which  t and x t follow AR(1) processes:

                                          r tC1 D              t C u tC1                                          (3)
                                          x tC1 D  C Ax t C v tC1                                                 (4)
                                          tC1 D ˛ C ˇ t C w tC1 :                                                (5)

The residuals in the system are assumed to be distributed identically and independently across t as
                         2     3      02 3 2 2                         31
                           ut              0         u uv uw
                         4 v t 5  N @4 0 5 ; 4 vu ˙vv vw 5A :                                (6)
                                                                   2
                           wt              0         wu wv w

We assume throughout that 0 < ˇ < 1 and that the eigenvalues of A lie inside the unit circle.

         The predictive system is a version of a state space model.2 Equation (3) defines the unobserved
conditional expected return  t . Equation (4) is a standard assumption in the predictability liter-
ature. A special case of the predictive system arises when there are no predictors, in which case
equation (4) is absent and the data include only returns. Equation (5) postulates a simple persistent
process for  t . This reduced-form model could be consistent with a variety of economic models,
rational or behavioral, in which the expected return varies over time in a persistent fashion.


2.1. Conditional Expected Return

The value of  t is unobservable, but the predictive system implies a value for E. t jD t / D
E.r tC1 jD t /, where D t denotes the history of returns and predictors observed through time t . Using
     2
    Harvey (1989) provides a textbook treatment of state-space models, including a brief discussion of the case with
non-zero correlations among all of the model’s disturbances, which is the case here. In the Appendix, we provide an
independent treatment specific to the system in (3) through (6). Studies that analyze return predictability using state
space models include Conrad and Kaul (1988), Lamoureux and Zhou (1996), Johannes, Polson, and Stroud (2002),
Ang and Piazzesi (2003), Brandt and Kang (2004), Dangl and Halling (2006), Duffee (2006), and Rytchkov (2007).


                                                          5
the Kalman filter, we find that this conditional expected return can be written as the unconditional
expected return plus linear combinations of past return forecast errors and innovations in the pre-
dictors. Specifically, if we define the forecast error for the return in each period t as

                                              t D rt      E.r t jD t   1 /;                                         (7)

then the expected return conditional on the history of returns and predictors is given by
                                                            1
                                                            X                                  
                               E.r tC1 jD t / D E.r/ C            s  t   s   C s0 v t   s       ;                 (8)
                                                            sD0

where the unconditional mean return E.r/ D ˛=.1                   ˇ/ and, in steady state,

                                                    s D mˇ s                                                        (9)
                                                    s D nˇ s ;                                                    (10)

where m and n are functions of the parameters in equations (3) through (6).3 The conditional
expected return thus depends on the full history of returns and predictor realizations. We analyze
this dependence in more detail in Section 3.2., where we plot s as a function of s and uw .

    Since the forecast errors  t in equation (8) are defined relative to conditional expectations that
are updated through time based on the available return histories, part of the effects of past return
realizations are impounded in those earlier conditional expectations. To isolate the full effect of
each past period’s total return, we can subtract the unconditional mean from each return, defining
U
 t D rt      E.r/, and then rewrite the conditional expected return in equation (8) as
                                                            1
                                                            X                                  
                                E.r tC1 jD t / D E.r/ C           !s  U      0
                                                                       t s C ıs v t        s       ;               (11)
                                                            sD0

where, again in steady state,

                                               !s D m.ˇ             m/s                                            (12)
                                                ıs D n.ˇ           m/s :                                           (13)

It can be verified that the rate of decay in !s and ıs , ˇ                     m, is nonnegative. This alternative
representation of the conditional expected return will also be useful in Section 3.2.
   3
     In general, m and n are also functions of time, but as the length of the history in D t grows long, they converge
to steady-state values that do not depend on t. That convergence is reached fairly quickly in the settings we consider.
We first present the steady-state expressions, for simplicity, but later employ the finite-sample Kalman filter as well.
The Appendix derives the functions m and n in finite samples as well as in steady state. The Appendix also shows (in
equation A41) that the finite-sample versions of m and n can be interpreted as the slope coefficients from the regression
of  t on r t and x t , respectively, conditional on the sample information at time t 1.


                                                           6
2.2. Temporal Dependence in Returns

Returns in the predictive system exhibit interesting temporal dependence. Given the AR(1) process
for  t in equation (5), we can rewrite  t as an MA(1) process (the Wold representation):
                                                                     1
                                                                     X
                                             t D E.r/ C                   ˇi wt i :                                   (14)
                                                                     iD0

Using equations (3) and (14), the return k periods ahead can be written as
                                             1
                                             X
                     r tCk D E.r/ C                ˇ i w tCk   1 i   C u tCk                                           (15)
                                             iD0
                                                                            k 1
                                                                            X
                                            k 1                k 1
                             D .1       ˇ         /E.r/ C ˇ          t C         ˇk   1 i
                                                                                             w tCi C u tCk :           (16)
                                                                            iD1

This equation implies that the autocovariance of returns is equal to
                                                                   
                               Cov.r t ; r t k / D ˇ k 1 ˇ2 C uw ;                                                  (17)

where 2 D w2 =.1         ˇ 2 / is the unconditional variance of  t . As a result, the serial correlation in
returns can be positive or negative, depending on the parameter values. The positive component of
(17) is due to persistence in  t ; the negative component is due to uw < 0, or mean reversion in
stock returns. The knife-edge case of zero autocorrelation obtains for uw D                            ˇw =.u .1   ˇ 2 //.

       The AR(1) process for  t also implies that returns follow an ARMA(1,1) process,

                                   r tC1 D .1        ˇ/E.r/ C ˇr t C  tC1               t :                        (18)

When we implement the Kalman filter without using any predictor information, so that D t includes
                                                                       P
only the return history, we obtain equation (8) without the last term ( 1    0
                                                                        sD0 s v t s ). That equation
implies a specification of (18) with  t D  t and                  Dˇ      m.


2.3. Predictive Regression

The traditional approach to modeling return predictability, a predictive regression,

                                              r tC1 D a C b 0 x t C e tC1 ;                                            (19)

arises as a special case of the predictive system if the predictors in x t are “perfect” in that  t D
a C b 0 x t . The predictors are perfect if there exists a b such that w t D b 0 v t and A0 b D ˇb.4
   4
       A0 b D ˇb means that ˇ is an eigenvalue of A0 corresponding to the eigenvector b; one example is A D ˇI .


                                                               7
For example, if x t contains one predictor, this predictor is perfect if its innovations are perfectly
correlated with the innovations in  t (i.e., vw D ˙1) and if its autocorrelation is the same as that
of  t (i.e., A D ˇ). In general, though, the predictors in x t are imperfect in that  t ¤ a C b 0 x t .

       When the predictors approach perfection, then m ! 0, n ! b, and equation (8) becomes
                                              1
                                              X
              E.r tC1 jD t / D E.r/ C b 0           As v t   s   D E.r/ C b 0 Œx t     E.x/ D a C b 0 x t ;    (20)
                                              sD0

where E.x/ D .I         A/ 1  is the unconditional mean of x t . That is, when the predictors approach
perfection, the system-based conditional expected return approaches the regression-based condi-
tional mean, a C b 0 x t . When the predictors are imperfect, however, their entire history enters the
conditional expected return, since the weighted sum of their past innovations in equation (8) does
not then reduce to a function of just x t . Moreover, when the predictors are imperfect, the expected
return depends also on the full history of returns in addition to the history of the predictors.

       Predictive systems have interesting implications for predictive regressions. All parameters of
the predictive regression in equation (19) can be computed from the parameters of the predictive
system in equations (3) through (6).5 Most interesting, the residual autocovariance is given by

                    Cov.e t ; e tC1 / D ˇ.2           0
                                                       Vx Vxx1 Vx/ C uw              0
                                                                                       Vx Vxx1 vu
                                       D ˇVar. t jx t / C Cov.u t ; w t             b 0 v t /:                 (21)

If the predictors are perfect, Var. t jx t / D 0 and w t D b 0 v t , so Cov.e t ; e tC1 / is zero. With
imperfect predictors, though, Var. t jx t / > 0, w t ¤ b 0 v t , and Cov.e t ; e tC1 / is generally non-zero.
This observation suggests a simple diagnostic for predictor imperfection: if the residuals from the
predictive regression exhibit non-zero autocorrelation, then the predictors x t are imperfect.

       The serial correlation in the residuals complicates the calculation of standard errors in the pre-
dictive regression approach. Since the first term in equation (21) is nonnegative, Cov.e t ; e tC1 / is
often positive, in which case assuming uncorrelated predictive regression residuals leads to un-
derstated standard errors. Ferson, Sarkissian, and Simin (2003) make a similar point when the
predictor is “spurious,” or uncorrelated with expected return. Their setting is a special case of (3)–
(6) with one predictor and a diagonal covariance matrix in (6).6 In specifying a diagonal covariance
matrix for the disturbances, they assume not only that the predictor is spurious but also that the
innovations in expected return are uncorrelated with unexpected returns (i.e., uw D 0). In this
   5
      For example, the regression slope b can be computed from the system’s parameters as b D Vxx1 Vx , where Vxx
is given in the Appendix in equation (A10), Vx D .IK ˇA/ 1 vw , and IK is a K  K identity matrix.
    6
      The objectives of Ferson et al. differ from ours. For example, they do not use this multiple-equation setting to
estimate expected return or to examine its dependence on lagged returns and predictors.


                                                             8
special case, we see from (21) that Cov.e t ; e tC1 / D ˇ2 . Ferson et al. do not report this expression
but do find, using simulations, that the positive residual serial correlation can substantially affect
inference in predictive regressions. Duffee (2006) also uses simulations to make a related point in
the context of bond predictability.


3.     Correlation between Expected and Unexpected Returns
A key quantity in this paper is uw , the correlation between the unexpected return, u t , and the
innovation in the expected return, w t . Henceforth, we refer to uw simply as the “correlation
between expected and unexpected returns,” a slightly inaccurate but much shorter description.
This correlation is important for return predictability in several ways. First, it determines how
past returns affect the forecasts of future returns, as we show in Section 3.2. Second, economically
motivated prior beliefs about uw play an important role in various inferences about predictability,
as we show in Section 4. The ability to incorporate prior beliefs about uw is a key feature of the
predictive system. We begin this section by discussing some theoretical properties of uw .


3.1. Why uw Is Likely to Be Negative

The basic motivation behind the belief that uw < 0 is that asset prices tend to fall when discount
rates rise. More precisely, uw < 0 means that unanticipated increases in expected returns tend to
be accompanied by unexpected negative returns. This intuition holds perfectly for nominal returns
on Treasury bonds. Since the nominal cash flows of Treasury bonds are fixed, the bond price
variation is driven only by discount rate shocks, and uw D 1. Stock returns, however, are driven
also by cash flow shocks. It is possible, at least in principle, that positive discount rate shocks could
be accompanied by such large positive cash flow shocks that stock prices rise rather than fall as a
result. In this section, we derive conditions under which uw < 0, and argue that these conditions
are likely to be satisfied for the aggregate stock market.

     Following Campbell (1991), the unexpected return can be decomposed approximately as

                                       u tC1 D C;tC1     E;tC1 ;                                   (22)

where C;tC1 represents the unanticipated revisions in expected future cash flows and E;tC1 cap-
tures the revisions in expected future returns. If the expected return follows the process in equation
(2) with 0 < ˇ < 1, then E;tC1 is equal to w tC1 multiplied by a positive constant, so that

                                       uw D .u tC1 ; E;tC1 /:                                     (23)


                                                    9
Since the partial correlation between u tC1 and E;tC1 is minus one (equation (22)), it seems natural
to believe a priori that the simple correlation between u tC1 and E;tC1 , or uw , is negative. Before
seeing any data, it is not obvious why C;tC1 and E;tC1 should be correlated, and a belief that
C;tC1 and E;tC1 are uncorrelated translates into a belief that uw is negative. More precisely, it
follows directly from equations (22) and (23) that uw < 0 if and only if

                                                                   .E;tC1/
                                        .C;tC1 ; E;tC1 / <                 ;                                   (24)
                                                                   .C;tC1 /

where the  ’s denote standard deviations. In order for uw < 0 to be violated, cash flow shocks
would have to be more important than discount rate shocks in explaining the variance of stock
returns, i.e.,  .C;tC1 / >  .E;tC1/, and the correlation between those shocks, .C;tC1 ; E;tC1 /,
would have to be positive and sufficiently high. It seems difficult to argue that one could expect
such a high correlation a priori. In fact, such a high correlation between the shocks to cash flows
and discount rates seems unlikely because it would make stock returns unrealistically smooth. It is
easy to see from equation (22) that a violation of the condition in (24) would require that

  Var.u tC1 / < Var.C;tC1 /        Cov.C;tC1 ; E;tC1 / < Var.C;tC1 /           Var.E;tC1/ < Var.C;tC1 /:
                                                                                                          (25)
That is, for uw < 0 to be violated, stock returns would have to be less volatile than when the
expected return is constant (i.e., when Var.E;tC1/ D 0). In reality, though, stock returns appear
to be more volatile than when the expected return is constant. Shiller (1981) and LeRoy and Porter
(1981) find that stock returns are much more volatile than the present value of future dividends
discounted at constant rates. To explain this “excess volatility puzzle,” discount rates must vary
over time in a way that increases stock volatility. But if uw were positive, discount rate variation
would reduce stock volatility, thereby deepening the puzzle, which seems unappealing a priori.7

    The analysis of uw presented above is somewhat theoretical, but there is also more direct
empirical evidence suggesting that uw is negative. Campbell (1991) uses a vector-autoregressive
approach to decompose unexpected stock market returns into components due to cash flow shocks,
C;tC1 , and discount rate shocks, E;tC1 , as in our equation (22). He considers two subperiods,
1927–1951 and 1952–1988. Since our empirical sample in Section 4. begins in 1952, we can use
Campbell’s results from the 1927–1951 period as one source of prior information about uw . Based
on quarterly data (which we also use in our empirical work), Campbell estimates in his Table 2
    7
      Our argument is not fully precise because Shiller and LeRoy and Porter analyze the unconditional variance of
stock returns, whereas Var.u t C1 / in equation (25) represents the conditional variance. But the difference between the
two variances is relatively small because the variance of  t is generally agreed to be much smaller than the variance
of u t C1 in equation (1). Moreover, we can allow plenty of margin for error on both sides of our argument. On one
side, uw > 0 implies Var.u t C1 / < Var.C;t C1 / with as many as three inequality signs in equation (25). On the other
side, the evidence of Shiller and others that Var.u t C1 / > Var.C;t C1 / seems quite strong (see Shiller’s Figure 1).


                                                          10
that  .E;tC1 / >  .C;tC1 / in 1927–1951, meaning that discount rate news is more important than
cash flow news in explaining the variance of stock market returns. This result makes the condition
(24) hold trivially, since .C;tC1 ; E;tC1 / < 1. Moreover, Campbell obtains negative estimates of
.C;tC1 ; E;tC1 / for the 1927–1951 period, which again makes (24) hold trivially independent of
 .E;tC1/= .C;tC1 /. Campbell’s empirical results from a sample period that predates our sample
therefore provide further support to the prior belief that uw < 0.

   In fact, prior empirical evidence points to large negative estimates of uw . Table 2 of Campbell
(1991) reports estimates of the variance of E;tC1 , 2E , and its covariance with C;tC1 ,  .C ; E /,
both as fractions of u2 , from which the implied estimates of .u tC1 ; E;tC1 / can be computed as
                                    
   .C ; E /=u2 2E =u2 = E =u . Given equation (23), we interpret these values as implied
estimates of uw . For the 1927–1951 period, Campbell’s results imply values of uw ranging from
-0.67 to -0.87 across three different specifications. In 1952–1988, the implied estimates of uw
range from -0.92 to -0.94, and in the full sample, 1927–1988, they range from -0.71 to -0.86.

   Campbell’s evidence on uw is not definitive because it treats VAR-based estimates of expected
returns as the true expected returns. Campbell uses three predictors: the dividend-price ratio (D/P),
the lagged stock return, and the (relative) one-month T-bill rate. He reports that his results are sen-
sitive to the exclusion of D/P, but they are robust as long as D/P is included among the predictors.
Campbell and Ammer (1993) use seven predictors, including D/P, and report estimates in their
Table III that imply even more negative estimates of uw , ranging from -0.93 to -0.95 in postwar
data. Van Binsbergen and Koijen (2007) estimate uw ranging from -0.67 to -0.45, with an av-
erage of -0.60, in postwar data. They do not rely on prespecified predictors but instead use data
on dividend growth and returns in the context of a present-value model. They also find a positive
correlation between shocks to expected return and dividend growth, similar to Menzly, Santos, and
Veronesi (2004), Lettau and Ludvigson (2005), and Kothari, Lewellen, and Warner (2006). Note
that .C;tC1 ; E;tC1 / can be positive without violating the condition in equation (24).

   While we believe that uw < 0 is a sensible prior belief, we entertain three different priors on
uw , including a noninformative prior, in our empirical work in Section 4.


3.2. The Role of uw in Determining Expected Returns

    The correlation between expected and unexpected returns, uw , plays a critical role in de-
termining the conditional expected return. To illustrate this role, we consider a special case of
the predictive system in which there are no predictors. With no predictors, D t includes only
the return history, and the conditional expected return in equation (8) is simply E.r tC1 jD t / D

                                                   11
         P1
E.r/ C     sD0 s  t s ;   a weighted sum of past forecast errors in returns (the Wold representation).
Panel A of Figure 1 plots the values of s in an example with the predictive R-squared (the fraction
of the variance in r tC1 explained by  t ) equal to 0.05, ˇ equal to 0.9, and four different values of
uw ranging from -0.99 to 0. The figure shows that different values of uw produce different values
of m, and hence also different behaviors for s .D mˇ s /.

    The results in Figure 1 can be understood by noting that there are essentially two effects of
the return history on the current expected return. The first might be termed the “level” effect.
Observing recent realized returns that were higher than expected suggests that they were generated
from a distribution with a higher mean. If the expected return is persistent, as it is in this example
with ˇ D 0:9, then that recent history suggests that the current mean is higher as well. So the level
effect positively associates past forecast errors in returns with expected future returns. The second
effect, which might be termed the “change” effect, operates via the correlation between expected
and unexpected returns. In particular, suppose uw is negative, as we suggest is reasonable. Then
observing recent realized returns that were higher than expected suggests that expected returns fell
in those periods. That is, part of the reason that realized returns were higher than expected is that
there were price increases associated with negative shocks to expected future returns and thus to
discount rates applied to expected future cash flows. So the change effect negatively associates
past forecast errors in returns with expected future returns. Overall, the net impact of the return
history on the current return depends on the relative strengths of the level and change effects.

    The level and change effects can be mapped into the return autocovariance in (17). When uw
is sufficiently negative, then ˇ2 < uw , returns are negatively autocorrelated, and the change
effect prevails. Also, m < 0 in that case, so the s ’s in (9) are negative. When ˇ2 >           uw ,
returns are positively autocorrelated, the s ’s are positive, and the level effect prevails.

    When uw D 0, there is no change effect and only the level effect is present. For that case,
the s ’s in Figure 1 start at a positive value for the first lag, about 0.04, and then decay toward
zero. The level and change effects offset each other when uw D             0:47 (this is the knife-edge
case of zero autocorrelation in equation (17)), or when the fraction of the variance in unexpected
                                                 2
returns explained by expected-return shocks, uw   , is about 22%. In that case, the s ’s plot as a
flat line at zero. This result is worth emphasizing: for uw D       0:47, rational investors who know
the unconditional expected return do not update their beliefs about the conditional expected return,
regardless of what realized returns they observe. The change effect dominates when uw D 0:85,
where the s ’s start around -0.04 at the first lag, and it is even stronger when uw D 0:99, where
the s ’s start around -0.08. Clearly, the correlation between expected and unexpected returns is a
critical determinant of the relation between the return history and the current expected return.


                                                     12
   The conditional expected return depends on the true unconditional mean, E.r/, which must be
estimated in practice. A natural estimator is the sample mean. Consider again the no-predictor
case where the summation on the right-hand side of equation (11) is truncated at s D t 1 and
                                             P
E.r/ is replaced by the sample mean, .1=t / tl D1 rl . The estimated conditional expected return
then becomes a weighted average of past returns,
                                                        t 1
                                                        X
                                     E.r tC1 jD t / D         s r t s ;                        (26)
                                                        sD0

where                                                         !
                                                  t
                                         1        X
                                    s D   1              !l C !s ;                             (27)
                                         t
                                                   l D1
    Pt    1
and         D 1. The weights (s ’s) are plotted in Panel B of Figure 1 for t D 208, corresponding
        sD0 s
to the number of quarters used in our empirical analysis. When uw D 0, all past returns enter
positively but recent returns are weighted more heavily. In the uw D         0:47 case, where the
level and change effects exactly offset each other, all of the weights equal 1=t , so the conditional
expected return is then just the historical sample average. For the larger negative uw values, where
the change effect is stronger, the weights switch from negative at more recent lags to positive at
more distant lags (as the weights must sum to one). For example, when changes in expected returns
explain about 72% of the variance in unexpected returns (uw D 0:85), the returns from the most
recent 10 years (40 quarters) contribute negatively to the estimated current expected return, while
the returns from the earlier 42 years contribute positively.

   An additional perspective on the role of uw is provided by the time series of conditional
expected returns plotted in Figure 2. In constructing these series, we maintain the same setting
and same parameter values as in Figure 1. The unconditional mean return E.r/ is set equal to the
sample average for our 208-quarter sample period, and then, starting from the first quarter in the
sample, the conditional mean is updated through time using the finite-sample Kalman filter applied
to the realized returns data. As before, the level and change effects exactly offset each other when
uw D      0:47, so the conditional expected return in that case is simply a flat line at the sample
average for the period. A striking feature of the plot is that the expected return series for uw D 0
is virtually the mirror image of the series for uw D 0:85. Moreover, the differences among the
various series of conditional expected returns are large in economic terms, often several percent
per quarter. As before, we see that uw plays a key role in estimating expected returns.

   Figure 3 compares the R2 ’s from three approaches to predicting r tC1 using some or all of
the information observable at time t , which includes the history of returns and a single predictor
x t . The first approach is the predictive system, which uses all of that available history. The

                                                  13
second is the predictive regression, which uses only the current value x t . The third approach is the
ARMA(1,1) model in equation (18), which uses only past returns. The four panels correspond to
the values f0; 0:3; 0:6; 0:9g for vw , the conditional correlation between  t and the single predictor
x t . In all four panels, ˇ D A D 0:9, and the true predictive R2 (from the regression of r tC1 on  t )
is 0.05. We consider two values of uv , “high” and “low”, which correspond to partial correlations
between u t and v t given w t of uvjw D 0:9 and uvjw D 0:9, respectively.8

         Since all three approaches compared in Figure 3 use only information observable at time t ,
they all produce R2 ’s smaller than 0.05. The R2 from the predictive regression rises from 0 to 0.04
as vw rises from 0 to 0.9 across the four panels. This increase is intuitive: as  t and x t become
more highly correlated, the predictive regression becomes more useful in predicting returns. The
predictive regression R2 is invariant to uw . In contrast, the R2 from the ARMA(1,1) model,
which summarizes the usefulness of past returns in predicting future returns, is heavily influenced
by uw . When uw D 0:47, this R2 is zero: past returns contain no information about future
returns because the level and change effects cancel out. For uw ¤ 0:47, stock returns are serially
correlated and the ARMA(1,1) R2 is positive; in fact, it can be higher than the predictive regression
R2 . For example, when vw D 0:3 and uw … . 0:74; 0:13/, past returns are more useful than
x t in predicting r tC1 . The highest R2 ’s are invariably achieved by the predictive system, which
uses more information to predict future returns than do the other two approaches.

4.         Empirical Analysis

In this section we use the predictive system to conduct an empirical analysis of return predictability.
We first present evidence from predictive regressions, for benchmark purposes. Then we discuss
identification issues and use the system to estimate expected returns via maximum likelihood.
Finally, we turn to the main analysis, which takes a Bayesian approach.


4.1. Evidence from Predictive Regressions

We begin by estimating predictive regressions on quarterly data in 1952–2003 for three predictors.
The first predictor is the market-wide dividend yield, which is equal to total dividends paid over
the previous 12 months divided by the current total market capitalization. We compute the div-
idend yield from the with-dividend and without-dividend monthly returns on the value-weighted
     8
     We specify the partial correlation uvjw instead of the simple correlation uv because our control over uv is
limited. The permissible range of values for uv depends on vw and uw (see equation (32)) and we vary both vw
and uw . By choosing uvjw of 0.9 and -0.9, we are specifying uv close to the boundaries of its permissible range.



                                                        14
portfolio of all NYSE, Amex, and Nasdaq stocks, which we obtain from the Center for Research
in Security Prices (CRSP) at the University of Chicago. The second predictor is CAY from Lettau
and Ludvigson (2001), whose updated quarterly data we obtain from Martin Lettau’s website. The
third predictor is the “bond yield,” which we define as minus the yield on the 30-year Treasury
bond in excess of its most recent 12-month moving average. The bond yield data are from the
Fixed Term Indices in the CRSP Monthly Treasury file. The three predictors are used to predict
quarterly returns on the value-weighted portfolio of all NYSE, Amex, and Nasdaq stocks in excess
of the quarterly return on a one-month T-bill, which is also obtained from CRSP.

       Whereas the first two predictors have been used extensively, the third predictor appears to
be new. It seems plausible for the long-term T-bond yield to be related to future stock returns
since expected returns on stocks and T-bonds may comove due to discount-rate-related factors.
Subtracting the 12-month average yield is an adjustment that is commonly applied to the short-
term risk-free rate (e.g., Campbell, 1991, and Campbell and Ammer, 1993).

                                                                                                   O the
       Table I reports, for various predictive regressions, the estimated slope coefficient vector b,
R2 , and the estimated correlation between unexpected returns and the innovations in expected
returns. This correlation, which represents the regression-based counterpart of uw , is computed
as Corr.e t ; b 0 v t /, following equations (4) and (19). Table I also reports the OLS t -statistics and the
bootstrapped p-values associated with these t -statistics as well as with the R2 .9 Panel A reports
the full-sample results covering 1952 Q1 – 2003 Q4. Panels B and C report sub-sample results.10

   The results suggest that all three predictors have some forecasting ability. The dividend yield
produces the weakest evidence (highest p-values, lowest R2 s) in all three sample periods. When
included as the single predictor, the dividend yield is marginally significant in the full sample (p-
value of 5:7%). It is significant in the first subperiod (p D 1:4%) but not in the second subperiod
(p D 40:9%). The significance of the dividend yield weakens further when the other two predictors
are included in the predictive regression.

       In contrast, both the bond yield and CAY are highly significant predictors. When used alone,
   9
       In the bootstrap, we repeat the following procedure 20,000 times: (i) Resample T pairs of .vO t ; eO t /, with replace-
ment, from the set of OLS residuals from regressions (4) and (19); (ii) Build up the time series of x t , starting from the
unconditional mean and iterating forward on equation (4), using the OLS estimates .O ; A/            O and the resampled values
of vO t ; (iii) Construct the time series of returns, r t , by adding the resampled values of eO t to the sample mean (i.e., under
the null that returns are not predictable); (iv) Use the resulting series of x t and r t to estimate regressions (4) and (19)
by OLS. The bootstrapped p-value associated with the reported t-statistic (or R2 ) is the relative frequency with which
the reported quantity is smaller than its 20,000 counterparts bootstrapped under the null of no predictability.
   10
       Since we use the T-bond and T-bill yields in our analysis, we begin our sample in 1952, after the 1951 Treasury-
Fed accord that made possible the independent conduct of monetary policy. Campbell and Ammer (1993), Campbell
and Yogo (2006), and others also begin their samples in 1952 for this reason.



                                                               15
both predictors exhibit p-values of 0.1% or less in the full sample, and they are also significant in
both subperiods. If judged by the p-values, CAY is the stronger predictor in the first subperiod but
the bond yield is stronger in the second subperiod. When all three predictors are used together,
both CAY and the bond yield are highly and about equally significant in the full sample.

    In addition to the p-values and R2 s, it is also informative to examine the correlations between
the expected and unexpected returns, Corr.b 0 v t ; e t /, shown in the fourth column of Table I. When
the single predictor is either the dividend yield or CAY, these correlations are negative and highly
significant: -91.9% for the dividend yield and -53.6% for CAY in the full sample. These negative
correlations are not surprising since both predictors are negatively related to stock prices, by con-
struction. For the bond yield, however, this correlation is positive and highly significant in all three
sample periods, ranging from 21.7% to 25.1%. This positive correlation makes it unlikely that the
bond yield is perfectly correlated with the true conditional expected return.

   The correlation between expected and unexpected returns is a useful diagnostic that should be
considered when examining the output of a predictive regression. Since this correlation is likely
to be negative, predictive models in which this correlation is positive seem less plausible.11 The
model in which the bond yield is the single predictor is a good example. Based on the predictive-
regression p-value, the bond yield would appear to be a highly successful predictor whose fore-
casting ability is better than that of the dividend yield and comparable to that of CAY. However,
the bond yield produces expected return estimates whose innovations are positively correlated with
unexpected returns, suggesting that this predictor is imperfect. We suspect that the same statement
can be made about many macroeconomic variables that the literature has related to expected re-
turns. In the rest of the paper, we develop a predictive framework that allows us to incorporate the
prior belief that the correlation between expected and unexpected returns is negative.


4.2. Identification and Maximum Likelihood Estimation

In the absence of any priors or parameter restrictions, not all of the parameters in equations (3)
through (6) are identified. We can nevertheless obtain estimates of conditional expected returns
using equation (4) and the recursive representation for returns,

                          r tC1 D .1      ˇ/E.r/ C ˇr t C n0 v t        .ˇ     m/ t C  tC1 ;                      (28)
  11
     Strictly speaking, the arguments based on equations (22) and (24) apply when r t C1 denotes the total stock return,
but they should hold to a close approximation also when r t C1 denotes the excess stock return, as used here. For excess
returns, Campbell (1991) shows that equation (22) has an additional term representing news about future interest rates,
and he estimates the variance of that term to be an order of magnitude smaller than the variances of C;t C1 and E;t C1 .



                                                           16
which follows directly from the steady-state representation of the conditional expected return in
(8). The parameters in (4) and (28) are identified and can be estimated using maximum likelihood,
by representing those two equations as a state-space system and applying standard methodology
(e.g., Hamilton, 1994, section 13.4).12 The parameters in those equations, along with the covari-
ance matrix of Œ v 0t , identify the parameters appearing in equations (3) through (5) but not all of
the parameters in the covariance matrix in (6). Only ˙vv is identified just by the data. Identifying
the remaining elements of ˙ requires additional information about at least one of them.13

    Figure 4 plots the time series of expected returns obtained via maximum likelihood estimation
as well as the expected-return estimates obtained from OLS estimation of the predictive regression.
Panels A and B display results with a single predictor, either the dividend yield or CAY. In Panel
C, those variables are combined with the bond-yield variable in the three-predictor case. First,
observe that the fluctuation of the expected return estimates seems too large to be plausible. In
Panel B, for example, expected returns range from -5% to 8% per quarter, and the range is even
wider in Panel C. Later on, we obtain smoother time series of  t by specifying informative prior
beliefs. Second, observe that although the series of estimated expected returns exhibit marked
differences across the three sets of predictors, the differences between the predictive-regression
estimates and the predictive-system estimates for a given set of predictors are much smaller.14

    As the length of the sample grows, posterior beliefs about the parameters in (28) and thus (8)
will converge to values that do not depend on prior beliefs about the parameters in the predictive
system (as long as those priors do not strictly preclude such values). Therefore, after observing a
sufficiently long sample, prior beliefs about uw , for example, will not impact forecasts of future
returns. (Our actual sample is evidently not long in that sense, as prior beliefs about uw exert a
substantial effect on estimates of expected returns.) On the other hand, given the lack of full iden-
tification of ˙ , prior beliefs about uw will matter even in large samples when making inferences
about the correlation between the predictors and the true unobservable expected return  t .


4.3. Bayesian Approach

We develop a Bayesian approach for estimating the predictive system. This approach has sev-
eral advantages over frequentist alternatives such as the maximum likelihood approach. First, the
  12
      The likelihood function is detailed in the Appendix.
  13
      Rytchkov (2007) discusses identification issues in a similar setting.
   14
      We also estimate expected returns from the predictive system under diffuse priors (the discussion of prior beliefs
follows later in the text). We find that the resulting estimates (not plotted here) behave similarly to both the OLS
estimates from the predictive regression and the maximum likelihood estimates from the predictive system.



                                                          17
Bayesian approach allows us to specify economically motivated prior distributions for the param-
eters of interest. Second, it produces posterior distributions that deliver finite-sample inferences
about relatively complicated functions of the underlying parameters, such as the correlations be-
tween  t and x t and the R2 s from the regression of r tC1 on  t . Finally, it incorporates parameter
uncertainty as well as uncertainty about the path of the unobservable expected return  t .

    We obtain posterior distributions using Gibbs sampling, a Markov Chain Monte Carlo (MCMC)
technique (e.g., Casella and George, 1992). In each step of the MCMC chain, we first draw the pa-
rameters .; A; ˛; ˇ; ˙ / conditional on the current draw of f t g, and then we use the forward filter-
ing, backward sampling algorithm developed by Carter and Kohn (1994) and Frühwirth-Schnatter
(1994) to draw the time series of f t g conditional on the current draw of .; A; ˛; ˇ; ˙ /.

   We impose informative prior distributions on three quantities: the correlation uw between ex-
pected and unexpected returns, the persistence ˇ of the true expected return  t , and the predictive
R2 from the regression of r tC1 on  t . These prior distributions are plotted in Figure 5.

   The key prior distribution is the one on uw . We consider three priors on uw , all of which are
plotted in Panel A of Figure 5. The “noninformative” prior is flat on most of the . 1; 1/ range,
with prior mass tailing off near ˙1 to avoid potential singularity problems. The “less informative”
prior imposes uw < 0 in that 99.9% of the prior mass of uw is below zero. As shown in Panel
                                                            2
B, this prior implies a relatively noninformative prior on uw , with most prior mass between 0
and 0.8. Finally, the “more informative” prior on uw is specified such that the implied prior on
  2                                                                          2
uw  has 99.9% of its mass above 0.5, with a mean of about 0.77. Since uw       is the R2 from the
regression of unexpected returns on shocks to expected returns, it represents the fraction of market
variance that is due to news about discount rates. Therefore, the more informative prior reflects the
belief that at least half of the variance of market returns is due to discount rate news. This belief is
motivated by empirical evidence. For example, the evidence of Campbell (1991) implies estimates
    2
of uw ranging from 0.50 to 0.74 across three different specifications in his full sample period
1927–1988. Campbell’s evidence from 1927–1951, a sample period that predates ours, implies
              2
estimates of uw ranging from 0.44 to 0.76 (uw ranging from -0.67 to -0.87). The estimates of
 2
uw implied by the postwar evidence are even larger; they range from 0.84 to 0.88 in 1952–1988,
and the estimates of Campbell and Ammer (1993) in their Table III range from 0.86 to 0.91. All of
these estimates are in line with the more informative prior.

   Putting a prior on uw presents a technical challenge. We do not impose the standard inverted
Wishart prior on the covariance matrix ˙ because such a prior would be informative about all
elements of ˙ , not only about uw , and we do not wish to be informative about the elements that
involve v t . Instead, we build on Stambaugh (1997) and form the prior on ˙ as the posterior from

                                                  18
a hypothetical sample that contains more information about u t and w t than about v t . In addition,
we develop a hyperparameter approach that allows us to change the prior on uw without changing
the priors on any other parameters. The details are in the Appendix.

   In addition to putting a prior on uw , we also impose a prior belief that the conditional expected
return  t is stable and persistent. To capture the belief that  t is stable, we impose a prior that the
predictive R2 from the regression of r tC1 on  t is not very large, which is equivalent to the belief
that the total variance of  t is not very large. The prior on the R2 , which is plotted in Panel C of
Figure 5, has a mode close to 1%, most of its mass is below 5%, and there is very little prior mass
above 10%. To capture the belief that  t is persistent, we impose a prior that ˇ, the slope of the
AR(1) process for  t , is smaller than one but not by much.15 The prior on ˇ, which is plotted in
Panel D of Figure 5, has most of its mass above 0.7 and there is virtually no prior mass below 0.5.
We do not impose a prior belief that  t > 0. Although such a belief is reasonable under a fully
rational view, we do not wish to preclude the possibility that some of the variation in  t is driven
by investor sentiment. The prior distributions on all other parameters (; A; ˛, and most elements
of ˙ ) are noninformative. Separately, we also consider a “diffuse” prior, which is completely
noninformative about all model parameters, including uw , ˇ, and R2 .


4.4. Predictive System vs. Predictive Regression

In contrast to a predictive regression, the predictive system allows us to conduct finite-sample
inferences that explicitly incorporate predictor imperfection. The predictive system also produces
more precise inferences about expected returns. To demonstrate this, we compare the explanatory
powers of the system and the regression for a broad range of parameter values. Specifically, we
compare the R2 in the regression of r tC1 on x t for the predictive regression with the R2 in the
regression of r tC1 on E.r tC1 jD t /  E. t jD t / for the predictive system. The ratio of these R2
values when r tC1 is the dependent variable is the same as when  t is the dependent variable,

                                    R2 .r tC1 on x t /         R2 . t on x t /
                                                           D 2                      ;                                (29)
                                R2 .r tC1 on E. t jD t //  R . t on E. t jD t //

since each of the R2 values in the latter ratio is equal to its corresponding value in the first ratio
multiplied by Var.r tC1 /=Var. t /. The parameters in equations (3) through (6) can be used to
obtain the covariance matrix of  t and x t and thereby the R2 in the regression of  t on x t ,
                                                             VarŒE. t jx t /
                                             R2 .reg/ D                        :                                     (30)
                                                               Var. t /
  15
       Ferson, Sarkissian, and Simin (2003, footnote 2) discuss several reasons to believe expected return is persistent.


                                                            19
As shown in the Appendix, we can solve analytically for the steady-state value of Var. t jD t /,
which allows us to compute the R2 in the regression of  t on E. t jD t / as

                                       VarŒE. t jD t /      Var. t jD t /
                          R2 .sys/ D                     D1                  :                  (31)
                                          Var. t /            Var. t /

The ratio in equation (29) is computed as R2 .reg/=R2 .sys/. Note that this R2 ratio cannot exceed
1 because x t 2 D t . In other words, the estimates of  t from the predictive system are at least
as precise as the estimates from the predictive regression, simply because the system uses more
information. The smaller the R2 ratio, the larger the advantage of using the predictive system.

    We use the R2 ratios to quantify the explanatory advantage of the predictive system, using
the same sample as in Section 4.1. Panel A of Table II shows the posterior means and standard
deviations of the R2 ratios for four different priors and four different sets of predictors. First,
observe that the posterior means of the R2 ratios are all comfortably lower than one, ranging from
0.08 to 0.86 across the 16 cases, and from 0.46 to 0.70 when all three predictors are used jointly.
Second, the R2 ratios are sensitive to the prior on uw . For example, with the bond yield as the
single predictor, the R2 ratio is estimated to be 0.73 under the diffuse prior. When we impose the
prior belief that uw is negative, the R2 ratio declines to 0.34 under the less informative prior and
then further to 0.08 under the more informative prior. In other words, under the prior that more
than half of the market variance is due to discount rate news, the expected return estimates from
the predictive system are about 12.5 times more precise than those from the predictive regression.
For the dividend yield, we observe the opposite pattern—the R2 ratio increases from 0.28 to 0.59
to 0.81 for the same priors. The opposite patterns result from the opposite effects that the prior on
uw has on the adequacy of x t as a predictor in the two cases, as we will see later.

    Panel B of Table II shows the posterior means and standard deviations of one minus the ratio
of the mean squared errors from the predictive system and the predictive regression. The mean
                                      ˚
squared error is defined as M SE D E .r tC1 f t /2 , where f t is a return forecast. All posterior
means in Panel B are positive, ranging from 0.01 to 0.17, confirming that the predictive system
forecasts returns more precisely than the predictive regression does.

   Another way of comparing the predictive system with the predictive regression is to compare
their estimates of the slope coefficient b from the predictive regression. Figure 6 plots the pos-
terior distributions of b computed under three scenarios. The dashed line is the posterior of b
computed from the predictive regression under no prior information. This posterior has a Student t
distribution whose mean is equal to the maximum likelihood estimate (MLE) of b (Zellner, 1971,
pp. 65–67). The dashed line thus represents “conventional inference” on predictability. The other


                                                 20
two lines in Figure 6 plot the implied posteriors of b computed from the predictive system.16 The
dotted line corresponds to the prior that is noninformative about uw but informative about ˇ and
R2 . In all three panels of Figure 6, the dotted line is substantially different from the dashed line,
which means that imposing the prior that  t is stable and persistent significantly affects the in-
ference about predictability. In addition, the dotted line is shifted toward zero compared to the
dashed line, which means that the prior belief that  t is stable and persistent weakens the evidence
of predictability. Finally, the solid line corresponds to the prior that is informative not only about
ˇ and R2 but also about uw . The prior on uw clearly affects the inference about predictability.
Consider Panel A, in which the single predictor is the bond yield. Whereas the traditional inference
(dashed line) would conclude with almost 100% certainty that the bond yield is a useful predictor
(b > 0), the system-based inference with the more informative prior on uw (solid line) concludes
no such thing because almost half of the posterior mass of b is below zero. This prior also slightly
weakens the predictive power of CAY but it strengthens the predictive power of the dividend yield.


4.5. How Imperfect Are Predictors?

The predictive system also allows us to learn about the correlation between the expected return
 t and the predictors. Since  t is not observed, the manner in which one learns about such
correlations merits some discussion. Consider, for simplicity, the case of a single predictor x t
whose autocorrelation A is equal to ˇ. The unconditional correlation between the expected return
and the predictor, x , is then equal to vw , the conditional correlation.17 By virtue of the fact that
the correlation matrix for .u t v t w t / must be non-negative definite, it is readily verified that
                                                  2                      2          2
                      vw D uv uw C  ; where   .1                uv /.1    uw /:                   (32)

In other words, even though correlations are not transitive (two correlations don’t imply the third),
they become nearly transitive when at least one of them approaches ˙1.

    We specify noninformative priors for vw and uv . Doing otherwise would most likely involve
priors about each predictor’s usefulness—directly through vw but indirectly through uv as well,
given informative priors about uw . While such an approach could be reasonable, especially in a
forecasting setting, we wish to illustrate here how our framework can deliver inferences about each
  16
     Although b does not appear explicitly in the predictive system, its value can be computed from the system’s
parameters (see footnote 5), so its posterior draws can be constructed from the draws of the system’s parameters.
  17
     More generally,
                                                                         1
                                                        .1 ˇ 2 /.1 A2 / 2
                                          x D vw                          ;
                                                            .1 ˇA/2
         2     2
so that x  vw .


                                                       21
predictor’s usefulness without making prior judgments about that property. Moreover, we prefer
not to add such complexity to this initial exploration of predictive systems. The data are quite
informative about uv anyway, in that with only modest predictability in returns, the value of uv is
close to that of r v , which can be estimated from the series of r t and x t . (When the predictive R2
is low, r2u D .1 R2/ is close to one, and equation (32) then implies that uv is well approximated
by r v .) Information about uw enters largely through the prior. When the prior is concentrated
on large negative values, then the likely values of  in (32) are small, so the prior information
about uw and the sample information about uv get combined to provide information about vw .
Alternatively, if the data indicate that uv is close to ˙1 (e.g., in Table I, uv                       0:9 when the
predictor is the dividend yield), then again  is likely to be small, so that uv and uw are again
jointly informative about vw .

    Figures 7 and 8 analyze the degree to which two of our predictors can capture the unobservable
true expected return  t . We report results for two predictive systems, in which the predictors are
the dividend yield alone (Figure 7) and the bond yield alone (Figure 8). Panel A of each figure
plots the posterior distribution of the R2 from the regression of  t on x t . This R2 is assumed to
be one in a predictive regression, but its posterior in the predictive system has very little mass at
values close to one. In both figures, the R2 s larger than 0.8 receive very little posterior probability
and the values larger than 0.9 are deemed almost impossible, regardless of the prior. This evidence
suggests that neither of the two predictors is likely to be perfectly correlated with  t .18

    The R2 depends on the prior for uw in an interesting way. In Panel A of Figure 7, becoming
increasingly informative about uw shifts the posterior of the R2 to the right, with the mode shifting
from about 0.3 under the noninformative prior to about 0.6 under the more informative prior. This
makes sense: since the dividend yield exhibits a highly negative contemporaneous correlation with
stock returns, imposing a prior that  t also possesses such negative correlation makes the dividend
yield more closely related to  t . Exactly the opposite happens in Panel A of Figure 8, where
becoming increasingly informative about uw shifts the posterior of the R2 to the left so that its
mode is close to zero under the more informative prior. This makes sense as well because the bond
yield is positively correlated with stock returns (Table I).

    Panel B of both figures plots the posterior of the predictive R2 from the regression of r tC1 on
 t . Putting a more informative prior on uw increases the R2 in both figures, but these effects are
relatively small. Since we put a fairly informative prior on the predictive R2 (see Panel C of Figure
   18
      Note that even if x t were perfectly correlated with  t in population, the posterior of their correlation would have
nontrivial mass below one in any finite sample. Since we always observe finite samples, we always perceive imperfect
correlation between x t and  t . Also note that in the NBER version of this article, we report results analogous to those
in Figures 8 and 9 for a predictive system that uses three predictors: the dividend yield, bond yield, and CAY.


                                                           22
5), the posterior is not dramatically different from the prior in either figure.

   Panels C and D of both figures plot the posteriors of the correlations between each predictor
and  t , both conditional (vw ) and unconditional (x). These correlations are all well below 1
and they are quite sensitive to the prior on uw . As we become increasingly informative about
uw , we perceive the dividend yield to be more highly correlated with  t but the bond yield to be
less highly correlated with  t . The effect for the bond yield is dramatic, judging by the posterior
modes in Panel C of Figure 8. Under the noninformative prior, the bond yield has 90% conditional
correlation with  t , but under the more informative prior, this correlation drops to 20%.

   Overall, Figures 8 and 9 show that our predictors are imperfectly correlated with  t and that
the inference about this correlation is substantially affected by the prior beliefs about uw . Prior
beliefs informed by economic principles strengthen the predictive appeal of the dividend yield but
they weaken the predictive appeal of the bond yield.


4.6. Estimates of Expected Return

Figure 9 plots the time series of expected returns estimated by three different approaches. The
dashed line plots the fitted values from the predictive regression. These traditional expected return
estimates seem too volatile to be plausible, as we also observed in Figure 4. For example, in Panel
C, which includes all three predictors, expected returns range from -6% to 9% per quarter. Not
surprisingly, imposing the prior that  t is stable and persistent (dotted line) produces smoother
expected return estimates. Adding the more informative prior on uw (solid line) further smoothes
the expected return estimates: in Panel C, they range from -1.5% to 3.5% per quarter. The infor-
mative priors have substantial effects on expected returns not only in Panel C but also in Panel B in
which CAY is the single predictor: while the regression-fitted values range from -5.5% to 7.5% per
quarter, the solid line ranges from -1.5% to 2.5%. Only in Panel A, in which the dividend yield is
the single predictor, the effect of the prior is relatively mild. The reason is that the regression-fitted
values in Panel A are already fairly smooth and negatively correlated with stock returns.

   While eyeballing the expected return estimates seems informative, we also compute measures
summarizing their differences. Table III compares five different series of expected return estimates.
The first is the series of fitted values from the predictive regression, and the others are produced
by four different approaches to estimating the predictive system. One of the latter approaches
estimates the predictive system by MLE, while the other three impose the prior that  t is stable and
persistent but differ in their prior on uw . We compare the five series of expected return estimates
in three different ways: pairwise correlations, mean absolute differences, and average utility losses.

                                                   23
The utility losses are computed for a mean-variance investor allocating between the market and the
T-bill who knows the variance of market returns but must estimate the market’s expected return.
The investor’s risk aversion, 2.54, is such that the optimal portfolio is fully invested in the market,
on average. We compute the investor’s certainty equivalent loss resulting from holding a portfolio
that is optimal under a different approach for estimating expected returns. For example, the 0.11%
per quarter average utility loss in the first row of Panel A is suffered by an investor who wants to
estimate expected return in the predictive system by MLE but is forced to use the fitted values from
the predictive regression. Finally, the three panels consider three different sets of predictors: the
dividend yield, CAY, and the two predictors combined with the bond yield.

    Panel A of Table III shows that when the dividend yield is the single predictor, the expected
return estimates are fairly similar across the five estimation approaches, confirming the evidence
from Panel A of Figure 9. No average utility loss exceeds 0.20% per quarter, no mean absolute
difference is larger than 0.65% per quarter, and all correlations exceed 81%. We also observe
that imposing informative priors makes the system-based estimates closer to the regression-based
estimates. For example, the utility losses fall monotonically from 0.11% to 0.03% as move from
column two to column five in the first row of Panel A.

  The differences across the five approaches are substantially larger in Panel B where we use
CAY to predict returns. For example, compare the system-based estimates obtained by MLE versus
the more informative prior. The mean absolute difference in expected returns is 1.65% per quarter
and the average certainty equivalent loss from using one estimate in place of the other is 1.38% per
quarter. Both quantities are highly economically significant. In Panel C, where we use all three
predictors, the differences across the five approaches are also large and similar in magnitude.

   In all three panels, the smallest differences are obtained for the noninformative versus the less
informative prior on uw . No average utility loss exceeds 0.06% per quarter, no mean absolute
difference is larger than 0.37% per quarter, and all correlations exceed 95.4%. However, moving
from the less informative to the more informative prior on uw can produce sizeable differences in
expected returns. For example, the mean absolute difference in Panel C is 1.46% per quarter and
the average utility loss is 0.84% per quarter.

   To sum up, when we use the dividend yield as the single predictor, the system-based expected
return estimates are close to the regression-based estimates. In all other cases, the system and the
regression generate substantially different expected returns, and the system-based estimates are
significantly affected by the prior on uw .




                                                  24
4.7. Variance Decomposition of Expected Return

In the predictive regression approach, expected return  t is modeled as an exact linear function
of the predictors in x t . In a predictive system, however, the data provide additional information
about  t because the lagged values of unexpected returns and predictor innovations also enter the
expected return estimates (see Section 3.2.). In this section, we decompose the variance of  t to
assess the relative importance of the various sources of information in a predictive system.

   We can rewrite the AR(1) process for x t as an MA(1) process, as we did for  t in equation
                 P1
(14): x t D Ex C iD0 Ai v t i . Then we project w t linearly on u t and v t :
                                2          1        
                                  u uv           ut
             w t D Œwu wv                             C t D u u t C v vt C t :       (33)
                                 vu ˙vv           vt
Substituting for w t from equation (33) into equation (14), we obtain
                                             1
                                             X                        1
                                                                      X                            1
                                                                                                   X
                                                                                       
  t D .E.r/       v E.x//C     v xt C   u         ˇi ut   iC     v         ˇ i IK   Ai v t   iC         ˇ i  t i ; (34)
                                             iD0                      iD0                          iD0

where K is the number of predictors and IK is a K  K identity matrix. Equation (34) shows
how the lagged values of unexpected returns u t            i   and predictor innovations v t       i   affect  t in the
presence of the current predictor values in x t . Based on this equation, we can decompose the
variance of  t into the components due to x t , fus gst , and fvs gst . See the Appendix for details.

    Table IV reports the posterior means and standard deviations of the R2 s from the regressions
of  t on x t (column 1),  t on x t and fus gst (column 2), and  t on x t and fus ; vs gst (column 3).
We consider four sets of predictors x t : the dividend yield, bond yield, CAY, and the combination
of all three predictors. For each set of predictors, we estimate the predictive system under three
different priors. All three priors assume that  t is stable and persistent but they differ in their
degree of informativeness about uw .

    First, note that x t never accounts for more than 63% of the variance of  t and that it can
account for as little as 3% of this variance. In contrast, x t combined with fus ; vs gst can account
for as much as 95% of the variance of  t , and those components account for more than 80% of the
variance in 10 of the 12 cases in Table IV. The most striking effect obtains for the bond yield, for
which adding fus ; vs gst to x t increases the R2 from 0.03 to 0.95. It seems clear that a predictive
regression, which uses only x t to predict returns, does not use the data as effectively as a predictive
system, which also uses fus ; vs gst in addition to x t .

     The R2 ’s in Table IV are substantially affected by the prior on uw . For example, consider the
first columns of Panels A and B. Under the noninformative prior on uw , both the dividend yield

                                                       25
and the bond yield explain about a third of the variance of  t . As we become more informative
about uw , this fraction increases from 0.34 to 0.40 to 0.57 for the dividend yield, but it decreases
from 0.33 to 0.24 to 0.03 for the bond yield. These opposite patterns reflect the opposite signs of
the correlations between stock returns and the two predictors, as explained earlier.

   The lagged unexpected returns fus gst contain a significant amount of information about  t
beyond that included in x t . When fus gst is added to x t in estimating  t , the R2 ’s increase
by anywhere between 7% and 83%. For example, under the more informative prior on uw , the
R2 increases from 0.03 to 0.86 for the bond yield, from 0.53 to 0.87 for CAY, and from 0.63 to
0.85 when fus gst is added to all three predictors. The lagged predictor innovations fvs gst also
contain useful information about  t . When fvs gst is added to x t and fus gst , the R2 ’s increase
by between 1% and 41%. The smallest increases, of 1% to 5%, obtain for the dividend yield, while
the largest increases, of 9% to 41%, obtain for all three predictors combined.

    To summarize, the past values of unexpected returns and predictor innovations contain use-
ful incremental information about the current expected return. This information is used by the
predictive system but not by the standard predictive regression.


4.8. Variance of Multiperiod Returns

In this final section, we analyze the effect of prior beliefs about uw on the variance of multiperiod
returns. Let rT;T Ck denote the multiperiod log return in periods T through T C k. Consider the
problem of assessing the variance of rT;T Ck conditional on the sample information DT . Stambaugh
(1999) and Barberis (2000) analyze this “predictive” variance in a VAR setting in which expected
return is given by a predictive regression. We examine this variance in our setting with imperfect
predictors, and find that it is substantially affected by prior beliefs about uw .

   From the definition rT;T Ck D rT C1 C rT C2 C    C rT CK , the conditional moments are

                                    1 ˇk
          E.rT;T Ck jT / D kE.r/ C      .T E.r//                                               (35)
                                     1 ˇ
                                                                        
                              2      w2          1 ˇk 1    21  ˇ 2.k 1/
        Var.rT;T Ck jT / D ku C         k 1 2ˇ         Cˇ
                                  .1 ˇ/2            1 ˇ        1 ˇ2
                                                  
                              2uw          1 ˇk 1
                            C        k 1 ˇ           ;                                           (36)
                              1 ˇ            1 ˇ

where T comprises T and the parameters of the predictive system. The predictive variance is

              Var.rT;T Ck jDT / D EŒVar.rT;T Ck jT /jDT  C VarŒE.rT;T Ck jT /jDT ;           (37)

                                                   26
by variance decomposition. We use repeated draws from the posterior distribution of T to com-
pute the two posterior moments on the right-hand side of equation (37).

     Figure 10 plots the predictive k-period variance on a per-period basis (i.e., divided by k), ob-
tained from the predictive system with the dividend yield as the predictor. The figure shows that the
prior for uw has a large effect on the long-run variance. For example, at the five-year investment
horizon, the variance is 0.0051 under the more informative prior, but it is 0.0068, or 33% larger,
under the noninformative prior. In addition, the effect of uw increases with the investment hori-
zon. At the one-year horizon, the variance under the noninformative prior exceeds its counterpart
under the more informative prior by 9%, but this difference grows to 33% at the five-year horizon.

    The prior for uw affects the long-run variance by affecting the perceived mean reversion in
stock returns. When uw is sufficiently negative, high (low) realized returns tend to mean-revert
because they tend to be accompanied by decreases (increases) in expected future returns. The effect
of mean reversion is stronger when uw takes larger negative values. As a result, the long-horizon
variance in Figure 10 is at its lowest under the more informative prior for uw .

     Figure 10 also shows that the predictive variance is a U-shaped function of the investment
horizon. This pattern is an outcome of several forces, including mean reversion in stock returns,
uncertainty about future values of  t , and parameter uncertainty. Analyzing these forces in detail
is beyond the scope of this study, but we are presently exploring them in separate work.


5.     Conclusions

Predictive systems allow predictors to be imperfectly correlated with the conditional expected re-
turn. When predictors are imperfect, expected returns conditional on available data depend not only
on the most recent values of those predictors but also on lagged returns and lags of the predictors.
Recent returns receive negative weights when a significant portion of the variance in unexpected
returns is due to changes in expected returns. The lags of returns and predictors often account for
a large fraction of the variation in estimates of conditional expected returns.

     Predictive systems also allow one to incorporate a prior belief that expected and unexpected
returns are negatively correlated. We find that such a belief has an important impact on estimates
of expected returns and various inferences about predictability, including a predictor’s correlation
with expected return as well as the variance of returns over longer investment horizons.

     Although our focus is on predictive systems, we also find two implications for predictive re-


                                                 27
gressions. First, we show that if predictors are imperfect, the predictive regression residuals are
autocorrelated. This autocorrelation should be incorporated when computing standard errors in
predictive regressions. In addition, this autocorrelation provides a simple diagnostic for predic-
tor imperfection: non-zero autocorrelation indicates imperfect predictors. Second, we argue that
researchers running predictive regressions should examine the regression-implied correlation be-
tween expected and unexpected returns. Predictive regressions in which this correlation is positive
are unlikely to perfectly capture time variation in expected stock market returns.

   Our initial exploration of predictive systems can be extended in many directions. First, we are
intentionally noninformative about the degree of imperfection in a predictor, but one could instead
incorporate an informative prior belief about a predictor’s correlation with expected return. The
latter approach is likely to be preferable when inference is less the objective than is producing the
best forecast given one’s own prior judgment. Along these lines, one could study the implications
of predictive systems for asset allocation. Second, we assume that the conditional mean return
follows an AR(1) process, but it would also make sense to consider more complicated processes.
For example, if the mean were allowed to have not only a slow-moving persistent component but
also a higher-frequency transient component, the bond yield, which is not very persistent, might be
inferred to be more highly correlated with the conditional mean. Third, we assume that the return
variance is constant, but one could allow it to be time-varying, potentially in a manner correlated
with expected return (e.g., Brandt and Kang, 2004). Fourth, we consider three predictors but it
would also be interesting to examine the degrees of imperfection in various other predictors that
have been proposed in the literature (e.g., Ferson and Harvey (1991), Lamont (1998), Lewellen
(1999), Ang and Bekaert (2007), Santos and Veronesi (2006), etc.). Fifth, we analyze predictabil-
ity in U.S. stock market returns, but it would also be interesting to apply predictive systems to
international markets (e.g., Ferson and Harvey, 1993).

   It could also be useful to expand the predictive system to incorporate cash flow news. We have
argued that the innovation in the expected return should be negatively correlated with the unex-
pected return, but if one could account for the portion of the latter that is correlated with cash flow
news, the remaining portion would be driven entirely by news about expected return. These issues
are beyond the scope of this paper but they merit more attention. See Cochrane (2008), Rytchkov
(2007), and van Binsbergen and Koijen (2007) for recent analyses of the interaction between return
predictability and cash flow predictability. Cash flow forecasts also enter the calculations of the
implied cost of capital, which is used by Pástor, Sinha, and Swaminathan (2008) to proxy for the
conditional expected market return.

   One might ask whether the predictive system produces out-of-sample forecasts with lower


                                                  28
mean squared error (MSE) than a simpler approach such as a predictive regression or the sample
average.19 A Bayesian investor with a quadratic (MSE) loss function would prefer a forecast that
combines his priors and the data to estimate the conditional expected return based on the correct
model. The correct model, when estimated using a finite sample, tends to produce out-of-sample
MSEs higher than those from estimates of simpler models when the true degree of predictability is
sufficiently small, as discussed by Clark and West (2006, 2007) and Hjalmarsson (2006). Thus, a
simple comparison of out-of-sample MSEs would not speak directly to the question of whether the
predictive system is the right model from the investor’s perspective. That question, one of model
selection, is beyond the scope of this study but could be an interesting area for future research.




   19
      Goyal and Welch (2003, 2006), Campbell and Thompson (2008), and Rapach, Strauss and Zhou (2007), among
others, investigate the abilities of predictive regressions and sample averages to forecast stock returns out of sample.


                                                          29
Appendix.
    We begin working with a generalized version of the predictive system with more than one asset,
so that r t , x t , and  t are all vectors. To maintain the usual convention that matrices are denoted
by uppercase letters, we replace ˇ by B, the  ’s by the corresponding ˙ ’s, etc.

    We restate the predictive system from equations (3) through (5) here in the multi-asset case:

                                     r tC1 D      t C u tC1                                         (A1)
                                     x tC1 D  C Ax t C v tC1                                        (A2)
                                      tC1 D ˛ C B t C w tC1 ;                                      (A3)

with the disturbances distributed identically and independently across t as
                       2      3       02 3 2                           31
                           ut               0       ˙uu ˙uv ˙uw
                       4 v t 5  N @4 0 5 ; 4 ˙vu ˙vv ˙vw 5A :                                       (A4)
                          wt                0       ˙wu ˙wv ˙ww

Let D0 denote the null information set, so that the unconditional moments are given as
                    2      3            02        3 2                  31
                        rt                   Er         Vr r Vr x Vr
                    4 x t 5 jD0  N @4 Ex 5 ; 4 Vxr Vxx Vx 5A :                                     (A5)
                        t                   Er         Vr Vx V

Let z t denote the vector of the observed data at time t , z t D .r t0 x 0t /0 . Denote the data we observe
through time t as D t D .z1 ; : : : ; z t /, and note that our complete data consist of DT . Also define
                                                                                 
                              Er                      Vr r Vr x                  Vr
                    Ez D                 ; Vzz D                   ; Vz D              :              (A6)
                              Ex                      Vxr Vxx                    Vx

From the above we obtain

     Er D .I         B/ 1 ˛;         Vr r D V C ˙uu ;                Vr x D Vx A0 C ˙uv ;         (A7)
     Ex D .I         A/ 1 ;         Vxx D AVxx A0 C ˙vv ;             Vx D BVx A0 C ˙wv ;         (A8)
                                     V D BV B 0 C ˙ww ;            Vr D BV C ˙wu :            (A9)

Given the well known identity vec .DF G/ D .G 0 ˝ D/vec .F /, we can write

                             vec .Vxx / D ŒI        .A ˝ A/ 1 vec .˙vv /                           (A10)
                             vec .V / D ŒI        .B ˝ B/ 1 vec .˙ww /                           (A11)
                             vec .Vx / D ŒI        .A ˝ B/ 1 vec .˙wv /:                          (A12)

                                    Drawing the time series of  t
   To draw the time series of the unobservable values of  t conditional on the current parameter
draws, we apply the forward filtering, backward sampling (FFBS) approach developed by Carter
and Kohn (1994) and Frühwirth-Schnatter (1994). See also West and Harrison (1997, chapter 15).

                                                    30
Filtering

   The first stage follows the standard methodology of Kalman filtering. Define

     a t D E. t jD t 1 /                 b t D E. t jD t /                 e t D E.z t j t ; D t 1 /   (A13)
     f t D E.z t jD t 1 /                 P t D Var. t jD t 1 /             Q t D Var. t jD t /         (A14)
                                                                                                 0
     R t D Var.z t j t ; D t   1/        S t D Var.z t jD t 1 /             G t D Cov.z t ;  t jD t 1 / (A15)

Conditioning on the (unknown) parameters of the model is assumed throughout but suppressed in
the notation for convenience. First note that

                                         0 jD0  N .b0 ; Q0 /;                                         (A16)

where b0 D Er and Q0 D V ,
                                         1 jD0  N .a1 ; P1 /;                                         (A17)
where a1 D Er and P1 D V , and

                                          z1 jD0  N .f1 ; S1 /;                                        (A18)

where f1 D Ez and S1 D Vzz . Note that

                                               G1 D Vz                                                 (A19)

and that
                                        z1 j1 ; D0  N .e1 ; R1 /;                                     (A20)
where

                                     e1 D f1 C G1 P1 1 .1            a1 /                              (A21)
                                     R1 D S1 G1 P1 1 G10 :                                              (A22)

Combining this density with equation (A17) using Bayes rule gives

                                         1 jD1  N .b1 ; Q1 /;                                         (A23)

where

                     b1 D a1 C P1 .P1 C G10 R1 1 G1 / 1 G10 R1 1 .z1                 f1 /               (A24)
                     Q1 D P1 .P1 C G10 R1 1 G1 / 1 P1 :                                                 (A25)

Continuing in this fashion, we find that all conditional densities are normally distributed, and we
obtain all the required moments for t D 2; : : : ; T :

                       a t D ˛ C Bb t 1                                                                 (A26)
                       P t D BQ t 1 B 0 C ˙ww                                                           (A27)
                                          
                                  bt 1
                       ft D                                                                             (A28)
                                C Ax t 1

                                                    31
                                                                
                                          C ˙uu ˙uv
                                         Qt   1
                          St    D                                                                                (A29)
                                        ˙vu     ˙vv
                                          0
                                                  
                                    Q t 1 B C ˙uw
                          Gt    D                                                                                (A30)
                                          ˙vw
                          e t D f t C G t P t 1 . t a t /                                                       (A31)
                          R t D S t G t P t 1 G t0                                                               (A32)
                          b t D a t C P t .P t C G t0 R t 1 G t / 1 G t0 R t 1 .z t     ft /                     (A33)
                              D a t C G t0 S t 1 .z t f t /                                                      (A34)
                          Q t D P t .P t C G t0 R t 1 G t / 1 P t :                                              (A35)

The values of fa t ; b t ; Q t ; P t g for t D 1; : : : ; T are retained for the next stage.

Sampling

   Let  t D Œr t x t  t 0 . We wish to draw .0 ; 1 ; : : : ; T / conditional on DT . The backward-
sampling approach relies on the Markov property of the evolution of  t and the resulting identity,

      p.0 ; 1 ; : : : ; T jDT / D p.T jDT /p.T    1 jT ; DT 1 /    p.1 j2 ; D1 /p.0 j1 ; D0 /:      (A36)

We first sample T from p.T jDT /, the normal density obtained in the last step of the filtering.
Then, for t D T 1; T 2; : : : ; 1; 0, we sample  t from the conditional density p. t j tC1 ; D t /.
(Note that the first two subvectors of  t are already observed and thus need not be sampled.) To
obtain that conditional density, first note that
                       02               3 2                                    31
                               bt               Q t C ˙uu ˙uv Q t B 0 C ˙uw
         tC1 jD t  N @4  C Ax t 5 ; 4           ˙vu      ˙vv       ˙vw      5A ;          (A37)
                              a tC1           BQ t C ˙wu ˙wv          P tC1
                                                  02 3 2          31
                                                 rt        0 0 0
                                  t jD t  N @4 x t 5 ; 4 0 0 0 5A ;                                            (A38)
                                                 bt        0 0 Qt
and                                                       2             3
                                                            0 0   0
                                             0
                                 Cov. t ;  tC1 jD t / D 4 0 0   0 5:                                           (A39)
                                                            Qt 0 Qt B 0
Therefore,
                                           t j tC1 ; D t  N .h t ; H t /;                                     (A40)
where
     2      3 2             32                              3                                  1   2                   3
        rt      0 0   0         Q t C ˙uu ˙uv Q t B 0 C ˙uw                                             r tC1     bt
h t D 4 x t 5C4 0 0   0 54         ˙vu    ˙vv      ˙vw      5                                      4 x tC1       Ax t 5
                          0
        bt      Qt 0 Qt B      BQ t C ˙wu ˙wv      P tC1                                                tC1     a tC1



                                                         32
and
        2     3 2             32                              3                                             1   2             30
       0 0 0      0 0   0         Q t C ˙uu ˙uv Q t B 0 C ˙uw                                                     0 0   0
Ht D 4 0 0 0 5 4 0 0    0 54         ˙vu    ˙vv      ˙vw      5                                                 4 0 0   0 5
                            0
       0 0 Qt     Qt 0 Qt B      BQ t C ˙wu ˙wv      P tC1                                                        Qt 0 Qt B 0

The mean and covariance matrix of  t are taken as the relevant elements of h t and H t .


                                   Expected returns and past values


    In this section, we derive the equations (8), (11), and (26). We still work in the general case
in which r t is a vector of returns rather than a scalar. Therefore, to continue denoting matrices by
uppercase letters, we replace m by M , n by N ,  by ,  by ˚, ı by , ! by ˝, and  by K.

    Below, we express the vector of conditional expected returns, b t D E.r tC1 jD t /, as a function
of past returns and predictors. Denote

                        ŒM t N t   P t .P t C G t0 R t 1 G t / 1 G t0 R t 1 D G t0 S t 1 ;                        (A41)

so that, from equation (A33), for t > 1,

                       b t D a t C ŒM t N t .z t           ft /
                                                                   
                                                             bt 1          rt
                             D ˛ C Bb t 1 C ŒM t N t 
                                                       x t  Ax t 1
                             D .I B/Er C .B M t /b t 1 C M t r t C N t v t ;                                        (A42)

or
                        bt    Er D B.b t       1    Er / C M t .r t             bt    1/   C Nt vt :                (A43)
For t D 1, we obtain

                                  b1     Er D M1 .r1                   b0 / C N1 v1 ;

where v1 denotes x1      Ex . Repeated substitution for the lagged values of .b t                      Er / gives
                                             t
                                             X                                  t
                                                                                X
                              b t D Er C           s .rs      bs 1 / C               ˚s vs ;                       (A44)
                                             sD1                                sD1

where

                                              s D B t s Ms                                                         (A45)
                                              ˚s D B t s Ns :                                                       (A46)

That is, the expected return conditional on data observed through period t can be written as the
unconditional mean Er plus a linear combination of past return forecast errors, s D rs bs 1 ,
plus a linear combination of past innovations in the predictors. This is equation (8) in the text.

                                                        33
    The conditional expected return b t can be rewritten so that past forecast errors are replaced by
returns in excess of the unconditional mean Er . To do so, modify equation (A42) as

                        bt   Er D .B      M t /.b t     1      Er / C M t .r t         Er / C N t v t     (A47)

so that repeated substitution for the lagged values of .b t                   Er / then yields
                                                 t
                                                 X                             t
                                                                               X
                                  b t D Er C           ˝s .rs       Er / C            s vs               (A48)
                                                 sD1                           sD1

where                        
                                 .B   M t /.B      Mt       1 /    .B      MsC1 /Ms        for s < t
                   ˝s D                                                                                   (A49)
                                 Ms                                                           for s D t
                             
                                 .B   M t /.B      Mt       1 /    .B      MsC1 /Ns        for s < t
                   s D                                                                                   (A50)
                                 Ns                                                           for s D t

That is, b t is then equal to the unconditional mean return Er plus linear combinations of past
returns in excess of Er and past innovations in the predictors. This is equation (11) in the text.

      If Er is replaced by the sample mean in equation (11), then the estimate of b t becomes
                                                 t
                                                 X                  t
                                                                    X
                                        bO t D         Ks rs C             s vs ;                        (A51)
                                                 sD1                sD1

where                                                                    !
                                                             t
                                            1                X
                                       Ks D   I                     ˝l       C ˝s ;                       (A52)
                                            t
                                                             l D1
      Pt
and      sD1   Ks D I . This is a generalized version of equation (26) in the text.

     In the rest of the Appendix, we discuss the special case (implemented in the paper) in which
r t is a scalar. This simplification turns  t , ˛, and B into scalars as well. Therefore, we now turn
back to the notation from the text in which B is replaced by ˇ and the relevant ˙ ’s by  ’s.

                                         Drawing the parameters

    This section describes how we obtain the posterior draws of all parameters conditional on the
current draw of the time series of  t .

Prior distributions

    First, we discuss the prior on .; A; ˛; ˇ/. We require both x t and  t to be stationary, so that
all eigenvalues of A must lie inside the unit circle and ˇ 2 . 1; 1/. Apart from this restriction,
our prior is noninformative about A but informative about ˇ, ˇ  N .0:99; 0:152 / (see Figure
5). We reparameterize the model to replace the intercepts  and ˛ by the unconditional means
of  t and x t , which we denote by E and Ex , respectively. The equations (4) and (5) then read
x tC1 D Ex C A.x t Ex / C v tC1 and  tC1 D E C ˇ. t E / C w tC1 . This reparameterization

                                                            34
allows us to increase the speed of convergence of our MCMC chain by putting a mildly informative
prior on E , E  N .;  N E2  /, centered at the sample mean return with a large prior standard
deviation of 1% per quarter. We use a noninformative prior for Ex , Ex  N .0; E2 x IK / with a
large Ex . All four parameters, A, ˇ, E , and Ex , are independent a priori.

    The prior on ˙ is more complicated. We divide the elements of ˙ into two subsets: first,
the 2  2 submatrix ˙11  Œu2 uw I wu w2 , and second, the elements of ˙ that involve v:
˙.v/  .˙vv ; vu ; vw /. We choose a prior that is informative about ˙11 but noninformative about
˙.v/ . Such a prior is obtained as a posterior of ˙ when a noninformative prior is updated with a
hypothetical sample in which there are T0 observations of .u; w/ but only S0  T0 observations of
v (see Stambaugh, 1997). We choose T0 equal to one fifth of the sample size, which makes the prior
on ˙11 informative (five times less informative than the actual sample). We choose S0 D K C 3,
where K is the number of predictors, which makes the prior on ˙.v/ virtually noninformative (as
informative as a sample of only K C 3 observations, where K D 1 or 3).

    The prior on ˙11 is inverted Wishart, ˙11  I W .T0 ˙O 11;0 ; T0 K 1/, so the prior mean is
E.˙11 / D ˙O 11;0 .T0 =.T0 K 4//. Denote the .i; j / element of ˙O 11;0 by Mij , for i D 1; 2 and
j D 1; 2. The value of M11 is chosen such that the prior mean of u2 is equal to 95% of the sample
variance of market returns. The value of M22 is chosen to deliver the prior mean of w2 which, com-
bined with ˇ of 0.97, sets the variance of  t equal to 5% of the sample variance of market returns.
These values of M11 and M22 lead to a prior for the R2 from the regression of r tC1 on  t that we
find plausible (see Figure 5). To be able to put different priors on uw while keeping the same prior
on u2 and w2 , we adopt a hyperparameter approach. We assume   p that M12 ispan unknown hyperpa-
                                                       p . c M11 M22 ; c M11 M22 /. Since the
rameter with a uniform prior distribution on the interval
prior mean of uw is approximately equal to M12 = M11 M22 , this prior mean is approximately
uniformly distributed as U. c; c/. For all three priors on uw , we specify c D 0:90 and we vary
c as follows: 0.9 for the noninformative prior, -0.35 for the less informative prior, and -0.87 for the
more informative prior. These choices produce the priors on uw plotted in Figure 5.

      The prior on ˙.v/ is obtained by changing variables from (˙vv ; vu ; vw ) to the slope C (3K)
and the residual covariance matrix ˝ (K  K) from the regression of v t on .u t ; w t /: C D .0 C2 /0
(zero intercept), C2 D Œvu vw ˙111 , and ˝ D ˙vv C2 ˙11 C20 . We put a normal-inverted-Wishart
prior on C and ˝: ˝  I W .S0˝O 0 ; S0 1/ and vec .C /j˝  N .cO0 ; ˝ ˝ .X00 X0 / 1 /, where ˝O 0 ,
cO0 , and X00 X0 represent estimates from the S0 periods in the hypothetical sample in which both v t
and .u t ; w t / are available. The choices of ˝O 0 and cO0 are inconsequential because they represent
means of distributions with large variances. The prior variances are large, for two reasons. First,
we choose a very small value for S0 , as explained above. Second, we choose X00 X0 equal to S0
times a diagonal matrix whose .1; 1/ element is one and the remaining diagonal elements are tiny
positive numbers (so .X00 X0 / 1 is large). That is, we choose a hypothetical sample in which v is
much more volatile than .u; w/ in the short overlapping period of S0 observations. As a result, the
priors on C and ˝ are noninformative.

    As mentioned above, these priors on ˙11 , C , and ˝ can be thought of as posteriors. After
changing variables from ˙ to .˙11 ; C; ˝/, the diffuse prior on ˙ , p.˙ / / j˙ j .K C3/=2 , translates
into p.˙11 ; C; ˝/ / j˙11 j.K 3/=2 j˝j .K C3/=2 . When this noninformative prior is updated with


                                                  35
the hypothetical sample of T0 observations of .u; w/ and S0 observations of v, the posteriors of
˙11 , C , and ˝ are exactly the same as the priors described above. See Stambaugh (1997).

Posterior distributions

Drawing .; A; ˛; ˇ/ given ˙

    After changing variables from .; ˛/ into .Ex ; E /, the equations (4) and (5) can be written as
                                                                                
          x tC1        A 0        xt          IK A         0         Ex              v tC1
                                                                             D               ;
           tC1        0 ˇ        t            0       1 ˇ          E              w tC1
       „ ƒ‚ … „ ƒ‚ … „ ƒ‚ … „                       ƒ‚          … „ ƒ‚ …
           q tC1           L1          qt                 L2               Ex
                                                                             
where the covariance matrix of the residuals is ˙.vw/  ˙vv vw I wv w2 . The prior for Ex is
                                                                
                                    Ex  N Ex0 ; Vx0 ;
                                     h                 i
                      0                 2           2
where Ex0  .0 / N and Vx0  Ex IK 0I 0 E . Since both the prior and the likelihood are
normally distributed, the full conditional posterior distribution of Ex is also normal,
                                                               
                                    Ex j  N EQ x ; VQx ;                              (A53)
                                                           h                       PT            i
       Q          1        0    1      1      Q        Q        1           0    1
where Vx D .Vx0 CT L2 ˙.vw/ L2 / and Ex D Vx Vx0 Ex0 C L2 ˙.vw/ tD1 .q tC1 L1 q t / .

     Let x k  .x2k ; : : : ; xTk /0 denote the .T 1/  1 vector of realizations of predictor k in periods
2; : : : ; T , for k D 1; : : : ; K. Also, let x.l / denote the .T 1/  K vectors of realizations of all K
predictors in periods 1; : : : ; T 1. Similarly, let   .2 ; : : : ; T /0 and .l /  .1 ; : : : ; T 1 /0 ,
and let Ex k be the k-th element of Ex . Denote
           0                       1            0                                                           1
                x 1 T 1 Ex 1                        x.l / T 1 Ex0 0         0                  0
           B          ::           C            B                   ::                                      C
           B           :           C            B          0           :      0                  0          C
z D B K                            C; Z D B                                          0
                                                                                                            C;
           @ x       T 1 Ex K A                @          0         0 x.l / T 1 Ex             0          A
                  T 1 E                                 0         0        0         .l / T 1 E
where T 1 is a .T 1/  1 vector of ones, the dimensions of z are Œ.T 1/.K C 1/  1, and the
dimensions of Z are Œ.T 1/.K C 1/  .K 2 C 1/. Then we can write the equations (4) and (5) as
                                            z D Zb C errors ;
where b D .vec .A0/0 ˇ/0 and the covariance matrix of the error terms is ˙.vw/ ˝ IT              1.   The prior
distribution on b is given by
                                       b  N .b0 ; Vb0 /  1b2S ;
where b0 and Vb0 are chosen as explained earlier and 1b2S is equal to one when x t and  t are
                                          h                      i 1
stationary and zero otherwise. Let VOb D Z 0.˙.vw/  1
                                                       ˝ IT 1 /Z     and bO D VOb Z 0.˙.vw/
                                                                                         1
                                                                                            ˝IT 1 /z.
The full conditional posterior distribution of b is then given by
                                                       
                                     bj  N b;    Q VQb  1b2S ;                              (A54)

                                                     36
                                                             
where VQb D .Vb01 C VOb 1 / 1 and bQ D VQb Vb0 1 b0 C VOb 1 bO . We obtain the posterior draws of b
                               
                           Q  Q
by making draws from N b; Vb and retaining only draws that satisfy b 2 S . The posterior draws
of A and ˇ are constructed from the posterior draws of b from the definition b D .vec .A0 /0 ˇ/0.

Drawing ˙ given .; A; ˛; ˇ/
                                                                                   
    Recall that we change variables from ˙ D u2 uv uw I vu ˙vv vw I wu wv w2 to the set
of .˙11 ; C; ˝/, where ˙11  Œu2 uw I wu w2 , and C and ˝ are the slope and the residual
covariance matrix from the regression of v on .u; w/.

    The prior for ˙11 is conditional on the hyperparameter M12 . This hyperparameter can be
drawn from its full conditional posterior density, p.M12 j; D t /, which is given by
                                                       
                  O
                        T0 K 1         T0       1 O
                                                                          p           p
p.M12 j˙11 / / j˙11;0 j     2   exp       tr.˙11 ˙11;0 / ; M12 2 . c M11 M22 ; c M11 M22 /;
                                        2
                                                                                             (A55)
where M12 is the .1; 2/ element of ˙O 11;0 . Although this is not a density of a well known distri-
bution, we can make posterior draws of M12 easily. We approximate    p this density
                                                                                  p by a piecewise
linear function, using a fine (250-point) grid on the interval . c M11 M22 ; c M11 M22 /. For a
random draw z  U.0; 1/, we find the points on the grid whose cumulative probability densities
are immediately above and below z, and we compute the value of M12 by linear interpolation.

    Conditional on M12 , we have the matrix ˙O 11;0 in the prior distribution for ˙11 . In addition,
conditional on .; A; ˛; ˇ/, we have the sample of the residuals .u t ; v t ; w t /, t D 1; : : : ; T . Let Y1;T
denote the T  2 matrix of Œu t w t , let Y2;T denote the T  K matrix of v t , and let X D ŒT Y1;T .
The sample estimates from the regression of Y2;T on Y1;T are given by CO D .X 0X / 1 X 0 Y2;T ,
˝O D .Y2;T      X CO /0 .Y2;T  X CO /=T , and ˙O 11 D Y1;T  0
                                                              Y1;T =T . The posterior of ˙11 has an
inverted Wishart distribution:
                          ˙11 j  I W .T0 ˙O 11;0 C T ˙O 11 ; T C T0 K 1/:                          (A56)
                                                             h                       i
In addition, let VC D .X00 X0 C X 0 X / 1 , CQ D VC .X00 X0 /CO0 C .X 0X /CO , cQ D vec .CQ /, and
D D CO00 X00 X0 CO0 C CO 0 X 0 X CO CQ 0 VC 1 CQ . The posterior of ˝ has an inverted Wishart distribution:
                            ˝j  I W .S0 ˝O 0 C T ˝O C D; T C S0               1/;                      (A57)
and the conditional posterior of c D vec .C / is normal:
                                        cj˝;   N .c;
                                                    Q ˝ ˝ VC /:                                          (A58)
Given the posterior draws of .˙11 ; C; ˝/, we construct the remaining (non-˙11) elements of ˙ as
follows: Œvu vw  D C2 ˙11 and ˙vv D ˝ C C2 ˙11 C20 , where C D .C1 C2 /0.

    Our inference is based on 25,000 draws from the posterior distribution. First, we generate a
sequence of 76,000 draws. We discard the first 1,000 draws as a “burn-in” and take every third
draw from the rest to obtain a series of 25,000 draws that exhibit little serial correlation. The
posterior draws of the relevant quantities such as uw , x , R2 . t on x t /, R2 .r tC1 on  t /, etc.
are constructed easily from the posterior draws of the basic parameters in the model.

                                                      37
                                       Maximum likelihood estimation


Denote the variance-covariance matrix of the disturbances in equations (4) and (28) as
                                                         2       0
                                                                       
                              t                           v
                      Cov.         ; t vt / D ˙ D                       :                                                                 (A59)
                              vt                             v ˙vv

Maximum likelihood estimates are computed as the values of Ez , ˇ, m, n, A, 2 , v , and ˙vv
that minimize
                                     T
                                     X ˇ            ˇ                          0
                                                                                                                    
                     2 ln L D           ˇV tjt   1
                                                     ˇ C .z t     zO tjt   1/       V tjt1 1 .z t     zO tjt   1/       ;                  (A60)
                                     tD1

where zO1j0 D Ez ,                                                         
                                                             r2 xr
                                                                  0
                                            V1j0 D                                   ;
                                                             xr Vxx

                                     0                                         
       r2 D .1          ˇ2 /   1
                                     n ˙vv n C .1        ˇ 2 C m2 /2 C 2mv
                                                                            0
                                                                               n ;
        xr D .I ˇA/ 1 ŒA˙vv n C ŒI .ˇ m/Av  ;
                                                                         
    zO tjt 1 D Ez C F11 .z t 1 Ez / C F12 ˙  V t 11jt 2 z t 1 zO t 1jt 2 ;                                                 t D 2; : : : ; T;
                                         0
    V tjt 1 D F12 ˙  ˙  V t 11jt 2 ˙  F12 C ˙  ; t D 2; : : : ; T;
                                                                                                    
                                           ˇ 0                              .ˇ          m/ n0
                           F11 D                     ;    F12 D                                            ;
                                           0 A                                        0    0
and Vxx is given by (A10).


                                                 The R2 ratios.


   The numerator of the R2 ratio in equation (29) is computed as

                         Var.E. t jx t //   Var.E. t / C Vx Vxx1 .x t                            E.x t ///               Vx Vxx1 Vx
                                                                                                                                      0
    R2 . t on x t / D                     D                                                                        D                       ;
                            Var. t /                      Var. t /                                                             V
                                                                                                                                           (A61)
where Vxx , V , and Vx are given in equations (A10), (A11), and (A12), respectively.

   The denominator of the R2 ratio in equation (29) is computed as

                                     Var.E. t jD t //   Var. t / Var. t jD t //                                           Qt
            R2 . t on D t / D                         D                           D1                                            ;         (A62)
                                        Var. t /                 Var. t /                                                  V




                                                             38
where Q t is given in equation (A35). We replace Q t by its steady-state value, Q, which can be
shown to be equal to a solution of a quadratic equation:
              q
                 12 42 1
     Q D                       ;                                                          (A63)
                      2
     1 D .1 ˇ 2 /.u2 uv ˙vv1 vu/ C 2ˇ.uw wv ˙vv1 vu/ .w2 wv ˙vv1 vw /
         D .1 ˇ 2 /Var.ujv/ C 2ˇCov.u; wjv/ Var.wjv/
     2 D .uw wv ˙vv1 vu /2 .u2 uv ˙vv1 vu /.w2 wv ˙vv1 vw /
         D Cov.u; wjv/2 Var.ujv/Var.wjv/ < 0

   The value of Q is also used in computing the steady-state values of M t and N t from equation
(A41), denoted by m t and n t in the scalar case:
                                                                            1
                         m D .ˇQ C Cov.u; wjv//.Q C Var.ujv//                                  (A64)
                         n D .wv muv /˙vv1 :                                                 (A65)

                          Variance decomposition of expected return.

    In equation (34), the conditional expected return  t depends on three time-varying variables:
   1. C1 D x Pt ,1the current predictor values
                       i
   2. C2 D PiD0 ˇ u t i , an infinite
                                       sum of current and lagged unexpected returns
                 1
   3. C3 D iD0 ˇ i IK Ai v t i , an infinite sum of current and lagged predictor innovations ,
plus an error term. In the variance decomposition in Table IV, we consider regressions of  t on
various subsets of .C 1; C 2; C 3/. Let C denote a given subset of .C 1; C 2; C 3/. The R2 from the
regression of  t on C is equal to
                                                       0
                                    2
                                                      VC VC 1 VC
                                 R . t on C / D                     :                         (A66)
                                                          V
The matrix VC , the covariance matrix of C , is pieced together from
               Var.C1/ D Vxx
               Var.C2/ D u2 .1 ˇ 2 / 1
                         
         vec .Var.C3// D .1 ˇ 2 / 1 IK 2       .IK ˇA/ 1 ˝ IK               IK ˝ .IK     ˇA/ 1 C
                                                 
                              C .IK 2    A ˝ A/ 1 vec .˙vv /
          Cov.C1, C2/ D .IK ˇA/ 1 vu
                                                
          Cov.C2, C3/ D .1 ˇ 2 / 1 IK .IK ˇA/ 1 vu
                                                        
    vec .Cov.C1, C30 // D IK ˝ .IK ˇA/ 1 C .IK 2 A ˝ A/ 1 vec .˙vv /;
and VC , the vector of covariances between  t and C , is built from
             Cov. t ; C10 / D     v Var.C1/ C    u Cov.C1, C2/0 C       v Cov.C1,  C30 /0
             Cov. t ; C2/ D       u Var.C2/ C    v Cov.C1, C2/ C        v Cov.C2, C3/
             Cov. t ; C30 / D     v Var.C3/ C
                                                              0
                                                  v Cov.C1, C3 / C
                                                                                        0
                                                                          u Cov.C2, C3/ :



                                                 39
                          Panel A.     Coefficients on lagged forecast errors in E(r    |D)
                                                                                   t+1       t
 0.04

 0.02

    0

−0.02

−0.04                                                                                             ρuw = 0
                                                                                                  ρ     = −0.47
                                                                                                   uw
−0.06
                                                                                                  ρ     = −0.85
                                                                                                   uw
−0.08                                                                                             ρ     = −0.99
                                                                                                   uw

        0      20       40        60         80       100       120         140        160       180         200



                     Panel B.    Weights on lagged returns in E(r | D , E(r) = sample mean)
                                                                  t+1   t

 0.04


 0.02


    0


−0.02                                                                                             ρ     =0
                                                                                                   uw
                                                                                                  ρuw = −0.47
                                                                                                  ρ     = −0.85
−0.04                                                                                              uw
                                                                                                  ρ     = −0.99
                                                                                                   uw

        0      20       40        60         80       100       120         140        160       180         200
                                                     Return Lag

Figure 1. The effect of lagged returns on E.r tC1 jD t / when no predictors are used. Panel A plots s ,
the coefficients on lagged forecast errors ( t s D r t s E.r t s jD t s 1 /) in E.r tC1 jD t /. Panel B plots
s , the weights on lagged total returns in E.r tC1 jD t / when the unconditional mean return is estimated
by the sample mean over the previous 208 quarters (which is the length of the sample used in subsequent
analysis). No predictors are used in the predictive system. The steady-state values of all coefficients are
plotted. The different lines correspond to different values of uw , the correlation between expected and
unexpected returns. The mean reversion coefficient in the AR(1) process for the conditional expected return
 t is set equal to ˇ D 0:9. The predictive R2 —the fraction of variation in r tC1 than can be explained by
 t —is set equal to R2 D 0:05.




                                                      40
                             ρ     =0
                              uw
                    0.08
                             ρuw = −0.47
                             ρuw = −0.85
                    0.07     ρuw = −0.99


                    0.06



                    0.05



                    0.04
 Expected return




                    0.03



                    0.02



                    0.01



                      0



                   −0.01



                   −0.02

                      1952         1960    1969         1977            1986             1994            2003
                                                         Year


Figure 2. The equity premium E.r tC1 jD t / from the predictive system with no predictors. This figure
plots the time series of the quarterly equity premium estimated for four different values of uw , the correla-
tion between expected and unexpected returns. The mean reversion coefficient in the AR(1) process for the
conditional expected return  t is set equal to ˇ D 0:9. The predictive R2 —the fraction of variation in r tC1
than can be explained by  t —is set equal to R2 D 0:05. The parameters represent quarterly values.




                                                      41
                         Panel A. ρ        =0                                  Panel B. ρ        = 0.3
                                     vw                                                     vw
      0.05                                                      0.05
                    System, Low ρ
                                     uv
     0.045                                                     0.045
                    System, High ρ
                                      uv
      0.04          Pred Regression
                                                                0.04
                    All Past Returns
     0.035
                                                               0.035

      0.03
                                                                0.03
2




     0.025
 R




                                                          R2
                                                               0.025
      0.02
                                                                0.02
     0.015
                                                               0.015
      0.01
                                                                0.01
     0.005
                                                               0.005
         0
         −1       −0.5          0               0.5   1
                               ρ                                  0
                                uw                                −1       −0.5         0            0.5     1
                                                                                       ρ
                                                                                        uw




                      Panel C. ρ         = 0.6                                 Panel D. ρ        = 0.9
                                    vw                                                      vw
      0.05                                                      0.05

     0.045                                                     0.045

      0.04                                                      0.04

     0.035                                                     0.035

      0.03                                                      0.03
2




                                                          R2




     0.025                                                     0.025
 R




      0.02                                                      0.02

     0.015                                                     0.015

      0.01                                                      0.01

     0.005                                                     0.005

         0                                                        0
         −1       −0.5          0               0.5   1           −1       −0.5         0            0.5     1
                               ρuw                                                     ρuw




Figure 3. Predictive R2 ’s. Each panel plots the R2 ’s from three approaches to predicting stock returns r tC1
using information observable at time t. The approaches are: the predictive regression of r tC1 on a single
predictor x t (solid line), the ARMA(1,1) model that uses the full history of past returns but no predictor
data (dotted line), and the predictive system, which uses the full history of returns and predictor realizations
(dashed and dash-dot lines). The dashed (dash-dot) line corresponds to a “low” (“high”) value of uv , which
represents the value obtained when the partial correlation between u t and v t given w t equals uvjw D 0:9
(0.9). The conditional correlation between  t and x t , vw , ranges from 0 in Panel A to 0.9 in Panel D. In
all four panels, ˇ D A D 0:9, and the true predictive R2 (from the regression of r tC1 on  t ) is 0.05. In
Panel A, the solid line coincides with the x axis, and the dashed and dash-dot lines overlap.

                                                      42
                                      Panel A. Predictor: Dividend Yield



 0.05


    0

                Regression, fitted values
−0.05           System, maximum likelihood

    1952                     1965                    1978                     1991                     2004

                                           Panel B. Predictor: CAY



 0.05


    0


−0.05

    1952                     1965                    1978                     1991                     2004

                           Panel C. Predictors: Dividend Yield, CAY, and Bond Yield



 0.05


    0


−0.05

    1952                     1965                    1978                     1991                     2004

Figure 4. The equity premium: Regression vs. system with no prior information. This figure plots the
time series of the quarterly equity premium estimated in two different environments. The dotted line plots
the OLS fitted values from the predictive regression of r tC1 on the given predictor(s). The solid line plots
the maximum likelihood estimates of E.r tC1 jD t / from the predictive system. In Panel A, the estimation
uses one predictor, dividend yield. In Panel B, the single predictor is CAY. In Panel C, three predictors are
used: dividend yield, CAY, and the bond yield. The sample period is 1952Q1–2003Q4.




                                                     43
             Panel A. Priors for ρ                                  Panel B. Implied priors for ρ2
                                   uw                                                                  uw
 9                                                         6
                              More informative                          More informative
 8                            Less informative                          Less informative
                              Noninformative                            Noninformative
                                                           5
 7


 6                                                         4


 5
                                                           3
 4


 3                                                         2


 2
                                                           1
 1


 0                                                         0
 −1        −0.5          0           0.5         1             0     0.2       0.4         0.6     0.8      1




               Panel C. Prior for R2                                       Panel D. Prior for β
60                                                         6



50                                                         5



40                                                         4



30                                                         3



20                                                         2



10                                                         1



 0                                                         0
     0      0.05        0.1       0.15        0.2          0.2         0.4           0.6         0.8        1



Figure 5. Prior distributions. Panel A plots three prior distributions for the correlation between expected
and unexpected returns, uw . The noninformative prior (dotted line) is flat between -0.9 and 0.9, with
tails fading away as uw approaches ˙1. The less informative prior (dashed line) has 99.9% of its mass
below zero (uw < 0). The more informative prior (solid line) has 99.9% of its mass below -0.71, so that
  2 > 0:5 (i.e., unexpected changes in the discount rate explain over half of the variance of unexpected
uw
                                                                         2
market returns). Panel B plots the corresponding implied priors on uw      . Panel C plots the prior on the
              2
predictive R from the regression of returns r tC1 on expected returns  t . Panel D plots the prior on the
slope coefficient ˇ in the AR(1) process for  t . All parameters correspond to quarterly data.



                                                     44
                                           Panel A. Slope on Bond Yield
200
                                                                     Regression, diffuse prior
150                                                                  System, noninformative about ρuw
                                                                     System, more informative about ρ
                                                                                                        uw
100

 50

  0
 −0.01     −0.005         0        0.005        0.01        0.015    0.02       0.025       0.03      0.035

                                       Panel B. Slope on Dividend Yield


100



 50



  0
 −0.01     −0.005         0        0.005        0.01        0.015    0.02       0.025       0.03      0.035

                                             Panel C. Slope on CAY

150


100


 50


  0
 −0.01     −0.005         0        0.005        0.01        0.015    0.02       0.025       0.03      0.035

Figure 6. Posterior distributions of slope coefficients from predictive regressions. We estimate both the
predictive system and the predictive regression with three predictors: the bond yield, dividend yield, and
CAY. The dashed line plots the posteriors from the standard predictive regression of r tC1 on x t under the
diffuse prior. The dotted line plots the implied posteriors constructed from the results of the predictive sys-
tem under the “noninformative” prior on uw . The solid line plots the implied posteriors from the predictive
system under the “more informative” prior on uw (uw < 0:71). To facilitate comparisons across panels,
all predictors are scaled to have unit variance. The sample period is 1952Q1–2003Q4.




                                                       45
                    2                                                            2
          Panel A. R from Regression of µ on x                       Panel B. R from Regression of r           on µ
                                         t    t                                                          t+1        t
 3                                                         30
                                                                                         Noninformative (ρuw)
2.5                                                        25                            Less informative (ρ )
                                                                                                               uw
                                                                                         More informative (ρuw)
 2                                                         20


1.5                                                        15


 1                                                         10


0.5                                                         5


 0                                                          0
      0       0.2       0.4   0.6     0.8         1              0            0.05        0.1         0.15          0.2



                     Panel C. ρ                                                      Panel D. ρx,µ
                                v,w
 5
                                                            4

 4                                                         3.5

                                                            3
 3                                                         2.5

                                                            2
 2
                                                           1.5

                                                            1
 1
                                                           0.5

 0                                                          0
              0               0.5                 1                       0                     0.5                     1
Figure 7. Posterior distributions for the relations between the dividend yield and expected return.
Panel A plots the posterior of the fraction of variation in the expected return  t that can be explained by
the predictor x t , which is the dividend yield. Panel B plots the posterior of the predictive R2 . Panel C
plots the posterior of the conditional correlation vw between the dividend yield and  t . Panel D plots the
posterior of the unconditional correlation x between the dividend yield and  t . The three lines in each
panel represent three different prior distributions. The solid line represents the “more informative” prior on
uw (uw < 0:71), the dashed line is the “less informative” prior on uw (uw < 0), and the dotted line
is the “noninformative” prior on uw . The sample period is 1952Q1–2003Q4.




                                                      46
                     2                                                           2
          Panel A. R from Regression of µ on x                        Panel B. R from Regression of r          on µ
                                         t     t                                                         t+1        t
                                                            35
10                                                                                       Noninformative (ρuw)
                                                            30                           Less informative (ρ )
                                                                                                               uw
  8                                                                                      More informative (ρuw)
                                                            25

  6                                                         20

                                                            15
  4
                                                            10
  2
                                                             5

  0                                                          0
      0       0.2        0.4   0.6    0.8          1              0          0.05         0.1         0.15          0.2



                     Panel C. ρ                                                      Panel D. ρx,µ
                                v,w
  4

3.5                                                          4

                                                            3.5
  3
                                                             3
2.5
                                                            2.5
  2
                                                             2
1.5
                                                            1.5
  1
                                                             1
0.5                                                         0.5

  0                                                          0
                 0             0.5                 1                         0                  0.5                     1
Figure 8. Posterior distributions for the relations between the bond yield and expected return. Panel
A plots the posterior of the fraction of variation in the expected return  t that can be explained by the
predictor x t , which is the bond yield. Panel B plots the posterior of the predictive R2 . Panel C plots the
posterior of the conditional correlation vw between the bond yield and  t . Panel D plots the posterior of the
unconditional correlation x between the bond yield and  t . The three lines in each panel represent three
different prior distributions. The solid line represents the “more informative” prior on uw (uw < 0:71),
the dashed line is the “less informative” prior on uw (uw < 0), and the dotted line is the “noninformative”
prior on uw . The sample period is 1952Q1–2003Q4.




                                                       47
                                       Panel A. Predictor: Dividend Yield

 0.04
 0.02
    0
−0.02           Regression, fitted values
−0.04           System, noninformative about ρuw

−0.06           System, more informative about ρ
                                                 uw

    1952                     1965                     1978                     1991                     2004

                                            Panel B. Predictor: CAY

 0.06
 0.04
 0.02
    0
−0.02
−0.04

    1952                     1965                     1978                     1991                     2004

                            Panel C. Predictors: Dividend Yield, CAY, and Bond Yield
  0.1


 0.05


    0


−0.05

    1952                     1965                     1978                     1991                     2004

Figure 9. The equity premium: Regression vs. system with prior information. This figure plots the time
series of the quarterly equity premium estimated in three different environments. The dashed line plots the
OLS fitted values from the predictive regression of r tC1 on the given predictor(s). The dotted line plots the
posterior means of E.r tC1 jD t / from the predictive system under the “noninformative” prior on uw . The
solid line plots the posterior means of E.r tC1 jD t / from the predictive system under the “more informative”
prior on uw (uw < 0:71). In Panel A, the estimation uses one predictor, dividend yield. In Panel B, the
single predictor is CAY. In Panel C, three predictors are used: dividend yield, CAY, and bond yield. The
sample period is 1952Q1–2003Q4.

                                                      48
                                  −3
                               x 10

                          7



                         6.8



                         6.6
Variance (per quarter)




                         6.4



                         6.2



                          6



                         5.8



                         5.6



                         5.4


                                          Noninformative (ρ     )
                                                             uw
                         5.2
                                          Less informative (ρ       )
                                                              uw
                                          More informative (ρ       )
                                                                uw
                          5
                                      2        4         6              8          10        12         14   16   18   20
                                                                        Predictive Horizon (quarters)



Figure 10. Variance of multiperiod returns. This figure plots the per-quarter variance of the predictive
distribution of k-period stock returns. The investment horizon k ranges from 1 to 20 quarters. The variances
are estimated in the predictive system using the dividend yield as an imperfect predictor. The solid line
represents the “more informative” prior on uw (uw < 0:71), the dashed line is the “less informative”
prior on uw (uw < 0), and the dotted line is the “noninformative” prior on uw . The sample period is
1952Q1–2003Q4.




                                                                                  49
                                                          Table I
                                                  Predictive Regressions
This table summarizes the results from predictive regressions r t D a C b 0 x t 1 C e t , where x t D  C Ax t 1 C v t . r t
denotes quarterly excess stock market return and x t 1 denotes the predictors (listed in the column headings) lagged
by one quarter. The table reports the estimated slope coefficients b,   O the correlation Corr.e t ; b 0 v t / between unexpected
returns and shocks to expected returns, and the (unadjusted) R2 from the predictive regression. The correlations
and R2 s are reported in percent (i.e., 100). The OLS t-statistics are given in parentheses “( )”. The t-statistic of
Corr.e t ; b 0 v t / is computed as the t-statistic of the slope from the regression of the sample residuals eO t on bO vO t . The
p-values associated with all t-statistics and R2 s are computed by bootstrapping and reported in brackets “[ ]”.
                          Bond Yield        Dividend Yield        CAY           Corr.e t ; b 0 v t /    R2
                                                 Panel A. 1952 Q1 – 2003 Q4
                              2.716                                                 21.735              4.231
                             (3.024)                                                (3.204)            [0.002]
                             [0.001]                                                [0.001]
                                                 1.153                             -91.887              2.252
                                                (2.184)                           (-33.506)            [0.059]
                                                [0.057]                            [1.000]
                                                                  1.704            -53.556              7.292
                                                                 (4.035)           (-9.124)            [0.000]
                                                                 [0.000]           [1.000]
                              2.573              1.028            1.346            -35.635             11.777
                             (2.902)            (1.966)          (3.139)           (-5.487)            [0.000]
                             [0.003]            [0.058]          [0.003]           [1.000]
                                                 Panel B. 1952 Q1 – 1977 Q4
                              6.385                                                 25.079              7.080
                             (2.801)                                                (2.629)            [0.007]
                             [0.004]                                                [0.008]
                                                 2.658                             -96.531              7.003
                                                (2.785)                           (-37.522)            [0.015]
                                                [0.014]                            [1.000]
                                                                  3.028            -47.663             15.024
                                                                 (4.267)           (-5.503)            [0.000]
                                                                 [0.000]           [1.000]
                              3.489              1.345            2.129            -53.153             17.975
                             (1.490)            (1.349)          (2.534)           (-6.369)            [0.000]
                             [0.090]            [0.177]          [0.012]           [1.000]
                                                 Panel C. 1978 Q1 – 2003 Q4
                              2.073                                                 22.624              3.931
                             (2.053)                                                (2.357)            [0.047]
                             [0.020]                                                [0.011]
                                                 0.784                             -88.194              1.273
                                                (1.152)                           (-18.989)            [0.423]
                                                [0.409]                            [1.000]
                                                                  1.165            -56.949              4.122
                                                                 (2.104)           (-7.031)            [0.045]
                                                                 [0.037]           [1.000]
                              2.203              0.755            0.968            -18.619              8.828
                             (2.197)            (1.101)          (1.734)           (-1.923)            [0.053]
                             [0.023]            [0.313]          [0.118]           [0.967]

                                                               50
                                               Table II
           Explanatory Power of the Predictive Regression Relative to the Predictive System:
                                         Empirical Results

Panel A shows the posterior means and standard deviations (the latter in parentheses) of the ratios of two R-squareds,
R2 .r eg/=R2 .sys/. A ratio smaller than one indicates that the predictive system estimates  t more precisely than the
predictive regression does. The smaller the ratio, the larger the advantage of using the predictive system. R2 .r eg/,
computed as the R-squared from the regression of  t on the given predictors, summarizes the usefulness of the pre-
dictive regression in estimating  t . R2 .sys/, computed as 1 Var. t jD t /=Var. t / where D t contains all historical
market returns and predictor realizations, summarizes the usefulness of the predictive system in estimating  t . Panel B
shows the posterior means and standard deviations of 1 M SE.sys/=M SE.r eg/. M SE.r eg/ is the mean squared
error from the predictive regression of r t C1 on the given predictors. M SE.sys/ is the mean squared error from the
predictive system. Positive values of one minus the MSE ratio indicate that the predictive system forecasts returns
more precisely than the predictive regression does. The results are reported for four different prior distributions on
uw , the correlation between expected and unexpected returns. Four sets of predictors are considered: dividend yield,
bond yield, CAY, and all three predictors combined. The sample period is 1952Q1–2003Q4.


                                                                  Predictors

                                Dividend Yield            Bond Yield           CAY           All 3 Predictors
                                         Panel A. The R-squared Ratios, R2 .r eg/=R2 .sys/.

       Diffuse                        0.28                    0.73              0.86               0.59
       Prior                         (0.17)                  (0.23)            (0.16)             (0.30)

       Noninformative                 0.50                    0.44              0.61               0.46
       Prior on uw                  (0.27)                  (0.25)            (0.27)             (0.22)

       Less Informative               0.59                    0.34              0.73               0.50
       Prior on uw                  (0.22)                  (0.20)            (0.23)             (0.22)

       More Informative               0.81                    0.08              0.64               0.70
       Prior on uw                  (0.19)                  (0.08)            (0.22)             (0.19)

                              Panel B. The Mean Squared Error Ratios, 1            M SE.sys/=M SE.r eg/.

       Diffuse                        0.04                    0.06              0.03               0.17
       Prior                         (0.07)                  (0.15)            (0.09)             (0.22)

       Noninformative                 0.02                    0.02              0.02               0.03
       Prior on uw                  (0.04)                  (0.05)            (0.04)             (0.04)

       Less Informative               0.02                    0.02              0.01               0.04
       Prior on uw                  (0.04)                  (0.06)            (0.04)             (0.05)

       More Informative               0.02                    0.03              0.02               0.02
       Prior on uw                  (0.07)                  (0.07)            (0.06)             (0.07)




                                                          51
                                                 Table III
                                  Comparing Estimates of Expected Return.

This table compares the time series of the posterior means of E.r t C1 jD t / obtained in five different environments:
(1) Predictive regression: OLS fitted values
(2) Predictive system: Maximum likelihood estimates
(3) Predictive system: Noninformative prior about uw
(4) Predictive system: Less informative prior about uw
(5) Predictive system: More informative prior about uw
The priors in (3)-(5) are informative about the persistence and volatility of  t . The correlations between the quarterly
series of the posterior means of E.r t C1 jD t / are reported in italics below the main diagonal of each left-panel 5  5
matrix. Above the main diagonal of the same matrix are the mean absolute differences between the posterior means
of E.r t C1 jD t / in percent per quarter. Each right-panel 5  5 matrix reports the average utility losses, in percent per
quarter, of a mean-variance investor who is forced to hold a suboptimal portfolio of the stock market and a risk-free
T-bill: a portfolio that is optimal under the beliefs in the given row when the true beliefs are in the given column.
(For example, the (2,5) cell of the 5  5 matrix reports the certainty equivalent loss of an investor who has the more
informative prior but is forced to hold the portfolio that is optimal under the maximum likelihood estimates.) The risk
aversion is chosen such that there is no borrowing or lending given the sample mean and variance of market returns.
The sample period is 1952Q1-2003Q4.



                     Correlation (%) n Mean Abs Diff (%)                        Average Utility Loss (%)

                       (1)       (2)      (3)       (4)      (5)          (1)      (2)    (3)     (4)      (5)

                                              Panel A. Predictor: Dividend Yield

           (1)                  0.57     0.44      0.41     0.29          0        0.11   0.09    0.07    0.03
           (2)       97.49               0.65      0.62     0.49         0.11        0    0.20    0.18    0.10
           (3)       90.47     81.11               0.06     0.27         0.09      0.19    0      0.00    0.03
           (4)       92.42     83.49     99.78              0.22         0.07      0.17   0.00     0      0.02
           (5)       97.67     91.32     95.81    97.41                  0.03      0.10   0.03    0.02      0

                                                   Panel B. Predictor: CAY

           (1)                  0.66     1.13      1.22     1.60          0        0.19   0.57    0.65    1.13
           (2)       94.82               1.36      1.39     1.65         0.19        0    0.88    0.95    1.38
           (3)       86.28     82.02               0.37     0.96         0.57      0.85    0      0.06    0.39
           (4)       96.88     93.26     95.43              0.63         0.64      0.92   0.06     0      0.16
           (5)       89.71     92.03     59.38    80.11                  1.09      1.32   0.38    0.16      0

                                  Panel C. Predictors: Dividend Yield, CAY, Bond Yield

           (1)                  0.92     1.33      1.27     1.60          0        0.40   0.82    0.74    1.19
           (2)       91.06               1.27      1.22     1.62         0.42        0    0.76    0.68    1.16
           (3)       80.38     82.36               0.14     1.51         0.80      0.72    0      0.01    0.94
           (4)       82.30     84.11     99.79              1.46         0.72      0.64   0.01     0      0.84
           (5)       83.42     89.93     80.75    84.00                  1.19      1.13   0.96    0.87      0



                                                           52
                                                         Table IV
                                         Variance Decomposition of Expected Return.

     This table reports the posterior means and standard deviations (the latter in parentheses) of the R2 s from the regressions
     of the market’s expected excess return  t on its selected components. The first column of each panel, labeled x t , shows
     the fraction of variance of  t that can be explained by the set of predictors listed in the panel heading. Four sets of
     predictors are considered: the dividend yield, bond yield, CAY, and the combination of all three of these predictors.
     The second column of each panel, labeled x t ; fus gst , shows the fraction of variance of  t that can be explained
     jointly by the predictors and by the innovations to stock market returns u t ; u t 1; u t 2; : : :. The third column, labeled
     x t ; fus ; vs gst , shows the fraction of variance of  t that can be explained jointly by the predictors, by the innovations
     to stock market returns u t ; u t 1; u t 2; : : :, and by the innovations to the predictors v t ; v t 1; v t 2 ; : : :. For each set of
     predictors, the predictive system is estimated under three different priors, which are described in the row labels. The
     sample period is 1952Q1-2003Q4.




                                Components of Expected Return                                 Components of Expected Return

                               xt          x t , fus gst    x t , fus ; vs gst             xt         x t , fus gst    x t , fus ; vs gst

                                      Panel A. Dividend Yield                                        Panel B. Bond Yield

Noninformative                0.34             0.43                0.48                     0.33             0.64                0.83
Prior on uw                 (0.20)           (0.21)              (0.21)                   (0.21)           (0.21)              (0.13)
Less Informative              0.40             0.49                0.53                     0.24             0.73                0.86
Prior on uw                 (0.18)           (0.18)              (0.18)                   (0.17)           (0.16)              (0.11)
More Informative              0.57             0.80                0.81                     0.03             0.86                0.95
Prior on uw                 (0.15)           (0.06)              (0.06)                   (0.03)           (0.05)              (0.04)

                                            Panel C. CAY                                        Panel D. All Three Predictors

Noninformative                0.50             0.59                0.81                     0.42             0.49                0.90
Prior on uw                 (0.22)           (0.22)              (0.12)                   (0.20)           (0.22)              (0.07)
Less Informative              0.60             0.70                0.83                     0.46             0.55                0.90
Prior on uw                 (0.20)           (0.17)              (0.10)                   (0.20)           (0.22)              (0.07)
More Informative              0.53             0.87                0.92                     0.63             0.85                0.94
Prior on uw                 (0.18)           (0.07)              (0.05)                   (0.17)           (0.12)              (0.04)




                                                                      53
                                          References
Ang, Andrew, and Geert Bekaert, 2007, Stock return predictability: Is it there?, Review of Finan-
   cial Studies 20, 651–707.
Ang, Andrew, and Monika Piazzesi, 2003, A no-arbitrage vector autoregression of term structure
   dynamics with macroeconomic and latent variables, Journal of Monetary Economics 50, 745–
   787.
Avramov, Doron, 2002, Stock return predictability and model uncertainty, Journal of Financial
   Economics 64, 423–458.
Avramov, Doron, 2004, Stock return predictability and asset pricing models, Review of Financial
   Studies 17, 699–738.
Avramov, Doron, and Russ Wermers, 2006, Investing in mutual funds when returns are predictable,
   Journal of Financial Economics 81, 339–377.
Baks, Klaas P., Andrew Metrick, and Jessica Wachter, 2001, Should investors avoid all actively
   managed mutual funds? A study in Bayesian performance evaluation, Journal of Finance 56,
   45–85.
Barberis, Nicholas, 2000, Investing for the long run when returns are predictable, Journal of Fi-
   nance 55, 225–264.
van Binsbergen, Jules H., and Ralph S. J. Koijen, 2007, Predictive Regressions: A Present-Value
   Approach, Working paper, Duke University.
Brandt, Michael W., and Qiang Kang, 2004, On the relationship between the conditional mean
   and volatility of stock returns: A latent VAR approach, Journal of Financial Economics 72,
   217–257.
Campbell, John Y., 1987, Stock returns and the term structure, Journal of Financial Economics 18,
   373–399.
Campbell, John Y., 1991, A variance decomposition for stock returns, The Economic Journal 101,
   157–179.
Campbell, John Y., and John Ammer, 1993, What moves the stock and bond markets? A variance
   decomposition for long-term asset returns, Journal of Finance 48, 3–37.
Campbell, John Y., and Motohiro Yogo, 2006, Efficient tests of stock return predictability, Journal
   of Financial Economics 81, 27–60.
Campbell, John Y., and Samuel B. Thompson, 2008, Predicting the equity premium out of sample:
   Can anything beat the historical average?, Review of Financial Studies, forthcoming.
Carter, Chris K., and Robert Kohn, 1994, On Gibbs sampling for state space models, Biometrika
   81, 541–553.
Casella, G., and E.I. George, 1992, Explaining the Gibbs sampler, The American Statistician 46,
   167–174.



                                                54
Cavanagh, Christopher L., Graham Elliott, and James H. Stock, 1995, Inference in models with
   nearly integrated regressors, Econometric Theory 11, 1131–1147.
Clark, Todd E., and Kenneth D. West, 2006, Using out-of-sample mean squared prediction errors
   to test the martingale difference hypothesis, Journal of Econometrics 135, 155–186.
Clark, Todd E., and Kenneth D. West, 2007, Approximately normal tests for equal predictive
   accuracy in nested models, Journal of Econometrics 138, 291–311.
Cochrane, John H., 2008, The dog that did not bark: A defense of return predictability, Review of
   Financial Studies, forthcoming.
Conrad, Jennifer, and Gautam Kaul, 1988, Time-variation in expected returns, Journal of Business
   61, 409-425.
Cremers, K.J. Martijn, 2002, Stock return predictability: A Bayesian model selection perspective,
   Review of Financial Studies 15, 1223–1249.
Dangl, Thomas, and Michael Halling, 2006, Equity return prediction: Are coefficients time-
   varying?, Working paper, University of Vienna.
Duffee, Gregory R., 2006, Are variations in term premia related to the macroeconomy?, Working
   paper, University of California, Berkeley.
Elliott, Graham, and James H. Stock, 1994, Inference in time series regression when the order of
    integration of a regressor is unknown, Econometric Theory 10, 672–700.
Fama, Eugene F., and Kenneth R. French, 1988, Dividend yields and expected stock returns, Jour-
   nal of Financial Economics 22, 3–26.
Fama, Eugene F., and G. William Schwert, 1977, Asset returns and inflation, Journal of Financial
   Economics 5, 115–146.
Ferson, Wayne E., and Campbell R. Harvey, 1991, “The Variation of Economic Risk Premiums,”
   Journal of Political Economy 99, 385–415.
Ferson, Wayne E., and Campbell R. Harvey, 1993, “The Risk and Predictability of International
   Equity Returns,” Review of Financial Studies 6, 527–566.
Ferson, Wayne E., Sergei Sarkissian, and Timothy T. Simin, 2003, Spurious regressions in financial
   economics?, Journal of Finance 58, 1393–1413.
Frühwirth-Schnatter, Sylvia, 1994, Data augmentation and dynamic linear models, Journal of Time
    Series Analysis 15, 183–202.
Goyal, Amit and Ivo Welch, 2003, Predicting the equity premium with dividend ratios, Manage-
   mente Science 49, 639–654.
Goyal, Amit and Ivo Welch, 2006, A Comprehensive look at the empirical performance of equity
   premium prediction, Review of Financial Studies, forthcoming.
Hamilton, James D., 1994, Time Series Analysis (Princeton University Press, Princeton, NJ).
Harvey, Andrew C., 1989, Forecasting, structural time series models and the Kalman filter (Cam-
   bridge University Press, Cambridge, UK).

                                               55
Hjalmarsson, Erik, 2006, Should we expect significant out-of-sample results when predicting stock
   returns?, Working paper, Board of Governors of the Federal Reserve System.
Jansson, Michael, and Marcelo J. Moreira, 2006, Optimal inference in regressions with nearly
   integrated regressors, Econometrica 74, 681–714.
Johannes, Michael, Nicholas Polson, and Jon Stroud, 2002, Sequential optimal portfolio perfor-
   mance: Market and volatility timing, Working paper, Columbia University.
Jones, Christopher S., and Jay Shanken, 2005, Mutual fund performance with learning across
   funds, Journal of Financial Economics 78, 507–552.
Kandel, Shmuel, and Robert F. Stambaugh, 1996, On the predictability of stock returns: An asset
   allocation perspective, Journal of Finance 51, 385–424.
Keim, Donald B., and Robert F. Stambaugh, 1986, Predicting returns in the stock and bond mar-
   kets, Journal of Financial Economics 17, 357–390.
Kothari, S.P., Jonathan Lewellen, and Jerold B. Warner, 2006, Stock returns, aggregate earnings
   surprises, and behavioral finance, Journal of Financial Economics 79, 537–568.
Lamont, Owen, 1998, Earnings and expected returns, Journal of Finance 53, 1563–1587.
Lamoureux, Christopher G., and Guofu Zhou, 1996, Temporary components of stock returns:
   What do the data tell us?, Review of Financial Studies 9, 1033–1059.
Lettau, Martin, and Sydney C. Ludvigson, 2001, Consumption, aggregate wealth, and expected
    stock returns, Journal of Finance 56, 815–849.
Lettau, Martin, and Sydney C. Ludvigson, 2005, Expected returns and dividend growth, Journal
    of Financial Economics 76, 583–626.
Lettau, Martin, and Stijn Van Nieuwerburgh, 2007, Reconciling the Return Predictability Evi-
    dence, Review of Financial Studies, forthcoming.
Leroy, Stephen F. and Richard D. Porter, 1981, The present-value relation: Tests based on implied
   variance bounds, Econometrica 49, 555–574
Lewellen, Jonathan, 1999, The time-series relations among expected return, risk, and book-to-
   market, Journal of Financial Economics 54, 5–43.
Lewellen, Jonathan, 2004, Predicting returns with financial ratios, Journal of Financial Economics
   74, 209–235.
Mankiw, N. Gregory, and Matthew D. Shapiro, 1986, Do we reject too often?: Small sample
  properties of tests of rational expectations models, Economics Letters 20, 139–145.
Menzly, Lior, Tano Santos, and Pietro Veronesi, 2004, Understanding predictability, Journal of
  Political Economy 112, 1–47.
Nelson, Charles R., Myung J. Kim, 1993, Predictable stock returns: the role of small sample bias,
   Journal of Finance 48, 641–661.
Pástor, Ľuboš, 2000, Portfolio selection and asset pricing models, Journal of Finance 55, 179–223.



                                                 56
Pástor, Ľuboš, Meenakshi Sinha, and Bhaskaran Swaminathan, 2008, Estimating the intertemporal
    risk-return tradeoff using the implied cost of capital, Journal of Finance, forthcoming.
Pástor, Ľuboš, and Robert F. Stambaugh, 1999, Costs of equity capital and model mispricing,
    Journal of Finance 54, 67–121.
Pástor, Ľuboš, and Robert F. Stambaugh, 2000, Comparing asset pricing models: an investment
    perspective, Journal of Financial Economics 56, 335–381.
Pástor, Ľuboš, and Robert F. Stambaugh, 2001, The equity premium and structural breaks, Journal
    of Finance 56, 1207–1239.
Pástor, Ľuboš, and Robert F. Stambaugh, 2002a, Mutual fund performance and seemingly unre-
    lated assets, Journal of Financial Economics 63, 315–349.
Pástor, Ľuboš, and Robert F. Stambaugh, 2002b, Investing in equity mutual funds, Journal of
    Financial Economics 63, 351–380.
Rapach, David E., Jack K. Strauss, and Guofu Zhou, 2007, Out-of-sample equity premium predic-
   tion: Consistently beating the historical average, Working paper, Washington University.
Rozeff, Michael S., 1984. Dividend yields are equity risk premiums, Journal of Portfolio Manage-
   ment 68–75.
Rytchkov, Oleg, 2007, Filtering out expected dividends and expected returns, Working paper, MIT.
Santos, Tano, and Pietro Veronesi, 2006, Labor income and predictable stock returns, Review of
   Financial Studies 19, 1–44.
Shiller, Robert J., 1981, Do stock prices move too much to be justified by subsequent changes in
    dividends?, American Economic Review 71, 421–436.
Stambaugh, Robert F., 1986, Bias in regressions with lagged stochastic regressors, Working paper,
   University of Chicago.
Stambaugh, Robert F., 1997, Analyzing investments whose histories differ in length, Journal of
   Financial Economics, 45, 285–331.
Stambaugh, Robert F., 1999, Predictive regressions, Journal of Financial Economics 54, 375–421.
Wachter, Jessica, and Missaka Warusawitharana, 2006, Predictable returns and asset allocation:
  Should a skeptical investor time the market?, Working paper, University of Pennsylvania.
West, Mike, and Jeff Harrison, 1997, Bayesian Forecasting and Dynamic Models (Springer-Verlag,
   New York, NY).
Zellner, Arnold, 1971, An Introduction to Bayesian Inference in Econometrics (John Wiley and
    Sons, New York, NY).




                                               57
