                                 NBER WORKING PAPER SERIES




                                CITATIONS IN ECONOMICS:
                             MEASUREMENT, USES AND IMPACTS

                                         Daniel S. Hamermesh

                                         Working Paper 21754
                                 http://www.nber.org/papers/w21754


                       NATIONAL BUREAU OF ECONOMIC RESEARCH
                                1050 Massachusetts Avenue
                                  Cambridge, MA 02138
                                     November 2015




I am indebted to V. Bhaskar, Marika Cabral, Donald Davis, Jeffrey Frankel, Brendan Kline, Paul Menchik,
Hugo Mialon, Andrew Oswald, Gerard Pfann, Ellis Tallman, Manuelita Ureta and participants in seminars
at several institutions for helpful suggestions. Frances Hamermesh greatly improved the exposition.
The views expressed herein are those of the author and do not necessarily reflect the views of the National
Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2015 by Daniel S. Hamermesh. All rights reserved. Short sections of text, not to exceed two paragraphs,
may be quoted without explicit permission provided that full credit, including © notice, is given to
the source.
Citations in Economics: Measurement, Uses and Impacts
Daniel S. Hamermesh
NBER Working Paper No. 21754
November 2015, Revised January 2016
JEL No. A11,A14,J31

                                             ABSTRACT

I describe and compare sources of data on citations in economics and the statistics that can be constructed
from them. Constructing data sets of the post-publication citation histories of articles published in
the “Top 5” journals in the 1970s and the 2000s, I examine distributions and life cycles of citations,
compare citation histories of articles in different sub-specialties in economics and present evidence
on the history and heterogeneity of those journals’ impacts and the marginal citation productivity of
additional coauthors. I use a new data set of the lifetime citation histories of over 1000 economists from
30 universities to rank economics departments by various measures and to demonstrate the importance
of intra- and inter-departmental heterogeneity in productivity. Throughout, the discussion summarizes
earlier work. I survey research on the impacts of citations on salaries and non-monetary rewards and
discuss how citations reflect judgments about research quality in economics.


Daniel S. Hamermesh
Department of Economics
Royal Holloway University of London
Egham, TW20 0EX
UNITED KINGDOM
and NBER
Daniel.Hamermesh@rhul.ac.uk
To be occasionally quoted is the only fame I care for. 1 [Smith, 1863, p. 144]

    I.       Introduction

         Why should one care about citations to scholarly articles in economics? Why might looking at

citations stem from anything more than narcissistic concerns or reflect behavior among an isolated group

of very competitive individuals? Two answers suggest themselves: 1) On a narrow level, within any

group of what are essentially independent contractors quality has to be judged. That is true in our business

for decisions about tenure and salary; it is also true for decisions about hiring researchers for positions

above the entry level. Are citations better indicators of the quality of a person’s scholarly work than

numbers of publications or the kinds of outlets where the research appeared? 2) More broadly, what rates

of citation indicate an impressive career in economics—say something about a person’s scholarly

achievements? One’s citations may be viewed as a reflection of what one has contributed to society in

that part of one’s job that is devoted to research.

         The first purposes of this essay are to illustrate the kinds of data on citations to economic research

that are available and to discuss their validity. Having done that, I examine what these data tell us about

the nature of citations to scholarly work in our field, looking at differences across sub-fields and journals,

both over time, and across individual scholars of different ages/cohorts. While I compare individuals and

institutions, to reduce the prurient interest in the topic no individuals are mentioned by name, although

institutions are. I then examine how citation analysis has been used to explain market outcomes, such as

remuneration and honors.

         How to measure citations—what sources are available for this, and how does one use them—is

covered in Section II? Given their availability, what kinds of statistics can be constructed based on these

sources? Does it matter what source(s) or statistics one uses to evaluate the quality of published research

or of the authors’ contributions?




1
 Although we used this as the epigraph in the first paper on citations in which I was involved (Hamermesh et al,
1982), it is so apropos, since it speaks to the impact of our research on others’, that it belongs up front again.

                                                        1
        Given these measurements, Section III discusses how citations vary across individuals, sub-fields

and journals. Who is a “superstar” scholar in terms of citations? How do citations vary among individuals

who differ by experience?      More junior people typically have fewer works to be cited (and less time

since they were published for them to have been cited), so that I obtain a metric by which to compare the

progress of, especially, younger scholars. Focusing still on individuals, I consider differences in citations

by gender and by the institutional affiliation of a scholar. The article proceeds to differences in the

measured quality of aggregates of economists—the departments of economics in the universities in which

they work. In addition to providing a series of possibly novel rankings of institutions, it also sheds light

on the extent of heterogeneity across institutions.

        Section III also considers differences by article, examining whether resurrection occurs for

previously ignored publications, or whether some articles are mere “flashes in the pan.” Focusing still on

articles, it considers the marginal productivity, in terms of scholarly impact, of having more authors on a

paper, thus estimating a short-run production function for quality. This information is useful for assessing

individual scholars’ contributions, an increasingly important issue in light of the now ubiquitous practice

of joint authorship in economics as compared to practices before 1990 (Hamermesh, 2013). It also

examines differences in citations by sub-field of economics, concentrating on methodologies rather than

on the decreasingly meaningful distinctions by area of concentration. It considers how differences in

citations to articles in different journals have varied over time and examines whether the widespread

focus on the “Top 5” journals is sensible. Finally, it points out how different citation practices are in

economics compared to other social sciences.

        Most of the published research on citations in economics has considered them as measures of

quality that might be useful in explaining various outcomes. Chief among these have been salaries

(compensation), but several studies have examined other outcomes that are arguably affected by

differences in the quality of scholarship. Section IV summarizes this body of research and also examines

how citations have been used to evaluate editors’ and committees’ decisions about publication and

awards. Section V concludes with a longer apologia for this kind of research and for the enhanced use of

                                                      2
citations in judging articles, individuals and institutions; and it offers suggestions for future directions of

research relating various outcomes to citations.

          Readers should keep in mind that citations in academic journals and books, although important

measures of the productivity of an economist’s research, are only partial indicators of its quality. One’s

research can, for better or worse, influence broad public attitudes and/or public policy, and it can affect

how businesses and individuals organize their activities and make decisions. Moreover, these effects may

be direct or indirect, as our research filters through others’ research and through the media. While these

additional and often inchoate impacts are likely to be highly correlated with one’s academic citations, I

doubt that the correlation is perfect.

    II.      Measuring Citations and Impacts

    A. Sources of Information on Citations

          Citation analysis in economics has come a long way since Stigler and Friedland (1975) used hand

counts through published articles to obtain measures of citations.2 Today the two most commonly used

online methods of acquiring citation counts to a scholarly work or to a person’s works are the Web of

Science (WoS), created by the Institute for Scientific Information, and Google Scholar (GS). 3 The WoS

http://apps.webofknowledge.com/ has three sub-categories, for articles in Science, Social Science (SSCI)

and Arts and Humanities, as well as two sub-categories for conference proceedings, in Science and in

Social Science and Humanities (CPI-SSH). Except for a very few economists whose main work has been

adopted in more technical fields, obtaining information from the SSCI and the CPI-SSH provides a

complete or almost-complete measure of a scholar’s impact. 4 The majority of the studies summarized

here use the WoS because they were written before GS became available.


2
 Their graduate students performed these tabulations. Given the possibilities for error discussed below when using
the online sources now available, I doubt the desirability of using such labor in analyzing citations today.
3
 Other, apparently less frequently employed (in economics) databases are Microsoft Academic Search and Elsevier
Scopus.
4
 The most extreme case I have encountered is an economist with a meager 407 citations in the WoS whose total
count rises to 6658 when the two science indexes are included.

                                                        3
         Citation counts in the WoS are based on references included in published articles. Each citation is

included under each author’s name, so that a multi-authored article or book will be listed as a citation for

each author separately. This was not true when the WoS was only available in print editions (before

2000), but citations to all past articles in the online version now reference all authors. The WoS can also

be used to obtain citations to a particular paper by selecting author, year and volume number on the Cited

Reference Search screen.

        The usual way of using the WoS works from the URL and screen shown in Figure 1, which are

specific to a particular author. Its basis is the cited articles themselves, so that only citations to published

articles are included in the count. Proceeding with this search generates lists of all of an author’s (journal)

publications in descending order of the number of times cited, with citations per year and in total for each,

along with additional information shown in Figure 1. 5

         The WoS cites people using the initials of their given names. This method is not a problem for a

name like Hamermesh, D (except for misspellings of the surname), but it is a difficulty for someone with

a common surname. This problem makes it hard to collect accurate citation counts for some individuals,

leading to measurement error of unknown magnitude. The problem can be reduced in a search by

excluding articles that are obviously classified far outside sub-fields in which an economist might have

written, but it is inherent in the WoS.

         Searching for citations using GS has the virtue that it is based on references to an article, working

paper or book on the internet. It thus has the advantage over the WoS of allowing citations to junior

scholars to be available earlier in their professional careers, which is especially important given the long

publication lags in economics that lead to lags in WoS citations. For those economists who have

established a GS profile, it provides information like that shown in Figure 2, plus a list of all the

referenced publications. If the title, as opposed to the outlet, of an article or book does not change

between avatars, all references to the piece are accumulated into one aggregate.

5
 A second method that is used less is based on citing articles and thus includes citations to unpublished working
papers, books and other non-journal publications. In this method an article may be listed several times, for example,
in its avatars as working papers disseminated by different organizations and in its published form.

                                                         4
         The main problem with using GS is that many researchers have not established user profiles, so

that for them it is extremely time-consuming to obtain a complete citation count. That difficulty can be

partly mitigated by searching for the person in GS, which lists publications arrayed in more or less

descending order of the number of citations obtained (and even lists the paper’s citations in the WoS).

Instead of counting citations to all the listed publications, counting those to the person’s five most-cited

papers may be a reasonable substitute. Another problem is that, if an article changes titles during its

progression from working paper to publication, it will be counted as separate pieces. Still another serious

but much rarer difficulty is that GS will occasionally attribute a paper by a scholar with an identical or

even similar name as the person of interest, a problem that, as in the WoS, arises most frequently for

scholars with frequently occurring surnames. 6

         The problem of identifying names correctly in the GS and the similar difficulty with the WoS

makes counting citations accurately by web-scraping or having a student who is unfamiliar with the

works of particular scholars problematic. Obtaining accurate citation counts is difficult even if one has a

feel for who might have written what. Taken together, all of these problems suggest that the most accurate

counts by either method will be done by hand and by someone familiar with the economics literature

(perhaps the researcher her/himself).

         Both the WoS and GS provide statistics that seek to aggregate an individual’s citations into one

number beyond the total count. The most commonly used of these is the h-statistic, proposed by Hirsch

(2005). 7 The h-statistic is calculated by arraying citations to all of a person’s works from the most- to the

least-cited, then defining h as in Figure 3 as the h-most-cited paper that has h citations. Clearly, this



6
 For example, the GS citation list for an economist with a quite common surname lists an article on “alcoholic
subtypes,” authored by a group of medical doctors, as his most-cited publication, accounting for nearly half of his
total citation count. This difficulty arises especially often in citation counts for Chinese or Korean scholars, given
the relative paucity of distinct surnames in those cultures.
7
 Hirsch proposed this statistic as a way of making achievements in various “hard-scientific” fields commensurate,
noting that it appeared that Nobel Prizes in these fields were typically awarded to researchers who had attained an h-
statistic of 35 in the WoS. An examination of WoS h-statistics among recent Nobel recipients in economics at the
time of their award shows huge variations, from below 30 to over 60.


                                                          5
statistic does not measure the mean, variance or any other moment. Instead, it combines both breadth and

depth of impact.

         As Figure 2 shows, profiles in GS also provide a statistic, i10, indicating the number of a

scholar’s papers that have been cited ten or more times. This measure provides a fairly low hurdle, so this

statistic can be viewed chiefly as a measure of breadth. Other statistics have been proposed for

aggregating and ranking citation records. The g-index, offered by Egghe (2006), computes the statistic g*

such that a scholar’s g most frequently cited publications have g2 total citations. Clearly, the g-index will

increase when a much-cited publication i ranked gi < g* receives an additional citation, while there will

be no change in the h-statistic when such a publication, ranked hi < h*, receives an additional citation.

Numerous variations on both of these statistics have been offered, including by Ellison (2013) for a

sample of economists. No doubt more can and will be proposed; but the h-statistic (in both the WoS and

the GS) and i10 (in GS for those scholars with profiles) are the only ones readily available, and the former

is the only one that has been widely used.8

    B. Comparing Methods and Constructing Data Sets

         Nobody wishes to spend time using both the WoS and GS to compare scholars or to construct

data sets to examine the impacts of citations on outcomes. Given the differences between them, how

much does it matter which source one uses? How much does it matter which statistic describing citation

counts one focuses on given one’s choice of the WoS or GS?

         To examine these and other questions I construct three new data sets based on the WoS and GS:

1) Citations to articles published in the “Top 5” journals—the American Economic Review,

Econometrica, the Journal of Political Economy, the Quarterly Journal of Economics, and the Review of

Economic Studies, referred to hereafter as the J5, in 1974-75. (Pieters and Baumgartner, 2002,

demonstrated that these were the top five economics journals in terms of generating citations from articles

outside the journal.) This data base includes all 444 articles that are at least five pages long in print; it

8
 Still other calculations and statistics describing individuals’ citations are possible using the software available at
http://www.harzing.com/pop.htm .


                                                          6
excludes papers (e.g., presidential and Nobel addresses) that were obviously not refereed or that were

clearly comments. 2) Citations to articles in the 2007 and 2008 volumes of the J5, with the same

exclusions but limited, given the increased logorrhea in articles in economics journals, to the 497

publications that are at least ten pages long in print. Citations were obtained from both the WoS and GS

in this data set. 3) Citations to all 1043 tenure-stream or tenured faculty members with primary

appointments in the economics departments at the top thirty U.S. economics faculties (ranked based on a

history of recent publications in Kalaitzidakis et al, 2003). 9 For all of these scholars I obtained

information on citations to their five most-cited papers from GS (C5 hereafter). For the 543 people for

whom they were available I also obtained entire GS profiles; and for some of those in the sample I

obtained total WoS citations. 10

         Clearly the journals that provide the articles used in constructing the first two samples are an elite

small fraction of all the scholarly outlets available to economists. In Section III.E. I do provide some

evidence on how moving further down the journal prestige hierarchy might affect the inferences made

here, but the main results apply only to research in the most visible outlets in the field. Similarly, results

based on the sample of economists from a set of 30 leading institutions will not describe the results one

might obtain at other, typically lower-ranked institutions. In Section III.D., however, I do make a brief

excursion to examining comparisons within a broader set of institutions.

         Examine first the differences that result from using the different methods by comparing WoS to

GS citations to J5 articles published in 2007-08. Taking all but the eleven most heavily (GS) cited papers

among the 497 in this sample (to exclude extreme outliers), Figure 4 shows the scatter of the relationship

between WoS citations and GS citations for the first six or seven years post-publication (through the end


9
 Faculty who were in what was clearly a “teaching track,” as indicated by titles such as “professor of the practice” or
“senior lecturer,” are excluded from this sample. Faculty members’ names and appointments were obtained from
each department’s website.
10
  The first two data sets contain all citations through December 2014. The third is based on citations cumulated
through early May 2015. The likelihood of having a profile is significantly greater among those who, at the same
post-Ph.D. experience, had more GS citations to their top five papers and among male economists. It rises
significantly as experience increases up through ten years and falls significantly thereafter.


                                                          7
of 2014). Clearly, the scatter lies quite close to the regression line (and would look even closer, and the

fit would be still tighter, if I included the eleven outliers). A good rule of thumb in extrapolating from GS

to WoS citations to an article is that the latter will be about one-fifth of the former.

        Since much of the argument for using GS rather than the WoS is the former’s inclusion of

citations to unpublished papers, I concentrate on differences between the two sources for more junior

faculty. Figure 5 presents a regression of WoS on GS citations based on 102 of the 109 faculty members

in the third data set who had GS profiles and who received their Ph.D. degrees after 2005 (again

excluding extreme positive outliers). As with the comparison of citations counts by articles, here too it

appears not to matter very much which method one uses (and again, were the 7 heavily-cited individuals

included, the fit would be even tighter). 11

         Consider the Spearman rank correlations shown in Table 1 for the 52 percent of scholars in the

third data set who had GS profiles. Examining pairwise correlations among h, i10, C5 and total citations,

the rankings are remarkably similar across metrics. Just as it does not matter much what source one uses,

it also does not matter much what statistic one uses to measure citations given the source. Since there is

no “right” source or “right” statistic, one should aim for breadth and ease of acquisition. With GS profiles

only available for barely half of the researchers at top economics departments, these considerations

suggest using C5 citations when comparing individuals or using citation data to predict outcomes. In the

end, however, for papers published in J5 journals, and for individuals located in top economics

departments, one’s choice of source and statistic is unlikely to greatly affect any conclusions or rankings.




11
 Restricting the sample still further to those who received their doctoral degree later than 2006, or even later than
2007 or 2008, does not qualitatively alter the slope of the regression nor the adjusted R2.

                                                         8
    III.      Differences in Citations across Individuals, Articles, Sub-Fields, Institutions and
              Journals

           In this Section I consider a wide range of issues focusing solely on the bibliometrics of

citations—on their variation across a wide range of indicators. A central fact runs through all these

comparisons: The distributions of citations measures are highly right-skewed. For that reason, throughout

this Section I present data describing the shapes of the distributions of citations, not merely measures of

central tendency, particularly means, which have been the main focus in the literature.

           A. Differences among Individuals

           Within the sample of 1043 economists from elite institutions there are tremendous differences in

the extent to which their works have been cited and have thus presumably influenced the work of other

scholars. As Table 2 shows, the mean citations to their five most-cited papers is about 3000; but the

median is less than 1100. The mean h-statistic (in the positively-selected subsample that has established

GS profiles) is 25, while the median is 17, suggesting less skewness in this summary measure than in the

C5 measure or, as the table shows, in the counts of total citations in this sub-sample.

           While the term “superstar” is widely and loosely applied, one might ask what constitutes a

“super-economist?” In this sample the few scholars with citations in the top 1 percent of the distributions

shown in Table 2, whose average C5=38,818 and h=101, might be viewed as true super-economists.

Clearly, these are the most influential people in this profession, at least as measured by the impact of their

research on that of other scholars.

           The citation measures in this sample reflect cumulative citations over a scholar’s publishing

lifetime. Other things equal, this generates a positive citations-age profile, so that the raw data described

in Table 2 tend to place more senior scholars higher in the distributions of the various measures. This

tendency may be offset in the GS measures by their inclusion of references to discussion/working papers,

series of which and their availability have proliferated with the creation and expansion of the internet. The

availability of online indexes may also have generated a positive trend in citations per article (McCabe




                                                      9
and Snyder, 2015). It is also more than offset in the WoS by the growth in the numbers of sources

included in that data set.12

         I examine the life cycle of citations (cross-section data, and thus not indicative of the life cycle of

citations of a particular cohort) by including for each individual in the sample his/her years of post-Ph.D.

experience (2015 – Ph.D. year), a. This information might be useful for comparing younger scholars who

are being considered for re-appointment or for tenure, and perhaps even for considering the achievements

of more senior scholars who might be appointed to new positions. Table 3 presents these citation-

experience profiles, aggregating more experienced individuals into broader categories. Perhaps most

noteworthy in these data is that the skewness demonstrated above in the aggregate persists as experience

increases and, indeed, remains roughly constant (comparing the ratios of means to medians).

         These are not longitudinal data, and following individuals’ citations through their careers is

extremely cumbersome. Some inkling about their time paths is easily obtained for those scholars with GS

profiles. Estimating the regression of GS citations from 2010 through May 2015 on lifetime GS citations

among scholars with at least 31 years of experience yields a coefficient of 0.39, with an adjusted R2 of

0.98. 13 A least-absolute deviations (LAD) regression yields the identical slope, although the pseudo-R2 is

“only” 0.85. Despite the skewness in citations in all the samples used, parameter estimates generally

differ very little between least-squares (OLS) and LAD regressions, a result that persists in all the

relationships estimated in this study.




12
  Without longitudinal data one cannot distinguish among trends in citations, individuals’ life-cycles of citations,
and differences in citations across cohorts (as in Borjas, 1985, describing immigrants’ earnings). Using an
unbalanced panel of WoS data describing citations from 1991-2013 to 111 high-achieving labor economists whose
Ph.D. degrees were received between 1955 and 2002 (clearly not a random sample), one finds that at the same age
(or cohort—we cannot distinguish between them) there was a near doubling of the number of WoS citations over the
22 years in the sample. Alternatively, examining the history of citations to articles published in the American
Economic Review, Margo (2011) finds roughly a quintupling of early post-publication citations to them in six elite
economics journals (J5 plus Review of Economics and Statistics) between 1960 and 2000.
13
  Lifetime total GS citations in this sample of 108 very senior scholars range from 593 to 214,046. Even with the
selection generated by less successful researchers leaving university jobs at earlier ages, the data still cover a very
broad range of scholarly recognition.


                                                          10
          One could use this information mutatis mutandis to compare scholars within and across cohorts.

Let Ci and Cj be the citation counts of scholars i and j, and let ai > aj be their (Ph.D.) ages. Then if F(a) is

the cross-section citations-age profile, under the assumption that F is independent of the (constant) secular

annual rate of growth of citations per article, θ, the appropriate comparison of i’s and j’s scholarly impact

is:

      (1) Ci - F(ai) – θ [ai-aj] to Cj - F(aj) . 14

One could extend this calculation, if one could determine average differences in citation practices across

sub-fields (see below), to compare scholars in different sub-fields in economics.

          The data set also allows one to examine whether there are gender differences in the extent to

which scholars are cited, a claim that was made and disputed in research from the 1980s (Ferber, 1988;

Laband, 1987). Finding a differential rate of citation to male and female authors would tell us nothing

about the presence of discrimination (just as demonstrating the existence of a wage differential by gender

cannot inform us whether it arises from discrimination or differences in productivity), unless one assumes

that published papers are inherently of equal quality across gender. I thus present an outcome, saying

nothing about causation.

          To examine these issues I expand LAD regressions that relate C5 to the vector of indicators of

experience by adding an indicator of gender (female). These equations are estimated separately for half

sub-samples arrayed by post-Ph.D. experience (with the median experience being 17 years). In the older

half-sample only 9 percent of the faculty are female, while in the younger half 19 percent are. The

estimates, shown in Columns (1) and (3) of Table 4, suggest that older female faculty have indeed been

cited less than their male contemporaries, but the difference is small and statistically insignificant; and the

difference is essentially zero among younger faculty. When we add the rank of the economist’s

department (1 being the department whose median-cited faculty member has the most citations), to

examine whether female economists’ citation totals differ from those of men in equal-quality


14
 I am indebted to Jeffrey Frankel for suggesting this exercise. If the two underlying assumptions are violated, a
more complex formula can be constructed fairly easily with some additional work.

                                                       11
departments, the results, shown in Columns (2) and (4) suggest little change in the conclusions. There

may have been gender discrimination in citation practices in economics in the past, but this evidence

suggests that it was small, and it is not apparent in the records of younger economists.

        Several studies (e.g., Eiran and Yariv, 2006) have argued that the position of one’s name in the

alphabet affects success in the economics profession, such as the attainment of a tenured position, perhaps

due to the practice of listing coauthors’ names alphabetically (Engers et al, 1999). This may have been

true before the growth of the internet, when hard-copy annual volumes of the WoS listed only first

authors, but it seems less likely to be true now that all authors are included in citation counts. To examine

whether there are alphabetical effects on citations, the names of the authors in the sample were

alphabetized and a variable ranging from 1 to 1043 was added to the LAD estimating equation. Moving

from the first to the median (in the alphabet) author reduces C5 GS citations in the more senior half-

sample by 1.8 percent (t = -0.55), and among their younger colleagues it reduces citations by 0.7 percent

(t = -0.22). Those whose surnames are late in the alphabet are cited less frequently than their “A”

colleagues, but the differences are slight, statistically unimportant and, as expected, smaller in percentage

terms among more junior scholars. 15

         Another difference that may be viewed as a kind of discrimination might be the existence of

bandwagon effects in citations: One paper among several similar ones is cited shortly after it appears, and

subsequent papers disproportionately cite it rather than equally important papers that appeared at the same

time—or even earlier. Partly this sort of herding may reflect what Merton (1968) called “the Matthew

Effect”—the attribution of credit to the better-known scholar of a pair or even group of scholars who have

done very similar work. 16 No doubt this kind of behavior occurs, and it is unfortunate (for the lesser-

known person), but its extent is completely unclear.


15
 OLS estimates of the impacts of being female or of place in the alphabet do not differ qualitatively from these
LAD estimates.
16
 I was the “victim” of the Matthew Effect when a visiting lecturer referred in his paper and presentation to “Gary
Becker’s important work on suicide.” I was not in the audience, but one of my colleagues pointed out that it was my
work, and that Becker had never published on the topic.

                                                        12
         One might worry that some citations are self-citations and do not reflect an article’s true scholarly

productivity. One study of articles in the AER and the Economic Journal found WoS self-citation rates,

which I denote as the “Ego Index,” approaching ten percent (Hudson, 2007). The extent of self-citation is

impossible to obtain from GS, but WoS citation totals are listed in total and without self-citations.

Rankings among individuals or articles are hardly altered when self-citations are excluded. Randomly

selecting 100 of the junior economists (Ph.D. year 2006 or later) in the sample of 1043 faculty, the rank

correlation of total citations and citations by others is 0.99, with an Ego Index having a mean of 0.07

(median of 0.05, range 0 to 1). Randomly selecting 100 of the senior (31+ years of experience), the rank

correlation of total citations and citations by others is 0.99, with an Ego Index having mean and median of

0.03 (range of 0 to 0.16). Moreover, additional self-citations do not generate more citations by other

scholars (Medoff, 2006). 17

         B. Differences among Articles

         Just as the distribution of citations across a group of scholars is highly right-skewed, so too is the

distribution of citations across articles, including those in the J5 journals. In the sample of 444 articles

published in these journals in 1974 and 1975, total SSCI citations from 1974 through 2014 ranged from 0

to 2466, with a mean (median) of 75 (22); and even the paper at the 95th percentile received “only” 360

citations over the forty post-publication years. Only five percent of these elite articles were uncited

(compared to 26 percent of articles in a much broader set of journals examined by Laband and Tollison,

2003). The extent of skewness is illustrated by the distribution graphed in Figure 6a, in which, to make

citations across journals commensurate, I have standardized articles by page size (using AER2008 pages)

and divided the citations of each article by its AER2008-equivalent pages.18 As the figure shows, most




17
  Frances Hamermesh noted that this article may be aimed at adding to my citation count (even though its Ego Index
is “only” 0.12). In the 2007-8 sample of articles used here one study cited 72 other works, of which 20 were by the
sole author of the paper (Ego Index of 0.28). A four-authored article in the sample had a collective Ego Index of
0.40.
18
  In this sample each additional AER2008-equivalent page generated an extra 7 citations over the 40 or 41 years after
the article’s publication.

                                                         13
papers, even in these prestigious outlets, are very rarely if ever cited, with relatively few articles

accounting for the overwhelming attention that scholars devote to these journals.

        Of course, we cannot be sure that such skewness will prevail over the future lifetimes of articles

published more recently; but taking articles published in these journals in 2007 and 2008, the distribution

of citations (through 2014) per AER2008-equivalent page, shown in Figure 6b, looks very much like the

earlier distribution. The statistics describing the distribution are also similar, with total SSCI citations

ranging from 0 to 478 and a mean (median) of 50 (36); and even the paper at the 95th percentile received

“only” 142 citations in its first seven post-publication years. Only one of these articles was uncited,

perhaps reflecting the growth of the literature, perhaps indicating an increase in the focus on J5 journals

by authors publishing elsewhere.

        The distribution of citations to more recent articles exhibits less skewness than that to the older

articles over their longer post-publication lifetimes. Has there been a secular broadening of interest in

papers published in these journals, or do articles that are little-cited early post-publication die off more

quickly than those that have become “superstar articles” in the eyes of future scholars? Taking the first

ten years of post-publication WoS citations to articles in the 1974-75 sample, the mean (median) citations

are 22 (11), with a range of 0 to 333, and “only” 82 citations at the 95th percentile. Like the distribution of

citations to 2007-08 articles, the distribution of initial (first-decade) citations to 1974-75 articles exhibits

less skewness than the distribution of citations over a much longer period. “Star” articles become

superstar articles, while “asteroid” articles become dust.

        This evidence suggests that there generally are few “flashes in the pan”—papers that attract much

attention initially and are then ignored—and few “resurrections”—papers that are ignored and come to

life much later—among scholarly articles in economics. Perhaps, however, there are at least a few

exceptions—perhaps there is hope for articles that were once ignored, and perhaps some that receive

substantial early attention are soon ignored. Table 5 examines citations in the post-publication years 21-

40 of the 1974-75 articles in comparison to their ranks in citation counts in their first ten post-publication

years. None of the twenty percent of initially least-cited papers is among the top twenty percent in

                                                      14
citations in the most recent ten years; and most remain ignored. Obversely, while a few of the papers that

are in the top fifth of citations in the first ten post-publication years are eventually ignored, most remain

well-cited even in post-publication “old age.” There are very few flashes in the pan or resurrections, even

among articles published in the most visible outlets.

        The mean citation in this sample (among the 423 articles that received at least one citation)

occurred 14 years after the article’s publication. Consistent with the evidence above, however, the mean

age of citations to the 40 percent of articles with the fewest (nonzero) citations was 11 years, while that to

the 17 percent with the most citations (at least 100) was 20 years. The most-cited papers “last longer”

than others. 19 One’s publications do not last as long, however, if one is no longer active: Aizenman and

Kletzer (2011) provide stark evidence of this, showing that the citation paths of articles by very

productive economists who died young drop below those to contemporaneous articles written by

otherwise identical contemporaries who survived them. And explicit citations to the work even of

historical superstars eventually diminish (Anderson et al, 1989); their work is presumably either ignored

or becomes part of the background that today’s scholars take for granted.

        As I have shown (Hamermesh, 2013), scholarly research in economics is increasingly

characterized by joint production. Indeed, the entire distribution of the number of authors listed on an

article in economics has been moving steadily rightward. An important issue in evaluating people’s

records for appointment or promotion is how to account for multiple authorship. Even ignoring the

possibility that one’s coauthors are surely more likely to cite their own works, which will increase one’s

citation count the more coauthors are listed on an article, does having more authors on a paper add to its

scholarly impact? What should be the divisor of citations when evaluating an article, with extremes being

1 and N, the number of authors?

        In the very long run the answer to this question should be: Divide by N, since eventually the full

impact of an article has become known and credit should be divided equally. This rule is of little practical

19
  This evidence is related to that of Quandt (1976), who examined the mean age of references in articles in the J5
(except the Review of Economic Studies) plus the Economic Journal, Economica, the Review of Economics and
Statistics, and the Southern Economic Journal).

                                                       15
help, since the evaluation of research proceeds in the very short-run. One can at least get a hint of what

the long-run production function might look like by considering citations that have occurred in various

intermediate runs. Think of the production function of the quality of article i as:

(2)      Qit = Q(Nit, Tt),

where Q can be proxied by citations, N is the number of authors, and T is the technology of producing

research of a given quality at time t. This relationship has been examined in several studies using specific

samples, but not comparing publications in two widely-separated time periods, and not concentrating on

J5 publications. We could estimate regressions describing QN and even infer something about QNT, thus

whether the expansion path of Q in N is homothetic in changes in technology. The size of QN and the sign

of QNT can, however, be inferred simply from an examination of the means and centile points of the

distributions of WoS citations as functions of the number of authors listed on the paper.

         Table 6 presents these statistics. There is no significant difference at any point of the distribution

of lifetime citations between the one-fourth of papers that were multi-authored in the early period and

those of single-authored papers. In the more recent period, however, multi-authored papers did receive

more citations in their first seven post-publication years, a difference that is visible across the

distributions of citations. But going from one to four or more authors roughly only doubles the number of

citations at each point of the distribution—it does not quadruple it. Moreover, since additional authors

lengthen papers in these samples, if we use citations per page the differences by number of authors are

even smaller. 20 Taken together, the results suggest that today QN >0 and QNN <0, and indicate that QNT>0.

These findings are consistent with those of Hollis (2001), who measured production by the impact factors

of the journals where the articles appear, and Medoff (2003), who used citations to individual articles

published in 1990. Having more coauthors on one’s papers may induce additional citations to one’s other

works, partly since coauthors become familiar with papers that they may feel obligated to cite (Bosquet


20
  The results tabulated in the upper part of Table 6 do not qualitatively change if we restrict citations to the first
seven post-publication years, thus making them comparable to those in the later sample. They also do not change
very much if we use GS rather than WoS citations to describe the production function in the later sample.


                                                         16
and Combes, 2013), but the effect of additional authors on a paper’s citations is clearly not proportionate

to the number of authors.

        Given the impossibility of evaluating the long-term impact of an article and dividing by the

number of authors N, a good rule of thumb suggested by the results in the bottom part of Table 6 is that

candidates’ citations—their influence on other scholars—might well be divided by N/2 for N>1. At the

very least, one should discount citations to multi-authored articles by some factor. 21

         Articles with the same number of co-authors differ in the extent to which interactions

between/among the coauthors is possible. Some coauthors might have offices next door to each other,

others might be halfway around the world from each other. At a time when the costs of co-authoring with

someone further away have been decreased by lower prices of air travel and especially by electronic

communication, one might wonder how the productivity, in terms of citations, of distant versus nearby

co-authorships differs? Hamermesh and Oster (2002) show that distant co-authorships increased to over

half of all co-authorships between the 1970s and 1990s, a time when the fraction of co-authored works

nearly doubled. Holding constant the journal in which a paper was published, citations in the first four

post-publication years were significantly fewer to papers with co-authors who were physically distant

from each other, a difference that was even greater in the 1990s when distant communication was

presumably easier than in the 1970s.

         C. Differences among Sub-Fields

         For over 50 years the JEL and its predecessor, the Index of Economic Abstracts, have maintained

a (changing) coding system for classifying journal articles. At a time when macroeconomic research is

increasingly micro-based, and when applied work in such formerly diverse fields as economic

development, labor economics, public finance, and perhaps others seems increasingly characterized by

similar approaches, it may make sense for purposes of analysis to use a methodology- rather than a


21
  There is also some evidence that having additional authors does not help get papers into more prestigious journals.
The mean number of authors of articles in the later J5 sample was 2.01. The mean number of authors of articles
published in 2007-08 in a pair of journals that might be viewed as the next rung down the journal prestige hierarchy,
the Economic Journal and the Review of Economics and Statistics, was 1.97, not significantly less.

                                                         17
subject-based classification of published research. This approach currently requires hands-on examination

of each paper rather than an easy reliance on author-provided codes, but it may be more appropriate given

the changes that have occurred in the field over time. (See Cherrier, 2015, for a discussion of the

difficulties with the JEL coding system.)

          I thus classify the articles in the 2007-08 data set into the following six categories: Theory; theory

with simulation (including the substantial part of recent macroeconomic research that uses calibration

methods); empirical with borrowed data; empirical with own data; experimental; and econometric

theory. 22 Because no experimental studies are included in the 1974-75 data set, and very little empirical

research was based on data sets that the author(s) had created, for the earlier sample I divide publications

into those in theory (including theory articles that also involved simulations), empirical and econometric

theory.

          Statistics in the upper two panels of Table 7 describing the two data sets based on J5 articles show

that in their first seven post-publication years empirical and experimental studies generated more citations

than did articles in economic theory or econometric theory in these journals. This is true for the average

paper, true at the medians and even true at the 90th percentile. The only caveat to the conclusion that

empirical research appears to generate more subsequent attention than theoretical work is at the upper

extreme of citations: The most heavily-cited study in the earlier sample was in economic theory. 23

          One might argue that this comparison ignores the long-lasting effect of theoretical work (a point

similar to that made by Chiappori and Levitt, 2003), and that a fair examination of the attention paid to

articles based on different methodologies would cover a longer period of time. To examine this

hypothesis for the earlier data set, thus allowing forty years of post-publication citations to accumulate,

modifies the conclusion based on the first seven post-publication years of citations only slightly. At the



 Excluding econometric theory, because it did not include Econometrica, the same classification was used by
22

Hamermesh (2013).
23
 OLS and LAD estimates that include indicators for the journal where the article was published yield inferences
very much like those reflected in the statistics in this table.


                                                       18
median and 90th percentiles of the distributions, empirical articles dominate articles in the other

categories; but because four theory articles are the most cited in this sample, there is no significant

difference at the mean between lifetime citations to articles in economic theory and those in empirical

economics. Since in the later sample the upper tail of the distribution of citations to empirical studies was

denser than that to theoretical studies, there may be some doubt whether the long-run impact of theory

articles, as measured by citations, will be as relatively large now as in the past.24

         Yet another possibility is that there are differences across sub-fields in the amount of citing,

which, if citations are disproportionately to other research in the same sub-field, could generate the results

shown in Table 7. To examine this possibility I counted the number of items cited (excluding data

sources) in the sample of articles from 2007-08. Across the six sub-fields listed in the table, the median

numbers of cited items per article were: Theory, 32; theory with simulation, 37; borrowed-data empirical,

39; own-data empirical, 35; experimental, 33, and econometric theory, 38. These differences seem fairly

small, although taken at face value they do reduce the inter-field differences shown in the Table. The

extent to which citations are concentrated within sub-fields is, however, unclear, as many of the theory

papers motivate themselves by references to empirical findings, and many of the empirical studies cite

theoretical work. 25

         Even though the quantities of citations do not appear to differ greatly across sub-fields, ideally in

comparing individuals one would like to account for differences in citation practices across the sub-fields

in which they work. The difficulty, of course, is that many economists work in several sub-fields, whether

classified by major JEL category or by the six-fold classification used here. Perhaps the best conclusion is




24
  Citations to even the most heavily-cited article in econometric theory in the later sample were surpassed by those
to seven empirical articles, suggesting that the relative long-run impact of recent empirical studies and those in
econometric theory will not differ from their short-run impact.
25
  Still another possibility is that, despite the general evidence of the similarity of patterns of WoS and GS citations
shown in Figures 4 and 5, the relationships between the two measures differ across sub-fields. Holding constant
indicators of the journals where the articles in the 2007-08 sample appeared, the only significant differences across
sub-fields are that both experimental work and econometric theory receive more relatively more WoS than GS
citations compared to those received by articles in the other four sub-fields.

                                                          19
that this consideration should be an important caveat in any comparisons of individual scholars’

productivity.




                                              20
        D. Differences Among Institutions

        Ranking economics departments by quality is a long-standing enterprise in our profession. See,

e.g., Cartter (1966), Roose and Anderson (1970) and National Research Council (1995), based on surveys

of impressions; Gerrity and McKenzie (1978), perhaps the first article to use citations in economics, and

Davis and Papanek (1984) more broadly, based on average citations; Scott and Mitias (1996), based on

pages published in selected sets of journals; Dusansky and Vernon (1998), based upon impact factors of

the journals in which faculty members published; Mixon and Upadhyaya (2012), based on the awards

faculty received; and Ellison (2013), based on average h-statistics and other transformations of citation

counts. I produce yet another such prurient ranking, using the data on the 1043 scholars in the 30

economics departments. The novelty here is several-fold: 1) I examine differences in rankings of

institutions based on comparisons at various quantiles of the distributions of their faculties’ citations

(rather than at the means); 2) I measure the extent to which the quality of faculties, as indicated by

citations, overlaps across departments; and 3) I adjust for inter-departmental differences in the

distributions of seniority.

        Authors of any study that compares departments by citation counts must decide on several issues:

1) Use the count of citations to articles/books ever published by the current staff, or use citation counts

based on authors’ affiliations at the time of publication? The former approach advantages institutions that

recruit much of their staff to senior positions, the latter helps schools that might be viewed as “training

grounds.” 2) Use lifetime citations, or recent citations? 3) Use order statistics or averages of citations? 4)

Use individual-based statistics, or use departmental totals (thus advantaging larger faculties)? There is no

correct answer to these questions, as they depend on one’s view of what determines the scholarly

reputation of an institution. In what follows I choose the first answer to each question. I do provide

enough information, however, that one can, more or less, construct rankings based on the alternatives to

the third and fourth choices.

        I first calculate rd50, the rank of the department by the citations of the median-cited faculty

member, C550. I then also calculate rd50 - x*, the highest-ranked department in which, if they were

                                                     21
transferred there, the top quartile of faculty members (ranked by Ci) in Department d would be above the

median faculty member’s citation count in that department.

         The results of these calculations are presented in Table 8.26 The first column lists the number of

faculty members included in each department, while the second shows the department’s ranking, rd50 and

the third presents C550. 27 The median citations in the top-ranked department are an extreme outlier from

the rest of the sample; beyond that, the differences in C550 among departments are much smaller over the

broad range from the 5th through the 14th-ranked department. 28

         The fourth column shows the rankings in the study from which this sample of 30 departments was

taken (KMS, 2003). Even though rd50 is based on lifetime citations to articles ever published by the

median-cited person in an economics department in 2014-15, while KMS (2003) used journals’ impact

factors and total impact-factor weighted publications of articles published in 1995-99, the correlation

between these rankings that differ greatly in methodology and timing is 0.70.29

         The apparent inter-institutional diversity in quality, however, does not reflect the tremendous

intra-institutional diversity that underlies these statistics. The calculations of x* in the fifth column show

that the scholars in the top quartiles of schools in all but three of the departments ranked 11 through 20

are more heavily-cited than the median scholar in at least the tenth-ranked department; and those in the

top quartile of economists in schools ranked 6 through 10 are more heavily-cited than the median scholar

26
  The rankings in Kalaitzidakis et al are necessarily higher in larger departments; and, as they include all publishers
from an institution, not only those in an economics department, an institution ranked more highly if it contained
more people outside an economics department who published in economics journals.
27
  The rank correlation between the medians measure shown in the Table and the mean citations across departments
is 0.87. Listing the ranks of schools by mean citations in the same order as in the Table yields: 1, 3, 4, 6, 2, 5, 8, 7,
19, 9, 11, 21, 16, 12, 10, 18, 15, 17, 20, 26, 22, 28, 14, 25, 13, 24, 27, 29, 23, 30.
28
  In a preliminary version of this paper I included an additional 9 departments, chosen as roughly every sixth U.S.
institution between those ranked 31st and 88th by KMS. Calculating rd50 for these schools placed even the highest-
ranked among them only 21st among the schools listed in Table 8. Three of the 9 were below the school ranked 30th
in the Table.
29
  With one exception, the replacement of Yale by NYU, the top seven schools in this ranking are the same seven
viewed as distinguished in the Cartter (1966) ranking, which was based on a survey fielded in 1964. Cartter’s
compendium sought to be a nationally representative set of opinions, following on earlier studies that were viewed
as parochial (Hughes, 1925, and Keniston, 1959).


                                                           22
in the school ranked second. The top people in lower-ranked institutions (in this elite group) would, if the

correlation of departmental and individual rankings were one, be placed far higher than their current

employment indicates. The overlap is consistent with evidence from Kim et al (2009) of a decline in the

effect of one’s academic affiliation—the quality of one’s colleagues—on one’s scholarly productivity,

although obviously the Table does not provide longitudinal evidence.

        The most productive (top quartile) economists in a large number of institutions would raise the

median quality of the faculties in departments ranked far above theirs. This overlap suggests that newly-

minted Ph.D.s might be a bit less concerned about the average quality of departments in which they are

seeking or being offered employment. The same comment applies to university graduates applying to

Ph.D. programs: There are large numbers of high-quality potential mentors at many institutions.

        One might argue that faculties should be judged by the achievements of their stars, not by those

of their median faculty members. While the Table demonstrates the extent of overlaps in quality,

alternative rankings of departments based on single statistics measuring faculty quality in the upper tail do

not hugely change one’s perception of distinctions among departments. The sixth column in Table 8 ranks

departments by the citations of the faculty members at the 90th percentile of this measure within

departments. Even using this extreme ranking 7 of the top 10 departments ranked by median quality

remain in the top 10, and 16 of the top 20 ranked by median quality remain in the top 20. The correlation

of the two rankings is 0.68 (rising to 0.78 if the two Los Angeles–area outliers are deleted).

        As suggested by Table 3, those institutions with more junior faculty will exhibit lower average

and median citations. To account for such differences I generate predicted median citations for each

individual i, C*i, by estimating a LAD regression describing each scholar’s citations Ci as a function of the

large vector of indicators of post-Ph.D. experience, a, shown in Table 3. I then use these predictions to

adjust the citations of each faculty member in department d for experience by taking the residual Ci - C*i,

and then rank departments by the median of this residual to obtain r*d50. The results of this re-ranking are

presented in the final column in Table 8. The correlation of this measure, r*d50, with rd50 is 0.78. While the

adjustment matters, except for a few institutions (those with unusually junior or senior faculty), it does

                                                     23
not change the rankings very much (with this re-ranking particularly advantaging Cornell and UT-Austin,

and disadvantaging Minnesota).

         The sample was constructed using faculty members located in each university’s economics

department, so that the rankings necessarily ignore the facts that some institutions have large and often

highly productive groups of economists located in other units on campus and that the importance of this

phenomenon differs across institutions. Since it is impossible to identify all units where economists might

be located in each of these institutions, I leave it to readers to make their own mental adjustments of these

rankings to account for these inter-institutional differences.

         This discussion has concentrated on the top-ranked American economics faculties, perhaps

appropriately given the concentration of articles in leading journals written by U.S.-based scholars

(Hodgson and Rothman, 1999), a dominance that citation analysis shows is not diminishing (Drèze and

Estevan, 2007). Given the increasing flows of economists to and from Europe (and to a much lesser

extent, to and from Asia and Australia), however, one might wonder where the top-ranked schools in

other English-speaking countries would fit into the rankings presented in Table 8. To characterize this

issue simply, I examined median C5 citations of faculty members in the institution in the United Kingdom

that received the highest ranking in the 2013 Research Excellence Framework (REF), University College

London (UCL), computing the same measures for it that underlay the rankings in Table 8. By this

measure UCL would have ranked twenty-first in Table 8; and, as with the faculties in the U.S., its faculty

members are very diverse, so that x*=14. A similar exercise for what is perhaps the leading economics

faculty in Canada, that of the University of British Columbia, would have it ranked 26th in Table 8, with

x*=16.

         These and the many other institutional rankings concentrate on predominantly research-oriented

institutions, those offering advanced degrees. Yet faculty at liberal arts colleges are expected to do

research too. In a study including 51 schools and 439 faculty members, Bodenhorn (2003) showed sharp

differences among institutions, with a citation-based count (most recent ten years of citations in the WoS)

ranking colleges generally according with perceptions of their quality. While the statistics are difficult to

                                                      24
compare, my best estimate is that eight full professors in these colleges received citations above the

median full professor in Hamermesh’s (1989) sample of full professors in six large public research

universities’ economics departments.

            E. Differences among Journals

            The history of citations to articles in the J5 is suggested by Fourcade et al (2015, Figure 4),

showing that the AER and QJE have increased in relative importance since the early 1980s, while the JPE

and Econometrica have decreased in relative importance (measured as shares of total citations to

publications in the journal). 30 This history, while very useful, does not consider differences across

journals in the number of issues, articles per issue and length of articles. To account for these differences

in the samples of articles in the J5 in 1974-75 and 2007-08, I calculate each journal’s citations per

AER2008-equivalent page, thus focusing not on the overall contribution of each journal, but instead on

the attention paid to each published paper adjusted for its length. The resulting statistics might be viewed

as article-based long-term impact factors. 31

            Mean citations per AER2008-equivalent page to articles published in 1974-75 in the five journals

are: AER, 7.0; Econometrica, 4.7; JPE, 9.4; QJE, 3.4; and REStud, 3.6. The mean citations per equivalent

page in the 2007-08 sample are: AER, 2.3; Econometrica, 2.1; JPE, 2.0; QJE, 2.7; and REStud, 1.1. 32

These calculations reinforce the inference that the QJE has increased in influence over this period, while

the influence of articles published in the JPE and the REStud has decreased. Indeed, given that the

REStud is such an outlier, perhaps it makes sense for economists to begin referring to the “Top 4”

journals and restrict the elite group in our discussions.




30
     See Laband (2013) for a discussion and references on rankings of journals in economics.
31
     Looking at different points in the distributions of these statistics yields the same inferences.
32
  Since the averages are based on citations through 2014, the citation counts for the earlier sample exceed those for
the later sample. The opposite would be true if we only use the first seven years of citations to articles in the earlier
sample, but the relative ranking of the journals in that earlier period would remain unchanged.


                                                               25
            More important, however, we devote excessive attention, and excessive rewards, to publications

in the J5. Inspired by Oswald (2007), consider articles published in the Economic Journal and the Review

of Economics and Statistics in 2007-08, the same years as the later sample of J5, and using the same

selection criteria (10+ pages, all in regular issues of journals, no comments, citations through calendar

2014). 33 Obtaining the WoS citations to each of the 230 articles in this sample, I assigned each its

percentile position in the distribution of J5 citations per AER2008-page equivalent. While the citations per

page of the median-cited article in these journals ranks “only” at the 29th percentile of articles in the J5

(average citations per AER2008-equivalent page of 1.27), as Figure 7 shows there is substantial overlap

with citations to J5 publications: Many articles in these two “lesser” general journals are cited more than

the majority of articles published in the J5, a result that holds for scholarly journals more generally (Stern,

2013). 34

            Just as there is substantial overlap in quality across economics faculties, so too there is substantial

overlap in scholarly influence across journals. The main reason that a few economics journals are ranked

more highly than all others is that a very few papers in these journals generate immensely more citations

than other papers published in those journals or elsewhere. A very few outliers determine our perceptions

of journal quality, a perception that ignores the tremendous heterogeneity of articles within and across

journals. 35 (See also Verma, 2015.)

            The WoS publishes “journal impact factors,” measuring for each journal in Year t the average

citations per article published in Years t-1 and t-2 (although five-year impact factors, counting citations to

articles published in Years t-5 through t-1 are also presented). Note first that impact factors ignore the


 Oswald (2007) compared articles in one issue each of the Economic Journal, Journal of Public Economics,
33

Journal of Industrial Economics, and the Oxford Bulletin of Economics and Statistics, to citations to articles in one
contemporaneous issue each of Econometrica and the American Economic Review.
34
  As with the comparison to the J5, there are substantial differences between these two journals. The median article
in the EJ would be at the 23rd percentile of articles in the J5, while the median article in the REStat would be at the
38th percentile. But one-fourth of the EJ articles are cited more per page than the median REStat article.
35
 Nonetheless, rankings of a large number of journals based on average citations per published paper are very highly
correlated with subjective views of their relative quality in a large sample of economists worldwide (Axarloglu and
Theoharakis, 2003).

                                                          26
heterogeneity in citations to individual articles across journals demonstrated above. Also, as Palacios-

Huerta and Volij (2004) point out, ranking systems based on impact factors lack any basis in economic

theory and are just one of very many ways of ranking journals’ influence.

          The very short rear-view of the two-year calculation of impact factors severely disadvantages

articles in economics compared to the “hard” sciences, given the extremely long lags between acceptance

of an economics article and its publication. Indeed, even comparing within the social sciences this short

rear-view and economists’ stinginess with citations disadvantages economics: The AER’s two-year impact

factor was 3.3 in 2013, while those of the American Political Science Review and the American Journal of

Sociology were 3.8 and 4.3 respectively. As broad indicators within a discipline, journal impact factors

are useful, but they are of little use comparing across disciplines. Most important, they ignore the

heterogeneity of the influence of articles published in the same journal.

    IV.       Citations, Outcomes and Behavior

    A. Salaries

          A large literature has arisen examining the determinants of economists’ salaries, both in terms of

the usual human capital inputs and direct measures of production (articles and books). Early examples of

this genre are Johnson and Stafford (1974), Tuckman and Leahey (1975) and Hansen et al (1978). My

purpose in this sub-section is not to summarize this literature; instead, I focus on the much smaller part

that has examined how citations affect salaries and whether their effects are independent of the role of

numbers and quality of publications per se in determining salaries.

          Examining the role of citations in determining academic salaries is interesting for several reasons.

First, since the purpose of scholarship is to generate knowledge, and generated knowledge is reflected in

citations, examining the impact of citations on salaries provides information on whether monetary

incentives are in line with what we implicitly believe should be rewarded. Second, while other criteria

should and will help determine salaries, a demonstration that citations as a measure of productivity are an

important independent determinant might reassure scholars that objective reflections of their intellectual



                                                      27
activities matter and that some notion of equity prevails in salary determination. Beyond these notions of

equity and efficiency, simple navel-gazing may justify looking at this issue.

        Throughout the discussion I restrict this short survey to studies that have considered the role of

citations in affecting salaries. Estimating equations describing wage determination has been the stock-in-

trade of labor economists for nearly a half century, with the basic equation including human capital

measures—experience and educational attainment. All the studies discussed here include quadratics in

(post-Ph.D.) experience, and all ignore educational attainment, since nearly all sample members have

Ph.D. degrees. In all cases the dependent variable was the logarithm of nine-month salary.

        There is a certain probably unavoidable homogeneity and narrowness in the citations-salaries

literature. Most important, because information on individuals’ remuneration in private universities and

colleges is not publicly available, all of the studies are based on economics faculties in public institutions.

At the very least, results cannot be extrapolated to salary-setting outside the public sector. Moreover, the

institutions studied are typically among the larger and more prestigious state schools, so that the results

should not be extrapolated to salary determination in less well-known public institutions. Also, and no

doubt because of the difficulties in obtaining data in the pre-internet age, only the most recent studies

have fairly large samples of institutions and faculty members.

        All studies attempt to measure the impacts of citations only on university salaries, usually only

academic-year salaries. (Twelve-month salaries of low-level administrators are typically de-annualized

for estimation.) Thus outside professional income—e.g., from research grants, consulting, or book

royalties—is not included. To the extent that those who are more heavily cited obtain more than

proportionate additional income from these sources, which seems likely, the estimated impacts of

citations on salaries may understate their impacts on total professional income.

        All of the studies summarized here use WoS (typically only SSCI) citation counts (since GS

counts were not available at the time when most were conducted). Given the comparisons in Section II,

the reliance on the WoS instead of GS probably causes little bias. In both, however, as pointed out above,

the difficulty in counting citations to frequently-appearing surnames may produce problems. Including

                                                      28
these scholars in a study could introduce the errors into the estimated impacts of citations, while

excluding them, as a few studies do, may have generated biased estimates of the nonlinear returns to

citations if their citations depart from the average.

        Within the WoS in these studies researchers must choose the length of the individuals’ citation

history to include in the estimation. For more recent research the choice is almost surely dictated by the

much easier accessibility of (professional) lifetime than of annual citation counts. The opposite is true for

pre-internet studies, when counting citations in annual hard-copy compendia made obtaining lifetime

counts much more difficult. Thus it is not surprising that earlier studies used a few recent years of citation

counts, while more recent studies used lifetime totals.

        Table 9 presents in chronological order a summary of the ten studies of U.S. institutions that

include citations as determinants of economists’ salaries. Each holds constant a set of covariates that are

listed in the table, and for each study I indicate in the last column its novelty in the literature. Most

important, in the penultimate column in the table I summarize the impact of a ten-percent increase in the

citation measure on the average sample member’s academic-year salary. The most important result

answers the question whether citations matter in salary determination. Even in the first study of citations

and salaries in economics (Hamermesh et al, 1982, which built upon Holtmann and Bayer, 1970, for the

physical sciences), citations had a substantial positive impact on salaries at the same post-Ph.D.

experience. A two-standard deviation in annual citations in that sample implied a nearly forty percent

difference in salaries. While the estimates obviously differ across studies, within those differences they

seem remarkably consistent, suggesting unequivocally that, through whatever mechanism, additional

citations do increase salary.

        Do citations increase salaries themselves, or is it simply that publication in “higher-quality”

journals is rewarded and is positively correlated with citations? Some of the studies in Table 9, beginning

with Sauer (1988), included covariates indicating the number, length and journal level of each scholar’s

publications. Even in those cases, as the Sauer, Moore et al (1998), Bratsberg et al (2010) and Hilmer et

al (2015) studies suggest, the evidence shows that citations have an independent impact on salaries.

                                                        29
            Are all citations created equal, or does being cited in higher-quality journals (presumably by

articles that themselves receive more scholarly attention) have a greater impact on salary? Kenny and

Studley (1995) adjusted each citation in their counts by the impact factor of the journal where the citing

article appeared. While they did not report effects of raw citation counts on salaries, the fact that the

implied impact of their adjusted citation measure differed little from others shown in the table suggests

that the locus of the citation is unimportant. 36 Moreover, a search over various values of discount rates

implied that no discounting of citations exists (at least in salary-setting). Taken together, the results

suggest that, in terms of impacts on salaries, a citation is a citation. Nor does having one’s citations

concentrated on a few “home runs” have an extra impact on salary (Hamermesh and Pfann, 2012). While,

as several of the studies clearly indicate, the quality of the journal where an article appears affects

salaries, neither the locations of an article’s citations nor the relative concentration of an author’s citations

appears to matter in salary determination.

            Books are not the major vehicle of scholarly communication in economics, but citations to them

are included in GS totals. 37 No study has separated out citations to books from those to articles, so the

question of their role in salary-setting has not yet been answered. Indeed, the evidence on whether the

publication of books itself even increases the academic salaries of economists is unclear.

            Does it matter when one’s work is cited? Siow (1991) shows that the impact on lifetime salary is

greater for citations received earlier in one’s career.          Given the evidence in Section III that most

publications either “make a hit” or disappear from view, this suggests that earlier publication generates

greater lifetime income. Whether this unsurprising result results from the attention that early-publishing

scholars achieve relative to their equally successful colleagues who publish later in their careers, or

because academic salaries seem to be capped by social (within-university) custom, is unclear. Do

incremental citations lead to incremental increases in salary, or is it merely that better scholars generally


36
  A similar adjustment by Bodenhorn (2003) hardly altered the citation-based ranking of economics departments in
liberal-arts colleges.
37
     A book was the most-cited work of two of the five most-cited economists in the sample of 1043 faculty members.

                                                          30
produce more citations and receive higher salaries? Hamermesh (1989) used longitudinal data to estimate

models with person fixed effects and showed that much of the cross-section impact of citations on salaries

remains.

        One might think that the market allocates talent so that the quality of scholars is lexicographic in

school quality, and that the top schools are richer and pay higher salaries generally. (Of course, this

implicit assumption about the allocation of academic talent is sharply contradicted by the evidence on x*

in Table 8.) Several of the tabulated studies include institution fixed effects, so that any estimated impact

on salaries is within-faculty. The estimates clearly show that, even within an economics department,

being cited more raises one’s salary relative to those of one’s otherwise observationally identical

colleagues, other things equal.

        Currently the citation count of each author of a co-authored article is increased when their article

is cited. This practice and the apparent impact of citations on salaries leads to the question of whether

citations affect salaries regardless of the number of authors of the cited article. The first article to examine

this issue (Sauer, 1988) suggested that the returns to citations to a two-authored article were almost

exactly half of those to citations to a solo-authored paper. (This result is consistent with the finding in

Section III that in the 1970s the marginal productivity of an additional author was about zero.) Recently

Hilmer et al (2015) have examined the same issue, with results suggesting that citation counts are not

discounted for co-authorship at all, which suggests market behavior that is inconsistent with the evidence

in Section III on the marginal productivity of an additional author of articles published in 2007-08.

        This literature gives rise to the question why citation rates have independent (of the quantity and

quality of publications) effects on salary. Most schools (thirty-five out of forty-three in a survey by

Hamermesh and Pfann, 2012) do not obtain citation information for annual merit reviews, and Liebowitz

(2014) reports on a similar survey of forty-six department chairs showing that citations are considered

less important in decisions about promotion to full professor than are the outlets where the candidates’

articles appeared. One explanation is that, in a profession with substantial threatened and actual mobility,

citations contribute directly to reputation which in turn enhances salaries.

                                                      31
32
    B. Other Outcomes and Behavior

        The various studies of the impact of citations on salaries were made commensurable using an

elasticity-like measure. Other effects of citations are too diverse to allow direct numerical comparisons.

The nearest to the salary studies are the several pieces of research that have examined how citations affect

the likelihood of promotion, usually in the context of measuring gender differentials. Thus Ginther and

Kahn (2004), using a sample of junior economists, show that additional WoS citations, holding article

quantity and quality constant, do increase the probability of having a tenured academic position ten years

post-Ph.D., but the impact is small and statistically insignificant. Using GS data, de Paola and Scoppa

(2015) show that selections by boards of examiners for appointments as associate or full professor in Italy

are very sensitive to the relative h-statistics of the candidates,

        The age of citations might be viewed as an indicator of the rate of technical progress in a sub-

specialty (or taking a dimmer view, as showing the rate at which new fads appear). McDowell et al

(2011) used longitudinal data on the age of citations across eight sub-fields to examine individuals’

choices about whether to become a department chair and about activities post-chair position. Economists

working in sub-fields in which citation age indicates more rapid depreciation of knowledge are less likely

to become department heads; but if they do, they exit the position more slowly and are more likely to

become “higher-level” administrators. Similar apparent rationality is shown by economists’ choices about

how to treat refereeing requests. Those whose work is more heavily cited take longer to referee and are

more likely to refuse editors’ refereeing requests (Hamermesh, 1995). Not only do additional citations

affect salary; these results suggest that economists behave as if citations proxied the value of their time.

        A substantial literature has examined whether scholarly achievement, as indicated by citations,

affects the likelihood that an individual receives some academic honor or award. Effects on rewards may

be especially important, given limits on monetary returns in academe and the visibility of awards and

honors. There are two groups of decision-makers that determine these outcomes: Small committees (of

oligarchs) that select individuals for awards (such as Nobel Prizes; AEA Distinguished Fellows, Clark

Medalists and Presidents; and their European and other foreign equivalents) and electorates that explicitly

                                                       33
choose among small sets of pre-selected candidates (e.g., for AEA vice-president or Executive Committee

membership, or Econometric Society Fellow). The questions here are whether the choices of each type of

decision-maker are at least partly based on achievement, measured by citations, and whether the

importance of citations in these choices generally differs between the two types.

        One might argue that scholarly achievement alone should determine choices of committees

making selections for candidates in elections where voters determine outcomes or directly for

awards/positions. The evidence does make it clear that scholarly achievement, as reflected by others

scholars’ citations to the awardees’ work, is important in these choices. Thus Diamond and Toth (2007)

show that those AEA Presidents who have been selected from among earlier Executive Committee

members have been more heavily (WoS) cited, conditional on the quantity of publications, age, location

and Ph.D. “pedigree,” than others. Similarly, Hamermesh and Pfann (2012) demonstrate that among all

full professors at 88 economics departments, those who are awarded Nobel Prizes, named AEA

Distinguished Fellow, President or Clark Medalist, have been more heavily (WoS) cited, conditional on a

vector of their other characteristics. Given the sample size this result is unsurprising; but conditional on

the total citation count, having more heavily concentrated citations—more “home runs”—also increases

the likelihood of selection.

        Relative citation rates also partly determine outcomes of elections in which association members

or prior electoral winners vote for awards. Thus in a regional economics association in which voters chose

within pairs of candidates, individual voters tended to elect candidates with higher relative citation rates

(although various affinities, particularly gender among female voters, mitigated or even vitiated the

importance of relative citations, as Dillingham et al, 1994, showed). Similar results, and a similar

importance of gender, is shown by outcomes of the four-person elections for AEA Vice-Presidents and

Executive Committee members (Donald and Hamermesh, 2006). In elections of Econometric Society

Fellows, the more heavily-cited nominees from among lists that typically include around fifty economists

are more likely to obtain the requisite thirty percent of approval votes (Hamermesh and Schmidt, 2003).



                                                    34
        Scholarly achievement, as measured by others’ citations to one’s publications, is a major

determinant of awards, honors and elections in the economics profession, regardless of whether these

outcomes are determined by committees or electorates. Candidates’ other characteristics are also

important determinants, and no doubt one’s own marketing efforts and the parochial preferences of

committees or voters matter too. People can differ about the appropriate relative importance of scholarly

achievement compared to these other factors, but the evidence is clear that the explicit recognition of

one’s work by other scholars does matter in these contexts.

    C. Judging Results by Subsequent Citations

        Since citations are a measure of quality, subsequent citations can provide a useful check on

decisions about publishing, presenting awards, promotions and other outcomes. Many journals give

awards (plaques or small honoraria) to the “best article” of the year. These awards are typically chosen by

editorial or association committees. Are they given to the article with the most impact, or even to articles

with impacts above the median? Coupé (2013) showed that award-winning papers are more likely than

others to be the most WoS or GS-cited articles in a volume, but very few are the most-cited paper in a

volume. Since the very fact that an article received an award may stimulate additional subsequent

citations, this evidence suggests a somewhat tenuous relation between the citations that an article would

have received absent the prize itself.

        While we showed that several awards are partly determined by authors’ prior citations, one might

wonder how well selection committees and voters succeed in choosing one or several winners among a

set of similar candidates. Using Clark Medalists and newly-elected Econometric Society Fellows, Chan et

al (2014) estimate double-differences between them and arguably comparable economists pre- and post-

selection/election. The awardees/winners generate more and better subsequent citations than the control

group, and even more citations per publication. Whether this double-difference reflects the perspicacity of

committees and voters about awardees’/winners’ staying power, or simply that their “victory” per se leads

others to cite their works more frequently, is unknowable from this statistical design.



                                                     35
          The very acceptance of an article is itself an “award,” especially given the low acceptance rates

even at second-level economics journals. Are these awards conferred based on successful perceptions of

quality, or do they reflect, as so many economists believe (especially when their paper is rejected!)

editorial favoritism—toward friends and colleagues? Laband and Piette (1994) show that five-year WoS

citations are greater, conditional on authors’ prior citations, to papers in top economics journals when the

author and editor have some kind of connection—as student or colleague. Similar results are obtained by

Brogaard et al (2014) using a much larger sample, defining “connection” as being colleagues with an

editor, and accounting for school and even author fixed effects.

          Editors’ choices also do not seem to be biased toward accepting papers that cite the editors’ own

articles more. While it is true that scholars receive more citations in those journals and at those times

when they are editing, a keyword count (Kim and Koh, 2014) suggests that these fillips to their citation

counts do not arise from their choosing papers that cite them a lot. Perhaps instead authors select their

submissions based on editorial interests, which guarantees more citations to the editor during his/her

tenure.

          There is no question that an article’s place in a journal issue matters: Lead articles receive many

more citations than do subsequent articles (Smart and Waldfogel, 1996). But is this because their

visibility leads readers to cite them more, or because editors’ decisions to place them in the lead reflects

editorial beliefs about their quality, which are then validated in the market for research? Using a journal

that ordered papers alphabetically (by author’s surname) in some issues, by editorial choice in others,

Coupé et al (2010) show that about two-thirds of a lead article’s extra citations were due to its position,

one-third to its inherent higher quality than subsequently placed articles in the same issue.

          The title that one chooses for an article in an economics journal also matters for its subsequent

(six-year, WoS) citations. Taking the three leading U.S. general journals, Mialon (2015) classifies articles

as having a “poetic” title, one that includes some poetic device or figure of speech, as opposed to a purely

informative title. The former are more heavily cited in the first six post-publication years, conditional on



                                                      36
their authors’ prior citations, but only among empirical papers. Indeed, the opposite is true for theory

papers.

    V.          Main Points and Implications

          Citations are obviously just one possible measure of academic productivity; but they are a

quantifiable way of distinguishing among different participants in a particular academic enterprise.

Obtaining accurate citations counts by whatever method—be it WoS, GS profiles, or counting each

person’s most-cited articles, requires substantial effort to ensure accuracy. The gratifying result here is

that the particular citation count or statistic chosen yields a ranking that is very similar to other rankings

that might have been chosen. Thus, for example, there is a very tight relationship between citation counts

using the WoS and GS sources, with the latter typically five times the former. So long as great care is

taken in counting people’s productivity as measured by citations, individuals and institutions can be

ranked appropriately and similarly using a number of different metrics. So too, any of these carefully

constructed measures can be used to examine the relationship between this proxy for academic influence

and outcomes in academe.

          The overarching result throughout much of this article was the heterogeneity of citation counts

across units being studied. There is substantial overlap in citations of articles that are published in

journals that are viewed as being of different quality. Many of the articles in top journals receive little

attention (though on average they receive more attention than those in lower-ranked journals). So too,

many economists at lower-ranked faculties are cited more than the median faculty members at higher-

ranked schools (though on average the former are cited less frequently than the latter). Indeed, these

overlaps are the best justifications for examining citations—they allow for studying the achievement of

individuals and their research instead of relying on opinions about average reputations of journals and

institutions.

          Fortunately the market for economists implicitly recognizes the importance of citations, since

they help determine economists’ pay independently of the quantity and perceived quality of the articles

that economists publish. But, while citations do matter in the market, do they matter enough, especially in

                                                     37
salary-setting, promotions and appointments? I would argue that they do not, and that the profession puts

too much weight on publications in a few top journals. At a time when acceptance rates at the top five

journals are below ten percent, relying on editors’ parochial preferences in choosing which articles to

publish when we assign credit in this profession may be inferior to letting the market for ideas, as

reflected in citations, bear more weight in assigning elite monetary and reputational rewards. Indeed, that

citations appear to matter more than the mere quantity or even the journal quality of publications in

determining non-monetary rewards suggests that in certain circumstances this reflection of the market

attains a greater importance.

        No doubt the importance of average journal quality in salary and other decisions is due to the

greater ease of obtaining information about them than about citations. What could be done to enhance the

use of citations as measures of academic impact, to account for the obvious heterogeneity across articles,

individuals and institutions? Asking faculty members to count and report citations annually is both

demeaning and a creator of tedium (and unlikely to be successful). But asking faculty to register and

create a GS profile gives them a simple discrete task and allows administrators to obtain and, I hope, use

annual information on scholarly impact in determining salaries of current faculty and in targeting faculty

elsewhere as potential recruits.

        I am not arguing that citations should be the only determinant of rewards in economics. Indeed, a

greater reliance on citations will lead some academics to increase their efforts to game the system and

raise their citation counts. Rather, I am merely arguing that their impact is currently undervalued, a

situation that is easily remediable. We remain at a point where the provision and use of more information

on measured impacts of scholarly work would be better than less.

        Economists have conducted a lot of research on the sociology of the economics profession, and

research on how to count citations, citation-based rankings and citations as determinants of outcomes has

burgeoned. Particularly interesting would be studies paralleling those summarized in Table 9, but

concentrating on salary determination in private institutions. Is the impact of citations greater or less there

than what has been found for public schools? Do citations affect one’s ability to obtain grant money or

                                                      38
other non-nine-month professional income? In other words, do the percentage payoffs summarized in

Table 9 under- or overstate the returns to scholarly impact? The growth of web-based citation counts has

made answers to these and other questions related to citations much easier than before.

        Perhaps this judgment is too harsh, but very little of the research on the role of citations in

economics has been used to shed light on more general economic issues. Just as the economics of

particular sports has been used to study issues in the functioning of labor and product markets, there is a

lot of research to be done in which the empirical study of our citation practices can lead to greater

understanding of such issues. Indeed, the general analysis of such diverse topics as the role of norms of

interpersonal behavior, how discourse alters market behavior, and others, might be fruitfully studied using

citation analyses as examples. In short, there is room for broad-ranging and broadly applicable research

using citations in this profession.




                                                    39
REFERENCES

Aizenman, Joshua, and Kenneth Kletzer. 2011. “The Life Cycle of Scholars and Papers in Economics:
      The ‘Citation Death Tax’.” Applied Economics 43(27): 4135-48.
Anderson, Gary, David Levy and Robert Tollison. 1989. “The Half-Life of Dead Economists.” Canadian
       Journal of Economics 22(1): 174-83.
Axarloglou, Kostas, and Vasilis Theoharakis. 2003. “Diversity in Economics: An Analysis of Journal
       Quality Perceptions.” Journal of the European Economic Association 1(6): 1402-23.
Bodenhorn, Howard. 2003. “Economic Scholarship at Elite Liberal Arts Colleges: A Citation Analysis
      with Rankings.” Journal of Economic Education 34(4): 341-59.
Borjas, George. 1985. “Assimilation, Changes in Cohort Quality, and the Earnings of Immigrants.”
        Journal of Labor Economics 3(4): 463-89.

Bosquet, Clément, and Pierre-Philippe Combes. 2013. Are Academics Who Publish More Also More
       Cited? Individual Determinants of Publication and Citation Records.” Scientometrics 97(3): 831-
       57

Bratsberg, Bernt, James Ragan and John Warren. 2010. “Does Raiding Explain the Negative Returns to
        Faculty Seniority?” Economic Inquiry 48(3): 704-21.

Brogaard, Jonathan, Joseph Engelberg and Christopher Parsons. 2014. “Networks and Productivity:
       Causal Evidence from Editor Rotations.” Journal of Financial Economics 111(1): 251-70).

Cartter, Allan. 1966. An Assessment of Quality in Graduate Education. American Council for Education.
Chan, Ho Fai, Bruno Frey, Jana Gallus and Benno Torgler. 2014. “Academic Honors and Performance.
       Labour Economics 31(1): 188-204.
Cherrier, Béatrice. 2015. “Classifying Economics: A History of the JEL Codes.” Unpublished paper,
        University of Caen.
Chiappori, Pierre-André, and Steven Levitt. 2003. “An Examination of the Influence of Theory and
       Individual Theorists on Empirical Research in Microeconomics.” American Economic Review
       93(2): 151-5.
Coupé, Tom. 2013. “Peer Review versus Citations—An Analysis of Best Paper Prizes.” Research Policy
       42(1): 295-301.
---------------, Victor Ginsburgh and Abdul Noury. 2010. “Are Leading Papers of Better Quality?
          Evidence from a Natural Experiment.” Oxford Economic Papers 62(1): 1-11.
Davis, Paul, and Gustav Papanek. 1984. “Faculty Ratings of Major Economics Department by Citations.”
        American Economic Review 74(1): 225-50.
Diamond, Arthur. 1986. “What Is a Citation Worth?” Journal of Human Resources 21(2): 200-15.
--------------------, and Robert Toth. 2007. “The Determinants of Election to the Presidency of the
          American Economic Association: Evidence from a Cohort of Distinguished 1950s Economists.”
          Scientometrics 73(2): 131-7.
Dillingham, Alan, Marianne Ferber and Daniel Hamermesh. 1994. “Gender Discrimination by Gender:
        Voting in a Professional Society.” Industrial and Labor Relations Review 47(4): 622-33.

                                                 40
Donald, Stephen, and Daniel Hamermesh. 2006. “What Is Discrimination? Gender in the American
       Economic Association, 1935-2004.” American Economic Review 96(4): 1283-92.
Drèze, Jacques, and Fernanda Estevan. 2007. “Research and Higher Education in Economics: Can We
        Deliver the Lisbon Objectives?” Journal of the European Economic Association 5(2): 271-304.
Dusansky, Richard, and Clayton Vernon. 1998. “Rankings of U.S. Economics Departments.” Journal of
       Economic Perspectives 12(1): 157-70.
Egghe, Leo. 2006. “Theory and Practise of the g-Index.” Scientometrics 69(1): 131-52.
Einav, Liran, and Leeat Yariv. 2006. “What’s in a Surname? The Effects of Surname Initials on
       Academic Success.” Journal of Economic Perspectives 20(1): 175-88.
Ellison, Glenn. 2013. “How Does the Market Use Citation Data? The Hirsch Index in Economics.”
        American Economic Journal: Applied Economics 5(3): 63-90.
Engers, Maxim, Joshua Gans, Simon Grant and Steven King. 1999. “First Author Conditions.” Journal of
        Political Economy 101(4): 859-83.
Ferber, Marianne. 1988. “Citations and Networking.” Gender and Society 2(1): 82-9.
Gerrity, Dennis, and Richard McKenzie. 1978. “The Ranking of Southern Economics Departments: New
        Criterion and Further Evidence.” Southern Economic Journal 45(2): 608-14.
Ginther, Donna, and Shulamit Kahn. “Women in Economics: Moving Up or Falling Off the Academic
        Career Ladder?” Journal of Economic Perspectives 18(3): 193-214.
Hamermesh, Daniel. 1989. “Why Do Individual-Effects Models Perform so Poorly? The Case of
      Academic Salaries.” Southern Economic Journal 56(1): 39-45.
-----------------------. 1995. “Nonprice Rationing of Services, with Applications to Refereeing and
          Medicine.” Research in Labor Economics 14: 283-306.

------------------------. 2013. “Six Decades of Top Economics Publishing.” Journal of Economic Literature
          51(1): 162-72.
------------------------, and Sharon Oster. 2002. “Tools or Toys: The Impact of High Technology on
          Scholarly Productivity.” Economic Inquiry 40(4): 539-55.
------------------------, and Gerard Pfann. 2012. “Reputation and Earnings: The Roles of Quality and
          Quantity in Academe.” Economic Inquiry 50(1): 1-16.
------------------------, and Peter Schmidt. 2003. “The Determinants of Econometric Society Fellows
          Elections.” Econometrica 71(1): 399-407.
------------------------, George Johnson, and Burton Weisbrod. 1982. “Scholarship, Citations and Salaries:
          Economic Rewards in Economics.” Southern Economic Journal 49(2): 472-81.
Hansen, W. Lee, Burton Weisbrod and Robert Strauss. 1978. “Modeling the Earnings and Research
       Productivity of Academic Economists.” Journal of Political Economy 82(4): 729-41.
Hilmer, Christine, Michael Hilmer and Michael Ransom. 2015. “Fame and the Fortune of Academic
       Economists: How the Market Rewards Influential Research in Economics.” Southern Economic
       Journal 82(2): 430-52.
Hodgson, Geoffrey, and Harry Rothman. 1999. “The Editors and Authors of Economics Journals: A Case
      of Institutional Oligopoly?” Economic Journal 109(1): F165-86.

                                                   41
Hollis, Aidan. 2001. “Co-authorship and the Output of Academic Economists.” Labour Economics 8(4):
        503-30.
Holtmann, Alphonse, and Alan Bayer. 1970. “Determinants of Professional Income among Recent
       Recipients of Natural Science Doctorates.” Journal of Business 43(4): 410-18.
Hudson, John. 2007. “Be Known by the Company You Keep: Citations—Quality or Chance?”
       Scientometrics 74(2): 231-8.
Hughes, Raymond. 1925. A Study of Graduate Schools of America. Oxford, OH: Miami University Press.
Johnson, George, and Frank Stafford. 1974. “Lifetime Earnings in a Professional Labor Market:
       Academic Economists.” Journal of Political Economy 82(3): 549-69.
Kalaitzidakis, Pantelis, Theofanis Mamuneas and Thanasis Stengos. 2003. “Rankings of Academic
        Journals and Institutions in Economics.” Journal of the European Economic Association 1(6):
        1346-66.
Keniston, Hayward. 1959. Graduate Study and Research in the Arts and Sciences at the University of
       Pennsylvania. Philadelphia: University of Pennsylvania Press.
Kenny, Lawrence, and Roger Studley. 1995. “Economists’ Salaries and Lifetime Productivity.” Southern
       Economic Journal 62(2): 382-93.
Kim, E. Han, Adair Morse and Luigi Zingales. 2009. “Are Elite Universities Losing Their Competitive
       Edge?” Journal of Financial Economics 93(3): 353-81.
Kim, Jinyoung, and Kanghyock Koh. 2014. “Incentives for Journal Editors.” Canadian Journal of
       Economics 47(1): 348-71.
Laband, David. 1987. “A Qualitative Test of Journal Discrimination against Women.” Eastern Economic
       Journal 13(2): 149-53.
-----------------. 2013. “On the Use and Abuse of Economics Journal Rankings.” Economic Journal
          123(August): F223-54.
------------------ and Michael Piette. 1994. “Favoritism versus Search for Good Papers: Empirical Evidence
          Regarding the Behavior of Journal Editors.” Journal of Political Economy 102(1): 194-203.
------------------ and Robert Tollison. 2003. “Dry Holes in Economic Research.” Kyklos 56(2): 161-73.
Liebowitz, Stanely. 2014. “Willful Blindness: The Inefficient Reward Structure in Academic Research.”
       Economic Inquiry 52(4): 1267-83.
McCabe, Mark, and Christopher Snyder. 2015. “Does Online Availability Increase Citations? Theory and
      Evidence from a Panel of Business Journals.” Review of Economics and Statistics 97(1): 144-65.
McDowell, John, Larry Singell and Mark Stater. 2011. “On (and Off) the Hot Seat: An Analysis of Entry
     into and out of University Administration.” Industrial and Labor Relations Review 64(5): 889-
     909.
Margo, Robert. “The Economic History of the ‘American Economic Review:’ A Century’s Explosion of
       Economics Research.” American Economic Review 101(1): 9-35.
Medoff, Marshall. 2003. “Collaboration and the Quality of Economics Research.” Labour Economics
       10(5): 597-608.
---------------------. 2006. “The Efficiency of Self-Citations in Economics.” Scientometrics 69(1): 69-84.

                                                     42
Merton, Robert. 1968. “The Matthew Effect in Science.” Science 159(3810): 56-63.
Mialon, Hugo. 2015. “What’s in a Title: Predictors of the Productivity of Publications.” Unpublished
       paper, Emory University, Department of Economics.
Mixon, Franklin, and Kamal Upadhyaya. 2013. “The Economics Olympics: Ranking U.S. Economics
       Departments Based on Prizes, Medals, and Other Awards.” Southern Economic Journal 79(1):
       90-6.
National Research Council. 1995. Research-Doctorate Programs in the United States: Continuity and
       Change. Washington: National Academy of Sciences.
Oswald, Andrew. 2007. “An Examination of the Reliability of Prestigious Scholarly Journals: Evidence
       and Implications for Decision-Makers.” Economica 74(1): 21-31.
Palacios-Huerta, Ignacio, and Oscar Volij. 2004. “The Measurement of Intellectual Influence.”
        Econometrica 72(3): 963-77.
De Paola, Maria, and Vincenzo Scoppa. 2015. “Gender Discrimination and Evaluators’ Gender: Evidence
       from Italian Academia.” Economica 82(1): 162-88.
Pieters, Rik, and Hans Baumgartner. 2002. “Who Talks to Whom? Intra- and Interdisciplinary
        Communication of Economics Journals.” Journal of Economic Literature 40(2): 483-509.
Quandt, Richard. 1976. “Some Quantitative Aspects of the Economic Journal Literature.” Journal of
       Political Economy 84(4): 741-56.
Roose, Kenneth, and Charles Anderson. 1970. A Rating of Graduate Programs. Washington, American
       Council on Education.
Sauer, Raymond. 1988. “Estimates of the Returns to Quality and Coauthorship in Economic Academia.”
        Journal of Political Economy 96(4): 855-66.
Scott, Loren, and Peter Mitias. 1996. “Trends in Rankings of Economics Departments in the U.S.: An
        Update.” Economic Inquiry 34(1): 378-400.
Siow, Aloysius. 1991. “Are First Impressions Important in Academia?” Journal of Human Resources
       26(2): 236-55.
Smart, Scott, and Joel Waldfogel. 1996. “A Citation-Based Test for Discrimination at Economics and
       Finance Journals.” National Bureau of Economic Research, Working Paper No. 5460.
Smith, Alexander. 1863. Dreamthorp: A Book of Essays Written in the Country. London, Strahan.
Stern, David. 2013. “Uncertainty Measures for Economics Journal Impact Factors.” Journal of Economic
        Literature 51(1): 173-89.
Stigler, George, and Claire Friedland. 1975. “The Citation Practices of Doctorates in Economics.”
         Journal of Political Economy 83(3): 477-507.
Tuckman, Howard, and Jack Leahey. 1975. “What Is an Article Worth?” Journal of Political Economy,
      83(5): 951-68.
Verma, Inder. 2015. “Impact, Not Impact Factor.” PNAS, 112(26): 7875-6.




                                                 43
http://apps.webofknowledge.com/WOS_GeneralSearch_input.do?product=WOS&search_mode=
GeneralSearch&SID=3FjfjXSbrzIWDHD7U5a&preferencesSaved=

Basic Search
Click here for tips to improve your search.

Timespan




     From


to


Web of Science Core Collection: Citation Indexes

     •      Science Citation Index Expanded (SCI-EXPANDED) --1900-present
     •      Social Sciences Citation Index (SSCI) --1900-present
     •      Arts & Humanities Citation Index (A&HCI) --1975-present
     •      Conference Proceedings Citation Index- Science (CPCI-S) --1990-present
     •      Conference Proceedings Citation Index- Social Science & Humanities (CPCI-SSH) --
         1990-present

Results found:                                 148
Sum of the Times Cited [?] :                   2679
Sum of Times Cited without self-citations [?] :2533
Citing Articles [?] :                          2202
Citing Articles without self-citations [?] :   2130
Average Citations per Item [?] :               18.10
h-index [?] :                                  27



Figure 1. Screens from a Search in the Web of Science




                                               44
https://scholar.google.com/
                               All Since 2010
Citation indices
Citations                     16608   5615
h-index                         61     36
i10-index                      156     89


Figure 2. A Screen from a Search in Google Scholar




                                                45
46
   200
   150
   100
   50
   0




         0            200             400             600             800   1000
                                            gscites

                               sscicites              Fitted values




WoSCitations = 7.860 + 0.182 GSCitations, Adj. R2 = 0.856, N = 486
                   (0.930) (0.0034)

Figure 4. Relation between WoS Citations and Google Scholar Citations, Articles in J5
Journals, 2007-08*


*Excludes 11 articles with 1000+ GS citations.




                                                         47
   400
   300
   200
   100
   0




         0            500              1000               1500           2000
                       GoogleScholar Citations as of May 2015

                SSCI Citations as of May 2015            Fitted values



WoSCitations = -0.318 + 0.173 GSCitations , Adj. R2 = 0.909, N = 102
                   (2.81) (0.0055)

Figure 5. Relation between WoS Citations and Google Scholar Citations, Faculty in “Top
Schools,” Ph.D. 2006 or Later*


*Excludes 7 faculty with 2000+ GS citations.




                                                            48
Table 1. Rank Correlations, Citation Measures from Google Scholar*
                          All Faculty (N = 543)

                       i10           Total           C5 Citations
h                     0.989          0.967              0.933
i10                   -------        0.951              0.913
Total Citations       -------       --------            0.982


                      Ph.D. > 2004 (N = 195)

                       i10            Total          C5 Citations
h                     0.973           0.926             0.884
i10                   --------        0.929             0.893
Total Citations       --------       -------            0.992

                        Ph.D. <2005 (N = 348)

                        i10           Total          C5 Citations
h                      0.971          0.932             0.843
i10                   --------        0.884             0.783
Total Citations       --------        -------           0.959




                                                49
Table 2. Distribution of Citation Measures of Faculty in 30 “Top” Economics Departments
                                            Citation Statistic


                            C5           Total             h         i10

Mean                       3013           8395             25        42
Std. Error                 (171)          (831)          (0.98)    (2.74)
10th Percentile             54             105              5         3
Median                     1078           2101             17        20
90th Percentile            7931         21,2105            55       100
95th Percentile           13,149         39,844            70       142
99th Percentile           25,688        103,727           117       278
Range                    [0, 66,118]   [0, 214,046]     [0, 171]   [0, 754]
N                          1043           543             543       543




                                            50
Table 3. Experience Profiles of Citation Measures in 30 “Top” Economics Departments
                              h (N = 543)                            C5 (N = 1043)
Experience
                         10th                   90th                10th                   90th
             Mean      percentile   Median    percentile   Mean   percentile   Median   percentile
   0-1       2.8           0          2            6         40        1         16         86
    2        3.8           1          4            7        103        3         55        325
    3        4.5           2          4            8        109       10         42        306
    4        5.7           2          5            9        141       19         80        338
    5        5.8           2          6            9        176        20       124        394
    6        9.1           5          8           15        306       34        206        702
   7-8       10.3          5          9           16        465       82        280        1233
  9-10       12.7          7         12           19        687      120        555        1479
  11-12      14.7          8         13           23       1043      190        829        2257
  13-15      20.0         12         19           33       1571      356        1027       3212
  16-20      26.2         13         23           47       2423      449        1435       6866
  21-25      30.5         13         28           55       3699      531        2381       8242
  26-30      42.3         20         35           76       6098      842        3681      14113
  31-35      48.0         27         38           82       5716      643        3452      16146
  36-40      45.1         22         43           67       5172      587        3223      12652
  41-45      56.1         17         54           96       7111      458        4270      19004
   46+       67.1         27         62          171       6498      728        3418      15342




                                             51
Table 4. Women and Citations, Means and Regression Estimates*
                          Experience>17            Experience<18
Mean:

Fraction Female                  0.09                  0.19

C5 Citations                  5223                      706

Ind. Var.:
Female                    -459          -352.5      -9.0        0.48
                          (704)         (577)       (66.6)     (72.6)

Rank of                                 -166.5                 -34.3
Dept.                                    (18.4)                 (4.9)

Pseudo- R2                0.019         0.108       0.257      0.277

N                          521          521         522        522


*These LAD regressions include a large vector of indicators of post-Ph.D. experience.




                                                        52
      .4
      .3
   Density
    .2.1
      0




             0          5            10           15             20            25
                                     cites/pagesaer



Figure 6a. Distribution of Citations per AER2008-Equivalent Page, J5 Articles, 1974-75*
      .4
      .3
   Density
    .2.1
      0




             0          5             10            15           20            25
                                    sscicitesperaerpage



Figure 6b. Distribution of Citations per AER2008-Equivalent Page, J5 Articles, 2007-08


*Excludes the top 4 percent of papers, with citations/equivalent page greater than 25


                                                          53
Table 5. “Resurrections” and “Flashes in the Pan” of J5 Articles 1974-75, N=444


                                   Rank in Most Recent 20 Years
                   Mean    Maximum 90th Percentile Median 10th Percentile Minimum
Rank in
 First 10 Years

Bottom Ventile      372      244         295          372           424           426
Bottom Decile       366      235         288          366           426           441
9th Decile          339       90         217          363           431           442


2th Decile          115        9         28            80           234           389
Top Decile           46        1          4            31           109           187
Top Ventile          28        1          2            18            84           117




                                           54
Table 6. Citations through 2014 by Number of Authors, J5 Articles, 1974-
75 and 2007-08*

                  N Authors         1            2            3+

                                             1974-75

Mean                              78.2         72.0          38.4
Std. Error                        (12.2)       (9.9)        (23.5)
Median                             20           38            10
90th Percentile                    171         164            88
Range                            [0,2466]     [0, 679]      [0, 435]
N                                  312         114            18


                                              2007-08

                  N Authors         1            2             3         4+

Mean                              41.2         48.5          61.7       72.7
Std. Error                        (3.2)        (3.4)         (6.3)      (2.9)
Median                             28           33            44          61
90th Percentile                    86          105           128         168
Range                            [3, 206]     [2, 478]      [0, 365]   [9, 193]
N                                  134         246            94         23

*Excludes papers from 1974-75 that were fewer than 5 nominal pages long, fewer than 10 nominal pages long in
2007-08.




                                                       55
Table 7. SSCI Citations per Article by Type in J5 Journals, 1974-75 and 2007-08*
                              1974-75 Publications, First 7 Years*
                                                                       Econometric
                          Theory             Empirical                   Theory

Mean                        12.8                   16.7                    8.5
Std. Error                  (1.3)                  (1.9)                  (1.3)
Median                        6                     11                       6
90th Percentile              29                     38                      25
Range                      [0, 171]            [0, 109]                   [0, 39]
N                           280                    113                      46

                                              2007-08 Publications, First 7 Years
                                                           Empirical:                                Economet-
                                       Theory w/           Borrowed        Empirical:    Experi-     ric Theory
                          Theory       Simulation            Data          Own Data      mental

Mean                        33.5           60.5               65.2               54.7      64.9       46.2
Std. Error                  (2.9)          (9.3)              (5.7)              (5.4)     (7.9)      (6.8)
Median                       24             43                 47                 40        50         37
90th Percentile              63            120                147                 99       123        110
Range                     [2, 304]       [0, 478]           [4, 348]          [4, 365]   [11, 281]   [4, 179]
N                           176             55                 99                 95        39         33


                           1974-75 Publications, Citations through 2014*
                                                                       Econometric
                          Theory             Empirical                   Theory

Mean                        78.1                   83.3                   41.7
Std. Error                  (13.2)               (12.8)                   (8.9)
Median                       20                     37                      21
90th Percentile             160                    186                     141
Range                     [0, 2466]            [0, 879]                  [0, 259]
N                           280                    113                      46

*Excludes 5 articles that were not classifiable under any of these rubrics.




                                                             56
Table 8. Rankings and Overlaps in Faculty Quality across the Top 30 Economics
Departments, Based on GS Citations to Each Author’s Five Most-Cited Publications (C5),
and Ranks Using 50th or 90th Centiles
    SCHOOL                 N          rd50       C550      KMS           x*         rd90       r*d50

Harvard                   51          1          7000          1        ----          1           1
Princeton                 52          2          2534          7          0           4           4
Chicago                   26          3          2373          2          1          11           6
Stanford                  40          4          2272          8          2          10           5
MIT                       30          5          2115          3          4           2           2
Berkeley                  41          6          1894          9          4           9           3
NYU                       46          7          1737         10          5           7          14
Columbia                  45          8          1693         11          6           5           7
Maryland                  34          9          1596         25          7          25          18
Yale                      42          10         1560          6          8          12          10
San Diego                 39          11         1413         12          9          13          11
Minnesota                 20          12         1290         19         10          20          24
Penn                      32          13         1205          5         11          22           8
Boston Uni                40          14         1200         20         12          14          19
Cal Tech                  19          15         968          26         13           3          25
Brown                     33          16         950          21         12          17          11
Northwestern              37          17         945           4         15           8           9
Michigan                  45          18         896          13         16          18          11
Duke                      50          19         858          22         12          21          20
UCLA                      38          20         850          14         15          24          16
Wisconsin                 33          21         717          18         10          15          21
Ohio State                29          22         688          29          7          28          28
Michigan State            43          23         672          23         17          23          30
Illinois                  26          24         666          27         11          26          23
Southern Cal              23          25         649          28         19           6          29
Cornell                   29          26         525          15         20          19          15
UT-Austin                 28          27         474          16         12          29          17
Rochester                 24          28         306          17          1          27          27
Carnegie-Mellon           24          29         284          24         23          16          22
Pittsburgh                24          30         267          30         12          30          26



*N is the number of faculty members included; rd50 is the department’s rank as measured by the median unadjusted
citations, and C550 is that median; KMS (2003) is the ranking in the U.S. based on the impact factors of five years
of publications in economics journals of all faculty at the institution; x* is the number of ranks above rd50 in which
the department’s top quartile of faculty members, as ranked by citations, would be above the rd50 - x* department’s
median faculty member; rd90 is the department’s rank as measured by the of the faculty member at the 90th percentile
of this measure in the department; r*d50 is the department’s rank as measured by the median experience-adjusted
citations.



                                                         57
      .025
      .02
       .015
  Density
      .01
      .005
      0




              0      20          40           60        80       100
                               centileJ5perpage



Figure 7. Position of Citations to Articles in the Economic Journal and Review of Economics
 and Statistics in the Distribution of Citations to J5 Articles




                                                   58
Table 9. The Impact of Citations on Economists' Salariesa

                                                                             % impact of
Study             Group and           Citation         Covariates            10% increase     Novelty
                  year(s)             measure                                In citationsb

Hamermesh et al
(1982)            Fulls, 7 large      Average SSCI     Experience,           1.4
                   public schools     last 5 years      admin, theorist
                  1979                                  school FE
                                                       (# articles, books)

                                                       Experience,
Diamond (1986)    All ranks,          SSCI per year    cohort                2.6% for each    Long longit.
                  1961-79,                                                   extra 10 cites
                   UC-Berkeley

                  Assocs & fulls, 7
Sauer (1988)      large departments   SSCI 7 years     Experience,           1.2              Co-authorship,
                                                        admin, top
                                                       journal,                               pages, journal
                                                        pages, other papers,                  quality
                                                       books, school FE

Hamermesh                                              Experience,
(1989)c           Fulls, 6 large      Average SSCI     admin,                0.5              Person FE
                  public schools      last 5 years     person FE
                  1979, 1985

                                                       Experience,
Siow (1991)       Hamermesh           Average SSCI     admin,                ↓ as             Timing of
                  (1989) data         last 5 years     person FE             experience       citations in
                                                                             ↑                 career


                                                       Experience,
Kenny and         All faculty, 10     SSCI lifetime    admin, gender.        1.2              Impact-factor
Studley (1995)    large depart-                         quality-adjusted                      adjustment of
                  ments, 1987                           articles                               citations




                                                      59
                                                                   Experience,
Moore                    Tenured faculty,       SSCI lifetime      gender, seniority,    0.4   Productivity
et al (1998)              9 state schools,                          quality/quantity,          and wage-
                         1992                                       foreign, Ph.D.             seniority
                                                                    quality, teaching          relation
                                                                    award

                                                                   Experience,                 Wage-
Bratsberg                All faculty,           SSCI Lifetime      gender, seniority,    0.1   seniority and
et al (2010)             5 midwestern                              quality/quantity             turnover
                          state schools,                            pages, books
                          1974-2004

                                                                   Experience,                 Distribution of
Hamermesh and            Fulls, 43 large        Lifetime           gender,               0.9   citations
                                                                   foreign, field, no.
Pfann (2012)              public schools         SSCI              of pubs,                     of citations
                         2008                   citations          "home runs”, sur-           among
                                                                    name, school FE            pubs

                                                                   Experience,
Hilmer                   All faculty, 53        Lifetime           gender, seniority,    0.8   Large sample,
et al (2015)             large public            SSCI               quality/quantity.          detail on co-
                         schools,               citations          pages                       authorship

a
    log(9-month or 9-month equivalent salary) is the dependent variable.

b
    Unless otherwise noted.

c
 Uses log(9-month real compensation) as
the dependent variable.




                                                                 60
