                    NBER WORKLNG PAPER SERIES




                 GENERAL ALORITHN FOR SIMULTANEOUS

          ESTIMATION OF CONSTANT AND RANIX)MLY-VARYING

                 PARAMETEBS IN LINEAR RELATIONS



                       Alexander H. SarTis


                     Working Paper No. 38


CO1FUTER RESEARCH CEN'IER FOR ECONOMECS AND MANAGEMENT SCIENCE
          Nà±iotial Bureau of Economic Research, Inc.
                     575 Technology Square
               Caithridge, Massachusetts 02139



                          -April 1974


                 Preliminary: not for quotation
NBER wordng papers are distributed informally and in limited
ni.uthers for comments only. They should not be quoted without
written permission.
This report has not undergone the review accorded official NBER
publications; in particular, it has not yet been submitted for
approval by the Board of Directors.
NBER Computer Research Center and Massachusetts Institute of
Technology. Research supported in part by National Science
Foundation Grant GJ-l154X3 to the National Bureau of Economic
Research, Inc.
                                 Abstract          /

A   recursive algorithm for estluating linear models with both constant
and time-varying parameters is derived by rraxiization of a likelihood
function. Recursive formulas are also derived for derivatives of the
likelihood function; the derivatives are needed for numerical evaluation
of some parameters. Smoothing formulas are also derived. The esti—
rnation algorithm is compared with others for similar classes of models.
                               Contents




1. Introduction                            1
2. Problem   Statement                     2
3. Maximum Likelihood Estijiation          5
L•   Smoothed   Estimates of              17
5. Conclusion                             19
     References                           20
1. INTRODUCFION

       In recent years many economists and statisticians have observed that
  the traditional linear model with constant coefficients is inadequate for
  the description of several phenomena.      Individual effects across a
  population or systematically varying behaviour over time offer two examples.
  The literature on stochastic parameters has provided several arguments
  justifying the nonconstancy of parameters over different observations,
  the reader is ref erred to Rosenberg (1968) and Cooley (1971) among others.
  A survey of the existing literature can be found in Rosenberg (197 3a).
  The state of the art in estimation techniques for several stochastic

  parameter   models has been sununarized in Annals of Economic and Social
  Measurements, Vol. 2, No.     (October 1973).
        This paper. presents an algorithm for estimating stochastic variations
   in a model that is more general than the most general model that has been
  analyzed before, that of Rosenberg (197 3b).
       In Section 2 the problem that is solved is stated, in Section 3 the
  recursive formulas for the maximum likelihood estimators are computed,
  Section - L restates some well known results about smoothed estimates, and

  Section 5 gives a brief summary of the results.
                                               —2—




2.   PROBLEM STATEMENT

          The model    that will be analyzed is the following
               t       At
                              +
                                  Btt+st                              (1)

               t= 1,2,..., T
     where

               y - 2 x 1 vector of observations at time t
               a- rx         1 vector of constant parameters

                             1 vector of randomly varying parameters
               At,Bt
                       - (2.,xr) and (Qxk) matrices of observations of explanatory
                         variables at time t.
                   -        x 1 gaussian vector of random terms with the following
                       properties
                             0 t=1,2,h, T                             (2)

               E(c1!)=             Q
                                       ci 1= 1,2,...,T
     where Q   is a symmetric 2,x9. positive definite matrix,2o is a scale
     factor, and ii.. is the Kronecker delta.
          The random parameters will be assumed to obey a transition relation
     of the Markov kind
                   =
                       t—1 +      t-l                                 (4)
     where u are normal with           E(ut)   0 t=l ,   2,...,   T   (5)




         The apostrophe (') denotes transposition.
                                     —3—



            E(u.u!) =
               1]
                        ci2R..
                            1]    i,j=l,2,... ,T             (6)



where   R is a syrrmetric kxk positive definite matrix. The initial vector
  will be assumed constant but unknown.
    A rrodel like this can arise for exaule in the analysis of a cross-
section of tine series. Ros:enberg (1973b) analyzed a model like this
with the exception that he did not consider the constant term c. The
recursive formulas that he presented do not hold when such a vector of
constant parameters coexists with the vector of stochastic ones. A
particular case of model (1) with 2..=1, kl and Bt1 was analyzed by Cooley
(1971) and Cooley and Prescott (1973). However, their procedures are not
recursive and can quickly become uimanageable for large sample sizes.
Sarris (1973) has analyzed a particular case with At0 £.=l but his
procedure is also not recursive.
        The particular case of AO has also been analyzed extensively in
the engineering literature under the name of iKalman filters (see e.g.
Sage and Melsa (1971)). The difference between those models and the
special case of the one considered here (with Ao) is that in our case
the initial vector        is constant but unknown, instead of having a well
defined prior density. The case of constant l gives rise to what is
called the starting problem which proved quite troublesome until
Rosenberg (1973b) solved it correctly.
        We shall assume that the transition matrix 4   depends   on a vector,
of constant and unknown parameters         (whose location in is known).
                                               —4—




Furthermore     Q and   R will be assumed to contain vectors of unknown                para-
meters       and
                   0r   respectively (with known          locations    in Q arid R)   We shall

define

              0 (e       ,    o;    op                                      (7)

       It   is important to emphasize the assumptions that Q                and   R are both
positive definite. The most general model one would want to analyze in
this framework would be the following

             yt              +                                              (8)


             Yt=   4i

                     It-i
                              +
                                  ii_                                       (9)



where    all the previous assumptions on the             random   terms hold except that
             E(p..) =

with     R positive sernidefinite. It is clear that (1) is a special case

of this model. If                 is given a proper prior density then the results

of   the Kalman filter            literature   cover   this problem.     However, when
is   constant and unknown, a case          which     is most prevalent in     statistics

and econometerics,           the Kalnian algorithm does not hold and         Posenberg' s
(l973b) algorithm holds only if R is positive definite. The general

problem     of arbitrary positive semidefinite R has not                  as yet been solved.

       The   complete problem is now restated.

Problem.       Estimate c,$1,ci 0 and $2             $,... 'T given the data yt,At,Bt
               t=l,2, :,T.
                                                       —5--




          where

                              At+tt+ct                                 (10)



                              t-l +          t-1                       (11)


3. MAXIMUM LIKELIHOOD ESTIMATION

          In this section we shall derive a recursive formula for the likeli-

   hood   function.        Let us make the following definitions
                                                                       (12)
                            {y±
                      =
                  p         {ct,i, a2, e}                              (13)



                   i - {Ai' AL+i'"'A}
                  A=                                                   (l'4)
                                    j
                  B   a     {B1,B.1,.. . ,B.}


          Then the        likelihood of given the data          is
   L(p;,4,B)
                      =
                                                   i          , B,p)   (16)




   where

                  p(y11y10, A, B, p) = p (y1A1,B1,p)                   (17)



3.1 Computation of p(y y1
                                  t.-l       t t         p)
                                         ,
                                             A1, B1,
          In this subsection we shall compute one of the factors of (16).

   In the process we shall also derive the Kalman filter for
                                                —6—



                                      t t
                           t—1
                                  A1, B1,i) =      p(yI,yt-l,A1,B1,B1,p)
                                                              t t t

                     t-1 t t                           t t t
                     y1 ,A1,B1,p) p( y1,A1,B1,i)
                     t-l t t
              pt Y1 ,A1,B1,i)                                             (18)

Let us     assume   that the                           t-1 t t
                                  density p( y1 ,A1,B1,p) is normal with mean

            and   covariance matrix             -•       From (10) we obtain


                                                                 e         (ytt_Btt)Q

              (yt_At _B)}                                                  (19)


      We thus obtain that the left hand side of (18) is equal to

                     1
                    k+
              (2n)—-- c
                            k+2
                                  Q
                                      ;5
                                                        e-       2c
                                                                                  tt
                                                                      [ (y -A -B )Q
                                           MtIti

                                 +
                                      (t_t1t_i)Mt_i                                    (20)



By   rearrangement of the         terms     i,nside    the brackets of the.exponent in (20)

we   obtain the following equality.
              ttYt                Q'(yt ta_Bt)

                                                   +
                                                          +
                                                              tt i-i     Mtti           i-i
                           tt
              (Q +                                                         (21)
                     BtMttiB)1              (yt_Ata_Btt

where

                                  +
              MI4M;t1                 Bt                                [Q+BtMtiiBJ'
                         BtMtIti                                           (22)




     Two   bars      denote      determinant.
                                                  -.7-.




                             Mtt[Mti tIt-l +              t
                             +                                                                   (23)
                     tIti              i B (Q+BtMtJtiB)'(yt_Ata_BtIti)

      The   formulas (22) - (23) are with         minor variations the ones known            as
      the Ka:Lmmn filter in the engineering literature.


             It is now easy to see fnm (18) and (21) that                                        is
      normal with mean                 and covariance matrix a2Mt!t given in (22) and
      (23) respectively. Consequently p(ytIy',A,B,p) is also normal with
      mean
             Aa ÷ BttI             and   covariance matrix equal to ci
      along with the eth'apolation formulas
                                                                            2
                                                                                (Q+BtMtI   t-l
                     t÷ltt       tJt                                               (2k)


                                                                                   (26)
                 Mt÷ilt       Mtit+R

      and   the initial conditions
                                                                                   (26)

                             0                                                     (27)




3.2   Computation of the. Likelihood Function
           After the results of Subsection 3.1 we can apply (16) to obtain
      an expression for the likelihood function. We obtain


             L (p;
                                       llp(yJy_l,4,Bt,p)
                                  1                                  1
                                                              exp {--—.-
                                                                           (y_A _Btti1)
                 t1 (2n) 2JQ+BtMtltiBj
                                                         —8--




                  (Q+BtNtitiBP' (ytAta_t tIt-1

                                                                    e -
                                      T
                                                                            _1(Yt_A_BttitiY
                                                                            2a
                  (2)TQ/2T 1tMt ItiB!

                                                                                               (28).
                  (Q+BtMt!tlB)' (yt_Ata_Bttit_i)}


             For maximum likelihood           estimation        of p, this quantity or its logarithm

      must   be maximized   with respect to c,,cr2 and 8.



3.3   Estimation of Cr,1,    cY




           In this section we present recursive estimators of a and and an
      estimator for 02. It can be easily seen from (22) — (27), that M.
      does not depend on  ,a or cr, and that I3                           depends linearly on both
        and a. Let us thus denote 1
                                          0         a+                                         (29).
                  sJk      slk                sjk           sik

             We can irrirnediately see from (26) that p1 o 0                      0                   (30)
                                                                              ,          0 E
      (Note that the 0's. and E's are kxr and kxk matrices respectively).

      Using the formulas     (23)     and      (2)   we can      obtain   recursive relations for

      p, 0,     and .   They   are the         following.

                                                                                               (31)
                  't+1lt

                                  ®                                                            (32)




                    t+lIt tt                                                                   (33)
                                                 —9—




           tIt
                    =
                            It-1 +Mtlt-1 B          (Q+BtMtlt_i           (yt_Btptit_i)



           o                 o         —
                                           MtiiB(Q+BtMtit_1B)' (At+Bto tIt-l

                                   —
                                       Mt t_i (Q+BM11Bp BtE tlt-i                           (36).


These   formulas are identical to those of Rosenberg (1973b) with the

addition of (32) and (35).

     We   rewrite the exponent of the likelihood function in                    (28) as follows


            T
           1t_Ata_Btt i-i                      (Q+BtMti    _1c (yt_Ata_Yt it-i
            T
                                   iti     —
                                               Bt 0    tt-l a _BtEtItii) (Q+BtMtItiBY'
            (yt__BtPtlt_i_Bt 0 tlt-l a_Bttitii)
            T
                                                                                +
          tlt_BtPtIt_1 (Q+BM11Bp yt_BttIt_i


where
            a'FTa
                        +
                             i   HTl + 2a'             —   2
                                                               a T 2i r
                    T
            F =
                T
                    E
                            (At+B i-i' (Q+BtM±iiB
                                                                    )_l ( A+B       0            (38)
                                                                                        tlt—l)

                    T
                                                                                             (39)
                    t=l          k-l       (Q+BtMtiiB) BtEtIti

                    T
                        E                                                                    (Lo).
                    t=l (At+Bt 0 tit-i (Q+BtMtIt_i BPBtEt,t_i
                                                       —10—




                                             e
            fTF   E(A+B                                  (Q+BtMtiisY' (yt_BtPtit_i) (41).



            b itQ+BtMtIt_lBP' (ytBtt i-i                                                (42).


     Maximization             of the likelihood function with respect        to a and

is equivalent to minimization of the quadratic form in (37). We take
the derivatives of that expression and set them equal to zero.                    We

obtain   the following two equations:
            FTa
                  +                      =                                              (43)

                                - hT         = 0                                        (44).

or

            FTGT                                   [T
                                I                  I                                    (45).



therefore
            a                       FT
                      =                                                                 (46).
                          •

                                    C                     bTj
or

                  (FT_'CY1                                                              (47)


                      (-                           ' (- F1                              (48).




     The only thing that needs to be examined is the inveribi1ity of
the matrix that prenultiplies the parameters in (45). We now state and

prove a theorem that will guarantee it. Before we do this we define the
following two matrices
                                             —11-




                    A1

                    A2

                                                                      (50)


                    Ar


                    B1

                    B24

            X2E     B342                                              (51)



                          T-1
                    BT
the theorem can now be stated.

Theorem 1 .      A sufficient condition for the invertibility of the matrix

                                                     is that
w rFT G]
                                defined in. ('45)              the matrix
    I




             X                  has   full   colunu-i rank.




Proof. Let us assinne that the matrix X [x : x2] has full, column rank.
Then it will be true that the matrix X ' X is positive definite and hence
invertible, arid also that the matrix X'i)X, where i is positive definite,
is also positive definite and hence invertible.
        We now consider the original data and substitute              as follows
                                              -12-



             tt-1 t-l =                        +
                                                   t-2    +
                                                              Utl
               tl  -
                          +              1.
                                         u.                                (52)
                              jzl


We   then   obtain

             y1        A1 ;+B21+c1

                       A2 a±B21+B+E2


                   =      ;+              + BT
                                                   (T_l_l_) +
or   more' compactly.

             y =
                    X1;+X21
                                +    v                                     (53).

Where    y    is   the vector of left hand side           observations   arid v is     the

vector of obviously non-spherical residuals. Let                    us denote the covariance
matrix of     v by V. Then it is well known              that if the matrix X [x1:
                                                                                   x2]
is of full column rank, the best              linear estimates of a and l are

obtained by Aitkents          estimator   which in this case is




                              (x'v
                                     1—1
                                     X)
                                           —1
                                         x'v y                             (5'i.).




The same     exact     expression for a              is   obtained if we write       the   like-
lihood function of         y from (53)        and maximize it with respect to; and
                                                      -13-




    However, this is exactly how we arrived at (5) only via a recursive

    formula.       Therefore, the normal equations Trust give            the same   solution
    in both cases. We conclude that the matrix W must be equal to the
    product of a nonsingular matrix and X I_i                 '—1
                                           V X. Hence, since X V X is

    invertible we are done.            JI.
         The maximum likelihood estimator of a2 is easy to find after the
    expressions for a and       are substituted in (28). By differentiating
    the likelihood function and setting the derivative equal to zero we obtain:

               -               T             A                           A            I'
                   a                             -
                         -_    1(yt_Ata              Btlltjti_Bt   0!_ a.BtEtIt j



                   (Q+BtMt jt_iBt)                           It_i_Bt e tlt-l   a_BtE t-
                   =
                                       t     Bp tIt-l'       (Q+BtMtItiB)' (YtBttlt_l)] -

                                   IFTGT             -l
                                                             T11
                   —
                       (j1h)       I

                                                             iJI               (55)

3.'i. Estimation of 8.:

         The estimators of                               a2 all depend on the
                                                     Mt It-i' a,1, and
    elements of the vector 0. The log-likelihood function for 0 is found after

    substituting out the estimated values of a2 ,cx, andy     It is given by
    the following expression.

                   log   L   (8; y, 4,       B)        - T log


                   (Q+BtMtitiB)'
                                        _lL_




              tTtIt-l          -
                                    (FT_G)             fT-hi   (_FT)T_
                  2f (FT_GT G) GTI
                         log   IQ+Bt Mtlt   B     +       (log T + log   2+l) (56)



The   above expression is                    analytically with respect
                               impossible to maximize

to 0. Numerical maximization of it will require the derivatives of this
expression with respect to the elements of 0. We now proceed to derive
recursive formulas for these derivatives. The procedure will             be to

consider each term of (56) separately and obtain           its derivatives with
respect to each component of 0. We          will bear   in mind the decomposition
of 0 into un1aown elements of ,             Q   and R respectively given in (7).

      The quantities whose derivatives will be needed are the following.

                         +
              tt-l'          BtMtiti,   Bt, Q+BtMtItThB!, Ft,,GT,fT,hT

      Pr'om   (38)—(42) it can be    seen that   the derivatives of FT,HT,GT,fT
and hT will be determined by the derivatives of H
                                                                     EtIt_i,
              +
                  Bt   Mt,ti B
So the quantities whose derivatives must be computed are
                         H
                                   Etiti, Q+BtMtti B and IQ+BtMtIt_iBI

for all 1< t< T

Refering.to
found.
                  formulas (31) and (3k) the derivative of
                                                                ttl    is easily
                                                   —15—




                                               tIt
                                                                     ::
                                                           +                     (57)

          :::+hlt



                                               +   MtIti        B (Q+BtMtitiB)' (yt_BtPt it—i
                                   ::tlt_1
                                      ij             i.j




          _Mtlti                 (Q+BMtit 1BY' Bt                         Bt   (Q+BiiB)'
                                                               ::




          (yt_Bttit_i) MtItiB (Q+BM                             !tlB' Bt tlt-1 (58)
                    t 1  -         can
The matrices                             be computed recursively by differentiating
                  't1J
(25) and (22).

          t÷ilt
          30
                             =      tt                                           (59)




                                           ÷
          ::tlt              ::tlt_1               :tlt_1


                                                                     ÷ Bt
           MtltiB (Q+BtMtIt_1BP1 (:q.
                                                                1]
                                                                                        B)
                   (Q+BMiiB)'
                                        —16-




                            -                      ____
            (Yt.Bt1Jtiti)       MtitiB (Q+BMi1BY Bttt_l                        (60).




           Mt Iti        is again generated recursively by differentiating

                         (25) and    (22).


                    -
                    -   A____
                          Lt                                       (61).
             r. •
              3_I
                            r..
                             11



                        'tt- +        Mt!t_l
            ::t                                    Bt (Q+BtMtt_i BY'(yt_Btpti) —



                                                tlt—1 B(Q+BtMt jt_1BP'
           •.MIB(Q+BM1B) Bt                    Or..
                                                 11


                            -                           1             1
                                                                               (62).
                                MttiB(Q+BtMtItiBP           Bt




            MtIti                 is recursively   computed by   (25) and   (22).
                                  The initial conditions for (57)-(62) are
              •11

                                  all zero.
From (32), (33), (35) and (36) it can be seen that the derivatives of

 °tIti, k- Q+BtMtItiB depend on the derivatives of ft—i and

          which we just analyzed.
M
     So   the last thing needed to complete the       analysis is   the computation of
the derivatives of the determinant quantity. Let X denote a mm natrix with
rows , x,..., x. Then the following formula can be proved easily by the
definition   of the determinant function (n is a scalar)
                                                —17—




                                        xl                    xl
          JxH    —
                       x1                   2
                                                               •2                     (63)
                       X2
                                        :
                               +        :         +...+         n
                                                              ____
                       xn               xn                     1]




If we denote   the rois of Bt      by       b 1=1,. . P and the rows
                                                          .                of   Mt jt_i by
   j=1,..., k, then the 1th element of BtMtItiB is given by

         B M      B} ..                     E    b. m b                               (6'4).
             tt!t-lt 1]                 n1             n j

It can thus be seen that the derivative of the determinant depends upon the
derivatives of
                 Mt Iti   which    we analyzed.

     These   recursive formulas     for         the derivatives are   lengthier than the
computation   required to compute the numerical               derivatives. The advantage,
however, is   that we obtain the exact derivatives as opposed to numerical

approximations. The exact tradeoffs must be evaluated through numerical
experiments which will be deferred to a later paper.
SM99JD

     The procedure desàribed in the previous section led to rriaximum
likelihod estimates of all the unknown constants i.e.                              and 0.

We also obtained the iraxijium likelihood estima.tor of T namely
                          ;2
             tIt(1;;                =              (0) +      T!T(0) c +               (66)
                                               -18-



For the solution to be complete we need estimators for                    1<t<T         that are
based on all the data from 1 to T and not only up to t as we have obtained.

We thus need "smoothed" estimates of                      e. the quantities              as opposed

to "filtered" estimates

     What we would like to obtain ideally would be the density

               p(2 ,,•.        'TIY    ). As we formulated the problem we have

available
                  T T                                TT           2
                             2                P(2,y 1Ift1 ,a,a
                                                           :.;
                                                               O)
               p(2jy1, 1,a,a ,O)
                                                 .

                                                              2
                                                                          (66).
                                                     T
                                              p(y1                ,O)


For the true values of the unknown conditioning parameters the posterior
                                                                                               as
density if normal.          We ta]ce the empirical Bayes view and estimate
the posterior mode of (66) conditioned upon the estimated values of
         2
i,c,Ci and          0.
     Sarris (1973) has shown formulas for the posterior mean and covariance
matrix of                but those formulas are not recursive and        hence of        limited
practical value.            Recursive fprmulas have been computed by Pauch et. al.

(1965)       and   are repeated     here   for completeness.



                   tIT        tJt+MtJt     Mtit t)_1 t+1ITtIt                           (67)


                                +                                                 $
               MtIT Mt!t                                     (÷1lTMt+l!t)


                                            (R+M'Y'                       (68)


The procedure stares at time T with TlT and MTIT computed by the

'ormulas       of Section 3.
     Another method that could be used would be to consider the density
                          T 1a2
and maximize it with respect to 1,u,a2,0 as well as                           .       The conditional

estimates of 13' conditioned on (3i,ct,G2, 0 would be the same as obtained
  above. However, the estimates of a,i,a2,O would not be the same.             This
  method has been used by Bar-Shalom (1973) and   Masiello   (1972). It is not

  clear at this   point how these two different methods   of estiiiating the

  unknown parameters compare.
5. CONCLUSIONS

        The contribution of this paper is two fold. First it presents a
  recursive solution to the geneml problem of estimating a cothination of
   constant and time varying parameters. Then it presents recursive fonnulas
   for the derivatives that are undoubtedly required to numerically maximize
   the likelihood function with respect to the parameters that appear
   nonlinearly.
        It is hoped that this type of approach will help the rrore general
   problem of estirrating corrinations of time varying and constant parameters
   which obey some non-random restrictions.
                                        —20—




       REFERENCES



(1) Bar-Shalom, Y., "Optimal Simultaneous State Estimation and Pa-
         rameter Identification in Linear Discrete-Time Systems,"
          IEEE Transactions on Automatic control, Vol. AC-17, No. 3,
          June 1973.

(2)    Cooley, T. F.., "Estimation in the Presence of Sequential Parameter
          Variation," Unpublished Ph.D. Dissertation, Department of
          Economics, University of Pennsylvania, 1971.
(3) Cooley, T.F. and E.C. Prescott, "The Adaptive Regression Model,"
        International Economic Review, 114, June 1973.
(4)    Masiello, R.D., "Adaptive 'bdel1ing and Control of Electric Power
          Systems," Ph.D. Thesis, Deparbnent of Electrical Engineering,
          Massachusetts Institute of Technology, October 1972.
(5) Pauch, H.E., F. Tung, and C.T. Striebel, "Maximum Likelihood Estimates
         of Linear Dynamic Systems , Journal of the American Institute
           of Aeronautics   and Astronautics, 3, August   1965.
(6)    Rosenberg, B., "Varying-Parameter Estimation," Unpublished Ph.D.
          Dissertation, Departmnent of Economics, Harvard University, 1968.
(7)    Rosenberg, B., "A Survey of Stochastic Parameter Regression,"
          Annals of Economic and Social Measurement, Vol. 2., NO.4,
           October 1973.

(8) Rosenberg, B., "The Analysis of a Cross Section of Time Series by
         Stochastically Convergent Parameter Regression," Annals     of
         Economic and Social Measurement, Vol. 2, No.4, October 1973.

(9) Sage, A.P. and     J.L.   Melsa, Estimation Theory with Applications to
           Communications   and Controls New York: McGraw-Hill,197l.

(10)   SarTis, A.H., "A Bayesian Approach to Estimation of Time Varying
           Regression Coefficients," Annals of Economic and Social
           Measurement, Vol. 2, No.4, October 1973.




                                  —
