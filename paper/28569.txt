                              NBER WORKING PAPER SERIES




   CONSISTENT LOCAL SPECTRUM (LCM) INFERENCE FOR PREDICTIVE RETURN
                            REGRESSIONS

                                      Torben G. Andersen
                                     Rasmus T. Varneskov

                                      Working Paper 28569
                              http://www.nber.org/papers/w28569


                    NATIONAL BUREAU OF ECONOMIC RESEARCH
                             1050 Massachusetts Avenue
                               Cambridge, MA 02138
                                   March 2021




We thank Guido Kuersteiner and participants at the 2018 conference in honor of Peter C.B.
Phillips at Yale University for helpful comments and suggestions. Financial support from
CREATES, Center for Research in Econometric Analysis of Time Series, funded by the Danish
National Research Foundation, is gratefully acknowledged. Varneskov further acknowledges
support from the Danish Finance Institute (DFI). The views expressed herein are those of the
authors and do not necessarily reflect the views of the National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.

© 2021 by Torben G. Andersen and Rasmus T. Varneskov. All rights reserved. Short sections of
text, not to exceed two paragraphs, may be quoted without explicit permission provided that full
credit, including © notice, is given to the source.
Consistent Local Spectrum (LCM) Inference for Predictive Return Regressions
Torben G. Andersen and Rasmus T. Varneskov
NBER Working Paper No. 28569
March 2021
JEL No. G12,G17

                                            ABSTRACT

This paper studies the properties of predictive regressions for asset returns in economic systems
governed by persistent vector autoregressive dynamics. In particular, we allow for the state
variables to be fractionally integrated, potentially of different orders, and for the returns to have a
latent persistent conditional mean, whose memory is difficult to estimate consistently by standard
techniques in finite samples. Moreover, the predictors may be endogenous and "imperfect". In
this setting, we provide a cointegration rank test to determine the predictive model framework as
well as the latent persistence of returns. This motivates a rank-augmented Local Spectrum (LCM)
procedure, which is consistent and delivers asymptotic Gaussian inference. Simulations illustrate
the theoretical arguments. Finally, in an empirical application concerning monthly S&P 500
return prediction, we provide evidence for a fractionally integrated conditional mean component.
Moreover, using the rank-augmented LCM procedure, we document significant predictive power
for key state variables such as the price-earnings ratio and the default spread.


Torben G. Andersen
Kellogg School of Management
Northwestern University
2001 Sheridan Road
Evanston, IL 60208
and NBER
t-andersen@kellogg.northwestern.edu

Rasmus T. Varneskov
Copenhagen Business School
Department of Finance
Solberg Plads 3
2000 Frederiksberg
Denmark
rtv.fi@cbs.dk
    1     Introduction and Literature Review
    Return predictability remains a hotly debated topic. In the early financial economics literature, the
    fact that short-horizon equity-index returns are largely unpredictable and return innovations highly
    volatile was seen as a manifestation of a no-arbitrage condition, consistent with no predictability and
    efficient markets; see, e.g., Fama (1970). This view started to change in the 1980's with the recognition
    that the relevant risk factors may vary over time and across the business cycle, implying that expected
    stock returns must exhibit time-variation to retain an equilibrium risk-reward trade-off.
        Theoretically, dynamic present value models stipulate that valuation ratios, such as the price-
    earnings, dividend-price, or book-to-market ratios predict future equity returns; see, e.g., Lettau &
    Ludvigson (2010) and Campbell (2018, Chapter 5). Similarly, equilibrium asset pricing models such
    as the long-run risk model (Bansal & Yaron 2004), dynamic disaster model (Gabaix 2012) or regime-
    switching CCAPM (Lettau, Ludvigson & Wachter 2008) suggests that returns are predictable by
    persistent state variables, such as the mean and volatility of consumption growth as well as the time-
    varying disaster recovery rate; see Neuhierl & Varneskov (2020). Nonetheless, the reliability of the
    empirical findings and the appropriate econometric methodology remains highly contentious. For
    example, the large-scale empirical study of Welch & Goyal (2008) concludes that skepticism regarding
    genuine out-of-sample predictability is warranted. From a methodological perspective, the primary
    complication is that many candidate regressors display a very high degree of persistence, inducing
    severe finite-sample biases under standard regularity conditions. These problems are only recently
    being addressed in a comprehensive manner, and the research continues unabated in the search for
    techniques that deliver better finite-sample performance and improved robustness.
        This section first highlights the pitfalls that arise when applying standard regression inference for
    return predictions with persistent regressors, before reviewing potential solutions that have adopted
    local-to-unity and related asymptotic settings. Finally, we explain how these ideas map into the long
    memory framework developed in this paper and clarify what our main contributions are.

    1.1    Standard Regression Inference
    To illustrate the key methodological points in a concise manner, we follow Phillips (2015) by initially
    considering the simplest form of a predictive regression, relating the future asset returns, yt , to a single
    lagged predictor, xt-1 , through a linear regression without an intercept,

                                           yt = B xt-1 + t ,            t = 1, . . . , n,                                 (1)

    where the innovations, t , follow a martingale difference sequence (mds) with respect to the filtration
    generated by the past observables in the system.1 Importantly, note that the notation and model

1
    These assumptions simplifies the exposition, but nothing of essence changes, if returns are allowed to exhibit weak
    dependence or to have an intercept. The mds assumption for the error term is consistent with the intuition that
    simple profitable strategies, unrelated to systematic risk exposures, should be absent in liquid financial markets. Weakly


                                                                1
 specifications in this section are only expository. We will formalize our setting in Section 2.
    If it is sensible to invoke standard assumptions, including weak dependence and stationarity of the
 returns and regressor, then it is straightforward to test for return predictability via the ordinary least
                                             n                n    2
 squares (OLS) estimator BOLS =              t=1 yt xt-1 /    t=1 xt-1 .   The null hypothesis of no predictability
 implies that B = 0, and a regular t-test for significance may be constructed. However, many relevant
 predictors are inherently stochastic and persistent. The impact of these features is studied by Stam-
 baugh (1986), who amends the predictive regression with an AR(1) representation for the regressor
 dynamics, so that the inference problem is embedded within a closed system. In Stambaugh (1999),
 this approach is utilized to analyze predictive return regressions. Specifically, ignoring the intercept,
 the regressor obeys,
                                       xt = n xt-1 + wt ,             t = 1, . . . , n,                                (2)
                                                                 2 ] =  2 , E[w 2 ] =  2 , and E[ w ] =  .
 for a fixed initial value x0 , where (t , wt ) is an mds with E[t             t      ww         t t    w
    Often, xt is assumed stationary, n =  < 1, even if the series is close to featuring a unit root.2
 Invoking results of Kendall (1954) and Marriott & Pope (1954), Stambaugh (1986) establishes the
 presence of a finite-sample bias, whenever the return and regressor innovations are correlated, that is,
 w = 0. Marriott & Pope (1954) show that this endogeneity bias asymptotically (n  ), to first
                    2 )(1 + 3)/n, if the mean of x is unknown a priori.3 For common predictors
 order, equals -(w /ww                            t
 like the dividend-price or the price-earnings ratio, the covariance w is inevitably non-trivial due to the
 joint dependence of y and x on the price innovation, while, as noted previously,  is often close to unity.
 Finally, because the return innovations typically are an order of magnitude larger than the innovations
                                 2 ), the bias may be substantial. This motivates Stambaugh (1986)
 in the regressor, inflating (w /ww
 to implement a bias-correction, which is applied frequently in the subsequent literature.
    Whether the endogeneity correction ensures satisfactory inference hinges on the quality of the
 asymptotic approximation to the distribution for the regression coefficient, BOLS . In this regard,
 the strong persistence of many candidate regressors points towards a potential "spurious regression"
 problem, although the absence of strong return correlation may alleviate this concern. Still, under the
 alternative hypothesis, B = 0, the mean return inherits the persistence of the (true) regressor, even if it
 likely will be disguised by the large return innovations. The theoretical justification for predictability
 implies we should pay close attention to this scenario. Indeed, through extensive simulations under
 carefully calibrated, strictly stationary, alternatives, Ferson, Sarkissian & Simin (2003) demonstrate
 that a spurious regression problem is present, if the mean return is strongly persistent.4 Moreover, by
 design, these simulations exclude correlations among the innovation series, so endogeneity and spurious
 regression features may constitute separate confounding challenges for inference in practice.

  dependent return innovations, uncorrelated with past innovations to the regressor, may be accommodated through a
  one-sided long-run covariance correction term for most of the discussion below.
2
  The subscript n in the autoregressive coefficient n is merely introduced for convenience here. It will be utilized in the
  exposition below, however, when we move beyond the strictly stationary setting.
3                                                                                                             2
  Alternatively, if the mean is known (zero in our setting), the bias is given by the smaller quantity, -(w /ww   )(2)/n.
4
  They further demonstrate that the spurious regression problem is absent under the null hypothesis of no predictability.


                                                             2
   The presence of a highly persistent mean return has implications beyond the need to adapt the
finite-sample inference accordingly. On the one hand, it improves our ability to identify the true
predictive relationship, as the signal-to-noise is enhanced, when we examine the "correct" regressor.
On the other hand, the concern about misleading inference is exacerbated by the high correlation
among many candidate regressors. If one is found significant, a number of others are also likely
to display predictive ability. This implies that a significant regressor is not necessarily the "true"
predictor, and the associated predictive relation should, at best, be viewed as providing an "imperfect"
or noisy indicator for the conditional mean. In the parlance of Pastor & Stambaugh (2009), we have
an imperfect predictor. It constitutes another feature we should seek to accommodate in the design
of suitable inference techniques. An additional implication, stressed by Ferson et al. (2003), is that
the existing evidence for predictability based on conventional inference procedures is subject to a
substantial "data mining" problem. Because many potential regressors have been examined and there
is a potentially significant inferential bias, many such predictors may appear significant ­ and by
extrapolation, so will many other regressors with which the original predictor is correlated.
   A common response to the problems noted above is to turn towards longer-horizon regressions, as-
suming the persistent signal would be more readily identified in that setting. However, the same issues
surface in this setting, along with additional complications introduced by the use of overlapping obser-
vations. In fact, Boudoukh, Richardson & Whitelaw (2008), and more recently Kostakis, Magdalinos
& Stamatogiannis (2015), find that no significant gains are obtained through this approach.

1.2   The Local-to-Unit Root Approach
The inferential problems associated with persistent regressors under the alternative, B = 0, have
spurred a large literature on techniques for improved asymptotic approximation schemes. A general
representation enabling an analysis for autoregressive coefficients near unity takes the form,

                                          C
                              n = 1 -        ,     C  0,      0 <   1.                               (3)
                                          n 
In particular, for C = 0, we obtain the regular unit root model, n = 1, while C > 0 and  = 1
yields the local-to-unit-root (LUR) specification, n = 1 - C /n, which ensures that the asymptotic
distribution captures the effect of having a root in the vicinity of unity, irrespective of sample size.
The LUR representation for autoregressions is first analyzed in depth by Phillips (1986), while early
developments for the predictive regression setting are provided by Cavanagh, Elliott & Stock (1995)
and Valkanov (2003), with the latter focusing on applications in financial economics.
   The LUR approximation to the asymptotic distribution in the near unit root scenario for the pre-
dictor has two important implications. First, the rate of convergence of BOLS increases to n, reflecting
the enhanced signal-to-noise ratio associated with unit root-style regressions. Second, inference gen-
erally becomes non-standard. Specifically, if w = 0, the interaction between the persistent regressor
and the lagged return residual generates a random endogeneity bias that depends on C . Under the


                                                   3
    LUR specification, the deviation of the autoregressive root n from unity shrinks at the same speed as
    the rate of convergence, rendering consistent inference for this coefficient infeasible. This implies that
    C is an unidentified nuisance parameter, and the asymptotic distribution for B has a discontinuity
    around unity, relative to the stationary case (n =  < 1), complicating inference in the absence of
    prior knowledge about the underlying strength of the regressor persistence.
       Various techniques have been developed to handle the above inference problem within the univariate
    regression setting. The most common procedure is the construction of Bonferroni bounds, combining
    the confidence intervals obtained across a range of relevant values for C , as explored systematically
    by Campbell & Yogo (2006). The main shortcoming of this approach, as noted in Phillips (2014),
    is the lack of robustness to the stationary scenario, n =  < 1. The latter scenario will entail
    spurious rejections of the null hypothesis of no predictability with probability approaching one, as
    the sample size increases. Instead, Phillips (2014) advocates reliance on the usual (asymptotically
    centered) estimate for the autoregressive coefficient under stationarity in the construction of the LUR
    Bonferroni bounds, as Mikusheva (2007) shows this leads to uniformly valid confidence intervals for
    n under a broad set of conditions. Moreover, the induced confidence bands are asymptotically valid
    and provide a good approximation to the ones obtained under stationary asymptotics.5
       However, even if the the robust Bonferroni approach provides sensible inference in the case of
    highly persistent regressors in univariate predictive regressions, it falters for multivariate predictive
    regressions due to the complications associated with handling of multiple distinct localizing coefficients.
    Moreover, this limitation is shared by many of the other alternative inference techniques for univariate
    predictive regressions, as reviewed by Phillips (2015). Consequently, in the next section, we turn to
    an approach that has proven successful, also for cases involving multiple predictors.

    1.3     The IVX Approach
    A tractable approach to multivariate predictive return regressions with highly persistent regressors and
    potential endogeneity was obtained only following the developments of Magdalinos & Phillips (2009),
    who introduce endogenous instrumentation designed to eliminate the nonstandard asymptotics arising
    from the choice of  = 1 for the autoregressive coefficient in the regressor dynamics. This is achieved
    by ensuring the instrument induces less persistence than the LUR and unit root scenarios, yet retains
    a sufficiently high degree of time series dependence to annihilate the potentially severe finite-sample
    endogeneity bias and to secure a relatively fast convergence rate, as explained below.

    1.3.1    Univariate IVX Estimation

    We continue to illustrate the main points within the univariate setting for brevity, noting, however,
    that all aspects of the discussion may be extended to multivariate systems. The key deviation in this
    section is that prior knowledge about the nature of the persistence of the regressor is not assumed,

5
    For another procedure to obtain near optimal tests in the univariate setting, see Elliott, M¨
                                                                                                uller & Watson (2015).


                                                               4
    as the IVX framework allows the regressors to contain a unit root, a LUR representation, moderate
    integration (C > 0, 0 <  < 1), and stationarity (C > 0,  = 0). Specifically, in this setting, the
                                                                                                 t
    IVX procedure obtains valid inference by generating an instrument for xt =                   s=1 xs    directly from
    the series itself through a filter that ensures a mild reduction in the degree of persistence,

                                  t
                                         -s                         Cz
                         zt =          t
                                       nz xs ,        nz = 1 -          ,   0 < z < 1,      Cz > 0.                    (4)
                                                                    n z
                                 s=1

    When z is chosen below, but near, unity, zt is at most mildly integrated, and its dynamics is governed
    exclusively through deliberate choices of Cz and z , which may, thus, be designed to generate a desir-
    able limit distribution.6 The IVX estimator is, then, simply the standard IV estimator, with zt serving
                                n               n
    as instrument, BIVX =       t=1 yt zt-1 /   t=1 xt-1 zt-1 . In the unit root and LUR      scenarios, the estimation
                         n               n    2
    error for OLS,       t=1 t xt-1 /    t=1 xt-1 will have asymptotically dependent          numerator and denomi-
    nator, generating a non-standard limiting distribution. In contrast, the lower degree of dependence
    associated with the moderately integrated IVX instrument is sufficient to ensure asymptotic indepen-
    dence and a tractable limit distribution, as shown in Phillips & Magdalinos (2007). Specifically, letting
                                                                                                       D
    the errors obey a mds, then, under suitable regularity conditions, n(1+z )/2 (BIVX -B ) -        2 ).
                                                                                             M N (0, IVX
                             2 , is generally stochastic, if the IVX instrument is moderately integrated,
    The asymptotic variance, IVX
    but a feasible, consistent estimator may readily be constructed using the standard linear regression
    approach, as detailed in Phillips (2015), and a standard t-test may be constructed. Consequently, the
    IVX instrumentation restores standard inference for return regressions, in cases where the predictor
    possesses an unknown degree of integration and may be an I (1) or LUR process.
       The main cost of the IVX approach is the lower rate of convergence, n(1+z )/2 , compared to n for
    the I (1) or LUR scenarios. This suggests picking a value for z near unity, while still ensuring a finite
    sample performance, that avoids mimicking the nonstandard unit root asymptotics. The extensive
    simulation evidence in Kostakis et al. (2015) demonstrates that picking z = 0.95 is sufficient to
    ensure reliable inference and induce good power properties in many typical settings.

    1.3.2    Multivariate IVX Estimators

    As noted previously, the IVX methodology can be generalized to return regressions with multiple
    predictors. However, this does require the imposition of additional assumptions. For example, Kostakis
    et al. (2015) provide theory for the multivariate regressor case, but impose that the unknown localizing
    coefficient is identical for all regressors. That is, they can display memory characteristics ranging from
    strictly stationary to non-stationary unit root processes, but they all possess the identical degree of
    persistence. Given the range of predictors used in empirical work, including near-unit root valuation
    ratios, macroeconomic variables, lagged returns, and realized volatility measures, it is a very strong

6
    To see this, note that zt = zt - (C /n ) t        t-s
                                                 s=1 nz xs-1 , where zt = nz zt-1 + wt , implying zt equals zt , except for
    a term that is asymptotically negligible. The notion of moderate deviation from unity was introduced by Phillips &
    Magdalinos (2007) to capture slightly wider deviations from a unit root than accomplished through LUR specifications.


                                                              5
    requirement. Phillips & Lee (2016) show that results can be obtained for mixed localization coefficients
    on the regressors, including the presence of both moderately integrated and moderately explosive
    regressors, but their general setting does require imposition of various bounds on the size of the IVX
    parameter z relative to the set of (unknown) localizing coefficients for the regressors, which does
    not include the strictly stationary case. Likewise, non-trivial conditions must be imposed on the
    specification of the linear set of restrictions imposed on the autoregressive coefficient matrix for the
    usual multivariate Wald test. Although their findings, combined with the Monte Carlo results in
    Kostakis et al. (2015), suggest that the IVX ultimately can deal with multiple regressors possessing
    mixed and wide ranging degrees of persistence and long run properties, a fully unified theory is still
    not established, as explicitly discussed in the concluding section of Phillips & Lee (2016).
       Besides these caveats, Xu (2020) points to the issue of potential cointegration among the multiple
    regressors employed within a predictive return regression. This can easily arise, especially if more
    than one of the typical valuation ratios are used, as they all represent scaled versions of the stock
    price level.7 Xu (2020) proceeds to show that the Kostakis et al. (2015) approach can be robust to an
    unknown degree of cointegration among the regressors, but it requires a strong assumption, namely
    that the regressors are "perfect" in the sense of Pastor & Stambaugh (2009).

    1.3.3   Extensions and Related Inference Principles

    The IVX principle induces tractable inference procedures within highly persistent regression systems
    through the use of instruments that proxy the original predictors, but are engineered to display a
    lower degree of persistence. This bears a resemblance to prior insights, noting that asymptotic normal
    inference will obtain for parameters expressed as coefficients on stationary regressors, even within
    I (1) systems, see, e.g., Park & Phillips (1989) and Sims, Stock & Watson (1990). The same line of
    reasoning inspired the idea of adding lagged regressors and/or regressands to linear regression systems
    in settings, where there is uncertainty about the orders of integration among the variables. For
    example, if a specific regressor is assumed to have a root close to unity, one may include an additional
    lag of this persistent regressor or, alternatively, its first difference, as an additional regressor.8
       The idea of variable addition has been adopted for predictive regressions with unknown degrees of
    persistence for either the regressand, the regressors or both. Breitung & Demetrescu (2015) compare
    the size and power properties of IVX and related variable addition techniques in a LUR setting; Ren,
    Tu & Yi (2019) adopt a similar setting with potentially strongly dependent regressors and add an extra
                                                                             
    lag of all regressors to obtain the slower, standard rate of convergence, n, along with 2 -distributed
    Wald tests. Likewise, Liu, Yang, Cai & Peng (2019) consider univariate predictive regressions, where
7
  In fact, Lettau & Ludvigson (2001) directly employ a theoretically motivated cointegrating relation to generate a predic-
  tive regressor, the so-called cay variable, involving aggregate consumption, income and wealth.
8
  The point is illustrated in Hamilton (1994, Chapter 18) for scenarios subject to potential spurious regression issues in a
  unit root setting, while Choi (1993) explores inference in AR systems with I (1) processes. These procedures are studied
  more broadly for inference in possibly (co-)integrated VAR systems by, e.g., Toda & Yamamoto (1995) and Dolado
  & L¨ utkepohl (1996). Moreover, Bauer & Maynard (2012) show how an infinite order VAR system can accommodate
  unknown strong persistence in an additional set of forcing variables via the same type of variable augmentation.


                                                             6
the regressand cannot be stationary under the alternative of predictability, if the regressor is strongly
dependent. They augment the regression with the first-differenced predictor and an additional lagged
predictor, and then conduct inference through an empirical likelihood approach, obtaining standard 2
distributed test statistics. This particular method is, however, quite unwieldy in multivariate settings.
Moreover, Lin & Tu (2020) study the univariate regression case, where the regressand is strongly
persistent, while the (persistent) predictor is imperfect, so that the persistence spills over into the
regression residuals. They propose a robust inference strategy by including both a lagged regressand
                                                                                     
and predictor as extra regressors. Not surprisingly, this generates the usual rate of n convergence
for the slope coefficient, allowing for regular inference procedures. Their results also hold if the system
displays ("perfect", in the sense of Pastor & Stambaugh (2009)) cointegration. Finally, Georgiev,
Harvey, Leybourne & Taylor (2020) develops a fixed regressor wild bootstrap test for whether the
predictive regression is invalid in a setting where the regressors are persistent and, possibly, imperfect
such that the persistence spills over to the residuals, leading to potential spurious inference.

1.4   Final Observations: Bridging the Gap to LCM
In summary, a variety of econometric issues continue to complicate the analysis of multivariate predic-
tive return regressions. The predictors may possibly be "imperfect", and they may display unknown
and differing degrees of persistence. The issue of imperfect predictors looms particularly large, as this
feature, intuitively, provides a realistic characterization of the type of scenario encountered in practice.
To alleviate this issue, it is tempting to include a large set of regressors to maximize the ability to
span the most persistent conditional mean component of the regressand. However, currently, there is
no uniform approach that can handle inference for the multivariate, imperfect predictor case.
   In our previous work Andersen & Varneskov (2020), we develop a different asymptotic framework for
analyzing predictive regressions within persistent systems. Specifically, we assume that all variables
are fractionally integrated of potentially different orders, and that the regression may, or may not,
feature cointegration. Let L and (1 - L)d be the usual lag and fractional differencing operators, then,
drawing parallels to the predictive systems (1)-(4), we stipulate a predictive relation of the form,

                         yt = B (1 - L)dx -dy xt-1 + t ,   (1 - L)dx xt-1 = ut-1 ,                      (5)

where ut-1  I (0) is weakly dependent, and t  I (dy - b) with 0  b  dy captures the possibility
of cointegration (when b > 0). As a result, it follows that yt  I (dy ) and xt-1  I (dx ) may exhibit
either weak or strong dependence by allowing their fractional integration orders to fall within a wide
range 0  d < 2, for d = {dy , dx }. Importantly, the framework in Andersen & Varneskov (2020) is not
confined to univariate predictive regressions (with trivial means or initial values), but accommodates
diverse persistence (i.e., d's) among the predictors, thus providing a flexible setting to analyze systems
with various financial and macroeconomic variables. This feature corresponds to having different
localization coefficients in the LUR setting (3). Andersen & Varneskov (2020) propose a two-step Local


                                                     7
speCtruM (LCM) approach that delivers asymptotically Gaussian inference, regardless of persistence
of the variables and cointegration in the predictive relation, by first stripping the persistence of the
variables using a consistent estimate of their integration orders and subsequently by applying a robust,
medium band least squares (MBLS) estimator. However, while tackling the issue of "spurious" inference
in persistent systems, they do not consider scenarios where the predictors may be "imperfect".
   Hence, in this paper, we extend the framework in Andersen & Varneskov (2020) to further allow for
imperfect regressors (in the spirit of Pastor & Stambaugh (2009)) that may exhibit general forms of
endogeneity. That is, we tackle empirically relevant scenarios where the regressors may be imperfect,
persistent and endogenous, for which, as discussed above, there is currently no uniform solution in the
literature. However, as the LCM procedure critically relies on consistent estimation of the fractional
integration orders of the variables, this problem turns out to be particularly difficult for return regres-
sions, since the signal-to-noise ratio of the conditional mean return to its innovations is too "low" for
standard univariate time series techniques to detect (strong) serial dependence in finite samples. We
overcome this issue by proposing a new rank testing procedure, that allows us to discriminate between
the "imperfect" and "perfect" regressor scenario and to determine the persistence of the conditional
return mean. Our, important, identifying condition is that the integration of the returns belongs to
the set of integration orders from the multiple candidate predictors. That is, the set of predictors
have been chosen "sensibly". If this is the case, the procedure can verify that the conditional mean is,
indeed, persistent and distinguish between inference scenarios. Once we have determined the return
persistence, we may implement the two steps of the LCM procedure, without modification.
   We establish the asymptotic properties of our new rank test and rank-augmented LCM procedure
in an endogenous, imperfect, and persistent regressor setting, demonstrating that the asymptotic
distribution theory is Gaussian, regardless of the inference scenario; stationary versus non-stationary
persistence and perfect versus imperfect predictors. Moreover, we examine the finite sample properties
of predictability tests using OLS, IVX and LCM procedures. Specifically, we find that OLS and IVX
may suffer from considerable size distortions in our long memory setting, thus providing "spurious"
inference. Importantly, we also show that our rank selection procedure has considerable finite sample
power to detect a persistent conditional mean return, and that our rank-augmented LCM procedure
is (almost) as efficient as if we had known the true persistence of the system ex-ante, i.e., as an oracle
implementation of LCM. Finally, in an empirical application to monthly S&P 500 return prediction, we
find corroborating evidence that returns contain a fractionally integrated conditional mean component.
In addition, by applying the rank-augmented LCM procedure, we find key state variables, such as the
price-earnings ratio and the default spread, to possess significant predictive power for future returns.
   The paper proceeds as follows. Section 2 introduces the setting, draws parallels to the imperfect
regressor model of Pastor & Stambaugh (2009) and describes the LCM procedure. Section 3 provides
our new rank test and rank-augmented LCM procedure as well as examines their asymptotic proper-
ties. Section 4 contains the simulation study, and Section 5 provides the empirical analysis of return
predictions. Finally, Section 6 concludes. The Appendix contains additional theory and proofs.


                                                    8
    2     Predictive Returns Regressions with Persistent Variables
    This section introduces a predictive regression framework for asset returns, where all the variables
    may exhibit fractional integration of potentially different orders. The framework is inspired by the
    persistent economic systems studied by Andersen & Varneskov (2020) as well as the predictive system
    for expected returns with imperfect predictors developed by Pastor & Stambaugh (2009). Finally, we
    motivate and review the Local speCtruM (LCM) approach, introduced in the former.

    2.1    Predictive System and Assumptions
    Suppose we observe a (k + 1) × 1 vector Zt = (yt , X t-1 ) at times t = 1, . . . , n, where yt contains the
    asset returns and X t-1 is a vector of candidate predictors, which has a multi-component structure,

                                  X t-1 = xt-1 + ct-1 ,          xt  cs ,    for all t, s,                           (6)

    with xt-1 capturing the most persistent signal, and ct-1  I (0) being mean-zero and collecting either
    measurement errors, additional weakly dependent components embedded in the variables, or both.
    Moreover, let us define zt = (yt , xt-1 ) , which is assumed to obey a Type II fractional model,

                                             D (L)(zt - µ) = vt 1{t1} ,                                              (7)

    where µ is a (k + 1) × 1 vector of nonrandom, unknown finite numbers, capturing either the means or
    initial values of zt , the vector process vt = (et , ut-1 ) is weakly dependent, and,

                                                                                         
                                                                                                   (i - d)
            D (L) = diag (1 - L)d1 , . . . , (1 - L)dk+1 ,       with (1 - L)d =                             Li ,    (8)
                                                                                                 (i + 1)(-d)
                                                                                        i=0


    where ( · ) is the gamma function.9 In this setting, in which all variables may exhibit high degrees
    of persistence, the predictive relation between yt and the observable regressors X t-1 will be defined
    through the weakly dependent components of the persistent signals. Specifically, we assume,

                                       (b)
                         et = t-1 + t ,      t-1 = B ut-1 + t-1 ,           ut  s ,       for all t, s,              (9)

            (b)
    where t       = (1 - L)b t for some constant b  0 and t  I (0), and with t-1  I (0). Importantly,
    however, by combining the relations (7) and (9), this is tantamount to a balanced predictive model
    for asset returns,
                                                             (-d )
                                yt = a + B Q(L)xt-1 + t-1 1 + t ,            t = 1, . . . , n.                      (10)

    where Q(L) = Dx (L)(1 - L)-d1 , with Dx (L) being the k × k lower-right submatrix of D (L), a =
                                                                                    (-d )
    µy - B Q(L)µx for µ = (µy , µx ) as well as t = (1 - L)b-d1 t and t-1 1 = (1 - L)-d1 t-1 .

9
    Formal assumptions on the components of the system are stated below.


                                                             9
   The predictive system (6)-(10) encompasses most multivariate fractionally integrated systems in
the literature, in addition to features specific to the problem of predicting asset returns. To see this,
suppose ct-1 = 0 and t-1 = 0, t, as well as 0  b  d1 , then the most persistent components of
the explanatory variables are directly observable, the predictive relation is well-defined and balanced,
and the system may (b > 0) or may not (b = 0) feature (fractional) cointegration. By relaxing these
restrictions, however, the system more accurately describe the inferential issues surrounding return
regressions. In particular, ct-1 is included to accommodate endogeneity, multiple components and
measurement errors in the regressors, rendering their signals latent, t-1 captures the possibility that
the predictors may imperfectly describe the conditional mean, and, by letting b = d1 , the return re-
gression have a weakly dependent innovation that may dominate the persistent signal in finite samples.
We will detail these points, provide examples and draw parallels to the extant literature, particularly
to Pastor & Stambaugh (2009) and Andersen & Varneskov (2020), in the next section.
   Before proceeding, we impose some formal structure on the system. The conditions mirror those
imposed by Andersen & Varneskov (2020) and the assumptions for the semiparametric fractional
cointegration analyses in, e.g., Robinson & Marinucci (2003), Christensen & Nielsen (2006) and Chris-
tensen & Varneskov (2017), but with subtle differences due to the differing model features. To this end,
let "" signify that the ratio of the left- and right-hand-side tends to one in the limit, element-wise.
We then impose assumptions in terms of qt = (ut-1 , t ) and t-1 = (ct-1 , t-1 ) rather than vt , when
exploring the asymptotic properties for the LCM procedure below.

Assumption D1. The vector process qt , t = 1, . . . , is covariance stationary with spectral density
matrix satisfying fqq ()  Gqq as   0+ , where the upper left k × k submatrix, Guu , has full rank,
and the (k + 1)th element of the diagonal, G , is strictly greater than zero. Moreover, there exists a
    (0, 2] such that |fqq () - Gqq | = O( ) as   0+ . Finally, let Gqq (i, k + 1) be the (i, k + 1)th
element of Gqq , which has Gqq (i, k + 1) = Gqq (k + 1, i) = 0 for all i = 1, . . . , k .
                                                                    
Assumption D2. qt is a linear process, qt =                         j =0 Aj t-j ,      with square summable coefficients
              2
  j =0   Aj       < , the innovations satisfy, almost surely, E[ t |Ft-1 ] = 0 and E[                   t t |Ft-1 ]   = Ik+1 ,
and the matrices E[ t        t t |Ft-1 ]   and E[   t t      t t |Ft-1 ]   are nonstochastic, finite, and do not depend
on t, with Ft =  ( s , s  t). There exists a random variable  such that E[ 2 ] <  and, for all c and
some C , P[ qt > c]  C P[| | > c]. Finally, the periodogram of                    t    is denoted by J ().
                                                                                      ij
Assumption D3. For A(, i), the i-th row of A() =                             j =0 Aj e ,    its partial derivative satisfies
  A(, i)/ =           O(-1   A(, i) ) as            0+ ,   for i = 1, . . . , k + 1.

Assumption C. Suppose t-1 = t-1 1{t1} is a mean-zero (k + 1) × 1 vector satisfying the same
Assumption D1-D3 as ut-1 , except that it has t co-spectrum f ()  G , as   0+ , where the
constant vector G may have non-zero entries. Moreover, let ut  s for all t, s  1. Finally, if
the i-th element of the vector t-1 is trivial, that is, if t-1 (i) = 0 for all t  1, then the co-spectrum
condition G (i) = G (i, g ) = G (g, i) = 0 for g = 1, . . . , k + 1, is naturally also required.


                                                              10
Assumption M. Let 0  d1  1 and 0  di  2 for all i = 2, . . . , k + 1. Define dx = min(di ; 2  i 
                           ¯x = max(di ; 2  i  k + 1), and let d > 0 and b = d1 .
k + 1), d = min(d1 , dx ), d                                    x

   Assumptions D1-D3 are standard in the literature studying fractional (co-)integration. Specifically,
D1 and D3 impose a rate of convergence for the spectral density fqq () as   0+ , which depends
on the smoothness parameter              (0, 2]. In addition, D1 requires full rank of ut-1 and it being
locally exogenous to t as        0+ ,   but not global exogeneity. Finally, condition D2 specifies linearity,
martingale and moment conditions for qt , allowing for general multivariate dependence among the
variables, thus accommodating flexible lead-lag and predictive structures.
   Whereas D1 allows the latent predictive signals, xt-1 , to exhibit mild endogeneity (as   c > 0)
through ut-1 , Assumption C lets the observable explanatory variables exhibit stronger forms of endo-
geneity, that is, to display non-trivial correlations with the innovations to asset returns. These corre-
lations are captured via the co-spectrum between the less persistent component (and/or measurement
errors) ct-1 and the innovations t , which, furthermore, may both be non-trivially correlated with the
"conditional mean errors" from the, possibly, imperfect predictors, t-1 . This treatment of endogenous
predictors is similar in spirit to Stambaugh (1999) and Pastor & Stambaugh (2009).
   Assumption M imposes a mild structure on the memory of the system. Specifically, we restrict the
persistent component of returns to maximally exhibit unit root persistence, whereas the explanatory
variables can be explosive, di > 1. In general, however, the assumptions accommodate flexible per-
sistence among the variables; if 0 < di < 1/2, the variable is (asymptotically) stationary with long
memory; if di  1/2, the variable is non-stationary, but has a well-defined mean for di < 1. This
flexibility is particularly useful for characterizing the properties of multivariate predictive systems,
whose components are very persistent, yet display different degrees of persistence, which is often the
case for applications with multiple financial and macroeconomic variables.
   Finally, we impose b = d1 , which implies t = t and, consequently, that the return prediction
model exhibits (fractional) cointegration, if t-1 is trivial. Hence, we equip returns with a persistent
conditional mean and weakly dependent innovations. This is consistent with a vast literature, that
finds limited serial correlation in return innovations; see, e.g., the introduction for references.

Remark 1. Assumption M stipulates that dx > 0, i.e., that all predictors have long memory. This
condition is necessary, when the requisite elements of ct-1 are non-trivial. That is, we obtain iden-
tification of the persistent predictive signals through differences in memory relative to their weakly
dependent components (and by using the LCM approach). We can accommodate cases, where di = 0,
when ct-1 (i) = 0, t  1, which is analogous to assuming exogeneity in OLS settings. Our assumption
is reminiscent of the approach in Pastor & Stambaugh (2009), who also, as will be explained below,
utilize memory differentials to identify the conditional mean properties of asset returns, but within a
more standard weakly dependent setting. Importantly, our empirical application in Section 5 illustrates
that popular return predictors from recent macro-finance models, e.g., Bansal, Kiku, Shaliastovich &
Yaron (2014) and Campbell, Giglio, Polk & Turley (2018), exhibit strong persistence and may be


                                                       11
     characteristized as either stationary or non-stationary fractionally integrated processes. Hence, de-
     spite Assumption M deviating from the literature by requiring fractional integration, rather than weak,
     local-to-unity or I (1) dependence, this assumption has a solid empirical foundation.

     2.2    Return Regressions: Dynamics and Implications
     The predictive system (6)-(10) has several distinct features. First, the regression model is balanced,
     irrespective of the forecasting prowess of the predictors, that is, yt  I (d1 ) under both H0 : B = 0
     and HA : B = 0. The null hypothesis, H0 , allows for the scenario, where the regressors imperfectly
     span the conditional mean, i.e., t-1 = t-1 = 0. Under the alternative hypothesis, HA , the fractional
     filter adjusts the persistence of the "latent" signals, xt-1 , to ensure regression balance. If the system
     is balanced, then Q(L) = Ik , a k -dimensional identity matrix, and the adjustment is trivial. To
     further appreciate the mechanics of the fractional filter, consider a scenario where the conditional mean
     component has d1 = 0.8, thereby being nonstationary with a well-defined mean. Then, if we observe
     an explanatory variable with dx = 1.8, the regressor must be transformed to match the persistence of
     the conditional mean. In this case, the predictor requires a simple difference transformation.
        Second, even under HA , the regressors may be imperfect, that is, t-1 may be non-trivial. This
     captures a scenario, where the predictors contain information about the conditional mean, but fail to
     fully span its variation. In contrast, if the predictors are "perfect", we have t-1 = B ut-1 .
        Third, the system accommodates endogenous regressors through, ct-1 , which is independent of the
     persistent signal, xt-1 . To motivate this model feature, let us draw a parallel to the long-run risk
     model of Bansal & Yaron (2004), where persistent shocks to the mean and volatility of consumption
     growth determine the conditional equity premium. In our setting, the persistence of the risk factors is
     captured by fractionally integrated processes instead of persistent first-order autoregressive (AR) ones,
     whose half-lives have been stipulated to exceed 52 months (coefficients of 0.979 and 0.987). Moreover,
     Bansal & Yaron (2004) assume, that these shocks are independent of the innovations to consumption
     growth. In contrast, we can accommodate a second component in both factors, that are less persistent,
     but allowed to exhibit non-trivial correlation with the return innovations. These components contain
     no information about the conditional equity premium, but facilitates richer system dynamics.10
        Fourth, the model facilitates non-trivial correlation between unspanned component of the condi-
     tional mean, t-1 , and the observable explanatory variables (again, through ct-1 ) as well as with the
     innovations to asset returns, t = t . This allows for endogeneity through different channels.
        Finally, the model allows asset returns to have a weakly dependent component t , which may have a
 "large" volatility relative to the persistent conditional mean, thereby producing a "low" signal-to-noise
     ratio for the return regression and rendering predictability hard to detect empirically. This feature
     is consistent with a comprehensive empirical literature, that find limited return serial correlation, yet
10
     A multi-component structure of the conditional mean of consumption growth is consistent with the dynamic decomposi-
     tion in, e.g., Ortu, Tamoni & Tebaldi (2013), who show that consumption growth has a very persistent component with
     low volatility as well as a less persistent "error" component with high volatility. Moreover, multi-factor volatility models
     are used extensively in financial econometrics; see, e.g., Andersen & Benzoni (2012) and many references therein.


                                                                 12
predictive power from highly persistent financial and macroeconomic variables; see, e.g., Welch &
Goyal (2008), Lettau & Ludvigson (2010) and the many references therein. Likewise, many prominent
asset pricing theories, e.g., the present value, long-run risk and dynamic disaster models, stipulate the
existence of a persistent conditional mean return with a "low" signal-to-noise ratio.
   Altogether, these features mimic the qualitative implications of the predictive system for asset
returns in Pastor & Stambaugh (2009), despite arising in our fractionally integrated setting rather
than their first-order AR economy. The following remark outline these similarities.

Remark 2. Pastor & Stambaugh (2009) analyze an asset return system with imperfect predictors,
whose components follow stationary AR(1) processes. Adapted to our notation, it takes the form,

                  yt = t-1 + t ,     t-1 = a + B X t-1 + t-1 ,
                  t = (1 - )µ + t-1 + wt ,          X t = (Ik - A)µX + AX t-1 + ut ,

where 0 <  < 1, the eigenvalues of A are inside the unit circle, and the innovation vector (t , wt , ut )
is i.i.d. Gaussian. The system features return predictability via the conditional mean (since  > 0),
endogenous regressors, and it accommodates imperfect predictors, when t-1 = a +B X t-1 . Moreover,
if the predictors are imperfect, this generates unspanned return persistence, as we obtain by inclusion
                   (-d )
of the component t-1 1 in equation (10). Finally, their key identifying assumption for B is 0 <  < 1,
allowing the persistent conditional mean to be disentangled from the noise. If this assumption fails,
they need exogenous regressors. It is analogous to assuming dx > 0 in Assumption M.

   The model (6)-(10) features four competing hypotheses for the return dynamics:

  (i) B = 0 and t-1 is trivial, t = 1, . . . , n; returns are not predictable.

 (ii) B = 0 and t-1 non-trivial, t = 1, . . . , n; returns are not predictable by X t-1 .

(iii) B = 0 and t-1 non-trivial, t = 1, . . . , n; returns are predictable, and X t-1 is "imperfect".

 (iv) B = 0 and t-1 trivial, t = 1, . . . , n; returns are predictable, and X t-1 is "perfect".

   The hypotheses (i) and (ii) imply that X t-1 possess no predictive power for returns, but they have
different dynamic implications; namely, returns are I (0) and I (d1 ), respectively. Moreover, the first
hypothesis stipulates, that returns are not predictable by any persistent regressor, whereas the second
allows for predictability with, however, the "wrong" set of predictors having been examined. The vast
empirical literature on return predictability and extensive theoretical developments (again, see the
introduction) suggest that, in many settings, we should focus on the null hypothesis given by scenario
(ii) rather than (i), especially if examining a set of predictor variables sequentially in single-regressor
models. In addition, hypotheses (iii) and (iv) also carry different dynamic implications. Specifically,
                                                                                             (-d )
both hypotheses imply yt  I (d1 ), but (iii) has regression errors, that are comprised of t-1 1  I (d1 )
and t  I (0) processes, while (iv) describes a fractional cointegration model with I (0) innovations.

                                                    13
        The hypotheses imply different inference regime for persistent variables, for which standard OLS is
     known to deliver spurious inference; see, e.g., Granger & Newbold (1974), Phillips (1987), and Tsay
     & Chung (2000). Estimation and inference is further complicated by the fact, that the particular
     scenario as well as the persistence properties of zt = (yt , xt-1 ) are unknown ex-ante. For example, if
     we know that yt and X t-1 = xt-1 form a fractional cointegration model (i.e., the signals are significant,
     observable and "perfect"), one may readily apply inference procedures such as Robinson & Marinucci
     (2003), Robinson & Hualde (2003), Christensen & Nielsen (2006) and Johansen & Nielsen (2012).
     Generally, however, we do not know, a priori, which of the hypotheses capture the inference scenario,
     i.e., whether the regressors are endogenous and/or the predictors are "perfect", and we need to estimate
     the persistence of zt , which is complicated due to the "low" signal-to-noise ratio for the returns.
        As exemplified in Remark 2 and the introduction, related issues have been examined in different
     predictive settings, assuming stationary first-order AR dynamics, (near) local-to-unity, unit root or
     locally-explosive persistence. In contrast, we assume a flexible long memory system with similar
     qualitative features, and we analyze the return predictability via the LCM approach. Moreover,
     compared with Andersen & Varneskov (2020), we allow for "imperfect" predictors and the simultaneous
     presence of endogeneity and cointegration.11 Hence, all subsequent results are new.
        Specifically, we provide a (cointegration) rank testing framework that facilitates discriminating
     between hypotheses (i)-(iv) and allows us to determine the persistence of the conditional mean asset
     returns. Moreover, we propose a rank-augmented LCM procedure to study return predictability.
     These are developed with hypotheses (ii)-(iv) in mind, that is, thinking about inference scenarios,
     where returns have a persistent mean component, and the predictors are either insignificant, imperfect
     or perfect. However, we emphasize that both our rank test and rank-augmented LCM procedure
     remain valid in scenario (i), and we provide comments regarding this case throughout.

     2.3    The Local Spectrum Approach
     The motivation behind the LCM inference and testing procedure is readily conveyed by considering
     decompositions of the spectral density for the observable regressors, X t-1 , and their co-spectrum with
     the asset returns, yt . Specifically, using that fxc ()  0, as   0+ , we may write,

                                                 -1
                       fX X ()  - 1
                                xx Guu xx + Gcc ,                                                                    (11)
                                                   -1      (-d1 )                          -1
                        fX y ()  - 1
                                 xx Guu B yy + fx                   () + fx () + Gc yy + Gc ,                        (12)

     for   0+ , where yy and xx are the complex conjugates of yy , respectively, xx , defined as,

                        yy = (1 - ei )d1 ,        xx = diag (1 - ei )d2 , . . . , (1 - ei )dk+1 .



11
     Andersen & Varneskov (2020) study the asymptotic properties of LCM approach in a general predictive setting. However,
     when examining the the effect of regressor endogeneity on the inference, they assume cointegration is absent.


                                                              14
These decompositions are intuitive. First, fX X () shares the multi-component structure of the ob-
servable regressors X t-1 , with the spectral density of the persistent signal dominating the frequencies
in the vicinity of the origin. However, the speed of divergence may differ across elements, depending
on the fractional integration orders of the regressors. Second, fX y () not only contains information
about the forecasting prowess of the regressors, B, the first term dominates the remaining ones at
                                                               -1
lower frequency ordinates as   0+ . Moreover, Gc yy + Gc captures an endogeneity-induced bias,
which may be persistent and even diverge (when d1 > 0) as   0+ , however, at a slower rate than
                                           (-d1 )
the first term. Finally, the co-spectra fx          () and fx () introduce sampling errors for estimators
                                                                        (-d )
of B, with their respective asymptotic orders differing due to t-1 1  I (d1 ) and t  I (0).
   In general, the (co-)spectral densities in equations (11) and (12) diverge with rates depending on the,
possibly, different integration orders of the predictors and asset returns. In contrast, the corresponding
co-spectral densities for the unobserved weakly dependent components of the predictive system, ut-1
and et , are,
                                                                    (d1 )
                     fuu ()  Guu       and fue ()  Guu B + fu () + fu     (),                                    (13)

which are both asymptotically bounded and convey similar information about B. This suggests that
inference based on ut-1 and et may circumvent issues regarding balance, degeneracy of point estimates
and spurious inference, motivating Andersen & Varneskov (2020) to introduce the LCM procedure,
which consists of two main steps. First, the procedure carries out fractional filtering of the observed
variables Zt = (yt , X t-1 ) to obtain an estimate of vt = (et , ut-1 ) . Second, it uses medium band
least squares (MBLS) estimation for robust inference. These steps are detailed next, together with
additional subtleties created by the specific problem of predicting asset returns.
   Step 1: Fractional Filtering. As we seek to retain flexibility, allowing for different estimators of
the fractional integration orders, we abstain from dedicating a specific estimator and, instead, assume
to have one available, di for i = 1, . . . , k + 1, that satisfies mild consistency requirements.

Assumption F. Let md          n be a sequence of integers where 0 <              1, then, for all i = 1, . . . , k + 1
elements of zt , we assume to have an estimator with the property,
                      
       di - di = Op 1/ md ,         and we then let,         D (L) = diag (1 - L)d1 , . . . , (1 - L)dk+1 .

   Assumption F is very mild, essentially only requiring existence of an estimator which, under ap-
propriate assumptions on equation (7), is consistent. However, since we accommodate both (asymp-
totically) stationary and non-stationary variables in Assumption M, the estimator must apply for a
wide range of di . Examples include the semi-parametric exact local Whittle (ELW), see Shimotsu &
Phillips (2005) and Shimotsu (2010), the trimmed ELW (TELW) by Andersen & Varneskov (2020),
and parametric (long) fractional ARIMA(p, d, q ) models using information criteria to determine the
short-memory dynamics; see, e.g., Hualde & Robinson (2011) and Nielsen (2015).




                                                        15
   Once we obtain the filtering matrix, D (L), the estimates for vt are,

                                      c
                                     vt  (et , (uc
                                                 t-1 ) ) = D (L)Zt ,                                    (14)

where uc
       t-1 = ut-1 + ct-1 , with ut-1 = Dx (L)xt-1 and ct-1 = Dx (L)ct-1 . Similarly, we define
vt  (et , ut-1 ) , which is the equivalent (albeit, unobservable) estimate of vt , without an endogenous
component in the regressors. We will, then, utilize frequency domain estimation to extract asymptot-
                                   c as for v . Moreover, we leave the mean, or initial value, of the
ically identical information from vt         t
variables unspecified at the filtering stage. Instead, we account for their residual impact on the mean
in a Type-II fractional model, D (L) µ1{t1} , in a unified manner during second stage estimation.
   Step 2: Medium band least squares estimation. We estimate and draw inference about B
                                                      c . To define the former, we let,
using a frequency-domain least squares estimator and vt
                                          n
                                   1
                     wh (j ) =                 ht eitj ,    Ihk (j ) = wh (j ) wk (j ),                 (15)
                                   2n    t=1

be the discrete Fourier transform (DFT) and cross-periodogram, respectively, where ht and kt are
generic (and compatible) vector time series, and j = 2j/n denotes the Fourier frequencies. Moreover,
we define the trimmed discretely averaged co-periodogram (TDAC), using the real part of Ihk (j ), as,

                                               m
                                       2
                          Fhk ( , m) =              (Ihk (j )),    1   m  n,                            (16)
                                        n
                                               j=


where   = (n) and m = m(n) comprise the trimming and bandwidth functions, respectively. Hence,
we may write the TDAC of uc       c                             c              c
                          t-1 as Fuu ( , m) and, similarly, of ut-1 and et as Fue ( , m). Finally,
these are used to define the medium band least squares (MBLS) estimator,

                                                c        -1 c
                                   Bc ( , m) = Fuu ( , m)  Fue ( , m),                                  (17)

for which , m   and /m + m/n  0, as n  . The MBLS estimator has some distinct
advantages for predictive inference and testing with persistent variables. Specifically, by combining
sample-size-dependent trimming with a bandwidth m/n  0, equation (17) turns out to be first-order
equivalent to,
                                    B( , m) = Fuu ( , m)-1 Fue ( , m),                                  (18)

that is, the corresponding estimator based on vt . In other words, trimming and a local bandwidth
suffice to annihilate biases arising as a result of endogenous regressors. Intuitively, this follows from the
MBLS estimator utilizing frequencies, that are asymptotically "close" to the origin, which, as shown
by the decompositions (11) and (12), are dominated by information about B, whereas the higher
frequencies are more prone to endogenous regressor biases. Moreover, the trimming and bandwidth
sequences aid in asymptotic elimination of the residual impact from the filtered mean component (mean

                                                       16
slippage contamination), occurring at lower frequencies, and first-stage estimation errors from the
filtering procedure, occurring at higher frequencies. This suggests that LCM procedure, particularly
the second step, should be well-suited to draw inference regarding return predictability.
    The main obstacle for using LCM to analyze return regressions is the fractional filtering step. It
is complicated due to the low signal-to-noise ratio of the conditional mean relative to the weakly
dependent innovations; the return serial dependence is limited, although some highly persistent series
often provide significant predictive power for the returns. This suggest that we cannot draw inference
about d1 in finite samples using standard univariate time series techniques and, in fact, we verify
these results in both our simulation study and the empirical analysis below. Consequently, the next
section provides a new (cointegration) rank testing framework, that not only facilitates discriminating
between the model hypotheses (i)-(iv), but also allows us to determine the persistence of the conditional
mean return component. Subsequently, in Section 4, we document that this multivariate procedure
overcomes the shortcomings of univariate time series techniques in realistic finite sample settings.


3     LCM Rank Testing and Inference
This section provides a new rank test for fractional cointegration, that facilitates discriminating be-
tween the model hypotheses (i)-(iv). First, we establish the properties of the test, requiring that
Assumption F holds, i.e., we can consistently estimate the fractional integration order for the predic-
tors and the asset returns. As argued above, this assumption is unlikely to hold in finite samples,
using standard univariate techniques, as the signal-to-noise ratio of the conditional mean to the return
innovations is too "low". Hence, we subsequently outline how a sequence of rank tests may be used to
deduce, whether returns have a persistent conditional mean and to determine its fractional integration
order. Finally, we establish central limit theory for a rank-augmented LCM (RLCM) procedure.

3.1    LCM Rank Testing for Cointegration
Initially, we suppose that the returns are equipped with a conditional mean, and we know its fractional
integration order, 0  d1  1. Then our filtering, heuristically, implies,
                
                (1 - L)d1 y                    (d1 )
                            t       B ut-1 + t                  under models (i) and (iv),
                                                        (d1 )
                                                                                                    (19)
                (1 - L)d1 y         B ut-1 + t-1 + t            under models (ii) and (iii),
                               t


with, again, t-1  ut-1 by Assumption C. Hence, we can apply this decomposition to test for the
presence of t-1 . The interpretation of the test, however, depends on the magnitude of d1 . If the
returns do not feature a persistent mean component, d1 = 0, in line with scenario (i), then we cannot
                       (d1 )
distinguish t-1 and t          = t , which are both I (0). As noted in Remark 2, this corresponds to
identification failure (when  = 0) in the imperfect predictor model of Pastor & Stambaugh (2009).
However, given the extensive empirical and theoretical evidence on return predictability, our primary


                                                       17
     focus is on the persistent mean return case, d1 > 0, corresponding to scenarios (ii)-(iv). In these
     cases, we may utilize the low-frequency spectrum to design a cointegration rank test for the presence
                   c = (e , (uc ) ) . This test design works, since               (d1 )
     of t-1 using vt     t    t-1                                    t                    is a lower-order residual and has a
     degenerate spectral density for         0+ ,   as discussed below equations (11) and (12).12
        Formally, to design a rank testing procedure, we leverage insights from equation (19) and use the
                                    c . Hence, we must accommodate estimation errors from filtering, mean-
     fractionally filtered series, vt
     slippage, as well as bias and errors induced by regressor endogeneity, in analogy to the challenges
     detailed for the second-stage MBLS in Section 2. To this end, we turn to the trimmed long-run
     covariance estimator,
                                                                            mG
                                                               1
                                  Gc
                                   vv (   G , mG ) =
                                                                                      c
                                                                                    (Iv v (j )) ,                               (20)
                                                           mG - G + 1
                                                                        j=   G


     where we use separate bandwidth and trimming functions, mG = mG (n) and                                   G   =   G (n).   This
     class of long-run covariance estimators is used for inference and testing in Andersen & Varneskov
     (2020) and is akin to those in Christensen & Varneskov (2017). If we restrict                            = 1, the estimator
     also resembles those employed by Robinson & Yajima (2002) and Nielsen & Shimotsu (2007) to design
     semiparametric tests for fractional cointegration rank in LW and ELW settings, respectively. However,
     we face additional challenges due to the, possibly, endogenous regressors, fractional filtering induced
                                                                                                     (d1 )
     mean-slippage and estimation errors as well as the lower-order filtering error t                        . Hence, we seek ap-
     propriate conditions to prevent either feature from impacting the limiting properties. Moreover, while
     Andersen & Varneskov (2020) establish consistency of the trimmed estimator (20) for the covariance
     matrix G with t-1  (t-1 , ut-1 ) ­ either in the case with weak endogeneity, as in Assumption
     D1, or for the case of stronger endogeneity, but absent cointegration ­ we now require an associated
     central limit theory, covering models (ii)-(iv), to design a suitable rank test for (iv).13

     Assumption T-G. let the bandwidth mG                   nG and      G        nG , with 0 < G < G <                  1. Then,
     for some arbitrarily small    > 0, the following cross-restrictions are imposed on                      G,   mG , md and n,
                                                                                          
            m1+2
             G         n            1    mG            d        G  mG              dx        mG
                    + 2                +                    +    +                         1+     +  0,       as    n  .
             n        G mG          mG    n                   mG    n                      G

        The first condition in Assumption T-G is familiar from semiparametric frequency domain estima-
     tion, e.g., Robinson & Yajima (2002) and Nielsen & Shimotsu (2007). For the empirically relevant
     vector ARFIMA process (with            = 2), it requires G < 4/5. In contrast, the last three conditions
     impose joint bounds on the bandwidth and trimming rates. Specifically, two and four stipulate a
     lower bound on the trimming to eliminate the bias from mean-slippage and regressor endogeneity,


12
   We explain in the next section how to use sequential rank tests to verify that d1 > 0, indeed, holds. Still, it is worth
   noting that our rank test will work, even when d1 = 0; indicating full rank due to the presence of both t-1 and t .
13
   Again, we use the definition t-1  (t-1 , ut-1 ) to indicate that weakly dependent return innovations have no asymp-
   totic impact on the limit theory, when d1 > 0 in scenarios (ii)-(iv).


                                                               18
respectively, and condition three restricts the loss of information. Taken together,

                       1 - G (1 - G )(1 - d) + G /2 G                   G
                                                      - (1 - G )dx <  <   ,                                      (21)
                          2             2           2                   2

These bounds are quite restrictive, if 0 < d  dx is small. Moreover, we require dx > 0 for  to be
defined on an open interval, again, illustrating the importance of this identifying condition, when the
regressors are endogenous. The fourth restriction can be dispensed with if ct-1 = 0, t = 1, . . . , n. It
is important to note that, if the conditional mean of returns and the regressors are strongly persistent,
e.g., d   1, the lower bound simplifies to (1 - G )/2  G /4  3G /2 - 1 <  , which is very mild.

Theorem 1. Suppose Assumptions D1-D3, C, M, F and T-G hold, 0 < d1  1, and n1/2 /mG  0.
                       (i)
Then, by letting G be the i = 1, . . . , k + 1 column of G , it follows,

                 1/2
            mG vec Gc
                    vv (            G , mG )   - G
                             D                                                (1)              (k+1)
                             -
                              N 0, G  G + G  G , . . . , G  G                                          /2 .

Theorem 1 shows that the trimmed long-run covariance estimator attains asymptotic properties mir-
roring those of Robinson & Yajima (2002, Propositions 2-3) and Nielsen & Shimotsu (2007, Lemmas
4-5). Hence, despite the additional challenges in the current environment, we may utilize their pro-
cedures to study the (cointegration) rank of G and, thus, whether predictive model (iv) should
replace models (ii) or (iii). Moreover, the analysis simplifies, as we do not seek to determine an exact
cointegration rank of a system, but rather to test the null hypothesis H0 : rank(G ) = k + 1 against
the specific alternative HA : rank(G ) = k , because ut-1 is of full rank.

Remark 3. A result analogous to Theorem 1 still holds, if b = d1 = 0, as for scenario (i). Specifically,
                                                                   (b)
we need to write t  vt and G  Gvv , since t                              = t and t-1 are both I (0). Moreover, we can
relax the second trimming condition in Assumption T-G, which is required to eliminate the lower-
                                                  (d1 )
order error (when d1 > 0); namely, t                      . In scenario (i), G will be of full rank, and this will be
indicated by our subsequent rank selecetion procedures with probability approaching 1. Hence, despite
being developed with models (ii)-(iv) and d1 > 0 in mind, our approach still applies for scenario (i).

   Next, to estimate the rank, we let i and i , i = 1, . . . , k + 1 denote the eigenvalues of the covariance
matrices G and Gc
                vv (             G , mG ),   listed in descending order, 0 < k < · · · < 1 , with 0 < k+1 < k and
k+1 = 0 under H0 and HA , respectively. Then, for r = 0 and r = 1 indicating the rank reduction
under the two hypotheses, we follow Robinson & Yajima (2002) and Nielsen & Shimotsu (2007) and
estimate r as,
                                                                                        k+1-
                             r = argmin L( ),               L( ) = (n)(k + 1 - ) -             i ,               (22)
                                     {0,1}                                               i=1

for some (n) > 0, which is assumed to obey the conditions:

                                                                 19
                                                                      1
Assumption V. The sequence (n) satisfies (n) +                        
                                                                   (n) mG    0 as n  .

Theorem 2. Suppose the conditions of Theorem 1 and Assumption V hold, then,

                                                   lim P(r = r) = 1.
                                                  n

   The rank selection procedure is consistent and, thus, facilitates discrimination between the pre-
dictive models (ii) or (iii) and the cointegration model (iv). Moreover, as discussed in, e.g., Phillips
& Ouliaris (1988), Robinson & Yajima (2002) and Nielsen & Shimotsu (2007), the rank selection
procedure may be implemented using the corresponding (trimmed) correlation matrix estimator,

              c                                         -1/2                                         -1/2
             Pv v(   G , mG )    diag(Gc
                                       vv (    G , mG ))     Gc
                                                              v v ( G , mG )   diag(Gc
                                                                                     vv (   G , mG ))     .   (23)

In line with their numerical evidence, we find that the correlation-based procedure performs substan-
tially better in (unreported) finite sample simulation settings, which we return to in Section 4.
   As discussed above, the validity of the described rank testing procedure depends on a reliable
finite sample estimate of d1 via Assumption F and, to this end, we cannot rely on univariate time
series techniques. Hence, we continue by demonstrating how a sequence of rank tests may be used to
determine d1 , in addition to discriminating between the hypotheses H0 and HA .

Remark 4. In Appendix A.1, we describe an alternative rank test based on Theorem 1 and feasible
inference for the eigenvalues, inspired by Phillips & Ouliaris (1988). This performance of this testing
procedure, however, falls short of the corresponding one based on equations (22)-(23) in realistically
calibrated finite-sample simulation settings. Hence, these results are omitted for brevity.

3.2   Assessing Cointegration and Persistence in Returns
The vector of (latent) regressors, ut-1 , is required to have full rank by Assumption D1, implying
that the consistent LCM-based rank selection procedure based on equations (22)-(23) may be applied
bivariately, sequentially pairing each regressor with the returns. Because we cannot estimate d1 reliably
in finite samples via standard univariate techniques, we implement a restricted version of the test by
replacing d1 with di , i = 2, . . . , k + 1, corresponding to the relevant regressor, assuming that,

                                  d1  {d2 , . . . , dk+1 },    and still 0 < d1  1,                           (24)

i.e., the vector of regressors has been chosen "sensibly". For example, the regressors may include theory-
guided state variables from the long-run risk, dynamic disaster and present-value models. Then, under
equation (24), an estimate ri = 1 with 0 < di  1, i = 2, . . . , k + 1, constitutes evidence in favor of
fractional cointegration using the (i - 1)th regressor. Similarly, a corresponding estimate ri = 0
indicates that returns have not been over-differenced, and that the predictive relation is "imperfect",
suggesting that we may "search" among the integration orders of the candidate predictors. However, the


                                                              20
     procedure must be implemented thoughtfully. An indiscriminate inclusion of (irrelevant) predictors, or
     regressors with integration orders larger than 1, will render the imposition of the maximal integration
     order among the regressors in our testing procedure for the conditional mean return problematic. The
     issue is, of course, that it will tend to generate rejections of the full rank hypothesis due to the ex-ante
     restricted integration order in the test d1 = di , i = 2, . . . , k + 1, being too large. Specifically, suppose
     we include some auxiliary I (dx ) predictor with dx > d1 , then, heuristically,
            
                                                           (dx )
            (1 - L)dx y
                       t       B (1 - L)dx -d1 ut-1 + t                                under model (iv),
                                                                               (dx )
            (1 - L)dx y
                          t    B (1 - L)dx -d1 ut-1 + (1 - L)dx -d1 t + t              under models (ii) and (iii),

     generating a rank estimate rx = 1 with probability approaching one, because the spectral density of
     the, in this case, over-differenced return series will be degenerate as   0+ .
        To guard against this type of mechanical over-differencing, we advocate a screening procedure. Let
     R denote the set of regressors, that generate estimates ri = 1 with 0 < di  1 in a bivariate rank test
     with returns ­ indicating cointegration. Our sequential screening procedure now takes the form,
     Step 1. Carry out k bivariate rank tests with the returns. If R = , suggesting no cointegration,
           choose d1 = maxi=2,...,k+1 (di | 0 < di  1) and stop. If R = , proceed to Step 2.

     Step 2. Impose d1 = maxi=2,...,k+1 (di | ut-1 (i - 1)  R) and carry out all k bivariate rank test. If
           ri = 0 is maintained for at least one variable, stop. If not, choose the second-largest di among the
           variables in R and repeat step 2. Continue until ri = 0 is maintained for at least one variable.
        Step 2 exploits the fact that a single non-rejection indicates the returns, for the given selec-
     tion d1  (0, 1], have not been over-differenced. The identical argument motivates selecting d1 =
     maxi=2,...,k+1 (di | 0 < di  1), if R = . In fact, if ri = 0 is estimated for just a single persistent
     (d1 > 0) candidate predictor, even an irrelevant one, it provides consistent evidence against a weakly
     dependent return series. That is, if we find, for fractionally filtered returns, that the rank is non-
     degenerate relative to a fractionally integrated series, then the conditional mean return must display
     fractional persistence, as (1 - L)d1 t will be a lower-order residual for all d1 > 0. Moreover, by im-
     plementing the rank selection procedure sequentially, we may directly assess the (induced) degree of
     persistence of the conditional mean, and which of the inference scenarios (ii)-(iv) apply.
     Remark 5. To illustrate the workings of two-step procedure, we consider an example. Suppose there
     are two candidate predictors X t-1 = (X1,t-1 , X2,t-1 ) with integration orders (d2 , d3 ) = (0.45, 0.80) .
     Moreover, let X1,t-1 be insignificant and X2,t-1 be "perfect", implying that model (iv) applies and
     d1 = d3 . The integration orders (d2 , d3 ) are estimated consistently via Assumption F. In this case, our
     bivariate rank test for returns and X2,t-1 using d3 will (asymptotically) indicate cointegration, belonging
     to the set R. The test for X1,t-1 using d2 may or may not indicate cointegration.14 Regardless, since
14
     The reason is that returns have not been filtered sufficiently, (1 - L)d2 -d1 (But-1 + t )  I (d1 - d2 ), with d1 > d2 .
     Hence, the first element of the vector in the rank test may be of "large enough" asymptotic order relative to the second
     fractionally filtered element of the vector, which is I (0), to indicate (spurious) cointegration in finite samples.


                                                                   21
     d3 > d2 , we repeat the test for X1,t-1 with d3 in Step 2, which will now (asymptotically) indicate full
     rank. Hence, the procedure stops here; we select d1 = d3 and know that inference scenario (iv) applies.

     Remark 6. If scenario (i) describes the return model, then we will find ri = 1 for all i = 2, . . . k + 1
     and di > 0, with probability approaching one, since returns are over-differenced for any di > 0. Hence,
     our two-step procedure implicitly provide information about the validity of this model.

     3.3    Limit Theory for LCM
     Beyond reliable estimates for D (L), satisfying Assumption F, obtained either via univariate time series
     and/or rank-augmented techniques, we also require trimming and bandwidth conditions for the second-
     stage MBLS estimator, along the lines of Assumption T-G. As noted, we impose b = d1 throughout.
     We state the requisite conditions in terms of b for comparability with Andersen & Varneskov (2020).
        Before proceeding, note, again, that we develop LCM inference with scenarios (ii)-(iv) in mind, but,
     as described below, the asymptotic results pertain equally to model (i).

     Assumption T. Let the bandwidth m                n ,       n , and md      n with 0 <  <  <            1. Moreover,
     recall that the parameter         (0, 2] measures smoothness of the spectral density in Assumption D1.
     The following cross-restrictions are assumed to apply for , m, md and n, as n  ,

                m1+2           1+ +b           n1/2+b             n1-2d+b             nb         m1/2+dx -b
                         +                +                 +                   +            +               0.
                 n2          n m1/2+b           1/2
                                              md mb             m1/2-2d+b   2       m1/2+b        ndx -b

        The restrictions in Assumption T are mild. The first term is standard for semiparametric estimation
     in the frequency domain, see, e.g., Robinson (1995) and Lobato (1999), while the remaining conditions
     are specific to the second-stage MBLS estimator, adopted in the LCM procedure. Specifically, condition
     two, implying  < (         + (1/2 + b))/(1 +           + b), restricts the loss of information from trimming
     frequencies; three, (1 - )/2 + b(1 - ) <  in conjunction with 0 <  <  <                               1, eliminates
     errors from estimating the integration orders; four, (1 - /2 - (2d - b)(1 - ))/2 <  , alleviates the
     low-frequency bias from mean-slippage following fractional filtering; five, b/(1/2 + b) <  imposes a
     mild bound on the bandwidth; six, /2 - (1 - )(dx - b) <  eliminates the endogeneity bias.15
        If we consider the empirically relevant vector ARFIMA process (with                      = 2) and select  close to
     its upper bound 4/5, conditions two and four imply (3/5 - (2d - b)/5)/2 <  < 4/5. The lower bound
     is strictly decreasing in 2d - b  0 (as assumed below), implying that its most restrictive scenario is
     obtained when d = 0, equaling 3/10. The third condition is (essentially) trivial, if we adopt a para-
     metric first-stage estimator with        = 1 and  close to 4/5. If the estimator is semiparametric, however,
     and we select  <        as well as     arbitrarily close to 4/5, the additional lower bound requirement on
     the trimming rate becomes 1/10 + b/5  3/10 <  . Finally, if the regressors are endogenous and we
     select  close to 4/5 for efficiency, we require 2/5 - (dx - b)/5 <  , with most conservative bound being
15
     We note that the trimming and bandwidth functions in Assumption T are mutually consistent for all values of 0 < dx < 2
     and 0  d1  1 if the (implied) condition max(0, (1 - 3/2)/(1 + /2)) <  2 holds.


                                                                 22
     obtained when dx - b = 0. Intuitively, we require stronger trimming to obtain the same asymptotic
     efficiency in the presence of endogenous regressors, if the excess persistence of the system is small.

     Theorem 3. Suppose Assumptions D1-D3, C, M, F and T hold as well as the conditions 0 < d1  1,
     b  d, n1/2 /m  0, and max(0, (1 - 3/2)/(1 + /2)) <                    2, then,
            
                              N 0, G-
                             D
             m Bc ( , m) - B -        1
                                    uu G /2 ,                                      under models (ii) and (iii),
            m -b B ( , m) - B - N 0, G-
                               D         1
               m      c                 uu G /(2(1 + 2b)) ,                        under model (iv).

        Theorem 3 demonstrates that the LCM procedure, possibly augmented with the rank test to deter-
     mine d1 , is asymptotically Gaussian for both the predictive models (ii) and (iii) and the cointegration
     model (iv). The asymptotic distribution theory differs, however, in the two cases. First, when the
     regressors are "imperfect", t-1 is an asymptotic order larger than t and drives the limit theory. The
                        
     convergence rate is m, in line with well-known results for semiparametric estimators in the frequency
     domain, e.g., Brillinger (1981, Chapters 7-8), Robinson (1995) and Shimotsu & Phillips (2005). Sec-
     ond, if the regressors are "perfect", t-1 is trivial and the limit theory is determined by t . The rate
                   
     is m-   m
               b    m(n/m)b and the asymptotic variance is scaled by 1/(2(1 + 2b)). Hence, cointegration
     improves efficiency of the MBLS estimator, in analogy with super consistency properties.
        Despite the limit theory differing across models (ii)-(iii) and (iv), it remains Gaussian in both cases,
     regardless of whether the variables are (asymptotically) stationary or non-stationary, whether there is
     cointegration, and irrespective of the cointegration being weak (b < 1/2) or strong (b  1/2). This
     is unique within a fractional cointegration context, as similar uniformity applies neither to OLS, the
     narrow band least squares (NBLS) estimator, nor for maximum likelihood inference in the fractionally
     cointegrated VAR model, where the inference is Gaussian under stationary and exhibits different
     forms of non-Gaussianity in non-stationary cases; see Robinson & Marinucci (2003), Christensen &
     Nielsen (2006) and Johansen & Nielsen (2012).16 Similarly, the Gaussian limit theory for the MBLS
     estimator without fractional filtering in Christensen & Varneskov (2017) holds only for stationary
     systems with weak cointegration. Intuitively, the Gaussian limits in Theorem 3 follow from having
     fractionally filtered the variables such that the inference, after eliminating various errors and biases
     through trimming, becomes reminiscent of the ELW inference in Shimotsu & Phillips (2005).
        Moreover, the asymptotic distribution theory of the LCM procedure is correctly centered, thus free
     from bias due to persistent and endogenous regressors, which are detailed by Stambaugh (1999), Pastor
     & Stambaugh (2009) and Phillips & Lee (2013). Interestingly, since the fractional filtering lowers the
     asymptotic order of the weakly dependent innovations, t , regardless of the inference scenario, the
     LCM procedure may also provide finite sample improvements by alleviating attenuation biases. An
16
     Such methods generally do not accommodate non-trivial means, or initial values as well as strong endogeneity among
     the regressors that may or may not be "perfect". Moreover, the limit theory of these alternatives rely on the presence
     of cointegration. Finally, as demonstrated by Andersen & Varneskov (2020, Theorem 5), the LCM procedure can
     accommodate regressors that are generated from pre-estimated fractional cointegration residuals. Consequently, the
     LCM procedure remains desirable in this context, delivering added robustness along with a fast convergence rate.


                                                              23
additional advantage of the Gaussian limit theory is that feasible inference and testing is standard,
once we obtain a consistent estimator of the asymptotic variance in the requisite inference scenario.
Specifically, the latter can be determined by estimating b using our rank-selection procedure from
Section 3.2. We detail how to draw feasible inference in Appendix A.2.

Remark 7. We impose 0 < d1 < 1 for Theorem 3, but can accommodate the case d1 = 0, with
appropriate changes to the asymptotic variance for both models (ii)-(iii) and (iv). In particular, for
the former, we have to replace G with G + G . Moreover, since cointegration no longer features
in model (iv), by b = d1 = 0, the result is readily obtained by setting b = 0 in the limit theory, thereby
slowing down the convergence rate. Similarly, the condition d - b  0 is equivalent to the "balanced
cointegration" requirement in Andersen & Varneskov (2020, Eq. (8)), implying that the cointegration
cannot be stronger than the persistence of the regressors. This condition is not strictly binding, but
simplifies the tuning parameter restrictions on     and m in Assumption T considerably.
                                                    
Remark 8. The conditions         n/mG  0 and           n/m  0, as n  , in Theorems 1 and 3, respec-
tively, are not strictly binding, but are imposed for ease of exposition. Specifically, they are used to
bound the endogenous regressor bias in auxiliary Lemmas B.1(a)-(d) (cf., Appendix B). Define,

                 ¯(m, n)  1  (m/n)dx n1/2 /m,
                 f                                     ¯
                                                       f                    d 1/2
                                                        G (m, n)  1  (mG /n) x n  /mG ,

                                                               - 
then the conditions can be relaxed to          md ) + n/(m1
                                           n/(m1-             G    md )  0, for some small > 0, if
                                                            ¯            ¯
multiplying the bounds in Lemma B.1(a)-(b) and (c)-(d) with f (m, n) and fG (m, n), respectively. This
feature is important, as we also entertain the selection mG      nG , with G = 2/5 for our cointegration
rank procedure, which is valid, albeit with stronger cross-restrictions on the tuning parameters.

    While the asymptotic properties for the LCM procedure are highly desirable, it is prudent to study
its finite sample performance for return regressions in realistic settings, in particular, the interplay
between fractional filtering, MBLS estimation and rank testing. This is examined next.


4     Return Regressions and Numerical Evidence
This section illustrates inferential problems surrounding return regressions in a transparent numer-
ical setting. Specifically, we explore the effects of increasing the noise-to-signal ratio of the return
regressions for estimates of its fractional integration order as well as the size and power properties of
predictability tests relying on either OLS, IVX or LCM inference. In particular, the size is assessed
within an imperfect predictor specification. Moreover, we examine the size and power properties of
the cointegration rank selection procedure based on equations (22)-(23). Finally, we study the bias
and RMSE of LCM estimates of B, with and without applying a rank-augmented estimate of d1 .




                                                       24
     4.1     Simulation Setting
     We study inference problems for return regressions in a setting reminiscent of the ones in Hong (1996)
     and Shao (2009), albeit allowing the variables to exhibit non-stationary fractional integration. Specif-
     ically, we suppose B and X t-1 are univariate (written as B and Xt-1 , respectively) and stipulate that
     Xt-1 = xt-1 , which renders the signal of the persistent regressor directly observable and excludes
     endogeneity. Then, we generate fractional ARMA(0, 0) processes as,

                                         (d1 )
        (1 - L)d1 (yt - µy ) = t-1 + t           ,    t-1 = B ut-1 + B t-1 ,   (1 - L)d2 (xt-1 - µx ) = ut-1 ,   (25)

           (d1 )
     and t         = (1 - L)d1 t , where t  i.i.d. N (0, 2 ) for   { , 
                                                                 t  t t-1 , ut-1 }. Moreover, to highlight the
     impact of the noise-to-signal ratio for drawing inference about return persistence and predictability,
     we fix d1 = d2 = d, set µy = µx = 1/2 and, without loss of generality,  = u = 1, while varying
     the volatility of the return innovations,  . This ensures that the dynamic properties of the predictive
     system are captured solely by d, B , B , and  . We consider two long-memory regimes: d = 0.45
     and d = 0.80, corresponding to a stationary predictable return component versus one that is non-
     stationary, yet mean-reverting. As we vary   [0, 25], the noise-to-signal of the predictive relation is
     altered, possibly rendering the persistence of the conditional mean undetectable in finite samples.17
        Initially, we entertain univariate predictions using xt-1 , but fix B = 0 and B = 1.2 in equation
     (25), implying that asset returns are comprised of a persistent mean, but the empiricist employs an
     irrelevant "imperfect" predictor, so that the persistence "spills over" into the residuals. In this scenario,
     we assess if and when  is sufficiently large to induce "incorrect" inference regarding the fractional
     integration order of the returns, d1            0, as is generally found empirically. Moreover, we examine the
     size properties of predictability tests with, seemingly, I (0) returns using either OLS, IVX or LCM.
     These three inference procedures are all "misspecified," in the sense that OLS and IVX inference
     generally does not apply to fractionally integrated systems, as discussed in the introduction, whereas
     LCM is implemented using the "wrong" fractional integration order for the returns.
        We implement IVX with parameters CIVX = 1 and IVX = 0.95 to construct the self-filtered
     instrument, an additional deterministic instrument sin((t - 1)/n), t = 1, . . . , n, and Eicker-White
     standard errors, in line with the recommendations in Breitung & Demetrescu (2015) and Kostakis et al.
     (2015). Similarly, we employ Eicker-White inference for OLS.18 Moreover, we implement LCM using
     trimming and bandwidth parameters (, ) = (0.20, 0.60). These are similar to the ones considered
     in Andersen & Varneskov (2020) and reflect the dynamic properties of returns and, especially, the
     persistent predictor variables in Section 5. Specifically, the bandwidth is chosen locally (m/n  0)
     to avoid placing excessive weight on the higher-frequency errors from t and the trimming reflects
     condition two in Assumption T, with dx = 0.3 in the empirical analysis. Despite the results not

17
   We have run similar experiments with ARMA(1, 0) short-run dynamics. The corresponding results, when allowing for
   mild autoregressive dynamics in the processes, are almost identical to those presented in Figures 1-2 below.
18
   We have also carried out OLS-based testing for return predictability using Newey & West (1987) standard errors. The
   results are almost identical to those presented in Figure 1, and thus omitted for brevity.


                                                                25
being reported, we note that, importantly, the results are qualitatively robust to varying the tuning
parameters  and  by ±0.05 and ±0.10, respectively. The first stage estimates of d1 and d2 are
constructed using the TELW estimator of Andersen & Varneskov (2020), with corresponding trimming
and bandwidth parameters          d   = n0.3 and md = n0.7 . Moreover, the significance tests for LCM is
implemented using the feasible inference procedure in Andersen & Varneskov (2020), see also Appendix
A.2, where the consistent spectrum estimator of the asymptotic variance is implemented with G = 0.20
and G = 0.60. Finally, we consider a sample size n = 650, mimicking the one for the empirical analysis
(n = 661), a 5% nominal test size, and using 1000 replications.

4.2    Persistence, Size, Bias and RMSE
The estimated integration order of the returns, d1 , and the OLS, IVX and (misspecified, depending on
d1 ) LCM test sizes are displayed as functions of  in Figure 1, whereas Figure 2 provides bias and RM-
SEs of the corresponding LCM coefficient estimates, benchmarked against an oracle implementation
of LCM, where the true d1 and d2 are treated as known in the fractional filtering. Several features are
noteworthy. One, the estimated persistence decreases as a function of  , eventually implying failure
to reject d1 = 0. This occurs, not surprisingly, more rapidly for the weaker signal, d = 0.45, than for
the stronger one, d = 0.80, illustrating that returns may, possibly, have a persistent component that
is hard to identify using standard univariate time series techniques.
   Two, OLS and IVX are oversized for a wide range of  , even when the estimated fractional inte-
gration order, seemingly, suggests that the return series is I (0). This is akin to the well-established
spurious inference problem, arising when applying least squares to fractionally integrated processes,
e.g., Tsay & Chung (2000), and the size distortions for return regressions, when applying persistent
AR(1) predictors, e.g., Ferson et al. (2003). The current results demonstrate, that similar problems
may arise for return regressions with "imperfect" predictors in fractionally integrated settings. More-
over, not only is OLS-based tests unreliable, generating serious size distortions, but similar problems
arise for IVX, although the procedure, otherwise, is equipped to handle local-to-unity regressors.
   Three, whereas the size of misspecified LCM-based tests appears considerably more accurate, the
bias and RMSE of the coefficient estimates depend critically on whether the regressors are significant,
i.e., whether (B , B ) = (0, 1.2) or (B , B ) = (1.2, 0). When (B , B ) = (0, 1.2), the LCM estimates are
unbiased, but, not surprisingly, less efficient that the oracle ones. In contrast, when the regressors
are significant, (B , B ) = (1.2, 0), the coefficient estimates are severely biased, and this bias raises the
RMSE, in particular, for smaller values of  . The bias is intuitive; the latent signal of yt has not been
fractionally filtered (since d1       0) and the resulting higher asymptotic order of the conditional mean,
thus, blows up the estimate, since B > 0 and uc
                                              t-1 has been filtered "correctly".
   At face value, these results are discouraging. OLS and IVX suffer from large size distortions, and
our original LCM procedure is also ill-equipped to analyze significant return regressors. Hence, to
explore whether it provides a potential remedy, we now examine the finite-sample properties of our
rank test and the rank-augmented LCM procedure.


                                                       26
     4.3   Rank Testing and Rank-augmented LCM Estimation
     We examine the properties of the rank selection procedure based on equations (22) and (23) using the
     setting above, with d  {0.45, 0.80} and either (B , B ) = (0, 1.2) or (B , B ) = (1.2, 0), corresponding to
     the hypotheses H0 : r = 0 and HA : r = 1, respectively. Specifically, we implement the test with tuning
     parameters G = 0.20 and G  {0.40, 0.50, 0.60}. Moreover, we let the sequence in Assumption V
     be specified as (n) = n- and examine   {0.10, 0.20, 0.30}. Finally, to gauge the large(r) sample
     properties of the procedure, we consider n = 650 as well as n = 2000. Tables 1-2 provide rejection
     frequencies of H0 in favor of HA , when H0 is correct and, similarly, Tables 3-4 display corresponding
     rejection rates, when HA is correct, reflecting "size and power" properties, respectively.19
        First, we observe the that the "size" of the tests is very good, except when combining the tuning
     parameter selections G = 0.40 and  = 0.10. For example, when G = 0.40,  = 0.20 and n = 650,
     the procedure selects the wrong rank in approximately 5% of the simulations, reminiscent of a 5%
     test size. Second, we observe that the selection procedure generally has good "power" properties,
     especially when G < 0.60 and   0.20, far exceding what is achieved by the corresponding univariate
     significance tests for the TELW estimator in Figure 1. Again, if considering G = 0.40 and  = 0.20,
     the rejection rates are substantially above 5% for both d = 0.45 and d = 0.80 in Tables 3 and 4,
     respectively. Moreover, the rejection rates remain non-trivial for large values of the return innovations,
      , and they are, not surprisingly, larger for the stronger signal, d = 0.80, than the weaker, d = 0.45.
     Finally, when the sample size increases to n = 2000, the rejection frequencies in Tables 1 and 2 converge
     to zero as expected, and the "power" generally improves in Tables 3 and 4.
        The finite sample results in Tables 1-4 are striking, demonstrating that a persistent conditional
     mean of asset returns can be identified (with good power) through a multivariate rank test, even if
     standard univariate techniques suggest serial dependence is absent. In particular, the tuning parameter
     selections G = 0.40 and  = 0.20 balance "size" and "power" well. Hence, relying on these choices, we
     next seek to augment the LCM procedure with a rank test to determine the memory of the conditional
     mean return, as described in Section 3.2. Specifically, to mirror the 2-step testing procedure, if the test
     indicates full rank, when restricting d1 = d2 , with d2 computed by the TELW estimator, the return
     series has not been over-differenced and the restricted memory estimate is maintained. In contrast,
     if the rank test indicates cointegration, we test for over-differencing by simulating a fractional noise
     process, as for xt-1 in equation (25), but independent of yt , and perform a rank test with et and the
     filtered residuals from the auxiliary variable with, naturally, its own estimated integration order. If this
     test maintains full rank, the estimated series et has not been over-differenced and we use the restricted
     memory estimate. If the test, once again, rejects full rank, the series has been over-differenced, and
     we implement the LCM procedure with the TELW estimate for d1 .20

19
   While Tables 1-2 report false rejection rates for the null hypothesis, akin to test size, the rank selection test is not calibrated
   to generate any specific rejection rate asymptotically, so lower (false) rejection rates are simply better. Nonetheless, the
   usual tradeoff between size and power properties applies, so comparing the "size" tables with the "power" reflected in
   Tables 3-4 provides the best guide towards identifying desirable test configurations.
20
   It is important to note that even though we only consider one candidate predictor and one auxilliary variable, this mimics


                                                                  27
    Once the rank-augmented estimate of d1 has been obtained, the remaining filtering and estima-
tion steps of the LCM procedure are the same. In Figure 3, we compare the bias and RMSE of this
rank-augmented LCM (RLCM) procedure to the oracle implementation of LCM, thus mirroring the
structure of Figure 2. The results, however, are in stark contrast. Whereas the unrestricted LCM
procedure suffers from a pronounced bias, when (B , B ) = (1.2, 0), the RLCM procedure is unbiased
and essentially as efficient as the oracle version of LCM, demonstrating that our rank-testing step
successfully ameliorates the estimation errors from using a misspecified estimate of d1 . That is, the
RLCM procedure overcomes the challenges introduced by the "low" signal-to-noise ratio for the con-
ditional mean return, and our rank selection procedure (22)-(23) represents an effective technique for
detecting "hidden" persistence in the conditional mean and determining its integration order.


5     Empirical Illustration: Forecasting Equity Market Returns
This section analyzes predictions of monthly S&P 500 index returns via persistent and popular state
variables in the macro-finance literature using OLS, IVX and RLCM procedures. Specifically, we
examine the predictive content of the regressors in Bansal et al. (2014) and Campbell et al. (2018).

5.1    Data Description
We employ a data set of monthly observations for log-returns and corresponding realized variance (RV)
measures of the aggregate U.S. stock market, proxied by the S&P 500 index, spanning the period from
March 1960 through March 2015, which amounts to n = 661 observations. Specifically, following,
e.g., Andersen & Bollerslev (1998), Barndorff-Nielsen & Shephard (2002), and Andersen, Bollerslev,
Diebold & Labys (2003), RV is constructed by summing up daily squared returns for each month.
Moreover, inspired by the VAR system in Campbell et al. (2018), we include the default spread (DS),
three-month U.S. Treasury bills (TB), and price-earnings ratio (PE) as additional state variables.
They have all been argued to be successful predictors of equity index returns, see, e.g., Lettau &
Ludvigson (2010) and Campbell (2018, Chapters 5.3-5.4). The construction of these variables follows
literature standards, with the DS being defined as the difference between logarithmic percentage yields
on Moody's BAA and AAA bonds, TB is log-transformed, and PE is constructed as the log-ratio of the
S&P 500 index to the ten-year trailing moving average of the aggregate S&P 500 constituent earnings.
The DS and TB data are obtained from the website of the Federal Reserve Bank of St. Louis, while
the PE data stem from Robert Shiller's website, see Shiller (2000).

5.2    RLCM Analysis of Return Predictability
First, we estimate the fractional integration order of returns and the four state variables; RV, DS,
TB and PE. Specifically, we adopt the TELW estimator of Andersen & Varneskov (2020) and the
the two-step selection procedure more generally since we implement bivariate tests and only require one non-rejection of
cointegration (for some di > 0) among all our regressors to conclude that the returns have not been overdifferenced.


                                                          28
     exact local Whittle (ELW) estimator with a correction for the mean, or initial value, of Shimotsu &
     Phillips (2005) and Shimotsu (2010).21 The results, reported in the top half of Table 5, show that
     the returns are, seemingly, I (0), RV is stationary and fractionally integrated, and the remaining three
     state variables are non-stationary long-memory processes. However, as argued earlier, these results do
     not exclude returns from having a "latent" persistent conditional mean.
        We proceed by implementing the sequential bivariate, LCM-based, rank selection procedure de-
     scribed in Section 3.2, using the TELW estimates from Table 5 and tuning parameters (G , G ,  ) =
     (0.20, 0.40, 0.20), as advocated in Section 4. The results are provided in the bottom half of Table 5.
     From Step 1 of the procedure, we find that H0 : r = 0 is rejected for RV and PE. Hence, we select
     the larger fractional integration order for PE and continue with Step 2. Once restricting the memory
     in the second step, we maintain H0 (i.e., full rank) for the three remaining predictors and, thus, stop
     there. These findings have striking implications. First, they provide consistent evidence that asset
     returns contain a fractionally integrated conditional mean, which we cannot detect using standard
     univariate time series techniques. Second, the conditional mean cointegrates with PE, suggesting that
     the latter is a "perfect" predictor of returns, as described in Section 2.2. Interestingly, this is consistent
     with dynamic present-value models for stock returns, e.g., Campbell (2018, Chapter 5.3), for which
     significance of standard predictability tests is often illusive; see, among others, Welch & Goyal (2008)
     and Lettau & Ludvigson (2010). Third, the rank selection procedure suggests that the remaining
     three candidate predictors are "imperfect", if at all significant. Fourth, this has implications for the
     statistical properties of our subsequent RLCM estimates, as demonstrated by Theorem 3. Specifi-
     cally, it suggests that the limit theory for model (iv), with its super consistency properties, applies
     to PE, whereas the limit theory for models (ii)-(iii) may be used to test significance of the remaining
     predictors. We rely on these insights, when drawing feasible inference, as described in Appendix A.2.
        Next, we estimate the coefficients of the four candidate predictors and test for their significance using
     OLS and IVX with Eicker-White inference, as described in Section 4, as well as our rank-augmented
     LCM (RLCM) procedure, where the integration order of the returns is fixed to that for PE.22 The
     results are reported in Table 6. There are several interesting findings. First, using both OLS and IVX,
     we only find RV to contain statistically significant information about future returns. However, it has
     a negative coefficient, running counter to a traditional risk-return trade-off. In contrast, we find a
     positive predictive risk-return relation for RLCM, albeit insignificant. Second, using RLCM, we find
     significant predictability for both DS and PE. The positive sign for the former is consistent with a
     risk-return trade-off, and the negative for the latter reflects return-valuation theory (Campbell 2018).
     The sign of the corresponding coefficient estimates for OLS and IVX are similar, but the magnitudes
     are smaller, and the results are insignificant. Third, the significance for TB also improves using RLCM,
     but not sufficiently to render it a significant predictor at conventional significance levels.
        These results are much sharper than typically obtained through return prediction studies, especially
21
   The TELW estimator, similarly to the mean-corrected ELW of Shimotsu (2010), is more robust to the mean, or initial
   value, of the process. Both estimators are valid for stationary and non-stationary fractionally integrated processes.
22
   We implement LCM with  = 0.20 and  = 0.60 to reduce the impact from the contemporaneous return innovations.


                                                           29
at shorter horizons, e.g., Welch & Goyal (2008), Lettau & Ludvigson (2010), Campbell (2018, Chapter
5) and references therein. We attribute this to the various advantages of our RLCM procedure. First,
uncovering the persistence of the conditional mean via rank testing, we may adequately filter returns,
reducing the impact of the "large" contemporaneous innovations, thus mitigating its asymptotic and
finite sample effect. Second, by letting /m + m/n  0 as n  , we further reduce the impact from
the error t  I (0) by sampling in a part of the spectrum, where the signal-to-noise ratio is larger.
Finally, as discussed in Section 2.2, the (rank-augmented) LCM procedure is robust to endogenous
innovations, which typically generate severe biases, e.g., Stambaugh (1999), Pastor & Stambaugh
(2009) and Phillips & Lee (2013). These LCM features all alleviate critical attenuation biases, and we
see from Table 6 that the coefficient estimates from RLCM is larger than those from OLS and IVX
for DS, PE and TB. In contrast, in Andersen & Varneskov (2020), LCM is found to provide robust
and reliable inference for return volatility forecasting, and to negate prior claims of auxiliary forecast
power for a number of macro-finance variables. The critical difference across the applications is, that
there are no evidence of latent unidentifiable integrated components in the return volatility series.


6    Conclusion
This paper studies the properties of predictive regressions for asset returns in economic systems gov-
erned by persistent vector autoregressive dynamics and considers robust estimation and inference. In
particular, the dynamic properties of the state variables are captured by fractionally integrated pro-
cesses, potentially of different orders, and returns have a latent persistent conditional mean, whose
memory cannot be consistently estimated in finite samples. The latter feature is consistent with the
typical findings in empirical studies, for which standard time series techniques almost invariably indi-
cate only weak dependence in the return dynamics. We further allow for the regressors in the system to
be endogenous and "imperfect". In this setting, we provide a cointegration rank test to determine the
suitable predictive model framework as well as the latent persistence of the conditional mean return.
By leveraging this additional source of information, we provide a rank-augmented LCM procedure,
which is consistent and delivers asymptotic Gaussian inference. Simulations illustrate the theoretical
arguments as well as favorable finite sample properties of the rank test and rank-augmented LCM
procedure. Finally, in an empirical application to monthly S&P 500 return predictions, we find con-
sistent evidence, that returns contain a (latent) fractionally integrated conditional mean component.
Moreover, by applying the rank-augmented LCM procedure, we find strong predictive power for key
economic state variables such as the price-earnings ratio and the default spread.




                                                   30
                       FI estimate: d = 0.45                                                           FI estimate: d = 0.80




                                                                                       1.2
                 d(1)                                                                            d(1)
                 (2.5,97.5)% Quantile: d(1)                                                      (2.5,97.5)% Quantile: d(1)
       0.6




                 d(2)                                                                            d(2)




                                                                                       1.0
                                                                                       0.8
       0.4




                                                                                       0.6
d




                                                                                d
       0.2




                                                                                       0.4
                                                                                       0.2
       0.0




                                                                                       0.0
             0            5                   10                15   20   25                 0            5                   10                15   20   25

                                                   sigma(eta)                                                                      sigma(eta)




                           Test size: d = 0.45                                                             Test size: d = 0.80
       0.7




                                                                                       1.2
                 LCMU                                                                            LCMU
       0.6




                 IVX                                                                             IVX
                 OLS                                                                             OLS




                                                                                       1.0
       0.5




                                                                                       0.8
       0.4
size




                                                                                size

                                                                                       0.6
       0.3




                                                                                       0.4
       0.2




                                                                                       0.2
       0.1




             0            5                   10                15   20   25                 0            5                   10                15   20   25

                                                   sigma(eta)                                                                      sigma(eta)


Figure 1: Fractional integration estimation and size. The top panels provide estimates of d1 and d2 as
a function of the standard deviation of the weakly dependent return innovations,  . Moreover, 95% confidence
intervals are provided for d1 . The estimates are constructed using the TELW estimator with tuning parameters
        0.3
 d = n      and md = n0.7 . The dotted vertical line highlights the value of  where the empirical (unrestricted)
estimate, d1 , is no longer significantly different from zero. The bottom panels provide the size of OLS, IVX and
unrestricted LCM (LCMU) significance tests for  = 0. LCM is implemented with (, ) = (0.2, 0.6) as well as
(G , G ) = (0.2, 0.6) for feasible inference; see Andersen & Varneskov (2020) and Appendix A.2 for details. Inference
for OLS and IVX is drawn using Eicker-White standard errors. The left- and right-hand-side panels have d = 0.45
and d = 0.80. Finally, we consider a sample size n = 650, a 5% nominal test size and use 1000 replications.




                                                                               31
                       LCM bias: d = 0.45                                             LCM bias: d = 0.80
       1.2




                 LCMU, H0                                                       LCMU, H0




                                                                      1.2
       1.0




                 LCMO, H0                                                       LCMO, H0
                 LCMU, HA                                                       LCMU, HA




                                                                      1.0
                 LCMO, HA                                                       LCMO, HA
       0.8




                                                                      0.8
       0.6
bias




                                                               bias

                                                                      0.6
       0.4




                                                                      0.4
       0.2




                                                                      0.2
                                                                      0.0
       0.0




             0          5    10                15   20   25                 0          5    10                15   20   25

                                  sigma(eta)                                                     sigma(eta)




                    LCM RMSE: d = 0.45                                             LCM RMSE: d = 0.80




                                                                      3.0
       3.0




                 LCMU, H0                                                       LCMU, H0
                 LCMO, H0                                                       LCMO, H0




                                                                      2.5
       2.5




                 LCMU, HA                                                       LCMU, HA
                 LCMO, HA                                                       LCMO, HA




                                                                      2.0
       2.0
RMSE




                                                               RMSE

                                                                      1.5
       1.5




                                                                      1.0
       1.0




                                                                      0.5
       0.5
       0.0




                                                                      0.0




             0          5    10                15   20   25                 0          5    10                15   20   25

                                  sigma(eta)                                                     sigma(eta)


Figure 2: Bias and RMSE of LCM. The two top panels illustrate bias of LCM coefficient estimates in two
scenarios when either (B, B ) = (0, 1.2) (H0 ) or (B, B ) = (1.2, 0) (HA ) as a function of the standard deviation of
the weakly dependent return innovations,  . The two bottom panels provide corresponding RMSEs. Two versions
of LCM is considered: An unrestricted LCM (LCMU), which uses the (biased) estimates d1 and d2 from Figure
1; an oracle LCM (LCMO), where d1 = d2 = d is treated as known in the fractional filtering. Both versions of
LCM are implemented with (, ) = (0.2, 0.6). The left- and right-hand-side panels have d = 0.45 and d = 0.80,
respectively. The dotted vertical line highlights the value of  where the empirical (unrestricted) estimate, d1 , is
no longer significantly different from zero. Finally, we consider a sample size n = 650 and use 1000 replications.




                                                              32
                              "Size" Properties of the Rank Test: d = 0.45
     Panel A          G   = 0.40,  =               G = 0.50,  =                G              = 0.60,    =
       =          0.10      0.20    0.30       0.10    0.20    0.30        0.10                 0.20      0.30
        0        0.2780    0.0530 0.0080      0.0540 0.0010 0.0000       0.0010                0.0000    0.0000
       2.5       0.2820    0.0530 0.0130      0.0660 0.0010 0.0000       0.0010                0.0000    0.0000
        5        0.2870    0.0580 0.0100      0.0610 0.0010 0.0000       0.0020                0.0000    0.0000
       7.5       0.3000    0.0530 0.0100      0.0570 0.0000 0.0000       0.0020                0.0000    0.0000
       10        0.2980    0.0530 0.0110      0.0550 0.0000 0.0000       0.0020                0.0000    0.0000
      12.5       0.2950    0.0560 0.0090      0.0550 0.0000 0.0000       0.0020                0.0000    0.0000
       15        0.2960    0.0570 0.0090      0.0530 0.0000 0.0000       0.0010                0.0000    0.0000
      17.5       0.2940    0.0600 0.0090      0.0520 0.0000 0.0000       0.0010                0.0000    0.0000
       20        0.2970    0.0630 0.0090      0.0520 0.0000 0.0000       0.0010                0.0000    0.0000
      22.5       0.2980    0.0620 0.0090      0.0530 0.0000 0.0000       0.0010                0.0000    0.0000
       25        0.3020    0.0620 0.0090      0.0530 0.0000 0.0000       0.0010                0.0000    0.0000
     Panel B          G   = 0.40,  =               G = 0.50,  =                G              = 0.60,    =
       =          0.10      0.20    0.30       0.10    0.20    0.30        0.10                 0.20      0.30
        0        0.1360    0.0040 0.0000      0.0020 0.0000 0.0000       0.0000                0.0000    0.0000
       2.5       0.1120    0.0050 0.0000      0.0030 0.0000 0.0000       0.0000                0.0000    0.0000
        5        0.1140    0.0100 0.0000      0.0050 0.0000 0.0000       0.0000                0.0000    0.0000
       7.5       0.1110    0.0090 0.0000      0.0060 0.0000 0.0000       0.0000                0.0000    0.0000
       10        0.1120    0.0100 0.0000      0.0060 0.0000 0.0000       0.0000                0.0000    0.0000
      12.5       0.1160    0.0100 0.0000      0.0060 0.0000 0.0000       0.0000                0.0000    0.0000
       15        0.1180    0.0100 0.0000      0.0060 0.0000 0.0000       0.0000                0.0000    0.0000
      17.5       0.1230    0.0110 0.0000      0.0050 0.0000 0.0000       0.0000                0.0000    0.0000
       20        0.1250    0.0110 0.0010      0.0040 0.0000 0.0000       0.0000                0.0000    0.0000
      22.5       0.1250    0.0100 0.0010      0.0040 0.0000 0.0000       0.0000                0.0000    0.0000
       25        0.1230    0.0110 0.0010      0.0040 0.0000 0.0000       0.0000                0.0000    0.0000

Table 1: "Size" results. This tables show the frequency of rejecting full rank H0 : r = 0 in favor of finding
reduced rank HA : r = 1 when H0 is correct, using the LCM-based rank selection procedure in (22) and (23). This
is in analogy with the size properties of a test. The memory of the system is d = 0.45, and the standard deviation of
the weakly dependent return innovations,  , is varied in [0, 25]. The rank test is implemented with the restricted
estimate d2 for both yt and xt , and the trimming rate G = 0.20 is fixed. Moreover, we consider tuning parameter
selections G = {0.4, 0.5, 0.6} and  = {0.1, 0.2, 0.3}. The analysis uses two sample sizes, n = 650 and n = 2000, in
Panels A and B, respectively, and 1000 replications. Finally, the dashed horizontal line highlights the value of  in
Figure 1, where the empirical (unrestricted) estimate, d1 , is no longer significantly different from zero.




                                                        33
                              "Size" Properties of the Rank Test: d = 0.80
     Panel A          G   = 0.40,  =               G = 0.50,  =                G              = 0.60,    =
       =          0.10      0.20    0.30       0.10    0.20    0.30        0.10                 0.20      0.30
        0        0.2780    0.0530 0.0080      0.0540 0.0010 0.0000       0.0010                0.0000    0.0000
       2.5       0.2860    0.0470 0.0070      0.0550 0.0020 0.0000       0.0030                0.0000    0.0000
        5        0.2990    0.0560 0.0110      0.0530 0.0010 0.0000       0.0020                0.0000    0.0000
       7.5       0.3030    0.0540 0.0090      0.0530 0.0010 0.0000       0.0020                0.0000    0.0000
       10        0.2950    0.0580 0.0120      0.0540 0.0010 0.0000       0.0020                0.0000    0.0000
      12.5       0.2970    0.0580 0.0130      0.0530 0.0010 0.0000       0.0020                0.0000    0.0000
       15        0.2960    0.0620 0.0100      0.0560 0.0000 0.0000       0.0000                0.0000    0.0000
      17.5       0.3010    0.0620 0.0110      0.0560 0.0000 0.0000       0.0000                0.0000    0.0000
       20        0.3000    0.0650 0.0110      0.0570 0.0000 0.0000       0.0000                0.0000    0.0000
      22.5       0.3010    0.0630 0.0090      0.0560 0.0000 0.0000       0.0000                0.0000    0.0000
       25        0.3030    0.0650 0.0080      0.0560 0.0000 0.0000       0.0000                0.0000    0.0000
     Panel B          G   = 0.40,  =               G = 0.50,  =                G              = 0.60,    =
       =          0.10      0.20    0.30       0.10    0.20    0.30        0.10                 0.20      0.30
        0        0.1360    0.0040 0.0000      0.0020 0.0000 0.0000       0.0000                0.0000    0.0000
       2.5       0.1290    0.0030 0.0000      0.0010 0.0000 0.0000       0.0000                0.0000    0.0000
        5        0.1270    0.0050 0.0000      0.0030 0.0000 0.0000       0.0000                0.0000    0.0000
       7.5       0.1220    0.0050 0.0000      0.0030 0.0000 0.0000       0.0000                0.0000    0.0000
       10        0.1200    0.0070 0.0000      0.0050 0.0000 0.0000       0.0000                0.0000    0.0000
      12.5       0.1090    0.0070 0.0010      0.0060 0.0000 0.0000       0.0000                0.0000    0.0000
       15        0.1050    0.0070 0.0010      0.0050 0.0000 0.0000       0.0000                0.0000    0.0000
      17.5       0.1050    0.0060 0.0010      0.0050 0.0000 0.0000       0.0000                0.0000    0.0000
       20        0.1080    0.0060 0.0010      0.0050 0.0000 0.0000       0.0000                0.0000    0.0000
      22.5       0.1080    0.0070 0.0010      0.0060 0.0000 0.0000       0.0000                0.0000    0.0000
       25        0.1070    0.0070 0.0010      0.0060 0.0000 0.0000       0.0000                0.0000    0.0000

Table 2: "Size" results. This tables show the frequency of rejecting full rank H0 : r = 0 in favor of finding
reduced rank HA : r = 1 when H0 is correct, using the LCM-based rank selection procedure in (22) and (23). This
is in analogy with the size properties of a test. The memory of the system is d = 0.80, and the standard deviation of
the weakly dependent return innovations,  , is varied in [0, 25]. The rank test is implemented with the restricted
estimate d2 for both yt and xt , and the trimming rate G = 0.20 is fixed. Moreover, we consider tuning parameter
selections G = {0.4, 0.5, 0.6} and  = {0.1, 0.2, 0.3}. The analysis uses two sample sizes, n = 650 and n = 2000, in
Panels A and B, respectively, and 1000 replications. Finally, the dashed horizontal line highlights the value of  in
Figure 1, where the empirical (unrestricted) estimate, d1 , is no longer significantly different from zero.




                                                        34
                             "Power" Properties of the Rank Test: d = 0.45
     Panel A          G   = 0.40,  =              G = 0.50,  =               G                = 0.60,    =
       =          0.10      0.20    0.30      0.10    0.20    0.30       0.10                   0.20      0.30
        0        1.0000    1.0000 1.0000    1.0000 1.0000 1.0000        1.0000                 1.0000    1.0000
       2.5       1.0000    1.0000 0.9990    1.0000 1.0000 0.9760        1.0000                 0.9800    0.3950
        5        0.9850    0.8930 0.6910    0.9720 0.6530 0.1370        0.8470                 0.0630    0.0000
       7.5       0.8740    0.6140 0.2990    0.7760 0.1950 0.0120        0.3650                 0.0010    0.0000
       10        0.7370    0.4070 0.1400    0.5450 0.0680 0.0030        0.1410                 0.0000    0.0000
      12.5       0.6260    0.2670 0.0940    0.3830 0.0280 0.0020        0.0690                 0.0000    0.0000
       15        0.5460    0.1980 0.0620    0.2710 0.0160 0.0000        0.0390                 0.0000    0.0000
      17.5       0.5040    0.1540 0.0460    0.2300 0.0110 0.0000        0.0230                 0.0000    0.0000
       20        0.4480    0.1340 0.0350    0.1950 0.0090 0.0000        0.0190                 0.0000    0.0000
      22.5       0.4220    0.1210 0.0270    0.1670 0.0080 0.0000        0.0150                 0.0000    0.0000
       25        0.3910    0.1100 0.0220    0.1490 0.0030 0.0000        0.0080                 0.0000    0.0000
     Panel B          G   = 0.40,  =              G = 0.50,  =               G                = 0.60,    =
       =          0.10      0.20    0.30      0.10    0.20    0.30       0.10                   0.20      0.30
        0        1.0000    1.0000 1.0000    1.0000 1.0000 1.0000        1.0000                 1.0000    1.0000
       2.5       1.0000    1.0000 1.0000    1.0000 1.0000 1.0000        1.0000                 1.0000    0.5030
        5        1.0000    0.9930 0.9200    0.9990 0.8620 0.1480        0.9820                 0.0110    0.0000
       7.5       0.9890    0.8370 0.4440    0.9320 0.1930 0.0010        0.3830                 0.0000    0.0000
       10        0.9240    0.5540 0.1590    0.6620 0.0270 0.0000        0.0610                 0.0000    0.0000
      12.5       0.8000    0.3330 0.0620    0.4060 0.0060 0.0000        0.0100                 0.0000    0.0000
       15        0.6830    0.2200 0.0360    0.2590 0.0020 0.0000        0.0010                 0.0000    0.0000
      17.5       0.5740    0.1470 0.0190    0.1760 0.0000 0.0000        0.0000                 0.0000    0.0000
       20        0.4900    0.1030 0.0120    0.1180 0.0000 0.0000        0.0000                 0.0000    0.0000
      22.5       0.4280    0.0770 0.0070    0.0960 0.0000 0.0000        0.0000                 0.0000    0.0000
       25        0.3810    0.0580 0.0050    0.0740 0.0000 0.0000        0.0000                 0.0000    0.0000

Table 3: "Power" results. This tables show the frequency of rejecting full rank H0 : r = 0 in favor of finding
reduced rank HA : r = 1 when HA is correct, using the LCM-based rank selection procedure in (22) and (23). This
is in analogy with the power properties of a test. The memory of the system is d = 0.45, and the standard deviation
of the weakly dependent return innovations,  , is varied in [0, 25]. The rank test is implemented with the restricted
estimate d2 for both yt and xt , and the trimming rate G = 0.20 is fixed. Moreover, we consider tuning parameter
selections G = {0.4, 0.5, 0.6} and  = {0.1, 0.2, 0.3}. The analysis uses two sample sizes, n = 650 and n = 2000, in
Panels A and B, respectively, and 1000 replications. Finally, the dashed horizontal line highlights the value of  in
Figure 1, where the empirical (unrestricted) estimate, d1 , is no longer significantly different from zero.




                                                        35
                             "Power" Properties of the Rank Test: d = 0.80
     Panel A          G   = 0.40,  =              G = 0.50,  =               G                = 0.60,    =
       =          0.10      0.20    0.30      0.10    0.20    0.30       0.10                   0.20      0.30
        0        1.0000    1.0000 1.0000    1.0000 1.0000 1.0000        1.0000                 1.0000    1.0000
       2.5       1.0000    1.0000 1.0000    1.0000 1.0000 1.0000        1.0000                 1.0000    0.9960
        5        1.0000    1.0000 1.0000    1.0000 1.0000 0.9560        1.0000                 0.6670    0.0320
       7.5       1.0000    0.9920 0.9620    0.9980 0.8850 0.4240        0.8590                 0.0640    0.0000
       10        0.9890    0.9500 0.7860    0.9630 0.5560 0.0900        0.5330                 0.0030    0.0000
      12.5       0.9700    0.8340 0.5820    0.8550 0.3070 0.0260        0.2740                 0.0000    0.0000
       15        0.9230    0.7120 0.4040    0.7310 0.1560 0.0070        0.1570                 0.0000    0.0000
      17.5       0.8570    0.5870 0.2910    0.6030 0.0740 0.0040        0.0920                 0.0000    0.0000
       20        0.7990    0.4860 0.1980    0.5020 0.0510 0.0010        0.0580                 0.0000    0.0000
      22.5       0.7400    0.4070 0.1430    0.4310 0.0350 0.0010        0.0420                 0.0000    0.0000
       25        0.6900    0.3450 0.1130    0.3580 0.0240 0.0000        0.0290                 0.0000    0.0000
     Panel B          G   = 0.40,  =              G = 0.50,  =               G                = 0.60,    =
       =          0.10      0.20    0.30      0.10    0.20    0.30       0.10                   0.20      0.30
        0        1.0000    1.0000 1.0000    1.0000 1.0000 1.0000        1.0000                 1.0000    1.0000
       2.5       1.0000    1.0000 1.0000    1.0000 1.0000 1.0000        1.0000                 1.0000    1.0000
        5        1.0000    1.0000 1.0000    1.0000 1.0000 1.0000        1.0000                 0.9950    0.1370
       7.5       1.0000    1.0000 1.0000    1.0000 1.0000 0.9430        0.9980                 0.1800    0.0000
       10        1.0000    0.9990 0.9960    1.0000 0.9710 0.3960        0.9000                 0.0000    0.0000
      12.5       1.0000    0.9970 0.9760    0.9980 0.7380 0.0820        0.5080                 0.0000    0.0000
       15        0.9990    0.9900 0.8850    0.9880 0.3870 0.0190        0.2120                 0.0000    0.0000
      17.5       0.9970    0.9640 0.7600    0.9240 0.1870 0.0020        0.0740                 0.0000    0.0000
       20        0.9920    0.8890 0.5780    0.8300 0.0870 0.0000        0.0190                 0.0000    0.0000
      22.5       0.9830    0.8210 0.4490    0.6940 0.0420 0.0000        0.0050                 0.0000    0.0000
       25        0.9700    0.7330 0.2930    0.5640 0.0220 0.0000        0.0030                 0.0000    0.0000

Table 4: "Power" results. This tables show the frequency of rejecting full rank H0 : r = 0 in favor of finding
reduced rank HA : r = 1 when HA is correct, using the LCM-based rank selection procedure in (22) and (23). This
is in analogy with the power properties of a test. The memory of the system is d = 0.80, and the standard deviation
of the weakly dependent return innovations,  , is varied in [0, 25]. The rank test is implemented with the restricted
estimate d2 for both yt and xt , and the trimming rate G = 0.20 is fixed. Moreover, we consider tuning parameter
selections G = {0.4, 0.5, 0.6} and  = {0.1, 0.2, 0.3}. The analysis uses two sample sizes, n = 650 and n = 2000, in
Panels A and B, respectively, and 1000 replications. Finally, the dashed horizontal line highlights the value of  in
Figure 1, where the empirical (unrestricted) estimate, d1 , is no longer significantly different from zero.




                                                        36
                     RLCM bias: d = 0.45                                           RLCM bias: d = 0.80
                 RLCM, H0                                                      RLCM, H0
       0.4




                                                                     0.4
                 LCMO, H0                                                      LCMO, H0
                 RLCM, HA                                                      RLCM, HA
                 LCMO, HA                                                      LCMO, HA
       0.3




                                                                     0.3
bias




                                                              bias
       0.2




                                                                     0.2
       0.1




                                                                     0.1
       0.0




                                                                     0.0
             0          5   10                15   20   25                 0          5   10                15   20   25

                                 sigma(eta)                                                    sigma(eta)




                  RLCM RMSE: d = 0.45                                           RLCM RMSE: d = 0.80
                 RLCM, H0                                                      RLCM, H0




                                                                     1.2
                 LCMO, H0                                                      LCMO, H0
       1.5




                 RLCM, HA                                                      RLCM, HA




                                                                     1.0
                 LCMO, HA                                                      LCMO, HA




                                                                     0.8
       1.0
RMSE




                                                              RMSE

                                                                     0.6
                                                                     0.4
       0.5




                                                                     0.2
       0.0




                                                                     0.0




             0          5   10                15   20   25                 0          5   10                15   20   25

                                 sigma(eta)                                                    sigma(eta)


Figure 3: Bias and RMSE of RLCM. The two top panels illustrate bias of LCM coefficient estimates in two
scenarios when either (B, B ) = (0, 1.2) (H0 ) or (B, B ) = (1.2, 0) (HA ) as a function of the standard deviation of
the weakly dependent return innovations,  . The two bottom panels provide corresponding RMSEs. Two versions
of LCM is considered: A rank-augmented LCM (RLCM), which uses a rank-test based estimate of d1 as well as the
TELW estimate d2 from Figure 1; an oracle LCM (LCMO), where d1 = d2 = d is treated as known in the fractional
filtering. Both versions of LCM use (, ) = (0.2, 0.6). The left- and right-hand-side panels have d = 0.45 and
d = 0.80. The dotted vertical line highlights the value of  where the empirical (unrestricted) estimate, d1 , is no
longer significantly different from zero. Finally, we consider a sample size n = 650 and use 1000 replications.




                                                             37
                                     Temporal      Dependence and Rank
                                   Returnst         RVt-1     DSt-1              PEt-1        TBt-1
               Mean                 0.0055          0.0021    0.1346            0.0290        0.0468
               Std. Dev.            0.0429          0.0047    0.0567             0.0042       0.0294
               Skewness            -0.6833         11.0620    2.4834            -0.3293       0.5800
               Kurtosis             5.5338         161.5564  12.5860            2.5694        3.7378
               ACF(1)               0.0527          0.4287    0.9663            0.9955        0.9887
               TELW                 0.0631          0.2853    0.9405            1.0290        0.9188
                                     (0.0516)       (0.0516)       (0.0516)     (0.0516)      (0.0516)
               ELWM                  0.0662         0.2882         0.8668       1.1107        0.9019
                                     (0.0516)       (0.0516)       (0.0516)     (0.0516)      (0.0516)
               L(   = 0)-unr            -           -0.8026       -0.8026       -0.8026       -0.8026
               L(   = 1)-unr            -           -0.9305       -0.6227       -0.8088       -0.5697
               r                        -              1             0             1             0
               L(   = 0)-res            -           -0.8026       -0.8026       -0.8026       -0.8026
               L(   = 1)-res            -           -0.5473       -0.6079       -0.8088       -0.6252
               r                        -              0             0             1             0

Table 5: Descriptive statistics. This table displays statistics describing the unconditional and temporal depen-
dence properties of returns and the four candidate predictors: RV, DS, PE and TB. Specifically, for the latter, we
provide estimates of the first-order autocorrelation function (ACF), trimmed exact local Whittle (TELW) estimator
of the fractional integration order (Andersen & Varneskov 2020) as well as exact local Whittle (ELWM) estimates
with correction for the mean, or initial value, (Shimotsu 2010). The ELW estimators are implemented with band-
width n0.7 and, for TELW, trimming n0.1 to reduce sensitivity to the mean. Moreover, LCM rank tests uses
(G ,  ) = (0.4, 0.2). The rank tests are implemented using the individual TELW-estimated integration orders of
the predictors (unr) and the integration order for PE in all tests (res), following the two-step procedure in Section
3.2. Finally, the sample of monthly observations spans March 1960 through March 2015 (n = 661).




                                  RLCM Analysis of Return Predictions
                                     RVt-1                             DSt-1
                          OLS         IVX    RLCM             OLS       IVX                        RLCM
           Bc           -1.1252     -1.1638  0.3257         0.0236     0.0200                      0.1407
           Wald          8.3130      8.2154  1.1619          0.3203    0.2326                      5.7476
           P-Wald       0.0039       0.0042  0.2811          0.5714    0.6296                      0.0165
                                     PEt-1                            TBt-1
                          OLS         IVX    RLCM             OLS       IVX                        RLCM
           Bc           -0.4956     -0.3987 -1.1421         -0.0280   -0.0277                      -0.3953
           Wald          1.3162      0.8460 12.4072          0.2218    0.2098                      2.1507
           P-Wald       0.2513       0.3577  0.0004          0.6377    0.6469                       0.1425

Table 6: Return predictions. This table provides coefficient estimates and significance tests based on Wald
statistics (and associated P-values) for three different methodologies; OLS, IVX and RLCM, where the fractional
integration order of returns have been restricted to that of PE. The integration orders required for the fractional
filtering procedure are provided by the TELW estimates in Table 5. The LCM procedure and feasible inference
(cf., Appendix A.2) are implemented with  = G = 0.2 and  = G = 0.6. Inference for OLS and IVX employs
Eicker-White standard errors. IVX is implemented with two instruments as in Section 4. Finally, the sample of
monthly observations spans March 1960 through March 2015 (n = 661).



                                                        38
References
Andersen, T. G. & Benzoni, L. (2012), Stochastic volatility, in R. A. Meyers, ed., `Encyclopedia of Complexity
    and Systems Science', Springer-Verlag. forthcoming.

Andersen, T. G. & Bollerslev, T. (1998), `Answering the skeptics: Yes, standard volatility models do provide
    accurate forecasts', International Economic Review 39, 885­905.

Andersen, T. G., Bollerslev, T., Diebold, F. X. & Labys, P. (2003), `Modeling and forecasting realized volatility',
    Econometrica 71, 579­625.

Andersen, T. G. & Varneskov, R. T. (2020), `Consistent inference for predictive regressions in persistent economic
    systems', Journal of Econometrics forthcoming.

Bansal, R., Kiku, D., Shaliastovich, I. & Yaron, A. (2014), `Volatility, the macroeconomy and asset prices', The
    Journal of Finance LXIX, 2471­2511.

Bansal, R. & Yaron, A. (2004), `Risks for the long run: A potential resolution of asset pricing puzzles', Journal
    of Finance 59, 1481­1509.

Barndorff-Nielsen, O. E. & Shephard, N. (2002), `Econometric analysis of realized volatility and its use in
    estimating stochastic volatility models', Journal of the Royal Statistical Society Series B 64, 253­280.

Bauer, D. & Maynard, A. (2012), `Persistence-robust surplus-lag granger causality testing', Journal of Econo-
    metrics 169, 293­300.

Boudoukh, J., Richardson, M. & Whitelaw, R. F. (2008), `The myth of long-horizon predictability', Review of
    Financial Studies 21(4), 1577­1605.

Breitung, J. & Demetrescu, M. (2015), `Instrumental variable and variable addition based inference in predictive
     regressions', Journal of Econometrics 187, 358­375.

Brillinger, D. R. (1981), Time Series. Data Analysis and Theory, Siam: Classics in Applied Mathematics.

Campbell, J. Y. (2018), Financial Decisions and Markets: A Course in Asset Pricing, Princeton University
   Press.

Campbell, J. Y., Giglio, S., Polk, C. & Turley, R. (2018), `An intertemporal CAPM with stochastic volatility',
   Journal of Financial Economics 128, 207­233.

Campbell, J. Y. & Yogo, M. (2006), `Efficient tests of stock return predictability', Journal of Financial Eco-
   nomics 81, 27­60.

Cavanagh, C., Elliott, G. & Stock, J. (1995), `Inference in models with nearly integrated regressors', Econometric
    Theory 11, 1131­1147.

Choi, I. (1993), `Asymptotic normality of the least-squares estimates for higher order autoregressive integrated
    processes with some applications', Econometric Theory 9, 263­282.

Christensen, B. J. & Nielsen, M. O. (2006), `Asymptotic normality of narrow-band least squares in the stationary
     fractional cointegration model and volatility forecasting', Journal of Econometrics 133, 343­371.

Christensen, B. J. & Varneskov, R. T. (2017), `Medium band least squares estimation of fractional cointegration
     in the presence of low-frequency contamination', Journal of Econometrics 197, 218­244.

                                                        39
Dolado, J. J. & L¨
                 utkepohl, H. (1996), `Making wald tests work for cointegrated var systems', Econometric
    Reviews 15, 369­386.
Elliott, G., M¨
              uller, U. & Watson, M. (2015), `Nearly optimal tests when a nuisance parameter is present under
     the null hypothesis', Econometrica 83, 771­811.
Fama, E. F. (1970), `Efficient capital markets: A review of theory and empirical work', Journal of Finance
    25, 383­417.
Ferson, W. E., Sarkissian, S. & Simin, T. (2003), `Spurious regressions in financial economics', Journal of
     Finance 58, 1393­1414.
Gabaix, X. (2012), `Variable rare disasters: An exactly solved framework for ten puzzles in macro finance',
    Quarterly Journal of Economics 127, 645­700.
Georgiev, I., Harvey, D. I., Leybourne, S. J. & Taylor, A. R. (2020), `A bootstrap stationarity test for predictive
    regression invalidity', Journal of Business & Economic Statistics forthcoming.
Granger, C. V. J. & Newbold, P. (1974), `Spurious regression in econometrics', Journal of Econometrics 2, 111­
    120.
Hamilton, J. D. (1994), Time Series Analysis, Princeton University Press, Princeton, New Jersey.
Hong, Y. (1996), `Testing for independence between two covariance stationary time series', Biometrika 83, 615­
    625.
Hualde, J. & Robinson, P. M. (2011), `Gaussian pseudo-maximum likelihood estimation of fractional time series
    models', Annals of Statistics 39, 3152­3181.
Johansen, S. & Nielsen, M. O. (2012), `Likelihood inference for a fractionally cointegrated vector autoregressive
    model', Econometrica 80, 2667­2732.
Kendall, M. (1954), `Note on bias in the estimation of autocorrelation', Biometrika 41, 403­404.
Kostakis, A., Magdalinos, T. & Stamatogiannis, M. P. (2015), `Robust econometric inference for stock return
    predictability', Review of Financial Studies 28, 1506­1553.
Lettau, M. & Ludvigson, S. (2001), `Consumption, aggregate wealth, and expected stock returns', Journal of
     Finance 56, 815­849.
Lettau, M. & Ludvigson, S. (2010), Measuring and modeling variation in risk-return trade-off, in Y. A¨      it-
     Sahalia & L. P. Hansen, eds, `Handbook of Financial Econometrics', Elsevier Science B. V., North Holland,
     Amsterdam.
Lettau, M., Ludvigson, S. & Wachter, J. (2008), `The declining equity premium: What role does macroeconomic
     risk play?', Review of Financial Studies 21, 1653­1687.
Lin, Y. & Tu, Y. (2020), `Robust inference for spurious regressions and cointegrations involving processes
     moderately deviated from a unit root', Journal of Econometrics forthcoming.
Liu, X., Yang, B., Cai, Z. & Peng, L. (2019), `A unified test for predictability of asset returns regardless of
     properties of predicting variables', Journal of Econometrics 208, 141­159.
Lobato, I. (1999), `A semiparametric two-step estimator in a multivariate long memory model', Journal of
    Econometrics 90, 129­155.

                                                        40
Magdalinos, T. & Phillips, P. C. B. (2009), Econometric inference in the vicinity of unity. CoFie Working Paper
   (7), Singapore Management University.

Marriott, F. & Pope, J. (1954), `Bias in the estimation of autocorrelations', Biometrika 41, 390­402.

Mikusheva, A. (2007), `Uniform inference in autoregressive models', Econometrica 75, 1411­1452.

Neuhierl, A. & Varneskov, R. T. (2020), `Frequency dependent risk', Journal of Financial Economics forth-
    coming.

Newey, W. K. & West, K. D. (1987), `A simple positive semi-definite, heteroskedasticity and autocorrelation
    consistent covariance matrix', Econometrica 55, 703­708.

Nielsen, M. O. (2015), `Asymptotics for the conditional-sum-of-squares estimator in mutivariate fractional time
     series models', Journal of Time Series Analysis 36, 154­188.

Nielsen, M. O. & Shimotsu, K. (2007), `Determining the cointegrating rank in nonstationary fractional systems
     by the exact local whittle approach', Journal of Econometrics 141, 574­596.

Ortu, F., Tamoni, A. & Tebaldi, C. (2013), `Long-run risk and the persistence of consumption shocks', Review
    of Financial Studies 26, 2876­2915.

Park, J. Y. & Phillips, P. C. B. (1989), `Statistical inference in regressions with integrated processes: Part 2',
    Econometric Theory 5, 95­131.

Pastor, L. & Stambaugh, R. F. (2009), `Predictive systems: Living with imperfect predictors', Journal of Finance
    64, 1583­1628.

Phillips, P. C. B. (1986), `Understanding spurious regressions in econometrics', Journal of Econometrics 33, 311­
     340.

Phillips, P. C. B. (1987), `Towards a unified asymptotic theory for autoregression', Biometrika 74, 535­547.

Phillips, P. C. B. (2014), `On confidence intervals for autoregressive roots and predictive regression', Economet-
     rica 82, 1177­1195.

Phillips, P. C. B. (2015), `Halbert White Jr. memorial JFEC lecture: Pitfalls and possibilities in predictive
     regression', Journal of Financial Econometrics 13, 521­555.

Phillips, P. C. B. & Lee, J. H. (2013), `Predictive regression under various degrees of persistence and robust
     long-horizon regression', Journal of Econometrics 177, 250­264.

Phillips, P. C. B. & Lee, J. H. (2016), `Robust econometric inference with mixed integrated and mildly explosive
     regressors', Journal of Econometrics 192, 433­450.

Phillips, P. C. B. & Magdalinos, T. (2007), `Limit theory for moderate deviations from a unit root', Journal of
     Econometrics 136, 115­130.

Phillips, P. C. B. & Ouliaris, S. (1988), `Testing for cointegration using principal component methods', Journal
     of Economic Dynamics and Control 12, 205­230.

Ren, Y., Tu, Y. & Yi, Y. (2019), `Balanced predictive regressions', Journal of Empirical Finance 54, 118­142.

Robinson, P. M. (1995), `Gaussian semiparametric estimation of long range dependence', The Annals of Statistics
    23, 1630­1661.

                                                       41
Robinson, P. M. & Hualde, J. (2003), `Cointegration in fractional systems with unknown integration orders',
    Econometrica 71, 1727­1766.

Robinson, P. M. & Marinucci, D. (2003), `Semiparametric frequency domain analysis of fractional cointegration'.
    In: Robinson, P.M. (Ed.), Time Series with Long Memory. Oxford University Press, Oxford, pp. 334-373.

Robinson, P. M. & Yajima, Y. (2002), `Determination of cointegrating rank in fractional systems', Journal of
    Econometrics 106, 217­241.

Shao, X. (2009), `A generalized portmanteau test for independence between two stationary time series', Econo-
    metric Theory 25, 195­210.

Shenton, L. & Johnson, W. (1965), `Moments of a serial correlation coefficient', Journal of the Royal Statistical
    Society, Series B 27, 308­320.

Shiller, R. (2000), Irrational Exuberance, Princeton University Press, United States.

Shimotsu, K. (2010), `Exact local whittle estimation of fractional integration with unkown mean and time trend',
    Econometric Theory 26, 501­540.

Shimotsu, K. & Phillips, P. C. B. (2005), `Exact local whittle estimation of fractional integration', The Annals
    of Statistics 32, 656­692.

Sims, C. A., Stock, J. H. & Watson, M. W. (1990), `Inference in linear time series models with some unit roots',
    Econometrica 58, 113­144.

Stambaugh, R. F. (1986), Bias in regressions with lagged stochastic regressors. Working Paper 156, CRSP,
    Graduate School of Business, University of Chicago.

Stambaugh, R. F. (1999), `Predictive regressions', Journal of Financial Economics 54, 783­820.

Toda, H. & Yamamoto, T. (1995), `Statistical inference in vector autoregressions with possibly integrated
    processes', Journal of Econometrics 66, 225­250.

Tsay, W.-J. & Chung, C.-F. (2000), `The spurious regression of fractionally integrated processes', Journal of
    Econometrics 96, 155­182.

Valkanov, R. (2003), `Long-horizon regressions: Theoretical results and applications', Journal of Financial
    Economics 68, 201­232.

Varneskov, R. T. (2017), `Estimating the quadratic variation spectrum of noisy asset prices using generalized
    flat-top realized kernels', Econometric Theory 33(6), 1457­1501.

Welch, I. & Goyal, A. (2008), `A comprehensive look at the empirical performance of equity premium prediction',
    Review of Financial Studies 21, 1455­1508.

Xu, K.-L. (2020), Testing for return predictability with co-moving predictors of unknown form. Unpublished
    manuscript, Department of Economics, Indiana University.


A     Additional Theory and Feasible Inference
This section presents an alternative rank test and describes how to draw feasible inference.


                                                       42
A.1    Alternative Cointegration Rank Test
The eigenvalues i and ^i , i = 1, . . . , k +1, defined in main text, may be utilized to design a cointegration
rank test of the null hypothesis H0 against HA based on the ratio statistics

                                             ^k+1                         ^2     k   ^2      ^2 (    k   ^ 2
                       k+1                                                 k+1   i=1 i    +   k+1    i=1 i )
            k+1 =       k+1
                                ,   k+1 =     k+1 ^
                                                         ,    2
                                                              k+1 =                       k+1 ^ 4
                                                                                                               ,     (A.1)
                        i=1 i                 i=1 i                                 (     i=1 i )

whose asymptotic properties under H0 follow from Theorem 1 and the delta method.

Theorem 4. Under the conditions of Theorem 1 and H0 ,

                                      1/2                                  D
                                    mG      k+1 - k+1 /k+1 -
                                                            N (0, 1).

   As noted by Phillips & Ouliaris (1988) and Robinson & Yajima (2002) in (fractional) cointegration
contexts, Theorem 4 relies on H0 and cannot be used to test against HA , because the distribution
becomes degenerate for k+1  0. However, for testing whether that the cointegration rank is unity,
that is, against HA , we may use the 100(1 - )% upper confidence interval for k+1 ,

                                                                                  1/2
                                    CI (, k + 1) = k+1 + k+1 Q()/mG ,                                                (A.2)

with Q() being the (1 - )th quantile of the standard Gaussian distribution, and compare it to a
pre-specified threshold, as suggested by Phillips & Ouliaris (1988). Following their recommendation,
and motivated by the numerical results in Nielsen & Shimotsu (2007), we have applied the test with
a threshold 0.1/(k + 1) in the simulation study in Section 4. However, the test is strictly dominated
by the selection procedure based on (22) and (23). These results are left our for brevity.

A.2    Feasible Inference
When drawing feasible inference with the LCM approach, we must provide consistent estimators of
the long-run covariance matrix Guu and either G or G , depending on whether we are drawing
inference for models (ii)-(iii) or the cointegration model (iv). To this end, we need information from
the residuals, after estimation of B. The main challenge is, again, that we observe v  c , not v
                                                                                      ^t       ^t nor vt .
As a result, the residuals t are latent, and we need to estimate them as,

                                             (b,c)            (b,c)
                         c
                         t = (1 - L)-b t             ,       t         = et - Bc ( , m) uc
                                                                                         t-1 ,                       (A.3)

                                                                                           (b)
where b is some consistent estimator of b. Similarly, we can define t and t as in (A.3), but computed
with (the unobservable) B( , m) and ut-1 and, thus, free of regressor endogeneity bias.
                                                                     c and                       (b,c)
   Despite the notation, it is important to note that the estimators t      t                            can be utilized to
estimate both G in models (ii) and (iii) as well as G in (iv). Specifically, when drawing inference for
                                          c =                (b,c)
the former, we estimate the variance with t   t                      , where the contribution from t-1 will dominate

                                                             43
           (b)
that from t , b = d1 > 0, asymptotically. In contrast, for model (iv), we use (A.3) the consistent
                                                                                           c is
estimate b = d1 , following Assumption M, since t-1 = 0, t = 1, . . . n. Once the estimate t
computed for a given inference scenario, we, then, use the trimmed long-run covariance estimators,
described in Section 3 for the purpose of rank testing, and compute the asymptotic variance as,
                          
                          Gc (
                       -1              G , mG )/(2m),                    under models (ii) and (iii),
    AVAR = Gc (
            uu G , mG )                                                                                        (A.4)
                          Gc (                 2b
                                       G , mG )m /      2(1 + 2b)m ,     under model (iv).

with, again, mG = mG (n) and   G   =   G (n).   Specifically, we determine which of the inference scenarios
to apply by means of the rank selection procedure in (22) and (23).
                                                                                          
Assumption B. Let mb        n be a sequence of integers where 0 <   1, then b - b = Op (1/ mb ).
                                                                                 ln(n)      ln(n)      b
Assumption T-B. Define mn = md  mb  m and g
                                          ¯n (m, mb , md ) =                     
                                                                                   mb       
                                                                                              md       m ,
                                                                                                       m
                                                                                                             then the
following cross-restrictions are imposed on the trimming and bandwidth parameters:
                                                                   b
                   n   m   mG                                 n
                     +   +    +g
                               ¯n (m, mb , md )                         0,     as     n  .
                  mG mG    G                                 mG

Finally, additional conditions are imposed, when n  , depending on the model:
                 
                 (m /n)dx / 1+ + (m/n)dx /         1+    0,    under models (ii) and (iii),
                   G        G
                 (m /n)d-b / 1+  0,                            under model (iv).
                     G         G


  Assumption B is similar to Andersen & Varneskov (2020, Assumption B), imposing a mild consis-
tency requirement on the estimator of the (fractional) integration order of the residuals. Moreover,
Assumption T-B imposes additional (mild) conditions on the trimming and bandwidth parameters to
eliminate the endogenous regressor bias when estimating the variance of the residuals. In addition to
these, we invoke Assumption T-G for the covariance estimators. However, it is worth noting that the
necessary conditions on the tuning parameters for consistency of the asymptotic variance estimator
for feasible inference are milder than those stated in Assumption T-G. In particular, instead of the
first three conditions, we require only mG         nG and              nG , with 0 < G < G <                  1 and
n/(mG 2      2     2
      G ) + n /(mG G mn )    0 as n  , the same as in Andersen & Varneskov (2020, Assumption
                                                             
T-G). Similarly, the equivalent condition four is a factor 1/ mG smaller than the one stated. The
reason is that we only need consistency for feasible inference, not a central limit theorem. However,
we refrain from stating separate assumptions to distinguish the two cases.

Theorem 5. Suppose Assumption B, T-B and the conditions of Theorems 1 and 3 hold, then
                 
                         G-
                 mAVAR -P    1
                           uu G /2,                               under models (ii) and (iii),
                 m-2b AVAR -
                            G-
                           P    1
                   m           uu G /(2(1 + 2b)),                 under model (iv).


                                                        44
         Feasible inference and testing for the LCM procedure, then, follows by applying Theorems 3 and 5
     in conjunction with the continuous mapping theorem and Slutsky's theorem.


     B     Proofs
     This section provides proofs of the main asymptotic results in the paper. Before proceeding, however,
     we introduce some notation. For a generic vector V , let V (i) index the ith element, and, similarly, for
     a matrix M , let M (i, q ) denote its (i, q )th element. Moreover, K  (0, ) denotes a generic constant,
     which may take different values from line to line or from (in)equality to (in)equality. Sometimes
     the (stochastic) orders refer to scalars, sometimes to vectors and matrices. We refrain from making
     distinctions. Finally, we provide an auxiliary lemma that expands on Theorem 4 in Andersen &
     Varneskov (2020). We will henceforth refer to the latter as AV (2020) and, similarly, to their Online
     Appendix as AVOA (2020). Specifically, the lemma provides bounds for the differences,

                                c                                 c
                               Fuu ( , m) - Fuu ( , m),          Fue ( , m) - Fue ( , m),                                          (B.1)
                               Gc
                                v v ( G , mG )   - Gvv (   G , mG ),        Gc
                                                                               ( G , mG )   - G (   G , mG ),                      (B.2)

                                          c constitutes an estimate of  that is employed when imple-
     where, as described in Appendix A.2, t                            t
     menting feasible inference. In other words, we provide asymptotic bounds to describe the errors arising
                                                        c rather than the unobservable v when calculating
     when using the fractionally filtered observations vt                               t
     key measures and statistics, thus quantifying the impact of regressor endogeneity. The auxiliary lemma
     differs from AV (2020, Theorem 4) by allowing for cointegration, b > 0.

     Lemma B.1. Suppose Assumptions D1-D3, C, M, F, T-G, T hold. Moreover, suppose that the
     bandwidths satisfy n1/2 /m  0, n1/2 /mG  0, then, for some arbitrarily small                               > 0, it follows,

      (a) - 1 c                                dx
          m Fuu ( , m) - Fuu ( , m) = Op ((m/n) /
                                                                       1+    ),

      (b) - 1 c                                dx
          m Fue ( , m) - Fue ( , m) = Op ((m/n) /
                                                                       1+    ),
                                                                             1+
      (c) Gc
           uu (    G , mG )   - Guu (   G , mG )    Op ((mG /n)dx /          G    ),
                                                                            1+
      (d) Gc
           ue (   G , mG )    - Gue (   G , mG )    Op ((mG /n)dx /         G     ),

      (e) Suppose further Assumption B and T-B (instead of T-G) hold as well as d - b  0, then
                                          
                                          O ((m /n)dx / 1+ ) + O ((m/n)dx /                               1+    ),   for (ii)-(iii),
                                           p   G        G       p
               Gc
                 G( , mG ) - G (
                                G , mG ) 
                                          O (m /n)d-b / 1+ ,                                                         for (iv).
                                                             p         G               G


     Proof. First, (a) and (c) follows directly from AV (2020, Theorems 4(a) and 4(c)), since the specifi-
     cation of the regressors in this paper readily follows their framework.23
23
     While AV (2020) state their results for d rather than dx to maintain notational simplicity in their framework, it is clear


                                                                       45
                                                         (1)          (2)
   For (b), let us first define et = et + et , where

                 (1)                                                                                             (-d )            (2)
               et       (1 - L)d1 a + B Q(L)(1 - L)d1 xt-1 + (1 - L)d1 t-1 1 ,                                                 et        (1 - L)d1 t ,                    (B.3)

                                          (1)
for which the component et                       is equivalent to the case without cointegration considered by AV (2020,
Theorem 4(b)) due to Assumptions D1-D3 and C. By applying the decomposition (B.3), we have

                                 c                         c                                    (c,1)                          (c,2)
                                Fue ( , m) - Fue ( , m) = Fce ( , m) = Fce                                ( , m) + Fce                 ( , m)                             (B.4)

             (c,1)                       (c,2)                                                                              (1)                                 (2)
where Fce            ( , m) and Fce              ( , m) are the TDAC between ct-1 and et , respectively, et . Now, by
applying, AV (2020, Theorem 4(b)) and AVOA (2020, Lemma A.12(b)), we have,

                     (c,1)                                                       (2)
         - 1
         m Fce               ( , m)  Op ((m/n)dx /               1+
                                                                       ),    we (j ) = Op (d1
                                                                                           j ),                         wc (j , i) = Op (di
                                                                                                                                         j ),                             (B.5)

for i = 2, . . . , k + 1. Hence, since 0 < di  d1 + di , i = 2, . . . , k + 1, we may further write

                                    m                                       m                        dx
     (c,2)                   2                   d       2m1+dx                             j                1                           m      1+dx        1
   Fce       ( , m)                     Op (j x )                                Op                                      Op                                           ,   (B.6)
                              n                           n1+dx                             m             j 1+                           n              1+
                                  j=                                        j=

                                                                      m
for some arbitrarily small                   > 0, using |             j=    Op (j -p )|  Op (             -p )    for some p > 1 by Varneskov (2017,
                                                                                                                      (c,1)                         (c,2)
Lemma C.4). The stated result follows by combining bounds for Fce                                                             ( , m) and Fce                ( , m).
   For (d), by applying the same decomposition as for (b), we have

                                                                                                     (c,1)                              (c,2)
                Gc
                 ue (        G , mG )   - Gue (       G , mG )    = Gc
                                                                     ce (        G , mG )   = Gce (              G , mG )       + Gce (          G , mG ),                (B.7)

                                                                                       (2)
where, again, the DFT bounds in (B.5) apply to we (j ) and wc (j , i). Moreover, by AV (2020,
Theorem 4(c)), we have
                                                       (c,1)                                                     1+
                                                     Gce (     G , mG )       Op ((mG /n)dx /                    G     ).                                                 (B.8)

Next, using, again, 0 < di  d1 + di , i = 2, . . . , k + 1, we may similarly write
                                                                            mG
                        (c,2)                            1                             d
                     Gce (        G , mG )                                        Op (j x )
                                                     mG - G + 1
                                                                  j=         G
                                                         dx mG                          dx
                                                     KmG                           j             1                                mG     dx     1
                                                                 Op                                           Op                                1+          ,             (B.9)
                                                      ndx                         mG           j 1+                                n            G
                                                            j= G


using mG /(mG -                 G   + 1)  K and Varneskov (2017, Lemma C.4). The stated result follows by
                                                          (c,1)                              (c,2)
combining asymptotic bounds for Gce (                                 G , mG )    and Gce (               G , mG ).




that their results apply to dx as the parameter appears when applying the differencing operator to ut-1 and ct-1 .


                                                                                  46
                               c  (1 - L)-b                              (b,c)
   For (e), let us first write t            t                                    , with

                                       (b,c)                                                  (b,1)           (2)          (1)          (2)
                                      t        = et - Bc ( , m) uc
                                                                 t-1 = t                                + et - t-1 - t-1                                           (B.10)

                (1)           (2)                                 (b)           (b,1)             (2)
using et = et + et , where we also let t                                 = t            + et ,

       (b,1)        (1)                                   (1)                                                                     (2)
       t       = et - B( , m) ut-1 ,                    t-1 = (Bc ( , m) - B( , m)) uc
                                                                                     t-1 ,                                       t-1 = B( , m)ct-1 .               (B.11)

The main difference between this decomposition and the corresponding in AV (2020, Theorem 4) is
                          (2)
the presence of et                and the fact that we may have b, b = 0. Hence, we need to distinguish between
cases without cointegration b = b = 0, i.e., scenarios (ii) and (iii), as determined by the cointegration
rank test, and scenario (iv), where b satisfies Assumption B; see Appendix A.2.
                                         c =                                    (0,c)        (0)
   The case without cointegration. Here, t   t                                          , t         = t and let us make the decomposition,

                                                                        (1,1)                                (2,2)                            (1,2)
               Gc
                 (     G , mG )      - G (       G , mG )       = G  (            G , mG )         + G  (                G , mG )   + 2 G  (           G , mG )
                                                                            (1)                                    (2)
                                                                  - 2G (              G , mG )          - 2G (             G , mG ),                               (B.12)

                                                                                                                                                                  (1)
where the first three terms are the (trimmed) long-run variance and covariance estimates for t-1 and
 (2)
t-1 , and the final two terms are the respective long-run covariances with t . Let us further write,

                                     (i)                        (i,1)                              (i,2)
                                    G  (     G , mG )   = G (             G , mG )          + G  (            G , mG ),           i = 1, 2,                        (B.13)

                                                                        (0,1)               (1)               (2)
to indicate the decomposition of t into t                                        = t              and et . Now, in this case, the asymptotic
                      (1,1)                     (2,2)                           (1,2)                              (1,1)                               (1,2)
bounds for G  (                 G , mG ),      G  (       G , mG ),       G  (              G , mG ),         G (              G , mG ),      and G (        G , mG ) are
                                                                                                                                                       (2,2)
derived in equations (A.26), (A.27), (A.29) and (A.30) of AVOA (2020) since                                                                           G  ( G , mG ) 
       (1,1)
Op (G  (        G , mG )).          Hence, to complete the proof, we need to establish corresponding asymptotic
                                     (2,1)                              (2,2)                                                                 (2)
bounds for the terms G (                       G , mG )   and G (                 G , mG ),         i.e., covariances with et . To this end, let us
first use the bounds in (B.5), B( , m) = Op (1), uniformly by AV (2020, Theorem 1), and 0 < di  d1 +di
to write,
                                                                                 mG
                           (2,2)                       1                                                d                      mG       dx    1
                          G ( G , mG )                                                      Op (j x )  Op                                     1+      ,            (B.14)
                                                   mG - G + 1                                                                   n             G
                                                                                 j=     G


similarly to (B.9). Now, make the decomposition,

                          (2,1)                                                                     (2)                            (c,2)
                      G (           G , mG )   = (Bc ( , m) - B( , m))                            Gue (       G , mG )      + G ce (          G , mG )             (B.15)

            (c,2)                                                1+
where Gce (         G , mG )         Op ((mG /n)dx 1/            G       ) by (B.9) and since

                                                                n1/2-di                             ln(n)n1/2
                    wu (j , i) = Op (1) + Op                                      + Op                                     ,      i = 2, . . . , k + 1,            (B.16)
                                                                 j 1-di                                      1/2
                                                                                                            md j


                                                                                   47
by AVOA (2020, Lemma A.12(b)), we may write

                             mG                                 mG                  d +dx 1/2                       mG
     (2)               K                               K                          j 1    n                    K                          d1
                                                                                                                                         j ln(n)n
                                                                                                                                                  1/2
   Gue (   G , mG )                  Op (d1
                                         j )+                            Op                             +                    Op                 1/2
                       mG                              mG                                 j                   mG                               md j
                            j=   G                              j=   G                                              j=   G

                                               mG      dx           n1/2                          n1/2 ln(n)
                   Op (1) + Op                                 -                   + Op                                      ,                        (B.17)
                                                n           m1
                                                             G
                                                                           1+
                                                                           G                   m1 -
                                                                                                G md
                                                                                                              1/2 1+
                                                                                                                  G

for some arbitrarily small > 0, using d1  0 and Varneskov (2017, Lemma C.4). Hence, by combining
bounds, n1/2 /mG  0, Lemmas B.1(a)-(b) in the absence of cointegration in conjunction with the
                                                          (2)
continuous mapping theorem, we have Gue (                           G , mG )       Op (1) and, thus,

                                               (2,1)
                                         G (             G , mG )         Op ((m/n)dx /           1+
                                                                                                         ).                                           (B.18)

Consequently, by collecting bounds for all components in (B.12),

                            (2,1)                                                  1+
                       G (           G , mG )     Op ((mG /n)dx /                  G      ) + Op ((m/n)dx /            1+
                                                                                                                             ),                       (B.19)

thereby providing the requisite result when cointegration is absent.
   The case with cointegration. First, recall b = d1 > 0 and let us make the decomposition,

                                       (b,c)                                (b)       (1)        (2)                     (1)             (2)
                 c
                 t  (1 - L)-b t                = (1 - L)-b t - t-1 - t-1  t - t-1 - t-1 ,                                                             (B.20)

                                                                     (1)                              (b,1)
noting that t-1 = 0, for all t = 1, . . . , n in et                        and, thus, in t                    . Hence, we may decompose the
estimators with and without regressor endogeneity, similarly to (B.21),

                                                             (1,1)                            (2,2)                              (1,2)
           Gc
             (   G , mG )   - G (      G , mG )        = G  (              G , mG )   + G  (            G , mG )    + 2 G  (               G , mG )
                                                               (1)                              (2)
                                                         -   2G ( G , mG )                -   2G ( G , mG ).                                          (B.21)

                                                                                    (1)                       (2)
Next, we need the asymptotic bounds for w (j ), w (j ) and w (j ). To this end, recall the
                                                 ln(n)   ln(n)               b
bounding function g
                  ¯n (m, mb , md ) =             
                                                   mb      md  m , then, by AVOA (2020, Lemma A.9(b)), we have
                                                               m


                                                                     d-b
                                                                j            n1/2
                  w (j ) = w (j ) + Op                                                         ¯n (m, mb , md )-
                                                                                          + Op g               j
                                                                                                                 b
                                                                                                                   ,                                  (B.22)
                                                                n             j

                                                                     (2)
where w (j ) = Op (1). Moreover, by writing t-1 = B( , m) (1 - L)-b ct-1  B( , m) ct-1 as well as by
defining x = dx - b, we may use B( , m) = Op (1), uniformly by AV (2020, Theorem 1), in conjunction
                                                                                        (2)
with AVOA (2020, Lemmas A.8 and A.9(a)) to deduce w (j ) = B( , m) wc (j ) = Op (wc (j )), with

                              d -b                     d -b ln(n)                                                                d -b
           wc (j )  Op j x             + Op j x                                  + Op n-(dx -b)-1 = Op j x                                 ,          (B.23)
                                                             j 1/2


                                                                           48
                                                        (2)               d -b                           (1)
     as j   when n  .24 Hence, w (j )  Op (j x                                   ). Next, for t-1 , we may similarly write

         (1)
        t-1 = Bc ( , m) - B( , m)                 (1 - L)-b (ut-1 + ct-1 )  Bc ( , m) - B( , m)                                (ut-1 + ct-1 ) ,

     and, thus, decompose its DFT as,

                                      (1)
                                     w (j ) = Bc ( , m) - B( , m)                      (wu (j ) + wc (j )) ,                                (B.24)

                              d -b                (2)
     with wc (j )  Op (j x           ), as for t-1 . For wu (j ), let us further write,

                                                                                                               (1)       (2)
                             ut-1 = (1 - L)-b (ut-1 - ut-1 ) + (1 - L)-b ut-1  ut-1 + ut-1 ,                                                (B.25)

                                                                                 (1)                (2)                           (1)
     and, accordingly, decompose the DFT as wu (j ) = wu (j ) + wu (j ). First, for wu (j ), we use
     AVOA (Lemmas A.9(a) and A.10) with x  0 to show,

                                                                                                 dx -b
                                      (1)           b ln(n)                                  j            n1/2
                                     wu (j ) = Op -
                                                  j                         + Op                                     .                      (B.26)
                                                        mb                                   n             j

                                                                                                   (2)
     Similarly, by AVOA (Lemmas A.8 and A.9(a)), with d - b  0, wu (j ) = Op (- b
                                                                              j ) and, thus,

                                                                                           dx -b
                                                                                       j           n1/2
                                            wu (j ) = Op -
                                                         j
                                                           b
                                                             + Op                                               ,                           (B.27)
                                                                                       n            j

                                            (1)
     by Assumption B. Hence, w (j ) = Op (Bc ( , m) - B( , m)) × Op (wu (j )), where, as for the case
     without cointegration, Bc ( , m) - B( , m)  Op ((m/n)dx /                          1+   ). Hence, it suffices to establish bounds
     for the terms in (B.21) with wu (j ) and apply the bound for Bc ( , m) - B( , m).
                                                                          (2)                      d -b
        Now, for the decomposition in (B.21), we use w (j )  Op (j x                                      ) and b  d  dx to write,

                                                              mG
                             (2,2)                 K                  2(dx -b)                      mG         2(d-b)    1
                            G  (      G , mG )                     Op j                 Op                               1+                 (B.28)
                                                   mG                                                n                   G
                                                              G


     for some arbitrarily small         > 0, using mG /(mG -              G     + 1)  K and Varneskov (2017, Lemma C.4) to
     derive the final bound. Moreover, since we have the decomposition,

         (1,1)
       G  (      G , mG )   = Op      Bc ( , m) - B( , m)             Bc ( , m) - B( , m)                  × Op Guu (          G , mG )   , (B.29)




24
     Note that one term from AVOA (Lemma A.8) may be dropped since x  0.


                                                                       49
we may use the DFT bound in (B.27) to show

                                                      mG                                    mG                                 2d
                                              K                                       K                                    j        n
                     Guu (   G , mG )                           Op -
                                                                   j
                                                                     2b
                                                                        +                            Op        -
                                                                                                               j
                                                                                                                 2b
                                              mG                                      mG                                   n        j2
                                                      j=   G                               j=   G
                                                           mG                              2d
                                                  K                                   j         n1/2
                                             +                      Op     -
                                                                           j
                                                                             2b
                                                                                                                E1 + E2 + E3 ,                              (B.30)
                                                  mG                                  n          j
                                                           j=   G


using, again, mG /(mG -           G    + 1)  K . For the terms in the decomposition, we have

                         mG                            2(1-b)                                        2d
            Kn2b mG                           j                     1                      n                   mG    2(d-b)    mG
      E1                          Op                                          Op                                                 2          ,
             m2G
                 b
                         j=
                                             mG                     j2                    mG                    n                G
                              G
                2(d-b)-1 mG                                 2(d-b)
            KmG                                     j                    1                       n             mG     2(d-b)     1
      E2                     Op                                                    Op                                               2       ,
             n2(d-b)-1 j =                         mG                    j2                     mG              n                   G
                           G
                                                  mG                            1+d-2b
            Kn1/2+1          mG    1+d-2b                                j                1                      n1/2+d         mG          2(d-b)   1
      E3                                                  Op                                      Op                                                 2      ,
             mG               n                                         mG                j2                          d
                                                                                                                     mG          n                   G
                                                 j=   G


using, again, Varneskov (2017, Lemma C.4). Hence, by collecting results, we may write,

                                                                              2d                                                        d
                                  mG        2(d-b)     1                 n          mG            n             1               n           n1/2
  Guu (   G , mG )    Op                               1+                           1-    +                     1-    +                         1-          (B.31)
                                   n                   G
                                                                        mG          G
                                                                                                 mG             G
                                                                                                                               mG               G


Furthermore, since Op ((Bc ( , m) - B( , m)) (Bc ( , m) - B( , m)))  Op ((m/n)2dx /                                                         2(1+ ) ),    it suffices
to show that the second term inside the parenthesis in (B.31) multiplied by                                                ((m/n)2dx / 2(1+ ) )             is o(1)
                (1,1)                                                      1+
to establish   G  ( G , mG )        op ((mG /n)2(d-b) /                    G      ). For the first of these terms, we have

                                       2d                                                        d
                               n             mG        m        2dx       1               m                    mG
                                              1-                         2(1+ )
                                                                                                      1-       2(1+ )
                                                                                                                            0,
                              mG              G
                                                       n                                  mG          G

by Assumption T-B, as             > 0 is arbitrarily small. Similarly, for the second and third term,

                         n         1          m      2dx        1                n              1
                                   1-                       2(1+ )
                                                                                           1-    2(1+ )
                                                                                                                0,
                        mG         G
                                              n                                 mG         G
                                  d 1/2                                                   dx
                         n         n           m      2dx           1              m            m         dx        n1/2
                                       1-                       2(1+ )
                                                                                                                1-    2(1+ )
                                                                                                                                 0,
                        mG             G
                                               n                                   mG           n               G


by invoking n1/2 /mG  0 and Assumption T-B. Hence, this implies

                                             (1,1)
                                        G  (           G , mG )          op (mG /n)2(d-b) /                1+
                                                                                                           G                                                (B.32)




                                                                              50
and, by the Cauchy-Schwarz inequality,

           (1,2)                                    (1,1)               (2,2)
        |G  (         G , mG )|                 G  (           G , mG )G  ( G , mG )                  op (mG /n)2(d-b) /                         1+
                                                                                                                                                 G       .                       (B.33)

            (2)                                                               (2)                        d -b
Next, for G (         G , mG ),   we may use (B.22), w (j )  Op (j x                                            ) and b  d  dx to write

                                                               mG                                        mG                                1/2
                       (2)                       K                           d -b              K                             2(d-b) n
                      G ( G , mG )                                     Op    j x             +                  Op       j
                                                 mG                                            mG                                          j
                                                              j=   G                                  j=   G
                                                                                         mG
                                                      Kg
                                                       ¯n (m, mb , md )                                   d-2b
                                                    +                                             Op j                A1 + A2 + A3 .                                             (B.34)
                                                            mG
                                                                                         j=   G



By applying the same arguments as for E1 , E2 and E3 , we have, with g
                                                                     ¯n (·)  g
                                                                             ¯n (m, mb , md ),

                        mG    d-b           1
     A1  Op                              1+           ,
                         n               G
                                                    mG                            2(d-b)
         Kn1/2             mG      2(d-b)                                j                     1                     n1/2             mG         2(d-b)          1
     A2     -                                                 Op                                          Op            -                                                    ,
         m1
          G
                            n
                                                j=
                                                                        mG                    j 1+                   m1
                                                                                                                      G
                                                                                                                                       n                     1+
                                                                                                                                                             G
                                                          G
                                       d mG                                 2(d-b)                                                     b                 d-b
                             n                                      j                    1                                     n               mG                    1
     A3  K g
           ¯n (·)                                   Op                                              Op          ¯n (·)
                                                                                                                g                                                    1+           ,
                            mG                                     mG                j 1+                                     mG                n                    G
                                         j=     G



implying that, by Assumption T-B, A2 + A3  op (A1 ) and, thus,

                                                     (2)
                                                G  (           G , mG )       Op (mG /n)d-b /                   1+
                                                                                                                G        .                                                       (B.35)

                            (1)
For the final term, G (                G , mG ),      we have

                            (1)
                           G  (    G , mG )          = Op Bc ( , m) - B( , m) × Op Gu (                                           G , mG )       .

Hence, we may use (B.22), (B.27) and b  d  dx to write,

                                   mG                                               mG               d-b                              mG                  d-2b
                           K                                           K n1/2                        j               K n1/2                              j
   Gu (   G , mG )                          Op -
                                               j
                                                 b
                                                   +                                         Op                  +                             Op
                           mG                                           mG                            j               mG                                     j
                                  j=    G                                           j=   G                                            j=   G

            mG               2(d-b)                                    mG                                                    mG                  d-2b                    6
      Kn                    j                      ¯n (·)
                                                  Kg                                                K n1/2 g
                                                                                                           ¯n (·)                              j
    +                 Op                        +                            Op      -
                                                                                     j
                                                                                       2b
                                                                                                  +                                   Op                                         Bg .
      mG                          j2               mG                                                  mG                                            j
           j=     G                                                 j=   G                                                   j=   G                                  g =1


First, note that we readily have B2 + B6  Op (B3 ). For the remaining terms, we may apply the same




                                                                                    51
arguments used to establish bounds for {Ai , Ei }, i = 1, 2, 3, to show,

                   mG                          1-b                                             d
            K nb                      j                  1                             n               mG       d-b      mG
      B1       -
                            Op                                      Op                                                   1+       ,
            mb
             G     j=
                                     mG                 j 1+                          mG                n                G
                        G

                                      n             mG         2(d-b)     1
      B4 = Op E2  Op                                                        2     ,
                                     mG              n                      G
                                                         2d
                                                n                 mG      2(d-b)      ¯n (·)mG
                                                                                      g
      B5 = Op g
              ¯n (·)E1  Op                                                                 2              ,
                                               mG                  n                       G
                                               mG                               2(d-b)
          K n1/2+d           mG     2(d-b)                              j                  1                          n1/2+d              mG       2(d-b)        1
      B3                                                Op                                                Op                                                          ,
            mG                n                                        mG                j 1+d                         mG                  n                 1+d
                                             j=     G                                                                                                        G


which, needs to be scaled with Bc ( , m) - B( , m) = Op ((m/n)dx /                                                  1+   ) to determine the asymptotic
          (1)
order of G ( G , mG ). Specifically, we will use the trimming and bandwidth conditions in Assumption
                    (1)
T-B to show that G ( G , mG )  op ((mG /n)d / 1+    G ), similarly to the arguments used to bound the
           (1,1)
terms in G  ( G , mG ) above. First, for B1 and B4 , this follow by

                   d                                          d                                                     d-b
             n          m    dx    mG           m                 mG                      n              mG                   m    dx              1
                                   1+
                                                                  1+
                                                                          0,                                                                  1-       1+
                                                                                                                                                             0,
            mG          n                       mG                                       mG               n                   n               G

respectively, using Assumption T-B and                                 > 0 being arbitrarily small. To see this, note that the
conditions   n1/2 /m    G    0 and mG /(             G     )  0 implies (n/mG )/(                         G     )  0. Similarly, for B3 and B5 ,

                                                                                                                d                         d
             n1/2+d         mG     d-b     m    dx        1        1      mG             d-b           m            n1/2           mG
                                                         d-       1+
                                                                                                                                  d-
                                                                                                                                                        0,
              mG             n             n                               n                           mG           mG                    1+
                                                         G                                                                        G
                        2d                                                                         d                          b
                n            mG      d-b     mG          m        dx   ¯n (·)
                                                                       g               m                             n                mG
                                               1-                       1+
                                                                                                       ¯n (·)
                                                                                                       g                          1-          1+
                                                                                                                                                    0,
               mG             n                G
                                                         n                             mG                           mG            G

                                                                                                   (1)                                                      1+
respectively, using Assumption T-B and d  1. Hence, G (                                                   G , mG )          op ((mG /n)d /                  G    ) and, by
collecting results from all components of the decomposition (B.21), we have

                                 Gc
                                   (      G , mG )      - G  (         G , mG )        Op (mG /n)d-b /                     1+
                                                                                                                           G          ,                              (B.36)

thereby providing the result in the presence of cointegration, concluding the proof.

B.1     Proof of Theorem 1
First, recall that vt = (et , ut-1 ) , then, by invoking Lemmas B.1(c)-(d), we have,

                                                                                                                          1+
                                  Gc
                                   vv (   G , mG )       - Gvv (        G , mG )       Op (mG /n)dx /                     G       ,                                  (B.37)



                                                                                52
for some arbitrarily small > 0. Hence, we may continue by working with the corresponding estimate
without regressor endogeneity, vt . Next, define A(L)  D (L)D (L)-1 and at  D (L)zt such that we
have vt = A(L)at . Moreover, we may write at = µt + vt , where µt  D (L)µ1{t1} and vt = (et , ut-1 )
                         (d1 )
with et = t-1 + t                and t-1 = B ut-1 + t-1 . Finally, let us define t-1 = (t-1 , ut-1 ) and write,

  Gvv (    G , mG )    - G (1, mG ) = G (                    G , mG )    - G (1, mG ) + Gvv (                           G , mG )   - G (        G , mG )

                                                                                                                  (G)        (G)          (G)
                                                 + Gvv (         G , mG )   - Gvv (     G , mG )          U1            + U2       + U3 ,                   (B.38)

                                                                                               (G)        (G)               (G)
Then, the following lemma provides asymptotic bounds for U 1 , U 2                                                and U 3          as well as a central
limit theorem for G (1, mG ). Hence, the stated limit theory follows by applying Assumption T-G to
eliminate the sampling and trimming errors in conjunction with Slutsky's theorem.

Lemma B.2. Under the conditions of Theorem 1, the following uniform bounds hold:
          1/2    (G)                
 (a) mG U 1            = Op       G/       mG .
          1/2    (G)                                        d1                    
 (b) mG U 2             Op (       G/      mG )(   G /n)          + Op (mG /n)d1 / mG .

 (c) For some arbitrarily small                    > 0,

           1/2    (G)                   n          mG       2dx                  n1/2     mG         d   mG
       mG U 3            Op                  2                     + Op                                   1+        + Op ln(n)(mG /md )1/2 .
                                       mG    G      n                             mG       n              G


 (d) Let m1+2
          G   /n              for mG          nG and               nG , with 0 < G < G <                            1, then

                        1/2
                   mG vec G (1, mG ) - G
                                       D                                                       (1)                            (k+1)
                                    -
                                     N 0, G  G + G  G , . . . , G  G                                                                      /2 .

                                                                                                                   (G)
Proof. For (a). First, by the cancellation of terms in the summation, U 1                                                 = -G (1,              G   - 1). The
result, then, follows by Christensen & Varneskov (2017, Lemma 6).
                                                      (d )
                                                 t  (t 1 , 0k ) , such that vt - t = c
   For (b). First, define the (k + 1) × 1 vector c                                   t , and make the
decomposition,
                                       (G)
                                   U2        = Gc
                                                c (     G , mG )        (
                                                                    + G c         G , mG )   + Gc
                                                                                                 (            G , mG )                                      (B.39)
                                                                                                                                                    (d1 )
Next, as for the bounds in (B.5), we invoke AVOA (2020, Lemma A.12(b)) to show that w                                                                       (j ) =
Op (d1
    j ),   when j         n ,     > 0. Hence, uniformly,

                                                    mG                                                   mG                         2d1
                         1                                                   mG          2d1   K                              G
     Gc
      c ( G , mG ) =                                        Op    2
                                                                  j
                                                                    d1
                                                                                                                  Op                        .               (B.40)
                     mG - G + 1                                               n                mG                           mG
                                                   j=   G                                                j=   G




                                                                            53
Moreover, by applying Shimotsu & Phillips (2005, Lemma 5.4(a)), we have
                 mG                  2d1          1
           1                 G
                                           =           x2d1 dx + O m-
                                                                    G
                                                                      1
                                                                        =O (                        G /mG )
                                                                                                           1+2d1
                                                                                                                              + O m- 1
                                                                                                                                   G ,                         (B.41)
          mG               mG                      G
                 j=   G                           mG



and, as a result, Gc                               Op ((                                 2d1 )   + Op ((mG /n)2d1 /mG ). Now, by combin-
                   c (               G , mG )                    G /mG )( G /n)
ing Assumptions D1-D3, the same arguments and the Cauchy-Schwarz inequality, Gc
                                                                               (                                                                         G , mG )   
Op ((   G /mG )( G    /n)d1 )    + Op ((mG         /n)d1 /m      G ),   providing the result as                  G /n    + mG /n  0 and d1 > 0.
   For (c). First, let us further decompose the error term as,

          (G)                                                                                                                                 (G)        (G)
        U3      = Gaa (        G , mG )     - Gvv (       G , mG )        + Gvv (            G , mG )   - Gaa (    G , mG )            U 31 + U 32 .

                                                                                                                        (1)          (2)
                              t  bt + c
Moreover, write at = t + µt + c                            t )  v
                                      t and vt = A(L)(bt + c    ~t                                                             ~t . Now, as,
                                                                                                                              +v

         (G)
        U 31 = Gµµ (        G , mG )       + Gµ (         G , mG )    + Gµ (            G , mG )    + Gc
                                                                                                       µ (         G , mG )         + Gµc
                                                                                                                                        (          G , mG )    ,

we may apply the same arguments as for AVOA (2020, Lemma A.4(a)) (cf., the error term G 2 ) to
provide the following stochastic bounds,

                (G)
             U 311  Gµµ (        G , mG )       + Gµ (           G , mG )     + Gµ (          G , mG )

                                                                 n        mG           2dx                n1/2     mG          dx    mG
                                                 Op                                           + Op                                             ,               (B.42)
                                                             mG 2
                                                                G          n                              mG        n                 1+
                                                                                                                                      G

                                                                               (d1 )
for some arbitrarily small > 0. Moreover, by w                                         (j ) = Op (d 1
                                                                                                  j ) and Shimotsu (2010, Lemma B.2),

                                     mG                                                                                         mG
                            K                                                                 KmG n1/2           mG       d1
   Gµc
     (       G , mG )                       Op n1/2 j -1 × Op d
                                                              j
                                                                1
                                                                                                                                           Op (j -1- ),        (B.43)
                            mG                                                                  mG                n
                                 j=     G                                                                                      j=    G


                                                               -1                                                                   -1-
which, by Varneskov (2017, Lemma C.4), is uniformly Op (n1/2 mG   (mG /n)d1                                                         G        ), implying

                                 (G)                     n           mG       2dx                  n1/2     mG      d    mG
                              U 31  Op                                                 + Op                                           .                        (B.44)
                                                       mG 2
                                                          G           n                            mG        n            1+
                                                                                                                          G

                                            (G)
Next, for the second term U 32 , we may write,

                Gaa (     G , mG )   = Gbb (      G , mG )   + Gc
                                                                c (           G , mG )           (
                                                                                             + Gbc      G , mG )   + Gc
                                                                                                                      b (           G , mG ),
                                            (1)                         (2)                        (12)                             (21)
                Gvv (     G , mG )   = Gv
                                        ~v~(          G , mG )   + Gv
                                                                    ~v~(        G , mG )      + Gv
                                                                                                 ~v~ (     G , mG )      + Gv
                                                                                                                            ~v~ (            G , mG ).




                                                                                 54
Moreover, by applying AVOA (2020, Lemmas A.8-A.9(a), Equations (A.60) and (A.65)), we have

      (1)                                      ln(n)                   ln(n)n1/2
    wv
     ~ (j , i) = wb (j , i) + Op                   1/2
                                                             + Op                1/2
                                                                                              ,        wb (j , i)  wu (j , i) + wµ (j , i),
                                               md                           md j
          (2)                                d1
                                             j ln(n)                        d1
                                                                            j ln(n)                                    d1
      wv
       ~ (j ) = wc
                 (j ) + Op                                    + Op                                ,         wc
                                                                                                             (j ) = Op j  ,
                                               j 1/2                              1/2
                                                                                 md

                                                                                                      (1)
when j       n ,  > 0, for i = 1, . . . , k + 1. Now, the difference Gv
                                                                      ~v~(                                      G , mG )   - Gbb (      G , mG )     has already
been considered in the proof of AVOA (2020, Lemma A.4(a)) (cf. the term G 3 ). Hence, by letting,

                        ¯                           n1/2 mG    n                                        ¯
                        fG(    G , mG , n) = 1           1+  m                   2 ,    with            fG(       G , mG , n)       1,                     (B.45)
                                                    mG G      G                  G

as n   by Assumptions T and T-G (condition two), we have, by their arguments,

    (1)                                                ln(n)2 ¯                                                   ln(n)      ¯
  Gv
   ~v~(     G , mG ) - Gbb (     G , mG )  Op                fG (          G , mG , n)        + Op                           fG(       G , mG , n)    ,    (B.46)
                                                         md                                                         md

                 (1)                                                             
and, thus, Gv
            ~v~(        G , mG )   - Gbb (
                                         G , mG )  Op (ln(n)/ md ).                           Next, by applying the periodogram ap-
                                                (2)
proximation error           decomposition for wv~ (j ), we have

                                                    mG
 (2)                                  K                            2 d1
                                                                   j ln(n)
                                                                          2                                     2 d1
                                                                                                                j ln(n)
                                                                                                                       2                       2 d1
                                                                                                                                               j ln(n)
                                                                                                                                                      2
Gv
 ~v~ ( G , mG )      - Gc
                        c ( G , mG )                         Op                            + Op                            1/2
                                                                                                                                       + Op
                                      mG                                    j                                    j 1/2 md                             md
                                                   j=    G
                                                                                                                                   1+2d1
                                 mG   2d1      mG ln(n)2   mG ln(n)2   ln(n)2                                               G                   1
                      Op                                 +           +                                                                     +
                                  n             mG 1+
                                                    G
                                                           1+
                                                           G   mG md     md                                              mG                    mG

for some arbitrarily small > 0, using (B.41) and the same arguments as for (B.42) and (B.43). Hence,
                                         (2)                                                                (G)
by Assumptions T and T-G, Gv
                           ~v~(                 G , mG )      - Gc
                                                                 c (        G , mG )     op (U 311 ). Similarly,

                                                       mG
  (12)                                 K                                   ln(n)2 d
                                                                                  j
                                                                                    1

 Gv
  ~v~ ( G , mG )       - Gbc
                           ( G , mG )                             Op            1/2
                                       mG                                  md j 1/2
                                                    j=   G

            ln(n)2 d
                   j
                     1
                                      ln(n)2 d1 1/2
                                             j n                                ln(n)2 d 1 1/2
                                                                                       j n
 + Op                         + Op           1/2
                                                                  + Op
                md                       md j 3/2                                      md j
                                                                                1+d1
                mG     d1     mG ln(n)2   ln(n)2                       G                    1                     ln(n)2 n1/2              n1/2 ln(n)2
  Op                          1+        +                                              +                    +      1/2           3/2
                                                                                                                                       +   1+    1-            ,
                 n            G   mG md     md                     mG                      mG                   md mG                      G mG md
                                                                                                                                 G

                                                                                (12)                                                            (G)
which, by Assumptions T and T-G, similarly has Gv
                                                ~v~ (                                   G , mG )            - Gbc
                                                                                                                (        G , mG )       op (U 311 ), and the
                                                (21)
equivalent result for Gbc
                        (          G , mG ) - Gv ~ ( G , mG ) follows by symmetry.
                                                ~v                                                                   The final bound, thus, follows
                                   (G)        (1)
by collecting results for        U 311 and Gv  ~ ( G , mG ) - Gbb ( G , mG ).
                                              ~v
   For (d). The central limit theory follows by Nielsen & Shimotsu (2007, Lemma 5).

                                                                           55
B.2      Proof of Theorem 2
The result follows by combining Theorem 1 and Robinson & Yajima (2002, Theorem 4).

B.3      Proof of Theorem 3
First, recall that vt = (et , ut-1 ) , then, by invoking Lemmas B.1(a)-(b) and the continuous mapping
theorem,
                                                                dx -b 
                             m- b
                              m B c ( , m) - B ( , m)  Op (m/n)        m/                                 1+
                                                                                                                ,           (B.47)

for some arbitrarily small > 0. Hence, we may continue by working with the corresponding estimate
without regressor endogeneity, vt . As for the proof of Theorem 1, define A(L)  D (L)D (L)-1 and
at  D (L)zt such that vt = A(L)at , and further write at = µt + vt , where µt  D (L)µ1{t1} and,
                                                      (d1 )
again, vt = (et , ut-1 ) with et = t-1 + t                    , d1 = b, and t-1 = B ut-1 + t-1 . Finally, define
 (e)                                                          (u)
µt     as the first element of the vector µt and µt                 as the remaining k × 1 subvector. Then, as in the
corresponding proof of AV (2020, Theorem 1), we can write by addition and subtraction,

                                                                                                                     (b)
       B( , m) - B = Fuu ( , m)-1 Fu
                                   (b)
                                       (1, m) + Fuu ( , m)-1 Fu (1, m) - C 1 + C 2 + C 3 + C 4 ,                            (B.48)

                                              (b)
where the four error terms, C 1 , C 2 , C 3 , and C 4 are defined as

                                (u)                                                    (e)
          C 1  Fuu ( , m)-1 Fuµ ( , m)B,             C 2  Fuu ( , m)-1 Fuµ ( , m),
          (b)                     (b)                                  (b)                   (b)
        C 3  Fuu ( , m)-1 Fu ( , m) - Fu
                                       (b)
                                           ( , m) + D 1                      ,                 (b)
                                                                                         D 1  Fu             (b)
                                                                                                   ( , m) - Fu   (1, m),

          C 4  Fuu ( , m)-1 Fu ( , m) - Fu ( , m) + D 2 ,                          D 2  Fu ( , m) - Fu (1, m),

                                        (u)          (e)
with the superscripts indicating µt           and µt , respectively. Whereas the asymptotic properties of the
                   (b)                         (b)
terms C 1 , C 2 , C 3    and Fuu ( , m)-1 Fu (1, m) are the same irrespective of the models (ii)-(iii) and
model (iv), the properties C 4 and Fuu ( , m)-1 Fu (1, m) depend on the inference regime.
   Inference for model (iv): Since t-1 = 0, t = 1, . . . , n, we have C 4 = 0 and Fu (1, m) = 0. Next,
by applying AVOA (2020, Lemma A.2), we have,
                                                     (b)
                              m- b
                                                                             - 1                      P
                               m C 1 + C 2 + C 3 ) = op (1),                 m Fuu ( , m) -
                                                                                           Guu .                            (B.49)

                                                                                                               (b)
The result, then, follows by applying AVOA (2020, Lemma A.3) to                                    m-
                                                                                                    m
                                                                                                      1-b F
                                                                                                           u (1, m) in conjunction
with (B.49), the continuous mapping theorem and Slutsky's theorem.
                                                                                 (b)
   Inference for models (ii) and (iii): Since Fuu ( , m)-1 Fu (1, m) = Op (b
                                                                           mm
                                                                               -1/2 ), with 0 < b =
                                                              
d1  1, and we may use AVOA (2020, Lemma A.2) to show m C 4 = op (1), despite t-1 being non-
                                                                                
trivial, the central limit theory follows by applying AVOA (2020, Lemma A.3) to m-      1
                                                                                       m Fu (1, m) in
conjunction with (B.49), the continuous mapping theorem and Slutsky's theorem.
   The mutual consistency condition follows by the corresponding in AV (2020, Theorem 1) since it


                                                                56
is derived for the worst case bound d = b = 0 and, thus, applies to both inference scenarios.

B.4    Proof of Theorem 5
The result follows by combining Lemmas B.1(c) and (e) with AVOA (2020, Lemma A.4).




                                                  57
