                               NBER WORKING PAPER SERIES




                  MATCHING ON THE ESTIMATED PROPENSITY SCORE

                                          Alberto Abadie
                                         Guido W. Imbens

                                       Working Paper 15301
                               http://www.nber.org/papers/w15301


                      NATIONAL BUREAU OF ECONOMIC RESEARCH
                               1050 Massachusetts Avenue
                                 Cambridge, MA 02138
                                     August 2009




We are grateful to Ben Hansen, James Robins, Paul Rosenbaum, Donald Rubin, and participants in
seminars at the Banff Center, Brown, Georgetown, Harvard/MIT, Montreal, and UPenn for comments
and discussions. Software implementing these methods is available on our websites. The views expressed
herein are those of the author(s) and do not necessarily reflect the views of the National Bureau of
Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2009 by Alberto Abadie and Guido W. Imbens. All rights reserved. Short sections of text, not to
exceed two paragraphs, may be quoted without explicit permission provided that full credit, including
© notice, is given to the source.
Matching on the Estimated Propensity Score
Alberto Abadie and Guido W. Imbens
NBER Working Paper No. 15301
August 2009, Revised December 2009
JEL No. C13,C14

                                              ABSTRACT

Propensity score matching estimators (Rosenbaum and Rubin, 1983) are widely used in evaluation
research to estimate average treatment effects. In this article, we derive the large sample distribution
of propensity score matching estimators. Our derivations take into account that the propensity score
is itself estimated in a first step, prior to matching. We prove that first step estimation of the propensity
score affects the large sample distribution of propensity score matching estimators. Moreover, we
derive an adjustment to the large sample variance of propensity score matching estimators that corrects
for first step estimation of the propensity score. In spite of the great popularity of propensity score
matching estimators, these results were previously unavailable in the literature.


Alberto Abadie
John F. Kennedy School of Government
Harvard University
79 JFK Street
Cambridge, MA 02138
and NBER
alberto_abadie@harvard.edu

Guido W. Imbens
Department of Economics
Littauer Center
Harvard University
1805 Cambridge Street
Cambridge, MA 02138
and NBER
imbens@fas.harvard.edu
                                          I. Introduction

Propensity score matching estimators (Rosenbaum and Rubin, 1983) are widely used to
estimate treatment effects when all treatment confounders are measured. Rosenbaum and
Rubin (1983) define the propensity score as the conditional probability of assignment to
a treatment given a vector of covariates including the values of all treatment confounders.
Their key insight is that adjusting for the propensity score is enough to remove the bias
created by all treatment confounders. Relative to matching directly on the covariates,
propensity score matching has the advantage of reducing the dimensionality of matching
to a single dimension. This greatly facilitates the matching process, because units with
dissimilar covariate values may nevertheless have similar values in their propensity scores.
       Propensity score values are rarely observed in practice. Usually the propensity score
has to be estimated prior to matching. In spite of the great popularity that propensity
score matching methods have gained since they were proposed by Rosenbaum and Rubin
in 1983, their large sample distribution has not yet been derived for the case when the
propensity score is estimated in a first step. A possible reason for this void in the literature
is that matching estimators are highly non-smooth functionals of the distribution of the
matching variables, which makes it difficult to establish an asymptotic approximation to
the distribution of matching estimators when a matching variable is estimated in a first
step. This has motivated the use of bootstrap standard errors for propensity score matching
estimators. However, recently it has been shown that the bootstrap is not in general valid
for matching estimators (Abadie and Imbens, 2008).1
       In this article, we derive the large sample distribution of propensity score matching
estimators. Our derivations take into account that the propensity score is itself estimated
in a first step. We prove that first step estimation of the propensity score affects the
large sample distribution of propensity score matching estimators. Moreover, we derive
   1
    In contexts different than matching, Hirano, Imbens and Ridder (2003), Abadie (2005), Wooldridge
(2007), and Angrist and Kuersteiner (2009) derive large sample properties of statistics based on a first step
estimator of the propensity score. In all these cases, the second step statistics are smooth functionals of
the propensity scores and, therefore, standard expansions for two-step estimators apply (see, e.g., Newey
and McFadden, 1994).



                                                     1
an adjustment to the large sample variance of propensity score matching estimators that
corrects for first step estimation of the propensity score. Finally, we use a small simulation
exercise to illustrate the implications of our theoretical results.
   To preview our results, let F (x0 θ) be a parametric model for the propensity score,
with unknown parameters θ, and let θ̂N be the maximum likelihood estimator for θ. We
show that, under regularity conditions, the estimator τ̂N , for the average treatment effect
τ = E[Y (1)−Y (0)], based on matching on the estimated propensity score F (Xi0 θ̂N ), satisfies
                             √                 d
                                 N (τ̂N − τ ) → N (0, σ 2 − c0 Iθ−1 c).

In this expression, σ 2 is the variance of the matching estimator based on matching on the
true propensity score F (Xi0 θ) (which follows from results in Abadie and Imbens, 2006), Iθ
is the Fisher information matrix for the parametric model for the propensity score, and
c is a vector that depends on the covariance between the covariates and the outcome,
conditional on the propensity score and the treatment. Thus, matching on the estimated
propensity score has a smaller asymptotic variance than matching on the true propensity
score. This is in line with results in Rubin and Thomas (1992ab) who argue that, in
settings with normally distributed covariates, matching on the estimated rather than the
true propensity score improves the properties of matching estimators. Hirano, Imbens and
Ridder (2003) obtain a similar result for weighting estimators.
   The rest of the article is organized as follows. Section II provides an introduction to
propensity score matching. Section III is the main section of the article. In this section
we derive the large sample properties of an estimator that match on estimated propensity
scores. Section IV proposes an estimator for the adjusted standard errors derived in section
III. In section V we report the results of a small simulation exercise. Section VI concludes.


                II. Matching on the Estimated Propensity Score

In evaluation research the focus of the analysis is typically the effect of a binary treatment,
represented in this paper by the indicator variable W , on some outcome variable, Y . More
specifically, W = 1 indicates exposure to treatment, while W = 0 indicates lack of exposure

                                                   2
to treatment. Following Rubin (1974), we define treatment effects in terms of potential
outcomes. We define Y (1) as the potential outcome under exposure to treatment, and
Y (0) as the potential outcome under no exposure to treatment. Our goal is to estimate
the average treatment effect,
                                             h               i
                                     τ = E Y (1) − Y (0) ,

where the expectation is taken over the population of interest, based on a random sample
from this population. Estimation of treatment effects is complicated by the fact that
for each unit in the population, the observed outcome reflects only one of the potential
outcomes:                               
                                            Y (0)       if W = 0,
                                  Y =
                                            Y (1)       if W = 1.
Let X be a vector of covariates that includes treatment confounders, that is, variables that
affect the probability of treatment exposure and the potential outcomes. The propensity
score is p(X) = Pr(W = 1|X). The following assumption is often referred to as “strong
ignorability” (Rosenbaum and Rubin, 1983).

Assumption 1: (i) Y (1), Y (0) ⊥
                               ⊥ W |X almost surely; (ii) 0 < p(X) < 1 almost surely.

Assumption 1(i) will hold if all treatment confounders are included in X; so, after control-
ling for X, treatment exposure is independent of the potential outcomes. Assumption 1(ii)
states that for almost all values of X the population includes treated and untreated units.
   Let µ(w, x) = E[Y |W = w, X = x] and µ̄(w, p) = E[Y |W = w, p(X) = p] be the regres-
sion of the outcome on the treatment indicator and the covariates, and on the treatment
indicator and the propensity score respectively. Rosenbaum and Rubin (1983) prove that,
under Assumption 1,
                                     h                         i
                                τ = E µ̄(1, p(X)) − µ̄(0, p(X)) .

In other words, adjusting for the propensity score is enough to eliminate the bias created
by all treatment confounders.
   This result by Rosenbaum and Rubin (1983) motivates the use of propensity score
matching estimators. Following Rosenbaum and Rubin (1983) and the vast majority of

                                                    3
the empirical literature, consider a generalized linear specification for the propensity score
p(X) = F (x0 θ).2 In empirical research the link function F is usually specified as logit or
probit. Assume for the moment that the parameters of the propensity score, θ, are known.
For each observation, i, let JM (i, θ) be a set of M observations in the treatment group
opposite to i and with values of F (X 0 θ) similar to F (Xi0 θ). A propensity score matching
estimator can be defined as:
                                                                                       
                                     N
                                  1 X                   1                     X
                        τbN (θ) =       (2Wi − 1) Yi −                              Yj  .
                                  N i=1                 M
                                                                        j∈JM (i,θ)

In this article we will consider matching with replacement, so each unit in the sample can
be used as a match multiple times. In the absence of matching ties, the sets JM (i, θ) can
be defined as:
            (                          N
                                                                                                            !        )
                                       X
 JM (i, θ) =     j : Wj = 1 − Wi ,            1{Wk =1−Wi } 1{|F (Xi0 θ)−F (Xk0 θ)|≤|F (Xi0 θ)−F (Xj0 θ)|}       ≤M       .
                                        k=1

Let KN,i (θ) be the number of times that observation i is used as a match (when matching
on F (X 0 θ)):
                                                      N
                                                      X
                                      KN,i (θ) =            1{i∈JM (k,θ)} .
                                                      k=1

The estimator τbN (θ) can be represented as:
                                        N                        
                                     1 X                 KN,i (θ)
                           τbN (θ) =       (2Wi − 1) 1 +            Yi .
                                     N i=1                M

       In practice, propensity scores are not directly observed and estimators that match on the
true propensity score are therefore unfeasible. For some random sample {Yi , Wi , Xi }N
                                                                                      i=1 , let

θbN be an estimator of θ. A matching estimator of τ that matches on estimated propensity
scores is given by:
                                                                                         
                                       N
                                    1 X                   1                   X
                       τbN (θbN ) =       (2Wi − 1) Yi −                               Yj  .
                                    N i=1                 M
                                                                        j∈JM (i,θbN )

   2
    It is easy to extend our results to more general parametric models for the propensity score. We restrict
our attention to generalized linear specifications because in practice they are widely used to estimate
propensity scores.

                                                        4
We assume, in concordance with the literature, that θbN is the Maximum Likelihood esti-
mator of θ.3 In the next section, we derive the large sample distribution of τbN (θbN ).


                                III. Large Sample Distribution

We begin by introducing a decomposition of τbN (θ) that will be used later in this section.
Define
                                √                   
                    TN (θ) =        N τbN (θ) − τ
                                           N
                                                                                    !
                                √
                                                                    
                                        1 X                 KN,i (θ)
                            =       N         (2Wi − 1) 1 +            Yi − τ           .
                                        N i=1                M

Notice that TN (θ) = DN (θ) + RN (θ), where
                             N
                         1 X                                             
               DN (θ) = √         µ̄(1, F (Xi0 θ)) − µ̄(0, F (Xi0 θ)) − τ
                          N i=1
                             N                              
                         1 X                        KN,i (θ)                            
                      =√        (2Wi − 1) 1 +                     Yi − µ̄(Wi , F (Xi0 θ)) ,
                          N i=1                        M

and
                      N
                  1 X                                      1        X                              
        RN (θ) = √       (2Wi − 1) µ̄(1 − Wi , F (Xi0 θ)) −                   µ̄(1 − Wi , F (Xi0 θ)) .
                   N i=1                                    M
                                                                   j∈JM (i)


       Let P θ be the distribution of Z = {Y, W, X}, induced by the propensity score, F (x0 θ),
the marginal distribution of X, and the conditional distribution of Y given X and W .
To simplify the exposition, we will implicitly assume that Y and X are bounded, so all
moments exist for these two variables. Consider ZN,i = {YN,i , WN,i , XN,i } with distribution
                                                 √
given by the local “shift” P θN with θN = θ + h/ N , where h is a conformable vector of
constants.

Assumption 2: (i) For some ε > 0, all x in the support of X, and all θ ∗ ∈ Rk such that
kθ − θ∗ k ≤ ε, the distribution of F (X 0 θ∗ ) is continuous with support equal to an interval
bounded away from zero and one. (ii) For all θ∗ ∈ Rk such that kθ − θ∗ k ≤ ε, all F in the
   3
    This is done to conform with empirical practice. Our results can be readily extended to estimators of
θ other than Maximum Likelihood.

                                                        5
support of F (X 0 θ∗ ), and all w = 0, 1, the regression function E[YN,i |WN,i = w, F (XN,i
                                                                                        0
                                                                                            θ∗ ) =
F ] is Lipschitz-continuous in F .

                                                            p
Proposition 1: If Assumption 2 holds, RN (θN ) → 0 under P θN .

(All proof are provided in the appendix.)
   Proposition 1 implies that TN (θN ) = DN (θN ) + op (1) under P θN . Let
                                                N
                                                X            dP θ
                                 ΛN (θ|θN ) =         log         (ZN,i ),
                                                i=1
                                                            dP θN

and
                                 N                        0
                             1 X             WN,i − F (XN,i θ)          0
                   ∆N (θ) = √       XN,i      0              0
                                                                    f (XN,i θ).
                              N i=1      F (XN,i θ)(1 − F (XN,i θ))
In addition, let
                                            f (X 0 θ)2
                                                                
                                                               0
                             Iθ = E                          XX ,
                                    F (X 0 θ)(1 − F (X 0 θ))
be the Fisher Information Matrix for θ.

Assumption 3: Under P θN :

                                                     1
                          ΛN (θ|θN ) = −h0 ∆N (θN ) − h0 Iθ h + op (1),                       (1)
                                                     2
                                                  d
                                        ∆N (θN ) → N (0, Iθ ),                                (2)

and
                             √
                                 N (θbN − θN ) = Iθ−1 ∆N (θN ) + op (1).                      (3)

For regular parametric models, equation (1) can be established using Proposition 2.1.2 in
Bickel et al. (1998). Also for regular parametric models, equation (2) is derived in the
proof of Proposition 2.1.2 in Bickel et al. (1998). Equation (3) can be established using the
same set of results plus classical conditions for asymptotic linearity of maximum likelihood
estimators (see, e.g., van der Vaart (1998) Theorem 5.39; Lehmann and Romano (2005)
Theorem 12.4.1).
   The following assumption collects some technical regularity conditions that will be used
later in this section.

                                                  6
Assumption 4: (i) The function F has a continuous derivative. (ii) There is some ε > 0,
such that for all θ∗ such that kθ∗ − θk ≤ ε the density of F (X 0 θ∗ ) is bounded and bounded
away from zero. (iii) For all bounded functions h(Y, W, X), EθN [h(Y, W, X)|F (X 0 θN ), W ]
converges to E[h(Y, W, X)|F (X 0 θ), W ] (where EθN denotes an expectation with respect to
P θN ).

Assumption 4(i) is satisfied in the most usual binary choice models employed for the esti-
mation of the propensity score (Probit, Logit). We adopt Assumption 4(ii) for technical
reasons, because it simplifies matters considerably in the proof of our main theorem. This
assumption typically implies some trimming on the population of interest to discard low-
density values of the propensity score. (To avoid cluttering, we leave such trimming implicit
in our notation.) Primitive conditions for assumption 4(iii) can be established using the
results in Ganssler and Pfanzagl (1971).
                                                         √
    Our derivation of the limit distribution of                 τN − τ ) is based on the techniques
                                                             N (b
developed in Andreou and Werker (2005) to analyze to limit distribution of residual-
based statistics. We proceed in four steps. First, we derive the joint limit distribution
of (TN (θN ), ∆N (θN )) under P θN . The following result is useful in that respect.

Proposition 2: Suppose that Assumption 3 holds. Then, under P θN :
                                                       2 0 
                            DN (θN )       d         0      σ c
                                           →N             ,         ,
                            ∆N (θN )                 0       c Iθ
where σ 2 is the asymptotic variance of TN (θ) and
                                                                                           
                                     0          0                 W           1−W
       c = E cov(X, µ(W, X)|F (X θ), W )f (X θ)                    0
                                                                        +                         .
                                                               F (X θ)2   (1 − F (X 0 θ))2
Notice that propositions 1 and 2 imply:
                                         2 0 
                          TN (θN )   d    0   σ c
                                     →N     ,         ,                                               (4)
                          ∆N (θN )        0    c Iθ
under P θN .
   Second, we use equation (4), along with Assumption 3, to obtain the joint limit distri-
                    √
bution of (TN (θN ), N (θbN − θN ), ΛN (θ|θN )) under P θN :
                                                                    0 −1     0
                                                            2                 !
               TN (θN )                        0    !        σ    c  I    −c   h
            √                 d
                                                                      θ
         N (θbN − θN )  →      N            0      , Iθ−1 c Iθ−1       −h      .
                                             0                  0       0  0
              ΛN (θ|θN )                 −h Iθ h/2        −h c −h h Iθ h

                                                 7
Third, applying Le Cam’s third lemma, we obtain
                                                 !                                                 !!
                                             0
                                                                                      c0 Iθ−1
                                
                 √
                    TN (θN )       d      −c   h                                σ2
                                   →N             ,                                                     ,
                   N (θbN − θN )           −h                                  Iθ−1 c Iθ−1

or equivalently:
                               √                                      !                               !!
                                                                 0
                                                                                         c0 Iθ−1
                                                                                 2
                     TN (θ + h/ N )
                      √               d                        −c h              σ
                                      →N                                ,                                  ,
                        N (θN − θ)
                            b                                   0               Iθ−1 c    Iθ−1
                                                                                        √
under P θ , for any h ∈ Rk . Finally, we calculate the limit distribution of TN (θbN ) = N (b
                                                                                            τN −
                                           √                                  √         √
τ ) as the limit distribution of TN (θ + h/ N ) conditional on θbN = θ + h/ N (i.e. N (θbN −
                                               √
θ) = h), integrated over the distribution of N (θbN − θ).

Theorem 1: Under P θ
                                 √                   d
                                        τN − τ ) → N (0, σ 2 − c0 Iθ−1 c).
                                     N (b

The asymptotic variance of τbN is adjusted by −c0 Iθ−1 c to account for first-step estimation
of the propensity score. In this case, the adjustment reduces the asymptotic variance. This
need not be the case for matching estimators of other treatment parameters, such as the
average treatment effect on the treated.
   Formally, the proof of Theorem 1 requires a discretization of the first step estimator θbN
(see Andreou and Werker, 2005, for details). This discretization can be arbitrarily fine and
the result of Theorem 1 arises in the limit, as we make the discretization increasingly finer.


                      IV. Estimation of the Asymptotic Variance

Let HJ (i, θ) be the set of the J units with W = Wi and closest values of F (X 0 θ) to F (Xi0 θ),
          (J,θ)
and let Ȳi       be the average of Y for for the units in {i ∪ HJ (i, θ)}. Consider the following
             c i |F (Xi0 θ), Wi ):
estimator of var(Y

                           2          1       X                       (J,θ)              (J,θ)
                         σ
                         bN,i (θ) =                        (Yj − Ȳi          )(Yj − Ȳi         ).
                                      J
                                          j∈{i∪HJ (i,θ)}

Abadie and Imbens (2006) propose the following estimator for σ 2 (θ):
                         N
                                                           !          !2
               2      1 X                     1 X
             σ
             bN (θ) =        (2Wi − 1) Yi −              Yj − τbN (θ)
                      N i=1                   M
                                                                      j∈JM (i,θ)


                                                           8
                              N
                                                    !2                                   !!
                           1 X          KN,i (θ)           2M − 1 KN,i (θ)                     2
                         =                               +                                   σ
                                                                                             bN,i (θ).
                           N i=1         M                   M     M
       (J,θ)
Let X̄i        be the averages of X for for the units in {i ∪ HJ (i, θ)}. Notice that Y =
µ(W, X) + ε, where E[ε | X, W ] = 0. As a result:

                       cov(X, µ(W, X)|F (X 0 θ), W ) = cov(X, Y |F (X 0 θ), W ).

Consider the following estimator of cov(X, Y |F (X 0 θ), W ):
                                             1         X                      (J,θbN )            (J,θbN )
               c i , Yi |F (Xi0 θ), Wi ) =
               cov(X                                                 (Xj − X̄i           )(Yj − Ȳi          ).
                                             J
                                                 j∈{i∪HJ (i,θbN )}

Our estimator of c is:
                N
                                                                                                                  !
             1 X                                                        Wi             (1 − Wi )
          c=
          b        c i , Yi |F (Xi0 θ), Wi )f (Xi0 θbN )
                   cov(X                                                          +                                   .
             N i=1                                                   F (Xi0 θbN )2 (1 − F (Xi0 θbN ))2
Finally, let
                                      N
                                   1 X            f (Xi0 θbN )2
                            Iθ,N =
                            b                                         Xi Xi0 .
                                   N i=1 F (Xi θbN )(1 − F (Xi θbN ))
                                             0                  0


Because Iθ is non-singular, the inverse of Ibθ,N exists with probability approaching one. Our
estimator of the large sample variance of the propensity score matching estimator, adjusted
for first step estimation of the propensity score, is:

                                                                   −1
                                   σ 2
                                   badj,N (θbN ) = σ
                                                   bN2 b
                                                              c0 Ibθ,N
                                                      (θN ) − b        c.
                                                                       b

Consistency of this estimator can be shown using the results in Abadie and Imbens (2006)
and the contiguity arguments employed in section III.


                              V. A Small Simulation Exercise

In this section, we run a small Monte Carlo exercise to investigate the sampling distribution
of propensity score matching estimators and of the approximation to that distribution that
we propose in the article.
   We use a simple Monte Carlo design. The outcome variable is generated by Y =
5W + 4(X1 + X2 ) + U , where X1 and X2 are independent and uniform on [0, 1] and U

                                                         9
is a standard Normal variable independent of (W, X1 , X2 ). The treatment variable, W , is
related to (X1 , X2 ) through the propensity score, which is logistic

                                                       exp(1 + x1 − x2 )
                   Pr(W = 1|X1 = x1 , X2 = x2 ) =                          .
                                                     1 + exp(1 + x1 − x2 )

Table I reports the results of our Monte Carlo simulation for M = 1 and N = 5000. As
in our theoretical results, the variance of τbN (θ), the estimator that matches on the true
propensity score, is larger than the variance of τbN (θbN ), the estimator that matches on the
estimated propensity score. The estimator of the variance of τbN (θ) proposed in Abadie
                   b2 (θ), is centered at the variance of τbN (θ). σ
and Imbens (2006), σ                                               bN2 b
                                                                      (θN ) is the estimator of
the variance that treats the first step estimate of the propensity score θbN as if it was the
                             2
true propensity score, and σ
                           badj,N (θbN ) is the adjusted estimator of the variance that takes
into account that the propensity score is itself estimated in a first step. Finally, the table
reports also confidence interval constructed with adjusted and unadjusted standard errors.
                                                                         2 b
In concordance with out theoretical results, the simulation shows that σ
                                                                       bN (θN ) is biased and
                                                                           2 b
too large on average. In addition, confidence intervals constructed with σ
                                                                         bN (θN ) have larger
                                            2
than nominal coverage rates. In contrast, σ
                                          badj,N (θbN ) is unbiased and produce confidence
intervals that have coverage rates close to nominal rates.


                                    VI. Conclusions

In this article, we propose a method to correct to the asymptotic variance of propensity
score matching estimators when the propensity scores are estimated in a first step. Our
results allow valid large sample inference for propensity score matching estimators.




                                              10
                                               Appendix
For the proof of Proposition 1 we will need some preliminary lemmas.
Lemma A.1: Consider two independent samples of sizes n0 and n1 from continuous distributions
F0 and F1 with common support: V0,1 , . . . , V0,n0 ∼ i.i.d. F0 and V1,1 , . . . , V1,n1 ∼ i.i.d. F1 . Let
N = n0 + n1 . Assume that the support of F0 and F1 is an interval inside [0, 1]. Let f0 and f1
be the densities of F0 and F1 , respectively. Suppose that for any v in the supports of F0 and F1 ,
f1 (v)/f0 (v) ≤ r̄. For 1 ≤ i ≤ n1 and 1 ≤ m ≤ M ≤ n0 , let |Un0 ,n1 ,i |(m) be the m-th order statistic
of {|V1,i − V0,1 |, . . . , |V1,i − V0,n0 |}. Then, for n0 ≥ 3:
                       n1         M
             "                                       #
                1 X           1 X                               n1         n1 M −1/4       1/4
          E √                        |Un0 ,n1 ,i |(m) ≤ r̄         3/4
                                                                       + M 1/2 n0    exp(−n0 ).
                 N i=1 M m=1                              0N 1/2 bn c     N


Proof: Consider N balls assigned at random among n bins of equal probability. It is known
that the mean of the number of bins with exactly m balls is equal to

                                                     1 N −m
                                        m          
                                    N      1
                               n                  1−
                                    m      n         n
(see Johnson and Kotz, 1977, p. 114). Because f1 (v)/f0 (v) ≤ r̄, for any measurable set A:
                             Z              Z         
                                                f1 (v)
              Pr(V1,i ∈ A) =    f1 (v) dv =              f0 (v) dv ≤ r̄ Pr(V0,i ∈ A).
                              A              A f0 (v)
                                         3/4                                  3/4
Divide the support of F0 and F1 in bn0 c cells of equal probability 1/bn0 c under F0 . Let ZM,n0
be the number of such cells are not occupied by at least M observations from the second sample:
                                                                                                  3/4
V0,1 , . . . , V0,n0 . For i = 1, . . . , N . Let µM,n0 = E[ZM,n0 ]. Notice that n0 ≥ 3 implies bn0 c ≥ 2.
Then,
                                        M −1                       !m             !n0 −m
                                        X       3/4    n0        1             1
                        µM,n0 =              bn0 c                       1 − 3/4
                                                       m         3/4
                                                               bn0 c         bn0 c
                                        m=0
                                        M −1
                                                                 ! m           !n0 −m
                                                     m
                                                3/4 n0       1             1
                                        X
                                ≤            bn0 c                    1 − 3/4
                                                    m! bn3/4 c           bn0 c
                                        m=0                  0
                                                               !n0
                                             M −1/4        1
                                ≤ M n0               1 − 3/4       .
                                                          n0
Using Markov’s inequality,
                                                                                          !n0
                                                                 M −1/4             1
               Pr(ZM,n0 > 0) = Pr(ZM,n0 ≥ 1) ≤ µM,n0 ≤        M n0          1−      3/4
                                                                                                .
                                                                                 n0
Notice that for any positive a, we have that a − 1 ≥ log(a). Therefore, for any b < N , we have
that log(1 − b/N ) ≤ −b/N and (1 − b/N )N ≤ exp(−b). As a result, we obtain:
                                                 1/4 n0
                                   !n0              !
                                 1              n0                1/4
                           1 − 3/4      = 1−            ≤ exp(−n0 ).
                               n                 n0
                                   0

                                                   11
Putting together the last two displayed equations, we obtain the following exponential bound for
Pr(ZM,n0 > 0):
                                                  M −1/4       1/4
                            Pr(ZM,n0 > 0) ≤ M n0         exp(−n0 ).
                                                               3/4                                3/4
Notice that |Un0 ,n1 ,i |(m) ≤ 1. For 0 ≤ n ≤ bn0 c, let cn0 ,n = F −1 (n/bn0 c), then
                                                     3/4
                                                   bn0
        M                                           Xc
    "                                  #
        X                                                                                                              
E             |Un0 ,n1 ,i |(m) ZM,n0 = 0       ≤               M cn0 ,n − cn0 ,n−1 Pr cn0 ,n−1 ≤ V1,i ≤ cn0 ,n ZM,n0 = 0
        m=1                                        n=1
                                                                    3/4
                                                                bn0
                                                    M r̄            Xc                        
                                               =     3/4
                                                                          cn0 ,n − cn0 ,n−1
                                                   bn0 c n=1
                                                    M r̄
                                               ≤         3/4
                                                                .
                                                   bn0 c
Now,
       n1    M                                                 n1     M
 "                                         #         "                                               #
   1 X    1 X                                              1 X     1 X
E √             |Un0 ,n1 ,i |(m)               =   E √                   |Un0 ,n1 ,i |(m) ZM,n0 = 0 Pr(ZM,n0 = 0)
    N i=1 M m=1                                            N i=1 M m=1
                                                               n1     M
                                                       "                                             #
                                                           1 X     1 X
                                               +   E √                   |Un0 ,n1 ,i |(m) ZM,n0 > 0 Pr(ZM,n0 > 0)
                                                           N i=1 M m=1
                                                               n1     M
                                                       "                                             #
                                                           1 X     1 X
                                               ≤   E √                   |Un0 ,n1 ,i |(m) ZM,n0 = 0
                                                           N i=1 M m=1
                                                      n1
                                               +           Pr(ZM,n0 > 0)
                                                   N 1/2
                                                           n1         M
                                                                 "                                   #
                                                      1 X          1 X
                                               =   √           E         |Un0 ,n1 ,i |(m) ZM,n0 = 0
                                                      N i=1        M
                                                                     m=1
                                                      n1
                                               +           Pr(ZM,n0 > 0)
                                                   N 1/2
                                                            n1           n1 M −1/4               1/4
                                               ≤   r̄          3/4
                                                                    + M 1/2 n0            exp(−n0 ).
                                                         1/2
                                                      N bn0 c           N
                                                                                                                   

Lemma A.2: (Inverse Moments of the Doubly Truncated Binomial Distribution) Let N0 be a
Binomial variable with parameters (N, (1 − p)) that is left-truncated for values smaller than M
and right-truncated for values greater than N − M , where M < N/2. Then, for any r > 0, there
exist a constant Cr , such that            r 
                                             N
                                        E           ≤ Cr ,
                                             N0
for all N > 2M .

Proof: Let N1 = N − N0 . For all q̄ > 0,
             r            r                     r          
               N                   N       N               N     N
          E            = E               1    > q̄    +E      1     ≤ q̄
               N0                 N0       N0              N0    N0

                                                                    12
                                 N r
                                              
                                          N
                              ≤      Pr      > q̄ + q̄ r
                                 M        N0
                                 r                  
                                 N                  1
                              =      Pr N1 > 1 −         N + q̄ r
                                 M                  q̄

Notice that:
                                                                 x≤N −M                 
                                                                   X                N
                                                                                             px (1 − p)N −x
                                                                                x
                                         1                    x>(1−1/q̄)N,x≥M
                Pr N1 >           1−              N       =      x≤N −M
                                         q̄                        X      
                                                                              N
                                                                                    
                                                                                        px (1 − p)N −x
                                                                              x
                                                                   x≥M

For N > 2M the denominator can be bounded away from zero. Therefore, for some positive
constant C, and q̄ > 1/(1 − p),
                                                               x≤N −M        
                                    1                                X        N
               Pr N1 >           1−           N           ≤ C                       px (1 − p)N −x
                                    q̄                                        x
                                                            x>(1−1/q̄)N,x≥M
                                                                             
                                                                X          N
                                                          ≤ C                   px (1 − p)N −x
                                                                            x
                                                             x>(1−1/q̄)N

                                                          ≤ C exp −2(1 − 1/q̄ − p)2 N ,
                                                                  


by Hoeffding’s Inequality (e.g. van der Vaart and Wellner (1996), p. 459). Therefore E[(N/N0 )r ]
is uniformly bounded for N > 2M .                                                             

Lemma A.3: Suppose that the propensity score, Pr(W = 1|X), is continuously distributed and
that there exist cL > 0 and cU < 1 such that cL ≤ Pr(W = 1|X = x) ≤ cU for all x ∈ X . Let f1
be the distribution of the propensity score conditional on W = 1, and let f0 be the distribution of
the propensity score conditional on W = 0. Then, the ratio f1 (p)/f0 (p) is bounded and bounded
away from zero.

Proof: Use Bayes’ Theorem to show that f1 (p)/f0 (p) = (p/(1 − p))(Pr(W = 1)/ Pr(W = 0)). 

Proof of Proposition 1: Let f1θN be the distribution of the propensity score conditional on
W = 1, and let f0θN be the distribution of the propensity score conditional on W = 0. By lemma
A.3 the ratio f1θN (p)/f0θN (p) is uniformly bounded by some constant r̄. Consider N0 and N1 as
in Lemma A.2. Let
                       (1)                        N1                   N1    M −1/4       1/4
                      ψM,N0 ,N1 = r̄                            +M         N        exp(−N0 ).
                                                 3/4
                                         N 1/2 bN0 c                  N 1/2 0

        (1)       p
Then, ψM,N0 ,N1 → 0. Rearrange the observations in the sample so that the                                     first N1 obser-
vations have W = 1 and the remaining N0 = N − N1 observations have W                                          = 0. For 1 ≤
i ≤ N1 and 1 ≤ m ≤ M , let |UN0 ,N1 ,i |(m) be the m-th order statistic of                                           0 θ ) −
                                                                                                              {|F (XN,i N
     0                         0 θ ) − F (X 0
            θ )|, . . . , |F (XN,i
F (XN,N 1 +1 N                     N       N,N θN )|}. For N1 + 1 ≤ i ≤ N and                                 1 ≤ m ≤ M,

                                                                 13
                                                          0 θ ) − F (X 0 θ )|, . . . , |F (X 0 θ ) −
let |UN0 ,N1 ,i |(m) be the m-th order statistic of {|F (XN,i N       N,1 N                 N,i N
     0
F (XN,N1 θN )|}. Lemma A.1 implies that for large enough N :

                             N1     M
                          "                            #
                          1 X    1 X                       h
                                                             (1)
                                                                     i
                       E √             |UN0 ,N1 ,i |(m) ≤ E ψM,N0 ,N1 .                        (A.1)
                           N i=1 M m=1

Therefore, to prove that the left-hand-side of equation (A.1) converges to zero, it is enough to
            (1)
show that ψM,N0 ,N1 is asymptotically uniformly integrable:
                                       h                          i
                                         (1)         (1)
                          lim lim sup E ψM,N0 ,N1 1{ψM,N0 ,N1 > k} = 0
                         k→∞ N →∞


(see, e.g., van der Vaart (1998), p. 17). Notice that the ratio N 3/4 /bN 3/4 c is bounded. This, in
combination with Lemma A.2, implies that for all k > 0 and some positive constant C,
            h                         i        h           i
              (1)        (1)                      (1)
         E ψM,N0 ,N1 1{ψM,N0 ,N1 > k} ≤ E ψM,N0 ,N1
                                               "                                           #
                                                    N 1/2        N 1/2 M +1/2          1/4
                                         ≤ E r̄       3/4
                                                            + M 3/4 N0         exp(−N0 )
                                                   bN c          N0
                                                 " 0 #
                                                    N 1/2
                                         ≤ CE         3/4
                                                    N0
                                                      "       #
                                              C         N 3/4
                                         =         E            → 0.
                                            N 1/4       N
                                                          3/4
                                                             0

Similarly,                                                           
                                        N          M
                                  1     X        1 X                    p
                              E √                   |UN0 ,N1 ,i |(m)  → 0.
                                   N             M
                                       i=N1 +1      m=1

Using Markov’s inequality, we obtain that for any ε > 0:
                                                           N     M
                                                    "                                #
                                                       1 X 1 X
               N      M
                                              !   E √               |UN0 ,N1 ,i |(m)
          1 X 1 X                                       N i=1 M m=1
     Pr √                 |UN0 ,N1 ,i |(m) > ε  ≤                                      −→ 0.
           N      M                                             ε
               i=1     m=1

The result now follows from Lipschitz-continuity of the regression functions, E[YN,i |WN,i =
       0 θ ∗ ) = F ] for some ε > 0 and kθ ∗ − θk ≤ ε.
w, F (XN,i                                                                                 

Proof of Proposition 2 (Sketch): In this proof, we first establish a extend the martingale
representation of matching estimators (Abadie and Imbens, 2009) to the propensity score matching
estimator studied in this article. Consider the linear combination CN = z1 DN (θN ) + z20 ∆N (θN ).
                        N
                    1 X             0                     0
                                                                         
         CN   = z1 √       µ̄(1, F (XN,i θN )) − µ̄(0, F (XN,i θN )) − τ
                     N i=1


                                                     14
                           N                           
                       1 X                    KN,i (θN )                       0
                                                                                         
                 + z1 √       (2WN,i − 1) 1 +              YN,i − µ̄(WN,i , F (XN,i θN ))
                        N i=1                   M
                              N                           0 θ )
                          1   X                WN,i − F (XN,i N
                 +   z20 √          XN,i       0 θ )(1 − F (X 0 θ ))
                                                                             0
                                                                         f (XN,i θN ).
                          N                F (XN,i N         N,i N
                              i=1

CN can be analyzed using martingale methods. Notice that:
                                                         3N
                                                         X
                                                  CN =         ξN,k ,
                                                         k=1

where
                      1            0                      0
                                                                        
           ξN,k = z1 √    µ̄(1, F (XN,k θN )) − µ̄(0, F (XN,k θN )) − τ
                       N
                                                                      0 θ )
                                                         WN,k − F (XN,k
                   0 1                    0                               N        0
                + z2 √   EθN [XN,k | F (XN,k θN )]       0 θ )(1 − F (X 0 θ )) f (XN,k θN ),
                       N                           F  (X N,k N            N,k N

for 1 ≤ k ≤ N ,
                                                                   0
                                                    (WN,k−N − F (XN,k−N           0
                                                                         θN ))f (XN,k−N  θN )
            1                          0
ξN,k = z20 √ (XN,k−N −EθN [XN,k−N |F (XN,k−N θN )])       0                   0
            N                                         F (XN,k−N θN )(1 − F (XN,k−N θN ))
                                            
            1                   KN,k−N (θN )                                           0
                                                                                                   
     + z1 √ (2WN,k−N − 1) 1 +                   µ(WN,k−N , XN,k−N ) − µ̄(WN,k−N , F (XN,k−N   θN )) .
            N                        M

for N + 1 ≤ k ≤ 2N ,
                                                   
                 1                     KN,k−2N (θN )                                 
    ξN,k   = z1 √   (2WN,k−2N − 1) 1 +                 YN,k−2N − µ(WN,k−2N , XN,k−2N ) ,
                  N                        M

for 2N + 1 ≤ k ≤ 3N . Consider the σ-fields FN,k = σ{WN,1 , . . . , WN,k , XN,1             0 θ , . . . , X0 θ }
                                                                                                N          N,k N
                                                 0 θ , . . . , X0
for 1 ≤ k ≤ N , FN,k = σ{WN,1 , . . . , WN,N , XN,1     N         N,N θN , XN,1 , . . . , XN,k−N } for N + 1 ≤
k ≤ 2N , and FN,k = σ{WN,1 , . . . , WN,N , XN,1 , . . . , XN,N , YN,1 , . . . , YN,k−N } for 2N +1 ≤ k ≤ 3N .
Then,                                                                
                                     X i                             
                                          ξN,j , FN,i , 1 ≤ i ≤ 3N
                                                                     
                                            j=1

is a martingale for each N ≥ 1. Therefore, the limiting distribution of CN can be studied using
Martingale Central Limit Theorem (e.g., Theorem 35.12 in Billingsley (1995), p. 476; importantly,
notice that this theorem allows that the probability space varies with N ). Because YN,i , XN,i , and
WN,i are bounded random variables (uniformly in N ), and because KN,i has uniformly bounded
moments (see Abadie and Imbens, 2009), it follows that:
                                     3N
                                     X
                                              2+δ
                                           E[ξN,k ]→0          for some δ > 0.
                                     k=1




                                                         15
Lindeberg’s condition in Billingsley’s theorem follows easily from last equation (Lyapounov’s
condition). As a result, we obtain that under P θN
                                            d
                                       CN → N (0, σ12 + σ22 + σ32 ),

where
                                                 N
                                                 X
                                 σ12   = plim                2
                                                       EθN [ξN,k | FN,k−1 ],
                                                 k=1
                                                 2N
                                                 X
                                σ22 = plim                     2
                                                         EθN [ξN,k | FN,k−1 ],
                                                k=N +1

and
                                                 3N
                                                 X
                               σ32 = plim                      2
                                                         EθN [ξN,k | FN,k−1 ].
                                             k=2N +1

After some algebra, we obtain:
                          h                                       2 i
              σ12 = z12 E µ̄(1, F (X 0 θ)) − µ̄(0, F (X 0 θ)) − τ
                                    f 2 (X 0 θ)
                                                                                
                       0                                        0      0     0
                  + z2 E                             E[X | F (X θ)]E[X | F (X θ)] z2 .
                            F (X 0 θ)(1 − F (X 0 θ))

Following the calculations in Abadie and Imbens (2006, additional proofs) for the expectation of
(1 + KN,i /M )2 :

                                    f 2 (X 0 θ)
                                                                       
              2       0                                            0
            σ 2 = z2 E                                var(X | F (X θ)) z2
                            F (X 0 θ)(1 − F (X 0 θ))
                            var(µ(1, X)|F (X 0 θ)) var(µ(0, X)|F (X 0 θ))
                                                                                
                      2
                  + z1 E                              +
                                    F (X 0 θ)                1 − F (X 0 θ)
                                                                                
                      2 1               1             0                        0
                  + z1      E                  − F (X θ) var(µ(1, X)|F (X θ))
                        2M         F (X 0 θ)
                                                                                            
                      2 1                 1                    0                          0
                  + z1      E                      − (1 − F (X θ)) var(µ(0, X)|F (X θ))
                        2M         1 − F (X 0 θ)
                                                                       f (X 0 θ)
                                                                                       
                        0                               0
                  + 2 z2 E cov(X, µ(W, X)|F (X θ), W )                                    z1 .
                                                               F (X 0 θ)(1 − F (X 0 θ))

Here we use the fact that, conditional on the propensity score, X is independent of W . To derive
the constant vector of the cross-product notice that:
                                      (W − F (X 0 θ))(2W − 1)
                                                                                    
                               0                                       0       KN (θ)
      E cov X, µ(X, W ) F (X θ), W                                f (X θ) 1 +
                                        F (X 0 θ)(1 − F (X 0 θ))                M
                                                       0
                                                                              
                                             0
                                                  f (X θ)        KN (θ)
                  = E cov X, µ(X, 1) F (X θ)                   1+           W =1 p
                                                   F (X 0 θ)         M
                                                 f (X 0 θ)
                                                                                   
                                            0                         KN (θ)
                  + E cov X, µ(X, 0) F (X θ)                      1+           W = 0 (1 − p)
                                                  1 − F (X 0 θ)          M


                                                       16
                                                     f (X 0 θ)
                                                                       
                                                0
                      → E cov X, µ(X, 1) F (X θ)                  W =1 p
                                                      F (X 0 θ)2
                                                         f (X 0 θ)
                                                                              
                      + E cov X, µ(X, 0) F (X 0 θ)
                                                   
                                                                       W  =   0  (1 − p)
                                                     (1 − F (X 0 θ))2
                                                                                               
                                                 0              0       W              1−W
                      = E cov(X, µ(W, X)|F (X θ), W )f (X θ)                    +                   .
                                                                     F (X 0 θ)2 (1 − F (X 0 θ))2

Finally,
                                                                        
                               var(Y |X, W = 1) var(Y |X, W = 0)
                σ32   =   z12 E                   +
                                    F (X 0 θ)            1 − F (X 0 θ)
                                                                          
                             1          1            0
                      + z12    E              − F (X   θ)  var(Y  |X,  W = 1)
                            2M      F (X 0 θ)
                                                                               
                          2 1              1                    0
                      + z1     E                  − (1 − F (X θ)) var(Y |X, W = 0) .
                            2M      1 − F (X 0 θ)

Notice that for any integrable function g(F (X 0 θ)):
   h                                                     i
  E g(F (X 0 θ)) var(µ(w, X)|F (X 0 θ)) + var(Y |X, W = w)
                 h                                     h                            ii
            = E g(F (X 0 θ)) var(µ(w, X)|F (X 0 θ)) + E var(Y |X, W = w) F (X 0 θ)
       h                                               h                                    ii
   = E g(F (X 0 θ)) var(µ(w, X)|F (X 0 θ), W = w) + E var(Y |X, W = w) F (X 0 θ), W = w
                                                           h                                    i
                                                       = E g(F (X 0 θ)) var(Y |F (X 0 θ), W = w) .

As a result, under P θN :
                                                   d
                                            CN → N (0, z 0 V z),
where z = (z1 , z20 )0 , and
                                                       σ 2 c0
                                                               
                                             V =                    ,
                                                        c Iθ
where                                                                                       
                                      0           0                W           1−W
             c = E cov(X, µ(W, X)|F (X θ), W )f (X θ)               0
                                                                         +                         ,
                                                                F (X θ)2   (1 − F (X 0 θ))2
and σ 2 is the asymptotic variance calculated in Abadie and Imbens for the case of a known
propensity score. Applying the Cramer-Wold device, under P θN :
                                             
                                     DN (θN )    d
                                                → N (0, V ).
                                     ∆N (θN )

                                                                                    
Proof of Theorem 1: Given our preliminary results, Theorem 1 follows from Andreou and
Werker (2005).                                                                      




                                                       17
                                      References

Abadie, A. (2005) ”Semiparametric Difference-in-Differences Estimators,” Review of Economic
   Studies, vol. 72, no. 1, 1-19.

Abadie, A. and Imbens, G.W. (2006) “Large Sample Properties of Matching Estimators for
   Average Treatment Effects,” Econometrica , vol. 74, no. 1, 235-267.

Abadie, A. and Imbens, G.W. (2008) “On the Failure of the Bootstrap for Matching Estima-
   tors,” Econometrica , vol. 76, no. 6, 1537-1558.

Abadie, A. and Imbens, G.W. (2009) “A Martingale Representation for Matching Estimators,”
   NBER working paper, no. 14756.

Andreou, E. and Werker, B.J.M. (2005) “An Alternative Asymptotic Analysis of Residual-
   Based Statistics,” mimeo.

Angrist, J.D. and Kuersteiner, G.M. (2005) “Causal Effects of Monetary Shocks: Semi-
   parametric Conditional Independence Tests with a Multinomial Propensity Score,” mimeo.

Bickel, P.J., Klaassen, C.A., Ritov, Y. and Wellner, J.A. (1998) Efficient and Adaptive
    Estimation for Semiparametric Models, Springer, New York.

Billingsley, P. (1995), Probability and Measure, third edition. Wiley, New York.

Ganssler, P. and Pfanzagl, J. (1971) “Convergence of Conditional Expectations,” The An-
   nals of Mathematical Statistics, vol. 42, no. 1, 315-324.

Hirano, K., G. Imbens, and G. Ridder (2003) “Efficient Estimation of Average Treatment
    Effects Using the Estimated Propensity Score,” Econometrica, vol. 71, no. 4, 1161-1189.

Johnson, N. and Kotz, S. (1977) Urn Models and Their Applications, John Wiley & Sons,
    New York.

Newey, W.K. and McFadden, D. (1994) ”Large sample estimation and hypothesis testing.”
   In: Engle, R.F., McFadden, D. (Eds.), Handbook of Econometrics, vol. 4. Elsevier Science,
   Amsterdam.

Lehmann, E.L. and Romano, J.P. (2005) Testing Statistical Hypothesis. Springer, New York.

Rosenbaum, P. and Rubin, D.B. (1983) “The Central Role of the Propensity Score in Obser-
    vational Studies for Causal Effects,” Biometrika, vol. 70, 4155.

Rubin, D.B. (1974) “Estimating Causal Effects of Treatments in Randomized and Non-randomized
    Studies,” Journal of Educational Psychology, vol. 66, 688-701.

Rubin, D., and N. Thomas (1992a) “Characterizing the effect of matching using linear propen-
    sity score methods with normal distributions,” Biometrika, vol. 79, 797-809.

Rubin, D., and N. Thomas (1992b) “Affinely Invariant Matching Methods with Ellipsoidal
    Distributions,” Annals of Statistics, vol. 20, no. 2, 1079-1093.

                                            18
van der Vaart, A. (1998), Asymptotic Statistics, Cambridge University Press, New York.

van der Vaart, A.W. and Wellner, J.A. (1996), Weak Convergence and Empirical Pro-
    cesses, Springer-Verlag, New York.

Wooldridge, J.M. (2007) “Inverse Probability Weighted Estimation for General Missing Data
   Problems,” Journal of Econometrics, vol. 141, 1281-1301.




                                          19
                Table I – Simulation Results
        (N = 5000, Number of simulations = 10000)
Variances over simulations          Coverage of 95% C.I.
                                   (asymp. s.e. = 0.0022)
τbN (θ)         0.0053         (b
                                τN (θ), σ
                                        bN2 (θ))            0.9532
τbN (θbN )      0.0027         (b
                                τN (θbN ), σ
                                           bN2 (θ
                                                bN ))       0.9947
                               (b
                                τN (θbN ), σ 2
                                           badj,N (θbN ))   0.9488
Averages over simulations
σ
bN2 (θ)         0.0054
σ
bN2 (θ
     bN )       0.0053
σ 2
badj,N (θbN )   0.0027




                            20
