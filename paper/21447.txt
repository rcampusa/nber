                                 NBER WORKING PAPER SERIES




                     MEASURING THE MEASUREMENT ERROR:
               A METHOD TO QUALITATIVELY VALIDATE SURVEY DATA

                                         Christopher Blattman
                                           Julian C. Jamison
                                        Tricia Koroknay-Palicz
                                         Katherine Rodrigues
                                          Margaret Sheridan

                                         Working Paper 21447
                                 http://www.nber.org/papers/w21447


                       NATIONAL BUREAU OF ECONOMIC RESEARCH
                                1050 Massachusetts Avenue
                                  Cambridge, MA 02138
                                      August 2015


For comments we thank Neal Beck, Alex Coppock, Dan Corstange, Macartan Humphreys, Don Green,
Cyrus Samii, Chris Udry, several anonymous referees, and participants at the NYU 2014 CESS conference.
This study was funded by the National Science Foundation (SES-1317506), the World Bank's Learning
on Gender and Conflict in Africa (LOGiCA) trust fund, the World Bank's Italian Children and Youth
(CHYAO) trust fund, the UK Department for International Development (DFID) via the Institute for
the Study of Labor (IZA), a Vanguard Charitable Trust, the American People through the United States
Agency for International Development’s (USAID) DCHA/CMM office, and the Robert Wood Johnson
Health and Society Scholars Program at Harvard University (Cohort 5). The contents of this study
are the sole responsibility of authors and do not necessarily reflect the views of their employers or
any of these funding agencies or governments. Finally, for research assistance we thank Foday Bayoh
Jr., Natalie Carlson, Camelia Dureng, Mathilde Emeriau, Yuequan Guo, Rufus Kapwolo, James Kollie,
Rebecca Littman, Richard Peck, Patryk Perkowski, Colombine Peze-Heidsieck, Joe St. Clair, Joseph
Sango Jr., Helen Smith, Abel Welwean, Prince Williams, and John Zayzay through Innovations for
Poverty Action (IPA). The views expressed herein are those of the authors and do not necessarily reflect
the views of the National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2015 by Christopher Blattman, Julian C. Jamison, Tricia Koroknay-Palicz, Katherine Rodrigues,
and Margaret Sheridan. All rights reserved. Short sections of text, not to exceed two paragraphs, may
be quoted without explicit permission provided that full credit, including © notice, is given to the source.
Measuring the Measurement Error: A Method to Qualitatively Validate Survey Data
Christopher Blattman, Julian C. Jamison, Tricia Koroknay-Palicz, Katherine Rodrigues, and
Margaret Sheridan
NBER Working Paper No. 21447
August 2015
JEL No. C81,C93,I32,K4,O1

                                             ABSTRACT

Field experiments rely heavily on self-reported data, but subjects may misreport behaviors, especially
sensitive ones such as crime. If treatment influences survey responses, it biases experimental estimates.
We develop a validation technique that uses intensive qualitative work to assess survey measurement
error. Subjects were assigned to receive cash, therapy, both, or neither. According to survey responses,
receiving both treatments dramatically reduced crime and other sensitive behaviors. Local researchers
spent several days with a random subsample of subjects following their endline surveys, building trust
and seeking verbal confirmation of six behaviors: theft, drug use, homelessness, gambling, and two
expenditures. This validation suggests that subjects in the control and cash only groups underreported
sensitive behaviors and expenditures in the survey relative to the other treatment arms. We bound
survey-based treatment effects estimates, and find the impacts of cash and therapy on crime may be
larger than suggested by surveys alone.


Christopher Blattman                                Katherine Rodrigues
School of International and Public Affairs          Research Department
Columbia University                                 International Rescue Committee
420 West 118th Street                               122 East 42nd St.
New York, NY 10027                                  New York, NY
and NBER                                            katherine.rodrigues@rescue.org
chrisblattman@columbia.edu
                                                    Margaret Sheridan
Julian C. Jamison                                   University of North Carolina
Office of Research                                  Department of Psychology and Neuroscience
Consumer Financial Protection Bureau                235 E. Cameron Avenue
1700 G Street NW                                    Chapel Hill, NC
Washington, DC 20552                                sheridan.margaret@unc.edu
julison@gmail.com

Tricia Koroknay-Palicz
The World Bank
1818 H St NW
Washington, DC
t.gonwa@gmail.com
1       Introduction
The trouble with many survey topics, whether it’s abortion, drug use, crime, domestic vi-
olence, or support for terrorism, is that people may not tell the truth. This makes survey
data on any sensitive topic suspect. Even without incentives to misreport, self-reported data
are often inaccurate. Studies show people even misreport their sex and education.1 When
measuring subjects that can embarrass or endanger the respondent, we worry that people
underreport their attitudes or actions.2
    When we are interested in the impact of a program or event, measurement error will
also affect our ability to estimate unbiased causal effects. In dependent variables, random
measurement error reduces precision but won’t bias estimates.3 Systematic reporting er-
rors, however, generally bias causal estimates, especially when the measurement error is
correlated with treatment. For instance, people who receive an anti-crime message or an
addiction treatment might be more likely to respond that they are non-violent or drug free,
both because it’s socially desirable and because of perceived experimenter demand (where
participants conform to the expectations of the people who ran the program).
    Researchers have come up with a number of ways to avoid bias in self-reported data.
In developed countries, it’s common to use administrative data. For example, studies of
crime-reduction programs (such as the one we study in this paper) often prefer arrest and
incarceration records to self-reported crime (e.g. Deming, 2011). Such data are seldom
available in developing countries, however. Moreover, arrest data have serious systematic
measurement error problems of their own.4
    Others use survey experiments and indirect questioning. In list experiments, respondents
report the number of items they agree with on a list, which randomly includes or excludes
a sensitive item.5 In endorsement experiments, respondents rate their support for actors
expressing sensitive ideas (Bullock et al., 2011). These are valuable tools, albeit with limita-
tions. They can be imprecise and require large samples, and they can be cumbersome when
measuring an array of items. Survey experiments also rely on two key assumptions: that
people do not lie when counting on a list or endorsing a person, and that the presence of
sensitive items doesn’t affect reporting of non-sensitive ones (Blair and Imai, 2012).
    1
      See Asher (1974); Bound et al. (2001).
    2
      For instance, Karlan and Zinman (2008) find that large numbers of borrowers do not report high-interest
consumer loans, potentially because they feel embarrassed.
    3
      See Asher (1974); Hausman (2001). This statement applies primarily to linear models.
    4
      Arrests underreport true criminal behavior, and they require strong assumptions: that arrests are re-
sponses to crimes rather than statistical or other discrimination; and that the treatment doesn’t affect the
likelihood of being arrested for a crime, by changing the location and observability of the crime for example.
    5
      e.g. Raghavarao and Federer (1979). For recent applications see Blair and Imai (2012); Karlan and
Zinman (2012); Jamison et al. (2013).


                                                      1
    Finally, in some cases data are physically verifiable and researchers can use a little of
what Freedman (1991) called “shoe leather” and simply verify behavior. For instance, in
Mexico, the government sent administrators to audit self-reported asset data used to decide
who was in or out of a cash transfer program and found underreporting of assets to increase
eligibility (Martinelli and Parker, 2009).
    This paper develops and field tests an alternative approach for testing the direction and
degree of survey misreporting. It is intended to be useful when objective administrative
data are not available or survey experiments are impractical. We use the approach on self-
reported measures of crime, drug use, homelessness, gambling, and discretionary spending.
In principle the method could be applied to other sensitive topics where objective assessments
are difficult—intimate partner violence, prostitution, risky sex behaviors, participation in
communal violence, voting behavior, sexual identity, stigmatized diseases, and so forth.
    The approach is relatively simple. We use intense qualitative work—including in-depth
participant observation, open-ended questioning, and efforts to build relationships and trust—
to try to elicit more truthful answers from a random subsample of experimental subjects.
We focus on a very small number of key behaviors, and over several days of trust-building
and conversation, we try to elicit a direct admission or discussion of the behavior.
    We then compare these qualitative observations to survey responses, and use the dif-
ference to estimate the direction, magnitude, and patterns of measurement error. It is
effectively a shoe leather approach for difficult-to-verify, often covert behaviors. Like survey
experiments, the method relies on the assumption that people are more truthful than in a
survey. The techniques we use—spending time with respondents, interacting in their natural
environment, developing a rapport, and trying to attain “insider” status—are central tech-
niques in qualitative and ethnographic research to obtain honest and valid responses (e.g.
Wilson, 1977; Bryman, 2003).
    This paper illustrates the approach, including when, where, and how it could be applied
to other field experiments. It also describes the patterns of reporting bias we observe in this
particular crime-reduction study, upending the priors we held about the nature and direction
of experimenter demand in these circumstances.
    The study recruited a thousand destitute young men in the slums of Liberia’s capital,
Monrovia, with an emphasis on men involved in petty crime or drugs. The formal eval-
uation by Blattman et al. (2015) randomized two interventions designed to reduce crime
and violence: an 8-week program of group cognitive behavior therapy (CBT) to discourage
impulsive, angry, and criminal behaviors; and an unconditional cash transfer of $200.
    Obviously, we should be wary of self-reported survey measures of illegal or immoral be-
havior, especially from a population suspicious of authority, some of whom make their living


                                               2
illicitly. We should be doubly concerned when one of the treatments (therapy) tried to per-
suade people away from “bad” behaviors, potentially triggering additional social desirability
bias or the perception of experimenter demand among the treated. We can imagine any
informational or behavioral intervention would raise similar concerns. List experiments were
one option, but we found them difficult to implement with a largely uneducated, illiter-
ate population that were selected in part for impulsive behavior.6 Thus we developed this
alternative.
     Of more than 4,000 endline surveys conducted over the study, we randomly selected
roughly 7.3% and attempted to validate survey responses on just six behaviors. Within days
of the survey, one of a small team of Liberian qualitative research staff (“validators”) would
visit the respondent four times over ten days, each day spending several hours as a partici-
pant observer or in active conversation with the man, his peers, and community members.
Validators sought a direct admission of the behavior after building trust and familiarity. In
effect the method is a very intensive, relationship-based form of survey auditing, which cost
roughly as much as a regular survey to implement.
     Validators and the authors then coded an indicator for whether or not the respondent
had engaged in each behavior in the two weeks prior to the survey (i.e. during the time frame
about which survey questions on recent behavior were asked). Beforehand, we deemed four
behaviors “potentially sensitive”: marijuana use, thievery, gambling, and homelessness. Two
others were common, non-sensitive behaviors that could be subject to recall bias or other
forms of error: paying to watch movies in a video club, and paying to charge their mobile
phone at a kiosk. We call these the “expenditure” measures.
     This qualitative approach is not free from error: validators could still miss behaviors,
make faulty inferences, or let suspicions of treatment status influence their judgment (among
other things). These limits of participant observation are well-known (Power, 1989). But
these errors, we argue, are less likely to bias treatment effect estimates than the experimenter
demand and social desirability bias we worried would cause underreporting in the survey.
It comes down to the proposition that four days building trust and extensive information
on the subject, and focusing on just six behaviors, reduces the appearance of experimenter
demand and other biases correlated with treatment relative to the survey, during which a
stranger asks about the same six behaviors in a 300-question, 90-minute questionnaire.
     This is the key assumption underlying the technique. It parallels the “no liars” and
“no design effects” assumptions in list experiments. Like list experiments, the assumptions
  6
    For instance, a list experiment read aloud would require many ideas to be held in mind, and we were
concerned that answers would be correlated with cognitive abilities.




                                                  3
cannot be tested directly. But if we accept them we can estimate the direction and magnitude
of systematic measurement error, especially the association with treatment.
    Naturally, no method is free of measurement error, and an in-depth qualitative study
could still be vulnerable to bias. For instance, if the presence of an observer prompted good
behavior, we would underestimate sensitive behaviors in both the survey and validation.
People have been shown to increase hand-washing behavior, for instance, when directly
observed.7 Our main goal, however, is to mitigate measurement error that is correlated with
treatment status, and in general this paper argues that the trust-building and time invested
by validators plausibly reduces such systematic error.
    In this specific crime study, the results increase our confidence in the survey-based treat-
ment effects for sensitive behaviors, but decrease our confidence in the economic treatment
effects. Potentially sensitive behaviors are relatively common according to the survey. At
endline, 22% of men reported stealing in the past two weeks, and 48% admitted to marijuana
use. For the four sensitive behaviors, survey responses and validated measures are identical
about 80% of the time. Expenditure measures are identical about 70% of the time.
    On average, sensitive behaviors are slightly underreported, but this seems to be driven
mainly by the underreporting of gambling, especially in the control and cash only treatment
arms. Otherwise most of the other sensitive measures occur just as frequently in the survey
and validated data. If anything, the group that received both cash and therapy slightly
over-reported sensitive behaviors.
    Expenditures seem to be underreported in the survey relative to validation, in the full
sample and across all treatment arms. Underreporting of expenditures is largest in the
control group, however.
    Using the complete set of survey-based outcomes, Blattman et al. (2015) found that cash
led to short run income gains that dissipated after a year. They also found that therapy
reduced anti-social behaviors, such as crime, immediately and dramatically, but that this
change persisted only if the men received therapy and cash.
    If we accept the validated measures indicate systematic measurement error correlated
with treatment status, our conclusions about the interventions change. The core result—that
therapy plus cash led to sustained falls in anti-social behavior—is bolstered. The evidence
suggests the survey-based effects could underestimate the true effects by up to 20%. The
validation calls into question, however, the short term increase in consumption from cash.
   7
     These studies put a direct observer in the home for several hours who counts hand-washing instances. In
Bangladesh, Ram et al. (2010) compare hand-washing instances on non-observed days (using an electronic
counter) to observed days and find that hand-washing increases from 3.7 to 5 times, suggesting a Hawthorne
effect of observation.




                                                     4
    Our qualitative work suggests explanations. One reason most men do not underreport
drugs, crime, or gambling is that the men most enmeshed in these activities were less likely
to feel stigma than “normal” society members, as they are part of a counterculture in which
these behaviors are common, as these men are already seen by “normal” society as pariahs.
This suggests that bias in a high-risk sample is lower than expected, but that bias in a
population-based sample could have sizable measurement error correlated with initial levels
of the behavior.
    So why do we see modest underreporting of sensitive behaviors in the control and cash
groups? One possibility is that therapy recipients were more comfortable admitting to sen-
sitive behaviors while the cash and control groups behaved more strategically, hoping to be
eligible for programs in the future—the reverse form of experimenter demand we expected.
Therapy could also have helped to reduce any marginal stigma, by helping the men talk
more openly about the behaviors, at least with the leaders of the therapy groups, who were
seen as affiliated, in a way, with the study.
    We see similar explanations for underreporting of expenditures. One is a strategic inter-
est in over-reporting poverty in order to appear eligible for future programs. A second is
recall bias in consumption and expenditure data. In principle both could be correlated with
treatment.
    Altogether, these findings are crucial to the credibility of the experimental estimates, in
this case bolstering the claim that the therapy reduced crime and other anti-social behaviors,
and moderating the claim that the cash transfer increased incomes. Perhaps more broadly,
the findings also challenge conventional notions of the direction of measurement error.
    Interestingly, Baird and Özler (2012) find a similar pattern with girls’ school participation
in Malawi: all subjects overstate their school participation, but over-reporting is highest in
the control group. In aid projects, where control subjects hope to enter programs, the
incentives to misreport may be high.
    It would be a mistake, however, to cite this paper as evidence that systematic measure-
ment error of sensitive behaviors in high-risk populations is low; that behavioral treatments
foster trust and reduce measurement error; or that low-salience expenditures are especially
vulnerable to experimenter or recall bias. These are all plausibly true, but before we can
generalize more validation needs to be done in more places. An important takeaway message
is that, despite several years working with this and similar populations, including extensive
quantitative surveys and qualitative interviews, our priors about the most important sources
of measurement error were wrong.
    We include a detailed description of our procedures to make it easier for other researchers
to adapt and use the method. In principle, we think it is applicable to a wide range of risky or


                                               5
stigmatized sexual, health, and economic activities. The cost, in our case, was roughly 3% of
the total evaluation budget, a modest amount given that measurement error in self-reported
data was the key causal identification concern in the evaluation.
    One analog to our approach is in psychology, where virtually every self-reported survey
measure of mental health has been validated using structured clinical interviews (e.g. Spitzer
et al., 1999). Another is a recent surge of behavioral and other measures to validate survey
data on violence, prejudice, and other troublesome outcomes. In addition to the list and en-
dorsement experiments mentioned before, Scacco (2010) interviewed a random subsample of
potential religious rioters behind a screen that shielded their identity, and Paluck and Green
(2009) measure cooperation by the patterns of distribution of a group survey gift. Finally,
business profits and consumption have also proven troublesome to measure and have been
the subject of experimental measurement studies. de Mel et al. (2009) experimentally test
alternative approaches to measuring microenterprise profits, and find (counterintuitively)
that the least intensive methods yield the least biased estimates. Beegle et al. (2012) have
experimentally tested various consumption measures against one another. These studies
have proven important to the studies where income is the crucial outcome. Ours could prove
as useful to interventions targeted at violence, crime, and other risky or stigmatized behav-
iors. One thing is certain: systematic measurement error is a large and largely unaddressed
problem, calling for more such new tools and their refinement and replication.


2     Context and experimental design
In poor countries like Liberia, governments are especially fearful of urban young men and the
possibility they will commit crimes, rioting, or election violence. We designed a study to test
the economic and behavioral roots of crime and violence among high risk men. Blattman
et al. (2015) describes the study in full detail.


2.1    Full experimental sample
The study recruited 999 young adult men in five neighborhoods Monrovia, a city of roughly
1.5 million. The study sought out “hard-core street” men—men in their 20s and 30s who
live in extreme poverty and may be involved in violence, drugs and crime. We recruited
and implemented the study in three phases over two years, typically in different, distant
neighborhoods (see Appendix A). Table 1 describes the study sample at baseline.
    On average the men were age 25, had nearly eight years of schooling, earned about $68
in the past month working 46 hours per week (mainly in low-skill labor and illicit work),


                                              6
                              Table 1: Description of the study sample (n=999)
 Baseline covariate                     Mean    Std. Err.       Baseline covariate                 Mean   Std. Err.

 Age                                    25.4     (4.86)         Average weekly work hours in:
 Married/living with partner            16%      (0.37)          Potentially illicit activities    13.6    (27.26)
 # of women supported                    0.5     (0.64)          Agricultural Labor                0.4     (3.69)
 # children under 15                     2.2     (3.17)          Low-skill wage labor              19.4    (28.85)
 Muslim                                 10%      (0.30)          Low-skill business                11.5    (23.98)
 Years of schooling                     7.72     (3.29)          High-skill work                   1.5     (7.59)
 Literacy score (0-2)                   1.23     (0.90)         Ex-combatant                       38%     (0.49)
 Math score (0-5)                        2.8     (1.57)         Currently sleeping on the street   24%     (0.43)
 Health index (0-6)                      4.9     (1.38)         Times went hungry last week        1.26    (1.36)
 Disabled                                8%      (0.26)         Sells drugs                        20%     (0.40)
 Monthly cash earnings (USD)            68.30    (84.49)        Drinks alcohol                     75%     (0.43)
 Durable assets index, z-score          0.00     (1.00)         Uses marijuana daily               44%     (0.50)
 Savings stock (USD)                    33.75    (67.39)        Uses hard drugs daily              15%     (0.35)
 Able to get a loan of $300             11%      (0.31)         Stole in past two weeks            53%     (0.50)

  Notes: Surveys were completed with all men, but there are a small number of missing baseline values
  per respondent. For purposes of regression analysis, these are imputed with the sample median to avoid
  losing the observation.


and had $34 saved. 38% were members of an armed group during the two civil wars that
ravaged the country between 1989 and 2003. 20% reported selling drugs, 44% reported daily
marijuana use, 15% reported daily use of hard drugs, 53% reported stealing something in
the past two weeks, and 24% reported they were homeless in the last two weeks.


2.2       Intervention and experimental design
We designed, implemented, and evaluated two interventions—group cognitive behavior ther-
apy and cash—in a factorial experimental design. We first randomly assigned half the sample
to an offer of therapy. Therapy was completed within eight weeks. Following this, we held
a second lottery for grants of $200 with the full sample.8

Treatment 1: Cognitive behavior therapy and counseling The therapy was designed
and implemented by a local non-profit organization, Network for Empowerment and Pro-
gressive Initiatives (NEPI) Liberia. The 8-week program had two main goals. The first was
    8
      None knew of the cash grant until after therapy was completed. Randomization was done through public
draw in blocks of roughly 50. There is balance across treatment and control groups. 90% of all men assigned
to the therapy attended at least six days of the therapy. Those who did not attend had slightly less schooling,
slightly higher earnings and assets, and are less likely to use drugs or alcohol or steal. Thus it appears the
highest risk young men were the most likely to attend. See Appendix A for details.


                                                            7
“transformation,” or the shift from the position (and self-identity) as an outcast living on the
fringe of society to an economically- and socially-integrated member of mainstream society.
The second goal was to shift men from present-oriented decision-making to future-oriented
goals and behavior.
    The approach and curriculum grew out of NEPI’s experience and practice, but were
largely grounded in cognitive behavioral therapy (CBT) theory and practice. Group-based
CBT approaches have been validated, typically in US populations, to reduce substance abuse,
criminality, and aggression.
    Participants met three times a week in groups of about 20, for four hours at a time, led
by two facilitators. The only compensation provided for attendance was a bowl of rice and
simple stew. On alternate days when the group did not meet, the facilitators visited the
men at their homes or work areas to provide individual advising and encouragement. Many
of the facilitators who ran the group intervention and individual counseling were themselves
ex-combatants or reformed street youth.
    The CBT element of the program manifested itself in the emphasis on small practical
changes each session, which are reinforced through encouragement and praise. These in-
cluded reducing substance use and abuse, improving body cleanliness, improving the clean-
liness of the area in which they live, and managing their anger without resorting to violence.
Facilitators also formally encouraged participants to engage with society in planned and
unaccustomed ways.
    Facilitators also taught skills around planning and goal setting to help participants en-
hance their future-oriented attitudes, anticipate potential setbacks, and build skills for deal-
ing with adversity. Finally, throughout the eight weeks, facilitators articulated a set of
mainstream social norms and encouraged them to adopt these norms.9

Treatment 2: Unconditional cash grant All men were eligible for a cash grant of
$200. The cash was both a treatment and also a measurement tool (to see whether spending
patterns were affected by the therapy).10 The framing of the grant was minimalist—people
were told that it was random, one-time, and unconditional.11
   9
     These include discouragement of crime, substance abuse, and interpersonal violence (encouraging instead
the use of peaceful solutions to conflict). The program also encouraged good financial management, especially
saving money, as an important aspect of future- and goal-oriented behavior.
  10
     An international non-profit, Global Communities, conducted the cash distribution. These partners
conducted all recruitment and program implementation to minimize the perceived connection between the
research team and programs.
  11
     Prior to the lottery, the group merely received a short lecture (15-30 minutes) on how to safeguard the
funds once received. Of those assigned to the cash grant, 98% received it.




                                                     8
2.3      Survey data collection
The research team, a Liberia branch of the non-profit research organization Innovations for
Poverty Action (IPA), presented themselves as independent evaluators.12
    We attempted to collect survey data from each recruit five times: at baseline prior to the
intervention; at “short-run” endline surveys roughly 2 and 5 weeks after the cash transfers;
and at two “long-run” endline surveys 12 and 13 months after the cash grants.13
    This sample was exceptionally mobile and difficult to track over time. By making at
least four attempts to track each man, we were able to track and survey 92% of the target
sample across all endline survey rounds. Attrition is not strongly correlated with baseline
covariates or treatment assignment.14


3        Empirical strategy
To motivate the empirical tests, we outline a simple model of the effect of different forms of
measurement error in outcomes in the context of an experimental intervention. We adapt
the simple linear model from Bound et al. (2001) review of measurement error for these
illustrative purposes. In particular, we suppose the true treatment effect specification is:

                                              y ∗ = α + θT + ε                                              (1)

where y ∗ is the true outcome and T is an indicator for assignment to treatment.15 The
observed survey outcome y s , however, measures the true outcome with both systematic and
random error:
    12
      They visually distinguished themselves from other organizations by wearing uniquely colored emerald
green t-shirts and identification badges over the years of the study. The exception to this is the validators,
who wore street clothes that helped them blend in with the study participants.
   13
      The exception is the 100 men in the pilot phase, who had a single “short run” survey 3 weeks after the
grant, and a pair of “medium-run” surveys at 5 and 7 months in addition to the 12- and 13-month surveys.
We ran pairs of short-run and long run surveys because it allowed us to take two measures of relatively noisy
outcomes with potentially low autocorrelation such as earnings, expenditures, criminal activity, drug use, and
so forth. Taking multiple measurements at short intervals allows one to average out noise, increasing power
(McKenzie, 2012). Each survey was roughly 90 minutes long, followed by roughly 90 minutes of interactive
behavioral games and psychological tests. Liberian enumerators conducted face-to-face interviews in Liberian
English using handheld electronic devices.
   14
      A majority changed locations between each round, many changing sleeping places every few weeks or
nights. We generally made at least four attempts to locate each person, in all corners of the country, including
prison (to be interviewed only when released). See Appendix A for formal analysis of attrition. The joint
significance of all covariates and treatment assignment for survey attrition has a p-value of .53. Attrition is
also roughly one percentage point lower in the treatment groups (not statistically significant).
   15
      Bound et al. (2001) consider a continuous covariate X rather than indicator T . They also assume that
other right-hand side variables are measured without error and have been partialled out. We ignore other
covariates in this simple example, but the basic intuitions would hold with them present.


                                                       9
                                     ys = δsy∗ + γ sT + µ                                 (2)

where we assume the random error µ is uncorrelated with y ∗ , T and ε. Throughout this
illustration, δ denotes systematic measurement error of the true outcome (such as underre-
porting due to social desirability bias) and γ indicates error associated with treatment only
(as in the case of experimenter demand, for example).
    To calculate treatment effects on y s , the researcher estimates the following potentially
erroneous equation:

                                        y s = α̂ + θ̂T + ε̂                               (3)

By substituting equation 1 into 2 and comparing to 3, we can see that the researcher estimates
the treatment effect θ̂ = δ s θ + γ s , and the bias from the true treatment effect θ is:

                                  E(θ̂ − θ) = (δ s − 1)θ + γ s                            (4)

   There are three main cases to consider:

   • δ s = 1 and γ s = 0 is the special case of classical (random) measurement error;

   • 0 < δ s < 1 and γ s = 0 is the case where the survey measure systematically underreports
     the true outcome (but underreporting is uncorrelated with treatment status), in which
     case under-reporting would bias the estimated treatment effect towards the null, and
     over-reporting away from it, proportional to δ; and

   • γ s > 0, which is the more worrisome case is in which case we mistake measurement
     error (such as experimenter demand) for a treatment effect.

Now imagine we can collect validation data, y ν , for a random subsample:

                                     yν = δν y∗ + γ ν T + η                               (5)

where η is uncorrelated with T , y ∗ , ε, and µ. We define the difference in the survey and
validation measures as:

                     y ∆ = y s − y ν = (δ s − δ ν )y ∗ + (γ s − γ ν )T + µ − η            (6)

The key assumption in this paper is that validation data correspond more closely to y ∗ than
survey data. That is:
                                  0 ≤ |δ ν − 1| < |δ s − 1|                               (7)

                                                10
                                        0 ≤ |γ ν | < |γ s |                                 (8)

We also assume that the measurement error in the survey and validation data are in the
same direction, and that different bias mechanisms are not at work in validation. If true,
then y ∆ is a proxy for over-reporting (under-reporting if negative). If a validation technique
does not meet these assumptions, it is unsuitable for the purposes of this paper. In practice
one cannot test this formally for clandestine behaviors, and the assumptions must be argued
based on context and quality of the process. In much the same way, list experiments rely
on the assumption of less lying and no design effects, and instrumental variables estimation
rely on the exclusion restriction.
    If assumptions 7 and 8 hold, however, it means we can identify the direction and approx-
imate magnitude of systematic survey error from the sample mean of y ∆ and assess whether
the survey error is correlated with treatment by estimating the treatment regression:

                                     y ∆ = α∆ + θ ∆ T + ζ                                   (9)

where, since there is a treatment indicator in y ∗ , θ∆ = (δ s − δ ν )θ + γ s − γ ν .
    As the validated measure approaches the true outcome measure, then θ∆ approaches the
value of the treatment effect bias described in equation 4. That is, as δ ν → 1 and γ ν → 0
then θ∆ → E(θ̂ − θ). The main focus of our analysis will be to calculate y ∆ in equation 6
and estimate θ∆ from equation 9.
    This formalization draws attention to several important caveats associated with any
validation technique of this nature:

  1. Identification of the bias E(θ̂ − θ) hinges entirely on the credibility of the validation
     method and measure. The assumption of lower systematic measurement error is gen-
     erally untestable and is a judgment call based on the nature and quality of the process.

  2. Validation data cannot help us to separately identify the bias arising from general
     systematic error δ apart from treatment-specific error γ, except in the case where
     γ = 0 in the control group (i.e. no “John Henry” effects or other forms of experimenter
     demand in the control group). In theory, the systematic and treatment-specific errors
     could run in opposite directions and cancel one another out. In that case, however,
     y ∆ 6= 0.

  3. So long as the validation measures are imperfect and 0 < |γ ν | or 0 < |δ ν − 1|, the esti-
     mates from equation 9 will tend to underestimate measurement error. The confidence
     interval on θ∆ also increases with any noise in the validated measure, η.


                                               11
     4. Nonetheless, to the extent that the validation measures are credible, if we validate a
        random subset of the study sample we can adjust the distribution of y ∗ (conditional
        on T or other covariates) or estimate the “true” treatment effect θ using θ̂ − θ∆ .


4        Validation methodology
We selected six variables for validation, all with recall periods of two weeks. We chose
outcomes with varying degrees of salience (or memorability) and potential social stigma and
experimenter bias. The variables were:

     1. Stealing. The survey asked how many times in the last two weeks the respondent
        stole someone’s belongings or deceived or conned someone of money or goods.16 Based
        on our fieldwork, we hypothesized that stealing would be the most salient and least
        socially desirable of all six measures.

     2. Gambling. The survey asked how many times in the last two weeks the respondent
        gambled or bet on sports. Beforehand, we hypothesized gambling had a lower level of
        salience and sensitivity than stealing, but was still somewhat stigmatized.

     3. Marijuana use. The survey asked how many times in the last two weeks the respondent
        smoked marijuana. Marijuana use is not socially acceptable across Liberian society
        overall, but is fairly prevalent in our target demographic. We initially hypothesized
        underreporting could arise not so much from social stigma but from the discouragement
        of drug use in the therapy treatment.

     4. Homelessness. The survey asked how many times in the last two weeks the respondent
        had to sleep outside, on the street, or in a market stall because they had no other
        place to sleep or stay. This is a salient variable where we hypothesized respondents
        might have under-reported from embarrassment or over-reported in order to appear
        more needy (and eligible for more programs).

     5. Phone charging. In the expenditure section of the survey, the survey asked how many
        times in the last two weeks the respondent charged his phone for money. This cor-
        responds to taking one’s phone to a kiosk with electricity where one pays a small fee
        to recharge the battery, a common and routine expense for many Liberians, without
        stigma and possibly not very memorable. 38% of our sample had a mobile phone at
        the endline, and 38% reported charging a phone in the last two weeks.
    16
    The survey also measured more serious forms of theft, such as armed robbery, but our qualitative
validation focussed on non-violent theft.

                                                12
   6. Video Club Attendance. In the expenditure section of the survey, the survey asked
      how many times in the last two weeks the respondent went to a video club. These
      clubs are private businesses where one can go to watch a movie, television show, or
      football match for a small fee. This is a popular and socially acceptable pastime, as
      most Liberians do not have electricity or home entertainment. Salience was unclear
      but likely greater than phone charging.


4.1     Validator staff
Eight local staff performed validations over the two years of data collection. We selected
validators from the study’s qualitative research staff. These people typically began as survey
enumerators, but displayed such skill and rapport with the subjects that we hired and trained
them to conduct a separate qualitative research component: longitudinal, formal, open-ended
interviews with a different subsample of subjects. All conducted the qualitative validation
when they were not working on the formal open-ended interviews.17 Each validator received
at least 10 days of training on the methods, including both classroom learning and extensive
field training.18 Like any qualitative study, we believe staff recruitment and training to have
been among the most important tasks and also the largest start-up cost of this method.


4.2     Approach
For each respondent, validators tried to determine whether the respondent had engaged in
any of the measured behaviors, even once, in the two weeks preceding the respondent’s survey
date, as the survey asked about behaviors occurring during the two weeks prior to the survey
. We found it optimal for validators to visit each respondent four times, on four separate
days, with each visit or “hangout session” lasting approximately three hours. The validator
aimed to begin hanging out the day after subjects completed their quantitative surveys and
to conduct all four visits in the days following the respondent’s endline survey date.
    Validators deliberately avoided the feeling of a formal interview and would typically
accompany respondents as they went about their business.19 Validators sometimes took
  17
     All but one were men, and all had a high school education. Two of the men completed roughly half the
validations with the remainder doing roughly 10 to 20% each. To find these validators, we trained roughly
two to three times the number of people needed from the pool of research staff, selecting only those with the
most natural questioning and rapport-building skills for the validation exercise.
  18
     Details of validator selection and training, team structure, tools and forms are in Appendix B.
  19
     On the first visit validators would obtain verbal consent. We designed the consent script to be informal,
and explained that the goal of hanging out with the respondent was to talk about some of the same things
they discussed in the survey. In addition to this verbal consent, the formal consent form that preceded the
recent survey said that qualitative staff may come and visit them again to gather more information.



                                                     13
notes during visits, but only in isolated areas out of sight from the respondent.20 The idea
follows from basic principles of ethnography, which seeks to study subjects in their natural
settings, similar to those the researcher hopes to generalize about (Wilson, 1977). The intent
is to reduce the sense of being in an experimental situation, which ethnographers perceive
as creating bias.
    The main approach was to engage in casual conversation on a wide range of topics, in-
cluding the six target topics/measures. The target topics were raised mainly through indirect
questions while informally chatting. For example, validators typically started conversations
with discussions of family. This was both customary among peers in Liberia and a sign of
respect and interest in respondents’ lives. It was also a stepping stone for discussing the
target behaviors—either because the validator can discuss an issue in their family (someone
engaging in one of the activities) or how the respondent’s family feels about their current
lifestyle and circumstances.
    In general, validators found it helpful to tell respondents stories or scenarios about another
person or themselves, related to the target measures, then steer the conversation to get
information about how respondents had behaved in similar situations, eventually discussing
the past two weeks. Validators were careful to present these behaviors and incidents in a non-
stigmatized light, for instance by discussing a friend who stole in order to get enough to eat,
or how they themselves had periods of homelessness or used drugs and alcohol. Validators
found these personal stories (all of which were truthful) and genuineness were essential to
building rapport and trust.
    Validators might hold these conversations once or twice over the three hours, spending
perhaps twenty or thirty minutes in conversation each time, to avoid unnaturally long or
awkward conversations. The validator spent the remainder of the three hours in the general
vicinity, observing respondents engaging in their daily activities. This could involve taking
a rest in the shade or in a tea shop (as is common) or engaging others in conversation.
Validators would also try to talk casually with the respondent’s friends, relatives, or neighbors
to learn about him (although we considered information from these second-hand sources
as insufficient to support a conclusion about the respondents’ behaviors, but merely as
supporting information).
    We found that building a rapport with participants in a short space of time was crucial.
To develop trusting and open relationships, validators used techniques, including becoming
close to respected local community and street leaders, eating meals together, sharing per-
sonal information about themselves, assisting subjects with daily activities, and mirroring
  20
    e.g. in a toilet stall or teashop. If validators were unable to find a secluded area in which to take notes,
they sometimes recorded information in their cell phones, pretending to send a text message.



                                                      14
participants’ appearances and vernacular, as appropriate. In addition, validators tried to
maintain neutrality and openness while discussing potentially sensitive topics. For instance,
conveying—through stories or otherwise—that illicit behaviors were not perceived nega-
tively, allowed respondents to feel comfortable sharing their involvement in such activities.
Validators did not lie to or deceive respondents, however.
    Overall, this approach—trust-building, spending time together over the course of several
days, assuming the role of an “insider,” seeking admission or discussion of the behavior, clan-
destine but fairly immediate note-taking, and (as discussed below) close examination of the
evidence for each respondent with the investigators—was designed to counter the observer
bias and selective recall that concern participant observation.21 Developing a rapport with
respondents, spending time to develop a relationship, and obtaining insider status are con-
sidered central to obtaining more honest and valid responses (Baruch, 1981; Bryman, 2003;
Fox, 2004). We are not aware of any study, however, that has quantitatively tested this
proposition.


4.3     Validation sampling and non-response
In each endline survey round we randomly selected study respondents to be validated, strati-
fied by treatment group.22 Table 2 describes the samples selected for validation in each survey
round over the course of the study. In total, we randomly selected 7.4% of all surveys, 297
in total, for validation.
     We found 240 (81%) of the 297.23 This attrition is an identification concern, but there is
little evidence of biased attrition. Excess validation attrition (those who were surveyed but
not validated) was not robustly associated with baseline characteristics (see Appendix A).

Statistical power. In order to minimize the confidence intervals surrounding any treatment-
measurement error correlation, we chose the sample size that maximized the number of in-
  21
     For general discussions of validity in qualitative methods, see Wilson (1977); LeCompte and Goetz
(1982); Power (1989).
  22
     For each pair of survey rounds, study participants were randomly divided into blocks (e.g. 1, 2, 3, 4),
and block 1 study participants were surveyed before block 2, and block 2 before block 3, etc. Within each
block we randomly selected validation subjects using a computer-generated uniform random variable. The
selection was performed without replacement in a given pair of survey rounds (e.g. the short-term endline
surveys in a given phase), but sampling was performed with replacement across survey rounds. Twenty
subjects were validated in more than one round.
  23
     We could not find 15 for even the endline survey. We could not validate a further 42 because they were
difficult to find even immediately after the survey or (more commonly) because they lived a long distance
away. In general, we surveyed respondents who had moved far out of Monrovia, but we were unlikely to
validate them because of the time and expense and opportunity cost.




                                                    15
                                                  Table 2: Validation sample, totals and attrition
                       Surveys                    Validation                                     Reason for no validation data
                                                                          Unfound      Unfound for       % validated    % validated     % validated
     Phase       Round       Target #        Selected   Validated        at endline     validation          (all)       (treatment)      (control)
                  3-week         100             0
                 5-month         100            24          18                2              4              75%             75%             75%
       1         7-month         100            24          12                1              11             50%             50%             50%
                12-month         100            10          6                 3              1              60%             63%             50%
                13-month         100            10          8                 2              0              80%             86%             67%
                 3-week          398            26          24                0               2             92%             94%              89%
                 5-week          398            27          17                0              10             63%             68%              40%




16
       2
                12-month         398            28          25                2              1              89%             86%             100%
                13-month         398            44          38                1              5              86%             85%             91%
                 3-week          501            0
                 5-week          501            0
       3
                12-month         501            35          31                2              2              89%             89%             88%
                13-month         501            69          61                5              3              88%             88%             88%
      All                       4096           297          240              18              39             81%             81%             80%

               Notes: The proportion selected in each round was principally a function of logistical feasibility (e.g. number of available staff),
           and in some none were selected. As procedures became more familiar and staff more experienced, more could be done over time. The
           percentage validated in the treatment group includes any treatment (cash, CBT, or both).
terviews we felt qualified validators could manage logistically.24 Post hoc calculations of
statistical power confirm the estimates we made at the design stage. With a sample of 240,
we can detect general over- or under-reporting greater than 17% of the survey mean (14%
of the “true” validated mean).25 Because each treatment arm is a subsample, however, we
cannot precisely measure the effect of treatment on misreporting—it is difficult to detect
effects greater than 33% of the survey mean (28% of the validated mean). Thus we are
principally interested in the sign and magnitude of the treatment effect on misreporting by
treatment group.


4.4     Coding validated data
Validators were unaware of the respondents’ survey responses, and formed their own opinions
(based on the evidence collected) about whether respondents engaged in the six activities
during the time period captured by the quantitative survey. Every coding recommendation
was then discussed with and vetted by one of the authors.
    A core part of the validator training included logical reasoning, supporting reasoning with
evidence, and writing this down in a clear and structured manner. After each visit, validators
made written notes about the relevant data collected, including evidence to support their
conclusions, on a standardized form. At the conclusion of the four visits, the validator coded
six indicators, one for each behavior, where “1” meant that he had relatively direct evidence
that the respondent engaged in the behavior during the recall period, and “0” otherwise.26
  24
      In general, the validation sample was a balanced subsample of the full sample (see Appendix A for sam-
pling and balance details). Power calculations, based on roughly the first 60 validator interviews, indicated
that there was a modest degree of underreporting of all behaviors, sensitive and non-sensitive, but that the
correlation between treatment status and measurement error was uncertain—across outcomes it varied in
sign and magnitude, but was about zero on average. Thus the chief advantage of maximizing the sample
conditional on time available was to shrink the confidence interval to build confidence in our method and
the main outcomes of interest. Further validation was mainly limited by the number of validators we felt
could be trained and supervised.
   25
      We calculated this minimum detectible effect (MDE) using a two-sided hypothesis test with 80% power
at a 0.05 significance level, using baseline and block controls when calculating the R-squared statistic.
We calculated an MDE for both the 0–2 expenditures index and the 0–4 sensitive behaviors index. The
expenditures index had a mean of .82 in the survey and an MDE of .13 for general over- and under-reporting
and .29 for a treatment effect on misreporting. The sensitive behaviors index had a mean of 1.12 in the survey
and an MDE of .2 for general over- and under-reporting and .36 for any treatment effect on misreporting.
We estimate that doubling the sample size would have increased power by about a third.
   26
      Over the course of the exercise, different measures offered different experiences and lessons. Because
of its relative frequency and visibility, we suspect marijuana use was the easiest to directly observe. But
validators found other behaviors straightforward to discuss in conversation. In the survey and (especially)
the validation, phone battery charging led to the most confusion—in particular, did simply charging one’s
phone count, or did only paying to charge one’s phone count? Paid charging was the focus of the survey
question (it appeared in an expenditure survey module), but we were concerned that the validators would
use a more expansive definition. We attempted to mitigate such differences through trainings and regular
discussions on the coding.


                                                     17
    Validators recorded an average of 1.35 “major” pieces of evidence per respondent per
behavior to support their coding decision sheets. This was typically the most persuasive piece
or pieces of evidence rather than all evidence collected.27 Table 3 reports evidentiary methods
by behavior. In general, the validators used some form of direct or indirect questioning—a
direct admission of the behavior or persuasive statements that they did not engage in the
behavior. The validators only witnessed or found direct evidence of the behavior in a fifth of
cases, or had third party verification in about 6% of cases. In any event, witnessing or third
party verification were not sufficient evidence for a final coding. For instance, witnessing
had to be followed by questions confirming that the respondent also engaged in the behavior
in the two weeks prior to the survey. This accounts for most of the cases where there was
more than one piece of evidence highlighted.
    In general, the patterns of evidence are fairly commonsensical. Witnessing is limited
to observable behaviors such as marijuana, gambling, homelessness, and phone charging.
Stories and scenarios where the respondent is invited to comment or discuss are especially
common for the most sensitive subject, stealing. Indirect questioning is most common for
everyday topics such as homelessness (“Is this your house?”) and phone charging (“I need to
charge my phone. Where do you usually charge yours?”).


4.5     Limitations of the approach
While we think, based on our experiences, that this validation exercise gave enough time to
gather detailed, accurate information and fostered trust and frankness, there are nonetheless
limitations to this approach.

   1. Potential disruption. The presence, and interactions and conversations with the
      validators may be intrusive and might disrupt respondents’ daily activities, thereby
      altering the findings. To mitigate this risk, validators wore clothes that would blend in
      with their respondent’s environment, and typically accompanied and assisted respon-
      dents in their activities as appropriate (e.g. helping a scrap metal collector scavenge).
   Homelessness also proved somewhat challenging to measure and validate, as we discovered its definition
is subjective. Circumstances arose that were somewhat ambiguous, such as having no home of one’s own
but regularly sleeping on a friend’s floor or in an acquaintance’s market stall. To account for the potential
variability in perceptions of homelessness, validators were instructed to include as much information as
possible about respondents’ living situations in their summary reports. The authors then worked with
validators to code a somewhat broad definition of homelessness that included any ambiguous circumstances.
Prior to analysis, it was not clear whether survey respondents applied the same definition, and hence we err
on the side of finding underreporting in the survey.
   27
      We do not have complete paper records of all evidence collected, and so the 1.35 pieces of evidence is
probably an understatement of the full amount of evidence.



                                                     18
         Table 3: Evidentiary methods reported by validators, by behavior
                                                       Potentially sensitive behaviors           Expenditures

Main evidence techniques                       Steal     Marijuana     Gamble     Homeless      Video      Phone
                                                (1)          (2)         (3)         (4)         (5)        (6)

Avg. pieces of evidence                         1.1          1.3         1.1         1.7         1.0        1.2

Obs. (All)                                      240          240         239         240         239        240

  Direct question                              36%           35%         38%         5%         32%         1%
  Indirect question                            28%           46%         42%         62%        59%        92%
  Story / scenario                             36%           6%          13%         12%         2%         1%
  Witnessed / found evidence                    3%           31%         9%          62%         5%        18%
  Third party account                           3%           6%          4%          21%         0%         0%
  Other / unclear                               3%           9%          6%          13%         6%         5%

Obs. (Coded “did not engage” in behavior)       191          118         170         190         93         125

  Direct question                              38%           44%         39%         5%         34%         0%
  Indirect question                            26%           46%         44%         60%        58%        98%
  Story / scenario                             37%           7%          15%         12%         3%         2%
  Witnessed / found evidence                    2%           3%          1%          65%         2%         1%
  Third party account                           3%           10%         4%          24%         0%         1%
  Other / unclear                               2%           1%          1%          14%         4%         0%

Obs. (Coded “did engage” in behavior)           49           122          69             50      146        115

  Direct question                              29%           25%         36%         4%         30%         2%
  Indirect question                            33%           46%         38%         70%        60%        86%
  Story / scenario                             33%           5%          9%          10%         1%         0%
  Witnessed / found evidence                   10%           59%         28%         52%         7%        37%
  Third party account                           4%           2%          4%          8%          0%         0%
  Other / unclear                               8%           17%         17%         6%          8%        10%

             Notes: Direct questions imply the validator asked the respondent directly about his engage-
       ment in the activity. Indirect questions imply the validator brought up the subject in general
       conversation (Where do you live? What do you do to make money?). Stories and scenarios are
       a form of indirect questioning where the respondent is invited to comment. Witnessing or found
       evidence implies the validator saw the respondent engaging in the activity in question or found
       physical evidence that the respondent recently engaged in the activity. Third party accounts
       imply the validator asked the family and friends of the respondent whether or not he engaged
       in the activity. Other or unclear methods include a handful of cases of unprompted information
       from the respondent, and also cases where the behavior could be inferred from other knowledge.
       Mainly it implies that coding was inconclusive or incomplete but is likely a form of questioning.




                                                        19
2. Differences in recall periods. The validation occurred after the time period
   about which the survey questions had asked, and validators or respondents could have
   made errors about the relevant window of time (e.g. homelessness could have been
   observed the week after the survey, and inferred to the time of the survey incorrectly).
   This is most likely a source of random measurement error.

3. Inconsistent questions. The survey and validation questions might have been
   interpreted differently, making it difficult to compare results. As discussed above,
   phone charging and homelessness proved somewhat difficult to measure consistently.
   We used close consultations and reviews of the data, and focus groups with survey and
   validation staff, to maximize consistency.

4. Reverse Hawthorne effect. Training validators to look for certain behaviors
   could lead them to overreport those behaviors (akin to the problem of “when you have
   a hammer everything looks like a nail”). This reverse Hawthorne effect would probably
   be more of a risk if the validation method relied on passive observation. Rather,
   validation involved active discussion and (usually) a direct admission of the behavior.
   Also, one of the authors reviewed and discussed the evidence for every subject with
   the validator.

5. Increasing social desirability bias. In principle the participant observation
   method, by building rapport, could lead to a different source of measurement error by
   (for example) increasing social desirability bias. Our strong sense is that the opposite
   is true, that trust and rapport reduced the bias, but this is a subjective interpretation
   and not independently verifiable.

6. Consistency bias. In principle, respondents could recall their survey response and
   try to remain consistent despite trust-building. This could motivate randomizing the
   order of validation and survey in the future.

7. Non-blinded validators. The researcher is not immune from bias in qualitative
   research (LeCompte and Goetz, 1982; LeCompte, 1987). We are especially concerned
   with any bias correlated with treatment. While validators weren’t given the subject’s
   treatment status, it’s possible and even likely that this could come up during the
   extended conversations. Thus there is a danger that the validators’ biases will be
   correlated with treatment. The trust-building and preference for direct admission of
   the behavior was intended to mitigate this risk, but it still remains.




                                           20
Most importantly, it seems unlikely that validators would commit most of these errors differ-
entially across study arms. Misreporting correlated with treatment is still a risk under the
consistency bias and non-blinded limitations, but the in-depth focus on a handful of ques-
tions, time invested, and trust-building is designed to counteract these biases as much as
possible. If so, the qualitative validation method may be most useful at building confidence
estimated treatment effects.
    Finally, like any qualitative work, this is not an off-the-shelf tool. To select and refine the
variables, recruit and train validators, and monitor quality of the data requires the researcher
to have some familiarity with the context and population and at least basic experience in
qualitative data collection.


4.6     Replicability of the approach
There are three reasons to think that this method could be replicated in other developing
country field experiments and surveys. First, the expertise needed to implement the method
effectively exists in most countries. Indeed, it should be considerably simpler to implement
outside than inside Liberia. After fourteen years of civil war, and with one of the lowest
human development indices in the world, Liberia has very low local research capacity, even
compared to other poor and post-conflict states.
    Second, most social scientists are nearly as well prepared to design and implement the
approach as they are a new survey instrument or measure. Like any measure or method,
it takes local knowledge, care, and extensive pretesting to develop a credible approach, and
can benefit from someone with expertise in the subject area. In our case, one of the field
research managers had some background in qualitative work and quality assurance, which
we believe improved the quality of training and selection of the validator staff.
    Third, the cost of the data collection is not necessarily large relative to many field exper-
iments or large-scale panel surveys. In this instance, the fixed cost of startup was primarily
in the recruitment and training of the small number of validators—approximately 2 to 3
weeks of work. We estimate the marginal cost of validation was roughly $80 per respondent,
mainly in wages and transport. By comparison, the marginal cost of surveying a respondent
was roughly $70.28
    While this method is considerably more expensive than survey experiments, it is more in
line with the depth and cost of commonplace efforts to improve consumption measurement
  28
     Both figures were driven by the fact that it typically took one to two days of searching to find each
respondent for surveying, plus the time to survey itself. Both surveying and validating in Liberia were
expensive by the standards of household surveys, largely because of the cost of operating in a fragile, post-
conflict state and the great difficulties in tracking such an unstable population.



                                                     21
through the use of diaries physical measurement.29 For crucial measures in large program
evaluations, or for statistics informing major policies, the cost is small relative to the inter-
vention, larger study, or larger purpose. For instance, as a proportion of total expenditures
on the study, this validation exercise cost under 3% of all research-related costs, and less
than 1–2% of program plus research costs.


5        Results
For each of the six behaviors, we construct indicators for that behavior using survey data
and the qualitative validation technique, pooling responses from all endline surveys. We
also construct additive indices of the four potentially sensitive behaviors and the two expen-
ditures. Table 4 reports means in the full sample and each treatment arm, as well as the
percentage of times the two measures are in agreement. Table 5 reports estimates of y ∆ ,
the difference between the survey and validation measures. Table 6 reports the treatment
effects for each arm on the survey-based outcomes (θ̂), our estimated association of treat-
ment and measurement error (θ∆ ), and the adjusted treatment effect correcting for observed
measurement error (θ̂ − θ∆ ). 30


Rates of behavior Overall these are relatively common behaviors. According to the
survey data reported in Table 4, in the two weeks prior to the survey, 22% stole, 48% used
marijuana, 18% gambled, 23% were homeless for at least a night, 42% attended a video club,
and 39% paid to charge a mobile phone.

Correspondence in the survey and validator data.                 In general, the survey and
validated data are identical about 80% of the time for sensitive measures and about 70% of
the time for expenditures (Table 4). Correspondence is lowest for video club expenditures
(62% overall), perhaps because of the low salience.
    On average, however, the unadjusted validation means were higher than the survey means,
suggesting slight underreporting on the survey. The average person reported 1.21 sensitive
behaviors and 1.09 expenditures in validation, and 1.12 sensitive behaviors and 0.82 expen-
ditures in the survey.
    29
     In one extreme example, in the India NSS consumption survey, enumerators physically measure the
volume of all food consumption (NSSO Expert Group, 2003).
  30
     To estimate θ∆ , we follow the regression in equation 9 and regress the proxy for survey over-reporting, y ∆ ,
on indicators for the three treatment arms, controlling for randomization block and survey round fixed effects
and baseline covariates. Validators were not randomly assigned and in principal could introduce endogeneity
bias if better validators were assigned to harder cases. Validator fixed effects are nearly collinear with the
block and survey round fixed effects, however, and so this endogeneity is partly accounted for.


                                                        22
        Table 4: Comparison of survey and qualitative validation means at endline
                                Potentially sensitive behaviors                         Expenditures              All

                    All (0-4)    Steal   Marijuana        Gamble    Homeless   All (0-2)    Video      Phone     (0-6)
                       (1)        (2)           (3)         (4)         (5)       (6)        (7)        (8)       (9)
                                                        a. Full sample
Survey mean           1.12       0.22           0.48        0.18        0.23     0.82        0.42      0.39       1.93
                     (1.14)     (0.42)      (0.50)         (0.39)    (0.42)      (0.73)     (0.50)     (0.49)    (1.31)
Validation mean       1.21       0.20           0.51        0.29        0.21     1.09        0.61      0.48       2.30
                     (1.18)     (0.40)      (0.50)         (0.45)    (0.41)      (0.74)     (0.49)     (0.50)    (1.21)
% in agreement                   79%            85%         72%         82%                  62%       82%

                                                       b. Control group
Survey mean           1.25       0.27           0.48        0.23        0.27     0.68        0.37      0.32       1.93
                     (1.31)     (0.45)      (0.50)         (0.43)    (0.45)      (0.70)     (0.49)     (0.47)    (1.44)
Validation mean       1.30       0.23           0.49        0.34        0.23     1.18        0.65      0.54       2.48
                     (1.23)     (0.42)      (0.50)         (0.48)    (0.42)      (0.70)     (0.48)     (0.50)    (1.21)
% in agreement                   80%            88%         72%         77%                  47%       75%

                                                        c. Therapy only
Survey mean           1.06       0.19           0.48        0.17        0.22     0.81        0.41      0.41       1.87
                     (1.11)     (0.39)      (0.50)         (0.38)    (0.42)      (0.75)     (0.50)     (0.50)    (1.35)
Validation mean       1.09       0.17           0.48        0.24        0.20     0.98        0.54      0.44       2.07
                     (1.14)     (0.38)      (0.50)         (0.43)    (0.41)      (0.76)     (0.50)     (0.50)    (1.24)
% in agreement                   80%            89%         74%         80%                  72%       81%

                                                         d. Cash only
Survey mean           1.03       0.21           0.49        0.13        0.21     0.77        0.37      0.40       1.81
                     (1.16)     (0.41)      (0.50)         (0.34)    (0.41)      (0.71)     (0.49)     (0.49)    (1.35)
Validation mean       1.32       0.23           0.53        0.33        0.24     1.00        0.55      0.45       2.32
                     (1.26)     (0.42)      (0.50)         (0.47)    (0.43)      (0.81)     (0.50)     (0.50)    (1.33)
% in agreement                   76%            82%         74%         90%                  56%       85%

                                                       e. Therapy + cash
Survey mean           1.13       0.22           0.48        0.21        0.22     0.98        0.54      0.44       2.11
                     (0.98)     (0.42)      (0.50)         (0.41)    (0.42)      (0.73)     (0.50)     (0.50)    (1.11)
Validation mean       1.11       0.19           0.52        0.24        0.16     1.17        0.70      0.48       2.29
                     (1.11)     (0.40)      (0.50)         (0.43)    (0.37)      (0.68)     (0.46)     (0.50)    (1.05)
% in agreement                   81%            83%         68%         81%                  71%       87%
Observations          239        238            238         238         239       239        238        239       239

    Notes: The table reports the means (standard deviations) of the survey and the qualitatively validated measures for
the full sample and by treatment arm. “% in agreement” is the percentage of respondents for whom the survey indicator
equals the qualitatively validated indicator.




                                                              23
Table 5: Survey over-reporting, estimated by the mean difference between survey and vali-
dation measures (y ∆ )

                                      Potentially sensitive behaviors                           Expenditures

                         All (0-4)    Steal    Marijuana    Gamble      Homeless      All (0-2)     Video      Phone
                            (1)        (2)        (3)           (4)       (5)            (6)         (7)        (8)

       Full sample         -0.10      0.017      -0.03        -0.11       0.02          -0.27       -0.19      -0.08
                           0.17      0.573       0.24         0.00        0.45         0.00         0.00       0.00

       Control group       -0.07      0.034      -0.02        -0.12       0.03          -0.50       -0.29      -0.22
                           0.64      0.568       0.71           0.09      0.60         0.00         0.00       0.00

       Therapy only        -0.04      0.019       0.00        -0.07       0.02          -0.17       -0.13      -0.04
                           0.80      0.766       1.00           0.29      0.77          0.08         0.07      0.53

       Cash only           -0.29     -0.016      -0.05        -0.20      -0.03          -0.23       -0.18      -0.05
                           0.04      0.799       0.37         0.00        0.42         0.03         0.03       0.32

       Therapy + cash       0.02      0.032      -0.05        -0.03       0.06          -0.19       -0.16      -0.03
                           0.91      0.568       0.37           0.66      0.25         0.01         0.02       0.48

       Observations         239       238         238           238       239           239          238        239

           Notes: Columns 1 to 8 report the simple mean differences in the survey and validation measures for the
       full sample and for each treatment arm, along with p-values for as t-test of whether the mean is different
       from zero. We bold p-values ≤0.05.



    Note that this is an average, however, and the qualitative validation finds cases of over-
and under-reporting in the survey relative to the validation. There are 328 instances where
the measures are not equal: 208 cases of survey under-reporting (the survey indicator is a
zero and validation indicator is a one), and 120 cases of over-reporting (see Appendix C.1).
Most of these differences are assessed using some form of direct or indirect questioning. Some
form of external evidence, such as direct observation or third party confirmation, was used to
validate 21% of underreports (especially marijuana use, homelessness, gambling and phone
charging) and 14% of over-reports (mainly homelessness).31

Underreporting of sensitive behaviors, particularly gambling. Table 5 reports our
proxy of survey over-reporting: the simple survey-validation differences, with p-values from a
t-test of the difference from zero. Negative values indicate survey under-reporting, assuming
the validator measure is more accurate of course. As noted above, we have the statistical
power to detect differences greater than about 17% of the survey mean.
    Overall, gambling seems to be slightly underreported in every treatment arm, and highly
underreported by men in the control and cash only groups. For instance, 33% of the cash
  31
   Direct observation was more likely to identify underreporting than other forms of evidence, but this is
mechanical since (with the exception of homelessness) it’s not possible to observe a non-behavior.

                                                           24
only group admitted to gambling during validation, compared to 13% during the survey.
Some of this underreporting could be due to ambiguous behaviors being coded as gambling
in validation interviews but not in the survey. But the fact that underreporting is smaller in
the therapy arms suggests that the underreporting is not an artifact of different definitions,
but rather reflects a strategic response to treatment status.
    If we look at stealing, marijuana use, and homelessness, however, none of the survey-
validation differences are statistically significant. There is possibly some slight underreport-
ing of drug use and slight over-reporting of stealing, but the magnitudes are generally small
in the sense that they are less than 10% of the survey means reported in Table 4. The sample
size is small, however, and so many of these differences are not precisely estimated.

Underreporting of expenditures. We see much stronger evidence of underreporting of
expenditures in the survey. The difference for both expenditures is -0.27 in the full sample
(Table 5, Column 6). This difference is large—about a third of the survey mean reported
in Table 4. Expenditure underreporting is largest for the video club measure, but both
expenditures appear to be underreported. Interestingly, the mean differences appear to be
smaller and less statistically significant if the men received one of the treatments. We return
to these differences across treatment arms below.

Survey-based treatment effects. Blattman et al. (2015) report full treatment effect
estimates, short-term and long-term, based on the survey data. These results indicate that
cash (alone or in combination with therapy) led to a slight increase in consumption in the
month after the grant, including a fall in homelessness, in part because the men spent the
grant directly, but also because they invested in petty business and increased their earnings.
After a year, however, these earnings and consumption gains had disappeared, in part because
adverse economic shocks eliminated the men’s additional cash, savings and investments.
    Therapy, meanwhile, led to self-reported falls in anti-social behaviors ranging from 30
to 50%, especially in inter-personal aggression, drug dealing, and theft. After a month, the
falls were similar in both the therapy only and therapy plus cash groups. After a year,
however, the fall was only sustained in the therapy plus cash group. The paper hypothesizes
that therapy plus cash had a more sustained effect on anti-social behaviors because the
cash grant positively reinforced the behavior change and enabled the men to practice their
new skills and carry on with their new identity. This large, sustained fall in self-reported
anti-social behaviors in the therapy plus cash group is the central finding of the study.
    Turning to Table 6, we see the same patterns reflected in our pooled treatment effects on
the six validated behaviors. Cash weakly increased expenditures and reduced homelessness,


                                              25
                  Table 6: Estimates of treatment effects (θ)
                                                           ˆ and treatment effect bias (θ∆ ) by outcome and treatment

                                                                      Potentially sensitive behaviors                                      Expenditures

                                                    All (0-4)        Steal      Marijuana       Gamble       Homeless          All (0-2)       Video         Phone
                                                       (1)            (2)           (3)           (4)            (5)              (6)           (7)            (8)

                                                                                  a. Cash only
                                     ˆ
              ATE using survey data (θ)               -0.057        -0.031         0.012         0.025         -0.062            0.080         0.027          0.053
                                                      [.095]        [.029]         [.040]        [.029]       [.031]**           [.052]        [.034]         [.037]

              ATE on measurement error (θ∆ )          -0.178        -0.018        -0.046         -0.047        -0.063            0.285         0.106          0.185
                                                      [.190]        [.087]         [.067]        [.088]        [.076]           [.130]**       [.109]       [.079]**

              Adjusted ATE (θ̂ − θ∆ )                 0.121         -0.014         0.058         0.072         0.001             -0.205        -0.079        -0.132
                                                      [.196]        [.088]         [.074]        [.093]        [.077]            [.143]        [.117]        [.079]*

                                                                                b. Therapy only
                                     ˆ
              ATE using survey data (θ)               -0.186        -0.045        -0.022         -0.087        -0.031            0.000         0.001         -0.001
                                                     [.092]**       [.028]         [.040]      [.026]***       [.031]            [.050]        [.032]         [.035]

              ATE on measurement error     (θ∆ )      0.004         -0.003         0.009         0.044         -0.038            0.335         0.169          0.171




26
                                                      [.199]        [.082]         [.056]        [.101]        [.088]           [.134]**       [.105]        [.087]*
              Adjusted ATE (θ̂ − θ∆ )                 -0.190        -0.041        -0.031         -0.131        0.007             -0.334        -0.167        -0.172
                                                      [.202]        [.082]         [.066]        [.104]        [.087]           [.148]**       [.121]        [.090]*

                                                                               c. Therapy + Cash
                                     ˆ
              ATE using survey data (θ)               -0.398        -0.105        -0.069         -0.099        -0.125            0.076         0.012          0.064
                                                    [.090]***      [.027]***      [.040]*      [.026]***      [.029]***          [.050]        [.033]        [.035]*

              ATE on measurement error (θ∆ )          0.118         0.016         -0.021         0.099         0.031             0.314         0.137          0.183
                                                      [.182]        [.076]         [.062]        [.093]        [.086]          [.116]***       [.101]       [.076]**

              Adjusted ATE (θ̂ − θ∆ )                 -0.516        -0.122        -0.048         -0.198        -0.156            -0.239        -0.125        -0.120
                                                    [.194]***       [.075]         [.071]       [.095]**       [.082]*          [.137]*        [.113]         [.077]
              Observations, survey/validation      3765 / 239     3764 / 238    3762 / 238    3763 / 238     3765 / 239       3763 / 239    3761 / 238     3759 / 239

         Notes: The survey-based ATE estimates, θ̂, pool all survey rounds and regress each outcome on treatment indicators and block fixed effects. Standard errors are
     robust and clustered by individual. Estimates of the bias from treatment, θ∆ , come from a regression of the difference in the survey and validation measures on an
     indicator for treatment arms, controlling for block fixed effects and each endline round. Standard errors are robust and clustered by block. The difference, θ̂ − θ∆ , is
     an estimate of the true treatment effect after adjusting for observed bias. It is calculated as the linear difference of the estimates and the standard error is calculated
     via bootstrapping (we performed 1000 draws from the sample, with replacement; we calculated θ̂, θ∆ , and θ̂ − θ∆ for each draw; and we generated the standard error
     on θ̂ − θ∆ using the distribution from these draws). *** p<0.01, ** p<0.05, * p<0.1
but had little effect on stealing, marijuana use and gambling. These are driven mainly by
short term impacts. Therapy decreased stealing and gambling especially, and to a lesser
extent drug use. With therapy plus cash, the effects are largest and more statistically
significant, in large part because they are sustained in the long run.

Adjusted treatment effects. We also estimate the effect of each treatment on survey
over-reporting, θ∆ , in Table 6. These estimates effectively take the simple survey-validation
differences in Table 5 and estimate the difference across treatment arms, adjusting for base-
line covariates as well as survey round and randomization block fixed effects. We use these
to calculate an adjusted treatment effect, θ̂ − θ∆ .32
    First, the results imply that the adjusted treatment effect of therapy and cash on sensitive
behaviors overall is no lower than what we estimate with self-reported survey data, and may
even be larger (Column 1). This holds true for each of the individual sensitive behaviors,
save marijuana use. Despite the large standard errors introduced by the small validation
sample, the adjusted treatment effect on all sensitive behaviors is larger and significant at
the 1% level.
    Meanwhile, the underreporting of gambling does not have a statistically significant as-
sociation with treatment. Those who received cash alone underreported gambling to the
surveyors more often than control group members, and so the measurement error in gam-
bling is probably a combination of a general desirability bias as well as one correlated with
treatments. A larger sample size would be needed to separate these more precisely.
    In contrast, the slight underreporting of expenditures behaviors in the survey (seen in
Table 5 above, but also the estimates of θ∆ in Table 6) implies that the short term increase
in survey-based expenditures due to cash could be due to measurement error correlated with
treatment. The adjusted treatment effect of therapy plus cash, θ̂ − θ∆ , is generally neg-
ative but not statistically significant (Column 6). We see a similar pattern with another
expenditure-related item, homelessness, in Table 6—the survey-reported decline in home-
lessness tends to disappear with adjustment.


6        Discussion and conclusions
Perhaps the most important lesson from this exercise is that structured, in-depth, and rep-
resentative qualitative work revealed patterns of measurement error that were quite different
from our priors, despite extensive experience with the study group. There is little data on
    Recall that θ∆ → E(θ̂ −θ) as the validation measure approaches the “truth”. If we omit baseline controls,
    32

our results and conclusions are qualitatively the same (see Appendix C.2).


                                                     27
measurement error, however, and so (like many) our priors were unavoidably rooted more in
common sense and common causal identification concerns rather than an informed under-
standing.
    We worried, for instance, that high-risk young men might have special reasons to conceal
their behavior—such as suspicion of outsiders, or a desire to receive programs in future.
Given that we were focused on measuring the treatment effects of a therapy program that
discouraged various anti-social and unhealthy behaviors, we were also concerned that the
treated would underreport such behaviors out of experimenter demand or social desirabil-
ity bias induced by the therapy. Our multi-method approach revealed that the nature of
measurement error was quite different.
    For this specific field experiment, two findings stand out. First, the qualitative valida-
tion suggests that the underreporting in sensitive behaviors was modest, not statistically
significant, concentrated in the control and cash only groups, and limited to one behavior in
particular (gambling). Meanwhile, expenditures seemed to be broadly underreported in the
survey, most of all in the control group.
    Based on qualitative interviews, our impression is that these “sensitive” behaviors, while
not acceptable within Liberian society as a whole, men in our study sample belong to a
sub-culture of outcasts, and within this sub-culture these behaviors are not so stigmatized
that most men in our sample feel ashamed to report them. Moreover, the risk of punishment
was minuscule.33 Hence underreporting tended to be modest overall.
    An exception was gambling. After receiving a cash handout, it’s possible that men were
reluctant to admit they’d gambled some of it away. The same was true of the control group,
to a lesser degree, who may have hoped for cash in future. It is possible that members
of the cash and control groups were behaving strategically, reporting fewer anti-social and
unhealthy behaviors in the hope this would improve their chances of a program in the
future.34 Alternatively, the therapy treatment could have increased the familiarity, trust or
reciprocity between the subjects and implementers, and so men who received therapy were
less likely to underreport.
    The second major finding is that the expenditure-related activities were systematically
underreported across all arms, and especially large in the control group. The effect of
treatment on measurement error is large and statistically significant in all arms. This finding
  33
     The Liberian police are largely incapable of investigating and prosecuting all but the most grave crimes.
Thus, these behaviors are not endangering, embarrassing or socially undesirable to most of our sample and
their peers, and hence they discuss them freely.
  34
     There was no future program (this was communicated repeatedly), and the original field experiment
actually used the opposite criteria for recruiting subjects, but these features of our program were unusual
compared to usual NGO practice.



                                                     28
is extremely important given that expenditure and consumption surveys are the principal
means of measuring material well-being and poverty in most developing countries. We see
three main possible explanations:

  1. Strategic behavior. Since there was underreporting across all treatment arms, every
     study participant may have had an incentive to exaggerate their neediness in the hopes
     of future programs. This echoes our gambling result.
     Why more so in the control group? It’s possible that the fewer the treatments a man
     received, the more strategically he behaved on the survey, trying to appear poorer to
     encourage eligibility for future treatment. Those who received therapy, for example,
     might be interested in the cash. Phone charging and going to a video club are consid-
     ered discretionary spending, and if a respondent wanted to signal destitution, he might
     underreport spending on these items.
     We view this explanation as plausible, though there are caveats. First, the control
     group did not over-report homelessness to the same degree, which is an obvious in-
     dicator of need (although perhaps observable enough that it was harder to falsify on
     a survey). Second, drug use is technically an expenditure, and this was not under-
     reported to signal poverty. Third, in principle those who received one of the earlier
     treatments also had incentives to behave strategically in the hopes for future programs.
     Treated men almost universally lobbied for additional assistance.

  2. Salience and recall bias. Expenditures could be more subject to more recall error,
     because they are less regular and possibly less salient than drug use or crime. There is
     ample evidence that consumption and expenditure data are underreported, and that
     underreporting increases with the period of recall, the lower the reported consumption
     per standardized unit of time, and the less salient the purchase (Deaton and Grosh,
     1997; Gibson, 2006; Beegle et al., 2012). Neither video clubs nor mobile phone charging
     were particularly salient. People may also make cognitive errors when aggregating over
     a construct such as “the last two weeks.” Finally, the expenditures survey module was
     long and much more subject to fatigue, compounding underreporting.
     Recall bias is plausible, but we are also looking for explanations that would correlate
     with treatment. There are a few possibilities. Treatment could have increased attention
     and mindfulness. The therapy was explicitly designed to reduce impulsive behavior and
     to increase planning. There is some evidence that impulsivity improved (Blattman
     et al., 2015). The cash transfer could have had a similar effect for different reasons.
     Studies have also shown that recall bias in consumption data increases with poverty


                                            29
     (Beegle et al., 2012). This is consistent with evidence that cognition is taxed by poverty
     and scarcity (Mani et al., 2013). Presumably hunger would affect survey fatigue and
     mindfulness. The cash grant (and short run decrease in poverty) could have had a
     similar effect on the margin. Finally, receiving either treatment could have produced
     enough reciprocity that the treated exercised more care in recalling less salient data.
     We regard these explanations with caution but cannot reject them.

Both explanations are plausible but come with caveats, and so we refrain from a firm conclu-
sion about the sources of measurement error. Given the importance of expenditure surveys
in research it bears replication and other future research.
    In retrospect, we also see that, had the measurement error run in the opposite direction,
it would have been difficult to distinguish the large effect of therapy and cash on crime from
systematic measurement error. The effect of treatment on our proxy for survey over-reporting
would have been underpowered. We estimate that doubling the size of the validation sample
would have increased power by about a third. The marginal cost of validation was roughly
equal to that of running a survey. Thus we estimate could have doubled the number of
validations by either increasing the evaluation budget by 3%, or reducing the total sample
size by 3%. Given how much the credibility of the experiment rests on self-reported data,
this strikes us as a reasonable investment.
    Overall, these results reinforce a fundamental principle of survey methodology: the im-
portance of validating measurements with multiple instruments. To some extent our findings
are unexpected (even puzzling) and the explanations somewhat speculative because valida-
tion and other studies of measurement error remain rare. As more evidence accumulates,
our priors will become more accurate rather than ad hoc. We regard our multi-method ap-
proach as largely complementary to list and endorsement experiments. Survey experiments
will not always be feasible or credible enough to satisfy (as in our case). They also attempt
to validate data by bringing the survey into a more hypothetical or game-like domain, and
besides some practical difficulties (such as sample size demands) require strong assumptions
such as no liars and no design effects. Thus it is useful to have more tools available.
    Like other methods, ours requires strong assumptions—in this case, that in-depth obser-
vation is less prone to bias and does not introduce major new biases. Our method is also
more costly to implement (though not necessarily relative to the average cost of large surveys
or modest impact evaluations). The stakes are high enough in many field experiments and
impact evaluations that validating a handful of key outcomes seems important for the indi-
vidual project. We argue qualitative validation performs well enough, and yields sufficiently
important results, that our approach deserves more systematic use and examination, ideally
alongside these other methods.

                                             30
References
Asher, H. B. (1974). Some consequences of measurement error in survey data. American
  Journal of Political Science 18 (2), 469–485.

Baird, S. and B. Özler (2012). Examining the reliability of self-reported data on school
  participation. Journal of Development Economics 98 (1), 89–93.

Baruch, G. (1981). Moral tales: parents’ stories of encounters with the health professions.
  Sociology of Health & Illness 3 (3), 275–295.

Beegle, K., J. De Weerdt, J. Friedman, and J. Gibson (2012). Methods of household con-
  sumption measurement through surveys: Experimental results from tanzania. Journal of
  Development Economics 98, 3–18. 1.

Blair, G. and K. Imai (2012). Statistical analysis of list experiments. Political Analysis 20 (1),
  47–77.

Blattman, C., J. Jamison, and M. Sheridan (2015). Reducing crime and violence: Experi-
  mental evidence on adult noncognitive investments in Liberia. Working paper .

Bound, J., C. Brown, and N. Mathiowetz (2001). Measurement error in survey data. In
  Handbooks in Economics, Volume 2, pp. 3705–3843.

Bryman, A. (2003). Quantity and quality in social research. Routledge.

Bullock, W., K. Imai, and J. N. Shapiro (2011). Statistical analysis of endorsement ex-
  periments: Measuring support for militant groups in pakistan. Political Analysis 19 (4),
  363–384.

de Mel, S., D. J. McKenzie, and C. Woodruff (2009). Measuring microenterprise profits:
  Must we ask how the sausage is made? Journal of Development Economics 88 (1), 19–31.

Deaton, A. and M. Grosh (1997). Consumption. Designing Household Survey Questionnaires
  for Developing Countries: Lessons from Ten Years of LSMS Experience.

Deming, D. J. (2011). Better schools, less crime?             The Quarterly Journal of Eco-
  nomics 126 (4), 2063–2115.

Fox, R. C. (2004). Observations and reflections of a perpetual fieldworker. The ANNALS of
  the American Academy of Political and Social Science 595 (1), 309–326.



                                               31
Freedman, D. A. (1991). Statistical models and shoe leather. Sociological methodology 21 (2),
  291–313.

Gibson, J. (2006). Statistical tools and estimation methods for poverty measures based on
  cross-sectional household surveys. Handbook on Poverty Statistics.

Hausman, J. (2001, October). Mismeasured variables in econometric analysis: Problems
  from the right and problems from the left. The Journal of Economic Perspectives 15 (4),
  57–67.

Jamison, J. C., D. Karlan, and P. Raffler (2013). Mixed-method evaluation of a passive
  mHealth sexual information texting service in uganda. Information Technologies & Inter-
  national Development 9 (3).

Karlan, D. and J. Zinman (2008). Lying about borrowing. Journal of the European Economic
 Association 6 (2-3), 510–521.

Karlan, D. S. and J. Zinman (2012). List randomization for sensitive behavior: An ap-
 plication for measuring use of loan proceeds. Journal of Development Economics 98 (1),
 71–75.

LeCompte, M. D. (1987). Bias in the biography: Bias and subjectivity in ethnographic
  research. Anthropology & Education Quarterly 18 (1), 43–52.

LeCompte, M. D. and J. P. Goetz (1982). Problems of reliability and validity in ethnographic
  research. Review of educational research 52 (1), 31–60.

Mani, A., S. Mullainathan, E. Shafir, and J. Zhao (2013). Poverty impedes cognitive function.
 Science 341 (6149), 976–980.

Martinelli, C. and S. W. Parker (2009). Deception and misreporting in a social program.
 Journal of the European Economic Association 7 (4), 886–908.

McKenzie, D. (2012). Beyond baseline and follow-up: The case for more t in experiments.
 Journal of Development Economics 99 (2), 210–221.

NSSO Expert Group (2003). Suitability of different reference periods for measuring household
 consumption. results in pilot survey. Economic and Political Weekly, 25–31.

Paluck, E. L. and D. P. Green (2009). Deference, dissent, and dispute resolution: An
  experimental intervention using mass media to change norms and behavior in rwanda.
  American Political Science Review 103, 622–644.

                                             32
Power, R. (1989). Participant observation and its place in the study of illicit drug abuse.
  British Journal of Addiction 84 (1), 43–52.

Raghavarao, D. and W. T. Federer (1979). Block total response as an alternative to the
  randomized response method in surveys. Journal of the Royal Statistical Society. Series
  B (Methodological), 40–45.

Ram, P. K., A. K. Halder, S. P. Granger, T. Jones, P. Hall, D. Hitchcock, R. Wright,
  B. Nygren, M. S. Islam, and J. W. Molyneaux (2010). Is structured observation a valid
  technique to measure handwashing behavior? Use of acceleration sensors embedded in soap
  to assess reactivity to structured observation. The American journal of tropical medicine
  and hygiene 83 (5), 1070–1076.

Scacco, A. (2010). Who riots? explaining individual participation in ethnic violence. Dis-
  sertation, New York University.

Spitzer, R. L., K. Kroenke, and J. B. W. Williams (1999). Validation and utility of a self-
  report version of PRIME-MD: the PHQ primary care study. Journal of the American
  Medical Association 282 (18), 1737–1744.

Wilson, S. (1977). The use of ethnographic techniques in educational research. Review of
 educational research, 245–265.




                                            33
Appendix for online publication

A        Sample details
A.1        Recruitment and timing
The implementers of the therapy, NEPI, approached roughly 1500 men, and 999 agreed to
speak and to enter the sample (we do not have data on those who refused). To avoid recruiting
groups of friends and colleagues (i.e. to minimize correlated outcomes and spillovers) NEPI
approached just one out of every seven to ten high-risk men they identified. Of those assigned
to therapy, nearly all attended at least a day, and two thirds completed it. The higher risk
men were the most likely to finish.
    Table A.1 describes the structure of the sample and treatment assignment. For logistical
reasons we recruited, treated, and studied the men in three phases. The first phase was a
small, successful pilot. Based on its results, we decided to scale the program to a further 900
with only minor changes to the interventions and study protocols. To accommodate the large
number of men we scaled up in two phases. An advantage of this phased implementation is
that we calculate impacts over cohorts with different spatial and time shocks.


A.2        Randomization balance and survey attrition
In general, all randomizations display balance. Table A.2 reports an ordinary least squares
(OLS) regression of an indicator for random assignment on baseline variables and block fixed
effects (fixed effect coefficients not shown). At the base of the table we report the p-value
from an F-test of joint significance of all baseline covariates.35 Columns 1 and 2 look at
assignment to cash treatment, Columns 3 and 4 assignment to the CBT treatment, and
Columns 5 and 6 assignment to validation.
    The baseline covariates are not jointly significant in predicting any treatment (Columns
1 to 4). A small number of baseline variables are individually statistically significant, but
no more than would be expected at random. In terms of selection into validation, selection
is not very systematic. The covariates (including fixed effects) explain just 13% of the
variation, and the baseline covariates are jointly significant with a p-value of 0.095. 5 of
the 28 displayed baseline covariates (18%) have a p-value less than 0.1. Those selected into
validation are slightly less likely to be married, support slightly more women, are slightly
more likely to have been an ex-combatant, and are slightly less likely to drink alcohol. There
 35
      Randomization block fixed effects are omitted from the table and the F-test.


                                                      i
            Table A.1: Study sample and treatment assignment by block and phase

                            Start                                      % recruits assigned to:

                    Phase   MM/YY         Block              N     Therapy   Cash    Both        None

                      1     12/10       Red Light            100    28%      24%      25%        23%

                            06/11       Red Light            219    27%      25%      24%        24%
                      2
                            06/11    Central Monrovia        179    32%      19%      32%        17%

                            03/12      Clara Town            175    29%      27%      23%        21%
                      3     02/12      Logan Town            86     27%      29%      20%        24%
                            02/12     New Kru Town           240    26%      27%      24%        23%


       Notes: By design, percentages assigned to each treatment can vary from 25%. Treatments were
       allocated by public draw of colored chips from a bag, without replacement, in groups (blocks)
       of about 50 people. There were more chips in the bag than people by design (to avoid the last
       person having a predetermined probability of assignment).



are few substantively or statistically significant differences in the sensitive behaviors we are
interested in measuring and validating. Overall, selection imbalance seems relatively modest.
    In addition, attrition appears to be relatively unsystematic. Table A.3 reports OLS
regression of an indicator for being unfound on baseline variables and block fixed effects
(fixed effect coefficients not shown). At the base of the table we report the p-value from an
F-test of joint significance of all baseline covariates. Columns 1 and 2 look at attrition in the
endline survey (n=4096), and Columns 3 and 4 selection into the validation sample (n=297).
Each endline survey is a separate observation, and robust standard errors are clustered at
the individual level.
    For the endline survey, attrition was just 8% on average across all endline survey rounds,
and this attrition appears to be unsystematic. Of the 144 men we could not survey, 2 had
died, 4 were incarcerated, 2 refused to answer the survey, and the remainder could simply not
be found despite repeated attempts over several weeks. As a result, there are 315 surveys that
were not completed (of 4,096).36 From Columns 1 and 2, only 4 of the 30 (13%) covariates
have a p-value below .1 and jointly the covariates have a p-value of 0.53.
    For the validation, 3 of the 30 (10%) covariates have a p-value less than .1 and jointly
they have a p-value of .38. Attritors from validation are less educated and less disabled,
and are less likely to have received the cash treatment. There are some covariates that show
a substantive difference (e.g. attritors are 9 percentage points less likely to be hard drug
  36
   Of the men who are missing at least 1 survey (155), 9 died (7% of missing surveys), 19 were in prison
(11% of missing surveys), 6 refused (3%).


                                                        ii
     Table A.2: Randomization balance to treatments and to selection into validation
                                                                   Dependent variable: Assigned to

                                                           Cash               Therapy         Validation sample

      Baseline variable                           Coeff.          SE      Coeff.     SE       Coeff.           SE
                                                    (1)           (2)      (3)       (4)       (5)             (6)

      Age                                          0.001      [.004]      0.000     [.004]    0.001        [.001]
      Married/living with partner                 -0.024      [.050]      0.019     [.049]    -0.033      [.017]**
      # of women supported                         0.009      [.027]     -0.040     [.027]    0.033      [.010]***
      # children under 15                         -0.007      [.006]     -0.001     [.006]    0.000        [.002]
      Muslim                                      -0.014      [.056]      0.074     [.055]    0.016        [.020]
      Years of schooling                           0.002      [.006]     -0.001     [.006]    0.001        [.002]
      Literacy score (0-2)                        -0.008      [.024]      0.020     [.024]    0.004        [.008]
      Math score (0-5)                            -0.009      [.012]     -0.021    [.012]*    -0.003       [.004]
      Health index (0-6)                          -0.006      [.012]      0.020     [.012]    -0.002       [.004]
      Disabled                                    -0.072      [.064]      0.043     [.063]    0.008        [.019]
      Monthly cash earnings (USD)                  0.000     [0000]*      0.000    [0000]     0.000        [0000]
      Durable assets index, z-score                0.035      [.019]*    -0.013     [.019]    -0.002       [.006]
      Savings stock (100s of USD)                 -0.027      [.027]     -0.015     [.026]    -0.008       [.008]
      Able to get a loan of $300                   0.009      [.055]      0.020     [.054]    0.052       [.021]**
      Average weekly work hours in:
       Potentially illicit activities              0.000      [.001]      0.000     [.001]    0.000        [0000]
       Agricultural Labor                         -0.001      [.004]      0.006     [.004]    0.000        [.001]
       Low-skill wage labor                        0.001      [.001]      0.000     [.001]    0.000        [0000]
       Low-skill business                          0.001      [.001]*     0.002    [.001]**   0.000        [0000]
       High-skill work                             0.004     [.002]**     0.002     [.002]    0.000        [.001]
      Ex-combatant                                 0.037      [.038]      0.010     [.038]    0.026       [.012]**
      Currently sleeping on the street            -0.025      [.044]     -0.033     [.043]    -0.008       [.014]
      Times went hungry last week                  0.013      [.012]      0.004     [.012]    -0.001       [.004]
      Sells drugs                                 -0.002      [.050]      0.017     [.049]    -0.012       [.017]
      Drinks alcohol                               0.035      [.039]      0.055     [.038]    -0.025      [.014]*
      Uses marijuana daily                         0.014      [.039]      0.038     [.038]    0.019        [.013]
      Uses hard drugs daily                        0.071      [.052]     -0.117    [.051]**   -0.017       [.017]
      Stole in past two weeks                      0.002      [.038]      0.030     [.038]    -0.009       [.013]

      Joint significance (p-value)                 0.773                  0.319                        0.109
      R-squared                                    0.510                  0.549                        0.136
      Observations                                 999                     999                         2994

Notes: Columns 1 to 4 report the coefficient and standard error on each variable from an OLS regression of
a treatment indicator on all variables listed in the table plus block fixed effects. Columns 5 and 6 do the
same where the dependent variable is selection into validation. For assignment to treatment the sample is
the 999 men in the study. For the validation exercise it is the 4096 endline surveys run at various points
in time, although because some rounds have none selected for validation we exclude these rounds and have
2994 remaining. Each endline survey is a separate observation, and robust standard errors are clustered at
the individual level.
*** p<0.01, ** p<0.05, * p<0.1
                                                    iii
                Table A.3: Attrition from the endline survey and from validation
                                                                        Dependent variable

                                                            Unfound at endline     Unable to validate

              Baseline variable                             Coeff.           SE    Coeff.           SE
                                                              (1)            (2)    (3)             (4)

              Assigned to cash only                         -0.013       [.018]    -0.044         [.061]
              Assigned to therapy only                      -0.008       [.017]    0.060          [.069]
              Assigned to therapy plus cash                 -0.014       [.018]    -0.056         [.065]
              Age                                            0.000       [.001]    0.000          [.005]
              Married/living with partner                   -0.007       [.021]    0.040          [.071]
              # of women supported                          -0.002       [.009]    0.023          [.052]
              # children under 15                           -0.003      [.002]*    -0.010         [.007]
              Muslim                                         0.084     [.029]***   -0.047         [.070]
              Years of schooling                             0.002       [.002]    -0.018      [.009]*
              Literacy score (0-2)                           0.004       [.009]    0.035          [.038]
              Math score (0-5)                               0.002       [.004]    -0.013         [.018]
              Health index (0-6)                            -0.002       [.005]    -0.015         [.018]
              Disabled                                       0.004       [.021]    -0.136      [.077]*
              Monthly cash earnings (USD)                    0.000       [0000]    0.000        [0000]
              Durable assets index, z-score                  0.014      [.008]*    0.044          [.029]
              Savings stock (100s of USD)                    0.011       [.012]    0.008          [.041]
              Able to get a loan of $300                    -0.008       [.022]    -0.049         [.077]
              Average weekly work hours in:
               Potentially illicit activities                0.000       [0000]    0.000          [.001]
               Agricultural Labor                            0.000       [.001]    -0.002         [.002]
               Low-skill wage labor                          0.000       [0000]    -0.002      [.001]*
               Low-skill business                           -0.001      [0000]**   0.000          [.001]
               High-skill work                               0.000       [.001]    0.002          [.003]
              Ex-combatant                                   0.002       [.014]    -0.048         [.057]
              Currently sleeping on the street               0.004       [.017]    0.102          [.069]
              Times went hungry last week                   -0.002       [.005]    -0.024         [.018]
              Sells drugs                                   -0.013       [.016]    0.062          [.076]
              Drinks alcohol                                -0.011       [.015]    -0.036         [.064]
              Uses marijuana daily                          -0.001       [.016]    0.008          [.059]
              Uses hard drugs daily                         -0.017       [.020]    -0.090         [.077]
              Stole in past two weeks                       -0.005       [.015]    0.039          [.057]

              Joint significance (p-value)                           0.532                  0.382
              R-squared                                              0.167                  0.318
              Observations                                           4096                   297

Notes: Columns 1 to 4 report the coefficient and standard error on each covariate from an OLS regression
of an attrition indicator on all covariates listed in the table plus block fixed effects. Each endline survey is
a separate observation, and robust standard errors are clustered at the individual level.
*** p<0.01, ** p<0.05, * p<0.1
                                                      iv
users, and 10.2 percentage points more likely to be sleeping on the street) but these are not
statistically significant.


B        Further details of validation methodology
The goal of this section is to expand on certain activities and aspects of the approach in
order to facilitate replication or adaption of the method.


B.1        Validator roles and responsibilities
The validation team was comprised of three main roles (in order of seniority): the Project
Coordinator, Team Leader, and Validator.
       • The primary responsibilities of the Project Coordinator were to: (i) build an effective
         team, including hiring and training, (ii) identify target respondents, (iii) supervise the
         team leader and validators, and (iv) monitor field progress.

       • The Team Leader was expected to perform all duties, functions, and responsibilities of
         a qualitative researcher in addition to: (i) assisting with trainings, (ii) overseeing field
         logistics, such as assigning validators to respondents, (iii) mentoring validators, as well
         as (iv) monitoring and reporting on the team’s progress.

       • Validators were principally responsible for the following duties: (i) locating respon-
         dents, (ii) collecting and accurately recording data, and (iii) routinely communicating
         progress to the Team Leader.
Validators underwent an intensive 10-day training, involving both classroom learning and
extensive field training, before being selected.37 The aim of the training was to develop
and refine trainees’ skills in acquiring informed consent, building rapport with respondents,
collecting and recording data, and analytical reasoning. Trainings were held for eight hours
each day and, over the course of 10 days, transitioned gradually from exclusive classroom
learning to field trainings with short debriefing sessions. Field trainings provided trainees
with opportunities to practice the skills and techniques they had learned.


B.2        Logistical organization
Consent Process Validators obtained informed consent from each respondent prior to
commencing the first “hangout” session. Validators were provided with a consent script
  37
    We trained more qualitative researchers than were needed for the exercise. Those who exhibited superior
performance during the trainings were selected as validators.

                                                    v
which they recited to respondents by memory to maintain informality. During the consenting
process, study participants were advised of the research team’s interest in learning about
their lives. However, respondents were not informed of the precise data being collected.
This exercise was discussed in the formal consent process to the endline survey, however.

Strategies for assigning respondents Respondents were assigned based on their prox-
imity to the validators’ other assigned respondents or their workload distribution. Respon-
dents whom we had difficulty tracking or getting to answer the survey were often assigned
to one of the more experienced validators.

Coordination and communication The validators aimed to begin “hangout” sessions
the day after subjects completed their quantitative surveys, and to conduct all four visits
within 10 days of respondents having been surveyed. To facilitate coordination and com-
munication, validators were assigned to a survey team. The survey team would alert the
validator when the respondent was being surveyed. A validator would go to the location
where the respondent was being surveyed, the enumerator would introduce the respondent
to the validator if possible, and the validator would set up a time to meet with the respon-
dent the next day. This coordination and communication were essential and typically saved
hours or days of tracking.

Workflow management The Team Leader had several strategies for developing work
plans. For instance:

   • Validators would try when possible to validate two people per day, though this was
     not always possible because of tracking time.

   • Their work hours were flexible, and might include early mornings or evening visits.

   • If a validator was assigned three respondents, he would rotate them, such that all four
     visits with the three respondents were completed over the course of six days.

   • If validators knew a particular respondent was difficult to locate, they would often begin
     their day early trying to find him. If after a few hours they were not able to locate
     him, they would have a “backup” respondent for the morning slot in mind (that is,
     one they knew would be easy to locate). In this way, validators tried to make sure too
     much time would not elapse between hangouts with “difficult-to-locate” respondents.

   • Validators also needed time before and after their “hangout” sessions to locate respon-
     dents and record collected data. Thus, if a validator were assigned three respondents,

                                              vi
       she typically needed eight days to complete the entire exercise (one day for locating
       respondents, six days for visiting with respondents, and one day for transferring her
       field notes into the appropriate template).

Strategies Validators were provided with cell phones (and cell phone credit) to contact
respondents, each other, and the survey team. Petty cash for transportation to and from
hangout sessions was also given to validators on a weekly basis.
    Since this population was largely transient, the use of tracking sheets (that specified the
whereabouts and contact information of respondents, their relatives and friends) proved cru-
cial. This form was used to locate subjects and was continuously updated as new information
became available. Validators also carried a small photograph of their assigned respondent in
order to identify him.38

Recording Data Finally, the Validators and Project Coordinator kept track of the hang-
out sessions completed each day and pertinent information about the validation sample (see
Appendix Figure 1 for the form used). This information was obtained from the Team Leader
during debriefing sessions at the end of each day, notes were added, and these were the basis
of the review process with the authors for case-by-case final coding.

Training activities

The Liberia training manual is available from the authors on request.


B.3     Evidence collection techniques
Below we outline the kinds of evidence gathering techniques used by the validators.

   1. Direct Questioning. The validator directly asked the respondent about his engagement
      in the activity.

         • Stealing: Having established a strong rapport, and observing that the respondent
           did not get along well with others in the community, a validator asked whether
           the respondent ever stole from them. The subject replied that he steals when
           he has the opportunity and explained that he stole two cartons of frozen chicken
           from a warehouse just last week (i.e., during the survey window).
  38
    We obtained respondent consent for tracking photos. This photo was not shown to anyone outside of
the study team and was destroyed after meeting with the respondent.




                                                 vii
     • Phone Charging: After commenting on how nice the respondent’s phone was, the
       validator asked whether the respondent how often he pays to charge it.

2. Indirect Questioning. The respondent brings up a topic or question closely related to
   engagement in the activity.

     • Homelessness: A validator met a participant in front of a house and asked who
       lived there. The respondent replied that this was his friend’s home, though he
       had been living there for the last three months. This information was then cor-
       roborated in casual conversations with neighbors.
     • Video Club: After discussing their enjoyment of football (soccer), a validator
       asked a participant whether he preferred to watch games from home or the video
       club. The subject noted that he loves watching games from the video club and
       had recently watched the Under 20 World Cup at a video club the week prior.

3. Story/Scenario. The validator discusses a personal story or hypothetical scenario in-
   volving another person engaging in the activity. The respondent is invited to comment.

     • Homelessness: To introduce the topic of homelessness, a validator told the story
       of a childhood friend who sleeps on the streets, as opposed to his parents’ home,
       because of the freedom it allows him. The validator then asked for the subjects’
       thoughts on the matter, which segued into a conversation about the respondents’
       own living situation.
     • Video Club: A validator noted that if he lived in the respondents’ community,
       he would take advantage of the close proximity to the video club and frequent it
       to watch football (soccer) games. This scenario led to a conversation about the
       subject going to the video club the previous week (i.e., during the survey window)
       with his friends to watch a game.

4. Witnessing or found evidence. The validator sees the respondent engaging in the
   activity in question or finds physical evidence that the respondent has recently engaged
   in the activity. Note that this was almost solely used as supporting evidence.

     • Marijuana: While a respondent openly smoked marijuana, a validator asked him
       to share his experiences smoking – probing to understand when the participant
       began smoking and how frequently he engaged in the activity. In turn, the valida-
       tor offered stories of his struggles with drinking. The respondent then explained


                                          viii
       that he began smoking in 2002 and had smoked daily for the last several months.
       In another case, a validator noted that the respondent walked behind a building
       and when he returned, his eyes were red, he appeared high, and he smelled heavily
       of marijuana.
     • Phone charging: On the first day of a hangout, a validator walked with a respon-
       dent to a phone charging booth where the respondent needed to pay to charge his
       phone.

5. Third party account. The validator asks the family and friends of the respondent
   whether or not he engages in the activity. Note that this was almost solely used as
   supporting evidence.

     • Homelessness: The respondent took the validator to his home, where the validator
       met his family. The family all confirmed that he lived there.
     • Phone Charging: The respondent told the validator that he did not have a phone.
       The validator asked the respondents’ sister if this was true, and she confirmed
       that the respondent had lost his phone two months prior to the survey.

6. Other or unclear method. This includes unprompted information from the respondent,
   and also inferring from other knowledge such as when the validator learns about the
   respondents life and is able to infer that the respondent did or did not engage in the
   activity.

     • Homelessness: While hanging out with the validator, the respondent announced
       that he wanted to show the validator the home that he had been living in for the
       past year.
     • Video Club: The validator learned from the respondent that he was in jail for
       the two weeks prior to the survey. Because of this, the respondent could not have
       attended the video club.
     • In a few cases, the validator’s notes are unclear about how he prompted the
       respondent to discuss the activity. In these cases, we give the evidence a separate
       code, although these are likely cases of questioning.




                                          ix
Table C.1: Survey under-reporting and over-reporting by evidence type: With and without
any external evidence (direct observation or third party confirmation)

                                              Sensitive behaviors                       Expenditures

                                All   Steal   Marijuana       Gamble   Homeless   All     Video   Phone      All
                                (1)    (2)        (3)          (4)       (5)      (6)      (7)         (8)   (9)

                                                 a. Underreports
 Any external evidence          38     2          11            15        10      11        4          7     49
 No external evidence           71     21         10            31        9       88        64         24    159

 Total count                    109    23         21            46        19      99        68         31    208
 % with any external evidence   35%   9%         52%           33%       53%      11%      6%      23%       24%

                                                 b. Over-reports
 Any external evidence          23     3           1            2         17       1        1          0     24
 No external evidence           63     24         13            19        7       33        22         11    96

 Total count                    86     27         14            21        24      34        23         11    120
 % with any external evidence   27%   11%         7%           10%       71%      3%       4%          0%    20%



C      Other analysis
C.1     Patterns of misreporting
Table C.1 reports the number of cases where the survey and validation measures do not agree,
divided into cases of survey over- and under-reporting relative to the validation measure.


C.2     Robustness
Below we report two alternate versions of Table 6. First, we rerun the regressions without
controlling for block fixed effects and round fixed effects.
   Second, we report the same regressions, controlling for baseline covariates, randomization
block fixed effects, and survey round fixed effects.




                                                          x
     Table C.2: Estimates of treatment effects (θ)
                                                ˆ and treatment effect bias (θ∆ ) by outcome and treatment, no block controls

                                                                      Potentially sensitive behaviors                                      Expenditures

                                                    All (0-4)        Steal      Marijuana       Gamble       Homeless          All (0-2)       Video         Phone
                                                       (1)            (2)           (3)           (4)            (5)              (6)           (7)            (8)

                                                                                  a. Cash only
                                     ˆ
              ATE using survey data (θ)               -0.045        -0.028         0.015         0.027         -0.059            0.077         0.026          0.050
                                                      [.099]        [.030]         [.042]        [.030]        [.032]*           [.052]        [.034]         [.037]

              ATE on measurement error     (θ∆ )      -0.224        -0.050        -0.033         -0.080        -0.066            0.274         0.111          0.168
                                                      [.195]        [.089]         [.066]        [.089]        [.078]           [.138]**       [.115]       [.078]**

              Adjusted ATE (θ̂ − θ∆ )                 0.179         0.022          0.048         0.107         0.007             -0.198        -0.084        -0.118
                                                      [.194]        [.085]         [.073]        [.091]        [.074]            [.141]        [.114]         [.077]

                                                                                b. Therapy only
                                     ˆ
              ATE using survey data (θ)               -0.180        -0.044        -0.021         -0.085        -0.030            -0.001        0.001         -0.003
                                                      [.095]*       [.029]         [.041]      [.026]***       [.032]            [.050]        [.033]         [.035]

              ATE on measurement error     (θ∆ )      0.030         -0.015         0.017         0.043         -0.015            0.333         0.159          0.180
                                                      [.203]        [.089]         [.059]        [.095]        [.090]           [.133]**       [.105]       [.085]**




xi
              Adjusted ATE (θ̂ − θ∆ )                 -0.209        -0.028        -0.037         -0.127        -0.016            -0.334        -0.157        -0.182
                                                      [.202]        [.087]         [.067]        [.093]        [.087]           [.139]**       [.109]       [.083]**

                                                                               c. Therapy + Cash
                                     ˆ
              ATE using survey data (θ)               -0.397        -0.105        -0.068         -0.097        -0.126            0.078         0.013          0.065
                                                    [.093]***      [.027]***      [.041]*      [.027]***      [.030]***          [.051]        [.033]        [.036]*

              ATE on measurement error (θ∆ )          0.083         -0.002        -0.031         0.085         0.030             0.310         0.129          0.185
                                                      [.187]        [.083]         [.064]        [.094]        [.089]          [.116]***       [.103]       [.076]**

              Adjusted ATE (θ̂ −   θ∆ )               -0.480        -0.103        -0.037         -0.182        -0.157            -0.232        -0.116        -0.120
                                                     [.199]**       [.083]         [.071]       [.101]*       [.078]**          [.128]*        [.106]        [.073]
              Observations, survey/validation      3765 / 239     3764 / 238    3762 / 238    3763 / 238     3765 / 239       3763 / 239    3761 / 238     3759 / 239

         Notes: The survey-based ATE estimates, θ̂, pool all survey rounds and regress each outcome on treatment indicators, without controlling for baseline characteristics,
     survey round, or randomization block fixed effects. Standard errors are robust and clustered by individual. Estimates of the bias from treatment, θ∆ , come from a
     regression of the difference in the survey and validation measures on an indicator for treatment arms, without controlling for baseline characteristics, survey round, or
     randomization block fixed effects. Standard errors are robust and clustered by block. The difference, θ̂ − θ∆ , is an estimate of the true treatment effect after adjusting
     for observed bias. It is calculated as the linear difference of the estimates and the standard error is calculated via bootstrapping (we performed 1000 draws from the
     sample, with replacement; we calculated θ̂, θ∆ , and θ̂ − θ∆ for each draw; and we generated the standard error on θ̂ − θ∆ using the distribution from these draws). ***
     p<0.01, ** p<0.05, * p<0.1
                                                 ˆ and treatment effect bias (θ∆ ) by outcome and treatment, block and baseline
      Table C.3: Estimates of treatment effects (θ)
      controls
                                                                          Potentially sensitive behaviors                                      Expenditures

                                                        All (0-4)        Steal      Marijuana       Gamble       Homeless          All (0-2)       Video         Phone
                                                           (1)            (2)           (3)           (4)            (5)              (6)           (7)            (8)

                                                                                      a. Cash only
                                         ˆ
                  ATE using survey data (θ)               -0.080        -0.033        -0.008         0.019         -0.056            0.082         0.021          0.061
                                                          [.071]        [.024]         [.028]        [.026]       [.026]**          [.046]*        [.032]        [.032]*

                  ATE on measurement error (θ∆ )          -0.300        -0.126        -0.091         -0.070        -0.006            0.170         0.035          0.140
                                                          [.181]        [.087]         [.062]        [.081]        [.089]            [.137]        [.103]        [.084]*

                  Adjusted ATE (θ̂ − θ∆ )                 0.220         0.093          0.083         0.088         -0.050            -0.088        -0.013        -0.079
                                                          [.279]        [.125]         [.102]        [.134]        [.123]            [.207]        [.155]         [.122]

                                                                                    b. Therapy only
                                         ˆ
                  ATE using survey data (θ)               -0.195        -0.040        -0.047         -0.086        -0.023            -0.009        -0.005        -0.004
                                                        [.071]***       [.024]*        [.029]      [.024]***       [.026]            [.046]        [.032]         [.031]

                  ATE on measurement error (θ∆ )          -0.213        -0.067        -0.055         -0.014        -0.065            0.273         0.117          0.167




xii
                                                          [.181]        [.085]         [.064]        [.090]        [.091]           [.145]*        [.108]       [.083]**
                  Adjusted ATE (θ̂ − θ∆ )                 0.019         0.028          0.008         -0.071        0.042             -0.281        -0.122        -0.171
                                                          [.271]        [.125]         [.103]        [.136]        [.125]            [.202]        [.162]         [.120]

                                                                                   c. Therapy + Cash
                                         ˆ
                  ATE using survey data (θ)               -0.405        -0.105        -0.089         -0.098        -0.114            0.073         0.004          0.069
                                                        [.069]***      [.023]***     [.028]***     [.024]***      [.026]***          [.045]        [.032]       [.031]**

                  ATE on measurement error (θ∆ )          0.054         -0.034        -0.041         0.056         0.095             0.273         0.070          0.219
                                                          [.183]        [.083]         [.061]        [.097]        [.084]           [.123]**       [.103]       [.074]***

                  Adjusted ATE (θ̂ − θ∆ )                 -0.459        -0.071        -0.048         -0.153        -0.208            -0.200        -0.066        -0.150
                                                          [.269]*       [.110]         [.097]        [.143]        [.117]*           [.200]        [.156]         [.111]
                  Observations, survey/validation      3765 / 239     3764 / 238    3762 / 238    3763 / 238     3765 / 239       3763 / 239    3761 / 238     3759 / 239

             Notes: The survey-based ATE estimates, θ̂, pool all survey rounds and regress each outcome on treatment indicators and block fixed effects. Standard errors are
         robust and clustered by individual. Estimates of the bias from treatment, θ∆ , come from a regression of the difference in the survey and validation measures on an
         indicator for treatment arms, controlling for block fixed effects and each endline round. Standard errors are robust and clustered by block. The difference, θ̂ − θ∆ , is
         an estimate of the true treatment effect after adjusting for observed bias. It is calculated as the linear difference of the estimates and the standard error is calculated
         via bootstrapping (we performed 1000 draws from the sample, with replacement; we calculated θ̂, θ∆ , and θ̂ − θ∆ for each draw; and we generated the standard error
         on θ̂ − θ∆ using the distribution from these draws). *** p<0.01, ** p<0.05, * p<0.1
