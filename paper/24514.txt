                               NBER WORKING PAPER SERIES




      USING MASSIVE ONLINE CHOICE EXPERIMENTS TO MEASURE CHANGES
                             IN WELL-BEING

                                        Erik Brynjolfsson
                                          Felix Eggers
                                      Avinash Gannamaneni

                                       Working Paper 24514
                               http://www.nber.org/papers/w24514


                     NATIONAL BUREAU OF ECONOMIC RESEARCH
                              1050 Massachusetts Avenue
                                Cambridge, MA 02138
                                     April 2018




We thank Susanto Basu, Carol Corrado, Erwin Diewert, Kevin Fox, Robert Hall, John Hauser,
Andrea Meyer, Dana Meyer, Leonard Nakamura, Jeff Prince, Adam Saunders, Christopher
Stanton and participants of the NBER Summer workshop for the Conference on Research on
Income and Wealth (2016), NBER Winter Digitization meeting (2017), NBER Productivity lunch
(2017) and the IMF Statistical Forum (2017) for helpful comments and the MIT Initiative on the
Digital Economy, via a grant from the Markle Foundation, for generous funding. The views
expressed herein are those of the authors and do not necessarily reflect the views of the National
Bureau of Economic Research.

At least one co-author has disclosed a financial relationship of potential relevance for this
research. Further information is available online at http://www.nber.org/papers/w24514.ack

NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.

© 2018 by Erik Brynjolfsson, Felix Eggers, and Avinash Gannamaneni. All rights reserved. Short
sections of text, not to exceed two paragraphs, may be quoted without explicit permission
provided that full credit, including © notice, is given to the source.
Using Massive Online Choice Experiments to Measure Changes in Well-being
Erik Brynjolfsson, Felix Eggers, and Avinash Gannamaneni
NBER Working Paper No. 24514
April 2018
JEL No. E01,E16,O0,O4

                                         ABSTRACT

GDP and derived metrics (e.g., productivity) have been central to understanding economic
progress and well-being. In principle, the change in consumer surplus (compensating
expenditure) provides a superior, and more direct, measure of the change in well-being,
especially for digital goods, but in practice, it has been difficult to measure. We explore the
potential of massive online choice experiments to measure consumers’ willingness to accept
compensation for losing access to various digital goods and thereby estimate the consumer
surplus generated from these goods. We test the robustness of the approach and benchmark it
against established methods, including incentive compatible choice experiments that require
participants to give up Facebook for a certain period in exchange for compensation. The proposed
choice experiments show convergent validity and are massively scalable. Our results indicate that
digital goods have created large gains in well-being that are missed by conventional measures of
GDP and productivity. By periodically querying a large, representative sample of goods and
services, including those which are not priced in existing markets, changes in consumer surplus
and other new measures of well-being derived from these online choice experiments have the
potential for providing cost-effective supplements to existing national income and product
accounts.

Erik Brynjolfsson                               Avinash Gannamaneni
MIT Sloan School of Management                  MIT Sloan School of Management
100 Main Street, E62-414                        100 Main Street
Cambridge, MA 02142                             Cambridge, MA 02142
and NBER                                        avinashg@mit.edu
erikb@mit.edu

Felix Eggers
University of Groningen
Nettelbosje 2
9747 AE Groningen
Netherlands
f.eggers@rug.nl
“If you don't know where you're going, you might not get there.” -- Yogi Berra



1. Introduction

         Digital technologies have transformed both the nature of production and the types of

goods and services consumed in modern economies. Yet, our measurement framework for

economic growth and well-being has not fundamentally changed since the 1930s. In principle, a

more comprehensive approach is now feasible. By using massive online choice experiments to

estimate changes in consumer surplus (compensating variation) we can supplement the

traditional metrics based on Gross Domestic Product (GDP).

         GDP measures the monetary value of the purchases of all final goods by households,

businesses and government. It is the most widely used measure of economic activity and heavily

influences policymakers in setting economic objectives and enacting interventions. GDP has

been heralded as one of the greatest inventions of the 20th century by Paul Samuelson and

William Nordhaus (Landefeld 2000). Economists and journalists routinely use GDP as if it were

a welfare measure. Media articles regularly mention that the “economy grew by x%”1 by

measuring the growth in GDP and use this figure as a casual metric for the improvement in

economic well-being. Similarly, economists widely use GDP per hour worked as a measure of

productivity and infer links between productivity and improvement in living standards (OECD

2008).

         Nonetheless, many economists consider GDP to be a significantly flawed measure of

well-being, and several attempts have been made to design alternative measures (Stiglitz et al.


1
 E.g., “U.S. Economy Grew 1.4% in Fourth Quarter” (http://www.bloomberg.com/news/articles/2016-03-25/u-s-
economy-grew-1-4-in-fourth-quarter-supported-by-consumers), “China’s Economy Grew by 6.7% in First Quarter
of 2016” (http://blogs.wsj.com/chinarealtime/2016/04/15/chinas-economy-grew-by-6-7-in-first-quarter-of-2016/)

                                                                                                                2
2009). In fact, Simon Kuznets, the founding father of the system of national accounts that

include GDP, explicitly warned against using it this way, writing, “The welfare of a nation can

scarcely be inferred from a measurement of national income as defined [by the GDP.]” (Kuznets

1934).2 Despite Kuznets’ warning, growth in GDP is still the most widely used indicator of

progress in our economic well-being.

         For goods with a non-zero price, in theory it is often possible to infer welfare from

national accounts including GDP measures (Hulten 1978; Jorgenson and Slesnick 2014),

although in practice official estimates of welfare are not published. Research has looked at

factors such as introduction of new goods, intangibles, quality adjustments and household

production, in which GDP is biased away from welfare, and ways to correct these biases have

been proposed. Feldstein (2017) provides an excellent survey and concludes that official measure

significantly underestimate the true growth of GDP, personal income and productivity.

         Using GDP as a welfare measure is especially problematic when prices are zero. This is

the case in the emerging digital economy because most digital goods have nearly zero marginal

cost and often a zero price. This makes it difficult to discern their contributions to welfare by

looking at GDP calculations (Brynjolfsson and Saunders 2009; Brynjolfsson and McAfee 2014).

For instance, although information goods have unquestionably become increasingly ubiquitous

and important in our daily lives, the share of the information sector as a fraction of the total GDP

(~ 4-5%) has not changed in the last 35 years (Figure 1). Moreover, in many sectors (e.g., music,

media, encyclopedias) people substitute zero-price online services (e.g., Spotify, YouTube,

Wikipedia) for goods with a positive price. As a result, the total revenue contributions of these

sectors to GDP figures can fall even while consumers get access to better quality and more

2
 He underscored his views when accepting his Nobel Prize in 1971, saying that the conventional measures of
national product (including GDP) omit various costs (e.g., pollution) and benefits (e.g., more leisure time) associated
with technological innovations and predicted major changes in the way we measure the economy (Kuznets 1973).

                                                                                                                     3
variety of digital goods (Brynjolfsson and Saunders 2009). In other words, not only the

magnitude, but even the sign of the change in well-being may be incorrectly inferred if decision

makers rely solely on existing measures of GDP and productivity as a proxy for well-being.

                                       [Insert Figure 1 here]

       The societal benefits of technological advances are distinct from the expenditures on

goods and services or profits to innovators. Nordhaus (2005) estimated that between 1948 and

2001 corporations were able to retain only 3.7% of the social returns from their technological

advances while the remaining 96.3% of social returns went to consumers. Consumer surplus thus

reflects most of the returns to improvements in technology.

       Historically, the change in consumer surplus hasn’t been widely used as a measure of

change in well-being not because it is a poor measure of well-being, but because it is difficult to

measure at scale. Estimating demand curves using traditional market data requires exogenous

variations that shift the supply curve but not the demand curve, and it has not been practical to

identify these variations for large bundles of goods.

       However, with advances in digital technologies, it is now feasible to collect data about

thousands of goods easily. Private sector organizations such as Microsoft, Amazon, Google and

Facebook routinely conduct millions of online experiments to better understand consumer

preferences and behavior. This scale of experimentation and inference would have been

infeasible 20 years ago but is now routine at many organizations.

       In this research, we propose a way of measuring changes in consumer surplus, not only

for goods and services in the digital economy but also more broadly. Specifically, we implement

a series of discrete choice experiments that measure consumers’ willingness to accept payments

in exchange for losing access to various goods. These experiments allow us to estimate the



                                                                                                      4
demand curves for these goods using data from thousands of consumers that are representative of

the US population. We conclude that our approach is easily scalable and can be used to develop a

system that tracks changes in consumer surplus of numerous goods and services in (near) real

time via massive online choice experiments.

          The paper proceeds as follows. In section 2, we illustrate the ways that GDP and

consumer surplus change when prices change or new products are introduced, and the

implications for welfare estimates. Section 3 describes the key methodologies we use to

empirically assess consumer surplus. Section 4 presents results and sensitivity analyses of the

proposed method. Section 5 applies the method to a broader set of goods. Section 6 concludes

with a summary and discussion.



2. Background


2.1 GDP, consumer surplus and well-being

          Perhaps no one has described the shortcomings of GDP3 as a welfare measure as

eloquently as Robert F. Kennedy:

          Gross National Product counts air pollution and cigarette advertising, and ambulances

          to clear our highways of carnage. It counts special locks for our doors and the jails for

          the people who break them. It counts napalm and counts nuclear warheads and armored

          cars for the police to fight the riots in our cities...

          Yet the gross national product does not allow for the health of our children, the quality of

          their education or the joy of their play. It does not include the beauty of our poetry or the

3
    Kennedy was technically discussing GNP, but his comments are equally applicable to GDP.

                                                                                                      5
        strength of our marriages, the intelligence of our public debate or the integrity of our

        public officials.

        It measures neither our wit nor our courage, neither our wisdom nor our learning,

        neither our compassion nor our devotion to our country, it measures everything in short,

        except that which makes life worthwhile.4

        Kennedy’s poetic words contribute much to our understanding (if not to our GDP!).

Subsequently, there have been a number of efforts to create a more comprehensive estimate of

well-being. Since 2012, the United Nations Sustainable Development Solutions Network

published an annual World Happiness Report, ranking countries based on measures of happiness

(Helliwell et al. 2017). Jones and Klenow (2016) propose a measure that incorporates

consumption, leisure, mortality and inequality to measure the economic well-being of a country.

There is a growing stream of literature focusing on measuring subjective well-being and life

satisfaction. However, while progress has been made (Krueger and Stone 2014), a survey of

leading macroeconomists indicates that we are a long way off from reaching consensus on how

to measure well-being so that they are reliable for policymaking (den Haan et al. 2017).

        In this paper, we seek to stick more closely to a traditional microeconomic framework. In

particular, we focus on the changes in consumer surplus generated by changes in consumption of

digital goods and discuss ways in which our approach can be expanded to more goods and

services. Brynjolfsson and Saunders (2009) paraphrase Robert Solow in noting that the influence

of the information age is seen everywhere except in the GDP statistics. Almost all of us use more

and more digital goods such as search engines, smartphones, social networking sites, and e-

commerce platforms, but their revenues don’t always reflect this increased use.

4
 Robert Kennedy speaking at University of Kansas in 1968 (Ref: http://www.jfklibrary.org/Research/Research-
Aids/Ready-Reference/RFK-Speeches/Remarks-of-Robert-F-Kennedy-at-the-University-of-Kansas-March-18-
1968.aspx).

                                                                                                              6
       One of the hypothesized explanations for productivity slowdown in the US since the past

decade is that existing economic indicators (including GDP) do not properly measure the

contributions of the latest wave to technological innovation, particularly digital goods and

services (Brynjolfsson, Rock and Syverson 2017). Whereas average annual labor productivity

growth was 2.8% per year over 1995-2004, it shrunk to 1.3% per year over 2005-2015. An

optimistic interpretation is that recent productivity gains due to innovations in IT-related goods

and services are not properly reflected in the current productivity measures (e.g., Brynjolfsson

and McAfee 2014; Aeppel 2015; Hatzius 2015). However, recent literature (Byrne et al. 2016;

Syverson 2016) has emphasized that while productivity mismeasurement may have been

substantive in recent years, it was also likely substantive in the past, so its power to explain the

productivity slowdown is limited.

       Although motivated in part by this puzzle, our research focuses on the more fundamental

issue that GDP, and thus productivity, is not a direct measure of well-being in the first place.

Thus, whether or not GDP or productivity mismeasurement has grown is a distinct, albeit related,

question from how well-being is changing. The gap between production (as measured by GDP)

and well-being has been an issue since GDP was invented and, as we illustrate below, it is

arguably an even bigger issue in the current digital era.

       Consider the case of the music industry. Consumers shifted from buying physical units

such as CDs, cassettes and vinyl records to downloading or streaming songs digitally through

platforms such as iTunes, Pandora and Spotify. Digital goods have near-zero marginal cost and

are hence priced much lower (often even at zero) than physical goods. Between 2004 and 2008,

consumers listened to more music (units of music purchased increased from under 1 billion to

over 1.5 billion without counting illegal downloads), but the recording industry’s revenues



                                                                                                       7
declined by 40% (Brynjolfsson and Saunders 2009), and this trend has continued. Moreover,

Waldfogel (2012) provides compelling evidence that the quality of music has likely increased

since 1999. Therefore, although the financial contribution of music industry to GDP statistics has

decreased, consumer well-being has presumably increased; consumers are listening to more and

better music.

           The relationships among GDP, consumer surplus and well-being can be understood by

looking at three illustrative cases. First, consider a case that broadly describes many classic

physical goods such as cars: consumer surplus is more or less proportional to firm revenue

(Figure 2). Keeping the supply curve fixed, as more consumers enter the market, the size of the

market increases, and the demand curve simply angles further to the right. In this case, both

consumer surplus and quantity sold increase approximately proportionately.5 The increased

quantity sold shows up in GDP statistics as sales revenues increase, and hence both GDP and

consumer welfare move in the same direction. At a given price, doubling the number of cars sold

is likely to roughly double the revenues and contribution to GDP and consumer surplus. A

similar logic applies to many services like haircuts, meals served or windows washed.

                                                 [Insert Figure 2 here]

           A second case describes many purely digital goods such as email, messaging apps,

Facebook and Google search, which have essentially zero marginal cost and are typically offered

to the consumers for free. Although some digital goods may earn revenues from advertising, this

is an intermediate good and does not contribute to GDP. As the value of these free goods

increases, consumer surplus will also increase, but this change in well-being does not necessarily




5
    In the special case of horizontal supply curve and thus constant price, the effect is exactly proportional.

                                                                                                                  8
accrue to GDP (Figure 3). GDP may be completely unchanged due to this shift, even though

consumers are better off.

                                      [Insert Figure 3 here]

       A third case illustrates the transitional situation faced by a number of sectors, in which

physical goods and services are being substituted with digital goods and services. An apropos

example of such a transition good is an encyclopedia. Since the 2000s, people have increasingly

flocked to Wikipedia to get information about a wide variety of topics updated in real time by

volunteers. In 2012, Encyclopedia Britannica, which had been one of the most popular

encyclopedias, ceased printing books after 244 years (Pepitone 2012). Wikipedia has over 60

times as many articles as Britannica had, and its accuracy has been found to be on a par with

Britannica (Giles 2005). Far more people use Wikipedia than ever used Britannica – demand and

well-being have presumably increased substantially. But while the revenues from Britannica

sales were counted in GDP statistics, Wikipedia has virtually no revenues and therefore doesn’t

contribute anything to GDP other than a few minimal costs for running servers and related

activities and some voluntary contributions to cover these costs. Likewise, the transition from

chemical to digital photography followed a similar arc. What’s more, many people now have

digital maps, streaming music, online newspapers, and other services available for no extra cost

once they are able to access the Internet on mobile devices or home computers. For such

transition goods, consumer surplus increases as free access spurs demand, but revenue decreases

as prices become zero (Figure 4). Hence GDP and consumer welfare actually move in opposite

directions.

                                      [Insert Figure 4 here]




                                                                                                    9
          Figures 3 and 4 suggest that changes in consumer surplus are an important supplement to

GDP as a measure of well-being for the current digital economy for either transition goods or

purely digital goods. This is likely to become increasingly relevant as more and more goods

transition from physical to digital in a variety of areas, including financial advising, customer

service and law.

          Total surplus can be thought of as the sum of consumer surplus and producer surplus.6

While producer surplus cannot be inferred from consumer surplus, when it comes to

technological advances, firms have typically been able to appropriate only a small fraction of the

social returns (Nordhaus 2005). Accordingly, we can focus on consumer surplus. If the share of

producer surplus contribution to the total social surplus remains relatively stable, then our results

would have to be scaled up only slightly if one wanted to estimate total surplus. However,

Furman and Orszag (2015) provide evidence that the top performing companies have been

earning increasingly larger returns to capital. Therefore, measuring simply changes in consumer

surplus might underestimate changes in total surplus more significantly if the producer surplus

grows relative to the consumer surplus.



2.2 Prior work measuring consumer surplus from digital goods

          Recently, there has been growing interest from researchers to estimate the changes in

consumer surplus from digital goods. For instance, Greenstein and McDevitt (2011) estimate the

additional consumer surplus created by broadband internet when consumers switched from dial-

up to broadband. They estimate it to be between $4.8 and $6.7 billion per year in the US from

1999-2006. For 2015, this figure is estimated to be $55 billion (Syverson 2016). Although this


6
    More generally, there may also be externalities affecting neither consumers nor producers.

                                                                                                    10
approach captures the welfare gains due to better internet access, it does not capture the

increasing value of the digital information goods available online.

       Another stream of literature has tried to measure the value of digital information goods

by measuring the time spent using them. The underlying assumption behind these papers is that

the value of free digital goods can be inferred from the time consumers spend on them. Using

this approach, Goolsbee and Klenow (2006) estimate the effect of consumer gains from the

internet for the median US resident to be $3000 per year as of the year 2005. Brynjolfsson and

Oh (2012) extend this method to include substitutability between online and offline goods (e.g.,

TV). After accounting for this, they estimate the average annual change in consumer surplus of

the internet to be about $25 billion per year between 2007 and 2011.

       Nakamura and Soloveichik (2015) estimate the value of free media by computing the

online advertising revenues generated by websites. Including ad revenues from free media in

GDP increases real GDP growth by 0.019% according to their estimates, reflecting in part

adjustments to price deflators. However, advertising is an intermediate good so advertising

revenues do not contribute directly to GDP and do not track the value for consumers. More

generally, advertising revenues are not proportional to consumer surplus. For example, in 2011

Google earned around $36 billion ad revenue (Miller 2012) while Varian (2011) estimated the

consumer surplus of Google to be between $65-$150 billion. Spence and Owen (1977) argue that

advertisers pay for numbers of views regardless of whether these views created low or high value

for a consumer. For example, advertising revenues can be high for a program of broad interest

(more views) but welfare need not be very high because consumers might only be marginally

interested in that program. Conversely, for a niche program that is valued very highly by a small

group of consumers, welfare will be high but advertising revenues will be low.



                                                                                                  11
       While these estimates of consumer surplus are based on available market data, our

method uses choice experiments to elicit consumers’ own valuation of goods. Specifically, we

ask consumers to make a choice between keeping a digital good or taking a monetary equivalent

compensation when foregoing it. This approach measures willingness-to-accept rather than

willingness-to-pay money and experimentally varies the offered monetary values. It therefore

addresses the limitation of market data in which the price of many digital goods is zero so that

the market price in conjunction with demand does not reflect their consumer surplus value.

Moreover, an experimental setting may be better able to isolate consumers’ valuation of goods

compared to market data that is typically confounded by many other variables; although,

depending on the design of the experiment, it may come at the expense of being “hypothetical,”

i.e., inconsequential (Carson and Groves 2007) and therefore either noisy or biased, as we

discuss and address below.



3. Methodology


3.1 Approaches to measuring consumer surplus

       There are two general approaches for obtaining input data to measure consumer surplus:

1) market data and 2) choice experiments or survey techniques.

        Approaches based on market data analyze longitudinal or cross-sectional variation in

observed market prices for a good to derive demand curves and price elasticity (e.g., Cohen et al.

2016; Greenwood and Kopecky 2013). Similarly, hedonic pricing models try to decompose the

overall value of a good into the value contribution of its characteristics by applying regression-

type models to the cross-sectional covariation between observed market prices and

                                                                                                     12
characteristics of the goods (Williams 2008). However, both of these approaches require

variance in the observed market prices and are therefore not directly applicable to goods that are

provided for free. Alternatively, revealed preferences can be inferred if there is a proxy for

market price, e.g., time spent using the digital goods (Goolsbee and Klenow 2006; Brynjolfsson

and Oh 2012).

       Choice experiments and survey techniques provide more flexibility because they do not

require non-zero market prices or transactions to exist and they can be applied to contingent

scenarios (leading to contingent valuation studies) (Bishop et al., 2017). One approach to

determining stated preferences is to ask consumers directly about their maximum willingness-to-

pay (WTP) in monetary terms. This question reveals a (potentially ratio-scaled) measure of a

consumer’s value of the good. However, this type of question can be less reliable because

consumers are not used to formulating their own prices and because they may feel an incentive to

hide their true preferences (Miller et al. 2011; Carson and Groves 2007).

       The introduction of non-hypothetical, incentive compatible variants to elicit WTP in the

form of auctions (e.g., Vickrey auctions, Vickrey 1961) or lotteries (e.g., BDM, Becker,

DeGroot, and Marschak 1964; Wertenbroch and Skiera 2002) mitigates some of these

disadvantages, but at the expense of being more complex and by introducing (artificial)

competitive pressure in auctions (Carson, Groves, and List 2014; Völckner 2006). These

incentive compatible direct question formats may thus be ill-suited to either digital goods, in

which supply is not restricted, or to large-scale online choice experiments in which consumers

need to easily comprehend and answer a preference-related question.

       An alternative, indirect form of measuring preferences are discrete choice experiments

(DCE) (Louviere, Hensher, and Swait 2000). DCEs ask consumers to choose between specific



                                                                                                  13
options and select the alternative that they value most. By experimental variation of the

characteristics of the presented options (including prices) and applying logit or probit estimation

models, it is then possible to estimate consumers’ utility function for the characteristics, i.e., their

valuation of features and sensitivity to price changes. DCEs have become a common synonym

for choice-based conjoint experiments that typically involve about eight to 12 sequential choice

tasks that present multiple alternatives, e.g., two to five alternatives, with variations on multiple

attributes (Rao 2014). These DCEs have a long tradition in, among others, marketing (e.g., value

of product features), transportation (e.g., valuation of travel time savings), contingent valuation

(Carson et al. 2003), and are also applied to economic valuation contexts (e.g., Rosston, Savage,

Waldman 2011). They are widely relied upon in the legal proceedings to estimate values of

goods for the purposes of damages calculations (e.g., in the 2011-2014 Apple-Samsung lawsuit;

see also McFadden 2014).



3.2 Proposed approach

        We propose to measure consumer surplus of digital goods with DCEs. Instead of a

conjoint-type experiment, we suggest a simpler implementation in which we only ask consumers

to make a single choice among two options: Whether to keep access to a certain good or to

forego the good in return for a specific amount of money. We only ask one question per

consumer and vary the price points systematically between consumers. The procedure can

therefore be termed single binary discrete choice (SBDC) experiment (Carson and Groves 2007;

Carson, Groves, and List 2014). We deliberately elicit only limited information from each

consumer, i.e., data that is nominal-scaled, with the benefit that this information can be captured

faster and more reliably. Consumers only have to decide between two options instead of


                                                                                                     14
formulating and inputting a monetary figure themselves. Moreover, we can compensate for the

lack in information at the individual level by using large-scale choice experiments and

aggregating the responses from the overall sample in order to derive ratio-scaled demand data.

Thus we use large (thousands of respondents), and potentially massive (hundreds of thousands or

millions of respondents), sample sizes to overcome some of the limitations of earlier research

relying on smaller samples. In some of the experiments, we enforce the consumers’ choices, for

instance be requiring them to give up Facebook for a given period before they get any payment.

This makes their choices incentive-compatible: the rational thing to do is tell the truth when

comparing alternatives options or being asked about valuations.



3.3 Utility theory and choice model

       DCEs in general, including SBDC questions, are compatible with economic theory and

can be used to estimate neoclassical Hicksian welfare measures (McFadden 1974, Carson and

Czajkowski 2014). We will use utility theory and the random utility model to conceptualize the

surplus that individual consumers obtain from consuming digital goods and the monetary value

that they attach to them.

       Specifically, we represent the utility that a consumer experiences from consuming a

digital good g by U(g). In our SBDC questions, utility is only affected by a change in the

availability of the good with consumption quantities restricted to 1 and 0, i.e., a consumer can

either use a good within a defined time period (g1) or not (g0). We abstract away from the

intensity or duration of usage in this conceptual model but can account for it in our empirical

application. We assume a constant market price of zero for the goods, which therefore does not

have to be added to the utility function. We also do not need to explicitly consider the influence


                                                                                                   15
of other attributes, such as negative utility effects of advertising or limited privacy, because they

are nested within g1. These components can be easily added to the utility function when they are

subject to experimental variation. We further assume that U(g1) ≥ U(g0), i.e., that consumers

derive a non-negative utility of consuming the good (and would otherwise not use it). A measure

of monetary value can then be estimated by introducing two Hicksian measures, either the

compensating measure, C, or the equivalent measure, E, that have an effect on the consumer’s

income y (Carson and Czajkowski 2014), such that:

   (3) U(g1, y – C*) = U(g0, y), or

   (4) U(g1, y) = U(g0, y + E*),

   with C > 0 and E > 0.

C* is typically referred to as willingness-to-pay (WTP) for getting access to the good, while E*

can be seen as willingness-to-accept (WTA) to forego it.

       While, theoretically, C* should have the same magnitude as E*, empirical studies show

that typically E* > C*, e.g., due to an endowment effect (Hanemann 1991; Kahneman, Knetsch,

and Thaler 1990; Kahneman, Knetsch, and Thaler 1991). It therefore becomes relevant to define

the status quo of the valuation approach. When valuing the availability of free digital goods, it

seems reasonable to focus on WTA and assume that U(g1, y) is the status quo because using the

good requires no upfront investment (y – C) from consumers.

       When observing in the SBDC experiment that a consumer chooses to forego using a good

for an offered amount E instead of keeping it, then we can assume that U(g0, y + E) > U(g1, y), or

U(g0, y + E) – U(g1, y) > 0. Therefore, only differential effects need to be considered between

the choice options so that the overall income can be excluded and only the marginal effect of E

needs to be considered. Without loss of generality, we can define the status quo utility as U(g1) =



                                                                                                    16
0. Consequently, a consumer will forego the good for amount E if U(g0, E) is positive, and will

not if it is negative.

        In order to estimate the equivalent measure E*, we need estimates of how valuable

consumers find using the good and how sensitive their choices are to differences in E. The

random utility model is the standard framework to estimate the underlying utilities. It assumes

that utility U consists of a systematic component V and a random component e that is inherent to

consumer choice behavior and/or unobservable to the researcher (Manski 1977; Thurstone 1927),

such that U(g0, E) = V(g0, E) + e. Typically, it is assumed that the systematic utility consists of

part-worth utilities for each of the goods components, i.e., V = b0 g0 + b1 E. The framework then

allows us to express the observed choices as probabilities P within a binary logit model, i.e., the

probability that a consumer chooses to forego the service (or, on an aggregate level, the share of

consumers who are willing to accept E) is:

    (5) P(g0, E) = exp(b0 g0 + b1 E) / (1 + exp(b0 g0 + b1 E))

or 1 - P(g0, E), for keeping the service. The parameters can be estimated using closed-form

maximum likelihood procedures. The median equivalent measure E* is then the price that makes

consumers indifferent between the two options so that P(g0, E*) = 0.5 or b0 g0 + b1 E = 0, which

leads to E* = - b0 g0 / b1.

        Here, we represent the utility function as linear in terms of monetary amounts. We will

relax this assumption in the empirical application to handle non-linear terms and include further

demographic variables to account for consumer differences.




                                                                                                  17
4. Consumer Surplus of Facebook

         We use Facebook as a useful case in order to measure the consumer surplus with SBDC

choice experiments. We benchmark the approach against a BDM lottery and explore its

robustness in sensitivity analyses. In section 5, we apply the proposed SBDC approach to a

broader list of goods and present an additional benchmarking study using best-worst scaling.



4.1 Incentive-compatible Single Binary Discrete Choice experiment

         In order to avoid any bias that may affect consumer choices when the options are purely

hypothetical choices, we applied the SBDC experiment in a non-hypothetical, incentive

compatible procedure to measure the consumer surplus of Facebook. We asked consumers if

they would prefer to 1) keep access to the Facebook or 2) give up Facebook for one month7 in

return for a payment of $E. We varied $E across twelve8 price points (E = 1, 10, 20, 30, 40, 50,

60, 70, 80, 90, 100, 1000). To make the SBDC question consequential for the consumer, we

informed them that we will randomly pick one consumer out of every 200 respondents9 and

fulfill that person’s selection. Specifically, we told respondents that if they choose “Keep access

to Facebook” nothing will change for them, however, they will also not receive any money. If

they choose “Give up Facebook and get paid $E,” we promised them the money in cash provided

that they do not access Facebook for one month. We further informed them of our procedure for


7
  We initially restrict the time frame to one month in order to keep the incentive compatibility procedure
manageable. We address the sensitivity of the valuation depending on the time frame in the sensitivity analysis.
8
  In a follow-up study, we included additional price points, i.e., $0.01, $5, $200, $500 and found consistent results.
9
  Carson, Groves, and List (2014) show that stochastically binding procedures (here: one out of every 200
respondents) do not significantly affect the results compared to deterministically binding procedures. We can
confirm this result for our Facebook study, in which we also tested a condition in which one out of every 50
respondents was selected (E was kept at $50 in this condition). We did not find significant differences in the choice
behavior when varying the chances to win (p = 0.236).

                                                                                                                    18
remotely monitoring their Facebook online status and the requirement to provide their email

address (see Figure A.1 in the appendix for the exact question wording and monitoring process).

        We recruited consumers for this study from a professional panel provider with 2.9 million

active panelists and member of several survey research organizations, including CASRO,

ESOMAR, and MRA (Peanut Labs 2015). We invited respondents in June/July 2016 and 2017 to

be able to measure annual changes. We targeted consumers who were 18 years or older and lived

in the US. We further asked consumers to select all online services they had used in the last

twelve months from a list of 14 options, including a non-existent online service. Consumers had

to select Facebook in order to qualify for the survey; if they (also) selected the nonexistent

service that we included in the survey, they were disqualified. We set quotas for gender, age, and

US regions to match US census data (File and Ryan 2014) and applied post-stratification for

education and household income.

        Consumers who accessed the survey were randomly allocated to one of the tested price

points. We sampled the highest and lowest price points twice as often in order to obtain more

reliable estimates for the endpoints of the demand function. We received 2885 complete

responses (n2016 = 1497, n2017 = 1388).

        Figure 5 plots the estimated WTA demand curves, separated for 2016 and 2017.10

                                            [Insert Figure 5 here]

        In order to measure the WTA and quantify the annual change, we estimated a binary logit

model that accounts for the magnitude of E (here, log(E) provided a better fit to the data), year

(dummy variable), and whether the samples in the different years differ in sensitivity towards E.

Table 1 shows the estimation results. The intercept represents the share of consumers in 2016

10
   In order to be consistent to normal practice for representing demand curves, the plots show the shares of
consumers who prefer to keep using Facebook instead of being willing to accept the money. That is, we plotted the
data in a way that makes it easier to see the negative effect of price.

                                                                                                                19
who prefer to keep Facebook at E = $1 (i.e., log(1) = 0). This share is estimated to be exp(1.2)/(1

+ exp(1.2)) = 76.9%. This share is slightly larger in 2017 (p = 0.166) with exp(1.2 + 0.29)/(1 +

exp(1.2 + 0.29)) = 81.6% but the difference is not statistically significant. In 2016, the sample’s

utility decreased by -0.309 with every one-unit increase in log(E), implying a median WTA2016 =

$48.49 per month. In other words, 50% of the Facebook users in our sample would give up all

access to Facebook for one month if we paid them about $50 or more. The Facebook users in

2017 appear to be more sensitive to differences in E (p = 0.049). A one-unit increase in log(E)

results in a utility decrease of -0.309 - 0.101 = -0.410. As a consequence, consumers in 2017

were willing to accept a lower amount to give up Facebook, i.e., median WTA2017 = $37.76 per

month. Since the sample consists solely of Facebook users, a surplus measure also needs to

consider the overall number of consumers who use Facebook. However, the share of Facebook

users in the US increased from 2016 to 2017 by just 2.6%11, which does not offset the negative

tendency in median WTA.

           We used bootstrapping to calculate 95% confidence intervals for the median WTA

values, i.e., CI2016 = [$32.04, $72.24], CI2017 = [$27.19, $51.97]. The range of the confidence

intervals illustrates the limitation of the approach in being less precise, given the current sample

size. Although the median WTA values suggest a substantial drop in value, the confidence

intervals are very broad, so we can’t reasonably rule out that this is simply due to chance. We

address the effect on precision by using larger sample sizes in the sensitivity analyses below.

                                               [Insert Table 1 here]

           We added usage and demographic variables to further understand differences in

consumer value. The estimation results can be found in Table 2. The usage of Facebook per



11
     https://www.statista.com/statistics/408971/number-of-us-facebook-users/

                                                                                                   20
week (self-reported, measured on a 5-point scale from “less than 1 hour” to “more than 14

hours”) is a significant predictor for the value of Facebook (p = 0.006). The more time a

consumer spends on Facebook, the more likely they are to keep their access.12 Similarly, the

more friends someone has on Facebook (self-reported, measured on a 6-point scale from “less

than 50” to “more than 1000”) the more compensation they require to leave Facebook (p =

0.024). In terms of activities on Facebook (measured on a 6-point scale ranging from “never” to

“several times a day,”) consumers perceive significantly more value in Facebook the more they

post status updates or share pictures and videos (p = 0.010), the more they like and comment (p =

0.018), and play games (p = 0.025). Watching videos is marginally significant (p = 0.080), while

using the messenger and chat is associated with no additional value (p = 0.100). Consistently, we

find significant substitution effects due other social media services, i.e., Instagram (p = 0.025),

and video platforms, i.e., YouTube (p = 0.003). Thus, consumers who also use Instagram or

YouTube are more likely to give up Facebook. Services that are not related to activities that

provide value on Facebook show no significant substitution effects (e.g., Wikipedia, p = 0.601).

        In terms of socio-demographics, we find significant effects for gender and age of the

respondent, as well as household income. Specifically, we see that female respondents are more

likely to keep Facebook than male users (p = 0.011). The same holds for older consumers (p <

0.001). The effects for household income are less consistent. Households with an income

between 100K and 150K perceive significantly less value in Facebook (p = 0.019), while higher

income households value Facebook more (p = 0.008). The effect is also significantly positive for

consumers who preferred not to disclose their income (p = 0.004). Education and US region are

not significant (not shown in Table 2).


12
  This confirms the assumptions made in estimating consumer surplus from consumer time allocation (Brynjolfsson
and Oh, 2012)

                                                                                                            21
                                        [Insert Table 2 here]

       To summarize, the SBDC experiment leads to plausible demand functions and plausible

effects of usage and demographic variables. The results indicate that Facebook provides

substantial value to consumers who would require a median compensation of about $40-$50 per

month for leaving this service. We find no evidence that this valuation increased from 2016 to

2017; if anything, it may have declined somewhat. However, given the nature of choice data, the

estimated median WTA values are limited in terms of precision compared to directly elicited

values, which we will use as a benchmark method in the next section.



4.2 Benchmark method: BDM lottery

       As a benchmark to check the convergent validity of the SBDC approach, we applied an

incentive compatible BDM lottery procedure (Becker, DeGroot, and Marschak 1964) in order to

elicit direct, numeric responses from consumers about their WTA. Specifically, we asked

consumers about the minimum amount of money they would request in order to give up

Facebook for one month. In order to achieve incentive compatibility, we informed respondents

that the amount will serve as their bid in a lottery. The BDM lottery process instructs that, after

the survey, a random price will be drawn from a uniform distribution of values. If the random

price is higher than the bid, the respondent will be paid the random price when giving up

Facebook for one month. If the random price is lower than the bid, the respondent will receive no

money but can keep the access to Facebook. Thus, the rational, utility-maximizing strategy for

the respondent is to bid exactly their true value for Facebook.

       We conducted the BDM lottery in the lab of a European university, parallel to an

incentive compatible SBDC experiment. The lab setting allowed us to explain the BDM


                                                                                                  22
procedure in detail and ensure that the respondents understood the pay-off mechanism. In total,

139 students took part in the lottery. We compare this sample to a sample of respondents that

took part in the incentive compatible SBDC experiment of the same lab (n = 356). The SBDC

procedure was identical to the Peanut Labs study but used monetary offers in Euros (€).

       Figure 6 shows the estimated demand functions that result from both approaches. The

SBDC derived function is closely aligned to the BDM demand function. The observed shares

correlate strongly (correl. = 0.891). Fitting a regression model to the observed shares (R2 =

0.755) shows that the BDM approach estimates a larger intercept than the SBDC approach (p =

0.013), i.e., more respondents are willing to keep Facebook even at low monetary values. This is

plausible because BDM gives respondents more control over their bids and few respondents

submitted low monetary values, while the SBDC approach follows a take-it-or-leave-it

mechanism with exogenous monetary offers. More importantly, however, there is no significant

statistical difference (p = 0.278) in the estimated price sensitivity for the two approaches. While

this result gives us confidence in our estimates from the SBDC experiment, we explore its

robustness in further sensitivity analyses.

                                       [Insert Figure 6 here]



4.3 Sensitivity analyses

       We assess the robustness of the SBDC approach regarding its sensitivity to a hypothetical

bias, random responses, sample size, and the analyzed time frame.



4.3.1 Hypothetical bias




                                                                                                  23
        In order to measure the hypothetical bias, we applied a hypothetical scenario parallel to

the incentive compatible SBDC experiments in section 4.1. Specifically, we conducted the same

surveys as in the incentive compatible scenarios with Peanut Labs in June/July 2016 and 2017

but without informing consumers that their answers were consequential. We allocated

respondents randomly to the incentive compatible (IC) and non-incentive-compatible (NIC)

scenarios. In addition to the 2885 respondents in the IC studies, we tested 2878 consumers under

NIC conditions (n2016,NIC = 1500, n2017,NIC = 1378).

        For illustration, we detail the results for the 2016 study first. Figure 7 compares the

observed shares between IC and NIC groups. For very low prices, i.e., a price of $1, the IC and

NIC condition produce almost identical shares, which is reasonable. For higher prices, the

disparities increase, leading to consistently higher shares in willingness to keep Facebook in the

IC condition. The estimation of the binary logit model confirms that the IC consumers do not

differ in the intercept (p = 0.905) but they react significantly less sensitive towards differences in

E (p = 0.002, see Table 3). Consequently, the IC consumers are less attracted by the monetary

offers and require a significantly higher amount in order to give up Facebook (WTAIC,2016 =

$48.49, CIIC,2016 = [$32.04, $72.24]). Consumers in the NIC setting are satisfied with lower

amounts, i.e., WTANIC,2016 = $13.80 per month (95% CINIC,2016 = [$9.80, $19.19]). Consequently,

the hypothetical WTA is understated in this research context and needs to be calibrated by a

factor of 3.5.

                                        [Insert Figure 7 here]

                                        [Insert Table 3 here]

        The results for the 2017 study are consistent. In this case, the median WTA in the NIC

condition is $9.18 (95% CINIC,2017 = [$6.07, $13.70]), compared to $37.76 in the IC scenario



                                                                                                    24
(CIIC,2017 = [$27.19, $51.97]. ), which leads to a calibration factor of 4.1 (see appendix, Table A.1

for the full estimation model that accounts for year and group membership).


        Our results suggest that the hypothetical bias can be substantial. More importantly,

however, our primary interest is not the absolute amount of consumer surplus for Facebook but

annual changes in value. In this case, the incentive compatible study would estimate a loss in

value of 20% from 2016 to 2017, while the hypothetical study calculates a loss of 32%. Despite

the hypothetical bias, the annual changes move in the same direction and are more closely

aligned than the absolute valuations.



4.3.3 Effect of random answers

        Random answers increase the error variance in choice model estimations. The error

variance, in turn, has a negative effect on the precision, i.e., scale of the estimates S in logit

choice models (Hauser, Eggers, and Selove 2016). Specifically, the scale S is inversely

proportional to the error variance. The scale S cannot be separately identified, such that it is

incorporated in the “raw” estimated utilities b:

V = (S * b0) g0 + (S * b1) E.

        Lower scaled estimates (more error), i.e., estimates with lower magnitude, cause the logit

function to become more linear. Higher scaled estimates (less error) lead to a stepwise function

that allows us to predict decisions and identify the median WTA more precisely (see Figure 8).

                                        [Insert Figure 8 here]

        The effect is demonstrated empirically in Table 4. The table shows the result of a

modified bootstrapping procedure in which 1,000 subsamples were drawn from the 2016 IC




                                                                                                     25
Facebook sample for illustration.13 In each subsample, we replaced R randomly selected original

responses with the same amount of random answers and re-estimated the logit model. The results

show that more random noise in the answers decreases the scale of the estimates. The scale S is

proportional to the relative share of non-random answers. Having more random answers than

original responses (R = 800) causes the magnitude of the estimates to be less than half the size of

the original estimation without additional random answers (R = 0). However, the median WTA

(averaged across the 1,000 subsamples) as well as the absolute standard error of the estimates

remain largely unaffected. Surplus measures that consider the overall demand function by

integrating the demand function, here in the interval from $1 to $1000, are biased by random

answers. We therefore only report WTA measures in our analyses.

                                               [Insert Table 4 here]

           The simulated results illustrate that the possibility of random answers cannot account for

the observed hypothetical bias. Interestingly, when we compare how many of the observed

choices can be predicted correctly based on the estimated model, we find a better fit for the NIC

group (hit rateNIC = 69.9%) than for the IC group (hit rateIC = 62.1%). This suggests that

consumers in the IC group faced a decision that was more difficult to make, likely because their

choices were consequential. It is important to note that the misclassified choices are not

necessarily due to purely random responses. These cases can also be explained by heterogeneity

among consumers that is not accounted for in the estimation models, either with respect to their

valuation of Facebook or regarding their general price sensitivity (or both).



4.3.4 Effect of sample size



13
     We obtain similar results for the NIC group and for the 2017 samples.

                                                                                                   26
       Next to random noise, the precision of the WTA estimates also depends on the sample

size. To analyze the magnitude of the effect, we used bootstrapping with varying subsample sizes

to observe the effect on standard errors and confidence intervals for the WTA estimate. Each

subsample of a given size was again randomly drawn 1000 times from the original sample (IC

group in 2016). As expected, Table 5 demonstrates that the standard errors of the estimates are

reduced by the square-root of 2 when doubling the sample size (in this case the scale of the

estimates remains largely unaffected). This general pattern also holds for the standard error of

the WTA estimate. However, since WTA is a ratio of two stochastic variables, this

generalization is approximate. The results show how the 95% confidence interval narrows when

increasing the sample size. There is uncertainty in the measure even with a sample size of 1500.

For the case of Facebook, a 95% confidence interval of ±$10 can be achieved with a sample of

6000 consumers. This result highlights the need for more massive sample sizes to measure

consumer surplus precisely.

                                       [Insert Table 5 here]



4.3.2 Effect of the analyzed time frame

       In the previous incentive compatible studies, we used one month instead of one year as

the time frame that respondents should forego Facebook. This raises the question to what extent

consumers are sensitive to the time frame. To address this question, we conducted SBDC

experiments in an incentive compatible setting in which, in addition to prices E, we varied the

time frame across three periods, i.e., T = 1 week, 2 weeks, 1 month. We recruited another sample

from Peanut Labs in 2017 using the same criteria as in the previous studies, but we did not

screen out respondents who do not use Facebook (assuming that these respondents would accept



                                                                                                   27
any low monetary compensation; empirical valuations are therefore lower than in the previous

study). A total of 1499 respondents were available for the analysis.

       Table 6 shows the estimation results. As expected, the time frame has a significant

positive effect on the probability to keep Facebook. Accordingly, the median WTAs for the

different time frames are $3.92 for one week, $10.53 for two weeks, and $17.61 for one month.

Interestingly, these values and coefficient estimates suggest that the effect of time is not

necessarily linear. As with any good, the value of Facebook depends on the context.

                                        [Insert Table 6 here]

       In order to get a better overview of the effect of time, we sampled 5021 additional

respondents in a hypothetical setting using Google Consumer Surveys (see section 5). We

allocated these respondents randomly to one of ten conditions that differ in the time frame: T = 1

hour, 1 day, 1 week, 2 weeks, 3 weeks, 4 weeks, 1 month, 2 months, 3 months, 6 months, 1 year

(operationalized in the estimation model in terms of number of days). We kept E constant at $50

in this study. Figure 9 shows the observed shares of respondents who prefer to keep Facebook at

the different time frames and the predicted time function according to the binary logit model

(using log(T) and log(T)2 as predictors, see Table A.2 in the appendix). It confirms a positive

effect of time with increasing marginal effects. Accordingly, consumers are more likely to keep

Facebook the longer the time frame, and this effect is reinforced with increasing duration. We

use a time frame of one year in the large-scale studies we present next.

                                       [Insert Figure 9 here]




                                                                                                  28
5. Large-scale Studies to Measure Consumer Surplus


5.1 Google Consumer Surveys: Single Binary Discrete Choices

         For the implementation of our large-scale studies, we use Google Consumer Surveys

(GCS) as our primary platform. GCS allows us to run short one-question surveys inexpensively

and quickly and is therefore well suited for our SBDC experiments. A number of online

publishers (including news and arts/ entertainment sites) participate in GCS and host these

choice experiments on their site as a gateway to access premium content (Stephens-Davidowitz

and Varian 2015). Users must answer the survey in order to unlock premium content (Figure 10).

Survey creators pay per response, part of which goes to the publisher for hosting it. In addition to

the responses, some demographic characteristics of the respondents such as region, age, gender

and income are also provided, which are inferred from IP address, location, browsing history

(provided by Google’s DoubleClick cookies which are also used to serve ads) and census data.

Prior research has found that GCS results are very similar to those obtained from other surveys

conducted by professional organizations such as Pew (Stephens-Davidowitz and Varian 2015).14

                                                 [Insert Figure 10 here]

         We identified the most widely used apps and websites on various devices and combined

them into the following eight product categories: Email, Search Engines, Maps, E-commerce,

Video, Music, Social Media, and Instant Messaging. We ran SBDC surveys for each of these

categories in June/July 2016 and 2017. In these studies, we asked consumers to consider giving

14
   To confirm that there is no selection bias, we compared the NIC group from the Peanut Labs sample (see section
4.3.1) to a GCS sample (n = 1451). Because Google Surveys do not screen respondents if they are Facebook users or
not, unlike in the Peanut Labs study, we matched the NIC group by accounting for the share of non-Facebook users.
A binary logit model confirms that there are no significant differences between both samples, either in terms of their
intercept (p = 0.991) or their sensitivity towards E (p = 0.474). See appendix for details (Table A.3, Figure A.2).

                                                                                                                   29
up access to these categories for one year. As compensation, we offered one of 6 to 15 price

levels for each product category and gathered around 500 responses per price level per year. If

the median WTA was outside the range of our initial set of price levels, we increased the number

of price levels in the following year in order to accommodate higher prices (for Search Engines,

Email, Maps).

       The observed shares and estimated demand curves are shown in Figure 11. The demand

curves appear plausible and are consistent across time (solid lines represent 2016, dashed lines

2017). The annual changes suggest an increase in the valuation for these categories, albeit not

statistically significant. This notion is confirmed when inspecting the median annual WTA

values per year in Table 7. As in the Facebook study, the range of the confidence intervals is

large, meaning that the significance of the changes cannot be estimated reliably.

       According to the median WTA estimates for 2017, Search Engines ($17,530) is the most

valued category of digital goods followed by Email ($8,414) and digital Maps ($3,648). One

possible reason that these values are high relative to the other goods in our analysis may be the

lack of effective substitutes for search engines, email or digital maps compared to the other

categories in our sample. Because most consumers do not directly pay for these services, almost

all of the WTA for these goods contributes towards consumer surplus. What’s more, for many

people, these services are essential to their jobs, making them reluctant to give up these goods.

       Video streaming services (e.g., YouTube, Netflix) are valued by consumers with a

median WTA of $1,173 per year. Some consumers do pay for some of these services. However,

these amounts are of the order of $10-$20 per month, or $120-$240 per year (for those who pay).

Our measure suggests that the surplus the median consumers receive from these goods is a 5-10

multiple of what they actually pay (and which is visible in national accounts). The remaining



                                                                                                    30
categories for which we estimated the median WTA are (in descending order) E-Commerce

($842), Social Media ($322), Music ($168), and Instant Messaging ($155).

       These estimates are potentially biased downwards due to lack of incentive compatibility

in these studies. Nevertheless, the sum of these estimates suggests there is a significant amount

of consumer surplus from digital goods and a positive tendency over time.

                                      [Insert Figure 11 here]

                                       [Insert Table 7 here]

       The available demographic variables (gender, age, income and urban density) reported by

Google were added to an extended model to determine effects for different consumer segments.

These extended logit models are reported in Table A.4 in the appendix. These results reveal a

number of patterns that are interesting and may have implications for research and business. For

instance: The value of search engines increases by age and income and is higher for female

consumers. Similar effects of age and gender can be observed for the email category. In this

case, consumers in urban areas and with a median income of $50K to $75K also perceive a

higher value. For maps, the effect of age on WTA follows an inverse U-shape. Middle-aged

consumers of 35-44 years value maps most. Income has a positive effect on the valuation of

maps. A similar inverse U-shaped effect between age and valuation can be seen for e-commerce.

In this case, the maximum value is experienced by 55-64 year-old consumers. In addition, female

consumers perceive a higher value from online shopping. Age has a negative effect for the video

and music categories. Whereas this pattern is consistent across all age groups for videos, the

negative pattern only starts at an age of 45 years or older for music. Male consumers value

videos more. The music category is preferred in urban areas. Female users value social media

more. The same holds for instant messaging. In this category, the youngest age group (18-24



                                                                                                    31
years) perceives the highest value. Older consumers perceive significantly less value. Our

approach opens the door to testing a variety of hypotheses and uncovering most such patterns

relatively easily.

        Our approach can be used for digital and non-digital goods alike. As an example, we also

ran SBDC surveys to estimate the WTA to give up the option of eating breakfast cereal15 for one

year. Figure 12 plots the WTA demand curve for breakfast cereal. We estimate the median WTA

to give up breakfast cereal to be $44.27 in the US in 2017 (95% CI2017: [$37.19; $52.47]).16 This

estimate is almost identical to the results from 2016 (95% CI2016: [$37.98; $49.74]). Examining

non-digital goods can help us calibrate the relative importance of some of the digital goods we

examine. We therefore also incorporate non-digital goods in the benchmark study using best-

worst scaling.

                                           [Insert Figure 12 here]



5.2 Benchmark method: Best-worst scaling

        As a benchmark to GCS, we conducted additional choice experiments based on the best-

worst scaling (BWS) approach (Flynn et al. 2007; Marley and Louviere 2005). Best-worst

scaling asks consumers to repeatedly select the best and worst options from sets of alternatives.

Collecting more information, both within the choice set and across sequential choice sets, for

each consumer makes this approach more efficient compared to the SBDC approach, which

elicits only one decision. Moreover, consumers are required to make a tradeoff when deciding




15
   Economists have studied this industry using a variety of approaches. See e.g., Hausman (1996), Schmalensee
(1978), Nevo (2001), and others. Hausman (1996) estimates the consumer surplus due to entry of a new cereal brand
(Apple-Cinnamon Cheerios) to be $0.3136 per person per year.
16
   This figure is in addition to the price paid by consumers for buying breakfast cereal.

                                                                                                              32
which goods they perceive as most and least valuable. This may mitigate or even eliminate the

systematic hypothetical bias, at least with respect to the ordinal ranking of the choices.

        We used nineteen digital goods, six non-digital goods and nine price points ranging from

$1 to $20,000, that consumers compared three at a time. Because we examined the value of

foregoing access to specific services or amenities for one year, the price options were also

expressed as losses (foregoing a specific amount of salary for one year) in order to be

comparable, e.g., “earning $10,000 less for 1 year.” The price sensitivity we are measuring is

therefore closer to WTP than WTA.

        We presented three options within each choice set for each individual so that respondents

created a full ranking of the three options in a set by indicating the best and worst options. Figure

13 shows an example of such a choice set. Respondents answered 10 or 11 sets17 in order to be

exposed to each good. We randomized the allocation of goods and prices to choice sets across

respondents.

                                           [Insert Figure 13 here]

        We recruited consumers for this online study via Peanut Labs in 2017. We targeted

consumers that were 18 years or older and lived in the US. Consumers who did not fulfill these

criteria were screened out. We controlled quotas for gender, age, and US regions to match US

census data (File and Ryan 2014). In total, 503 respondents completed the study.

        We estimated utility parameters using a multinomial logit model. We considered both

best and worst choices in the same model by interpreting utilities from best choices as the

negative of worst choices. The estimation leads to interval-scaled utility scores that represent the

disutility of not having access to the goods (or earning less income) for one year, which are

17
   We used two subsamples that differed in the number of goods and number of choice sets in order to accommodate
different goods and price points. One subsample (n = 204) evaluated 30 options in 10 choice sets; the other
subsample (n = 299) 33 options in 11 sets.

                                                                                                             33
depicted in Figure 14 (see also Table A.5 in the appendix). We have set the lowest ranked

service for the US, WhatsApp, as a reference category so that utilities are expressed relative to

WhatsApp. The ranking of the goods is consistent to the SBDC experiments for the eight most

widely used categories using GCS, with only one exception: online shopping is valued more than

maps and video streaming in the best-worst scaling approach, while we find it to be valued less

in the GCS surveys. When comparing the utilities of the services to the utility scores of the price

levels we find, as expected, consistently lower implied WTP values than WTA estimates

according to the GCS survey. Estimating a demand function and interpolating WTP shows very

strong correlation among BWS and SBDC valuations (Correl. = 0.911). Overall, comparing the

results of both approaches indicates convergent validity.

                                              [Insert Figure 14 here]



6. Discussion

           With advances in information technologies, we can now gather data at a large scale in

close to real time. Initiatives such as MIT’s Billion Prices project18 and Adobe’s Digital Price

Index19 are collecting price data from online retailers in real time to compute price and inflation

indices. We explore the potential to reinvent and supplement the measurement of economic well-

being by taking advantage of the ease of gathering data in the digital era. The end goal of this

research agenda is to design a scalable method of measuring changes in consumer surplus

induced by technological advancements. We explore a potential way of measuring changes in

consumer surplus through SBDC experiments. Our method is highly scalable and relatively


18
     http://bpp.mit.edu/
19
     https://blogs.adobe.com/digitalmarketing/analytics/introducing-digital-economy-project/

                                                                                                    34
inexpensive. Therefore, it can be run at very frequent, regular intervals to track changes in

consumer surplus. As argued previously, this measure can be an important complementary

indicator of consumer well-being for the digital economy.

         In a series of online experiments, we show that the SBDC approach leads to plausible

demand functions that are consistent with other validated approaches. We find that free digital

goods provide substantial value to consumers even if they don’t contribute substantially to GDP

and may even displace products that do contribute to GDP. We further find that our approach can

detect consumers’ sensitivity towards different time frames, e.g., whether consumers use (or not

use) the goods for one week, one month, or one year. We find that time has a positive effect on

the probability to keep a service with increasing marginal returns.20

         In order to address the limitation of the bias in answering hypothetical questions of the

proposed approach, we have compared consumers’ valuation of Facebook in an incentive

compatible and a hypothetical setting. We confirm that a hypothetical bias exists, such that

valuations of Facebook in the hypothetical scenarios tend to be significantly underestimated. The

magnitude of the bias and potential correction factors need to be analyzed further in future

studies. However, the differences between hypothetical and incentive compatible approaches are

much less severe when analyzing annual changes in valuations, rather than levels.

         A major limitation of our study remains the lack of precision in our estimates. While the

BEA is able to measure GDP very precisely (e.g. US GDP was reported as $19,736.5 billion in




20
   Some consumers seem to be willing to undergo “digital detox” for a short duration by giving up internet or
individual services like Facebook either through self-control or by installing software which blocks particular sites.
This might explain consumers’ weaker sensitivity towards short term abstinence and raises interesting questions
about neoclassical economic models of rational choice, self-control and the nature of utility functions. Economics
continues to evolve to take account mental biases that deviate from traditional notions of rationality, e.g., Kahneman
et al. 1990, Kahneman 2011, Thaler 2015.

                                                                                                                   35
the fourth quarter of 201721), we are only able to provide a relatively coarse estimate of changes

in consumer surplus, even in our large-scale studies. Future work should use larger sample sizes

to narrow the confidence interval of the WTA estimates.

        Although the median WTA is robust to random noise in the data, the overall demand

functions are not: small numbers of extreme valuations can have undue influence. In contrast,

focusing only on the median valuations, while much more robust to noise, limits the application

of the SBDC approach to those goods that are used by at least 50% of the consumers. Thus,

research can benefit from reporting other key percentiles, e.g., the valuation for people at the 90th

percentile, or other benchmarks, when comparing goods to each other. Before being able to

derive surplus measures along the overall demand curve, we need further evidence to confirm

that the error variance in the data remains consistent over time and therefore cancels out when

calculating annual changes.

        Another limitation of our study is that it is biased towards people using the internet. The

massive variants of our choice experiments are only accessible online, therefore people not using

the internet at all are excluded. Pew estimates that about 15% of Americans don’t use the

internet.22 Accordingly, our results must be interpreted as relevant to this audience, but not

necessarily others.

        That said, our approach is at least attempting to directly measure a concept that we know

is not correctly measured by other official data. In short, we believe it is better to be

approximately correct than precisely wrong.



21
   https://www.bea.gov/newsreleases/national/gdp/gdpnewsrelease.htm (Accessed March 2018)
22
  http://www.pewresearch.org/fact-tank/2015/07/28/15-of-americans-dont-use-the-internet-who-are-they/. Of
course, to the extent that the unmeasured consumer surplus dynamics are occuring in digital goods, it may be safe to
surmise that those who are not on the internet are probably not using many digital goods and have negligible effects
on such surplus.

                                                                                                                 36
37
References

Aeppel, T. (2015). “Silicon Valley Doesn’t Believe U.S. Productivity Is Down,” Wall Street
Journal, July 16, www.wsj.com/articles/silicon-valley-doesnt-believe-u-s-productivity-is-
down1437100700?tesla=y&cb=logged0.28855257923714817.

Ariely, D., Loewenstein, G., & Prelec, D. (2003). “Coherent arbitrariness”: Stable demand
curves without stable preferences. The Quarterly Journal of Economics, 118(1), 73-106.

Bates, W. (2009). Gross national happiness. Asian-Pacific Economic Literature, 23(2), 1-16.

Becker, G.M., DeGroot, M.H., & Marschak, J. (1964). Measuring utility by a single-response
sequential method. Behavioral Science, 9, 226–232.

Bishop, R. C., Boyle, K. J., Carson, R. T., Chapman, D., Hanemann, W. M., Kanninen, B., ... &
Paterson, R. (2017). Putting a value on injuries to natural assets: The BP oil
spill. Science, 356(6335), 253-254.

Bishop, R.C. and Heberlein, T.A. (1979). Measuring values of extramarket goods: Are indirect
measures biased? American Journal of Agricultural Economics, 61(5), 926-930.

Brynjolfsson, E. and Oh, J. (2012). The attention economy: measuring the value of free digital
services on the Internet. Proceedings of the 33rd International Conference on Information
Systems, Orlando, December 2012.

Brynjolfsson, E., & McAfee, A. (2014). The second machine age: Work, progress, and
prosperity in a time of brilliant technologies. WW Norton & Company.

Brynjolfsson, E., Rock, D, and Syverson, C. (2017). Artificial Intelligence and the Modern
Productivity Paradox: A Clash of Expectations and Statistics. NBER Working Paper No. 24001.
Issued in November 2017

Brynjolfsson, E., & Saunders, A. (2009). What the GDP gets wrong. (Why managers should
care). MIT Sloan Management Review, 51(1), 95.

Byrne, D. M., Fernald, J. G., & Reinsdorf, M. B. (2016). Does the United States have a
productivity slowdown or a measurement problem?. Brookings Papers on Economic Activity,
2016(1), 109-182.

Carson, R. and Czajkowski, M. (2014). The discrete choice experiment approach to
environmental contingent valuation, in: Hess, S. and Daly, A. eds., 2014. Handbook of Choice
Modelling. Edward Elgar Publishing.

Carson, R.T. (2012). Contingent valuation: A practical alternative when prices aren't available.
The Journal of Economic Perspectives, 26(4), 27-42.


                                                                                                   38
Carson, R.T., Groves, T. (2007). Incentive and informational properties of preference questions,
Environmental and Resource Economics, 37(1), 181-210.

Carson, R.T., Groves, T., List, J.A. (2014). Consequentiality: A theoretical and experimental
exploration of a single binary choice. Journal of the Association of Environmental and Resource
Economists, 1(1/2), 71-207.

Carson, R.T., Mitchell, R.C., Hanemann, M., Kopp, R.J., Presser, S. and Ruud, P.A. (2003).
Contingent valuation and lost passive use: damages from the Exxon Valdez oil spill.
Environmental and Resource Economics, 25(3), 257-286.

Cohen, P., Hahn, R., Hall, J., Levitt, S., & Metcalfe, R. (2016). Using Big Data to Estimate
Consumer Surplus: The Case of Uber (No. w22627). National Bureau of Economic Research.

Diamond, P.A. and Hausman, J.A. (1994). Contingent valuation: Is some number better than no
number? The Journal of Economic Perspectives, 8(4), 45-64.

Ding, M. (2007). An incentive-aligned mechanism for conjoint analysis. Journal of Marketing
Research, 44(2), 214-223.

Ding, M., Grewal, R. and Liechty, J. (2005). Incentive-aligned conjoint analysis. Journal of
marketing research, 42(1), 67-82.

den Haan, W., Ellison, M. Ilzetzki, E., McMahon, M., and R Reis (2017) “Happiness and
wellbeing as objectives of macroeconomic policy: Views of economists”
https://voxeu.org/article/views-happiness-and-wellbeing-objectives-macroeconomic-policy

Feldstein, M. (2017). Underestimating the Real Growth of GDP, Personal Income and
Productivity, Journal of Economic Perspectives, 31(2), 145-164

File, T. and Ryan, C. (2014). Computer and Internet Use in the United States: 2013, U.S. Census
Bureau. (Accessed at: http://www.census.gov/history/pdf/2013computeruse.pdf)
Flynn, T. N., Louviere, J. J., Peters, T. J., & Coast, J. (2007). Best–worst scaling: what it can do
for health care research and how to do it. Journal of Health Economics, 26(1), 171-189.

Furman, J., & Orszag, P. (2015). A firm-level perspective on the role of rents in the rise in
inequality. Presentation at “A Just Society” Centennial Event in Honor of Joseph Stiglitz
Columbia University.

Giles, J. (2005). Internet encyclopaedias go head to head: Jimmy Wales' Wikipedia comes close
to Britannica in terms of the accuracy of its science entries. Nature 438(7070), 900–1

Goolsbee, A., & Klenow, P. J. (2006). Valuing consumer products by the time spent using them:
An application to the Internet. American Economic Review, 96(2), 108-113.

Greenstein, S. and McDevitt, R.C., 2011. The broadband bonus: Estimating broadband Internet's
economic value. Telecommunications Policy, 35(7), 617-632.


                                                                                                  39
Greenwood, J., & Kopecky, K. A. (2013). Measuring the welfare gain from personal
computers. Economic Inquiry, 51(1), 336-347.

Haab, T.C., Interis, M.G., Petrolia, D.R. and Whitehead, J.C. (2013). From hopeless to curious?
Thoughts on Hausman's “dubious to hopeless” critique of contingent valuation. Applied
Economic Perspectives and Policy, p.ppt029.

Hanemann, W.M. (1991). Willingness to pay and willingness to accept: how much can they
differ? The American Economic Review, 81(3), 635-647.

Hatzius, J. (2015). “Productivity Paradox 2.0.” Top of Mind, Issue 39, p. 6–7. Goldman Sachs.
http://www.goldmansachs.com/our-thinking/pages/macroeconomic-insights-folder/the-
productivity-paradox/report.pdf.

Hauser, J.R., Eggers, F., and Selove, M. (2016), The Strategic Implications of Precision in
Conjoint Analysis, MIT Sloan School of Management working paper.

Hausman, J. A. (1996). Valuation of new goods under perfect and imperfect competition. In The
Economics of New Goods (pp. 207-248). University of Chicago Press.

Hausman, J., (2012.) Contingent valuation: from dubious to hopeless. The Journal of Economic
Perspectives, 26(4), 43-56.

Helliwell, J., Layard, R., & Sachs, J. (2017). World Happiness Report 2017, New York:
Sustainable Development Solutions Network.

Hulten, C. R. (1978). Growth accounting with intermediate inputs. The Review of Economic
Studies, 45(3), 511-518.

Jones, C. I., & Klenow, P. J. (2016). Beyond GDP? Welfare across countries and time. The
American Economic Review, 106(9), 2426-2457.

Jorgenson, D. W., & Slesnick, D. T. (2014). Measuring social welfare in the US national
accounts. In Measuring Economic Sustainability and Progress (pp. 43-88). University of Chicago
Press.

Kahneman, D., Knetsch, J. L., & Thaler, R. H. (1990). Experimental tests of the endowment
effect and the Coase theorem. Journal of Political Economy, 98(6), 1325-1348.

Kahneman, D., Knetsch, J. L., & Thaler, R. H. (1991). Anomalies: The endowment effect, loss
aversion, and status quo bias. The Journal of Economic Perspectives, 5(1), 193-206.

Kahneman, D. (2011). Thinking, Fast and Slow. Macmillan.

Krueger, A. B., & Stone, A. A. (2014). Progress in measuring subjective well-
being. Science, 346(6205), 42-43.



                                                                                                40
Kuznets, S. (1934). “National Income, 1929-1932” 73rd Congress, 2nd Session, Senate
Document no. 124.

Kuznets, S. (1973). Modern economic growth: findings and reflections. The American Economic
Review, 63(3), 247-258.

Landefeld, J. S. (2000). GDP: One of the great inventions of the 20th century. Survey of Current
Business, 80(1), 6-14.

List, J.A. and Gallet, C.A. (2001). What experimental protocol influence disparities between
actual and hypothetical stated values? Environmental and Resource Economics, 20(3), 241-254.

Louviere, J.J., Hensher, D.A. and Swait, J.D. (2000). Stated Choice Methods: Analysis and
Applications. Cambridge University Press.

Manski, C.F. (1977). The structure of random utility models. Theory and Decision, 8(3), 229-
254.

Marley, A. A. J. and Louviere, J. J. (2005). Some probabilistic models of Best, Worst, and Best-
Worst choices. Journal of Mathematical Psychology. 49, 464-480.

McFadden, D.L. (1974). Conditional logit analysis of qualitative choice behavior, in P.
Zarembka (ed.), Frontiers in Econometrics, New York: Academic Press.

McFadden, D.L. (2014). In the Matter of Determination of Rates and Terms for Digital
Performance in Sound Recordings and Ephemeral Recordings (WEB IV), Before the Copyright
Royalty Board Library of Congress, Washington DC, Docket No. 14-CRB-0001-WR, October 6.

Miller, K.M., Hofstetter, R., Krohmer, H. and Zhang, Z.J. (2011). How should consumers'
willingness to pay be measured? An empirical comparison of state-of-the-art approaches. Journal
of Marketing Research, 48(1), 172-184.

Miller, M. (2012). How Google made $37.9 billion in 2011. Search Engine Watch. (Accessed at:
https://searchenginewatch.com/sew/news/2140712/google-usd379-billion-2011)

Morwitz, V.G., Steckel, J.H. and Gupta, A. (2007). When do purchase intentions predict sales?
International Journal of Forecasting, 23(3), 347-364.

Murphy, J.J., Allen, P.G., Stevens, T.H., Weatherhead, D. (2005). A meta-analysis of
hypothetical bias in stated preference valuation, Environmental and Resource Economics, 30(3),
313-325.

Nakamura LI, and Soloveichik R. (2015). Valuing ‘free’ media across countries in GDP. Federal
Reserve Board of Philadelphia Working Paper No. 15-25.

Nevo, A. (2001). Measuring market power in the ready-to-eat cereal industry. Econometrica,
69(2), 307-342.


                                                                                               41
Nordhaus, W. D. (2005). Schumpeterian Profits and the Alchemist Fallacy.Yale Economic
Applications and Policy Discussion Paper, (6).

OECD (2008). OECD compendium of productivity indicators. (Accessed at:
http://www.oecd.org/std/productivity-stats/40605524.pdf)

Peanut Labs (2015). Peanut Labs Panel Book. (Accessed at:
http://news.peanutlabs.com/rs/peanutlabs/images/PL-PanelBook_2015.pdf)

Pepitone, J. (2012). Encyclopedia Britannica to stop printing books. CNN. (Accessed at:
http://money.cnn.com/2012/03/13/technology/encyclopedia-britannica-books)

Plott, C.R. and Zeiler, K. (2005). The willingness to pay–willingness to accept gap, The
American Economic Review, 95(3), 530-545.

Rao, V.R. (2014). Applied Conjoint Analysis (p. 56). New York, NY: Springer.

Rosston, G., Savage, S. and Waldman, D. (2011). Household demand for broadband internet
service. Communications of the ACM, 54(2), 29-31.

Schmalensee, R. (1978). Entry deterrence in the ready-to-eat breakfast cereal industry. The Bell
Journal of Economics, 9(2), 305-327.

Spence, M., B. Owen. 1977. Television programming, monopolistic competition, and welfare.
Quarterly Journal of Economics, 91(1). 103–126.

Stephens-Davidowitz, S., H. Varian. (2015). A Hands-on Guide to Google Data. (Accessed at:
http://people.ischool.berkeley.edu/~hal/Papers/2015/primer.pdf)

Stiglitz, J. E., Sen, A., & Fitoussi, J. P. (2009). Report by the commission on the measurement of
economic performance and social progress. Paris: Commission on the Measurement of Economic
Performance and Social Progress.

Syverson, C. (2016). Challenges to Mismeasurement Explanations for the US Productivity
Slowdown (No. w21974). National Bureau of Economic Research.

Thaler, R. H. (2015). Misbehaving: The Making of Behavioral Economics. WW Norton &
Company.

Thurstone, L.L. (1927). A law of comparative judgment. Psychological Review, 34(4), 273.

Varian H. (2011, March 29). The economic value of Google. Presentation, San Francisco.

Varian, H. (Sep 2016). A microeconomist looks at productivity: A view from the valley.
Presentation at Brookings (Accessed at: https://www.brookings.edu/wp-
content/uploads/2016/08/varian.pdf)

Vickrey, W. (1961). Counterspeculation, auctions and competitive sealed tenders. Journal of
Finance, 16, 8–37.
                                                                                               42
Völckner, F. (2006). An empirical comparison of methods for measuring consumers’ willingness
to pay. Marketing Letters, 17(2), 137-149.

Waldfogel, J. (2012). Copyright protection, technological change, and the quality of new
products: Evidence from recorded music since Napster. Journal of Law and Economics, 55(4),
715-740.

Wertenbroch, K., & Skiera, B. (2002). Measuring consumers’ willingness to pay at the point of
purchase. Journal of Marketing Research, 39, 228–241.

Whitehead, J.C. (2002). Incentive incompatibility and starting-point bias in iterative valuation
questions. Land Economics, 78(2), 285-297.

Williams, B. (2008). “A hedonic model for Internet access service in the Consumer Price Index.”
Monthly Labor Review, July, 33-48.

Wlömert, N. and Eggers, F. (2016). Predicting new service adoption with conjoint analysis:
external validity of BDM-based incentive-aligned and dual-response choice designs. Marketing
Letters, 27(1), 195-210.




                                                                                                   43
Tables and Figures

Table 1: Estimation results of binary logit model comparing consumers’ valuation of Facebook

in 2016 and 2017

                                    beta      Std. Error       z            p

               (Intercept)         1.200        0.125        9.624        <0.001

                   log(E)          -0.309       0.030       -10.327       <0.001

               Year_2017           0.290        0.209        1.385        0.166

           Year_2017*log(E)        -0.101       0.051        -1.966       0.049




                                                                                           44
Table 2: Facebook value diagnostic

                                                        beta     Std. Error      z        p

(Intercept)                                            0.321       0.254      1.261     0.207

log(E)                                                 -0.346      0.032      -10.801   <0.001

Year_2017                                              0.306       0.220      1.392     0.164

Year_2017*log(E)                                       -0.105      0.054      -1.940    0.052

Facebook usage per week (scale)                        0.117       0.043      2.740     0.006

Facebook number of friends (scale)                     0.074       0.033      2.257     0.024

Facebook activity: Posting status updates or sharing   0.095       0.037      2.577     0.010
pictures and videos (scale)

Facebook activity: Liking and commenting (scale)       0.093       0.039      2.363     0.018

Facebook activity: Playing games (scale)               0.054       0.024      2.234     0.025

Facebook activity: Using the messenger or chat         0.053       0.032      1.643     0.100
(scale)

Facebook activity: Watching videos (scale)             0.066       0.037      1.748     0.080

Instagram user                                         -0.225      0.100      -2.245    0.025

Skype user                                             -0.067      0.092      -0.733    0.464

Google maps user                                       -0.076      0.107      -0.712    0.477

Google search user                                     -0.188      0.127      -1.482    0.138

YouTube user                                           -0.420      0.141      -2.983    0.003

Wikipedia user                                         0.049       0.096      0.510     0.610

Gender female (reference)                              (0.000)

Gender male                                            -0.220      0.086      -2.546    0.011

Age 18-24 (reference level)                            (0.000)

Age 25-34                                              -0.012      0.152      -0.079    0.937

Age 35-44                                              0.245       0.151      1.620     0.105

Age 45-54                                              0.367       0.155      2.371     0.018

Age 55-64                                              0.590       0.161      3.669     <0.001



                                                                                                 45
Age 65+                            0.936     0.176   5.335    <0.001

Income less than 25K (reference)   (0.000)

Income 25K to 50K                  0.081     0.140   0.578    0.563

Income 50K to 100K                 -0.030    0.131   -0.229   0.819

Income 100K to 150K                -0.370    0.157   -2.355   0.019

Income 150K or more                0.441     0.165   2.671    0.008

Income “prefer not to answer”      0.784     0.273   2.873    0.004




                                                                       46
Table 3: Estimation results of binary logit model comparing IC and NIC scenarios (2016 study)

                                    beta      Std. Error        z            p

               (Intercept)
                                    1.178        0.135        8.726       <0.001

                 log(E)
                                   -0.449        0.034       -13.147      <0.001

                   IC
                                    0.022        0.184        0.119        0.905

               IC*log(E)
                                    0.140        0.045        3.076        0.002




                                                                                           47
Table 4: Effects of random answers on estimate errors

Random     Non-random     Mean      Mean beta   Std. error    Std. error   WTA      Surplus   Scale
sample R     sample     intercept    log (E)    Intercept    beta log(E)                        S

  800         700        0.517       -0.135       0.139        0.033       $46.29   $430.53   0.431

  400         1100       0.846       -0.218       0.149        0.035       $48.52   $390.61   0.700

  200         1300       1.020       -0.262       0.151        0.037       $49.06   $371.32   0.844

  100         1400       1.122       -0.289       0.157        0.038       $48.91   $359.69   0.929

   0          1500       1.206       -0.311       0.163        0.039       $48.18   $349.72   (1.000)




                                                                                                      48
Table 5: Effects of sample size on estimate errors

 Sample        Mean      Mean beta   Std. error   Std. error             95% CI   95% CI
  size       intercept    log (E)    Intercept    beta log(E)   WTA       lower    upper

   200        1.242       -0.319       0.462         0.110      $49.65   $13.13   $187.73

   400        1.227       -0.316       0.324         0.077      $48.72   $21.16   $112.28

   800        1.214       -0.311       0.226         0.053      $49.30   $27.83   $87.27

  1500        1.206       -0.311       0.163         0.039      $48.18   $31.69   $73.26




                                                                                            49
Table 6: Estimation results for the marginal effect of time (IC study)

                                              beta        Std. Error       z         p

             (Intercept)
                                             0.324          0.126        2.572     0.010

                log(E)
                                             -0.237         0.024        -10.009   <0.001

      Time 1 week (reference)
                                            (0.000)

            Time 2 weeks
                                             0.235          0.135        1.734     0.083

           Time 1 month
                                             0.357          0.133        2.688     0.007




                                                                                            50
Table 7: Median WTA estimates for eight digital goods categories

    Category         WTA/yea   WTA/year             95% CI              95% CI           n
                      r 2016    2017                 2016                2017

                                          lower         upper      lower     upper


All Search Engines   $14,760    $17,530   $11,211       $19,332    $13,947   $22,080   8,074

    All Email         $6,139    $8,414    $4,844        $7,898     $6,886    $10,218   9,102

    All Maps          $2,693    $3,648    $1,897        $3,930     $2,687    $5,051    7,515

    All Video         $991      $1,173     $813         $1,203      $940     $1,490    11,092

 All E-Commerce       $634       $842      $540          $751       $700     $1,020    11,051

 All Social Media     $205       $322      $156          $272       $240      $432     6,023

  All Messaging       $135       $155      $98           $186       $114      $210     6,076

    All Music         $140       $168      $112          $173       $129      $217     6,007




                                                                                                51
Figure 1: Share of information sector’s contribution to GDP (Source: BEA)




                                                                            52
Figure 2: Consumer surplus and revenue for classic goods such as cars




                                                                        53
Figure 3: Consumer surplus and revenue for purely digital goods




                                                                  54
Figure 4: Consumer surplus and revenue for transition goods such as encyclopedias




                                                                                    55
Figure 5: WTA demand curves for Facebook in 2016 and 2017




                                                            56
Figure 6: Comparison of demand curves estimated by BDM lottery and SBDC survey




                                                                                 57
Figure 7: Assessment of hypothetical bias for Facebook




                                                         58
Figure 8: Effects of scale of the estimates on logit function




                                                                59
Figure 9: Effects of required abstinence time on the probability to keep Facebook




                                                                                    60
Figure 10: Example of Google Consumer Surveys




                                                61
Figure 11: WTA demand curves comparing 2016 (solid line) and 2017 (dashed line) for the

most widely used categories of digital goods




                                                                                          62
Figure 12: WTA demand curves for breakfast cereal




                                                    63
Figure 13: Example of a best-worst scaling survey question




                                                             64
Figure 14: (Dis-)Utility according to best-worst scaling




                                                           65
Appendix

Table A.1: Full estimation model for Facebook study

                                          beta        Std. Error     z         p



(Intercept)                               1.178         0.135      8.726     <0.001

log(E)                                   -0.449         0.034      -13.147   <0.001


IC                                        0.022         0.184      0.119     0.905

IC*log(E)                                 0.140         0.045      3.076     0.002

Year_2017                                -0.097         0.208      -0.465    0.642

Year_2017*log(E)                         -0.039         0.054      -0.721    0.471

Year_2017*IC                              0.386         0.295      1.310     0.190

IC*Year_2017*log(E)                      -0.062         0.074      -0.838    0.402




                                                                                      66
Table A.2: Effect of experimentally varied time frame

                                            beta        Std. Error     z         p


(Intercept)                                -1.650         0.060      -27.550   <0.001


log(T)                                      0.137         0.021      6.419     <0.001


log(T)^2                                    0.025         0.005      5.520     <0.001




                                                                                        67
Table A.3: Estimation results of binary logit model comparing Peanut Labs (non-incentive

compatible group) and GCS

                                    beta      Std. Error        z            p

              (Intercept)
                                   0.579        0.114         5.091        <0.001

                 log(E)
                                   -0.374       0.029        -12.686       <0.001

                 GCS
                                   0.002        0.168         0.011        0.991

              GCS*log(E)
                                   0.031        0.043         0.715        0.474




                                                                                           68
Table A.4: Estimated logistic functions for eight widely used categories of digital goods

                        E-commerce   Email       Maps        Messaging   Music       Search      Social      Video



(Intercept)             2.108***     2.49***     2.12***     1.555***    1.669***    2.587***    1.827***    2.626***



log(E)                  -0.351***    -0.342***   -0.316***   -0.234***   -0.362***   -0.313***   -0.282***   -0.345***



Year_2017               -0.013       0.033       0.028       0.054       -0.309.     -0.135      0.195       -0.279.



Year_2017*log(E)        0.014        -0.002      0.009       -0.003      0.084*      0.016       -0.012      0.057*



Age 18-24 (reference)   0.000        0.000       0.000       0.000       0.000       0.000       0.000       0.000



Age 25-34               0.113        0.012       0.125       -0.257**    -0.022      -0.008      -0.175.     -0.092



Age 35-44               0.295***     0.096       0.339**     -0.2.       0.025       0.171.      0.001       -0.181*



Age 45-54               0.359***     0.472***    0.309**     -0.254*     -0.174      0.159       0.096       -0.301***



Age 55-64               0.401***     0.684***    0.255*      -0.295**    -0.314**    0.382***    -0.119      -0.588***



Age 65+                 0.282**      1.089***    0.053       -0.338**    -0.552***   0.518***    -0.078      -0.555***



Age Unknown             -0.035       0.195       -0.108      0.078       0.308       0.248       0.013       0.096



Gender Female           0.000        0.000       0.000       0.000       0.000       0.000       0.000       0.000
(reference)


Gender Male             -0.099*      -0.117*     -0.099.     -0.355***   -0.023      -0.204***   -0.486***   -0.03



Gender Unknown          0.192        0.095       0.14        -0.669***   -0.582**    -0.209      -0.55*      -0.509***



Income $0-$24.999       0.000        0.000       0.000       0.000       0.000       0.000       0.000       0.000
(reference)


Income $25,000-         -0.073       0.101       0.092       0.105       0.051       0.293**     -0.038      -0.074
$49,999




                                                                                                                        69
Income $50,000-                 -0.025              0.297**   0.29**     -0.058   0.02      0.348***   -0.039    0.019
$74,999


Income $75,000-                 -0.018              0.143     0.405**    0.131    -0.083    0.479***   -0.134    0.004
$99,999


Income $100,000-                -0.012              0.036     0.992***   -0.101   0.357     0.435*     0.051     0.046
$149,999


Income $150,000+                0.344               0.026     0.843.     -0.503   -0.17     0.888.     0.373     -0.183



Income “Prefer not to           0.046               -0.562*   -0.068     -0.018   -0.023    0.399.     -0.148    0.034
say”


Income Unknown                  -0.154              0.149     -0.093     -0.354   0.094     0.146      -0.517.   -0.276



Urban Density Rural             0.000               0.000     0.000      0.000    0.000     0.000      0.000     0.000
(reference)


Urban Density                   0.064               0.101     0.073      -0.051   0.197*    -0.007     0.112     0.072
Suburban


Urban Density Urban             0.024               0.218**   0.141.     -0.029   0.37***   0.137.     0.015     0.094



Urban Density                   -0.038              -0.067    0.227      0.269    0.17      0.343*     0.128     -0.273.
Unknown

***
      p < 0.001, ** p < 0.01, * p < 0.05, . < 0.1




                                                                                                                           70
Table A.5: Best-worst scaling estimation results

Good                                         Utility   Str. Error   WTP implied from demand
                                                                    function

No toilets in my home for 1 year             -4.331    0.139        $346,345.39


Earning $20,000 less for 1 year              -3.540    0.144        $18,079.67

Earning $10,000 less for 1 year              -3.424    0.123        $11,729.62


Earning $5,000 less for 1 year               -3.382    0.161        $10,023.70


No access to all Internet for 1 year         -3.373    0.123        $9,694.25

No access to personal computers for 1 year   -2.870    0.134        $1,482.92

Earning $1000 less for 1 year                -2.839    0.117        $1,323.45

Not meeting friends in person for 1 year     -2.725    0.116        $866.24

No TVs in my home for 1 year                 -2.647    0.116        $645.66

No access to all search engines for 1 year   -2.610    0.115        $563.80

No access to all email services for 1 year   -2.592    0.115        $525.43


No access to a smartphone for 1 year         -2.542    0.115        $437.16

Earning $500 less for 1 year                 -2.371    0.114        $230.40

No access to online shopping for 1 year      -1.967    0.113        $51.13

Earning $100 less for 1 year                 -1.933    0.113        $45.03

No access to online maps for 1 year          -1.756    0.113        $23.24

No access to video streaming for 1 year      -1.695    0.112        $18.56


No access to Facebook for 1 year             -1.654    0.112        $15.91

No access to music streaming for 1 year      -1.587    0.112        $12.36

Earning $10 less for 1 year                  -1.565    0.112        $11.41

No breakfast cereal for 1 year               -1.307    0.113        $4.36



                                                                                              71
No access to airline travel for 1 year          -1.287   0.112   $4.04

Earning $5 less for 1 year                      -1.254   0.127   $3.58

No access to public transportation for 1 year   -1.120   0.113   $2.17

Earning $1 less for 1 year                      -1.097   0.128   $1.99

No access to Wikipedia for 1 year               -1.016   0.112   $1.47

No access to Instagram for 1 year               -0.754   0.114   $0.55

No access to all ride-sharing services for 1    -0.621   0.115   $0.34
year

No access to Twitter for 1 year                 -0.621   0.114   $0.34

No access to Skype for 1 year                   -0.586   0.114   $0.30


No access to Snapchat for 1 year                -0.474   0.116   $0.19

No access to LinkedIn for 1 year                -0.415   0.115   $0.16


No access to Uber for 1 year                    -0.326   0.117   $0.11

No access to WhatsApp for 1 year (reference)    0.000            $0.03




                                                                         72
Figure A.1: Example of Incentive Compatible (IC) Questionnaire for Facebook SBDC question

(for E = $80)




                                                                                       73
Figure A.2: Assessment of selection bias




                                           74
