                               NBER WORKING PAPER SERIES




          ENDOGENOUS STRATIFICATION IN RANDOMIZED EXPERIMENTS

                                         Alberto Abadie
                                       Matthew M. Chingos
                                         Martin R. West

                                       Working Paper 19742
                               http://www.nber.org/papers/w19742


                      NATIONAL BUREAU OF ECONOMIC RESEARCH
                               1050 Massachusetts Avenue
                                 Cambridge, MA 02138
                                    December 2013




We thank Beth Akers, Josh Angrist, Matias Cattaneo, Gary Chamberlain, David Deming, Sara Goldrick-
Rab, Josh Goodman, Jerry Hausman, Guido Imbens, Max Kasy, Larry Katz, Amanda Pallais, Paul
Peterson, Russ Whitehurst, and seminar participants at Harvard/MIT for helpful comments and
discussions, and Jeremy Ferwerda for developing estrat (available at SSC), a Stata package that
calculates the leave-one-out and repeated split sample endogenous stratification estimators considered
in this study. The views expressed herein are those of the authors and do not necessarily reflect the
views of the National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2013 by Alberto Abadie, Matthew M. Chingos, and Martin R. West. All rights reserved. Short sections
of text, not to exceed two paragraphs, may be quoted without explicit permission provided that full
credit, including © notice, is given to the source.
Endogenous Stratification in Randomized Experiments
Alberto Abadie, Matthew M. Chingos, and Martin R. West
NBER Working Paper No. 19742
December 2013, Revised April 2014
JEL No. C01,C21,C9

                                            ABSTRACT

Researchers and policy makers are often interested in estimating how treatments or policy interventions
affect the outcomes of those most in need of help. This concern has motivated the increasingly common
practice of disaggregating experimental data by groups constructed on the basis of an index of baseline
characteristics that predicts the values that individual outcomes would take on in the absence of the
treatment. This article shows that substantial biases may arise in practice if the index is estimated,
as is often the case, by regressing the outcome variable on baseline characteristics for the full sample
of experimental controls. We analyze the behavior of leave-one-out and repeated split sample estimators
and show they behave well in realistic scenarios, correcting the large bias problem of the full sample
estimator. We use data from the National JTPA Study and the Tennessee STAR experiment to demonstrate
the performance of alternative estimators and the magnitude of their biases.


Alberto Abadie                                     Martin R. West
John F. Kennedy School of Government               Harvard Graduate School of Education
Harvard University                                 6 Appian Way, Gutman 454
79 JFK Street                                      Cambridge, MA 02138
Cambridge, MA 02138                                martin_west@gse.harvard.edu
and NBER
alberto_abadie@harvard.edu

Matthew M. Chingos
Brown Center on Education Policy
Brookings Institution
1775 Massachusetts Ave. NW
Washington, DC 20036
mchingos@brookings.edu
                                      1. Introduction

Recent years have seen rapid growth in the use of randomized experiments in social sci-
ence research. In part, this has been motivated by the “credibility revolution” in which
researchers have devoted much attention to the study of the conditions that allow estima-
tion of treatment effects (Angrist and Pischke 2010; Murnane and Willett 2011). The main
advantage of a large and well-executed randomized experiment is that the researcher can
confidently rule out the possibility that unobserved differences between the treatment and
control groups could explain the study’s results.
   In addition to allowing estimation of average treatment effects, experiments also make
it possible to obtain unbiased estimates of treatment effects for subgroups. Subgroup
treatment effects are of particular interest to policymakers seeking to target policies on those
most likely to benefit. As a general rule, subgroups must be created based on characteristics
that are either immutable (e.g., race) or observed before randomization (e.g., on a baseline
survey) so that they could not possibly have been affected by the treatment.
   However, many researchers and policy makers are interested in estimating how treat-
ments affect those most in need of help, that is, those who would attain unfavorable out-
comes in the absence of the treatment. Treatment parameters of this nature depend on the
joint distribution of potential outcomes with and without treatment, which is not identified
by randomization (see, e.g., Heckman et al., 1997). A solution to this problem is to com-
bine baseline characteristics into a single index that reflects each participant’s predicted
outcome without treatment, and conduct separate analysis for subgroups of participants
defined in terms of intervals of the predicted outcome without treatment.
   A well-known implementation of this idea is the use of data on out-of-sample untreated
units to estimate a prediction model for the outcome variable, which can then be applied to
predict outcomes without treatment for the experimental units. This approach is common
in medical research, where validated risk models are available to stratify experimental
subjects based on their predicted probability of certain health outcomes (Kent and Hayward
2007).


                                               1
       However, experimental studies in the social sciences often lack externally validated
models that can be employed to predict the outcomes that experimental units would attain
in the absence of the treatment. A potential approach to this problem that is gaining
popularity among empirical researchers is to use in-sample information on the relationship
between the outcome of interest and covariates for the experimental controls to estimate
potential outcomes without treatment for all experimental units. We call this practice
endogenous stratification, because it uses in-sample data on the outcome variable to stratify
the sample.
       Endogenous stratification is typically implemented in practice by first regressing the
outcome variable on baseline characteristics using the full sample of experimental controls,
and then using the coefficients from this regression to generate predicted potential outcomes
without treatment for all sample units.
       Unfortunately, as we show below, this procedure generates estimators of treatment
effects that are substantially biased, and the bias follows a predictable pattern: results
are biased upward for individuals with low predicted outcomes and biased downward for
individuals with high predicted outcomes.
       This bias pattern matches the results of several recent experimental studies that use this
procedure and estimate strong positive effects for individuals with low predicted outcomes
and, in some cases, negative effects for individuals with high predicted outcomes. For
example, a 2011 working paper by Goldrick-Rab et al. reports that a Wisconsin need-
based financial aid program for post-secondary education had no overall impacts on college
enrollment or college persistence among eligible students as a whole. Looking separately at
subgroups based on predicted persistence, however, the study finds large positive effects on
enrollment after three years for students in the bottom third of predicted persistence and
almost equally large negative effects for students in the top third of predicted persistence.1
   1
    Goldrick-Rab et al. (2011) report that, for students in the bottom third group of predicted persistence,
grant receipt was associated with an increase of 17 percentage points in enrollment three years after they
started college. Conversely, for students in the top third group of predicted persistence, grant receipt
was associated with a decrease of 15 percentage points in enrollment three years after the start of college.
These findings were characterized by the authors as “exploratory” but received widespread media coverage,
including articles in the Chronicle of Higher Education, Inside Higher Education, and Education Week. In


                                                     2
A 2011 working paper by Dynarski et al. analyzing long-term impacts of the Project STAR
experiment similarly finds that assignment to a small class in grades K-3 increased college
enrollment rates among the quintile of students with the lowest ex-ante probability to enroll
by 11 percentage points, but had no impact on students in the top four quintiles. Pane et al.
(2013) report experimental estimates of the effects of a technology-based algebra curriculum
on the test scores of middle and high school students disaggregated by quintiles of predicted
test scores. For middle school students exposed to the program in the first year of its
implementation they find “potentially moderately large positive treatment effects in the
lowest quintile and small negative effects of treatment in the highest two quintiles”. Hemelt
et al. (2012) find no significant average impacts in a experimental evaluation of the effects
of two elementary school interventions on college enrolment or degree receipt. They report,
however, significant positive impacts on two-year college enrollment for both interventions
and on associate’s degree completion for one of the interventions when they restrict the
sample to students in the bottom quartile of in-sample predicted probability of college-
attendance. Rodriguez-Planas (2012) reports that a mentoring program for adolescents
reduced risky behavior and improved educational attainment for students in the top half
of the risk distribution but increased risky behavior in the bottom half.2
    Endogenous stratification also plays a supporting role in Angrist and Lavy’s (2009)
experimental evaluation of a cash incentive program aimed at increasing matriculation
certification rates for Israeli high school students. In order to test whether the program
was most effective for girls on the certification margin, the researchers first group students
by baseline test scores. They also, however, report results for students grouped by ex-ante
certification probability based on a broader set of background characteristics as “a check
on the notion that high lagged scores identify students who have a shot at classification”
a related paper on the design of randomized experiments, Harris and Goldrick-Rab (2012) discuss potential
explanations for the unexpected heterogeneity in their impact estimates based on full-sample endogenous
stratification.
    2
      We should note that endogenous stratification estimates do not appear in the published versions of two
of the studies described here, see Dynarski et al. (2013) and Hemelt et al. (2013), or in a subsequent working
paper on the grant program evaluated in Goldrick-Rab et al. 2011 by the same authors, see Goldrick-Rab
et al. 2012.



                                                      3
(p. 1396).
       The possibility of bias arising from endogenous stratification has been previously ac-
knowledged in the evaluation literature (see, e.g., Peck, 2003), in statistics (Hansen, 2008),
and in economics (Sanbonmatsu et al., 2006, and Giné et al, 2012), but the size and sig-
nificance of the bias in realistic evaluation settings is not well understood.3 A deceivingly
comforting property of the bias is that it vanishes as sample size increases, under weak reg-
ularity conditions. However, as we demonstrate below using data from the National JTPA
Study and the Tennessee STAR experiment, biases resulting from endogenous stratification
can completely alter the quantitative and qualitative conclusions of empirical studies.
       In the remainder of this article, we first describe in more detail the increasingly popular
practice of stratifying experimental data by groups constructed on the basis of the pre-
dicted values from a regression of the outcome on baseline covariates for the full sample of
experimental controls. We next explain why this method generates biases and describe the
direction of those biases. We then describe leave-one-out and repeated split sample proce-
dures that generate consistent estimators and show that the biases of these estimators are
substantially lower than the bias of the full sample estimator in two realistic scenarios. We
use data from the National JTPA Study and the Tennessee STAR experiment to demon-
strate the performance of endogenous stratification estimators and the magnitude of their
biases. We restrict our attention to randomized experiments, because this is the setting
where endogenous stratification is typically used. However, similarly large biases may arise
from endogenous stratification in observational studies.


             2. Using Control Group Data to Create Predicted Outcomes

We begin by describing in detail the endogenous stratification method outlined above, which
aims to classify study participants into groups based on their predicted value of the outcome
variable in the absence of the treatment. Suppose that the sample consists of N observations
   3
    Hausman and Wise (1977) and Hausman and Wise (1981), from which we borrow the term “endogenous
stratification”, study the related problem of biased sampling in randomized experiments. Altonji and Segal
(1996) study biases that arise in the context of efficient generalized methods of moments estimation for
reasons that are related to those that explain the bias of the full sample endogenous stratification estimator.


                                                      4
of the triple (y, w, x), where y is an outcome variable, w is the treatment, and x is a vector
of baseline characteristics. When the object of interest is the average treatment effect,
which in a randomized experiment is equal to τ = E[y|w = 1] − E[y|w = 0], researchers
typically compare sample average outcomes for the treated and the control groups:
                                        N
                                        X                         N
                                                                  X
                                             yi wi                    yi (1 − wi )
                                        i=1                       i=1
                                 τb =     N
                                                          −         N
                                                                                     .
                                         X                         X
                                                 wi                     (1 − wi )
                                           i=1                    i=1

As discussed above, researchers sometimes aim to compare treated and non-treated after
stratifying on a predictor of the outcome in the absence of the treatment. To our knowl-
edge, most studies that use endogenous stratification implement it roughly as follows:

 (1) Regress the outcome variable on a set of baseline characteristics using the control
     group only. The regression coefficients are:
                                     N
                                                                    !−1    N
                                     X                                     X
                             β
                             b=            xi (1 − wi )x0i                       xi (1 − wi )yi .
                                     i=1                                   i=1


 (2) Use the estimated coefficients to generate predicted outcome values for all participants
     (both treatment and control groups), x0i β.
                                              b

 (3) Divide participants into groups based on their predicted outcomes. Typically, unit
     i is assigned to group k if x0i β
                                     b falls in some interval delimited by ck−1 and ck . The

     interval limits may be fixed or could be quantiles of the empirical distribution of
     x0i β.
         b Many authors use a three-bin classification scheme of low, medium, and high

     predicted outcomes.

 (4) Estimate treatment effects for each of the subgroups,
                             N
                             X                                            N
                                                                          X
                                 yi I[wi =1,ck−1 <x0 β≤c
                                                     b k]                     yi I[wi =0,ck−1 <x0 β≤c
                                                                                                  b k]
                                                          i                                        i
                             i=1                                          i=1
                     τbk =     N
                                                                      −     N
                                                                                                         ,
                              X                                            X
                                   I[wi =1,ck−1 <x0 β≤c
                                                    b k]                         I[wi =0,ck−1 <x0 β≤c
                                                                                                  b k]
                                                      i                                        i
                             i=1                                           i=1



                                                              5
      where IA is the indicator function that takes values one if event A is realized, and
      value zero otherwise. Alternatively, treatment effect estimates could be computed
      after controlling for a set of covariates using regression.

   For example, Goldrick-Rab et al. (2011) in their study of the impact of a need-based
grant regress college persistence on baseline characteristics using only observations from
the control group, generate predicted probabilities of college persistence for all students,
classify students into three equal-sized groups based on their ex-ante predicted probability,
and then estimate treatment effects for each of the three groups.
   This is a simple and direct approach to stratification, which has great intuitive appeal.
Moreover, it is easy to show that under usual regularity conditions, τbk converges to

             τk = E[y|w = 1, ck−1 < x0 β ≤ ck ] − E[y|w = 0, ck−1 < x0 β ≤ ck ].

As we will see next, however, τbk is biased in finite samples, and the bias follows a predictable
pattern.
   To simplify the exposition, suppose that predicted outcomes are divided into three
groups (low, medium, high). Let β = (E[xx0 |w = 0])−1 E[xy|w = 0] be the population
               b and let ei = yi − x0 β be the regression error. In a finite sample, untreated
counterpart of β,                   i
                                                                                        b < x0 β,
observations with large negative values for ei tend to be over-fitted, so we expect x0i β    i

which pushes these observations towards the lower interval of predicted outcomes. This
creates a negative bias in the average outcome among control observations that fall into
the lower interval for x0i β
                           b and, therefore, a positive bias in the average treatment effect

estimated for that group. Analogously, average treatment effect estimators for the upper
intervals of predicted outcomes are biased downward. Endogenous stratification results in a
predictable pattern: average treatment effect estimators are biased upward for individuals
with low predicted outcomes and biased downward for individuals with high predicted
outcomes. As we will demonstrate below, because the finite sample bias of the endogenous
stratification estimator is created by over-fitting, this bias tends to be more pronounced
when the number of observations is small and the dimensionality of xi is large.


                                               6
       A natural solution to the over-fitting issue is provided by leave-one-out estimators.4 Let
                                                       !−1
                                   X                       X
                         β
                         b (−i) =       xi (1 − wi )x0         xi (1 − wi )yi ,
                                                               i
                                         j6=i                          j6=i


be the regression coefficients estimators that discard observation i. Over-fitting is precluded
by not allowing the outcome, yi , of each observation to contribute to the estimation of its
own predicted value, x0i β
                         b . Because only untreated observations are employed in the
                           (−i)

estimation of β
              b (−i) and β,
                         b if i is a treated observation then β
                                                              b (−i) = β.
                                                                       b We consider the

following leave-one-out estimator of τk :
                               N
                               X                                   N
                                                                   X
                                   yi I[wi =1,ck−1 <x0 β≤c
                                                       b k]            yi I[wi =0,ck−1 <x0 βb (−i) ≤ck ]
                                                       i                                      i
                               i=1                                 i=1
                  τbkLOO   =     N
                                                               −     N
                                                                                                           .
                                X                                   X
                                     I[wi =1,ck−1 <x0 β≤c
                                                      b k]               I[wi =0,ck−1 <x0 βb (−i) ≤ck ]
                                                   i                                      i
                               i=1                                 i=1


Under weak assumptions, it can be seen that both τbk and τbkLOO are consistent estimators
of τk . Moreover, τbk and τbkLOO have the same large sample distribution.5 However, we show
in section 4 that τbk is substantially biased in two realistic scenarios, while τbkLOO is not.6
       Another way to avoid over-fitting is sample splitting. We consider a repeated split
sample estimator. In each repetition, m, the untreated sample is randomly divided into
two groups, which we will call the prediction and the estimation groups. Let vim = 0 if
untreated observation i is assigned the prediction group in repetition m, and vim = 1 if it
is assigned to the estimation group. In each repetition, m, we estimate β using only the
observations in the prediction group:
                           N
                                                               !−1   N
                           X                                         X
                 β
                 b =
                   m             xi (1 − wi )(1 − vim )x0i                 xi (1 − wi )(1 − vim )yi .
                           i=1                                       i=1

   4
      This is the approach followed in Sanbonmatsu et al. (2006). Harvill et al. (2013) propose a variant of
this approach based on 10-fold cross-validation.
    5
      Proofs of these and other formal statements made in this paper are provided in Appendix 1.
    6
      A separate issue in the estimation of τk is that first step estimation of β affects the large sample
distribution of the estimator (see Appendix 1 for a derivation of the large sample distribution of τbk and
τbkLOO ). The contribution of the estimation of β to the variance of τbk has been ignored in empirical practice.




                                                           7
For each repetition, m, the split sample estimator of τk is
                           N
                           X                                        N
                                                                    X
                               yi I[wi =1,ck−1 <x0 βb m ≤ck ]           yi I[wi =0,vim =1,ck−1 <x0 βb m ≤ck ]
                                                      i                                               i
                  SS       i=1                                      i=1
                τbkm   =     N
                                                                −     N
                                                                                                                .
                            X                                        X
                                 I[wi =1,ck−1 <x0 βb m ≤ck ]              I[wi =0,vim =1,ck−1 <x0 βb m ≤ck ]
                                                  i                                               i
                           i=1                                      i=1
                  SS
We then average τbkm over M repetitions to obtain the repeated split sample estimator:
                                                               M
                                                            1 X SS
                                                 τbkRSS   =      τb .
                                                            M m=1 km
The repeated split sample estimator is asymptotically unbiased and Normal but, unlike
the leave-one-out estimator, its large sample distribution does not coincide with the large
sample distribution of the full-sample endogenous stratification estimator. For large M ,
however, the difference between the large sample distribution of the repeated split sample
estimator and the large sample distribution of the full-sample and leave-one-out estimators
is small.7,8
       In the next section, we apply the estimators described above to the analysis of data
from two well-know experimental studies: the National JTPA Study and the Tennessee
Project STAR experiment.


               3. Evidence of Large Biases in Two Actual Applications

To demonstrate the performance of the estimators described in the previous section and the
magnitude of their biases in realistic scenarios we use data from two randomized evaluations:
the National JTPA Study, an evaluation of a vocational training program in the U.S., and
the kindergarten cohort of the Tennessee Project STAR class-size experiment.


3.1. The National JTPA Experiment

We first examine data from the National JTPA Study. The National JTPA Study was a
large experimental evaluation of a job training program commissioned by the U.S. Depart-
   7
     This is proven in Appendix 1.
   8
     The methods described in this section do not exhaust the possible approaches to the bias of the
full-sample endogenous stratification estimator. Bootstrap/jackknife bias corrections on τbk and shrinkage
estimation of β are potentially fruitful approaches that we are starting to explore.

                                                                8
ment of Labor in the late 1980’s. The National JTPA Study data have been extensively
analyzed by Orr et al. (1996), Bloom, et al. (1997), and many others. The National
JTPA Study randomized access to vocational training to applicants in 16 service delivery
areas, or SDAs, across the U.S. Randomized assignment was done after applicants were
deemed eligible for the program and recommended to one of three possible JTPA service
strategies: on the job training/job search assistance, classroom training, and other ser-
vices. Individuals in the treatment group were provided with access to JTPA services,
and individuals in the control group were excluded from JTPA services for an 18-month
period after randomization. We use data for the sample of male applicants recommended
to the job training/job search assistance service strategy, and discard three SDAs with few
observations. Our sample consists of 1681 treated observations and 849 untreated obser-
vations, for a total of 2530 observations in 13 SDAs.9 In this example, wi is an indicator
of a randomized offer of JTPA services, yi is nominal 30-month earnings in U.S. dollars
after randomization, and xi includes age, age squared, marital status, previous earnings,
indicators for having worked less than 13 weeks during the year previous to randomization,
having a high-school degree, being African-American, and being Hispanic, as well as SDA
indicators.
      Table 1 reports estimates for the JTPA sample. The first row reports two treatment
effect estimates. The “unadjusted” estimate is the difference in outcome means between
treated and controls. The “adjusted” estimate is the coefficient on the treatment indicator
in a linear regression of the outcome variable, yi , on the treatment indicator, wi , and the
covariates, xi , listed above. The unadjusted estimate suggests a $1516 effect on 30-month
earnings. This estimate is significant at the 10 percent level. Regression adjustment reduces
the point estimate to $1207, which becomes marginally non-significant at the 10 percent
level. The rest of Table 1 reports average treatment effects by predicted outcome group.
The first set of estimates correspond to τbk , the full-sample endogenous stratification esti-
mator. This estimator produces a large and significant effect for the low predicted outcome
group. The unadjusted estimate is $2380 and significant at the 5 percent level. This rep-
  9
      See Appendix 2 for detailed information on sample selection and estimation methods.

                                                    9
resents a 12.6 percent effect on 30-month earnings, once we divide it by the average value
of 30-month earnings among the experimental controls. It also represents an effect that is
57 percent higher than the corresponding unadjusted estimated for the average treatment
effect in the first row of the table. The adjusted estimate is $2012, similarly large, and
significant at the 10 percent level. For the high predicted outcome group, the estimates
are also large, but not statistically significant at conventional test levels. For the middle
predicted outcome group, the estimates are negative, but of moderate magnitude and not
statistically significant. All in all, the full-sample endogenous stratification estimates pro-
vide a much more favorable picture of JTPA effectiveness relative to the average treatment
effects reported on the first row. Now, the bulk of the effect seems to be concentrated on
the low predicted outcome group, precisely the one in most need of help, with more diffuse
effects estimated for the middle and high predicted outcome groups.
       The next two sets of estimates reported in Table 1 correspond to the leave-one-out
estimator, τbkLOO , and the repeated split sample estimator τbkRSS , with number of repetitions,
M , equal to 100. These two estimators, which avoid over-fitting bias arising from the
estimation of β, produce results that are substantially different than those obtained with
the full-sample endogenous stratification estimator, τbk . Relative to the τbk estimates, the
τbkLOO and τbkRSS estimates are substantially smaller for the low predicted outcome group,
and substantially larger for the high predicted outcome group. For the high predicted
outcome group we obtain unadjusted estimates of $3647 (leave-one-out) and $3569 (split
sample) both significant at the 5 percent level, and adjusted estimates of $3118 (leave-one-
out) and $2943 (split sample) significant at the 10 percent and 5 percent levels respectively.
The estimates for the low and middle predicted outcome groups are small in magnitude
and not statistically significant. These results place the bulk of the treatment effect on
the high predicted outcome group and do not provide substantial statistical evidence of
beneficial effects for the low and middle predicted outcome groups.10 The comparison of
  10
    This is loosely consistent with the findings in Abadie, Angrist, and Imbens (2002) who report large
JTPA effects at the upper tail of the distribution of earnings for male trainees, and no discernible effects
at the middle or lower parts of the distribution.



                                                    10
estimates produced with the full sample endogenous stratification estimator and the leave-
one-out and split sample estimators suggests that the over-fitting bias of the full sample
endogenous stratification estimator is of substantial magnitude and dramatically changes
the qualitative and quantitative interpretations of the results.
       As a further check on the magnitude of endogenous stratification biases in the analysis
of the National JTPA Study data, Table 1 reports a last set of treatment effects estimates,
which are stratified using data on earnings before randomization. The National JTPA
Study data include individual earnings during the 12 months before randomization. We
use the sorting of the experimental subjects in terms of pre-randomization earnings to
approximate how the experimental subjects would have been sorted in terms of earnings in
the absence of the treatment. We construct the estimator τbkP REV in the same way as τbk but
using previous earnings, instead of predicted earnings, to divide the individuals into three
groups of approximately equal size. Notice that, because previous earnings is a baseline
characteristic, τbkP REV is not affected by over-fitting bias. As shown on the bottom of Table
1, stratification on previous earning produces results similar to those obtained with τbkLOO
and τbkRSS : large and significant effects for the high predicted outcome group and smaller
and non-significant effects for the middle and low predicted outcome groups.


3.2. The Tennessee Project STAR Experiment

Our second example uses data from the Tennessee Project STAR class-size study. In the
Project STAR experiment, students in 79 schools were randomly assigned to small, regular-
size, and regular-size classes with a teacher’s aide. Krueger (1999) analyzes the STAR data
set and provides detailed explanations of the STAR experiment. For our analysis, we use
the 3764 students who entered the study in kindergarten and were assigned to small classes
or to regular-size classes (without a teacher’s aide). Our outcome variable is standardized
end-of-the-year kindergarten math test scores.11 The covariates are indicators for African-
American, female, eligibility for the free lunch program, and school attended. We discard
  11
    Standardized test scores are computed dividing raw test scores by the standard deviation of the dis-
tribution of the scores in regular-size classes.



                                                  11
observations with missing values in any of these variables.
       Results for the STAR experiment data are reported in Table 2. The adjusted and unad-
justed estimators of the average treatment effect on the first row of Table 2 show positive
and significant effects. Using a simple difference in means, the effect of small classes is es-
timated as 0.1659 of the regular class standard deviation in math test scores, and 0.1892 of
the same standard deviation when we use a regression-adjusted estimator.12 In both cases,
the estimates are significant at the 5 percent level. For the low and middle predicted out-
comes groups, the full-sample endogenous stratification estimator, τbk , produces estimates
that are positive and roughly double the average treatment effects estimates on the first
row of the table. Counter-intuitively, however, the full sample endogenous stratification
estimates for the high predicted outcome group are negative and significant. They seem to
suggest that being assigned to small classes was detrimental for those students predicted
to obtain high math scores if all students had remained in regular-size classes. We deem
this result counter-intuitive because it implies that reductions in the student/teacher ratio
have detrimental effects on average for a large group of students. Notice that the magni-
tudes of the negative effects estimated for high predicted outcome group are substantial:
smaller, but not far from the positive average treatment effects reported in the first row of
the table. We will see that the large and significant negative effect for the high predicted
outcome group disappears when the leave-one-out or the repeated split sample procedures
are used for estimation. Indeed, the leave-one-out and repeated split sample estimates on
the two bottom rows of Table 2 suggest positive, significant, and large effects on the low
and middle predicted outcome groups and effects of small magnitude and not reaching sta-
tistical significance at conventional test levels for the high predicted outcome group. Like
for the JTPA, the qualitative and quantitative interpretations of the STAR experiment
results change dramatically when the leave-one-out or the repeated split sample estimators
  12
    To be consistent with much of the previous literature on the STAR experiment, we report both
regression-adjusted and unadjusted estimates. Because the probability of assignment to a small class
varied by school in the STAR experiment, the regression-adjusted estimator is most relevant in this set-
ting. Like in Krueger (1999), however, covariate regression adjustment does not substantially change our
estimates.



                                                  12
are used instead of the full-sample endogenous stratification estimator.
      In this section, we have used data from two well-known and influential experimental
studies to investigate the magnitude of the distortion that over-fitting may induce on en-
dogenous stratification estimators. In the next section, we use Monte Carlo simulations to
assess the magnitude of the biases of the different estimators considered Section 2. To keep
the exercise as realistic as possible, in two of our simulations we choose data generating
processes that mimic the features of the JTPA and STAR data sets.


       4. Simulation Evidence on the Behavior of Endogenous Stratification
                                    Estimators

This section reports simulation evidence on the finite sample behavior of endogenous strat-
ification estimators. We run Monte Carlo simulations in three different settings. In the
first two Monte Carlo simulations, we make use of the JTPA and STAR data sets to assess
the magnitudes of biases and other distortions to inference in realistic scenarios. The third
and fourth Monte Carlo simulations use computer-generated data to investigate how the
bias of endogenous stratification estimators changes when the sample size or the number
of covariates changes.
      In the JTPA-based simulation, we first use the JTPA control units to estimate a two-
part model for the distribution of earnings conditional on the covariates of the adjusted
estimates in Table 1. The two-part model consists of a Logit specification for the probability
of zero earnings and a Box-Cox model for positive earnings.13 In each Monte Carlo iteration
we draw 2530 observations, that is, the same number of observations as in the JTPA sample,
from the empirical distribution of the covariates in the JTPA sample. Next, we use the
estimated two-part model to generate earnings data for each observation in the Monte
Carlo sample. Then, we randomly assign 1681 observations to the treatment group and
849 observations to the control group, to match the numbers of treated and control units
in the original JTPA sample. Finally, in each Monte Carlo iteration, we compute the full-
sample, leave-one-out, and repeated split sample endogenous stratification estimates. We
 13
      Additional details about the simulation models can be found in Appendix 2.


                                                   13
also compute the value of the unfeasible estimator, τbkU N F , obtained by stratification on the
population regression function (which can be calculated from the estimated parameters of
the two-part model by simulation). We conduct a total of 10000 Monte Carlo iterations.
       Figure 1 reports the Monte Carlo distributions of the endogenous stratification esti-
mators that divide the experimental sample into three categories of predicted earnings of
roughly equal size (bottom third, middle third, and top third). To economize space this
figure shows only the distribution of the unadjusted estimators.14 Because assignment to
the treatment and control groups is randomized in our simulation and because the process
that generates earnings data is the same for treated and controls, it follows that the average
effect of the treatment in the simulations is equal to zero unconditionally as well as con-
ditional on the covariates. As a result, unbiased estimators should generate Monte Carlo
distributions centered around zero. The first plot of Figure 1 shows the Monte Carlo distri-
bution of the full-sample endogenous stratification estimator of average treatment effects
conditional on predicted earnings group. The pattern of the distribution of the average
treatment effect estimator for the bottom, middle, and top third predicted earnings groups
matches the directions of the biases discussed in Section 2. That is, τbk is biased upwards
for the low predicted earnings group and downwards for the high predicted earnings group.
The remaining three plots of Figure 1 do not provide evidence of substantial biases for the
leave-one-out, repeated split sample, or unfeasible estimators. These three estimators pro-
duce Monte Carlo distributions that are centered around zero for each predicted earnings
category.
       Table 3 reports biases, coverage rates for nominal 0.05 confidence intervals based on
the Normal approximation, and root mean square error (root-MSE) values for endogenous
stratification estimators in the JTPA-based Monte Carlo simulation. In addition to the
estimators considered in Figure 1, we compute a single split sample estimator, τbkSSS , which
is defined like the repeated split sample estimator but with M = 1. The full-sample
endogenous stratification estimator is subject to substantial distortions for the low and
  14
    Simulation results for unadjusted and adjusted estimators are very similar, as reflected in Tables 3 to
6 below.


                                                    14
high predicted earnings group. The magnitude of the bias in each these two groups is more
than $1000, which is substantial compared to the $1516 and $1207 unadjusted and adjusted
average effect estimates in the JTPA data. As reflected in Figure 1, the bias is positive
for the low predicted earnings group and negative for the high predicted earnings group.
Biases are uniformly small for the leave-one-out, repeated split sample, and unfeasible
estimators, but the leave-one-out estimator has higher biases than the repeated split sample
and the unfeasible estimator. Similar results emerge for coverage rates and mean square
errors. The full-sample endogenous stratification estimator produces substantially higher
than nominal coverage rates and substantially higher root-MSE than the leave-one-out and
repeated split sample estimators for the low and high predicted income categories. The
repeated split sample estimator dominates in terms of root-MSE. The single split sample
estimators produce small biases and close to nominal coverage rates, but have root-MSE
values consistently higher than the full-sample endogenous stratification estimator.
   Figure 2 and Table 4 report simulation results for the STAR-based Monte Carlo sim-
ulation. For this simulation, the data generating process is based on a linear model with
Normal errors. The model is estimated using data for STAR students in regular size classes.
The results are qualitatively identical to those obtained in the JTPA-based simulation. The
biases of τbk are around 0.05 and -0.05 for the low and medium predicted test score groups,
respectively. These are sizable magnitudes, compared to the STAR effect estimates in Table
2. Also, like in the JTPA-based simulation, for the low and high predicted outcome groups
coverage rates of the full-sample endogenous stratification estimator are heavily distorted
and root-MSE values are larger than for the leave-one-out and the repeated split sample
estimators. The repeated split sample estimator has the lowest root-MSE, and single sam-
ple splits produce root-MSE values larger than any other estimator with the exception of
the full-sample endogenous stratification estimator.
   The analysis of how average treatment effects co-vary with predicted outcomes without
the treatment can also be based on a regression equation with interaction terms, such as:

                       yi = α0 + (x0i β)α1 + wi α2 + wi (x0i β)α3 + ui ,


                                              15
where ui is a regression error orthogonal to the included regressors. A negative sign of α3
would indicate average treatment effects inversely related to x0 β. Under the data generating
processes employed in our simulations α3 is equal to zero. Figure 3 reports Monte Carlo
distributions of estimators of α3 for the JTPA-based and STAR-based simulations. The
full sample and leave-one-out estimators use β
                                             b and β
                                                   b (−i) , respectively, instead of β, and

estimate the regression equation by ordinary least squares. The unfeasible estimator uses
the true value, β. For m = 1, . . . , M , the repeated split sample estimator uses β
                                                                                   b and
                                                                                     m

average the resulting estimates of α3 over the M repetitions. Finally, we also report the
distribution of the estimator of α3 given by one-step nonlinear least squares estimation of
the regression equation above. The one-step nature of the nonlinear least squares estimator
implies that predicted outcomes are fitted to all experimental units, and not only to the
units in the control group.15 The results in Figure 3 are consistent with our previous
evidence on the performance of estimators that stratify on subgroups of predicted values.
The Monte Carlo distributions of the leave-one-out, repeated split sample, nonlinear least
squares and unfeasible estimators are all centered around zero. In contrast, the full-sample
endogenous stratification estimator of α3 is negatively biased.
       The third and fourth Monte Carlo simulations use computer generated data only. The
purpose of these simulations is to demonstrate how the bias of endogenous stratification
estimators changes with changes in the sample size and in the number of covariates. The
data generating model for the third simulation is
                                                  40
                                                  X
                                       yi = 1 +         zli + vi
                                                  l=1

for i = 1, . . . , N , where the variables zli have independent Standard Normal distributions,
and the variable vi has a independent Normal distribution with variance equal to 60. As a
result, the unconditional variance of yi is equal to 100. In each Monte Carlo simulation the
sample is divided at random into two equally-sized treated and control groups. Predicted
  15
    We thank Gary Chamberlain for suggesting this estimator. Nonlinear least squares estimation of
the regression equation above uses the normalization α0 = 0 and α1 = 1 to ensure that the regression
parameters are properly defined.



                                                  16
outcomes are computed using data for the control group to estimate

                                    yi = α + x0Ki β K + uKi

by least squares, where xKi is the (K × 1)-vector (z1i , . . . , zKi )0 , for K ≤ 40. That is, xKi
contains the values of the first K regressors in z1i , . . . , z40i . The data generating process
implies that α is equal to one, β K is a (K × 1)-vector of ones, uKi = zK+1i + · · · + z40i + vi
if K < 40 and u40i = vi . We run Monte Carlo simulations for samples sizes N = 200,
N = 1000, and N = 5000, and numbers of included regressors K = 10, K = 20, and
K = 40.
   The results are reported in Table 5. To economize space, we omit results on the single
split sample estimator and report bias results only. Coverage rate and root-MSE results are
available upon request. The magnitude of the biases in the Table 5 are easily understood
when compared to the standard deviation of the outcome, which is equal to 10. As ex-
pected, the bias of the full-sample endogenous stratification estimator is particularly severe
when the sample size is small or when the number of included regressors is large, because in
both cases significant over-fitting may occur. The increase in bias resulting from increasing
the number of regressors is particularly severe when the sample size is small, N = 200. The
biases of the leave-one-out, repeated split sample, and unfeasible estimators are negligible
in most cases and consistently smaller than the bias of the full-sample endogenous strat-
ification estimators, although the leave-one-out estimator tends to produce larger biases
than the repeated split sample and unfeasible estimators.
   The bias of the full-sample endogenous stratification estimator increases with K in spite
of the fact that, as K increases, each additional included regressor has the same explanatory
power as each of the regressors included in simulations with smaller K. Our final simulation
studies a setting where each additional included regressor has lower explanatory power than
the previously included ones. Consider:
                                               40
                                               X
                                    yi = 1 +         ρl−1 zli + ṽi ,
                                               l=1

where the variables zli have independent Standard Normal distributions as before, and the

                                                17
variable ṽi has a independent Normal distribution with a variance such that the variance
of yi is equal to 100. Table 6 reports the biases of the endogenous stratification estimators
across Monte Carlo simulations under the new data generating process (with ρ = 0.80). The
biases of the endogenous stratification estimator are larger than in the previous simulation.
Their magnitudes increase faster than in the previous simulation when the number of
included covariates increases, and decrease slower than in the previous simulation when the
number of observations increases. The biases of the leave-one-out, repeated split sample,
and unfeasible estimators are smaller and less sensitive to changes in the number of included
covariates and sample size.
          Overall, among the estimators that address the over-fitting problem of full-sample en-
dogenous stratification, the repeated split sample estimator out-performs leave-one-out in
the simulations. Moreover, the leave-one-out estimator can behave erratically in settings
where the regressors take on only a few values and the variance of ei is large.16 The single
split sample estimator has low bias and produces close-to-nominal coverage rates, but also
large dispersion induced by the reduction in sample size. The increased variance of the sin-
gle split sample estimator can make root-MSE of this estimator larger than the root-MSE
of the full-sample endogenous stratification estimator (Table 3). All in all, the repeated
split sample estimator displays the best performance in our simulations. It has low bias,
accurate coverage rates, and out-performs alternative estimators in terms of root-MSE.




     16
     This is the case, for example, in the Tennessee STAR experiment if school indicators are excluded from
the vector xi . In that case, xi only includes three indicator variables for race, gender, and eligibility for
a free lunch program. As a result, x0i β b takes on only eight different values. In this setting, over-fitting is
not an issue and the full-sample endogenous stratification estimator produces small biases in simulations.
However, the leave-one-out estimator generates large biases. The reason is that, in this setting, choosing
c1 and c2 to be the quantiles 1/3 and 2/3 of the distribution of the predicted outcomes results in a
large number of observations being located exactly at the boundaries of the values of x0i β   b that define the
predicted outcome groups. To be concrete, consider the untreated observations with x0i β        b = c1 . These
observations are classified by the full-sample endogenous stratification estimator as members of the low
predicted outcome group. However, it is easy to see that if x0i β   b = c1 , then x0 β
                                                                                   i (−i) > c1 if yi < c1 and
                                                                                     b
x0 β
 i
   b     ≤ c1 if yi ≥ c1 , which induces biases in the leave-one-out estimator.
      (−i)


                                                       18
                                     5. Conclusions

In this paper, we have argued that the increasingly popular practice of stratifying exper-
imental units on the basis of a prediction of the outcome without treatment estimated
using full sample data from the control group leads to substantially biased estimates of
treatment effects. We illustrate the magnitude of this bias using data from two well-known
social experiments: the National JTPA Study and the Tennessee STAR Class Size Exper-
iment. The full-sample endogenous stratification approach is most problematic in studies
with small sample sizes and many regressors, where the predictor of the outcome without
treatment may be severely over-fitted in the control sample. We demonstrate that alterna-
tive endogenous stratification estimators based on leave-one-out and repeated split sample
techniques display substantially improved small sample behavior in our simulations relative
to the full-sample endogenous stratification estimator.
   Some questions remain open to future research. First, the methods described in this
article do not exhaust the possible approaches to the bias of the full-sample endogenous
stratification estimator. Bootstrap/jackknife bias corrections on τbk and shrinkage estima-
tion of β are potentially fruitful approaches that we are starting to explore. In addition, a
question of interest is whether the good small-sample behavior of the repeated split sample
estimator generalizes to other settings, like the two-step generalized method of moments
setting analyzed by Altonji and Segal (1996).




                                             19
                                          Appendix 1: Proofs

Suppose that we have data from an experiment where a fraction p of experimental units are
assigned to a treatment group and a fraction 1 − p to a control group, with 0 < p < 1. Let N1
be the number of units assigned to the treatment group and N0 the number of units assigned
to the control group, with N = N0 + N1 . We will derive the limit distributions of endogenous
stratification estimators as N → ∞. For each experimental unit, i, we observe the triple uN i =
(yN i , wN i , xN i ), where wN i is a binary indicator that takes value one if observation i is in the
treatment group, and value zero otherwise, yN i is the outcome of interest for observation i, and
xN i is a vector of baseline characteristics for observation i. We conduct our analysis assuming that
the experimental units are sampled at random from some large population of interest, so the values
of (yN i , xN i ) for the treated and the non-treated can be regarded as independent i.i.d. samples of
sizes N1 and N0 from some distributions P1 and P0 , respectively. Probability statements about
u = (y, w, x) are understood to refer to the distribution induced by first sampling w at random
from a Bernoulli with parameter p and then sampling (y, x) from P1 with probability p and from
P0 with probability 1 − p. Because w is randomized, the marginal distribution of x is the same
under P1 and P0 . Let

                                 β = (E[xx0 |w = 0])−1 E[xy|w = 0].

That is, x0 β is the linear least-squares predictor of E[y|x, w = 0]. Let c be a known constant
such that Pr(x0 β ≤ c) > 0. We aim to estimate:

                           τ = E[y|w = 1, x0 β ≤ c] − E[y|w = 0, x0 β ≤ c].

This is the average effect of the treatment for individuals with x0 β ≤ c. Consider the full-sample
endogenous stratification estimator:
                               N
                               X                                   N
                                                                   X
                                   yN i I[wN i =1,x0 β≤c]
                                                      b                yN i I[wN i =0,x0 β≤c]
                                                                                          b
                                                   Ni                                  Ni
                               i=1                                 i=1
                      τb(β)
                         b =
                                 N
                                                               −     N
                                                                                              ,
                                 X                                   X
                                       I[wN i =1,x0                        I[wN i =0,x0
                                                  N i β≤c]                            N i β≤c]
                                                      b                                   b
                                 i=1                                 i=1

where β
      b is a first-step estimator of the linear regression parameters that uses the untreated sample
only:
                               N
                                                        !−1 N
                              X                            X
                        β
                        b=        xN i (1 − wN i )x0N i       xN i (1 − wN i )yN i .
                               i=1                                 i=1

Notice that, because wN i is randomized, the entire sample could be used to estimate E[xx0 |w = 0].
To our knowledge, this is not done in empirical research, so we do not follow that route in the
derivations. However, all our large sample results would remain unchanged if we used both treated
and untreated observations to estimate E[xx0 |w = 0]. We will assume that x has bounded support
                                                    p
and that E[y 2 ] < 0. Under these assumptions, β b→   β, and

          √                                    N
                                      0 −1 1
                                              X
              N (β − β) = (E[x(1 − w)x ])
                 b                        √       xN i (1 − wN i )(yN i − x0N i β) + op (1)
                                            N i=1

                                                          20
                           = Op (1).

For j = 0, 1 and any l, let
                                            µjl = E[I[x0 β≤c] y l |w = j].
Notice that µ10 = µ00 = Pr(x0 β ≤ c) > 0. Let µ
                                              bjl (β)
                                                   b be

                                                  N
                                               1 X                        l
                                  µ
                                  b0l (β)
                                       b =          (1 − wN i )I[x0 β≤c] yN i
                                               N0
                                                                     b
                                                                  Ni
                                                   i=1
                                                N
                                              1 X (1 − wN i )
                                            =                 I 0 b yl ,
                                              N    (1 − p) [xN i β≤c] N i
                                                   i=1

for j = 0, and
                                                    N
                                                 1 X                  l
                                       µ
                                       b1l (β) =
                                            b         wN i I[x0 β≤c] yN i
                                                 N1
                                                                 b
                                                              Ni
                                                       i=1
                                                       N
                                                   1   X     wN i
                                               =                  I 0 b yl ,
                                                   N          p [xN i β≤c] N i
                                                       i=1

for j = 1. Then,
                                               b =µ
                                                  b11 (β)   µ
                                                            b01 (β)
                                                       b         b
                                            τb(β)         −         .
                                                  µ
                                                  b10 (β)
                                                       b    µ
                                                            b00 (β)
                                                                 b
Notice that
  √                          N
                        1 X (1 − wN i )                l
                                                                       
        b0l − µ0l (β) = √
      N µ                                  I[x0N i β≤c] yN i − µ0l (β)
                          N i=1 (1 − p)
                               N
                           1 X (1 − wN i ) h            l
                                                                       
                                                                                       l
                                                                                                    i
                         +√                   I[x0 β≤c] yN i − µ0l (β) − I[x0N i β≤c] yN i − µ0l (β)
                                                                    b
                            N i=1 (1 − p)
                                                    b
                                                 Ni

                          √                 
                                  b − µ0l (β) .
                         + N µ0l (β)

Consider now M1 = {yI[x0 b≤c] : b ∈ Θ} and M0 = {I[x0 b≤c] : b ∈ Θ}. If follows from Andrews
(1994, Theorems 2 and 3) that M1 satisfies Pollard’s entropy condition with envelope max{1, y},
while M0 satisfies Pollard’s entropy condition with envelope 1. By Andrews (1994, Theorem 1),
if E[|y|2+δ ] < ∞ for some δ > 0, we obtain that the second term on the right hand side of last
equation converges in probability to zero. As a result:

                   √                                   N
                                         1 X (1 − wN i )                l
                                                                                        
                         b0l − µ0l (β) = √
                       N µ                                  I[x0N i β≤c] yN i − µ0l (β)
                                           N i=1 (1 − p)
                                        √                  
                                                b − µ0l (β) + op (1).
                                       + N µ0l (β)

For j, l = 0, 1, we will assume that µjl (b) is differentiable at β (Kim and Pollard 1990 section 5
provides high-level sufficient conditions). Then,

                                 µjl (β + h) − µjl (β) − r jl (β)h = o(khk)

                                                           21
where
                                               ∂µjl (b)
                                                  r jl (β) =
                                                        (β).
                                                 ∂b0
As a result (see, e.g., Lemma 2.12 in van der Vaart, 1998),
                                      b − µjl (β) − r jl (β)(β
                                 µjl (β)                     b − β) = op (kβ
                                                                           b − βk).
                  √
Therefore, because N kβ b − βk = Op (1), we obtain
             √                    √
               N µ0l (β) − µ0l (β) = N r 0l (β)0 (β
                      b                           b − β) + op (1)
                                                                    N
                                                                 1 X
                                                = r 0l (β)0 Q−1 √      xN i (1 − wN i )eN i + op (1),
                                                                 N i=1

where Q = E[x(1 − w)x0 ]. Then,

                      √                          N
                                            1 X (1 − wN i )                l
                                                                                           
                            b0l − µ0l (β) = √
                          N µ                                  I[x0N i β≤c] yN i − µ0l (β)
                                              N i=1 (1 − p)
                                                                   N
                                                               1 X
                                              + r 0l (β)0 Q−1 √       xN i (1 − wN i )eN i + op (1).
                                                                N i=1

Similarly,

                      √                          N
                                             1 X wN i                             
                                                        I[x0N i β≤c] yil − µ1l (β)
                                         
                            b1l − µ1l (β) = √
                          N µ
                                              N i=1 p
                                                                          N
                                                            0   −1    1 X
                                              + r 1l (β) Q           √       xN i (1 − wN i )eN i + op (1).
                                                                       N i=1

Consider now,
                  "               !1−j           !j                                                                    #
             1        1 − wN i            wN i        
                                                                        l
                                                                                     
                                                                                                0 −1
 ξjl,N i   =√                                             I[x0N i β≤c] yN i − µjl (β) + r jl (β) Q xN i (1 − wN i )eN i ,
              N        1−p                 p

and let                                                               
                                                               ξ11,N i
                                                              ξ10,N i 
                                                   ξN i       ξ01,N i  .
                                                            =         

                                                               ξ00,N i
For any a ∈ R4 , the terms a0 ξ N i are martingale differences with respect to the filtration FN i
spanned by (yN 1 , wN 1 , xN 1 ), . . . , (yN i , wN i , xN i ). For j, l = 0, 1 and j 0 , l0 = 0, 1, let,
                           !(2−j−j 0 )             !j+j 0                                                                   
                   1 − wN i                    wN i                                                              0
   sjlj 0 l0 = E                                           (I[x0i β≤c,wi =j] yil − µjl )(I[x0i β≤c,wi =j 0 ] yil − µj 0 l0 )
                    1−p                           p


                                                                 22
and let S be a (4 × 4) matrix with element (2(1 − j) + (2 − l), 2(1 − j 0 ) + (2 − l0 )) equal to sjlj 0 l0 .
Then,

            (µ12 − µ211 )/p µ11 (1 − µ10 )/p
                                                                                               
                                                        0                         0
          µ11 (1 − µ10 )/p µ10 (1 − µ10 )/p            0                         0             
    S=                                                2
                                                                                                .
                  0                0        (µ02 − µ01 )/(1 − p) µ01 (1 − µ00 )/(1 − p) 
                   0                0        µ01 (1 − µ00 )/(1 − p) µ00 (1 − µ00 )/(1 − p)

Let                                                                     0
                                                1 −µ11    1 µ01
                                    D=            , 2 ,−    ,                 .
                                               µ10 µ10   µ00 µ200
The asymptotic distribution of the unfeasible estimator that employs the treated and control units
with x0i β ≤ c is
                                √                 d
                                  N τbkU N F − τk → N (0, D 0 SD),
where                              "                  2 #                "              #
                                                                                    µ01 2
                                                                                 
                               1 µ12            µ11                 1       µ02
                  D 0 SD =           −                       +                  −           .
                           µ10 p µ10            µ10            µ00 (1 − p) µ00      µ00
Let                                                            
                                                0
                                              0                0  −1 0
                    C =E
                         I[x0 β≤c] y(y − x β)(1 − w)/(1 − p)  x  Q r ,
                                             0
                                                                 

                            I[x0 β≤c] (y − x0 β)(1 − w)/(1 − p)
                                          Σ = E[(1 − wi )xi e2i x0i ],
and
                                   V = S + rQ−1 ΣQ−1 r 0 + C + C 0 .
We obtain that the asymptotic distribution of the full-sample endogenous stratification estimator
is                              √              d
                                     τk − τk ) → N (0, D 0 V D).
                                  N (b
Notice that estimation of the derivative vector r can be accomplished using numerical methods
(see, e.g., Newey and McFadden 1994, Theorem 7.4).
Similar derivations can be used to find the large sample distribution of τbkRSS . Let ps be the
fraction of untreated observations that are assigned to the estimation group. We present the
result next, omitting details. Consider the matrix partition
                                                        
                                              S 11 S 12
                                       S=                  ,
                                              S 21 S 22

where each of the sub-matrices is (2 × 2). Let
                                                         
                                       ∗       S 11 S 12
                                      S =                   ,
                                               S 21 S ∗22

where                                                     
                                                   1 1 − ps
                                       S ∗22 = 1 +             S 22 .
                                                   M   ps

                                                        23
Let                                               
                   ∗      ∗1   ps        −1 −1 0     1
                 V =S + 1+             rQ ΣQ r + 1 −     (C + C 0 ).
                           M 1 − ps                  M
The asymptotic distribution of the repeated split sample estimator is:
                                      √                 d
                                          N τbkRSS − τk → N (0, D 0 V ∗ D).

The following intermediate lemma will be useful to derive the properties of the leave-one-out
estimator.

Lemma A.1: Let xN be a sequence of random variables, aN a sequence of real numbers, and IAN
be the indicator function for the event AN . Suppose that aN Pr(IAN = 0) → 0 and aN Pr (IAN xN > ε)
→ 0 for some ε > 0. Then, aN Pr (xN > ε) → 0.

Proof:

               aN Pr(xN > ε) = aN Pr(xN > ε, IAN = 1) + aN Pr(xN > ε, IAN = 0)
                                  = aN Pr(IAN xN > ε, IAN = 1) + aN Pr(xN > ε, IAN = 0)
                                  ≤ aN Pr(IAN xN > ε) + aN Pr(IAN = 0) → 0.

                                                                                                                     
Next, we prove that the leave-one-out estimator has the same large sample distribution as the
full-sample estimator. For simplicity and because it does not play any role in the calculations
below, we omit the subscript N from the notation for sample units. Consider the leave-one-out
estimator:
                                    N                                           N
                                  1 X                                         1 X
                                      yi I[wi =1,x0 βb ≤c]                        yi I[wi =0,x0 βb ≤c]
                                  N               i (−i)                      N               i (−i)
                                      i=1                                         i=1
                       τbLOO =          N
                                                                          −         N
                                  1    X                                      1    X
                                            I[wi =1,x0 βb                                 I[wi =0,x0 βb
                                  N                    i       (−i) ≤c]       N                      i    (−i) ≤c]
                                      i=1                                           i=1
                                      N                                     N
                                  1   X                                   1 X
                                           yi I[wi =1,x0 β≤c]                 yi I[wi =0,x0 βb ≤c]
                                  N                                       N
                                                         b
                                                           i                              i (−i)
                                      i=1                                     i=1
                              =         N
                                                                     −          N
                                                                                                               .
                                  1    X                                  1    X
                                            I[wi =1,x0 β≤c]                         I[wi =0,x0 βb
                                  N                                       N                         (−i) ≤c]
                                                       b
                                                       i                                        i
                                      i=1                                     i=1

Therefore,
                                                                                      
                                   N                             N
                                1 X                          1 X
                                     yi I[wi =0,x0 βb ≤c]          yi I[wi =0,x0 β≤c] 
         √                 √  N                   i (−i)    N                   i
                                                                                   b   
                b − τbLOO = N 
                                 i=1                           i=1
           N τb(β)                                         −
                                                                                       
                                    N                             N
                                                                                      
                               1 X                           1  X                     
                                      I[wi =0,x0 βb ≤c]             I[wi =0,x0 β≤c]   
                                N                            N
                                                                                 b
                                                 i (−i)                        i
                                                 i=1                                           i=1



                                                                   24
                                     N
                                 1 X                                          
                                √       yi I[wi =0,x0 βb ≤c] − I[wi =0,x0 β≤c]
                                  N i=1
                                                                          b
                                                    i (−i)              i
                            =                N
                                           1 X
                                               I[wi =0,x0 βb ≤c]
                                           N            i (−i)
                                                i=1
                                   N
                               1 X                                        
                              √        I[wi =0,x0 βb ≤c] − I[wi =0,x0 β≤c]      N
                               N i=1
                                                                         b
                                                 i (−i)                i
                                                                             1 X
                            −    N                      N
                                                                                   yi I[wi =0,x0 β≤c] .
                                                                             N
                                                                                                 b
                                                                                               i
                              1 X                   1 X                        i=1
                                    I[wi =0,x0 β≤c]       I[wi =0,x0 βb ≤c]
                              N                     N
                                               b
                                             i                     i (−i)
                                   i=1                      i=1
                     √                      p
We will show that N (b     b − τbLOO ) → 0. Suppose that the r-th moment of |y| exists, where
                        τ (β)
r > 1 (later we will strengthen this requirement to r > 2 and eventually to r > 3). Then, by
Holder’s Inequality:
     N                                            N
                                                                  !1/r     N
                                                                                                       !(r−1)/r
 1 X                                            1 X                      1 X
√       |yi | I[x0 β≤c] −I[x0 βb ≤c] ≤ N 1/2        |yi |r                   I[x0 β≤c] − I[x0 βb ≤c]              .
  N i=1                                         N                        N
                   b                                                              b
                 i          i (−i)                                              i           i (−i)
                                                      i=1                  i=1

The first sample average on the right hand side of last equation is bounded in probability. Now
we need to show that the second sample average on the right hand side of last equation goes
to zero fast enough to beat N 1/2 after taking the (r − 1)/r power. Because the distribution of
     b x0 β
(x0i β, i (−i) ) does not depend on i, we obtain:
          b

                 N                                     N
           "                               #     "                                               #
             1 X                                   1 X
        E           I[x0 β≤c] − I[x0 βb ≤c] = E           I[x0 β≤c<x 0β        0b            0b
             N                                     N                 i (−i) ∪ xi β (−i) ≤c<xi β]
                         b                                     b      b
                       i           i (−i)                    i
                i=1                                   i=1
                                                                                                      
                                             = Pr x0i β
                                                      b ≤ c < x0 β b
                                                                  i (−i) ∪   x 0b
                                                                                β
                                                                               i (−i)    ≤ c  <  x 0b
                                                                                                   i β   .

Therefore, by Markov’s inequality, it is enough to show that:
                       r
                                                                        p
                  N 2(r−1) Pr x0i β
                                  b ≤ c < x0 β
                                             b
                                            i (−i) ∪ x0b
                                                       β
                                                      i (−i) ≤ c < x0b
                                                                    i β  → 0.

Let ζN = N α , where α > r/(2(r − 1)). Notice that,
                                                          
          Pr x0i β
                 b ≤ c < x0 β
                            b
                           i (−i) ∪  x0b
                                       β
                                      i (−i)   ≤ c <  x0b
                                                       i β
                                                                                            
                   ≤ Pr x0i β
                            b ≤ c < x0 β b
                                       i (−i)  ∪  x 0b
                                                     β
                                                    i (−i)  ≤ c <  x 0b
                                                                     i β, |x 0b
                                                                             i β − c| > 1/ζN
                                            
                   + Pr |x0i β
                             b − c| ≤ 1/ζN
                                                                                
                   ≤ Pr |x0i (β
                              b −βb                             0b
                                    (−i) )| > 1/ζN + Pr |xi β − c| ≤ 1/ζN .

Suppose that there exists  > 0 such that for b ∈ B(β, ), the distribution of x0 b is absolutely
continuous with density bounded (uniformly) by a constant C. Assume that r > 2. Consider a
sequence N = N −γ , where 0 < γ < (r − 2)/4(r − 1). Then, for large enough N (so N < )
                                                                                     
      Pr |x0i β
              b − c| ≤ 1/ζN ≤ Pr |x0 β
                                     i
                                       b − c| ≤ 1/ζN , β
                                                       b ∈ B(β, N ) + Pr βb 6∈ B(β, N )


                                                       25
                                                                                      
                               ≤       sup       Pr |x0i b − c| ≤ 1/ζN + Pr kβ
                                                                             b − βk > N
                                    b∈B(β,N )
                                    2C                    
                               ≤       + Pr N γ kβ
                                                 b − βk > 1 .
                                    ζN

The first term on the right hand side of last equation multiplied by N r/(2(r−1)) converges to zero
because N r/(2(r−1)) /ζN → 0. For any arbitrary real square matrix A, let λmin (A) and λmax (A)
be the minimum and maximum eigenvalue of A, respectively. Assume λmin (Q) > 0. Because
kxi k is bounded by some constant, C, it follows that

                                                                             C2
                                            
                            1
                     λmax     xi (1 − wi )xi = max v 0 xi (1 − wi )x0i v/N ≤
                                           0
                                                                                .
                            N                  kvk=1                         N

Let
                                                 N
                                               1 X
                                       QN    =     xi (1 − wi )x0i .
                                               N
                                                    i=1

Now, Corollary 5.2 in Tropp (2012) implies
                                                                  N (1−t)2 λmin (Q)
                         Pr (λmin (QN ) ≤ tλmin (Q)) ≤ K e−             2C 2          ,                (A.1)

where K is the length of xi and t ∈ [0, 1]. Define the event

                                        AN = {λmin (QN ) ≥ Cλ } ,

for some 0 < Cλ < λmin (Q), and let IAN be the indicator function for the event AN . The
concentration inequality in (A.1) implies
                                             r
                                        N 2(r−1) Pr(IAN = 0) → 0.                                      (A.2)

Let ei = yi − x0i β. Notice that,

IAN N kβ               b − β)0 (β
       b − βk2 = IA N (β        b − β)
                   N

                               N
                                                    !    N
                                                                             !−2           N
                                                                                                             !
                           1 X                       1 X                            1 X
                = IAN     √       ei (1 − wi )x0i            xi (1 − wi )xi0
                                                                                   √          xi (1 − wi )ei
                            N i=1                    N                               N
                                                        i=1                               i=1
                                         N                             N
                                                             !                            !
                                     1  X                           1  X
                ≤ IAN λ−2
                       min (QN ) √           ei (1 − wi )x0i     √         xi (1 − wi )ei
                                     N i=1                          N i=1
                              N                              N
                                                  !                           !
                    −2    1 X                   0     1 X
                ≤ Cλ    √        ei (1 − wi )xi      √          xi (1 − wi )ei .
                           N i=1                       N i=1

Given that x is bounded, E[e2i ] < ∞ (which follows from r > 2), and given that E[ei (1 −
wi )x0i xj (1 − wj )ej ] = E[ei (1 − wi )x0i ]E[xj (1 − wj )ej ] = 0 for any 1 ≤ i < j ≤ N , we obtain

                                                     b − βk2 ] < ∞.
                                    lim sup E[IAN N kβ                                                 (A.3)
                                     N →∞



                                                        26
By Markov’s inequality, equation (A.3), and because γ < (r − 2)/4(r − 1)
               r                                        r
                                         
                               b − βk > 1 ≤ N 2γ−1+ 2(r−1) E[IA N kβ
          N 2(r−1) Pr IAN N γ kβ                                     b − βk2 ] → 0.                           (A.4)
                                                                N



From equations (A.2), (A.4), and Lemma A.1, it follows that
                                   r
                                                        
                              N 2(r−1) Pr N γ kβ
                                               b − βk > 1 → 0.

Therefore,
                                           r
                                                                   
                                      N 2(r−1) Pr |x0i β
                                                       b − c| ≤ 1/ζN → 0.
                              r
Next, we will prove N 2(r−1) Pr(|x0i (β
                                      b−β
                                        b
                                          (−i) )| > 1/ζN ) → 0. Notice that (see Hansen, 2012
sections 4.12 and 4.13)

                                                  hN i
                        |x0i (β
                              b −β
                                 b
                                   (−i) )| =             |yi − x0i β|
                                                                   b
                                                1 − hN i
                                                                       
                                                    maxi=1,...,N hN i
                                              ≤                           |yi − x0i β|,
                                                                                    b
                                                  1 − maxi=1,...,N hN i

where the leverage values hN i are

                                                         N
                                                                                 !−1
                                                         X
                                  hN i = (1 − wi )x0i          xi (1 − wi )x0i         xi .                   (A.5)
                                                         i=1

Therefore,

                                                                         C2
                                           max hN i ≤ λ−1
                                                       min (QN )            .
                                          1≤i≤N                          N
Notice also that,
         b − βkr
    IAN kβ
              
                    N
                                              !     N
                                                                            !−2          N
                                                                                                          !r/2
                 1                               1 X                                  1
                    X                                                                    X
        ≤ IAN         (1 − wi )ei x0i                 xi (1 − wi )x0i                      xi ei (1 − wi ) 
                  N                               N                                    N
                        i=1                             i=1                                   i=1

                                   N
                                                          !    N
                                                                                              !!r/2
                               1 X                         1 X
        ≤ IAN λ−r
               min (QN )              (1 − wi )ei x0i             xi ei (1 − wi )
                               N                          N
                                  i=1                         i=1
                        N
                                           !      N
                                                                     !!r/2
            −r      1 X                  0    1 X
        ≤ Cλ               (1 − wi )ei xi             xi ei (1 − wi )
                    N                         N
                       i=1                       i=1
                                        r/2
                          N X N
                      1  X
        = Cλ−r C r  2           |ei ej | .
                     N
                          i=1 j=1




                                                          27
If r ≥ 2, by Minkowski’s and Cauchy-Schwartz’s inequalities:
                                 r/2 2/r
                        N N                         N N
                  1 X X                       1 XX h              i2/r
                                                                  r/2
                 E          |e  e  |           ≤        E |e e  |
                                         
                               i j                         i j
                     N2                          N2
                                       
                              i=1 j=1                              i=1 j=1

                                                                 N N
                                                               1 XX
                                                             ≤ 2     (E [|e1 |r ])2/r .
                                                              N
                                                                   i=1 j=1

Because E[|ei |r ] = E[|yi − x0i β|r ], E[|yi |r ] < ∞ and kxi k is bounded, we obtain

                                                       b − βkr ] < ∞,
                                        lim sup E[IAN kβ
                                        N →∞

which, by Minkowski’s inequality, implies

                                     lim sup E[IAN |yi − x0i β|
                                                             b r ] < ∞.                               (A.6)
                                        N →∞

By Markov’s inequality:
            r                                              r
                                                                   h                          i
                                                               +rα
       N 2(r−1) Pr IAN |x0i (β
                             b −β
                                b
                                  (−i) )| > 1/ζ N   ≤ N 2(r−1)     E  I A N
                                                                            |x0 b
                                                                              i (β − β
                                                                                     b
                                                                                       (−i) )|r
                                                                                                  .

The condition α < (2r − 3)/(2(r − 1)) implies r/(2(r − 1)) + rα < r. So, under that condition, it
is left to be proven that for any ϑ > 0
                                        h                       i
                                N r−ϑ E IAN |x0i (β
                                                  b −β
                                                     b
                                                       (−i) )|
                                                              r
                                                                  → 0.                     (A.7)

Consider N large enough so that there is a positive constant Cd , such that 1/(1−Cλ−1 C 2 /N ) < Cd .
Then,
                                                               2
                                                          −1 C
                           IAN |x0i (β
                                     b −β
                                        b
                                          (−i) )| ≤ C d C            0b
                                                          λ N |yi − xi β|.

This result, along with equation (A.6) implies:

                                lim sup E[IAN N r |x0i (β
                                                        b −β
                                                           b        r
                                                             (−i) )| ] < ∞,
                                 N →∞

so equation (A.7) holds. Notice that for the condition
                                              r         2r − 3
                                                   <α<
                                          2(r − 1)     2(r − 1)
to hold we need r > 3.

                     Appendix 2: Estimation and Simulation Details

Leave-one-out predictions can be efficiently calculated using:
                                                          hN i
                                  x0i β
                                      b         0b
                                        (−i) = xi β −            (yi − x0i β),
                                                                           b
                                                        1 − hN i

                                                        28
where hN i is the leverage value defined in equation (A.5). Let ybi be generic notation for a
predicted outcome without treatment. The prediction, ybi , may come from full-sample, leave-one-
out, or split sample endogenous stratification, stratification on previous earnings in the JTPA
example of Section 3, or stratification on the true regression value in the simulations of Section
4. We group observations on the basis of predicted outcomes, ŷi , in the following way. First we
sort the observations based on predicted outcomes: yb(1) ≤ yb(2) ≤ · · · ≤ yb(N ) . We then define t1
and t2 as N/3 and 2N/3 rounded to the nearest integer, respectively. We classify unit i in the
low, medium, and high predicted outcome groups if ybi ≤ yb(t1 ) , yb(t1 ) < ybi ≤ yb(t2 ) , and yb(t2 ) < ybi ,
respectively.
For the repeated split sample estimator estimation is as follows. For the JTPA data we randomly
select 425 control observations and use them to estimate β. We use the remaining 424 observations
and all the treated JTPA units for the second step estimation of τk . For the STAR data, we use
1009 untreated observations to estimate β and the remaining 1008 and all the treated observations
in the second step. We average the split sample estimators over 100 repetitions to obtain τbkRSS .
As explained in Section 3, the JTPA sample consists of male applicants assigned to on the job
training/job search assistance. We discard three of the sixteen SDAs, Jersey City (21 observa-
tions), Butte (15 observations), and Oakland (5 observations) because of small sample sizes. The
STAR sample consists of use 3764 students who entered the study in kindergarten, were assigned
to small classes or to regular-size classes without a teacher’s aide, and for whom there is complete
information on all the variables used in our analysis.
Standard errors in Tables 1 and 2 are calculated using the nonparametric bootstrap (conditioning
on the number of treated and untreated observations in the original samples).
                                                                                          0           0
In the JTPA-based simulation we first estimate a Logit model, p(x, γ) = ex γ /(1 + ex γ ), for
the probability of employment, measured as positive labor market earnings, using the sample
of experimental controls. Next, using only the experimental control with positive earnings, we
estimate a Box-Cox regression model

                                            yλ − 1
                                                   = x0 θ + σu,
                                              λ
where u has a Standard Normal distribution. We will use γ ∗ to refer to the estimate of γ,
and analogous notation for the estimated parameters of the Box-Cox model. We create each
simulated data set in the following manner. We first resample 2530 observations from the empirical
distribution of x among all the JTPA sample units. We assign zero earnings with probability
1 − p(x, γ ∗ ). With probability p(x, γ ∗ ) we assign earnings using
                                                                       1/λ∗
                                 y = max{1 + λ∗ (x0 θ ∗ + σ ∗ u), 0}           ,

where u has a Standard Normal distribution. We randomly label 1681 observations as treated
and 849 as untreated. As a result, all treatment effects are equal to zero by construction. The
coefficients of the regression function of y on x under this data generating process, which are
needed to compute τbkU N F , are calculated by simulation.
For the STAR-based simulations, we estimate the linear model

                                               y = x0 β + σu,

                                                      29
where u has a Standard Normal distribution, using the sample of experimental controls. We use
least squares to estimate β and the variance of the regression residuals corrected for degrees of
freedom to estimate σ. To construct each simulated sample, we first randomly resample 3764
observations from the empirical distribution of x in the STAR sample. We simulate math scores
using
                                        y = x0 β ∗ + σ ∗ u,
where u has a Standard Normal distribution, and β ∗ and σ ∗ are the estimates of β and σ.
Section 4 contains detailed information on data generating processes for the simulations of Tables
5 and 6.
Bias, coverage rates, and root-MSE are calculated as follows. Because the simulations impose
zero treatments effects, the bias and the MSE are calculated as the mean of the estimates and the
mean of the square of the estimates, respectively, across all simulation repetitions. We calculate
t-ratios dividing the estimates from each simulation repetition by the standard deviation of the
estimates across repetitions. Coverage rates are the frequencies of the t-ratios falling outside the
[−1.96, 1.96] interval across repetitions. Root-MSE is the square-root of the MSE.




                                                30
                                        References


Abadie, Alberto, Joshua D. Angrist and Guido W. Imbens. 2002. “Instrumental Variables Es-
    timates of the Effect of Subsidized Training on the Quantiles of Trainee Earnings.” Econo-
    metrica 70(1): 91-117.

Altonji, Joseph G. and Lewis M. Segal. 1996. “Small-Sample Bias in GMM Estimation of
    Covariance Structures.” Journal of Business and Economic Statistics 14(3): 353-366.

Andrews, Donald W. K. 1994. “Empirical Process Methods in Econometrics.” In R.F. Engle and
    D.L. McFadden (Eds.), Handbook of Econometrics, Vol. IV. Amsterdam: Elsevier Science.

Angrist, Joshua D. and Victor Lavy. 2009. “The Effects of High Stakes High School Achievement
    Awards: Evidence from a Randomized Trial.” American Economic Review 99(4): 1384-
    1414.

Angrist, Joshua D. and Jorn-Steffen Pischke. 2010. “The Credibility Revolution in Empirical
    Economics: How Better Research Design Is Taking the Con out of Econometrics.” Journal
    of Economic Perspectives 24(2): 3-30.

Billingsley, Patrick. 1995. Probability and Measure, third edition. New York: Wiley.

Bloom, Howard S., Larry L. Orr, Stephen H. Bell, George Cave, Fred Doolittle, Winston Lin
    and Johannes M. Bos. 1997. “The Benefits and Costs of JTPA Title II-A Programs.” The
    Journal of Human Resources, 32(3): 549-576.

Dynarski, Susan, Joshua M. Hyman and Diane Schanzenbach. 2011. “Experimental Evidence
    on the Effect of Childhood Investments on Post-secondary Attainment and Degree Com-
    pletion.” National Bureau of Economic Research Working Paper No. 17533.

Dynarski, Susan, Joshua M. Hyman and Diane Schanzenbach. 2013. “Experimental Evidence on
    the Effect of Childhood Investments on Postsecondary Attainment and Degree Completion.”
    Journal of Policy Analysis and Management 32(4): 692-717.

Giné, Xavier, Jessica Goldberg and Dean Yang. 2012. “Credit Market Consequences of Improved
     Personal Identification.” American Economic Review 102(6): 2923-54.

Goldrick-Rab, Sara, Douglas N. Harris, James Benson and Robert Kelchen. 2011. “Condi-
    tional cash transfers and college persistence: Evidence from a randomized need-based grant
    program.” Institute for Research on Poverty Discussion Paper No. 1393-11.

Goldrick-Rab, Sara, Douglas N. Harris, James Benson and Robert Kelchen. 2012. “Need-based
    financial aid and college persistence: Experimental evidence from Wisconsin.” Available at
    SSRN 1887826.

Hansen, Ben B. 2008. “The Prognostic Analogue of the Propensity Score.” Biometrika 95(2):
    481-488.

Hansen, Bruce E. 2012. Econometrics. Available at: http://www.ssc.wisc.edu/~bhansen/


                                             31
Harris, Douglas N. and Sara Goldrick-Rab. 2012. “Improving the Productivity of Education
    Experiments: Lessons from a Randomized Study of Need-Based Financial Aid.” Education
    Finance and Policy 7(2): 143-169.
Harvill, Eleanor L., Laura R. Peck and Stephen H. Bell. 2013. “On Overfitting in Analysis of
    Symmetrically Predicted Endogenous Subgroups From Randomized Experimental Samples:
    Part Three of a Method Note in Three Parts.” American Journal of Evaluation 34(4): 545-
    566.
Hausman, Jerry A., and David A. Wise. 1977. “Social Experimentation, Truncated Distribu-
    tions, and Efficient Estimation.” Econometrica 45(4): 919-938.
Hausman, Jerry A., and David A. Wise. 1981. “Stratification on Endogenous Variables and Es-
    timation: The Gary Income Maintenance Experiment.” In: Manski, C.F., McFadden, D.L.
    (Eds.), Structural Analysis of Discrete Data with Econometric Applications. Cambridge:
    MIT Press.
Heckman, James J., Jeffrey Smith and Nancy Clements. 1997. “Making the Most Out of Pro-
    gramme Evaluations and Social Experiments: Accounting for Heterogeneity in Programme
    Impacts.” Review of Economic Studies 64(4): 487-535.
Hemelt, Steven W., Kimberly B. Roth and William W. Eaton. 2012. “Childhood Educational
   Interventions: Experimental Evidence on Post-secondary Outcomes.” Paper presented at
   the Association for Education Finance and Policy Conference, Boston, MA.
Kent, David M. and Rodney A. Hayward. 2007. “Limitations of Applying Summary Results
    of Clinical Trials to Individual Patients: The Need for Risk Stratification.” Journal of the
    American Medical Association 298(10): 1209-12.
Kim, Jeankyung and David Pollard. 1990. “Cube Root Asymptotics.” The Annals of Statistics
    18(1): 191-219.
Krueger, Alan B. 1999. “Experimental Estimates of Education Production Functions.” Quar-
    terly Journal of Economics 114(2): 497-532.
Murnane, Richard J. and John B. Willett. 2011. Methods Matter: Improving Causal Inference
    in Educational and Social Science Research. Oxford: Oxford University Press.
Newey, Whitney K. and Daniel McFadden. 1994. “Large Sample Estimation and Hypothesis
    Testing.” In: Engle, R.F., McFadden, D.L. (Eds.), Handbook of Econometrics, Vol. IV.
    Amsterdam: Elsevier Science.
Orr, Larry L., Howard S. Bloom, Stephen H. Bell, Fred Doolittle, Winston Lin and George Cave.
     1996. Does Training for the Disadvantaged Work? Washington, DC: The Urban Institute.
Pane, John F., Beth Ann Griffin, Daniel F. McCaffrey and Rita Karam. 2013. “Effectiveness of
    Cognitive Tutor Algebra I at Scale.” Educational Evaluation and Policy Analysis, Online-
    First: http://epa.sagepub.com/content/early/2013/11/08/0162373713507480.
Peck, Laura R. 2003. “Subgroup Analysis in Social Experiments: Measuring Program Impacts
    Based on Post-Treatment Choice.” American Journal of Evaluation 24(2): 157-187.

                                              32
Rodriguez-Planas, Nuria. 2012. “Schools and Drugs: Closing the Gap. Evidence from a Ran-
    domized Trial in the US.” IZA Discussion Paper No. 6770.

Sanbonmatsu, Lisa, Jeffrey R. Kling, Greg J. Duncan and Jeanne Brooks-Gunn. 2006. “Neigh-
    borhoods and Academic Achievement: Results from the Moving to Opportunity Experi-
    ment.” Journal of Human Resources 41(4): 649-691.

Tropp, Joel A. 2012. “User-Friendly Tail Bounds for Sums of Random Matrices.” Foundations
    of Computational Mathematics 12: 389-434.

van der Vaart, Aad W. 1998. Asymptotic Statistics. New York: Cambridge University Press.




                                           33
       -4                                                  -4
    x 10            Full Sample                       x 10           Leave-one-out
4                                                 4


3                                                 3


2                                                 2


1                                                 1


0                                                 0
           -5000        0          5000                      -5000        0          5000


       -4                                                  -4
    x 10       Repeated Split Sample                  x 10            Unfeasible
4                                                 4


3                                                 3


2                                                 2


1                                                 1


0                                                 0
           -5000        0          5000                      -5000        0          5000


                                            bottom third
                                            middle third
                                            top third




                                          Figure 1
                   Distributions of the Estimators in the JTPA Simulation




                                            34
               Full Sample                                     Leave-one-out
10                                            10

8                                              8

6                                              6

4                                              4

2                                              2

0                                              0
-0.2   -0.1        0         0.1   0.2         -0.2     -0.1        0          0.1   0.2


        Repeated Split Sample                                   Unfeasible
10                                            10

8                                              8

6                                              6

4                                              4

2                                              2

0                                              0
-0.2   -0.1        0         0.1   0.2         -0.2     -0.1        0          0.1   0.2


                                         bottom third
                                         middle third
                                         top third




                                      Figure 2
              Distributions of the Estimators in the STAR Simulation




                                         35
             JTPA-based simulation                                   STAR-based simulation
                                                      10
 3
                                                       8
2.5

 2                                                     6

1.5
                                                       4
 1
                                                       2
0.5

 0                                                     0
      -0.4   -0.2      0      0.2    0.4                   -0.2      -0.1     0       0.1    0.2

                                           full-sample
                                           leave-one-out
                                           repeated split sample
                                           nonlinear least squares
                                           unfeasible




                                           Figure 3
                      Distribution of the Regression Interaction Estimator




                                                 36
                                          Table 1
                                    JTPA Estimation Results
Panel A: Average treatment effect
                          unadjusted                                      adjusted
 τb                        1516.49*                                      1207.22
                           (807.27)                                      (763.54)

Panel B: Average treatment effect by predicted outcome group
                          unadjusted                                      adjusted
                 low       medium          high                low        medium         high
 τbk          2379.65**    −719.38       2397.26            2011.70*    −554.65        1769.03
             (1151.07)     (1474.81)    (1672.62)          (1150.68)    (1482.32)     (1639.06)

 τbkLOO        573.74         35.31      3646.53**           173.45       172.28       3118.17*
             (1201.33)     (1509.30)    (1727.08)          (1213.25)    (1513.70)     (1679.62)

 τbkRSS        788.75        254.25      3569.41**           412.01       181.81       2942.69**
             (1027.47)     (1092.85)    (1496.73)          (1042.17)    (1087.51)     (1454.16)

 τbkP REV     1278.88       −67.95       3972.21**           822.05     −150.89        3146.85**
             (1221.96)     (1284.77)    (1497.47)          (1235.13)    (1274.45)     (1430.37)

Note: The JTPA sample includes 1681 treated observations and 849 untreated observations, for
a total of 2530 observations. Bootstrap standard errors, based on 1000 bootstrap repetitions,
are reported in parentheses. The repeated split sample estimator, τbkRSS , uses 100 repetitions.
Each repetition randomly permutes the order of the untreated observations. Then, the first 425
untreated observations after re-ordering are used to estimate β. The remaining 424 untreated
observations and the 1681 treated observations are used in the second step estimation of τk . The
“unadjusted” estimates are differences in mean outcomes between treated and non-treated. The
“adjusted” estimates are regression coefficients on the treatment variable in a linear regression
that includes the list of covariates detailed in Section 3.
 ∗ indicates statistical significance at the 0.10 level.
∗∗ indicates statistical significance at the 0.05 level.




                                              37
                                           Table 2
                                     STAR Estimation Results
Panel A: Average treatment effect
                         unadjusted                                      adjusted
 τb                       0.1659**                                      0.1892**
                         (0.0329)                                      (0.0294)

Panel B: Average treatment effect by predicted outcome group
                         unadjusted                                     adjusted
               low        medium          high                 low      medium          high
 τbk        0.3705**      0.2688**     −0.1330**           0.3908**     0.3023**     −0.1242**
           (0.0521)      (0.0655)       (0.0636)          (0.0509)     (0.0678)      (0.0614)

 τbkLOO     0.3277**      0.2499**     −0.0486             0.3440**     0.2730**     −0.0660
           (0.0547)      (0.0670)       (0.0654)          (0.0519)     (0.0696)      (0.0634)

 τbkRSS     0.3152**      0.2617**     −0.0520             0.3130**     0.3005**     −0.0374
           (0.0467)      (0.0505)       (0.0567)          (0.0459)     (0.0526)      (0.0552)

Note: The STAR sample includes 1747 treated observations and 2017 untreated observations, for
a total of 3764 observations. Bootstrap standard errors, based on 1000 bootstrap repetitions,
are reported in parentheses. The repeated split sample estimator, τbkRSS , uses 100 repetitions.
Each repetition randomly permutes the order of the untreated observations. Then, the first 1009
untreated observations after re-ordering are used to estimate β. The remaining 1008 untreated
observations and the 1747 treated observations are used in the second step estimation of τk . The
“unadjusted” estimates are differences in mean outcomes between treated and non-treated. The
“adjusted” estimates are regression coefficients on the treatment variable in a linear regression
that includes the list of covariates detailed in Section 3.
 ∗ indicates statistical significance at the 0.10 level.
∗∗ indicates statistical significance at the 0.05 level.




                                              38
                                   Table 3
                            JTPA Simulation Results
Panel A: Bias
                        unadjusted                        adjusted
                low      medium       high        low     medium      high
 τbk        1017.51        -4.81     -1082.42   1017.60     -0.98    -1062.39
 τbkLOO      -88.23       -23.28        96.57    -59.08    -54.01       42.86
 τbkRSS       -2.74        -2.34       -20.75     -3.30     -5.96      -17.56
 τbkSSS        5.62        -9.88         6.62      1.34    -11.24        1.39
 τbkU N F     -1.50        -8.56       -16.85     -2.50     -9.04      -11.67

Panel B: Coverage rates for nominal 0.05 C.I.
                    unadjusted                            adjusted
             low     medium      high             low     medium      high
 τbk            0.152      0.049       0.089      0.154     0.050      0.089
 τbkLOO         0.051      0.048       0.050      0.051     0.049      0.051
 τbkRSS         0.051      0.048       0.049      0.052     0.048      0.050
 τbkSSS         0.050      0.048       0.051      0.052     0.049      0.050
 τbkU N F       0.053      0.050       0.050      0.053     0.051      0.051

Panel C: Root-MSE
                        unadjusted                        adjusted
                low      medium       high        low     medium      high
 τbk        1492.26      1364.27     2145.78    1489.89   1375.04    2065.87
 τbkLOO     1192.35      1399.74     1895.93    1180.13   1398.76    1800.86
 τbkRSS     1031.61      1101.14     1751.43    1022.53   1103.53    1660.50
 τbkSSS     1500.97      1797.74     2383.17    1493.40   1792.14    2271.90
 τbkU N F   1119.34      1372.67     1867.51    1118.52   1383.76    1792.25

Note: Averages over 10000 simulations. See Section 4 and Appendix 2 for
details.




                                         39
                                  Table 4
                           STAR Simulation Results
Panel A: Bias
                        unadjusted                        adjusted
                low      medium       high       low      medium      high
 τbk         0.0483        0.0006    -0.0511     0.0487     0.0010   -0.0506
 τbkLOO     -0.0025        0.0005     0.0046     0.0028    -0.0025   -0.0075
 τbkRSS      0.0001       -0.0000    -0.0012     0.0002     0.0001   -0.0010
 τbkSSS     -0.0005        0.0004    -0.0017    -0.0002     0.0003   -0.0015
 τbkU N F    0.0004       -0.0003    -0.0009     0.0002    -0.0002   -0.0006

Panel B: Coverage rates for nominal 0.05 C.I.
                   unadjusted                             adjusted
             low    medium      high             low      medium      high
 τbk            0.161      0.051      0.178      0.178      0.049     0.191
 τbkLOO         0.048      0.050      0.051      0.050      0.049     0.056
 τbkRSS         0.053      0.051      0.048      0.052      0.051     0.049
 τbkSSS         0.051      0.050      0.052      0.052      0.052     0.050
 τbkU N F       0.051      0.052      0.049      0.051      0.050     0.050

Panel C: Root-MSE
                        unadjusted                        adjusted
                low      medium       high       low      medium      high
 τbk        0.0695        0.0472     0.0716     0.0677     0.0471    0.0691
 τbkLOO     0.0526        0.0509     0.0530     0.0492     0.0507    0.0494
 τbkRSS     0.0473        0.0402     0.0470     0.0444     0.0399    0.0439
 τbkSSS     0.0617        0.0589     0.0615     0.0577     0.0583    0.0571
 τbkU N F   0.0501        0.0469     0.0503     0.0480     0.0475    0.0476

Note: Averages over 10000 simulations. See Section 4 and Appendix 2 for
details.




                                        40
                                                                           Table 5
                                                           Bias in Simulations Using Artificial Data
                                                               (constant regression coefficients)

                                K = 10                                            K = 20                                            K = 40
                     unadjusted           adjusted                     unadjusted           adjusted                     unadjusted           adjusted
                 low   med. high      low   med. high              low   med. high      low   med. high              low   med. high      low   med. high
     N = 200
     τbk         2.24   -0.05   -2.28     2.23   -0.06   -2.27     3.15   -0.00   -3.13     3.12   -0.01   -3.08     4.09   0.02    -4.06    3.89   0.04    -3.88
     τbkLOO     -0.32   -0.03    0.25    -0.07   -0.04    0.01    -0.18    0.00    0.17     0.30    0.01   -0.26    -0.09   0.00     0.13    0.89   0.02    -0.88
     τbkRSS     -0.03   -0.04   -0.04    -0.02   -0.04   -0.05     0.01   -0.00   -0.01     0.03    0.01    0.01     0.03   0.01    -0.00    0.04   0.02    -0.01
     τbkU N F   -0.04   -0.04   -0.03    -0.02   -0.03   -0.04     0.03    0.00    0.00     0.02   -0.01    0.03     0.02   0.00     0.01   -0.01   0.05     0.04




41
     N = 1000
     τbk         0.55   0.01    -0.54     0.55   0.01    -0.54     0.71   -0.01   -0.71     0.70   -0.00   -0.70     0.82   0.01    -0.82    0.81   0.01    -0.82
     τbkLOO     -0.05   0.01     0.07    -0.05   0.01     0.06    -0.05   -0.01    0.05    -0.05   -0.00    0.05    -0.01   0.01     0.01   -0.00   0.01    -0.01
     τbkRSS      0.02   0.00    -0.00     0.02   0.01    -0.00    -0.01   -0.01    0.01    -0.01   -0.00    0.01     0.01   0.00    -0.01    0.01   0.00    -0.01
     τbkU N F    0.01   0.01     0.00     0.00   0.01     0.00    -0.00   -0.01   -0.01    -0.00   -0.00    0.00     0.01   0.00    -0.01    0.01   0.01    -0.01

     N = 5000
     τbk         0.12   -0.00   -0.12     0.12   -0.00   -0.12     0.15    0.00   -0.16     0.15    0.00   -0.15     0.16   -0.00   -0.16    0.16   -0.00   -0.16
     τbkLOO     -0.01   -0.00    0.01    -0.01   -0.00    0.01    -0.01    0.00    0.00    -0.00    0.00    0.00    -0.01   -0.00    0.01   -0.01   -0.00    0.01
     τbkRSS      0.01   -0.00   -0.01     0.01   -0.00   -0.01     0.00   -0.00   -0.01     0.01   -0.00   -0.01    -0.00   -0.00    0.00    0.00   -0.00    0.00
     τbkU N F    0.01   -0.00   -0.01     0.01   -0.00   -0.01     0.01   -0.01   -0.01     0.01   -0.01   -0.00     0.00   -0.00    0.00    0.00   -0.00    0.01
     Note: Averages over 10000 simulations. K is the number of included regressors. See Section 4 and Appendix 2 for details.
                                                                          Table 6
                                                          Bias in Simulations Using Artificial Data
                                                              (decaying regression coefficients)

                                K = 10                                           K = 20                                               K = 40
                     unadjusted           adjusted                    unadjusted           adjusted                        unadjusted           adjusted
                 low   med. high      low   med. high             low   med. high      low   med. high                 low   med. high      low   med. high
     N = 200
     τbk         2.94   -0.00   -2.97    2.97   -0.00   -2.99     4.57    -0.01   -4.60       4.63    -0.02   -4.69    6.86    0.04   -6.87    7.20   -0.01   -7.24
     τbkLOO     -0.41   -0.01    0.40    0.01   -0.01   -0.02    -0.26     0.01    0.23       0.76    -0.04   -0.79   -0.14   -0.00    0.17    2.48   -0.00   -2.48
     τbkRSS     -0.03   -0.01    0.01   -0.02   -0.01    0.01    -0.01    -0.00   -0.01      -0.02    -0.02   -0.02    0.00    0.01    0.01   -0.02    0.01    0.02
     τbkU N F   -0.01   -0.01   -0.01   -0.02   -0.01    0.00    -0.01    -0.03    0.02       0.00    -0.05   -0.01   -0.03    0.02    0.04   -0.03    0.02    0.01




42
     N = 1000
     τbk         0.95   0.01    -0.94    0.95   0.01    -0.94     1.64    -0.01   -1.64       1.64    -0.00   -1.64    2.64   -0.01   -2.64    2.66   -0.01   -2.65
     τbkLOO     -0.11   0.01     0.12   -0.10   0.01     0.11    -0.10    -0.01    0.11      -0.03    -0.01    0.04   -0.07   -0.00    0.08    0.14    0.00   -0.13
     τbkRSS      0.01   0.00     0.01    0.01   0.00     0.01    -0.00    -0.00    0.00      -0.00    -0.00    0.01    0.00   -0.00    0.00   -0.00   -0.00    0.01
     τbkU N F    0.01   0.00     0.01    0.01   0.00     0.01     0.00    -0.00   -0.00      -0.00    -0.01    0.00   -0.01   -0.01    0.02   -0.01   -0.01    0.03

     N = 5000
     τbk         0.23 -0.01 -0.22         0.23 -0.01 -0.22         0.43    -0.00 -0.43        0.43     -0.01 -0.43      0.81 -0.00    -0.81    0.81   -0.00   -0.81
        LOO
     τbk        -0.02 -0.01     0.04     -0.02 -0.01    0.03      -0.03    -0.01    0.03     -0.03     -0.01  0.03     -0.03 -0.00     0.02   -0.02   -0.01    0.02
     τbkRSS      0.00 -0.00     0.00      0.00 -0.00    0.00      -0.00    -0.00    0.00     -0.00     -0.00  0.00     -0.00 -0.00    -0.00    0.00   -0.00   -0.00
     τbkU N F    0.00 -0.00     0.01      0.00 -0.00    0.01       0.00    -0.01    0.00      0.00     -0.01  0.00     -0.00 -0.00     0.00   -0.00   -0.00    0.00
     Note: Averages over 10000 simulations. K is the number of included   regressors. See Section 4   and Appendix 2 for details.
