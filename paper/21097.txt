                                NBER WORKING PAPER SERIES




               THE USE AND MISUSE OF MODELS FOR CLIMATE POLICY

                                          Robert S. Pindyck

                                        Working Paper 21097
                                http://www.nber.org/papers/w21097


                      NATIONAL BUREAU OF ECONOMIC RESEARCH
                               1050 Massachusetts Avenue
                                 Cambridge, MA 02138
                                      April 2015




I have no relevant material or financial relationships with any organization. The views expressed herein
are those of the author and do not necessarily reflect the views of the National Bureau of Economic
Research.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2015 by Robert S. Pindyck. All rights reserved. Short sections of text, not to exceed two paragraphs,
may be quoted without explicit permission provided that full credit, including © notice, is given to
the source.
The Use and Misuse of Models for Climate Policy
Robert S. Pindyck
NBER Working Paper No. 21097
April 2015
JEL No. D81,Q51,Q54

                                             ABSTRACT

In recent articles, I have argued that integrated assessment models (IAMs) have flaws that make them
close to useless as tools for policy analysis. IAM-based analyses of climate policy create a perception
of knowledge and precision that is illusory, and can fool policy-makers into thinking that the forecasts
the models generate have some kind of scientific legitimacy. But some have claimed that we need
some kind of model, and that IAMs can be structured and used in ways that correct for their shortcomings.
 For example, it has been argued that although we know little or nothing about key relationships in
the model, we can get around this problem by attaching probability distributions to various parameters
and then simulating the model using Monte Carlo methods. I argue that this would buy us nothing,
and that a simpler and more transparent approach to the design of climate change policy is preferable.
I briefly outline what that approach would look like.


Robert S. Pindyck
MIT Sloan School of Management
100 Main Street, E62-522
Cambridge, MA 02142
and NBER
RPINDYCK@MIT.EDU
                                                         1


           “Pay no attention to the man behind the curtain!”
             L. Frank Baum, The Wonderful Wizard of Oz

1. Introduction.
           In a recent article, I argued that integrated assessment models (IAMs) “have crucial flaws
that make them close to useless as tools for policy analysis.”1 In fact, I would argue that calling
these models “close to useless” is generous: IAM-based analyses of climate policy create a
perception of knowledge and precision that is illusory, and can fool policy-makers into thinking
that the forecasts the models generate have some kind of scientific legitimacy. IAMs can be
misleading – and are inappropriate – as guides for policy, and yet they have been used by the
government to estimate the social cost of carbon (SCC) and evaluate tax and abatement policies.2
           What are the crucial flaws that make IAMs so unsuitable for policy analysis? They are
discussed in detail in Pindyck (2013b), but the most important ones can be briefly summarized as
follows:
      1. Certain inputs – functional forms and parameter values – are arbitrary, but have huge
           effects on the results the models produce. An example is the discount rate. There is no
           consensus among economists as to the “correct” discount rate, but different rates will
           yield wildly different estimates of the SCC and the optimal amount of abatement that any
           IAM generates. For example, these differences in inputs largely explain why the IAM-
           based analyses of Nordhaus (2008) and Stern (2007) come to such strikingly different
           conclusions regarding optimal abatement. Because the modeler has so much freedom in
           choosing functional forms, parameter values, and other inputs, the model can be used to
           obtain almost any result one desires, and thereby legitimize what is essentially a
           subjective opinion about climate policy.
      2. We know very little about climate sensitivity, i.e., the temperature increase that would
           eventually result from a doubling of the atmospheric CO2 concentration, but this is a key
           input to any IAM. The problem is that the physical mechanisms that determine climate
           sensitivity involve crucial feedback loops, and the parameter values that determine the


1
    Pindyck (2013b). Some of these arguments are also discussed in Pindyck (2013a).
2
  See the technical reports from the Interagency Working Group on Social Cost of Carbon (2010, 2013), which used
three IAMs to arrive at estimates of the SCC. Also, see Greenstone, Kopits and Wolverton (2013) for an
illuminating explanation of the process used by the Interagency Working Group to estimate the SCC.
                                                         2


         strength (and even the sign) of those feedback loops are largely unknown, and are likely
         to remain unknown for the foreseeable future.3 As Freeman, Wagner, and Zeckhauser
         (2015) have shown, over the past decade our uncertainty over climate sensitivity has
         increased.
    3. One of the most important parts of an IAM is the damage function, i.e., the relationship
         between an increase in temperature and GDP (or the growth rate of GDP). When
         assessing climate sensitivity, we can at least draw on the underlying physical science and
         argue coherently about the relevant probability distributions. But when it comes to the
         damage function, we know virtually nothing – there is no theory and no data that we can
         draw from.4 As a result, developers of IAMs simply make up arbitrary functional forms
         and corresponding parameter values.
    4. IAMs can tell us nothing about the likelihood or possible impact of a catastrophic climate
         outcome, e.g., a temperature increase above 5°C that has a very large impact on GDP.
         And yet it is the possibility of a climate catastrophe that is (or should be) the main driving
         force behind a stringent abatement policy.
         Although many would agree that IAMs have their flaws, some (e.g., Metcalf and Stock
(2015) and Weyant (2015) in their articles for this symposium) might argue that my assessment
of their usefulness for policy analysis is too harsh, and that if used properly, the models could
help us formulate and evaluate alternative climate policies.                   Arguments in support of the
development and use of IAMs include the following:
    1. All models have flaws – after all, any model is a simplification of reality – and yet
         economists build and use models all the time. A related argument is that the complexity
         of climate change and its economic impact makes it especially important to have some
         kind of model to account for the dynamic interactions among key variables, and to guide
         our thinking about policy design.


3
  See, e.g., Roe and Baker (2007). Allen and Frame (2007) argue our lack of knowledge will not change in the
coming years, and that climate sensitivity is essentially “unknowable.”
4
  There is a large and growing literature on the use of weather data to estimate the impact of temperature (and other
measures of climate), especially with respect to agriculture. For surveys of this literature, see Auffhammer et al.
(2013) and Dell, Jones and Olken (2014). However, these studies are limited to short time periods and small
fluctuations in temperature and other weather variables. They do not, for example, describe what has happened over
20 or 50 years following a 5°C increase in mean temperature, and thus cannot enable us to specify and calibrate
IAM damage functions.
                                                 3


   2. Yes, there is uncertainty over climate sensitivity, and we know very little about the
       damages likely to result from higher temperatures. But can’t our uncertainty over climate
       sensitivity or the “correct” damage function be handled by assigning probability
       distributions to certain key parameters and then running Monte Carlo simulations?
   3. We have no alternative. We must develop the best models possible in order to estimate
       the social cost of carbon and/or evaluate particular policies. In other words, working with
       even a highly imperfect model is better than having no model at all.
   4. Finally, if we don’t use IAMs, how can we possibly estimate the SCC and evaluate
       alternative GHG abatement polices? Should we rely instead on expert opinion? And
       don’t experts have some kind of implicit mental models that drive their opinions? If so,
       isn’t it better to make the model explicit?
       Put simply, even with their faults, can’t IAMs be useful as a tool to help inform policy?
That is the question I address in this paper. In doing so, I respond to the arguments mentioned
above. In the next section I briefly discuss the use and misuse of IAMs, i.e., how the models can
indeed be helpful, versus how they can be misleading. I then address the question of whether our
uncertainty over climate sensitivity and climate impacts can be handled by assigning probability
distributions to various parameters and then running Monte Carlo simulations. (The answer is
no.) Next I turn to the issue of scientific honesty. I will argue that the use of IAMs to estimate
the SCC or evaluate alternative policies is in some ways dishonest, in that it creates a veneer of
scientific legitimacy that is misleading. Finally, I address the question of what we can rely on to
formulate climate policy if not IAMs. I argue that the best we can do is rely on “expert” opinion,
perhaps combined with relatively simple, transparent, and easy-to-understand models. After all,
the ad hoc equations that go into most IAMs are no more than reflections of the modeler’s own
“expert” opinion.


2. The Use and Misuse of Climate Change Models.
       Economists find models useful because they provide a logically consistent way to
organize our thinking about the relationships among variables of interest.          They help us
understand the implications of those relationships, and identify the roles of various functional
forms and parameter values. That is what made the early efforts at climate change modeling so
valuable. The models developed by Nordhaus (1991) and others over two decades ago were
                                                  4


early attempts to integrate climate science with the economic effects of greenhouse gas (GHG)
emissions. Those models helped economists understand how GHG emissions accumulate in the
atmosphere, how that accumulation can affect global mean temperatures, and how higher
temperatures might affect GDP and consumption. By including a social welfare function that
values the flow of consumption over time, the models can also be used to illustrate the possible
welfare effects of different GHG abatement policies.
       In effect, these early IAMs can be viewed as pedagogical devices. And indeed, Nordhaus
(2013) uses his DICE (Dynamic Integrated Climate and Economy) model to help explain – at a
textbook level – how unrestricted GHG emissions can cause climate change and lead to serious
problems in the future. The book also utilizes the model to illustrate some of the uncertainties
we face when thinking about the climate system and when trying to predict the changes to expect
under different policies.    The book thereby provides students (and others) with a good
introduction to climate change policy.
       So far, well and good. The problem comes up when we take these models so seriously
that we use them to try to evaluate alternative policies and/or come up with an “optimal” (i.e.,
welfare-maximizing) policy. Yes, economists often build and use models, but usually they
understand the limits of those models. They know that a model can help to tell a story in a
logically coherent way, but the model might not be able to provide the numerical details of the
story. In other words, the model might not be suitable for forecasting or quantitative policy
analysis. That is the case for the various versions of the Nordhaus DICE model, as well as the
plethora of IAMs (most much more complex than DICE) that have been developed over the past
couple of decades. As I explained in my earlier article, the key relationships and parameter
values in these models have no empirical (or even theoretical) grounding, and thus the models
cannot be used to provide any kind of reliable quantitative policy guidance.
       Is there any way around this problem? Can IAMs be salvaged as a tool for policy
analysis if we somehow account for our lack of knowledge about key relationships and
parameter values? I turn to that question next.



3. The Treatment of Uncertainty.
       Some developers of IAMs understand that there is considerable uncertainty over climate
                                                          5


sensitivity and that we don’t know what the “correct” damage function is. But they think they
have a solution to this problem. They believe that the uncertainty can be handled by assigning
probability distributions to certain parameters and then running Monte Carlo simulations.
Unfortunately this won’t help. The problem is that we don’t know the correct probability
distributions that should be applied to various parameters, and different distributions – even if
they all have the same mean and variance – can yield very different results for expected
outcomes, and thus for estimates of the SCC.5
         To make matters worse, we don’t even know the correct functional forms for some of the
key relationships. This is particularly a problem when it comes to the damage function. The
damage function used in the Nordhaus DICE model, for example, is a simple inverse-quadratic
relationship:
                                      L(T )  1/ (1  1T   2T 2 )                                     (1)
Here T is the anthropomorphic increase in temperature, and L(T) gives the reduction in GDP and
consumption for any value of T. (Thus GDP = L(T)GDP', where GDP' is what GDP would be if
there were no warming.) But remember that this damage function is made up out of thin air. It
isn’t based on any economic (or other) theory, or any data. Furthermore, even if this inverse-
quadratic function were somehow the true damage function, there is no theory or data that can
tell us the values for the parameters π1 or π2, or the correct probability distributions for those
parameters, or even the correct means and variances.
         For example, suppose we (somehow) chose probability distributions for π1 and π2. A
Monte Carlo simulation would then give us the expected loss L(T) for any particular temperature
increase T. But suppose that we then come to believe that damages are likely to rise very rapidly
as T grows, more rapidly than eqn. (1) would indicate. This might lead us to conclude that the
damage function should be an inverse-cubic, rather than quadratic. For example, we might
decide that the following damage function is preferred:
                                      L(T )  1/ (1  1T   2T 3 )                                     (2)
The Monte Carlo simulation will now give us a very different expected loss. Likewise, one

5
  In Pindyck (2013a), I took three different but plausible distributions for temperature change: a gamma distribution,
a Frechet distribution (also called a Generalized Extreme Value, Type 2 distribution), and the distribution derived by
Roe and Baker (2007). I calibrated all three distributions so they have the same mean and variance, and I
demonstrated that they imply very different values for the willingness to pay (WTP) to avoid the temperature
change. In Pindyck (2007), I discuss the implications of uncertainty for environmental policy more generally.
                                                 6


might argue that we are using the wrong probability distributions for π1 and π2, or we have the
correct distributions but the wrong means and/or variances.             Changing the probability
distributions or the means and variances of the distributions will also lead to a very different
estimate of the expected loss.
       The basic problem here is that the probability distributions are completely arbitrary, as is
the damage function that we are applying them to. What can we learn from assigning arbitrary
probability distributions to the parameters of an arbitrary function and running Monte Carlo
simulations? Nothing. The bottom line here is quite simple: If we don’t understand how A
affects B, but we create some kind of arbitrary model of how A affects B, running Monte Carlo
simulations of the model won’t make up for our lack of understanding.


4. Scientific Honesty.
       The argument is sometimes made that we have no choice; without a model we will end
up relying on biased opinions, guesswork, or even worse. Thus we must develop the best models
possible, and use them to evaluate alternative policies. In other words, working with even a
highly imperfect model is better than having no model at all. That might be true if we were
honest and upfront about the limitations of the model. But often we are not.
       Models sometimes convey the impression that we know much more than we really do.
They create a veneer of scientific legitimacy that can be used to bolster the argument for a
particular policy. This is particularly the case for IAMs, which tend to be large and complicated,
and are sometimes poorly documented. IAMs are typically made up of many equations, and the
equations are hard to evaluate individually (especially given that they are often ad hoc and
without a theoretical or empirical foundation), and even harder to understand in terms of their
interactions as a complete system. In effect, the model is just a black box: we put in some
assumptions about GHG emissions, discount rates, etc., and we get out some results about
temperature change, damages, etc. And because the black box is “scientific,” we are supposed to
take those results seriously and use them for policy analysis.
       To understand the problem, go back 40 years or so and recall the “Limits to Growth”
controversy. It was based on a simple sequence of ideas that appeared quite reasonable to some
environmentalists at the time: (1) The earth contains finite amounts of oil, coal, copper, iron, and
other nonrenewable resources. (2) These resources are important inputs for the production of a
                                                  7


large fraction of GDP. (3) Because they are finite, we will eventually run out of these resources.
In fact, partly because of population growth, we are likely to run out very soon. (4) When we
run out, the world’s developed economies will contract dramatically, greatly reducing our
standard of living, and even causing wide-spread poverty. (5) Therefore, we should immediately
and substantially reduce our use of natural resources (and slow or stop population growth). This
will reduce our standard of living now, but will give us time to adapt and will push back (or even
avoid) that day of reckoning when our resources run out and we are reduced to abject poverty.
       Points (1) and (2) are indisputable.       Points (3), (4) and (5), however, ignore basic
economics. As reserves of oil, copper, and other resources are depleted, the costs of extraction
and therefore the prices of these resources will rise, causing their use to decline. Higher prices
also create the incentive to find substitutes. Thus we may never actually run out of these
resources, although we will eventually stop using them. Most important, given the incentives
created by rising prices and the likelihood of finding substitutes, there is no reason to expect the
gradual depletion of natural resources to result in economic decline. Indeed, partly due to
technological change and partly due to the discovery of new reserves, the real prices of most
resources have gone down over the past 40 years, and there is no evidence that reserve depletion
has been or is likely to be a drag on economic growth.
       Although it made little economic sense, the “Limits to Growth” argument gained
considerable traction in the press and in public discourse over environmental policy. This was
due in part to a lack of understanding of basic economics on the part of the public (and many
politicians). But it was also due to the publication and promotion of some simulation models
that gave the “Limits” argument a veneer of scientific legitimacy. The most widely promoted
and cited models were those of Forrester (1973) and Meadows et al (1974). These models were
actually quite simple; as Nordhaus (1973, 1992) and others explained, they essentially boiled
down to an elaboration of points (1) to (5) above, with some growth rates and other numbers
attached. What mattered, however, was that these models required a computer to solve and
simulate. The fact that some of the underlying relationships in the models were completely ad
hoc and made little sense didn’t matter – the fact that they were computer models made them
“scientific” and inspired a certain degree of trust.
       Still another example of an attempt to create a veneer of scientific legitimacy is the
“technical analysis” used by stock market analysts to make buy/sell recommendations for
                                                8


particular stocks and for the market as a whole. Sometimes this involves a formal computer
model, and sometimes just an “analysis” of the up-and-down movements of what is essentially a
random walk.     By dressing up the “analysis” with terms like “resistance levels,” “support
points,” potential or actual “breakouts,” etc., the buy/sell recommendations of technical analysts
are given a scientific aura: Uninformed (or misinformed) investors think these recommendations
are based on some kind of “science,” even though countless studies have shown that “technical
analysis” is totally uninformative.
       I do not mean to equate IAMs with the “Limits to Growth” models of the early 1970s,
never mind the models used by those who promote the “technical analysis” of stock prices.
Developers of IAMs generally try to base their models’ equations as much as possible on climate
science and economic principles. Unfortunately, climate science and economic principles can’t
tell us much about how these equations should be specified and parameterized, which is why
IAMs cannot tell us much about the design of climate policy. The problem has been that the
developers and users of IAMs have tended to oversell their validity, and have failed to be clear
about their inadequacies. The result is that policy makers who rely on the projections of IAMs
are being misled.
       I believe that we need to be much more honest and upfront about the inherent limitations
of IAMs. Claiming that IAMs can be used to evaluate policies and determine the SCC is
misleading to say the least, and gives economics a bad name. If economics is indeed a science,
scientific honesty is paramount.


5. Isn’t the Use of IAMs the Best We Can Do?
       Suppose our job is to come up with an estimate of the SCC, which will be used as a basis
for determining the size of a carbon tax. We know that IAMs are deeply flawed, but aren’t they
still the best game in town? If we acknowledge these flaws and explain that the projections and
SCC estimates that are generated have large standard errors, isn’t the use of one or more IAMs
better than having no model? Not necessarily. Putting aside the question of scientific honesty,
there are three additional problems with the use of IAMs.
5.1. The Modeler Has Too Much Flexibility.
       Put simply, it is much too easy to use a model to generate, and thus seemingly validate,
the results one wants. Take any one of the three IAMs that were used by the Interagency
                                                 9


Working Group (2010, 2013) to estimate the SCC.6 With a judicious choice of parameter values
(varying the discount rate is probably sufficient), the model will yield an SCC estimate as low as
a few dollars per ton, as high as several hundred dollars per ton, or anything in between. Thus a
modeler who, for whatever reason, believes that a stringent abatement policy is (or is not)
needed, can choose a low (or high) discount rate, or choose other inputs that will yield the
desired results.
        The Interagency Working Group did not try to determine the “correct” values for the
discount rate. Instead, they used middle of the road assumptions about the discount rate as well
as other parameters, and arrived at an estimate of around $33 per ton for the SCC (recently
updated to $39 per ton). But other well-known studies have deviated from using these middle-
of-the-road assumptions and arrived at very different estimates of the SCC. For example, using a
version of his DICE model (one of the three models used by the Interagency Working Group),
Nordhaus (2011) obtained an estimate of $11 per ton for the SCC. On the other hand, Stern
(2007), using the PAGE model, obtained optimal abatement policies consistent with an SCC of
over $200 per ton. Although the models differed, the main reason for these wildly different SCC
estimates is that Nordhaus used a relatively high discount rate, and Stern a relatively low rate.
        The problem here is that there is no consensus regarding the “correct” discount rate. (The
Interagency Working Group simply chose a mid-range number – 3 percent – that the members of
the Group could all live with; the Group’s reports never claimed that this number was in any
sense “correct.”) Because reasonable arguments can be made for a low discount rate or for a
high rate, the modeler simply has too much flexibility. If the modeler is at all biased towards a
more or less stringent abatement policy, he/she can choose the discount rate accordingly. And
while I have focused on the discount rate, IAMs have other parameters whose choice can lead to
a higher or lower SCC estimate, as I discuss in Pindyck (2013a).
5.2. The Choice of Model is Largely Irrelevant.
        Suppose we could take away the flexibility that the modeler has in choosing parameters.
Perhaps some government agency tells the developer of each model to use a specific set of
parameter values, including the discount rate. Are we then home free?


6
  The three IAMS were DICE (Dynamic Integrated Climate and Economy), PAGE (Policy Analysis of the
Greenhouse Effect), and FUND (Climate Framework for Uncertainty, Distribution, and Negotiation). For
descriptions of the models, see Nordhaus (2008), Hope (2006), and Tol (2002).
                                                         10


         You might say that first we need to decide which model to rely on. But the choice of
model illustrates a second problem. Let’s go back to the wildly different SCC estimates of
Nordhaus ($11) and Stern ($200 plus). Which one should we rely on? The answer boils down
almost entirely to our belief about the discount rate. The choice of model – DICE versus PAGE
versus some other IAM – doesn’t matter all that much. Yes, for a fixed set of parameter values
DICE will give a different SCC estimate than PAGE, but the difference will be small compared
to the effect of changing the discount rate for any one model.
         If one believes that we should use market-based discount rates (i.e., the rates we actually
observe in financial markets), then $11 is roughly the right number for the SCC. But if instead,
one believes (perhaps based on some kind of “ethical” argument regarding intergenerational
welfare comparisons) that we should use a very low discount rate, then $200 or so is the right
number. The point here is that there is hardly any need for a model; decide on the discount rate,
and you pretty much have an estimate of the SCC. The model itself is almost a distraction.
         Why is the SCC determined almost entirely by the discount rate, rather than by the
specific IAM used in the analysis? The reason is that the impact of GHG emissions on climate is
a very slow and gradual process. Even with no abatement, most studies indicate that any
significant warming will not occur for several decades. Thus the costs of a GHG abatement
policy are incurred starting now, but most of the benefits come in the distant future. If those
future benefits are discounted at a market-based rate (say around 5%), their present value will be
very small, and the implied SCC will be very small. To get a large SCC, we need to discount
future benefits at a very low rate (say around 1%). So, is the SCC small or large? To answer
that, we only have to agree on the discount rate.7 We don’t have to agree on which model to use.
5.3. Catastrophic Outcomes.
         As I stated in the Introduction and explained in detail in my 2013 articles, what really
matters for the SCC is the likelihood and possible impact of a catastrophic climate outcome: a
much larger-than-expected temperature increase and/or a much larger-than-expected reduction in
GDP caused by even a moderate temperature increase. IAMs, however, simply cannot account



7
 Economists are sharply divided on the discount rate that should be used for the analysis of climate change policy.
There is a large and growing literature on the discount rates (plural, because some argue that the rate should decline
over time) that should be used for very long time horizons. For an overview, see Gollier (2013).
                                                    11


for catastrophic outcomes. It is easy to see why by looking at eqns. (1) and (2), which are
alternative representations for the damage function.
           Eqn. (1) is the damage function used in the DICE model. The parameters π1 and π2 are
chosen to be roughly consistent with the common wisdom regarding the loss of GDP likely to
result from T in the range of 1° to 4°C. That common wisdom, which might be totally wrong,
puts the loss for these kinds of temperature increases at a few percent. The problem is that these
damage functions tell us nothing about what to expect if the temperature increases are larger,
e.g., 5º C or more. Given the arbitrary nature of eqn. (1), putting in T = 5 or 7 is a meaningless
exercise, and will tell us nothing about the damages we should expect if the temperature were
indeed to increase this much. Because of the cubic term, eqn. (2) will yield much higher damage
numbers for T = 5° or more, but eqn. (2) is just as arbitrary, and the damage numbers will be just
as meaningless.
           How do we know that the possibility of a catastrophic outcome is what matters for the
SCC? Because unless we are ready to accept a discount rate that is very small, the “most likely”
scenarios for climate change simply don’t generate enough damages – in present value terms – to
matter.8 That is why the Interagency Working Group, which used a 3 percent discount rate,
obtained the rather low estimate of $33 per ton for the SCC.


6. So What to Do?
           Focusing on catastrophic outcomes actually simplifies the problem somewhat. First, it is
only economic outcomes that matter, not the causes of the outcomes. In other words, it doesn’t
matter whether a large drop in GDP is the result of a dramatic increase in temperature (but a
moderate effect of temperature on output) or a moderate increase in temperature (but a dramatic
effect of temperature on output). What we have to worry about is the possibility of a drop in
GDP so large as to be considered catastrophic. (Of course climate change could also cause non-
economic damages, such as greater morbidity and mortality, the extinction of species, and social
disruptions. I am assuming – as is typically done in the estimation of the SCC – that these non-
economic damages could all be monetized and included as part of the drop in GDP.)



8
    I show this formally in Pindyck (2011, 2012).
                                               12


       Starting with some scenario for GHG emissions (e.g., no abatement), we could therefore
begin by considering a plausible range of catastrophic outcomes, as measured by percentage
declines in GDP broadly defined. Next, what are plausible probabilities that we can attach to
these possible outcomes? Here, “plausible” would mean acceptable to a range of economists and
climate scientists. Given these plausible outcomes and probabilities, one can calculate the
present value of the benefits from averting those outcomes, or reducing the probabilities of their
occurrence. In present value terms, the benefits will depend on the discount rate and perhaps
other parameters, but if those benefits are sufficiently large and robust to reasonable ranges for
those parameters, it would support a stringent abatement policy. Let’s denote this present value
of benefits by B.
       The second step would be to ask how great the reduction in annual CO2 emissions would
have to be to avoid these catastrophic outcomes. Sum these annual reductions over some time
horizon (say 50 or more years), and denote the total reduction by ΔE. Given B and ΔE, a rough
estimate of the SCC is just B/ΔE.
       Determining plausible outcomes and probabilities, and the emission reductions needed to
avert these outcomes, would mean relying on “expert” opinion. For an economist, this is not
very satisfying.    Economists often build models to avoid relying on subjective (expert or
otherwise) opinions. But remember that the inputs to IAMs (equations and parameter values) are
already the result of “expert” opinion; in this case the modeler is the “expert.” And of course
experts are likely to disagree, particularly when it comes to climate change, where our
knowledge is so limited. On the other hand, focusing on the extreme tail (i.e., catastrophic
outcomes), and the emission reductions needed to eliminate that tail, may reduce the extent of
disagreement, and will center the debate on what really matters as the driver of policy.
Compared to agreeing on the details of some IAM, it should be relatively easy for climate
scientists and economists to reach a consensus on the answers to the questions raised above, or at
least agree on a range of answers.
       In effect, we would use expert opinion to determine the inputs to a simple, transparent,
and easy-to-understand model (and I stress the importance of easy-to-understand).          As an
example of how this might be done, start with three or four potential catastrophic outcomes that,
under BAU, might occur at, say, 50 years in the future. Those outcomes might be a 10%, 30%,
or 50% drop in GDP and consumption (or something worse). Now attach probabilities to those
                                                 13


outcomes, say .2, .1, and .05 respectively (so the probability of no catastrophe is .65). Given
these outcomes and probabilities, and given a discount rate, we can calculate the present value of
the expected benefits from avoiding these outcomes. Next, come up with an estimate (or set of
estimates and associated probabilities) of the reduction in CO2 emissions needed to eliminate the
catastrophic scenarios. A simple ratio then gives us an estimate of the SCC. Of course the result
will still depend on the discount rate that is used, so we might use a range of discount rates.
       Yes, the calculations I have just described constitute a “model,” but one that is
exceedingly simple and straightforward, and involves no pretense that we know the damage
function, the feedback parameters that affect climate sensitivity, or other details of the climate-
economy system. And yes, some experts might base their opinions on one or more IAMs, on a
more limited climate science model, or simply on their research experience and/or general
knowledge of climate change and its impact. That’s fine, because we are using a range of expert
opinions to summarize our current understanding of catastrophic climate outcomes, and the
range of disagreement over those outcomes.
       Some might argue that the approach I have outlined above is insufficiently precise. But I
believe we have no choice. Building and using elaborate models might let us think that we are
approaching the climate policy problem more scientifically, but like the Wizard of Oz, we would
just be drawing a curtain around our lack of knowledge.


7. Conclusions.
       I have stressed that as economists, we need to be honest about what we know and don’t
know about climate change and its impact. Just as financial economists would (or should) be
ashamed to sell “technical analysis” to investors, environmental economists should be ashamed
to claim that IAMs can forecast climate change and its impact, or tell us what the SCC is.
       Atmospheric scientists have made great progress in understanding how weather patterns
develop and change, but they don’t claim to be able to forecast next month’s weather or when the
next hurricane will arrive. There has also been great progress in our understanding of the drivers
of climate, how GHG emissions can affect climate, and (to a lesser extent) how changes in
climate can affect GDP and other economic variables. But that progress does not enable us to
build and use IAMs as tools for forecasting and policy analysis, and we would be deluding
ourselves if we thought otherwise.
                                               14


       It would be nice if the problem were simply imprecise knowledge of certain parameters,
so that uncertainty could be handled by assigning probability distributions to those parameters
and then running Monte Carlo simulations. Unfortunately, not only don’t we know the correct
probability distributions that should be applied to these parameters – we can’t even write down
the correct equations to which those parameters apply.
       This does not mean we have to throw up our hands and give up on the estimation of the
SCC and the analysis of climate change policy more generally. I have argued that the problem is
somewhat simplified by the fact that what matters for policy is the possibility of a catastrophic
climate outcome. How probable is such an outcome (or set of outcomes), and how bad would
they be? And by how much would emissions have to be reduced to avoid these outcomes? I
have argued that the best we can do at this point is come up with plausible answers to these
questions, perhaps relying at least in part on consensus numbers supplied by climate scientists
and environmental economists. This kind of analysis would be simple, transparent, and easy-to-
understand. It might not inspire the kind of awe and sense of scientific legitimacy conveyed by a
large-scale IAM, but that is exactly the point. It would draw back the curtain and clarify our
beliefs about climate change and its impact.
                                                15


                                           References

Allen, Myles R., and David J. Frame, “Call Off the Quest,” Science, October 26, 2007, 318,
       582—583.

Auffhammer, Maximilian, Solomon M. Hsiang, Wolfram Schlenker, and Adam Sobel, “Using
      Weather Data and Climate Model Output in Economic Analysis of Climate Change.”
      National Bureau of Economic Research Working Paper 19087, 2013.

Dell, Melissa, Benjamin F. Jones, and Benjamin A. Olken, “What Do We Learn from the
       Weather? The New Climate-Economy Literature,” Journal of Economic Literature, Sept.
       2014, 52(3), 740—798.

Forrester, Jay W., World Dynamics, Wright-Allen Press, 1973.

Freeman, Mark C., Gernot Wagner, and Richard J. Zeckhauser, “Climate Sensitivity
      Uncertainty: When is Good News Bad?” National Bureau of Economic Research
      Working Paper 20900, January 2015.

Gollier, Christian, Pricing the Planet’s Future, Princeton University Press, 2013.

Greenstone, Michael, Elizabeth Kopits, and Ann Wolverton, “Developing a Social Cost of
       Carbon for U.S. Regulatory Analysis: A Methodology and Interpretation,” Review of
       Environmental Economics and Policy, 2013, 7(1), 23—46.

Hope, Chris. 2006. “The Marginal Impact of CO2 from PAGE2002: An Integrated Assessment
      Model Incorporating the IPCC’s Five Reasons for Concern.” Integrated Assessment 6 (1):
      19–56.

Interagency Working Group on Social Cost of Carbon, “Technical Support Document: Social
       Cost of Carbon for Regulatory Impact Analysis,” U.S. Government, February 2010.

Interagency Working Group on Social Cost of Carbon, “Technical Support Document: Technical
       Update of the Social Cost of Carbon for Regulatory Impact Analysis,” U.S. Government,
       2013.

Meadows, Donella H., Dennis L. Meadows, Jorgen Randers, and William W. Behrens III, The
     Limits to Growth: A Report for the Club of Rome’s Project on the Predicament of
     Mankind, Universe Books, 1974.

Metcalf, Gilbert E., and James Stock, “The Role of Integrated Assessment Models in Climate
       Policy: A User's Guide and Assessment,” Review of Environmental Economics and
       Policy, 2015 (this issue).
                                             16


Nordhaus, William D., “World Dynamics: Measurement Without Data,” The Economic Journal,
      Dec. 1973, Vol 83, No. 332 .

Nordhaus, William D., “To Slow or Not to Slow: The Economics of the Greenhouse Effect,” The
      Economic Journal, 1991, 101, 920—937.

Nordhaus, William D., “Lethal Model 2: The Limits to Growth Revisited,” Brookings Papers on
      Economic Activity, 1992:2.

Nordhaus, William D., A Question of Balance: Weighing the Options on Global Warming
      Policies, Yale University Press, 2008.

Nordhaus, William D., “Estimates of the Social Cost of Carbon: Background and Results from
      the RICE-2011 Model,” National Bureau of Economic Research Working Paper 17540,
      2011.

Nordhaus, William D., The Climate Casino, Yale University Press, 2013.

Pindyck, Robert S., “Uncertainty in Environmental Economics,” Review of Environmental
      Economics and Policy, Winter 2007.

Pindyck, Robert S., “Modeling the Impact of Warming in Climate Change Economics,” Chapter
      2 in G. Libecap and R. Steckel (Eds.), The Economics of Climate Change, University of
      Chicago Press, 2011.

Pindyck, Robert S., “Uncertain Outcomes and Climate Change Policy,” Journal of
      Environmental Economics and Management, February 2012.

Pindyck, Robert S., “The Climate Policy Dilemma” Review of Environmental Economics and
      Policy, Summer 2013a, 7(2), 219—237.

Pindyck, Robert S., “Climate Change Policy: What Do the Models Tell Us?” Journal of
      Economic Literature, September 2013b, 51(3), 860—872.

Roe, Gerard H., and Marcia B. Baker, “Why is Climate Sensitivity So Unpredictable?" Science,
      October 26, 2007, 318, 629—632.

Stern, Nicholas, The Economics of Climate Change: The Stern Review, Cambridge University
       Press, 2007.

Tol, R., “Estimates of the Damage Costs of Climate Change, Part I: Benchmark Estimates,”
       Environmental and Resource Economics, 2002, 22, 47—73.

Weyant, John, “Contributions of Integrated Assessment Models,” Review of Environmental
      Economics and Policy, 2015 (this issue).
