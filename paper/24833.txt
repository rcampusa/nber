                               NBER WORKING PAPER SERIES




                                   A CAUSAL BOOTSTRAP

                                         Guido Imbens
                                         Konrad Menzel

                                       Working Paper 24833
                               http://www.nber.org/papers/w24833


                     NATIONAL BUREAU OF ECONOMIC RESEARCH
                              1050 Massachusetts Avenue
                                Cambridge, MA 02138
                                     July 2018




The views expressed herein are those of the authors and do not necessarily reflect the views of the
National Bureau of Economic Research.

At least one co-author has disclosed a financial relationship of potential relevance for this
research. Further information is available online at http://www.nber.org/papers/w24833.ack

NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.

© 2018 by Guido Imbens and Konrad Menzel. All rights reserved. Short sections of text, not to
exceed two paragraphs, may be quoted without explicit permission provided that full credit,
including © notice, is given to the source.
A Causal Bootstrap
Guido Imbens and Konrad Menzel
NBER Working Paper No. 24833
July 2018
JEL No. C01,C31

                                        ABSTRACT

The bootstrap, introduced by Efron (1982), has become a very popular method for estimating
variances and constructing confidence intervals. A key insight is that one can approximate the
properties of estimators by using the empirical distribution function of the sample as an
approximation for the true distribution function. This approach views the uncertainty in the
estimator as coming exclusively from sampling uncertainty. We argue that for causal estimands
the uncertainty arises entirely, or partially, from a different source, corresponding to the
stochastic nature of the treatment received. We develop a bootstrap procedure that accounts for
this uncertainty, and compare its properties to that of the classical bootstrap.


Guido Imbens
Graduate School of Business
Stanford University
655 Knight Way
Stanford, CA 94305
and NBER
Imbens@stanford.edu

Konrad Menzel
New York University
19 West 4th Street, 6 Floor
New York, NY 10012
km125@nyu.edu
                                   A CAUSAL BOOTSTRAP

                                 GUIDO IMBENS          KONRAD MENZEL


       Abstract. The bootstrap, introduced by Efron (1982), has become a very popular method
       for estimating variances and constructing confidence intervals. A key insight is that one can
       approximate the properties of estimators by using the empirical distribution function of
       the sample as an approximation for the true distribution function. This approach views
       the uncertainty in the estimator as coming exclusively from sampling uncertainty. We argue
       that for causal estimands the uncertainty arises entirely, or partially, from a different source,
       corresponding to the stochastic nature of the treatment received. We develop a bootstrap
       procedure that accounts for this uncertainty, and compare its properties to that of the
       classical bootstrap.
       JEL Classification:
       Keywords: Potential Outcomes, Causality, Randomization Inference, Bootstrap, Copula,
       Partial Identification




                                           1. Introduction

1.1. Problem Description. Using the potential outcome framework, e.g., Imbens and Ru-
bin (2015), we are interested in the average causal effect of a binary variable Wi ∈ {0, 1} (the
“treatment”) on an outcome variable whose potential outcomes we denote with Yi (0), Yi (1),
for a population of N units i = 1, . . . , N. Implicitly we assume that the potential outcomes
Yi (w) for unit i do not vary with the treatment status assigned to other units, known as the
Stable Unit Treatment Value Assumption (SUTVA, Rubin (1978)). For all units in the pop-
ulation we observe the treatment Wi and the realized outcome Yi := Yi (Wi ). One common
estimand is the average effect for the N units in the population:
                                                  N
                                               1 X                 
                                    τAT E :=         Yi (1) − Yi (0) .                                     (1.1)
                                               N i=1
We assume that the data arise from a completely randomized experiment, where n ≤ N units
are selected at random from the population as experimental subjects, of which n1 units are
then randomly assigned to receive the treatment, and the remaining n0 = n − n1 units are
assigned to the control group. We let Ri ∈ {0, 1} denote an indicator whether the ith unit
is included in the sample.

Date: March 2018.
                                                      1
  Specifically, for R := (r1 , . . . , rN )′ and W := (w1 , . . . , wN )′ we have
                               N −1
                         (
                                              if r ∈ {0, 1}N and
                                                                    P
                                n
                                                                        ri = n
        pr(R = r) =
                                  0           otherwise
                         (
                                n −1
                                                    ri wi = n1 , w ∈ {0, 1}N and (1 − ri )wi = 0 for all i
                                                 P
                               n
                                               if
pr(W = w|R = r) =                1

                                  0            otherwise
1.2. Sampling Uncertainty and Design Uncertainty. We wish to distinguish between
two types of uncertainty in estimators, sampling uncertainty arising from the stochastic
nature of R, and design uncertainty arising from the stochastic nature of W.
   To characterize sampling uncertainty we postulate the existence of a large, possibly infinite,
population. We draw a random sample from this population, and observe for each unit in
this sample a set of values, say, a pair (Yi , Wi ). We may be interested in in the difference
between the population averages of Yi for the subpopulations with Wi = 0 and Wi = 1.
We can estimate this object using the difference in average outcomes by Wi values in the
sample. This estimator differs from the target because we do not observe all units in the
population. Had we drawn a different random sample, with different units, the value of the
estimator would have been different. See Table 1, where Ri is the sampling indicator, equal
to 1 for sampled units and 0 otherwise. The uncertainty arising from the randomness in R
is captured by the conventional standard error.
      Table 1. : Sampling-based Uncertainty (X is observed, ? is missing)



                      Actual                Alternative           Alternative      ...
               Unit   Sample                 Sample I              Sample II       ...
                    Yi Wi Ri                Yi Wi Ri              Yi Wi Ri         ...

                 1     X    X    1          ?    ?     0          ?    ?     0     ...
                 2     ?    ?    0          ?    ?     0          ?    ?     0     ...
                 3     ?    ?    0          X    X     1          X    X     1     ...
                 4     ?    ?    0          X    X     1          ?    ?     0     ...
                 ..    ..   ..   ..         ..   ..    ..         ..   ..    ..
                  .     .    .    .          .    .     .          .    .     .    ...
                 N     X    X    1           ?    ?    0           ?    ?    0     ...

  In a randomized experiment the uncertainty is not necessarily of this sampling variety.
Instead we can think of the uncertainty arising from the stochastic nature of the assignment,
W. For units with Wi = 0 we observe Yi (0), and for units with Wi = 1 we observe Yi (1).
In our sample units have a particular set of assignments. In a repeated sampling thought
experiment the units in the sample would have remained the same, but their assignments
would might been different, leading to a different value for the estimator. See Table 2.
                                                  2
        Table 2. : Design-based Uncertainty (X is observed, ? is missing)



                  Actual                        Alternative                   Alternative         ...
       Unit       Sample                         Sample I                     Sample II           ...
            Yi (1) Yi (0) Wi                Yi (1) Yi (0) Wi              Yi (1) Yi (0) Wi        ...

        1       X        ?      1             X        ?     1             ?       X       0      ...
        2       ?        X      0             ?        X     0             ?       X       0      ...
        3       ?        X      0             X        ?     1             X       ?       1      ...
        4       ?        X      0             ?        X     0             X       ?       1      ...
        ..      ..       ..     ..            ..       ..    ..            ..      ..      ..
         .       .        .      .             .        .     .             .       .       .     ...
        N       X        ?      1             ?        X     0             ?       X       0      ...


1.3. The Bootstrap. The classical bootstrap corresponds to the case where the uncer-
tainty is purely sampling uncertainty. The bootstrap approximates the cumulative distribu-
tion function of the pairs (Yi , Wi ), FY W (·, ·) in the population by the empirical distribution
function F̂Y W (·, ·), where
                                                   N
                                              1X
                              F̂Y W (w, y) :=       Ri 1l{Yi ≤ y, Wi ≤ w}.
                                              n i=1

It then calculates properties of the estimator given that approximate distribution F̂Y W (·, ·).
One can interpret the standard bootstrap as imputing all the missing values of (Yi , Wi )
in the population by replicates of the observed values, and thus constructing an artificial
population from which we then draw random samples. This perspective is helpful to contrast
the different approach underlying the causal bootstrap.


1.4. Notation. In the following, we denote the distribution of potential outcomes in the
                       p
                         (y0 , y1) := N1 N
                                        P
population with F01                       i=1 1l{Yi (0) ≤ y0 , Yi (1) ≤ y1 } and the size of that pop-
                                                                                              s
ulation with N. The distribution in the sample of size n is denoted with F01                    (y0 , y1 ) :=
1
  PN                                             s
n     i=1 Ri 1l{Yi (0) ≤ y0 , Yi (1) ≤ y1 }. F01 (·, ·) is not quite the empirical distribution be-
cause we only observe one of the values in each pair (Yi (0), Yi (1)). We use p superscripts
throughout to indicate population quantities, and s superscripts to denote their sample
analogs. The number of treated units in the sample is denoted with n1 , the number of
control units with n0 = n − n1 , and the respective shares of treated and control units with
p := n1 /n, so that 1 − p = n0 /n. We also define the empirical c.d.f. for either potential
outcome given the randomized treatment as F̂0 (y0 ) := n10 N
                                                                    P
                                                                      i=1 Ri (1 − Wi )1l{Yi (0) ≤ y0 } and
              1
                PN
F̂1 (y1 ) := n1 i=1 Ri Wi 1l{Yi (1) ≤ y1 }.
                                                       3
             2. The Causal Bootstrap for Average Treatment Effects

   In this section we consider causal bootstrap inference for the population average treatment
effect τAT E defined in (1.1). The estimator we use is the difference in sample averages by
treatment status:
                                       τ̂AT E := Y 1 − Y 0 ,
where
                                N                                    N
                            1 X                                  1 X
                     Y 1 :=        Ri Wi Yi ,         and Y 0 :=        Ri (1 − Wi )Yi .
                            n1 i=1                               n0 i=1
The repeated sampling perspective we take is one where the potential outcomes (Yi (0), Yi (1))
are fixed for all N units in the population. The stochastic properties of the estimator
arise from the stochastic nature of the assignment and sampling, which are both sources
of randomness in the average of realized outcomes by treatment status, where we regard
n, n0 , n1 as fixed.


2.1. The True Variance of the Estimator for the Average Treatment Effect. Here
we present the true variance of the estimator τ̂AT E under random assignment of the treat-
ment. From the n experimental subjects, n1 are selected at random to receive the active
treatment, and the remainder are assigned to the control group. Define
                                         N                                    N
                                      1 X                                  1 X
                            Y (0) =         Yi (0),         Y (1) =              Yi (1),
                                      N i=1                                N i=1
                             N                                                  N
                       1 X                   2                       1 X                   2
             S02   =           Yi (0) − Y (0) ,             S12   =           Yi (1) − Y (1) ,
                     N − 1 i=1                                      N − 1 i=1
and
                                                 N
                                           1 X
                                  2
                                 S01 =             (Yi (1) − Yi (0) − τAT E )2 .
                                         N − 1 i=1
Then the exact variance of τ̂ , over the randomization distribution, is
                                                    S02 S12 S012
                                           V(τ̂ ) =    +    −    .
                                                    n0   n1   N
See, for example, Neyman (1923,1990), Aronow, Green, and Lee (2014), Ding (2017), and
Abadie, Athey, Imbens, and Wooldridge (2017).


2.2. An Analytical Variance Estimator. Define
                      N                                                               N
               1 X                          2                                  1 X                   2
   Ŝ02   =            Ri (1 − Wi ) Yi − Y 0 ,              and     Ŝ12   =            Ri Wi Yi − Y 1 .
            n0 − 1 i=1                                                       n1 − 1 i=1
                                                        4
Then the standard variance estimator is
                                                              Ŝ02 Ŝ12
                                              V̂Neyman :=         + .
                                                              n0   n1
This estimator ignores the third term in the variance, which is negative, so V̂Neyman in
                                                                                    2
general overestimates the true variance. It is possible to give sharp bounds for S01   given
the respective marginal distributions of Yi (0) and Yi (1). Aronow, Green, and Lee (2014)
proposed a consistent estimator for the resulting bounds on V(τ̂ ) that can be expressed as
                                                       Ŝ02 Ŝ12 Ŝ 01
                                            V̂AGL :=       +    −
                                                       n0    n1   N
           ∗                                              2 1
where Ŝ 01 is an estimator of the sharp lower bound for S01 .

2.3. The Causal Bootstrap. Here we initially take the perspective that the uncertainty
is solely arising from the stochastic nature of the assignment, as in Table 2. In the spirit
of the above interpretation of the standard bootstrap, we use the observed data to impute
all the missing values in the population. Then we simulate the estimator using this partly
imputed population.
   The difference with the standard bootstrap is in the nature of the missing data process,
and how we impute them. Consider unit 1 in Table 2. In the actual sample this unit receives
the active treatment, and so we observe Y1 (1), but we do not know the value of the control
outcome for this unit, Y1 (0).
   A natural approach is to impute the missing value of Y1 (0) using one of the observed values
for Yi (0), that is, one of the realized values of Yi for control units. The question is which one
to use. It turns out that it matters how we choose to impute the missing values from the
                                                         2
observed values. This issue is related to the term S01      in the true variance of the estimator τ̂
for the average treatment effect, the term that is not consistently estimable, and which we
typically ignore in practice.
   To frame this question, it is useful to start with the joint distribution function of the pairs
of potential outcomes in the population,
                                                  N
                             p                 1 X
                            F01 (y0 , y1)   :=       1l {Yi (0) ≤ y0 , Yi (1) ≤ y1 } .
                                               N i=1
The average treatment effect, and any other causal parameters of interest, can be written as
a functional of this distribution,
                                                p
                                       τ := τ (F01 ).
1Such   an estimator is
                                            Ŝ 01 := Ŝ02 + Ŝ12 − 2σ̂N
                                                                      H
                                                                        (y0 , y1 )
        H
where σ̂N (y0 , y1 ) is a consistent estimator for the upper bound for Cov(Yi (0), Yi (1)), see equation (8) of their
paper.
                                                          5
         p
Given F01  , the assignment mechanism completely determines the distribution of any esti-
mator, for example the difference in averages by treatment status, τ̂ . This is similar to the
way in which in the sampling case knowledge of the joint population distribution allows us
to deduce the properties of any estimator.
  The problem, and the main difference with the sampling case is that for each unit in the
population, at most one of the two potential outcomes Yi (0) and Yi (1) is observed so that
                                         p
there is no consistent estimator for F01   (·, ·): In general, the joint distribution of potential
values can be written as
                                p
                              F01 (y0 , y1 ) = C(F0p (y0 ), F1p (y1 ))
where the copula C : [0, 1]2 7→ [0, 1] is a function that is nondecreasing in either argument
for each value of x. By Sklar’s theorem (e.g. stated as Theorem 2.3.3 in Nelsen (2006)), such
a copula exists even though it need not be unique unless the marginal distributions F0p , F1p
are continuous. In the following, we let

                 C := C : [0, 1]2 7→ [0, 1], C(u, v) nondecreasing in u and v
                      

denote the set of all possible copulae.
  It is important to note that although the marginal distributions F0p , F1p can be estimated
consistently from a completely randomized experiment as sample size grows, the data on
realized treatments and outcomes impose no empirical restrictions on the copula C(u, v) for
                                                                             s
the joint distribution of (Yi (0), Yi (1)). Hence, neither the parameter τ (F01 ) = τ (C(F0s , F1s ))
nor the distribution of an estimator τ̂ need in general not be point-identified.
   In the spirit of the variance estimator in Aronow, Green, and Lee (2014), we address
this challenge by simulating the distribution of τ̂ using an estimator for the population
                p
distribution F01  that is conservative with respect to the copula in a sense to be made more
precise below. To illustrate the broader conceptual idea, consider an estimator

                                                τ̂ := τ (F̂0 , F̂1 )

for a general functional τ (F01 ) of the distribution of potential values. Under regularity
conditions,2 such an estimator admits a stochastic expansion of the form
                              p         p               p              p
                     τ̂ − τ (F01 ) = µ(F01 ) + n−1/2 σ(F01 )Z + n−1 κ(F01 ) + oP (n−1 )

where Z ∼ N(0, 1). The first-order “bias” term
                                            p                        p
                                         µ(F01        p [τ̂ ] − τ (F
                                               ) := EF01            01 )

and the scale parameter
                                               p
                                         σ 2 (F01 ) := lim nVarF01 (τ̂ )
                                                        N


2See   e.g. Bloznelis and Götze (2001) for regularity conditions for finite-population expansions of this type.
                                                         6
                                                                   p
are deterministic functions of the unknown distribution F01          = C(F0p , F1p ), and the limit
for the asymptotic variance is taken as N and n := nN grow large. The second-order
                             p
approximation error κ(F01      ) is a tight random variable whose distribution also depends on
  p
F01 .
                         p                                     p
   If the functional τ (F01 ) is not point-identified, then µ(F01 ) may take values in a set whose
bounds may be characterized in terms of the marginal distributions F0p , F1p . Specifically,
given the marginal distributions F0p , F1p we have sharp bounds of the form

        µL (F0p , F1p ) := inf µ(C̃(F0s , F1s ) ≤ µ(F01
                                                     s
                                                        ) ≤ sup µ(C̃(F0s , F1s )) =: µU (F0p , F1p ),
                            C̃∈C                             C̃∈C

that are generally available, see e.g. Heckman, Smith, and Clements (1997) and Manski
(1997). Similarly we can form bounds for the variance,

         σL (F0p , F1p ) := inf σ(C̃(F0s , F1s ) ≤ σ(F01
                                                      s
                                                         ) ≤ sup σ(C̃(F0s , F1s )) =: σU (F0p , F1p )
                            C̃∈C                              C̃∈C

    For a given inference problem, the bootstrap has to estimate these quantities conservatively
with respect to the unknown copula C(·), which can be done iteratively as follows: we
first need to determine which couplings C0∗ attain the value of µ(C0∗ (F0 , F1 )) which is least
favorable for the inference problem at hand. Within the (not necessarily singleton) set C0∗ of
such couplings, we then determine the least-favorable value of σ(C1∗ (F0 , F1 )) for C1∗ ∈ C0∗ . We
can apply this principle recursively either until the resulting set Ck∗ contains a unique copula,
or until we reach the order of approximation desired for formal results regarding the bootstrap
                                             ∗
procedure. This results in an estimate F̂01     := Ck∗ (F̂0 , F̂1 ) for the population distribution
   p
F01  that is conservative regarding the inference task at hand. The causal bootstrap then
                                                                                                  ∗
approximates the distribution of τ̂ by sampling and randomization from a population F̂01
using the known sampling and assignment mechanism.




2.4. Least Favorable Coupling for the Average Treatment Effect. In this paper, we
consider the special case of two-sided confidence intervals based on a t-ratio for the sample
average treatment effect. The case of the average treatment effect has been the main focus
of the previous literature. It is a special case for our problem in that the copula does not
matter for estimation - by inspection, the functional

         τAT E (F01 ) = EF01 [Yi (1)] − EF01 [Yi (0)] = EF1 [Yi (1)] − EF0 [Yi (0)] =: τ (F0 , F1 )

does not depend on the copula, and the default estimator
                                                    N                 N
                                                1 X               1 X
                   τ̂AT E   := τ (F̂0 , F̂1 ) ≡        Ri Wi Yi −        Ri (1 − Wi )Yi
                                                n1 i=1            n0 i=1
                                                      7
is known to be unbiased for τ (C(F0 , F1 )) under any coupling so that µ(C(F0 , F1 )) ≡ 0 for
each C. In order to ensure that the estimand is well-defined and satisfies other regularity
conditions for the bootstrap, we make the following assumptions:

Assumption 2.1. The first four moments of F0 (y0) and F1 (y1 ) are bounded.

  Also for a two-sided confidence interval constructed from inverting a t-test based on τ̂AT E ,
the least favorable coupling must attain the upper bound for the asymptotic variance,

                                 σU2 (F0 , F1 ) = sup σ 2 (C(F0 , F1 )) =: σ 2 (F0 , F1 )
                                                 C∈C

We next show that σ 2 (C(F0 , F1 )) is uniquely maximized at the joint distribution correspond-
ing to the isotone assignment which matches values of Yi (0) to values of Yi (1) while preserving
their respective marginal distributions. More formally, the joint distribution of the potential
outcomes under the isotone coupling is characterized by the copula

                                              C iso (u, v) := min{u, v}

We find that the upper bound on the variance is in fact uniquely attained at the isotone
coupling. Therefore an estimator for the distribution of τ̂AT E which assumes the isotone
coupling is asymptotically conservative at any order of approximation.

Proposition 2.1. (Least Favorable Coupling for the ATE). Suppose that Assumption
2.1 holds. Then, given the marginal distributions F0 , F1 , the variance bound is uniquely
attained at
                            σ 2 (F0 , F1 ) := lim nVarF01
                                                       iso (τ̂ )
                                                             N
         iso          iso
where   F01    := C         (F0 , F1 ) is the joint distribution corresponding to the isotone coupling.

   The fact that the variance bound is attained at the isotone coupling is widely known (see
e.g. Becker (1973), Fan and Park (2010), Stoye (2010), and Aronow, Green, and Lee (2014)),
for expositional purposes we provide a proof in the appendix. We establish the slightly
stronger conclusion that the distribution under the isotone coupling is in fact maximal with
respect to second-order stochastic dominance. For our approach it is also important to
establish that this maximum is unique in the sense that the joint distribution resulting from
any other coupling yields a variance that is strictly lower than σU2 (F0 , F1 ). In particular,
for confidence intervals based on the Gaussian asymptotic distribution, the isotone coupling
does indeed constitute the least favorable coupling.

2.5. Related Literature. Worst case bounds on the distributions of potential outcomes and
treatment effects and their quantiles have been analyzed by Heckman, Smith, and Clements
(1997), Manski (1997), Firpo and Ridder (2008), Fan and Park (2010), Fan and Park (2012),
and Fan and Wu (2010). This literature uses theoretical results on dependency bounds for
                                                            8
functions of several random variables which were developed among others by Makarov (1982),
Frank, Nelsen, and Schweizer (1987), and Williamson and Downs (1990). Stoye (2010) estab-
lishes that a class of spread parameters is monotone with respect to conventional stochastic
orders of distribution, and shows how to derive parameter bounds for causal inference. Sev-
eral of these studies also propose inference procedures that account for sampling uncertainty
rather than randomization error. In contrast, for our problem we need to explicitly construct
the respective couplings that achieve the lower and upper bounds to the parameter, and in
addition the largest randomization variance for an estimator of either bound.
   Robins (1988) proposes a confidence interval for a causal parameter based on the least-
favorable coupling for a binary outcome variable. Aronow, Green, and Lee (2014) pro-
pose an estimator of the sharp upper bound for the randomization variance of the average
treatment effect in completely randomized experiments. Our approach of embedding the
finite-population randomization distribution into an asymptotic sequence of sampling ex-
periments closely follows Abadie, Athey, Imbens, and Wooldridge (2017). Our results make
use of a finite-population CLT for the empirical process developed by Bickel (1969) for the
two-sample problem. Finite-sample central limit theorems for randomization inference were
also provided by Li and Ding (2017). Bootstrap methods for sampling from finite popula-
tions (without replacement) have been proposed by Bickel and Freedman (1984) and Booth,
Butler, and Hall (1994). For this problem the main challenge in generating the finite boot-
strap population is that the size of the super-population N may be a non-integer multiple
of n. We propose a new alternative for estimating the potential outcome distribution for
a super-population of exact size N and for which the marginal distributions coincide with
their empirical analogs up to rounding error.

2.6. Comparison to Fisher’s Exact Test. Bootstrap inference on the average treatment
effect as proposed in this paper bears some conceptual similarities with Fisher’s exact test
of the sharp null of no unit-level treatment effect (see e.g. Rosenbaum (2002), Imbens and
Rubin (2015), Ding (2017)), Yi (0) = Yi (1) with probability 1. One important distinction is
that the justification for our procedure is only asymptotic, whereas the Fisher exact test is
valid in finite samples.
   Furthermore, the Fisher exact test evaluates the randomization distribution of the esti-
mated ATE under the sharp null of no or a constant unit-level treatment effect. The sharp
null not only implies that the joint distribution of Yi (0) and Yi (1) corresponds to the isotone
assignment, but also equality of the marginal distributions F0 (y) = F1 (y), which may in
fact be rejected by the data under the null of a zero average treatment effect. In that case
even a conservative estimator of the randomization variance may in fact be smaller than
that implied by zero, or constant, unit-level effects. More generally, when Fisher’s sharp null
fails and F0 (y) 6= F1 (y), the bootstrap estimate of the randomization variance can in several
                                                9
important scenarios be smaller than that implicit in Fisher’s exact test, in which case our
procedure is asymptotically more powerful.
  Specifically, standard variance calculations (see e.g. Ding (2017)) imply that the implicit
variance estimate for Fisher’s exact test under the null of no average effect is
                                                        n0 S02 + n1 S12 1
                                                                            
                                              1   1                          1
              VF isher (τ̂AT E ) = Var(Yi )     +     =                    +     .
                                              n1 n0            n         n1 n0
We can compare this to the actual variance stated in Section 2.1,
                                                S02 S12 S012
                                     V(τ̂ ) =      +    −    .
                                                n0   n1   N
   Our bootstrap procedure implies a conservative estimate, i.e. a sharp lower bound for
 2
S01  from the isotone coupling of the potential outcomes, which is strictly positive whenever
the marginal distributions of Yi   (0) and Yi (1) are not the same. The comparison between
             S12    S02
the terms n1 + n0 and Var(Yi ) n11 + n10 is generally ambiguous - Ding (2017) describes
several cases in which the randomization variance implied by Fisher’s test is strictly larger,
and his conclusions carry over to the bootstrap procedure in this paper.       On the other
                                              S12    S02            1    1
hand it is important to note that when n1 + n0 > Var(Yi ) n1 + n0 , Fisher’s exact test
over-rejects under the null hypothesis of no average treatment effect, so that this potential
power advantage for the Fisher test only arises in situations in which the exact test does not
provide a valid test of that null. We illustrate this possibility using Monte Carlo simulations
in Section 4.
   The relationship between Fisher’s sharp null and Neyman’s null hypothesis of no average
effect is clarified in Ding (2017), who also shows that Neyman’s test of the null of no average
effect is weakly more powerful against alternatives than Fisher’s exact test. Fisher’s exact
null also implies that the distribution of ∆i is degenerate at a constant, however the power
comparison for the ATE does not carry over to set-identified objects like quantiles or the
c.d.f. of ∆i since the bounds for the identified set are typically not attained at the isotone
coupling that is implied by the sharp null.


                     3. General Setup and Bootstrap Procedure

   The proposed bootstrap procedure allows for sampling and randomization uncertainty,
where we consider a sampling experiment under which the researcher observes n units that
are selected at random out of a population of N units. For the purposes of asymptotic
approximations, we assume that the population of interest in turn consists of N i.i.d. draws
from an encompassing distribution F01 .

Assumption 3.1. (Sampling Experiment) The population consists of N units with po-
tential values (Yi (0), Yi (1))N
                               i=1 which are i.i.d. draws from the distribution F01 (y0 , y1 ). The n
                                                  10
observed units are sampled at random and without replacement from the population,

                                                      ⊥Ri
                                        Yi (0), Yi (1)⊥
                       n
where we denote q :=   N
                           ∈ (0, 1].

   We assume throughout that the treatment Wi ∈ {0, 1} is binary, and that the outcome
Yi (Wi ) for unit i does not vary with the treatment status assigned to other units. The latter
requirement is also known as individualistic treatment response, or Stable Unit Treatment
Value Assumption (SUTVA). We assume furthermore that the experiment is completely
randomized:

Assumption 3.2. (Complete Randomization) Treatment assignment is completely ran-
domized, that is for each unit with Ri = 1 we have

                                                       ⊥Wi
                                       (Yi (0), Yi (1))⊥

where Wi = 1 for n1 units selected at random and without replacement from the n observa-
tions with Ri = 1, and the propensity score p := nn1 satisfies 0 < p < 1.

  For greater clarity of exposition we also assume that the researcher observes no further co-
variate information. The approach of this paper can be generalized to observational studies
under unconfoundedness, and experiments with imperfect compliance for which unconfound-
edness fails, but intention to treat is (conditionally) independent of potential outcomes and
can serve as an instrumental variable to identify causal effects on a population of compliers.
  Given a sample generated according to Assumptions 3.1 and 3.2, we denote the point
estimate for the average treatment effect

                                         τ̂ := τ (F̂0 , F̂1 )

and the upper variance bound
                                         σ̂ := σ(F̂0 , F̂1 )
For the purposes of this paper, the main target of interest for the causal bootstrap is the
distribution of the t-ratio
                                          √ τ̂ − τ
                                     T := n
                                                σ̂

3.1. Bootstrap Algorithm. The proposed bootstrap algorithm proceeds in four main
steps:
   (1) We obtain nonparametric estimates of the potential outcome distributions F0 (y0 )
       and F1 (y1) from the units for which Wi = 0 (Wi = 1, respectively) in the actual
       experiment.
                                                 11
                                                                 N
   (2) We create an empirical population of size N, Ỹi , W̃i           by generating an appropriate
                                                                    i=1
       number of replicas of the sample of n draws for Wi , Yi . If the sample is the population,
       n = N, we can skip this step.
   (3) We then impute potential values Ỹi (0), Ỹi (1) for each unit i = 1, . . . , N, where
       Ỹi (W̃i ) = Ỹi and Ỹi (1 − W̃i ) is obtained from the estimated potential outcome distri-
       butions and the least-favorable copula for the parameter of interest.
   (4) Finally, we simulate the randomization distribution by repeatedly drawing n units
       Yi∗ (0), Yi∗ (1) out of that empirical population without replacement and generating
       randomization draws W1∗ , . . . , Wn∗ . We then evaluate the sample average treatment
       effect for the bootstrap sample (Yi∗ (Wi∗ ), Wi∗ )ni=1 obtained using the imputed potential
       outcomes.
  Given the simulated randomization distribution for the estimated bounds, we can esti-
mate the percentiles of the t-ratios that are needed to construct confidence intervals for the
functional. We next describe each of these steps in greater detail.

3.2. Generating the Empirical Population. To obtain the empirical population of size
N, we generate replicates of the n observed units, however not necessarily of the same
number for each observation when N is not an integer multiple of n. We propose the
following procedure for doing so:
                                         n0
      • We create the samples Yj0 , Ûj0        of values for Yi for the n0 units with Wi = 0,
                       n1                 j=1

        and Yj1 , Ûj1      with values Yi for the n1 units with Wi = 1. We assume that each
                       j=1
                                                                                   j
        sample is ordered, Ykw ≤ Yk+1
                                   w
                                      for all k, and the rank variable Ûjw = for w = c, t.
                                                                                  nw
                                                                                        N
     • Let N0 = ⌈ nn0 N⌉ and N1 = N −N0 . We generate the empirical population Ỹi , W̃i
                                                                                                i=1
        by including Mj0 := ⌈Ûj+1   0
                                        N0 ⌉ − ⌈Ûj0 N0 ⌉ copies of Yj0 with W̃j = 0 and Mj1 :=
           1
        ⌈Ûj+1 N1 ⌉ − ⌈Ûj1 N1 ⌉ copies of Yj1 with W̃j = 1.
    Since the respective maxima of Ûj0 , Ûj1 are equal to 1 for either of the two strata (cor-
                                                           Pn                   0            1
responding to Wi = 0 and Wi = 1, respectively),               j=1 ((1 − W̃(j) )M(j) + W̃(j) M(j) ) =
⌈N0 ⌉ + ⌈N1 ⌉ = N so that this procedure ensures that the empirical population has size
equal to N. Also, for n and N sufficiently large, the respective empirical distributions of
Ỹi among units with W̃i = 0 and Ỹi among units with W̃i = 1 are, up to an approximation
error of the order n−1 , equal to F̂0 and F̂1 , respectively.

3.3. Imputing Missing Counterfactuals. For the specific case of two-sided inference
for the average treatment effect, the least favorable coupling corresponds to the isotone
assignment C iso (u, v) := min{u, v}, as argued before. In order to generate an empirical
                                      iso
population with joint distribution F̂01   := C iso (F̂0 , F̂1 ), we can simply impute the missing
                                                 12
counterfactuals according to:
                                           (
                                               Ỹi                     if W̃i = 0
                             Ỹi (0) :=                           
                                               F̂0−1    F̂1 (Ỹi )     otherwise
                                           (
                                               Ỹi                     if W̃i = 1
                             Ỹi (1) :=                                                                  (3.1)
                                               F̂1−1    F̂0 (Ỹi )     otherwise

For functionals τ (F01 ) of the potential outcome distribution other that the average treatment
effect, or inference problems other than two-sided confidence intervals, the least favorable
coupling will be of a different form, so this step would have to be replaced by a procedure
imputing the missing counterfactuals from a different coupling.


3.4. Resampling Algorithm. For the bth bootstrap replication, we initially draw n units
(Yib∗ (0), Yib∗ (1)) from the empirical population at random and without replacement.
  Given a known propensity score p := P (Wi |Ri = 1), for the bth bootstrap replication we
                      ∗              ∗
can generate W1b        , . . . , Wnb   as independent Bernoulli draws with success probability P (Wib∗ =
1) = p and obtain the bootstrap sample Y1b∗ , . . . , Ynb          ∗
                                                                     , where Yib∗ := Yib∗ (Wib∗ ).
                                                                                           ∗
                                                                                             (y0 ) := n10 N    ∗
                                                                                                         P
  We can then compute the bootstrap analogs of the estimated c.d.f.s F̂0b                                 i=1 Rib (1−
Wib∗ )1l{Yib∗ ≤ y0 } and F̂1b      ∗
                                       := n11 N     ∗    ∗      ∗
                                             P
                                               i=1 Rib Wib 1l{Yib ≤ y1 }, the corresponding estimates of the
average treatment effect, and the variance bound,

                                            τ̂b∗ := τ (F̂0b
                                                         ∗      ∗
                                                            , F̂1b )
                                            σ̂b∗ := σ(F̂0b
                                                        ∗      ∗
                                                           , F̂1b )

  We then record the studentized values of the bootstrap estimates,
                                                         √ τ̂b∗ − τ̂
                                               Tb∗ :=     n
                                                               σ̂b∗
Repeating the resampling step B times, we obtain a sample (T1∗ , . . . , TB∗ ) that constitutes
independent draws from the bootstrap estimator of the randomization distribution and can
be used to construct critical values for tests or confidence intervals.


3.5. Confidence Intervals. We consider confidence intervals constructed by inverting a t-
test based on the point estimate τ̂ := τ (F̂0 , F̂1 ) and given the variance bound σ̂ := σ(F̂0 , F̂1 )
introduced before. The proposed confidence intervals for τ are then of the form

                    CI(1 − α) := τ̂ − n−1/2 σ̂ĉ(1 − α), τ̂ − n−1/2 σ̂ĉ(α)
                                                                            
                                                                                              (3.2)

We use bootstrap approximations to the randomization distribution of the t-ratio n1/2 (τ̂ −
τ )/σ̂ under the least favorable coupling in order to determine the critical values. Specifically,
let Ĝ(z) := B1 B          ∗
               P
                   b=1 1l{Tb ≤ z} denote the empirical distribution for the bootstrap samples
                                                          13
obtained from the previous step. We then estimate the critical values using ĉ(α) := Ĝ−1 (α)
and ĉ(1 − α) := Ĝ−1 (1 − α).


                              4. Monte Carlo Simulations

   We next compare the performance of this causal bootstrap with the standard bootstrap
and other alternative methods based on sampling or randomization designs. Specifically, we
consider confidence intervals using Gaussian critical values the respective analytic estimators
of the sampling variance V̂Neyman and the causal variance V̂AGL given in Section 2.2. We also
consider Gaussian inference using the variance estimators V̂s−boot and V̂c−boot obtained from
the classical (sampling) bootstrap, and the causal bootstrap proposed in this paper. We
compare these to confidence intervals from inverting Fisher’s exact test, and confidence
intervals from the standard and the causal bootstrap for the t-statistic based on either
sampling or causal variance estimate. Throughout we will restrict our attention to the case
n = N, i.e. when the full population of interest is observed.
   We first consider three different simulation designs to illustrate the main points of com-
parison between the causal bootstrap and its main alternatives.
     • Design I sets n0 = n1 = 100 and draws potential outcomes according to Yi (0) ∼
       N (0, 1), Yi (1) = Yi (0). In this setting, treatment effects are constant at Yi (1)−Yi (0) ≡
       0 and the marginal distributions F0p (y) ≡ F1p (y), so that all procedures should be
       expected to do well.
     • For Design II we again have n0 = n1 = 100, but generate potential outcomes as
       Yi (0) ∼ N (0, 1), and Yi (1) = 0. In that case, the marginal distributions F0p (y) and
       F1p (y) are different, so that causal standard errors and the causal bootstrap should
       do better than their sampling analogs.
     • For Design III, n0 = n1 = 10, and we generate non-Gaussian potential outcomes
       where Yi (0) is mixture, with probability 0.9 it is N (0, 1) and with probability 0.1 it
       is N (0, 16), Yi (1) = 0. This design highlights the difference between the bootstrap
       and the Gaussian distribution, which is no longer exact for this design.
Simulation results are shown in Table 4, where we compare coverage rates of nominal 95%
confidence intervals, and the corresponding standard errors for each of the three designs. If
a particular method does not directly calculate standard errors, we calculate the standard
errors by taking the ratio of the difference between the upper and lower limit of the confidence
interval and dividing by 2 times 1.96. As expected, all methods do well for the first design.
In the second design, only inference based on V̂AGL and the causal bootstrap (pivotal or not)
do well. In the third design the pivotal causal bootstrap does better than Gaussian inference
using either V̂AGL or V̂c−boot as an estimator for the asymptotic variance.
                                                14
                     Table 3. 95% Confidence Intervals And Standard Errors
                                                Design I          Design II         Design III
         Variance     Bootstrap   Pivotal     Cov     Med        Cov    Med        Cov     Med
         Estimator     Version    Statistic   Rate    s.e.       Rate    s.e.      Rate    s.e.

          V̂Neyman       N/A         No       0.9928    0.0997   0.9524   0.1410   0.9588   0.4487
            V̂AGL        N/A         No       0.9480    0.0705   0.9476   0.1394   0.8858   0.3173
          V̂s−boot     Standard      No       0.9922    0.0993   0.9502   0.1403   0.9506   0.4255
          V̂c−boot      Causal       No       0.9472    0.0703   0.9480   0.1390   0.8792   0.3090
               Fisher’s Exact Test            0.9584    0.0997   0.9772   0.1410   0.9644   0.4494
          V̂Neyman     Standard      Yes      0.9938    0.1011   0.9600   0.1421   0.9758   0.6212
            V̂AGL      Standard      Yes      0.9938    0.1011   0.9576   0.1444   0.9758   0.6212
          V̂Neyman      Causal       Yes      0.9492    0.0714   0.9564   0.1407   0.9224   0.4375
            V̂AGL       Causal       Yes      0.9492    0.0714   0.9560   0.1423   0.9224   0.4375




   We next illustrate the role of the coupling of the potential values where we draw (Yi (0), Yi (1))
from a bivariate Gaussian distribution with variances Var(Yi (0)) = 0.5 and Var(Yi (1)) = 2
and correlation coefficient of the two potential values, ̺01 ∈ {−1, 0, 1}. From our theoretical
results, we should expect Gaussian inference using causal standard errors and the causal
bootstrap to have asymptotically exact coverage under the isotonic coupling ̺01 = 1 and be
conservative when ̺01 < 1. Furthermore, for any coupling this design implies heterogeneous
treatment effects, so that Fisher’s exact test does not in general control nominal confidence
size for the average treatment effect. Given the calculations in Section 2.6 we designed the ex-
periment deliberately to illustrate the potential of Fisher’s exact procedure to underestimate
the spread of the randomization distribution, where n0 > n1 and Var(Yi (1)) > Var(Yi (0)).
Since the potential outcomes follow a Gaussian distribution, we should not expect refine-
ments for the bootstrap relative to Gaussian inference.
   In Table 4 we report simulated coverage rates for nominal 95% confidence intervals for
the average treatment effect, where for either sample size we report the lowest coverage rate
across the three different couplings in a separate column. The simulation results broadly
confirm the theoretical predictions. Overcoverage from using sampling-based, rather than
causal estimators for the variance or the bootstrap is not evident from the design with smaller
sample sizes (n0 = 50, n1 = 20), but becomes clearly visible once we move to the design with
a larger number of units (n0 = 200, n1 = 80). The confidence interval based on Fisher’s
exact test has coverage that is consistently below the nominal 95% level.
   Next we compare coverage rates of these confidence intervals as the size of the sample in-
creases, where we choose a design with non-Gaussian distributions for the potential outcomes.
Specifically, we let Yi (0) ≡ 0 and Yi (1)|Si (1) ∼ N(0, Si2 ), where Si = 1 with probability 0.9,
and Si = 4 with probability 0.1. Since the marginal distributions for Yi (0) and Yi (1) are
                                                       15
            Table 4. Coverage of nominal 95% Confidence Intervals, Gaussian Potential
            Outcomes with Different Couplings

Variance    Bootstrap    Pivotal                     (n0 , n1 ) = (50, 20)                                (n0 , n1 ) = (200, 80)
Estimator    Version     Statistic    ̺01 = 1       ̺01 = 0 ̺01 = −1 minimum                  ̺01 = 1    ̺01 = 0 ̺01 = −1 minimum

V̂Neyman      N/A           No          0.9560      0.9656      0.9832         0.9560         0.9650      0.9796      0.9880   0.9650
  V̂AGL       N/A           No          0.9352      0.9510      0.9730         0.9352         0.9462      0.9664      0.9818   0.9462
V̂s−boot    Standard        No          0.9508      0.9616      0.9804         0.9508         0.9636      0.9778      0.9878   0.9636
V̂c−boot     Causal         No          0.9308      0.9490      0.9706         0.9308         0.9452      0.9654      0.9838   0.9452
      Fisher’s Exact Test               0.9332      0.9112      0.8948         0.8948         0.8624      0.8638      0.8616   0.8616
V̂Neyman    Standard        Yes         0.9652      0.9754      0.9878         0.9652         0.9660      0.9792      0.9886   0.9660
  V̂AGL     Standard        Yes         0.9632      0.9744      0.9878         0.9632         0.9656      0.9786      0.9886   0.9656
V̂Neyman     Causal         Yes         0.9444      0.9610      0.9776         0.9444         0.9492      0.9684      0.9836   0.9492
  V̂AGL      Causal         Yes         0.9432      0.9608      0.9774         0.9432         0.9490      0.9684      0.9832   0.9490




    different, the difference between sampling variance and the upper bound for the causal vari-
    ance is nontrivial. Furthermore, while we do not give formal results, under certain regularity
    conditions the pivotal causal bootstrap should be expected to provide refinements over the
    Gaussian limiting approximation to the randomization distribution.

            Table 5. Coverage of nominal 95% Confidence Intervals, non-Gaussian Po-
            tential Values with Isotone Coupling
               Variance      Bootstrap    Pivotal        (n0 , n1 )    (n0 , n1 )    (n0 , n1 )    (n0 , n1 )    (n0 , n1 )
               Estimator      Version     Statistic      (20, 20)      (50, 50)     (100, 100)    (200, 200)    (500, 500)

                V̂Neyman       N/A            No             0.9768    0.9866        0.9914        0.9932          0.9924
                  V̂AGL        N/A            No             0.9186    0.9358        0.9396        0.9450          0.9436
                V̂s−boot     Standard         No             0.9752    0.9864        0.9912        0.9928          0.9924
                V̂c−boot      Causal          No             0.9144    0.9336        0.9378        0.9436          0.9436
                        Fisher’s Exact Test                  0.9752    0.9652        0.9672        0.9560          0.9592
                V̂Neyman     Standard         Yes            0.9870    0.9912        0.9940        0.9942          0.9934
                  V̂AGL      Standard         Yes            0.9870    0.9912        0.9940        0.9942          0.9934
                V̂Neyman      Causal          Yes            0.9470    0.9532        0.9582        0.9548          0.9482
                  V̂AGL       Causal          Yes            0.9470    0.9532        0.9582        0.9548          0.9482




      Table 4 shows simulated coverage rates for the different confidence intervals at the nominal
    95% significance level under this design. The results show that coverage rates for both the
    sampling-based variance estimators and bootstrap are higher throughout than for their causal
    analogs. The comparison between Gaussian confidence intervals using the causal variance
    estimators, V̂AGL and V̂c−boot , respectively, to the pivotal causal bootstrap is also indicative
    of refinements, where the confidence interval based on the pivotal causal bootstrap has
                                                                  16
coverage rates much closer to the nominal level for small sample sizes, but that advantage
vanishes as n0 , n1 grow large.




                                          5. Large Sample Theory

   To characterize the asymptotic properties of the bootstrap procedure, we can cast the
statistical experiment of sampling from a finite population with subsequent randomization
of treatment among the sampled units as a two-stage scheme of sampling without replace-
ment from nested finite populations. Specifically, in a first step we draw n units without
replacement from the population of N units. In a second step, we draw n1 units at ran-
dom and without replacement from that sample to receive the treatment Wi = 1, whereas
the remaining n0 = n − n1 units are assigned Wi = 0. This second step is conditionally
independent of the first.
   To characterize the contribution of sampling uncertainty to the distribution of the func-
tional we define
                                                  N
                         p                     1 X
                        F01 (y0 , y1 )      :=       1l{Yi (0) ≤ y0 , Yi (1) ≤ y1 }
                                               N i=1
                                                         N
                         s                     1X
                        F01 (y0 , y1 )      :=       Ri 1l{Yi (0) ≤ y0 , Yi (1) ≤ y1 }
                                               n i=1

with corresponding marginals F0p , F1p , F0s , F1s . In particular,
                                                             N
               s                    p                 1X
              F01 (y0 , y1 )   −   F01 (y0 , y1 )   =       (Ri − q) 1l{Yi (0) ≤ y0 , Yi (1) ≤ y1 }       (5.1)
                                                      n i=1
  Turning to the contribution of design uncertainty, we define
                                                                 n
                                        1     X
                        F̂0 (y0 ) :=              Ri (1 − Wi )1l{Yi (0) ≤ y0 }
                                     n(1 − p) i=1
                                                     n
                                      1 X
                        F̂1 (y1 ) :=        Ri Wi 1l{Yi (1) ≤ y1 }
                                     np i=1
where we can rewrite
                                              N           
                                       p     X          Wi
                        F̂0 (y0 ) =              Ri 1 −      1l{Yi (0) ≤ y0 }
                                    n(1 − p) i=1        p
Hence, we have
                                        !                    N
                                                                                                      !
                                                                                 p
               F̂0 (y0 ) − F0s (y0 )           1 X                           − 1−p 1l{Yi (0) ≤ y0 }
                                             =        Ri (Wi − p)                                         (5.2)
               F̂1 (y1 ) − F1s (y1 )           np i=1                           1l{Yi (1) ≤ y1 }
                                                                     17
   Taken together, (5.1) and (5.2) characterize the uncertainty from sampling and random-
ization in estimating the respective marginal distributions of Yi (0) and Yi (1) as a two-stage
process of drawing without replacement from nested finite populations. An asymptotic
Donsker Theorem for empirical processes based on sampling without replacement from a
finite population is available from Bickel (1969).
   We now state the limiting properties of the bootstrap as N and n grow large. Specifically,
we derive the limits of the randomization and bootstrap distributions. We then show that the
latter is an asymptotically conservative estimator of the former for the purposes of forming
confidence intervals.

5.1. Consistency and Randomization CLT. Consistency of the estimated bounds fol-
lows from consistency of F̂0 (y0 ) and F̂1 (y1 ) for F0 (y0) and F1 (y1 ), respectively, and the
continuous mapping theorem, noting that the conditions in Assumption 2.1 are sufficient for
the parameter bounds to be continuous functions of F0 (y0 ) and F1 (y1).

Theorem 5.1. (Consistency) Suppose Assumptions 2.1, 3.1, and 3.2 hold. Then τ̂ and
σ̂ are consistent for τ (F0p , F1p ) and σ(F0p , F1p ), respectively.

  For a randomization CLT for the estimated bounds we first establish a functional CLT for
the randomization processes
                                          √
                                 Ĝ0 :=     n(F̂0 − F0p )
                                          √
                                 Ĝ1 :=     n(F̂1 − F1p )

for conditional distributions of potential outcomes. We argue that Assumption 2.1 is suffi-
cient to establish Hadamard differentiability of the functionals τ (F̂0 , F̂1 ), σ(F̂0 , F̂1 ) so that
                         √
asymptotic normality of n τ̂ −τ
                              σ̂
                                 follows from the functional Delta rule and Slutsky’s theorem.

Theorem 5.2. (Randomization CLT) Suppose Assumptions 2.1, 3.1, and 3.2 hold. Then
the asymptotic distribution of the t-ratio for τ̂AT E is given by
                                                             p
                              √ τ̂ − τ d               σ 2 (F01
                                                                 
                                                                )
                                 n       → N 0, 2 p p
                                     σ̂              σ (F0 , F1 )
where σ(F01 )2 := limn nVarF01 (τ̂ ).

  The proof of this result is given in the appendix. The formal argument adapts a finite-
population CLT for the empirical process developed by Bickel (1969) for the two-sample
problem to the case of sampling and randomization in a finite population.

5.2. Bootstrap CLT. For a bootstrap replication, denote the empirical distributions of
Yi∗ |Wi∗ = 0 and Yi∗ |Wi∗ = 1 with F̂0∗ and F̂1∗ , respectively. Also, let τ̂ ∗ = τ (F̂0∗ , F̂1∗ ) and
σ̂ ∗ = σ(F̂0∗ , F̂1∗ )
                                                  18
   We then establish a CLT for the bootstrap analogs
                                                   √
                                      Ĝ∗0 :=        n(F̂0∗ − F̂0 )
                                                   √
                                      Ĝ∗1 :=        n(F̂1∗ − F̂1 ).
                                          √ ∗
A CLT for the bootstrapped bounds n τ̂ σ̂−τ̂     ∗  then relies again on Hadamard differentiability
of the variance bounds and the Delta rule for the bootstrap.
   A bootstrap CLT can be shown using analogous steps as in a proof for Theorem 5.2,
                                                                                        p
where the randomization distribution is generated based on an estimator for F01            based on
                              p           p
the estimated distributions F̂0 (y0 ), F̂1 (y1 ) and the respective least-favorable coupling C iso (·).

Theorem 5.3. (Bootstrap CLT) Suppose Assumptions 2.1, 3.1, and 3.2 hold. Then the
asymptotic distribution of the bootstrapped t-ratio for τ̂AT E is given by
                                         √ τ̂ ∗ − τ̂ d
                                          n          → N (0, 1)
                                               σ̂ ∗

  Most importantly, by Theorem 5.1 the bootstrap estimator for the randomization distri-
bution for τ̂ −τ
              σ̂
                 has asymptotic variance equal to 1, whereas the asymptotic variance of the
                                    σ2 (F p )
randomization distribution is σ2 (F p 01 p which is less than 1 by construction. That is, the
                                   0 ,F1 )
bootstrap algorithm in section 3.1 converges to a “least-favorable” limiting experiment in an
appropriate sense. Note also that the formal argument in the proofs of Theorems 5.1-5.3 im-
mediately apply to any other functional τ (F0 , F1 ) that is Hadamard-differentiable in F0 , F1 ,
and for which the variance bound σ 2 (F0 , F1 ) is continuous in F0 , F1 .



5.3. Asymptotic Validity of Confidence Intervals. It remains to show that confidence
intervals of the form (3.2) that are constructed under the “least-favorable” limiting ex-
periment are indeed conservative given the CLT under the true randomization distribu-
tion in Theorem 5.2. This can be proven by combining the randomization and bootstrap
CLTs, replacing the unidentified randomization variance with an estimate of the bound
σ(F0 , F1 ) ≥ σ(F01 ).

Corollary 5.1. (Asymptotic Validity of Confidence Intervals) Under Assumptions
2.1, 3.1, and 3.2, the 1 − α confidence interval (3.2) using bootstrap critical values is asymp-
totically valid,
                                          p
                        lim inf          01 ) ∈ CI(1 − α)) ≥ 1 − α
                                  p (τ (F
                                PF01                                 a.s.
                          n   p
                              F01


  Given Theorems 5.2 and 5.3 this result follows immediately from the definition of the
variance bound σ(F0 , F1 ).
                                                   19
                       Appendix A. Randomization Distribution for F̂0 (y0 ), F̂1 (y1 )

   We first compute the randomization covariance CovpW,R (F̂0 (y0 ), F̂1 (y1 )) given the population distribution
 p
F01 (y0 , y1 ), where
                                                              N
                                                   1     X
                                 F̂0 (y0 ) =                 Ri (1 − Wi )1l{Yi (0) ≤ y0 }
                                                n(1 − p) i=1
                                                     N
                                                1 X
                                 F̂1 (y1 ) =           Ri Wi 1l{Yi (1) ≤ y1 }
                                                np i=1

In the following we write A0i := 1l{Yi (0) ≤ y0 } and A1i := 1l{Yi (1) ≤ y1 }, and take any moments to be with
                                                                                          N
respect to the distribution of Ri and Wi and conditional on the values of (Yi (0), Yi (1))i=1 in the population.
We then have
                                                                                   
                                                   N X N
                                           1       X
          Cov(F̂0 (y0 ), F̂1 (y1 )) =                    Ri Rj (1 − Wi )Wj A0i A1j 
                                      n2 p(1 − p) i=1 j=1
                                                         N   N
                                            1      XX
                                   =     2
                                                           E [Ri Rj (1 − Wi )Wj ] A0i A1j
                                        n p(1 − p) i=1 j=1
                                                         N
                                            1      XX
                                   =     2
                                                       E [Ri Rj ] E [(1 − Wi )Wj ] A0i A1j
                                        n p(1 − p) i=1
                                                             j6=i
                                                         N
                                             1      X X n(n − 1)                     n2 p(1 − p)
                                   =                              E [(1 − W i )W j ]             A0i A1j
                                        n2 p(1 − p) i=1      N2                       n(n − 1)
                                                        j6=i
                                                                "         # N                         
                                             N X                    N                         N
                                         1  X                 1    X           X             X
                                   =                A0i A1j = 2        A0i         A1j  −     A0i A1i 
                                        N 2 i=1              N
                                                 j6=i              i=1         j=1           i=1

                                           1     p
                                   = − (F01        (y0 , y1 ) − F0p (y0 )F1p (y1 ))
                                          N
                                   
To evaluate Cov F̂0 (y0 ), F̂0 (y1 ) , let B0i := 1l{Yi (0) ≤ y0 } − F0p (y0 ) and B0i := 1l{Yi (0) ≤ y1 } − F0p (y1 ). We
can then write
                                             N X  N
                                  1       X
Cov F̂0 (y0 ), F̂0 (y1 )  =                          E [Ri Rj (1 − Wi )(1 − Wj )] B0i B1j
                              n2 (1 − p)2 i=1 j=1
                                                                                                                                 
                                               n                               N X
                                    1         X    n  n(1   −  p)             X         n(n − 1) n(1  − p)(n(1  − p) − 1)
                          =                                      B0i B1i +                                               B0i B1j 
                              n2 (1 − p)2 i=1 N            n                  i=1 j6=i
                                                                                          N2                 n2
                                                                                                               
                                             N                                                  N X
                                    1       X                 (n − 1)(n(1 − p) − 1)  1        X
                          =                     B0i B1i +                                               B0i B1j 
                              n(1 − p)N i=1                           n2 (1 − p)           N 2 i=1
                                                                                                   j6=i
                              "      N
                                                #                                            
                                 1 X                      1          (n − 1)(n(1 − p) − 1)
                          =             B0i B1i                   −
                                N i=1                n(1 − p)              N n2 (1 − p)
                                                                                                            
                                        p        p              p       p              1        1           1
                          = (min{F0 (y0 ), F0 (y1 )} − F0 (y0 )F0 (y1 ))                    −      +O
                                                                                    n(1 − p) N            nN




                                                                 20
Similarly,
                                                                                                   
                                    
                                              p         p           p       p          1   1       1
             Cov F̂1 (y0 ), F̂1 (y1 ) = (min{F1 (y0 ), F1 (y1 )} − F1 (y0 )F1 (y1 ))     −   +O
                                                                                       np N       nN
  Furthermore,
                                                        1
                        Cov F̂0 (y0 ), F̂0 (y1 )    =       min{F0p (y0 ), F0p (y1 )} − F0p (y0 )F0p (y1 )
                                                          n
                                                        1
                        Cov F̂1 (y0 ), F̂1 (y1 )    =       min{F1p (y0 ), F1p (y1 )} − F1p (y0 )F1p (y1 )
                                                          n
We let H denote the      covariance kernel of the randomization process with elements
                                                                        
                                                                1      n
    H00 (y0 , y0′ ) =                               ′
                         lim nCov(F̂0 (y0 ), F̂0 (y0 )) =          −         (min{F0p (y0 ), F0p (y0′ )} − F0p (y0 )F0p (y0′ ))
                           n                                  1−p N
                                                                n    p
    H01 (y0 , y1 ) =     lim nCov(F̂0 (y0 ), F̂1 (y1′ )) = lim (F01    (y0 , y1 ) − F0p (y0 )F1p (y1 ))                       (A.1)
                           n                                 n N
                                                                   
                                                              1   n
    H11 (y1 , y1′ ) =    lim nCov(F̂1 (y1 ), F̂1 (y1′ )) =      −      (min{F1p (y1 ), F1p (y1′ )} − F1p (y1 )F0p (y1′ ))
                           n                                  p N
                   1           n             1          n
Note also that    1−p   ≥1≥    N   ≥ 0 and   p   ≥1≥    N   ≥ 0, so that H00 (·, ·) and H11 (·, ·) are nonnegative.

                                       Appendix B. Proofs for Section 2.4

B.1. Least Favorable Coupling for the Average Treatment Effect. We first prove a more general
result than Proposition 2.1 by showing that the isotone coupling of potential outcomes in fact results in a
distribution for the ATE parameter which dominates that under any other coupling in the sense of second-
order stochastic dominance (SOSD):

Lemma B.1. (Ordering of Distributions) Let F01 be an arbitrary joint distribution with marginal dis-
                                iso
tributions F0 and F1 , and let F01  := C iso (F0 , F1 ) be the joint distribution under the isotone coupling. Then
for any convex function, the randomization distribution for τ̂AT E satisfies

                                               iso [v(τ̂AT E )] ≥ EF
                                             EF01                   01 [v(τ̂AT E )]


                                                                                 iso
For any strictly convex function v(·) this inequality is strict whenever F01 6= F01  .

   This result is a straightforward consequence of the familiar observation that the isotone (assortative)
coupling of potential outcomes results in the distribution of Yi (0) + Yi (1) which second-order stochastic
dominates that resulting from any other coupling (see e.g. Becker (1973), Fan and Park (2010), and Stoye
(2010)). For illustrative purposes, we give a complete proof here.
   Proof: In order to establish second-order stochastic dominance of the isotone assignment Yi (1) =
  −1
F1 (F0 (Yi (0))), consider the expectation of v(τ̂AT E ) for any convex function v(u). Note that for any pair
of observations i, j we can write
                       1
            τ̂AT E = (B−ij + Ri Wi (Yi (0)/(1 − p) + Yi (1)/p) + Rj Wj (Yj (0)/(1 − p) + Yj (1)/p))
                      n
                 P
where B−ij := k6=i,j Rk (Wk (Yk (0)/(1 − p) + Yk (1)/p) − Yk (0)/(1 − p)) − (Yi (0) + Yj (0))/(1 − p).
   We can now consider the change in E[v(τ̂AT E )] from pairwise substitutions of potential outcomes between
units i and j. Specifically suppose that under the initial coupling, the potential outcomes for unit i are
given by Yi (0), Yi (1), and the potential outcomes for unit j are Yj (0), Yj (1). We then consider the effect of
switching the assignment to potential outcomes Yi (0), Yj (1) for unit i, and potential outcomes Yj (0), Yi (1)
for unit j.
                                                               21
   Since Wi , Wj are independent of Wk , that change leads to an increase in E[v(τ̂AT E )] if and only if
                             n                                                                            o
  0 ≤ P (Wi = 1, Wj = 0) v(B−ij + Yi (0)/(1 − p) + Yi (1)/p) − v(B−ij + Yi (0)/(1 − p) + Yj (1)/p)
                               n                                                                            o
         +P (Wi = 0, Wj = 1) v(B−ij + Yj (0)/(1 − p) + Yj (1)/p) − v(B−ij + Yj (0)/(1 − p) + Yi (1)/p)
                  n
      = p(1 − p) v(B−ij + Yi (0)/(1 − p) + Yi (1)/p) + v(B−ij + Yj (0)/(1 − p) + Yj (1)/p)
                                                                                     o
          −v(B−ij + Yi (0)/(1 − p) + Yj (1)/p) − v(B−ij + Yj (0)/(1 − p) + Yi (1)/p)

for any pair of observations with Ri = Rj = 1. Noting that for any convex function v(·), v(b + x0 + x1 ) is
supermodular in x = (x0 , x1 )′ , this difference is nonnegative if and only if Yi (0) − Yj (0) and Yi (1) − Yj (1)
have the same sign. Furthermore, if in addition v(·) is strictly convex, the first inequality is strict.
   Since any coupling of potential outcomes can be obtained from the isotone assignment by pairwise sub-
stitutions of this form, the isotone assignment maximizes the expectation
                                      "       N
                                                                                           !#
                                            1X
                     E[v(τ̂AT E )] = E v         Ri {Wi Yi (1)/p − (1 − Wi )Yi (0)/(1 − p)}
                                           n i=1

for all convex functions v(·). Therefore the distribution of τ̂AT E under the isotone assignment dominates
that under any alternative coupling, as claimed above.                                                  


Proof of Proposition 2.1: The claim in the proposition follows immediately from Lemma B.1 and the obser-
vation that the function v(y) = y 2 is strictly convex                                                



                                    Appendix C. Proofs for Section 5

C.0.1. Proof of Theorem 5.1. From standard results (see e.g. Example 19.6 in van der Vaart (1998)), the
class F := {(−∞, y] : y ∈ R} is Glivenko-Cantelli, so that (F̂0 − F0p , F̂1 − F1p ) converges to zero almost
surely as an element of the space of bounded functions on R. Since Assumption 2.1 is sufficient to guarantee
that the functionals τ (F0 , F1 ) and σ(F0 , F1 ), are continuous in F0 , F1 , the claim of the Theorem follows
immediately from the continuous mapping theorem (see e.g. Theorem 18.11 in van der Vaart (1998))             

   For the proof of Theorem 5.2, we need to characterize functional convergence of the randomization process.
To that end, we first introduce some standard notation from empirical process theory (see van der Vaart
and Wellner (1996)). Let F := {1l{y ≤ (−∞, t]} : t ∈ R} be the class of indicator functions for the
left-open half-lines on R and let ℓ∞ (F ) be the space of bounded functions from F to R endowed with
the norm kzkF := supf ∈F |z(f )|. Also, let BL1 denote the set of all functions h : ℓ∞ F 7→ [0, 1] with
|h(z1 ) − h(z2 )| ≤ kz1 − z2 kF .

                                            iid
Lemma C.1. Suppose that (Yi (0), Yi (1)) ∼ F01 . Then the randomization process
                                                               !
                                               √     F̂0 − F0p
                                        Ĝn := n
                                                     F̂1 − F1p
converges in outer probability to G under the bounded Lipschitz metric,

                                          sup |EW h(Ĝn ) − Eh(G)| → 0
                                        h∈BL1

in outer probability, where G is a Gaussian process with covariance kernel H.
                                                        22
  Proof: As before, denote the joint c.d.f. of potential outcomes (observed and counterfactuals) for the n
units included in the sample with
                                                                 N
                                          s                  1X
                                         F01 (y0 , y1 )   :=       Ri 1l{Yi (0) ≤ y0 , Yi (1) ≤ y1 }
                                                             n i=1

and the empirical c.d.f. among the units included in the sample for which Wi = 1,
                                                                 N
                                        t                  1 X
                                       F01 (y0 , y1 ) :=          Ri Wi 1l{Yi (0) ≤ y0 , Yi (1) ≤ y1 }
                                                           np i=1

Using this notation we can write
       √     t               p               √   t                s               √   s                p
         n(F01 (y0 , y1 ) − F01 (y0 , y1 )) = n(F01 (y0 , y1 ) − F01 (y0 , y1 )) + n(F01 (y0 , y1 ) − F01 (y0 , y1 ))

   Since Ri , Wi are drawn at random and without replacement, it follows from Theorem 3.1 in Bickel (1969)
that
                                  √    t               s
                                   n(F01 (y0 , y1 ) − F01 (y0 , y1 )) GF01
                                                                        s

                                  √    s               p
                                   n(F01 (y0 , y1 ) − F01 (y0 , y1 )) GF01
                                                                        p



for Brownian bridges GF01     s  and GF01 p .   Since for any joint distribution F01 (y0 , y1 ) the marginals satisfy
limy1 →∞ F01 (y0 , y1 ) = F0 (y0 ) for each y0 , weak convergence of the joint process implies weak convergence of
the marginal empirical processes,
                                            √
                                              n(F0t − F0p )      GF0s + GF0p
                                            √
                                              n(F1t − F1p )      GF1s + GF1p
                                                 1
Finally, F̂1 (y1 ) ≡ F1t (y1 ) and F̂0 (y0 ) ≡ (p−1) (F0s (y0 ) − pF0t (y0 )), establishing the claim, where the structure
of the covariance kernel follows from the point-wise calculations in the derivation of (A.1)                            


C.0.2. Proof of Theorem 5.2: From Assumption 2.1 it is immediate that τ (F0 , F1 ) is Hadamard-differentiable.
Lemma C.1 and the functional delta method, see e.g. Theorem 20.8 in van der Vaart (1998), then imply
                        √
asymptotic normality of n(τ̂ − τ )/σ(F0 , F1 ). Theorem 5.2 then follows from Slutsky’s theorem and consis-
tency of σ̂ from Theorem 5.1                                                                                

   We next turn to the bootstrap distribution: Consider the bootstrap replicates
                                   n                                                                       n
                           1     X                                                                  1 X ∗ ∗
        F̂0∗ (y0 )   :=             R∗ (1 − Wi∗ )1l{Yi∗ (0) ≤ y0 },                 F̂1∗ (y1 )   :=       R W 1l{Yi∗ (1) ≤ y1 }
                        n(1 − p) i=1 i                                                              np i=1 i i

by randomizing from F̂01 . We also define the asymptotic covariance kernel Hiso corresponding to the coupling
C iso in analogy to (A.1) where F01 is replaced with C iso (F0 , F1 ). We first show the two following Lemmas:

                                                           iid
Lemma C.2. Suppose that (Yi (0), Yi (1)) ∼ F01 . Then for any copula C : [0, 1]2 → [0, 1],
                                                                                                    a.s.
                                         sup       C(F̂0 , F̂1 )(y0 , y1 ) − C(F0p , F1p )(y0 , y1 ) → 0
                                       y0 ,y1 ∈R


   Proof: From standard results, the class F := {(−∞, y] : y ∈ R} is Glivenko-Cantelli, so that (F̂0 −
F0p , F̂1
       − F1p ) converges to zero almost surely as an element of the space of bounded functions on R. Noting
that any copula C : [0, 1]2 → [0, 1] is a bounded nondecreasing function in each of its arguments, it follows
                                                                       23
that
                                                                                        a.s.
                               sup       C(F̂0 , F̂1 )(y0 , y1 ) − C(F0p , F1p )(y0 , y1 ) → 0
                             y0 ,y1 ∈R
establishing the claim                                                                                         
                                                iid
Lemma C.3. Suppose that (Yi (0), Yi (1)) ∼ F01 . Then the bootstrap process
                                                                !
                                               √        ∗
                                                     F̂0  − F̂0
                                        Ĝ∗n := n
                                                     F̂1∗ − F̂1
converges in outer probability to G under the bounded Lipschitz metric, that is

                                          sup     EW h(Ĝ∗n ) − Eh(G)        → 0
                                         h∈BL1

in outer probability, where G is a Gaussian processes with covariance kernel H.

   Proof: By construction of the coupling (Yi∗ (0), Yi∗ (1)), the marginal distributions of Yi∗ (0) and Yi∗ (1)
are equal to F̂0 and F̂1 , respectively. By construction of the bootstrap, the bootstrap replications F̂0∗ , F̂1∗
are generated by randomization from the samples (Ỹi (1), Ỹi (1))ni=1 corresponding to the joint distribution
F̂01 := C iso (F̂0 , F̂1 ).
   Now let Ĥiso the covariance kernel obtained from (A.1) replacing F0 with F̂0 , F1 with F̂1 , and F01 with
C (F̂0 , F̂1 ), respectively. By construction, the bootstrap distribution of Ĝ∗n conditional on F̂0 , F̂1 have
  iso

covariance given by Ĥiso . Finally, Ĥiso is a continuous function of C iso (F̂0 , F̂1 ). Hence by Lemma C.2 and
the continuous mapping theorem we have that
                                                                     a.s.
                                                  kĤiso − Hiso k → 0

which completes the proof.
  The claim of the Lemma then follows from the same arguments as in Lemma C.1 and the continuous
mapping theorem                                                                               

C.0.3. Proof of Theorem 5.3: It follows from Assumption 2.1 that τ (F0 , F1 ), σ(F0 , F1 ) are Hadamard differ-
entiable, so that Theorem C.3 follows from Lemma C.3 and the functional Delta method (e.g. Theorem 20.8
in van der Vaart (1998))                                                                                     

                                                      References

Abadie, A., S. Athey, G. Imbens, and J. Wooldridge (2017): “Sampling-based
  vs. Design-based Uncertainty in Regression Analysis,” working paper, MIT, Stanford,
  Michigan State.
Aronow, P., D. Green, and D. Lee (2014): “Sharp Bounds on the Variance in Ran-
  domized Experiments,” Annals of Statistics, 42(3), 850–871.
Becker, G. (1973): “A Theory of Marriage, Part I,” Journal of Political Economy, 81(4),
  813–846.
Bickel, P. (1969): “A Distribution Free Version of the Smirnov Two Sample Test in the
  p-Variate Case,” Annals of Mathematical Statistics, 40(1), 1–23.
Bickel, P., and D. Freedman (1984): “Asymptotic Normality and the Bootstrap in
  Stratified Sampling,” Annals of Statistics, 12(2), 470–482.
                                                             24
Bloznelis, M., and F. Götze (2001): “Orthogonal Decomposition of Finite Population
  Statistics and its Applications to Distributional Asymptotics,” Annals of Statistics, 29(3),
  899–917.
Booth, J., R. Butler, and P. Hall (1994): “Bootstrap Methods for Finite Populations,”
  Journal of the American Statistical Association, 89, 1282–1289.
Ding, P. (2017): “A Paradox From Randomization-Based Causal Inference,” Statistical
  Science, forthcoming.
Efron, B. (1982): The Jackknife, the Bootstrap and Other Resampling Plans, CBMS-NSF
  Regional Conference Series in Applied Mathematics. Society for Industrial and Applied
  Mathematics (SIAM), Philadelphia.
Fan, Y., and S. Park (2010): “Sharp Bounds on the Distribution of Treatment Effects
  and their Statistical Inference,” Econometric Theory, 26(3), 931–951.
         (2012): “Confidence Intervals for the Quantile of Treatment Effects in Randomized
  Experiments,” Journal of Econometrics, 167, 330–344.
Fan, Y., and J. Wu (2010): “Partial Identification of the Distribution of Treatment Effects
  in Switching Regime Models and its Confidence Sets,” Review of Economic Studies, 77(3),
  1002–1041.
Firpo, S., and G. Ridder (2008): “Bounds on Functionals of the Distribution of Treat-
  ment Effects,” working paper, Insper and USC.
Frank, M., R. Nelsen, and B. Schweizer (1987): “Best-Possible Bounds for the Dis-
  tribution of a Sum - a Problem of Kolmogorov,” Probability Theory and Related Fields,
  74, 199–211.
Heckman, J., J. Smith, and N. Clements (1997): “Making the Most Out Of Programme
  Evaluations and Social Experiments: Accounting For Heterogeneity in Programme Im-
  pacts,” Review of Economic Studies, 64, 487–535.
Imbens, G., and D. Rubin (2015): Causal Inference for Statistics, Social, and Biomedical
  Sciences. Cambridge University Press.
Li, X., and P. Ding (2017): “General forms of finite population central limit theorems
  with applications to causal inference,” JASA, forthcoming.
Makarov, G. (1982): “Estimates for the Distribution Function of a Sum of Two Ran-
  dom Variables when the Marginal Distributions are Fixed,” Theory of Probability and its
  Applications, 26(4), 803–806.
Manski, C. (1997): “The Mixing Problem in Programme Evaluation,” Review of Economic
  Studies, 64(4), 537–553.
Nelsen, R. (2006): An Introduction to Copulas. Springer, New York.
Neyman, J. (1923,1990): “On the Application of Probability Theory to Agricultural Experi-
  ments, Essays on Principles. Section 9.,” translated in Statistical Science (with discussion),

                                               25
  5(4), 465-480.
Robins, J. (1988): “Confidence Intervals for Causal Parameters,” Statistics in Medicine, 7,
  773–785.
Rosenbaum, P. (2002): Observational Studies. Springer, New York, 2 edn.
Rubin, D. (1978): “Bayesian Inference for Causal Effects: The Role of Randomization,”
  Annals of Statistics, 6, 34–58.
Stoye, J. (2010): “Partial Identification of Spread Parameters,” Quantitative Economics,
  1, 323–357.
van der Vaart, A. (1998): Asymptotic Statistics. Cambridge University Press, Cambridge.
van der Vaart, A., and J. Wellner (1996): Weak Convergence and Empirical Pro-
  cesses. Springer, New York.
Williamson, R., and T. Downs (1990): “Probabilistic Arithmetic. I. Numerical Methods
  for Calculating Convolutions and Dependency Bounds,” International Journal of Approx-
  imate Reasoning, 4, 89–158.




                                            26
