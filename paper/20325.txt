                                NBER WORKING PAPER SERIES




                    FINITE POPULATION CAUSAL STANDARD ERRORS

                                            Alberto Abadie
                                              Susan Athey
                                           Guido W. Imbens
                                        Jeffrey M. Wooldridge

                                        Working Paper 20325
                                http://www.nber.org/papers/w20325


                      NATIONAL BUREAU OF ECONOMIC RESEARCH
                               1050 Massachusetts Avenue
                                 Cambridge, MA 02138
                                      July 2014




We are grateful for comments by Daron Acemoglu, Joshua Angrist, Matias Cattaneo, Jim Poterba,
Bas Werker, and seminar participants at Microsoft Research, Michigan, MIT, Stanford, Princeton,
NYU, Columbia, Tilburg University, the Tinbergen Institute, and University College London, and
especially for discussions with Gary Chamberlain. The views expressed herein are those of the authors
and do not necessarily reflect the views of the National Bureau of Economic Research.

At least one co-author has disclosed a financial relationship of potential relevance for this research.
Further information is available online at http://www.nber.org/papers/w20325.ack

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2014 by Alberto Abadie, Susan Athey, Guido W. Imbens, and Jeffrey M. Wooldridge. All rights
reserved. Short sections of text, not to exceed two paragraphs, may be quoted without explicit permission
provided that full credit, including © notice, is given to the source.
Finite Population Causal Standard Errors
Alberto Abadie, Susan Athey, Guido W. Imbens, and Jeffrey M. Wooldridge
NBER Working Paper No. 20325
July 2014
JEL No. C01,C18

                                               ABSTRACT

When a researcher estimates the parameters of a regression function using information on all 50 states
in the United States, or information on all visits to a website, what is the interpretation of the standard
errors? Researchers typically report standard errors that are designed to capture sampling variation,
based on viewing the data as a random sample drawn from a large population of interest, even in applications
where it is difficult to articulate what that population of interest is and how it differs from the sample.
In this paper we explore alternative interpretations for the uncertainty associated with regression estimates.
As a leading example we focus on the case where some parameters of the regression function are intended
to capture causal effects. We derive standard errors for causal effects using a generalization of randomization
inference. Intuitively, these standard errors capture the fact that even if we observe outcomes for all
units in the population of interest, there are for each unit missing potential outcomes for the treatment
levels the unit was not exposed to. We show that our randomization-based standard errors in general
are smaller than the conventional robust standard errors, and provide conditions under which they
agree with them. More generally, correct statistical inference requires precise characterizations of
the population of interest, the parameters that we aim to estimate within such population, and the sampling
process. Estimation of causal parameters is one example where appropriate inferential methods may
differ from conventional practice, but there are others.


Alberto Abadie                                         Guido W. Imbens
John F. Kennedy School of Government                   Graduate School of Business
Harvard University                                     Stanford University
79 JFK Street                                          655 Knight Way
Cambridge, MA 02138                                    Stanford, CA 94305
and NBER                                               and NBER
alberto_abadie@harvard.edu                             Imbens@stanford.edu

Susan Athey                                            Jeffrey M. Wooldridge
Graduate School of Business                            Department of Economics
Stanford University                                    Michigan State University
655 Knight Way                                         wooldri1@msu.edu
Stanford, CA 94305
and NBER
athey@stanford.edu
1     Introduction
In many empirical studies in economics, researchers specify a parametric relation between ob-
servable variables in a population of interest. They then proceed to estimate and do inference
for the parameters of this relation. Point estimates are based on matching the relation between
the variables in the population to the relation observed in the sample, following what Gold-
berger (1968) and Manski (1988) call the “analogy principle.” In the simplest setting with an
observed outcome and no covariates the parameter of interest might simply be the population
mean, estimated by the sample average. Given a single covariate, the parameters of interest
might consist of the slope and intercept of the best linear predictor for the relationship between
the outcome and the covariate. The estimated value of a slope parameter might be used to
answer an economics question such as, what is the average impact of a change in the minimum
wage on employment? Or, what will be the average (over markets) of the increase in demand if
a firm lowers its posted price? A common hypothesis to test is that the population value of the
slope parameter of the best linear predictor is equal to zero.
    The textbook approach to conducting inference in such contexts relies on the assumptions
that (i) the observed units are a random sample from a large population, and (ii) the parameters
in this population are the objects of interest. Uncertainty regarding the parameters of interest
arises from sampling variation, due to the difference between the sample and the population. A
95% confidence interval has the interpretation that if one repeatedly draws new random samples
from this population and construct new confidence intervals for each sample, the estimand should
be contained in the confidence interval 95% of the time. In many cases this random sampling
perspective is attractive. If one analyzes individual-level data from the Current Population
Survey, the Panel Study of Income Dynamics, the 1% public use sample from the Census, or
other public use surveys, it is clear that the sample analyzed is only a small subset of the
population of interest. However, in this paper we argue that there are other settings where
there is no population such that the sample can be viewed as small relative to that population,
randomly drawn from it, and when the estimand is the population value of that parameter. For
example, suppose that the units are all fifty states of the United States, all the countries in the
world, or all visits to a website. If we observe a cross-section of outcomes at a single point in time
and ask how the average outcome varies with attributes of the units, the answer is a quantity
that is known with certainty. For example, the difference in average outcome between coastal


                                                 [1]
and inland states for the observed year is known: the sample average difference is equal to the
population average difference. Thus the standard error on the estimate of the difference should
be zero. However, without exception researchers report positive standard errors in such settings.
More precisely, researchers typically report standard errors using formulas that are formally
justified by the assumption that the sample is drawn randomly from an infinite population.
The theme in this paper is that this random-sampling-from-a-large-population assumption is
often not the natural one for the problem at hand, and that there are other, more natural
interpretations of the uncertainty in the estimates.
   The general perspective we take is that statistics is fundamentally about drawing inferences
with incomplete data. If the researcher sees all relevant data, there is no need for inference, since
any question can be answered by simply doing calculations on the data. Outside of this polar
case, it is important to be precise in what sense the data are incomplete. Often we can consider
a population of units and a set of possible states of the world. There is a set of variables that
takes on different values for each unit depending on the state of the world. The sampling scheme
tells us how units and states of the world are chosen to form a sample, and what variables are
observed, and what repeated sampling perspective may be reasonable.
   Although there are many settings to consider, in the current paper we focus on the specific
case where the state of the world corresponds to the level of a causal variable for each unit,
e.g., a government regulation or a price set by a firm. The question of interest concerns the
average causal effect of the variable: for example, the difference between the average outcome
if (counterfactually) all units in the population are treated, and the average outcome if (coun-
terfactually) all units in the population are not. Note that we will never observe the values for
all variables of interest, because by definition we observe each physical unit at most once, either
in the state where it is treated or the state where it is not, with the value of the outcome in
the other state missing. Questions about causal effects can be contrasted with descriptive or
predictive questions. An example of a descriptive estimand is the difference between the average
outcome for countries with one set of institutions and the average outcome for countries with
a different set of institutions. Although researchers often focus on causal effects in discussions
about the interpretation of findings, standard practice does not distinguish between descriptive
and causal estimands when conducting estimation and inference. In this paper, we show that
this distinction matters. Although the distinction between descriptive estimands and causal
estimands is typically not important for estimation under exogeneity assumptions, and is also

                                                 [2]
immaterial for inference if population size is large relative to the sample size, the distinction
between causal and descriptive estimands matters for inference if the sample size is more than a
negligible fraction of the population size. As a result the researcher should explicitly distinguish
between regressors that are potential causes and those that are fixed attributes.
   Although this focus on causal estimands is rarely made explicit in regression settings, it
does have a long tradition in randomized experiments. In that case the natural estimator for
the average causal effect is the difference in average outcomes by treatment status. In the
setting where the sample and population coincide, Neyman (1923) derived the variance for this
estimator and proposed a conservative estimator for this variance. The results in the current
paper can be thought of as extending Neyman’s analysis to general regression estimators in
observational studies. Our formal analysis allows for discrete or continuous treatments and
for the presence of attributes that are potentially correlated with the treatments. Thus, our
analysis applies to a wide range of regression models that might be used to answer questions
about the impact of government programs or about counterfactual effects of business policy
changes, such as changes in prices, quality, or advertising about a product. We make four
formal contributions. First, the main contribution of the study is to generalize the results for
the approximate variance for multiple linear regression estimators associated with the work by
Eicker (1967), Huber (1967), and White (1980ab, 1982), EHW from hereon, in two directions.
We allow the population to be finite, and we allow the regressors to be potential causes or
attributes, or a combination of both. We take account of both the uncertainty arising from
random sampling and the uncertainty arising from conditional randomization of the potential
causes. This contribution can also be viewed as generalizing results from Neyman (1923) to
settings with multiple linear regression estimators with both treatments and attributes that are
possibly correlated. In the second contribution, we show that in general, as in the special, single-
binary-covariate case that Neyman considers, the conventional EHW robust standard errors are
conservative for the standard errors for the estimators for the causal parameters. Third, we
show that in the case with attributes that are correlated with the treatments one can generally
improve on the EHW variance estimator if the population is finite, and we propose estimators
for the standard errors that are generally smaller than the EHW standard errors. Fourth, we
show that in a few special cases the EHW standard errors are consistent for the true standard
deviation of the least squares estimator.
   By using a randomization inference approach the current paper builds on a large litera-

                                                [3]
ture going back to Fisher (1935) and Neyman (1923). The early literature focused on settings
with randomized assignment without additional covariates. See Rosenbaum (1995) and Imbens
and Rubin (2014) for textbook discussions. More recent studies analyze regression methods
with additional covariates under the randomization distribution in randomized experiments,
e.g., Freedman (2008ab), Lin (2013), Samii and Aronow (2012), and Schochet (2010). For ap-
plications of randomization inference in observational studies see Rosenbaum (2002), Abadie,
Diamond and Hainmueller (2010), Imbens and Rosenbaum (2005), Frandsen (2012), Bertrand,
Duflo, and Mullainathan (2004) and Barrios, Diamond, Imbens and Kolesar (2012). In most
of these studies, the assignment of the covariates is assumed to be completely random, as in
a randomized experiment. Rosenbaum (2002) allows for dependence between the assignment
mechanism and the attributes by assuming a logit model for the conditional probability of as-
signment to a binary treatment. He estimates the effects of interest by minimizing test statistics
based on conditional randomization. In the current paper, we allow explicitly for general depen-
dendence of the assignment mechanism of potential causes (discrete or continuous) on the fixed
attributes (discrete or continuous) of the units, thus making the methods applicable to general
regression settings.
    Beyond questions of causality in a given cross-section, there are other kinds of questions
one could ask where the definition of the population and the sampling scheme look different;
for example, we might consider the population as consisting of units in a variety of potential
states of the world, where the state of the world affects outcomes through an unobservable
variable. For example, we could think of a population where a member consists of a country
with different realizations of weather, where weather is not in the observed data, and we wish
to draw inferences about what the impact of regulation on country-level outcomes would be in
a future year with different realizations of weather outcomes. We present some thoughts on this
type of question in Section 6.


2     Three Examples
In this section we set the stage for the problems discussed in the current paper by introducing
three simple examples for which the results are well known from either the finite population
survey literature (e.g., Cochran, 1977; Kish, 1995), or the causal literature (e.g., Neyman,
1923; Rubin, 1974; Holland, 1986; Imbens and Wooldridge, 2008; Imbens and Rubin, 2014).


                                               [4]
Juxtaposing these examples will provide the motivation for, and insight into, the problems we
study in the current paper.

2.1     Inference for a Finite Population Mean with Random Sampling
Suppose we have a population of size M, where M may be small, large, or infinite. In the first
example we focus on the simplest setting where the regression model includes only an intercept.
Associated with each unit i is a non-stochastic variable Yi , with YM denoting the M−vector
with ith element Yi . The target, or estimand, is the population mean of Yi ,
                          M
               pop     1 X
      µM = Y   M     =       Yi .
                       M i=1

We index µM by the population size M because for some of the formal results we consider
sequences of experiments with populations of increasing size. In that case we make assumptions
that ensure that the sequence {µM : M = 1, 2 . . .} converges to a finite constant µ, but allow
for the possibility that the population mean varies over the sequence. The dual notation for the
                           pop
same object, µM and Y M , captures the dual aspects of this quantity: on the one hand it is a
population quantity, for which it is common to use Greek symbols. On the other hand, because
                                                                    pop
the population is finite, it is a simple average, and the Y M notation shows the connection to
averages. To make the example specific, one can think of the units being the 50 states (M = 50),
and Yi being state-level average earnings.
   We do not necessarily observe all units in this population. Let Wi be a binary variable
indicating whether we observe Yi (if Wi = 1) or not (if Wi = 0), with WM the M-vector with
                                 P
ith element equal to Wi , and N = M  i=1 Wi the sample size. We let {ρM }M =1,2,... be a sequence

of sampling probabilities, one for each population size M, where ρM ∈ (0, 1). If the sequence
{ρM }M =1,2,... has a limit, we denote it s limit by ρ. We make the following assumption about
the sampling process.

Assumption 1. (Random Sampling without Replacement) Given the sequence of sam-
pling probabilities {ρM }M =1,2,...,
                           PM
                                 wi                 M− M
                                                      P
                                                       i=1 wi
      pr (WM = w) = ρM i=1            · (1 − ρM )               ,

for all w with i-th element wi ∈ {0, 1}, and all M.


                                                         [5]
   This sampling scheme makes the sample size N random. An alternative is to draw a random
sample of fixed size. Here we focus on the case with a random sample size in order to allow for
the generalizations we consider later. Often the sample is much smaller than the population but
it may be that the sample coincides with the population.
   The natural estimator for the population average µM is the sample average:
                              M
                sample     1 X
      µ̂M = Y   M        =       Wi · Yi .
                           N i=1

To be formal, let us define µ̂M = 0 if N = 0, so µ̂M is always defined. Conditional on N > 0
this estimator is unbiased for the population average µM :
                              h sample       i
      EW [ µ̂M | N > 0] = EW Y M       N > 0 = µM .

The subscript W for the expectations operator (and later for the variance operator) indicates
that this expectation is over the distribution generated by the randomness in the vector of
sampling indicators WM : the M-vector YM is fixed. We are interested in the variance of the
estimator µ̂M conditional on N:
                                                                                        
                                            2                     sample        pop 2
      VW ( µ̂M | N) = EW (µ̂M − µM ) N = EW                     Y   M        −Y   M       N .

Because we condition on N this variance is itself a random variable. It is also useful to define
the normalized variance, that is, the variance normalized by the sample size N:

      Vnorm (µ̂M ) = N · VW ( µ̂M | N) ,

which again is a random variable. Also define
                         M
       2       1 X            pop 2
      σM   =           Yi − Y        ,
             M − 1 i=1

which we refer to as the population variance (note that, in contrast to some definitions, we
divide by M − 1 rather than M).
   Here we state a slight modification of a well-known result from the survey sampling literature.
The case with a fixed sample size can be found in various places in the survey sampling literature,
such as Cochran (1977) and Kish (1995). Deaton (1997) also covers the result. We provide a
proof because of the slight modification and because the basic argument is used in subsequent
results.

                                                     [6]
Lemma 1. (Exact Variance under Random Sampling) Suppose Assumption 1 holds.
Then
                               2
                                       
                              σM      N
       VW ( µ̂M | N, N > 0) =    · 1−     .
                              N       M
All proofs are in the appendix.
   If the sample is close in size to the population, then the variance of the sample average as an
estimator of the population average will be close to zero. The adjustment factor for the finite
population, 1 − N/M, is proportional to one minus the ratio of the sample and population size.
It is rare to see this adjustment factor used in empirical studies in economics.
   For the next result we rely on assumptions about sequences of populations with increasing
size, indexed by the population size M. These sequences are not stochastic. We assume that
the first and second moments of the population outcomes converge as the population size grows.
                                                     PM
Let µk,M be the k th population moment of Yi , µk,M = i=1 Yik /M.

Assumption 2. (Sequence of Populations) For k = 1, 2, and some constants µ1 , µ2 ,

        lim µk,M = µk .
       M →∞

   Define σ 2 = µ2 − µ21 . We will also rely on the following assumptions on the sampling rate.

Assumption 3. (Sampling Rate) The sequence of sampling rates ρM satisfies

       M · ρM → ∞,            and ρM → ρ ∈ [0, 1].

   The first part of the assumption guarantees that as the population size diverges, the (random)
sample size also diverges. The second part of the assumption allows for the possibility that
asymptotically the sample size is a neglible fraction of the population size.

Lemma 2. (Variance in Large Populations) Suppose Assumptions 1-3 hold. Then: (i)
                         σ2
       VW (µ̂M |N) −        = Op ((ρM · M)−1 ),
                         N
(where σ 2/N is ∞ if N = 0), and (ii), as M → ∞,
                     p
       Vnorm (µ̂M ) −→ σ 2 · (1 − ρ).

   In particular, if ρ = 0, the normalized variance converges to σ 2 , corresponding to the con-
ventional result for the normalized variance.

                                                  [7]
2.2    Inference for the Difference of Two Means with Random Sam-
       pling from a Finite Population
Now suppose we are interested in the difference between two population means, say the difference
in state-level average earnings for coastal and landlocked states for the 50 states in the United
States. We have to be careful, because if we draw a relatively small, completely random, sample
there may be no coastal or landlocked states in the sample, but the result is essentially still the
same: as N approaches M, the variance of the standard estimator for the difference in average
earnings goes to zero, even after normalizing by the sample size.
   Let Xi ∈ {coast, land} denote the geographical status of state i. Define, for x = coast, land,
                          P
the population size Mx = M  i=1 1Xi =x , and the population averages and variances

                     pop    1 X                       2                1    X                     pop    2
      µx,M = Y x,M =            Yi ,             and σx,M =                                 Yi − Y x,M        .
                            Mx                                       Mx − 1
                                  i:Xi =x                                         i:Xi =x

The estimand is the difference in the two population means,

                 pop             pop
      θM = Y coast,M − Y land,M ,

and the natural estimator for θM is the difference in sample averages by state type,

                 sample         sample
      θ̂M = Y coast − Y land ,

where the averages of observed outcomes and sample sizes by type are

        sample       1 X                                          X
      Yx         =             Wi · Yi ,       and Nx =                    Wi ,
                     Nx i:X =x                                   i:Xi =x
                            i



for x = coast, land. The estimator θ̂M can also be thought of as the least squares estimator for
θ based on minimizing
                 M
                 X
      arg min          Wi · (Yi − γ − θ · 1Xi =coast)2 .
           γ,θ
                 i=1

   The extension of part (i) of Lemma 1 to this case is fairly immediate. Again the outcomes
Yi are viewed as fixed quantities. So are the attributes Xi , with the only stochastic component
the vector WM . We condition on Ncoast and Nland being positive.



                                                           [8]
Lemma 3. (Random Sampling and Regression) Suppose Assumption 1 holds. Then

                                                
      VW θ̂ Nland, Ncoast, Nland > 0, Ncoast > 0 =
                 2                          2                
               σcoast,M          Ncoast      σland,M      Nland
                        · 1−              +          · 1−         .
                Ncoast           Mcoast       Nland       Mland
   Again, as in Lemma 1, as the sample size approaches the population size, for a fixed popula-
tion, the variance converges to zero. In the special case where the two sampled fractions are the
same, Ncoastal/Mcoastal = Nland/Mland = ρ, the adjustment relative to the conventional variance
is again simply the factor 1 − ρ, one minus the sample size over the population size.

2.3    Inference for the Difference in Means given Random Assignment
This is the most of important of the three examples, and the one where many (but not all) of the
issues that are central in the paper are present. Again it is a case with a single binary regressor.
However, the nature of the regressor is conceptually different. To make the discussion specific,
suppose the binary indicator or regressor is an indicator for the state having a minimum wage
higher than the federal minimum wage, so Xi ∈ {low, high}. One possibility is to view this
example as isomorphic to the previous example. This would imply that for a fixed population
size the variance would go to zero as the sample size approaches the population size. However,
we take a different approach to this problem that leads to a variance that remains positive even
if the sample is identical to the population. The key to this approach is the view that this
regressor is not a fixed attribute or characteristic of each state, but instead is a potential cause.
The regressor takes on a particular value for each state in our sample, but its value could have
been different. For example, in the real world Massachusetts has a state minimum wage that
exceeds the federal one. We are interested in the comparison of the outcome, say state-level
earnings, that was observed, and the counterfactual outcome that would have been observed had
Massachusetts not had a state minimum wage that exceeded the federal one. Formally, using
the Rubin causal model or potential outcome framework (Neyman, 1923; Rubin, 1974; Holland,
1986; Imbens and Rubin, 2014), we postulate the existence of two potential outcomes for each
state, denoted by Yi (low) and Yi (high), for earnings without and with a state minimum wage,
with Yi the outcome corresponding to the actual or prevailing minimum wage:
                     
                       Yi (high)     if Xi = high,
     Yi = Yi (Xi ) =
                       Yi (low)      otherwise.

                                                 [9]
It is important that these potential outcomes (Yi (low), Yi (high)) are well defined for each unit
(the 50 states in our example), irrespective of whether that state has a minimum wage higher
than the federal one or not. Let YM , and XM be the M-vectors with ith element equal to Yi ,
and Xi respectively.
       We now define two distinct estimands. The first is the population average causal effect of
the state minimum wage, defined as

                     1 X                      
                        M
          causal
         θM        =       Yi (high) − Yi (low) .                                                           (2.1)
                     M i=1

We disinguish this causal estimand from the descriptive or predictive difference in population
averages by minimum wage,
                     1        X                  1       X
          descr
         θM     =                        Yi −                      Yi .                                     (2.2)
                    Mhigh                       Mlow
                            i:Xi =high                 i:Xi =low


It is the difference between the two estimands, θcausal and θdescr, that is at the core of our paper.
First, we argue that although researchers are often interested in causal rather than descriptive
estimands, this distinction is not often made explicit. However, many textbook discussions
formally define estimands in a way that corresponds to descriptive estimands.1 Second, we show
that in settings where the sample size is of the same order of magnitude as the population
size, the distinction between the causal and descriptive estimands matters. In such settings the
researcher therefore needs to be explicit about the causal or descriptive nature of the estimand.
                                                                                       causal      descr
       Let us start with the first point, the relative interest in the two estimands, θM      and θM     .
Consider a setting where a key regressor is a state regulation. The descriptive estimand is
the average difference in outcomes between states with and states without the regulation. The
causal estimand is the average difference, over all states, of the outcome with and without that
regulation for that state. We would argue that in such settings the causal estimand is of more
interest than the descriptive estimand.
   1
    For example, Goldberger (1968) writes “Regression analysis is essentially concerned with estimation of such
a population regression function on the basis of a sample of observations drawn from the joint probability
distribution of Yi , Xi .” (Goldberger, 1968, p. 3). Wooldridge (2002) writes: “More precisely, we assume that
(1) a population model has been specified and (2) an independent identically distributed (i.i.d.) sample can
be drawn from the population.” (Wooldridge, 2002 p. 5). Angrist and Pischke (2008) write: “We therefore
use samples to make inferences about populations” (Angrist and Pishke, 2008, p. 30). Gelman and Hill (2007)
write: “Statistical inference is used to learn from incomplete or imperfect data. ... In the sampling model we are
interested in learning some characteristic of a population ... which we must estimate from a sample, or subset,
of the population. (Gelman and Hill, 2007).

                                                                   [10]
    Now let us study the statistical properties of the difference between the two estimands. We
assume random assignment of the binary covariate Xi :

Assumption 4. (Random Assignment) For some sequence {qM : M = 1, 2, . . .}, with qM ∈
(0, 1),
                          PM
                               1xi =high
                                                           PM
      pr (X = x) = qM i=1                  · (1 − qM )M −    i=1   1xi =low
                                                                              ,

for all M-vectors x with xi ∈ {low, high}, and all M.

    In the context of the example with the state minimum wage, the assumption requires that
whether a state has a state minimum wage exceeding the federal wage is unrelated to the poten-
tial outcomes. This assumption, and similar ones in other cases, is arguably unrealistic, outside
of randomized experiments. Often such an assumption is more plausible within homogenous
subpopulations defined by observable attributes of the units. This is the motivation for in-
cluding additional covariates in the specification of the regression function, and we consider
such settings in the next section. For expositional purposes we proceed in this section with the
simpler setting.
                                       descr      causal
    To formalize the relation between θM     and θM      we introduce notation for the means of the
two potential outcomes, for x = low, high, over the entire population and by treatment status:
                       M
          pop       1 X                              pop        1 X
      Y   M (x)   =       Yi (x),             and Y x,M =                 Yi (x),
                    M i=1                                       Mx i:X =x
                                                                          i

                           PM
where, as before, Mx =         i=1   1Xi =x is the population size by treatment group. Note that because
Xi is a random variable, Mhigh and Mlow are random variables too. Now we can write the two
estimands as

       causal       pop              pop                                          pop     pop
      θM      = Y M (high) − Y M (low),                 and θdescr = Y high,M − Y low,M .

Define the population variances of the two potential outcomes Yi (low) and Yi (high),
                           M
       2           1 X                pop  2
      σM (x)   =           Yi (x) − Y M (x) ,                        for x = low, high,
                 M − 1 i=1

and the population variance of the unit-level causal effect Yi (high) − Yi (low):
                                      M
       2                    1 X                             pop        pop   2
      σM (low, high) =              Yi (high) − Yi (low) − Y (high) − Y (low) .
                          M − 1 i=1

                                                            [11]
     The following lemma describes the relation between the two population quantities. Note
      causal                                                    descr
that θM      is a fixed quantity given the population, whereas θM     is a random variable because
it depends on XM , which is random by Assumption 4. To stress where the randomness in
 descr
θM     stems from, and in particular to distinguish this from the sampling variation, we use the
subscript X on the expectations and variance operators here. Note that at this stage there is
no sampling yet: the statements are about quantities in the population.

Lemma 4. (Causal versus Descriptive Estimands) Suppose Assumption 4 holds. Then
(i) the descriptive estimand is unbiased for the causal estimand,

            descr                                 causal
       EX [θM     |Mhigh, Mlow > 0, Mhigh > 0] = θM      ,

and (ii),

            descr
                                            
       VX θ M     Mhigh, Mlow > 0, Mhigh > 0
                               h                      2                             i
                                    descr    causal
                    = EX           θM     − θM             Mhigh, Mlow > 0, Mhigh > 0
                         2        2         2
                        σM (low) σM (high) σM (low, high)
                    =           +         −               ≥ 0.
                         Mlow      Mhigh         M
     These results are well-known from the causality literature, starting with Neyman (1923). See
for a recent discussion and details Imbens and Rubin (2014).
     Now let us generalize these results to the case where we only observe values for Xi and Yi for
a subset of the units in the population. As before in Assumption 1, we assume this is a random
subset, but we strengthen Assumption 1 by assuming the sampling is random, conditional on
X.

Assumption 5. (Random Sampling Without Replacement) Given the sequence of sam-
pling probabilities {ρM : M = 1, 2, . . .}, and conditional on XM ,
                                      PM                         PM
                                            wi
       pr (WM = w|XM ) = ρM i=1                  · (1 − ρM )M −      i=1   wi
                                                                                ,

for all M-vectors w with i-th element wi ∈ {0, 1}, and all M.

     We focus on the properties of the same estimator as in the second example in Section 2.2,

              obs       obs
       θ̂ = Y high − Y low ,

                                                              [12]
where, for x ∈ {low, high},
                                                             M
             obs     1 X                                     X
         Y   x     =           Wi · Yi ,         and Nx =           Wi · 1Xi =x .
                     Nx i:X =x                                i=1
                            i


       The following results are closely related to results in the causal literature. Some of the results
rely on uncertainty from random sampling, some on uncertainty from random assignment, and
some rely on both sources of uncertainty: the superscripts W and X clarify these distinctions.

Lemma 5. (Expectations and Variances for Causal and Descriptive Estimands)
Suppose that Assumptions 4 and 5 hold. Then:
       (i)
                    h                                      i
                                                               causal
         EW ,X θ̂ Nlow , Nhigh, Nlow > 0, Nhigh         > 0 = θM      ,

(ii)
                                                            
                     causal
         VW ,X θ̂ − θM      Nlow , Nhigh, Nlow > 0, Nhigh > 0

                         2        2         2
                        σM (low) σM (high) σM (low, high)
                   =            +         −               ,
                          Nlow     Nhigh         M
(iii)
           h                                i
                                                descr
         EW θ̂ X, Mlow , Nlow > 0, Nhigh > 0 = θM     ,

(iv)
                                                                            
                            descr
         VW ,X θ̂ −        θM       Mlow , Nlow , Nhigh, Nlow > 0, Nhigh   >0

                      2
                                           2
                                                                
                     σM (low)      Nlow     σM (high)      Nhigh
                   =          · 1−        +           · 1−         ,
                       Nlow        Mlow       Nhigh        Mhigh
(v)
                                                            
                     causal
         VW ,X θ̂ − θM      Nlow , Nhigh, Nlow > 0, Nhigh > 0
                                                                              
                                       descr
                       −VW ,X θ̂ − θM        Nlow , Nhigh, Nlow > 0, Nhigh > 0
                            desc    causal
                                                                             
                   = VW ,X θM    − θM       Nlow , Nhigh, Nlow > 0, Nhigh > 0
                         2        2         2
                        σM (low) σM (high) σM (low, high)
                   =            +         −               ≥ 0.
                         Mlow      Mhigh         M

                                                            [13]
Part (ii) of Lemma 5 is a re-statement of results in Neyman (1923). Part (iv) is essentially
the same result as in Lemma 2. Parts (ii) and (iv) of the lemma, in combination with Lemma
4, imply part (v). Although part (ii) and (iv) of Lemma 5 are both known in their respective
literatures, the juxtaposition of the two variances has not received much attention.
   Next, we study what happens in large populations. In order to do so we need to modify
Assumption 2 for the current context. First, define
                    M
                 1 X k
      µk,m,M   =      Y (low) · Yim (high).
                 M i=1 i
We assume that all (cross-)moments up to second order converge to finite limits.

Assumption 6. (Sequence of Populations) For nonnegative integers k, m such that k +
m ≤ 2, and some constants µk,m ,

       lim µk,m,M = µk,m .
      M →∞

   Then define σ 2(low) = µ2,0 − µ21,0 and σ 2(high) = µ0,2 − µ20,1, so that under Assumption
           2
6 limM →∞ σM (low) = σ 2(low) and limM →∞ σM
                                           2
                                             (high) = σ 2(high). Also define, again under
                       2
Assumption 6, limM →∞ σM (low, high) = σ 2(low, high).
   Define the normalized variances for the causal and descriptive estimands,
                                                       
     Vnorm
       causal = N · V W ,X   θ̂ − θ causal
                                    M      Nhigh , Nlow   ,

and
                                                    
      Vnorm
       descr = N · V W ,X   θ̂ − θ descr
                                   M     Nhigh , Nlow ,


where the variances are zero if Nhigh or Nlow are zero.

Assumption 7. (Limiting Assignment Rate) The sequence of assignment rates qM satisfies

       lim qM = q ∈ (0, 1).
      M →∞

Lemma 6. (Variances for Causal and Descriptive Estimands in Large Popula-
tions) Suppose that Assumptions 3–7 hold. Then, as M → ∞, (i),
                p   σ 2(low) σ 2 (high)
      Vnorm
       causal −→            +           − ρ · σ 2(low, high)                            (2.3)
                     1−q          q
and (ii),
                                             
               p        σ 2 (low) σ 2(high)
      Vnorm
       descr   −→                +                · (1 − ρ).                            (2.4)
                         1−q          q

                                                        [14]
     Lemma 6 contains some key insights. It shows that we do not need to be concerned about
                                          causal      descr
the difference between the two estimands θM      and θM     in settings where the population is
large relative to the sample (ρ close to zero). In that case both normalized variances are equal
to
                                                    σ 2(low) σ 2 (high)
        lim Vnorm
             causal         = lim Vnorm
                                   desc         =           +           .
       M →∞           ρ=0     M →∞        ρ=0        1−q          q
It is only in settings with the ratio of the sample size to the population size non-neglible, and in
particular if the sample size is equal to the population size, that there are substantial differences
between the two variances.


3      The Variance of Regression Estimators when the Re-
       gression includes Attributes and Causes
In this section and the next we turn to the setting that is the main focus of the current paper.
We allow for the presence of covariates of the potential cause type (say a state institution or a
regulation such as the state minimum wage), which can be discrete or continuous, and vector-
valued. We also allow for the presence of covariates of the attribute or characteristic type, say
an indicator whether a state is landlocked or coastal, which again can be discrete or continuous
or discrete, and vector-valued. We allow the potential causes and attributes to be systematically
correlated in the sample, because the distribution of the potential causes differs between units.

3.1     Set Up
We denote the potential causes for unit i in population M by XiM , and the attributes for unit
i in population M by ZiM . The vector of attributes ZiM typically includes an intercept. We
assume there exists a set of potential outcomes YiM (x), with the realized outcome for unit i in
population M equal to YiM = YiM (XiM ). We sample units from this population, with WiM the
binary indicator for the event that unit i in population M is sampled. We view the potential
outcome functions YiM (x) and the attributes ZiM as deterministic, and the potential causes XiM
and the sampling indicator WiM as stochastic. However, unlike in a randomized experiment,
the potential cause XiM is in general not identically distributed.
     For general regression we have no finite sample results for the properties of the least squares
estimator. Instead we rely on large sample arguments. We formulate assumptions on the

                                                      [15]
sequence of populations, characterized by sets of covariates or attributes ZM and potential
outcomes YM (x), as well as on the sequence of assignment mechanisms. To be technically
precise we use a double index on all variables, whether deterministic or stochastic, to reflect
the fact that the distributions general depend on the population. For the asymptotics we let
the size of the population M go to infinity. We allow the sampling rate, ρM , to be a function
of the population, allowing for ρM = 1 (the sample is the population) as well as ρM → 0
(random sampling from a large population). In the latter case case our results agree with the
standard robust Eicker-Huber-White variance results based on random sampling from an infinite
population. The only stochastic component is the matrix (XM , WM ). The randomness in XM
generates randomness in the realized outcomes YiM = YiM (XiM ) even though the potential
outcome functions YiM (x) are non-stochastic.
   For a given population, indexed by M, define the population moments
                                   0
                XM     YiM       YiM
              1
     Ωpop
       M =
                    ZiM   ZiM  ,
             M i=1
                      XiM        XiM
and the expected population moments, where the expectation is taken over the assignment X,
                                                   0 
                                 XM    YiM        YiM
     Ω∗,pop
       M    = EX [Ωpop
                   M ] = EX
                             1       ZiM   ZiM   .
                               M i=1
                                       XiM       XiM
Also define the sample moments,
                                       0
                   M
                   X          YiM     YiM
                 1
     Ωsample
       M     =        WiM ·  ZiM   ZiM 
                N i=1
                              XiM     XiM
where N is the random sample size.
   The partitioned   versions of these three matrices will be written as
                                  
             ΩY Y    ΩY Z 0 ΩY X 0
    Ω=      ΩZY     ΩZZ 0 ΩZX 0  ,
             ΩXY     ΩXZ 0 ΩXX 0

for Ω = Ωpop  ∗,pop
         M , ΩM     and Ωsample
                         M      . Below we state formal assumptions that ensure that these
quantities are well-defined and finite, at least in large populations.
   For a population of size M, we estimate a linear regression model

            0       0
     YiM = XiM θ + ZiM γ + εiM ,

                                                [16]
by ordinary least squares, with the estimated least squares coefficients equal to
                                  M
                                  X
                                                      0       0         2
      (θ̂ols , γ̂ols) = arg min         WiM · (YiM − XiM θ − ZiM γ) ,
                           θ,γ
                                  i=1
where WiM simply selects the sample that we use in estimation. The unique solution, assuming
no perfect collinearity in the sample, is
                                       !−1                    !
        θ̂ols         Ωsample
                       XX,M    Ω sample
                                 XZ ,M
                                    0                 Ωsample
                                                       XY,M
                =                                                   .
        γ̂ols        Ωsample
                       ZX ,M
                          0    Ω sample
                                 ZZ 0 ,M              Ωsample
                                                       ZY,M

We are interested in the properties of the least squares estimator for descriptive and causal
estimands.

3.2    Descriptive and Causal Estimands
                                                                    causal      descr
We now define the descriptive and causal estimands that generalize θM      and θM     from Section
2.3. For the descriptive estimand the generalization is obvious: we are interested in the value of
the least squares estimator if all units in the population are observed:
       descr   pop                      −1  pop 
         θM            ΩXX,M Ωpop  XZ 0 ,M       ΩXY,M
                 =      pop        pop                    .
          descr
         γM            ΩZX 0 ,M ΩZZ 0 ,M         Ωpop
                                                  ZY,M

This estimand, even though a population quantity, is stochastic because it is a function of
XM = (X1M , X2M , ..., XM M )0 . For the causal estimand we look at the same expression, with
expectations taken over X in both components:
      causal   ∗,pop                  −1  ∗,pop 
        θM            ΩXX,M Ω∗,pop
                                 XZ 0 ,M      ΩXY,M
                 =                                     .
         causal
        γM            Ω∗,pop
                        ZX 0 ,M Ω∗,pop
                                 ZZ 0 ,M      Ω∗,pop
                                               ZY,M

These causal parameters are non-stochastic.
   To build some insight for the definition of the causal parameters, consider the special case
with the attributes consisting of an intercept only, ZiM = 1, and a single randomly assigned
binary cause, XiM ∈ {0, 1}, the case considered in Section 2.3. In that case, let, as before,
        P
qM = E[ M i=1 Xi /M]. Then:
      causal         ∗,pop                −1  ∗,pop 
        θM               ΩXX,M Ω∗,popXZ 0 ,M           ΩXY,M
         causal    =      ∗,pop      ∗,pop              ∗,pop
        γM              ΩZX 0 ,M ΩZZ 0 ,M              ΩZY,M
                                  −1                        pop             
                        qM qM                           qM · Y M (1)
                   =                                pop                  pop
                        qM 1                qM · Y M (1) + (1 − qM ) · Y M (0)
                       pop            pop     
                        Y M (1) − Y M (0)
                   =             pop              .
                               Y M (0)
      causal       pop            pop
Thus θM      = Y M (1) − Y M (0), identical to the causal estimand considered in Section 2.3.

                                                       [17]
3.3    Population Residuals
We define the population residuals, denoted by εiM , to be the residual relative to the population
causal estimands,

                   0   causal    0   causal
      εiM = YiM − XiM θM      − ZiM γM      .

The definitions of these residuals mirrors that in conventional regression analyses, but their
properties are conceptually different. For example, the residuals need not be stochastic. Consider
the special case where YiM (x) = YiM (0) + x0θ, the potential causes XiM are randomly assigned,
                                                                         P
and there are no attributes beyond the intercept. Then εiM = YiM (0) − M   j=1 YjM (0)/M, which

is non-stochastic. In other cases the residuals may be stochastic. (To be clear, the residuals are
generally non-zero even though they are non-stochastic.)
   Under the assumptions we make, in particular the assumption that the XiM are jointly in-
dependent (but not necessarily identically distributed), the products XiM · εiM and ZiM · εiM ,
are jointly independent but not identically distributed. Most importantly, in general the expec-
tations EX [XiM · εiM ] and EX [ZiM · εiM ] may vary across i, although under the assumptions
stated below and the definition of the residuals, the averages of these expectations over the
population are guaranteed to be zero.

3.4    Assumptions
A key feature is that we now allow for more complicated assignment mechanisms. In particular,
we maintain the assumption that the XiM , for i = 1, . . . , M, are independent but we relax the
assumption that the distributions of the XiM are identical. For stating general results, where
the parameters are simply defined by the limits of the expected moment matrices, we do not
need to restrict the distributions of the XiM . However, in the case where the regression function
is correctly specified, for some purposes we restrict the distribution of XiM so that it depends
on the ZiM and not generally on the potential outcomes YiM (x). We also assume independence
between XiM and the sampling indicators, WiM .

Assumption 8. (Assignment Mechanism) The assignments X1M , . . . , XM M are indepen-
dent, but not (necessarily) identically distributed, or inid.

   Because of the independence assumption we can apply laws of large numbers and central


                                                [18]
limit theorems for inid (independent but not identically distributed) sequences. For the latter
we rely on sufficient conditions for the Liapunov Central Limit Theorem.
   To facilitate the asymptotic analysis we assume that the fourth moments of the triple
(YiM , ZiM , XiM ) are finite and uniformly bounded. We could relax this assumption at the cost of
complicating the proofs. If we assume the sampling frequency ρM is bounded below by ρ > 0 we
can get by with something less than uniformly bounded fourth moments, but here we want to
include ρM → 0 as a special case (leading to the EHW results) and keep the proofs transparent.

                                                                      k    l     m
Assumption 9. (Moments) For all M the expected value µk,l,m,M = EX [YiM · XiM · ZiM ] is
bounded by a common constant C for all nonnegative integers k, l, m such that k + l + m ≤ 4.

   For convenience, we assume that the population moment matrices converge to fixed values.
This is a technical simplification that could be relaxed, but relaxing it offers little in terms of
substance. We also make a full rank assumption.

Assumption 10. (Convergence of moments) The sequences YM , ZM and XM satisfy
                                     0                                                      
                       XM    YiM     YiM               ΩY Y                          ΩY Z 0 ΩY X 0
                     1
     Ωpop
      M       = EX         ZiM   ZiM    −→ Ω =  ΩZY                           ΩZZ 0 ΩZX 0  ,
                     M i=1
                             XiM     XiM               ΩXY                           ΩXZ 0 ΩXX 0

with Ω full rank.

   For future reference define
                         
             ΩXX ΩXZ 0
     Γ=                     .
            ΩZX 0 ΩZZ 0

Given Assumption 10 we can define the limiting population estimands
                            causal
                                                                −1                             
         θ∞                   θM                    ΩXX ΩXZ 0             ΩXY         −1       ΩXY
                  = lim        causal       =                                       =Γ                   .
         γ∞        M →∞       γM                    ΩZX 0 ΩZZ 0           ΩZY                  ΩZY

We maintain the random sampling assumption, Assumption 5. This implies that
       h                i
     EW Ωsample
         M      N, N > 0  = Ωpop
                             M .


In the proofs of the main results, we combine Assumptions 5 and 8, and use the fact that for all
population sizes M, {(XiM , WiM ) : i = 1, ..., M} is an inid sequence where WiM and XiM are
independent for all i = 1, ..., M, and all populations. We also maintain Assumption 3 concerning

                                                           [19]
the sampling rate, which guarantees that as the population size increases, the sample size N
also tends to infinity. Allowing ρM to converge to zero allows for the possibility that the sample
size is a neglible fraction of the population size: ρM = E[N]/M → 0, so the EHW results are
included as a special case of our general results. Technically we should write NM as the sample
size but we drop the M subscript for notational convenience.

3.5       The General Case
First we state a result regarding the common limiting values of the least squares estimators and
the causal and descriptive estimands:

Lemma 7. Suppose Assumptions 3, 5 and 8-10 hold. Then (i)
               
     θ̂ols − θ∞    p
                  −→ 0,
     γ̂ols − γ∞

(ii)
             descr
                           
             θM     − θ∞        p
              descr            −→ 0,
             γM − γ∞

and (iii)
      causal         
         θM      − θ∞
          causal        −→ 0.
         γM      − γ∞

       This result follows fairly directly from the assumptions about the moments and the sequence
of populations, although allowing for the limiting case ρM → 0 requires a little care in showing
consistency of the least squares estimators. Note that part (iii) is about deterministic conver-
gence and follows directly from Assumption 10 and the definition of the causal parameters.
       Next we study the limiting distribution of the least squares estimator. The key component
is the stochastic behavior of the normalized sample average of the product of the residuals and
the covariates,
              M                   
          1 X            XiM · εiM
         √       WiM ·               .                                                       (3.1)
           N i=1         ZiM · εiM

In our approach this normalized sum of independent but non-identically distributed terms has
expectation zero – something we verify below – even though each of the separate terms XiM · εiM
and ZiM · εiM may have non-zero expectations. To conclude that (3.1) has a limiting normal

                                                [20]
distribution we must apply a central limit theorem for independent double arrays. Here we use
the Liapunov central limit theorem as stated in Davidson (1994, Theorem 23.11).
       Define the limits of the population quantities
                                  M             !
                              1 X XiM · εiM
         ∆V = lim VX √                                ,                                                   (3.2)
                M →∞          M i=1    ZiM · εiM

                          M                           0 
                       1 X         XiM · εiM    XiM · εiM
         ∆ehw    = lim       EX                                                                           (3.3)
                  M →∞ M           ZiM · εiM    ZiM · εiM
                         i=1

and their difference
                                 M                               0
                              1 X         XiM · εiM          XiM · εiM
         ∆E = ∆ehw − ∆V = lim        EX                 EX                 .                              (3.4)
                         M →∞ M           ZiM · εiM          ZiM · εiM
                                i=1

Lemma 8. Suppose Assumptions 3, 5, and 8-10 hold. Then:
              M                   
          1 X            XiM · εiM    d
         √       WiM ·               −→ N (0, ρ · ∆V + (1 − ρ) · ∆ehw ).                                  (3.5)
           N i=1         ZiM · εiM

       The first part of the asymptotic variance, ρ · ∆V , captures the variation due to random
assignment of the treatment. This component vanishes if the sample is small relative to the
population. The second part, (1 − ρ) · ∆ehw , captures the variation due to random sampling.
This is equal to zero if we observe the entire population.
       Now we present the first of the two main results of the paper, describing the properties of the
least squares estimator viewed as an estimator of the causal estimand and, separately, viewed
as an estimator of the descriptive estimand:

Theorem 1. Suppose Assumptions 3, 5 and 8-10 hold. Then (i)
                                                          
    √              causal
          θ̂ols − θM         d     0     −1                 −1
      N            causal   −→ N      , Γ (∆ehw − ρ · ∆E ) Γ     ,
          γ̂ols − γM               0

(ii)
                                                                                             
         √                    descr
                     θ̂ols − θM           d            0                       −1            −1
             N                descr       −→ N                 , (1 − ρ) · Γ ∆ehw Γ                   ,
                     γ̂ols − γM                        0

and (iii)
                                                                                    
         √            descr
                     θM        causal
                            − θM              d            0              −1        −1
             N        descr    causal         −→ N                 , ρ · Γ ∆V Γ              .
                     γM     − γM                           0

                                                                   [21]
Proof: See Appendix.

   The standard EHW case is the special case in this theorem corresponding to ρ = 0. For
both the causal and the descriptive estimand the asymptotic variance in the case with ρ = 0
                                                                               causal      desc
reduces to Γ−1 ∆ehw Γ−1 . Moreover, the difference between the two estimands, θM      and θM    ,
normalized by the sample size, vanishes in this case. If the sample size is non-neglible as a
fraction of the population sizes, ρ > 0, the difference between the EHW variance and the finite
population causal variance is positive semi-definite, with the difference equal to ρ · Γ−1 ∆E Γ−1 .
This shows that the conventional robust sampling variance Γ−1 ∆ehw Γ−1 is appropriate if either
the sample size is small relative to population size, or if the expected values of XiM · εiM and
ZiM · εiM are close to zero for all i (and thus ∆E vanishes).

3.6    The Variance when the Regression Function is Correctly Speci-
       fied
In general the difference between the causal variance and the conventional robust EHW variance,
normalized by the sample size, is ρ · Γ−1 ∆E Γ−1 . Here we investigate when the component of this
                                               causal
difference corresponding to the causal effect θM      is equal to zero. The difference in variances
obviously vanishes if the sample size is small relative to the population size, ρ ≈ 0, but there is
another interesting case where only the difference between the two variances that corresponds
                      causal
to the estimator for θM      vanishes, without ∆E being equal to zero. This case arises when
the regression function, as a function of the potential cause XiM , is correctly specified. It is
important to be explicit here about what we mean by “correctly specified.” In the conventional
approach, with random sampling from a large population, the notion of a correct specification
is defined by reference to this large population. In that setting the linear specification is correct
if the population average of the outcome for each value of the covariates lies on a straight
line. Here we define the notion in the finite population where it need not be the case that
there are multiple units with the same values for the covariates XiM and ZiM so that the large
population definition does not apply. We make two specific assumptions. First, and this takes
account of the potential causes part of the specification, we restrict the values of the potential
outcomes. Second, and this takes account of the attributes part of the specification, we restrict
the distribution of the assignments XiM .



                                                [22]
Assumption 11. (Linearity of Potential Outcomes) The potential outcomes satisfy

      YiM (x) = YiM (0) + x0θ.

    Assumption 11 is not enough to conclude that the least squares estimator consistently esti-
mates the causal parameters θ. We must also restrict the way in which the causes, XiM , depend
on {(ZiM , YiM (0) : i = 1, 2, ..., M}, in an exogeneity or unconfoundedness-type assumption. To
this end, define the vector of slope coefficients from the population regression YiM (0) on ZiM ,
i = 1, 2, ..., M, as

                    M
                                     !−1          M
                                                                !
                 1 X        0                  1 X
      γM =             ZiM ZiM                       ZiM YiM (0) .                           (3.6)
                 M i=1                         M i=1

This vector γM is non-stochastic because it depends only on attributes and potential outcomes.

Assumption 12. (Orthgonality of Assignment) For all M,
      M
      X                                    h     i
                           0
             (YiM (0) −   ZiM γM )   · EX XiM = 0.
       i=1

    This assumption requires the mean of XiM to be orthogonal to the population residuals
           0
YiM (0) − ZiM γM , which measure the part of YiM (0) not explained by ZiM . A special case is that
where the XiM are independent and identically distributed (as, for example, in a completely
randomized experiment), so that E[XiM ] = µX , in which case Assumption 12 holds as long as
                                                                               PM
there is an intercept in ZiM because by the definition of γM , it follows that  i=1 (YiM (0) −
 0
ZiM γM ) = 0. More interesting is another special case where EX [XiM ] is a linear function of
ZiM , say EX [XiM ] = ΛM ZiM , i = 1, ..., M , for some matrix ΛM . It is easily seen that in that
case Assumption 12 holds because, by definition of γM ,
      M
      X
                             0
             ZiM (YiM (0) − ZiM γM ) = 0.
       i=1

In general, Assumption 12 allows XiM to be systematically related to ZiM , and even related to
YiM (0), provided the expected value of XiM is uncorrelated in the population with the residual
                            0
from regressing YiM (0) on ZiM . Notice that only the first moments of the XiM are restricted;
the rest of the distributions are unrestricted.



                                                        [23]
Definition 1. The regression function

             0       0
      YiM = XiM θ + ZiM γ + εiM ,

is correctly specified if Assumptions 11 and 12 hold.

                                                                          causal
   Now we can establish the relationship between the population estimand θM      and the slope
of the potential outcome function.

Theorem 2. Suppose Assumptions 8, 9, 11, and 12 hold. Then for all M,
     causal        
      θM            θ
       causal =
      γM           γM

   Given Assumptions 11 and 12 we can immediately apply the result from Theorem 1 with θ
            causal                                                 causal
instead of θM      , and we also have a simple interpretation for γM      .
   An implication of Assumptions 11 and 12 is that the population residual εiM is no longer
stochastic:

                      0       0   causal    0   causal
      εiM = Yi (0) + XiM θ − XiM θM      − ZiM γM
                          0
              = Yi (0) − ZiM γM ,

which does not involve the stochastic components XM or WM . This leads to simplifications
in the variance components. The Γ component of the variance remains unchanged, but under
Assumptions 11 and 12, ∆V simplifies, with only the top-left block different from zero:
                          PM 2                     
                limM →∞ M1    i=1 εiM · VX (XiM ) 0
     ∆V =                                             .                                  (3.7)
                                0                 0
                                                  √          
   In order to simplify the asymptotic variance of N θ̂ols − θ we add the linearity assumption
mentioned above.

Assumption 13. (Linearity of the Treatment In Attributes) For some K × J matrix
ΛM , and for i = 1, . . . , M,

      EX [XiM ] = ΛM ZiM .

    Recall that this assumption implies Assumption 12, and so we know least squares consistently
                                                                     √
estimates θ, and it has a limiting normal distribution when scaled by N . But with Assumption

                                                 [24]
                                                                                        causal
13 we can say more. Namely, the usual EHW variance is asymptotically valid for θ̂ols − θM
                                       causal
(but remains conservative for γ̂ols − γM      ). Define

      ẊiM = XiM − ΛM ZiM

Because under Assumptions 11 and 12 the residual εiM is non-stochastic it follows that
         h        i      h    i
     EX ẊiM · εiM = EX ẊiM · εiM = (EX [XiM ] − ΛM ZiM ) · εiM = 0,

by Assumption 13.
   Now define

                1 X
                   N    h          i
                                0
      ΓẊ = lim       EX ẊiM ẊiM   ,
           M →∞ M
                  i=1

and

                      1 X
                         N    h               i
      ∆ehw,Ẋ   = lim       EX ε2iM ẊiM ẊiM
                                           0
                                                .
                 M →∞ M
                        i=1

Theorem 3. Suppose Assumptions 8–13 hold. Then
    √           
                   d
                                         
      N θ̂ols − θ −→ N 0, Γ−1
                           Ẋ
                              ∆       Γ−1
                                ehw,Ẋ Ẋ   .

   The key insight in this theorem is that the asymptotic variance of θ̂ols does not depend on
the ratio of the sample to the population size, ρ. We also know from Theorem 1 that if ρ is close
to zero the proposed variance agrees with the EHW variance. Therefore it follows that the usual
EHW variance matrix is correct for θ̂ols under these assumptions, and it can be obtained, as in
standard asymptotic theory for least squares, by partially out ZiM from XiM in the population.
For this result it is not sufficient that the regression function is correctly specified (Assumptions
11 and 12); we have also assumed linearity of the potential cause in the attributes (Assumption
13). Nevertheless, no other features of the distribution of XiM are restricted.
   For the case with XiM binary and no attributes beyond the intercept this result can be
inferred directly from Neyman’s results for randomized experiments. In that case the focus is on
the constant treatment assumption, which is extended to the linearity in Assumption 11. In that
binary-treatment randomized-experiment case without attributes Assumptions 12 and 13 hold
trivially. Generally, if linearity holds and XiM is completely randomized then the conclusions of
Theorem 3 hold.

                                                 [25]
    The asymptotic variance of γ̂ols , the least squares estimates of the coefficients on the at-
tributes, still depends on the ratio of sample to population size, and the the conventional robust
EHW variance estimates over-estimates the uncertainty in these estimates.


4     Estimating the Variance
Now let us turn to the problem of estimating the variance for our descriptive and causal esti-
mands. This is a complicated issue. The variance in the conventional setting is easy to estimate.
One can consistently estimate Γ as the average of the matrix of outer products over the sample:
              M                  0
           1 X           ZiM    ZiM
      Γ̂ =       WiM ·                 .
           N i=1         XiM    XiM

Also ∆ehw is easy to estimate. First we estimate the residuals

                    0           0
      ε̂iM = YiM − XiM θ̂ols − ZiM γ̂ols ,

and then we estimate ∆ehw as:
                   M                                                          0
      ˆ ehw     1 X           XiM · ε̂iM − XiM · ε̂iM    XiM · ε̂iM − XiM · ε̂iM
      ∆       =       WiM ·                                                         ,            (4.1)
                N i=1         ZiM · ε̂iM − ZiM · ε̂iM    ZiM · ε̂iM − ZiM · ε̂iM

where
                        M                                                M
                     1 X                                              1 X
      XiM · ε̂iM   =       WiM · XiM · ε̂iM ,      and ZiM · ε̂iM   =       WiM · ZiM · ε̂iM .
                     N i=1                                            N i=1

In this case we do not need to subtract the averages, which in fact will be equal to zero, but
this form is useful for subsequent variance estimators. The variance is then estimated as

                   ˆ ehw Γ−1 .
      V̂ehw = Γ̂−1 ∆                                                                             (4.2)

Alternatively one can use resampling methods such as the bootstrap (e.g., Efron, 1987).
    If we are interested in the descriptive estimand it is straightforward to modify the variance
estimator. We simply multiply the ’ehww variance estimator by one minus the ratio of the
sample size over the population size.
                                                                causal
    It is more challenging to estimate the variance of θ̂ols − θM      . The difficult is in estimating
∆V (or, equivalently, ∆E = ∆ehw − ∆V ). The reason is the same that makes it impossible to

                                                 [26]
obtain unbiased estimates of the variance of the estimator for the average treatment effect in the
example in Section 2.3. In that case there are three terms in the expression for Vnorm
                                                                                  causal presented

in (2.3). The first two are straightforward to estimate, but the third one, σ 2(low, high) cannot
be estimated consistently because we do not observe both potential outcomes for the same units.
In that case researchers often use the conservative estimator based on ignoring that term. Here
we can do the same. Because
                M             !      "   M                       0 #
            1 X XiM · εiM                1 X XiM · εiM      XiM · εiM
     VX √                         ≤ EX                                     ,
             M       ZiM · εiM           M     ZiM · εiM    ZiM · εiM
                    i=1                                    i=1

it follows that
                                  M            !
                               1 X XiM · εiM
      ∆V = lim VX             √
             M →∞              M i=1  ZiM · εiM
                                     "       M                       0 #
                                          1 X XiM · εiM       XiM · εiM
                    ≤ lim EX                                                 = ∆ehw ,
                        M →∞              M i=1  ZiM · εiM    ZiM · εiM

and we can use the estimator in (4.2) as the basis for a conservative estimator for the variance,
     ˆ ehw Γ̂−1 . However, we can do better. Instead of using the average of the outer product,
Γ̂−1 ∆
ˆ ehw, as an upwardly biased estimator for ∆V , we can remove part of the expected value.
∆
   Suppose we split the population into S strata, on the basis of the values of the non-stochastic
variables YiM (x) and ZiM . Let SiM ∈ {1, . . . , S}, for i = 1, . . . , M, M = 1, 2, . . ., be the
indicator for the subpopulations, and let Ms be the stratum-specific population size. Then, by
independence of the XiM (and thus independence of the εiM ), it follows that
                 M             ! X   S                     X  XiM · εiM 
                                                                             !
            1 X XiM · εiM                 Ms          1
     VX √                           =        · VX √                            .
            M i=1     ZiM · εiM           M           M s i:S =s ZiM · εiM
                                      s=1                              iM


Now we can obtain a conservative estimator of ∆V by averaging within-stratum estimates after
taking out with within-stratum averages:
                  S
                  X
      ˆ strat =     Ms        ˆ ehw,s ,
      ∆                      ·∆
                  s=1
                        M

where
                                                    !
                              1    X  XiM · εiM 
      ˆ ehw,s = V̂X
      ∆                     √                         =
                             M s i:SiM =s ZiM · εiM

                                                        [27]
                                                                               0 !
               1 X         XiM · ε̂iM − XiM · ε̂iM s    XiM · ε̂iM − XiM · ε̂iM s
                                                                                       ,
               Ms i:S =s   ZiM · ε̂iM − ZiM · ε̂iM s    ZiM · ε̂iM − ZiM · ε̂iM s
                       i

with
                           1 X                                              1 X
        XiM · ε̂iM s =               XiM · ε̂iM ,      and ZiM · ε̂iM s =             ZiM · ε̂iM .
                           Ms i:S =s                                        Ms i:S =s
                                 iM                                               iM


Assumption 14. For s = 1, . . . , S, Ms /M → δs > 0.

Lemma 9. Suppose Assumptions 8-14 hold. Then

        ∆V ≤ ∆strat ≤ ∆ehw ,

where

                      ˆ strat).
        ∆strat = plim(∆

    The proposed estimator for the normalized variance is then

                       ˆ stratΓ̂−1 .
        V̂strat = Γ̂−1 ∆                                                                             (4.3)

A natural way to define the strata is in terms of values of the attributes ZiM . If the attributes
are discrete we can simply stratify by their exact values. If the ZiM take on many values we can
partition the attribute space into a finite number of subspaces.
    If one is willing to make the additional assumption that the potential outcome function is
correctly specified, then we can make additional progress in estimating ∆V . In that case only
the top-left block of the ∆V matrix differs from zero, as we discussed in Section 3.6. In addition,
this assumption implies that the residuals εiM are non-stochastic, and so they can be used in
partitioning the population.


5       Simulations
Here we present some evidence on the difference between the conventional ehw variance and the
variance for causal effects. We focus on the case with a single binary cause Xi ∈ {−1, 1} and a
single binary attribute Zi ∈ {−1, 1}. The potential outcome function has the form

        Yi (x) = Yi (0) + (τ0 + τ1 · Zi + τ2 · ηi ) · x,

                                                       [28]
where τ1 and τ2 are parameters we vary across simulations. The ηi ∈ {−1, 1} is an unobserved
source of heterogeneity in the treatment effect. If τ1 = τ2 = 0, the regression function is correctly
specified and because the attribute is binary the linearity condition is also satisfied, and thus
the conventional ehw variance will be valid. If either τ1 or τ2 differs from zero, the conventional
variance estimator will be over-estimating the variance. If τ1 is different from zero the variance
estimator based on partitioning the sample by values of Zi will be an improvement over the
conventional variance estimator.
    In the population the Zi and ηi are uncorrelated, and satisfy
          M               M                                M               M
       1 X             1 X                              1 X             1 X
             1ηi =−1 =      1η =1 = 1/2,          and         1Zi =−1 =      1Z =1 = 1/2.
       M i=1           M i=1 i                          M i=1           M i=1 i

Moreover,
          M                       M
       1 X                     1 X 2
             Yi (0) = 0,            Y (0) = 1,
       M i=1                   M i=1 i

          M                   M                   M
       1 X                 1 X                 1 X
             ηi · Yi (0) =       Zi · Yi (0) =       Zi · ηi = 0,
       M i=1               M i=1               M i=1
and

      pr(Xi = 1) = pr(Xi = −1) = 1/2.

The Yi (0) were generated by first drawing νi from a normal distribution with mean zero and
variance equal to one, and then calculating Yi (0) as the residual from regressing νi on ηi , Zi ,
and ηi · Zi .
    We estimate a linear regression model using least squares:

      Yi = γ0 + γ1 · Zi + θ · Xi + εi .

We focus on the properties of the least squares estimator for θ.
    We consider nine designs. In these designs we consider three sets of values for the pair
(τ1, τ2 ), namely (τ1 = 0, τ2 = 0), (τ1 = 0, τ2 = 10), and (τ1 = 10, τ2 = 0). In all cases τ0 = 0.
The expected sample size ρ · M is in all cases equal to 1,000, but the value of ρ takes on different
values, ρ ∈ {0.01, 0.5, 1}, so the population size is M = 100, 000, M = 2, 000, or M = 1, 000 in
the three different designs. In each of the nine designs we start by constructing a population,

                                                [29]
as described above. Given the population we then repeatedly draw samples in the following
two steps. We first randomly assign the covariates according to the binomial distribution with
probability 1/2 for the two values x = −1, 1. Finally, we randomly sample units from this
population, where each unit is sampled with probability ρ. For each unit in the sample we
observe the triple (Yi , Xi , Zi ).

                            Table 1: Simulation Results, M · ρ = 1000


                       τ1 = 0, τ2 = 0      τ1 = 0, τ2 = 10      τ1 = 10, τ2 = 0
                   ρ = .01 ρ = .5 ρ = 1 ρ = .01 ρ = .5 ρ = 1 ρ = .01 ρ = .5 ρ = 1



    std(θcausal)    0.032      0.032   0.030    0.32      0.23    0.037     0.32      0.23    0.035

    std(θdesc)      0.031      0.023   0.000    0.32      0.23    0.000     0.32      0.22    0.000

    seehw           0.032      0.032   0.032    0.32      0.32    0.318     0.32      0.32    0.318

    secausal        0.032      0.032   0.032    0.32      0.23    0.032     0.32      0.23    0.032

    sedesc          0.032      0.022   0.000    0.32      0.22    0.000     0.32      0.22    0.000

    se
    b strat         0.032      0.032   0.030    0.32      0.23    0.041     0.32      0.32    0.318




    The results from the simulations are presented in Table 1. In the first two rows we present
for each of the nine designs the standard deviation of the least squares estimator θ̂ols as an
estimator for θcausal and as an estimator for θdesc. If ρ = 0.01 the two standard deviations are
very similar, irrespective of the values of τ1 and τ2 . If ρ = 1, the standard deviation of θ̂ols − θdesc
is zero, whereas the standard deviation of θ̂ols − θcausal remains positive. If ρ = 0.5, the ratio
of the variances depends on the other parameters of the design. The next three rows present
the results of analytic calculations for the three variances, first the conventional ehw variance,
then the variance for the causal estimand and finally the variance for the descriptive estimand.
The latter two closely match the standard deviation of the estimator over the repeated samples,
confirming that the theoretical calculations provide guidance for the sample sizes considered

                                                  [30]
here. Finally, in the last row we partition the sample by the values of Zi , and use the variance
estimator in (4.3). We see that in the case with τ1 = τ2 = 0 the properties of the proposed
variance estimator are very similar to those of the conventional ehw estimator. If τ1 > 0 and
τ2 = 0, so the variation in the coefficient on Xi is associated with the observed attribute Zi ,
then the proposed variance estimator outperforms the conventional EHW estimator. If τ1 = 0
and τ2 > 0, so the variation in the coefficient on Xi is associated with the unobserved attributes,
then the performance of the proposed variance estimator is similar to that of the conventional
EHW estimator.


6     Inference for Alternative Questions
This paper has focused on inference for descriptive and causal estimands in a single cross-section.
For example, we might have a sample that includes outcomes from all countries in a particular
year, say the year 2013. In words, we analyze inference for estimands of parameters that answer
the following question: “What is the difference between what the average outcome would have
been in those countries in the year 2013 if all had been treated, and what the average outcome
would have been if all had not been treated?” We also analyze inference for estimands of
parameters that can be used to answer descriptive questions, such as “What was the difference
in outcomes between Northern and Southern countries in the year 2013?”
    These are not the only questions a researcher could focus on. An alternative question might
be, “what is the expected difference in average outcomes between Northern and Southern coun-
tries in a future year, say the year 2015?” Arguably in most empirical analyses that are intended
to inform policy the object of interest depends on future, not just past, outcomes. This creates
substantial problems for inference. Here we discuss some of the complications, but our main
point is that the conventional robust standard errors were not designed to solve these problems,
and do not do so without strong, typically implausible assumptions. Formally questions that
involve future values of outcomes for countries could be formulated in terms of a population of
interest that includes each country in a variety of different states of the world that might be
realized in future years. This population is large if there are many possible realizations of states
of the world (e.g., rainfall, local political conditions, natural resource discoveries, etc.) Given
such a population the researcher may wish to estimate, say the difference in average 2015 out-
comes for two sets of countries, and calculate standard errors based on values for the outcomes


                                                [31]
for the same set of countries in an earlier year, say 2013. A natural estimator for the differ-
ence in average values for Northern and Southern countries in 2015 would be the corresponding
difference in average values in 2013. However, even though such data would allow us to infer
without uncertainty the difference in average outcomes for Northern and Southern countries in
2013, there would be uncertainty regarding the true value of that difference in the year 2015.
    In order to construct confidence intervals for the difference in 2015, the researcher must
make some assumptions about how country outcomes will vary from year to year. An extreme
assumption is that outcomes in 2015 and 2013 for the same country are independent conditional
on attributes, which would justify the conventional EHW variance estimator. However, assuming
that there is no correlation between outcomes for the same country in successive years appears
highly implausible. In fact any assumption about the magnitude of this correlation in the absence
of direct information about it in the form of panel data would appear to be controversial. Such
assumptions would also depend heavily on the future year for which we would wish to estimate
the difference in averages, again highlighting the importance of being precise about the estimand.
    Although in this case there is uncertainty regarding the difference in average outcomes in
2015 despite the fact that the researchers observes (some) information on all countries in the
population of interest, we emphasize that the assumptions required to validate the application of
EHW standard errors in this setting are strong and arguably implausible. Moreoever, researchers
rarely formally state the population of interest, let alone state and justify the assumptions that
justify inference. Generally, if future predictions are truly the primary question of interest, it
seems prudent to explicitly state the assumptions that justify particular calculations for standard
errors. In the absence of panel data the results are likely to be sensitive to such assumptions.
We leave this direction for future work.


7     Conclusion
In this paper we study the interpretation of standard errors in regression analysis when the
assumption that the sample is a random sample from a large population of interest is not at-
tractive. The conventional robust standard errors justified by this assumption do not necessarily
apply in this case. We show that by viewing covariates as potential causes in a Rubin Causal
Model or potential outcome framework we can provide a coherent interpretation for standard
errors that allows for uncertainty coming from both random sampling and from conditional


                                               [32]
random assignment. The proposed standard errors may be different from the conventional ones
under this approach.
   In the current paper we focus exclusively on regression models, and we provide a full analysis
of inference for only a certain class of regression models with some of the covariates causal and
some attributes. Thus, this paper is only a first step in a broader research program. The concerns
we have raised in this paper arise in many other settings and for other kinds of hypotheses, and
the implications would need to be worked out for those settings. Section 6 suggests some
directions we think are particularly natural to consider.




                                               [33]
                                          References

Abadie, A., A. Diamond, and J. Hainmueller, (2010), “Synthetic Control Methods for Com-
    parative Case Studies: Estimating the Effect of California’s Tobacco Control Program,”Journal
    of the American Statistical Association, Vol. 105(490), 493-505.

Abadie, A., G. Imbens, and F. Zheng, (2012), “Robust Inference for Misspecified Models Condi-
    tional on Covariates,” NBER Working Paper.

Angrist, J., and S. Pischke, (2009), Mostly Harmless Econometrics, Princeton University Press,
    Princeton, NJ.

Barrios, T., R. Diamond, G. Imbens, and M. Kolesar, (2012), “Clustering, Spatial Corre-
    lations, and Randomization Inference,” Journal of the American Statistical Association, Vol.
    107(498): 578-591.

Bertrand, M., E. Duflo, and S. Mullainathan, (2004), “How Much Should We Trust Difference-
    In-Differences Estimates,” Quarterly Journal of Economics, Vol. (119): 249-275.

Cattaneo, M., B. Frandsen, and R. Titiunik, (2013), “Randomization Inference in the Regres-
    sion Discontinuity Design: An Application to the Study of Party Advantages in the U.S. Senate,”
    Unpublished Working Paper.

Chow, G., (1984), “Maximum-likelihood estimation of misspecified models,” Economic Modelling,
    Vol. 1(2): 134-138.

Cochran, W. (1969), “The Use of Covariance in Observational Studies,” Journal of the Royal
    Statistical Society. Series C (Applied Statistics), Vol. 18(3): 270–275.

Cochran, W., (1977), Sampling Techniques. Wiley: New York.

Davidson, J., (1994), Stochastic Limit Theory: An Introduction for Econometricicans, Oxford Uni-
    versity Press.

Deaton, A., (1997), The Analysis of Household Surveys: A Microeconometric Approach to Develop-
    ment Policy. World Bank Publications.




                                                [34]
Efron, B., (1987), The Jackknife, the Bootstrap, and Other Resampling Plans, CBMS-NSF Regional
    Conference Series in Applied Mathematics.

Eicker, F., (1967), “Limit Theorems for Regression with Unequal and Dependent Errors,” Proceed-
    ings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability, Vol. 1, 59-82,
    University of California Press, Berkeley.

Fisher, R. A., (1935), The Design of Experiments, 1st ed, Oliver and Boyd, London.

Frandsen, B., (2012), “Exact inference for a weak instrument, a small sample, or extreme quantiles,”
    Unpublished Working Paper.

Freedman, D., (2008a), “On Regression Adjustments in Experiments with Several Treatments,”
    The Annals of Applied Statistics, Vol. 2(1): 176–196.

Freedman, D., (2008b), “On Regression Adjustments to Experimental Data,” Advances in Applied
    Mathematics, Vol. 40: 181–193.

Gelman, A., and J. Hill, (2007), Data Analysis Using Regression and Multilevel/Hierarchical
    Models, Cambridge University Press

Hayashi, F., (2000), Econometrics, Princeton University Press.

Holland, P., (1986), “Statistics and Causal Inference,” (with discussion), Journal of the American
    Statistical Association, 81, 945-970.

Huber, P., (1967), “The Behavior of Maximum Likelihood Estimates Under Nonstandard Condi-
    tions,” Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability,
    Vol. 1, 221-233, University of California Press, Berkeley.

Imbens, G., and P. Rosenbaum, (2005), “Robust, accurate confidence intervals with a weak in-
    strument: quarter of birth and education,” Journal of the Royal Statistical Society, Series A
    (Theoretical Statistics), 168: 109-126.

Imbens, G.W., and J.M. Wooldridge, (2009), “Recent Developments in the Econometrics of
    Program Evaluation,” Journal of Economic Literature 47 (1), 5-86.

Kish, L., (1995), Survey Sampling, Wiley.

                                                [35]
Lin, W., (2013), “Agnostic Notes on Regression Adjustments for Experimental Data: Reexamining
    Freedman’s Critique,” The Annals of Applied Statistics, Vol. 7:(1): 295–318.

Neyman, J., (1923), “On the Application of Probability Theory to Agricultural Experiments. Essay
    on Principles. Section 9,” translated in Statistical Science (with discussion), Vol 5, No 4, 465–480,
    1990.

Rosenbaum, P., (1995), Observational Studies. Springer Verlag: New York.

Rosenbaum, P., (2002), “Covariance Adjustment in Randomized Experiments and Observational
    Studies,” Statistical Science, Vol. 17:(3): 286–304.

Rubin, D. (1974), ”Estimating Causal Effects of Treatments in Randomized and Non-randomized
    Studies,” Journal of Educational Psychology, 66, 688-701.

Samii, C., and P. Aronow, (2012), “On equivalencies between design-based and regression-based
    variance estimators for randomized experiments” Statistics and Probability Letters Vol. 82: 365–
    370.

Schochet, P., (2010), “Is Regression Adjustment Supported by the Neyman Model for Causal
    Inference?” Journal of Statistical Planning and Inference, Vol. 140: 246–259.

White, H., (1980a), “Using Least Squares to Approximate Unknown Regression Functions,” Inter-
    national Economic Review, Vol. 21(1):149-170.

White, H. (1980b), “A Heteroskedasticity-Consistent Covariance Matrix Estimator and a Direct
    Test for Heteroskedasticity,” Econometrica, 48, 817-838.

White, H., (1982), “Maximum likelihood estimation of misspecified models,” Econometrica, Vol
    50(1): l-25.




                                                 [36]
                                          Appendix A: Proofs
                                            PM
Proof of Lemma 1: Conditional on N =           i=1 Wi   the vector W has a multinomial distribution with
                     −1                  N
                                           X
                     M
     pr(W = w|N ) =       , for all w with   wj = N
                     N
                                                     j=1
                        = 0, otherwise.

The expected value, variance and covariance of individual elements of W are given by
                      N
          E[Wj |N ] =
                      M
                                 
                       N         N      N · (M − N )
        V(Wj |N ) =         · 1−      =
                       M         M           M2
                       N · (M − N )
     C(Wj , Wh |N ) = − 2
                       M · (M − 1)
Now consider the sample average
               M
             1 X
     µ̂M =       Wj · Yj .
             N
               j=1

For notational simplicity we leave conditioning on N > 0 implicit. Then
                    M                    M              M
                  1 X                  1 X N            1 X
     E[µ̂M |N ] =     E[Wj |N ] · Yj =           · Yj =     Yj = µM .
                  N                    N     M          M
                      j=1                    j=1                  j=1

The sampling variance of µ̂M is can be obtained by writing µ̂M = W 0 Y /N so that
                     1 0
     V(µ̂M |N ) =       Y V(W |N )Y .
                     N2
From the conditional second moments of W it follows that
                                                                      
                                           M −1     −1   −1        −1
                                        −1     M −1       −1    −1 
                        N · (M − N )     
                                        0
                                                                       
       0
     Y V(W |N )Y =                     Y  −1                          Y
                          2
                        M · (M − 1)                                    
                                                        −1 M − 1  −1 
                                            −1           −1  −1   M −1

Straightforward algebra shows that
                                         
           M −1      −1    −1         −1                           2
          −1      M −1           −1  −1         M           M               M
                                               X           X               X
      Y 0
           −1                            Y = M
                                                    Yj
                                                       2
                                                         −      Yj   = M ·     (Yj − µM )2 ,
                          −1 M − 1   −1        j=1         j=1             i=1
            −1             −1     −1 M −1



                                                   [37]
so that
                                                                 
                                                 M
                                                 X
                         N · (M − N )  1                                N (M − N ) 2
      Y 0 V(W |N )Y =                                  (Yj − µM )2  =             σM .
                              M        M −1                                  M
                                                 j=1

Therefore,
                                             2       
                         1 N · (M − N ) 2     σM      N
      V(µ̂M |N, N > 0) = 2             · σM =    · 1−     .
                        N       M             N       M



Note that this result generalizes to any set of constants {cj }j=1,...,M , so that,
                                                   M
                                     N · (M − N ) X
      V(W 0 c|N ) = c0 V(W |N )c =               ·   (cj − cM )2 ,
                                     M · (M − 1)
                                                        j=1

             PM
where cM =      i=1 ci /M .


Before proving Lemma 2, we state a useful result.

Lemma A.1. Suppose Assumptions 1 and 3 hold. Then:
       N      p       M · ρM p
             −→ 1 and       −→ 1              as M −→ ∞.
      M · ρM           N

Proof: Under Assumption 1, for any positive integer M , N ∼ Binomial(M, ρM ), which implies
EW [N ] = M · ρM and VW (N ) = M · ρM · (1 − ρM ). Therefore,
                                             
              N                            N         M · ρM · (1 − ρM )   (1 − ρM )
     EW              =1     and VW                 =        2   2       =           ,
            M · ρM                      M · ρM            M · ρM           M · ρM

which converges to zero by Assumption 3. Therefore convergence in probability follows from conver-
gence in mean square. The second part follows from Slutsky’s Theorem because the reciprocal function
is continuous at all nonzero values. 

Proof of Lemma 2: From Lemma 1,
                           2
                                   
                          σM      N
     VW (µ̂M |N, N > 0) =    · 1−     ,
                          N       M

and so
                                                                                 
                              σ2  σ 2 − σ 2 σM
                                             2
                                                σ 2 − σ 2 σM
                                                           2
                                                              σ2 − σ2      ρM · M
      VW (µ̂M |N, N > 0) −       = M       −   = M       −   − M      · 1−          .
                              N       N      M   ρM · M    M   ρM · M        N
                        2
Given Assumption 2, σM    → σ 2 as M → ∞, and therefore {σM      2
                                                                   } is bounded.          It follows that
                      2               −1
VW (µ̂M |N, N > 0) − σ /N = Op((ρM M ) ), finishing the proof of part (i).



                                                       [38]
The normalized variance is
                                                                                       
                                                                                  N
      Vnorm
       W (µ̂M |N )              = N · VW (µ̂M |N ) =                 2
                                                                    σM         1−
                                                                                  M
                  2 → σ 2 . By Assumption 1, N ∼ Binomial(M, ρ ) and so E(N/M ) = ρ
By Assumption 2, σM                                           M                     M and
        
        N       ρM (1 − ρM )
     V       =               → 0,
        M            M
                                                        p                                                           p
which means that N/M − ρM → 0. Along with Assumption 3 (ρM → ρ) we get Vnorm
                                                                        W (µ̂M |N ) →
 2
σ (1 − ρ).

Proof of Lemma 3: Assumption 1 ensures that the vector of sampling indicators over the two
subpopulations, of size Mcoast and M∧ are independent. Further, conditional on Ncoast and N∧ , they
have the multinomial distribution described in the proof of Lemma 1. The result follows immediately
because the covariance between the two sample means, conditional on (Ncoast, N∧ ) and Ncoast > 0 and
N∧ > 0, is zero. 
                                                          descr as
Proof of Lemma 4: Conditional on Mlow , Mhigh > 0, write θM

                                    M
                                    X                                  M
       descr              1                                        1 X
      θM         =                        1Xi =high · Yi (high) −        1Xi =low · Yi (low)
                     Mhigh                                        Mlow
                                    i=1                                             i=1

Conditional on Mhigh (and therefore conditional on both Mhigh and Mlow ), E[1Xi =high |Mhigh ] = pr(Xi =
high|Mhigh ) = Mhigh /M , and so

          h                                 i           1
                                                              M
                                                              X Mhigh
                                                                                                  M
                                                                                              1 X Mlow
               descr
      E       θM     |Mhigh , Mlow              =                              · Yi (high) −           · Yi (low)
                                                     Mhigh             M                     Mlow   M
                                                              i=1                                    i=1


                     1 X                    
                       M
                                                 causal
                 =       Yi (high) − Yi (low) = θM      .
                     M
                              i=1
                            descr
To compute the variance of θM     , write
                     M
                     X                                                                M
                                                                                        X
       descr                                        Yi (high) Yi (low)                    Yi (low)
      θM         =            1Xi =high ·                    +                      −
                                                     Mhigh     Mlow                           Mlow
                     i=1                                                                i=1

Conditional on Mlow , Mhigh > 0, the calculation is very similar to that in Lemma 1. In fact, take

                Yi (high) Yi (low)
      ci =               +         ,
                 Mhigh     Mlow

and then Lemma 1 implies
               M                                                                        !
               X                Yi (high)                   Yi (low)
      V              Xi                             +                       Mlow , Mhigh
                                 Mhigh                       Mlow
               i=1


                                                                                [39]
                         "                                #
              Mlow Mhigh    1                 1
            =               2
                                 σ 2 (high) + 2 σ 2 (low)
                 M         Mhigh             Mlow

                                    X M                                       
                              2
                      +               Yi (high) − Ȳ (high) · Yi (low) − Ȳ (low) .
                          M (M − 1)
                                     i=1
Now
                                   M
       2                1 X
      σ (low, high) =       [(Yi (high) − Ȳ (high)) − (Yi (low) − Ȳ (low))]2
                      M −1
                                   i=1

                                                        M
                                                        X
            = σ 2 (high) + σ 2 (low) − 2(M − 1)−1         [Yi (high) − Ȳ (high)][Yi (low) − Ȳ (low)].
                                                        i=1
or
                      M
                      X
      2(M − 1)   −1
                        [Yi (high) − Ȳ (high)][Yi (low) − Ȳ (low)] = σ 2 (high) + σ 2 (low) − σ 2 (low, high).
                      i=1

Substituting gives

                                             "                                                                                  #
                                Mlow Mhigh    1                     1             [σ 2
                                                                                         (high) +  σ 2
                                                                                                       (low) − σ 2
                                                                                                                   (low, high)]
   descr
V θM     |Mlow , Mhigh   =                       2
                                                     σ 2 (high) + 2 σ 2 (low) +
                                     M         Mhigh                Mlow                            Mlow Mhigh
                                             "                                                                   #
                                  Mlow Mhigh                                                       2
                                                   M                        M                    σ   (low, high)
                              =                        2
                                                            σ 2 (high) +         2
                                                                                    σ 2 (low) −
                                     M         Mlow Mhigh                Mhigh Mlow               Mlow Mhigh
                                  σ 2 (high) σ 2 (low) σ 2 (low, high)
                              =             +         −                .
                                    Mhigh     Mlow            M



Proof of Lemma 5: We prove parts (i) and (ii), as the other parts are similar (and (v) follows
immediately). First, because X and W are independent, we have

      D(X|W , Nhigh, Nlow) = D(X|Nhigh, Nlow)

and the distribution is multinomial with

              E[1Xi =high |Nhigh, Nlow] = E[1Xi =high |Nhigh, N ) = Nhigh/N
                                            NhighNlow
                V(1Xi=high |Nhigh, N ) =
                                               N2
                                             Nhigh Nlow
      C(1Xi =high , 1Xh=high |Nhigh, N ) = − 2
                                             N (N − 1)
                                            Nhigh(Nhigh − 1)
      E[1Xi =high · 1Xh =high |Nhigh, N ] =
                                               N (N − 1)



                                                         [40]
Note that
                                                                                                 2
                                                                               (N − 1)Nhigh − Nhigh  + Nhigh
                                                     Nhigh Nhigh (Nhigh − 1)
      E[1Xi =high · 1Xh =low |Nhigh, N ] =                 −                 =
                                                      N         N (N − 1)                N (N − 1)
                                                                2
                                                     N Nhigh − Nhigh   Nhigh(N − Nhigh)   Nhigh Nlow
                                               =                     =                  =
                                                       N (N − 1)          N (N − 1)       N (N − 1)
Now
                                                   M
                                                   X                                       M
                                                                                           X
                                         −1                                           −1
                             θ̂M    =   Nhigh            Wi 1Xi =high Yi (high)   − Nlow         Wi 1 − Xi = lowYi (low)
                                                   i=1                                     i=1
                                                   XM
                                           1
      E(θ̂M |W , Nhigh, Nlow ) =                         Wi pr(Xi = high|Nhigh, Nlow )Yi (high)
                                         Nhigh
                                                   i=1
                                                          M
                                                      1 X
                                                   −        Wi pr(Xi = low|Nhigh, Nlow )]Yi(low)
                                                     Nlow
                                                             i=1
                                                   M
                                                   X                                        M
                                                                                            X
                                       −1                                          −1
                                    = Nhigh              Wi (Nhigh/N )Yi(high) − Nlow              Wi (Nlow /N )Yi (low)
                                                   i=1                                       i=1
                                               M
                                               X
                                    = N −1           Wi [Yi (high) − Yi (low)]
                                               i=1

and so
                                                                                                 M
                                                                                                 X
                              −1                                                           −1
E(θ̂M |Nhigh, Nlow) = N            E(Wi |Nhigh, Nlow)[Yi (high) − Yi (low)] = N                    (N/M )[Yi (high) − Yi (low)]
                                                                                                 i=1
                                                      causal
                      = µM high − µM low =           θM      ,
which proves part (i).
For part (ii) we find

      V(θ̂M |Nhigh, Nlow) = V(Ȳhigh |Nhigh, Nlow) + V(Ȳlow |Nhigh, Nlow ) − 2C(Ȳhigh, Ȳlow |Nhigh, Nlow).

If we define Zi = Wi ·1Xi=high and Ri = Wi ·1Xi=low then we can apply Lemma 4 to obtain the variances
because

      (Z1 , ..., ZM )|(Nlow, Nhigh)

has a multinomial distribution with pr(Zi = 1|Nlow , Nhigh) = Nhigh/M and (R1 , ..., RM )|(Nlow, Nhigh)
has the distribution with P (Ri = 1|Nlow, Nhigh) = Nlow /N . Therefore,
                                                      
                                σ 2 (high)       Nhigh     σ 2 (high) σ 2 (high)
      V(Ȳhigh |Nhigh, Nlow) =               1−          =           −
                                  Nhigh           M          Nhigh         M
                                  2           2
                                σ (low) σ (low)
       V(Ȳlow |Nhigh, Nlow) =            −
                                  Nlow          M
and so
                             σ 2 (high) σ 2 (low) σ 2 (high) + σ 2 (low)
      V(θ̂|Nhigh, Nlow ) =             +         −                       − 2C(Ȳhigh, Ȳlow |Nhigh, Nlow)
                               Nhigh      Nlow              M

                                                                 [41]
We showed in 4 that
                                                               X   M
                                                          2
      σ 2 (high) + σ 2 (low) = σlow,high
                                2
                                         +                       [Yi (high) − µhigh ][Yi (low) − µlow ]
                                                       (M − 1)
                                                                  i=1
                                    2
                              ≡    σlow,high      + 2ηlow,high

where ηlow,high is the population covariance of Yi (low) and Yi (high). So
                                                   2
                             σ 2 (high) σ 2 (low) σlow,high    hη                                           i
                                                                  low,high
      V(θ̂|Nhigh, Nlow ) =             +         −          −2             + C(Ȳhigh, Ȳlow |Nhigh, Nlow )
                               Nhigh      Nlow       M              M

The proof is complete if we show
                                            ηlow,high
      C(Ȳhigh , Ȳlow |Nhigh, Nlow) = −
                                               M
The usual algebra of covariances gives

                            X     M
      ηlow,high       1                           µhigh µlow
                =             Yi (high)Yi (low) −
         M        M (M − 1)                       (M − 1)
                                  i=1

and so it suffices to show

                                                                                 X        M
                                                          µhigh µlow       1
      E(Ȳhigh Ȳlow |Nhigh, Nlow ) − µhigh µlow =                   −             Yi (high)Yi (low)
                                                          (M − 1)      M (M − 1)
                                                                                          i=1

or
                                                                 P
                                            M µhigh µlow − M −1 M   i=1 Yi (high)Yi (low)
      E(Ȳhigh Ȳlow |Nhigh, Nlow ) =
                                                               (M − 1)
                                            P                 P               P                          
                                               M                   M                    M
                                               i=1 Y i (high)      i=1 Yi (low)  −      i=1 Yi (high)Yi (low)
                                      =
                                                                             M (M − 1)
                                            PM PM
                                                 i=1     h6=i+1   Yi (high)Yh (low)
                                      =                                               .
                                                          M (M − 1)
To show this equivalance, write
                                        M
                                                                         !   M
                                                                                                          !
                           1            X                                    X
      Ȳhigh Ȳlow =                            Wi 1Xi =high Yi (high)             Wh 1Xi =low Yh (low)
                       NhighNlow
                                          i=1                                h=1
                                                                                     
                                       XM X
                                          M
                           1          
                  =                         Wi 1Xi =high Yi (high)Wh 1Xh =low Yh (low)
                       NhighNlow
                                          i=1 h6=i




                                                              [42]
First condition on the sampling indicators W as well as (Nhigh, Nlow):
                                             
                                              X M X M
                                       1     
E(Ȳhigh Ȳlow |W , Nhigh, Nlow) =                      Wi Wh pr(Xi = high, Xh = low|W , Nhigh, Nlow)Yi (high)Yh (low)
                                   NhighNlow
                                               i=1 h6=i
                                                                                                                  
                                              X M X M
                                       1     
                                 =                      Wi Wh pr(Xi = high, Xh = low|Nhigh, Nlow )Yi (high)Yh (low)
                                   NhighNlow
                                               i=1 h6=i
                                                                                                   
                                              X M X M
                                       1     
                                 =                      Wi Wh [NhighNlow /N (N − 1)]Yi(high)Yh (low) =
                                   NhighNlow
                                                   i=1 h6=i

                                             XX   M    M
                                       1
                               =                Wi Wh Yi (high)Yh (low).
                                   N (N − 1)
                                                 i=1 h6=i

Finally, use iterated expectations:

                                                   XX M     M
                                             1
      E(ȲhighȲlow |Nhigh, Nlow) =                   E(WiWh |Nhigh, Nlow)Yi (high)Yh (low)
                                         N (N − 1)
                                                      i=1 h6=i

                                                   XX M     M
                                             1
                                   =                 [N (N − 1)/M (M − 1)]Yi (high)Yh (low)
                                         N (N − 1)
                                                      i=1 h6=i

                                                   XX M     M
                                             1
                                   =                 [N (N − 1)/M (M − 1)]Yi (high)Yh (low)
                                         N (N − 1)
                                                      i=1 h6=i

                                                   XX  M    M
                                             1
                                   =                  Yi (high)Yh (low),
                                         M (M − 1)
                                                      i=1 h6=i

which is what we needed to show. 

Proof of Lemma 6:
                   2        2
                  σM (low) σM (high)   N
      Vnorm
       causal =           +          −      2
                                         · σM (low, high).
                  Nlow /N   Nhigh/N    M

By Assumption 3 N/M → ρ. By Assumptions 3 and 7 Nlow /N → (1 − q) and Nhigh/N → q By
Assumption 6
       2
      σM (low) −→ σ 2 (low),            2
                                       σM (high) −→ σ 2 (high),    2
                                                                  σM (low, high) −→ σ 2 (low, high).

Together these imply the two results in the lemma. 

It is useful to state a lemma that we use repeatedly in the asymptotic theory.

Lemma A.2. For a sequence of random variables {UiM : i = 1, ..., M } assume that {(WiM , UiM ) :
i = 1, ..., M } is independent but not (necessarily) identically distributed. Further, WiM and UiM are



                                                        [43]
                                              2 ) < ∞ for i = 1, ..., M and
independent for all i=1,...,M. Assume that E(UiM
               M
               X
          −1
      M              E(UiM ) → µU
               i=1
               XM
                        2
      M −1           E(UiM ) → κ2U
               i=1

Finally, assume that Assumptions 1 and 3 hold. Then
               M
               X                         M
                                         X                   p
      N −1           WiM UiM − M −1            E(UiM ) → 0.
               i=1                       i=1

Proof: Write the first average as
               M
               X                                         M 
                                                           X       
          −1                           M ρM           −1       WiM
      N              WiM UiM =                    M                           UiM .
                                        N                          ρM
               i=1                                         i=1

                                                                                                       p
As argued in the text, because N ∼ Binomial(M, ρM ) and M ρM → ∞ by Assumption 3, (M ρM )/N −→
                             P
1. Because we assume M −1 M    i=1 E(UiM ) converges, it is bounded, and so it suffices to show that

               M 
               X                                 M
                                                  X
                   WiM                                             p
      M −1                       UiM − M −1             E(UiM ) → 0
                           ρM
               i=1                                i=1

Now because WiM is indepenent of UiM ,
       "     M            #          M                          M
             X   WiM                   X   E(WiM )                  X
          −1
     E M                 UiM = M −1                  E(UiM ) = M −1   E(UiM ),
                  ρM                         ρM
                     i=1                                   i=1                        i=1

and so the expected value of
               M 
               X                              M
                                               X
          −1       Wi
      M                         UiM − M −1           E(UiM )
                       ρM
               i=1                             i=1

is zero. Further, its variance exists by the second moment assumption, and by independence across i,
   "       M             #              M                           M                                       
          X    W iM
                                         X    1                      X     1
       −1                             −2                          −2                      2                  2
V M                    UiM     = M               V(WiM UiM ) = M              E[(WiM UiM ) ] − [E(WiM UiM )]
          i=1
               ρM                            ρ2
                                         i=1 M                       i=1
                                                                          ρ2M
                                         XM                                            XM
                                      −2        1        2      2          2      −2 −1          2
                               = M              2 ρM E(UiM ) − ρM [E(UiM )] ≤ M ρM           E(UiM )
                                         i=1
                                               ρM                                        i=1
                                          "       M
                                                           #
                                      1           X
                                              −1        2
                               =            M       E(UiM ) .
                                   M ρM
                                                                 i=1

By assumption, the term in brackets converges and by Assumption 3 M ρM → ∞. We have shown
mean square convergence and so convergence in probability follows. 

                                                                       [44]
We can apply the previous lemma to the second moment matrix of the data. Define
                               2          0
                                                        
              X M             YiM    YiM XiM    YiM Zi0
            1
     Ω̂M =         WiM ·  XiM YiM XiM XiM  0        0
                                               XiM ZiM  .
            N                              0         0
               i=1          ZiM YiM ZiM XiM ZiM ZiM
Lemma A.3. Suppose Assumptions 8–10 hold. Then:
                         p
      Ω̂M − ΩM −→ 0.

Proof: This follows from the previous lemma by letting UiM be an element of the above matrix in the
summand. The moment conditions are satisfied by Assumption 9 because fourth moments are assumed
to be finite. 

Note that in combination with the assumption that limM →∞ ΩM = Ω, Lemma A.3 implies that
              p
      Ω̂M −→ Ω.                                                                                  (A.1)

Proof of Lemma 7: The first claim follows in a straightforward manner from the assumptions and
Lemma A.3 because the OLS estimators can be written as
                                    !−1          !
       θ̂ols      Ω̂sample
                    XX,M    Ω̂sample
                              XZ 0 ,M     Ω̂sample
                                            XY,M
               =                                     .
       γ̂ols      Ω̂sample
                    ZX 0 ,M Ω̂sample
                              ZZ 0 ,M     Ω̂sample
                                            ZY,M

We know each element in the Ω̂M converges, and we assume its probability limit is positive definite.
The result follows. The other claims are even easier to verify because they do not involve the sampling
indicators WiM . 

Next we prove a lemma that is useful for establishing asympotic normality.
Lemma A.4. For a sequence of random variables {UiM : i = 1, ..., M } assume that {(WiM , UiM ) :
i = 1, ..., M } is independent but not (necessarily) identically distributed. Further, WiM and UiM are
independent for all i=1,...,M. Assume that for some δ > 0 and D < ∞, E(|UiM |2+δ ) ≤ D and
E(|UiM |) ≤ D, for i = 1, ..., M and all M. Also,
              M
              X
      M −1          E[UiM ] = 0
              i=1

and
                             M
                             X
       2
      σU,M        = M −1           V(UiM ) → σU2 > 0
                             i=1
                             XM
      κ2U,M       = M −1           [E(UiM )]2 → κ2U .
                             i=1

Finally, assume that Assumptions 1 and 3 hold. Then
                  M
                  X             d                         
      N −1/2            WiM UiM → N 0, [σU2 + (1 − ρ)κ2U ] .
                  i=1


                                                        [45]
Proof: First, write
                 M
                 X                                   1/2              M 
                                                                        X        
          −1/2                                M ρM               −1/2       WiM
      N                WiM UiM =                             M              √      UiM
                                               N                              ρM
                 i=1                                                    i=1
                            p          p
and, by Lemma A.2, note that (M ρM )/N → 1. Therefore, it suffices to show that
                           M 
                           X        
                               WiM        d                            
      RM = M       −1/2
                               √      UiM → N 0, [σU2 + (1 − ρ) · κ2U ] .
                                 ρM
                            i=1

Now

                                  M 
                                  X                                     M
                                                                         X
                                      E (WiM )              √
      E (RM ) = M −1/2                 √         E (UiM ) =   ρ M M −1/2
                                                                           E (UiM ) = 0
                                         ρM
                                  i=1                                                  i=1

and

                                M
                                X                         
                           −1                  WiM
      V (RM ) = M                     V        √          UiM .
                                                 ρM
                                i=1

The variance of each term can be computed as

                                                               2
               WiM                          WiM     2           WiM
      V        √           UiM        = E          UiM − E      √      UiM
                 ρM                          ρM                   ρM
                                           2
                                              
                                      = E UiM   − ρM [E(UiM )]2
                                      = V(UiM ) + (1 − ρM )[E(UiM )]2 .

Therefore,

                                M
                                X                                         M
                                                                          X
      V (RM ) = M −1                  V(UiM ) + (1 − ρM )M −1               [E(UiM )]2 → σU2 + (1 − ρ)κ2U .
                                i=1                                       i=1

The final step is to show that the double array

                            h                       √            i
                                  WiM
                 M −1/2           √
                                    ρM        UiM −       ρM αiM           1   (WiM UiM − ρM αiM )
      QiM      =     q                                                  =√    q                    ,
                             2
                            σU,M + (1 − ρM )κ2U,M                         M ρM σ 2 + (1 − ρM )κ2
                                                                                 U,M           U,M

where αiM = E(UiM ), satisfies the Lindeberg condition, as in Davidson (1994, Theorem 23.6). Sufficient
is the Liapunov condition

      M
      X
            E(|QiM |2+δ ) → 0 as M → ∞.
      i=1


                                                                         [46]
                  q
                    2                                                                              2
Now the term       σU,M + (1 − ρM )κ2U,M is bounded below by a strictly positive constant because σU,M →
σU2 > 0. Further,by the triangle inequality,

      n h                       io1/(2+δ)
       E |WiM UiM − ρM αiM |2+δ           ≤ [E (WiM ) E(|UiM |2+δ )]1/(2+δ) + ρM |αiM |
                                            h              i
                                              1/(2+δ)
                                          ≤ ρM        + ρ M D1

                                                                 1/(2+δ)
where D1 is constant. Because ρM ∈ [0, 1], ρM                              ≥ ρM , and so

       h                      i
      E |WiM UiM − ρM αiM |2+δ ≤ ρM D2 .

Therefore, the Liapunov condition is met if

      M
      X            ρM                       M ρM
             √           2+δ =                        = (M ρM )−δ/2 → 0,
      i=1         M ρM                  (M ρM )1+(δ/2)

which is true because δ > 0 and M ρM → ∞. We have shown that

                        h                     √            i
                             WiM
                  M
                  X          √
                               ρM       UiM −       ρM αiM       d
      M −1/2            q                      → N (0, 1)
                          2
                         σU,M + (1 − ρM )κ2U,M
                  i=1

                  q                       q
and so, with        2
                   σU,M + (1 − ρM )κ2U,M → σU2 + (1 − ρ)κ2U ,

                  M 
                  X                      
                       WiM         √        d                        
      M   −1/2
                       √      UiM − ρM αiM → N 0, [σU2 + (1 − ρ)κ2U ] . 
                         ρM
                  i=1



Proof of Lemma 8: This follows directly from Lemma A.4. 

Proof of Theorem 1: We prove part (i), as it is the most important. The other two parts follow
similar arguments. To show (i), it suffices to prove two claims. First,
        M               0
      1 X       ZiM    ZiM         p
          WiM                 − Γ −→ 0                                                            (A.2)
      N         XiM    XiM
            i=1

holds by Lemma A.3 and the comment following it. The second claim is
           M                                       
       1 X          XiM εiM    d     0
      √       WiM             −→ N      , ∆V + (1 − ρ)∆E .                                        (A.3)
        N i=1       ZiM εiM          0

If both claims hold then

                                                                     [47]
                                                 "  M                     0 #−1      M               
        √                    causal
                    θ̂ols − θM                     1 X          ZiM       ZiM           1 X         XiM εiM
            N                causal             =        WiM                          √       WiM
                    γ̂ols − γM                     N           XiM        XiM           N i=1       ZiM εiM
                                                     i=1
                                                         XM                 
                                                   −1 1             XiM εiM
                                                = Γ √        WiM               + op (1)
                                                       N i=1        ZiM εiM

and then we can apply the continuous convergence theorem and Lemma A.4. The first claim follows
from Lemma A.3 and the comment following. For the second claim, we use Lemma A.4 along with the
Cramér-Wold device. For a nonzero vector λ, define the scalar

                                      
                     0       XiM εiM
        UiM = λ                             ‘
                             ZiM εiM

Given Assumptions 8–10, all of the conditions of Lemma A.4 are met for {UiM : i = 1, ..., M }. There-
fore,

             M
         1 X            d                          
        √       WiM UiM → N 0, [σU2 + (1 − ρ)κ2U ]
          N i=1

where

                                      M
                                                           (
                                                           M                 )
                                      X                1  X        X iM εiM
        σU2     = lim M −1       V(UiM ) = λ0    lim           V                  λ = λ 0 ∆V λ
                  M →∞                          M →∞ M              ZiM εiM
                             i=1                          i=1
                      (         M                        0 )
                             1 X         Xi εi           Xi εi
        κ2U     = λ 0
                         lim         E              E                λ = λ 0 ∆E λ
                        M →∞ M           Zi εi           Zi εi
                                           i=1

and so

        [σU2 + (1 − ρ)κ2U ] = λ0 [∆V + (1 − ρ)∆E ]λ

By assumption this variance is strictly postive for all λ 6= 0, and so the Cramér-Wold Theorem proves
the second claim. The theorem now follows. 
                                                    causal
Proof of Theorem 2: For simplicity, let θ̃M denote θM      and similarly for γ̃M . Then θ̃M and γ̃M
solve the set of equations

        E(X 0 X)θ̃M + E(X 0 Z)γ̃M                  = E(X 0 Y )
                E(Z 0 X)θ̃M + Z 0 Z γ̃M            = E(Z 0 Y ),

where we drop the M subscript on the matrices for simplicity. Note that Z is nonrandom and that all
moments are well defined by Assumption 9. Multiply the second set of equations by E(X 0 Z)(Z 0 Z)−1
to get

        E(X 0 Z)(Z 0 Z)−1 E(Z 0 X)θ̃M + E(X 0 Z)γ̃M = E(X 0 Z)(Z 0 Z)−1 E(Z 0 Y )

                                                                  [48]
and subtract from the first set of equations to get
        [E(X 0 X) − E(X 0 Z)(Z 0 Z)−1 E(Z 0 X)]θ̃M = E(X 0 Y ) − E(X 0 Z)(Z 0 Z)−1 E(Z 0 Y )
Now, under Assumption 11,
        Y = Y (0) + Xθ
and so
        E(X 0 Y ) = E[X 0 Y (0)] + E(X 0 X)θ
        E(Z 0 Y ) = Z 0 Y (0) + E(Z 0 X)θ
It follows that
        E(X 0 Y ) − E(X 0 Z)(Z 0 Z)−1 E(Z 0 Y ) = E[X 0 Y (0)] + E(X 0 X)θ
                       −E(X 0 Z)(Z 0 Z)−1 Z 0 Y (0) − E(X 0 Z)(Z 0 Z)−1 E(Z 0 X)θ
                       = [E(X 0X) − E(X 0 Z)(Z 0 Z)−1 E(Z 0 X)]θ + E{X 0 [Y (0) − Z(Z 0 Z)−1 Z 0 Y (0)]}
                = [E(X 0X) − E(X 0 Z)(Z 0 Z)−1 E(Z 0 X)]θ + E{X 0 [Y (0) − ZγM ]}
                  P
The second term is M                         0
                    i=1 EX {XiM [YiM (0) − ZiM γM ]}, which is zero by Assumption 12. So we have
shown that
        [E(X 0 X) − E(X 0 Z)(Z 0 Z)−1 E(Z 0 X)]θ̃M = [E(X 0 X) − E(X 0 Z)(Z 0 Z)−1 E(Z 0 X)]θ
and solving gives θ̃M = θ. Invertibility holds for M sufficiently large by Assumption 10. Plugging
θ̃M = θ into the orginal second set of equations gives
        E(Z 0 X)θ + Z 0 Z γ̃M = Z 0 Y (0) + E(Z 0 X)θ
and so γ̃M = (Z 0 Z)−1 Z 0 Y (0) = γM . 

Proof of Theorem 3:
By the Frisch-Waugh Theorem (for example, Hayashi, 2000, page 73) we can write
            "     M
                                                          #−1      M
                  X                                                X
     θ̂ols = N −1   WiM (XiM − ZiM Π̂M )(XiM − ZiM Π̂M )0     N −1    WiM (XiM − ZiM Π̂M )YiM
                       i=1                                                               i=1

where YiM = YiM (XiM ) and
                           M
                                                !              M
                                                                                   !
                           X                                   X
                      −1                  0               −1                  0
        Π̂M =     N              WiM ZiM ZiM          N              WiM ZiM XiM
                           i=1                                 i=1

Plugging in for YiM =             0 γ
                                 ZiM      +    0 θ
                                              XiM    + εiM gives
                                      M
       M
       X                                                       M
                                                               X                                        M
                                                                                                        X
N −1         WiM (XiM − ZiM Π̂M )YiM            = N −1                                    0
                                                                     WiM (XiM − ZiM Π̂M )XiM θ + N −1         WiM (XiM − ZiM Π̂M )εiM
       i=1                                                  i=1                                         i=1
                                                      "       M
                                                                                                               #
                                                              X
                                                           −1
                                                =      N              WiM (XiM − ZiM Π̂M )(XiM − ZiM Π̂M )0 θ
                                                                i=1
                                                                XM
                                                      +N −1           WiM (XiM − ZiM Π̂M )εiM
                                                                i=1


                                                                     [49]
where we use the fact that
                  M
                  X
             −1                              0
        N               WiM (XiM − ZiM Π̂M )ZiM =0
                  i=1

by definition of Π̂M . It follows that
                   "                                                 #−1
√           
                       −1
                          XM
                                                                   0
                                                                                M
                                                                                X
  N θ̂ols − θ = N             WiM (XiM − ZiM Π̂M )(XiM − ZiM Π̂M )       N −1/2   WiM (XiM −ZiM Π̂M )εiM .
                                    i=1                                                                  i=1

Now
       M
       X                                                                            M
                                                                                    X
N −1         WiM (XiM − ZiM Π̂M )(XiM − ZiM Π̂M )0 = N −1                                 WiM (XiM − ZiM Π̂M )(XiM − ZiM ΛM )0
       i=1                                                                          i=1
                                                                                    M
                                                                                    X
                                                                      = N −1              WiM (XiM − ZiM ΛM )(XiM − ZiM ΛM )0
                                                                                    i=1
                                                                                      M
                                                                                      X
                                                                                   −1
                                                                             +N            WiM ZiM (Π̂M − ΛM )(XiM − ZiM ΛM )0
                                                                                     i=1
                                                                                    M
                                                                                    X
                                                                      = N −1              WiM (XiM − ZiM ΛM )(XiM − ZiM ΛM )0 + op (1)
                                                                                    i=1
                                                     −1
                                                        PM
because Π̂M − ΛM = op (1) and N                           i=1   WiM ZiM (XiM − ZiM ΛM )0 = Op(1). Further,
                      M
                      X                                                    M
                                                                           X
             −1/2                                                   −1/2
        N                   WiM (XiM − ZiM Π̂M )εiM = N                          WiM (XiM − ZiM ΛM )εiM + op (1)
                      i=1                                                  i=1
                        PM
because N −1/2              i=1   WiM ZiM εiM = Op (1) by the convergence to multivariate normality.
Next, if we let

        ẊiM = XiM − ZiM ΛM

then we have shown
                                                                     !−1
        √                                    M
                                               X                                    M
                                                                                    X
                                          −1                    0
             N θ̂ols − θ =            N              WiM ẊiM ẊiM         N −1/2         WiM ẊiM εiM + op (1)
                                               i=1                                  i=1

Now we can apply Theorems 1 and 2 directly. Importantly, εiM is nonstochastic and so

        E(ẊiM εiM ) = E(ẊiM )εiM = 0

because

        E(ẊiM ) = E(XiM ) − ZiM ΛM = 0

by Assumption 13. We have already assumed that WiM is independent of ẊiM . Therefore, using
Theorem 2, we conclude that
     √           
                    d
       N θ̂ols − θ −→ N (0, Γ−1
                             Ẋ
                                         −1
                                ∆ehw,Ẋ ΓẊ )

                                                                      [50]
where
                                   N
                                   X            
              ΓẊ   =   lim M −1              0
                                     E ẊiM ẊiM
                        M →∞
                                   i=1
                                   XN                    
        ∆ehw,Ẋ     =   lim M −1         E ε2iM ẊiM ẊiM
                                                       0
                                                            .
                        M →∞
                                   i=1


                                   Appendix B: A Bayesian Approach

Given that we are advocating for a different conceptual approach to modeling inference, it is useful to
look at the problem from more than one perspective. In this section we consider a Bayesian perspective
and re-analyze the example from Section 2.3. Using a simple parametric model we show that in a
Bayesian approach the same issues arise in the choice of estimand. Viewing it from this perspective
reinforces the point that formally modeling the population and the sampling process leads to the
conclusion that inference is different for descriptive and causal questions. Note that in this discussion
the notation will necessarily be slightly different from the rest of the paper; notation and assumptions
introduced in this subsection apply only within this subsection.
Define Y (low)M , Y ( high)M to be the M vectors with typical elements YiM (low) and YiM (high) respec-
tively. We view the M -vectors Y (low)M , Y (high)M , WM , and XM as random variables, some observed
and some unobserved. We assume the rows of the M × 4 matrix [Y (low)M , Y (high)M , WM , XM ] are
exchangeable. Then, by appealing to DeFinetti’s theorem, we model this, with (for large M ) no es-
sential loss of generality as the product of M independent and identically distributed random triples
(Yi (low), Yi(high), Xi) given some unknown parameter β:
                                             M
                                             Y
        f (Y (low)M , Y (high)M , XM ) =           f (Yi (low), Yi (high), Xi|β).
                                             i=1

Inference then proceeds by specifying a prior distribution for β, say p(β).
Let us make this specific, and use the following model. The Xi and Wi are assumed to have binomial
distributions with parameters q and ρ,

        pr(Xi = high|Yi (low), Yi(high), Wi) = q,              pr(Wi = 1|Yi (low), Yi (high)) = ρ.

The pairs (Yi (low), Yi(high)) are assumed to be jointly normally distributed:
                                                                                                    
  Yi (low)                        2        2                 µ(low)            σ 2 (low)   κσ(low)σ(high)
                µ(low), µ(high), σ (low), σ (high), κ ∼ N                ,                                   ,
  Yi (high)                                                  µ(high)        κσ(low)σ(high)    σ 2 (high)

so that the full parameter vector is β = (q, ρ, µ(low), µ(high), σ 2(low), σ 2 (high), κ).
We change the observational scheme slightly from the previous section to allow for the analytic deriva-
tion of posterior distributions. For all units in the population we observe the pair (Wi , Xi), and for
units with Wi = 1 we observe the outcome Yi = Yi (Xi). Define Ỹi = Wi ·Yi , so we can think of observing
for all units in the population the triple (Wi, Xi, Ỹi ). Let WM , XM , and ỸM be the M vectors of these
                         obs
variables. As before, Y high denotes the average of Yi in the subpopulation with Wi = 1 and Xi = 1,
        obs
and Y low denotes the average of Yi in the subpopulation with Wi = 1 and Xi = 0.


                                                           [51]
The issues studied in this paper arise in this Bayesian approach in the choice of estimand. The
descriptive estimand is
                          M
                          X                      M
       descr        1                        1 X
      θM     =                  Xi · Yi −          (1 − Xi ) · Yi .
                  Mhigh                     Mlow
                          i=1                     i=1

The causal estimand is
                    M
       causal     1 X                    
      θM      =       Yi (high) − Yi (low) .
                  M
                        i=1

It is interesting to compare these estimands to an additional estimand, the super-population average
treatment effect,
       causal
      θ∞      = µ(high) − µ(low).

In principle these three estimands are distinct, with their own posterior distributions, but in some
cases, notably when M is large, the three posterior distributions are similar.
For each of the three estimands we evaluate the posterior distribution in a special case. In many
cases there will not be an analytic solution. However, it is instructive to consider a very simple case
where analytic solutions are available. Suppose σ 2 (low), σ 2 (high), κ and q are known, so that the only
unknown parameters are the two means µ(low) and µ(high). Finally, let us use independent, diffuse
(improper), prior distributions for µlow and µ(high).
Then, a standard result is that the posterior distribution for (µlow , µ(high)) given (WM , XM , ỸM ) is
                                              obs
                                                      !                                      !
          µ(low)                              Y low         σ 2 (low)/Nlow           0
                     WM , XM , ỸM ∼ N          obs     ,                                        .
         µ(high)                              Y high                0        σ 2 (high)/Nhigh

                                                       causal = µ(high) − µ(low):
This directly leads to the posterior distribution for θ∞
                                                   2
                                                                         
       causal                        obs      obs σ (low)     σ 2 (high)
      θ∞ |WM , XM , ỸM ∼ N Y high − Y low ,               +               .
                                                    Nlow        Nhigh

A longer calculation leads to the posterior distribution for the descriptive estimand:
       descr
      θM     |WM , XM , ỸM ∼
                                2                                      
                   obs     obs σ (low)      Nlow     σ 2 (high)      Nhigh
              N Y high − Y low ,       · 1−        +            · 1−          .
                                 Nlow       Mlow       Nhigh         Mhigh
                                    descr is very similar to the corresponding confidence interval based
The implied posterior interval for θM
                                                                    obs    obs
on the normal approximation to the sampling distribution for Y high − Y low . If Mlow , Mhigh are large,
this posterior distribution converges to
       descr                                         causal
      θM     |WM , XM , ỸM , Mlow → ∞, Mhigh → ∞ ∼ θ∞      |WM , XM , ỸM .

If, on the other hand, Nlow = Mlow and Nhigh = Mhigh , then the distribution becomes degenerate:
                                                                                
        descr                                                    obs      obs
       θM     |WM , XM , ỸM , Nlow = Mlow , Nhigh = Mhigh ∼ N Y high − Y low , 0 .

                                                            [52]
                                   causal leads to
A somewhat longer calculation for θM
                               
      causal                       obs       obs Nlow 2                      Nhigh 2
     θM |WM , XM , ỸM ∼ N Y high − Y low ,         2
                                                      σ (high) · (1 − κ2 ) +      σ (low) · (1 − κ2 )
                                                 M                            M2

                      M −N 2             M −N 2              M −N
                  +        2
                              σ (high) +     2
                                                 σ (low) − 2         κσ(high)σ(low)
                        M                  M                  M2
                                                    2                                    2 !
               σ 2 (high)               σ(low) Nhigh         σ 2 (low)             σ(high) Nlow
             +            · 1− 1−κ                        +            · 1− 1−κ                      .
                 Nhigh                  σ(high)    M           Nlow                 σ(low)   M

Consider the special case where κ = 1, σ(low) = σ(high). Then
       causal                                             causal
      θM      |WM , XM , ỸM , κ = 1, σ(low) = σ(high) ∼ θ∞      |WM , XM , ỸM .

The same limiting posterior distribution applies if M goes to infinity.
       causal                                         causal
      θM      |WM , XM , ỸM , Mlow → ∞, Mhigh → ∞ ∼ θ∞      |WM , XM , ỸM .

The point is that if the population is large, relative to the sample, the three posterior distributions
agree. However, if the population is small, the three posterior distributions differ, and the researcher
needs to be precise in defining the estimand. In such cases simply focusing on the super-population
           causal = µ
estimand θ∞            high − µlow is arguably not appropriate, and the posterior inferences for such
                                                               causal     descr
estimands will differ from those for other estimands such as θM       or θM     .




                                                   [53]
