                                NBER WORKING PAPER SERIES




      IV QUANTILE REGRESSION FOR GROUP-LEVEL TREATMENTS, WITH AN
           APPLICATION TO THE DISTRIBUTIONAL EFFECTS OF TRADE

                                         Denis Chetverikov
                                          Bradley Larsen
                                         Christopher Palmer

                                        Working Paper 21033
                                http://www.nber.org/papers/w21033


                      NATIONAL BUREAU OF ECONOMIC RESEARCH
                               1050 Massachusetts Avenue
                                 Cambridge, MA 02138
                                     March 2015




We thank Moshe Buchinsky, Ivan Canay, Brigham Frandsen, Antonio Galvao, Wenshu Guo, Jerry
Hausman, Rosa Matzkin, Whitney Newey, and Christopher Taber for helpful comments; and Yuqi
Song and Caio Waisman for meticulous research assistance. We are especially grateful to Jin Hahn
for many useful discussions. The views expressed herein are those of the authors and do not necessarily
reflect the views of the National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2015 by Denis Chetverikov, Bradley Larsen, and Christopher Palmer. All rights reserved. Short
sections of text, not to exceed two paragraphs, may be quoted without explicit permission provided
that full credit, including © notice, is given to the source.
IV Quantile Regression for Group-level Treatments, with an Application to the Distributional
Effects of Trade
Denis Chetverikov, Bradley Larsen, and Christopher Palmer
NBER Working Paper No. 21033
March 2015
JEL No. C21,C31,C33,C36,F16,J30

                                                ABSTRACT

We present a methodology for estimating the distributional effects of an endogenous treatment that
varies at the group level when there are group-level unobservables, a quantile extension of Hausman
and Taylor (1981). Because of the presence of group-level unobservables, standard quantile regression
techniques are inconsistent in our setting even if the treatment is independent of unobservables. In
contrast, our estimation technique is consistent as well as computationally simple, consisting of group-by-group
quantile regression followed by two-stage least squares. Using the Bahadur representation of quantile
estimators, we derive weak conditions on the growth of the number of observations per group that
are sufficient for consistency and asymptotic zero-mean normality of our estimator. As in Hausman
and Taylor (1981), micro-level covariates can be used as internal instruments for the endogenous group-level
treatment if they satisfy relevance and exogeneity conditions. An empirical application indicates that
low-wage earners in the US from 1990--2007 were significantly more affected by increased Chinese
import competition than high-wage earners. Our approach applies to a broad range of settings in labor,
industrial organization, trade, public finance, and other applied fields.


Denis Chetverikov                                      Christopher Palmer
UCLA, Department of Economics                          Haas School of Business
315 Portola Plaza                                      University of California at Berkeley
Bunche Hall, Room 8283                                 545 Student Services Building
Los Angeles, CA 90095-1477                             Berkeley, CA 94720-1900
chetverikov@econ.ucla.edu                              cjpalmer@berkeley.edu

Bradley Larsen
Department of Economics
Stanford University
579 Serra Mall
Stanford, CA 94305
and NBER
bjlarsen@stanford.edu
                          IV QUANTILE REGRESSION FOR GROUP-LEVEL TREATMENTS                                         1

                                                 1. Introduction

  In classical panel-data models for mean regression, fixed effects are commonly used to obtain
identification when time-invariant unobservables are correlated with included variables. While
this approach yields consistent estimates of the coefficients on time-varying variables, it precludes
identification of the coefficients of any time-invariant variables, as these variables are eliminated by
the within-group transformation. In an influential paper, Hausman and Taylor (1981) demonstrated
that exogenous between variation of time-varying variables can help to identify the coefficients of
time-invariant variables after their within variation has been used to identify the coefficients on
time-varying variables, thus yielding identification of the whole model without external instruments.
Our paper provides a quantile extension of the Hausman and Taylor (1981) classical linear panel
estimator.
  We present our model in Section 2. To clarify the range of potential applications of our esti-
mator, we depart in the model from the usual panel-data terminology and refer to panel units as
groups (instead of as individuals; groups might be states, cities, schools, etc.) and to within-group
observations as individuals or micro-level observations (instead of as time observations; individuals
might be students, families, firms, etc.).1 The model is of practical significance when the researcher
has data on a group-level endogenous treatment and has microdata on the outcome of interest
within each group. For example, a researcher may be interested in the effect of a policy which
varies across states and years (a “group”) on the within-group distribution of micro-level outcomes.
In Section 2, we also explain how the problem we solve differs from others in the quantile regression
literature, and we demonstrate that, as in Hausman and Taylor (1981), micro-level covariates can
be used as internal instruments for the endogenous group-level treatment if they satisfy relevance
and exogeneity conditions. This last feature of the model is especially appealing because in practice
it may be difficult to find external instruments.
  We introduce our estimator in Section 3. The estimator is computationally simple to implement
and consists of two steps: 1) perform quantile regression within each group to estimate effects of
micro-level covariates, or, if no micro-level covariates are included, calculate the desired quantile for
the outcome within each group; and 2) regress the estimated group-specific effects on group-level
covariates using either 2SLS, if the group-level covariates are endogenous, or OLS, if the group-
level covariates are exogenous, either of which cases would render standard quantile regression
(e.g. Koenker and Bassett 1978) inconsistent.2 Section 3 also highlights a variety of applied
micro settings in which our estimator is useful (with detailed example applications discussed in

  1
      Similar terminology is used, for example, by Altonji and Matzkin (2005).
  2
      Even in the absence of endogeneity, the Koenker and Bassett (1978) estimator will be inconsistent in our setting
because of group-level unobservables, akin to left-hand side measurement error; see Section 2 for details on our
setting. While posing no problems for linear models, left-hand side errors-in-variables can bias quantile estimation
(see Hausman (2001) and Hausman, Luo, and Palmer (2014)).
2                                CHETVERIKOV, LARSEN, AND PALMER

Appendix A) and discusses Monte Carlo simulations (found in Appendix B) that demonstrate that
our estimator has much lower bias than that of the standard quantile regression estimator when
the group-level treatment is endogenous, even in small samples, and at larger sample sizes our
estimator outperforms quantile regression even when the treatment is exogenous. Section 3 also
highlights additional computational benefits of our estimator.
    We derive theoretical properties of the estimator in Section 4. The results are based on asymp-
totics where both the number of groups and the number of observations per group grow to infinity.
While linear panel models, including Hausman and Taylor (1981), admit a simple unbiased fixed
effects estimator and hence do not require asymptotics in the number of observations per group,
quantile estimators are biased in finite samples leading to inconsistency of our estimator if the
number of observations per group remains small as the number of groups increases, and making the
estimator inappropriate in the settings with a small number of observations per group and a large
number of groups. However, since quantile estimators are asymptotically unbiased, we are able to
employ Bahadur’s representation of quantile estimators to derive weak conditions on the growth of
the number of observations per group that are sufficient for the consistency and asymptotic zero-
mean normality of our estimator. Importantly, the attractive theoretical properties of the estimator
remain valid even if the number of observations per group is relatively small in comparison with
the number of groups. We demonstrate that standard errors for the proposed estimator can be
obtained using traditional robust variance estimators for 2SLS (heteroskedasticity-robust and clus-
tered), making inference particularly simple. Finally, we show how to construct confidence bands
for the coefficient of interest which hold uniformly over a set of quantiles of interest via multiplier
bootstrap procedure.
    Section 5 presents an empirical application which studies the effect of trade on the distribution
of wages within local labor markets. We build on the work of Autor, Dorn, and Hanson (2013), who
studied the effect of Chinese import competition on average wages in local labor markets. Using the
grouped IV quantile regression approach developed here, we find that Chinese import competition
reduced the wages of low-wage earners (individuals at the bottom quartile of the conditional wage
distribution) more than high-wage earners, particularly for females, heterogeneity which is missed
by focusing on traditional 2SLS estimates.
    To the best of our knowledge, our paper is the first to present a framework for estimating
distributional effects as a function of group-level covariates. There is, however, a large literature
studying quantile models for panel data when the researcher wishes to estimate distributional effects
of micro-level covariates. See, for example, Koenker (2004), Abrevaya and Dahl (2008), Lamarche
(2010), Canay (2011), Galvao (2011), Kato and Galvao (2011), Ponomareva (2011), Kato, Galvao,
and Montes-Rojas (2012), Rosen (2012), Arellano and Bonhomme (2013), and Galvao and Wang
(2013). Our paper also contributes to the growing literature on IV treatment effects in quantile
models, such as Abadie, Angrist, and Imbens (2002), Chernozhukov and Hansen (2005, 2006, 2008),
                         IV QUANTILE REGRESSION FOR GROUP-LEVEL TREATMENTS                                              3

Lee (2007), Chesher (2003), and Imbens and Newey (2009). Our paper differs, however, in that this
literature focuses on the case where individual-level unobserved heterogeneity is correlated with an
individual-level treatment, whereas we focus on the case where a group-level, additively separable
unobservable is correlated with a group-level treatment.
   Throughout the paper, we use the following notation. The symbol k · k denotes the Euclidean
norm. The symbol ⇒ signifies weak convergence, and l∞ (U) represents the set of bounded functions
on U. With some abuse of notation, `∞ (U) also denotes the set of component-wise bounded vector-
valued functions on U. All equalities and inequalities concerning random variables are implicitly
assumed to hold almost surely. All proofs and some extensions of our results are contained in the
Appendix.

                                                      2. Model

   We study a panel data quantile regression model for a response variable yig of individual i in
group g. We first present the model in its most general form in equations (1) and (2) below and
then discuss a particular operationalization of the model in equation (3) that will be particularly
appealing for applied work. In the general model, we assume that the uth quantile of the conditional
distribution of yig is given by
                                                            0
                                   Qyig |zig ,xg ,αg (u) = zig αg (u), u ∈ U                                          (1)
                                             αg,1 (u) = x0g β(u) + εg (u), u ∈ U,                                     (2)

where Qyig |zig ,xg ,αg (u) is the uth conditional quantile of yig given (zig , xg , αg ), zig is a dz -vector
of observable individual-level covariates (which we sometimes refer to as micro-level covariates),
αg = {αg (u), u ∈ U } is a set of group-specific effects with αg,1 (u) being the first component of the
vector αg (u) = (αg,1 (u), . . . , αg,dz (u))0 , xg is a dx -vector of observable group-level covariates (xg
contains a constant), β(u) is a dx -vector of coefficients, εg = {εg (u), u ∈ U} is a set of unobservable
group-level random scalar shifters,3 and U is a set of quantile indices of interest. Thus, we assume
that the response variable yig satisfies the quantile regression model in (1) with group-specific
effects αg (u). We are primarily interested in studying how these effects depend on the group-level
covariates xg , and, without loss of generality, we focus on αg,1 (u), the first component of the vector
αg (u). To make the problem operational, we assume that αg,1 (u) satisfies the linear regression
model (2), in which we are interested in estimating the vector of coefficients β(u).
   In empirical work, we envision that the most useful variant of the model (1)-(2) would be case
where the first element of zig corresponds to a constant and where coefficients on micro-level

   3One interpretation of the term ε (u) in (2) is that it accounts for all unobservable group-level covariates η that
                                    g                                                                            g

affect αg,1 (u) but are not included in xg . In this case, εg (u) = ε(u, ηg ). Note that we do not impose any parametric
restrictions on ε(u, ηg ), and so we allow for arbitrary nonlinear effects of the group-level unobservable covariates that
can affect different quantiles in different ways.
4                                      CHETVERIKOV, LARSEN, AND PALMER

covariates do not vary by group, given by the model
                                                        0
                             Qyig |ezig ,xg ,εg (u) = zeig γ(u) + x0g β(u) + εg (u), u ∈ U,                             (3)

which is obtained from (1)-(2) by assuming that (αg,2 (u), . . . , αg,dz (u))0 = γ(u) for some non-
stochastic (dz − 1)-vector γ(u) and all g = 1, . . . , G, setting zig = (1, zeig )0 , and substituting (2) into
(1). This model allows for the analysis of location-shift effects of the group-level covariates xg on
the conditional distribution of yig in the group g.
    As an example of where the above modeling framework is useful, consider a case in which a
researcher wishes to model the effects of a policy, contained in xg , which varies at the state-by-year
level (a “group” in this setting) on the distribution of micro-level outcomes (such as individuals’
wages within each state-by-year combination), denoted yig , conditional on micro-level covariates,
such as education level, denoted zig . The framework in (3) would model the location-shift effect of
the policy on conditional quantiles of wages within a group, given by β(u). The additional flexibility
of (1)-(2) would also allow for interaction effects. For example, a policy xg may have differential
effects on lower wage quantiles for the less-educated than for the higher-educated; model (1) would
capture this idea by allowing the researcher to specify a linear regression model of the form of (2)
for the component of αg that is the coefficient on education level, allowing the researcher to study
how the effect of education level on the wage distribution varies as a function of xg , the policy.4
    In many applications, it is likely that the group-level covariates xg may be endogenous in the
sense that E[xg εg (u)] 6= 0, at least for some values of the quantile index u ∈ U. Therefore, to
increase applicability of our results, we assume that there exists a dw -vector of observable instru-
ments wg such that E[wg εg (u)] = 0 for all u ∈ U, E[wg x0g ] is nonsingular, and yig is independent
of wg conditional on (zig , xg , αg ).5 The first two conditions are familiar from the classical lin-
ear instrumental variable regression analysis, and the third condition requires the distribution of
yig to be independent of wg once we control for zig , xg , and αg . It implies, in particular, that
                             0 α (u) for all u ∈ U.6
Qyig |zig ,xg ,αg ,wg (u) = zig g

    4If the researcher is interested in modeling several effects, for example location-shift and some interaction effects,

she can specify a linear regression model of the form (2) for each effect.
   5To understand the assumption that E[w ε (u)] = 0 holds jointly for all u ∈ U, assume, for example, that
                                               g g

εg (u) = ε(u, ηg ) where ηg is a vector of group-level omitted variables in regression (2). Then a sufficient condition for
the assumption E[wg εg (u)] = E[wg ε(u, ηg )] = 0 is that E[ε(u, ηg )|wg ] = 0. In turn, the restriction of the condition
E[ε(u, ηg )|wg ] = 0 is that E[ε(u, ηg )|wg ] does not depend on wg , which occurs (for example) if ηg is independent of
wg . Once we assume that E[ε(u, ηg )|wg ] does not depend on wg , the further restriction that E[ε(u, ηg )|wg ] = 0 is a
normalization of the component of the vector β(u) corresponding to the constant in the vector xg .
   6The setting we model differs from other IV quantile settings, such as Chernozhukov and Hansen (2005, 2006,

2008). Consider, for simplicity, our model (3) and assume that U = [0, 1]. Then the Skorohod representation implies
             0
that yig = zeig γ(uig ) + x0g β(uig ) + εg (uig ) where uig is a random variable that is distributed uniformly on [0, 1] and
is independent of (e
                   zig , xg , εg ). Here, one can think of uig as unobserved individual-level heterogeneity. In this model,
the unobserved group-level component εg (·) is modeled as an additively separable term. In contrast, the model in
                         IV QUANTILE REGRESSION FOR GROUP-LEVEL TREATMENTS                                             5

   We assume that a researcher has data on G groups and Ng individuals within group g = 1, ..., G.
Thus, the data consist of observations on {(zig , yig ), i = 1, . . . , Ng }, xg , and wg for g = 1, . . . , G.
Throughout the paper, we denote NG = min1≤g≤G Ng . For our asymptotic theory in Section 4, we
will assume that NG gets large as G → ∞. Specifically, for the asymptotic zero-mean normality of
our estimator β̂(u) of β(u), we will assume that G2/3 (log NG )/NG → 0 as G → ∞; see Assumption 3
below. Thus, our results are useful when both G and NG are large, which occurs in many empirical
applications, but we also note that our results apply even if the number of observations per group
is relatively small in comparison with the number of groups.
   We also emphasize that, like in the original panel data mean regression model of Hausman and
Taylor (1981), an important feature of our panel data quantile regression model is that it allows for
internal instruments. Specifically, if some component of the vector zig , say zig,k , is exogenous in the
                                                                        −1/2 PNg
sense that E[zig,k εg (u)] = 0 for all u ∈ U, we can use, for example, Ng     i=1 zig,k as an additional
instrument provided it is correlated with xg , including it into the vector wg . Since in practice it is
often difficult to find an appropriate external instrument, allowing for internal instruments greatly
increases applicability of our results.
   Our problem in this paper is different from that studied in Koenker (2004), Kato, Galvao, and
Montes-Rojas (2012), and Kato and Galvao (2011).7 Specifically, they considered the panel data
quantile regression model


                                                         0
                                    Qyig |zig ,αg (u) = zig γ(u) + αg (u), u ∈ U,                                    (4)


and developed estimators of γ(u). Building on Koenker (2004), Kato, Galvao, and Montes-Rojas
(2012) suggested estimating γ(u) in this model by running a quantile regression estimator of
Koenker and Bassett (1978) on the pooled data, treating {αg (u), g = 1, . . . , G} as a set of pa-
rameters to be estimated jointly with the vector of parameters γ(u) (the same technique can be
used to estimate γ(u) in our model (3) by setting αg (u) = x0g β(u) + εg (u)). They showed that their
estimator is asymptotically zero-mean normal if G2 (log G)3 /NG → 0 as G → ∞. Making further
progress, Kato and Galvao (2011) suggested an interesting smoothed quantile regression estimator




Chernozhukov and Hansen (2005, 2006, 2008) assumes that εg (u) = 0 for all u ∈ [0, 1] and instead assumes that uig
is not independent of (e
                       zig , xg ). Thus, these two models are different and require different analysis.
    7Our paper is also related to but different from Graham and Powell (2012) who studied the model that in our
                                    0
notation would take the form yig = zig αg (uig ) where uig represents (potentially multi-dimensional) random unob-
served heterogeneity, and developed an interesting identification and estimation strategy for the parameter E[αg (uig )],
achieving identification when the number of observations per group remains small as the number of groups gets large
and, under certain conditions, allowing αg (·) = αig (·) to depend on i.
6                                         CHETVERIKOV, LARSEN, AND PALMER

of γ(u) that is asymptotically zero-mean normal if G/NG → 0.8 These papers do not provide a
model for our estimator of β(u), our primary object of interest, but instead focus solely on γ(u).
    Our model is also different from that studied in Hahn and Meinecke (2005), who considered an
extension of Hausman and Taylor (1981) to cover non-linear panel data models. Formally, they
considered a non-linear panel data model defined by the following equation:
                                            0
                                               γ + x0g β + εg ) = 0
                                                              
                                 E ϕ(yig , zig

where ϕ(·, ·) is a vector of moment functions and x0g β + εg is the group-specific effect. As in this
paper, the authors were interested in estimating the effect of group-level covariates (coefficient β)
without assuming that εg is independent (or mean-independent) of xg but assuming instead that
there exists an instrument wg satisfying E[wg εg ] = 0. Importantly, however, they assumed that
ϕ(·, ·) is a vector of smooth functions, so that their results do not apply immediately to our model.
In addition, Hahn and Meinecke (2005) required that NG /G > c for some c > 0 uniformly over all
G to prove that their estimator is asymptotically zero-mean normal. In contrast, as emphasized
above, we only require that G2/3 (log NG )/NG → 0 as G → ∞, with the improvement coming from
a better control of the residuals in the Bahadur representation.9

                                                        3. Estimator

    In this section we develop our estimator. Our main emphasis is to derive a computationally
simple, yet consistent, estimator. The estimator consists of the following two stages.


Stage 1: For each group g and each quantile index u from the set U of indices of interest, estimate
uth quantile regression of yig on zig using the data {(yig , zig ) : i = 1, ..., Ng } by the classical quantile
regression estimator of Koenker and Bassett (1978):
                                                                  Ng
                                                                  X
                                                                                   0
                                          α̂g (u) = arg min             ρu (yig − zig a),
                                                          a∈Rdz
                                                                  i=1

    8To clarify the difference between the growth condition in our paper, which is G2/3 (log N )/N → 0, and the
                                                                                              G   G

growth condition, for example, in Kato, Galvao, and Montes-Rojas (2012), which is G2 (log G)3 /NG → 0, assume,
for simplicity, that dx = 1, dz = 2, and xg and the second component of zig are constants, that is, xg = 1 and
       zig , 1)0 . Then our model (1)-(2) reduces to Qyig |ezig ,εg ,αg (u) = zeig (β(u) + εg (u)) + αg (u), which is similar to the
zig = (e
model (4) studied in Kato, Galvao, and Montes-Rojas (2012) with the exception that we allow for additional group-
specific random shifter εg (u). When εg (u) is present, our estimator β̂(u) of β(u) satisfies G1/2 (β̂(u)−β(u)) ⇒ N (0, V1 )
for some non-vanishing variance V1 ; see Section 4. When εg (u) is set to zero, however, V1 vanishes, making the limiting
distribution degenerate and leading to faster convergence rate of the estimator β̂(u). In fact, when V1 vanishes, one
                                                                                                                1/2
obtains (GNG )1/2 (β̂(u) − β(u)) ⇒ N (0, V2 ) for some non-vanishing variance V2 . An additional NG                   factor in turn
appears in the residual terms of the Bahadur representation of the estimator β̂(u), which eventually lead to stronger
requirements on the growth of the number of observations per group NG relative to the number of groups, explaining
the difference between the growth condition in Kato, Galvao, and Montes-Rojas (2012) and our growth condition.
    9Appendix F contains additional discussion of the model, including an extension to a random coefficients setting.
                        IV QUANTILE REGRESSION FOR GROUP-LEVEL TREATMENTS                                            7

where ρu (x) = (u − 1{x < 0})x for x ∈ R. Denote α̂g (u) = (α̂g,1 (u), . . . , α̂g,dz )0 .


Stage 2: Estimate a 2SLS regression of α̂g,1 (u) on xg using wg as an instrument to get an estimator
β̂(u) of β(u), that is,
                                                          −1                   
                                      β̂(u) = X 0 PW X            X 0 PW Â(u)

where X = (x1 , ..., xG )0 , W = (w1 , ..., wG )0 , Â(u) = (α̂1,1 (u), . . . , α̂G,1 (u))0 , and PW = W (W 0 W )−1 W 0 .


   Intuitively, as the number of observations per group increases, α̂g,1 −αg,1 shrinks to zero uniformly
over g = 1, . . . , G, and we obtain a classical instrumental variables problem. The theory presented
below provides a mild condition on the growth of the number of observations per group that is
sufficient to achieve consistency and asymptotic zero-mean normality of β̂(u).
   Several special cases of our estimator are worth noting. First, when the model is given by
equation (3), the steps of our estimator consist of 1) group-by-group quantile regression of yig
on z̃ig and on a constant, saving the estimated coefficient α̂g,1 (u) corresponding to the constant,
αg,1 (u) = x0g β(u) + εg (u), in each group; and 2) regressing those saved coefficients α̂g,1 (u) on xg via
2SLS using wg as instruments. Second, if zig contains only a constant, the first stage simplifies to
selecting the uth quantile of the outcome variable yig within each group. Third, if xg is exogenous,
that is, E[xg εg (u)] = 0, OLS of α̂g,1 (u) on xg may be used rather than 2SLS in the second stage.
In this latter case, the grouped quantile estimation approach provides the advantage of handling
group-level unobservables (or, alternatively, left-hand-side measurement error), which would bias
the traditional Koenker and Bassett (1978) estimator. When zig only includes a constant and xg is
exogenous, the grouped IV quantile regression estimator β̂(u) simplifies to the minimum distance
estimator described in Chamberlain (1994) (see also Angrist, Chernozhukov, and Fernandez-Val
2006).
   This estimator has several computational benefits relative to alternative methods. First, note
that when the model is given by equation (3), another approach to perform the first stage of
our estimator would be to denote αg,1 (u) = x0g β(u) + εg (u) and estimate parameters γ(u) and
{αg,1 (u) : g = 1, . . . , G} jointly from the pooled dataset as in Kato, Galvao, and Montes-Rojas
(2012). This would provide an efficiency gain given that in this case, individual-level effects γ(u)
are group-independent. Although the method we use is less efficient, it is computationally much
less demanding since only few parameters are estimated in each regression, which can greatly
reduce computation times in large datasets with many fixed effects.10 Second, even if no group-
level unobservables exist (consider model (3) with εg (u) = 0 for all g = 1, . . . , G), the grouped
estimation approach can be considerably faster than the traditional Koenker and Bassett (1978)
   10In Monte Carlo experiments in Appendix B, we find that jointly estimating group-level effects can take over 150

times as long as the grouped quantile approach when G = 200. With G > 200, the computation time ratio drastically
increases further, with standard optimization packages often failing to converge appropriately.
8                                    CHETVERIKOV, LARSEN, AND PALMER

estimator (though both estimators will be consistent). This computational advantage occurs when
the dimension of xg is large: standard quantile regression estimates β(u) in a single, nonlinear step,
whereas the grouped quantile approach estimates β(u) in a linear second stage.11
    Monte Carlo simulations in Appendix B highlight the performance of our estimator for β(u) in
(3) relative to the traditional Koenker and Bassett (1978) estimator (which ignores endogeneity of
xg as well as the existence of εg (u)). Even when NG and G are both small, the grouped IV quantile
approach has lower bias than traditional quantile regression when xg is endogenous. When xg is
exogenous but group-level unobservables εg (u) are still present, the bias of the grouped quantile
approach shrinks quickly to zero as NG grows but the bias of traditional quantile estimator does not.
When no group-level unobservables are present, and hence both the grouped estimation approach
and traditional quantile regression should be consistent, our estimator still has small bias, although
traditional quantile regression outperforms our method in this case.
    As we demonstrate in Section 4 below, standard errors for our estimator β̂(u) may be obtained
using standard heteroskedasticity-robust or clustering approaches for 2SLS or OLS as if there were
no first stage.12 Section 4 also describes a multiplier bootstrap procedure that is suitable for
constructing uniform confidence bands for the case when the researcher is interested in the set U
of quantile indices u.
    To conclude this section, we note that our estimator applies to a wide variety of settings in labor,
industrial organization, trade, public finance, development, and other applied fields. Appendix A
illustrates examples from Angrist and Lang (2004), Larsen (2014), Palmer (2011), and Backus
(2014).

                                           4. Asymptotic Theory

    In this section, we formulate our assumptions and present the main theoretical results of the
paper.

4.1. Assumptions. Let cM , cf , CM , Cf , CL be strictly positive constants whose values are fixed
throughout the paper. Recall that NG = ming=1,...,G Ng . We start with specifying our main
assumptions.

A1 (Design). (i) Observations are independent across groups. (ii) For all g = 1, . . . , G, the pairs
(zig , yig ) are i.i.d. across i = 1, . . . , Ng conditional on (xg , αg ).

A 2 (Instruments). (i) For all u ∈ U and g = 1, . . . , G, E[wg εg (u)] = 0. (ii) As G → ∞,
G−1 G
    P           0               −1
                                   PG         0
      g=1 E[xg wg ] → Qxw and G     g=1 E[wg wg ] → Qww where Qxw and Qww are matrices with

    11One such example would be a case where a group is a state-by-year combination, and x contains many state
                                                                                          g

and year fixed effects, in addition to the treatment of interest, as in Example 2 of Appendix A.
  12Note that clustering in the second stage refers to dependence across groups, not within groups. For example,

if a group is a state-by-year combination, the researcher may wish to use standard errors which are clustered at the
state level.
                       IV QUANTILE REGRESSION FOR GROUP-LEVEL TREATMENTS                                       9

singular values bounded in absolute value from below by cM and from above by CM . (iii) For all
g = 1, . . . , G and i = 1, . . . , Ng , yig is independent of wg conditional on (zig , xg , αg ). (iv) For all
g = 1, . . . , G, E[kwg k4+cM ] ≤ CM .

A3 (Growth Condition). As G → ∞, we have G2/3 (log NG )/NG → 0.

  Assumption 1(i) holds, for example, if groups are sampled randomly from some population of
groups. This assumption precludes the possibility of clustering across groups (for example, if a
group is a state-by-year combination, there may be clustering on the state level). Since clustered
standard errors are important in practice, however, we derive an extension of our results relaxing
the independence across groups condition and allowing for clustering in Appendix E. Assumption
1(ii) allows for inter-dependence (clustering) within groups but imposes the restriction that the
inter-dependence between observations within the group g is fully controlled for by the group-level
covariates xg and the group-specific effect αg . Assumption 2 is our main identification condition.
                                                                                    −1/2 PNg
Note that Assumption 2 allows for internal instruments. In particular, if wg = Ng          i=1 zig,k
for some k, then Assumption 2(iii) automatically follows from Assumption 1(ii). Assumption 3
implies that the number of observations per group grows sufficiently fast as G gets large, and gives
a particular growth rate that suffices for our results. Note that our growth condition is rather weak
and, most importantly, allows for the case when the number of observations per group is small
relative to the number of groups.13
  Next, we specify technical conditions that are required for our analysis. Let Eg [·] = E[·|xg , αg ],
and let fg (·) denote the conditional density function of y1g given (z1g , xg , αg ) (dependence of fg (·) on
                                                                                0 α (u)−c, z 0 α (u)+c)
z1g is not shown explicitly for brevity of notation). Also denote Bg (u, c) = (z1g g        1g g
for c > 0. We will assume the following regularity conditions:

A4 (Covariates). (i) For all g = 1, . . . , G and i = 1, . . . , Ng , random vectors zig and xg satisfy
                                                                                        0 ] are bounded
kzig k ≤ CM and kxg k ≤ CM . (ii) For all g = 1, . . . , G, all eigenvalues of Eg [z1g z1g
from below by cM .

A5 (Coefficients). For all u1 , u2 ∈ U and g = 1, . . . , G, kαg (u2 ) − αg (u1 )k ≤ CL |u2 − u1 |.

A6 (Noise). (i) For all g = 1, . . . , G, E[supu∈U |εg (u)|4+cM ] ≤ CM . (ii) For some (matrix-valued)
function J : U × U → Rdw ×dw , G−1 G                               0
                                       P
                                         g=1 E[εg (u1 )εg (u2 )wg wg ] → J(u1 , u2 ) uniformly over u1 , u2 ∈
U. (iii) For all u1 , u2 ∈ U, |εg (u2 ) − εg (u1 )| ≤ CL |u2 − u1 |.

A7 (Density). (i) For all u ∈ U and g = 1, . . . , G, the conditional density function fg (·) is continu-
ously differentiable on Bg (u, cf ) with the derivative fg0 (·) satisfying |fg0 (y)| ≤ Cf for all y ∈ Bg (u, cf )

  13Using the more common notation of panel data models, where N is the number of individuals (groups) and T is

the number of time periods (individuals within the group), Assumption 3 would take the form: N 2/3 (log T )/T → 0
as N → ∞.
10                                 CHETVERIKOV, LARSEN, AND PALMER

and |fg0 (z1g
           0 α (u))| ≥ c . (ii) For all u ∈ U and g = 1, . . . , G, f (y) ≤ C for all y ∈ B (u, c ) and
              g         f                                            g       f             g     f
     0 α (u)) ≥ c .
fg (z1g g        f

A8 (Quantile indices). The set of quantile indices U is a compact set included in (0, 1).

     Assumption 4(i) requires that both individual and group-level observable covariates zig and xg are
bounded. Assumption 4(ii) is a familiar identification condition in regression analysis. Assumption
5 is a mild continuity condition. Assumption 6(i) requires sufficient integrability of the noise
εg (u), which is a mild regularity condition. In fact, under Assumption 6(iii), which is also a mild
continuity condition, Assumption 6(i) is satisfied as long as E[|εg (u)|4+cM ] ≤ CM for some u ∈ U
(with a possibly different constant CM ). Assumption 6(ii) is trivially satisfied if the pairs (wg , εg )
are i.i.d. across g. Assumption 7 is a mild regularity condition that is typically imposed in the
quantile regression analysis. Finally, Assumption 8 excludes quantile indices that are too close to
either 0 or 1 (when the quantile index u is close to either 0 or 1, one obtains a so called extremal
quantile model, which requires a rather different analysis; see, for example, Chernozhukov (2005)
and Chernozhukov and Fernández-Val (2011)).

4.2. Results. We now present our main results. We start by deriving the asymptotic distribution
of our estimator in Theorem 1. Further, we show how to estimate the asymptotic covariance of
our estimator in Theorem 2. Finally, we demonstrate how to obtain uniform over u ∈ U confidence
bands for the parameter of interest {β(u), u ∈ U } via a multiplier bootstrap method in Theorem
3. The first theorem derives the asymptotic distribution of our estimator.

Theorem 1 (Asymptotic Distribution). Let Assumptions 1-8 hold. Then
                           √
                             G(β̂(·) − β(·)) ⇒ G(·), in `∞ (U)

where G(·) is a zero-mean Gaussian process with uniformly continuous sample paths and covariance
                                                             −1
function C(u1 , u2 ) = SJ(u1 , u2 )S 0 where S = Qxw Q−1  0
                                                      ww Qxw     Qxw Q−1
                                                                      ww , Qxw and Qww appear in
Assumption 2, and J(u1 , u2 ) in Assumption 6.

Remark 1. (i) This is our main convergence result that establishes the asymptotic behavior of our
estimator. Note that we provide the joint asymptotic distribution of our estimator for all u ∈ U.
In addition, Theorem 1 implies that for any u ∈ U,
                                  √
                                    G(β̂(u) − β(u)) ⇒ N (0, V )

where V = SJ(u, u)S 0 , which is the asymptotic distribution of the classical 2SLS estimator.
     (ii) In order to establish the joint asymptotic distribution of our estimator for all u ∈ U, we have
to deal with G independent quantile processes {α̂g,1 (u) − αg,1 (u), u ∈ U}. Since G → ∞, classical
functional central limit theorems do not apply. Therefore, we employ a non-standard but powerful
Bracketing by Gaussian Hypotheses Theorem, which is also related to majorizing measures for
Gaussian processes; see Theorem 2.11.11 in Van der Vaart and Wellner (1996).
                      IV QUANTILE REGRESSION FOR GROUP-LEVEL TREATMENTS                                   11

  (iii) Since quantile regression estimators are biased in finite samples, our estimator α̂g,1 (u) of
αg,1 (u) does not necessarily satisfy E[(α̂g,1 (u) − αg,1 (u))wg ] = 0. For this reason, our estimator
β̂(u) of β(u) is not consistent if Ng is bounded from above uniformly over g = 1, . . . , G and
G ≥ 2. We note, however, that quantile estimators are asymptotically unbiased, and so we use the
Bahadur representation of quantile estimators to derive weak condition on the growth of NG =
min1≤g≤G Ng relative to G, so that consistent estimation of β(u) is indeed possible. Specifically,
we prove consistency and asymptotic zero-mean normality under Assumption 3 that states that
G2/3 (log NG )/NG → 0 as G → ∞, which is a mild growth condition. In principle, it is also possible
to consider bias correction of the quantile regression estimators. This would further relax the growth
condition on NG relative to G at the expense of stronger side assumptions and more complicated
estimation procedures.
  (iv) The requirement that NG → ∞ as G → ∞ is in contrast with the classical results of Hausman
and Taylor (1981) on estimation of panel data mean regression model. The main difference is
that the fixed effect estimator in the panel data mean regression model is unbiased even in finite
samples leading to consistent estimators of the effects of group-level covariates with the number of
observations per group being fixed.                                                                       
  The result in Theorem 1 derives asymptotic behavior of our estimator. In order to perform
inference, we also need an estimator of the asymptotic covariance function. We suggest using an
           ˆ ·) that is defined for all u1 , u2 ∈ U as
estimator C(·,

                                                         ˆ 1 , u2 )Ŝ 0
                                         ˆ 1 , u2 ) = Ŝ J(u
                                         C(u

where
                                  G
                ˆ 1 , u2 ) = 1
                               X                                                             
                J(u               (α̂g,1 (u1 ) − x0g β̂(u1 ))(α̂g,2 (u2 ) − x0g β̂(u2 ))wg wg0 ,
                             G
                                 g=1

Ŝ = (Q̂xw Q̂−1   0    −1       −1              0                       0
             ww Q̂xw ) Q̂xw Q̂ww , Q̂xw = X W/G, and Q̂ww = W W/G. In the theorem below, we
            ˆ 1 , u2 ) is consistent for C(u1 , u2 ) uniformly over u1 , u2 ∈ U.
show that C(u

                                                          ˆ 1 , u2 ) − C(u1 , u2 )k = op (1)
Theorem 2 (Estimating C). Let Assumptions 1-8 hold. Then kC(u
uniformly over u1 , u2 ∈ U.

Remark 2. Theorems 1 and 2 can be used for hypothesis testing concerning β(u) for a given
quantile index u ∈ U. In particular, we have that
                             √
                                 ˆ u)−1/2 (β̂(u) − β(u)) ⇒ N (0, 1).
                               GC(u,                                                                    (5)

Importantly for applied researchers, Theorems 1 and 2 demonstrate that heteroskedasticity-robust
standard errors for our estimator can be obtained by the traditional White (1980) standard errors
where we proceed as if α̂g,1 (u) were equal to αg,1 (u), that is, as if there were no first-stage estimation
error. Appendix E extends this result for clustered standard errors.                                      
12                                     CHETVERIKOV, LARSEN, AND PALMER

     Finally, we show how to obtain confidence bands for β(u) that hold uniformly over U.14 Observe
that β(u) is a dx -vector, that is, β(u) = (β1 (u), . . . , βdx (u))0 . Without loss of generality, we focus
on β1 (u), the first component of β(u). Let β̂1 (u), V (u), and V̂ (u) denote the first component of
                                                                      ˆ u), respectively. Define
β̂(u), the (1, 1) component of C(u, u), and the (1, 1) component of C(u,
                                        √
                               T = sup G|V̂ (u)−1/2 (β̂1 (u) − β1 (u))|,                         (6)
                                           u∈U

and let c1−α denote the (1 − α) quantile of T . Then uniform confidence bands of level α for β1 (u)
could be constructed as
                                                  s                             s         
                                 β̂1 (u) − c1−α       V̂ (u)                        V̂ (u) 
                                                              , β̂1 (u) + c1−α                .                         (7)
                                                         G                             G

These confidence bands are infeasible, however, because c1−α is unknown. We suggest estimating
c1−α by the multiplier bootstrap method. To describe the method, let 1 , ..., G be an i.i.d. se-
                                                                                  S denote the 1st
quence of N (0, 1) random variables that are independent of the data. Also, let ŵg,1
component of the vector Ŝwg . Then the multiplier bootstrap statistic is
                                                          G
                                           1      X                                  
                             T M B = sup q            g (α̂g,1 (u) − x0g β̂(u))ŵg,1
                                                                                  S
                                     u∈U  GV̂ (u) g=1
The multiplier bootstrap critical value ĉ1−α is the conditional (1 − α) quantile of T M B given the
data. Then a feasible version of uniform confidence bands is given by equation (7) with ĉ1−α
replacing c1−α . The validity of the method is established in the following theorem using the results
of Chernozhukov, Chetverikov, and Kato (2013).

Theorem 3 (Uniform Confidence Bands via Multiplier Bootstrap). Let Assumptions 1-8 hold. In
addition, suppose that all eigenvalues of J(u, u) are bounded away from zero uniformly over u ∈ U.
Then
                                         s                              s                       
                                               V̂ (u)                         V̂ (u) 
           P β1 (u) ∈ β̂1 (u) − ĉ1−α               , β̂1 (u) + ĉ1−α                for all u ∈ U  → 1 − α
                                                 G                              G

as G → ∞.

Remark 3. Uniform confidence bands are typically larger than the point-wise confidence bands
based on the result (5). The reason is that uniform confidence bands are constructed so that
the whole function {β(u), u ∈ U } is contained in the bands with approximately 1 − α probability
whereas point-wise bands are constructed so that for any given u ∈ U, β(u) is contained in the
bands with approximately 1 − α probability. Which confidence bands to use depends on the specific
purposes of the researcher.                                                                                              
     14In addition, Appendix C presents an approach for uniform inference on the {α (u)} in the model (1)–(2). In
                                                                                   g,1
                                                 l          r
particular, we construct the confidence bands [α̂g,1 (u), α̂g,1 (u)] that cover the true group-specific effects αg,1 for all
g = 1, . . . , G simultaneously with probability approximately 1 − α.
                        IV QUANTILE REGRESSION FOR GROUP-LEVEL TREATMENTS                                       13

    5. The effect of Chinese import competition on the local wage distribution

5.1. Background on wage inequality. Over the past 40 years, wage inequality within the United
States has increased drastically.15 Economists have engaged in heated debates about the primary
causes of the rising wage inequality—such as globalization, skill-biased technological change, or
the declining real minimum wage—and how the importance of these factors has changed over the
years.16 Recent work in Autor, Dorn, and Hanson (2013) (hereafter ADH) focused on import
competition and its effects on wages and employment in US local labor markets. ADH studied the
period 1990–2007, when the share of US spending on Chinese imports increased dramatically from
0.6% to 4.6%. For identification, the authors used spatial variation in manufacturing concentration,
showing that localized US labor markets which specialize in manufacturing were more affected by
increased import competition from China. The authors found that those markets which were more
exposed to increased import competition in turn had lower employment and lower wages.
  We contribute to this debate by studying the effect of increased trade, in the form of increased
import competition, on the distribution of local wages (rather than on the average local wages as in
ADH). Given that we exploit the same variation in import competition as in ADH, we first describe
the ADH framework below and then present our results.

5.2. Framework of Autor, Dorn, and Hanson (2013). To study the effect of Chinese import
competition on average domestic wages, ADH used Census microdata to calculate the mean wage
within each Commuting Zone (CZ) in the United States.17 The authors then estimated the following
regression:

                                     ∆ln wg = β1 ∆IP WgU + Xg0 β2 + εg                                         (8)

where ∆ln wg is the change in average individual log weekly wage in a given CZ in a given decade,
Xg are characteristics of the CZ and decade, including indicator variables for each decade. Note
that we have changed the notation slightly from that in ADH in order to improve clarity for our
application—a “group” g in this setting is a given CZ in a given decade. The variable of interest is
∆IP WgU , which represents the decadal change in Chinese imports per US worker for the CZ and
decade corresponding to group g.18

  15Autor, Katz, and Kearney (2008) documented that, from 1963 to 2005, the change in wages for the 90th percentile

earner was 55% higher than for the 10th percentile earner.
   16
      See, for example, Leamer (1994), Krugman (2000), Feenstra and Hanson (1999), Katz and Autor (1999), as well
as many other papers cited in Feenstra (2010) or in Haskel, Lawrence, Leamer, and Slaughter (2012).
   17
      The United States is covered exhaustively by 722 Commuting Zones (Tolbert and Sizer 1996), each roughly
corresponding to a local labor market.
   18Due to data limitations, ADH proxy for the change in actual local imports per worker with the weighted average

of industry-level changes in the value of Chinese imports to the US with the weights corresponding to the beginning
of decade employment share of each industry in each CZ.
14                                   CHETVERIKOV, LARSEN, AND PALMER

     To address endogeneity concerns (i.e. that imports from China may be correlated with unob-
served labor demand shocks), the authors instrumented for imports per last-period worker using
∆IP WgO , a measure of import exposure that replaces the change in Chinese imports to the US in
a given industry with the change in Chinese imports to other similarly developed nations for the
same industry and uses one decade lagged employment shares in calculating the weighted average.
Using this 2SLS approach, the authors found that a $1,000 increase in Chinese imports per worker
in a CZ decreases average log weekly wage by -0.76 log points, corresponding to decrease in wages
for the average CZ of 0.9% from 1990–2000 and 1.4% from 2000–2007. When estimated separately
by gender, the effect was more negative for males (-0.89 log points) and less so for females (-0.61
log points).19


5.3. Distributional effects of increased import competition. We build on the ADH frame-
work to analyze whether low-wage earners were more adversely affected than high-wage earners by
Chinese import competition. To apply the grouped IV quantile regression estimator to this setting,
we replace ∆ln wg , the change in the average log weekly wage in equation (8) with ∆ ln wgu , the
change in the u-quantile of log wages in the CZ and decade corresponding to group g. We calculate
these quantiles using micro-level observations from the Census Integrated Public Use Micro Samples
for 1990 and 2000 and the American Community Survey for 2006-2008, matching these observations
to CZs following the strategy described in ADH.20 We instrument for ∆IP WgU using ∆IP WgO as
described above. Recall that existing methods for handling endogeneity in quantile models are
suited for the case where the individual-level unobserved conditional quantile itself is correlated
with the treatment and would be inconsistent in this setting because the endogeneity consists of a
group-level treatment being correlated with the group-level unobservable additive term.
     Figures 1, 2, and 3 display the results of the grouped IV quantile regression estimator for the
full sample, for males only, and for females only. Each figure displays u-quantile estimates for
u ∈ {0.05, 0.1, ..., 0.95}, along with pointwise 95% confidence bands about each estimate. The
figures also display the 2SLS effect found in ADH and 95% confidence intervals corresponding to
their IV estimate of Chinese import penetration on the change in CZ-level average wages.


     19As discussed by ADH, the existence of an extensive-margin labor supply response—imports affecting whether

individuals are employed—makes these results likely a lower-bound for the effect on all workers because we don’t
observe wages for the unemployed population.
   20The thought experiment behind the asymptotics in this application is that the estimator is consistent as the

number of groups (G = 722 CZs × two decades) and the number of individuals within each group (NG = 543,
the size of the smallest group) both grow large. We follow ADH by clustering at the state level and weighting by
start-of-decade CZ population in the second stage of our estimator. To cluster, we are relying on Appendix E, which
relaxes Assumption 1 to allow for observations to be dependent across groups. We also follow the ADH individual
weighting procedure in the first stage given that not all individuals can be mapped to a unique CZ.
                        IV QUANTILE REGRESSION FOR GROUP-LEVEL TREATMENTS                                           15

  Each figure provides evidence that Chinese import competition affected the wages of low-wage
earners more than high-wage earners, demonstrating how increases in trade can causally exacer-
bate local income inequality. For all three samples, the magnitude of the estimated causal effect
of Chinese import penetration is much larger for lower quantiles of the conditional wage distribu-
tion. The point estimates suggest that the average negative effect of Chinese import penetration
estimated by ADH is primarily driven by large negative effects for those in the bottom tercile,
where the effect is twice as large as the average effect.21 Wages not in the bottom tercile were less
affected than the average—Figure 1 shows that for most wage-earners (from the 0.35 quantile and
above) the effect of Chinese import competition was one-third smaller in magnitude than the effect
on the average estimated by ADH. Comparing the pattern of the coefficients across two gender
subsamples in Figures 2 and 3, there is more distributional heterogeneity for females than males,
a finding that additional testing shows is even more pronounced for non-college educated females.
For each sample, we can reject an effect size of zero for almost all quantiles below the median but
cannot for all quantiles above the median.




  21A coefficient of -1.4 log points, e.g. for the lower quantiles of Figure 1, corresponds to a 2.6% decrease in wages

from 2000–2007 for the average commuting zone’s change in Chinese import exposure.
16                                CHETVERIKOV, LARSEN, AND PALMER

                                             References

Abadie, A., Angrist, J., and Imbens, G. (2002). Instrumental variables estimates of the effect of
     subsidized training on the quantiles of trainee earnings. Econometrica, 70, 91–117.
Abrevaya, J. and Dahl, C. (2008). The effects of birth inputs on birthweight. Journal of Business
     and Economic Statistics, 26, 379–397.
Altonji, J. and Matzkin, R. (2005). Cross section and panel data estimators for non separable
     models with endogenous regressors. Econometrica, 73, 1053–1102.
Angrist, J. and Lang, K. (2004). Does school integration generate peer effects? Evidence from
     Boston’s Metco Program. American Economic Review, 94, 1613–1634.
Angrist, J., Chernozhukov, V., and Fernandez-Val, I. (2006). Quantile regression under misspecifi-
     cation, with an application to the US wage structure. Econometrica, 74, 539–563.
Arellano, M. and Bonhomme, S. (2013). Random Effects Quantile Regression. Working paper.
Autor, D., Dorn, D., and Hanson, G. (2013). The China syndrome: Local labor market effects of
     import competition in the United States. American Economic Review, 103(6), 2121–2168.
Autor, D. H., Katz, L. F., and Kearney, M. S. (2008). Trends in U.S. wage inequality: Revising the
     revisionists. Review of Economics and Statistics, 90, 300–323.
Backus, M. (2014). Why is productivity correlated with competition? Working paper.
Baum-Snow, N. (2007). Did highways cause suburbanization? Quarterly Journal of Economics,
     122, 775-805.
Belloni, A., Chernozhukov, V., and Hansen, C. (2006). Conditional quantile processes based on
     series or many regressors. Working paper.
Canay, I. (2011). A simple approach to quantile regression for panel data. Econometrics Journal,
     14, 368–386.
Chamberlain, G. (1994). Quantile regression, censoring, and the structure of wages. Advances in
     Econometrics, Sixth World Congress, 171–209.
Chernozhukov, V. (2005). Extremal quantile regression. The Annals of Statistics, 33, 806–839.
Chernozhukov, V., Chetverikov, D., and Kato, K. (2013). Gaussian approximations and multiplier
     bootstrap for maxima of sums of high-dimensional random vectors. The Annals of Statistics, 41,
     2786–2819.
Chernozhukov, V., Chetverikov, D., and Kato, K. (2014a). Gaussian approximation of suprema of
     empirical processes. The Annals of Statistics, 42, 1564-1597.
Chernozhukov, V., Chetverikov, D., and Kato, K. (2014b). Anti-concentration and honest adaptive
     confidence bands. The Annals of Statistics, forthcoming.
Chernozhukov, V., Chetverikov, D., and Kato, K. (2014c). Comparison and anti-concentration
     bounds for maxima of Gaussian random vectors. Probab. Theory Related Fields, forthcoming.
     Available at arXiv:1301.4807v3.
                     IV QUANTILE REGRESSION FOR GROUP-LEVEL TREATMENTS                             17

Chernozhukov, V., Chetverikov, D., and Kato, K. (2014d). Central limit theorems and bootstrap
  in high dimensions. Available at arXiv:1412.3661v2.
Chernozhukov, V. and Fernández-Val, I. (2011). Inference for extremal conditional quantile models,
  with an application to market and birthweight risks. The Review of Economic Studies.
Chernozhukov, V. and Hansen, C. (2005). An IV model of quantile treatment effects. Econometrica,
  73, 245–261.
Chernozhukov, V. and Hansen, C. (2006). Instrumental quantile regression inference for structural
  and treatment effect models. Journal of Econometrics, 132, 491–525.
Chernozhukov, V. and Hansen, C. (2008). Instrumental variable quantile regression: A robust
  inference approach. Journal of Econometrics, 142, 379–398.
Chesher, A. (2003). Identification in nonseparable models. Econometrica, 71, 1405–1441.
Feenstra, R. C. (2010). Offshoring in the global economy: Microeconomic structure and macroeco-
  nomic implications. Cambridge, MA: MIT Press.
Feenstra, R. C. and Hanson, G. H. (1999). The Impact of outsourcing and high-technology capital
  on wages: Estimates for the U.S., 1979–1990. Quarterly Journal of Economics, 114(3), 907–940.
Florens, J. P., Heckman, J., Meghir, C., and Vytlacil, E. (2008). Identification of treatment ef-
  fects using control functions in models with continuous endogenous treatment and heterogeneous
  effects. Econometrica, 76, 1191–1206.
Galvao, A. (2011). Quantile regression for dynamic panel data with fixed effects. Journal of Econo-
  metrics, 164, 142–157.
Galvao, A. and Wang, L. (2013). Efficient minimum distance estimator for quantile regression fixed
  effects panel data. Working paper.
Graham, B. and Powell, J. (2012). Identification and estimation of average partial effects in “irreg-
  ular” correlated random coefficient panel data models. Econometrica, 80, 2105–2152.
Hahn, J. and Meinecke, J. (2005). Time-invariant regressor in nonlinear panel model with fixed
  effects. Econometric Theory, 21, 455–469.
Haskel, J., Lawrence, R., Leamer, E. E., and Slaughter, M. J. (2012). Globalization and U.S. wages:
  Modifying classic theory to explain recent facts. Journal of Economic Perspectives, 26(2): 119–
  40.
Hausman, J. (2001). Mismeasured variables in econometric analysis: Problems from the right and
  problems from the left. Journal of Economic Perspectives, 15, 57–67.
Hausman, J. and Taylor, W. (1981). Panel data and unobservable individual effects. Econometrica,
  49, 1377–1398.
Hausman, J., Luo, Y., and Palmer, C. (2014). Errors in the dependent variable of quantile regression
  models. Working paper.
Heckman, J. and Vytlacil, E. (1998). Instrumental variables methods for the correlated random
  coefficient model: Estimating the average rate of return to schooling when the return is correlated
18                                  CHETVERIKOV, LARSEN, AND PALMER

     with schooling. The Journal of Human Resources, 33, 974-987.
Imbens, G. and Angrist, J. (1994). Identification and estimation of local average treatment effects.
     Econometrica, 62, 467-475.
Imbens, G. and Newey, W. (2009). Identification and estimation of triangular simultaneous equa-
     tions models without additivity. Econometrica, 77, 1481–1512.
Kato, K. and Galvao, A. (2011). Smoothed quantile regression for panel data. Working paper.
Kato, K., Galvao, A., and Montes-Rojas, G. (2012). Asymptotics for panel quantile regression
     models with individual effects. Journal of Econometrics, 170, 76–91.
Katz, L. F. and Autor, D. (1999). Changes in the wage structure and earnings inequality, in Orley
     Ashenfelter and David Card, eds., Handbook of Labor Economics, Vol. 3A, Amsterdam: Elsevier
     Science, 1463–1555.
Koenker, R. (2004). Quantile regression for longitudinal data. Journal of Multivariate Analysis, 91,
     74–89.
Koenker, R. (2005). Quantile regression. Econometric Society Monographs.
Koenker, R. and Bassett, J. (1978). Regression quantiles. Econometrica, 46, 33–50.
Krugman, P. (2000). Technology, trade and factor prices. Journal of International Economics ,
     50(1), 51–71.
Lamarche, C. (2010). Robust penalized quantile regression estimation for panel data. Journal of
     Econometrics, 157, 396–408.
Larsen, B. (2014). Occupational licensing and quality: Distributional and heterogeneous effects in
     the teaching profession. Working paper.
Leamer, E. E. (1994). Trade, wages and revolving door ideas. NBER working paper no. 4716.
Lee, S. (2007). Endogeneity in quantile regression models: A control function approach. Journal of
     Econometrics, 141, 1131–1158.
Lehmann, E. and Romano, J. (2005). Testing statistical hypotheses. Springer Texts in Statistics.
Masten, M. and Torgovitsky, A. (2014). Instrumental variables estimation of a generalized correlated
     random coefficients model. Working paper.
Palmer, C. (2011). Suburbanization and urban decline. Working paper.
Ponomareva, M. (2011). Identification in quantile regression panel data models with fixed effects
     and small T . Working paper.
Powell, J. (1984). Least absolute deviations estimation for the censored regression model. Journal
     of Econometrics, 25, 303-325.
Rosen, A. (2012). Set identification via quantile restrictions in short panels. Journal of Economet-
     rics, 166, 127–137.
Tao, T. (2012). Topics in random matrix theory. American Mathematica Society.
Tolbert, C. M. and Sizer, M. (1996). U.S. commuting zones and labor market areas. A 1990 update.
     Economic Research Service Staff Paper No. 9614.
                      IV QUANTILE REGRESSION FOR GROUP-LEVEL TREATMENTS                          19

Van der Vaart, A. (1998). Asymptotic Statistics. Cambridge University Press.
Van der Vaart, A. and Wellner, J. (1996). Weak convergence and empirical processes. Springer
  Series in Statistics.
White, H. (1980). A Heteroskedasticity-consistent covariance matrix estimator and a direct test for
  heteroskedasticity. Econometrica, 48, 817–838.
20                                                    Figures



        Figure 1. Effect of Chinese Import Competition on Conditional Wage Distribution:
        Full Sample




Notes: Figure plots grouped IV quantile regression estimates of the effect of a $1,000 increase in Chinese imports
per worker on the conditional wage distribution (β1 in equation (8) in the text when the change in average log
wages for the commuting zone and decade corresponding to group g, ∆ln wg , is replaced with the change in the
u-quantile of log wages ∆ ln wgu ). The dashed horizontal line is the ADH estimate of β1 in equation (8). 95%
pointwise confidence intervals are constructed from robust standard errors clustered by state and observations are
weighted by CZ population, as in ADH. Units on the vertical axis are log points.
                                                       Figures                                                    21



        Figure 2. Effect of Chinese Import Competition on Conditional Wage Distribution:
        Males Only




Notes: Figure plots grouped IV quantile regression estimates for the male-only sample of the effect of a $1,000
increase in Chinese imports per worker on the male conditional wage distribution (β1 in equation (8) in the text
when the change in average log wages for the commuting zone and decade corresponding to group g, ∆ln wg , is
replaced with the change in the u-quantile of log wages ∆ ln wgu ). The dashed horizontal line is the ADH estimate of
β1 in equation (8). 95% pointwise confidence intervals are constructed from robust standard errors clustered by
state and observations are weighted by CZ population, as in ADH. Units on the vertical axis are log points.
22                                                     Figures



        Figure 3. Effect of Chinese Import Competition on Conditional Wage Distribution:
        Females Only




Notes: Figure plots grouped IV quantile regression estimates for the female-only sample of the effect of a $1,000
increase in Chinese imports per worker on the female conditional wage distribution (β1 in equation (8) in the text
when the change in average log wages for the commuting zone and decade corresponding to group g, ∆ln wg , is
replaced with the change in the u-quantile of log wages ∆ ln wgu ). The dashed horizontal line is the ADH estimate of
β1 in equation (8). 95% pointwise confidence intervals are constructed from robust standard errors clustered by
state and observations are weighted by CZ population, as in ADH. Units on the vertical axis are log points.
                                                        Appendix                                                   23

                    Appendix A. Examples of Grouped IV Quantile Regression

   To help the reader envision applications of our estimator, in this section, we provide several
motivating examples of settings for which our estimator may be useful. Example 2 also provides
additional discussion of computational advantages of our estimator. Note that each of the following
examples involves estimation of a treatment effect that varies at the group level with all endogeneity
concerns also existing only at the group level.22

Example 1: Peer Effects of School Integration. Angrist and Lang (2004) studied how sub-
urban student test scores were affected by the reassignment of participating urban students to
suburban schools through Boston’s Metco program. Before estimating their main instrumental
variables model, the authors tested for a relationship between the presence of urban students in
the classroom and the second decile of student test scores by estimating

                     Qyigjt |mgjt ,sgjt ,ξgjt ,αg ,βj ,γt (0.2) = αg + βj + γt + δmgjt + λsgjt + ξgjt             (9)

where the left-hand-side represents the second decile of student test scores within a group, where
each group is a grade g × school j × year t cell. The variables sgjt and mgjt denote the class size
and the fraction of Metco students within each g × j × t cell, and αg , βj , and γt represent grade,
school, and year effects. The unobserved component, ξgjt , is analogous to the εg (0.2) of the special
form (3) of our model (1)–(2).
   Angrist and Lang (2004) estimated equation (9) by OLS, which is equivalent to the non-IV
application of our estimator with no micro-level covariates. Similar to their OLS results on average
test scores, they found that classrooms with higher proportions of urban students have lower second
decile test scores. Once they instrumented for a classroom’s level of Metco exposure, the authors
found no effect on average test scores. However, by not estimating model (9) by 2SLS, they were
unable to address the causal distributional effects of Metco exposure.
   In estimating (9), Angrist and Lang (2004) used heteroskedasticity-robust standard errors, which
we demonstrate in Section 4 is valid. The extension in Appendix E implies that the authors
could have instead allowed for clustering across groups in computing standard errors (for example,
clustering at the school level).

Example 2: Occupational Licensing and Quality. Larsen (2014) applied the estimator devel-
oped in this paper to study the effects of occupational licensing laws on the distribution of quality
within the teaching profession. This application uses a difference-in-differences approach. Similar
to Example 1, the explanatory variable of interest is treated as exogenous and the researcher is
concerned that there may be unobserved group-level disturbances. In this application, a group is
   22
        This is in contrast to settings where the endogeneity exists at the individual level, i.e. when the individual
unobserved heterogeneity is correlated with treatment. Such situations require a different approach than the one
presented here, e.g. Chernozhukov and Hansen (2005), Abadie, Angrist, and Imbens (2002), or other approaches
referenced in Section 1.
24                                                  Appendix

a state-year combination (s, t), and micro-level data consists of teachers within a particular state
in a given year.
     Let qist represent the quality of teacher i in state s who began teaching in survey year t, where
quality is proxied for by a continuous measure of the selectivity of the teacher’s undergraduate
institution. The conditional uth quantile of quality is modeled as

                                                                       0
                          Qqist |Lawst ,εst (u) = γs (u) + λt (u) + Lawst δ(u) + εst (u)              (10)

where γs is a state effect; λt is a year effect; Lawst is a three-element vector containing dummies
equal to one if a subject test, basic skills test, or professional knowledge test was required in state
s in year t; and εst (u) represents group-level unobservables.
     Because no micro-level covariates are included, the first stage of the grouped quantile estimator
is obtained by simply selecting the uth quantile of quality in a given state-year cell. The second
stage is obtained via OLS, and the author used heteroskedasticity-robust standard errors, which
are valid by the results in Section 4. Using the grouped quantile estimator, Larsen (2014) found
that, for first-year teachers, occupational licensing laws requiring teachers to pass a subject test
lead to a small but significant decrease in the upper tail of quality, suggestive that these laws may
drive some highly qualified candidates from the occupation.
     In this setting, if micro-level covariates, zist , were included in the first stage of estimation, the
researcher could also estimate interaction effects of the group-level treatment and a micro-level
covariate, such as the percent of minority students at the teacher’s school. This would be done by
1) estimating quantile regression of qist on zist (which includes the percent of minority students
measure) separately for each (s, t) group and 2) saving each group-level estimate for the coefficient
corresponding to the percent minority variable, and then estimating a linear regression of these
coefficients on Lawst and on the state and year fixed effects.
     This example highlights another useful feature of grouped IV quantile regression. Including many
variables in a standard quantile regression drastically increases the computational time (see Koenker
(2004), Lamarche (2010), Galvao and Wang (2013), and Galvao (2011) for further discussion) and,
in our experience, can often lead standard optimization packages to fail to converge. The grouped
quantile approach, on the other hand, can handle large numbers of variables easily when these
variables happen to be constant within group, as in the case of state and year fixed effects in this
example, because the coefficients corresponding to these variables can be estimated in the second-
stage linear model, greatly reducing the number of parameters to be estimated in the nonlinear
first stage and hence reducing the computational burden significantly. Furthermore, this specific
computational advantage of the grouped quantile regression estimator exists even in cases where
both standard quantile regression and the grouped approach are valid (i.e. when no group-level
unobservables are present). Larsen (2014) found that the grouped approach was significantly faster
than estimating parameters in a single quantile regression.
                                                    Appendix                                            25

Example 3: Distributional Effects of Suburbanization. Palmer (2011) applied the grouped
quantile estimator to study the effects of suburbanization on resident outcomes. This application
illustrates the use of our estimator in an IV setting. In this application, a group is a metropolitan
statistical area (MSA), and individuals are MSA residents. As an identification strategy, Palmer
(2011) used the results of Baum-Snow (2007) in instrumenting suburbanization with planned high-
ways.23
  The model is

                  ∆Qyigt |xg ,sg ,εg (u) = β(u) · suburbanizationg + x0g γ1 (u) + εg (u)
                  suburbanizationg = π(u) · planned highway raysg + x0g γ2 (u) + vg (u)

where ∆Qy|xg ,sg ,εg (u) is the change in the uth quantile of log wages yigt within an MSA between
1950 and 1990 and xg is a vector of controls (including a constant) conditional upon which planned
highway raysg is uncorrelated with εg (u) and vg (u). The variable suburbanizationg is a proxy
measure of population decentralization, such as the amount of decline of central city population
density. β(u) is the coefficient of interest, capturing the effect of suburbanization on the within-
MSA conditional wage distribution. For example, if the process of suburbanization had particularly
acute effects on the prospects of low-wage workers, we may expect β(u) to be negative for u = 0.1.
For a given u, the grouped IV quantile approach estimates β(u) through a 2SLS regression.

Example 4: The Relationship Between Productivity and Competition. Backus (2014)
studied the relationship between competition and productivity in the ready-mix concrete industry.
The author discussed the fact that competition and productivity are positively correlated, and
studied whether this relationship is similar for firms of all productivity levels (e.g. through encour-
aging better monitoring of firm managers or better investments), or whether increased competition
primarily affects the lower tail of the productivity distribution (driving out less productive firms).
  Let ρimt represent a measure of productivity of firm i in market m and time period t. Using
our notation, define a group as a pair m × t. The author assumes that ρimt satisfies the following
quantile regression model:

                      Qρmt |cmt ,nmt ,εmt (u) = βt (u) + cmt βc (u) + g(nmt , u) + εmt (u)           (11)

where cmt is a group-level measure of competition, nmt is the number of firms in the group, g(nmt , u)
is the third order polynomial of nmt ), and εmt is an unobserved group-level disturbance, which is
possibly correlated with cmt .
  Backus (2014) instrumented for cmt using group-level measures which shift the demand for con-
crete. Thus, the IV regression in (11) represents an application of our estimator when group-level

  23Baum-Snow (2007) instrumented for actual constructed highways with planned highways and estimated that

each highway ray emanating out of a city caused an 18% decline in central-city population.
26                                                Appendix

shocks are endogenous and no micro-level covariates are present. The author found some evi-
dence that the effect of competition on the left tail of the productivity distribution may be more
positive than at some quantiles in the middle of the distribution (consistent with selection of low-
productivity firms out of the industry), but was unable to reject the hypothesis of a constant
effect.

                                      Appendix B. Simulations

     In order to investigate the properties of our estimator and compare to traditional quantile re-
gression, we generate data according to the following model:

                                yig = zig γ(uig ) + δ(u) + xg β(uig ) + εg (uig )                     (12)
                                  xg = πwg + ηg + νg                                                  (13)
                                             u
                              εg (u) = uηg −                                                          (14)
                                             2
where wg , νg , and zig are each distributed exp(0.25∗ N [0, 1]); uig and ηg are both distributed U [0, 1];
and random variables wg , νg , zig , uig , and ηg are mutually independent. Note that the form
                  u
εg (u) = uηg −    2   implies E[εg (u)|wg ] = E[uηg − u/2|wg ] = E[uηg − u/2] = u/2 − u/2 = 0. The
quantile coefficient functions are γ(u) = β(u) = u1/2 and δ(u) = u/2. The parameter π = 1.
     We employ three variants of the data generating process described in (12)–(14). The first case is
exactly as in (12)–(14), with the group-level treatment of interest, xg , being endogenous (correlated
with εg through ηg ). We estimate β(u) in this case using the grouped IV quantile estimator as well
as standard quantile regression (which ignores the endogeneity as well as the existence of εg ). In
the second case xg is exogenous, where we set xg = wg in (13). We estimate β(u) again in this
case using the grouped quantile approach as well as standard quantile regression, where the latter
ignores the existence of εg . In the third case xg is exogenous and no group-level unobservables are
included, where we set xg = wg and εg = 0. In this latter case, both grouped quantile regression
and standard quantile regression should be consistent.
     We perform these exercises with the number of groups (G) and the number of observations per
group (N ) given by (N, G) =(25,25), (200,25), (25,200), (200,200). 1,000 Monte Carlo replications
were used. The results are displayed in Table I. Each panel displays the bias from the procedure
for each decile (u = 0.1, ..., 0.9) as well as the average absolute value of that bias, averaged over the
nine deciles.
     The top panel of Table I demonstrates that in the endogenous group-level treatment case the
magnitude of the bias is much smaller in our estimator than in standard quantile regression, and the
bias of our estimator disappears as N and G increase, while the bias of quantile regression remains
constant (0.196 on average). The middle panel considers the case where xg is exogenous but group-
level unobservables are present (or, equivalently, left-hand-side measurement error exists in the
quantile regression). At some quantiles, standard quantile regression has a bias which is smaller in
                                                      Appendix                                               27

magnitude than the grouped approach, in particular in the cases where N = 25. However, as N
increases, the magnitude of the bias of the grouped estimator falls close to zero on average while
that of standard quantile regression remains about three times as high at 0.01. Finally, the bottom
panel focuses on the case in which no group-level unobservables exist and hence standard quantile
regression is unbiased. In this case, we find that the bias of standard quantile regression is indeed
lower than that of the grouped quantile approach, but the bias of the grouped quantile method
also diminishes rapidly as N and G grow.
   To illustrate the computational burden which our estimator overcomes, we redid the first stage
estimation with γ(·) and group-level fixed effects—αg from Section 2—estimated jointly in one large
quantile regression rather than estimating group-by-group quantile regression. We performed 100
replications due to the computational burden of the joint estimation. We found that in the (N, G) =
(25, 25) case the joint estimation took only slightly longer than than the group-by-group approach;
with (N, G) = (200, 25) the group-by-group approach was ten times faster; with (N, G) = (25, 200)
the group-by-group approach was over forty times as fast; and in the (N, G) = (200, 200) the group-
by-group approach was over 150 times as fast, with estimation on a single replication sample for
the nine deciles taking over three minutes, while the the grouped quantile approach performed the
same exercise in 1.22 seconds.24 This exercise illustrates the benefit of the group-by-group approach
to estimating αg and also illustrates that, in general, standard quantile regression can be very slow
when a large number of explanatory variable is included. The grouped quantile approach can
greatly reduce this computational burden by handling all group-level explanatory variables linearly
in the second stage (implying that the grouped quantile approach can be especially beneficial if the
dimension of xg is large).


                   Appendix C. Joint Inference on Group-Specific Effects

   In this section, we are concerned with inference on group-specific effects αg,1 (u), g = 1, . . . , G,
in the model (1)-(2) defined in Section 2. In particular, we are interested in constructing the
                    l , α̂r ] for α (u) that are adjusted for multiplicity of the effects, that is, we
confidence bands [α̂g,1   g,1      g,1
would like to have the bands satisfying

                                            l       r
                           P (αg,1 (u) ∈ [α̂g,1 , α̂g,1 ] for all g = 1, . . . , G) → 1 − α.               (15)

                              l , α̂r ] cover the true group-specific effects α
Thus, the confidence bands [α̂g,1   g,1                                        g,1 for all g = 1, . . . , G
simultaneously with probability approximately 1 − α.
   The main challenge here is that we have G parameters αg,1 (u), g = 1, . . . , G, and only Ng obser-
vations to estimate αg,1 where Ng is potentially smaller than G (recall that we impose Assumption

   24With G > 200, the computation time ratio drastically increases further, with standard optimization packages

often failing to converge appropriately.
28                                                      Appendix

3, according to which G2/3 (log NG )/NG → 0 as G → ∞ where NG = ming=1,...,G Ng ). To decrease
technicalities, in this section we assume that U = {u}, that is, U is a singleton.
                                                 1/2
     It is well-known that as Ng → ∞, Ng (α̂g,1 (u) − αg,1 (u)) ⇒ N (0, Ig ) where Ig is the (1, 1)th ele-
ment of the matrix u(1 − u)Jg (u)−1 Eg [zig zig
                                             0 ]J (u)−1 ; see, for example, Koenker (2005). Therefore,
                                                 g
letting c1−α be the (1 − α) quantile of |Y | where Y ∼ N (0, 1), we obtain
                       "               s                        s #!
                                          Ig                     Ig
         P αg,1 (u) ∈ α̂g,1 (u) − c1−α       , α̂g,1 (u) + c1−α       → 1 − α as Ng → ∞.                                  (16)
                                          Ng                     Ng

In practice, Ig is typically unknown, however, and has to be estimated from the data. For example,
one can use a method developed in Powell (1984). Letting Iˆg denote a suitable estimator of Ig , it
is standard to show that (16) continues to hold if we replace Ig with Iˆg as long as Iˆg →p Ig .
     The drawback of the confidence bands in (16), however, is that they do not take into account
multiplicity of the effects αg,1 (u), g = 1, . . . , G. This is especially important given that G is large.
To fix this problem, we would like to adjust the constant c1−α in (16) so that the events under the
probability sign in (16) hold simultaneously for all g = 1, . . . , G with probability asymptotically
equal to 1 − α. The theorem below shows that this can be achieved by replacing c1−α with cM
                                                                                          1−α ,
the (1 − α) quantile of max1≤g≤G |Yg | where Y1 , . . . , YG are i.i.d. N (0, 1) random variables. To
decrease technicalities, we assume in the theorem that all Ig ’s are known.

Theorem 4 (Joint Inference on Group-Specific Effects). Let Assumptions 1-8 hold. In addition,
suppose that Ig ≥ cM for all g = 1, . . . , G and N̄G /NG ≤ CM where NG = min1≤g≤G Ng and
N̄G = max1≤g≤G Ng . Let cM
                         1−α be the (1 − α) quantile of max1≤g≤G |Yg | where Y1 , . . . , YG are i.i.d.
N (0, 1) random variables. Then
                  "                        s                           s        #                              !
                                               Ig                          Ig
       P   αg,1 (u) ∈ α̂g,1 (u) −   cM
                                     1−α          , α̂g,1 (u) + cM
                                                                 1−α                for all g = 1, . . . , G       →1−α
                                               Ng                          Ng

as G → ∞.


                               Appendix D. Sub-gaussian Tail Bound

     In this section, we derive the sub-gaussian tail bound for the quantile regression estimator. This
bound plays an important role in deriving the asymptotic distribution of our estimator, which is
given in Theorem 1.

Theorem 5 (Sub-Gaussian Tail Bound for Quantile Estimator). Let Assumptions 1-8 hold. Then
there exist constants c̄, c, C > 0 that depend only on cM , cf , CM , Cf , CL such that for all g = 1, ..., G
and x ∈ (0, c̄),
                                                                 
                                                                          2
                               P        sup kα̂g (u) − αg (u)k > x ≤ Ce−cx Ng .                                           (17)
                                     u∈U
                                                    Appendix                                                   29

Remark 4. The bound provided in Theorem 5 is non-asymptotic. In principle, it is also possible
to calculate the exact constants in the inequality (17). We do not calculate these constants because
they are not needed for our results. Since α̂g,1 (u) is the classical Koenker and Bassett’s (1978)
quantile regression estimator of αg (u), Theorem 5 may also be of independent interest. The theorem
implies that large deviations of the quantile estimator from the true value are extremely unlikely
under our conditions.                                                                                          

                             Appendix E. Clustered Standard Errors

  In this section, we consider the model from the main text, which is defined in equations (1)–(2),
but we seek to relax the independence across groups condition appearing in Assumption 1(i). In
particular, in this section we allow for cluster sampling and derive the results that are analogous
to Theorems 1 - 3 in the main text.
  We assume that the data consist of M = MG clusters of groups, and that there exists a cor-
respondence CG : {1, . . . , M } ⇒ {1, . . . , G} such that (i) for each m = 1, . . . , M , CG (m) denotes
the set of groups corresponding to cluster m, (ii) for m, m0 = 1, . . . , M with m 6= m0 , the set
CG (m) ∩ CG (m0 ) is empty, and (iii) for any g = 1, . . . , G, there exists m = 1, . . . , M such that
g ∈ CG (m). Thus, the correspondence CG (·) partitions groups into M clusters. Using this nota-
tion, we replace Assumption 1 with the following condition:

A10 (Design). (i) Observations are independent across clusters m = 1, . . . , M . (ii) For all g =
1, . . . , G, the pairs (zig , yig ) are i.i.d. across i = 1, . . . , Ng conditional on (xg , αg ). (iii) For each
m = 1, . . . , M , the number of elements in the set CG (m) is bounded from above by some constant
C̄, which is independent of G.

  Assumption 10 (i) relaxes Assumption 1(i) from the main text by requiring independence across
clusters instead of independence across groups. Assumption 10 (ii) is the same as Assumption 1(ii).
Assumption 10 (iii) imposes the condition that the number of groups within each cluster remains
small as the number of groups gets large.
  In addition, we replace Assumption 6 with the following condition:

A60 (Noise). (i) For all g = 1, . . . , G, E[supu∈U |εg (u)|4+cM ] ≤ CM . (ii) For some (matrix-valued)
function J CS : U × U → Rdw ×dw ,
                                                                         
                   M
                1 X          X                   X
                      E         εg (u1 )wg                     εg (u1 )wg0  → J CS (u1 , u2 )
                G
                   m=1         g∈CG (m)                 g∈CG (m)

uniformly over u1 , u2 ∈ U. (iii) For all u1 , u2 ∈ U, |εg (u2 ) − εg (u1 )| ≤ CL |u2 − u1 |.

 Assumptions 60 (i) and 60 (iii) are the same as Assumptions 6(i) and 6(iii). Assumption 60 (ii) is a
modification of Assumption 6(ii) adjusting the asymptotic covariance function of G−1/2 G
                                                                                      P
                                                                                        g=1 εg (·)wg
30                                                      Appendix

to allow for clustering. When CG (m) contains only one group for each m = 1, . . . , M , Assumption
60 (ii) reduces to Assumption 6(ii).
     Like in the classical cross-section cluster sampling setup, allowing for clustering in our model
does not require adjusting the estimator. Therefore, we study the properties of the estimator β̂(u)
of parameter β(u), u ∈ U, defined in Section 3. Our first theorem in this section describes the
asymptotic distribution of β̂(u).

Theorem 6 (Asymptotic Distribution under Cluster Sampling). Let Assumptions 10 , 2-5, 60 , 7,
and 8 hold. Then
                                  √
                                      G(β̂(·) − β(·)) ⇒ GCS (·), in `∞ (U)
where GCS (·) is a zero-mean Gaussian process with uniformly continuous sample paths and covari-
ance function C CS (u1 , u2 ) = SJ CS (u1 , u2 )S 0 where S = (Qxw Q−1  0   −1    −1
                                                                    ww Qxw ) Qxw Qww , Qxw and Qww
appear in Assumption 2, and J CS (u1 , u2 ) in Assumption 60 .

  Next, we discuss how to estimate the covariance function C CS (·, ·) of the limiting Gaussian process
GCS (·). We suggest estimating C CS (·, ·) by CˆCS (·, ·) defined for all u1 , u2 ∈ U as

                                         CˆCS (u1 , u2 ) = Ŝ JˆCS (u1 , u2 )Ŝ 0

where
                                                                                               
                         M
                       1 X   X                                   X
     JˆCS (u1 , u2 ) =        (α̂g,1 (u1 ) − x0g β̂(u1 ))wg     (α̂g,2 (u2 ) − x0g β̂(u2 ))wg0  ,
                       G
                       m=1    g∈CG (m)                                       g∈CG (m)

Ŝ = (Q̂xw Q̂−1     0   −1       −1               0                 0
             ww Q̂xw ) Q̂xw Q̂ww , Q̂xw = X W/G, Q̂ww = W W/G. In the theorem below, we show
that CˆCS (u1 , u2 ) is consistent for C CS (u1 , u2 ) uniformly over u1 , u2 ∈ U.

Theorem 7 (Estimating C CS under Cluster Sampling). Let Assumptions 10 , 2-5, 60 , 7, and 8 hold.
Then kCˆCS (u1 , u2 ) − C CS (u1 , u2 )k = op (1) uniformly over u1 , u2 ∈ U.

     Finally, we show how to obtain confidence bands for β(u) that hold uniformly over U. Observe
that β(u) is a dx -vector, that is, β(u) = (β1 (u), . . . , βdx (u))0 . As in the main text, we focus on β1 (u),
the first component of β(u), and we suggest constructing uniform confidence bands via multiplier
bootstrap method. An important difference from the results in the main text is that now we should
bootstrap on the cluster level.
  Specifically, let β̂1 (u), V CS (u), and V̂ CS (u) denote the 1st component of β̂(u), the (1, 1)st
component of C CS (u, u), and the (1, 1)st component of CˆCS (u, u), respectively. Define
                                               √
                                 T = sup           G|V̂ (u)−1/2 (β̂1 (u) − β1 (u))|,                       (18)
                                         u∈U

and let c1−α denote the (1 − α) quantile of T . As in the main text, we estimate c1−α by the
multiplier bootstrap method. Let 1 , ..., M be an i.i.d. sequence of N (0, 1) random variables that
                                                   Appendix                                            31

                                         S denote the 1st component of the vector Ŝw . Then the
are independent of the data. Also, let ŵg,1                                         g
multiplier bootstrap statistic is
                                                                                   
                                          M
                                    1     X               X
                   T M B = sup q            m           (α̂g,1 (u) − x0g β̂(u))ŵg,1
                                                                                   S 
                           u∈U  GV̂ (u) m=1      g∈CG (m)

The multiplier bootstrap critical value ĉ1−α is the conditional (1 − α) quantile of T M B given the
data. Our final theorem in this section explains how to construct uniform confidence bands using
ĉ1−α .

Theorem 8 (Uniform Confidence Bands via Multiplier Bootstrap under Cluster Sampling). Let
Assumptions 10 , 2-5, 60 , 7, and 8 hold. In addition, suppose that all eigenvalues of J CS (u, u) are
bounded away from zero uniformly over u ∈ U. Then
                                  s                          s                       
                                      V̂ (u)                     V̂ (u) 
       P β1 (u) ∈ β̂1 (u) − ĉ1−α          , β̂1 (u) + ĉ1−α            for all u ∈ U  → 1 − α
                                        G                          G

as G → ∞.

               Appendix F. Further Discussion of the Model in Section 2

   In this section, we provide further discussion of our model in Section 2, give a structural inter-
pretation, and outline possible extensions.

F.1. Structural Model Justifying the Model in Section 2. Consider the following structural
model:
                                                      0
                                               yig = zig α
                                                         eg (e
                                                             uig )                                  (19)

where yig is the response variable of individual i in group g, zig is a vector of observable individual-
level covariates, u                                                                 eg = {e
                  eig is unobserved scalar heterogeneity with values in [0, 1], and α     αg (u), u ∈
[0, 1]} is a group-specific effect. We assume that the group-specific effect α
                                                                             eg is determined by
vectors of observable and unobservable group-level covariates xg and ψg , respectively, that is,
α
eg (u) = α
         e(u, xg , ψg ) for some function α
                                          e.
   In many empirical settings, it is natural to expect that the distribution of u
                                                                                eig varies across groups,
so that the distribution function Fg : [0, 1] → [0, 1] of u
                                                          eig in group g is indexed by g. We assume that
Fg is determined by a vector of unobservable group-level covariates νg , that is, Fg (u) = F (u, νg )
for some function F . Let F −1 (u, νg ) denote the (generalized) inverse of the function u 7→ F (u, νg ).
   Further, we assume that u
                           eig is independent of zig conditional on (xg , ψg , νg ), which can be
considered analogous to the usual independence condition of quantile regression analysis for cross-
sectional data adapted to group/panel data as considered here. Under this condition,

                                           eig = F −1 (uig , νg )
                                           u
32                                                   Appendix

for a random variable uig that is distributed uniformly on [0, 1] and that is independent of (zig , xg , ψg , νg ).
Therefore, denoting
                                                            e(F −1 (u, νg ), xg , ψg ),
                             αg (u) = α(u, xg , ψg , νg ) = α

rewriting the model (19) as
                                                 0
                                          yig = zig α(uig , xg , ψg , νg ),
                                     0 α(u , x , ψ , ν ) is strictly increasing with probability one,
and assuming that the function u 7→ zig   ig g    g g
we obtain the following quantile regression model:

                                                              0
                                 Qyig |zig ,xg ,ψg ,νg (u) = zig αg (u), u ∈ [0, 1],

which in turn implies that

                                                           0
                                  Qyig |zig ,xg ,αg (u) = zig αg (u), u ∈ [0, 1],                         (20)

where Qyig |zig ,xg ,ψg ,νg (u) denotes the uth quantile of the conditional distributional of yig given
(zig , xg , ψg , νg ) and Qyig |zig ,xg ,αg (u) denotes the uth quantile of the conditional distribution of yig
given (zig , xg , αg ) with αg = {αg (u), u ∈ [0, 1]}.
     Equation (1) with any U ⊂ [0, 1] in Section 2 follows from (20). In turn, equation (2) in Section
2 arises if α1 (u, xg , ψg , νg ), the first component of the vector α(u, xg , ψg , νg ), can be reasonably
well approximated by x0g β(u) + εg (u) where εg (u) = ε(u, ψg , νg ), which is a typical assumption in
applied regression analysis. This provides a structural interpretation of the model in Section 2.
     An advantage of this interpretation is that it yields additional intuition behind the condition
that E[wg εg (u)] = 0 for all u ∈ U imposed on the instrument wg in Section 2. In particular, as
explained in footnote 5, as long as the vector xg contains the constant, this condition follows if
wg is independent of ηg where ηg is such that εg (u) = ε(u, ηg ). In this section, we have εg (u) =
ε(u, ψg , νg ), so that ηg = (ψg , νg ). Thus, the instrument wg should be independent both of ψg , a
vector of unobserved group-level covariates governing group-specific effects α
                                                                             eg (u) = α
                                                                                      e(u, xg , ψg ),
and of νg , a vector of unobserved group-level covariates governing the distribution of unobserved
heterogeneity Fg (u) = F (u, νg ). Both of these conditions are reasonable in our empirical application
in Section 5.

F.2. Extension based on a Random Coefficient Model. One of the conditions we used in
the discussion above is a functional form assumption that α1 (u, xg , ψg , νg ) can be reasonably well
approximated by a linear form x0g β(u) + εg (u) where εg (u) = ε(u, ψg , νg ). Here linearity in xg is a
rather flexible assumption because we can always replace xg by a set of different transformations
of xg whose linear combinations can approximate the function xg 7→ α1 (u, xg , ψg , νg ) sufficiently
well. On the other hand, additive separability of x0g β(u) and εg (u) may be difficult to justify on
theoretical grounds. If this is the case, a better approximation of α1 (u, xg , ψg , νg ) can be given by
                                                   Appendix                                                 33

x0g βg (u) where βg (u) = β(u, ψg , νg ). Therefore, in this section, we briefly comment on how one can
estimate the model given by
                                                            0
                                   Qyig |zig ,xg ,αg (u) = zig αg (u), u ∈ U,
                                             αg,1 (u) = x0g βg (u), u ∈ U,                                (21)

where we use the same notation as above and where (21) can be thought of as a random coefficient
model since βg (u) = β(u, ψg , νg ). Throughout this section, we assume that the instrument wg is
independent of the pair (ψg , νg ), which, as explained above, strengthens the condition E[wg εg (u)] =
0 for all u ∈ U used in Section 2. Observe that this assumption implies that βg (u) is independent
of wg .
  In this model, one can use the same first stage procedure to estimate group-specific effects
αg (u), that is, one can run a quantile regression on the data {(zig , yig ), i = 1, . . . , Ng } separately in
each group g to find the estimators α̂g (u) of αg (u). If the number of observations per group grows
sufficiently fast as the number of groups gets large, α̂g (u) will consistently estimate αg (u) uniformly
over g = 1, . . . , G. In the second stage, we will have to replace the 2SLS estimator suitable for (2) by
an estimator suitable for (21). Given that βg (u) is independent of wg , several approaches developed
in the literature can be applied to learn some features of the distribution of βg (u) = β(u, ψg , νg )
depending on what side conditions we impose on the model; see, for example, Imbens and Angrist
(1994), Heckman and Vytlacil (1998), Florens, Heckman, Meghir, and Vytlacil (2008), and Masten
and Torgovitsky (2014). For concreteness, we describe here the approach developed by Masten and
Torgovitsky (2014), which, under certain control variable assumptions and some other technical
assumptions, yields consistent estimates of β̄(u) = E[βg (u)].
  To explain their procedure, assume, for simplicity, that there is only one endogenous covariate
                                                      eg , xg,dx )0 where x
among the vector of covariates xg , that is, xg = (1, x                   eg = (xg,2 , . . . , xg,dx −1 )0 is
independent of (ψg , νg ). Assume that xg is continuously distributed, and xg,dx = h(wg , vg ) where
the function v 7→ h(wg , v) is increasing with probability one, and the (scalar) random variable
vg is such that wg is independent of (ψg , νg , vg ) (control variable assumption). Then Masten and
Torgovitsky (2014) show that under some further technical conditions, β̄(u) = E[βg (u)] can be
consistently estimated by
                                                       Z   1
                                             β̂(u) =           β̂(u, r)dr
                                                       0
where
                                                     −1                      
                                      G                       G
                                    1 X                     1 X
                       β̂(u, r) =      k̂g (r)xg x0g         k̂g (u)xg αg (u) ,                       (22)
                                    G                       G
                                       g=1                             g=1

k̂g (r) = h−1 K(h−1 (R̂g − r)), h is the bandwidth value satisfying h = hG → 0 as G → ∞, K
is the kernel function, R̂g = F̂ (xg,dx |wg ), and F̂ (x|w) is an estimator of F (x|w), the conditional
probability that xg,dx ≤ x given wg = w.
34                                                      Appendix

     Note, however, that αg (u) is unknown in our setting, and so this estimator is infeasible. To
obtain a feasible estimator, one can substitute α̂g (u) calculated in the first stage instead of αg (u) in
(22). Using the same techniques as those developed in this paper, it is then possible to show that
the feasible estimator is asymptotically equivalent to the infeasible estimator under weak condition
on the growth of the number of observations per group as the number of groups gets large, and
so the feasible estimator has the same asymptotic properties as those of β̂(u), which are in turn
developed in Masten and Torgovitsky (2014).

                                              Appendix G. Proofs

     In this Appendix, we first prove some preliminary lemmas. Then we present the proofs of the
theorems stated in the main text as well as the proof of Theorems 4 and 5 stated in Appendices
C and D. In all proofs, c and C denote strictly positive generic constants that depend only on
cM , cf , CM , Cf , CL whose values can change at each appearance.
     We will use the following notation in addition to that appearing in the main text. Let

                                      A(u) = (α1,1 (u), ..., αG,1 (u))0 ,
                                      β(u)
                                      e    = (X 0 PW X)−1 (X 0 PW A(u)),                             (23)
                                                        0       0
                                      Jg (u) = Eg [z1g z1g fg (z1g αg (u))].

For η, α ∈ Rdz , and u ∈ U, consider the function fη,α,u : Rdz × R → R defined by

                                    fη,α,u (z, y) = (z 0 η) · (1{y ≤ z 0 α} − u).                    (24)

Let F = {fη,α,u : η, α ∈ Rdz ; u ∈ U}; that is, F is the class of functions fη,α,u as η, α vary over Rdz
and u varies over U. For α ∈ Rdz and u ∈ U, let the function hα,u : Rdz × R → Rdz be defined by

                                       hα,u (z, y) = z(1{y ≤ z 0 α} − u),

and let hk,α,u denote kth component of hα,u . Let Hk = {hk,α,u : α ∈ Rdz ; u ∈ U }. Note that
Hk ⊂ F for all k = 1, ..., dz .
     We will also use the following notation from the empirical process literature,
                                            Ng
                                         1 X
                              Gg (f ) = p       (f (zig , yig ) − Eg [f (zig , yig )])
                                         Ng i=1
for f ∈ F, H, or Hk , k = 1, . . . , dz .

Preliminary Lemmas. In all lemmas, we implicitly impose Assumptions 1-8.

Lemma 1. As G → ∞,
                                                        G
                                                     1 X
                                            Q̂xw   =     xg wg0 →p Qxw ,                             (25)
                                                     G
                                                       g=1
                                                              Appendix                                             35

                                                               G
                                                        1 X
                                             Q̂ww =         wg wg0 →p Qww                                        (26)
                                                        G
                                                              g=1

where Qxw and Qww appear in Assumption 2.
                                                                                                      PG
Proof. We only prove (25). The proof of (26) is similar. To prove (25), observe that G−1                             0
                                                                                                           g=1 E[xg wg ]   →
Qxw by Assumption 2. Therefore, it suffices to prove that
                                              G
                                            1 X
                                                xg wg0 − E[xg wg0 ] →p 0.
                                                                   
                                                                                                                 (27)
                                            G
                                                g=1

In turn, (27) follows from Assumptions 2(iv) and 4(i) and Chebyshev’s inequality. Hence, (25)
follows. This completes the proof of the lemma.                                                                    

Lemma 2. As G → ∞,
                                            G
                                      1 X
                                          εg (u1 )εg (u2 )wg wg0 →p J(u1 , u2 )
                                      G
                                         g=1
uniformly over u1 , u2 ∈ U.

Proof. Observe that we cannot apply a uniform law of large numbers with bracketing directly
because the data are not necessarily i.i.d. across g. Therefore, we provide a complete proof.
  Since
                                        G
                                    1 X 
                                       E εg (u1 )εg (u2 )wg wg0 → J(u1 , u2 )
                                                               
                                    G
                                       g=1
uniformly over u1 , u2 ∈ U by Assumption 6(ii), it suffices to prove that
                          G
                      1 X
                          (εg (u1 )εg (u2 )wg,k wg,l − E [εg (u1 )εg (u2 )wg,k wg,l ]) →p 0                      (28)
                      G
                         g=1

uniformly over u1 , u2 ∈ U for all k, l = 1, . . . , dw .
  To this end, fix u1 , u2 ∈ U and k, l = 1 . . . , dw . We first show (28) for these values of u1 , u2 ,
k, and l. Note that we cannot use Chebyshev’s inequality here because E[(εg (u1 )εg (u2 )wg,k wg,l )2 ]
is not necessarily finite. Instead, we use a more delicate method as presented in Theorem 2.1.7 of
Tao (2012). Let δ = cM /4. Then by Hölder’s inequality,
                                                                                                 1/2
          E[|εg (u1 )εg (u2 )wg,k wg,l |1+δ ] ≤ E[|εg (u1 )εg (u2 )|2+2δ ] · E[|wg,k wg,l |2+2δ ]      .

In turn,
                                                                                    
                                                      2+2δ                    4+4δ
                              E[|εg (u1 )εg (u2 )|           ] ≤ E sup |εg (u)|          ≤ CM ,
                                                                        u∈U
                                                      h         i
                              E[|wg,k wg,l |2+2δ ] ≤ E kwg k4+4δ ≤ CM

by Assumptions 6(i) and 2(iv). Hence,

                                       E[|εg (u1 )εg (u2 )wg,k wg,l |1+δ ] ≤ CM ,
36                                                                Appendix

and so denoting Xg = εg (u1 )εg (u2 )wg,k wg,l − E[εg (u1 )εg (u2 )wg,k wg,l ], we obtain

                                          E[|Xg |1+δ ] ≤ C.                                                                            (29)

With this notation, (28) is equivalent to G−1 G
                                              P
                                                 g=1 Xg →p 0.
     Now for N > 0 to be chosen later, denote Xg,≤N = Xg · 1{|Xg | ≤ N } and Xg,>N = Xg · 1{|Xg | >
N }. Then by Fubini’s theorem and Markov’s inequality,
                                             Z ∞
                 |E[Xg,>N ]| ≤ E[|Xg,>N |] =     P (|Xg | · 1{|Xg | > N } > t)dt
                                                                       0
                                                Z   N                                Z   ∞
                                           =            P (|Xg | > N )dt +                   P (|Xg | > t)dt
                                                0                                      N
                                                                                 ∞
                                               E[|Xg |1+δ ]                          E[|Xg |1+δ ]
                                                                            Z
                                           ≤N·              +                                     dt
                                                 N 1+δ                       N          t1+δ
                                                E[|Xg    |1+δ ]       E[|Xg |1+δ ]
                                           =                      +                ≤ CN −δ
                                                   Nδ                    δN δ
where in the last inequality we used (29). Hence, by Markov’s inequality, for any ε > 0,
                                        G
                                     1 X                G
                                                      1 X                C
                                  P       Xg,>N > ε ≤      E[|Xg,>N |] ≤      ,
                                      G               εG                 εN δ
                                          g=1                                    g=1

and since |E[Xg,≤N ]| = |E[Xg,>N ]| ≤ CN −δ ,
                            G
                         1 X                          G
                                                     1 X                        
                    P         Xg,≤N > ε + CN −δ ≤ P       (Xg,≤N − E[Xg,≤N ]) > ε
                          G                           G
                               g=1                                                   g=1
                                                                           G
                                                                        1 X      2       N2
                                                                  ≤          E[Xg,≤N ] ≤    .
                                                                       εG2               εG
                                                                                 g=1

                                                         −1
                                                            PG
Thus, setting N = G1/3 , we obtain G                          g=1 Xg        →p 0, which is equivalent to (28) for given u1 ,
u2 , k, and l.
     Next, to show that (28) holds uniformly over u1 , u2 ∈ U, for δ > 0, let Uδ be a finite subset of U
such that for any u ∈ U, there exists u0 ∈ Uδ satisfying |εg (u) − εg (u0 )| ≤ δ. Existence of such a set
Uδ follows from Assumption 6(iii). Then
                          G
                    1 X
          sup           (εg (u1 )εg (u2 )wg,k wg,l − E[εg (u1 )εg (u2 )wg,k wg,l ])
        u1 ,u2 ∈U   G
                         g=1
                                     G
                               1 X
           ≤ max                   (εg (u1 )εg (u2 )wg,k wg,l − E[εg (u1 )εg (u2 )wg,k wg,l ])
              u1 ,u2 ∈Uδ       G
                                 g=1
                         G                                                                            
                2δ       X
              +                  sup |εg (u)| · |wg,k wg,l | + E sup |εg (u)| · |wg,k wg,l |                   = op (1) + δ · Op (1)
                G               u∈U                                        u∈U
                         g=1

by the result above and Chebyshev’s inequality. Since δ is arbitrary, this completes the proof.                                          
                                                             Appendix                                                         37

Lemma 3. As G → ∞,
                                                  G
                                              1 X
                                             √       wg εg (·) ⇒ G0 (·), in `∞ (U)
                                               G g=1

where G0 is a zero-mean Gaussian process with uniformly continuous sample paths and covariance
function J(u1 , u2 ) for all u1 , u2 appearing in Assumption 6.

Proof. For any finite set U 0 ⊂ U, it follows from Assumption 6(ii), Lindeberg’s Central Limit
Theorem, and the Cramér-Wold device (see, for example, Theorems 11.2.5 and 11.2.3 in Lehmann
and Romano (2005)) that
                                          1 X G           
                                          √       wg εg (u)        ⇒ (N (u))u∈U 0
                                            G g=1            u∈U 0


where (N (u))u∈U 0 is a zero-mean Gaussian vector with covariance function J(u1 , u2 ) for all u1 , u2 ∈
U 0 . Therefore, it follows from the second part of Theorem 14 that the asserted claim of the lemma
holds if for any k = 1, . . . , dw and Zg (u) = G−1/2 wg,k εg (u), g = 1, . . . , G and u ∈ U, the sequence
PG                                           ∞
                                                                                              PG
  g=1 Zg (·) is asymptotically tight in ` (U). Fix k = 1, . . . , dw . To prove that            g=1 Zg (·) is
asymptotically tight in `∞ (U), we apply the first part of Theorem 14 with Gaussian-dominated
semi-metric ρ : U × U → R+ defined by ρ(u1 , u2 ) = C|u2 − u1 | for sufficiently large constant C > 0;
see discussion in front of Theorem 14 for the definition of Gaussian-dominated semi-metrics.
  Condition (i) of Theorem 14 holds because for any η > 0 and δ = 1 + cM /2,
       G                                                                         G                                   
       X                                                                   1         X
                                                                                                        1+δ         1+δ
             E sup |Zg (u)| · 1 sup |Zg (u) > η                   ≤                        E sup |εg (u)|     |wg,k |
       g=1         u∈U                   u∈U                          η δ G1/2+δ/2   g=1         u∈U

                                  G                                  i1/2
                         1        X                        h
              ≤                      E sup |εg (u)|2+2δ · E |wj,k |2+2δ       →0
                   η δ G1/2+δ/2    g=1         u∈U


by Hölder’s inequality and Assumptions 2(iv) and 6(i).
  Condition (ii) of Theorem 14 holds because for any u1 , u2 ∈ U,
             G                                          G
             X                                       1 X
                   E[(Z(u2 ) − Z(u1 ))2 ] =              E[(wg,k εg (u2 ) − wg,k εg (u1 ))2 ]
                                                     G
             g=1                                       g=1
                                                        G
                                                  CX    2
                                                ≤    E[wg,k |u2 − u1 |2 ] ≤ C|u2 − u1 |2 ≤ ρ2 (u1 , u2 )
                                                  G
                                                       g=1

by Assumptions 2(iv) and 6(iii) since the constant C in the definition of ρ(u1 , u2 ) is large enough.
  Finally, condition (iii) of Theorem 14 holds because by Markov’s inequality for any  > 0,
             G
                                                         !
            X
       sup     t2 P       sup   |Zg (u2 ) − Zg (u1 )| > t
         t>0 g=1             ρ(u1 ,u2 )≤2
38                                                        Appendix

                      G
                             "                                            #
                 1 X
               ≤     E             sup         |wg,k εg (u2 ) − wg,k ε(u1 )|2 ≤ C       sup         |u2 − u1 |2 ≤ 2
                 G             ρ(u1 ,u2 )≤2                                        ρ(u1 ,u2 )≤2
                     g=1

by Assumptions 2(iv) and 6(iii) since the constant C in the definition of ρ(u1 , u2 ) is large enough.
  Therefore, Theorem 14 implies that the sequence G                                              ∞
                                                    P
                                                       g=1 Zg (·) is asymptotically tight in ` (U).
The asserted claim follows.                                                                                              

Lemma 4. There exist constants c, C > 0 such that (i) for all u ∈ U and g = 1, . . . , G, all
eigenvalues of Jg (u) are bounded from below by c, and (ii) for all u1 , u2 ∈ U and g = 1 . . . , G,
kJg−1 (u2 ) − Jg−1 (u1 )k ≤ C|u2 − u1 |.

Proof. For any u ∈ U and α ∈ Rdz with kαk = 1,

                                       α0 Jg (u)α ≥ cf α0 Eg [z1g z1g
                                                                   0
                                                                      ]α ≥ cf cM                                       (30)

where the first inequality follows from Assumption 7(ii) and the second from Assumption 4(ii).
This gives the first asserted claim.
     To prove the second claim, observe that
                                                                                              kJg (u2 ) − Jg (u1 )k
         kJg−1 (u2 ) − Jg−1 (u1 )k ≤ kJg−1 (u1 )kkJg−1 (u2 )kkJg (u2 ) − Jg (u1 )k ≤
                                                                                                   (cf cM )2
where the second inequality follows from (30). Hence, it suffices to show that kJg (u2 ) − Jg (u1 )k ≤
C|u2 − u1 | for some C > 0. To this end, note that
                    0              0
                  |z1g αg (u2 ) − z1g αg (u1 )| ≤ kz1g kkαg (u2 ) − αg (u1 )k ≤ CM CL |u2 − u1 |

where the second inequality follows from Assumptions 4(i) and 5. Thus, if |u2 − u1 | < cf /(CM CL ),
      0 α (u ) ∈ B (u , c ), and so
then z1g g 2      g 1 f

                                               0         0                   0
              kJg (u2 ) − Jg (u1 )k ≤ Eg [z1g z1g · fg (z1g αg (u2 )) − fg (z1g αg (u1 )) ]
                                                                        0           3
                                      ≤ Cf CM CL |u2 − u1 | · kEg [z1g z1g ]k ≤ Cf CM CL |u2 − u1 |

where the second inequality follows from Assumption 7(i) and the derivation above, and the third
from Assumption 4(i). On the other hand, if |u2 − u1 | ≥ cf /(CM CL ), then
                                                                                      0
                      kJg (u2 ) − Jg (u1 )k ≤ kJg (u1 )k + kJg (u2 )k ≤ 2Cf kEg [z1g z1g ]k
                                                        2
                                                 ≤ 2Cf CM ≤ c−1    3
                                                             f Cf CM CL |u2 − u1 |

where the first inequality follows from the triangle inequality, the second from Assumption 7(ii),
and the third from Assumption 4(i). This gives the second asserted claim and completes the proof
of the lemma.                                                                                                            

Lemma 5. There exist constants c, C > 0 such that for all g = 1, . . . , G,

                          kEg [hα,u (z1g , y1g )] − Jg (u)(α − αg (u))k ≤ Ckα − αg (u)k2 ,                             (31)
                                                      Appendix                                                       39


                       Eg [(α − αg (u))0 hα,u (z1g , y1g )] ≥ ckα − αg (u)k2 .                                     (32)

for all u ∈ U and α ∈ Rdz satisfying kα − αg (u)k ≤ c.

Proof. Second-order Taylor expansion around αg (u) and the law of iterated expectation give

                                                         0                           0
              Eg [hα,u (z1g , y1g )] = Eg [z1g (1{y1g ≤ z1g α} − u)] = Eg [z1g (Fg (z1g α) − u)]
                                                  0
                                  = Eg [z1g (Fg (z1g αg (u)) − u)] + Jg (u)(α − αg (u)) + rn (u),

where rn (u) is the remainder and Fg (·) is the conditional distribution function of y1g given (z1g , αg ).
                                                        0 α (u)) − u)] = 0, which holds because
The first claim of the lemma follows from Eg [z1g (Fg (z1g g
 0 α (u) is the uth quantile of the conditional distribution of y , and from kr (u)k ≤ Ckα−α (u)k2
z1g g                                                            1g            n            g
for some C > 0, which holds by Assumptions 4(i) and 7(i).
  To prove the second claim, note that if kα−αg (u)k is sufficiently small, then k(α−αg (u))0 rn (u)k ≤
ckα − αg (u)k2 for an arbitrarily small constant c > 0. On the other hand,

                             (α − αg (u))0 Jg (u)(α − αg (u)) ≥ ckα − αg (u)k2

by Lemma 4. Combining these inequalities gives the second claim.                                                     

Lemma 6. The function class F, defined in the beginning of this section, is a VC subgraph class
of functions. Moreover, for all k = 1, ..., dz , Hk is a VC subgraph class of functions as well.

Proof. A similar proof can be found in Belloni, Chernozhukov, and Hansen (2006). We present the
proof here for the sake of completeness. Consider the class of sets {x ∈ Rdz +1 : a0 x ≤ 0} with a
varying over Rdz +1 . It is well known that this is a VC subgraph class of sets; see, for example,
exercise 14 of chapter 2.6 in Van der Vaart and Wellner (1996). Further, note that

                     {(z, y, t) : fη,α,u (z, y) > t} = {y ≤ z 0 α} ∩ {z 0 η > t/(1 − u)}
                                                                                         

                                                      ∪ {y > z 0 α} ∩ {z 0 η < −t/u} .
                                                                                      


Therefore, the first result follows from Lemma 2.6.17(ii,iii) in Van der Vaart and Wellner (1996).
The second result follows from the fact that Hk ⊂ F.                                                                 

Lemma 7. For any ϕ ≥ 1, there exists a constant C > 0 such that for all g = 1, . . . , G
                                                      
                                          g          ϕ
                              Eg sup kG (hαg (u),u )k ≤ C.
                                            u∈U

Proof. Observe that
                                        dz                                       dz
                                                                                        "                #
                                        X                                        X
     Eg sup kGg (hαg (u),u )kϕ ≤ C            Eg sup |Gg (hk,αg (u),u )|ϕ ≤ C          Eg       sup |Gg (f )|ϕ .
            u∈U                         k=1       u∈U                            k=1            f ∈Hk
40                                                              Appendix

Further, all functions in Hk are bounded by some constant C > 0 by Assumption 4(i) and the set
of functions Hk is a VC subgraph class by Lemma 6. Therefore, combining Theorems 9 and 11
gives Eg [supf ∈Hk |Gg (f )|] ≤ C, and so Theorem 13 shows that
                                           "            #
                                                      Eg    sup |Gg (f )|ϕ ≤ C.
                                                            f ∈Hk

The asserted claim follows.                                                                                                   

Lemma 8. There exist constants c, C > 0 such that for all g = 1, . . . , G,
                   "                                                     #
                     Eg               sup              kGg (hαg (u2 ),u2 ) − Gg (hαg (u1 ),u1 )k4 ≤ C
                               u2 ∈U :|u2 −u1 |≤

for all  ∈ (0, c) and u1 ∈ U.

Proof. Fix some u1 ∈ U. Observe that
                 "                                                                             #
                                                  g                        g               4
               Eg             sup             kG (hαg (u2 ),u2 ) − G (hαg (u1 ),u1 )k
                    u2 ∈U :|u2 −u1 |≤
                               dz
                                          "                                                                           #
                               X
                                                                       g                       g                  4
                     ≤C              Eg               sup           |G (hk,αg (u2 ),u2 ) − G (hk,αg (u1 ),u1 )|           .
                               k=1            u2 ∈U :|u2 −u1 |≤

Consider the function F : Rdz × R → R given by

                                     F (z, y) = C 1{|y − z 0 αg (u1 )| ≤ C} + 
                                                                                

                                                                0 (α (u ) − α (u ))| ≤ C|u − u |
for some sufficiently large C > 0. By Assumptions 4(i) and 5, |zig  g 2      g 1          2   1
for some C > 0. Therefore, for all u2 ∈ U satisfying |u2 − u1 | ≤ ,

                         hk,αg (u2 ),u2 (zig , yig ) − hk,αg (u1 ),u1 (zig , yig ) ≤ F (zig , yig )

by Assumption 4(i). Note that Eg [F 2 (zig , yig )] ≤ C for some C > 0 by Assumption 7(ii) if  ≤ 1.
Also, for M = max1≤i≤Ng F (zig , yig ), we have E[M 2 ] ≤ Cn. Further, by Lemma 6, Hk is a
VC subgraph class of functions, so that the function class H̃k = {hk,αg (u2 ),u2 − hk,αg (u1 ),u1 : u2 ∈
[u1 − , u1 + ]} is a VC type class by Theorem 9. So, applying Theorem 11 with F as an envelope
yields
                         "                                                                         #
                                                                                                   √
                    Eg              sup           |Gg (hk,αg (u2 ),u2 ) − Gg (hk,αg (u1 ),u1 )| ≤ C ,
                             u2 ∈U :|u2 −u1 |≤

and so Theorem 13 shows that
                    "                                                                                  #
                    Eg               sup              |Gg (hk,αg (u2 ),u2 ) − Gg (hk,αg (u1 ),u1 )|4 ≤ C.
                             u2 ∈U :|u2 −u1 |≤

The asserted claim follows.                                                                                                   
                                                           Appendix                                                  41

Lemma 9. There exist constants c, C > 0 such that for all g = 1, . . . , G,
     "                                                   #
                                                                                       
   Eg sup       sup        kG (hα,u ) − G (hαg (u),u )k ≤ C  log(1/) + Ng−1 log2 (1/)
                              g           g            2
           u∈U α∈Rdz :kα−αg (u)k≤

for all  ∈ (0, c).

Proof. Observe that
                "                                                                      #
                Eg sup              sup              kGg (hα,u ) − Gg (hαg (u),u )k2                              (33)
                      u∈U α∈Rdz :kα−αg (u)k≤

                             dz
                                         "                                                                #
                             X
                                                                         g                 g          2
                       ≤C          Eg sup               sup            |G (hk,α,u ) − G (hk,αg (u),u )|       .   (34)
                             k=1             u∈U α∈Rdz :kα−αg (u)k≤

Consider the function class

                       H̃k = {hk,α,u − hk,αg (u),u : u ∈ U; α ∈ Rdz ; kα − αg (u)k ≤ }.

By Lemma 6 and Theorem 9, F is a VC type class, and so Theorem 10 implies that H̃k ⊂ F − F
is also a VC type class. In addition, all functions from H̃k are bounded in absolute value by some
constant C > 0 by Assumption 4(i). Moreover, for any f ∈ H̃k , Eg [f (zig , yig )2 ] ≤ C if  ≤ 1.
Thus, applying Theorem 11 with the function class H̃k yields
     "                                                #
                                                                                  p                            
   Eg sup             sup          |Gg (hk,α,u ) − Gg (hk,αg (u),u )| ≤ C            log(1/) + Ng−1/2 log(1/) ,
        u∈U α∈Rdz :kα−αg (u)k≤

and so Theorem 13 gives
       "                                                                     #
                                                                                                        
      Eg sup           sup           |Gg (hk,α,u ) − Gg (hk,αg (u),u )|2 ≤ C  log(1/) + Ng−1 log2 (1/) .
          u∈U α∈Rdz :kα−αg (u)k≤

The asserted claim follows.                                                                                          

Lemma 10. Uniformly over u ∈ U,
                                                G
                                      1 X −1
                                     √       Jg (u)Gg (hαg (u),u )wg0 = Op (1).
                                       G g=1

Proof. To prove this lemma, we use Theorem 14 with the semi-metric ρ(u1 , u2 ) = C|u2 − u1 |1/4
defined for all u1 , u2 ∈ U and some sufficiently large constant C > 0. Clearly, ρ is Gaussian-
dominated; see discussion before Theorem 14 for the definition. Define vg (u) = Jg−1 (u)Gg (hαg (u),u )
and
                                                                           √
                                                Zg,k,m (u) = vg,k (u)wg,m / G
where vg,k (u) and wg,m denote kth and mth components of vg (u) and wg , respectively. Then the
asserted claim is equivalent to the statement that
                                   G
                                   X
                                         Zg,k,m (u) = Op (1) uniformly over u ∈ U                                 (35)
                                   g=1
42                                                                        Appendix

for all k and m. To prove (35), observe first that by Assumptions 1(i) and 2(iii), zero-mean processes
Zg,k,m (·) are independent across g. Also, for any a > 0,
                    G
                    X                                              
                          E sup |Zg,k,m (u)| · 1 sup |Zg,k,m (u)| > a
                    g=1         u∈U                                 u∈U

                                      G
                                      X                                                          
                                 −1                      2
                           ≤a               E       sup Zg,k,m (u)         · 1 sup |Zg,k,m (u)| > a
                                      g=1           u∈U                         u∈U

                                G                        
                                                                                √
                                                                                   
                              1 X                     2
                           ≤      E sup(vg,k (u)wg,m ) · 1 sup |vg,k (u)wg,m | > Ga .                                       (36)
                             aG     u∈U                     u∈U
                                      g=1

Further, pick some 0 < ϕ < 2. The expression under the sum in (36) is bounded from above by
Lemma 4 by
                                                      
             C             g             2+ϕ       2+ϕ
                  E sup kG (hαg (u),u )k     kwg k
          aϕ Gϕ/2     u∈U
                                                          2−ϕ
                     C                g
                                                    4(2+ϕ)     4            2+ϕ    C
                                                                  E kwg k4
                                                                   
               ≤ ϕ ϕ/2 E sup kG (hαg (u),u )k 2−ϕ                              4
                                                                                  ≤ ϕ ϕ/2 → 0
                  a G         u∈U                                                  a G
uniformly over g = 1, . . . , G where the second line follows from Hölder’s inequality, Assumption
2(iv), and Lemma 7. This gives condition (i) of Theorem 14.
     Next, we verify condition (ii) of Theorem 14. For any u1 , u2 ∈ U,
       G                                                            G
       X
                                                        2         1 X    4
                                                                              1/2                                 1/2
                                                                                   · E[(vg,k (u2 ) − vg,k (u1 ))4 ]
                                                           
              E (Zg,k,m (u2 ) − Zg,k,m (u1 ))                   =     E[wg,m ]                                          .
                                                                  G
        g=1                                                               g=1

Further, using an elementary inequality (a + b)4 ≤ C(a4 + b4 ) for all a, b ∈ Rp gives

                Eg [(vg,k (u2 ) − vg,k (u1 ))4 ] ≤ CEg [kJg−1 (u2 )k4 · kGg (hαg (u2 ),u2 − hαg (u1 ),u1 )k4 ]
                                                            + CEg [kJg−1 (u2 ) − Jg−1 (u1 )k4 · kGg (hαg (u1 ),u1 )k4 ]
                                                       ≤ CEg [kGg (hαg (u2 ),u2 − hαg (u1 ),u1 )k4 ]
                                                            + CEg [kGg (hαg (u1 ),u1 )k4 ] · |u2 − u1 |4

where the second inequality follows from Lemma 4. In addition,

               Eg [kGg (hαg (u2 ),u2 − hαg (u1 ),u1 )k4 ] ≤ C|u2 − u1 | and Eg [kGg (hαg (u1 ),u1 )k4 ] ≤ C                 (37)

where the first inequality follows from Lemma 8 and the second is easy to check directly. Therefore,

                                            Eg [(vg,k (u2 ) − vg,k (u1 ))4 ] ≤ C|u2 − u1 |,

and so
                        G
                        X
                              E (Zg,k,m (u2 ) − Zg,k,m (u1 ))2 ≤ C|u2 − u1 |1/2 ≤ ρ2 (u1 , u2 )
                                                             
                        g=1
                                                              Appendix                                     43

by Assumption 2(iv) since the constant C in the definition of ρ(u1 , u2 ) is sufficiently large. This
gives condition (ii) of Theorem 14.
  Finally, condition (iii) of Theorem 14 holds because for any  > 0 and u1 ∈ U,
                        G
                                                                              !
                      X
                            2
                  sup      t P       sup     |Zg,k,m (u2 ) − Zg,k,m (u1 )| > t
                   t>0 g=1              u2 ∈U :ρ(u1 ,u2 )≤

                             G
                                       "                                                           #
                             X
                                                                                               2
                        ≤          E              sup            |Zg,k,m (u2 ) − Zg,k,m (u1 )|
                             g=1           u2 ∈U :ρ(u1 ,u2 )≤

                                   G
                                            "                                                          #
                          1 X
                        =     E                       sup           |vg,k (u2 ) − vg,k (u1 )|2 wg,m
                                                                                                2
                                                                                                    ≤ 2
                          G                   u2 ∈U :ρ(u1 ,u2 )≤
                                  g=1

where the second line follows from Markov’s inequality, and the last inequality follows by selecting
sufficiently large constant C in the definition of ρ and using the same argument as that in verification
of condition (ii) since the first inequality in (37) used in the verification of condition (ii) can be
replaced by
                            "                                                              #
                       Eg               sup           kGg (hαg (u2 ),u2 − hδg (u1 ),u1 )k4 ≤ c4
                                u2 ∈U :ρ(u1 ,u2 )≤

for arbitrarily small c > 0 by selecting the constant C in the definition of ρ(u1 , u2 ) large enough
and using Lemma 8. The claim of the lemma now follows by applying Theorem 14.                              

Proofs of Theorems.
                                                                        √
Proof of Theorem 1. The proof consists of two steps. First, we show that G(β̂(u) − β(u))
                                                                                   e     = op (1)
                                                                         √
uniformly over u ∈ U where β(u)
                           e                                                 e − β(·)) ⇒ G(·)
                                is defined in (23). Second, we show that G(β(·)
in `∞ (U). Combining these steps gives the result.

Step 1. Denote Q̂xw = X 0 W/G and Q̂ww = W 0 W/G. Then
          √                                −1                               √ 
            G(β̂(u) − β(u))
                      e     = Q̂xw Q̂−1 Q̂
                                     ww xw
                                          0
                                                Q̂  Q̂−1
                                                  xw ww    W 0
                                                               (Â(u) − A(u))/  G .

By Lemma 1, X 0 W/G →p Qxw and W 0 W/G →p Qww where matrices Qxw and Qww have singular
values bounded in absolute values from above and away from zero by Assumption 2(ii), and so
                                −1                          −1
              Ŝ = Q̂xw Q̂−1 Q̂
                          ww xw
                               0
                                     Q̂xw Q̂−1              0
                                            ww →p Qxw Qww Qxw     Qxw Q−1
                                                                       ww = S.            (38)

Therefore, to prove the first step, it suffices to show that
                                                        G
                                          1 X
                                  S(u) = √       (α̂g (u) − αg (u))wg0 = op (1)
                                           G g=1

uniformly over u ∈ U. To this end, write S(u) = S1 (u) + S2 (u) where
                              G
                          1 X −1
                                 Jg (u)Gg (hαg (u),u )wg0 / Ng ,
                                                           p
              S1 (u) = − √
                           G g=1
44                                                        Appendix

                             G
                         1 X  −1                                            
                                Jg (u)Gg (hαg (u),u ) + Ng (α̂g (u) − αg (u)) wg0 / Ng .
                                                       p                           p
               S2 (u) = √
                          G g=1

Since NG = ming=1,...,G Ng → ∞ by Assumption 3, Lemma 10 implies that S1 (u) = op (1) uniformly
over u ∈ U.
     Consider S2 (u). Let
                                                            q
                                                 Kg = C         Ng−1 log Ng                                             (39)

for sufficiently large constant C > 0 so that Theorem 5 implies that
                                                          
                              P sup kα̂g (u) − αg (u)k > Kg ≤ CNg−3 .
                                        u∈U

Let DG be the event that
                                          max sup kα̂g (u) − αg (u)k ≤ Kg ,
                                        g=1,...,G u∈U

         c be the event that D does not hold. By the union bound, P (D c ) ≤ CGN −3 . By
and let DG                    G                                       G         g
Assumption 3, CGNg−3 → 0. Therefore,

                                                         c
                        S2 (u) = S2 (u)1{DG } + S2 (u)1{DG } = S2 (u)1{DG } + op (1)
                                                                   PG                             p
uniformly over u ∈ U. Further, kS2 (u)k1{DG } ≤ C                     g=1 (r1,g   + r2,g + r3,g )/ GNg where

         r1,g = sup           sup               Jg−1 (u)(Gg (hα,u ) − Gg (hαg (u),u )) kwg k,
                u∈U   α∈Rdz :kα−α   g (u)k≤Kg

                                        Ng
                                  1     X
         r2,g = sup    Jg−1 (u) √             hα̂g (u),u (zig , yig ) kwg k,
                u∈U              Ng     i=1
                                                     hp                                             i
         r3,g = sup           sup               Eg     Ng (Jg−1 (u)hα,u (zig , yig ) − (α − αg (u))) kwg k.
                u∈U α∈Rdz :kα−αg (u)k≤Kg

We bound the three terms r1,g , r2,g , and r3,g in turn. By Lemma 4 and Hölder’s inequality,
                                           "                                                                    #!1/2
                             1/2
                             2                                                    g          g              2
         E[r1,g ] ≤ E[kwg k ]           E sup                sup               G (hα,u ) − G (hαg (u),u )
                                               u∈U α∈Rdz :kα−αg (u)k≤Kg
                        s                     !1/2
                            log Ng                       (log Ng )3/4
                ≤C                 log Ng            =          1/4
                              Ng                            Ng

where the second line follows from the definition of Kg , Assumption 2(iv), and Lemma 9. Further,
using Lemma 4 again gives
                              Ng                                        Ng
                          1 X                                       1 X                                C
         sup Jg−1 (u) √           hα̂g (u),u (zig , yig ) ≤ C sup √         hα̂g (u),u (zig , yig ) ≤ p
         u∈U              N g i=1                             u∈U   N g i=1                            Ng
                                                       Appendix                                           45

by the optimality of α̂g (u) and since yig has a continuous conditional distribution. Hence, E[r2,g ] ≤
   p
C/ Ng . Finally, by Lemmas 4 and 5,
                                                                      C log Ng
                                                           Ng Kg2 ≤
                                                       p
                                        E[r3,g ] ≤ C                   p       .
                                                                          Ng
Hence, by Assumption 3,
                                                     √
                                                    C G(log NG )3/4
                                                
                             E sup kS2 (u)k1{DG } ≤       3/4
                                                                    = o(1),
                               u∈U                     NG
                 √
implying that  G(β̂(u) − β(u))
                         e     = op (1) uniformly over u ∈ U and completing the first step.
                      √
                           e − β(·)) ⇒ G(·) in `∞ (U), observe that
Step 2. To prove that G(β(·)
                                     √                        G
                                        e − β(·)) = Ŝ · √1
                                                             X
                                      G(β(·)                     wg εg (·).
                                                           G g=1

As explained in Step 1, Ŝ →p S. Also, by Lemma 3,
                                              G
                                        1 X
                                       √       wg εg (·) ⇒ G0 (·), in `∞ (U)
                                         G g=1

where G0 is a zero-mean Gaussian process with uniformly continuous sample paths and covariance
function J(u1 , u2 ). Therefore, by Slutsky’s theorem,
                                  √
                                       e − β(·)) ⇒ G(·), in `∞ (U)
                                    G(β(·)                                                              (40)

where G is a zero-mean Gaussian process with uniformly continuous sample paths and covari-
ance function C(u1 , u2 ) = SJ(u1 , u2 )S 0 . Combining (40) with Step 1 gives the asserted claim and
completes the proof of the theorem.                                                                       

Proof of Theorem 2. Equation (38) in the proof of Theorem 1 gives Ŝ →p S. Therefore, it suffices to
            ˆ 1 , u2 ) − J(u1 , u2 )k = op (1) uniformly over u1 , u2 ∈ U. Note that αg,1 (u) − x0 β(u) =
prove that kJ(u                                                                                     g
εg (u). Hence,

                     α̂g,1 (u) − x0g β̂(u) = (α̂g,1 (u) − αg,1 (u)) − x0g (β̂(u) − β(u)) + εg (u)
                                         = I1,g (u) − I2 (u) + εg (u)

where I1,g (u) = α̂g,1 (u) − αg,1 (u) and I2 (u) = x0g (β̂(u) − β(u)). Further, we have
                                         G
                                     1 X
                                         εg (u1 )εg (u2 )wg wg0 →p J(u1 , u2 )
                                     G
                                        g=1

uniformly over u1 , u2 ∈ U by Lemma 2. In addition, it was demonstrated in the proof of Theorem
1 that                                                                 
                         P       max sup kα̂g (u) − αg (u)k > Kg            ≤ CGNg−3 = o(1)
                             g=1,...,G u∈U
46                                                Appendix

by Assumption 3 where Kg = C(Ng−1 log Ng )1/2 for sufficiently large constant C. Thus, setting
KG = maxg=1,...,G Kg , we obtain

                            G                                G
                                                           2 X
                          1 X                         0   KG
                              I1,g (u1 )I1,g (u2 )wg wg ≤      kwg k2 + op (1)
                          G                               G
                              g=1                               g=1
                                                                2
                                                         ≤ Op (KG ) + op (1) = op (1)

uniformly over u1 , u2 ∈ U by Assumption 2(iv) and Chebyshev’s inequality. Further,

                    G                              G
                  1 X                       0   KG X
                      I1,g (u1 )εg (u2 )wg wg ≤      |εg (u2 )|kwg k2 + op (1)
                  G                             G
                    g=1                               g=1
                                                     G
                                                  KG X
                                              ≤        sup |εg (u)|kwg k2 + op (1) = op (1)
                                                  G    u∈U
                                                      g=1

uniformly over u1 , u2 ∈ U by same argument as that used in the proof of Lemma 2 since Hölder’s
inequality implies that
                                                   1/2
                                    2                2
                                                                       1/2
                 E sup |εg (u)|kwg k ≤ E sup |εg (u)|        E[kwg k4 ]     ≤C
                        u∈U                       u∈U

by Assumptions 2(iv) and 6(i). Similarly,

                   G                              G
              1 X                          CX
                  I2 (u1 )I2 (u2 )wg wg0 ≤    kwg k2 sup kβ̂(u) − β(u)k2 = op (1),
              G                            G         u∈U
                  g=1                           g=1

                   G                              G
              1 X                          CX
                  I2 (u1 )εg (u2 )wg wg0 ≤    |εg (u2 )|kwg k2 sup kβ̂(u) − β(u)k = op (1)
              G                            G                   u∈U
                  g=1                           g=1

uniformly over u1 , u2 ∈ U by Assumption 4(i). Finally,

           G                                  G
         1 X                              CKG X
             I1,g (u1 )I2,g (u2 )wg wg0 ≤       kwg k2 k sup kβ̂(u) − β(u)k + op (1) = op (1)
         G                                 G             u∈U
            g=1                                    g=1

uniformly over u1 , u2 ∈ U. Combining these inequalities gives the asserted claim.                

Proof of Theorem 3. Observe that the statement
                                       s                          s        
                                          V̂ (u)                     V̂ (u)
              β1 (u) ∈
                     / β̂1 (u) − ĉ1−α          , β̂1 (u) + ĉ1−α           for some u ∈ U
                                             G                          G

is equivalent to the statement that T > ĉ1−α . Therefore, it suffices to prove that

                                           P (T > ĉ1−α ) → α.                                  (41)
                                                     Appendix                                        47

To prove (41), recall the process G(·) = (G1 (u), . . . , Gdx (u))0 appearing in Theorem 1. Define a
                  e on U with values in R by
Gaussian process G(·)

                                        G(u)
                                        e    = V (u)−1/2 G1 (u), u ∈ U

where V (u) = C1,1 (u, u), the (1, 1)st component of C(u, u) = SJ(u, u)S 0 . It follows from conditions
of the theorem that V (u) is bounded away from zero uniformly over u ∈ U. Therefore, since G(·)
has uniformly continuous sample paths, the process G(·)
                                                    e     also has uniformly continuous sample
paths. The covariance function of the process G(·)
                                              e is

                            e 1 , u2 ) = V (u1 )−1/2 C1,1 (u1 , u2 )V (u2 )−1/2 .
                            C(u

Further, for G ≥ 1, define processes G           e G (·) on U with values in R by
                                     b G (·) and G

                                             G 
                    b G (u) = q 1
                                             X                                      
                    G                               g (α̂g,1 (u) − x0g β̂(u))ŵg,1
                                                                                S
                                                                                      , u∈U
                               GV̂ (u)       g=1

                                             G
                    e G (u) = p 1
                                             X
                                                             S
                    G                              g εg (u)wg,1 , u∈U
                               GV (u)        g=1

where wg,1S and ŵ S are the 1st component of the vectors Sw and Ŝw , respectively, and V̂ (u) =
                  g,1                                       g       g
ˆ
C1,1 (u, u).
   Observe that ĉ1−α is the (1 − α) conditional quantile of supu∈U |ĜG (u)| given the data. Also,
for β ∈ (0, 1) and V ⊂ U, let c0 be the βth quantile of supu∈V |G(u)|,
                                  β,V
                                                                  e       and let cβ,V,G be the βth
quantile of supu∈V |G
                    e G (u)| given the data.
                          e has uniformly continuous sample paths, it follows that supu∈U |G(u)|
  Now, since the process G(·)                                                              e     <
∞, and so Theorem 2.1 of Chernozhukov, Chetverikov, and Kato (2014b) implies that supu∈U |G(u)|
                                                                                          e
has continuous distribution. Therefore, for any δ > 0, there exists η > 0 such that
                                                          
                                               0
                             P sup |G(u)| > c1−α−η,U − η ≤ α + δ,
                                     e
                                    u∈U
                                                                   
                              P       sup |G(u)|
                                           e     > c01−α+η,U + η        ≥ α − δ.
                                    u∈U

In addition, Theorem 1 combined with continuous mapping theorem implies that T ⇒ supu∈U |G(u)|,
                                                                                         e
and so

                                  P (T > c01−α−η,U − η) ≤ α + δ + o(1),
                                  P (T > c01−α+η,U + η) ≥ α − δ + o(1).

Hence, to prove (41), it suffices to show that for any η > 0,

                            P (c01−α−η,U − η ≤ ĉ1−α ≤ c01−α+η,U + η) → 1.                        (42)
48                                                Appendix


To prove (42), fix some η > 0. Since G(·)
                                     e    has uniformly continuous sample paths, there exists a
finite U(η, 1) ⊂ U such that

                                  c01−α−η,U − η ≤ c01−α−η/2,U (η,1) − η/2,                                 (43)
                                  c01−α+η,U + η ≥ c01−α+η/2,U (η,1) + η/2.                                 (44)
                                            PG
Further, let AG be the event that G−1               S 2
                                              g=1 (wg,1 )    ≤ C for some sufficiently large C > 0. Note
that P (AG ) → 1 as G → ∞. Also, on AG , for any u1 , u2 ∈ U,
       h 1 X G                                         G
                                          S
                                              2 i   1 X
     E √        g (εg (u2 ) − εg (u1 ))wg,1      =       (εg (u2 ) − εg (u1 ))2 (wg,1
                                                                                    S 2
                                                                                        ) ≤ C|u2 − u1 |2
           G g=1                                     G
                                                       g=1

by Assumption 6(iii) where E [·] denotes expectation with respect to the distribution of 1 , . . . , G
(and keeping everything else fixed). Therefore, combining Borell’s inequality (see Proposition of
Van der Vaart and Wellner (1996)) and Corollary 2.2.8 of Van der Vaart and Wellner (1996) show
that one can find finite U(η, 2) ⊂ U such that on AG ,

                             c1−α+η/2,U (η,2),G + η/3 ≥ c1−α+η/3,U ,G + η/4,                               (45)
                             c1−α−η/2,U (η,2),G − η/3 ≤ c1−α−η/3,U ,G − η/4.                               (46)

Now, observe that whenever the inequalities (43) - (46) are satisfied, the same inequalities are also
satisfied with U(η, 1) and U(η, 2) replaced by U(η) = U(η, 1) ∪ U(η, 2).
   Next, conditional on the data, (G e G (u))u∈U (η) is a zero-mean Gaussian vector with covariance
function
                                                         G
                                                      1 X                        
                         CeG (u1 , u2 ) = V (u1 )−1/2                        S 2
                                                           εg (u1 )εg (u2 )(wg,1 ) .
                                                       G
                                                        g=1

By Lemma 2, C  eG (u1 , u2 ) →P C(u
                                e 1 , u2 ) uniformly over u1 , u2 ∈ U(η) where C(u
                                                                               e 1 , u2 ) is the covari-
ance function of a zero-mean Gaussian vector (G(u))
                                                 e    u∈U (η) . Hence, by Lemma 3.1 of Chernozhukov,
Chetverikov, and Kato (2013),

                         P (c01−α+η/2,U (η) + η/2 > c1−α+η/2,U (η),G + η/3) → 1,
                         P (c01−α−η/2,U (η) − η/2 < c1−α−η/2,U (η),G − η/3) → 1.

Combining this with inequalities (43) - (46) where we replace U(η, 1) and U(η, 2) by U(η) gives

                               P (c01−α+η,U + η > c1−α+η/3,U ,G + η/4) → 1,
                               P (c01−α−η,U − η < c1−α−η/3,U ,G − η/4) → 1.

To complete the proof, it suffices to show that

                      P (c1−α−η/3,U ,G − η/4 ≤ ĉ1−α ≤ c1−α+η/3,U (η) + η/4) → 1.                          (47)
                                                        Appendix                                                 49

  To prove (47), observe that
                  G                                                                     G
             1 X                                                      1 X
        sup √       g x0g (β̂(u) − β(u))wg,1
                                          S
                                              ≤ sup kβ̂(u) − β(u)k · √           S
                                                                             g wg,1 xg →P 0
        u∈U   G g=1                             u∈U                    G g=1
                                                                          PG
since supu∈U kβ̂(u) − β(u)k →P 0 by Theorem 1 and kG−1/2                             S
                                                                             g=1 g wg,1 xg k   = OP (1) by Assump-
tions 2(iv) and 4(i). Also,
                                            G
                                   1 X                              S
                              sup √       g (α̂g,1 (u) − αg,1 (u))wg,1 →P 0
                              u∈U   G g=1

by the same argument as that used in Step 1 of the proof of Theorem 1. Therefore, since εg (u) =
αg,1 (u) − x0g β(u), supu∈U |V̂ (u) − V (u)| →P 0 by Theorem 2, V (u) is bounded away from zero
uniformly over u ∈ U, and Ŝ →P S as in the proof of Theorem 1, we obtain

                                          sup kG
                                               e G (u) − G
                                                         b G (u)k →p 0.
                                          u∈U

Since ĉ1−α is the (1 − α) conditional quantile of supu∈U |G(u)|
                                                           b      given the data and cβ,U ,G is the βth
conditional quantile of supu∈U |G(u)|
                                 e      given the data, (47) follows. This completes the proof of the
theorem.                                                                                                         

Proof of Theorem 4. We split the proof into two steps.

  Step 1. Here we wish to show that for sufficiently large C > 0,
                                                                                                 !
                                                                         C(log NG )3/4
                           Jg−1 (u)Gg (hαg (u),u )
                                                       p
             P    max                                 + Ng (α̂g − αg ) >       1/4
                                                                                                     →0        (48)
                 1≤g≤G                                                       NG
Set Kg = C(Ng−1 log Ng )1/2 for sufficiently large C > 0 so that Theorem 5 implies that

                                     P (kα̂g (u) − αg (u)k > Kg ) ≤ CNg−3 .

Let DG be the event that
                                          max kα̂g (u) − αg (u)k ≤ Kg
                                         1≤g≤G
         c be the event that D does not hold. By the union bound, P (D c ) ≤ CGN −3 → 0.
and let DG                    G                                       G         g
  Now, on the event DG ,

                        Jg−1 (u)Gg (hαg (u),u ) +
                                                      p
                                                          Ng (α̂g − αg ) ≤ r1,g + r2,g + r3,g

where

             r1,g =           sup               kJg−1 (u)(Gg (hα,u ) − Gg (hαg (u),u ))k,
                      α∈Rdz :kα−α   g (u)k≤Kg

                                 Ng
                              1 X
             r2,g = Jg−1 (u) p      hα̂ (u),u (zig , yig ) ,
                              Ng i=1 g
50                                                         Appendix


                                                         Ng (Jg−1 (u)hα,u (zig , yig ) − (α − αg (u)))]k.
                                                        p
                r3,g =            sup           kEg [
                         α∈Rdz :kα−αg (u)k≤Kg

By Lemma 4 and optimality of α̂g (u),
                                                Ng
                                             C X                               C
                                   r2,g   ≤ p       hα̂g (u),u (zig , yig ) ≤ p .
                                             Ng i=1                            Ng

Also, by Lemmas 4 and 5,
                                                                 C log Ng
                                                         Ng Kg2 ≤ p
                                                        p
                                             r3,g ≤ C                     .
                                                                     Ng
Finally, by Lemma 4 and Talagrand’s inequality (see, for example, Theorem B.1 in Chernozhukov,
Chetverikov, and Kato (2014b)),
                                                                                                C log3/4 Ng
                                               kGg (hα,u ) − Gg (hαg (u),u )k ≤ C
                                                                                    p
        r1,g ≤ C sup             sup                                                 Kg log G =      1/4
                       α∈Rdz :kα−αg (u)k≤Kg                                                        Ng
with probability at least 1 − G−2 . Combining these bounds gives (48) and completes this step.

     Step 2. Here we complete the proof. For g = 1, . . . , G and i = 1, . . . , N̄G , define qig as follows.
If i > Ng , set qig = 0. If i ≤ Ng , set

                                qig = (N̄G /Ng )1/2 Ig−1/2 z̄ig (1{yig ≤ zig
                                                                          0
                                                                             αg (u)} − u)

where z̄ig denotes the first component of the vector Jg−1 (u)zig . By Step 1 and assumptions that
Ig ≥ cM and N̄G /NG ≤ CM , it follows that
                     q                                       
                                                           M
            P max       Ng /Ig |α̂g,1 (u) − αg,1 (u)| ≤ c1−α
                1≤g≤G
                                                                             
                                      N̄G                                 3/4
                                1     X                              C log Ng 
               ≤ P  max p                (qig − Eg [qig ]) ≤ cM
                                                               1−α +      1/4
                                                                                + o(1)                        (49)
                     1≤g≤G      N̄G g=1                                 Ng

In turn, since under our assumptions |qig | ≤ C, by Corollary 2.1 in Chernozhukov, Chetverikov,
and Kato (2014d), the probability in (49) is bounded from above by
                                              !
                          M       C log3/4 NG
         P max |Yg | ≤ c1−α +          1/4
                                                + o(1)
             1≤g≤G                   NG
                                         C(log3/4 NG ) · (log1/2 G)
                                   
                                M
            ≤ P max |Yg | ≤ c1−α +                  1/4
                                                                    + o(1) = 1 − α + o(1)
                  1≤g≤G                            NG
where in the second line we used Theorem 3 in Chernozhukov, Chetverikov, and Kato (2014c).
Thus,                                                                  
                                   q
                                                                     M
                       P       max  Ng /Ig |α̂g,1 (u) − αg,1 (u)| ≤ c1−α ≤ 1 − α + o(1).                      (50)
                            1≤g≤G
Similar arguments also give
                           q                                    
                                                              M
                 P max       Ng /Ig |α̂g,1 (u) − αg,1 (u)| ≤ c1−α ≥ 1 − α − o(1).                             (51)
                            1≤g≤G
                                                              Appendix                                                     51

Rearranging the terms under the probability signs in (50) and (51) completes the proof of the
theorem.                                                                                                                   

Proof of Theorem 5. Recall the definition of the function fη,α,u in (24). Since x 7→ ρu (x) = (u −
I{x < 0})x is convex, for x > 0, kα̂g (u) − αg (u)k ≤ x for all u ∈ U if
                                                     Ng
                                                     X
                               inf          inf            fη,αg (u)+xη,u (zig , yig )/Ng > 0.                           (52)
                               u∈U η∈Rdz ;kηk=1
                                                     i=1

Now, since fη,α,u = η 0 hα,u , Lemma 5 implies that

                                    inf      inf         Eg [fη,αg (u)+xη,u (zig , yig )] > cx
                                u∈U η∈Rdz ;kηk=1

if the constant c̄ in the statement of the theorem is sufficiently small. Therefore, it follows that
(52) holds if
                              Ng
                              X                                                                    
           inf         inf           fη,αg (u)+xη,u (zig , yig ) − Eg [fη,αg (u)+xη,u (zig , yig )] /Ng ≥ −cx,
           u∈U η∈Rdz ;kηk=1
                              i=1

which in turn follows if
                                                              Gg (fη,α,u ) ≥ −cx Ng .
                                                                                p
                                      inf          inf                                                                   (53)
                                      u∈U η,α∈Rdz ;kηk=1

Note that for any η ∈ Rdz satisfying kηk = 1, |fη,α,u | ≤ 2kzig k ≤ C for some C > 0 by Assumption
4(i). In addition, it follows from Lemma 6 and Theorem 9 that the conditions of Theorem 12 hold
for the function class {fη,α,u ∈ F : u ∈ U; η, α ∈ Rdz ; kηk = 1}. Therefore, Theorem 12 shows that
(53) holds with probability not smaller than

                                                    1 − C exp(−cx2 Ng )

for some c, C > 0. The asserted claim follows.                                                                             

                                    Appendix H. Proofs of Theorems 6-8

  The proofs are analogous to those of Theorems 1-3. Therefore, we only discuss important dif-
ferences. First, the constants c, C > 0 in the proofs now depend on cM , cf , CM , Cf , CL , and C̄.
Second, among Lemmas 1 - 10, Lemmas 4 - 9 deal with within group variation, and so apply under
our conditions without changes. The statement of Lemma 1 holds without changes but in the proof,
Chebyshev’s inequality applies on cluster level, that is, for k = 1, . . . , dx and l = 1, . . . , dw ,
                G
           h 1 X                                          M
                                                  2 i   1 X h                 X                                 2 i
       E               (xg,k wg,l − E[xg,k wg,l ])     = 2   E                         (xg,k wg,l − E[xg,k wg,l ])
            G                                           G
                 g=1                                              m=1        g∈CG (m)
                                                                 M
                                                              C X h           X                                      i
                                                          ≤        E                  (xg,k wg,l − E[xg,k wg,l ])2
                                                              G2
                                                                  m=1      g∈CG (m)
52                                                            Appendix

                                                                 G
                                                              C X
                                                          =        E[(xg,k wg,l − E[xg,k wg,l ])2 ] → 0
                                                              G2
                                                                  g=1

where in the second line we used Assumption 10 (iii) that the number of groups in each cluster is
bounded from above by C̄.
     Lemma 2 should be replaced with the statement that G → ∞,
                     M
                   1 X           X                            X                  
                                         εg (u1 )wg                      εg (u1 )wg0 →P J CS (u1 , u2 )            (54)
                   G
                      m=1   g∈CG (m)                          g∈CG (m)

uniformly over u1 , u2 ∈ U. To prove this statement, observe that by Assumption 60 (ii),
                    M
                  1 X h           X                             X                      i
                      E                    εg (u1 )wg                       εg (u1 )wg0        → J CS (u1 , u2 )
                  G
                     m=1      g∈CG (m)                           g∈CG (m)

uniformly over u1 , u2 ∈ U. Further, for δ = cM /4 and k, l = 1, . . . , dw ,
                  h X                      X                   1+δ i
               E              εg (u1 )wg,k          εg (u2 )wg,l
                       g∈CG (m)                          g∈CG (m)
                              h       X                                                i
                       ≤ CE                       |εg (u1 )wg,k εg0 (u2 )wg0 ,l |1+δ
                                  g,g 0 ∈CG (m)
                              h       X                                                               i
                       ≤ CE                           |εg (u1 )wg,k |2+2δ + |εg0 (u2 )wg0 ,l |2+2δ          ≤ C,
                                  g,g 0 ∈CG (m)

where the last inequality can be proven by the same argument as that used in the proof of Lemma
2. From this point, the proof of 54 is analogous to the proof used in Lemma 2.
     The statement of Lemma 3 holds with J(u1 , u2 ) replaced by J CS (u1 , u2 ). To prove the new
statement, first observe that for any finite U 0 ⊂ U,
                                     1 X G           
                                     √       wg εg (u)        ⇒ (N (u))u∈U 0
                                       G g=1            u∈U 0


where (N (u))u∈U 0 is a zero-mean Gaussian vector with covariance function J CS (u1 , u2 ) for all
u1 , u2 ∈ U 0 . The rest of the proof follows from Theorem 14 by the same arguments as those used
in Lemma 3 and those explained above where we replace Zg (u) = G−1/2 wg,k εg (u) by Zm (u) =
G−1/2 g∈CG (m) wg,k εg (u), and we replace sums over g = 1, . . . , G by sums over m = 1, . . . , M
      P

where appropriate.
   The statement of Lemma 10 holds without changes but in the proof, we replace Zg,k,l (u) =
              √                 P                       √
vg,k (u)wg,l / G by Zm,k,l (u) = g∈CG (m) vg,k (u)wg,l / G and we replace sums over g = 1, . . . , G
by sums over m = 1, . . . , M where appropriate, and employ the arguments explained above.
     With the new versions of Lemmas 1 - 10, the proof of Theorem 6 is the same as the proof of
Theorem 1. The proof of Theorem 7 is analogous to that of Theorem 2 where, using the same
                                                          Appendix                                            53

notation as that in the proof of Theorem 2, we employ the bound
             M
           1 X            X                          X                       
                                 I1,g (u1 )wg                   I1,g (u2 )wg0
           G
              m=1     g∈CG (m)                       g∈CG (m)
                    M                                                          G
               1    X        X                                             Kg2 X
             ≤                          kI1,g (u1 )I1,g0 (u2 )wg wg0 k   ≤       kwg k2 + oP (1) = oP (1),
               G                                                           G
                    m=1 g,g 0 ∈CG (m)                                                g=1

and we bound all other terms in the proof similarly. The proof of Theorem 8 is analogous to that
of Theorem 3.

                                                 Appendix I. Tools

  In Appendix G, we used several results from the empirical process theory. For ease of reference,
we describe these results in this section.
  Let (T, ρ) be a semi-metric space. For ε > 0, an ε-net of (T, ρ) is a subset Tε of T such
that for every t ∈ T , there exists a point tε ∈ Tε with ρ(t, tε ) < ε. The ε-covering number
N (ε, T, ρ) of T is the infimum of the cardinality of ε-nets of T , that is, N (ε, T, ρ) = inf{Card(Tε ) :
Tε is an ε net of T }.
  Let F be a class of measurable functions defined on some measurable space (S, S). For any
probability measure Q on (S, S) and p ≥ 1, let Lp (Q) denote the space of functions f on S with
the norm kf kQ,p = ( |f (s)|p dQ(s))1/p < ∞. The function class F is called VC-subgraph class if
                    R

the collection of all subgraphs of the functions in F forms a VC-class of sets; see Section 2.6.2 of
Van der Vaart and Wellner (1996) for the definitions. In addition, we say that the function class
F is VC type class of functions with an envelope F : S → R+ and constants A ≥ e, and v ≥ 1 if
all functions in F are bounded in absolute value by F and the following condition holds:

                                        sup N (εkF kQ,2 , F, L2 (Q)) ≤ (A/ε)v
                                         Q

for all ε ∈ (0, 1) where the supremum is taken over all finitely discrete probability measures Q on
(S, S).
  Finally, let X1 , . . . , Xn be an i.i.d. sequence of random variables taking values in (S, S) with a
common distribution P . Define the empirical process:
                                       n
                                    1 X                       
                         Gn (f ) = √       f (Xi ) − E[f (Xi )] , f ∈ F.
                                     n
                                                  i=1

The following theorems are used in Appendix G:

Theorem 9. There exists a universal constant K such that for any VC subgraph class F of func-
tions with an envelope F , any p ≥ 1, and 0 < ε < 1,
                                                                                              r(V (F )−1)
                                                                                    V (F )    1
                     sup N (εkF kQ,p , F, Lp (Q)) ≤ KV (F)(16e)
                       Q                                                                      ε
54                                                    Appendix

where V (F) is a finite constant that depends only on the function class F (and called VC dimension
of the class F). Thus, any VC-subgraph class of functions F is also a VC type class of functions
with some constants A ≥ e and v ≥ 1 depending only on F.

Proof. See Lemma 19.15 in Van der Vaart (1998).                                                                    

Theorem 10. Let F1 , . . . , Fk be classes of measurable functions S → R to which measurable
envelopes F1 , . . . , Fk are attached, respectively, and let φ : Rk → R be a map that is Lipschitz in the
sense that
                                                          k
                                                          X
                                                     2
                              |φ ◦ f (s) − φ ◦ g(s)| ≤          L2j (s)|fj (s) − gj (s)|2 ,
                                                          j=1

for every f = (f1 , . . . , fk ), g = (g1 , . . . , gk ) ∈ F1 ×. . . Fk = F and every s ∈ S, where L1 , . . . , Lk are
non-negative measurable functions on S. Consider the class of functions φ(F) = {φ ◦ f : f ∈ F}.
Denote ( kj=1 L2j Fj2 )1/2 by L · F . Then we have
        P

                                                            k
                                                            Y
                  sup N (εkL · F kQ,2 , φ(F), L2 (Q)) ≤           sup N (εkFj kQj ,2 , Fj , L2 (Qj ))
                   Q                                        j=1 Qj

for every 0 < ε < 1.

Proof. See Lemma A.6 in Chernozhukov, Chetverikov, and Kato (2014a).                                               

Theorem 11. Let F be a VC type class of functions with an envelope F and constants A ≥ e and
v ≥ 1. Denote σ 2 = supf ∈F E[f (X1 )2 ] and M = max1≤i≤n F (Xi ). Then
             "             #        s                                              !
                                                   AkF k P,2    vkM k2       AkF kP,2
           E sup |Gn (f )| ≤ K          vσ 2 log               + √     log
               f ∈F                                  σ             n           σ

for some absolute constant K where kM k2 = (E[M 2 ])1/2 .

Proof. See Corollary 5.1 of Chernozhukov, Chetverikov, and Kato (2014a).                                           

Theorem 12. Let F be a class of functions f : X → [0, 1] that satisfies
                                           V
                                            K
                   sup N (ε, C, L2 (Q)) ≤        , for every 0 < ε < K
                     Q                      ε
where supremum is taken over all probability measures Q. Then for every t > 0,
                                                ! 
                                                       Dt V −2t2
                                                          
                           P sup |Gn (f )| > t ≤      √      e
                                f ∈F                    V
for a constant D that depends on K only.

Proof. See Theorem 2.14.9 in Van der Vaart and Wellner (1996).                                                     
                                                        Appendix                                               55

Theorem 13. Let X1 , . . . , Xn be independent, zero-mean stochastic processes indexed by an arbi-
trary index set T with joint probability measure P . Then
                                                                                        
                                          p
                        kSn k      ≤K           kSn k     +            max kXi k
                               P,p      log p         P,1             1≤i≤n        P,p

for any p > 1 where Sn = X1 + · · · + Xn , kSn k = supt∈T |Sn (t)|, kXi k = supt∈T |Xi (t)|, and K is
a universal constant.

Proof. See Proposition A.1.6 in Van der Vaart and Wellner (1996).                                              

  Finally, we provide a reference for Central Limit Theorem with bracketing by Gaussian hypothe-
ses, which we use several times in Section G. A semi-metric ρ : F × F → R+ is called Gaussian if
it can be defined as
                                                                   1/2
                                      ρ(f, g) = E[(G(f ) − G(g))2 ]
where G is a tight, zero-mean, Gaussian random element in l∞ (F). A semi-metric ρ is called
Gaussian-dominated if it is bounded from above by Gaussian metric. In particular, it is known
that any semi-metric ρ satisfying
                                         Z       ∞p
                                                    log N (ε, F, ρ)dε < ∞
                                             0
is Gaussian-dominated; see discussion on page 212 in Van der Vaart and Wellner (1996).

Theorem 14 (Bracketing by Gaussian Hypotheses). For each n, let Zn1 , ..., Znmn be independent
stochastic processes indexed by an arbitrary index set F. Suppose that there exists a Gaussian-
dominated semi-metric ρ on F such that
                             mn
                             X
                       (i)         E [kZni kF · 1{kZni kF > η}] → 0, for every η > 0,
                             i=1
                             mn
                             X
                                   E (Zni (f ) − Zni (g))2 ≤ ρ2 (f, g), for every f, g,
                                                         
                    (ii)
                             i=1
                                   mn
                                                                               !
                                   X
                                         2
                    (iii)    sup         t P        sup |Zni (f ) − Zni (g)| > t   ≤ ε2 ,
                             t>0                  f,g∈B(ε)
                                   i=1
                                                                                                  Pmn
for every ρ-ball B(ε) ⊂ F of radius less than ε and for every n. Then the sequence                  i=1 (Zni   −
E[Zni ]) is asymptotically tight in   l∞ (F).     It converges in distribution provided it converges marginally.

Proof. See Theorem 2.11.11 in Van der Vaart and Wellner (1996).                                                
56                                                     Appendix

       Table A1. Bias of Grouped IV Quantile Regression vs. Standard Quantile Regression

                          (N,G) = (25, 25)       (N,G) = (200, 25)      (N,G) = (25, 200)     (N,G) = (200, 200)
                                I. Mean Bias for Endogenous Group-level Treatment
     Quantile True                 Grouped             Grouped               Grouped       Grouped
       (u)      Coeff.   Q. Reg. IV Q. Reg. Q. Reg. IV Q. Reg. Q. Reg. IV Q. Reg. Q. Reg. IV Q. Reg.
       0.1       0.316    0.042     -0.055     0.040    -0.007     0.038       0.018 0.039  -0.005
       0.2       0.447    0.076      0.015     0.078    -0.003     0.077       0.008 0.077   0.000
       0.3       0.548    0.116     -0.024     0.116    -0.044     0.117       0.005 0.116  -0.003
       0.4       0.632    0.155     -0.128     0.154    -0.031     0.154       0.007 0.155  -0.002
       0.5       0.707    0.194     -0.182     0.193    -0.023     0.192       0.010 0.194  -0.006
       0.6       0.775    0.236     -0.192     0.233    -0.039     0.228       0.003 0.232  -0.006
       0.7       0.837    0.273     -0.161     0.270    -0.067     0.267      -0.002 0.270  -0.004
       0.8       0.894    0.312     -0.106     0.311    -0.056     0.306      -0.010 0.309  -0.003
       0.9       0.949    0.365     -0.106     0.361    -0.060     0.360      -0.013 0.362  -0.001
     Avg. abs. bias      0.197      0.108     0.195     0.037      0.193      0.008  0.195  0.003
                                   II. Mean Bias for Exogenous Group-level Treatment
     Quantile True                    Grouped              Grouped              Grouped        Grouped
       (u)      Coeff.   Q. Reg.    IV Q. Reg. Q. Reg. IV Q. Reg. Q. Reg. IV Q. Reg. Q. Reg. IV Q. Reg.
       0.1       0.316    0.005         0.010    -0.004     -0.016     0.002     -0.011  0.001  -0.006
       0.2       0.447    0.005         0.027     0.001     -0.010     0.002     -0.018  0.003  -0.008
       0.3       0.548    0.006        -0.006     0.006     -0.012     0.003     -0.017  0.005  -0.005
       0.4       0.632    0.011        -0.021     0.007     -0.010     0.005     -0.017  0.007   0.002
       0.5       0.707    0.008        -0.039     0.008     -0.002     0.007     -0.020  0.009   0.003
       0.6       0.775    0.004        -0.021     0.009     -0.004     0.009     -0.015  0.011   0.002
       0.7       0.837    0.006        -0.011     0.007     -0.003     0.009     -0.014  0.011   0.000
       0.8       0.894   -0.010        -0.007    -0.011     -0.001    -0.011     -0.008 -0.011   0.000
       0.9       0.949   -0.031         0.008    -0.038      0.003    -0.028     -0.009 -0.031  -0.001
     Avg. abs. bias      0.010         0.017     0.010      0.007     0.009      0.014  0.010   0.003
               III. Mean Bias for Exogenous Group-level Treatment and No Group-level Unobservables
     Quantile True                 Grouped              Grouped             Grouped                Grouped
       (u)      Coeff. Q. Reg. IV Q. Reg. Q. Reg. IV Q. Reg. Q. Reg. IV Q. Reg. Q. Reg. IV Q. Reg.
       0.1       0.316    0.002      0.019    0.001      -0.006      0.000   -0.009      0.000      -0.004
       0.2       0.447    0.008      0.009    0.003      -0.002      0.000   -0.008     -0.001      -0.007
       0.3       0.548    0.005     -0.023    0.004       0.000      0.001   -0.010     -0.001      -0.007
       0.4       0.632    0.007     -0.015    0.004      -0.003      0.002   -0.001      0.000      -0.005
       0.5       0.707    0.005     -0.027    0.000      -0.003      0.001   -0.002      0.000      -0.004
       0.6       0.775    0.004     -0.037    0.001      -0.011      0.000   -0.002      0.000      -0.002
       0.7       0.837    0.003     -0.027    0.000      -0.005      0.000   -0.002      0.000       0.000
       0.8       0.894    0.000     -0.022    0.000      -0.003      0.001    0.000      0.000       0.002
       0.9       0.949   -0.003     -0.023    0.000      -0.003     -0.001   -0.005      0.000       0.001
     Avg. abs. bias      0.004      0.023     0.002      0.004      0.001    0.004      0.000       0.004

Notes: Table shows mean bias for estimation of β(u) from 1,000 Monte Carlo simulations using standard quantile
regression (Q. Reg.) and our estimator (Grouped IV Q. Reg.) for cases where (N, G) = (25,25), (200,25), (25,200),
(200,200). Panel I displays results when the group-level treatment is endogenous, panel II displays results when the
group-level treatment is independent of group-level unobservables, and panel III displays results when there are no
group-level unobservables. Each panel displays results for quantiles u ∈ {0.1, ..., 0.9} as well as the average absolute
value of the bias, averaged over the nine deciles.
