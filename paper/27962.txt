                              NBER WORKING PAPER SERIES




            SPARSE NETWORK ASYMPTOTICS FOR LOGISTIC REGRESSION

                                        Bryan S. Graham

                                      Working Paper 27962
                              http://www.nber.org/papers/w27962


                     NATIONAL BUREAU OF ECONOMIC RESEARCH
                              1050 Massachusetts Avenue
                                Cambridge, MA 02138
                                    October 2020




Financial support from NSF Grant SES #1851647 is gratefully acknowledged. Some of the
results contained in this paper were presented, albeit in more basic and preliminary forms, at an
invited session of the 2018 Latin American Meetings of the Econometric Society, and at a
plenary lecture of the 2019 meetings of the International Association of of Applied Econometrics.
I am thankful to Michael Jansson for several very helpful conversations and to Konrad Menzel
for feedback on the initial draft. All the usual disclaimers apply. The views expressed herein are
those of the author and do not necessarily reflect the views of the National Bureau of Economic
Research.

NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.

© 2020 by Bryan S. Graham. All rights reserved. Short sections of text, not to exceed two
paragraphs, may be quoted without explicit permission provided that full credit, including ©
notice, is given to the source.
Sparse Network Asymptotics for Logistic Regression
Bryan S. Graham
NBER Working Paper No. 27962
October 2020
JEL No. C01,C31,C33,C55

                                          ABSTRACT

Consider a bipartite network where N consumers choose to buy or not to buy M different
products. This paper considers the properties of the logistic regression of the N×M array of "i-
buys-j" purchase decisions, [Yij]1iN,jM, onto known functions of consumer and product
attributes under asymptotic sequences where (i) both N and M grow large and (ii) the average
number of products purchased per consumer is finite in the limit. This latter assumption implies
that the network of purchases is sparse: only a (very) small fraction of all possible purchases are
actually made (concordant with many real-world settings). Under sparse network asymptotics, the
first and last terms in an extended Hoeffding-type variance decomposition of the score of the logit
composite log-likelihood are of equal order. In contrast, under dense network asymptotics, the
last term is asymptotically negligible. Asymptotic normality of the logistic regression coefficients
is shown using a martingale central limit theorem (CLT) for triangular arrays. Unlike in the dense
case, the normality result derived here also holds under degeneracy of the network graphon.
Relatedly, when there "happens to be" no dyadic dependence in the dataset in hand, it specializes
to recently derived results on the behavior of logistic regression with rare events and iid data.
Sparse network asymptotics may lead to better inference in practice since they suggest variance
estimators which (i) incorporate additional sources of sampling variation and (ii) are valid under
varying degrees of dyadic dependence.


Bryan S. Graham
University of California - Berkeley
530 Evans Hall #3880
Berkeley, CA 94720-3880
and NBER
bgraham@econ.berkeley.edu
Let i = 1, . . . , N index a random sample of consumers and j = 1, . . . , M a random
sample of products. For each consumer-product pair ij we observe Yij = 1 if consumer
i purchases product j and Yij = 0 otherwise. Let Wi be a vector of observed consumer
attributes, Xj a vector of product attributes and n = M + N the total number of
sampled consumers and products. The conditional probability that consumer i buys
product j is given by

                                                           
                        Pr ( Yij = 1| Wi , Xj ) = e 0,n + Zij 0 ,                     (1)

          def
where Zij  z (Wi , Xj ) is a vector of known functions of Wi and Xj , 0,n an "inter-
cept" parameter (which may vary with n), 0 a vector of fixed "slope" parameters, and
e (·) a known increasing function mapping the real line into the unit interval. Below
I will emphasize the logit case with e (v ) = exp (v ) / [1 + exp (v )]; this case is con-
venient and dominates empirical work, but nothing which follows hinges essentially
upon it.
     I am interested in settings where both the number of consumers, N , and the
number of products, M , are very large. To motivate this focus, consider a large
book retailer. Such a retailer may service many customers and also stock many
books. Let x be the attribute vector associated with a newly released book,          ^n =
         
  ^n,  ^ estimates of the parameters in (1), constructed from some training sample,

and e
    ^i (x) = e ^ n + z (Wi , x) ^ the predicted probability that agent i purchases a
book of type Xj = x. With this knowledge the retailer might use

                                                  N
                                   ^n (x) =           e
                                                      ^i (x)                          (2)
                                              i=1


to predict total unit sales for the new book. This prediction could be useful for making
wholesale purchase decisions. Other objects of interest include various average partial
effects (e.g., Chamberlain, 1984; Wooldridge, 2005).
    In this paper I present a method of estimating the coefficient vector 0,n =
         
(0,n , 0 ) as well as one for conducting inference on it. I also explore estimation
and inference for aggregate effects, like (2), as well as for average effects. The econo-
metric framework outlined below is designed to accommodate two peculiarities of the
setting described above that appear to be important in practice and also consequential


                                              1
for inference.
     First, consider predicting whether randomly sampled consumer i purchases book
j , say The Clue in the Crossword Cipher, the forty-fourth novel in the celebrated
Nancy Drew mystery series. Knowledge of the frequency with which other consumers
k = 1, . . . , i - 1, i + 1, . . . , N purchase book j will generally alter the econometrician's
prediction of whether i also purchases book j . That is, for any k = i,

                            Pr ( Yij = 1| Ykj = 1) > Pr (Yij = 1)

or that Yi1 j1 and Yi2 j2 will covary whenever the two transactions correspond to a com-
mon book (such that j1 = j2 ).
    Similarly, if the econometrician knew that consumer i was a frequent book buyer,
she might conclude that this consumer is also more likely to purchase some other book
(relative to the average consumer). That is Yi1 j1 and Yi2 j2 will also covary whenever
the transactions correspond to a common buyer (such that i1 = i2 ).
    Importantly, dependence across Yi1 j1 and Yi2 j2 when {i1 , jj } and {i2 , j2 } share a
common buyer or book index may hold even conditional on observed consumer, Wi ,
and product attributes, Xj . Some consumers may have latent attributes (i.e., not
contained in Wi ) which induce them to buy many books and some books may be
especially popular (for reasons not captured adequately by Xj ). It might be, for
example, that

                Pr ( Yij = 1| Ykj = 1, Wi , Wk , Xj ) > Pr ( Yij = 1| Wi , Xj ) .

    The structured form of dependence across the elements of [Yij ]1iN,1j M de-
scribed above is a feature of separately exchangeable random arrays (Aldous, 1981;
Hoover, 1979). The inferential implications of such dependence, in the context of
subgraph counts, were first considered by Holland and Leinhardt (1976) almost fifty
years ago. Bickel et al. (2011) make an especially important recent contribution in
this area. In the context of regression models, the inferential implications of dyadic
dependence have been considered by, among others, Fafchamps and Gubert (2007),
Cameron and Miller (2014), Aronow et al. (2017), Graham (2020a), Davezies et al.
(2020) and Menzel (2017) (see Graham (2020b, Section 4) for a review and references).
Dyadic dependence will generate distinct issues here.
    The second peculiarity explored here is suggested by the observation that, even

                                               2
when presented with the opportunity to purchase many books, the typical customer
will only purchase a few. Similarly, a retailer only sells a few copies of most titles in a
given year. Put differently personal libraries are generally small and the market share
of most books is, for all practical purposes, infinitesimally small. These observations
have implications for what types of asymptotic approximations are likely to be useful
in practice. In this paper I consider sequences where both N and M grow at the same
rate such that, recalling that n = M + N ,

                                         M/n    (0, 1)

as n  .
            def
                           
    Let 0,n  E e 0,n + Zij  0 be probability that a randomly sampled consumer
purchases a randomly sampled book. The average number of books purchased by the
average consumer is then
                                      def
                                c 0,n  M 0,n .                              (3)

In network parlance c   0,n corresponds to average consumer degree. If 0,n is bounded
away from zero, then c  0,n   as N, M  . This implies that the number of actual
book purchases and the number of possible book purchases should be of equal order.
This not true in practice.1 To develop a distribution theory which is concordant with
the empirical regularity that consumers only purchase a small number of books (and,
similarly, that retailers only sell a small number of copies of any given title) I let
0,n  - at a rate which ensures that c         0,n converges to a non-zero and bounded
constant 0 < 0 <  as N, M  . In language of networks I consider bi-partite
                c

graphs which are sparse.
    The asymptotic analysis in this paper is, to my knowledge, novel, but it does
connect with two important areas of prior research by others. The first is the literature
on subgraph counts and dyadic regression cited above. However, with the partial
exception of Bickel et al.'s (2011) analysis of acyclic subgraph counts, this work has
been, starting with Holland and Leinhardt (1976), limited to to dense networks.2
The second connection is to the literature on "rare events" analysis (e.g., King and
   1
     There are tens of millions of print titles available on, for example, Amazon, even consumers who
buys hundreds of books in a year are completing only very small fraction of all possible purchases.
   2
     Graham (2017) and Jochmans (2018) also considered regression in the context of graphs which
are sparse in the limit. Both these papers utilize conditional likelihood type ideas; this has the effect
of "conditioning away" some of the dependence which is central to the analysis below.


                                                   3
Zeng, 2001); an area of special concern in political science and epidemology, but also
increasingly relevant in economics (especially in the era of "Big Data"). An interesting
feature of Theorem 1 below is that it contains Wang's (2020) recent result for logistic
regression with rare events and iid data as a special case.
    The formal analysis of this paper is confined to bipartite networks, but adapting
it to directed and/or undirected networks would be straightforward. Several con-
sumer demand settings might be appropriately modeled with the methods described
in this paper. Examples include (i) the listening behavior of streaming music service
customers and (ii) the purchase behavior of big box store customers. A limitation
vis-a-vis these applications, is that the basic set-up explored here is not useful for
understanding complementary and substitution patterns across products (cf., Lewbel
and Nesheim, 2019). Other possible applications include modeling plant locations in
an industry where firms typically operate multiple plants (here i = 1, . . . , N would
index firms and j = 1, . . . , M locations; see KPMG (2016)). Other many-to-many
matching problems that have drawn economists' interest include (i) bank-firm lend-
ing relationships (e.g., Marotta et al., 2015), (ii) the matching of venture capital with
start-ups (e.g., Bengtsson and Hsu, 2015), and (iii) supply chain settings with strong
bi-partite structure (e.g., automakers and parts supplies as in Fox (2018)). When
i = 1, . . . , N and j = 1, . . . , M index the same units with M = N , applications in-
clude the modeling of "rare events" in international relations data, such as interstate
wars (e.g., King and Zeng, 2001). More generally, the methods developed in this pa-
per, with minimal adaptation, can be used for link prediction in any sparse network
setting: bi-partite, directed or undirected.3 There are numerous applications of link
prediction in the other social sciences, the bench sciences as well as in industry.
    In what follows random variables are denoted by capital Roman letters, specific
realizations by lower case Roman letters and their support by blackboard bold Roman
letters. That is Y , y and Y respectively denote a generic random draw of, a specific
value of, and the support of, Y . A "0" subscript on a parameter denotes its population
value and may be omitted when doing so causes no confusion. In what follows I use
                                                         def
graph, network and purchase graph to refer to Y  [Yij ]1iN,1j M . All graph
theory terms and notation used below are standard (e.g., Chartrand and Zhang,
2012).
  3
      The methods outlined here are not appropriate for use in one-to-one matching settings.




                                                 4
1    Population and sampling assumptions
Let i  N index consumers in an infinite population of interest. Associated with
each consumer is the vector of observed attributes Wi  W = {w1 , . . . , wJ } . Let
j  M index products in a second infinite population of interest. The model is a two
population one (cf., Graham et al., 2018). Associated with each product is the vector
of characteristics Xi  X = {x1 , . . . , xK }. The finite support assumption on W and
X is not essential, but simplifies the discussion of exchangeability below.
    Let w : N  N be a permutation of a finite number of consumer indices which
satisfies the restriction
                                  Ww (i) iN = [Wi ]iN .                            (4)

Restriction (4) implies that w only permutes indices across observationally identical
consumers (i.e., with the same values of W ). Let x : M  M be an analogously con-
strained permutation of a finite number of product indices. Adapting the terminology
of Crane and Towsner (2018), I assume that the purchase graph is W -X -exchangeable

                                                   D
                           Yw (i)x (j )   iN,j M
                                                   = [Yij ]iN,j M .                (5)

     D
Here = denotes equality of distribution. One way to think about (5) is as a require-
ment that any probability law for [Yij ]iN,j M should attach equal probability to all
purchase graphs which are isomorphic as vertex-colored graphs. Here I associate Wi
and Xj with the color of the corresponding consumer and product vertices in the over-
all purchase graph. Virtually all single-population micro-econometric models assume
that agents are exchangeable, restriction (5) extends this idea to the two-population
setting considered here. Our probability law for the model should not change if we
re-label observationally identical units.


Graphon
It is well-known that exchangeability implies restrictions on the structure of depen-
dence across observations in the cross-section setting (e.g., de Finetti, 1931). Al-
dous (1981), Hoover (1979) and Crane and Towsner (2018) showed that exchangeable
random arrays also exhibit a special dependence structure. Let µ, {(Wi , Ai )}i1 ,
{(Xj , Bj )}j 1 and {Vij }i1,j 1 be sequences of i.i.d. random variables, additionally


                                              5
                                                                
independent of one another, and consider the purchase graph Yij       iN,j M
                                                                               , generated
according to
                             
                          Yij = h (µ, Wi , Xj , Ai , Bj , Vij )                        (6)

with h : [0, 1] × W × X × [0, 1]2  {0, 1} a measurable function, henceforth referred
to as a graphon (we can normalize µ, Ai , Bj and Vij to have support on the unit
interval, uniformly distributed, without loss of generality).
     The results of Crane and Towsner (2018), which extend the earlier work of Al-
dous (1981) and Hoover (1979), show that, for any W -X -exchangeable random array
                                            
[Yij ]iN,j M , there exists another array Yij iN,j M
                                                     , generated according to (6), such
that the two arrays have the same distribution. An implication of this result is that
we may use (6) as a nonparametric data generating process for [Yij ]iN,j M .
     Inspection of (6) indicates that exchangeability implies a particular pattern of de-
pendence across the elements of [Yij ]iN,j M . In particular Yi1 j1 and Yi2 j2 may covary
whenever i1 = i2 or j1 = j2 ; this covariance may be present even conditional on con-
sumer and product attributes. This is, of course, precisely the dependence structure
discussed in the introduction.


Sampling process
Let Y = [Yij ]1iN,1j M be the observed N × M matrix of consumer purchase deci-
sions. Let W and X be the associated matrices of consumer and product regressors.
I assume that Y is the adjacency matrix associated with the subgraph induced by a
random sample of consumers and products from a W -X -exchangeable infinite popula-
tion graph. Let G, denote this population network. Associated with this network
is some graphon (6). Let Vc and Vp denote the set of consumers and products ran-
domly sampled by the econometrician from G, . We have Y equal to the adjacency
matrix of the network:
                              GN,M = G, [Vc , Vp ] .                           (7)

An implication of (5), (6) and (7) is that we may proceed `as if' the adjacency matrix
in hand was generated according to

                             Yij = h (µ, Wi , Xj , Ai , Bj , Vij )




                                              6
for i = 1, . . . , N and j = 1, . . . , M . The marginal probability of the event, random
consumer i, purchases random product j , is thus

                           0 = E [h (µ, Wi , Xj , Ai , Bj , Vij )] .                 (8)

   Let {GN,M } be a sequence of networks indexed by, respectively, the cardinality of
the sampled consumer and product index sets, N = |Vc | and M = |Vp |. The average
number of products purchased per consumer, or average consumer degree,

                                         c
                                         0 = M 0                                     (9)

will diverge as M   when 0 > 0. Likewise the average number of times a given
product is purchased, or average product degree,

                                         p
                                         0 = N 0                                    (10)

will also diverge as N  . A consequence of this divergence is that the number
of possible purchases, and the number of actual purchases, will be of equal order.
In practice, however, only a small fraction of all possible purchases are made. To
capture this feature of the real world in our asymptotic approximations requires a
slightly more elaborate thought experiment; which I outline next.
    Instead of considering a sequence of graphs sampled from a fixed population, I
consider a sequence of graphs sampled from a corresponding sequence of populations.
The sequence of networks {GN,M } is one where both N and M grow at the same rate
such that, recalling that n = M + N ,

                                    M/n    (0, 1)

as n  . For each N, M the graphon describing the infinite population sampled
from is
                    Yij = hN,M (µ, Wi , Xj , Ai , Bj , Vij ) .          (11)

This sequence of graphons/populations {hN,M } has the property that network density

                     0,N,M = EN,M [hN,M (µ, Wi , Xj , Ai , Bj , Vij )]



                                              7
may approach zero as n  . Under this setup the order of c       0,N,M = M 0,N,M and
 p
0,N,M = N 0,N,M will depend upon the speed with which 0,N,M approaches zero as
n  . Here I use the notation EN,M [·] to emphasize that the probability law used
to compute expectations may vary with the sample size.
    As in other exercises in alternative asymptotics, indexing the population data
generating process by the sample size is not meant to capture a literal feature of
how the data are generated, rather it is done so that the limiting properties of the
model share important features ­ in this case sparseness ­ with the actual finite
network in hand. In other settings such an approach has led to more useful asymptotic
approximations, a premise I maintain here (e.g., Staiger and Stock, 1997).


2     Composite likelihood estimator
The estimation target is the regression function of Yij given Xi and Wj . This is a
predictive function and may, or may not, have structural economic meaning as well
(see Graham (2020b, Sections 4-5)). I assume that this regression function takes the
parametric form
                                                        
                                
                                          exp 0,n + Zij  0
                      e 0,n + Zij 0 =                     
                                                                                (12)
                                        1 + exp 0,n + Zij  0
          def
where Zij  z (Wi , Xj ) is a finite vector of known functions of Wi and Xj . It would
be interesting to extend what follows to semiparametric regression models, but this
is not done here.
    Assumption 1 formalizes the population and sampling set-up of the previous sec-
tion.

Assumption 1. (Sampling) The sampled network is the one induced by a random
sample of N consumers and M products drawn from the nodes of the infinite W -X -
exchangeable bipartite random array [Yij ]iN,j M with graphon (11); N and M grow
such that, for n = M + N,
                               M/n    (0, 1)

as n  .

    To allow the probability of making a purchase decline with n, let 0,n = ln (0 /n) .



                                           8
This gives, after some manipulation,
                                                      0        
                                                      n
                                                          exp Zij 0
                          e 0,n + Zij 0 =                 0        
                                                                                      (13)
                                                  1+      n
                                                              exp Zij 0

and hence an expression for average consumer degree (3) of, recalling that M/n  ,

                                         
                          0,n = 0 E exp Zij 0
                          c                                   + O n-1

                                                                     
which converges to a bounded constant as n   as long as E exp Zij     0 < .
By allowing 0,n  - as n   we ensure that, in the limit, the bipartite graph
Y = [Yij ] is sparse. Similar devices are used by Owen (2007) and Wang (2020) to
model "rare events" in cross-sectional binary outcome data.

Assumption 2. (Logit Regression Function) The mean regression function
(CEF) EN,M [ Yij | Wi , Xj ] belongs to the parametric family (12) with n = ln (/n),
 = (,   )  A × B = , A and B compact, and Zij  Z with Z a compact subset
                                                 
of Rdim(Zij ) . The true parameter 0 = (0 , 0   ) lies in the interior of the parameter
space.

    The compact support assumption on Zij is not essential, but simplifies the proofs.
Here I focus on estimation of n =(n ,   ) with n = ln (/N ). Observe that  =
(,   ) does not vary with n, while n does. Define 0,n = (0,n , 0      
                                                                      ) with 0,n =
ln (0 /n). Let ¯ = sup A, the parameter space for n , n = (-, ln (¯   )] × B, is the
one induced by  = A × B and the mapping from  to n .
    To estimate 0,n we maximize the composite log-likelihood function

                                     ^n = arg max Ln ()
                                     
                                                  n


            def
with Ln ()         1
                  NM
                        N
                        i=1
                              M
                              j =1 lij   () and

                                                                    
                  lij () = (2Yij - 1) Rij  - ln 1 + exp (2Yij - 1) Rij 

                                                               ^n is simply the coefficient
the logit kernel function and Rij = 1, Zij    . Observe that 
vector associated with a logistic regression of Yij onto a constant and Zij using all N M
dyads in the network. Although, by virtue of Assumption 2 above, Ln () correctly
represents the marginal (conditional) probability of Yij for each element of Y, it

                                                  9
does not accurately reflect the dependence structure across these elements; hence the
term "composite likelihood". See Lindsey (1988) an introduction to estimation by
composite likelihood and Graham (2020b) for discussion in the contexts of network
model estimation.


3    Sparse network asymptotics
If 0,n equals a fixed constant, then 0,n - network density ­ will also be fixed such
that the network will be dense in the limit. The limit distribution of ^n under such
"dense network asymptotics" was derived by Graham (2020b). More general results
for dyadic M-estimators under dense network asymptotics, including results on the
bootstrap, can be found in Menzel (2017) and Davezies et al. (2020). None of these
results apply here. To derive a result that does apply, begin with the mean value
expansion
                   
                     n  ^n - 0,n = nHn      ¯n + × n3/2 Sn (0,n ) .

where

                                           N   M
                                     1
                            Sn () =                  sij () ,                   (14)
                                    NM    i=1 j =1

               l ()                                           
with sij () = ij 
                      = (Yij - eij ()) Rij and eij () = e  + Zij  , corresponds to
the score vector of the composite likelihood and

                                           N   M
                                    1                 2 lij ()
                           Hn () =                                              (15)
                                   NM     i=1 j =1
                                                     

the associated Hessian matrix. Here ¯n is a mean value between 0,n and ^n which
may vary from row to row.
   Lemma 1, stated and proved in Appendix A, shows that, after re-scaling by n,
that nHn () converges uniformly to

                                                        
                                                 1   Z12
                    () = -E exp (Z12 )                   
                                                                 .              (16)
                                                Z12 Z12 Z12
An intuition for why Hn () needs to be rescaled to ensure convergence is that, under


                                         10
sparse network asymptotics, information accrues at a slower rate: the effective sample
size is not N M = (n2 ), but rather O (n). I return to this point briefly at the end of
the paper.
                                                                def
Assumption 3. (Identification) The matrix 0   (0 ) is of full rank.

   Assumption 3 is a standard identification condition (see, for example, Amemiya
(1985, p. 270)). This assumption, in conjunction with Lemma 1, gives the linear
approximation
                  
                    n   ^n - n = -1 × n3/2 Sn (0,n ) + op (1) .
                                    0

                                                  ^n - n
To derive the limit distribution of             n                 I show that the distribution
n3/2 Sn (0,n ) is well-approximated by a Gaussian random variable. The main tool
used is a martingale CLT for triangular arrays. That the variance stabilizing rate
for Sn (0,n ) is n3/2 , like the need to rescale the Hessian, is non-standard. The need
                                           
to "blow up" Sn (0,n ) at a faster than n rate is a consequence of the fact that the
summands in Sn (0,n ) are O (n-1 ) since 0,n  - as n  .
    A detailed proof of Theorem 1, stated below, is provided in Appendix B. Here
I outline the main arguments. Begin with the following three part decomposition of
the score vector

                          Sn () = U1n () + U2n () + Vn ()                                   (17)

where
                                N                      M
                           1                     1
                  U1n () =           ¯c
                                     s 1i   () +             ¯p
                                                             s 1j ( )                       (18)
                           N   i=1
                                                 M    j =1
                                     N      M
                              1
                  U2n () =                       ¯ij () - s
                                                 s        ¯c1i ( ) - s
                                                                     ¯p1j ( )               (19)
                             NM     i=1 j =1
                                     N      M
                            1
                   Vn () =                      {sij () - s
                                                          ¯ij ()}                           (20)
                           NM       i=1 j =1


with     s
         ¯ij ()       =        ¯ (Wi , Xj , Ai , Bj ; )
                               s                                 with    s
                                                                         ¯ (w, x, a, b; )     =




                                                11
E [ sij ()| Wi = w, Xj = x, Ai = a, Bj = b] and

                                            ¯c
                                            s         sc
                                              1i ( ) =¯1 (Wi , Ai ;  )

                                            ¯p
                                            s         sp
                                              1j ( ) =¯1 (Xj , Bj ;  )


with s 1 (w, a;  ) = E [¯
      ¯c                                          ¯p
                        s (w, Xj , a, Bj ; )] and s 1 (x, b;  ) = E [¯
                                                                     s (Wi , x, Ai , b; )].
    Decomposition (17) also features in Graham (2020a) and Menzel (2017).4 It can
be derived by first projecting Sn () on to A = [Ai ]1iN , W = [Wi ]1iN , B =
[Bj ]1j M , and X = [Xi ]1j N as follows:

            Sn () = E [ Sn ()| W, X, A, B] + {Sn () - E [ Sn ()| W, X, A, B]}
                                N     M                      N     M
                       1                             1
                    =                      s
                                           ¯ij () +                       {sij () - s
                                                                                    ¯ij ()} .         (21)
                      NM       i=1    j =1
                                                    NM      i=1 j =1


                         N     M
Next observe that N1 M   i=1   j =1 s
                                    ¯ij () is a two sample U-Statistic, albeit one de-
fined partially in terms of the latent variables Ai and Bj . Equation (18) corre-
sponds to the Hajek Projection of this U-statistic onto (separately) {(Wi , Ai )}N i=1
                M
and Xj , Bj j =1 . Equation (19) is the usual Hajek Projection error term.
                                      def                    def                                def
   Define n = M/n, s    1ni  s
                       ¯c       ¯c           ¯p
                                 1i (0,n ) , s 1nj  s
                                                    ¯p                   ¯nij  s
                                                      1j (0,n ) and also s     ¯ij (0,n ).
Similarly let Sn = Sn (0,n ) and so on. Applying the variance operator to Sn yields:

                     V (Sn ) =V (U1n ) + V (U2n ) + V (Vn )                                           (22)
                                  c         p         1                   3n
                              =   1n
                                       +    1n
                                                 +      [2n - c
                                                              1n - 1n ] +
                                                                   p
                                  N         M        NM                   NM

where

                                                                       
             1n = E s
             c      ¯c     sc
                      1ni (¯1ni )           1n = E s
                                            p      ¯p    ¯p
                                                     1nj s 1nj                                        (23)

             2n = E s    ¯nij = V (E [ snij | Wi , Xj , Ai , Bj ])
                    ¯nij s
             3n = E {snij - s              ¯nij } = E [V ( snij | Wi , Xj , Ai , Bj )] .
                            ¯nij } {snij - s

                            p
   In the dense case c 1n , 1n , 2n and 3n are all constant in n; hence the asymptotic
properties of Sn coincide with those of U1n . Since U1n is a sum of independent random
  4
      It is also implicit in the elegant proof in Bickel et al. (2011).



                                                       12
variables a standard argument gives

                                              D            c   p
                               n   1/2
                                         Sn  N         0,   1
                                                              + 1                          (24)
                                                          1-    
                       p
as long as c1 and/or 1 are non-zero.
                                                                              p
    Under the sparse network asymptotics considered here the order of c  1n , 1n , 2n
and 3n varies with n. This affects the order of the four variance terms in (22) and,
consequently, which components of Sn contribute to its asymptotic properties. In
Appendix A I show the order of the four terms in (22) are, respectively,

                        2                     2            2               n
          V (Sn ) =O     n
                                   +O          n
                                                      +O    n
                                                                 +O
                        N                     M            MN              MN
                                    2                                  3
                          c
                          0,n                1    1              c
                                                                 0,n       1
                 =O                                        +O
                          n              (1 - n ) n3             n         n3
                               c          2
                               0 ,n               1      1                c0,n    1
                   +O                                           +O                     .
                               n              n (1 - n ) n4            n (1 - n ) n3
                                                                       2


                                         -2
Since 1 c
          and p                  2                                     2
                 1 are both O (n ) = O (n ) we can multiply them by n to stabilize
them. Define   ~c                      2         ~p                  2 p
                 1 to be the limit of n 1n and 1 to be the limit of n 1n . Similarly
we can define   ~ 3 to be the limit of n3n , all as n  . Normalizing (22) by n3/2
therefore gives

                                       ~c
                                           ~p
                                                  ~3
                  V n   3/2
                              Sn    =   1
                                          + 1+         + O n-1                             (25)
                                      1-        (1 - )

where I also use the fact that 2n = O (n-2 ).
   Under sparse network asymptotics both U1n and Vn matter. In Appendix B I
show that U1n + Vn is a martingale difference sequence (MDS) to which a martingale
CLT can be applied; Theorem 1 then follows.
Theorem 1. Under Assumptions 1, 2 and 3

                                                 -1     ~c  ~p
                                                                   ~3
                                                                        -1
                 ^n - n D
               n          N                   0, 0       1
                                                           + 1+
                                                       1-        (1 - ) 0

as n  .
   Theorem 1 indicates that under sparse network asymptotics there are additional

                                                      13
                                     ^
sources of sampling variation in n       n - n relative to those which appear in the

dense case. Not incorporating these into inference procedures will lead to tests with
incorrect size and/or confidence intervals with incorrect coverage. A further advantage
of considering sparse network asymptotics is that Theorem 1 remains valid even under
degeneracy of the graphon, hN,M (µ, Wi , Xj , Ai , Bj , Vij ). For example, if the graphon
is constant in Ai and Bj such that Yij and Yik do not covary conditional on covariates
(and likewise for Yji and Yki ), then ~c   ~p
                                       1 = 1 = 0, but Theorem 1 nevertheless remains
valid. In contrast, under dense network asymptotics, degeneracy ­ as elegantly shown
by Menzel (2017) ­ generates additional complications. In that case the variance of
U1n is identically equal to zero, while that of U2n and Vn are of equal order. In some
cases, the behavior of U2n may even induce a non-Gaussian limit distribution (see
van der Vaart (2000)). In the sparse network cases, U2n is always negligible relative
to Vn . Furthermore Vn is well approximated ­ after suitable scaling ­ by a Gaussian
distribution.


4         Extensions and discussion
In this section I connect Theorem 1 to prior work on rare events logistic analysis,
sketch some results about the estimation of aggregate and average effects, and, finally,
close with a few ideas about possible areas of additional research.


Rare events with iid data
King and Zeng (2001) discuss, with a focus on finite sample bias, the behavior of
logistic regression under "rare events" with iid data. Evidently binary choice analyses
where the marginal frequency of positive events is quite small are common in empirical
work.5 The properties of logistic regression under sequences where the number of
"events" becomes small (i.e., "rare") relative to the sample size as it grows were
recently characterized by Wang (2020) (see also Owen (2007)). The main result in
Wang (2020) coincides with a special case Theorem 1 above. To see this observe that
if the graphon is constant in Ai and Bj , then s  ¯nij will be identically equal to zero
for all 1  i  N and 1  j  M . In this scenario there is no "dyadic dependence"
(after conditioning on Wi and Xj ) and    ~c     ~p
                                           1 = 1 = 0. Inspection of the calculations
    5
        The King and Zeng (2001) has close to four thousand citations on Google Scholar.


                                                  14
in Appendix A also reveals that, in this case, we further have an information-matrix
type equality of ~ 3 = 0 . Under these conditions Theorem 1 specializes to

                                 
                                               N 0, -
                                       ^ - n D        1
                                     n              0   ,

as n  . This is precisely, up to some small differences in notation, the result given
in Theorem 1 of Wang (2020).6
    In his analysis Wang (2020) emphasizes that information accumulates more slowly
under "rare event asymptotics". In the present setting this is reflected in the need to
rescale the Hessian matrix by n to achieve convergence (see Lemma 1 in Appendix
A). In the network setting dyadic dependence additionally slows down the rate of
convergence (cf., Graham et al., 2019). If a researcher is working with a sparse network
and concerned about dyadic dependence, then she should base inference on Theorem
1. If the graphon is degenerate or, more strongly, the elements of [Yij ]1iN,1j M
are, in fact, iid, then her inferences will remain valid (since Theorem 1 specializes to
the "rare events" result of Wang (2020) in that case).


Aggregate effects
Define eni (x) = e 0,n + z (Wi , x) 0 and e    ^ni (x) = e ^ n + z (Wi , x) ^ and con-
sider an estimate of total unit sales for a product with attribute vector Xj = x of

                                                  N
                                       ^n (x) =         e
                                                        ^ni (x) .
                                                  i=1

Under dense network asymptotics this statistic would diverge as n  . Under
sparse network asymptotics the sum Ni=1 eni (x) behaves like an average because its
                   -1
summands are O (N ). Consequently,   ^n (x) has a well-defined probability limit of

                                N
               0 (x) = lim           eni (x) = (1 - ) 0 E exp z (Wi , x) 0          .           (26)
                         n
                               i=1


This probability limit reflects the boundedness of average product degree in sparse
networks (i.e., in expectation, total sales of a product are finite, even asymptotically).
   6
     Wang (2020) scales by the square root of the number of events or "ones" in the dataset. This
is, of course, of the same order as n as defined here. This difference leads to a minor difference in
our two expressions for 0 . Making these adjustments the results coincide.


                                                  15
This result also holds within a sub-population of products with characteristics Xj = x.
Parameter (26) corresponds to a conditional version of p     0,n , the average product
degree parameter defined in Section 1 above.
   To derive the rate of convergence and limit distribution of   ^n (x) I proceed in the
usual way. A mean value expansion and Theorem (1) together yield

                                              N
                n (x) - 0 (x))  n
             n (^                                  {eni (x) - 0 (x)}
                                            i=1
                                            N
                                       +          ¯ni (x) [1 - e
                                                  e            ¯ni (x)]     1 z (Wi , x)        -
                                                                                                0
                                                                                                  1
                                                                                                          (27)
                                            i=1

                                       × n3/2 Sn (0,n ) .

By the conditional mean zero property of the score function, the two terms in (27)
are asymptotically uncorrelated. The variance of the first term in (27) is

                          N
              V       n         {eni (x) - 0 (x)}       = O n2 (1 - n ) 2
                                                                        n = O (1) ,
                          i=1


whereas the Jacobian in the second term has the approximation

   N
        ¯ni (x) [1 - e
        e            ¯ni (x)]      1 z (Wi , x)        =
  i=1
                                        N
                          0 (1 - n )
                                              exp z (Wi , x) 0            1 z (Wi , x)         + Op n-1 .
                              N         i=1

                   def                                                                                           
Defining 0 (x)  0 (1 - )               E exp z (Wi , x) 0             E exp z (Wi , x) 0 z (Wi , x)                 ,
                                                                                 ~c       ~p
0 (x) = lim n2 (1 - ) V (eni (x) - 0 (x)), and 0 = -                                                      -
                                                     1                                              ~3
                                                                                                            1
                                                   0                            1-
                                                                                  1
                                                                                      +   
                                                                                           1
                                                                                               +   (1-)   0
          n
suggest that
                  
                     n (x) - 0 (x))  N 0, 0 (x) + 0 (x) 0 0 (x)
                  n (^

as n  . Aggregate effects are estimable with the same degree of precision as the
logit coefficients themselves.




                                                      16
Average partial effects
Next consider estimating the average marginal effect of unit increases in the elements
of Zij on making a purchase:

                                               N       M
                                    1
                              ^n =                         ^nij [1 - e
                                                           e         ^nij ] Zij .                    (28)
                                   NM         i=1 j =1


                                                             ^
Recall that enij = e (0,n + Zij 0 ) and e ^nij = e  ^ n + Zij  . Interest in average par-
tial effects of this type is widespread in modern micro-econometric empirical research
(cf., Blundell and Powell, 2003; Wooldridge, 2005). Since (28) is an average of sum-
mands, each of which is O (n-1 ), we might expect some variance reduction relative
to the aggregate case just discussed. In a certain sense, this conjecture appears to be
correct.
    Define 0,n = EN,M [enij (1 - enij ) Zij ] = O (n ); a mean-value expansion and some
re-scaling yields

                                                N      M
            ^n - 0,n
                                        1
  n n 3/2
                            =n   3/2
                                                            [enij (1 - enij ) Zij - 0,n ]
               n                       NM      i=1 j =1
                                           N       M
                                    1
                            +n                         ¯nij [1 - e
                                                       e         ¯nij ] [1 - 2¯
                                                                              enij ]            
                                                                                       Zij Zij Zij
                                   NM     i=1 j =1

                                                                                                     (29)
                            × n1/2 ^n - 0,n .


As above, the conditional mean zero property of the score function ensures that
the two terms in (29) are asymptotically uncorrelated. We rescale the estimate and
parameter using n since 0,n  0 as n   (cf., Bickel et al., 2011). The need to
rescale the Jacobian in (29) stems from the observation that

            N    M
       1
  n                    ¯nij [1 - e
                       e         ¯nij ] [1 - 2¯
                                              enij ]                 
                                                            Zij Zij Zij         =
      NM    i=1 j =1
                                          N    M
                                   1                                                  
                                                       0 exp Zij 0           Zij Zij Zij    + Op n2
                                                                                                  n .
                                  NM     i=1 j =1




                                                       17
    Next observe that first term in (29) is a two-sample U-Statistics. A Hoeffding
variance decomposition gives

                 N     M
            1                                          c   p      1
       V                    enij (1 - enij ) Zij   =   1n
                                                          + 1n +          1n - 1n ]
                                                                    [2n - c    p
           NM    i=1 j =1
                                                       N    M    NM

with

                                                                         
                        1n = C (enij (1 - enij ) Zij , enik (1 - enik ) Zik )
                        c
                                                                         
                        1n = C (enji (1 - enji ) Zji , enki (1 - enki ) Zki )
                        p
                        2n = V (enij (1 - enij ) Zij ) .

                                              p
Inspection indicates that c            2              2     c       2
                            1n = O (n ), 1n = O (n ) and 2n = O (n ) and hence
that
                       N M                            ~c   ~p
                  1                                        
          nV3
                              enij (1 - enij ) Zij =    1
                                                          + 1 + O n-1 .
                 N M i=1 j =1                        1-     

   Putting these calculations together suggests that

                                ^n - 0,n
                                                         ~c
                                                             ~p
                                                             
                                                            + 1 + 0 0 0
                                              D
                     n n3/2                   N           1
                                   n                    1-    
           def
                 
with 0  0 E exp Zij 0                           
                                       Zij Zij Zij      .
    If we set T = N M = O (n2 ), then we have that T 1/4 ^n - 0,n has a Gaussian
limit distribution. The rate of convergence of ^n toward 0,n is slow. For average
partial effects we need to rescale in order ensure a meaningful probability limit. Let
                                                                                    
0 ,n = 0,n /n and similarly for   ^n ; the result above implies that T 1/4   ^n - 0 ,n is
also Gaussian. In this sense the rates-of-convergence for the logit coefficients and their
APEs coincide. However, if we think in terms of the resulting implied approximation
to the finite sample distribution of the two parameter estimates, we have V        ^n =
O T -1/2 = O (n-1 ), but V (^
                            n ) = O T -3/2 = O (n-3 ). In this sense inference on
APEs appears to be more precise.




                                                   18
Areas for additional research
For empirical researchers the main implication of this paper is to use an estimate for
the variance of SN that includes all components ­ even ones that are negligible under
certain asymptotic sequences ­ when constructing standard errors. This is not a new
idea. In the context of U-statistics it goes back to Hoeffding (1948). It is implicit in
Holland and Leinhardt (1976) in their work on subgraph counts; see also the recent
work on dyadic regression by Fafchamps and Gubert (2007), Cameron and Miller
(2014) and Aronow et al. (2017), as well as that on density weighted average deriva-
tives by Cattaneo et al. (2014). However, the small amount of extant formal limit
theory for dyadic regression (cited earlier) suggests different approaches to variance
estimation. This paper has outlined an asymptotic framework that provides formal
justification for one of the leading "practical" approaches to inference in the presence
of dyadic dependence. Graham (2020a) discusses variance estimation for dyadic re-
gression in detail, advocating a variant of the estimate proposed by Fafchamps and
Gubert (2007), Cameron and Miller (2014) and Aronow et al. (2017). Theorem 1
provides a formal justification for this recommendation.
     Many outstanding questions remain. Can the above framework be generalized to
a generic dyadic M-estimation setting? What is the "general" notion of "sparseness"
needed for this? Extensions to semiparametric regression models are also of interest.
In recent work, Menzel (2017) and Davezies et al. (2020) propose bootstrap proce-
dures for dyadic regression. Are these procedures also valid under sparse network
asymptotics and, if not, how might they be adapted to be so? The aggregate and
average effect examples sketched above suggest that the systematic exploration of
policy analysis questions ­ considered under dense network asymptotics by Graham
(2020b) ­ would be interesting. Finally, although it seems likely that ­ in the absence
of imposing more structure ­ that the composite maximum likelihood estimator is
efficient, this is currently only a conjecture.


                                   Appendix
    The appendix includes proofs of the theorems stated in the main text as well as
statements and proofs of supplemental lemmata ­ called limonata here. All notation
is as established in the main text unless stated otherwise. Equation number continues
in sequence with that established in the main text.

                                          19
A      Preliminary lemmata and proofs
                     
Let Rij = 1, Zij        and note that, for e (v ) = exp (v ) / [1 + exp (v )] , we have that
e (v ) = e (v ) [1 - e (v )] and e (v ) = e (v ) [1 - e (v )] [1 - 2e (v )]. With this notation
we can write the first three derivatives of the kernel function of the composite log-
likelihood with respect n as

                     sij (n ) = (Yij - eij (n )) Rij                                        (30)
                 sij (n )                                 
                       
                          = -eij (n ) [1 - eij (n )] Rij Rij                                (31)
                     
                 sij (n )                                                 
                          = -eij (n ) [1 - eij (n )] [1 - 2eij (n )] Rij Rij Rp,ij          (32)
                  p

with (32) holding for for p = 1, . . . , dim (n ).
    Let t = (n - 0,n ) and recall that n = ln (/n) and 0,n = ln (0 /n). This
                                             
implies that t = ln (/0 ) , ( - 0 ) does not vary with n and hence that t 
T with T compact by Assumption 2. Associated with any t  T is a n  n ;
furthermore associated with this n is a   . With these preliminaries we can show
that nHn (n ) converges uniformly to  (), as defined in equation (16) of the main
text.

Lemma 1. (Uniform Hessian Convergence) Under Assumptions 1, 2 and 3

                                                           p
                               sup nHn (n ) -  ()  0.
                               


Proof. Recall that n = 0,n + t and hence that Hn (0,n + t) = Hn (n ) . The mean
value theorem, as well as compatibility of the Frobenius matrix norm with the Eu-
                                         t both in T,
clidean vector norm, gives for any t and ¯

                                       dim(n )         N    M
                                                  1                  sij (0,n + t)
Hn (0,n + t) - Hn (0,n + ¯
                         t)2,1                                                              t - ¯
                                                                                                t2 .
                                         p=1
                                                 NM    i=1 j =1
                                                                          p
                                                                                        F


Since E [eij (n ) [1 - eij (n )] [1 - 2eij (n )]] = O (n ) = O (n-1 ) we have that, inspect-




                                               20
ing (31) above, for any t  T,

                        N    M
                   1                      sij (0,n + t)
                                                                 = Op n-1 .
                  NM    i=1 j =1
                                               p
                                                             F


This gives nHN (0,n + t) - nHn (0,n + ¯
                                      t)2,1  Op (1) · t - ¯
                                                          t2 . Next, again recall-
ing that 0,n + t = n , we have that

                                      N    M
                              1                                             
          Hn (0,n + t) = -                     eij (n ) [1 - eij (n )] Rij Rij
                             NM     i=1 j =1
                                      N    M
                            1                                                    1
                        =-                       exp Rij  Rij Rij + Op                ,
                           NM       i=1 j =1
                                               n                                 n2

                                                                                  p
which gives, using a law of large numbers for U-Statistics, nHn (n )   () for all
t  T. The claim then follows from an application of Lemma 2.9 of Newey and
McFadden (1994, p. 2138).


B     Proof of Theorem 1
Asymptotic variance of the score
To prove (22), the decomposition of the variance of the score given in the main text,
and hence that
                                    ~c
                                        ~p
                                               ~3
                  V n3/2 Sn =        1
                                       + 1+         + O n-1
                                   1-        (1 - )

                   
Let eij = e 0,n + Zij 0 ; using the definitions given in (23) of the main text we have
that

                                                             
                       1n =E [(Y12 - e12 ) (Y13 - e13 ) R12 R13 ]
                       c
                            =O 2
                               n                                                          (33)




                                               21
and also that

                                                                  
                            1n =E [(Y21 - e21 ) (Y31 - e31 ) R21 R31 ] .
                            p
                                  =O 2
                                     n .                                                 (34)

   Turning to 2n and 3n we get that

                        2n =E [E [(Y12 - e12 ) R21 | W1 , X2 , A1 , B2 ]
                                  ×E [(Y12 - e12 ) R21 | W1 , X2 , A1 , B2 ]
                                =O 2
                                   n                                                     (35)

and that

                              3n =E {snij - s              ¯nij }
                                            ¯nij } {snij - s
                                      =O (n )                                            (36)

                            2
by virtue of the equality Yij = Yij (which holds because Yij is binary-valued).
   From (13) we have that n = O (n-1 ), hence (33) implies that n2 c   1n = O (1), (34)
      2 p                             2 p
that n 1n = O (1), and (36) that n 1n = O (1). This gives


                2                2              2                n
 V (Sn ) =O      n
                     +O           n
                                       +O        n
                                                       +O
                N                M              MN               MN
                        2                         3                    2
                  c
                  0,n       1               c
                                            0,n                  c
                                                                 0,n        1        c
                                                                                     0,n 1
           =O                     +O                   +O                       +O
                  M         N               M                    M         MN        M MN
                        2                                    3
                  c
                  0,n           1    1                 c
                                                       0,n       1
           =O                               +O
                  n         (1 - n ) n3                n         n3
                             2
                     c
                     0,n             1      1                   c0,n    1
             +O                                       +O
                     n           n (1 - n ) n4               n (1 - n ) n3
                                                             2


           =O n3 + O n3 + O n4 + O n3 ,

as needed.




                                                  22
Triangular array setup
Consider the following triangular array {Znt }:

                                                1 c
                                    Zn1 =        s
                                                 ¯
                                                N 1n1
                                          .
                                          .
                                          .
                                                1 c
                                    ZnN =        s
                                                 ¯
                                                N 1nN
                                                1 p
                                 ZnN +1       =   s
                                                  ¯
                                                M 1n1
                                          .
                                          .
                                          .
                                                1 p
                                ZnN +M =          s
                                                  ¯
                                                M 1nM
                                                 1
                              ZnN +M +1       =     (sn11 - s
                                                            ¯n11 )
                                                NM
                                          .
                                          .
                                          .
                                                 1
                            ZnN +M +N M =          (snN M - s
                                                            ¯nN M ) ,
                                                NM

with T = T (n) = N + M + N M . For any vector Xi , let X1  t
                                                             = (X1 , . . . , Xt ) . Iterated
expectations, as well as the conditional independence relationships implied by dyadic
dependence (Assumptions 1 and 2), yield

                                              i-1
                                     E Zni | Zn 1 = 0,

establishing that {ZN i } is a martingale difference sequence (MDS). The variance of
this MDS is
                                                   T
                                 ¯ n def
                                      V                 Zni
                                                  t=1
                                      c    p     3n
                                     = 1n + 1n +    .
                                       N    M    NM

    To show asymptotic normality of N 3/2 Sn (0,n ) I first show, recalling decomposi-
tion (17) in the main test, that, for a vector of constants c,

                            -1/2                        -1/2 
                   c ¯ nc       c S n = c ¯ nc                c [U1n + Vn ] + op (1)   (37)


                                                  23
and subsequently that

                                      -1/2                       p
                          c ¯ nc             c [U1n + Vn ]  N (0, 1) .         (38)

   To show (37) observe that

                                      2   2     n
                  c ¯ nc = O           n
                                         + n +
                                      N   M    NM
                                       2
                                      n     1     1   1
                         =O                    +    +
                                      n 1 - n n (1 - n ) c
                                                         n
                                       2
                                      n
                         =O
                                      n

                         -1
and hence that c ¯ Nc    = O (n-  n ) as long as n  C > 0 and   (0, 1) (see
                                   2               c

Assumptions 1 and 2). Additionally using (35) yields

                                 -1/2 
                        c ¯ nc            c U2n = Op n1/2 -
                                                          n
                                                            1
                                                              Op  2
                                                                  n

                                                    = Op n1/2 n
                                                    = op (1) ,

as long as n = O (n- ) for  > 2
                              1
                                , as is maintained here. This proves assertion (37).


Central limit theorem
To show (38) I verify the conditions of Corollary 5.26 of Theorem 5.24 in White
(2001); specifically the Lyapunov condition, for r > 2

                              T (n)                          r
                                                   c Znt
                                      E             ¯ nt c       = o (1)       (39)
                              t=1
                                                  c 
and the stability condition
                                          T (n)
                                                  (c ZN t )2 p
                                                   c ¯ N c  1.                 (40)
                                          t=1




                                                      24
I will show (39) for r = 3. Observe that

                                                               3
                                             1  c                       3
                                         E    cs
                                               ¯                   =O    n
                                             N 1ni                      N3
                                                               3
                                             1  p                       3
                                         E    cs
                                               ¯                   =O    n
                                             M 1ni                      M3
                                                               3
                                  1                                       n
                           E        c (sn11 - s
                                              ¯n11 )               =O
                                 NM                                     N 3M 3

These calculations, as well as independence of summands 1 to N , N + 1 to N + M
and N + M + 1 to N + M + N M , imply that

                           
T (n)                  3
              
             c ZN t                                       3             3                  n
        E      ¯ Nc
                            =Op n3/2 -
                                     N
                                       3
                                                 O         n
                                                                   +O    n
                                                                                 +O
t=1
             c                                            N2            M2               N 2M 2
                                          1                             1                        1
                               =Op                         + Op                   + Op
                                     (1 - n )2 n1/2                 2
                                                                    nn
                                                                      1/2
                                                                                          (1 - n )2 n
                                                                                                    c n1/2

                                Op n-1/2
                                op (1)

as required.
    To verify the stability condition (40) I re-write it as

                       T (n)
                                  1                2             2                p

                               n c ¯ n c n (c Znt ) - E (c Znt )                 0                        (41)
                       t=1

                  -1
Since n c ¯ Nc   = O n · n- N
                             2
                                = O (1) the stability condition (40) will hold if the
numerator in (41) ­ t=1 n (c Znt )2 - E (c Znt )2 ­ converges in probability to
                      T (n)    

zero. Expanding the square we get that

                                             2                                                    2
            n (c Znt ) - E (c Znt )              = n2 E (c Znt )                - E (c Znt )
                       2                 2                                  4                 2
        E                                                                                             .




                                                     25
We then have
                 
                    1  c                             c
                                                                   2
                       c 1n c            =O           n                1
                                                                              , t = 1, . . . , N
                 
                    N2                            n (1-n )             n4
                                                      2
E (c Znt )
         2
             =      1  p                          c       1
                       c 1n c            =O       n
                                                                   ,              t = N + 1, . . . , N + M
                 
                    M2                            2
                                                  n       n4
                       1
                           c 3N c            =O         cn   1
                                                           2 5               , t = N + M + 1, . . . , N + M + N M
                    N 2M 2                          n -n ) n
                                                    3 (1


and
                        [            4
                                         ]
                 
                    E       (c s
                               ¯c1n1 )               1    4
                                             =O            n
                                                                       ,             t = 1, . . . , N
                        [
                              N4
                                     4
                                         ]        (1-n )4 n4

E (c Znt )
         4
             =      E       (c s
                               ¯p1n1 )                4
                                                                                                                         .
                                = O 14       n
                                               ,                                     t = N + 1, . . . , N + M
                 
                       M 4
                                          n n
                                             4

                  E[(c (sn11 -s
                                     4
                              ¯n11 )) ]          1                           n
                         N 4M 4
                                        = O 4 (1-                        4   n8
                                                                                  , t = N + M + 1, . . . , N + M + N M
                                                          n            n)



Since   T (n)    =    N + M + NM            =    O (n2 ), the summands of
                                 
  1
T (n) t=1 T (n) n (c Znt ) - E (c Znt )
      T (n)               2            2
                                         all have variances which are O (n-2 )
or smaller:

                                                               2
T (n)2 n2 E (c Znt ) - E (c Znt )
                                4                     2
                                                                       =

                -8         -8          -2
       2 2
 T (n) n [O (n ) + O (n )] = O (n ) ,                                      t = 1, . . . , N
 T (n)2 n2 [O (n-8 ) + O (n-8 )] = O (n-2 ) ,                              t = N + 1, . . . , N + M

 T (n)2 n2 [O (n-9 ) + O (n-10 )] = O (n-3 ) ,                             t = N + M + 1, . . . , N + M + N M

Since the summands of the numerator in (41) are all mean zero with variances shrink-
ing to zero as n   condition (41) holds as required.


References
Aldous, D. J. (1981). Representations for partially exchangeable arrays of random
  variables. Journal of Multivariate Analysis, 11(4):581 ­ 598.

Amemiya, T. (1985). Advanced Econometrics. Harvard University Press.

Aronow, P. M., Samii, C., and Assenova, V. A. (2017). Cluster-robust variance
  estimation for dyadic data. Political Analysis, 23(4):564 ­ 577.


                                                           26
Bengtsson, O. and Hsu, D. H. (2015). Ethnic matching in the u.s. venture capital
  market. Journal of Business Venturing, 30(2):338 ­ 354.

Bickel, P. J., Chen, A., and Levina, E. (2011). The method of moments and degree
  distributions for network models. Annals of Statistics, 39(5):2280 ­ 2301.

Blundell, R. and Powell, J. L. (2003). Advances in Economics and Econometrics:
  Theory and Applications, Eighth World Congress, volume 2, chapter Endogeneity in
  nonparametric and semiparametric regression models, pages 312 ­ 357. Cambridge
  University Press.

Cameron, A. C. and Miller, D. L. (2014). Robust inference for dyadic data. Technical
  report, University of California - Davis.

Cattaneo, M., Crump, R., and Jansson, M. (2014). Small bandwidth asymptotics for
  density-weighted average derivatives. Econometric Theory, 30(1):176 ­ 200.

Chamberlain, G. (1984). Handbook of Econometrics, volume 2, chapter Panel Data,
 pages 1247 ­ 1318. North-Holland, Amsterdam.

Chartrand, G. and Zhang, P. (2012). A First Course in Graph Theory. Dover Publi-
 cations.

Crane, H. and Towsner, H. (2018). Relatively exchangeable structures. Journal of
  Symbolic Logic, 83(2):416 ­ 442.

Davezies, L., d'Haultfoeuille, X., and Guyonvarch, Y. (2020). Empirical process
  results for exchangeable arrayes. Annals of Statistics.

de Finetti, B. (1931). Funzione caratteristica di un fenomeno aleatorio. Atti della
  R. Academia Nazionale dei Lincei, Serie 6. Memorie, Classe di Scienze Fisiche,
  Mathematice e Naturale, 4:251 ­ 299.

Fafchamps, M. and Gubert, F. (2007). The formation of risk sharing networks. Jour-
  nal of Development Economics, 83(2):326 ­ 350.

Fox, J. T. (2018). Estimating matching games with transfers. Quantitative Economics,
  9(1):1 ­ 38.



                                        27
Graham, B. S. (2017). An econometric model of network formation with degree
  heterogeneity. Econometrica, 85(4):1033 ­ 1063.

Graham, B. S. (2020a). The Econometrics of Social and Economic Networks, chapter
  Dyadic regression, pages 25 ­ 41. Elsevier, Amsterdam.

Graham, B. S. (2020b). Handbook of Econometrics, volume 7, chapter Network data.
  North-Holland, Amsterdam.

Graham, B. S., Imbens, G. W., and Ridder, G. (2018). Identification and efficiency
  bounds for the average match function under conditionally exogenous matching.
  Journal of Business and Economic Statistics.

Graham, B. S., Niu, F., and Powell, J. L. (2019). Kernel density estimation for
  undirected dyadic data. Technical report, University of California - Berkeley.

Hoeffding, W. (1948). A class of statistics with asymptotically normal distribution.
 Annals of Mathematical Statistics, 19(3):293 ­ 325.

Holland, P. W. and Leinhardt, S. (1976). Local structure in social networks. Socio-
  logical Methodology, 7:1 ­ 45.

Hoover, D. N. (1979). Relations on probability spaces and arrays of random variables.
 Technical report, Institute for Advanced Study, Princeton, NJ.

Jochmans, K. (2018). Semiparametric analysis of network formation. Journal of
  Business and Economic Statistics, 36(4):705 ­ 713.

King, G. and Zeng, L. (2001). Logistic regression in rare events data. Political
  Analysis, 9(2):137 ­ 163.

KPMG (2016). Competitive alternatives: Kpmg's guide to international business
 locations costs. Technical report, KMPG International Cooperative.

Lewbel, A. and Nesheim, L. (2019). Sparse demand systems: corners and comple-
  ments. CeMMAP Working Paper CWP45/19, Centre for Microdata Methods and
  Practice.

Lindsey, B. G. (1988). Composite likelihood. Contemporary Mathematics, 80:221 ­
  239.

                                         28
Marotta, L., Miccichè, S., Fujiwara, Y., Iyetomi, H., Aoyama, H., Gallegati, M.,
 and Mantegna, R. N. (2015). Bank-firm credit network in japan: an analysis of a
 bipartite network. Plos One, 10(5):e0123079.

Menzel, K. (2017). Bootstrap with clustering in two or more dimensions. Technical
 Report 1703.03043v2, arXiv.

Newey, W. K. and McFadden, D. (1994). Handbook of Econometrics, volume 4,
  chapter Large sample estimation and hypothesis testing, pages 2111 ­ 2245. North-
  Holland, Amsterdam.

Owen, A. B. (2007). Infinitely imbalanced logistic regression. Journal of Machine
 Learning Research, 8:761 ­ 773.

Staiger, D. and Stock, J. H. (1997). Instrumental variables regression with weak
  instruments. Econometrica, 65(3):557 ­ 586.

van der Vaart, A. W. (2000). Asymptotic Statistics. Cambridge University Press,
  Cambridge.

Wang, H. (2020). Pmlr. In Proceedings of the 37 th International Conference on
 Machine Learning, number 119.

White, H. (2001). Asymptotic Theory for Econometricians. Academic Press, San
 Diego.

Wooldridge, J. M. (2005). Identification and inference for econometric models, chapter
 Unobserved heterogeneity and the estimation of average partial effects, pages 27 ­
 55. Number 3. Cambridge University Press, Cambridge.




                                         29
