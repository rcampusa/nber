                              NBER WORKING PAPER SERIES




    PIECEWISE-LINEAR APPROXIMATIONS AND FILTERING FOR DSGE MODELS
                WITH OCCASIONALLY BINDING CONSTRAINTS

                                        S. Boraan Aruoba
                                        Pablo Cuba-Borda
                                        Kenji Higa-Flores
                                        Frank Schorfheide
                                         Sergio Villalvazo

                                       Working Paper 27991
                               http://www.nber.org/papers/w27991


                     NATIONAL BUREAU OF ECONOMIC RESEARCH
                              1050 Massachusetts Avenue
                                Cambridge, MA 02138
                                    October 2020



We are thankful for helpful comments and suggestions from participants of the 2018 and 2019
MFM conferences, the 2019 conference of the Society for Nonlinear Dynamics, and the
Alejandro Justiniano Memorial conference. Much of this paper was written while Aruoba and
Schorfheide visited the Federal Reserve Bank of Philadelphia, whose hospitality they are thankful
for. Higa-Flores and Villalvazo gratefully acknowledge financial support from the Becker
Friedman Institute under the Macro Financial Modeling Project. Aruoba and Schorfheide
gratefully acknowledge financial support from the National Science Foundation under Grant SES
1851634. The views expressed in this paper are solely the responsibility of the authors and should
not be interpreted as reflecting the views of the Board of Governors of the Federal Reserve
System, any other person associated with the Federal Reserve System, or the National Bureau of
Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.

© 2020 by S. Boraan Aruoba, Pablo Cuba-Borda, Kenji Higa-Flores, Frank Schorfheide, and
Sergio Villalvazo. All rights reserved. Short sections of text, not to exceed two paragraphs, may
be quoted without explicit permission provided that full credit, including © notice, is given to the
source.
Piecewise-Linear Approximations and Filtering for DSGE Models with Occasionally Binding
Constraints
S. Boraan Aruoba, Pablo Cuba-Borda, Kenji Higa-Flores, Frank Schorfheide, and Sergio
Villalvazo
NBER Working Paper No. 27991
October 2020
JEL No. C5,E4,E5

                                          ABSTRACT

We develop an algorithm to construct approximate decision rules that are piecewise-linear and
continuous for DSGE models with an occasionally binding constraint. The functional form of the
decision rules allows us to derive a conditionally optimal particle filter (COPF) for the evaluation
of the likelihood function that exploits the structure of the solution. We document the accuracy of
the likelihood approximation and embed it into a particle Markov chain Monte Carlo algorithm to
conduct Bayesian estimation. Compared with a standard bootstrap particle filter, the COPF
significantly reduces the persistence of the Markov chain, improves the accuracy of Monte Carlo
approximations of posterior moments, and drastically speeds up computations. We use the
techniques to estimate a small-scale DSGE model to assess the effects of the government
spending portion of the American Recovery and Reinvestment Act in 2009 when interest rates
reached the zero lower bound.

S. Boraan Aruoba                                 Frank Schorfheide
Department of Economics                          University of Pennsylvania
University of Maryland                           Department of Economics
3105 Tydings Hall                                The Ronald O. Perelman Center for
College Park, MD 20742                           Political Science and Economics
aruoba@econ.umd.edu                              133 South 36th Street
                                                 Philadelphia, PA 19104
Pablo Cuba-Borda                                 and NBER
Board of Governors of the                        schorf@ssc.upenn.edu
Federal Reserve System
20th Street and Constitution Avenue, NW          Sergio Villalvazo
Washington, DC 20551                             Department of Economics
pablo.a.cubaborda@frb.gov                        University of Pennsylvania
                                                 The Ronald O. Perelman Center
Kenji Higa-Flores                                for Political Science and Economics
Department of Economics                          133 South 36th Street
University of Maryland                           Philadelphia, PA 19104
3105 Tydings Hall                                vsergio@sas.upenn.edu
College Park, MD 20742
kenjihf@umd.edu


A data appendix is available at http://www.nber.org/data-appendix/w27991
This Version: October 9, 2020                                                                  1


1     Introduction

Dynamic stochastic general equilibrium (DSGE) models with financial frictions are widely
used in central banks, by regulators, and in academia to study the effects of monetary and
macroprudential policies and the propagation of shocks in the macro economy. The most
recent vintage of these models involves occasionally binding constraints arising from financial
frictions and the effective lower bound (ELB) on nominal interest rates. In order for these
models to be usable for a quantitative analysis, they need to be solved numerically, and their
parameters need to be estimated based on historical data.

    Two types of solution approaches for models with occasionally binding constraints have
been used in the literature. The first group of solution algorithms can be broadly clas-
sified as global methods. Agents' decision rules (or value functions associated with opti-
mization problems) are represented by a family of flexible functions--for example, Cheby-
shev polynomials--or by a discrete mapping on a finite state-space domain. The flexible
functions are parameterized by coefficients that are chosen such that the resulting decision
rules (approximately) satisfy the model's equilibrium conditions and solve the underlying
intertemporal optimization problems. Examples of this approach include Christiano and
Fisher (2000), Adam and Billi (2007), Fern´
                                          andez-Villaverde, Gordon, Guerr´
                                                                         on-Quintana,
and Rubio-Ram´
             irez (2015), Maliar and Maliar (2015), Nakata (2016), Gust, Herbst, Lopez-
Salido, and Smith (2017), Aruoba, Cuba-Borda, and Schorfheide (2018), Mendoza and Vil-
lalvazo (2020), and Atkinson, Richter, and Throckmorton (2020).

    The second type of solution approaches are variants of the extended perfect-foresight
path (EPFP) method that build on Fair and Taylor (1983). These algorithms rely on the
assumption that, after H periods, the system reverts back to the steady state in which the
constraint, say, is non-binding. With an initial guess about whether the constraint is binding
in periods t + h, h = 1, . . . , H , it is possible to solve the dynamic system for the values of
the endogenous variables. One can then compare the initial guess about the duration of the
binding regime to the backward solution and iterate until consistency is achieved. Because
the computations are based on the initial state, the previously described steps need to be
repeated for every t in a multi-period simulation. Variants of this approach have been
used in Eggertsson and Woodford (2003), Christiano, Eichenbaum, and Trabandt (2015),
Guerrieri and Iacoviello (2015), Kulish, Morley, and Robinson (2017), Holden (2019), and
Boehl (2019). The Guerrieri and Iacoviello (2015) paper is accompanied by a popular model
This Version: October 9, 2020                                                               2


solution toolbox called OccBin that implements a variant of the EFPF approach. We will
refer to OccBin in various instances throughout our paper.

    Given the model solution, one then constructs a state-space representation for an es-
timable empirical model. The solution itself generates the state transition equations. A
set of measurement equations can then be specified that links the state variables with the
observables. Because the model solution is nonlinear, so is the state-space representation.
Thus, a nonlinear filter is required to compute the likelihood function. For instance, in the
context of DSGE models with an ELB constraint, Gust et al. (2017) and Aruoba et al. (2018)
use a particle filter in combination with a global solution to construct likelihood functions.
Guerrieri and Iacoviello (2017) use an EPFP solution for a model in which the number of
observables equals the number of structural shocks and combine it with an inversion filter
that essentially solves for the innovations as a function of the observables conditional on an
initial state.

    Against this backdrop, the contribution of our paper is to construct an alternative model
solution that (i) is able to capture an important aspect of the decision rule nonlinearity
generated by an occasionally binding constraint, (ii) can be solved quickly, and (iii) allows
us to derive an accurate and fast filter for the evaluation of the likelihood function that
exploits the structure of the solution. One of our goals is to make the procedure efficient
enough that it can be run on a desktop computer in a reasonable amount of time. For
instance, the small-scale New Keynesian model in our empirical application is estimated
using U.S. data from 1984 to 2018 in about 13.5` hours on a single core.

    The basic idea of the proposed solution method is to approximate agents' decision rules
globally by piecewise-linear functions that are continuous but have a kink along the locus
of the state space in which a constraint becomes binding. The coefficients of the decision
rules are determined to ensure that the model's equilibrium conditions are (approximately)
satisfied. The equilibrium conditions typically take the form of nonlinear expectational
difference equations. We require that the (potentially transformed) state variables enter
the occasionally-binding constraint linearly. For the remaining equilibrium conditions a
(log)linearization is optional. In determining the decision rule coefficients, we take account
of the fact that, in the next period, the constraint could either be binding or non-binding.
Thus, we are capturing precautionary behavior. Importantly, the decision rule coefficients
only have to be computed once (as opposed to for each period t separately as in the EPFP
approach). Compared with higher-order Chebyshev polynomials, the piecewise-linearity and
This Version: October 9, 2020                                                                  3


continuity at the kink drastically reduce the number of coefficients that need to be determined
and hence simplify computations.

   The motivation for the piecewise-linear functional form is twofold. First, we have ob-
served in a number of models that more densely parameterized nonlinear decision rules look
approximately piecewise linear. For instance, in Aruoba et al. (2018) we considered a New
Keynesian DSGE model and stitched together higher-order Chebyshev polynomials along
the locus in the state-space where the ELB constraint becomes binding. We found that the
decision rules on both sides of the kink are approximately linear. In the Online Appendix we
solve a consumption-savings model with an occasionally-binding borrowing constraint and
demonstrate that a global solution technique produces approximately piecewise linear deci-
sion rules. Second, in Section 3 we solve a simplified version of the New Keynesian DSGE
model with an ELB constraint and show that the piecewise-linear structure is exact.

   To solve the nonlinear filtering problem, we develop a conditionally optimal particle filter
(COPF). A particle filter is a stochastic algorithm that approximates the distribution of a
vector of hidden states st conditional on the sequence of observations Y1:t available in time t
by a swarm of M particle values and weights {sj     j M
                                              t , Wt }j =1 . Because of its stochastic structure,

repeated runs of the filter generate a distribution of likelihood values. An important property
of the particle filter is that the average likelihood across repeated runs is equal to the exact
likelihood (unbiasedness). The tuning of the particle filter determines the precision of the
approximation. A key step in the specification of the algorithm is the mutation of time t-1
particle values into time t particle values. We show how, in the case of a piecewise linear
DSGE model solution, the mutation step can be executed optimally, conditional on the stage
t-1 particle values.

   In a sequence of numerical illustrations based on a small-scale New Keynesian DSGE
model with an ELB constraint, we document important properties of our solution algorithm
and the COPF likelihood approximation. We show that, compared to a naive bootstrap
particle filter (BSPF), which mutates particle values by simulating the model solution for-
ward, our COPF drastically reduces the variance of the likelihood approximation holding
the runtime fixed. In practice, this allows us to run the COPF with far fewer particles
than the BSPF (150 for COPF versus 1,000 for BSPF in our experiments), which in turn
speeds up the computations. When we embed the more accurate COPF into a random
walk Metropolis­Hastings (RWMH) algorithm, we are able to significantly reduce the per-
sistence of the resulting Markov chain and therefore improve the accuracy of Monte Carlo
approximations of moments of the posterior distribution.
This Version: October 9, 2020                                                                            4


    A key feature of our paper is that it integrates model solution, likelihood approximation,
and Bayesian estimation. There are a few papers that assess the interplay of existing model
solution and likelihood evaluation techniques in Monte Carlo experiments. The ones most
closely related to our work are Cuba-Borda, Guerrieri, Iacoviello, and Zhong (2019) and
Atkinson et al. (2020).1 Cuba-Borda et al. (2019) take a simple consumption-savings model
subject to a borrowing constraint. They illustrate that less accurate solution methods affect
inference even when the inversion filter is available. They also show that, as one increases
the measurement error variance in the BSPF, the likelihood misspecification becomes more
problematic, making it harder to retrieve the parameter values that govern the data gener-
ating process (DGP). In their setting, measurement error and solution approximation error
make it difficult for the econometrician to identify the model regime that generates the data,
and this incorrect classification of regimes leads to a bias in parameter estimates. In our
empirical application, one of the observed time series allows us to exactly identify the regime,
and we modify the COPF to capture this feature.
    Atkinson et al. (2020) compare the performance of a fully nonlinear solution and a variant
of the BSPF for estimation, with the approximated solution using OccBin and the inversion
filter. They simulate data from a DSGE model that includes more frictions and shocks than
the model used for estimation, and the latter is close to the model we use in this paper
in terms of size. As such, their estimated model is misspecified with respect to the DGP.
Their results show that the nonlinear approach performs slightly better than the OccBin
approach, but the differences are small. Moreover, relative to the pseudo-true parameters,
the estimates from both approaches show biases in some key parameters, such as the degree
of price rigidities. Since the OccBin-inversion filter approach can be scaled up easily and is
faster, they argue that building a bigger and less misspecified model using this approach may
be preferable. Similarly, our method offers scalability, even without multicore processing
or distributed computing, and allows for more general model structures and state-space
representations than the inversion filter.
    Based on U.S. data from 1984 to 2018 on output growth, inflation, interest rates, and
the government-spending to GDP ratio, we estimate the small-scale DSGE model using our
proposed piecewise-linear and continuous (PLC) solution in combination with the COPF.
From the estimated model, we compute dollar-for-dollar government spending multipliers
associated with the increase in government spending that was part of the 2009 American
   1
    Boehl (2019) combines his model solution, which is a variant of the EPFP, with a variant of an ensemble
Kalman filter. His paper presents an application but does not focus on accuracy comparisons of solution
and estimation methods.
This Version: October 9, 2020                                                                5


Recovery and Reinvestment Act (ARRA). The counterfactual output levels are computed
by lowering the exogenous government spending process in the model by an amount that
is commensurable to the ARRA intervention and keeping all other exogenous processes at
their historical levels. We find that the ex post multiplier during the Great Recession, when
the United States was at the ELB, was larger than in normal times when interest rates were
positive, albeit with a mean estimate of 0.7 still small in absolute terms. One novel result
we show through counterfactuals is that, in 2009 and 2010, there was very little room for
the Federal Reserve to stimulate the economy with conventional monetary policy over and
above what the policy rule implied, because adverse shocks kept the desired interest rate
near zero despite the large expansionary fiscal policy due to ARRA.

    The remainder of the paper is organized as follows. Section 2 describes the small-scale
New Keynesian DSGE model with ELB constraint used in the subsequent analysis. In Sec-
tion 3 we solve a simplified version of the New Keynesian model and show that the resulting
decision rules are piecewise linear and continuous. We also provide a comparison to the Oc-
cBin solution. In Section 4, we describe how to impose continuity on piecewise-linear decision
rules and derive a canonical form for the DSGE model solution. Section 5 discusses how the
decision rule coefficients are determined to approximately satisfy the model's equilibrium con-
ditions. The COPF is derived in Section 6. Section 7 presents some numerical experiments
to document the accuracy of the likelihood approximation through the COPF, and Section 8
contains the empirical analysis. Finally, Section 9 concludes. Derivations and further im-
plementation details are provided in the Online Appendix. The Appendix also contains a
section that shows how to solve a consumption-savings model with an occasionally-binding
borrowing constraint using the techniques proposed in this paper and compares our PLC
solution to an "exact" solution and a solution constructed with OccBin.



2     A Prototypical New Keynesian DSGE Model

We will illustrate our solution and filtering methods based on a prototypical New Keynesian
DSGE model. The model is identical to the one used in Aruoba et al. (2018). Variants of
this model have been widely studied in the literature, and its properties are discussed in
detail in Woodford (2003). To make this paper self-contained and introduce some important
notation, we briefly describe the preferences and technologies of the agents in Section 2.1
and summarize the equilibrium conditions in Section 2.2.
This Version: October 9, 2020                                                                                    6


2.1     Preferences and Technologies

Households. Households derive utility from consumption Ct relative to an exogenous habit
stock and disutility from hours worked Ht .2 The households maximize

                                                                                    1+1/
                                               (Ct+s /At+s )1- - 1  H
                        Et            s dt+s                       - t+s                              ,         (1)
                               s=0
                                                      1-            1 + 1/

subject to the budget constraint

                      Pt Ct + Tt + Bt = Pt Wt Ht + Rt-1 Bt-1 + Pt Dt + Pt SCt .

Here  is the discount factor, dt is an exogenous shock to the discount factor, 1/ is the
intertemporal elasticity of substitution, and  is the Frisch labor supply elasticity. Pt is
the price of the final good. The households receive the real wage Wt in exchange for labor
services. Bt is the quantity of nominal bonds, which pay gross interest Rt . Furthermore, the
households receive profits Dt from the firms and pay lump-sum taxes Tt . SCt is the net cash
inflow from trading a full set of state-contingent securities.

Firms. The final-goods producers generate aggregate output Yt aggregating intermediate
goods Yt (j ), j  [0, 1]. Under the assumption of perfect competition and free entry, the
demand for the intermediate inputs and the price of the aggregate final good are given by
                                                                                                       
                                          -1/                               1                          -1
                                Pt (j )                                                     -1
                   Yt (j ) =                    Yt   and Pt =                   Pt ( j )         dj         ,   (2)
                                 Pt                                     0


respectively. We define inflation as t = Pt /Pt-1 .

    Intermediate good j is produced by a monopolist who has access to the production
technology
                                                Yt (j ) = At Ht (j ),                                           (3)

where At is an exogenous productivity process that is common to all firms and Ht (j ) is the
firm-specific labor input. Intermediate-goods-producing firms face quadratic price adjust-



   2
     The habit stock is proxied by the level of technology At , which ensures that the economy evolves along
a balanced growth path. Since we will not focus on it in the subsequent analysis, we do not make a money-
holding motive, such as valuing transaction services from real money balances, explicit in the description of
the environment. Such a motive is necessary to make the ELB a relevant constraint in a model like this.
This Version: October 9, 2020                                                                              7


ment costs of the form
                                                                          2
                                                           Pt (j )
                                  ACt (j ) =                        -¯        Yt (j ),
                                             2            Pt-1 (j )
where  governs the price stickiness in the economy and ¯ is a baseline rate of price change
that does not require the payment of any adjustment costs. In our quantitative analysis, we
set ¯ =  , where  is the target inflation rate of the central bank. Firm j chooses its labor
input Ht (j ) and the price Pt (j ) to maximize the present value of future profits

                     
                                       Pt+s (j )
                Et          s Qt+s|t             Yt+s (j ) - Wt+s Ht+s (j ) - ACt+s (j )          .       (4)
                     s=0
                                        Pt+s

Here, Qt+s|t is the time t value to the household of a unit of the consumption good in period
t + s, which is treated as exogenous by the firm.

Government Policies. Monetary policy is described by an interest rate feedback rule.
Because the ELB constraint is an important part of our analysis we introduce it explicitly
as follows:

                                                                          1              2 1-R
                                R                                    t           Yt               R
              Rt = max {1,    Rt e     R,t
                                             },    Rt    = r                                     Rt-1 ,   (5)
                                                                                 Yt-1

      
Here Rt is the systematic part of monetary policy which reacts to an inflation gap and an
output growth gap, r is the steady-state real interest rate,  is the target-inflation rate, 
is the growth rate of the economy, and              R,t   is a monetary policy shock.

   The government consumes a stochastic fraction of aggregate output. We assume that
government spending evolves according to

                                                                1
                                                  Gt =     1-        Yt                                   (6)
                                                                gt

where gt is an exogenous process. The government levies a lump-sum tax Tt (or provides a
subsidy if Tt is negative) to finance any shortfalls in government revenues (or to rebate any
surplus). Its budget constraint is given by

                              Pt Gt + Mt-1 + Rt-1 Bt-1 = Tt + Mt + Bt .                                   (7)


Exogenous shocks. The model economy is perturbed by four exogenous processes. Aggre-
This Version: October 9, 2020                                                                                      8


gate productivity evolves according to

                 ln At = ln  + ln At-1 + ln zt , where ln zt = z ln zt-1 + z                        z,t .         (8)

Thus, on average, the economy grows at the rate  , and zt generates exogenous stationary
fluctuations of the technology growth rate around this long-run trend. We assume that the
government spending shock follows the AR(1) law of motion

                             ln gt = (1 - g ) ln g + g ln gt-1 + g                 g,t .                          (9)

The shock to the discount factor evolves according to

                                            ln dt = d ln dt-1 + d          d,t                                   (10)

The monetary policy shock             R,t   is assumed to be serially uncorrelated. We stack the four
innovations into the vector       t   =[    z,t , g,t , d,t , R,t ]   and assume that       t    iidN (0, I ).


2.2     Equilibrium Conditions

Because the exogenous productivity process has a stochastic trend, it is convenient to char-
acterize the equilibrium conditions of the model economy in terms of detrended consumption
ct  Ct /At and detrended output yt  Yt /At .

    It is well known that the New Keynesian model features multiple equilibria. In one of the
equilibria, the so-called targeted-inflation equilibrium, the endogenous variables fluctuate
around the steady state in which inflation equals the value targeted by the central bank.
Another important equilibrium is the so-called deflation equilibrium where the economy
fluctuates around the so-called deflation steady state in which nominal interest rates are
zero.3 In the remainder of the paper we mostly focus on the targeted-inflation equilibrium,
though we also discuss a deflation equilibrium in Section 3. The former is essentially the
equilibrium that arises in linearized New Keynesian DSGE models, adjusted for the presence
of the ELB constraint. The corresponding steady state is given by

                                                                       
                                                                                   1                y
                  ,    r =    ,        R = r  ,            y = (1 -  )g           +1/
                                                                                        ,   c =       .          (11)
                                                                                                    g

   3
     See, for instance, Benhabib et al. (2001), Aruoba and Schorfheide (2016) and Aruoba et al. (2018) for
a discussion of multiplicity of equilibria in this model.
This Version: October 9, 2020                                                                                         9


Without loss of generality, for any variable xt we can define the percentage deviations from
                    ^t = ln xt - ln x . Using this notation we can substitute xt by x ex
the steady state as x                                                                  ^t 4
                                                                                         . Our
goal is to write the equilibrium conditions as a system of expectational difference equations
of the form
                            Et R(^    ^t , 
                                 yt , c         ^t, y
                                           ^t , R   ^t+1 , c
                                                           ^t+1 ,        ^ t+1 , . . .) = 0,
                                                                  ^t+1 , R                                          (12)

where R(·) captures residuals in the equilibrium conditions.

    The residual function comprises of the following elements. The consumption Euler equa-
tion leads to
                        Rc (·) = d      ^t -  (^
                                 ^t+1 - d      ct+1 - c      ^t - 
                                                      ^t ) + R    ^t+1 - z
                                                                         ^t+1 .                                     (13)

In a symmetric equilibrium, in which all firms set the same price Pt (j ), the price-setting
decision of the firms leads to

                        1      1      1                                                         1               1
       R (·) = ln                + 1-              ^t +^
                                                 e c   yt /
                                                               - 2
                                                                   e^t
                                                                       -1                 1-           e^t
                                                                                                           +        (14)
                                                                                               2               2
                                  ^     ^
                    +2
                       edt+1 -dt                  ct+1 -c
                                              e- (^     ^t )    ^t+1 -y
                                                               ey     ^t
                                                                            e^t+1
                                                                                  - 1 e^t+1
                                                                                            .


The aggregate resource constraint leads to

                                                          1                           2
                             Ry (·) = y
                                      ^t - c
                                           ^t + ln        g^
                                                              - g  e^t
                                                                       -¯                 .                         (15)
                                                         e  t  2

It reflects both government spending as well as the resource cost (in terms of output) caused
by price changes. The monetary policy rule generates the residual function

          ^ t - max
 RR (·) = R                 (1 - R ) 1         yt - y
                                       ^t + 2 (^    ^t-1 + z
                                                           ^t ) + R R
                                                                    ^ t-1 + R                  R,t ,   - ln(r ) . (16)

We stack the residual functions for the exogenous shocks as follows:
                                                                                 
                                                  ^t - z z
                                                  z      ^t-1 - z          z,t
                                                  ^       ^t-1 - d               
                                                  dt - d d                 d,t   
                                      Rexo (·) =                                 .                                  (17)
                                                  ^t - g g
                                                         ^t-1 - g
                                                  g                              
                                                                           g,t   
                                                  eR,t - R R,t


   4
     Introducing x
                 ^t does not imply that we are log-linearizing all of the equilibrium conditions. It is foremost
a reparameterization. However, in our model it happens to be the case that the consumption Euler equation
and (abstracting from the max operator) the monetary policy rule are log-linear.
This Version: October 9, 2020                                                                                            10


3        Solving a Simplified Version of the DSGE Model

In order to highlight some important features of the proposed solution method, we first
consider a highly simplified version of the DSGE model introduced in Section 2 that can be
(almost) solved analytically.5 In particular, we will show that the PLC form emerges as the
exact solution to the simplified DSGE model. We also discuss the multiplicity of solutions
and provide a comparison to OccBin.

    On the model described in the previous section we impose the parameter restrictions
 = 1,  = 1,  = , g = 1, ¯ =  , 1 =  , 2 = 0, R = 0, z = 0, g = 0, and
d = 0. We log-linearize the equilibrium conditions (except for the ELB constraint) around
the targeted inflation steady state in (11) and regard the resulting equations as the model to
be solved. Some details of the calculations are relegated to the Online Appendix.

Equilibrium Conditions. The residual functions (13), (14), and (16) simplify to

                                         ^t+1 - d
                                Rc (·) = d      ^t - (^
                                                      ct+1 - c      ^t - 
                                                             ^t ) + R    ^t+1
                               R (·) = ^t -  ^t+1 - c
                                                    ^t                                                                  (18)
                                        ^ t - max
                               RR (·) = R                      ^ t + R     R,t ,   - ln(r )

Under parameterizations in which monteary policy is active, i.e.,  > 1, the model has typi-
                                        ^t, c
cally two stationary solution in which (R   ^t , ^t ) are independently and identically distributed
(iid) over time. These correspond to the targeted-inflation and the deflation equilibria de-
fined in the previous section.

    In both equilibria expected consumption and inflation are time invariant and can be
replaced by µc = Et [^
                     ct+1 ] and µ = Et [^
                                        t+1 ]. Thus, setting the expected value of the residual
function to zero, and conducting a few basic algebraic manipulations, we obtain:

    ^(                                      1
    R     d,t , R,t )   = max                 µc +  ( +  )µ + d                          d,t   + R   R,t   , - ln(r )
                                         1 + 
    ^(
    c     d,t , R,t )
                           ^(
                        = -R           d,t , R,t )   + µc + µ +  d   d,t                                                (19)
    ^(
          d,t , R,t )
                           ^(
                        = -R            d,t , R,t )   + µc + ( +  )µ + d           d,t


                 ^t, c
Here we replaced R   ^t , and ^t by decision rules that are time-invariant functions of the
state variables (       d,t , R,t ).


    5
        A similar model was solved in Mendes (2011).
This Version: October 9, 2020                                                                                 11


Constructing a Solution. In order to solve the system (19) we need to find constants µc
and µ such that
                                 µc = E[c(     d,t , R,t )]   and µ = E[ (      d,t , R,t )].


Because of the max operator in the monetary policy rule, this requires the computation
of the mean of a truncated Normal random variable. We rotate the vector of innovations
[   d,t , R,t ]    to separate the component that enters the monetary policy rule from a second
component that is orthogonal:

              1                                         1                                               2
    1,t =       (d        d,t   + R   R,t ),   2,t =      (R    d,t   - d   R,t ),    =         (d )2 + R . (20)
                                                        

By construction, the innovations 1,t and 2,t are also N (0, 1).

       Using the expression for 1,t in (20) we can rewrite the interest rate rule as

                                             1
                    R(1,t ) = max              µc +  ( +  )µ +  1,t , - ln(r ) .                            (21)
                                          1 + 

Define the cutoff value

                                         1
                                ¯1 = -
                                           (1 + ) ln(r ) + µc +  ( +  )µ                                    (22)
                                         

such that R(1,t ) = - ln(r ) whenever 1,t  1 . Using the formula for the mean of a
truncated standard normal random variable, we obtain

                  E[R(1,t )] = -N (¯ 1 ) ln(r )                                                             (23)
                                    1
                               +           1 - N (¯
                                                  1 ) µc +  ( +  )µ +  N (¯
                                                                          1 ) .
                                 1 + 

Here N (·) and N (·) are the cumulative density function (cdf) and the probability density
function (pdf) of a standard Normal random variable. Taking expectations of the second
                                                  ^ ( d,t , R,t )] = E[R(1,t )] using (23) leads
and third equation in (19) and substituting out E[R
to the following nonlinear system of equations:

          1 - N (¯
                 1 )                  N (¯
                                         1 )
µc =                 µc + (1 -  )µ -         + N (¯
                                                  1 ) ln(r ) + µc + µ .     (24)
            1 +                      1 + 
          1 - N (¯
                 1 )                  N (¯1 )
µ       =            µc + ( +  )µ -           + N (¯
                                                   1 )  ln(r ) + µc + ( +  )µ ,
            1 +                      1 + 
This Version: October 9, 2020                                                                                    12


where ¯1 is given in (22). Conditional on ¯1 the system is linear in (µc , µ ) which means
that it can be reduced to a single nonlinear equation in terms of ¯1 that needs to be solved
numerically. This equation typically has two solutions, which generate the targeted-inflation
and deflation equilibrium, respectively.

Properties of the Solution. (i) The decision rules are piecewise-linear and continuous for
                                            ^ ( d,t , R,t ) = R(1, ) in (23) into the consumption
each (µc , µ ). Plugging the expression for R
and inflation decision rules in (19) and using the relationship between the                       t 's   and t 's in
(20), we can rewrite the decision rules for consumption and inflation as follows:
                  
                       1                                            1
                  
                     1+
                           µc + (1 -  )µ + d,1 -     1,t + d,2  2,t if 1,t > 
                                                                  1+
                                                                               ¯1
c(1,t , 2,t ) =                                                                  (25)
                 ln(r ) + µc + µ + d,1  1,t + d,2  2,t                otherwise
                
                
                 1 µ + ( +  )µ +   - 1   +   
                 1+      c                  d,1  1+     1,t    d,2  2,t if 1,t >  ¯1
 (1,t , 2,t ) =
                  ln(r ) + µc + ( +  )µ + d,1  1,t + d,2  2,t           otherwise
                


Here, the  constants are functions of the structural parameters obtained from (20) by re-
arranging the equations to express the        t 's   as a function of the t 's. The ELB becomes
binding at the locus in the state space defined by

                                              1
                              ¯1 = 1,t =        (d          d,t   + R    R,t ).                                (26)
                                              

The second innovation, 2,t , does not enter the policy rule and therefore cannot push the
economy toward the ELB. The decision rules are piecewise linear functions of the innovations
1,t and 2,t . The slope coefficients associated with 1,t change when the economy hits the
ELB at 1,t = ¯1 , whereas the slope coefficients for 2,t do not change. We verify in the
Online Appendix that the decision rules are also continuous at 1,t = ¯1 , meaning that for
                               n
each 2,t and for each sequence 1,t - ¯1 as n - :

              n                                               n
            c(1 ,t , 2,t ) - c(¯
                               1 , 2,t ) - 0 and             (1,t , 2,t ) -  (¯
                                                                              1 , 2,t ) - 0.                   (27)

Because the transformation between (       d,t , R,t )   and (1,t , 2,t ) is continuous, the continuity
result also holds for the decision rules c
                                         ^(   d,t , R,t )   and ^(      d,t , R,t ),   expressed in terms of the
                                  n           n
original state variable, as (d    d,t   + R   R,t )/     - ¯1 .

   (ii) The law of motion given by the interest rate rule (21) and the consumption and
inflation decision rules (25) is coherent and complete for each (µc , µ ) and each realization
This Version: October 9, 2020                                                                          13


of the innovations (1,t , 2,t ). The system takes the form of a linear simultaneous equations
model with regime switches. The concepts of coherency and completeness were introduced
by Gourieroux et al. (1980) and more recently studied in the context of ELB applications
by Mavroeidis (2020) and Ascari and Mavroeidis (2020). Coherency requires that given an
innovation (1,t , 2,t ) there exists a solution to the system of equations. Completeness refers
to the uniqueness of that solution. Define

             (1)        1                                                 (2)
           Rt =           µc +  ( +  )µ +  1,t                    and Rt = - ln(r ).
                     1 + 

                                                                                    (1)
Coherency and completeness require that: (a) 1,t > ¯1 implies Rt = Rt                     is a solution to
                                                                           (2)
the maximization on the right-hand side of (23), whereas Rt =             Rt     is not. Likewise, it is
                                                (2)
required that (b) 1,t < ¯1 implies Rt =        Rt     is a solution to the maximization on the right-
                                        (1)
hand side of (23), whereas Rt =        Rt     is not. Coherency and completeness follows directly
                         (1)
from the linearity of   Rt     with respect to 1,t and the definition of ¯1 in (22) and require no
further restrictions on the domain of the innovations.

   (iii) Solutions for µc and µ . The means µc and µ together with the cutoff value ¯1 are
determined by the nonlinear system of equations (22) and (24). To understand the properties
of the nonlinear system, assume that  > 1, i.e., monetary policy is active, and  = 0, i.e.,
there is no uncertainty. First, suppose we start with the conjecture that ¯1 = -. Then
(24) simplifies to

                        1                                        1
             µc =         µc + (1 -  )µ ,               µ =        µc + ( +  )µ
                     1 +                                      1 + 

which is solved by
                                          µc = 0,       µ = 0.                                       (28)

Recall that the system was expressed in deviations from the targeted-inflation steady state.
Thus if µc = µ = 0, then the means of consumption and inflation are equal to the steady
state. The analysis is completed by noting that (22) implies that indeed ¯1 = - as initially
assumed.

   Second, suppose we start from the conjecture that ¯1 = +. Then (24) simplifies to

                   µc = ln(r ) + µc + µ ,        µ =  ln(r ) + µc + ( +  )µ ,
This Version: October 9, 2020                                                                  14


which implies
                                      1
                                µc = - (1 -  ) ln(r ),       µ = - ln(r ).                   (29)
                                      
Substituting the means into (22), we obtain ¯1 = ( - 1) ln(r )/ > 0. Thus, as  - 0,
¯1 - + as required. In this case, the system is in the so-called deflation steady state

and the ELB constraint is always binding.

      Once we allow for uncertainty,  > 0, then the means in (28) and (29) no longer solve
the system of equations (22) and (24). However, for values of  that are not "too large,"
one can obtain solutions that are "close" to the ones derived above. Due to the nonlinearity
of the N (·) and N (·) functions, these solutions can only be computed numerically.

Comparison to OccBin Solution. It is instructive to compare the above solution to the
one generated by OccBin. OccBin requires the choice of a reference regime. Because subse-
quently we focus on the targeted inflation equilibrium in which the ELB is non-binding with
high probability, we impose that the ELB is non-binding in the reference regime. Consider
a generic period t. The OccBin solution is based on the assumption that for   T the
economy will be in the reference regime. Given the lack of dynamics in the simple model,
we can choose T = t + 1 and assume that the regime will remain in the targeted-inflation
              ^ t+1 = 0, c
steady state: R          ^ = 0, and ^ = 0. For period t, the algorithm draws the shocks
(   d,t , R,t )   and solves the system

                                   ^ t = max
                                   R              ^t +    R,t ,   - ln(r )
                                   c                 ^ t + Et [^
                                            ct+1 ] - R
                                   ^t = Et [^                  t+1 ]
                                   ^t =  Et [^
                                             t+1 ] + c
                                                     ^t

under the restriction that Et [^
                               ct+1 ] = 0 and Et [^
                                                  t+1 ] = 0. The solution is identical to (21) and
(25) with µc = µ = 0 imposed. Thus, in the context of our stylized model the difference
between our solution, which happens to be exact, and the OccBin solution is that the latter
does not take into account the uncertainty about the regime in period t + 1.

Summary. We draw the following conclusions from the analysis of the simplified DSGE
model. First, the log-linearized model has static solutions that are exact and in which
the decision rules for consumption and inflation are piecewise-linear and continuous. We
use this as a motivation for subsequently considering a class of approximate solutions with
PLC decision rules for richer nonlinear DSGE models with occasionally-binding constraints.
Second, although we approximated the equilibrium conditions around the targeted-inflation
This Version: October 9, 2020                                                                 15


steady state, because of the nonlinearity generated by the occasionally-binding constraint,
for  > 0 the decision rules do not pass through the steady state around which the model
was initially approximated. In fact, because the decision rules are flexible enough to have
unrestricted intercepts, we can also generate the deflation equilibrium which is far away
from the targeted-inflation steady state. Third, an important difference between our pro-
posed PLC solution and the OccBin solution is that our decision rule coefficients capture
uncertainty about the future.



4     PLC Decision Rules and the Canonical Form

In the analysis of the simplified model in the previous section, the PLC decision rules emerged
from the analytical solution of the model. For more elaborate DSGE models, we will param-
eterize a family of piecewise-linear decision rules and then impose coefficient restrictions that
guarantee that the decision rules are continuous at the kink, where the constraint changes
from being slack to being binding. The remaining free coefficients of the decision rules can
then be used to (approximately) satisfy the equilibrium conditions of the model by setting
the residual functions (close) to zero. A discussion of how to do this numerically is deferred
to Section 5.


4.1    PLC Decision Rules

Let X = [x1 , X2 ]  X be an n × 1 vector of non-redundant state variables. We assume
that X also contains a constant. Here x1 is one particular element of X that enters the
characterization of the locus of points in the state space at which the constraint becomes
binding and the decision rules have their kink. The reason for separating out one of the X
elements will become clear below. Let Y denote a k × 1 vector of control variables. As we
make explicit below, we assume Y depends on x1 and X2 linearly where the coefficients may
depend on whether the constraint is binding or not.

    We assume that there is a linear(ized) scalar-valued function h x1 , X2 , Y that determines
whether the constraint is binding:

                                     > 0 if constraint is non-binding (n)
                  h x1 , X2 , Y =                                             .             (30)
                                      0 if constraint is binding (b)
This Version: October 9, 2020                                                                           16


The h(.) function may depend on the state variables (x1 , X2 ) and some of the elements in
Y . Because the function is assumed to be linear, we write it as

                                 h(x1 , X2 , y ) = 1 x1 + 2 X2 + Y Y.                                 (31)

The  's are not free coefficients. They are obtained from the equilibrium conditions of the
DSGE model. In the simplified model of Section 3 we set x1 = 1, X2 = [ d ,           R]   , and y = [^
                                                                                                     c, ^]
such that h(x1 , X2 , y ) = ln(r ) +    R   + c
                                              ^ with 1 = ln(r ), 2 = [0, 1] , and Y = [0,  ].

   We define the kink function x1 = (X2 ) such that [ (X2 ), X2 ]  X characterizes the locus
of points in the state space for which h (X2 ), X2 , Y ( (X2 ), X2 ) = 0, that is, the constraint
is just binding. Here Y (.) denotes the assumed piecewise linear decision rules for the control
variables. The linearity of h(·) in (31) and the assumed piecewise-linearity of the decision
rules for y imply that (X2 ) is a linear function and we parameterize it as

                                              (X2 ) =  X2 ,                                           (32)

where  is a (n - 1) × 1. The  coefficients will be determined as functions of the decision
rule coefficients and the coefficients of the constraint function h(.). So far we have not yet
made a determination whether the constraint is slack if x1 <  X2 . In the simplified model
the equilibrium kink function is given by (26).

   Returning to the control variables, we assume the decision rules for each y i are of the
piecewise-linear form

                                  i         i
                     i            1 ,1 x1 + 1,2 X2 if x1  (X2 )
                Y (x1 , X2 ) =    i         i
                                                                        i = 1, . . . , k ,            (33)
                                  2 ,1 x1 + 2,2 X2 if x1 < (X2 )


where each decision rule has 2n unknown coefficients. The decision rules are exactly linear
   i      i       i     i
if 1 ,1 = 2,1 and 1,2 = 2,2 . The specification in (33) makes the benefit of using the kink
function (.) clear: given the state variables x1 and X2 , we can easily determine on which side
of the constraint we need to be, even when the constraint contains some control variables.
In the simplified model the equilibrium decision rules are given by (25). Once we replace the
t 's by the   t 's   we obtain the same form as (33), where the  coefficients were determined
such that the decision rules satisfy the equilibrium conditions of the model.
This Version: October 9, 2020                                                                          17


4.2     Imposing Continuity on Piecewise-Linear Decision Rules

We now turn to imposing continuity on the decision rules at the kink, which means we
impose the restriction that the two parts of each decision rule are equal to each other along
the kink. Doing so will restrict a subset of the unknown  and  coefficients. Continuity at
x1 =  X2 requires that for each i = 1, ..., k

                          i          i        i         i
                          1 ,1  X2 + 1,2 X2 = 2,1  X2 + 2,2 X2                    X2 ,

which generates (n - 1) restrictions for each i:

                                      i       i     i      i
                                      1 ,1  + 1,2 = 2,1  + 2,2 .                                     (34)


    Next, we impose restrictions that make the (.) and Y (.) functions consistent with the
constraint in (30). The condition h[g (X2 ), X2 , Y            (X2 ), X2 ] = 0, which represents the kink
in terms of the h(.) function, can be written as

                                             k
                                                  i  i          i
                     1  X2 + 2 X2 +               Y (1 ,1  X2 + 1,2 X2 ) = 0 X2 ,
                                            i=1


which leads to another set of (n - 1) restrictions:

                                                  k
                                                        i  i       i
                                1  + 2 +                Y (1 ,1  + 1,2 ) = 0.                        (35)
                                                  i=1



    Counting all unknowns and restrictions, we have k (n + 1) degrees of freedom.6 Let us
                        i      i         i
assume the coefficients 1 ,1 , 1,2 , and 2,1 for each decision rule are free and collect them in
the vector  of size k (n + 1)

                             1             k     1             k     1             k
                          = [1,1 , . . . , 1,1 , 1,2 , . . . , 1,2 , 2,1 , . . . , 2,1 ] .           (36)

In other words, we treat all the decision rule coefficients for the "1" regime and the coefficient
in front of x1 in the "2" regime as free. The remaining decision rule coefficients in the "2"

   6
     There are 2n  coefficients for each decision rule and (n - 1)  coefficients, which yield 2nk + n - 1
unknowns. With (n - 1) restrictions for each decision rule as derived in (34) and the (n - 1) restrictions
in (35), we get (k + 1)(n - 1) restrictions. Subtracting the number of restrictions from the number of
unknowns, we get k (n + 1).
This Version: October 9, 2020                                                                         18

        i
regime, 2 ,2 , i = 1, . . . , k , as well as all of the  coefficients are determined as functions
of these free coefficients, which we now turn to. In our application the choice of which
coefficients go in to  is driven by numerical considerations. The "1" regime corresponds
to the ELB not being binding. In the targeted-inflation equilibrium, this is the more likely
regime and a good starting value for the numerical procedure that is used to determine 
is given by the decision-rule coefficients of a log-linear approximation that ignores the ELB
constraint.

   Conditional on , we can rewrite (35) as

                                    k                              k
                                        i i                             i i
                          1 +           Y 1,1    () +      2 +          Y 1,2   =0
                                i=1                               i=1

                                    a()                          -B ()


and solve for  as
                                                       1
                                             () =         B (),                                      (37)
                                                      a()
where a() is a scalar and B () is a (n - 1)-dimensional vector. By combining (37) with
                                                                            i
(34) we obtain an expression for the constrained decision rule coefficients 2 ,2 :


                           i          i     i              1         i
                           2 ,2 () = (1,1 - 2,1 )             B () + 1 ,2 .                          (38)
                                                          a()

   The last step is to determine which part of the decision rule in (33) corresponds to the
part of the state space where the constraint is slack and which part where the constraint is
binding. Take h x1 , X2 , Y (x1 , X2 ) for some x1 and X2 . Let us derive how its sign depends
on the sign of (x1 -  () X2 ). First, assume x1 >  ()X2 , then

                                                            k
                                                                 i  i         i
         h x1 , X2 , Y (x1 , X2 )       = 1 x1 + 2 X2 +          Y (1 ,1 x1 + 1,2 X2 )               (39)
                                                           i=1
                                                  k                             k
                                                        i i                           i i
                                        =   1 +         Y 1,1    x1 +     2 +         Y 1,2   X2 ,
                                                  i=1                           i=1

                                                  c()                           D ()


where c() is a scalar and D() is a (n - 1)-dimensional vector, which can be evaluated
given model parameters and free decision-rule coefficients. Collecting the  terms, we notice
that the restriction in (35) implies c() () + D () = 0, or D () = -c() (). Using this
This Version: October 9, 2020                                                                               19


on (39), we get

                            h[x1 , X2 , Y (x1 , X2 )] = c()[x1 -  ()X2 ].                                 (40)

Since we assumed x1 >  ()X2 above in the derivations, we conclude that h(.) > 0 if and
only if c() > 0. In other words, if c() > 0, then the "1" regime in (33) corresponds to the
constraint being slack.


4.3     Example: The Full New Keynesian Model

We now adapt the generic notation so far to the New Keynesian model with ELB con-
                                                                       ^ t-1 and X2,t =
straint described in Section 2. We partition the state space as x1,t = R
[1, y
    ^t-1 , z    ^t , g
           ^t , d    ^t , eR,t ] , and we have n = 7.7 As for the choice of control variables to approx-
imate, we have a few options. We approximate the decision rules      ^ (x1 , X2 ) and y
                                                                                      ^(x1 , X2 )
directly and let the remaining control variables c     ^ follow exactly from the equilibrium
                                                 ^ and R
conditions. Specifically, given x1 , X2 , y
                                          ^t = y  ^(x1 , X2 ) and ^t = ^ (x1 , X2 ), c^(x1 , X2 ) follows
                                          ^ (x1 , X2 ) follows from solving for R
                 ^t in Ry (·) in (15) and R
from solving for c                                                                ^ t in RR (·) in (16).
Thus, we set Y (·) = y
                     ^(·), ^ (·) and k = 2.
                                                                                  ^t +
    The ELB constraint can be written in terms of the variables defined so far as R
ln(r  )  0, which leads to the h(.) function

 h x1,t , X2,t , Yt (·) = (1 - ) 1 ^ (·) + 2 (^
                                              y (·) - y
                                                      ^t-1 + z        ^ t-1 + eR,t + ln(r  ). (41)
                                                             ^t ) + R R

Thus, the  coefficients in (31) are

  1 = R , 2 = [ln(r  ), -(1 - R )2 , (1 - R )2 , 0, 0] , Y = [(1 - R )1 , (1 - R )2 ] ,

and we can write c() in (39) as

                                                                y
                            c() = R + (1 - R )1 1,1 + (1 - R )2 1,1 .



               y
    If 1,1 and 1,1 are both positive, then c() is also positive because the remaining struc-
tural parameters are positive under standard parameterizations. Thus, we label the "1"
   7
     In principle, one of the other state variables could have been chosen as x1 . However, we found it natural
to use the lagged interest rates, because all else being equal, higher lagged interest rates move the economy
away from the ELB constraint.
This Version: October 9, 2020                                                                             20


regime as the regime where the ELB is slack, "n" (non-binding), and the "2" regime as the
"b" (binding) regime. We check that c() is indeed positive every time we solve the model.


4.4     Canonical Form

The final step in preparing the model solution for filtering is to cast the solution in the
following canonical form:

                               0 (n) + 1 (n)st-1 +  (n)t if 1,t <  (st-1 )
                      st =                                                                              (42)
                               0 (b) + 1 (b)st-1 +  (b)t           otherwise,

which is a VAR for st with endogenous regime switching. The innovations t are a function
of the structural innovations       t.   This transformation is done so that the first element of t ,
1,t , is a linear combination of structural shocks that determines whether the constraint is
binding in period t. Precise definitions of t , the impact matrices  (·) and the threshold
function  (·) will be provided below. Recall that we used the rotated t shocks also in the
construction of the solution to the simplified DSGE model in Section 3 and its decision rules
(25) were written in the canonical form.

    Equation (42) will serve as a transition equation in a state-space model. Thus, the vector
st needs to include all variables (whether or not they are directly approximated) that are
necessary for the construction of the measurement equations and all variables necessary to
determine the transition of such variables, which are all the state variables. The canonical
form resembles a regime-switching VAR with a "binding" (b) and "non-binding" (n) regime.
However, the regime transition is not determined by an exogenous Markov process. Instead,
it is determined by the realization of the shock innovations.8 Whether the coherency and
completeness conditions are satisfied ­ recall that we showed in Section 3 that they are
satisfied for the simplified New Keynesian model ­ depends on the structure of the (·)
matrices and is model specific. The construction of the canonical form for the New Keynesian
DSGE model of Section 2 is outlined in the Online Appendix.



   8
     Aruoba et al. (2020) estimate a structural VAR that takes the form of (42). Chen (2017) and Bianchi and
Melosi (2017) use an exogenous regime-switching process to characterize the ELB dynamics. Such models
can be solved using the tools proposed by Farmer, Waggoner, and Zha (2011). Benigno, Foerster, Otrok,
and Rebucci (2016) endogenize the regime-switching probability in a model of financial crisis, but, unlike in
our paper, the transition from one to the other regime remains partly decoupled from the realization of the
fundamental shocks.
This Version: October 9, 2020                                                                21


4.5    Measurement Equations

The key requirement for the conditionally optimal particle filter that is developed in Section 6
is that the conditional mean function (given st-1 ) of the observables is piecewise-linear.
This is guaranteed if the state-transition equation has the canonical form (42) and the
measurement equation is linear in st as in

                            o
                           yt = A0 + As st + ut ,   ut  N (0,  u ),                        (43)

       o
where yt is the vector of observable variables, ut is a vector of measurement errors, and the
constant  allows us to scale the measurement error covariance.

   The small-scale New Keynesian DSGE model is typically estimated using output growth
 o                o                       o
ygr,t , inflation t , and interest rates Rt . In addition, we will include a measure of the
consumption-output ratio. Starting from the definition of st given in (A.6), we define the
augmented vector s
                 ~t = [st , y
                            ^t-1 ] and add the trivial equation y
                                                                ^t-1 = y
                                                                       ^t-1 to the canonical
form in (42). Because the y
                          ^t-1 identity is linear, the structure of the canonical form is pre-
served. Assuming that output growth is measured in quarter-on-quarter percentages, and
inflation and interest rates are measured in annualized percentages, the system of measure-
ment equations is

                       o
                      ygr,t                   yt - y
                            = 100 ln( ) + 100(^    ^t-1 + z
                                                          ^t ) + u,y uy,t
                       o
                       t = 400 ln( ) + 400^
                                          t + u, u,t                                       (44)
                        o
                       Rt                   ^ t + u,R uR,t .
                          = 400 ln(R ) + 400R

We use data on government spending Gt to construct a measure of the consumption output
                                         o
ratio: Ct /Yt = 1 - Gt /Yt . We define cyt as linearly detrended 100 · ln(1 - Gt /Yt ). Because
in our model 1 - Gt /Yt = 1/gt , we obtain the additional measurement equation

                               o
                             cyt = -100 ln g - 100^
                                                  gt + u,c uc,t .                          (45)

Thus, we are treating the exogenous process g
                                            ^t as observed in the estimation. Because the
law of motion of g
                 ^t is linear, the PLC structure of the empirical model is maintained.
This Version: October 9, 2020                                                                                        22


5      PLC Model Solution

In this section, we describe how the free coefficients  in the PLC decision rules, as defined
in (36), are determined. We first discuss the equilibrium conditions that define the objective
function that we will minimize to find the optimal  coefficients. We then describe the choice
of the solution grid, the integration method and the optimization.

Equilibrium Conditions. More formally, let us denote the generic equilibrium conditions
as
                                           H [f0 (·), X] = 0,        X  X,                                          (46)

where f0 (X) corresponds to the optimal decision rules. To simplify the notation, we dropped
the vector of DSGE model parameters  from the conditioning set. For instance, for the
New Keynesian DSGE model (46) becomes
                                                                                                                 
           Rc y
              ^0 (Xt ), c
                        ^0 (Xt ),           ^ 0 (Xt ), y
                                  ^0 (Xt ), R          ^0 (Xt+1 ), c
                                                                   ^0 (Xt ),             ^ 0 (Xt+1 ), . . .
                                                                             ^0 (Xt+1 ), R
     Et                                                                                                           = 0,
           R     ^0 (Xt ), c
                 y         ^0 (Xt ),           ^ 0 (Xt ), y
                                     ^0 (Xt ), R          ^0 (Xt+1 ), c
                                                                      ^0 (Xt ),             ^ 0 (Xt+1 ), . . .
                                                                                ^0 (Xt+1 ), R

where we explained how we construct the decision rules y
                                                       ^(.), c
                                                             ^(.),            ^ (.) in Section 4.3.
                                                                   ^ (.), and R
Expectations over Xt+1 can be evaluated by using the law of motion of the exogenous shocks
                                                           ^ t , 1, and y
in (17) and noting that the first three elements of Xt+1 , R            ^t , are known at time t.
Thus, the equilibrium conditions only depend on the two decision rules y0 (.) and 0 (.) and
the current states Xt , just like (46) requires.

     We approximate f0 (X) by PLC decision rules g (X; )  G , where  contains the free
coefficients that are necessary to characterize the PLC function and G is the set of all PLC
functions. To determine , we minimize the norm of the vector-valued function H [g (X; ), X]
over a set of M grid points S obtained using a sparse Smolyak grid:

                                                     1                                 2
                                    = arg min                   H [g (X; ), X; ]           .
                                                     M   XS


In the simplified DSGE model in Section 3 we were able to find two sets of decision rule
coefficients  that set H[·] exactly equal to zero for all X and generate what we called the
targeted-inflation equilibrium and the deflation equilibrium. In the numerical illustrations
of Section 7 and the empirical application of Section 8 we will focus on the targeted-inflation
equilibrium by choosing starting values for the  optimization that generate the decision
This Version: October 9, 2020                                                                          23


rules for a linearized version of the model without ELB constraint.9

Solution Grid. There are two popular ways to choose the solution grid. In the collocation
approach, the grid points typically come from a grid that is constructed using a tensor
product of grids for each state variable, which in turn are constructed using the roots of a
set of complete polynomials. It is well known that tensor product grids used to approximate
the solution of nonlinear models suffer from the curse of dimensionality. Maliar and Maliar
(2014, 2015) propose a series of techniques based on stochastic simulations to construct
lower dimensional grids that represent the ergodic distribution of the model. However, these
simulation-based methods require a time-consuming iterative procedure, and, in general,
there does not seem to be a guarantee for the convergence of the grid and the approximate
solution.

    For our application, where we need to solve the model with different parameters tens of
thousands of times, neither the collocation approach that uses tensor grids, nor the iterative
approach that uses the ergodic distribution seem feasible. Coleman, Lyon, Maliar, and Maliar
(2018) propose the use of random and quasi-random grids on a fixed hypercube, because
they are easier and faster to construct but lack the dimensionality reduction. Smolyak grids
(Krueger and Kubler (2004), Malin, Krueger, and Kubler (2011), Judd, Maliar, Maliar, and
Valero (2014)) offer a balance in this trade-off, combining the advantages of a fixed and
predetermined domain and the dimensionality reduction of sparse grid methods.

    In constructing the grid S , we follow Judd et al. (2014) and build a sparse Smolyak grid.10
The Smolyak grid is a sparse grid defined on the interval [-1, 1]. To use it in an application,
it has to be scaled so that it represents the space of Xt . The scaling of the grid amounts to
picking minimum and maximum values for each state variable. The extrema correspond to
-1 and 1 in the original domain of the Smolyak grid, respectively. One of the properties of
the Smolyak grid is it places grid points at the edges of the domain ­ at -1 and 1. Thus,
we recommend picking values for the scaling that are not too extreme in order to have some
mass on both sides of the grid point.


   9
      While some progress has been made in Ascari and Mavroeidis (2020) studying parameter and innovation
domain restrictions that guarantee coherency and completeness, formal results for general DSGE models with
endogeneous state variables and continuously distributed innovations remain elusive.
   10
      One interpretation of our approach is that we are using the sum squared residual over the Smolyak
grid as a proxy for integrating the squared residual function over the ergodic distribution. Monte Carlo
experiments in Heiss and Winschel (2008) in the context of the calculation of the likelihood function of a
mixed logit model, which also involves evaluating an integral without a closed-form expression, show that
using a Smolyak grid provides superior performance over simulation techniques.
This Version: October 9, 2020                                                                 24


   In the context of the New Keynesian DSGE model, we proceed as follows. For the
exogenous state variables in Xt , we linearly scale the grid so that it starts from the 10th
percentile and goes to the 90th percentile of the distribution of each state variable. For the
endogenous state variable y^t-1 , we use the same scaling as the exogenous state z
                                                                                 ^t , since we
have verified that they have similar dispersion when simulating the model. Finally, for R ^ t-1 ,
we use the observed nominal interest rate data. Because we want to analyze the ELB, the
grid is scaled so that its minimum value matches the ELB with R^ t-1 = - ln(r  ), which
happens to be the 10th percentile in the data. The maximum value is matched to the 90th
percentile of Rt in the data. For this variable in particular, we scale the grid so that the
middle of the Smolyak grid coincides with the steady state at R ^ t-1 = 0.

Integration and Minimization. Expectations in the residual functions as in (12) are
computed using the monomial integration rule M2 as in Judd, Maliar, and Maliar (2010).
For a generic expression Et [v (xt+1 )], our implementation with four random variables requires
computing v (.) at 33 nodes and taking a weighted average. In our experience, this method
produces results that are very similar to using a Gauss-Hermite integration for each random
variable. As an example, with 5 nodes per random variable, the latter approach would make
it necessary to evaluate v (.) at 625 nodes and increase running time considerably.

   To minimize the objective function, we utilize a gradient-based nonlinear solver with
Jacobians evaluated analytically. As an initial guess for the solver, we use the decision rules
from a log-linear approximation. Because the log-linear decision rules are a special case of
the PLC decision rules--recall that we defined the model variables in log-deviations from
the steady state--we can denote them by g (0) (X). We find the free coefficients 0 (with
i    i      i    i
11 = 21 and 12 = 22 for all i = 1, ..., k ) that generate the same decision rules and use
this to initialize the minimization algorithm.

Interpretation of PLC Decision Rules. We offer two interpretations of the PLC decision
rules. First, they can be viewed as an approximation to the optimal decision rules f0 (X).
In fact, our motivation for constructing PLC rules was that the decision rules computed in
Aruoba et al. (2018) with Chebyshev polynomials for a New Keynesian DSGE model that is
essentially identical to the model in Section 2, appeared to be almost piecewise-linear. While
in any given model, the PLC decision rules may or may not provide accurate approximations
of the optimal decision rules, there is no sense in which the PLC rules become more accurate
"asymptotically."

   Second, the PLC rules can be viewed as describing the behavior of boundedly-rational
This Version: October 9, 2020                                                                                25


agents. In principle, bounded rationality can take many forms. The basic notion is that
decision making is constrained by agents' abilities to gather, retain, and process decision-
relevant information. Boundedly-rational agents may also be unable to solve a complicated
mathematical problem. Under this interpretation, the PLC rules can be viewed as more
easily computable decisions that have the additional benefit of being linear, except when the
constraint in the model becomes binding.



6        Particle Filters for PLC Models

The state-space representation associated with the PLC solution comprises the nonlinear
transition equation (42) and the linear measurement equation (43). The state-space repre-
sentation provides a joint density for the states st and the observations yt (omitting the o
superscript):
                                                     T
                               p(Y1:T , S1:T |) =         p(yt |st , )p(st |st-1 , ),                      (47)
                                                    t=1

where Yt1 :t2 and St1 :t2 denote the sequences yt1 , . . . , yt2 and st1 , . . . , st2 and  is the vector of
model parameters. Of particular interest are the sequence of estimates p(st |Y1:t ) of the state
vector and the likelihood function, which is defined as

                T                          T
 p(Y1:T |) =          p(yt |Y1:t-1 , ) =            p(yt |st , )p(st |st-1 , )p(st-1 |Y1:t-1 , )dst dst-1 . (48)
                t=1                        t=1


These objects can be obtained from a nonlinear filter. We describe below how p(st |Y1:t , )
and p(yt |Y1:t-1 , ) can be efficiently approximated by a particle filter.11


6.1      Generic Particle Filter

A particle filter represents the density p(st |Y1:t , ) through a swarm of particles {sj     j M
                                                                                       t , Wt }j =1

with the property that posterior expectations. Following the notation in Herbst and Schorfheide
(2015) we now use h(·) to denote a function of st for which a posterior expectation is sup-
posed to be evaluated. E[h(st )|Y1:t , ] can be approximated by Monte Carlo averages of the
    11
     Surveys and tutorials can be found, for instance, in Arulampalam, Maskell, Gordon, and Clapp (2002),
Capp´ e, Godsill, and Moulines (2007), Doucet and Johansen (2011), and Creal (2012). Kantas, Doucet, Singh,
Maciejowski, and Chopin (2014) discuss using particle filters in the context of estimating the parameters of
state-space models. Textbook treatments of the statistical theory underlying particle filters can be found in
Capp´ e, Moulines, and Ryden (2005), Liu (2001), and Del Moral (2013).
This Version: October 9, 2020                                                                       26

               M
form      1
          M    j =1   h(sj    j
                         t )Wt . The approximation typically holds in the form of a Law of Large

Numbers and a Central Limit Theorem. The particle filter can be implemented using the
following algorithm:12

Algorithm 1 (Generic Particle Filter)
                                                                                       iid
   1. Initialization. Draw the initial particles from the distribution sj
                                                                        0  p(s0 | ) and set
        j
      W0 = 1, j = 1, . . . , M .

   2. Recursion. For t = 1, . . . , T :

                                    ~j
          (a) Forecasting st . Draw s t from density gt (~  st |sjt-1 ,  ) and define the importance
              weights
                                       j
                                    t        sj
                                         = p(~    j
                                                               sj
                                              t |st-1 ,  ) gt (~
                                                                     j
                                                                 t |st-1 ,  ).                  (49)
          (b) Forecasting yt . Define the incremental weights
                                                    j
                                                  w
                                                  ~t          ~j
                                                      = p(yt |s       j
                                                                t ,  )t .                         (50)

               The predictive density p(yt |Y1:t-1 , ) can be approximated by
                                                                   M
                                                              1            j
                                           ^(yt |Y1:t-1 , ) =
                                           p                             w
                                                                         ~t  Wtj-1 .              (51)
                                                              M   j =1


          (c) Define the normalized weights
                                                                   M
                                           ~ tj = w j         1
                                           W      ~t  Wtj-1              w
                                                                         ~tj
                                                                             Wtj-1 .              (52)
                                                              M   j =1


          (d) Selection. Resample the particles, for instance, via multinomial resampling. Let
              { sj  M
                 t }j =1 denote M iid draws from a multinomial distribution characterized by sup-
              port points and weights {s  ~j    ~j             j
                                            t , Wt } and set Wt = 1 for j =, 1 . . . , M . An approxi-
              mation of E[h(st )|Y1:t , ] is given by h¯ t,M = 1  M       j    j
                                                               M  j =1 h(st )Wt .

   3. Likelihood Approximation. The approximation of the log-likelihood function is
      given by
                                        T        M
                                              1        j
                           ^(Y1:T |) =
                        ln p               ln        w
                                                     ~t  Wtj-1 .               (53)
                                       t=1
                                              M j =1

    The most important choice in the configuration of the algorithm is the proposal density
    st |sj
gt (~    t-1 ,  ). Different choices of the proposal density lead to different versions of the particle
filter.
   12
        Exposition and notation are based on Herbst and Schorfheide (2015).
This Version: October 9, 2020                                                                      27


6.2     Bootstrap Particle Filter

The BSPF was originally proposed by Gordon and Salmond (1993). It uses the state-
                                                          st |sj
transition equation as the proposal density, that is, gt (~                 st |sj
                                                               t-1 ,  ) = p(~    t-1 ,  ). This choice
is attractive because it is straightforward to implement the forecasting step by forward
                                                                             j
simulation of the transition equation and the importance weights simplify to t = 1. A
well-known disadvantage is that the proposal distribution is blind and hence ignores infor-
mation about st contained in the current observation yt . This can lead to a large variance
                            j
of the incremental weights ~t . This problem is exacerbated if the measurement error vari-
                        ~j
ance is small and p(yt |s t ) has thin tails or if the model is inappropriately parameterized or

misspecified and therefore has difficulties predicting yt one step ahead. Because the BSPF
has been used in the DSGE model literature (see Fern´
                                                    andez-Villaverde and Rubio-Ram´
                                                                                  irez
(2007), An and Schorfheide (2007) and Herbst and Schorfheide (2015)), we will include it as
a benchmark.


6.3     Conditionally Optimal Particle Filter

The proposal density for the COPF utilizes information in yt with the goal of minimizing
                                         j
the variance of the incremental weights ~t . It is given by

                     
                    gt  st |sj
                       (~                 st |yt , sj
                             t-1 ,  ) = p(~         t-1 ,  )  p(yt |s       st |sj
                                                                    ~t , )p(~    t-1 ,  ).       (54)

                           
Combining the formula for gt  st |sj
                             (~                                                             j
                                   t-1 ,  ) with the expressions for the importance weights t
                                     j
in (49) and the incremental weights ~t in (50), we obtain

                              j             ~j
                                      p(yt |s         sj
                                              t ,  )p(~
                                                           j
                                                       t |st-1 ,  )
                             ~t   =                                   = p(yt |sj
                                                                               t-1 ).            (55)
                                            sj
                                          p(~         j
                                             t |yt , st-1 ,  )


The second equality follows from Bayes Theorem. It can be shown that conditional on
{ sj                           
                                  st |sj
   t-1 } the proposal density gt (~    t-1 ,  ) minimizes the variance of the incremental weight
  j
w
~t  in (50).

   While direct sampling from the conditionally optimal proposal density is elusive for
most nonlinear state-space models, we can derive a convenient formula for the piecewise-
linear state-transition equation (42). Note that, conditional on st-1 , the current state st is
determined by t . It turns out, that it is more convenient to derive a conditionally optimal
                                     
proposal density for t , denoted by gt (t |sj
                                            t-1 ,  ).
This Version: October 9, 2020                                                                        28


   In order to state the result, we have to define the following objects:

                    tj (·) = yt - A0 - As 0 (·) - 1 (·)sj
                                                        t-1                                         (56)
                     j                                       -1           1 j
                    ¯t
                       (·) =     I +  (·)As - 1
                                            u As  (·)              (·)As -
                                                                         u t (·)
                    ¯ ·) =  I +  (·)As -1                      -1
                    (                  u As  (·)                    .

We use the argument (·) to indicate that the expressions are obtained either based on
 0 (n), 1 (n),  (n) or 0 (b), 1 (b),  (b) . Here tj (·) is the error made in forecasting
yt based on sj
             t-1 . 
                    j
                   ¯t          ¯ ·) are the posterior mean vector and covariance matrix of
                      (·), and (
t |(yt , sj                                            j
          t-1 ) absent any truncation--that is, for  (st-1 ) being + or -. Moreover, let


                j
               Dt (n) = (2 )-ny /2 |u |-1/2 |I +  (n) As -1
                                                         u As  (n)|
                                                                   1/2
                                                                                                    (57)
                                    1
                        × exp - tj (n) ( u + As  (n) (n)As )-1 tj (n)
                                    2
                            ×N ( (st-1 ) -  j
                                           ¯1         ¯
                                              ,t (n)/ 11 (n) ,
                 j
                Dt (b) = (2 )-ny /2 |u |-1/2 |I +  (b) As - 1
                                                          u As  (b)|
                                                                    1/2

                                     1
                         × exp - tj (b) ( u + As  (b) (b)As )-1 tj (b)
                                     2

                               1 - N ( (st-1 ) -  j
                                                 ¯1          ¯
                                                    ,t (b))/ 11 (b)        .


It can be shown that p(yt |sj        j        j
                            t-1 ) = Dt (n) + Dt (b).

   The characterization of the conditionally optimal proposal density is summarized in
Proposition 1. A proof of the proposition is provided in the Online Appendix.


Proposition 1 Suppose the state-transition equation is given by (42), t  N (0, I ), 1,t is a
scalar, and the measurement equation is given by (43). Draws from the conditionally optimal
                    
proposal densities gt  st |sj
                      (~    t-1 ,  ), j = 1, . . . , M , defined in (54) can be generated as follows:


   1. Let
                   j        `n' with prob. j
                                           t                                 Dtj
                                                                                 (n)
                   t =                                ,   where j
                                                                t =       j          j      .
                            `b' with prob. 1 - j
                                               t                         Dt (n) + Dt   ( b)

         j
   2. If t = `n' then generate t from the distribution

         j
         1 ,t  N 
                  j
                 ¯1         ¯        j       j
                    ,t (n), 11 (n) I{1,t   (st-1 )},
                                                          j
                                                          2     j
                                                            ,t |1,t  N (¯
                                                                        2j        j      ¯
                                                                           |1 (n, 1,t ), 2|1 (n))   (58)
This Version: October 9, 2020                                                                           29


      and let
                                     ~j
                                     s                   j         j
                                       t = 0 (n) + 1 (n)st-1 +  (n)t .

         j                     j
      If t = `b' then generate t from the distribution

            j
            1 ,t  N 
                     j
                    ¯1 (b), ¯ 11 (b) I{1
                                       j         j
                                         ,t >  (st-1 )},
                                                               j
                                                               2     j
                                                                 ,t |1,t  N 
                                                                             j
                                                                            ¯2        j      ¯
                                                                               |1 (b, 1,t ), 2|1 (b)   (59)

      and
                                      ~j
                                      s                   j         j
                                        t = 0 (b) + 1 (b)st-1 +  (b)t .

                                         j
  3. The incremental particle weight is ~t = D(n) + D(b).


Vanishing Measurement Errors. It is instructive to examine what happens as  - .
For the conditional density of yt |st-1 to be nonsingular in the limit, it has to be the case that
the number of rotated structural innovations is at least ny . Formally, the covariance matrices
As  (·) (·)As have to be non-singular. Suppose that ny = n and (As  (·)) are invertible
ny × ny matrices. This means, ignoring the truncation, under the invertibility assumption,
we can solve for the innovations t as a function of (yt , sj
                                                           t-1 ):


                         j                -1
                         t (·) = (As  (·)) (yt - As (0 (·) + 1 (·)st-1 )).



   Now consider what happens if we let the measurement error variance converge to zero.
First, the expressions in (56) remain well defined in the limit:

                                        j
                                       ¯t
                                            j
                                          - t (·),
                                                         ¯ ·) - 0.
                                                         (

The posterior variance converges to zero and the posterior mean converges to the innovation
j                                                    j                                j
t (·) that generates the observed yt conditional on st-1 . For the limit behavior on Dt (n),

the crucial term is

                                                  ¯ 11 (n) =      1 if  (sj
                                                                          t-1 ) - 
                                                                                   j
                                                                                  ¯1 ,t (n)  0
            lim   N ( (sj
                        t-1 )   -   ¯1
                                     j
                                       ,t (n))/                                                   .
             -0                                                   0 otherwise

This term measures whether it is possible to explain yt using the (n) coefficients, accounting
                                                  j            j
for the fact that the n regime is only active if ¯1 ,t (n)   (st-1 ). A similar analysis can
                           j
be conducted for the term Dt (b). Thus, for each particle j , there are four possible cases
This Version: October 9, 2020                                                                                   30


(ignoring equalities):

                                     j             j               j             j
                           Case 1 : ¯1 ,t (n) <  (st-1 ),         ¯1 ,t (b) <  (st-1 )
                                     j             j               j             j
                           Case 2 : ¯1 ,t (n) >  (st-1 ),         ¯1 ,t (b) >  (st-1 )
                                     j             j               j             j
                           Case 3 : ¯1 ,t (n) <  (st-1 ),         ¯1 ,t (b) >  (st-1 )
                                     j             j               j             j
                           Case 4 : ¯1 ,t (n) >  (st-1 ),         ¯1 ,t (b) <  (st-1 ).


           j
In Case 1 Dt (b) = 0 and j
                         t = 1. Here, only the (n) decision rules can rationalize the

data conditional on sj                              j          j
                     t-1 . Case 2 is the opposite: Dt (n) = 0, t = 0, and only the (b)
                                                               j          j
decision rules can rationalize the data. Under Case (3), both Dt (n) and Dt (b) are strictly
positive, 0 < j
              t < 1, and both decision rules could explain the data. Finally, in Case 4

yt is inconsistent with sj
                         t-1 , and none of the decision rules can explain the data. If each
j = 1, . . . , M falls into Case 4, then the particle-filter based likelihood approximation for this
particular parameterization of the DSGE model will be zero. Note that, if the measurement
error variance is strictly greater than zero, (potentially very large) measurement errors could
also rationalize the data under Case 4.13

    The previous calculations highlight that, unlike for the BSPF, the weights of the COPF
do not degenerate if one decreases the measurement error variance. In this case, if As  (·) is
a square matrix, the COPF specializes to the inversion filter that solves for the innovations
as a function of yt and sj
                         t-1 . Because our model is piecewise-linear, this inversion may have
one, two, or no solution(s).

Perfectly Observed Regimes. Our ELB application has the special feature that the
observation yt identifies the regime. Let yt = [y1,t , y2,t ] , where y2,t corresponds to the nominal
interest rate. Suppose the ELB is binding in the b regime and non-binding in the n regime.
Then, the ex post regime probability j is independent of sj
                                                          t-1 and given by the indicator
function.
                                               j = I{y2,t > c}.                                              (60)

While the distribution of yt is continuous in the n regime, for the binding regime the con-
tinuous part of the distribution concentrates in the lower-dimensional subspace defined by
                                  j                                                    j
y2,t = c. Thus, the formulas for Dt (b) in (57) and the moments of the distribution of t in
   13
     The four cases distinguished here are closely connected to the coherency and completeness conditions
discussed in Section 3. Previously, we asked whether conditional on a realization of t the vector st , using the
notation of the canonical form (42), is uniquely determined. If the condition is not satisfied, then p(st |st-1 , )
is not well defined. In addition, for the filtering it matters whether in the absence of measurement errors,
there exist one or more t 's that can rationalize the data.
This Version: October 9, 2020                                                                  31


Proposition 1 in the b regime have to be adjusted to account for the reduced dimensionality
of the continuous part of the yt distribution. Further details are provided in the Online
Appendix.



7     Numerical Illustrations

We now illustrate the proposed filtering method based on data simulated from the DSGE
model of Section 2. First, we compare the distribution of the stochastic likelihood approxi-
mation conditional on a particular parameter  between the proposed COPF and the basic
BSPF. It has been shown in the literature, that likelihood approximations of particle filters
are unbiased; see Herbst and Schorfheide (2015). In view of the unbiasedness, a low-variance
approximation is preferable to a high-variance approximation. Jensen's inequality implies
that log likelihood approximations are downward biased. The magnitude of the bias is con-
nected to the variability of the likelihood approximation: the larger the variance, the larger
also the downward bias.

    Second, we embed the particle filter approximation of the likelihood function into a
Metropolis-Hastings algorithm. It has also been shown in the literature that if the exact
likelihood function is replaced by a noisy but unbiased estimate, MCMC algorithms still
converge to the exact posterior distribution. However, there is no free lunch: the larger the
variability of the likelihood approximation, the slower the convergence. In the numerical
illustrations below, we will compare the speed of convergence as measured by the degree of
serial correlation in the parameter draws, across different versions of the particle filter.

    Throughout this section, we assume that all variables, including interest rates, are mea-
sured with error and that therefore regime (ELB binding versus non-binding) is not perfectly
observed. Thus, conditional on the states st , yt has a continuous distribution; see (43) and
(44). All computations reported below are executed on a single core of PC with an Intel
Xeon CPU E5-2687Wv3 at 3.10 GHz running Windows 10 (64bit) and JuliaPro 1.0.3.1.


7.1    Accuracy of Likelihood Approximation

We simulate a sample of T = 140 observations from the DSGE model, loosely parameterized
based on the empirical estimates reported in Section 8. The parameter values are summarized
in the second column of Table 1. The sample size matches the one used in the empirical
This Version: October 9, 2020                                                                       32


                                     Table 1: DGP and Prior

            Parameter      DGP                      Prior Distribution
                                     Density P(1)      P(2) HPD Low           HPD High
                           2.00      G       2.00      0.20       1.67          2.32
                           0.13      B       0.10      0.05       0.02          0.17
            1              2.60                        fixed at 2.60
            2              0.98                        fixed at 0.98
            R              0.80      B          0.80 0.10         0.65            0.96
            g              0.97      B          0.80 0.10         0.65            0.96
            d              0.91      B          0.80 0.10         0.65            0.96
            z              0.37      B
                                                0.40 0.20         0.08            0.73
            R              0.0019    IG         0.005 4.00       0.003           0.010
            g              0.0025    IG         0.005 4.00       0.003           0.010
            d              0.017     IG         0.01 4.00        0.005           0.020
            z              0.0058      IG       0.01 4.00        0.005           0.020
                           0.72                        fixed at 0.72
                           0.10                        fixed at 0.10
            H              1.00                        fixed at 1.00
            g              1.27      G          1.20 0.20         0.88           1.63
            rAnet          0.22      G          1.00 0.40         0.39           1.64
            gamQnet        0.33      N          0.50 0.25         0.09           0.91
            piAnet         0.50      N          2.50 1.00         0.87           4.14

Notes: G is Gamma distribution; B is Beta distribution; IG is Inverse Gamma distribution; and N
is Normal distribution. P(1) and P(2) are mean and standard deviations for Beta, Gamma, and Nor-
                                                                             2
mal distributions. The IG distribution is parameterized    as scaled inverse  distribution with density
    2 2         2 -/2-1           2    2                    2
p( |s ,  )  ( )            exp[-s
                                 /(2 )], where P(1) is s and P(2) is  . The density of  is obtained
by the change of variables  =  2 . HPD(Low,High) refers to the boundaries of 90% highest prior density
intervals. We use the following parameter transformations:  = exp{-rANet/400},  = exp{gamQnet/100},
and  = exp{piAnet/400}.


application in Section 8. In order to increase the likelihood of hitting the ELB in the
simulation, we lower the target inflation rate  to an annualized rate of 0.5%. In the
selected subsample, the ELB binds 22% of the periods, which is roughly consistent with our
actual sample.

   We begin by comparing the accuracy of the particle-filter-based likelihood approximation
using the same parameter values that were used to generate the data. The likelihood eval-
uation is conditional on a vector of initial states which are the true initial states associated
with the simulated observations. We consider two values for the scaling parameter for the
measurement errors,  : 0.1 and 0.05. The smaller  is, the larger is the penalty for deviations
between model-predicted and actual observations. Thus, small measurement errors can be
This Version: October 9, 2020                                                                                33


                         Figure 1: Density of Log-Likelihood Approximations

               Measurement Error  = 0.1                          Measurement Error  = 0.05
       0.5                                                 0.5
               BSPF M=1,000
               BSPF M=10,000
       0.4     COPF M=180                                  0.4


       0.3                                                 0.3


       0.2                                                 0.2


       0.1                                                 0.1


         0                                                   0
        -470    -465   -460    -455   -450   -445   -440    -440   -435   -430   -425   -420   -415   -410



Notes: Density plots are based on Nrun = 100 runs of the BSPF (M = 1, 000 is red solid and M = 10, 000
is red dashed) and COPF (M = 180, blue solid), respectively.


provided as a stress test for the filter in case the model (or its parameterization) is at odds
with the data.

    The accuracy of the filter depends on the number of particles M . For the COPF, we set
M = 180, and, for the BSPF, we consider M = 1, 000 and M = 10, 000. Starting from the
benchmark of the BSPF with M = 1, 000, the choice of M = 180 for the COPF equalizes
the run times of the two filters, which is 1.54 seconds for  = 0.1.14 Raising the number of
particles from 1,000 to 10,000 increases the run time of the BSPF roughly tenfold, to 16.2
seconds. Moreover, for  = 0.1 the BSPF is able to match the accuracy of the COPF with
the increased number of particles.

    We run the COPF and the BSPF Nrun = 100 times, respectively, and construct kernel
estimates of the likelihood from the repeated runs, which are depicted in Figure 1. The more
concentrated the densities, the more accurate the likelihood approximation. Holding the run
time fixed, the COPF is substantially more accurate than the BSPF. For  = 0.1 the number
of particles for the BSPF has to be increased from M = 1, 000 to M = 10, 000 to achieve
similar accuracy to the COPF. This which implies that the BSPF likelihood evaluation takes
roughly ten times as long as the COPF likelihood evaluation.

  14
     The absolute run times depend on the programming language ­ Julia in our case ­ and the efficiency of
the code. We are mainly interested in the relative run times. The code for the COPF and BSPF is almost
identical, except that the COPF requires a few additional steps to evaluate the expressions in Proposition 1,
which implies that for the same M , the COPF is slower than the BSPF.
This Version: October 9, 2020                                                                       34


                                                                                     ~j
   Because the BSPF ignores the information in yt when generating the proposal draws s t,

the variance of the particle weights increases as  falls. This, in turn, translates into an
increase in the variance (and by virtue of the concave transformation also the bias) of the
log-likelihood approximation, which is clearly visible by comparing the solid red densities
across panels for the two different values of  . The precision of the COPF, on the other
hand, increases as  falls. When  - 0, it is possible to uniquely determine the innovations
j
t conditional on sj
                  t-1 during non-ELB periods, because the number of observables equals
the number of shocks.

   Figure 2 depicts standard deviations of log-likelihood approximations as a function of the
mean log-likelihood value across Nrun = 100 runs of the filter for  = 0.1 and  = 0.05. Each
dot (or asterisk) in the two scatter plots corresponds to a different parameter value i .15
Two important findings emerge. First, as we have seen already from Figure 1, the COPF
likelihood approximation is less dispersed than the BSPF approximation. The accuracy gain
from the conditionally optimal proposal density increases as the measurement error variance
decreases, because the BSPF performance deteriorates. Second, while the accuracy of the
COPF is independent of the log-likelihood value associated with the posterior draw i , the
accuracy of the BSPF approximation deteriorates the further the i draw is in the tails of
the posterior distribution.


7.2     Particle MCMC

We now embed the particle filter likelihood approximations in a standard single-block RWMH
algorithm. The particle RWMH algorithm operates on an enlarged probability space that
includes all the random variables that are generated when running the particle filter; see
Herbst and Schorfheide (2015) for a textbook exposition and Andrieu et al. (2010) for a
formal analysis. The use of an enlarged probability space leads to an increase in the per-
sistence of the Markov chain generated by the posterior sampler. The noisier the likelihood
approximation, the larger the persistence of the resulting Markov chain. A high degree of
serial correlation is undesirable, because it leads to very noisy Monte Carlo approximations
of posterior moments. We will show that the use of an accurate particle filter such as the
COPF can alleviate this problem.



  15
     The parameter values are obtained from the output of the particle Markov chain Monte Carlo (PMCMC)
algorithm in Section 7.2.
This Version: October 9, 2020                                                                                                               35


                                       Figure 2: Comparison of Log-Likelihood Approximations

                                             = 0.1                                                            = 0.05
                        10                                                                    10
                               COPF M=180
                               BSPF M=1,000
                         8                                                                     8
     StdD(ln p(Y| i))




                                                                           StdD(ln p(Y| i))
                         6                                                                     6


                         4                                                                     4


                         2                                                                     2


                         0                                                                     0
                        -470        -460        -450         -440   -430                      -440   -430        -420         -410   -400
                                                        i                                                                i
                                           Mean(ln p(Y| ))                                                  Mean(ln p(Y| ))


Notes: Standard deviations of log-likelihood approximations are based on Nrun = 100 runs of the two filters.
Each dot (or asterisk) corresponds to a particular i . We are using M = 1, 000 particles for the BSPF (red
asterisks) and M = 180 particles for the COPF (blue dots).


     Under some regularity conditions, the sequence of posterior draws generated from a
RWMH algorithm satisfies a Central Limit Theorem (CLT) for dependent processes. The
numerical accuracy of the Monte Carlo approximation of posterior means depends on the
long-run covariance matrix of the sequence of parameter draws i , i = 1, . . . , N . The larger
the autocorrelation of these draws, the less precise the Monte Carlo approximation.

     The RWMH algorithm requires a covariance matrix for the proposal distribution (we
use a multivariate normal distribution) that is constructed as follows. We start from a
log-linearized version of the DSGE model that ignores the ELB constraint and sets the
measurement error variance to zero ( = 0). In this case the likelihood function can be
evaluated with the Kalman filter (KF). We conduct two preliminary MCMC runs using the
linearized model. The first run is based on a diagonal covariance matrix with scaled prior
variances on the diagonal and the second run is based on the posterior covariance matrix of
the first run. Finally, we compute the posterior covariance matrix from the second run and
denote it by V ^
               ¯ . The particle MCMC runs using the nonlinear DSGE model are based on
 ^ where c = 0.1.
 ¯
cV

     For the estimation of the nonlinear model we set the scale of the measurement error
variance to  = 0.1 and keep the number of particles at M = 180 for the COPF and M =
1, 000 for the BSPF. Thus, according to the left panel of Figure 2, the standard deviation of
the log-likelihood approximation of the COPF is around 1 whereas the standard deviation
This Version: October 9, 2020                                                                         36


for the BSPF ranges from 3 to 5. We generate 55,000 draws from the posterior distribution
and discard the first 5,000 draws. The overall run time using the two filters is approximately
the same: 24 hours and 37 minutes for the COPF-RWMH algorithm and 25 hours and 55
minutes for the BSPF-RWMH algorithm. Note that the codes for the COPF-RWMH and the
BSPF-RWMH algorithms are identical, except for the more complicated way of generating
               st |sj
draws from gt (~    t-1 ,  ) and evaluating 
                                             j
                                            ~t in the COPF likelihood approximation.

    Using a scaling of c = 0.1, the acceptance rate for the proposed draws is 28% for the
COPF-RWMH algorithm. Herbst and Schorfheide (2015) documented that for the estima-
tion of small-scale linearized DSGE models, an acceptance rate between 15% and 30% is
associated with the most accurate Monte Carlo approximations of posterior means. Replac-
ing the COPF by the BSPF while keeping the scale factor c = 0.1, the acceptance rate drops
to 2.6%. The lower acceptance rate is caused by the noisier likelihood approximation.16

    The first row of Figure 3 compares posterior densities constructed from the output of
the COPF- and BSPF-RWHM algorithms for two representative parameters:  and d . The
posterior densities for d look very similar and both peak near the "true" parameter values
depicted by the solid vertical line. For the parameter  , the posterior density obtained from
the COPF is less concentrated at the peak than the BSPF density. The reason is that BSPF
chain moves very slowly ­ recall the 2.6% acceptance rate ­ and has not fully explored the
high-density area of the posterior.

    The second row of Figure 3 shows the autocorrelation functions for the  i and d
                                                                                  i
                                                                                    se-
quences. Here, stark differences emerge. While under the BSPF-based sampler the auto-
correlation at lag 100 is still around 0.92 for the  i sequence, it is only 0.25 for draws from
the COPF-RWMH. Suppose the autocorrelation function of the draws is given by j , where
j is the temporal displacement among the draws. Then the variance of the mean of the
draws is given by V [h()](1 + )/(1 - ), where V [h()] is the posterior variance of h()
and (1 + )/(1 - ) can be viewed as inefficiency factor that arises due to the serial cor-
relation of the draws. A reduction from  = 0.921/100 = 0.999 to  = 0.251/100 = 0.986
lowers the inefficiency factor from approximately 2,000 to 142. This implies only 7% of the
draws are required to achieve the same accuracy of posterior mean approximations with the
COPF-based sampler as with the BSPF-based sampler, or, holding the numbers of draws



  16
     The acceptance rate could be increased by decreasing c, but it does not cure the persistence problem
because the chain moves very slowly due to the smaller size of the accepted steps. In our case, setting
c = 0.005 delivers an acceptance rate of 3.8% and an even more persistent sequence of parameter draws.
This Version: October 9, 2020                                                                                                                 37


                              Figure 3: Posterior Draws: Density and Autocorrelation

                                      Posterior                                                         Posterior d
                                                                                120
                    2                                             COPF                                                             COPF
                                                                  BSPF                                                             BSPF
                                                                                100

                   1.5
                                                                                    80


                    1                                                               60

                                                                                    40
                   0.5
                                                                                    20

                    0                                                                0
                                    1.5          2          2.5                               0.01 0.015 0.02 0.025 0.03 0.035 0.04


                                ACF of  Draws                                                    ACF of d Draws
                    1                                                                1


                   0.8                                                              0.8


                   0.6                                                              0.6


                   0.4                                                              0.4


                   0.2                                                              0.2
                             COPF                                                              COPF
                             BSPF                                                              BSPF
                    0                                                                0
                         0       20       40          60    80       100                  0        20      40          60    80       100
                                                Lag                                                              Lag

                                                      Autocorrelations, All Parameters
                    1                                                                1
                             Lag 10                                                            Lag 30
                             Lag 20                                                            Lag 40
                   0.9                                                              0.9


                   0.8                                                              0.8
            COPF




                                                                             COPF




                   0.7                                                              0.7


                   0.6                                                              0.6


                   0.5                                                              0.5
                     0.5        0.6       0.7         0.8   0.9          1            0.5         0.6      0.7         0.8   0.9          1
                                               BSPF                                                             BSPF


Notes: ME scale  = 0.1, proposal covariance scale c = 0.1, N = 55, 000 draws (drop first 10%). COPF:
number of particles M = 180, run time is 24:37:33 (hh:mm:ss). BSPF: number of particles M = 1, 000, run
time is 25:55:14 (hh:mm:ss). Top row: kernel density estimates of posterior distributions based on MCMC
output. Vertical lines indicate true parameter values. Center row: autocorrelation functions of posterior
draws based on COPF and BSPF. Bottom row: scatter plots of autocorrelations BSPF vs. COPF for various
lags. Solid line is 45 degree line.


fixed across samplers, the COPF-based sampler delivers Monte Carlo approximations that
are six times as accurate (in terms of sampling variance).
This Version: October 9, 2020                                                                38


    The last row of Figure 3 compares autocorrelations for lags 10, 20, 30, and 40 for all esti-
mated parameters. The solid lines are 45-degree lines. The two panels show that the COPF
is able to reduce the autocorrelation for all estimated parameters, and hence it drastically
improves the performance of the MCMC algorithm.

    To summarize, when we calibrated the COPF and the BSPF to run at the same time, the
former delivers a likelihood approximation that is as much as five times more accurate, and
when used inside a RWMH algorithm we showed that the COPF yields a posterior mean
that is substantially more accurate. Because the COPF needs a relatively small number
of particles, M = 180 in our numerical illustration, it is possible to accurately estimate a
DSGE model with an occasionally binding constraint without supercomputing capabilities
in a relatively short amount of time.



8     Empirical Application

We now estimate the small-scale New Keynesian DSGE model based on quarterly U.S. data
using the previously developed model solution and filtering techniques and conduct a fiscal
policy experiment. The estimation results are summarized in Section 8.1, and the fiscal
policy analysis appears in Section 8.2.


8.1    Estimation

The DSGE model is estimated based on data on GDP growth (q-o-q %), the log consumption-
GDP ratio (scaled by 100), GDP deflator inflation (annualized %), and nominal interest rates
(annualized %) with data from 1984:Q1 to 2018:Q4. The data for the estimation was ex-
tracted from the FRB St. Louis FRED database (vintage 2019-10-30). Output growth is
defined as real gross domestic product (GDPC1) growth converted into per capita terms.
Our measure of population is Civilian Noninstitutional Population (CNP16OV). We com-
pute population growth rates as log differences and apply an eight-quarter backward-looking
moving average filter to the growth rates to smooth out abrupt changes in the population
growth series. In constructing a measure of the consumption-GDP ratio, we define con-
sumption as the difference between output and government spending. Thus, our measure of
consumption includes investment and net exports. Government spending is constructed as
real government consumption expenditures and gross investment (GCEC1). We remove a
This Version: October 9, 2020                                                                             39


linear trend from the log consumption-GDP ratio to correct for different time trends in the
price deflators of the GDP components. Inflation is defined as the log difference in the GDP
deflator (GDPDEF), and the interest rate is the average effective federal funds rate (FED-
FUNDS) within each quarter. During the period 2009:Q1 to 2015:Q4, when the effective
federal funds rate was between 0 and 25 basis points, we set the interest rate exactly equal
to zero and regard the ELB as binding.

    The prior distribution used for the estimation is identical to the one in Table 1. We
absorb the initial values of the latent state variables into the parameter vector and specify
prior distributions over the initial states; see Table A-1 in the Online Appendix.17 We fix a
number of parameters prior to estimation. Because our sample does not include observations
on labor market variables, we fix the Frisch labor supply elasticity. Based on R´
                                                                                ios-Rull et al.
(2012), who provide a detailed discussion of parameter values that are appropriate for DSGE
models of U.S. data, we set  = 0.72. The parameter  , which captures the elasticity of
substitution between intermediate goods, is not separately identifiable from the slope of the
Phillips curve  which in turn determines the adjustment cost parameter . We set  = 0.1,
which generates a markup of 10%. We fix the preference parameter at H = 1. It determines
steady-state hours worked and is neither relevant for the model dynamics nor identifiable
based on our observables. We also fix the monetary policy coefficients 1 and 2 at 2.60 and
0.98, respectively, which are values estimated in Aruoba and Schorfheide (2016).

    We start out by estimating the log-linearized version of the DSGE model that ignores the
ELB constraint, setting the measurement error variances of the state-space model to zero.
Draws from the posterior are generated by a single-block RWMH algorithm. In an initial
run, we use a diagonal matrix with the prior variances to configure the covariance matrix of
the proposal distribution. In the main run, we use the estimated posterior covariance matrix
^
¯ from the initial run to construct a proposal covariance matrix  with the scaling factor
V
c = 0.2. We generate N = 110, 000 draws, discarding the first 10,000.

    For the estimation of the nonlinear version of the model we assume that interest rates are
                                              o
observed without error. Thus, conditional on Rt , it is known whether the ELB is binding or
not; see (60). We maintain the assumption that the remaining variables are observed subject
to a measurement error and set the scale factor for their measurement error variances to
 = 0.001. We generate N = 55, 000 draws from the posterior distribution of  using the
particle RWMH algorithm, discarding the first 5,000 draws. We use a scaling of c = 0.2 for
  17
     This approach has the advantage that uncertainty about the initial state does not add to the variability
of the particle-filter-based likelihood approximation conditional on a parameter i .
This Version: October 9, 2020                                                                          40


                         Table 2: Posterior Distribution (PLC / COPF)

                      Parameter      Mean      MAP       HPD Low       HPD High
                                     2.08      2.05      1.75          2.40
                                     0.12      0.10      0.08          0.16
                      R              0.82      0.83      0.79          0.85
                      g              0.981     0.986     0.967         0.996
                      d              0.95      0.96      0.93          0.97
                      z              0.32      0.29      0.17          0.47
                      R              0.0018    0.0018    0.0016        0.0020
                      g              0.0026    0.0029    0.0023        0.0028
                      d              0.0282    0.0290    0.0178        0.0399
                      z              0.0060    0.0069    0.0053        0.0068
                      g              1.27      1.28      1.25          1.29
                      rAnet          0.67      0.23      0.27          1.08
                      gamQnet        0.36      0.47      0.24          0.48
                      piAnet         2.98      2.69      2.54          3.42

Notes: The estimation period is 1984:Q1 to 2018:Q4. The following parameters are fixed during the es-
timation: 1 = 2.6, 2 = 0.98,  = 0.72,  = 0.10, and H = 1.00. We use the following parameter
transformations:  = exp{-rANet/400},  = exp{gamQnet/100}, and  = exp{piAnet/400}. MAP refers
to the maximum posterior probability estimate. HPD(Low,High) refers to the boundaries of 90% highest
posterior density intervals. COPF configuration: number of particles M = 150, ME scale  = 0.001, proposal
covariance scale c = 0.2, N = 55, 000 draws (drop first 10%), acceptance rate is 25%, run time is 13:27:23
(hh:mm:ss).


the covariance matrix of the proposal distribution. The likelihood function is approximated
using the COPF with M = 180 particles. The resulting acceptance rate of the particle
RWMH algorithm is 25%, and the run time is 16 hours and 14 minutes on a single core,
which comes to about 0.9 seconds per draw. The assumption of a perfectly observed regime
simplifies the COPF calculations and reduces the run time by 34%, compared to the results
reported in Section 7.2.

    The parameter estimates are summarized in Table 2. The table reports posterior means,
the maximum posterior probability (MAP) estimates, and lower and upper endpoints of
highest posterior density (HPD).18 The parameter estimates are similar to the ones reported
elsewhere in the literature for variants of the small-scale New Keynesian DSGE model. The
estimated slope of the Phillips curve is ^ = 0.12. The government spending shock is close to

  18
      We compared these estimates with those obtained from a linearized model using the Kalman Filter and
data that exclude the ELB episode. The most noteworthy differences are in d and d , both of which need
to be larger when the ELB episode is used in order to deliver large (negative) and persistent shocks that
take the economy to the ELB. We also find that rAnet and gamQnet estimates are somewhat smaller in the
full sample, both of which are consistent with related results in the literature.
This Version: October 9, 2020                                                                   41


a unit-root process (^
                     g = 0.981), and the estimated autoregressive parameter of the discount
factor shock is ^d = 0.95. Thus, innovations to these processes will have a long-lasting effect.
One of the outputs of the estimation is the set of filtered values for the exogenous variables.
We use these explicitly in our policy experiment and discuss them in the next section.


8.2     Fiscal Policy Analysis

The recent literature has emphasized that the effects of expansionary fiscal policies on output
may be larger if the economy is at or near the ELB; see, for instance, Eggertsson (2011) and
Christiano and Eichenbaum (2012). In the absence of the ELB, a typical interest rate
feedback rule implies that the central bank raises nominal interest rates in response to
rising inflation and output caused by an increase in government spending. This monetary
contraction raises the real interest rate, reduces private consumption, and overall dampens
the stimulating effect of the fiscal expansion. If, however, the economy remains at the ELB
despite the expansionary fiscal policy, then the increase in inflation that results from the fiscal
expansion reduces the real rate. In turn, current-period demand is stimulated, amplifying
the positive effect on output. We will use our model to provide a quantitative assessment of
this effect.

    Because our model solution is nonlinear, the effect of a fiscal intervention depends on
the initial condition and the size of the intervention. We use the Great Recession and the
subsequent period in the U.S. as our laboratory and consider a fiscal intervention that is
calibrated to a portion of the ARRA of February 2009 as we explain below. Our analysis is
conducted from an ex post perspective, where we extract the historical shocks that make our
model match the realized U.S. data, which include both a fiscal and monetary intervention,
and ask what would have happened if one or both of the policy interventions were not
implemented.

    ARRA of February 2009 consisted of a combination of tax cuts and benefits; entitlement
programs; and funding for federal contracts, grants, and loans. We focus on the third
component, because it can be interpreted as an increase in gt . We model the ARRA spending
as a one-period positive shock of  ARRA to the demand shock process, where we calibrated
 ARRA = 0.0077 using data on the disbursement of ARRA funds, as we explain in the Online
Appendix. This one-time shock is roughly 2.7g , and, since g
                                                           ^t is highly serially correlated,
the effect of the shock will slowly decay over time. We assume that the ARRA innovation
to government spending took place in the second quarter of 2009.
This Version: October 9, 2020                                                                                       42


                                        Figure 4: Ex Post Policy Analysis


                   4                                                   2
                                           Intervention                                          Intervention
                                           No Intervention             1                         No Intervention
                   2
                                                                       0
                   0
                                                                      -1

                   -2                                                 -2
                   07Q4      08Q4        09Q4          10Q4           07Q4        08Q4     09Q4             10Q4


                   2                               2                                0


                   1                               0                               -1

                                                  -2
                   0                                                               -2

                                                  -4
                   -1                                                              -3
                   07Q4   08Q4   09Q4   10Q4      07Q4       08Q4   09Q4   10Q4    07Q4   08Q4      09Q4     10Q4


                                                   4                                5
                   0                                                                4
                                                   3

                                                   2                                3
                   -2
                                                                                    2
                                                   1
                   -4                                                               1
                                                   0
                                                                                    0
                   -6                             -1
                   07Q4   08Q4   09Q4   10Q4      07Q4       08Q4   09Q4   10Q4    07Q4   08Q4      09Q4     10Q4



Notes: The vertical red line corresponds to 2009:Q2, which is the date of the ARRA intervention. Intervention
(black) versus no-intervention paths (blue). Along the no-intervention path, we set monetary policy shocks
to zero and lower the innovation to the government spending shock in 2009:Q2 by the size of the ARRA
intervention. Red dashed lines represent paths in the absence of exogenous shock innovations from 2009:Q2
onwards. The level processes in the second row of the figure are standardized by the unconditional standard
deviations of the corresponding AR(1) processes. Inflation and the interest rate are expressed in terms of
annualized percentage rates.


       We use the COPF to obtain estimates of the exogenous shock processes for the period
2009:Q2 through 2011:Q1. The subsequent results are based on the MAP estimator of the
DSGE model parameters. The panels in the first two rows of Figure 4 show the filtered
monetary policy and government spending innovations and the levels of the government
spending, technology growth, and discount factor shock processes. Recall that, in the model
 R,t   and   g,t    are N (0, 1) random variables. The level processes in the second row of the
figure are standardized by the unconditional standard deviations of the corresponding AR(1)
processes. For the government spending and the monetary policy shocks, we distinguish
between intervention and no-intervention paths.
This Version: October 9, 2020                                                                          43


    According to the estimated model, the drop in output during the Great Recession is gen-
erated by drastic falls in the technology growth and discount factor shock processes. Because
of the stylized structure of the DSGE model, these two shocks also absorb the contribution
of financial shocks and financial accelerator effects. While the technology shock is not very
persistent (^
            z = 0.29) and reverts back to zero by the end of 2009, the mean reversion of the
discount factor shock is very slow, and it remained below 2 standard deviations until the end
of 2010. Meanwhile the government spending process is positive, indicating that fiscal policy
started to become expansionary (relative to the historical average) in 2008. The filtered mon-
etary policy innovations ^R,t|t turn out to be negative past 2009:Q2, which captures an effort
by the Federal Reserve to keep the policy rate lower than what the policy rule implies. This
is how our model that abstracts from explicitly modeling unconventional monetary policies
implemented in this period (quantitative easing, forward guidance) handles the existence of
these policies in the data.
    Because the actual path of the government spending shock already contains the effect
of fiscal expansion due to ARRA, we compute the counterfactual path by subtracting the
effect of ARRA from the filtered demand shock g
                                              ^t|t using

                         C            -T ARRA
                       g
                       ^t     ^t|t - t
                         |t = g      g        for t = T , T + 1, ..., T + 7,                         (61)

where T corresponds to 2009:Q2, the period the ARRA intervention is implemented in. The
magnitude of the ARRA intervention is reflected in the difference between the intervention
(black) and no-intervention (blue) government spending innovation depicted in the top left
panel of Figure 4. The ARRA intervention shifts g
                                                ^t persistently downward as shown in the
center left panel of the figure. To measure the effect of the combined fiscal and monetary
policy, we set the counterfactual monetary policy shocks to zero.
    The main finding is depicted in the bottom panels. The ex post effect of the intervention
is defined as X o - X C , where X o is the observed value of a generic variable and X C is the
counterfactual path along which the policy intervention is removed.19 In the figure, the ex
post effect is given by the gap between the no intervention and the intervention path. In
the absence of the ARRA and monetary interventions, output and inflation would have been
persistently lower than they actually were. In particular, annualized inflation would have
been 41 basis points lower, and annualized output growth would have been 32 basis points
lower, on average, over these eight quarters.
  19
    Details on the algorithm to compute the effects of the policy interventions are reported in the Online
Appendix.
This Version: October 9, 2020                                                                            44


                                     Table 3: Ex Post Multipliers

                               Intervention      1Q 4Q 8Q
                               Fiscal Only       0.65 0.69 0.69
                               Fiscal + Monetary 0.63 0.77 0.82

Notes: Ex post analysis is conditional on filtered 2009:Q2 - 2011:Q1 shocks. The pure fiscal multipliers are
obtained by setting the monetary policy shocks to zero, whereas the combined fiscal and monetary multipliers
are based on leaving the monetary policy shocks at their filtered values.


    Based on the output and government spending paths, we can also compute cumulative
dollar-for-dollar multipliers

                                 H      o
                                  =1 (YT -1+    - YTC
                                                     -1+
                                                         )
                       µH =      H     0
                                                              where H = 1, ..., 8,
                                  =1 (GT -1+    - GC
                                                   T -1+ )


which are reported in Table 3. The pure fiscal multipliers are obtained by setting the
monetary policy shocks to zero, whereas the combined fiscal and monetary multipliers are
based on leaving the monetary policy shocks at their filtered values. The ex post multipliers
are around 0.7 according to our estimated model. We started out this section providing an
explanation for why fiscal multipliers tend to be higher when the economy is at the ELB.
If we compute fiscal multipliers conditional on the economy being in a state in which the
central bank would respond to rising output and inflation with an increase in interest rate
(for example starting the exercise in 2007:Q1), then the fiscal multipliers would be around
0.6, which is indeed lower than our ex post multiplier. However, the presence of the ELB
only generates a modest increase in the multiplier ­ on impact at the ELB the multiplier is
0.65 while away from the ELB the multiplier is 0.55.

    Focusing on the two different policies at the ELB, the difference between the two types of
multipliers reported in Table 3 is small, because as the no-intervention path of the nominal
interest rate indicates, the adverse discount factor shock kept the economy close to the ELB,
leaving very little room for conventional monetary policy interventions ex post.

    The red dashed lines in the second-row panels of Figure 4 represent the post-2009:Q1
path of the exogenous shock processes in the absence of further innovations, which is the
expected path conditional on 2009:Q1 innovation. Subsequent technology and discount factor
innovations had only small effects on the path of the respective exogenous processes, keeping
them roughly in line with expectations. Thus, the low level of the discount factor shock also
implied very low interest rates from an ex ante perspective, leaving little scope for the Fed
This Version: October 9, 2020                                                                45


to boost the effects of a fiscal expansion through a zero-interest-rate policy.



9      Conclusion

Likelihood-based estimation of nonlinear DSGE models is computationally challenging. While
it is becoming easier for economists to access powerful computer clusters that enable mas-
sive parallel computation, the ability to solve and estimate models on a desktop computer
remains useful and desirable. Computations can often be simplified and accelerated consid-
erably by taking shortcuts in regard to model solution or estimation techniques. The goal
of this paper has been to develop a new solution method that captures important aspects of
the nonlinearity generated by occasionally binding constraints and, at the same time, allows
for efficient filtering and likelihood-based estimation. The piecewise-linearity of the deci-
sion rules allows us to solve the model faster and to derive a conditionally optimal proposal
distribution for a particle filter. This filter delivers a much more accurate likelihood approx-
imation than a standard bootstrap particle filter and enables us to estimate a nonlinear New
Keynesian DSGE model with a ELB constraint in a relatively short amount of time on a
single core processor.



References
Adam, K. and R. Billi (2007): "Discretionary Monetary Policy and the Zero Lower
    Bound on Nominal Interest Rates," Journal of Monetary Economics, 54, 728­752.

An, S. and F. Schorfheide (2007): "Bayesian Analysis of DSGE Models," Econometric
    Reviews, 26, 113­172.

Andrieu, C., A. Doucet, and R. Holenstein (2010): "Particle Markov Chain Monte
    Carlo Methods," Journal of the Royal Statistical Society Series B, 72, 269­342.

Arulampalam, M. S., S. Maskell, N. Gordon, and T. Clapp (2002): "A Tutorial
    on Particle Filters for Online Nonlinear/Non-Gaussian Bayesian Tracking," IEEE Trans-
    actions on Signal Processing, 50, 174­188.

Aruoba, S. B., P. Cuba-Borda, and F. Schorfheide (2018): "Macroeconomic Dy-
    namics Near the ZLB: A Tale of Two Countries," Review of Economic Studies, 85, 87­118.
This Version: October 9, 2020                                                         46


Aruoba, S. B. and F. Schorfheide (2016): "Inflation Dynamics During and After
  The Zero Lower Bound," in Inflation Dynamics and Monetary Policy, 2015 Jackson Hole
  Symposium Volume Published by the Federal Reserve Bank of Kansas City.

Aruoba, S. B., F. Schorfheide, and S. Villalvazo (2020):                   "SVARs with
  Occasionally-Binding Constraints," Manuscript, University of Maryland and University
  of Pennsylvania.

Ascari, G. and S. Mavroeidis (2020): "The Unbearable Lightness of Equilibria in a
  Low Interest Rate Environment," Manuscript, Oxford University.

Atkinson, T., A. Richter, and N. A. Throckmorton (2020): "The Zero Lower
  Bound and Estimation Accuracy," Journal of Monetary Economics, forthcoming.

                             ´, and M. Uribe (2001): "The Perils of Taylor Rules,"
Benhabib, J., S. Schmitt-Grohe
  Journal of Economic Theory, 96, 40­69.

Benigno, G., A. Foerster, C. Otrok, and A. Rebucci (2016): "Estimating Macroe-
  conomic Models of Financial Crisis: An Endogenous Regime Switching Approach,"
  Manuscript, University of Missouri.

Bianchi, F. and L. Melosi (2017): "Escaping the Great Recession," American Economic
  Review, 107, 1030­58.

Boehl, G. (2019): "Efficient Solution, Filtering and Estimation of Models with OBCs,"
  Working Paper.

    ´, O., S. J. Godsill, and E. Moulines (2007): "An Overview of Existing Methods
Cappe
  and Recent Advances in Sequential Monte Carlo," Proceedings of the IEEE, 95, 899­924.

    ´, O., E. Moulines, and T. Ryden (2005): Inference in Hidden Markov Models,
Cappe
  Springer Verlag.

Chen, H. (2017): "The effects of the near-zero interest rate policy in a regime-switching
  dynamic stochastic general equilibrium model," Journal of Monetary Economics, 90, 176
  ­ 192.

Christiano, L. J. and M. Eichenbaum (2012): "Notes on Linear Approximations,
  Equilibrium Multiplicity and E-learnability in the Analysis of the Zero Lower Bound,"
  Manuscript, Northwestern University.
This Version: October 9, 2020                                                        47


Christiano, L. J., M. Eichenbaum, and M. Trabandt (2015): "Understanding the
  Great Recession," American Economic Journal: Macroeconomics, 7, 110­67.

Christiano, L. J. and J. D. Fisher (2000): "Algorithms for Solving Dynamic Models
  with Occasionally Binding Constraints," Journal of Economic Dynamics & Control, 24,
  1179­1232.

Coleman, C., S. Lyon, L. Maliar, and S. Maliar (2018): "Matlab, Python, Julia:
  What to Choose in Economics?" CEPR Discussion Papers 13210, C.E.P.R. Discussion
  Papers.

Creal, D. (2012): "A Survey of Sequential Monte Carlo Methods for Economics and
  Finance," Econometric Reviews, 31, 245­296.

Cuba-Borda, P., L. Guerrieri, M. Iacoviello, and M. Zhong (2019): "Likeli-
  hood Evaluation of Models with Occasionally Binding Constraints," Journal of Applied
  Econometrics, 1­13.

Del Moral, P. (2013): Mean Field Simulation for Monte Carlo Integration, Chapman &
  Hall/CRC.

Doucet, A. and A. M. Johansen (2011): "A Tutorial on Particle Filtering and Smooth-
  ing: Fifteen Years Later," in Handbook of Nonlinear Filtering, ed. by D. Crisan and
  B. Rozovsky, Oxford University Press.

Eggertsson, G. B. (2011): "What fiscal policy is effective at zero interest rates?" in
  NBER Macroeconomics Annual 2010, ed. by D. Acemoglu and M. Woodford, University
  of Chicago Press, vol. 25, 59­112.

Eggertsson, G. B. and M. Woodford (2003): "The Zero Bound on Interest Rates and
  Optimal Monetary Policy," Brookings Papers on Economic Activity, 34, 139­235.

Fair, R. C. and J. B. Taylor (1983): "Solution and Maximum Likelihood Estimation
  of Dynamic Nonlinear Rational Expectations Models," Econometrica, 51, 1169­1185.

Farmer, R. E., D. F. Waggoner, and T. Zha (2011): "Minimal state variable solutions
  to Markov-switching rational expectations models," Journal of Economic Dynamics and
  Control, 35, 2150 ­ 2166.
This Version: October 9, 2020                                                        48

    ´ ndez-Villaverde, J., G. Gordon, P. Guerro
Ferna                                                 ´ n-Quintana, and J. F. Rubio-
 Ram´ irez (2015): "Nonlinear Adventures at the Zero Lower Bound," Journal of Economic
  Dynamics and Control, 57, 182 ­ 204.

                                             irez (2007): "Estimating Macroeco-
    ´ ndez-Villaverde, J. and J. F. Rubio-Ram´
Ferna
  nomic Models: A Likelihood Approach," Review of Economic Studies, 74, 1059­1087.

Gordon, N. and D. Salmond (1993): "A Novel Approach to Nonlinear/Non-Gaussian
  Bayesian State Estimation," IEEE Proceedings-F, 140, 107­113.

Gourieroux, C., J. J. Laffont, and A. Monfort (1980): "Coherency Conditions in
  Simultaneous Linear Equation Models with Endogeneous Switching Regimes," Economet-
  rica, 48, 675­695.

Guerrieri, L. and M. Iacoviello (2015): "OccBin: A Toolkit for Solving Dynamic
  Models with Occasionally Binding Constraints Easily," Journal of Monetary Economics,
  70, 22­38.

------ (2017): "Collateral constraints and macroeconomic asymmetries," Journal of Mon-
  etary Economics, 90, 28 ­ 49.

Gust, C., E. Herbst, D. Lopez-Salido, and M. E. Smith (2017): "The Empirical
  Implications of the Interest-Rate Lower Bound," American Economic Review, 107, 1971­
  2006.

Heiss, F. and V. Winschel (2008): "Likelihood approximation by numerical integration
  on sparse grids," Journal of Econometrics, 144, 62 ­ 80.

Herbst, E. and F. Schorfheide (2015): Bayesian Estimation of DSGE Models, Prince-
  ton University Press.

Holden, T. D. (2019): "Existence and uniqueness of solutions to dynamic models with
  occasionally binding constraints," Manuscript, Deutsche Bundesbank.

Judd, K. L., L. Maliar, and S. Maliar (2010): "A Cluster-Grid Projection Method:
  Solving Problems with High Dimensionality," NBER Working Paper, 15965.

Judd, K. L., L. Maliar, S. Maliar, and R. Valero (2014): "Smolyak method for
  solving dynamic economic models: Lagrange interpolation, anisotropic grid and adaptive
  domain," Journal of Economic Dynamics and Control, 44, 92­123.
This Version: October 9, 2020                                                          49


Kantas, N., A. Doucet, S. Singh, J. Maciejowski, and N. Chopin (2014): "On
  Particle Methods for Parameter Estimation in State-Space Models," arXiv Working Paper,
  1412.8659v1.

Kopecky, K. A. and R. M. Suen (2010): "Finite state Markov-chain approximations to
  highly persistent processes," Review of Economic Dynamics, 13, 701 ­ 714.

Krueger, D. and F. Kubler (2004): "Computing Equilibrium in OLG Models with
  Stochastic Production," Journal of Economic Dynamics and Control, 28, 1411­1436.

Kulish, M., J. Morley, and T. Robinson (2017): "Estimating DSGE Models with
  Zero Interest Rate Policy," Journal of Monetary Economics, 88, 35­49.

Liu, J. S. (2001): Monte Carlo Strategies in Scientific Computing, Springer Verlag.

Maliar, L. and S. Maliar (2014): "Numerical Methods for Large-Scale Dynamic Eco-
  nomic Models," in Handbook of Computational Economics Vol. 3, ed. by K. Schmedders
  and K. L. Judd, Elsevier, vol. 3, 325­477.

------ (2015): "Merging Simulation and Projection Approaches to Solve High-Dimensional
  Problems with an Application to a New Keynesian Model," Quantitative Economics, 6,
  1­47.

Malin, B. A., D. Krueger, and F. Kubler (2011): "Solving the multi-country real
  business cycle model using a Smolyak-collocation method," Journal of Economic Dynamics
  and Control, 35, 229­239.

Mavroeidis, S. (2020): "Identification at the Zero Lower Bound," Manuscript, Oxford
  University.

Mendes, R. (2011): "Uncertainty at the Zero Lower Bound: A Theoretical Analysis,"
  MRPA Working Paper No. 60103.

Mendoza, E. G. and S. Villalvazo (2020): "FiPIt: A simple, fast global method for
  solving models with two endogenous states & occasionally binding constraints," Review of
  Economic Dynamics, 37, 81­102.

Nakata, T. (2016): "Optimal fiscal and monetary policy with occasionally binding zero
  bound constraints," Journal of Economic Dynamics and Control, 73, 220 ­ 240.
This Version: October 9, 2020                                                 50


R´
 ios-Rull, J.-V., F. Schorfheide, C. Fuentes-Albero, M. Kryshko, and
  R. Santaeulalia-Llopis (2012): "Methods versus Substance: Measuring the Effects
  of Technology Shocks," Journal of Monetary Economics, 59, 826­846.

Woodford, M. (2003): Interest and Prices, Princeton University Press.
This Version: October 9, 2020                                          A-1


Online Appendix to "Piecewise-Linear Approximations
  and Filtering for DSGE Models with Occasionally
                 Binding Constraints"
S. Boragan Aruoba, Pablo Cuba-Borda, Kenji Higa-Flores, Frank Schorfheide,
                                 and Sergio Villalvazo


This Appendix consists of the following sections:


  A. Equilibrium Conditions for the Model of Section 2

  B. Derivations for Section 3

  C. Canonical Form for the Model of Section 2

  D. Proofs and Derivations for Section 6

  E. Additional Details for the Empirical Application

  F. Consumption-Savings Model with Borrowing Constraint
This Version: October 9, 2020                                                                         A-2


A        Equilibrium Conditions for the Model of Section 2

In this section we sketch the derivation of the equilibrium conditions presented in Section 2.


A.1        Households

The representative household solves

                                                                                   1+1/
                                                  (Ct+s /At+s )1- - 1    H                  Mt+s
         max                Et          s
                                        dt+s                          - H t+s + M V                         ,
{Ct+s ,Ht+s ,Bt+s ,Mt+s }
                                 s=0
                                                         1-              1 + 1/           Pt+s At+s

subject to:

             Pt Ct + Tt + Bt + Mt = Pt Wt Ht + Mt-1 + Rt-1 Bt-1 + Pt Dt + Pt SCt .


Consumption and bond holdings. Let  s dt+s t+s be the Lagrange multiplier on the
household budget constraint. Then the first-order condition with respect to consumption
and bond holdings are given by:
                                                                   -
                                                            Ct      1
                                               Pt t      =
                                                            At      At
                                                            dt+1
                                                  t      =       Rt t+1 .
                                                             dt

Combining the two equations leads to the consumption Euler equation:

                                                                      -
                                               dt+1   Ct+1 /At+1           1 Rt
                                 1 =  Et                                           ,
                                                dt      Ct /At            zt+1 t+1

where zt+1 = At+1 /At . We define the stochastic discount factor as:

                                                                          -
                                                  dt+1     Ct+1 /At+1          1
                                       Qt+1|t =                                    .
                                                   dt        Ct /At           zt+1

Labor-Leisure Choice. Taking first-order conditions with respect to Ht yields the standard
intratemporal optimality condition for the allocation of labor
                                                                   
                                               Wt             Ct        1/
                                                  = H                 Ht .
                                               At             At
This Version: October 9, 2020                                                                               A-3


A.2          Intermediate Goods Firms

Each intermediate goods producer buys labor services Ht (j ) at the real wage Wt . Firms
face nominal rigidities in terms of price adjustment costs. The adjustment cost, expressed
                                                                                Pt (j )
as a fraction of firms' real output, is given by the function p                Pt-1 (j )
                                                                                            . We assume that the
adjustment cost function is twice-continously differentiable, weakly increasing and weakly
convex, p  0 and p  0. The firm maximizes expected discounted real profits with
respect to Ht (j ) and Pt (j ):

       
                         Pt+s (j )                       Pt+s (j )
  Et          s Qt+s|t             At+s Ht+s (j ) - p                   At+s Ht+s (j ) - Wt+s Ht+s (j ) ,
       s=0
                          Pt+s                          Pt+s-1 (j )

subject to
                                                                  -1/
                                                        Pt (j )
                                         At Ht (j ) =                   Yt .
                                                         Pt
We use µt+s  s Qt+s|t to denote the Lagrange multiplier associated with this constraint. In
equilibrium, the firms use the households' stochastic discount factor to discount future prof-
its.

Price setting decision. Setting Qt|t = 1, the first-order condition with respect to Pt (j ) is
given by:

                                                                                           -1/ -1
                       At Ht (j )         Pt ( j )  At Ht (j ) µt Pt (j )                           Yt
                   0 =            - p                         -
                          Pt             Pt-1 (j ) Pt-1 (j )           Pt                           Pt
                                          Pt+1 (j )                Pt+1 (j )
                           + Et Qt+1|t p             At+1 Ht+1 (j ) 2        .
                                           Pt (j )                  Pt (j )

Firms' labor demand. Taking first-order conditions with respect to Ht (j ) yields

                                         Pt (j )            Pt ( j )
                               Wt =              At - p                  At - µt At .
                                          Pt               Pt-1 (j )

Symmetric equilibrium. We restrict attention to a symmetric equilibrium where all firms
choose the same price Pt (j ) = Pt j . This assumption implies that in equilibrium all firms
face identical marginal costs and demand the same amount of labor input. Combining the
firms' price setting and labor demand first order conditions and assuming that the price
This Version: October 9, 2020                                                                A-4


adjustment costs are quadratic, i.e.,

                                                                           2
                                    Pt (j )                Pt (j )
                             p                 =                    -¯         ,
                                   Pt-1 (j )       2      Pt-1 (j )

we obtain:
                                                                                   2
                                                Ct          1/               Pt
                              (1 -  ) - H                 Ht     -              - ¯   +
                                                At                   2     Pt-1
                   Pt           Pt               Pt+1                    Pt+1 Yt+1
                       -¯           =  Et Qt+1|t                                    .
                  Pt-1         Pt-1               Pt p                    Pt    Yt


A.3     Equilibrium Conditions

Resource constraint. The derivation of the aggregate resource constraint is straightfor-
ward. In equilibrium real profits by intermediate producers is given by:

                               Dt = Yt - p (t ) Yt - Wt Ht .

Substituting this into the household budget constraint we obtain:

             Tt Mt Bt Mt-1 Rt-1 Bt-1
      Ct +      +    +    -    -     = Wt Ht + Yt - p (t ) Yt - Wt Ht .
             Pt   Pt   Pt   Pt   Pt

From the government budget constraint in (7) we can see that the term in square brackets
corresponds to real government expenditure Gt . Simplifying yields:

                                 Ct + Gt = [1 - p (t )] Yt .


   The technology process introduces a long-run trend in the variables of the model. To
make the model stationary we use the following transformations: yt = Yt /At , ct = Ct /At ,
                            yt
and note that Yt /Yt-1 =   yt-1
                                zt .   We also define the gross inflation rate t = Pt /Pt-1 . The
equilibrium conditions shown in Section 2.2 of the main text follow immediately.
This Version: October 9, 2020                                                               A-5


Steady States and Reparameterizations. Let

                                              (1 -  )       1
                                r = /,  =             , b=    .
                                                           2

The steady states are given by

                                                                             (1/( +1/ ))
                    1 -  +  (1 -  ) ( -    ¯ ) - 0.5  ( -   ¯ )2
             c   =
                          H ((1/(g )) - 0.5( -    ¯ )2 )-1/
                           c
             y   =
                   1/g - 0.5( -   ¯ )2
             R   =  r.




B      Derivations for Section 3

The log-linearized system (omitting hats) that characterizes the simplified model takes the
form

                        Rt = max        t + R     R,t ,   - ln(r )                         (A.1)

                         ct = dt + Et [ct+1 - dt+1 ] - (Rt - Et [t+1 ])
                        t =  Et [t+1 ] + ct .

The assumption that shocks are iid implies that Et [dt+1 ] = 0. Because we are focusing
on solutions under which the endogenous variables are iid, we can set Et [ct+1 ] = µc and
Et [t+1 ] = µ . This leads to


                          Rt = max        t + R      R,t ,   - ln(r )                      (A.2)
                           ct = -Rt + µc + µ + d             d,t

                           t = µ + ct .

The consumption equation corresponds to the second equation in (19) in the main text.
Combining the Euler equation and the Phillips curve leads to the following expression for
inflation:
                           t = -Rt + µc + ( +  )µ + d                d,t ,                 (A.3)
This Version: October 9, 2020                                                                       A-6


which is the third equation in (19) in the main text. We now can use (A.3) to eliminate
inflation from the monetary policy rule:


     Rt = max      - Rt + µc +  ( +  )µ + d                      d,t   + R     R,t ,    - ln(r )   (A.4)


Note that if the ELB is non-binding the interest rate is given by

                             1
                   Rt =        µc +  ( +  )µ + d                       d,t   + R       R,t   .
                          1 + 

Thus, we can also write

                      1
      Rt = max          µc +  ( +  )µ + d                  d,t   + R         R,t   , - ln(r ) ,    (A.5)
                   1 + 

which is the first equation in (19).

   The solution of the model requires the calculation of the mean of a truncated random
variable. If X  N (µ,  2 ) and C is a truncation constant, then

                                                       N ()
                               E[X |X  C ] = µ +              ,
                                                     1 - N ()

where  = (C - µ)/ , N (x) and N () are the probability density function (pdf) and the
cumulative density function (cdf) of a N (0, 1). To obtain E[R(1,t )], we need to compute
E[1,t |1,t  ¯1 ]. Because 1,t  N (0, 1) we obtain

                                                       N (¯
                                                          1 )
                                E[1,t |1,t  ¯1 ] =              ,
                                                     1 - N (  )

which is used in (23) in the main text.

   To show continuity of the consumption decision rule (see (25) in the main text), consider
This Version: October 9, 2020                                                                   A-7


the following limit from above:

              lim c(1,t , 2,t )
           1,t ¯1

                    1                             1
               =        µc + (1 -  )µ + d,1 -           ¯1 + d,2  2,t
                 1 +                          1 + 
                    1
               =        µc + (1 -  )µ + (1 + ) ln(r ) + µc +  ( +  )µ
                 1 + 
                 +d,1  ¯1 + d,2  2,t
               = ln(r ) + µc + µ + d,1  ¯1 + d,2  2,t
               = c(¯
                   1 , 2,t ).

To obtain the second equality, we use the formula for ¯1 from (22) in the main text:

                                  1
                         ¯1 = -
                                    (1 + ) ln(r ) + µc +  ( +  )µ .
                                  

The last two equalities establish the continuity. A similar calculation for the inflation decision
rule yields

           lim  (1,t , 2,t )
          1,t ¯1

                      1
            =           µc + ( +  )µ +  (1 + ) ln(r ) + µc +  ( +  )µ
                   1 + 
                   +d,1  ¯1 + d,2  2,t
            =  ln(r ) + µc + ( +  )µ + d,1  ¯1 + d,2  2,t
            =  (¯
                1 , 2,t ).

Replacing the i,t 's by the        i,t 's   does not affect continuity because the transformation is
linear.



C         Canonical Form for the New Keynesian DSGE Model

Because the definition of st is model and application specific, we outline the construction of
the canonical form in the context of the New Keynesian DSGE model with ELB constraint.
Define the vectors st
                                      st = y
                                           ^t ,      ^t, z
                                                ^t , R        ^t , g
                                                         ^t , d    ^t , eR,t                  (A.6)
This Version: October 9, 2020                                                                                  A-8


and recall that     t   =    z,t , d,t , g,t , R,t    . We will begin by expressing the law of motion of st
as a function of the innovations             t   and then, later on, we transform the       t 's   into  's:

                                        st = 0 (·) + 1 (·)st-1 +  (·) t .

Rather than providing detailed algebraic expressions for the elements of the (·) matrices,
we will provide an outline of how the expressions can be derived.

Output, inflation, and interest rates. We use the first three rows of the (·) matrices
to represent the decision rules for y
                                    ^t and                                                 ^t.
                                           ^t and the monetary policy rule that determines R
                                                                     ^ t-1 , 1, y
Note that the decision rules in (33) are expressed in terms of Xt = [R          ^t-1 , z    ^t , g
                                                                                       ^t , d    ^t , eR,t ] ,
whereas the canonical form is written in terms of st --see (A.6). Thus, in order to generate
the equations for y
                  ^t ,          ^ t for the canonical form, we have to express Xt as a linear
                       ^t , and R
function of st-1 and        t.

Exogenous shocks. The remaining four rows of the (·) matrices reproduce the law of
motion of the exogenous shock processes in (17).

From     t 's   to t 's and defining the threshold condition. To express the threshold
condition in the canonical form and transform the                   t    into t innovations, define

                                                                                      1
                  (st-1 ) = ln(r  ) + 0 (n) + 1 (n)st-1 ,                 1,t = -          (n)      t
                                                                                      (n)

such that the ELB constraint is non-binding if and only if

                                                       1,t <  (st-1 )

as in (42). Let Null(x) be an orthogonal basis for the null space for the vector x. We define
the vector t as
                                                        (n)/  (n)
                                       t =                                      t.
                                                     Null  (n)/  (n)

The transformation has the property that, if E[                   t t]   = I , then E[t t ] = I as well. The
definition of t as a function of                 t   allows us to convert the  (·) matrix into  (·) and
completes the derivation of the canonical form.
This Version: October 9, 2020                                                                    A-9


D        Derivations for Section 6

We provide a proof of Proposition 1, which contains the formulas for the terms t ,      ¯1,1 (·),
¯
11 (·),                 ¯
        ¯2|1,t (·), and 2|1 (·) that appear in the proposition. Throughout this section we set
the intercept in the measurement equation A0 = 0 and we drop the subscript from the matrix
As .

Proof of Proposition 1. Conditional on st-1 the current state st is determined by t . In
                 
order to derive gt  st |sj
                   (~    t-1 ) (we are omitting  from the conditioning set), we will work in the
(t , st-1 ) space and derive (also omitting tildes and j superscripts) the conditionally optimal
proposal distribution

                         
                        gt (t |yt , st-1 ) = p(t |yt , st-1 )  p(yt |t , st-1 )p(t )

and the incremental particle weights

                             j
                            ~t = p(yt |st-1 ) =      p(yt |t , st-1 )p(t )dt .

Define
             ^t|t-1 (·) = A(0 (·) + 1 (·)st-1 ) + A (·)t ,
             y                                                       t (·) = yt - y
                                                                                  ^t|t-1 (·).

We will denote the density of a N (µ, ) random variable Y by pN (y ; µ, ). Using this
notation, we write

            p(yt |t , st-1 )p(t )                                                               (A.7)
                        ^t|t-1 (n),  u pN (1,t ; 0, 1)pN (2,t ; 0, I )I{1,t   (st-1 )}
              = pN yt ; y
                            ^t|t-1 (b),  u pN (1,t ; 0, 1)pN (2,t ; 0, I )I{1,t >  (st-1 )}
                   +pN yt ; y
              = I + II.
This Version: October 9, 2020                                                             A-10


   We will begin by manipulating term I (n). Omitting the (n) arguments we obtain:

   I = I{1,t  ~(st-1 )}(2 )-ny /2 | u |-1/2 (2 )-n /2
               1                                               1 2
       × exp - (t - A t ) ( u )-1 (t - A t ) exp - 1                + 2,t 2,t
               2                                               2 ,t

     = I{1,t  ~(st-1 )}(2 )-ny /2 | u |-1/2 (2 )-n /2 exp - 1  -1 t
                                                            2 t u
               1                                                    1 2
       × exp - t  A ( u )-1 A t + t ( u )-1 A t exp - 1                   + 2,t 2,t        .
               2                                                    2 ,t

Note that term I takes the form of a product between "likelihood function" and "prior."
The prior covariance matrix of t is  = I , and the negative Hessian and the maximum of
the "log-likelihood" function are

                     ^ -1 =  A ( u )-1 A ,
                                                       ^t = ^  A ( u )-1 t .              (A.8)

With this notation we can write

                                                                      1
       I = I{1,t   (st-1 )}(2 )-ny /2 | u |-1/2 (2 )-n /2 ||-1/2 exp - t -1
                                                                         u t
                                                                      2
                   1 ^ -1              ^ -1 t exp - 1  t .
           × exp - t         t - 2^t 
                   2                                   2 t

Now define the quasi posterior mean and covariance matrices for t |(yt , st-1 )

                            ¯ = -1 + ^ -1     -1              ¯^ -1 
                                                   ,     ¯t =       ^t .                  (A.9)

This leads to

        I = (2 )-ny /2 | u |-1/2 ||-1/2 |¯ |1/2 exp - 1  -1 t exp 1         ¯¯ -1 ¯t
                                                       2 t u             2 t

            ×I{1,t   (st-1 )}(2 )-n /2 |    ¯ |-1/2 exp - 1 (t - ¯t ) ¯ -1 (t -  ¯t ) .
                                                          2

   We now decompose the kernel of the "posterior" of t |(yt , st-1 ) in the second line of the
preceding equation into a conditional and a marginal distribution. We use the "1" subscript
to indicate the marginal posterior of 1 and the "2|1" subscript to indicate the conditional
This Version: October 9, 2020                                                                         A-11


mean and variance associated with the posterior of 2,t given 1,t :

              ¯2|1,t (1,t ) = 
                              ¯2,t + ¯ 21 ¯- 1
                                           11 (1,t - ¯1,t ),    ¯ 2|1,t = 
                                                                          ¯ 22 - ¯ 21 ¯ -1 ¯
                                                                                        11 12 .     (A.10)

Thus,


    I = (2 )-ny /2 | u |-1/2 (2 )-n /2 ||-1/2 |    ¯ |1/2 exp - 1  -1 t exp 1               ¯ ¯ -1 ¯t
                                                                      2 t u               2 t

        ×(2 )-(n -1)/2 | ¯ 2|1 |-1/2 exp - 1 (2,t -       ¯2|1,t ) ¯ -1 (2,t -  ¯2|1,t )              (A.11)
                                                                     2|1
                                              2

        ×I{1,t   (st-1 )}(2 )-1/2 |     ¯ 11 |-1/2 exp - 1 (1,t -        ¯1,t ) ¯ -1 (1,t - 
                                                                                  11        ¯1,t ) .
                                                              2

This is the final form for term I in (A.7).

   Integrating I in (A.11) with respect to (1,t , 2,t ) and re-introducing the (n) arguments
yields

           D(n) =          I (1,t , 2,t )d2,t d1,t                                                  (A.12)

                                              ¯ n)|1/2 N ( (st-1 ) - 
               = (2 )-ny /2 | u |-1/2 ||-1/2 |(                      ¯1,t (n)/ ¯ 11 (n)
                           1                                   1      ¯ -1 (n)¯
                    × exp - t (n) - 1
                                  u t (n) exp                    ¯ (n)        t (n) .
                           2                                   2 t

The analysis of term II proceeds in almost identical manner, with the understanding that
term I depends on 0 (n), 1 (n), and  (n), whereas term II depends on 0 (b), 1 (b), and
 (b). As a consequence the posterior coefficient matrices    ^ 
                                                          ^, , ¯, and ¯ should also be
indexed by either (n) or (b). Because for term II the inequality in the indicator function is
reversed, we obtain

                                       ¯ b)|1/2 1 - N ( (st-1 ) - 
   D(b) = (2 )-ny /2 | u |-1/2 ||-1/2 |(                          ¯1,t (b))/ ¯ 11 (b)

                      1                                 1      ¯ -1 (b)¯
               × exp - t (n) - 1
                             u t (n) exp                  ¯ (b)        t (b) .                      (A.13)
                      2                                 2 t

   Using the formulas for I , II , D(n), and D(b), we can write the posterior density of t as
follows:
                                         p(yt |t , st-1 )p(st-1 )     I (n) + II (b)
                    p(t |yt , st-1 ) =                              =                .              (A.14)
                                         p(yt |t , st-1 )p(st-1 )dt   D(n) + D(b)
This Version: October 9, 2020                                                                           A-12


Thus, the resulting conditionally optimal proposal is given by the following mixture. Define

                                                   D(n)
                                           =                .                                       (A.15)
                                                D(n) + D(b)

Then, with probability 

     1,t  N (¯
             1,t (n), ¯ 11 (n)I{1,t   (st-1 )},        2,t |1,t  N (¯
                                                                    2|1,t (n, 1,t ), ¯ 2|1 (n))     (A.16)

and with probability 1 - 

     1,t  N ¯1,t (b), ¯ 11 (b) I{1,t >  (st-1 )},      2,t |1,t  N ¯2|1,t (b, 1,t ), ¯ 2|1 (b) .    (A.17)

The incremental weight is constant and given by the following formula:

                                    j
                                   ~t = p(yt |sj
                                               t-1 ) = D (n) + D (b).                               (A.18)

This completes the proof of the proposition.

     In the remainder of this section we consider two special cases: (i) yt identifies the regime
without error. This is the case, for instance, for a DSGE model with ELB constraint if the
interest rate is observed without error, at least when it hits the ELB. (ii) Measurement errors
that are zero or very close to zero.

(i) Known Regime. Let yt = [y1,t , y2,t ] and partition A = [A1 , A2 ] so that the partitions
of A conform with the partitions of yt . Assume that the n-regime is active if and only if
y2,t > c. In the b-regime y2,t = c. Moreover, let  (y2t ; c) denote the Dirac delta function with
the property that  (y2t ; c) = 0 for y2t = c and          (y2t ; c)dy2t = 1. Using the above notation,
we can rewrite (A.7) as

       p(yt |t , st-1 )p(t )
                   ^t|t-1 (n),  u pN (1,t ; 0, 1)pN (2,t ; 0, I )I{1,t   (st-1 )}
         = pN yt ; y
                        ^1t|t-1 (b),  u,11  (y2t ; c)pN (1,t ; 0, 1)pN (2,t ; 0, I )I{1,t >  (st-1 )}
              +pN y1t ; y
         = I + II.

The formula for D(n) in (A.12) remains unchanged. The formula for D(b) in (A.13) changes
to
                                          ~ (b) =  (y2t ; c)D(b)
                                          D
This Version: October 9, 2020                                                                   A-13


with the understanding that u in D(b) needs to be replaced by u,11 . After having observed
yt we know whether y2,t = c so that we can define

                                        = I{y2,t > c}.

Conditional on , we can simulate [1,t , 2,t ] from (A.16) and (A.17), respectively. Finally,


                           j                    D(n) if y2,t > c
                          ~t = p(yt |sj
                                      t-1 ) =                          ,
                                                D(b)     if y2,t = c

where here D(b) corresponds to (A.13) and does not include the Dirac function  (y2t ; c).

(ii) Zero Measurement Errors. Consider a linear state-space model without regimes:

                         yt = Ast + ut ,   st = 0 + 1 st-1 +  t .

Ignoring the censoring and dropping the regime indicator, note that D = p(yt |st-1 ). We can
write
                            yt = A(0 + 1 st-1 ) + A t + ut .

Note that
                           A t + ut  N 0, A  A +  u .

Thus, we can deduce that the term D(n) in (A.12) can be rewritten as

  D(n) = (2 )-ny /2 |A  A +  u |-1/2
                 1                                                 -1
       × exp - (yt - A(0 + 1 st-1 )) A  A +  u                          (yt - A(0 + 1 st-1 ))
                 2
       × N ( (st-1 ) - ¯1 )/ ¯ 11 .


A similar adjustment can be made to the term D(b) in (A.12). The advantage of this
alternative expression is that we can take the limit  - 0. The argument of the Gaussian
This Version: October 9, 2020                                                          A-14


CDF behaves as follows:

                     ¯    =    ¯
                                ^ -1 ^t
                                     ^ -1   -1
                          =     -1 +              A ( u )-1 t
                                                                   -1
                          =     -1 -1 +  -1  A -1
                                               u A                       A -1
                                                                           u t
                                                         -1
                          =      -1 +  A - 1
                                         u A                   A - 1
                                                                 u t ,
                                                 -1
                         -       A -1
                                   u A                 A - 1
                                                         u t


which eliminates divisions by  . Moreover,

                              ¯ =   -1 +  A -1 A              -1
                                            u                      - 0.

Thus,
                                            ¯ 11 =       1 if  (st-1 ) - ¯1  0.
                lim N ( (st-1 ) - ¯1 )/     
                -0                                       0 otherwise

Because the posterior covariances matrices are zero in the limit, the sampling in (A.16) and
(A.17) is replaced by setting 1,t and 2,t equal to their means.
This Version: October 9, 2020                                                                     A-15


E      Additional Details for the Empirical Application

Prior. Table A-1 summarizes the prior distribution for the initial states used in the empirical
analysis.

                        Table A-1: Prior Distributions for Initial States

                                Parameter        Density   P(1)   P(2)
                                   R,0             N       0.00   .002
                                g
                                ^0                 N       0.00   .012
                                z
                                ^0                 N       0.00   .008
                                ^0
                                d                  N       0.00   .170
                                ^0                 N       0.00   .030
                                c
                                ^0                 N       0.00   .030
                                R^ 0 + 0.01        G       0.01   .008

Notes: N is Normal distribution; G is Gamma distribution. P(1) and P(2) are mean and standard deviations
for Normal and Gamma distributions. We set y^0 = c^0 + g
                                                       ^0 and y
                                                              ^-1 = y
                                                                    ^0 .


Calibration of ARRA. Table A-2 summarizes the award and disbursements of funds for
federal contracts, grants, and loans. We translate the numbers in the table into a one-period
location shift of the distribution of    g,t   below.

                  Table A-2: ARRA Funds for Contracts, Grant, and Loans

                                     Awarded Received Nominal GDP
                          2009:2         158       36         3488
                          2009:3          17       18         3533
                          2009:4          26        8         3568
                          2010:1          16       24         3603
                          2010:2          33       26         3644
                          2010:3           9       21         3684
                          2010:4           4       19         3704
                          2011:1           4       20         3751
                          2011:2           8       17         3791
                          2011:3           0       12         3830
                          2011:4           3        9         3870
                          2012:1           0        8         3899

Notes: Data were obtained from www.recovery.gov.
This Version: October 9, 2020                                                                   A-16

                                              ARRA
   Note that a one-time innovation            g,t     generates a response

                                              ARRA
                                            g
                                            ^t +h  = h
                                                     g g
                                                                    ARRA
                                                                    g,t  .

We use the log-linear approximation

                                            ^ARRA =     1
                                             t             ^ARRA ,
                                                           g
                                                      g - 1 t

where t = Gt /Yt . We connect ^ARRA to the data in Table A-2 using the relationship
                               t


                                       ^ARRA = ln        GARRA
                                                          t    /Yt
                                        t                                     .
                                                          G /Y

                                       ARRA
Figure A-2 compares the time path of g
                                     ^t     constructed from the impulse response to a
ARRA
g,t       = 0.0077 (the red solid line) and the time path constructed from the disbursements
in Table A-2 (the blue dashed line).20

                        Figure A-1: Calibration of Fiscal Policy Intervention

                         0.014
                                                  Received     Simulated




                         0.011




                         0.0080




                         0.0050




                             09Q2    09Q4      10Q2          10Q4          11Q2   11Q4




Computational Details for Fiscal Policy Experiment. The following algorithm de-
scribes how we compute the effect of a combined fiscal and monetary intervention.



  20
       Recall that g = 0.0029, hence the ARRA impulse in our experiment is equal to 2.7 × g .
This Version: October 9, 2020                                                                                        A-17


Algorithm 2 (Effect of Combined Fiscal and Monetary Policy Intervention)


  1. Initialize the simulation by setting (R0 , y0 , z0 , g0 , d0 ) equal to the mean estimate obtained
      with the particle filter.

  2. Generate a baseline trajectory that includes the intervention based on the sequence of
                                                                                                     H
      innovations obtained from the COPF: {                   z,T  +s , g,T  +s , d,T  +s , R,T  +s }s=0 .


  3. Generate the innovation sequence for the counterfactual trajectories without interven-
      tion according to

                                     I
                    g,T      =       g,T    -  ARRA ;        g,T  +s     =   I
                                                                             g,T  +s     for   s = 1, . . . , H ;
                                     I
                  z,T  +s    =       z,t    for     s = 0, . . . , H ;
                                     I
                  d,T  +s    =       d,t    for     s = 0, . . . , H ;
                  R,T  +s    = 0           for    s = 0, . . . , H.


  4. Conditional on (R0 , y0 , z0 , g0 , d0 ), compute {RT  +s , yT  +s , T  +s }H
                                                                                 s=0 and
        I        I       I      H                                            I
      {RT  +s , yT  +s , T  +s }s=0 based on {          T  +s }   and {      T  +s },   respectively, and let

                                            IRF (xt | I              I
                                                      t , t ) = (ln xt - ln xt ).                                   (A.19)


We report results for  ARRA = 0.0077 and H = 7 in the main text. When we consider only
a fiscal policy, we set     I
                            R,t   = 0 for t = T  , ..., T  + 7 as well.
This Version: October 9, 2020                                                               A-18


F     Consumption-Savings Model with Borrowing Con-
      straint

In this section we consider a simple consumption-savings model with occasionally-binding
borrowing constraint. We compare the proposed PLC solution algorithm against OccBin and
a time-iteration algorithm. The time-iteration algorithm delivers an exact solution, whereas
the other two algorithms generate approximate solutions.


F.1     Model Specification

Consider the following model in which a representative agent chooses an path for consump-
tion, Ct and borrowing Bt+1 to maximizing the following expected utility function:
                                                  
                                                            Ct1- - 1
                                    max E0              t            ,
                                  {Ct ,Bt+1 }
                                                  t=0
                                                              1-

where  is the coefficient of relative risk aversion. The maximization problem is subject to
the budget constraint and to an exogenous borrowing limit that states that the amount of
debt chosen in the current period, Bt+1 , cannot exceed a fraction m of the current period
income Yt :

                                      Ct + RBt = Yt + Bt+1                                (A.20)
                                                Bt+1  mYt .                               (A.21)

In this economy, borrowing takes the form of one-period non-state contingent bonds that pay
a fixed interest rate R. The borrowing constraint can become occasionally binding depending
on the level of debt at the beginning of the period, Bt , and the realization of the exogenous
stream of income that follows a stochastic process: ln Yt =  ln Yt-1 +  t , with 0   < 1,
 > 0, and     t    N (0, 1). Denoting by t the Lagrange multiplier associated to the borrowing
constraint, we can define a competitive equilibrium allocation.

Definition 1 A competitive equilibrium in this economy is a sequence of consumption, bor-
rowing decisions, and Lagrange multipliers {Ct , Bt+1 , t }t=0 , that given initial conditions B0
This Version: October 9, 2020                                                                               A-19


and the exogenous sequence of income {Yt }t=0 , satisfy the following equations:


                                           (Ct ) - = REt (Ct+1 )- + t                                      (A.22)
                                      Ct + RBt = Yt + Bt+1                                                 (A.23)
                                t (mYt - Bt+1 ) = 0 .                                                      (A.24)


F.2        Solution Algorithms

F.2.1       Time Iteration

The time iteration procedure delivers a global approximation to the recursive equilibrium
associated with the optimality conditions of dynamic optimization problem. We follow the
implementation in Mendoza and Villalvazo (2020) and implement the Fixed-Point Iteration
algorithm (FiPIt).

   The time iteration solves for recursive policy functions of the form Bt+1 = b (B, Y )
and Ct = c(B, Y ) that define the optimal amount of borrowing and consumption given the
current level of debt B and income Y . Denote a candidate policy function as b ^ (B, Y ),
                                                                                                       j
where j denotes the sub-index that helps track the proposed policy function on the j th
step of the algorithm outlined below. The associated consumption decision rule cj (B, Y ) =
Y +b^ (B, Y ) - RB . Note that cj (^
                                   b (B, Y ), Y ) = Y - R^          ^ (b
                                                         b (B, Y )+ b  ^ (B, Y ), Y ). Using the
      j                                    j                        j           j     j
Euler equation (A.22), we can write the recursion under the assumption that the constraint
is slack in period t + 1:
                                                                                               1
                                                                                          -   -
              cj +1 (B, Y ) =   REY            Y - R^            ^ (b
                                                    bj (B, Y ) + b   ^
                                      |Y                           j j (B, Y ), Y )                .       (A.25)


The updated decision rule for bond holdings can be recovered directly from the budget con-
straint combined with borrowing constraint: ^
                                            b (B, Y ) = max {cj +1 (B, Y ) - Y + RB, mY }.
                                                         j +1



Algorithm 3 (FiPIt Algorithm)


  1. Define a grid of values for the state variables S  [Bmin , Bmax ] × [Ymin , Ymax ]. For
          current debt levels we define an equally-spaced grid B = {b1 < b2 < · · · < bNb } with a
          total of Nb = 200 grid points. For the income grid, we discretize the AR(1) process
          for Zt = ln(Yt ) using the method of Kopecky and Suen (2010) and obtain a grid Y =
This Version: October 9, 2020                                                                  A-20


     {y1 < y2 < · · · < yNz } with a total of Nz = 11 grid points. This method also generates
     a transition matrix, PNz ×Nz for the Markov-chain approximation for the evolution of
     Zt that we can use to compute expectations. Finally we construct the solution grid
     using a Cartesian product B  Y.

  2. Start with guess ^                                                          ^
                      bj (B, Y ) for every grid points. (B, Y )  S . To evaluate b    ^
                                                                                   j (bj (B, Y ), Y )

     off the grid points in S , we use one-dimensional linear interpolation.

  3. Compute cj +1 (B, Y ) using equation (A.25).

  4. Obtain the implied borrowing decision rule: ^
                                                 bj +1 (B, Y ) = max {cj +1 (B, Y ) - Y + RB, mY }.

  5. Compute the distance  = ||^
                               bj +1 (B, Y ) - ^
                                               bj (B, Y )||. If   10-7 stop, otherwise set
     j = j + 1 and go back to step 2.

  6. Once the recursion converges, for every (B, Y ), we can obtain c(B, Y ) using (A.23)
     and the ^
             b (B, Y ) that we obtain from the recursion.


F.2.2   PLC Approximation

Let X = {x1 , X2 } be the n × 1 vector of state variables. Specifically: x1 = {Bt } and
X2 = {1, Yt }. We proceed approximating the decision rule for the level of debt that will be
carried into the next period Bt+1 = b (x1 , X2 ) and in the standard generic notation we have
Y = {B }. The PLC decision rule takes the following form:
                                       
                                       1,1 x1 +  X2 , if x1  X2
                                                1,2
                        b (x1 , X2 ) =                             ,                         (A.26)
                                        x +  X        if x   > X
                                        2,1 1   2,2 2      1     2


where the linear function x1 = X2 characterizes the locus of points in the state space that
satisfy borrowing constraint just exactly.

   The borrowing constraint in (A.21) is linear and we can write it as:

                                           >0     if constraint is non-binding (n)
         h (x1 , X2 , y )  -B + mY                                                     .
                                           0      if constraint is binding (b)

Given the general form of the kink function h(x1 , X2 , y )  1 x1 + 2 X2 + Y y , we deduce
that the constants entering the PLC constraint are: 1 = 0, 2 = [0 m] and Y = -1.
This Version: October 9, 2020                                                               A-21


Plugging the decision rule coefficients into the kink functions yields the  coefficients:

                                    - 2 + Y 1,2   2 - 1,2
                               =                =         .
                                     1 + Y 1,1      1,1

Continuity at the kink requires:

                                 2,2 = (1,1 - 2,1 )  + 1,2 .

Replacing  :

                                                  2 - 1,2
                          2,2 = (1,1 - 2,1 )              + 1,2 .
                                                    1,1

From the complementary slackness condition we know that when the constraint binds b =
mY , implies that borrowing does not depend on the previous level of debt {B-1 }, so we can
set 2,1 = 0 and obtain:

                                              2 - 1,2
                                2,2 = 1,1             + 1,2
                                                1,1
                                2,2 = 2 .

Which correctly imposes the constraint. The last step is to verify which side of the constraint
corresponds to the non-binding regime. Following the derivation in the main text, it is easy
to show that, c() = -1,1 < 0. Hence for the constraint to be non-binding, h(.) > 0, we
need x1 <  X2 .

   The free coefficients 1,1 , 2,1 defining the decision rules, can be obtained through the
following numerical algorithm:


Algorithm 4 (PLC Algorithm)


  1. Define a grid S  [Bmin , Bmax ] × [Ymin , Ymax ]. For the PLC algorithm we construct
     the solution grid using the Smolyak algorithm for two dimensions and with an approx-
     imation order of µ = 2, delivering a total of 13 grid points--see Maliar and Maliar
     (2014) for details. The bounds of the hypercube are constructed using simulated series
     under the initial guess of the model, such that we cover 99% of the distribution of the
     endogenous and exogenous state variables.
This Version: October 9, 2020                                                                     A-22


   2. Start with guess  = {1,1 , 1,2 } and use (A.26) to construct b (B, Y ) for every grid
      point (B, Y )  S . Use the resource constraint to compute C (B, Y ) = Y + b (B, Y ) - RB
      which will also be piece-wise linear by construction.

   3. For every grid point (B, Y ), compute the following residuals:

        (a) When the constraint is slack (x1 >  X2 ):

                           (B, Y ) = 0
                           R(B, Y ) = C (B, Y )- - REY      |Y   C (b (B, Y ), Y )-


        (b) When the constraint is binding (x1   X2 ):

                           (B, Y ) = C (B, Y )- - REY       |Y   C (b (B, Y ), Y )-
                           R(B, Y ) = (B, Y ) (mY - b (B, Y ))

            The expectations are approximated using a Gaussian-Quadrature integration rule
            of order 5.

                                                                                   1   M
   4. The PLC solution solves the following minimization problem: min              M   i=1   R(B, Y )2


F.2.3    OccBin Algorithm

The toolkit OccBin (Guerrieri and Iacoviello, 2015) implements a non-linear solution proce-
dure based on a modified shooting algorithm. To solve the model, it is necessary to define
two sets of linearized equilibrium conditions that correspond to a reference regime, with a
binding constraint, and an alternative regime, when the constraint does not bind but it is
expected to bind again the future. OccBin produces a decision rule of the form:

                     Xt = P (Xt-1 , t ) Xt-1 + D (Xt-1 , t ) + Q (Xt-1 , t )   t                (A.27)

in which the evolution of all endogenous variables in the model, Xt , can be expressed as func-
tion of their past Xt-1 and the realization of an exogenous shock t . The solution of the model
with ocassionally binding constraints from periods t = 1, ..., T , is characterized by a sequence
of time-varying matrices {P(.)t , D(.)t , Q(.)t }T
                                                 t=1 . In terms of the simple consumption-savings
problem, we have Xt = [Bt+1 , Ct , t , Yt ] .
This Version: October 9, 2020                                                                A-23


   The decision rule in equation (A.27) shows that the solution matrices are state-dependent
and that the solution need not be piece-wise linear, despite the fact that the time-varying ma-
trices are obtained from linear approximations around the binding and non-binding regimes
of the original problem. The construction of the time-varying matrices requires an iterative
procedure that is briefly described in the following algorithm:

Algorithm 5 (OccBin Algorithm)

  1. Take a generic period t, guess a sequence of regimes, such that for t  T , the model
     returns and forever remains in the reference regime. In our case, this corresponds to
     the case in which the borrowing constraint is binding.

  2. For period t  T , solve the system of expectational equations that correspond to
     the reference regime: A1 Et Xt+1 + A0 Xt + A-1 Xt-1 + B          t   = 0. Where the matri-
     ces A1 , A0 , A-1 , B are functions of the model parameters and Et is the expectations
     operator.   Standard solution procedures, yield a decision rule of the form: Xt =
     D + P Xt-1 + Q t . Given we assumed there are no additional shocks beyond period
     t and that we approximate the solution around the steady state of the reference regime,
     we have PT = P , DT = 0, and XT = P XT -1 .

  3. For periods t < T proceed as follow to compute the solution for Xt :

      (a) If the guess implies the reference regime in period T - 1, solve the matrix equation:
           A1 PT XT -1 + A0 XT -1 + A-1 XT -2 + B   T -1   = 0.
      (b) If the guess implies a change to the alternative regime, solve the matrix equation:
           A                                  
            1 PT XT -1 +A0 XT -1 +A-1 XT -2 +C +B
                                                  
                                                      T -1   = 0. The matrices A                
                                                                                1 , A0 , A-1 , B , C
                                                                                                     

           correspond to the linearized system of equations in the alternative regime when the
           borrowing constraint is slack.
      (c) The solution yields matrices PT -1 , DT -1 , QT -1 . Given XT -1 , solve for XT -2 and
           repeat this step for all t < T . Recall the assumption of no future shocks, hence Qt
           is irrelevant beyond period t and can be set to the null matrix.

  4. Given Xt-1 , t , and the sequence of matrices Pt , Dt and Qt , simulate the model forward
     using Xt = Dt + Pt Xt-1 + Qt t . If the implied regimes in the forward simulation
     coincide with the regimes guessed in Step 1, a solution is given by P (Xt-1 , t ) = Pt ,
     D (Xt-1 , t ) = Dt and Q (Xt-1 , t ) = Qt . Otherwise, update the guess of regimes and
     return to Step 1.
This Version: October 9, 2020                                                                            A-24


                                          Table A-3: Solution Times

                                                  FiPIt                  OccBin      PLC
                                  Time in seconds 10.6                    3.9         0.5



                                          Figure A-2: Decision Rules



                                                            1.02
                1
                                                                  1
              0.95
                                                            0.98

               0.9                                          0.96

                                                            0.94
              0.85
                                                            0.92
                     0.95           1          1.05                      0.95        1          1.05



                                                                  1
                1

              0.95
                                                            0.95

               0.9

              0.85                                               0.9

                0.85        0.9    0.95    1      1.05            0.85       0.9   0.95     1     1.05

                                                 PLC     FiPIt           OccBin



F.3    Quantitative Comparison

We set the following parameters to perform a numerical evaluation of the three solution
algorithms for the consumption-savings problem. R = 1.05,  = 0.945,  = 0.9,  = 0.010,
m = 1,  = 1. The parameter configuration satisfy the condition R < 1 which ensures the
existence of a stationary equilibrium in the incomplete market setting of this model.

   We solve the model and compute solution times for the three methods. To compare with
OccBin, we record the time it takes to construct decision rules over the same solution grid
used to for the global solution implemented with the FiPIt method. The solution times for
the three algorithm are reported in Table A-3.

   Figure A-2 shows two different slices of the decision rules implied by each of the three
This Version: October 9, 2020                                                              A-25


                                   Figure A-3: Simulated Paths




                                                  1.03
        0.98
                                                  1.02
        0.96                                      1.01
                                                     1
        0.94
                                                  0.99
        0.92                                      0.98

                 10     20    30      40   50              10       20   30   40    50



                                                  1.04
        0.03

                                                  1.02
        0.02

                                                     1
        0.01

                                                  0.98
           0
                 10     20    30      40   50              10       20   30   40    50

                                           PLC    FiPIt    OccBin



solution algorithms. The top row shows the decision rules for consumption and borrowing
decisions as a function of income, while holding the current period level of debt at its steady
state of B = 1. The second row shows the decision rules for consumption and borrowing
decisions as a function of the current level of debt, while holding the income realization at its
steady state level of Y = 1. In each panel, the grade shaded areas show 95% of the ergodic
distribution of the associated state variable.

   Figure A-3 shows a simulated path of 50 observations for the three solution algorithms.
We show the time path for consumption, borrowing and the Lagrange multiplier as well as
the exogenous sequence of income realizations.

   One may also want to see how the three methods compare in terms of where the constraint
binds in the state space. To investigate this, we compare the three algorithms in terms of
their predictions about the constraint binding. Taking the global solution as the "truth", we
This Version: October 9, 2020                                                             A-26


find that when the constraint is slack in the global solution, the other two algorithms also
predict that the constraint will be slack with over 99.7% probability. When the constraint
in the global solution is binding, that is when  > 0, then for a vast majority of the cases,
both solutions find that the constraint will be binding as well. In particular, if we look at
quartiles of  values obtained in the global solution conditional on  > 0, in the third and
fourth quartiles, the match is virtually perfect, and in the second quartile match is over 92%.
Only in the first quartile ­ when  is small ­ both methods deliver a positive  in about 20%
of the cases.

   Looking at these results, we conclude that the PLC algorithm produces results that are
very close to OccBin. Relative to the global solution, the decision rules show small differences
around the kink, but the simulations show that this is not consequential: consumption
simulation is virtually indistinguishable while the borrowing simulation shows slightly bigger
but still small deviations. Similarly the multiplier is slightly different especially when it is
positive and small, but in terms of identification of when the constraint binds, the three
methods broadly agree. At the end of the day, for estimation purposes the simulation is
really what matters and the three methods very much agree, with PLC taking about 1/20
of the time of the global solution and about 1/8 of the time of OccBin.
