                               NBER WORKING PAPER SERIES




       NONLINEAR PROGRAMMING METHOD FOR DYNAMIC PROGRAMMING

                                          Yongyang Cai
                                         Kenneth L. Judd
                                       Thomas S. Lontzek
                                      Valentina Michelangeli
                                           Che-Lin Su

                                       Working Paper 19034
                               http://www.nber.org/papers/w19034


                     NATIONAL BUREAU OF ECONOMIC RESEARCH
                              1050 Massachusetts Avenue
                                Cambridge, MA 02138
                                     May 2013




Cai and Judd gratefully acknowledge NSF support (SES-0951576). Michelangeli acknowledges the
funding of the Bank of Italy research fellowship. The views expressed herein are those of the authors
and do not necessarily reflect the views of the National Bureau of Economic Research or the Bank
of Italy.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

¬© 2013 by Yongyang Cai, Kenneth L. Judd, Thomas S. Lontzek, Valentina Michelangeli, and Che-Lin
Su. All rights reserved. Short sections of text, not to exceed two paragraphs, may be quoted without
explicit permission provided that full credit, including ¬© notice, is given to the source.
Nonlinear Programming Method for Dynamic Programming
Yongyang Cai, Kenneth L. Judd, Thomas S. Lontzek, Valentina Michelangeli, and Che-Lin
Su
NBER Working Paper No. 19034
May 2013
JEL No. C61,C63

                                           ABSTRACT

A nonlinear programming formulation is introduced to solve infinite horizon dynamic programming
problems. This extends the linear approach to dynamic programming by using ideas from approximation
theory to avoid inefficient discretization. Our numerical results show that this nonlinear programming
method is efficient and accurate.


Yongyang Cai                                      Valentina Michelangeli
Hoover Institution                                Banca d'Italia
Stanford University                               Via Nazionale 91, 00184 Roma
Stanford, CA 94305                                valentina.michelangeli@gmail.com
yycai@stanford.edu
                                                  Che-Lin Su
Kenneth L. Judd                                   University of Chicago
Hoover Institution                                che-lin.su@ChicagoBooth.edu
Stanford University
Stanford, CA 94305-6010
and NBER
kennethjudd@mac.com

Thomas S. Lontzek
University of Zurich
Moussonstrasse 15, 8044 Zurich
Thomas.Lontzek@Business.uzh.ch
1    Introduction
Dynamic programming (DP) is the essential tool in solving problems of dy-
namic and stochastic controls in economic analysis. The nonlinearities of
dynamic economic problems make them numerically challenging. To avoid
the nonlinearity, linear programming (LP) approaches have been studied in
the literature; see De Farias and Van Roy (2003), and Trick and Zin (1997).
However, the LP approach has limited value for problems with continuous ac-
tions and/or states since an LP approach would have to discretize the states
and controls. The most common discretization technique is the Cartesian
grid, which leads to a curse-of-dimensionality in both the state and action
spaces: if each of action or state variable is discretized by m equally spaced
nodes, then the number of points is up to md , where d is the number of
both action and state variables. Moreover, in many economic problems, es-
pecially those involving policy evaluations or welfare analysis, it is necessary
to obtain accurate approximations of the decision rules, a task that is much
more difficult than approximating the value function. Therefore, if we want
to have 5-digit accuracy for a problem defined on one state variable and two
controls, all of which are mapped to a unit cube, the LP approach would
require 100,000 points in each dimension for a total of 1015 points, a problem
size that is currently infeasible.
    This paper presents a nonlinear programming (NLP) method, called
DPNLP, to solve the infinite horizon DP problems with or without stochas-
ticity. The method uses shape-preserving approximation methods to approx-
imate the optimal value function by adding some extra degree of freedom.
DPNLP solves the deterministic or stochastic DP problem with one or two
continuous state variables and several continuous control variables without
the curse-of-dimensionality of the action space. Moreover, in our numeri-
cal examples, DPNLP uses only 19 nodes of the continuous state and their
corresponding 19 two-dimensional actions and takes only seconds or about
one minute to achieve the 5-digit or higher accuracy for both determinis-
tic and stochastic DP problems with one continuous state (and one discrete
state for the stochastic DP) and two continuous control variables. Since
DPNLP has no curse-of-dimensionality of the action space, it can also solve
DP problems with many continuous control variables easily and quickly. In
our two-country optimal growth examples, the problems have two contin-
uous state variables and six continuous control variables. This makes the
LP approach infeasible even within only 2-digit accuracy, but DPNLP solves
them in minutes with up to 5-digit accuracy.
    In this paper, the DPNLP method is described as an adequate tool to
solve a DP problem with one or multiple continuous state variables (and
discrete state variables) and multiple continuous control variables. Many
economic problems involve models with one or several variables that is ‚Äúby
definition‚Äù continuous (such as wealth or capital). Discretizing such a state

                                       2
would require many grid points, with the computational costs associated
to it. However, other state variables, even though continuous, often follow
processes that make them suitable for discretization, without significant loss
in terms of accuracy of the solution.
    Our DPNLP method can also be a crucial component of empirical estima-
tion methods. Michelangeli (2009) used DPNLP inside an MPEC approach
(see Su and Judd, 2012) to estimating a model of the demand for reverse
mortgages.
    The paper is constructed as follows. Section 2 describes the kind of dy-
namic problem commonly used in economics and the subject of this paper.
Section 3 briefly reviews approximation methods such as Chebyshev poly-
nomial approximation. Section 4 defines the DPNLP method for solving
infinite horizon DP problems. Section 5, 6 and 7 apply DPNLP to opti-
mal accumulation problems similar to many economics problems. Section 8
concludes.


2    Dynamic Programming
An infinite horizon stochastic optimal decision-making problem has the fol-
lowing general form:
                                          (‚àû                   )
                                             X
                   V (x0 , Œ∏0 ) = max E         Œ≤ t u(xt , at ) ,      (1)
                                    at ‚ààD(xt ,Œ∏t )
                                                        t=0
                                       s.t.       xt+1 = g(xt , Œ∏t , at ),
                                                  Œ∏t+1 = h(Œ∏t , t ),

where xt is the discrete time continuous state vector process with initial state
x0 , Œ∏t is the discrete state vector process with initial state Œ∏0 , t is a serially
uncorrelated random vector process, g is a continuous function representing
the change in the state xt as a function of the state and action, at , and h
represents the transition process for Œ∏t respectively, D(xt , Œ∏t ) is a feasible
set of at dependent on (xt , Œ∏t ), Œ≤ is the discount factor with 0 < Œ≤ < 1, u
is a concave utility function, and E{¬∑} is the expectation operator. While
this description does not apply to many applications of dynamic program-
ming, it does apply to most models in dynamic economics. Examples include
economic growth, portfolio decisions, and investment decisions by firms.
     The DP model for the general infinite horizon problem is the following
Bellman equation (Bellman, 1957):

                V (x, Œ∏) = max u(x, a) + Œ≤E V (x+ , Œ∏+ ) | x, Œ∏, a ,
                                                
                                                                                  (2)
                          a‚ààD(x,Œ∏)

                             s.t.      x+ = g(x, Œ∏, a),
                                       Œ∏+ = h(Œ∏, ),


                                              3
where (x+ , Œ∏+ ) is the next-stage state conditional on the current-stage state
(x, Œ∏) and the action a,  is a random variable, and V (x, Œ∏) is the value
function.
    In the simpler case where there is no uncertainty, there is no stochastic
state Œ∏t , the problem (1) becomes
                                               ‚àû
                                               X
                       V (x0 ) = max                 Œ≤ t u(xt , at ),
                                 at ‚ààD(xt )
                                               t=0
                                    s.t.       xt+1 = g(xt , at ),

and its Bellman equation is:

                       V (x) = max           u(x, a) + Œ≤V (x+ ),           (3)
                                a‚ààD(x)

                                 s.t.        x+ = g(x, a).


3    Approximation
In the DP problem (3), we want to solve for the optimal value function. Even
though the method allows to compute both the value functions and the policy
functions, the implementation of the steps require to solve for the optimal
value functions. But when state and control variables are continuous such
that value functions are also continuous, we have to use some approximation
for the value functions, since computers cannot model the entire space of
continuous functions.
     An approximation scheme consists of two parts: basis functions and ap-
proximation nodes. Approximation nodes can be chosen as uniformly spaced
nodes, Chebyshev nodes, or some other specified nodes. From the view-
point of basis functions, approximation methods can be classified as either
spectral methods or finite element methods. A spectral method uses glob-
           P basis functions {œÜj (x)} and coefficients b = {bj } such that
ally nonzero
VÃÇ (x; b) = nj=0 bj œÜj (x) is a degree-n approximation. Examples of spectral
methods include ordinary polynomial approximation, Chebyshev polynomial
approximation, and shape-preserving Chebyshev polynomial approximation
(Cai and Judd, 2012b). In contrast, a finite element method uses locally basis
functions {œÜj (x)} that are nonzero over sub-domains of the approximation
domain. Examples of finite element methods include piecewise linear inter-
polation, Schumaker interpolation, shape-preserving rational function spline
Hermite interpolation (Cai and Judd, 2012a), cubic splines, and B-splines.
See Cai (2010), Cai and Judd (2010), and Judd (1998) for more details.




                                         4
3.1     Chebyshev Polynomial Approximation
Chebyshev polynomials on [‚àí1, 1] are defined as Tj (z) = cos(j cos‚àí1 (z)).
Economics problems typically live on an interval [xmin , xmax ]; if we let
                                          2x ‚àí xmin ‚àí xmax
                                Z (x) =                    ,
                                            xmax ‚àí xmin
then Tj (Z (x)) are Chebyshev polynomials adapted to [xmin , xmax ] for j =
0, 1, 2, . . .. These
                    ¬¥ xpolynomials   are orthogonal under the weighted inner prod-
                        max
uct: hf, gi = xmin f (x)g(x)w(x)dx with the weighting function w(x) =
                ‚àí1/2
 1 ‚àí Z(x)2            . A degree n Chebyshev polynomial approximation for V (x)
on [xmin , xmax ] is
                                           Xn
                              VÃÇ (x; b) =     bj Tj (Z (x)),                   (4)
                                             j=0

where b = {bj } are the Chebyshev coefficients. It is often more stable to use
the expanded Chebyshev polynomial interpolation (Cai, 2010), as the above
standard Chebyshev polynomial interpolation gives poor approximation in
the neighborhood of end points.
    In this section we describe the Chebyshev polynomial approximation be-
cause it is the approximation scheme used in our examples. While also other
approximation schemes may be adequate and with good performances, the
Chebyshev polynomial approximation presents advantages in terms of coding
simplicity and reliability, and easy extension to multidimensional approxi-
mation.

3.2     Multidimensional Complete Chebyshev Approximation
In a d-dimensional approximation problem, let the domain of the approxi-
mation function be

                   x = (x1 , . . . , xd ) : xmin ‚â§ xi ‚â§ xmax
               
                                             i           i   , i = 1, . . . d ,
for some real numbers xmin        i    and xmax
                                             i   with xmax
                                                        i     > xmin
                                                                   i    for i = 1, . . . , d.
Let x    min         min         min
               = (x1 , . . . , xd ) and x   max     max           max
                                                = (x1 , . . . , xd ). Then we denote
[xmin , xmax ] as the domain. Let Œ± = (Œ±1 , . . . , Œ±d ) be a vector of nonneg-
ative integers. Let TŒ± (z) denote the product TŒ±1 (z1 ) ¬∑ ¬∑ ¬∑ TŒ±d (zd ) for z =
(z1 , . . . , zd ) ‚àà [‚àí1, 1]d . Let

                            2x1 ‚àí xmin   ‚àí xmax       2xd ‚àí xmin     ‚àí xmax
                                                                           
                                     1       1                  d       d
                  Z(x) =                        ,...,
                                 xmax
                                  1   ‚àí xmin
                                          1               xmax
                                                           d     ‚àí xmin
                                                                      d

for any x = (x1 , . . . , xd ) ‚àà [xmin , xmax ].




                                               5
    Using these notations, the degree-n complete Chebyshev approximation
for V (x) is                       X
                      VÃÇn (x; b) =       bŒ± TŒ± (Z(x)) ,              (5)
                                           0‚â§|Œ±|‚â§n
              Pd
where |Œ±| = i=1 Œ±i for the nonnegative integer vector Œ± = (Œ±1 , . . . , Œ±d ). So
the number of terms with 0 ‚â§ |Œ±| = di=1 Œ±i ‚â§ n is n+d
                                   P                    
                                                      d   for the degree-n
complete Chebyshev approximation in Rd .


4     Nonlinear Programming Method to Solve Bell-
      man Equations
There are many approaches to solve Bellman equations, such as value func-
tion iteration and policy iteration methods, or LP approaches. This section
describes the general nonlinear programming method (DPNLP) to solve the
Bellman equations (3) or (2).

4.1   Basic DPNLP
To solve the problem (3), we discretize the nonlinear approximation of the
value function instead of the state and action spaces. The following nonlinear
programming problem expresses one possible formulation of this method:
                              m
                              X
             max                    vi ,
       ai ‚ààD(xi ),x+
                   i ,vi ,b   i=1
                 s.t.         vi ‚â§ u(xi , ai ) + Œ≤ VÃÇ (x+
                                                        i ; b),           i = 1, . . . , m,
                              x+
                               i    = g(xi, , ai ),    i = 1, . . . , m,
                              vi = VÃÇ (xi ; b),       i = 1, . . . , m,

where m is the number of the approximation nodes. In this method, the
choice variables are the actions a, the next-stage states x+ , the value func-
tions v, and the coefficients b.
    Unfortunately the solutions of the above model often has no shape prop-
erties, i.e., the value function is not increasing or concave. One approach
to improve it is to add shape-preservation in the model. See Cai and Judd
(2010, 2012a, 2012b) for the discussion of importance of shape-preservation




                                              6
in DP. Now we have the basic DPNLP model:
                                   m
                                   X
               max                       vi ,                                                               (6)
        ai ‚ààD(xi ),x+
                    i ,vi ,b       i=1
                   s.t.            vi ‚â§ u(xi , ai ) + Œ≤ VÃÇ (x+
                                                             i ; b),              i = 1, . . . , m,
                                   x+
                                    i    = g(xi, , ai ),    i = 1, . . . , m,
                                   vi = VÃÇ (xi ; b),       i = 1, . . . , m,
                                     0
                                   VÃÇ (yi0 ; b) ‚â• 0,       i0 = 1, . . . , m0 ,
                                   VÃÇ 00 (yi0 ; b) ‚â§ 0,     i0 = 1, . . . , m0 ,

where {yi0 : i0 = 1, . . . , m0 } are the set of shape nodes for shape preservation
constraints. Usually the number of shape nodes, m0 , is more than the number
of approximation nodes, m.
     To solve the stochastic Bellman equation (2) where Œ∏ ‚àà Œò = {œëj : j =
1, ..., J}, the basic DPNLP model becomes
                                     J X
                                     X m
                min                             vi,j ,
      ai,j ‚ààD(xi ,œëj ),x+
                        i ,vi ,b     j=1 i=1
                                                                    J
                                                                    X
                      s.t.           vi,j ‚â§ u(xi , ai,j ) + Œ≤                Pj,j 0 VÃÇ (x+
                                                                                         i,j , œëj 0 ; b),
                                                                    j 0 =1

                                     x+
                                      i,j   = g(xi , œëj , ai,j ),
                                     vi,j = VÃÇ (xi , œëj ; b),
                                     VÃÇ 0 (yi0 , œëj ; b) ‚â• 0,       i0 = 1, . . . , m0 ,
                                     VÃÇ 00 (yi0 , œëj ; b) ‚â§ 0,      i0 = 1, . . . , m0 ,
                                     i = 1, . . . , m, j = 1, ..., J,

where Pj,j 0 is the conditional   probability of Œ∏+ = Œ∏j 0 given Œ∏ = Œ∏j , i.e.,
Pj,j 0 = Pr Œ∏+ = Œ∏j 0 | Œ∏ = Œ∏j , for any j, j 0 = 1, . . . , J.
                              


4.2    Iterative DPNLP
One problem of the basic DPNLP model (6) is that an optimization solver of-
ten gives a solution where the equality in (7) does not hold while VÃÇ 0 (yi0 ; b) ‚â•
0 or VÃÇ 00 (yi0 ; b) ‚â§ 0 are binding at some shape nodes instead. However, the
true solution of the basic DPNLP model (6) should let the inequality con-
straints
                              vi ‚â§ u(xi , ai ) + Œ≤ VÃÇ (x+
                                                        i ; b),                 (7)
be binding for all i = 1, . . . , m, and VÃÇ (yi0 ; b) should be strictly increasing and
concave at all the shape nodes. We introduce an iterative DPNLP method
to solve these problems.

                                                    7
    In this paper, we use the Chebyshev polynomial approximation in VÃÇ . For
a smooth function, we know that the Chebyshev polynomial approximation
usually have a smaller coefficients in magnitude for higher-degree terms. This
tells us that a small-degree Chebyshev polynomial approximation in VÃÇ is a
good initial guess for a higher-degree Chebyshev polynomial approximation.
Another issue is that a quadratic Chebyshev polynomial approximation in VÃÇ
will be a good shape-preserving approximation with increasing and concave
properties. Therefore, we have the following iterative DPNLP method to
solve the infinite horizon deterministic optimal decision-making problems.

Algorithm 1 Iterative DPNLP Method for Infinite Horizon Deterministic
    Optimal Decision-Making Problems

Initialization. Choose m expanded Chebyshev nodes {xi : 1 ‚â§ i ‚â§ m} on
      the range [xmin , xmax ] as the approximation nodes (with an odd num-
      ber m), choose m0 expanded Chebyshev nodes {yi : 1 ‚â§ i ‚â§ m0 } on the
      range [xmin , xmax ] as the shape nodes, and choose the Chebyshev poly-
      nomial approximation for VÃÇ (x; b) with degree n. Then solve the Basic
      DPNLP model (6) with degree-2 Chebyshev polynomial approximation.
      For a degree n = 3, . . . , m ‚àí 1, iterate through steps 1 and 2.

Step 1. Use the solutions of the Basic DPNLP model (6) with degree n ‚àí 1
     Chebyshev polynomial approximation as the initial start point of the
     Basic DPNLP model (6) with degree n.

Step 2. Use a reliable optimizer to solve the Basic DPNLP model (6) with
     degree n.

It is easy to extend the algorithm to solve the infinite horizon stochastic
and/or multidimensional optimal decision-making problems.


5        Applications to Deterministic Optimal Growth
         Problems
An infinite-horizon economic problem is the discrete-time optimal growth
model with one good and one capital stock, which is a deterministic model1 .
The aim is to find the optimal consumption function and the optimal labor
supply function such that the total utility over the infinite-horizon time is
maximal, i.e.,
                                                 ‚àû
                                                 X
                              V (k0 ) = max            Œ≤ t u(ct , lt ),       (8)
                                          c,l
                                                 t=0
                                          s.t.   kt+1 = F (kt , lt ) ‚àí ct ,
    1
        Please see Judd (1998) for a detailed description of this.


                                                 8
where kt is the capital stock at time t with k0 given in [0.3, 2], ct is the
consumption, lt is the labor supply, Œ≤ is the discount factor, F (k, l) is the
aggregate production function, and u(ct , lt ) is the utility function.
   In the examples, the aggregate production function is F (k, l) = k +
Ak œà l1‚àíœà with œà = 0.25 and A = (1 ‚àí Œ≤)/(œàŒ≤). The utility function is

                                  (c/A)1‚àíŒ≥ ‚àí 1           l1+Œ∑ ‚àí 1
                 u(c, l) =                     ‚àí (1 ‚àí œà)          .                       (9)
                                      1‚àíŒ≥                  1+Œ∑
The functional forms for utility and production imply that the steady state
of the infinite horizon deterministic optimal growth problems is kss = 1, and
the optimal consumption and the optimal labor supply at kss are respectively
css = A and lss = 1. The code for DPNLP is written in GAMS (McCarl,
2011), and the optimization solver is CONOPT (in the GAMS environment).

5.1   True Solution
In order to estimate the accuracy of solution given by DPNLP, we compute
the ‚Äútrue‚Äù optimal solution on a large set of test points for initial capital
k0 ‚àà [0.3, 2], and then compare those results with the computed optimal
solution from DPNLP. To get the ‚Äútrue‚Äù optimal solution, we discretize the
range of capital, [0.3, 2], with one million equally-spaced capital nodes, and
also discretize the range of labor supply, [0.4, 2.5], with another one million
equally-spaced labor supply nodes. for a discrete capital node k among the
one million capital nodes and a discrete labor supply node l among the one
million labor supply nodes, we choose consumption c = F (k, l) ‚àí k + such
that k + is also one node among the one million capital nodes. Then using the
one million capital nodes as discrete states, we apply the alternating sweep
Gauss-Seidel algorithm (Judd, 1998) to compute the optimal value function
until it converges under the stopping criterion 10‚àí7 .

5.2   DPNLP Solution
We use the iterative DPNLP method (Algorithm 1) to solve the deterministic
optimal growth problem. The basic DPNLP model is
                            m
                            X
                max                vi ,                                                  (10)
              c,l,k+ ,v,b
                            i=1
                  s.t.      vi ‚â§ u(ci , li ) + Œ≤ VÃÇ (ki+ ; b),       i = 1, . . . , m,
                            ki+   ‚â§ F (ki, , li ) ‚àí ci ,    i = 1, . . . , m,
                            vi = VÃÇ (ki ; b),       i = 1, . . . , m,
                              0
                            VÃÇ (yi0 ; b) ‚â• 0,       i0 = 1, . . . , m0 ,
                            VÃÇ 00 (yi0 ; b) ‚â§ 0,     i0 = 1, . . . , m0 .


                                                9
    For our examples in this section, we always choose m = 19 expanded
Chebyshev nodes, ki , in [0.3, 2], as the approximation nodes, and the ap-
proximation method, VÃÇ , is the expanded Chebyshev polynomial up to the
maximal degree 18, and we choose m0 = 100 expanded Chebyshev nodes, yi0 ,
in [0.3, 2], as the shape nodes. In fact, in some cases among our examples,
we could use less numbers to save computational time but with almost the
same accuracy.

5.3    Error Analysis of DPNLP Solution
We next use some basic examples of the deterministic optimal growth prob-
lem to test DPNLP. We tries Œ≤ = 0.9, 0.95, 0.99, Œ≥ = 0.5, 2, 8, and Œ∑ =
0.2, 1, 5, all these examples give us good solutions.
    Table 1 lists relative errors of optimal solutions computed by DPNLP for
these cases in comparison with the ‚Äútrue‚Äù solution given by the high-precision
discretization method. The errors for optimal consumptions are computed
by
                                     |c‚àóDPNLP (k) ‚àí c‚àó (k)|
                            max                             ,
                           k‚àà[0.3,2]        |c‚àó (k)|
where c‚àóDPNLP (k) is the optimal consumption computed by DPNLP, and
c‚àó (k) is the ‚Äútrue‚Äù optimal consumption, for k ‚àà [0.3, 2]. The errors for
                         ‚àó
optimal labor supply, lDPNLP   , have the similar computation formula. The
last column of Table 1 lists the running time of the iterative DPNLP method
for various cases in the GAMS environment, on a single core of a Mac laptop
with a 2.5 GHz processor.
     Table 1 shows that DPNLP solves the examples with accuracy up to 5
digits or higher for optimal control policy functions in all cases. Moreover,
the DPNLP method is fast and takes only several seconds for each case For
example, row one in Table 1 assumes Œ≤ = 0.9, Œ≥ = 0.5, and Œ∑ = 0.2. For
this case, the error in consumption is 1.5 √ó 10‚àí6 , the error in labor supply
is 1.8 √ó 10‚àí6 , and the running time is only 6.8 seconds.


6     Applications to Stochastic Optimal Growth Prob-
      lems
When the capital stock is dependent on a random economic shock Œ∏t , the op-
timal growth problem (8) becomes a stochastic dynamic optimization prob-
lem. Assume that the random economic shock Œ∏t is a stochastic process
following Œ∏t+1 = h(Œ∏t , Œµt ), where t is a serially uncorrelated random process.
Let f (k, l, Œ∏) denote net production function, and F (k, l, Œ∏) = k + f (k, l, Œ∏).
Then the infinite-horizon discrete-time stochastic optimization problem be-




                                       10
Table 1: Relative Errors of DPNLP for Deterministic Optimal Growth Prob-
lems
    Œ≤     Œ≥     Œ∑   Error of c‚àóDPNLP Error of lDPNLP
                                               ‚àó        Time (seconds)
   0.9 0.5 0.2           1.5(‚àí6)          1.8(‚àí6)            6.8
                1        3.1(‚àí6)          1.5(‚àí6)            3.8
                5        3.0(‚àí6)          1.1(‚àí6)            4.4
          2 0.2          1.1(‚àí6)          3.6(‚àí6)            4.3
                1        1.4(‚àí6)          2.3(‚àí6)            7.0
                5        2.2(‚àí6)          1.2(‚àí6)            4.5
          8 0.2          9.7(‚àí6)          3.7(‚àí6)            5.7
                1        1.0(‚àí6)          2.6(‚àí6)            3.9
                5        1.5(‚àí6)          3.5(‚àí6)            3.8
   0.95 0.5 0.2          3.1(‚àí6)          3.7(‚àí6)            3.8
                1        4.7(‚àí6)          1.9(‚àí6)            3.7
                5        4.8(‚àí6)          1.2(‚àí6)            3.4
          2 0.2          1.6(‚àí6)          5.8(‚àí6)            4.2
                1        2.2(‚àí6)          3.4(‚àí6)            4.3
                5        3.5(‚àí6)          1.9(‚àí6)            3.7
          8 0.2          1.2(‚àí6)          6.7(‚àí6)            4.6
                1        1.2(‚àí6)          5.2(‚àí6)            4.3
                5        2.8(‚àí6)          4.8(‚àí6)            4.3
   0.99 0.5 0.2          1.2(‚àí5)          1.3(‚àí5)            4.8
                1        3.0(‚àí5)          1.1(‚àí5)            4.8
                5        4.2(‚àí5)          4.3(‚àí6)            3.9
          2 0.2          6.1(‚àí6)          2.4(‚àí5)            5.7
                1        1.0(‚àí5)          1.6(‚àí5)            5.3
                5        1.8(‚àí5)          7.7(‚àí6)            5.6
          8 0.2          2.0(‚àí6)          3.2(‚àí5)            7.4
                1        3.9(‚àí6)          2.2(‚àí5)            6.3
                5        1.1(‚àí5)          1.6(‚àí5)            6.9
Note: a(k) means a √ó 10k .




                                  11
comes
                                          (‚àû                   )
                                           X
                   V (k0 , Œ∏0 ) = max E           Œ≤ t u(ct , lt ) ,      (11)
                               k,c,l
                                            t=0
                                s.t.   kt+1 = F (kt , lt , Œ∏t ) ‚àí ct ,
                                        Œ∏t+1 = h(Œ∏t , Œµt ),

where k0 ‚àà [0.3, 2] and Œ∏0 are given. The parameter Œ∏ has many economic
interpretations. In the life-cycle interpretation, Œ∏ is a state variable that
may affect either asset income, labor income, or both. In the monopolist
interpretation, Œ∏ may reflect shocks to costs, demand, or both.
    We use the same utility function (9), but the production function is
changed to
                          F (k, l, Œ∏) = k + Œ∏Ak œà l1‚àíœà
where Œ∏ is the stochastic state, œà = 0.25 and A = (1 ‚àí Œ≤)/(œàŒ≤). In the
examples, Œ∏t is assumed to be a Markov chain with 3 possible values:

                      œë1 = 0.95, œë2 = 1.0, œë3 = 1.05,

and the probability transition matrix from Œ∏t to Œ∏t+1 is
                             Ô£Æ                   Ô£π
                                0.75 0.25    0
                        P = Ô£∞ 0.25 0.5 0.25 Ô£ª .
                                  0   0.25 0.75

The code for DPNLP is written in GAMS (McCarl, 2011), and the optimiza-
tion solver is CONOPT (in the GAMS environment).

6.1     True Solution
For the deterministic optimal growth problem (8), we use the discretized
method and the alternating sweep Gauss-Seidel algorithm to get the ‚Äútrue‚Äù
solution. But the DP method with high-precision discretization will be
too time-consuming for solving the stochastic optimal growth problem (11).
However, Cai and Judd (2012a) introduces a value function iteration method
using a shape-preserving rational spline interpolation and shows that it is
very accurate for solving multi-period portfolio optimization problems. For
the deterministic optimal growth problem, since the value function is smooth,
increasing and concave over the continuous state, capital k, we can also ap-
ply this shape-preserving DP algorithm to solve the deterministic optimal
growth problem and realize that it is also very accurate (by comparing its
solution with those given by the alternating sweep Gauss-Seidel algorithm).
    For the stochastic optimal growth problem, the value function for each
discrete state is also smooth, increasing and concave over the continuous


                                       12
state, capital k. Therefore, we can again choose the shape-preserving value
function iteration method to solve the stochastic optimal growth problem
and iterates until it converges under the stopping criterion 10‚àí7 . We use
1000 equally-spaced interpolation nodes on the range of the continuous state,
[0.3, 2], for each discrete state Œ∏.

6.2   DPNLP Solution
We use the iterative DPNLP method (stochastic version of Algorithm 1) to
solve the stochastic optimal growth problem. The basic DPNLP model is
                            J X
                            X m
               max                     vi,j ,                                                   (12)
             c,l,k+ ,v,b
                            j=1 i=1
                                                        J
                                                        X
                                                                             +
                 s.t.      vi,j ‚â§ u(ci,j , li,j ) + Œ≤            Pj,j 0 VÃÇ (ki,j , Œ∏j 0 ; b),
                                                        j 0 =1
                            +
                           ki,j ‚â§ F (ki, , li,j , Œ∏j ) ‚àí ci,j ,
                           vi,j = VÃÇ (ki , Œ∏j ; b),
                           VÃÇ 0 (yi0 , Œ∏j ; b) ‚â• 0,
                           VÃÇ 00 (yi0 , Œ∏j ; b) ‚â§ 0,
                           i = 1, . . . , m, j = 1, ..., J, i0 = 1, ..., m0 .
where J = 3, m = 19, m0 = 100, ki are expanded Chebyshev nodes in [0.3, 2],
VÃÇ is the expanded Chebyshev polynomial up to the maximal degree 18, and
yi0 are expanded Chebyshev nodes in [0.3, 2] as the shape nodes.

6.3   Error Analysis of DPNLP Solution
We examine the errors for the stochastic model in the similar manner we did
for the deterministic optimal growth problems: We apply the high-precision
value function iteration to get the ‚Äútrue‚Äù optimal solution for every test
point of initial capital k0 and every possible initial discrete state Œ∏0 , and
then use them to check the accuracy of the computed optimal solution from
the DPNLP model (12).
    Table 2 lists relative errors of optimal solutions computed by DPNLP
for the stochastic optimal growth problem with the following cases: Œ≤ =
0.9, 0.95, 0.99, Œ≥ = 0.5, 2, 8, and Œ∑ = 0.2, 1, 5. The errors for optimal con-
sumptions at time 0 are computed by
                                           |c‚àóDPNLP (k, Œ∏) ‚àí c‚àó (k, Œ∏)|
                           max                                          ,
               k‚àà[0.3,2],Œ∏‚àà{0.95,1.0,1.05}         |c‚àó (k, Œ∏)|
where c‚àóDPNLP is the optimal consumption computed by DPNLP on the
model (12), and c‚àó is the ‚Äútrue‚Äù optimal consumption computed by the high-
precision value function iteration method. The similar formula applies to

                                                13
Table 2: Relative Errors of DPNLP for Stochastic Optimal Growth Problems
    Œ≤     Œ≥     Œ∑    Error of c‚àóDPNLP Error of lDPNLP
                                                ‚àó       Time (seconds)
   0.9 0.5 0.2           1.9(‚àí7)          5.2(‚àí7)             11
                1        2.5(‚àí7)          4.5(‚àí7)             9
                5        2.5(‚àí7)          4.7(‚àí7)             9
          2 0.2          1.4(‚àí7)          5.0(‚àí7)             16
                1        2.0(‚àí7)          5.9(‚àí7)             12
                5        3.0(‚àí7)          4.4(‚àí7)             12
          8 0.2          1.1(‚àí7)          8.4(‚àí7)             22
                1        1.6(‚àí7)          8.8(‚àí7)             18
                5        8.5(‚àí7)          1.2(‚àí6)             15
  0.95 0.5 0.2           3.7(‚àí7)          4.8(‚àí7)             15
                1        3.9(‚àí7)          4.2(‚àí7)             11
                5        4.4(‚àí7)          4.4(‚àí7)             10
          2 0.2          2.9(‚àí7)          6.6(‚àí7)             22
                1        3.2(‚àí7)          5.9(‚àí7)             17
                5        4.4(‚àí7)          4.4(‚àí7)             13
          8 0.2          2.3(‚àí7)          9.6(‚àí7)             25
                1        3.0(‚àí7)          8.7(‚àí7)             21
                5        9.7(‚àí7)          1.3(‚àí6)             23
  0.99 0.5 0.2           4.1(‚àí7)          6.1(‚àí7)             30
                1        4.5(‚àí7)          4.6(‚àí7)             22
                5        4.1(‚àí7)          4.6(‚àí7)             17
          2 0.2          3.0(‚àí7)          1.1(‚àí6)             50
                1        3.4(‚àí7)          7.4(‚àí7)             40
                5        5.9(‚àí7)          5.4(‚àí7)             40
          8 0.2          1.5(‚àí7)          1.5(‚àí6)             55
                1        1.8(‚àí7)          1.3(‚àí6)             58
                5        2.2(‚àí6)          3.1(‚àí6)             56
Note: a(k) means a √ó 10 .k




                                  14
compute errors for optimal labor supply. The last column of Table 2 lists
the running time of the iterative DPNLP method for various cases in the
GAMS environment, on a single core of a Mac laptop with a 2.5 GHz pro-
cessor.
    From Table 2, we can also see the similar pattern shown in Table 1. That
is, DPNLP solves the examples with accuracy up to 6 or higher digits for
optimal control policy functions in all cases. Moreover, for these stochastic
examples, DPNLP is also fast, and takes less than one minute to solve any
one case. For example, row one in Table 2 assumes Œ≤ = 0.9, Œ≥ = 0.5, and
Œ∑ = 0.2. For this case, the error in consumption is 1.9 √ó 10‚àí7 , the error in
labor supply is 5.2 √ó 10‚àí7 , and the running time is only 11 seconds.


7    Applications to Two-Dimensional Optimal Growth
     Problem
The key DPNLP idea is clearly applicable to multidimensional problems. Of
course, multidimensional problems are more demanding. Our next exam-
ple illustrates DPNLP applied to a two-dimensional extension of our earlier
models. The results indicate that DPNLP is a reasonable method for low-
dimensional problems.
    We assume that there are two countries, and let kt = (kt,1 , kt,2 ) denote
the capital stocks of two countries which is a two-dimensional continuous
state vector at time t. Let lt = (lt,1 , lt,2 ) denote elastic labor supply levels of
the countries which is a two-dimensional continuous control vector variable
at time t. Assume that the net production of country i at time t is
                                                    œà 1‚àíœà
                              fi (kt,i , lt,i ) = Akt,i lt,i ,

with A = (1 ‚àí Œ≤)/(œàŒ≤), for i = 1, 2. Let ct = (ct,1 , ct,2 ) denote consumption
of the countries which is another two-dimensional continuous control vector
variable at time t. The utility function is
                        2
                           "                                     #
                        X    (ci /A)1‚àíŒ≥ ‚àí 1            li1+Œ∑ ‚àí 1
              u(c, l) =                     ‚àí (1 ‚àí œà)              .
                                  1‚àíŒ≥                     1+Œ∑
                        i=1

We want to find an optimal consumption and labor supply decisions such
that expected total utility over the infinite-horizon time is maximized. That




                                            15
is,
                                       ‚àû
                                       X
      V (k0 ) =       max                    Œ≤ t u(ct , lt )                                                (13)
                    kt ,It ,ct ,lt
                                       t=0
                        s.t.         kt+1,i = (1 ‚àí Œ¥)kt,i + It,i ,
                                                          2
                                            Œ∂      It,i
                                     Œìt,i = kt,i        ‚àíŒ¥ ,
                                            2      kt,i
                                     2
                                     X                                2
                                                                      X
                                           (ct,i + It,i ‚àí Œ¥kt,i ) =         (fi (kt,i , lt,i ) ‚àí Œìt,i ) ,
                                     i=1                              i=1

where Œ¥ is the depreciation rate of capital, It,i is the investment of country
i, Œìt,i is the investment adjustment cost of country i, and Œ∂ governs the
intensity of the friction. Detailed discussion of multi-country growth models
with infinite horizon can be seen in Den Haan et al (2011) and Juillard and
Villemot (2011). For the multi-country growth models with finite horizon,
they can be solved efficiently using dynamic programming with Hermite
approximation, see Cai and Judd (2012c). In our examples, we let œà = 0.36,
Œ¥ = 0.025, and Œ∂ = 0.5.
    The functional forms for utility and production imply that the steady
state of the infinite horizon deterministic optimal growth problems is kss,1 =
kss,2 = 1, and the optimal consumption, labor supply and investment at
the steady state are respectively css,1 = css,2 = A, lss,1 = lss,2 = 1, and
Iss,1 = Iss,2 = Œ¥. The code for DPNLP is written in GAMS (McCarl, 2011),
and the optimization solver is CONOPT (in the GAMS environment).

7.1     True Solution
Discretization method will be too time-consuming to solve the two-country
optimal growth problem with two continuous state variables (kt,1 , kt,2 ) and
six continuous control variables (ct,1 , ct,2 , lt,1 , lt,2 , It,1 , It,2 ). In order to get the
‚Äútrue‚Äù solution, we use the value function iteration with high-degree complete
Chebyshev polynomials and iterates until it converges under the stopping
criterion 10‚àí7 (i.e., the difference between two consecutive value functions
is less than 10‚àí7 ). We use 512 tensor Chebyshev nodes on the state space
[0.5, 1.5]2 , and the degree of the complete Chebyshev polynomials is 30.

7.2     DPNLP Solution
We use the iterative DPNLP method (multidimensional version of Algorithm
1) to solve the two-dimensional optimal growth problem. The basic DPNLP




                                                       16
model is the multidimensional extension of the model (10). That is,
                        m
                        X
         max                   vi ,                                                                      (14)
      c,l,I,k+ ,v,b
                        i=1
          s.t.        vi ‚â§ u(ci , li ) + Œ≤ VÃÇ (ki+ ; b),      i = 1, . . . , m,
                       +
                      ki,j   ‚â§ (1 ‚àí Œ¥)ki,j + Ii,j , i = 1, . . . , m, j = 1, 2,
                                             2
                               Œ∂      Ii,j
                  Œìi,j       = ki,j        ‚àí Œ¥ , i = 1, . . . , m, j = 1, 2,
                               2      ki,j
                  2
                  X                                    2
                                                       X
                            (ci,j + Ii,j ‚àí Œ¥ki,j ) =         (fj (ki,j , li,j ) ‚àí Œìi,j ) ,   i = 1, . . . , m,
                  j=1                                  j=1

                      vi = VÃÇ (ki ; b),      i = 1, . . . , m,
                        0
                      VÃÇ (yi0 ; b) ‚â• 0,      i0 = 1, . . . , m0 ,
                      VÃÇ 00 (yi0 ; b) ‚â§ 0,    i0 = 1, . . . , m0 ,
where ki = (ki,1 , ki,2 ), ci = (ci,1 , ci,2 ), li = (li,1 , li,2 ), ki+ = (ki,1
                                                                             +      +
                                                                                 , ki,2 ), yi0 =
(yi0 ,1 , yi0 ,2 ), and VÃÇ 0 is the 2-dimensional gradient of VÃÇ , and VÃÇ 00 is the 2-
dimensional second order derivatives of VÃÇ .
      For our examples in this section, we choose m = 112 tensor Chebyshev
nodes in the state space [0.5, 1.5]2 , as the approximation nodes. The ap-
proximation method, VÃÇ , is the complete Chebyshev polynomial up to the
maximal degree 10. And we choose m0 = 100 tensor Chebyshev nodes, yi0 ,
in the state space [0.5, 1.5]2 , as the shape nodes.

7.3      Error Analysis of DPNLP Solution
We examine the errors for the multidimensional model in the similar manner
we did for the unidimensional optimal growth problems: We apply the high-
precision value function iteration to get the ‚Äútrue‚Äù optimal solution for every
test point of initial capitals, and then use them to check the accuracy of the
computed optimal solution from the DPNLP model (12).
    Table 3 lists relative errors of optimal solutions computed by DPNLP
for the two-dimensional optimal growth problem with the following cases:
Œ≤ = 0.95, Œ≥ = 0.5, 2, 8, and Œ∑ = 0.2, 1, 5. The last column of Table 3 lists the
running time of the iterative DPNLP method for various cases in the GAMS
environment, on a single core of a Mac laptop with a 2.5 GHz processor.
    Table 3 shows that DPNLP solves the examples with accuracy up to 4
digits or higher for optimal control policy functions in all the cases besides
one case having 3 digits. Moreover, the DPNLP method is not slow and
takes only several minutes for each case. For example, row one in Table 3
assumes Œ≥ = 0.5, and Œ∑ = 0.2. For this case, the error in consumption is
6.4 √ó 10‚àí5 , the error in labor supply is 1.5 √ó 10‚àí4 , and the running time is
3.9 minutes.

                                                       17
Table 3: Relative Errors of DPNLP for Two-Country Optimal Growth Prob-
lems
       Œ≥    Œ∑    Error of c‚àóDPNLP Error of lDPNLP
                                            ‚àó      Time (minutes)
      0.5 0.2        6.4(‚àí5)          1.5(‚àí4)           3.9
            1        9.0(‚àí6)          3.0(‚àí5)           2.5
            5        8.0(‚àí6)          8.0(‚àí7)           1.4
       2 0.2         6.2(‚àí5)          2.3(‚àí4)           3.6
            1        4.5(‚àí5)          6.5(‚àí5)           3.1
            5        8.5(‚àí5)          3.2(‚àí5)           2.2
       8 0.2         8.4(‚àí5)          1.2(‚àí3)           5.0
            1        2.1(‚àí5)          1.1(‚àí4)           6.6
            5        1.3(‚àí4)          2.0(‚àí4)           5.6
Note: a(k) means a √ó 10k .


8    Conclusion
This paper presents a nonlinear programming formulation of dynamic pro-
gramming problems common in economic decision making. We have applied
it to a variety of optimal accumulation problems, showing that our DPNLP
method performs very well, with high accuracy, reliability and efficiency for
those problems. The variety of example problems indicate that our DPNLP
method could be applied to many problems.




                                     18
References
      [1] Bellman, R. (1957). Dynamic Programming. Princeton Univer-
          sity Press.
      [2] Cai, Y. (2010). Dynamic Programming and Its Application in
          Economics and Finance. PhD thesis, Stanford University.
      [3] Cai, Y., and K.L. Judd (2010). Stable and efficient computa-
          tional methods for dynamic programming. Journal of the Eu-
          ropean Economic Association, Vol. 8, No. 2-3, 626‚Äì634.
      [4] Cai, Y., and K.L. Judd (2012a). Dynamic programming with
          shape-preserving rational spline Hermite interpolation. Eco-
          nomics Letters, Vol. 117, No. 1, 161‚Äì164.
      [5] Cai, Y., and K.L. Judd (2012b). Shape-preserving dynamic
          programming. Mathematical Methods of Operations Research,
          DOI: 10.1007/s00186-012-0406-5.
      [6] Cai, Y., and K.L. Judd (2012c). Dynamic programming with
          Hermite approximation. NBER working paper No. w18540.
      [7] De Farias, D.P., and B. Van Roy (2003). The linear program-
          ming approach to approximate dynamic programming. Opera-
          tions Research, 51(6), 850‚Äì865.
      [8] Den Haan, W.J., K.L. Judd and M. Juillard (2011). Compu-
          tational suite of models with heterogeneous agents II: Multi-
          country real business cycle models. Journal of Economic Dy-
          namics & Control, 35, 175‚Äì177.
      [9] Judd, K.L. (1998). Numerical Methods in Economics. The MIT
          Press.
     [10] Juillard, M., and S. Villemot (2011). Multi-country real busi-
          ness cycle models: Accuracy tests and test bench. Journal of
          Economic Dynamics & Control, 35, 178‚Äì185.
     [11] McCarl, B., et al. (2011). McCarl GAMS User Guide. GAMS
          Development Corporation.
     [12] Michelangeli, V. (2009). Economics of the Life-Cycle: Reverse
          Mortgage, Mortgage and Marriage. PhD thesis, Boston Univer-
          sity.
     [13] Rust, J. (2008). Dynamic Programming. In: Durlauf, S.N.,
          Blume L.E. (Eds.), New Palgrave Dictionary of Economics. Pal-
          grave Macmillan, second edition.

                                 19
[14] Stokey, N.L., R.E. Lucas, and E.C. Prescott (1989). Recursive
     Methods in Economic Dynamics. Harvard University Press.

[15] Su, C.H, and K.L. Judd (2012). Constrained Optimization Ap-
     proaches to Estimation of Structural Models. Econometrica,
     Vol. 80 (5), 2213‚Äì2230.

[16] Trick, M.A., and S.E. Zin (1997). Spline approximations to
     value functions ‚Äî linear programming approach. Macroeco-
     nomic Dynamics, 1, 255‚Äì277.




                           20
