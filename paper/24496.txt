                              NBER WORKING PAPER SERIES




                    INSTRUMENT-BASED VS. TARGET-BASED RULES

                                          Marina Halac
                                          Pierre Yared

                                       Working Paper 24496
                               http://www.nber.org/papers/w24496


                     NATIONAL BUREAU OF ECONOMIC RESEARCH
                              1050 Massachusetts Avenue
                                  Cambridge, MA 02138
                           April 2018, Revised December 2018




We would like to thank Hassan Afrouzi, John Geanakoplos, Johannes Hörner, Navin Kartik,
Narayana Kocherlakota, Jen Jen La'O, George Mailath, Rick Mishkin, Ed Nelson, Larry
Samuelson, and Carl Walsh for comments. Jan Knoepfle provided excellent research assistance.
The views expressed herein are those of the authors and do not necessarily reflect the views of the
National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.

© 2018 by Marina Halac and Pierre Yared. All rights reserved. Short sections of text, not to
exceed two paragraphs, may be quoted without explicit permission provided that full credit,
including © notice, is given to the source.
Instrument-Based vs. Target-Based Rules
Marina Halac and Pierre Yared
NBER Working Paper No. 24496
April 2018, Revised December 2018
JEL No. D02,D82,E58,E61

                                          ABSTRACT

We develop a simple model to study rules based on instruments vs. targets. A principal faces a
better informed but biased agent and relies on joint punishments as incentives. Instrument-based
rules condition incentives on the agent's observable action; target-based rules condition incentives
on outcomes that depend on the agent's action and private information. In each class, an optimal
rule takes a threshold form and imposes the worst punishment upon violation. Target-based rules
dominate instrument-based rules if and only if the agent's information is sufficiently precise. An
optimal unconstrained rule relaxes the instrument threshold whenever the target threshold is
satisfied.


Marina Halac
Columbia University
616 Uris Hall, 3022 Broadway
New York, NY 10027
marina.halac@yale.edu

Pierre Yared
Columbia University
Graduate School of Business
3022 Broadway, Uris Hall 823
New York, NY 10027
and NBER
pyared@columbia.edu
1       Introduction
The question of whether to base incentives on agents’ actions or the outcomes of these
actions arises in various contexts. Perhaps most prominently, scholars and policymakers
have long debated on the merits of using instruments vs. targets for monetary policy. The
US House of Representatives and several notable economists support the use of a Taylor
(1993) rule that guides the interest rate choice of the central bank,1 whereas numerous
central banks such as the Bank of England and the Bank of Canada rely on inflation
targeting rules that are based on outcomes.2 Fiscal policy rules also vary in this dimension,
with some US states constraining instruments like tax rates and spending and others using
rules contingent on targets like deficits.3 More recently, these considerations have received
attention in the design of environmental policies. Environmental regulation may focus
on technology mandates—requirements on firms’ production processes, such as the choice
of equipment—or on performance standards—requirements on output, such as maximum
emission rates.4,5
    In this paper, we develop a stylized model to study and compare instrument-based and
target-based rules. Using mechanism design, we present a simple theory that elucidates the
benefits of each class of rule and shows which class will be preferred as a function of the
environment. Additionally, we characterize the optimal unconstrained or hybrid rule and
examine how combining instruments and targets can improve welfare.
    Our model builds on a canonical delegation framework. A principal delegates decision-
making to an agent who is biased towards higher actions. The agent’s action is observable,
but the agent has private information about its value, with a higher agent type correspond-
ing to a higher expected marginal benefit of the action for both the principal and the agent.
We extend this delegation setting by introducing an observable noisy outcome that is a
function of the agent’s action and his private information. For example, the agent may
be a policymaker who is biased towards expansionary monetary policy relative to society,
and the outcome is inflation which depends on the choice of policy and the realization of
    1
     The Financial Choice Act of 2017 passed by the US House of Representatives requires the Federal
Reserve to report on a rule to Congress. For discussions for and against Taylor rules, see Svensson
(2003), Bernanke (2004, 2015), Appelbaum (2014), Blinder (2014), Taylor (2014), Kocherlakota (2016),
and the Statement on Policy Rules Legislation signed by a number of economists, available at http://web.
stanford.edu/~johntayl/2016_pdfs/Statement_on_Policy_Rules_Legislation_2-29-2016.pdf.
   2
     See Bernanke and Mishkin (1997), Bernanke et al. (1999), Mishkin (1999, 2017), and Svensson (2003)
for discussions of inflation targeting regimes.
   3
     See National Conference of State Legislatures (1999).
   4
     See for example US Congress, Office of Technology Assessment (1995) and Goulder and Parry (2008).
   5
     These issues are also relevant to organizations, where non-monetary incentives like promotions or firing
may be based on both workers’ decisions and their performance.


                                                     1
economic shocks about which the agent has ex-ante private information. Due to his bias,
the agent’s preferred outcome exceeds that of the principal.6
    As is standard in delegation settings, transfers between the parties are infeasible, but
the principal can make use of joint punishments as incentives. That is, the principal can
engage in “money burning” by taking measures that mutually harm the principal and the
agent, like imposing sanctions or firing the agent. We distinguish between different classes
of rules depending on how punishments are structured: we say that the principal’s rule is
instrument-based if punishments depend only on the agent’s action, and the rule is target-
based if punishments depend only on the realized outcome. In the context of monetary
policy, an instrument-based rule conditions punishments on the choice of policy, whereas
a target-based rule conditions punishments on realized inflation.
    Under either class of rule, the principal can tailor punishments appropriately so as
to incentivize the agent to choose the principal’s preferred action, thus eliminating any
distortions in decision-making arising from the agent’s bias. However, because punishments
are costly to the principal, those incentives would not be optimal. An optimal rule for
the principal minimizes distortions due to the agent’s bias along with the cost of joint
punishments. Moreover, as a consequence, this rule induces an action by the agent and a
value of welfare for the principal which depend on the class of the rule.7
    Our analysis begins by showing that, within each class, an optimal rule takes a threshold
form, with violation of the threshold leading to the worst punishment. In the case of an
optimal instrument-based rule, the principal allows the agent to choose any action up to a
threshold and maximally punishes him for exceeding it. The logic is analogous to that in
other delegation models (reviewed below); since the agent prefers higher actions than the
principal, this punishment structure is optimal to deter the agent from taking actions that
are excessively high. In the case of an optimal target-based rule, the principal specifies
a threshold for the outcome, maximally punishing the agent if the realized outcome is
above it. This punishment structure also incentivizes the agent to not choose excessively
high actions, since these actions result in higher outcomes in expectation. High-powered
incentives of this form arise in moral hazard settings with hidden action.8
    Our main result uses this characterization of the optimal rules for each class to compare
their performance. We show that target-based rules dominate instrument-based rules if and
only if the agent’s private information is sufficiently precise. To illustrate, suppose that
   6
     Our analysis is unchanged if the agent instead prefers lower outcomes than the principal.
   7
     Instead, if punishments were not joint but costly only to the agent, then both rule classes would yield
the same solution, as distortions due to the agent’s bias could be eliminated at no cost.
   8
     See for example Innes (1990) and Levin (2003). Here these incentives arise because punishments cannot
depend directly on the agent’s action under a target-based rule.

                                                     2
the agent’s information is perfect. Then the principal guarantees her preferred action
by providing steep incentives under a target-based rule, where punishments do not occur
on path because the perfectly informed agent chooses the action that delivers the target
outcome. This target-based rule strictly dominates any instrument-based rule, as the
latter cannot incentivize the agent while giving him enough flexibility to respond to his
information. At the other extreme, suppose that the agent has no private information.
Then the principal guarantees her ex-ante preferred action with an instrument-based rule
that ties the hands of the agent, namely that punishes the agent if any higher action
is chosen. This instrument-based rule strictly dominates any target-based rule, as the
latter gives the agent unnecessary discretion and requires on-path punishments to provide
incentives. Our main result shows that this logic applies more generally as we locally vary
the precision of the agent’s private information, away from the extremes of perfect and no
information.
    We additionally show that the benefit of using a target-based rule over an instrument-
based rule is decreasing in the bias of the agent and increasing in the severity of punishment.
Intuitively, the less biased is the agent, the less costly is incentive provision under a target-
based rule, as the principal can deter the agent from choosing high actions with less frequent
punishments. Similarly, the harsher is the punishment imposed on the agent for exceeding
the target threshold, the less often the principal needs to exercise punishment on path
to implement a target outcome. These two forces therefore make target-based rules more
appealing than instrument-based rules on the margin.
    A natural question is how the principal can combine instruments and targets to improve
upon the above rules that rely exclusively on one of these tools. We study the optimal
hybrid rule, that is the optimal unconstrained rule in which punishments can depend freely
on the agent’s action and the realized outcome.9 We show that this rule admits a simple
implementation: the principal sets an instrument threshold which is relaxed whenever a
target threshold is satisfied. The optimal hybrid rule dominates instrument-based rules by
allowing the agent more flexibility to choose high actions under a target-based criterion,
and it dominates target-based rules by more efficiently limiting the agent’s discretion with
direct punishments. An example of an optimal hybrid rule in the context of monetary
policy would be a Taylor rule which, whenever violated, switches to an inflation target.
Notably, some policymakers and economists advocated such an approach in the US in
the aftermath of the Global Financial Crisis, when the Federal Reserve’s policy deviated
   9
   This rule yields the principal the highest welfare that she can achieve given the agent’s private infor-
mation.



                                                    3
significantly from the Taylor rule but realized inflation remained near the target.10
    This paper is related to several literatures. First, the paper fits into the mechanism
design literature that studies the tradeoff between commitment and flexibility in poli-
cymaking, including Athey, Atkeson, and Kehoe (2005) and Kocherlakota (2016) in the
context of monetary policy, and Amador, Werning, and Angeletos (2006) and Halac and
Yared (2014, 2018a,b) in the context of fiscal policy. Second, the paper contributes to an
extensive literature on delegation in principal-agent settings, which builds on the seminal
work of Holmström (1977, 1984).11 We extend the theoretical frameworks in both of these
literatures by introducing an observable outcome that partially reflects the agent’s infor-
mation. Our contribution is to compare incentives based on the agent’s action (as in these
literatures) with incentives based on the observable outcome, and to characterize optimal
incentives which can condition on both variables.
    Third, this paper relates to other theoretical literatures on optimal policy design, includ-
ing in the context of monetary policy where instruments and targets have been analyzed;
see, e.g., Barro and Gordon (1983), Rogoff (1985), McCallum and Nelson (2005), Svensson
(2005, 2010), and Giannoni and Woodford (2017). We contribute to this work by charac-
terizing rules as optimal mechanisms in a private information setting and by contrasting
incentive provision under each class of rule. It is worth noting that models of monetary
policy are concerned with additional questions, most importantly the role of inflationary
expectations.12 We address this issue in an extension in Subsection 5.3. We show that the
magnitude of the agent’s bias, and hence the optimal class of rule, depends on the cost of
inflation in a Barro and Gordon (1983) setting.
    Finally, there are other papers that relate to our work in that they consider the use
of joint punishments as incentives like we do, albeit in different environments. See for
example Acemoglu and Wolitzky (2011) and Padró i Miquel and Yared (2012).13
  10
     See for example Yellen (2015, 2017) and the discussion in Walsh (2015).
  11
     See Alonso and Matouschek (2008), Amador and Bagwell (2013), and Ambrus and Egorov (2017),
among others.
  12
     Frankel and Kartik (2017) examine how inflationary expectations and thus equilibrium outcomes
depend on not only the amount but also the kind of information that a central bank has.
  13
     Although the settings and analyses differ in many aspects, our paper also relates to some models of
career concerns for experts which study how the information a principal has affects reputational incentives.
In particular, Prat (2005) distinguishes between information on actions and on outcomes.




                                                     4
2      Model
We consider a stylized model with a principal and an agent. The agent observes a signal
s ∈ {sL , sH }, which is the agent’s private information or type, and chooses an action µ ∈ R.
Given this action choice, an outcome π = µ − θ is realized, where θ ∈ R is a shock. A
possible interpretation is that the action µ is a policy instrument, such as the level of
monetary policy expansion; the shock θ is a stochastic macroeconomic fundamental, such
as the level of economic slack; and the outcome π is a payoff-relevant outcome, such as the
level of inflation.
    The agent’s signal is informative about the shock. Specifically, we assume that the
conditional distribution of the shock is normal with mean equal to the signal, i.e., θ|si ∼
N (si , σ 2 ) for i = L, H. The precision of the agent’s information is given by σ −1 > 0. We
take sL = −∆ and sH = ∆ for some ∆ > 0 and assume that each signal occurs with
equal probability. The shock’s unconditional distribution is thus a mixture of two normal
distributions and has mean and variance given by

                                E(θ) = 0 and V ar(θ) = σ 2 + ∆2 .

    The principal observes the agent’s action µ and the realized shock θ (or, equivalently,
the agent’s action µ and the realized outcome π). She cannot however deduce the agent’s
private information si from these observations, as the distribution of θ has full support
over the entire real line for each signal si .
    As is standard in settings of delegation, transfers between the principal and the agent
are not feasible. Instead, as a function of the action µ and the shock θ, the principal can
commit to a continuation value for the principal and the agent, given by V (µ, θ) ∈ [V , V ]
for some finite V and V . This continuation value represents rewards and punishments,
including formal penalties due to sanction regimes or replacement of the agent, as well as
informal penalties in the form of suboptimal continuation play in a dynamic environment
(see Halac and Yared, 2018b).14
    Denote by φ(z|z, σ 2z ) the normal density of a variable z with mean z and variance σ 2z ,
and by Φ(z|z, σ 2z ) the corresponding normal cumulative distribution function. The agent’s
  14
    Our assumption that the signal s is privately observed by the agent can equivalently be interpreted
as a restriction on the rewards and punishments represented by V (µ, θ), which cannot explicitly condition
on the signal.




                                                    5
expected welfare conditional on information si and action µi , for i = L, H, is
                                "                 2
                                                                #
                            ∞
                                      (µi − θ − α)
                        Z
                                    −               + V (µi , θ) φ(θ|si , σ 2 )dθ,                 (1)
                          −∞                2

where α > 0. The principal’s expected welfare is
                                  "           2
                                                            #
                        X 1Z ∞       (µi − θ)
                                   −            + V (µi , θ) φ(θ|si , σ 2 )dθ.                     (2)
                       i=L,H
                             2 −∞        2

     The principal and the agent receive utility as a function of the observable outcome
π = µ − θ which is concave and single-peaked. For both parties, the preferred action µ that
maximizes this utility is increasing in the shock θ. However, as captured by the parameter
α, the agent is biased relative to the principal. For each signal si , the agent’s preferred
action, or flexible action, is equal to si + α (this follows from (1) since E(θ|si , σ 2 ) = si ).
This level exceeds the principal’s preferred action, or first-best action, which is equal to
si . Therefore, conditional on the signal, the agent always prefers a higher action than the
principal. We assume:
Assumption 1. α ≥ 2∆.
    Assumption 1 is analogous to an assumption in Halac and Yared (2014), and its role is
to take agent types which are relatively “close” to each other, i.e. with ∆ relatively small.15
The implication of this assumption is that the agent’s flexible action si + α exceeds the
first-best action under each signal:

                                       sL < sH ≤ sL + α < sH + α.                                  (3)

   As shown in (1)-(2), the principal and the agent can be jointly rewarded with a high
                                      
continuation value V (µ, θ) ∈ V , V or jointly punished with a low such value. This
continuation value being common to the two parties captures, in a stark way, the fact that
incentivizing the agent is costly for the principal.16 We will give the principal a sufficient
breadth of incentives to use in her relationship with the agent; specifically, our analysis in
the next sections assumes:
                                α2
Assumption 2. V − V ≥                  .
                            2φ(1|0, 1)
  15
     As we show in Subsection 5.1, our results therefore do not rely on a discrete distance between the
types.
  16
     In Subsection 5.2, we consider an extension in which punishments harm the agent and the principal
asymmetrically.

                                                      6
    We distinguish between different classes of rules according to how the principal struc-
tures incentives. We say that a rule is instrument-based if the principal commits to a
continuation value V (µ, θ) which depends only on the action µ. A rule instead is target-
based if the continuation value V (µ, θ) depends only on the realized outcome π = µ − θ.
Finally, if the continuation value V (µ, θ) depends freely on µ and θ (and therefore freely
on µ and π), we say that the rule is hybrid.
    We are interested in comparing the performance of these different classes of rules as
the environment changes. Our analysis will consider varying the precision of the agent’s
private information while holding fixed the mean and variance of the shock θ. At one
                             p
extreme, we can take σ → V ar(θ) and ∆ → 0, so the agent is uninformed with signal
                                                                  p
sL = sH = 0. At the other extreme, we can take σ → 0 and ∆ → V ar(θ), so the agent is
perfectly informed with signal si = θ.17 Note that since Assumption 1 holds for all feasible
                                                                    p
σ > 0 and ∆ > 0 given V ar(θ) fixed, the assumption implies α ≥ 2 V ar(θ).


3          Instrument-Based and Target-Based Rules
We examine rules based on instruments versus targets. Subsection 3.1 and Subsection 3.2
solve for the optimal rule within each class. Subsection 3.3 offers a comparison and shows
that the optimal class of rule for the principal depends on the precision of the agent’s
private information.


3.1        Optimal Instrument-Based Rule
An instrument-based rule specifies an action µi for each agent type i = L, H and a contin-
uation value V (µ, θ) as a function of the action µ only. Let V i ≡ V (µi ) for i = L, H. The
allocation {µL , µH , V L , V H } must satisfy private information, enforcement, and feasibility
constraints, as we describe next.
    The private information constraint captures the fact that the agent can misrepresent
his type. The rule must be such that, for i = L, H, an agent of type i has no incentive to
deviate privately to action µ−i :
           "                  2
                                     #              Z ∞"                2
                                                                                #
       ∞
                 (µi − θ − α)                              (µ−i − θ − α)
 Z
                                   i     i   2                               −i
               −                + V φ(θ|s , σ )dθ ≥      −                +V      φ(θ|si , σ 2 )dθ.
    −∞                 2                             −∞          2
                                                                                                 (4)
  17
    Note that in our setting, welfare under either rule class depends only on the mean and variance of θ.
This avoids additional complications stemming from the fact that higher moments of the distribution of θ
vary with σ and ∆.

                                                     7
    The enforcement constraint captures the fact that the agent can freely choose any
action µ ∈ R, including actions not assigned to either type. The rule must be such
that, for i = L, H, an agent of type i has no incentive to deviate publicly to any action
µ∈/ {µL , µH }:
         "                 2
                                  #                   Z ∞"                       #
     ∞                                                                  2
               (µi − θ − α)                                     −   −
 Z
                                                             (µ   θ   α)
             −               + V i φ(θ|si , σ 2 )dθ ≥      −              + V (µ) φ(θ|si , σ 2 )dθ.
  −∞                 2                                 −∞         2

Note that since the continuation value satisfies V (µ) ≥ V for all µ ∈ R, the above inequality
must hold under maximal punishment, i.e. when V (µ) = V . Moreover, since the inequality
must then hold for all µ ∈ R, it must necessarily hold when µ corresponds to type i’s
flexible action si + α. A necessary condition for the enforcement constraint to be satisfied
is thus
     Z ∞"       i        2
                                #                   Z ∞"     i     2
                                                                         #
             (µ − θ − α)                                   (s − θ)
           −               + V i φ(θ|si , σ 2 )dθ ≥      −           + V φ(θ|si , σ 2 )dθ  (5)
      −∞            2                                −∞        2

for i = L, H, where note that the right-hand side is the agent’s minmax payoff.
    Constraints (4) and (5) are clearly necessary for an instrument-based rule prescribing
{µ , µH , V L , V H } to be incentive compatible. Furthermore, if these constraints are satis-
   L

fied, then this allocation can be supported by specifying the worst continuation value V
following any choice µ ∈   / {µL , µH }. Since such a choice is off path, it is without loss to
assume that it is maximally punished.
    Lastly, feasibility requires that for i = L, H,

                                                    V i ∈ [V , V ].                              (6)

    An optimal instrument-based rule maximizes the principal’s expected welfare subject
to the private information, enforcement, and feasibility constraints:
                                                     "            2
                                                                         #
                                           X 1Z ∞        (µi − θ)
                           max                         −            + V i φ(θ|si , σ 2 )dθ       (7)
                       µL ,µH ,V L ,V H
                                          i=L,H
                                                2 −∞         2

                                 subject to, for i = L, H, (4), (5), and (6).

   Define a maximally-enforced instrument threshold µ∗ as a rule that prescribes the max-
imal reward V if the agent’s action is weakly below a threshold µ∗ and the maximal
punishment V if the action exceeds this threshold. We find:

                                                          8
Proposition 1. The optimal instrument-based rule specifies µL = µH = 0 and V L = V H =
V . This rule can be implemented with a maximally-enforced instrument threshold µ∗ = 0.

    The optimal instrument-based rule assigns both agent types the action that maximizes
the principal’s ex-ante welfare. The agent is given no discretion, and punishments occur
only off path, if the agent were to publicly deviate to a different action.18 Note that a
rule that induces the first-best action, µi = si for i = L, H, is available to the principal:
the low type can be dissuaded from choosing sH by specifying an on-path punishment
V H < V (and both types can be dissuaded from choosing any action µ ∈           / {sL , sH } by
specifying the worst punishment off path). However, because punishment is costly to the
principal, Proposition 1 shows that such a rule is strictly dominated by a maximally-
enforced instrument threshold µ∗ = 0.
    To prove Proposition 1, we solve a relaxed version of the program in (7) which ignores
the private information constraint (4) for the high type and the enforcement constraints (5)
for both types. We show that under Assumption 1, the solution to this relaxed problem
entails no discretion, and it thus satisfies (4) for both types. Moreover, Assumption 2
guarantees that (5) is also satisfied for both types.
    Proposition 1 is in line with the findings of an extensive literature on delegation, which
provides conditions under which threshold delegation with no money burning is optimal.
A general treatment can be found in Amador and Bagwell (2013). The analysis in Halac
and Yared (2018b) is also related in that it considers enforcement constraints like those
in (5) and shows the optimality of maximally-enforced thresholds, where on- and off-path
violations lead to the worst punishment. In the current setting with binary signals, enforce-
ment constraints are non-binding by Assumption 2, so punishments occur only off path.
Halac and Yared (2018b) study the issues that arise when the analog of this assumption is
relaxed in their context. We also address these issues in Subsection 5.1, where we extend
our analysis to a continuum of agent types.


3.2     Optimal Target-Based Rule
A target-based rule specifies an action µi for each agent type i = L, H and a continuation
value V (µ, θ) as a function of the outcome π = µ − θ only. We denote such a continuation
value by V (π), where note that V (π) is defined for π ∈ R since θ is normally distributed.
The allocation {µL , µH , {V (π)}π∈R } must satisfy incentive compatibility and feasibility
constraints, as we describe next.
  18
    In our extension to a continuum of agent types in Subsection 5.1, the optimal instrument-based rule
does provide some discretion to the agent, and it also involves punishments on path.

                                                  9
   Incentive compatibility requires that the prescribed action for each agent type solve this
type’s welfare-maximization problem. Given his private information and the continuation
value function specified by the principal, the agent takes into account how his action affects
the distribution of outcomes and, thus, continuation values. For i = L, H, µi must satisfy:
                                  (Z        "                             #               )
                                       ∞
                                                  (µ − θ − α)2
             µi ∈ arg max                                      + V (µ − θ) φ θ|si , σ 2 dθ .
                                                                                       
                                                −                                                         (8)
                          µ         −∞                 2

   Additionally, feasibility requires
                                                    
                                        V (π) ∈ V , V for all π ∈ R.                                      (9)

   An optimal target-based rule maximizes the principal’s expected welfare subject to the
incentive compatibility and feasibility constraints:
                                             "           2
                                                                       #
                                   X 1Z ∞        (µi − θ)
                                                           + V (µi − θ) φ θ|si , σ 2 dθ
                                                                                    
                  max                          −                                                         (10)
            µL ,µH ,{V (π)}π∈R
                                  i=L,H
                                        2 −∞         2

                                  subject to, for i = L, H, (8) and (9).

   Note that integration by substitution yields
              Z   ∞                                              Z    ∞
                                             i    2
                                                                          V (π)φ µ − si − π|0, σ 2 dπ,
                                                                                                 
                      V (µ − θ)φ θ|s , σ                  dθ =                                           (11)
                −∞                                                −∞


where we have used the fact that φ (θ|s, σ 2 ) = φ (θ − s|0, σ 2 ) since φ (·) is the density of a
normal distribution. Using (11) to substitute in (8), the first-order condition of the agent’s
problem is
                                   Z   ∞
                      i   i
                                            V (π)φ0 µi − si − π|0, σ 2 dπ = 0 for i = L, H.
                                                                     
            α− µ −s +                                                                                    (12)
                                       −∞


    Condition (12) is necessary for the rule to be incentive compatible. Its solution is
µi = si + κ for i = L, H and some κ R 0, where κ is independent of the agent’s type i.
The latter observation allows us to simplify the principal’s problem as her welfare then
also becomes independent of the agent’s type i.
    Define a maximally-enforced target threshold π ∗ as a rule that prescribes the maximal
reward V if the outcome is weakly below a threshold π ∗ and the maximal punishment V
if the outcome exceeds this threshold. We find:


                                                                 10
Proposition 2. The optimal target-based rule specifies µi = si + κ, V (π) = V if π ≤ π ∗ ,
                                                                          2
and V (π) = V if π > π ∗ , for i = L, H, some κ ∈ (0, α), and π ∗ = κ + σκ . This rule can be
implemented with a maximally-enforced target threshold π ∗ .
    The optimal target-based rule provides incentives with a maximally-enforced target
threshold π ∗ . Since a higher action µ results in a higher outcome π in expectation, an
agent of type i responds to this threshold by choosing an action si + κ which is below his
flexible action si + α. In contrast to the optimal instrument-based rule, here punishment
occurs along the equilibrium path whenever π > π ∗ , so as to appropriately incentivize the
agent. Note that since punishment is costly, the principal limits its frequency by keeping the
agent’s action above the first-best level. That is, while a rule that induces the principal’s
preferred action with κ = 0 is available to the principal, Proposition 2 shows that this rule
is strictly dominated by one that allows distortions with κ > 0. The proposition also shows
that the induced expected outcome is below the threshold, i.e. E (π) = κ < π ∗ . A rule that
yields E(π) = κ = π ∗ would be suboptimal, as it would entail punishing the agent half of
the time (the frequency with which π would exceed π ∗ ). In the optimal rule, the realized
outcome π exceeds π ∗ less than half of the time so that punishment occurs less often.
    To prove Proposition 2, we follow a first-order approach and solve a relaxed version
of the program in (10) that replaces the incentive compatibility constraint (8) with the
first-order condition (12) of the agent’s problem. Specifically, we consider a doubly-relaxed
problem that takes (12) as a weak inequality constraint (cf. Rogerson, 1985) in order to
establish the sign of the Lagrange multiplier on (12) and characterize the solution. We
prove that the solution to the relaxed problem takes the threshold form described above,
and we show that Assumption 1 and Assumption 2 are sufficient to guarantee the validity
of this first-order approach.
    High-powered incentives of the form described in Proposition 2 arise in moral hazard
settings where, as in our model, rewards and punishments are bounded and enter welfare
linearly; see for example Innes (1990) and Levin (2003). These incentives arise here because
rewards and punishments cannot directly depend on the agent’s action under a target-based
rule. As we discuss in Subsection 5.1, Proposition 2 remains valid under a continuum of
agent types: since the agent’s first-order condition implies that the principal’s welfare is
independent of the agent’s type i, this welfare is also independent of the number of types.


3.3    Optimal Class of Rule
Our main result uses the characterizations in Proposition 1 and Proposition 2 to compare
the performance of instrument-based and target-based rules. We find that which class of

                                             11
rule is optimal for the principal depends on the precision of the agent’s private information:

Proposition 3. Take instrument-based and target-based rules and consider changing σ
while keeping V ar(θ) unchanged. There exists σ ∗ > 0 such that a target-based rule is
strictly optimal if σ < σ ∗ and an instrument-based rule is strictly optimal if σ > σ ∗ . The
cutoff σ ∗ is decreasing in the agent’s bias α and the worst continuation value V .

   To see the logic, consider how the principal’s welfare under each class of rule changes
as we vary the precision of the agent’s information σ −1 , while keeping the shock variance
V ar(θ) unchanged. Since the optimal instrument-based rule gives no flexibility to the
agent to use his private information, the principal’s welfare under this rule is invariant to
σ. Specifically, by Proposition 1 and V ar(θ) = E(θ2 ) (since E(θ) = 0), the principal’s
welfare under the optimal instrument-based rule is given by

                                           V ar(θ)
                                       −           +V,
                                              2

independent of σ. In contrast, using Proposition 2, we can verify that the principal’s welfare
under the optimal target-based rule is decreasing in σ, that is increasing in the precision
of the agent’s information. Intuitively, a better informed agent can more closely tailor his
action to the shock, and is less likely to trigger punishment by overshooting the threshold
specified by the principal. As a result, a higher precision reduces the outcome volatility
and the principal’s cost of providing high-powered incentives under a target-based rule.
    These comparative statics imply that to prove the first part of Proposition 3, it suffices
to show that a target-based rule is optimal for high enough precision of the agent’s infor-
mation whereas an instrument-based rule is optimal otherwise. Consider the extreme in
                                                                 p
which the agent is perfectly informed, that is, σ → 0 and ∆ → V ar(θ). In this case, the
optimal target-based rule sets a threshold π ∗ = 0, providing steep incentives and inducing
the first-best action. Note that this rule involves no punishments along the equilibrium
path, as a perfectly informed agent of type i = L, H chooses µi = si to avoid punishment.
Consequently, in this limit case, the optimal target-based rule yields welfare

                                             V ar(θ)
                                    V >−             +V,
                                                2

and thus it dominates the optimal instrument-based rule.
                                                                             p
   Consider next the extreme in which the agent is uninformed, that is, σ → V ar(θ)
and ∆ → 0. In this case, the optimal instrument-based rule guarantees the principal her
preferred outcome given no information by tying the hands of the agent. Instead, the

                                              12
principal cannot implement her ex-ante optimum with a target-based rule, which gives the
agent unnecessary discretion and requires punishments to provide incentives. The optimal
target-based rule in this limit case sets a threshold π ∗ > 0, inducing an agent of type
i = L, H to choose µi = si + κ for κ > 0 and yielding welfare

                V ar(θ)      κ2                               V ar(θ)
                                − Φ κ − π ∗ |0, σ 2 V − V < −
                                                        
              −         +V −                                          +V.
                   2         2                                   2

Thus, this rule is dominated by the optimal instrument-based rule.
    The second part of Proposition 3 shows that the benefit of using a target-based rule
over an instrument-based rule is decreasing in the bias of the agent and increasing in the
severity of punishment. The less biased is the agent, the less costly is incentive provision
under a target-based rule, as relatively infrequent punishments become sufficient to deter
high actions. Similarly, the harsher is the punishment experienced by the agent for missing
the target threshold, the less often punishment needs to be used on the equilibrium path
to provide incentives under a target-based rule. In contrast, the optimal instrument-based
rule is independent of the agent’s bias and the severity of punishment. As such, target-
based rules dominate instrument-based rules for a larger range of parameters when the
agent’s bias is relatively low or punishment is relatively severe.19
    In addition to the results in Proposition 3, the comparison of instrument-based and
target-based rules reveals differences with regards to volatility. Specifically, the two rule
classes differ in how they trade off different kinds of volatility: the optimal instrument-
based rule minimizes the variance of the agent’s action, whereas the optimal target-based
rule achieves a lower variance of the outcome. We will return to this distinction in Sub-
section 5.3, where we extend our model to consider the role of expectations.


4     Hybrid Rules
A hybrid rule combines features of instrument-based and target-based rules, with a con-
tinuation value V (µ, θ) that depends freely on µ and θ. For i = L, H, denote by V i (θ) the
continuation value assigned to agent type i as a function of the shock θ. Analogous to the
  19
     One may also wonder how the comparison of the two rule classes changes if the unconditional variance
of the shock increases. The answer is ambiguous in that it depends on the extent to which the increase in
V ar(θ) is due to an increase in σ or ∆: if only σ increases, instrument-based rules become more beneficial
over target-based rules (as implied by Proposition 3), whereas if only ∆ increases, the opposite is true.




                                                    13
program in (7), an optimal hybrid rule solves:
                                                          "            2
                                                                                  #
                                                X 1Z ∞        (µi − θ)
                          max                               −            + V i (θ) φ(θ|si , σ 2 )dθ   (13)
                µL ,µH ,{V L (θ),V H (θ)}θ∈R
                                               i=L,H
                                                     2 −∞         2

                                 subject to, for i = L, H,
Z ∞"                2
                               #                   Z ∞"     −i         2
                                                                                   #
       (µi − θ − α)                                       (µ   − θ − α)
     −                + V i (θ) φ(θ|si , σ 2 )dθ ≥      −                + V −i (θ) φ(θ|si , σ 2 )dθ,
 −∞          2                                      −∞           2
                                                                                                      (14)
         "                 2
                                      #                   Z ∞"            2
                                                                               #
     ∞
               (µi − θ − α)                                        i
                                                                     −
 Z
                                                                (s     θ)
             −               + V i (θ) φ(θ|si , σ 2 )dθ ≥     −             + V φ(θ|si , σ 2 )dθ, (15)
  −∞                 2                                     −∞        2

                                               V i (θ) ∈ [V , V ] for all θ ∈ R.                      (16)

     The solution to this program gives the principal the highest welfare that she can achieve
given the private information of the agent. Note that constraints (14)-(15) are analogous
to (4)-(5) in the program that solves for the optimal instrument-based rule, but they now
allow the continuation value to depend on the shock θ in addition to the agent’s action µi .
Note also that by analogous arguments as those used to solve for the optimal target-based
rule, the continuation value V i (θ) can be equivalently written as a function of the outcome,
V i (π). We use this formulation in what follows to ease the interpretation.
     Define a maximally-enforced hybrid threshold {µ∗ , µ∗∗ , {π ∗ (µ)}µ∈R } as a rule that spec-
ifies the maximal reward V if the outcome is weakly below a threshold π ∗ (µ) and the
maximal punishment V if the outcome exceeds π ∗ (µ), where this threshold depends on the
action µ and satisfies                 
                                        ∞
                                                  if µ ≤ µ∗
                             π ∗ (µ) =    h (µ)    if µ ∈ (µ∗ , µ∗∗ ]                        (17)
                                       
                                                            ∗∗
                                          −∞       if µ > µ
                                       

for cutoffs µ∗ < µ∗∗ and some continuous function h (µ) ∈ (−∞, ∞) with limµ↓µ∗ h (µ) =
∞. The cutoff µ∗ is a soft instrument threshold, where any action µ ≤ µ∗ is rewarded
independently of the outcome with the maximal continuation value V . The cutoff µ∗∗ > µ∗
is a hard instrument threshold, where any action µ > µ∗∗ is punished independently of
the outcome with the worst continuation value V . Intermediate actions µ ∈ (µ∗ , µ∗∗ ]
are maximally rewarded if the outcome satisfies π ≤ π ∗ (µ) and maximally punished if
the outcome satisfies π > π ∗ (µ). Therefore, an interior target threshold only applies to


                                                              14
intermediate actions.
    We find:

Proposition 4. The optimal hybrid rule specifies µL < µH , V L (π) = V for all π, V H (π) =
V if π ≤ π ∗ (µH ), and V H (π) = V if π > π ∗ (µH ), for some π ∗ (µH ) ∈ (−∞, ∞). This
rule can be implemented with a maximally-enforced hybrid threshold {µ∗ , µ∗∗ , {π ∗ (µ)}µ∈R },
where µ∗ = µL and µ∗∗ = µH .

    The optimal hybrid rule assigns a low action and the maximal reward to the low type,
while specifying a higher action and a target threshold for the high type. To prove this re-
sult, we solve a relaxed version of (13)-(16) which ignores the private information constraint
(14) for the high type and the enforcement constraints (16) for both types. We establish
that the solution to this relaxed problem takes the form described in Proposition 4 and
satisfies these constraints.
    As implied by Proposition 4, the optimal hybrid rule strictly improves upon the rules
studied in Section 3. Intuitively, this rule dominates instrument-based rules by giving the
agent more flexibility to respond to his private information while preserving incentives. The
reason is that, under a hybrid rule, the principal can allow the agent to choose actions µ >
µ∗ and still deter excessively high actions by using a target-based criterion. Analogously,
the optimal hybrid rule dominates target-based rules by more efficiently limiting the agent’s
discretion to choose actions that are excessively high. The reason is that, under a hybrid
rule, the principal can avoid punishments under actions µ ≤ µ∗ and directly punish the
agent for actions µ > µ∗∗ , thus reducing the frequency of punishment on path.
    In principle, combining instruments and targets could yield rules with complicated
forms. Proposition 4 however shows that the optimal hybrid rule admits an intuitive
implementation. This rule essentially consists of an instrument threshold µ∗ which is
relaxed to µ∗∗ whenever the target threshold is satisfied. As noted in the Introduction,
rules of this form have been advocated in practice in the context of monetary policy.


5    Extensions
We consider three extensions of our model. The first two are important robustness checks:
we extend our setting to allow for a continuum of signals and for punishments that harm
the agent and the principal asymmetrically. The third extension is of particular interest
for applications to monetary and fiscal policy: we consider a welfare function that depends
on the deviation of the outcome from its expectation. In all three cases, we show that our
main results from Section 3 continue to hold.

                                             15
5.1        Continuum of Types
Suppose that instead of observing a binary signal s ∈ {−∆, ∆}, the agent observes a signal
as rich as the shock: s ∈ R with s ∼ N (0, ∆2 ). The shock’s conditional distribution obeys
θ|s ∼ N (s, σ 2 ), and thus the unconditional distribution has E(θ) = 0 and V ar(θ) = σ 2 +∆2
as in our baseline model. Defining the optimal instrument-based and target-based rules
analogously as in Section 3, with formal representations provided in the Online Appendix,
we obtain:20

Proposition 5. Consider an extended environment with a continuum of agent types.
The optimal instrument-based rule can be implemented with a maximally-enforced instru-
ment threshold µ∗ ∈ (−∞, ∞). The optimal target-based rule can be implemented with a
maximally-enforced target threshold π ∗ which coincides with that in Proposition 2. More-
over, take these two classes of rules and consider changing σ while keeping V ar(θ) un-
changed. There exists σ ∗ > 0 such that a target-based rule is strictly optimal if σ < σ ∗ and
an instrument-based rule is strictly optimal if σ > σ ∗ .

    The optimal instrument-based and target-based rules under a continuum of types take
the same implementations as under binary types. There are differences, however, with
regards to the agent’s behavior under the optimal instrument-based rule. In the case of
binary types, Proposition 1 shows that both types are bunched at the threshold µ∗ and
receive the maximal reward. Instead, under q   a continuum of types, this is only true for
types s ∈ [s , s ], where s = µ − α < s + 2(V − V ) = s∗∗ . Types s < s∗ satisfy the
             ∗ ∗∗            ∗    ∗        ∗

instrument threshold strictly by choosing their flexible action s + α < µ∗ and receive the
maximal reward, whereas types s > s∗∗ break the instrument threshold by choosing their
flexible action s + α > µ∗ and receive the maximal punishment. The cutoff s∗∗ corresponds
to the type that is indifferent between abiding to the instrument threshold and receiving a
continuation value V versus breaking the threshold and receiving a continuation value V :
               "                      #                    Z ∞"                 #
           ∞
                     (µ∗ − α − θ)2                                  ∗∗
                                                                       −   2
       Z
                                                                  (s     θ)
                   −               + V φ(θ|s∗∗ , σ 2 )dθ =      −            + V φ(θ|s∗∗ , σ 2 )dθ.
       −∞                  2                                −∞         2

   Compared to the analysis in Subsection 3.1, characterizing this optimal instrument-
based rule requires more involved arguments. These arguments are related to those devel-
oped in Halac and Yared (2018b), and we present them in the Online Appendix in three
  20
     We assume that an optimal instrument-based rule is piecewise continuously differentiable. Also, if the
program solving for this rule admits multiple solutions that differ only on a countable set of types, we
select the solution that maximizes the principal’s welfare for those types.


                                                      16
main steps.21 First, we show that the linearity of the principal and agent’s welfare in the
continuation value, together with the richness of the information structure, imply that only
the extreme values {V , V } are prescribed in an optimal rule; that is, the optimal continu-
ation values are bang-bang. Second, appealing to the log concavity of the normal density
function, we establish that the optimal continuation values are also monotonic, with types
below some cutoff s∗∗ being maximally rewarded with value V and types above s∗∗ being
maximally punished with value V . As an implication, types s > s∗∗ must choose their
flexible action s + α. Finally, we show that for types s ≤ s∗∗ , the optimal rule prescribes
an action that is continuous in the agent’s type and takes the form of a threshold.22
     The characterization of the optimal target-based rule under a continuum of types follows
the same steps as in Subsection 3.2. In fact, the optimal target threshold π ∗ is quantitatively
identical to that in the case of binary types, so the statement in Proposition 2 applies
directly to this continuum-of-types case. As mentioned in Subsection 3.2, the reason is
that the agent’s first-order condition in (12) implies a choice κ = µ − s by the agent which
is independent of his type. This means that the optimal target-based rule is independent of
the distribution of the agent’s signal s, and therefore the characterization in Proposition 2
holds independently of the number of types.
     The second part of Proposition 5 extends Proposition 3 to a continuum of types: we
show that target-based rules dominate instrument-based rules if and only if the agent’s
information is sufficiently precise. The result follows from comparing how an increase
in the precision of the agent’s signal affects the principal’s welfare under each rule class.
Under the optimal target-based rule, the effect takes the same form as in Subsection 3.3.
Specifically, if the agent’s information is more precise, then all types can better tailor
their actions to the shock, which lowers the outcome volatility and thus also the need to
utilize costly punishments on path. Under the optimal instrument-based rule, things are
different. Given the cutoffs s∗ and s∗∗ defined above, a higher precision allows types s < s∗
and s > s∗∗ to better tailor their actions to the shock; however, there are no target-based
punishments whose frequency is reduced as a result, and moreover nothing changes for
types s ∈ [s∗ , s∗∗ ] who have no discretion. Consequently, we show that the welfare effect of
an increase in precision is smaller under the optimal instrument-based rule than under the
optimal target-based rule. This comparison allows us to obtain the result in Proposition 5
  21
     The arguments in Halac and Yared (2018b) cannot be applied directly to the current setting as the
agent’s bias in that paper is multiplicative instead of additive.
  22
     We note that our proof in the Online Appendix, and thus the characterization of the optimal
instrument-based rule in Proposition 5, apply more broadly to any distribution of types with a log con-
cave density. This includes the exponential, gamma, log-normal, Pareto, and uniform distributions among
others; see Bagnoli and Bergstrom (2005).


                                                  17
and hence to establish the robustness of our findings to a continuum of agent types.


5.2    Asymmetric Punishments
Suppose that varying the continuation value has a larger effect on the agent’s welfare
compared to that of the principal. Specifically, let us modify the agent’s expected welfare
in (1) so that it is now given by
                              "                 2
                                                               #
                          ∞
                                    (µi − θ − α)
                      Z
                                  −               + cV (µi , θ) φ(θ|si , σ 2 )dθ         (18)
                       −∞                 2

for i = L, H and some c > 1 which we also require to satisfy c < α/2∆. Note that our
baseline model corresponds to setting c = 1. Under c > 1, any reduction in the continuation
value V (µi , θ) implies a harsher punishment on the agent than on the principal. Defining
the optimal instrument-based and target-based rules analogously as in Section 3, with
formal representations provided in the Online Appendix, we obtain:

Proposition 6. Consider an extended environment with asymmetric punishments. The
optimal instrument-based rule can be implemented with a maximally-enforced instrument
threshold µ∗ which coincides with that in Proposition 1. The optimal target-based rule
can be implemented with a maximally-enforced target threshold π ∗ ∈ (−∞, ∞). Moreover,
take these two classes of rules and consider changing σ while keeping V ar(θ) unchanged.
There exists σ ∗ > 0 such that a target-based rule is strictly optimal if σ < σ ∗ and an
instrument-based rule is strictly optimal if σ > σ ∗ .

    The optimal instrument-based and target-based rules under asymmetric punishments
take the same implementations as under symmetric punishments. The optimal instrument-
based rule is in fact quantitatively identical to that in Subsection 3.1, with µL = µH = 0 and
V L = V H = V . Intuitively, given c < α/2∆, it is still the case that providing incentives to
separate the two agent types is too costly for the principal, so the optimal instrument-based
rule continues to bunch both types. The optimal target-based rule takes the same form as
in Subsection 3.2; since the agent’s welfare remains linear in the continuation value under
(18), high-powered incentives remain optimal. Nonetheless, this rule is not quantitatively
identical to that under symmetric punishments. The reason is intuitive: since imposing
any given punishment on the agent, and therefore providing incentives, is now less costly
for the principal, the optimal target-based rule induces an action closer to first best (i.e.,
with a lower value of κ) in the case of asymmetric punishments.


                                                    18
    The second part of Proposition 6 extends Proposition 3 to asymmetric punishments:
we show that target-based rules dominate instrument-based rules if and only if the agent’s
information is sufficiently precise. The argument is the same as in the case of symmetric
punishments. In particular, note that since the optimal instrument-based rule provides no
discretion to the agent, the principal’s welfare under this rule is invariant to the precision
of the agent’s information, as in Subsection 3.3. Moreover, since the optimal target-based
rule takes the same form as under symmetric punishments, the principal’s welfare under
this rule increases with the precision of the agent’s information, also as in Subsection 3.3.
These comparative statics allow us to obtain the result in Proposition 6 and hence to
establish the robustness of our findings to asymmetric punishments.


5.3    Role of Expectations
In some applications of our model, the welfare of the principal and the agent may depend
not only on the payoff-relevant outcome π = µ−θ, but also on the deviation of this outcome
from its expectation, π − E(π). For example, in an application to monetary policy, welfare
depends not only on inflation, but also on realized output, which is increasing in the
difference between actual and expected inflation; see, e.g., Barro and Gordon (1983). In
an open economy capital taxation application, welfare depends on tax revenue, which not
only depends on the level of taxes but is also increasing in the difference between actual
and expected taxes; see, e.g., Aguiar, Amador, and Gopinath (2009). To incorporate these
considerations, let
                                   π e ≡ E(π) = E(µ − θ)                               (19)

be the expected outcome, computed after the principal commits to a continuation value
function V (µi , θ). That is, the expectation in (19) takes into account the distributions of
the shock θ and signal s, as well as the equilibrium behavior of the agent given V (µi , θ).
We modify the agent’s expected welfare in (1) so that it is now given by
                     "                 2
                                                                           #
                 ∞
                           (µi − θ − β)
             Z
                         −               + γ a [µi − θ − π e ] + V (µi , θ) φ(θ|si , σ 2 )dθ   (20)
               −∞                2

for i = L, H and some γ a ≥ 0 and β ≥ 0. The principal’s expected welfare in (2) becomes
                       "           2
                                                                   #
             X 1Z ∞        (µi − θ)
                         −           + γ p [µ − θ − π ] + V (µ , θ) φ(θ|si , σ 2 )dθ
                                             i       e        i
                                                                                               (21)
            i=L,H
                  2 −∞         2




                                                     19
for some γ p ≥ 0. In this formulation, γ a and γ p are the weights that the agent and the
principal place on the deviation of the outcome π from its expectation π e , and β is the
agent’s action bias. We note that if γ p = γ a > 0 and β = 0, then this environment
corresponds to that in Barro and Gordon (1983).
     To solve for the optimal instrument-based and target-based rules, note first that the
principal’s welfare in (21) is identical to that in (2) in our baseline model, since the expected
value of the second term in (21) equals zero by (19). The reason is that the principal
internalizes the impact of the agent’s action µi on the expected outcome π e when choosing
the continuation value function V (µi , θ). In contrast, given V (µi , θ) and his private signal
si , the agent optimally chooses µi taking π e as given, and so he does not internalize how his
behavior affects the expected outcome. Nevertheless, it turns out that the agent’s welfare
in (20) is also equivalent to that in our baseline model. Specifically, note that (20) can be
rewritten as
             Z ∞"                  2
                                                #
                     (µi − θ − α)
                                                                    2                
                                             i           i  2        γa             e
                   −                 + V (µ , θ) φ(θ|s , σ )dθ −        − γ a (β − π )       (22)
               −∞           2                                        2

for α ≡ β + γ a . Since the agent takes π e as given, the second term in (22) is constant and
independent of the agent’s action. Therefore, the agent’s welfare is equivalent to that in
(1) with a bias α arising from the combination of the direct action bias β and the weight
γ a that the agent places on the deviation of the outcome from its expectation.
    These observations imply that this extended environment is mathematically equivalent
to our baseline environment. Consequently, all of our results in Section 3 continue to apply:

Proposition 7. Consider an extended environment in which welfare depends on the differ-
ence between the actual outcome and the expected outcome. The optimal instrument-based
and target-based rules are as described in Proposition 1 and Proposition 2 respectively.
Moreover, take these two classes of rules and consider changing σ while keeping V ar(θ)
unchanged. There exists σ ∗ > 0 such that a target-based rule is strictly optimal if σ < σ ∗
and an instrument-based rule is strictly optimal if σ > σ ∗ . The cutoff σ ∗ is decreasing in
the agent’s bias α and the worst continuation value V .

    The result that σ ∗ is decreasing in α is of particular interest in the application to
monetary policy. This result says that instrument-based rules are more appealing on the
margin relative to target-based rules if the agent is more biased. In the context of Barro
and Gordon (1983), with γ p = γ a ≡ γ > 0 and β = 0, this implies that instrument-based
rules are preferred if the weight γ placed on the outcome surprise is sufficiently large. The


                                               20
magnitude of γ in this context would depend on the social benefit of output expansion
relative to the cost of high inflation: if the benefit is large enough relative to the cost, the
agent’s temptation to engage in surprise inflation is significant and instrument-based rules
dominate target-based rules.
    Relatedly, recall from Subsection 3.3 that the optimal instrument-based rule yields a
higher variance of the outcome, but a lower variance of the agent’s action, compared to
the optimal target-based rule. Applied to monetary policy, this means that the optimal
instrument-based rule yields more volatile output and inflation, but lower average inflation,
than the optimal target-based rule. This lower average inflation makes the instrument-
based rule particularly beneficial when the agent’s temptation to inflate, and thus expected
inflation, are high.


6     Concluding Remarks
Using mechanism design, we have characterized optimal instrument-based and target-based
rules, studied the conditions under which each class is optimal, and characterized the opti-
mal hybrid rule that combines instruments and targets. As discussed in the Introduction,
our results may shed light on a number of applications. For example, in the context of mon-
etary policy, our analysis implies that inflation targeting should be adopted if the central
bank has significantly superior information relative to the public; otherwise a Taylor rule
would perform better. We found that inflation targeting has a larger advantage over Taylor
rules if the central bank’s inflationary bias is relatively small or the sanctions that can be
imposed for missing the inflation target are relatively large. Furthermore, we showed that
an optimal hybrid rule would guide the choice of the interest rate as in a Taylor rule but
relax these requirements when realized inflation satisfies some specified target, similar to
the measures proposed by policymakers in the aftermath of the Global Financial Crisis.
    We presented a stylized model with binary signals and symmetric punishments, and
we showed in extensions that our main results apply also to settings with a continuum
of signals or asymmetric punishments. Our analysis could be further extended in various
directions. First, different specifications of asymmetric punishments could be studied by
allowing for non-linearities. For example, while the principal may suffer a lower cost than
the agent from any given punishment, as in our setting of Subsection 5.2, her cost may also
increase at a faster rate with the severity of punishment, becoming closer to that of the
agent at higher levels. Note that different specifications could give rise to different penal
codes. In particular, unlike the extreme punishments that we found to be optimal in our


                                              21
model, the solution could feature punishment that “fits the crime.”
    A second extension could consider an agent bias that is unknown to the principal and
may take different signs. In the application to monetary policy, central bankers may be
biased in favor of or against inflation relative to society, and their preferences may not
be public. Under binary signals and assumptions analogous to our Assumption 1 and
Assumption 2, an instrument-based rule that bunches the agent types regardless of their
bias would be optimal, so our characterization in Proposition 1 would remain valid. A
characterization of the optimal target-based rule, on the other hand, would have to deal
with the problem that assigned actions may now depend on the agent’s bias.
    Finally, it could be interesting to consider the quantitative representation of the optimal
hybrid rule. Using quantitative methods, one could solve for this unconstrained optimal
rule in a richer model than the one we have studied, and the analysis could show how
predictions may depend on different features of the environment.


References
Acemoglu, D. and A. Wolitzky (2011): “The Economics of Labor Coercion,” Econo-
 metrica, 79, 555–600.

Aguiar, M., M. Amador, and G. Gopinath (2009): “Investment Cycles and Sovereign
 Debt Overhang,” Review of Economic Studies, 76, 1–31.

Alonso, R. and N. Matouschek (2008): “Optimal Delegation,” Review of Economic
 Studies, 75, 259–93.

Amador, M. and K. Bagwell (2013): “The Theory of Delegation with an Application
 to Tariff Caps,” Econometrica, 81, 1541–600.

Amador, M., I. Werning, and G.-M. Angeletos (2006): “Commitment Vs. Flexi-
 bility,” Econometrica, 74, 365–396.

Ambrus, A. and G. Egorov (2017): “Delegation and Nonmonetary Incentives,” Journal
 of Economic Theory, 171, 101–135.

Appelbaum, B. (2014): “Yellen Says Restraining the Fed’s Oversight Would be
 a ‘Grave Mistake’,” New York Times, https://www.nytimes.com/2014/07/17/
 business/yellen-says-constraining-fed-would-be-a-grave-mistake.html.



                                              22
Athey, S., A. Atkeson, and P. J. Kehoe (2005): “The Optimal Degree of Discretion
 in Monetary Policy,” Econometrica, 73, 1431–75.

Bagnoli, M. and T. Bergstrom (2005): “Log-Concave Probability and Its Applica-
 tions,” Economic Theory, 26, 445–69.

Barro, R. J. and D. B. Gordon (1983): “Rules, Discretion and Reputation in a Model
 of Monetary Policy,” Journal of Monetary Economics, 12, 101–121.

Bernanke, B. S. (2004): “Remarks by Governor Ben S. Bernanke,” Before the National
 Economists Club, Washington, D.C. December 2, 2004.

——— (2015):     “The Taylor Rule:    A Benchmark for Monetary Policy?”
 Blog   entry:    https://www.brookings.edu/blog/ben-bernanke/2015/04/28/
 the-taylor-rule-a-benchmark-for-monetary-policy/.

Bernanke, B. S., T. Laubach, F. S. Mishkin, and A. S. Posen (1999): Infla-
 tion Targeting: Lessons from the International Experience, Princeton, N.J., Princeton
 University Press.

Bernanke, B. S. and F. S. Mishkin (1997): “Inflation Targeting: A New Framework
 for Monetary Policy,” Journal of Economic Perspectives, 11, 97–116.

Blinder,    A.    S.     (2014):       “An  Unnecessary    Fix  for   the
 Fed,”      Wall     Street      Journal,   https://www.wsj.com/articles/
 alan-s-blinder-an-unnecessary-fix-for-the-fed-1405639582.

Frankel, A. and N. Kartik (2017): “What Kind of Central Bank Competence?”
 Theoretical Economics, forthcoming.

Fudenberg, D. and J. Tirole (1991): Game Theory, Cambridge, Mass.: MIT Press.

Giannoni, M. P. and M. Woodford (2017): “Optimal Target Criteria for Stabilization
 Policy,” Journal of Economic Theory, 168, 55–106.

Goulder, L. H. and I. W. H. Parry (2008): “Instrument Choice in Environmental
 Policy,” Review of Environmental Economics and Policy, 2, 152–174.

Halac, M. and P. Yared (2014): “Fiscal Rules and Discretion under Persistent Shocks,”
 Econometrica, 82, 1557–614.



                                         23
——— (2018a): “Fiscal Rules and Discretion in a World Economy,” American Economic
 Review, 108, 2305–2334.

——— (2018b): “Fiscal Rules and Discretion under Limited Enforcement,” Working Pa-
 per.

Holmström, B. (1977): “On Incentives and Control in Organizations,” Ph.D. Thesis,
 Stanford University.

——— (1984): “On the Theory of Delegation,” in Bayesian Models in Economic Theory,
 ed. by M. Boyer and R. Kihlstrom, North-Holland; New York.

Innes, R. D. (1990): “Limited Liability and Incentive Contracting with Ex-Ante Action
  Choices,” Journal of Economic Theory, 52, 45–67.

Kocherlakota, N. (2016): “Rules versus Discretion: A Reconsideration,” Brookings
 Papers on Economic Activity, Fall.

Levin, J. (2003): “Relational Incentive Contracts,” American Economic Review, 93, 835–
  57.

McCallum, B. T. and E. Nelson (2005): “Targeting versus Instrument Rules for
 Monetary Policy,” Federal Reserve Bank of St. Louis Review, 87, 597–611.

Mishkin, F. S. (1999): “International Experiences with Different Monetary Policy
 Regimes,” Journal of Monetary Economics, 43, 579–606.

——— (2017): “Improving the Use of Discretion in Monetary Policy,” Working Paper.

National Conference of State Legislatures (1999):         “State Balanced
 Budget     Requirements,”    http://www.ncsl.org/research/fiscal-policy/
 state-balanced-budget-requirements.aspx.

Padró i Miquel, G. and P. Yared (2012): “The Political Economy of Indirect Con-
  trol,” Quarterly Journal of Economics, 127, 947–1015.

Prat, A. (2005): “The Wrong Kind of Transparency,” American Economic Review, 95,
 862–877.

Rogerson, W. (1985): “The First-Order Approach to Principal-Agent Problems,” Econo-
 metrica, 53, 1357–1368.


                                         24
Rogoff, K. (1985): “The Optimal Degree of Commitment to an Intermediate Monetary
 Target,” Quarterly Journal of Economics, 100, 1169–1189.

Svensson, L. E. (2003): “What is Wrong with Taylor Rules? Using Judgment in Mone-
  tary Policy Through Targeting Rules,” Journal of Economic Literature, 41, 426–477.

——— (2005): “Targeting versus Instrument Rules for Monetary Policy: What Is Wrong
 with McCallum and Nelson?” Federal Reserve Bank of St. Louis Review, 87, 613–625.

——— (2010): “Inflation Targeting,” in Handbook of Monetary Economics, ed. by B. M.
 Friedman and M. Woodford, Elsevier, vol. 3b, chap. 22.

Taylor, J. B. (1993): “Discretion versus Policy Rules in Practice,” Carnegie-Rochester
  Conference Series on Public Policy, 39, 195–214.

——— (2014):      “Requirements for Policy Rules for the Fed,”        Testi-
 mony Before The Committee on Financial Services, US House of Rep-
 resentatives,          https://financialservices.house.gov/uploadedfiles/
 hhrg-113-ba00-wstate-jtaylor-20140710.pdf.

US Congress, Office of Technology Assessment (1995): Environmental Policy
 Tools: A User’s Guide, Washington, DC: U.S. Government Printing Office (OTA-ENV-
 634).

Walsh, C. (2015): “Goals and Rules in Central Bank Design,” International Journal of
 Central Banking, 11, 295–352.

Yellen, J. L. (2015): “Normalizing Monetary Policy: Prospects and Perspec-
 tives,” Speech at the The New Normal Monetary Policy Conference, https://www.
 federalreserve.gov/newsevents/speech/yellen20150327a.htm.

——— (2017): “The Economic Outlook and the Conduct of Monetary Policy,” Speech at
 At the SIEPR, Stanford University, https://www.federalreserve.gov/newsevents/
 speech/yellen20170119a.htm.



A     Appendix
In this Appendix, we provide proofs for the results in Section 3 and Section 4. Proofs for
the results in Section 5 can be found in the Online Appendix.

                                           25
A.1             Proof of Proposition 1
We proceed in three steps.

Step 1. We solve a relaxed version of the program in (7) which ignores (4) for i = H and
(5) for i = L, H. Step 2 verifies that the solution to this relaxed problem satisfies these
constraints.

Step 1a. We show that the solution to this relaxed problem satisfies (4) for i = L as an
equality. If this were not the case, then the principal would optimally set µi = si and
V i = V for i = L, H. However, (4) for i = L would then become

    ∞
        "                  2 #                   Z ∞"                              2 #
                 sL − θ − α                                      sL − θ + sH − sL − α
Z
                                 φ θ|sL , σ 2 dθ ≥                                          φ θ|sL , σ 2 dθ,
                                                                                                       
            −                                           −
 −∞                   2                             −∞                    2

which after some algebra yields

                                         sH − sL       sH − sL − 2α ≥ 0.
                                                                  


This inequality contradicts Assumption 1. Thus, (4) for i = L must bind.

Step 1b. We show that the solution to this relaxed problem satisfies µH ≥ µL . Suppose
by contradiction that µH < µL . Consider two perturbations, one assigning µL and V to
both types, and another assigning µH and V to both types. Since these perturbations are
feasible and incentive compatible, the contradiction assumption requires that neither of
them strictly increase welfare, which requires:

                           (µH )2           (µL )2             (µL )2           (µH )2
                 sH µH −          ≥ sH µL −        and sL µL −        ≥ sL µH −        .
                             2                2                  2                2

This is equivalent to
                           "                  #                             "                  #
                                    µH + µL                                           µH + µL
            µH − µ   L
                               sH −                           µH − µ   L
                                                                                 sL −
                                                                        
                                                   ≥ 0 and                                           ≤ 0.
                                       2                                                 2

For µH < µL , these inequalities require

                                          H     L
                                                                H     L
                                                                         
                                        µ   + µ                µ   + µ
                                   sH −             ≤ 0 ≤ sL −             ,
                                            2                      2

which cannot hold given sH > sL . Therefore, µH ≥ µL .

                                                         26
Step 1c. We show that the solution to this relaxed problem satisfies V L = V H = V . Note
first that if V L < V , then an increase in V L is feasible, relaxes constraint (4) for i = L,
and strictly increases the objective. Hence, V L = V , and therefore (4) for i = L (which
binds by Step 1a) can be rewritten as:

                               (µL )2                   (µH )2
                   sL + α µL −        + V = sL + α µH −        + V H.
                                                 
                                                                                            (23)
                                 2                        2

This equation implies that, up to an additive constant independent of the allocation, the
high type’s welfare satisfies

                      (µH )2                     (µL )2
          sH + α µH −        + V H = sL + α µL −        + V + sH − sL µH .
                                                                   
                                                                                            (24)
                        2                          2

Now suppose by contradiction that V H < V . Then it follows from (23) and Step 1b that
µH > µL . Substituting (24) into the objective in (7), the principal’s welfare up to an
additive constant independent of the allocation is equal to

                               (µL )2 1
                        
                       1
                    s + α µL −
                     L
                                         α − sH − sL µH + V .
                                                    
                                     −                                                      (25)
                       2         2     2

Consider a perturbation that reduces µH to µL and increases V H to V . This perturbation is
feasible, satisfies (23), and strictly increases the principal’s welfare given the representation
in (25) and Assumption 1. It follows that V H < V cannot hold, and thus V H = V in the
solution.

Step 1d. We show that the solution to this relaxed problem satisfies µL = µH = 0. By Step
1b, if µL 6= µH , then µH > µL . However, a perturbation that reduces µH to µL is then
feasible, satisfies (23), and strictly increases the principal’s welfare given the representation
in (25) and Assumption 1, yielding a contradiction. It follows that µL = µH , and since
E(θ) = 0, the principal’s welfare in (7) is then maximized at µL = µH = 0.

Step 2. We verify that the solution to the relaxed problem in Step 1 satisfies the con-
straints of the original problem. Since µL = µH and V L = V H , constraint (4) for i = H is
satisfied. As for the constraints in (5), given µL = µH = 0, these require, for i = L, H,

                                                         2
                                            (si + α)
                                     V −V ≥          .
                                                2




                                               27
By Assumption 1, this inequality holds for i = L, H if

                                               α2 /4 + α2 + α2
                                  V −V ≥                       ,
                                                      2

which is satisfied by Assumption 2 and the fact that 2φ(1|0, 1) < 0.5.

Step 3. We verify that a maximally-enforced instrument threshold µ∗ = 0 implements
the solution. Given (1) and (3), conditional on choosing an action µ ≤ µ∗ and receiving
continuation value V , the agent’s optimal action choice is µ = µ∗ regardless of his type.
Moreover, conditional on choosing an action µ > µ∗ and receiving continuation value V ,
the agent’s optimal choice is si + α for each i = L, H. The enforcement constraints in (5)
guarantee that the agent has no incentive to deviate to µ > µ∗ .


A.2     Proof of Proposition 2
We proceed in two steps.

Step 1. We follow a first-order approach by solving a relaxed version of the program in
(10) that replaces (8) with the agent’s first-order condition (12). Step 2 verifies the validity
of this approach.
    As noted in the text, the solution to (12) is µi = si + κ for i = L, H and some κ R 0.
Hence, the relaxed problem, up to an additive constant independent of the allocation, can
be written as:
                                    2 Z ∞                             
                                     κ                            2
                         max        − +         V (π) φ(κ − π|0, σ )dπ                      (26)
                      κ,{V (π)}π∈R   2      −∞

                                             subject to
                                   Z   ∞
                          α−κ+             V (π) φ0 (κ − π|0, σ 2 )dπ = 0,                 (27)
                                    −∞
                                            
                                V (π) ∈ V , V for all π ∈ R.                               (28)


Step 1a. Denote by λ the Lagrange multiplier on (27). We show that λ < 0. To do this, we
consider a doubly-relaxed problem in which constraint (27) is replaced with an inequality




                                                 28
constraint (cf. Rogerson, 1985):
                                   Z   ∞
                         α−κ+               V (π)φ0 (κ − π|0, σ 2 )dπ ≤ 0.              (29)
                                       −∞


Since this is an inequality constraint, the multiplier satisfies λ ≤ 0. We show that (29)
holds as an equality in the solution to the doubly-relaxed problem, and thus this problem
is equivalent to (26)-(28) with λ < 0. Suppose by contradiction that (29) holds as a
strict inequality. Then to maximize (26) the principal chooses κ = 0 and V (π) = V
for all π. However, substituting back into the left-hand side of (29), using the fact that
φ0 (κ − π|0, σ 2 ) = π−κ
                      σ2
                         φ (κ − π|0, σ 2 ), yields
                                   Z   ∞
                                            π
                           α+V                2
                                                φ(−π|0, σ 2 )dπ = α ≤ 0,
                                   −∞       σ

which is a contradiction since α > 0. Therefore, (29) holds as an equality in the doubly-
relaxed problem and λ < 0.

Step 1b. We show that the solution to (26)-(28) satisfies V (π) = V if π ≤ π ∗ and V (π) = V
if π > π ∗ , for some π ∗ ∈ (−∞, ∞). Denote by ψ (π) and ψ (π) the Lagrange multipliers on
the upper bounds and the lower bounds on V (π). The first-order condition with respect
to V (π) is
                     φ(κ − π|0, σ 2 ) + λφ0 (κ − π|0, σ 2 ) + ψ (π) − ψ (π) = 0.         (30)

Suppose that V (π) is strictly interior with ψ (π) = ψ (π) = 0. Then (30) yields

                               1  φ0 (κ − π|0, σ 2 )   π−κ
                              − =               2
                                                     =     .                            (31)
                               λ  φ(κ − π|0, σ )        σ2

Since the right-hand side of (31) is strictly increasing in π whereas the left-hand side is
constant, it follows that (31) holds for only one value of π ∈ (−∞, ∞), which we label π ∗ .
By (30) and (31), the solution has V (π) = V if π ≤ π ∗ and V (π) = V if π > π ∗ .

Step 1c. We show that π ∗ > κ and κ ∈ (0, α). To show the first inequality, recall from Step
1a that λ < 0; hence, (31) yields π ∗ > κ. To show κ < α, note that by Step 1b, (27) can
be rewritten as
                              α − κ − φ κ − π ∗ |0, σ 2 V − V = 0.
                                                            
                                                                                        (32)

Since φ (κ − π ∗ |0, σ 2 ) V − V > 0, (32) requires κ < α.
                                 

   We are left to show that κ > 0. By Step 1b, we can write the Lagrangian of the


                                                  29
principal solving for the optimal level of κ and π ∗ as

                        κ2
                           + 1 − Φ(κ − π ∗ |0, σ 2 )V + Φ(κ − π ∗ |0, σ 2 )V
                                                  
                    −                                                                    (33)
                        2
                           +λ α − κ − φ(κ − π ∗ |0, σ 2 ) V − V .
                                                               


The first-order condition with respect to κ is

          − κ − φ(κ − π ∗ |0, σ 2 ) V − V − λ 1 + φ0 (κ − π ∗ |0, σ 2 ) V − V = 0,
                                                                           
                                                                                         (34)

and the first-order condition with respect to π ∗ is

                 φ(κ − π ∗ |0, σ 2 ) V − V + λφ0 (κ − π ∗ |0, σ 2 ) V − V = 0.
                                                                        
                                                                                         (35)

Substituting (35) into (34) yields
                                            − λ = κ.                                     (36)

Since λ < 0 by Step 1a, (36) implies κ > 0.

Step 2. We verify the validity of the first-order approach: we establish that the choice
of κ in the relaxed problem satisfies (8) and therefore corresponds to the agent’s global
optimum.

Step 2a. We begin by showing that the agent has no incentive to choose some κ0 6= κ,
κ0 ≤ π ∗ . Differentiating the first-order condition (32) with respect to κ yields

                                − 1 − φ0 (κ − π ∗ |0, σ 2 ) V − V .
                                                                 
                                                                                         (37)

Note that (37) is strictly negative for all κ ≤ π ∗ , and thus the agent’s welfare is strictly
concave over this range. Since by Step 1c the solution to the relaxed problem sets κ < π ∗ ,
we conclude that this κ is a maximum and dominates any other κ0 ≤ π ∗ .

Step 2b. We next show that the agent has no incentive to choose some κ0 6= κ, κ0 > π ∗ .
To prove this, we first establish that in the solution to the relaxed problem, given π ∗ , κ
satisfies κ − π ∗ ≤ −σ. Suppose by contradiction that κ − π ∗ > −σ. Note that by (31)
                        2
and (36), κ − π ∗ = − σκ . Hence, the contradiction assumption implies κ > σ. Substituting
              2
κ − π ∗ = − σκ into (32) yields
                                      2        
                                       σ      2
                                                       
                            α − κ − φ − |0, σ     V − V = 0.                             (38)
                                       κ

                                               30
Since the left-hand side of (38) is decreasing in κ and (by the contradiction assumption)
κ > σ, (38) requires
                            α − σ − φ −σ|0, σ 2 V − V > 0.
                                                        

Multiply both sides of this equation by σ > 0 to obtain:

                             σ(α − σ) − σφ −σ|0, σ 2
                                                                 
                                                             V − V > 0.                            (39)
                           p                         p
Note that since 0 < σ < V ar(θ) and, by Assumption 1, V ar(θ) ≤ α/2, we have
σ (α − σ) < α2 /2. Hence, (39) yields

                                          α2
                                                      >V −V.
                                     2σφ (−σ|0, σ 2 )

However, this inequality violates Assumption 2 since σφ (−σ|0, σ 2 ) = φ (1|0, 1). Thus,
given π ∗ , κ satisfies κ − π ∗ ≤ −σ.
    We can now establish that the agent has no incentive to deviate to κ0 6= κ, κ0 > π ∗ .
Consider some κ0 > π ∗ that is a local maximum for the agent. The difference in welfare for
the agent from choosing the value of κ given by the solution to the relaxed problem versus
κ0 is equal to

           κ2        (κ0 )2
                           
                  0
                              + Φ κ0 − π ∗ |0, σ 2 − Φ κ − π ∗ |0, σ 2
                                                                           
      ακ −    − ακ −                                                      V −V .                   (40)
           2           2

Note that by the arguments in Step 1c and κ and κ0 satisfying the agent’s first-order
condition, it follows that both κ and κ0 are between 0 and α. Thus, (40) is bounded from
below by
                          α2
                             + Φ κ0 − π ∗ |0, σ 2 − Φ κ − π ∗ |0, σ 2
                                                                                  
                       −                                                    V −V .                 (41)
                          2
Since (32) is satisfied for both κ and κ0 and κ0 > π ∗ > κ, we must have φ (κ − π ∗ |0, σ 2 ) >
φ (κ0 − π ∗ |0, σ 2 ). Moreover, by the symmetry of the normal distribution, φ (κ − π ∗ |0, σ 2 ) =
φ (− (κ − π ∗ ) |0, σ 2 ) and thus Φ (− (κ − π ∗ ) |0, σ 2 ) < Φ (κ0 − π ∗ |0, σ 2 ). Therefore, (41) is
bounded from below by

                      α2
                         + Φ − (κ − π ∗ ) |0, σ 2 − Φ κ − π ∗ |0, σ 2
                                                                          
                  −                                                      V −V .                    (42)
                      2

Since, as shown above, κ − π ∗ ≤ −σ, we obtain that (42) is itself bounded from below by

    α2                                        α2
       + Φ σ|0, σ 2 − Φ −σ|0, σ 2
                                                                            
−                                    V − V = − +(Φ (1|0, 1) − Φ (−1|0, 1)) V − V > 0,
    2                                         2

                                                  31
where the last inequality follows from Assumption 2 and the fact that φ (1|0, 1) < Φ (1|0, 1)−
Φ (−1|0, 1). Therefore, the agent strictly prefers κ over κ0 .


A.3       Proof of Proposition 3
We begin by proving the following lemma:

Lemma 1. Consider changing σ while keeping V ar(θ) unchanged. The principal’s welfare
is independent of σ under the optimal instrument-based rule and it is strictly decreasing in
σ under the optimal target-based rule.

Proof. By Proposition 1, an optimal instrument-based rule sets µi = 0 and V i = V for
i = L, H. Since V ar(θ) = E(θ2 ) − (E(θ))2 = E(θ2 ) (by E(θ) = 0), the principal’s welfare
under this rule is equal to − V ar(θ)
                                 2
                                      + V , which is independent of σ.
   To evaluate the principal’s welfare under an optimal target-based rule, consider the
Lagrangian taking into account the conditional variance term (which is exogenous and
thus excluded from (33)):

                               σ 2 κ2
                                      + 1 − Φ(κ − π ∗ |0, σ 2 )V + Φ(κ − π ∗ |0, σ 2 )V
                                                             
                           −      −
                               2    2
                                    +λ α − κ − φ(κ − π ∗ |0, σ 2 ) V − V .
                                                                       


The derivative with respect to σ is:
                               "Z                                                                          #
                                    ∞                                               ∗ 2
                                        2
                                          σ − z2                        2
                                                 
                                                                      σ   − (κ  − π  )
                                                   φ(z|0, σ 2 )dz + λ                   φ(κ − π ∗ |0, σ 2 ) .
                           
    −σ + V − V                          −     3                                3
                                 κ−π ∗      σ                                σ

The first term is strictly negative. Using (31) and (36) to substitute in for λ and π ∗ , the
sign of the second term is the same as the sign of
                                                                             "                 2 #  2
                           ∞
                                                                                          σ2
                       Z                                                                                     
                                                                                                      σ
                                    σ2 − z   2
                                                   φ(z|0, σ 2 )dz − κ σ 2 −                                 2
                                               
                   −                                                                               φ − |0, σ .               (43)
                       − σκ
                           2                                                              κ           κ

We next show that this expression is strictly negative, which proves the claim. To show
this, consider the derivative of (43) with respect to κ:
"                  2 !                                     2 !                 2                       2 !      # 
              σ2               σ2                      σ2                   σ2                       σ2          σ2     σ2
                                                                                                                            
     2                                   2                                                 2                                  2
    σ −                           −     σ −                        −2                 −   σ −                        φ − |0, σ .
              κ                κ2                      κ                    κ                        κ           κ2     κ



                                                                        32
This derivative takes the same sign as
                                                  2 !                 2
                                             σ2                   σ2
                                                             
                                    2
                                − σ −                    −2                 ,
                                             κ                    κ

which is strictly negative. Hence, since κ > 0, it suffices to show that the sign of (43) is
weakly negative for κ → 0. By the definition of variance, the first term in (43) goes to zero
as κ → 0. The second term in (43) can be rewritten as:
                                2
                                           σ4
                                              2    
                            2    σ     2        σ  2
                         − σ κφ − |0, σ + φ − |0, σ .                                     (44)
                                 κ         κ    κ

As κ → 0, the first term in (44) goes to zero. Moreover, applying L’Hopital’s Rule on

                                                  1/κ
                                                  2      −1
                                        φ − σκ |0, σ 2

shows that the second term also goes to zero.

    We now proceed with the proof of Proposition 3. By Lemma 1, welfare under the
optimal instrument-based rule is invariant to σ, whereas welfare under the optimal target-
based rule is decreasing in σ. To prove the first part of the proposition, it thus suffices to
show that a target-based rule is optimal at one extreme, for σ → 0, and an instrument-
                                                          p
based rule is optimal at the other extreme, for σ → V ar(θ). This is what we prove
next.
    Consider first the case of σ → 0. By the arguments in Step 1c and Step 2b of the
proof of Proposition 2, 0 < κ ≤ σ. Hence, κ → 0 as σ → 0. Moreover, as σ → 0,
φ (z|0, σ 2 ) corresponds to a Dirac’s delta function, with cumulative distribution function
Φ(z|0, σ 2 ) = 0 if z < 0 and Φ(z|0, σ 2 ) = 1 if z ≥ 0. Therefore, since κ − π ∗ < 0 in the
optimal target-based rule, the limit of the principal’s welfare under this rule, as σ → 0, is
given by

                 κ2
                                                                               
            lim − + 1 − Φ κ − π ∗ |0, σ 2 V + Φ κ − π ∗ |0, σ 2 V
                                                             
                                                                                    =V.
            σ→0  2

Since the principal’s welfare under the optimal instrument-based rule is − V ar(θ)
                                                                              2
                                                                                   + V , it
follows that the optimal target-based rule dominates the optimal instrument-based rule.
                                     p
    Consider next the case of σ → V ar (θ) and thus ∆ → 0. Since κ in the optimal
target-based rule satisfies equation (38), the solution in this case admits κ > 0. The

                                                  33
principal’s welfare under the optimal target-based rule is then equal to

                          V ar(θ)      κ2
                                          − Φ κ − π ∗ |0, σ 2 V − V .
                                                                  
                      −           +V −
                             2         2

Since this is strictly lower than − V ar(θ)
                                       2
                                            + V , it follows that the optimal instrument-based
rule dominates the optimal target-based rule.
    Finally, to prove the second part of the proposition, note that the principal’s welfare
under the optimal instrument-based rule is independent of the agent’s bias α and the
punishment V . Thus, it suffices to show that the principal’s welfare under the optimal
target-based rule is decreasing in α and V . The former follows from the fact that the
derivative of the Lagrangian in (33) with respect to α is equal to λ, which is strictly negative
by Step 1a in the proof of Proposition 2. To evaluate how welfare changes with V , consider
the representation of the program in (10). A reduction in V relaxes constraint (9). Since
this constraint is binding in the solution (by Step 1b of the proof of Proposition 2), it
follows that a reduction in V strictly increases the principal’s welfare under the optimal
target-based rule.


A.4     Proof of Proposition 4
We proceed in three steps.

Step 1. We solve a relaxed version of (13)-(16) which ignores (14) for i = H and (15) for
i = L, H. Step 2 verifies that the solution to this relaxed problem satisfies these constraints.

Step 1a. We show that the solution satisfies (14) for i = L as an equality. The proof of
this claim is analogous to that in Step 1a of the proof of Proposition 1 and thus omitted.

Step 1b. We show that the solution satisfies µH ≥ µL . The proof of this claim is analogous
to that in Step 1b of the proof of Proposition 1 and thus omitted.

Step 1c. We show that the solution satisfies V L (θ) = V for all θ. If V L (θ) < V for some θ,
then an increase in V L (θ) is feasible, relaxes constraint (14) for i = L, and strictly increases
the objective. The claim follows.

Step 1d. We show that the solution satisfies V H (θ) = V if θ < θ∗ and V H (θ) = V if
θ ≥ θ∗ , for some θ∗ ∈ (−∞, ∞). Let 12 λ be the Lagrange multiplier on (14) and denote
by ψ (θ) and ψ (θ) the Lagrange multipliers on the upper bounds and the lower bounds on


                                               34
V H (θ). The first-order condition with respect to V H (θ) yields

                     1                 1
                       φ(θ|sH , σ 2 ) − λφ(θ|sL , σ 2 ) + ψ (θ) − ψ (θ) = 0.              (45)
                     2                 2

Suppose that V H (θ) is strictly interior with ψ (θ) = ψ (θ) = 0. Then (45) implies
                                                             
                                           φ θ − sH |0, σ 2
                                        λ=                     .                          (46)
                                           φ (θ − sL |0, σ 2 )

Since the right-hand side of (46) is strictly increasing in θ whereas the left-hand side is
constant, it follows that (46) holds only for one value of θ ∈ (−∞, ∞), which we label θ∗ .
By (45) and (46), the solution has V (θ) = V if θ < θ∗ and V (θ) = V if θ ≥ θ∗ .
                                                                            
Step 1e. We show that the solution satisfies µL ∈ sL , sH and µH ∈ sL , sH with µH > µL .
Since λ > 0, it follows that θ∗ satisfying (46) is strictly interior. Hence, given Step 1b, the
binding constraint (14) for i = L implies µH > µL . The principal’s first-order condition
with respect to µL yields
                                                    λ
                                      µL = sL +         α,
                                                  1+λ
which implies that µL , and thus µH , exceed sL . The first-order condition with respect to
µH yields
                                             λ
                               µH = sH −          (α − 2∆) ,
                                           1−λ
and the second-order condition yields λ < 1. Using Assumption 1, it follows that µH , and
thus µL , are weakly below sH .
   We end this step by observing that since λ < 1 and sH = −sL = ∆, (46) implies θ∗ < 0.

Step 2. We verify that the solution to the relaxed problem satisfies the constraints of
the original problem. The binding constraint (14) for i = L implies

                                                             (µH )2               (µL )2
V − 1 − Φ θ∗ |sL , σ 2
                     V − Φ θ∗ |sL , σ 2 V = sL + α µH −             − sL + α µL +
                                                                          
                                                                                         .
                                                                2                   2
                                                                                    (47)
Since sH > sL and µH > µL , the right-hand side of (47) is strictly smaller than

                                          (µH )2               (µL )2
                              sH + α µH −        − sH + α µL +
                                                        
                                                                      .                   (48)
                                            2                    2




                                                  35
Moreover, the left-hand side of (47) is strictly larger than

                         V − 1 − Φ θ∗ |sH , σ 2        V − Φ θ∗ |sH , σ 2 V .
                                                                       
                                                                                               (49)

Therefore, (48) is strictly larger than (49), implying that constraint (14) for i = H is
satisfied.
    To verify that constraint (15) for i = L is satisfied, recall from Step 1e that µL ∈ [sL , sH ].
Given this range, the low type’s welfare in the optimal rule is no less than that under
µL = sL , and thus (15) for i = L is guaranteed to hold if

                                                 α2
                                          V −V ≥    ,
                                                 2

which is satisfied by Assumption 2.
    Finally, we verify that constraint (15) for i = H is also satisfied. Note that by constraint
(14) for i = H being satisfied, the high type’s welfare in the optimal rule is no less than that
achieved from mimicking the low type under µL = sL . Thus, (15) for i = H is guaranteed
to hold if
                                                (2∆ + α)2
                                     V −V ≥                ,
                                                    2
which is satisfied by Assumption 1 and Assumption 2.

Step 3. We verify that a maximally-enforced hybrid threshold {µ∗ , µ∗∗ , {π ∗ (µ)}µ∈R } im-
plements the solution. Let µ∗ = µL and µ∗∗ = µH . Construct the function π ∗ (µ) as
described in (17), with h(µ) solving

        V − 1 − Φ µ − h (µ) |sL , σ 2 V − Φ µ − h (µ) |sL , σ 2 V
                                                             
                                                                                               (50)
                                                     µ2                (µL )2
                                        = sL + α µ −     − sL + α µL +
                                                                 
                                                                              .
                                                      2                  2

    The left-hand side of (50) is increasing in µ − h (µ) and the right-hand side is increasing
in µ. Note that limµ↓µ∗ h (µ) = ∞ and, by (47), h (µ∗∗ ) = µ∗∗ −θ∗ . If follows that a solution
for h (µ) exists and µ − h (µ) is increasing in µ.
    We verify that both agent types choose their prescribed actions, µL = µ∗ and µH = µ∗∗ ,
under this maximally-enforced hybrid threshold. By Step 2, neither type has incentives to
deviate to µ > µ∗∗ , as the best such deviation entails choosing µ = si + α for i = L, H
which is suboptimal by (15). The low type has no incentive to deviate to µ = µ∗∗ by (14)
for i = L, and this type has no incentive to deviate to µ < µ∗ either as he is better off by


                                                36
instead choosing µ∗ < sL + α and receiving the same continuation value. The high type has
no incentive to deviate to µ ≤ µ∗ , as the best such deviation entails choosing µ∗ < sH + α
which is suboptimal by (14) for i = H. Therefore, it only remains to be shown that neither
type has incentives to deviate to µ ∈ (µ∗ , µ∗∗ ). This follows immediately from (50) for
the low type, as this equation ensures that the low type is indifferent between choosing µ∗
and choosing any µ ∈ (µ∗ , µ∗∗ ). To show that the high type has no incentive to deviate,
combine (47) and (50) to obtain:

                                                                 (µH )2              µ2
 Φ θ∗ |sL , σ 2 − Φ µ − h (µ) |sL , σ 2    V − V = sL + α µH −          − sL + α µ + .
                                                                           
                                                                   2                  2
                                                                                    (51)
Since sH > sL and µH > µ, the right-hand side of (51) is strictly smaller than

                                      (µH )2             µ2
                          sH + α µH −        − sH + α µ + .
                                                    
                                                                                        (52)
                                        2                2

Moreover, note that µ − h (µ) < θ∗ for µ ∈ (µ∗ , µ∗∗ ), and θ∗ < 0 by Step 1e. Hence, the
left-hand side of (51) is strictly larger than

                      Φ θ∗ |sH , σ 2 − Φ µ − h (µ) |sH , σ 2
                                                                 
                                                                V −V .                  (53)

Therefore, (52) is strictly larger than (53), implying that the high type has no incentive to
deviate to µ ∈ (µ∗ , µ∗∗ ).




                                             37
                                     Online Appendix for
          “Instrument-Based vs. Target-Based Rules”
                                 by Marina Halac and Pierre Yared


    In this Online Appendix, we provide proofs for the results in Section 5 of the paper.


B       Proofs for Section 5
B.1     Proof of Proposition 5
We first study the optimal instrument-based and target-based rules separately and then
compare them.

B.1.1    Optimal Instrument-Based Rule

The program that solves for the optimal instrument-based rule under a continuum of types
is analogous to that in (7) in Section 3.1 of the paper:

                                             "                        #
                                 ∞       ∞
                                                   (µ(s) − θ)2
                             Z       Z
               max                               −             + V (s) φ(θ|s, σ 2 )φ(s|0, ∆2 )dθds
           {µ(s),V (s)}s∈R   −∞      −∞                 2

                              subject to, for all s, s0 ∈ R,
Z ∞"                          #                  Z ∞"                                   #
       (µ(s) − θ − α)2                                    (µ(s 0
                                                                 ) −  θ − α) 2
     −                 + V (s) φ(θ|s, σ 2 )dθ ≥         −                      + V (s0 ) φ(θ|s, σ 2 )dθ,
 −∞           2                                   −∞                2
       "                          #                 Z ∞"                       #
           (µ(s) − θ − α)2                                              2
   Z ∞
                                                              (s   −  θ)
         −                 + V (s) φ(θ|s, σ 2 )dθ ≥         −             + V φ(θ|s, σ 2 )dθ,
    −∞            2                                   −∞           2

                                                     V (s) ∈ [V , V ].

    Integrating over θ, this program, up to a constant independent of the allocation, can




                                                            1
be rewritten as:
                                          ∞
                                                          µ(s)2
                                     Z                                
                       max                        sµ(s) −       + V (s) φ(s|0, ∆2 )ds      (54)
                   {µ(s),V (s)}s∈R     −∞                   2

                                         subject to, for all s, s0 ∈ R,

                                     µ(s)2                           µ(s0 )2
               (s + α)µ(s) −               + V (s) ≥ (s + α)µ(s0 ) −         + V (s0 ),    (55)
                                       2                               2
                                                  µ(s)2           (s + α)2
                         (s + α)µ(s) −                  + V (s) ≥          +V,             (56)
                                                    2                 2
                                                   V (s) ∈ [V , V ].                       (57)

    An instrument-based rule specifying {µ(s), V (s)}s∈R is incentive compatible if this al-
location satisfies (55)-(56), and it is incentive compatible and feasible, or incentive feasible
for short, if the allocation satisfies (55)-(57). As mentioned in the paper, we assume that
an optimal instrument-based rule is piecewise continuously differentiable. Additionally, if
the program above admits multiple solutions that differ only on a countable set of types,
we select the solution that maximizes the principal’s welfare for those types.
    We proceed in four steps. Step 1 establishes some preliminary results that we use in
the subsequent steps. Step 2 shows that any optimal instrument-based rule must prescribe
bang-bang continuation values. Step 3 shows that in any such rule, either all types receive
the maximal continuation value, or there exists an interior cutoff s∗∗ such that only types
above s∗∗ receive the worst continuation value. Step 4 concludes the proof by characterizing
the allocation of actions and showing that such an interior cutoff s∗∗ indeed exists in any
optimal instrument-based rule.
Step 1. We establish some preliminary results.
   The next lemma follows from standard arguments; see Fudenberg and Tirole (1991):

Lemma 2. {µ(s), V (s)}s∈R satisfies the private information constraint (55) if and only if:
(i) µ(s) is nondecreasing, and (ii) the following local private information constraints are
satisfied:

  1. At any point s at which µ(·), and thus V (·), are differentiable,

                                         µ0 (s)(s + α − µ(s)) + V 0 (s) = 0.




                                                          2
   2. At any point s at which µ(·) is not differentiable,

                               µ(s0 )2                                   µ(s0 )2
                                                                                      
                            0                0                      0                  0
          lim   (s + α)µ(s ) −         + V (s ) = lim     (s + α)µ(s ) −         + V (s ) .
          s0 ↑s                  2                s0 ↓s                    2

    The private information constraints imply that the derivative of the agent’s welfare
with respect to s is µ(s). Hence, in an incentive compatible rule, the welfare of type s ∈ R
satisfies
                                                                        Z s
                 µ(s)2                                 µ(s)2
                                                                                   
  (s + α)µ(s) −         + V (s) = lim (s + α)µ(s) −           + V (s) +     µ(es)des . (58)
                    2             s→−∞                   2                s


Following Amador, Werning, and Angeletos (2006), we can substitute (58) into the princi-
pal’s objective in (54) to rewrite this objective as
                                                       Z ∞
                                       µ(s)2
                                                                    
                     lim (s + α)µ(s) −       + V (s) +     µ(s)Q(s)ds ,                          (59)
                    s→−∞                 2              s


where
                                Q (s) ≡ 1 − Φ(s|0, ∆2 ) − αφ(s|0, ∆2 ).

Note that
                                Q0 (s) = −φ(s|0, ∆2 ) − αφ0 (s|0, ∆2 ),

and thus Q0 (s) < 0 if s < sb ≡ ∆2 /α and Q0 (s) > 0 if s > sb. (Observe that this property on
Q0 (s) holds for some sb for any density function φ that is log concave.) We next describe
two functions that we will use in our proofs.

Lemma 3. Given sL ≤ sM ≤ sH , define the functions
                            Z    sM
            L   L   M
                                      s − sL − α φ(s|0, ∆2 )ds + αφ(sM |0, ∆2 )(sM − sL ),
                                                
         B (s , s ) =
                                sL
                            Z    sH
        B H (sH , sM ) =              s − sH − α φ(s|0, ∆2 )ds + αφ(sM |0, ∆2 )(sH − sM ).
                                                
                                sM


Then B L (sL , sM ) > 0 if Q0 (s) < 0 for all s ∈ (sL , sM ), B L (sL , sM ) < 0 if Q0 (s) > 0 for
all s ∈ (sL , sM ), B H (sH , sM ) > 0 if Q0 (s) > 0 for all s ∈ (sM , sH ), and B H (sH , sM ) < 0 if
Q0 (s) < 0 for all s ∈ (sM , sH ).

Proof. Consider the claims about B L (sL , sM ). Note that B L (s, sM )|s=sM = 0, and hence



                                                   3
                      R sM   dB L (s,sM )
B L (sL , sM ) = −     sL         ds
                                          ds.   Moreover,

                                              sM
                 dB L (s, sM )
                                        Z
                               =−                    s|0, ∆2 )de
                                                   φ(e         s + αφ(s|0, ∆2 ) − αφ(sM |0, ∆2 ),
                      ds                  s

           dB L (s,sM )                                                  R sM R sM   d2 B L (e
                                                                                             s,sM )
and thus        ds
                        |s=sM    = 0. Therefore, B L (sL , sM ) =         sL   s          des2
                                                                                                    de
                                                                                                     sds,   where

                                  d2 B L (s, sM )
                                           2
                                                  = φ(s|0, ∆2 ) + αφ0 (s|0, ∆2 ).
                                       ds
             2    L    M                                 2   L       M                              2   L    M)
Note that d B ds(s,s
                 2
                     )
                       > 0 if Q0 (s) < 0, d BdB(s,s
                                                 2
                                                    )
                                                      = 0 if Q0 (s) = 0, and d B ds(s,s
                                                                                    2                             < 0 if
 0                                L L M
Q (s) > 0. The claims about B (s , s ) follow.
   The proof for the claims about B H (sH , sM ) is analogous and thus omitted.

Step 2. We show that if {µ(s), V (s)}s∈R is an optimal instrument-based rule, then V (s) ∈

  V , V for all s ∈ R.
   Take any solution to the program in (54)-(57). We proceed in three sub-steps.
Step 2a. We show V (s) is left-continuous at each s ∈ R.
   Suppose by contradiction that there exists s at which V (s) is not left-continuous. De-
note the left limit by {µ(s− ), V (s− )} = lims0 ↑s {µ(s0 ), V (s0 )}. By Lemma 2,

                               µ(s)2             −    µ(s− )2
                 (s + α)µ(s) −       − (s + α)µ(s ) +         = V (s− ) − V (s).
                                 2                      2

Given α > 0 and the fact that µ(s− ) < µ(s) by Lemma 2, this implies

                                      µ(s)2             µ(s− )2
                            sµ(s) −         − sµ(s− ) −         < V (s− ) − V (s).
                                        2                 2

It follows that a perturbation that assigns {µ(s− ), V (s− )} to type s is incentive feasible,
strictly increases the principal’s welfare from type s, and does not affect the principal’s
welfare from types other than s. Hence, V (s) must be left-continuous at each s ∈ R.
Step 2b. We show V (s) is a step function over any interval [sL , sH ] with V (s) ∈ (V , V ).
    By the private information constraints, V (s) is piecewise continuously differentiable
and nonincreasing. Suppose by contradiction that there is an interval [sL , sH ] over which
V (s) is continuously strictly decreasing in s and satisfies V < V (s) < V . By Lemma 2,
µ(s) must be continuously strictly increasing over the interval, and without loss we can
take an interval over which µ(s) is continuously differentiable. Moreover, by the properties

                                                                 4
of the normal distribution, we can take either an interval above sb with Q0 (s) > 0 for all
s ∈ [sL , sH ] or an interval below sb with Q0 (s) < 0 for all s ∈ [sL , sH ]. We consider each
possibility in turn.
Case 1: Suppose Q0 (s) < 0 for all s ∈ [sL , sH ]. We show that there exists an incentive
feasible perturbation that rotates the increasing schedule µ(s) clockwise over [sL , sH ] and
strictly increases the principal’s welfare. Define
                                                         Z   sH
                                       1
                                  µ= H                            µ(s)ds.
                                    (s − sL )            sL


For given τ ∈ [0, 1], let µ
                          e(s, τ ) be the solution to

                                  e(s, τ ) = τ µ + (1 − τ ) µ(s),
                                  µ                                                                  (60)

which clearly exists. Define Ve (s, τ ) as the solution to
                                                                                      s
                   e(s, τ )2 e                             µ(sL )2
                                                                                  Z
                   µ
        µ(s, τ ) −
 (s + α)e                   + V (s, τ ) = (sL + α)µ(sL ) −         + V (sL ) +            µ
                                                                                          e(e
                                                                                            s, τ )de
                                                                                                   s. (61)
                      2                                      2                    sL


    The original allocation corresponds to τ = 0. We consider a perturbation where we
increase τ marginally above zero if and only if s ∈ [sL , sH ]. Note that differentiating (60)
and (61) with respect to τ yields

                                               de
                                                µ (s, τ )
                                                           = µ − µ(s),                               (62)
                                                   dτ
                                                             Z s
               de
                µ (s, τ )                      dVe (s, τ )       de
                                                                  µ(es, τ )
                          (s + α − µ
                                   e(s, τ )) +             =                de
                                                                             s.                      (63)
                  dτ                              dτ          sL    dτ

Substituting (62) in (63) yields that for a type s ∈ [sL , sH ], the change in the agent’s
welfare from a marginal increase in τ , starting from τ = 0, is equal to
                                             Z   s
                                   D (s) ≡           (µ − µ(e
                                                            s)) de
                                                                 s.
                                              sL


    We begin by showing that the perturbation satisfies constraints (55)-(57). For the
private information constraint (55), note that D(sL ) = D(sH ) = 0, so the perturbation
leaves the welfare of types sL and sH (and that of types s < sL and s > sH ) unchanged.
Using Lemma 2 and the representation in (58), it then follows from equation (61) and the
fact that µ
          e(s, τ ) is nondecreasing in s that the perturbation satisfies constraint (55) for all
s and any τ ∈ [0, 1].

                                                     5
    To prove that the perturbation satisfies the enforcement constraint (56), we show that
the welfare of types s ∈ [sL , sH ] weakly rises when τ increases marginally. Since D(sL ) =
D(sH ) = 0, it is sufficient to show that D(s) is concave over (sL , sH ) to prove that D(s) ≥ 0
for all s in this interval. Indeed, we verify:

                                        D0 (s) = µ − µ(s),
                                        D00 (s) = −µ0 (s) < 0.

   Lastly, observe that constraint (57) is satisfied for τ > 0 small enough. This follows
from the fact that V (s) ∈ (V , V ) for s ∈ [sL , sH ] in the original allocation.
   We next show that the perturbation strictly increases the principal’s welfare. Using the
representation in (59), the change in the principal’s welfare from an increase in τ is equal
to                                   Z sH
                                          de
                                           µ (s, τ )
                                                     Q(s)ds.
                                      sL      dτ
Substituting with (62) and the expression for Q(s) yields that this is equal to
                       Z   sH
                                (µ − µ(s)) 1 − Φ(s|0, ∆2 ) − αφ(s|0, ∆2 ) ds.
                                                                         
                          sL


    This is an integral over the product of two terms. The first term is strictly decreasing in
s since µ(s) is strictly increasing over [sL , sH ]. The second term is also strictly decreasing
in s; this follows from Q0 (s) < 0 for all s ∈ [sL , sH ]. Therefore, these two terms are
positively correlated with one another, and thus the change in the principal’s welfare is
strictly greater than
                 Z   sH                   Z   sH
                                                   1 − Φ(s|0, ∆2 ) − αφ(s|0, ∆2 ) ds,
                                                                                 
                          (µ − µ(s)) ds
                  sL                      sL


which is equal to 0. It follows that the change in the principal’s welfare from the pertur-
bation is strictly positive. Hence, if V (s) is strictly interior and Q0 (s) < 0 over a given
interval, then V (s) must be a step function over the interval.
Case 2: Suppose Q0 (s) > 0 for all s ∈ [sL , sH ]. Recall that µ(s) is continuously strictly
increasing over [sL , sH ]. We begin by showing that the enforcement constraint (56) cannot
bind for all s ∈ [sL , sH ]. Suppose by contradiction that it does. Using the representation




                                                      6
of the agent’s welfare in (58), this implies
                                              Z    sH
                                                         s + α − µ(e
                                                        (e         s))de
                                                                       s=0
                                               s


for all s ∈ [sL , sH ], which requires {µ(s), V (s)} = {s + α, V } for all s ∈ (sL , sH ). However,
this contradicts the assumption that V (s) ∈ (V , V ) for all s ∈ [sL , sH ]. Hence, the enforce-
ment constraint cannot bind for all types in the interval, and without loss we can take an
interval with this constraint being slack for all s ∈ [sL , sH ].
     We next show that there exists an incentive feasible perturbation that strictly increases
the principal’s welfare. Specifically, consider drilling a hole around a type sM within
[sL , sH ] so that we marginally remove the allocation around this type. That is, type sM
can no longer choose {µ(sM ), V (sM )} and is indifferent between jumping to the lower or
upper limit of the hole. With some abuse of notation, denote the limits of the hole by sL
and sH , where the perturbation marginally increases sH from sM . Since the enforcement
constraint is slack for all s ∈ [sL , sH ], the perturbation is incentive feasible. The change in
the principal’s welfare from the perturbation is equal to
                              Z   sH
                                        µ0 (sH )(s − µ(sH )) + V 0 (sH ) φ(s|0, ∆2 )ds
                                                                        
                          s       M
             M
                            µ(sL )2                      µ(sH )2
                                                                        
         ds        M   L                  L     M   H
     +            s µ(s ) −         + V (s ) − s µ(s ) +         − V (s ) φ(sM |0, ∆2 ).
                                                                       H
         dsH                  2                            2

Note that by the private information constraint for type sH ,

                                       µ0 (sH )(sH + α − µ(sH )) + V 0 (sH ) = 0,             (64)

and by indifference of type sM ,

                 M            L   µ(sL )2       L      M       H    µ(sH )2
             (s       + α)µ(s ) −         + V (s ) = (s + α)µ(s ) −         + V (sH ).        (65)
                                    2                                 2

Substituting with these expressions, the change in the principal’s welfare is equal to

                      sH
                                                     dsM
                  Z
         0   H
                           s − sH − α φ(s|0, ∆2 )ds + H α µ(sH ) − µ(sL ) φ(sM |0, ∆2 ).
                                                                        
     µ (s )                                                                                   (66)
                  sM                                 ds




                                                              7
Differentiating (65) with respect to sH and substituting with (64) yields

                                   dsM     0 H     (sH − sM )
                                       = µ  (s )                 .
                                   dsH           µ(sH ) − µ(sL )

Substituting back into (66) and dividing by µ0 (sH ) > 0, we find that the change in the
principal’s welfare takes the same sign as
                          Z   sH
           H   H   M
                                   s − sH − α φ(s|0, ∆2 )ds + αφ(sM |0, ∆2 )(sH − sM ).
                                             
         B (s , s ) =
                           sM


Since Q0 (s) > 0 for all s ∈ [sM , sH ], Lemma 3 implies B H (sH , sM ) > 0, and thus the
perturbation strictly increases the principal’s welfare. Hence, if V (s) is strictly interior
and Q0 (s) > 0 over a given interval, then V (s) must be a step function over the interval.
Step 2c. We show V (s) ∈ {V , V } for all s ∈ R.
    Suppose by contradiction that V (s) ∈ (V , V ) for some s. By the previous steps and
Lemma 2, s belongs to a stand-alone segment (sL , sH ], such that µ(s) = µ and V (s) = V
for all s ∈ (sL , sH ], with V ∈ (V , V ) (by assumption), and µ(s) jumps at sL and sH .
    We first show that the enforcement constraint must be slack for all s ∈ (sL , sH ). Express
the enforcement constraint as the difference between the left-hand and right-hand sides of
(56), so that this constraint must be weakly positive and it is equal to zero if it binds.
By the private information constraints, the derivative of the enforcement constraint with
respect to s is equal to µ(s) − α − s. Since µ(s) is constant over (sL , sH ], it follows that
the enforcement constraint is strictly concave over the interval, and therefore slack for all
s ∈ (sL , sH ).
    We next show that there exists an incentive feasible perturbation that strictly increases
the principal’s welfare. We consider perturbations that marginally change the constant
action µ and continuation value V . As we describe next, the perturbation that we perform
depends on the shape of the function Q(s) over (sL , sH ]:
                      R sH             R sH
Case 1: Suppose sL Q(sL )ds < sL Q(s)ds. Consider a perturbation that marginally
changes the action by dµ > 0 and reduces V in order to keep type sH equally well off. This
means that dV   dµ
                   is given by
                                                    dV
                                      sH + α − µ +      = 0.                               (67)
                                                    dµ
Note that for any arbitrarily small dµ > 0, this perturbation makes the lowest types s in
(sL , sH ], arbitrarily close to sL , jump either to the allocation of type sL or to their flexible
allocation under maximal punishment {s + α, V }, where we let the perturbation introduce

                                                 8
the latter. In the limit as dµ goes to zero, the change in the principal’s welfare due to the
perturbation is thus equal to23
                                            Z    sH 
                                                 dV
                                        s−µ+           φ(s|0, ∆2 )ds                                          (68)
                                 s L             dµ
                  dsL               µ(sL )2                    µ2
                                                                    
                         L   L                    L     L
                +       s µ(s ) −           + V (s ) − s µ +      − V φ(sL |0, ∆2 ),
                  dµ                  2                        2

where the following indifference condition holds:

                                             µ2                        µ(sL )2
                           (sL + α)µ −          + V = (sL + α)µ(sL ) −         + V (sL ).
                                             2                           2

    To verify that the perturbation is incentive feasible, note that the enforcement con-
straint is slack for all s ∈ (sL , sH ), V is strictly interior, and the welfare of types sL and
sH remains unchanged with the perturbation. Hence, the perturbation is incentive feasible
for dµ arbitrarily close to zero.
    To verify that the perturbation strictly increases the principal’s welfare, substitute (67)
and the indifference condition of type sL into (68) to obtain:

                       sH
                                                          dsL
                   Z
                               s − sH − α φ(s|0, ∆2 )ds +     α µ(sL ) − µ φ(sL |0, ∆2 ).
                                                                         
                                                                                                              (69)
                   sL                                     dµ

       Differentiating the indifference condition of type sL and substituting with (67) yields

                                                          dsL    sH − sL
                                                              =            .
                                                          dµ    µ − µ(sL )

Substituting back into (69), we find that the change in the principal’s welfare takes the
same sign as
                                    Z   sH
               H       H    L
                                                 s − sH − α φ(s|0, ∆2 )ds + αφ(sL |0, ∆2 )(sH − sL ),
                                                           
             B (s , s ) =
                                    sL


which can be rewritten as
                                                 Z   sH   Z   s                 Z   sH
                           H    H   L                              0
                       B (s , s ) =                               Q (e
                                                                     s)de
                                                                        sds =            (Q(s) − Q(sL ))ds.
                                                  sL      sL                    sL
  23
   The arguments that follow are unchanged if {µ(sL ), V (sL )} is replaced with {sL + α, V } for the cases
where the enforcement constraint binds.




                                                                       9
                            R sH            R sH
By the assumption that sL Q(sL )ds < sL Q(s)ds, the above expression is strictly posi-
tive. The perturbation therefore strictly increases the principal’s welfare, yielding a con-
tradiction.
                  R sH              R sH
Case 2: Suppose sL Q(sL )ds ≥ sL Q(s)ds. Since Q0 (s) 6= 0 almost everywhere, there
                                    R sh           R sh
must exist sh ∈ (sL , sH ] such that sL Q(sL )ds > sL Q(s)ds. Then consider a perturbation
where, for s ∈ (sL , sh ], we marginally change the action by dµ < 0 and increase V in order
to keep type sh equally well off. This perturbation makes types arbitrarily close to sL jump
up to the allocation of the stand-alone segment. Arguments analogous to those in Case
1 above imply that the perturbation is incentive feasible. Moreover, following analogous
steps as in that case yields that the implied change in the principal’s welfare takes the
same sign as
                                     Z   sh
             H   h       L
                                              s − sh − α φ(s|0, ∆2 )ds − αφ(sL |0, ∆2 )(sh − sL ),
                                                        
         −B (s , s ) = −
                                     sL


which can be rewritten as
                                              Z   sh   Z   s                   Z   sh
                     H       h   L                              0
              −B (s , s ) = −                                  Q (e  sds = −
                                                                  s)de                  (Q(s) − Q(sL ))ds.
                                              sL       sL                      sL

                       R sh            R sh
By the assumption that sL Q(sL )ds > sL Q(s)ds, the above expression is strictly positive.
The perturbation therefore strictly increases the principal’s welfare, yielding a contradic-
tion.
Step 3. We show that if {µ(s), V (s)}s∈R is an optimal instrument-based rule, then either
V (s) = V for all s ∈ R, or there exists some s∗∗ ∈ (b
                                                     s, ∞) such that V (s) = V if s ≤ s∗∗
and V (s) = V is s > s∗∗ .
    Take any solution to the program in (54)-(57). We proceed in two sub-steps.
Step 3a. We show that if V (s∗∗ ) = V for some s∗∗ ∈ R, then s∗∗ ≥ sb.
    By Step 2a, if V (s∗∗ ) = V for some s∗∗ , then V (s) = V over an interval (sL , sH ]
that contains s∗∗ . Take the largest such interval. We establish that sL ≥ sb. Suppose by
contradiction that sL < sb and take a subinterval (sL , sh ] below sb. Note that the enforcement
constraint requires µ(s) = s + α for all s ∈ (sL , sh ]. Then we can perform a perturbation
that rotates the action schedule clockwise over this interval, analogous to the perturbation
used in Case 1 in Step 2b. By the arguments in that case, this perturbation is incentive
feasible. In particular, note that since the perturbation weakly increases the welfare of all
types s ∈ (sL , sh ] while simultaneously changing their action away from s + α, it follows

                                                                    10
that the perturbation must necessarily increase V (s) above V . Moreover, by Q0 (s) < 0 for
all types s ∈ (sL , sh ] (by this interval being below sb), the perturbation strictly increases
the principal’s welfare, yielding a contradiction.
Step 3b. We show that if V (s∗∗ ) = V for some s∗∗ ∈ R, then V (s) = V for all s ≥ s∗∗ .
    Suppose by contradiction that V (s∗∗ ) = V for s∗∗ ∈ (−∞, ∞) and V (s) > V for some
s > s∗∗ . By Step 3a above, s∗∗ ≥ sb. Moreover, by Step 2a, there exist sH > sL ≥ s∗∗ such
that V (s) = V for all s ∈ (sL , sH ].
    We begin by establishing that µ(s) = µ for all s ∈ (sL , sH ] and some µ. Suppose
by contradiction that µ0 (s) > 0 at some s0 ∈ (sL , sH ]. Note that the private information
constraint (55) (together with the constant continuation value over (sL , sH ]) implies µ(s) =
s + α, and thus a slack enforcement constraint, in the neighborhood of such type s0 . Then
we can perform an incentive feasible perturbation that drills a hole in the µ(s) schedule in
this neighborhood, as that described in Case 2 in Step 2b. By the arguments in that case,
this perturbation strictly increases the principal’s welfare, yielding a contradiction.
    We next show that a segment (sL , sH ] with µ(s) = µ and V (s) = V for all s ∈ (sL , sH ]
and sL ≥ s∗∗ cannot exist. Suppose by contradiction that it does. Take sL to be the lowest
point weakly above s∗∗ at which V jumps, and take sH to be the lowest point above sL at
which V jumps again. Note that sH < ∞ must exist, since (56) cannot be satisfied for all
s > sL with µ(s) = µ and V (s) = V for all such s. Then (sL , sH ] is a stand-alone segment
with constant action µ and continuation value V . Note that by arguments analogous to
those in Step 2c, the enforcement constraint must be slack for all s ∈ (sL , sH ). Moreover,
observe that µ < sH + α must hold, since otherwise by Lemma 2 and the monotonicity of
µ(s), (55) would be violated at sH . It follows that we can perform an incentive feasible
perturbation analogous to that used in Step 2c: for µ0 = µ + ε, ε > 0 arbitrarily small,
we increase µ marginally to µ0 and set V slightly below V so as to keep type sH ’s welfare
                                                             R sH            R sH
under this allocation unchanged. Since sL ≥ s∗∗ implies sL Q(sL )ds < sL Q(s)ds, this
perturbation strictly increases the principal’s welfare, yielding a contradiction.
Step 4. We characterize the optimal allocation of actions and show that any optimal
instrument-based rule specifies s∗∗ ∈ (b
                                       s, ∞) as defined in Step 3.
    Take any solution to program (54)-(57). By Step 3, either V (s) = V for all s ∈ R, or
there exists some s∗∗ ∈ (b
                         s, ∞) such that V (s) = V for s ≤ s∗∗ and V (s) = V for s > s∗∗ .
In the latter case, by the enforcement constraint (56), µ(s) = s + α for all s > s∗∗ , and
since (56) holds with equality at s∗∗ , this type’s allocation satisfies

                                          µ(s∗∗ )2      (s∗∗ + α)2
                     (s∗∗ + α)µ(s∗∗ ) −            +V =            +V.                    (70)
                                             2               2
                                              11
    These results characterize the allocation for types s ≥ s∗∗ when there exists an interior
type s∗∗ as defined above. We next proceed by characterizing the allocation that corre-
sponds either to types s < s∗∗ in this case, or to all types in the case that such an interior
type s∗∗ does not exist. The final step of the proof establishes that the latter scenario does
not arise, namely any optimal instrument-based rule specifies an interior type s∗∗ such that
V (s) = V for s ≤ s∗∗ and V (s) = V for s > s∗∗ .
Step 4a. We show µ(s) is continuous over any interval [sL , sH ] such that V (s) = V for all
s ∈ [sL , sH ].
   There are two cases to consider:
Case 1: Suppose by contradiction that µ(s) has a point of discontinuity below sb. Note that
if s∗∗ as defined above exists, then the assumed point of discontinuity is strictly below s∗∗ .
The discontinuity requires that a type sM < sb be indifferent between choosing lim µ(s)
                                                                                      s↑sM
and lim µ(s) > lim µ(s). Note that given V (s) = V around this point, there must be a
    s↓sM            s↑sM
hole with types s ∈ [sL , sM ) bunched at µ(sL ) = sL + α and types s ∈ (sM , sH ] bunched at
µ(sH ) = sH + α, for some sL < sM < sH . Now consider perturbing this rule by marginally
increasing sL , in an effort to slightly close the hole. This perturbation leaves the welfare of
types strictly above sM unchanged and is incentive feasible. The change in the principal’s
welfare from the perturbation is equal to
                                       Z   sM
                               0   L
                                    s − µ(sL ) φ(s|0, ∆2 )ds
                                              
                              µ (s )
                              sL
                   M
                             µ(sL )2               µ(sH )2
                                                          
                ds     M L              M     H
               + L s µ(s ) −         − s µ(s ) +             φ(sM |0, ∆2 ).
                ds              2                     2

Note that µ(sL ) = sL + α, µ(sH ) = sH + α, and µ0 (sL ) = 1. Moreover, by indifference of
type sM , we have

                                           µ(sL )2                    µ(sH )2
                      (sM + α)µ(sL ) −             = (sM + α)µ(sH ) −         .              (71)
                                             2                          2

Substituting into the expression above yields that the change in the principal’s welfare is
equal to

               sM
                                              dsM
           Z
                    s − sL − α φ(s|0, ∆2 )ds + L φ(sM |0, ∆2 )α µ(sH ) − µ(sL ) .
                                                                              
                                                                                             (72)
            sL                                ds

Note that differentiating the indifference condition (71) with respect to sL (and substituting


                                                  12
with µ(sL ) = sL + α) yields
                                    dsM     (sM − sL )
                                        =                 .
                                    dsL   µ(sH ) − µ(sL )
Substituting this back into (72), we find that the change in the principal’s welfare is equal
to                      Z sM
           L L M
                              s − sL − α φ(s|0, ∆2 )ds + αφ(sM |0, ∆2 )(sM − sL ).
                                          
         B (s , s ) =
                          sL

It follows from sM < sb and Lemma 3 that B L (sL , sM ) > 0. Thus, the perturbation strictly
increases the principal’s welfare, showing that µ(s) cannot jump at a point below sb.
Case 2: Suppose by contradiction that µ(s) has a point of discontinuity above sb. Note
that if s∗∗ as defined above exists, then the assumed point of discontinuity is strictly below
s∗∗ . By the same logic as in Step 3b, we can show that µ0 (s) = 0 over any continuous
interval above sb over which V (s) = V . It follows that there must exist a stand-alone
segment (sL , sH ] with constant action µ and continuation value V = V , satisfying sL ≥ sb.
However, using again the arguments in Step 3b, a perturbation that marginally increases
µ and sets V slightly below V would then be incentive feasible and would strictly increase
the principal’s welfare. Therefore, µ(s) cannot jump at a point above sb around which
V (s) = V .
Step 4b. We show µ(s) ≤ s + α for all s for which V (s) = V .
    Consider types s ≤ s∗∗ when an interior point s∗∗ as described above exists, or all types
s ∈ R when such a point s∗∗ does not exist. Step 4a above implies that the allocation for
these types must be bounded discretion, with either a minimum action level or a maximum
action level or both. Note that if a minimum action level is prescribed, then there must
exist some interior point s? such that the allocation satisfies {µ(s), V (s)} = {s? + α, V } for
all s ≤ s? . However, such an allocation would violate the enforcement constraint (56) for s
sufficiently low. Therefore, a minimum action level is not enforceable and only a maximum
action level can be imposed, establishing the claim.
Step 4c. We show µ(s) < s + α for some s for which V (s) = V . Moreover, there exists
s∗∗ ∈ (b
       s, ∞) as defined in Step 3.
  Suppose that an interior point s∗∗ as described above exists. By Steps 4a and 4b, (70)
    hold for µ(s∗∗ ) = s∗ + α, where the value of s∗ is unique given s∗∗ and satisfies s∗∗ =
mustq
s∗ + 2(V − V ). In this circumstance, the optimal instrument-based rule is implemented
with a strictly interior threshold µ∗ = s∗ + α, and the agent’s action satisfies µ(s) < s + α
for all s ∈ (s∗ , s∗∗ ).


                                              13
    We end the proof by showing that an interior point s∗∗ as described above must indeed
exist in any optimal instrument-based rule. Suppose by contradiction that this is not the
case. Then by the steps above, there must be an optimal instrument-based rule prescribing
{µ(s), V (s)} = {s + α, V } for all s ∈ R. Using the representation in (59), the principal’s
welfare under this rule is equal to
                                                                 ∞
                                    (s + α)2
                                                            Z                       
                          lim                +V +                    (s + α)Q(s)ds .                     (73)
                        s→−∞            2                    s


   Consider the principal’s welfare under a maximally enforced threshold µ∗ = s∗ + α:
                                    ∞                                s∗∗
               (s + α)2
                               Z                                Z                                 
                                                                             ∗
     lim                +V +            (s + α)Q(s)ds +                    [(s + α) − (s + α)]Q(s)ds ,   (74)
    s→−∞           2            s                                s∗

                  q
where s∗∗ = s∗ + 2(V − V ). The contradiction assumption requires that (73) weakly
exceed
q      (74) for all strictly interior s∗ . That is, for all s∗ ∈ (−∞, ∞) and s∗∗ = s∗ +
  2(V − V ), the following condition must hold:

                                        Z   s∗∗
                                                  (s∗ − s) Q(s)ds ≤ 0.                                   (75)
                                          s∗


Note that lims→∞ Q(s) = 0 and Q0 (s) > 0 for all s > sb. Thus, setting s∗ ≥ sb yields
Q(s) < 0 for all s ∈ [s∗ , s∗∗ ]. This implies that the left-hand side of (75) is an integral over
the product of two negative terms, and thus strictly positive, yielding a contradiction.

B.1.2      Optimal Target-Based Rule

The characterization of the optimal target-based rule under a continuum of types follows
the same steps as in our baseline model. Letting µ(s) denote the action of type s, condition
(12) characterizes the value of κ = µ(s) − s. The optimal target-based rule can then be
represented as the solution to (26)-(28), where it is clear that this solution is independent
of the distribution of types. As such, the arguments in the proof of Proposition 2 apply
directly to this setting with a continuum of types.

B.1.3      Optimal Class of Rule

Since the optimal target-based rule, and the principal’s welfare under this rule, are identical
to those in our baseline model, the same arguments as in the proof of Lemma 1 (in the
proof of Proposition 3) apply. Those arguments imply that the principal’s welfare is strictly

                                                        14
decreasing in σ under the optimal target-based rule. In fact, the proof of Lemma 1 shows
that the derivative of the principal’s welfare with respect to σ is strictly lower than −σ
under this rule.
   To compare with the optimal instrument-based rule, we prove the following lemma:

Lemma 4. Consider changing σ while keeping V ar(θ) unchanged. The change in the
principal’s welfare from a marginal increase in σ is strictly higher than −σ under the
optimal instrument-based rule.

Proof. The principal’s welfare under the optimal instrument-based rule can be written as
                                            ∞
                                 σ2                   (s − µ(s))2
                                        Z                               
                               −    +               −             + V (s) φ(s|0, ∆2 )ds.       (76)
                                 2      −∞                 2

Substituting with the structure of the optimal rule yields
                     (Z
                          s∗                     Z s∗∗ 
       σ2                    α2                           (s − s∗ − α)2
                                                                         
                                          2
      − +                   − + V φ(s|0, ∆ )ds +        −               + V φ(s|0, ∆2 )ds
       2                −∞   2                    s ∗           2
                                  Z ∞                         )
                                         α2
                                                
                                +      − + V φ(s|0, ∆2 )ds ,                             (77)
                                   s∗∗    2
                 q
where s∗∗ = s∗ + 2(V − V ). Since ∆ declines as σ rises, it is sufficient to show that the
term in curly brackets in (77) is decreasing in ∆. To evaluate the derivative
                                                                       q      of this term,
define se = s/∆, with se∗ = s∗ /∆ and se∗∗ = s∗∗ /∆, where se∗∗ = se∗ + ∆1 2(V − V ). Using
integration by substitution, the term in curly brackets in (77) can be written as

                                   " 
                                                       q               2
                                                ∗∗
                                            −                −     −
                                                                             #
Z   se∗
    
      α 2
                       Z     se∗∗
                                        ∆(e
                                          s   s
                                              e    ) +   2(V   V )   α
     − + V φ(e
             s|0, 1)de
                     s+             −                                           s|0, 1)de
                                                                          + V φ(e       s
 −∞   2                   se∗                           2
                         Z ∞
                                     α2
                                            
                       +            − + V φ(e     s|0, 1)de
                                                          s.                        (78)
                            se∗∗      2

Since the optimal instrument-based rule selects values for se∗ and se∗∗ to maximize (78), this
rule necessarily satisfies the following first-order condition:
     Z      se∗∗               q             
                            ∗∗
                      s − se ) + 2(V − V ) − α φ(e
                    ∆(e                                         s∗∗ − se∗ )φ(e
                                                 s|0, 1)ds = −α(e            s∗∗ |0, 1) < 0.   (79)
          se∗


The derivative of (77) with respect to ∆, taking into account the Envelope condition, is


                                                             15
thus equal to
                           Z    se∗∗                            q             
                       −                s − se∗∗ ) ∆(e
                                       (e            s − se∗∗ ) + 2(V − V ) − α φ(e
                                                                                  s|0, 1)ds.                (80)
                            se∗


Both terms in the integral are increasing in se, which means that (80) takes the same sign
as
         Z   se∗∗                                 Z     se∗∗                 q             
                           ∗∗
     −               s − se )φ(e
                    (e         s|0, 1)ds                          s − se∗∗ ) + 2(V − V ) − α φ(e
                                                                ∆(e                            s|0, 1)ds.   (81)
          se∗                                         se∗


The first integral in (81) is negative since se < se∗∗ for all se ∈ (e
                                                                     s∗ , se∗∗ ). The second integral
is also negative by (79). Therefore, (81) is strictly negative. It follows that the second
term in (77) is decreasing in ∆, establishing the claim.

    We now proceed with the proof of Proposition 5. By Lemma 4, when σ increases,
the principal’s welfare under the optimal instrument-based rule declines by less than her
welfare under the optimal target-based rule. To prove the claim in Proposition 5, it is thus
sufficient to show that, among these two rule classes, a target-based rule is optimal at one
extreme, for σ → 0, whereas an instrument-based rule is optimal at the other extreme, for
      p
σ → V ar(θ). This is what we prove next.
                                     p
    Take first σ → 0 and thus ∆ → V ar(θ). The same arguments as those used in the
proof of Proposition 3 imply that the principal’s welfare under the optimal target-based
rule approaches V in this limit. Using the representation in (76), the principal’s welfare
under the optimal instrument-based rule approaches
                                                  ∞
                                                              (s − µ(s))2
                                             Z                                             
                                                                                          2
                                lim
                                √                           −             + V (s) φ(s|0, ∆ )ds .
                         ∆→        V ar(θ)     −∞                  2

Note that this expression can only exceed V if µ(s) = s and V (s) = V for all s ∈ R, but
such an allocation would violate the private information constraint (55). Therefore, this
expression must be strictly lower than V . It follows that the optimal target-based rule
dominates the optimal instrument-based rule for σ → 0.
                    p
    Take next σ → V ar(θ) and thus ∆ → 0. The same arguments as those used in the
proof of Proposition 3 imply that the principal’s welfare under the optimal target-based
rule approaches a value strictly lower than − V ar(θ)
                                                 2
                                                      +V in this limit. The principal’s welfare




                                                                    16
under the optimal instrument-based rule approaches
                                              ∞
                                                  (s − µ(s))2
                                         Z                                     
                        V ar(θ)                                               2
                      −         + lim           −             + V (s) φ(s|0, ∆ )ds .                       (82)
                           2      ∆→0       −∞         2

In the limit as ∆ → 0, φ(s|0, ∆2 ) corresponds to a Dirac’s delta function, with cumulative
distribution function Φ(s|0, ∆2 ) = 0 if s < 0 and Φ(s|0, ∆2 ) = 1 if s ≥ 0. Consider the
                                                                                  ∗
principal’s limitingq welfare under an instrument-based threshold specifying s = −α < 0
and s∗∗ = −α + 2(V − V ), where s∗∗ > 0 by Assumption 2. Under this rule, µ(s) = 0 if
s ∈ [s∗ , s∗∗ ] and µ(s) = s + α otherwise, and V (s) = V if s ≤ s∗∗ and V (s) = V otherwise.
Therefore, (82) under this rule becomes equal to − V ar(θ)
                                                        2
                                                            + V . Since the principal’s welfare
under the optimal instrument-based rule must weakly exceed this value, it follows that the
                                                                                    p
optimal instrument-based rule dominates the optimal target-based rule for σ → V ar(θ).


B.2         Proof of Proposition 6
We first study the optimal instrument-based and target-based rules separately and then
compare them.

B.2.1           Optimal Instrument-Based Rule

The program that solves for the optimal instrument-based rule under asymmetric punish-
ments is analogous to that in (7), with constraint (4) replaced by
        "                    2
                                     #                          "                      2
                                                                                                #
    ∞             i                                         ∞            −i
                (µ − θ − α)                                                   − θ − α)
Z                                                       Z
                                                                        (µ
            −               + cV i φ(θ|si , σ 2 )dθ ≥               −                  + cV −i φ(θ|si , σ 2 )dθ,
 −∞                  2                                     −∞                  2
                                                                                                          (83)
and constraint (5) replaced by
            "                  2
                                       #                   Z ∞"            2
                                                                                 #
        ∞
                  (µi − θ − α)                                       i
                                                                       −
    Z
                                                                  (s     θ)
                −                + cV i φ(θ|si , σ 2 )dθ ≥      −            + cV φ(θ|si , σ 2 )dθ, (84)
    −∞                  2                                   −∞         2

for c > 1. To prove that the optimal instrument-based rule is as described in Proposition
6, we follow the same steps as in the proof of Proposition 1:
Step 1. We solve a relaxed program which ignores (83) for i = H and (84) for i = L, H.
Step 2 verifies that the solution to this relaxed program satisfies these constraints.

Step 1a. The analog of Step 1a in the proof of Proposition 1 applies directly without change
and establishes that (83) for i = L must hold as an equality.

                                                      17
Step 1b. The analog of Step 1b in the proof of Proposition 1 applies directly without change
and establishes that µH ≥ µL .

Step 1c. The analog of Step 1c in the proof of Proposition 1 applies, with V L = V and
(23) now replaced by:

                              (µL )2                    (µH )2
                  sL + α µL −        + cV = sL + α µH −        + cV H .
                                                 
                                                                                          (85)
                                2                         2

This equation implies that, up to an additive constant independent of the allocation, the
high type’s welfare satisfies the analog of (24), given by:

                   (µH )2                      (µL )2
       sH + α µH −        + cV H = sL + α µL −        + cV + sH − sL µH .
                                                                  
                                                                                          (86)
                     2                           2

We proceed as in Step 1c in the proof of Proposition 1 to establish that V H = V . Suppose
by contradiction that V H < V . Then it follows from (85) and Step 1b that µH > µL .
Substituting (86) into the objective in (7), the principal’s welfare up to an additive constant
independent of the allocation is equal to

                     (µL )2 1 
              
             1                                               (c − 1)
          s + α µL −
           L
                           − α − sH − sL µH + V + (V − V H )
                                        
                                                                     .                    (87)
             2         2    2                                   2

Consider a perturbation that changes µH by dµH < 0 arbitrarily small and changes V H so
as to keep (85) unchanged:
                                dV H    µH − (sL + α)
                                      =               .
                                 dµH          c
Using the representation in (87), the change in the principal’s welfare from this perturbation
is equal to

  1            dV H (c − 1)   1                               (c − 1)
    α − sH − sL +                   α − sH − sL + [µH − (sL + α)]
                                               
                     H
                               =                                          .               (88)
  2               dµ      2      2                                   2c

We show that this change in welfare is strictly positive. Suppose for the purpose of contra-
diction that this is not the case. Then the right-hand side of (88) must be weakly negative,
which is equivalent to
                               α                    (c − 1)
                                  − 2∆ + (µH − sL )         ≤ 0.                        (89)
                               c                       c
Recall that we have assumed 1 < c < α/2∆. The above inequality therefore requires
µH − sL < 0. However, by Step 1b, if µH − sL < 0, then µL < sL . In this case, we can
perform a perturbation that changes µL by dµL > 0 and changes V H so as to keep (85)

                                              18
unchanged:
                                         dV H   sL + α − µL
                                              =             .
                                         dµL         c
Using the representation in (87) and given µL < sL , the change in the principal’s welfare
from this perturbation is equal to

                             dV H (c − 1)
                    
                   1                                   (c + 1)   α
                s + α − µL −
                 L
                                L
                                          = (sL − µL )         +    > 0.                         (90)
                   2         dµ      2                    2c     2c

It follows that (89) cannot hold. Therefore, the change in the principal’s welfare in (88) is
strictly positive, establishing that V H = V in any optimal instrument-based rule.

Step 1d. The analog of Step 1d in the proof of Proposition 1 applies directly without change
and establishes that µH = µL = 0.
Step 2. The analog of Step 2 in the proof of Proposition 1 applies directly, and in fact it
is strengthened by the fact that c > 1. This step verifies that the solution to the relaxed
problem satisfies the constraints of the original problem.
Step 3. The analog of Step 3 in the proof of Proposition 1 applies directly, and in fact it
is strengthened by the fact that c > 1. Therefore, the optimal instrument-based rule can
be implemented with a maximally-enforced instrument threshold.

B.2.2   Optimal Target-Based Rule

To prove that the optimal target-based rule is as described in Proposition 6, we follow the
same steps as in the proof of Proposition 2:
Step 1. The program that solves for the optimal target-based rule under asymmetric
punishments is analogous to that in (10), with constraint (8) replaced by
                            (Z       "                                    #                )
                                 ∞                           2
                                             (µ − θ − α)
             µi ∈ arg max                                + cV (µ − θ) φ θ|si , σ    2
                                                                                      
                                         −                                                dθ .   (91)
                      µ      −∞                   2

  We follow the same first-order approach as in Step 1 in the proof of Proposition 2,
where equation (27) is replaced by
                                     Z       ∞
                          α−κ+                   cV (π) φ0 (κ − π|0, σ 2 )dπ = 0.                (92)
                                         −∞


Step 1a. Letting λ represent the Lagrange multiplier on (92), the same logic as in Step 1a

                                                        19
in the proof of Proposition 2 yields that λ < 0.
Step 1b. We show that the solution satisfies V (π) = V if π ≤ π ∗ and V (π) = V if π > π ∗ ,
for some π ∗ ∈ (−∞, ∞). The argument is analogous to that in Step 1b in the proof of
Proposition 2, where equations (30) and (31) are now given by

                φ(κ − π|0, σ 2 ) + λcφ0 (κ − π|0, σ 2 ) + ψ (π) − ψ (π) = 0, and        (93)

                                   1    φ0 (κ − π|0, σ 2 )   π−κ
                               −      =                    =     .                      (94)
                                   λc   φ(κ − π|0, σ 2 )      σ2

Step 1c. We show that π ∗ > κ and κ ∈ (0, α). The argument is analogous to that in Step
1c in the proof of Proposition 2, where (32) is now given by

                           α − κ − φ κ − π ∗ |0, σ 2 c V − V = 0,
                                                           
                                                                                        (95)

and the Lagrangian in (33) becomes

                        κ2
                           + 1 − Φ(κ − π ∗ |0, σ 2 )V + Φ(κ − π ∗ |0, σ 2 )V
                                                  
                    −                                                                   (96)
                        2
                           +λ α − κ − φ(κ − π ∗ |0, σ 2 )c V − V .
                                                                


Following analogous arguments as in Step 1c in the proof of Proposition 2, we can prove
the claim by obtaining equation (36).
Step 2. We verify the validity of the first-order approach: we establish that the choice
of κ in the relaxed problem satisfies (91) and therefore corresponds to the agent’s global
optimum.
Step 2a. We show that the agent has no incentive to choose some κ0 6= κ, κ0 ≤ π ∗ . The
argument is analogous to that in Step 2a in the proof of Proposition 2.
Step 2b. We show that the agent has no incentive to choose some κ0 6= κ, κ0 > π ∗ . The
argument proceeds analogously to Step 2b in the proof of Proposition 2. We first establish
that in the solution to the relaxed problem, given π ∗ , κ satisfies κ − π ∗ ≤ −σ. Suppose by
                                                                                 2
contradiction that κ − π ∗ > −σ. Note that by (94) and (36), κ − π ∗ = − σκc . Hence, the
                                                                        2
contradiction assumption implies κc > σ. Substituting κ − π ∗ = − σκc into (95) yields

                                      σ2
                                             
                                            2
                                                    
                           α − κ − φ − |0, σ c V − V = 0.                               (97)
                                      κc

Since the left-hand side of (97) is decreasing in κ and (by the contradiction assumption)

                                               20
κc > σ, (97) requires
                               σ
                                  − φ −σ|0, σ 2 c V − V > 0.
                                                         
                            α−
                               c
Multiply both sides of this equation by σc > 0 to obtain:

                       σ       σ
                                   − σφ −σ|0, σ 2 V − V > 0.
                                                      
                           α−                                              (98)
                        c       c
                          p                                  p
Note that since 0 < σ < V ar(θ), c > 1, and, by Assumption 1, V ar(θ) ≤ α/2, we
                   2
have σc α − σc < α2 . Hence, (98) yields
              


                                       α2
                                                   >V −V.
                                  2σφ (−σ|0, σ 2 )

However, this inequality violates Assumption 2 since σφ (−σ|0, σ 2 ) = φ (1|0, 1). Thus,
given π ∗ , κ satisfies κ − π ∗ ≤ −σ.
   We next establish that the agent has no incentive to deviate to κ0 6= κ, κ0 > π ∗ . Consider
some κ0 > π ∗ that is a local maximum for the agent. By the same reasoning as in Step
2b in the proof of Proposition 2, the difference in welfare for the agent from choosing the
value of κ given by the solution to the relaxed problem versus κ0 is no smaller than

                   α2
                      + Φ − (κ − π ∗ ) |0, σ 2 − Φ κ − π ∗ |0, σ 2 c V − V .
                                                                       
               −
                   2

Since c > 1 and we have shown that κ − π ∗ ≤ −σ, Step 2b in the proof of Proposition 2
implies that the value of this expression is bounded from below by the term in (42) which
is strictly positive. It follows that the agent strictly prefers κ over κ0 .

B.2.3    Optimal Class of Rule

We begin by proving the following lemma, which is analogous to Lemma 1 in the proof of
Proposition 3:

Lemma 5. Consider changing σ while keeping V ar(θ) unchanged. The principal’s welfare
is independent of σ under the optimal instrument-based rule and it is strictly decreasing in
σ under the optimal target-based rule.

Proof. Our characterization of the optimal instrument-based rule yields that the principal’s
welfare under this rule is equal to − V ar(θ)
                                         2
                                              + V , which is independent of σ.
   To evaluate the principal’s welfare under the optimal target-based rule, consider the
Lagrangian taking into account the conditional variance term (which is exogenous and thus

                                              21
excluded from (96)):

                         σ 2 κ2
                                + 1 − Φ(κ − π ∗ |0, σ 2 )V + Φ(κ − π ∗ |0, σ 2 )V
                                                       
                    −       −
                         2    2
                              +λ α − κ − φ(κ − π ∗ |0, σ 2 )c V − V .
                                                                  


The derivative with respect to σ is:
                        "Z                                                                           #
                             ∞                                                ∗ 2
                                 2
                                   σ − z2                         2
                                          
                                                                σ   − (κ  − π  )
                                            φ(z|0, σ 2 )dz + λc                   φ(κ − π ∗ |0, σ 2 ) .
                    
  −σ + V − V                     −     3                                 3
                          κ−π ∗      σ                                 σ

The first term is strictly negative. Using (94) and (36) to substitute in for λ and κ − π ∗ ,
the sign of the second term is the same as the sign of
                                                                "              2 #  2
                Z   ∞                                                       2
                                                                                              
                                                                            σ         σ
                             σ2 − z   2
                                            φ(z|0, σ 2 )dz − κc σ 2 −                       2
                                        
            −                                                                      φ − |0, σ .       (99)
                    2
                − σκc                                                       κc        κc

This expression is identical to that in equation (43) in the proof of Lemma 1, except that
κ has been replaced with κc. Analogous steps as in that proof then yield that the sign of
(99) is strictly negative, completing the argument.

   We now proceed with the proof of Proposition 6. By Lemma 5, welfare under the
optimal instrument-based rule is invariant to σ, whereas welfare under the optimal target-
based rule is decreasing in σ. To prove the claim in Proposition 6, it thus suffices to show
that, among these two rule classes, a target-based rule is optimal at one extreme, for σ → 0,
                                                                             p
and an instrument-based rule is optimal at the other extreme, for σ → V ar(θ). This
can be established using the same arguments as in the proof of Proposition 3.


B.3     Proof of Proposition 7
As shown in the paper, the environment of Section 5.3 is mathematically equivalent to our
baseline environment. Consequently, Proposition 7 follows directly from Propositions 1-3
in Section 3.




                                                           22
