                       NBER WORKING PAPER SERIES




                     MONTE CARLO FOR ROBUST REGRESSION:

                            THE SWINDLE UNMASKED


                              Paul W. .Holland*




                       Working Paper No.          10



        COMPUTER RESEARCH CENTER FOR ECONOMICS AND MANAGEMENT
                                                              SCIENCE
                  National Bureau of   Economic Research,      Inc.
                            575 Technology Square
                        Cambridge, Massachusetts       02139




                               September 1973




                      Preliminary: not for quotation

     NBER working papers are distributed informally and in limited numbers
for comments only. They should not be quoted without written permission.
     This report has not undergone the review accorded official
                                                                NBER
publications; in particular, it has not yet been submitted for approval by
the Board of Directors.

     *NBER Computer Research Center. Research supported in
Science Foundation Grant GJ-1154X2 to the National         part by National
                                                       Bureau cf Economic Research, Inc.
                                    Abstract


    Gives  an alternative derivation of a tlonte Carlo method that has been
used to study robust estimators. Extensions of the technique to the regressthn
case are also considered and some   computational points are briefly mentioned.




                                                                                  (




                                                                                  (
1.   Introduction

     In this paper, we      discuss a method for achieving more   accuracy from a
Monte Carlo study than is possible from
                                               simple random sampling.. Such Monte
Carlo "swindles" are important in the large scale use of Monte Carlo studies.

The particular method we discuss here has been described before in Relies
                                                                          [1970]
and Andrews et al. [1972], but their approaches are somewhat different from

the one we employ. A deeper
                                  understanding of the method and its properties

is gained by having alternative derivations available.

     The particular problem we        consider is the following. We begin with the
familiar linear regression problem


                 y = X8 + e
                                                                             (1-1)

where y is Nxl, X is Nxp, 8 is pxl and e is Nxl. We furthermore
                                                                assume that
the components of e, e., are independent
                                               and identically distributed random
variables with common density



                 :•   f()                                                   (1-2)

where f is assumed to be symmetric abou 0, i.e., f(-.x) =
                                                                f(x).   The linear
regression problem is to estimate 8 using y and X. Let 8 denote a generic

estimator of 8. Sometimes its dependence on y will be denoted by (y).

     There are two notions of invariance that will
                                                         be important in the rest
of this paper.


Scale Invariance

     An estimator 8(y) is said to be scale invariant if


                 8(c y)     C   (y)                                         (1-3)

for any constant, c.
                                         -2—




Rerjrssion Invariance
       An estiiator (y) is   said   to be regression (on X) invariant if

                  (y +.x1)    (y) + y                                      (1-4)


for any pxl vector, •y.

       We shall restrict our discussion to estimators, , which are both regres-

sion and scale invariant. The problem of main concern is to study


             •    Cov) =            -     -                                (1-5)

However because we have restricted attention to.regression and scale invariant
estimators we have


            E(y) -            - ))T     2E((Y -            X)T) =

             •                                                             (1-6)

Thus we. may assume without loss of generality that        0 and 2

       In order to compute Cov0(B), we must often resort to a Monte Carlo study.

The swindle we will consider is designed for such an investigation. When

p      1 and x.   1, the regression problem reduces to the "location" problem in

which we are estimating the center of a symmetric distribution. In

the    location case (1-5) reduces to the variance of .
       We shall divide our discussion of the swindle into four    cases: Location

with   Gaussian errors, Location with Gauss/independent errors, Regression with
Gaussian errors and Regression with Gauss/Independent errors.
2. Location with Gaussian Errors
      In this case we are concerned with computing


                 Var0() =         E0(2)                                                      (2-1).
for a general location and scale invariant estimator under the assumption that

f is the unit Gaussian density. However, when f is the Gaussiar distribution,

we know what the best location (regression) and scale invariant estimator is ——

our friend , the     sample average. Hence, instead of computing the variance of

,   we shall   try to compute the excess of the variance of over that of ,.                          We

now derive the important formula that allows us to                    do this.
Theorem 1: If       is y location and scale invariant estimator, then under
unit Gaussian errors we have

                 Var(s) = Var() + E( -                )2                                     (2-2)
                                                p.

                               = Var()    +
                                              E((5:                                          (2-3)

where 2 is the usual           unbiased estimate of o.

Proof:   We begin with (2—2) and then derive (2-3) from it. We have

           ((y))2    =
                          ((y - ) + )2 = ((y - ))2 + 2                               + ()2
                                                                                             (2-4)
Hence


           Var(s)
                         E0(2)       E0()2 + 2 E0( (y -           )) +     E0(B(y
                                                                                             (2—5)
But andy -          are   independent so that              and (y -     ) are    independent and
hence                      .




                 E0(      (y -    )) = 0
                                               -4-


Also E0G)2       Var() and (y -            =    -     so that Vr()    =   Var() + E( -
which was to be proved. To prove (2—3) we need to show that

                  E( -        =
                                                                                    (2-6)

This is done as follows

•                 E( -        •= E(S(           Y))2)                               (2-7)


Ft S     is independent of                 so that S2 is independent of(Y        )') and
hence (2—7) equals


                  E(s2) E( Y)2)                2 E(s        )2
                                                                                    (2-8)


Since we have assumed G2      =   1, (2-6) follows immediately. OED
        It should be noted that (2-2) only           requires    to be location invariant

while (2-3) requires both location and scale invariance.

        it is quite simple to use Theorem 1 to get a Monte Carlo swindle for the

variance of a location and scale invariant estimator, B. If we were going to

    use naive random sampling to estimate Var(s) we would draw repeated independent

samples of size N, y =
                             (y1,   ...,
                                           YN)T , compute (y) for each sample and
    average the value of 2 over the replications. But from (2—2) we see that we

    may also estimate Var(s) by computing (y) and . from each sample, averaging

    the value of (   — )2   over the replications and then adding N1 =           Var()   to

    the result, i.e.,


                     estimated Var(s) =     N+       Monte Carlo Average (   —



                                                                                    (2-9)

    In order to remove the first order effect of N when looking at different

    sample sizes it is customary to estimate N Var(B) instead This leads to
                                          — 5-.




                 estimated       (Var(s)) =   1   + rI(Monte Carlo Average (    - )2)
                                                                                   (2-10)
    What is the nature of this swindle? The main point is that it concentrates

the slow Monte Carlo convergence on the excess of the Var(B) over Var() rather

than allowing this to effect all of Var(S). This implies that if B is a

very efficient estimator (has a small excess of variance) then the swindle will

be more effective than if 5 has a large excess. This is because the smaller

the excess, the smaller the percentage of the Var(S) that is estimated by

Monte Carlo averaging. From (2—4) and (2—5) we see that the swindle utilizes

the theory of the Gaussian distribution to get exact resu1s for two              of   three
pieces of E(2)   ; these   are


                 E( (y -     )) = 0                                                (2-11)

                                                       .
         and     E(5)2 =   f11   .
                                                                                   (2-12)

Monte Carlo is then used to estimate the third piece.

    We nay use (2-3) to get more of a swindle via the estimate:


                 estimated N(Var()) =     1       + N(Monte Carlo Average(-( -


                                                                                   (2-13)
However, if N is at all appreciable, 1/S2           will   not, differ much from unity so

that this swindle should not significantly improve upon the earlier one unless

the sample size, N, is quite small. This agrees with the folklore.
                                                -6-


3. Location with     Gauss/Independent Errors
      Critical   to the swindle in the Gaussian case was the ability to evaluate

(2—li) and (2—12) exactly. These calculations lean heavily on properties of

  and S2 in the Gaussian distribution. It is not clear how to successfully

geiera1ize this to arbitrary symmetric unimodal densities, f. However, the

class of distributions given by


                 y. =    uS/v.                                                       (3-1)


where u. is unit Gaussian and v. i a positive random variable indeoendent of
       1                      1
u.., is such that an expression analogous to (2-li) cah be evaluated exactly
and one analogous to (2-12) can be evaluated exactly in some cases and partially

evaluated in all cases. This leads to a swindle that is not as effective as

the one for the Gaussian case, but which is better than simple random sampling.

The family of densities associated with (3-i) is a generalization of the t-

family and contains such meibers as: Cauchy, t, double exponential, logistic

and scale mixtures of Gaussian densities. Conditionally, gven V.                y.    is

Gaussian with mean zero and variance vT2. We may regard y. as Gaussian with

a random   scale; Andrews and Mallows [1973] give conditions under which a
density has the representation (3.-i).
      Let v = (v1,   ...,
                            vs);     then the key idea is that   given v,   we are   back in
much the same situation as we were in the pure Gaussian case. The only real

differences are (1) now      the variances are unequal and (2) we       must eventually

integrate over the      density of v.      We   let
                                 E v2.y.

                  .= (v) =                                                           (3-2).
                                   Z
                                       1
and

                     =   S2(v)   =         Z v(y. -   (v))2.                         (33)
                                                    -7—




Note that (v) and S2(v) can't be computed in real data since v is not an

observable but in a Monte Carlo study in which v is generated along with

u =          ...,
      (u1,          uN)T to produce y, v will be available.
       Now instead of knowing the best location-scale invariant estimator of

for the error distribution given by (3-1), we know an even better estimator,

(v). It is better than the best location—scale invariant estimator because

it uses unobservable information. Thus we will try to compute the excess

variances of        over that of (v). The formula for this is given in the next
theorem.


Theorem 2: If          is           location and scale invariant estimator, then if the

errors are given y_ (3-1) we have


                    Var(s) =        Var((v))      + E( -   (v))2                      (34)

                                =   Var((v)) + E( (v))2

Proof:

We first show (3-4) and then derive (3-5) from it. Given v we may compute
  =
      (v)    so that we have


                     ((y))2 =         ((y - ))2       + 2 (y -     )y   + ()2


and hence taking conditional expectations we get


              E[(B(y))2v] =          E[((y    -
                                                   ))21v] +2 E[ (y -      )fv] + E[()2tvJ
However given v, y and y -            y   are independent so the middle term vanishes.

Then taking expectations over v we get (3-4). To prove (3—5) we need to show

that


                            -
                                y(v))2    =       _______
                                                             -8--




We have


                 -
                     y(vfl2v]       =
                                           E[S2(V) si)Iv]                                              (3-6)

but given v, S(v) is independent of (y —                            (v))/S(v)   so that S2(v) is

independent of                             Hence (3-6) equals


                 E[S2(v)Iv] E[ )V))2Iv]

                 -
                 —          - (v))2                  V
                           S2(v)

Ting      expectations over v proves the result. OED

     To us Theorem 2 to get a swindle we need a little more work, namely we

need to be able to compute Var((vfl. In general this is difficult, but the

following result helps a little.

Theorem 3: Var((v))           E(                 )       .                                             (3-7)   (
                                   z       v.2
                                   i-I
Proof:    Condition on v.

     Depending on what the distribution of V. is, the simplification implied

by Theorem 3 may or may not lead to an exact solution. When qv has a chi—

square distributior with q degrees of freedom, then we may obtain an exact

result for Var( (v)). It is:

                       1
                 E(
                                       q    -2                                                         (3-8)

                      ii
     The swindle flOW comes in two forms depending on whether or not we have

an exact formula for Var((v)). As before we                             give    it for estimating N Var(s).

If Va•r((v)) is known then use:

                                  A                                                                A           (
             Estimated     t (Var(s)) = N Vr (v) + N(Monte                        Carlo Average(. — (v))2)
                                                                                                       (3-9)
                                                -9-




If Var(v)) cannot be found exactly then use


       Esttted 1 (Var(s)) = N(t1onte           Carlo Average(
                                                                   Zv1
                               + tl(oite    Carlo Average( —         (v))2)             (3-10)


As before, some extra swindle may be gained from using (3-5) rather than (3-4),

but unless N is small the gains are not likely to be appreciable.

       In this case the swindle has two things going against it. 1'ost obviously,

if
      E()        can't be computed exactly, and must be estimated by simple random

sampling    then not only are we       using Monte Carlo to estimate the excess variance,
we    are also    using it   to estimate   a portion       of   Var((v)). Secondly,   even if
Var((v))     can be computed exactly,         it may not be a very large      portion   of the
total variance of .          This   is because   (v) is a better estimator than         any
location    and scale invariant estimator, i.e.,


                    Var((v)) < Var(s)
                                                                                        (3-11)

for   any such estimator . Thus            relative   to   any given location—scale invariant
estimator    Varc())     may be very small, even relative to the best such estimator.

Because of these problems, the swindle should not be as effective here s it

is for the Gaussian case.
                                            -id-




4. The Regression Case

    With     the preparation given in      the previous two sections we may move easily

to the regression case. We first treat the case of Gaussian errors and then

Gauss/Independent errors. The theorems are stated without proof since they

are completely analogous to the corresponding ones for the location se.

Gaussian Errors

     If the errors are Gaussian, then we have the following basic result.

Theorai
_____      4: —
              If               _______ — ______ ______
                     is any rearession and scale invariant estimator and          the

'sual least sguares.estimator, then

                   Cov()       Cov(Ls) + E(( -          - LS                 (4-i)

                           =             + E(()( -          -                (4-2)
                               Cov(Ls)
where S2 is the usual unbiased estimate of2 based on the least          squares

residual mean-square.


     As was true for Theorem 1, (4-1) only requires regression invarianc,

while (4-2) requires both regression and scale invariance.

     Since Cov() is given exactly by


                        Cov(Ls) =    (xTxy                                   (4-3)

we are led to the following Monte Carlo s•,indle formula.


     Estimated Cov()           (XTXY1 + Monte Carlo Average(( -          -

                                                                             (4..4)

As before when N is small (actually when N-p is small) there        may be sc—e

additional    advantage to basingthe swindle on (4-2) rather than (4-1), but              (

otherwise •the     rnprovement over (4—4) is not likely to be noticeable.
                                                —11—




Gauss/Independent Errors

     When the errors have the structure given by (3-1), we
                                                           may define, for
each v, these quantities:


                     (v) (Xv2>x)xT<v2>,                                            (45)
                     y(v)      X(v)
                                                                                   (4-6)

         arid


                     S2(v)      (t-pr1 E (y. -    y.(v))2                          (47)
Where <v2> is the diagonal matrix based on v2 =
                                                              (V,      ...,
Then we have the following theorem.

Theorem 5:      If     is y regression and scale invariant estimator and          the
errors   are given           (3—1) then


                     Cov() =     Cov((v))   +   E((    -
                                                           (v))(   -
                                                                       (v))T)      (4-8)

                                 Cov((v))   +
                                                E((s)( (v))( (v))
                                                              -               -
                                                                                   (4-9)

     In order to use Theorem 5 to get a swindle we need to be able to
                                                                      compute
Cov(8(v)). As before, this is generally difficult, but can be partially

accomplished from the following result.

Theorem 6: Cov((v)) = E(XT<v2>x)_.
                                                                                  (4—10)
Proof:   Condition on v.


     There do not appear to be too many cases where E(XT<v2>x)_l
                                                                 can be computed
exactly so that either approximations or onte Carlo estimates must be used.

Again we get two swindle formulas depending on what we use for Cc.vC(v)). If

Cov((v)) is known or can be well approximated, then use
                                         -12-




     Estimated Cov() Cov((v)) + Monte Carlo Averace(( -                          - (v))T)
                                                                                 (4-11)
If E(X'<V2>X) must be estimated by Monte Carlo use


     Estimated Cov() = Monte Carlo Average((XT<v2>Xyl) +                     —

                                                                                 (4—12)
                           Monte Carlo   Average(( — (v))( — (v))        )



     In   the regression   case, because we   will   usually have to use Monte Carlo

is estimate E(XT<v2>X)_l it is likely that the siindle will not produce much

of an improvement over simple random sampling.




                                                                                            (




                                                                                            (
                                                  —13-




 5. Some Final Remarks

        The basic result that underlies all of
                                               this                           is (2-2). This is
 a   special case of a general result that holds for              the best location invariant
 estimator   for any given distribution (some conditions may be necessary).
This result is given but not proved in the next theorem.
Theorem 7: If     is any location invariant estimator and                     is the best
 location invariant estimator, then

                  Var(s)        Var(0) +        E( -                                    (5-1)
       It is evident that if 0.can             be ccputed easily, and 'If Var(s) can be
cOmputed   exactly then (5—1) provides             a basis for a Monte Carlo     swindle. Un-
fort.uately,    neither         nor       its va!'ianc can be easily Computed for many cases
outside of    the Gaussian. The use of the
                                                         3(v) is a compromise for this state
of affairs.


Role of Configurations

       In the discussion of this Monte
                                                  Carlo swindle by Andrews,     et al., [1972]
the concept of a Mconfiguration
                                              plays a prominent role.       In the development
here no such concept arises. We comment on this briefly now.

       A configuration is a sample (the y's) adjusted in a particular
                                                                      way. One
important example of a configuration is


                       -
                  Y        X3                                                          (5-2)
t!e note that a   configuration is unchanced             by the addition of Xy to y for any
choices of y.     A scale invariant COnfigUration
                                                              is given by


                  (y   - XLJ/S        .
                                                                                       (53)
                                           -14-



     The reader is referred to Andre,.s, et al., [1972] for the use of configur-
ations in the derivation of the swindles discussed in the previous sections.
     In our derivation here there is a place for regression or regression and
scale invariant configurations, but they are not central to the swindle as
such, rather they may be used to make some of the comDuting more efficient.
In (4—4) we may compute the Monte Carlo Average in one of two ways:

             Monte Carlo Average of ((      -             -                               (5...4)

      or              "
                                        ((y -       X3)B(y - XEJT)                        (5-5)

If   we   are going to compare several estimators then from           a computational stand

point     it makes sense to compute LS'     then      form the    configurations,   y -

and then compute          on these rather than      on y. This saves a large number of

subtractions. If a regression and scale swindle is going to he used then

raLhr than


             Monte Carlo Average of             -             -                           (5—6)

it   is more efficient to form the configurations given by (5—3) and compute

(5—6) via

                                         -'y- X ,y-
             Monte Carlo Average of ((               LS            S)T)                   (57)

Similar    remarks hold for the     configurations     that arise from the swindle in

the non-Gaussian case.
