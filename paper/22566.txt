                              NBER WORKING PAPER SERIES




                         PREDICTING EXPERIMENTAL RESULTS:
                                 WHO KNOWS WHAT?

                                      Stefano DellaVigna
                                          Devin Pope

                                      Working Paper 22566
                              http://www.nber.org/papers/w22566


                     NATIONAL BUREAU OF ECONOMIC RESEARCH
                              1050 Massachusetts Avenue
                                Cambridge, MA 02138
                                    August 2016




We thank Dan Benjamin, Jon de Quidt, Emir Kamenica, David Laibson, Barbara Mellers, Katie
Milkman, Sendhil Mullainathan, Uri Simonsohn, Erik Snowberg, Richard Thaler, Kevin Volpp,
and especially Ned Augenblick, Don Moore, Philipp Strack, and Dmitry Taubinsky for their
comments and suggestions. We are also grateful to the audiences at Bonn University, Frankfurt
University, the London School of Economics, the Max Planck Institute in Bonn, the Wharton
School, at the University of California, Berkeley, at the 2016 JDM Preconference, the 2015
Munich Behavioral Economics Conference and at the 2016 EWEBE conference for useful
comments. We thank Alden Cheng, Felix Chopra, Thomas Graeber, Johannes Hermle, Jana
Hofmeier, Lukas Kiessling, Tobias Raabe, Michael Sheldon, Patricia Sun, and Brian Wheaton for
excellent research assistance. We are also very appreciate of the time contributed by all the
experts, as well as the PhD students, undergraduate students, MBA students, and MTurk workers
who participated. We are very grateful for support from the Alfred P. Sloan Foundation (award
FP061020). The views expressed herein are those of the authors and do not necessarily reflect the
views of the National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.

© 2016 by Stefano DellaVigna and Devin Pope. All rights reserved. Short sections of text, not to
exceed two paragraphs, may be quoted without explicit permission provided that full credit,
including © notice, is given to the source.
Predicting Experimental Results: Who Knows What?
Stefano DellaVigna and Devin Pope
NBER Working Paper No. 22566
August 2016
JEL No. C9,C91,C93,D03

                                           ABSTRACT

Academic experts frequently recommend policies and treatments. But how well do they anticipate
the impact of different treatments? And how do their predictions compare to the predictions of
non-experts? We analyze how 208 experts forecast the results of 15 treatments
involving monetary and non-monetary motivators in a real-effort task. We compare these
forecasts to those made by PhD students and non-experts: undergraduates, MBAs, and an
online sample. We document seven main results. First, the average forecast of experts
predicts quite well the experimental results. Second, there is a strong wisdom-of-crowds
effect: the average forecast outperforms 96 percent of individual forecasts. Third, correlates of
expertise---citations, academic rank, field, and contextual experience--do not improve forecasting
accuracy. Fourth, experts as a group do better than non-experts, but not if accuracy is defined as
rank ordering treatments. Fifth, measures of effort, confidence, and revealed ability are
predictive of forecast accuracy to some extent, especially for non-experts. Sixth, using these
measures we identify `superforecasters' among the non-experts who outperform the experts
out of sample. Seventh, we document that these results on forecasting accuracy surprise the
forecasters themselves. We present a simple model that organizes several of these results and
we stress the implications for the collection of forecasts of future experimental results.


Stefano DellaVigna
University of California, Berkeley
Department of Economics
549 Evans Hall #3880
Berkeley, CA 94720-3880
and NBER
sdellavi@econ.berkeley.edu

Devin Pope
Booth School of Business
University of Chicago
5807 South Woodlawn Avenue
Chicago, IL 60637
and NBER
devin.pope@chicagobooth.edu




A randomized controlled trials registry entry is available at
https://www.socialscienceregistry.org/trials/731
1    Introduction
An economist meets a policy-maker eager to increase take-up of a program. The economist’s
recommendation? Change the wording of a letter. Later on, the economist advises an MBA
student to emphasize a diﬀerent reference price in the pricing scheme of the MBA student’s
company. At the end of the day, during oﬃce hours, the academic counsels a student against
running a particular arm of an RCT: ‘the result will be a null eﬀect.’
    Interactions such as these are regular occurrences, especially as economists are increasingly
tapped for advice. A common thread runs through the three interactions: the expert advice
relies on the forecast of a future research finding. In the policy-maker interaction, the expert is
guessing, based on past experience, that the suggested wording will increase take-up more than
other equally-expensive interventions. A similar guessing process underlies the other advice.
   These interactions lead to an obvious question: How well can experts predict experimental
results? The answer to this question is critical to navigate the trade-oﬀ between following
expert advice or choosing broad experimentation which can be time-consuming and costly.
    This naturally leads to a second group of questions: Which forms of expertise lead to more
accurate forecasts? Is it having deep experience and recognition in a field (vertical expertise)?
Or having worked on a particular topic (horizontal expertise)? Or is it knowing the specific
setting (contextual expertise)? Do experts outperform non-experts? Does the answer depend
on the definition of accuracy? And is it enough to poll one or two experts, or should one poll
a group, even though it may be time consuming?
    These questions do not have comprehensive answers, since forecasts of experimental results
are not typically recorded. In the absence of this evidence, we may depend too much on
informal forecasts, relying on the wrong experts, or conversely under-utilizing the experts.
   In this paper, we use data from a large experiment, and associated expert forecasts, designed
to provide evidence on the questions above in one particular setting. We compare the relative
eﬀectiveness of 18 treatments in a real-eﬀort online experiment with nearly 10,000 subjects,
analyzed in detail in DellaVigna and Pope (2016). The large sample size of about 550 subjects
per treatment ensures precision in the estimates of the treatment eﬀects.
    As part of the design, we survey a group of 314 experts, including behavioral economists,
standard economists, and psychologists. The experts are identified out of a group of partici-
pants to behavioral conferences. We provide the experts with the results of three benchmark
treatments with piece-rate variation to help them calibrate how responsive participant eﬀort
was to diﬀerent levels of motivation in this task. We then ask them to forecast the eﬀort par-
ticipants exerted in the other 15 conditions including monetary incentives and non-monetary
behavioral motivators, such as peer comparisons, reference dependence, and social preferences.
The treatments only diﬀer in essentially one paragraph in the instructions, facilitating the
comparison across treatments and thus the expert forecasts. Of the 314 experts contacted, 208


                                                1
provided a complete set of forecasts. The broad selection of experts and the high response rate
enables us to study how expertise influences forecasts of experimental results.
   In addition to the academic experts, we also survey 147 PhD students in economics to study
vertical expertise further. We also collect forecasts made by 158 undergraduate students, 160
MBA students, and 762 online workers in the sample in which we ran the experiment.
   We document seven main results. First, the average forecast among the 208 academic
experts is remarkably informative about the actual treatment eﬀects. Across the 15 treatments,
the correlation of the average forecast with the actual outcome is 0.77, and the average absolute
deviation between the average forecast and the outcome is just 5 percent of the average score.
   A policy-maker, a firm, or an advisee, though, will not typically have the benefit of taking
the average for a large set of expert forecasts, and will typically have the opinion of one expert,
or a few experts. How do individual experts do?
    We document a large diﬀerence in accuracy between the average forecast and individual
forecasts, our second result. The average absolute error in individual forecasts is 8 percent
of the score, compared to 5 percent for the average forecast. Indeed, the average forecast
outperforms 96 percent of individual forecasts. The comparison is equally striking using other
measures of forecast accuracy like sum of squared errors and correlation.
    What explains this large ‘wisdom-of-crowds’ eﬀect? We show that the diﬀerence between
average and individual accuracy does not hold in each treatment: in some treatments the
majority of experts outperform the average expert. Still, there is enough idiosyncratic noise
in the forecasts that, averaging over enough treatments, the mean outperforms nearly every
individual expert. We also show that taking the average forecast of just 5 experts leads to a
large improvement in accuracy over individual forecasts.
    Thus, contacting multiple experts has first-order benefits for forecasting accuracy. Still, so
far we have treated experts as interchangeable. Asking the ‘right’ expert may erase most of the
gains from averaging. We thus consider the impact of vertical expertise–academic rank and
citations–, horizontal expertise–field of expertise and having written a paper on the topic of
the treatment–, and contextual expertise–knowledge of the experimental setting.
   Our third finding is that none of these measures of expertise improve forecasting accuracy.
Full professors are, if anything, less accurate than assistant professors. Similarly, having more
Google Scholar citations is associated with lower accuracy. Thus, vertical expertise does not
appear predictive of accuracy. Our measure of horizontal expertise, which involves a detailed
coding of whether a given expert has worked on a particular topic, is orthogonal to accuracy,
controlling for expert and treatment fixed eﬀects. We also find no eﬀect of expertise in diﬀer-
ent sub-fields, such as psychology, behavioral economics, or applied microeconomics. Finally,
experience with the online sample (contextual expertise) does not increase accuracy.
   The findings for the sample of PhD students are similar. Consistent with the null eﬀect of
vertical expertise, PhD students are at least as good as the academic experts. Furthermore,

                                                2
confirming the null eﬀect of horizontal expertise, specialization in behavioral economics does
not improve the accuracy of the PhD students.
   Thus, various measures of expertise do not increase accuracy. Still, it is possible that
academics and academics in training (the PhD students) share an understanding of incentives
and behavioral forces which distinguish them from the non-experts. We thus consider forecasts
by undergraduate students, MBA students, and an online sample. These forecasters have
not received much training in formal economics, though some of them arguably have more
experience with incentives at work (the MBAs) and with the context (the online sample).
    Do non-experts make worse forecasts? The answer, our fourth finding, depends on the
definition of worse. By the measure of accuracy used so far–absolute error in forecasts–these
groups indeed do worse. The undergraduate students are somewhat less accurate, MBA stu-
dents are significantly less accurate, and online forecasters in the MTurk sample do much worse.
These results are similar for the squared error measure of accuracy. When making forecasts
about magnitudes of the experimental findings, yes, non-experts do worse than experts.
   Yet, while the above measures of accuracy were the main ones we envisioned1 , they are not
always the relevant ones. In our motivating examples, the policy-maker, the businessperson,
and the advisee may be looking for a recommendation of the most eﬀective treatment, or for
ways to weed out the least eﬀective ones. From this perspective, it is not as important to get
the levels right in the forecasts, as it is to get the order right. We thus revisit the results using
the rank-order correlation between the forecasts and the results as the measure of accuracy.2
    Rank-order correlation does not reverse the findings on vertical, horizontal, or contextual
expertise: the three forms of expertise do not help academics rank treatments better. However,
this metric drastically changes the comparison between experts and non-experts: undergrad-
uates, MBAs, and even MTurk workers rank treatments as well as the experts. Across these
samples, the average individual rank-order correlation with the realized eﬀort is about 0.4 and
the wisdom-of-crowds rank-order correlation is about 0.8. In fact, the wisdom-of-crowds rank-
order correlation by the online sample is a stunning 0.95 (compared to 0.83 for the experts).
   How is this discrepancy possible? We show that the non-experts, and especially the online
sample, are much more likely to be oﬀ in the guess of the average eﬀort across the 15 forecasts.
This oﬀset in levels impacts the absolute error, but not necessarily the rank order. This
result is consistent with psychological evidence suggesting that people struggle with absolute
judgments, but are better at making relative judgments (Laming, 1984; Kahneman, Schkade,
and Sunstein, 1998).
  So far, we found that expertise does not help much with forecasts. The fine-grained ex ante
measures of expertise do not increase forecasting accuracy, and experts as a group diﬀer from
   1
     In our pre-registration, we mention three measures of accuracy: absolute error, squared error, and number
of correct answers within 100 points of the truth (more on this below).
   2
     We deduce the ranking of treatments from the forecasts in levels.


                                                      3
non-experts only if the accuracy is about the levels, as opposed to the rank order, of treatments.
If expertise does not help much, are there other ways, then, to discriminate among forecasters
for accuracy? We consider measures of eﬀort, confidence, and revealed ability.
    Our fifth result is that such measures can be predictive of expert accuracy, but with impor-
tant caveats. The predictability mostly holds among non-experts and, while generally strong
for the absolute error measure, it is weak for the ordinal rank measure.
    We measure eﬀort in forecasting with the time taken for survey completion and with click-
throughs to the trial task and the instructions. The evidence is mixed. For the online sample,
longer time taken improves accuracy by the absolute error measure. There is much less evidence
for the other samples, and the relationship flattens or flips sign using the rank-order correlation.
Clicking on the trial or instruction does not have a discernible impact.
   We also measure confidence: each forecaster indicated the number of forecasts which they
expect to get right within 100 points. This measure is predictive of accuracy in levels among
PhDs, MBAs, and online workers. Respondents, thus, are aware of their own accuracy, to some
extent. Confidence is instead essentially uncorrelated with the rank-order measure of accuracy,
perhaps because we elicited confidence using a cardinal, not ordinal, measure of accuracy.
    We then construct a measure of revealed accuracy that captures both eﬀort and ability. We
test if accuracy in the forecast of a simple incentive-based treatment predicts accuracy in the
other treatments. This variable is remarkably predictive: a 100-point increase in accuracy in
the incentive treatment increases accuracy in other treatments by on average 30 points for the
non-expert samples and 9 points for the expert sample. This measure remains predictive, even
though less strongly so, of accuracy as measured with rank-order correlation. The measure of
revealed forecasting ability predicts accuracy also when constructed using other treatments,
suggesting that there is nothing special about the incentive treatment.
   Thus, while ex ante proxies of expertise are not helpful in our setting, other measures–
eﬀort, confidence, and especially revealed forecasting ability–are generally predictive of accu-
racy measured with absolute error. Can these measures then help identify ‘superforecasters’
(Tetlock and Gardner, 2015) among the non-experts? We use simple linear regressions with
an K-fold method to obtain out-of-sample predictions, and focus on absolute error since the
non-experts already equal the experts according to the rank-order measure.
    Our sixth result is that it is indeed possible to identify ‘superforecasters’. The top 20 percent
of undergraduates and PhD students identified with this procedure outperform at the individual
level the sample of experts by 15 percent. The outperformance is even more striking when
using the wisdom-of-crowds measure. We also identify ‘superforecasters’ within the MTurk
sample who parallel the accuracy of academic experts. Among the academic experts, instead,
there is a more limited improvement in accuracy from this procedure.
   Our seventh and final result addresses a meta-question: Did we know all this already? In
the spirit of the forecasting idea, we asked the experts to predict the accuracy of diﬀerent


                                                 4
groups of forecasters. The expert beliefs in this regard are systematically oﬀ target. The
experts expect high-citation experts to be significantly more accurate where, if anything, the
opposite is true. They also expect a diﬀerence by the field of the forecaster and lower accuracy
for PhD students, counterfactually.
    These results, while just a first step, draw out implications for increasing accuracy of fore-
casts of research findings. Clearly, one ought to elicit forecasts from multiple people. Further,
experts may not necessarily oﬀer a more precise forecast than a well-motivated audience, and
the latter sample is easier to reach. One can then screen the non-experts based on measures
of eﬀort, confidence, and accuracy on a trial question. We conjecture that more opportunities
to make forecasts, and see the feedback, could lead to significant improvements in forecasting
ability, and to beliefs about expertise that are more aligned with actual accuracy.
   Can we make sense of our key findings with a simple model? We assume that forecasters
observe a noisy signal of the truth, with some forecasters receiving more precise signals than
others. The heterogeneity in informativeness is motivated by the result that forecasters who
do better in one treatment also do better in other treatments.
   We calibrate the model based on five moments: three variances, mean accuracy, and the
cross-treatment correlation in accuracy. Our calibration implies that the non-experts on aver-
age have higher idiosyncratic noise in their signals, and also more heterogeneity, compared to
experts. Due to the higher heterogeneity, some non-experts receive more precise signals than
the experts (the ‘super-forecasters’). We can also approximately match the diﬀerences between
experts and non-experts in absolute error versus rank correlation.
    We explore complementary findings in a companion paper (DellaVigna and Pope, 2016),
focusing on what motivates eﬀort and providing evidence on some leading models in behavioral
economics. For each treatment, we analyze the eﬀort choice of the subjects and the average
forecast of the academic experts. The companion paper does not consider measures of accuracy
of forecasts, diﬀerences in expertise, forecasts by non-experts, or beliefs about expertise.
    This paper is related to several literatures that span diﬀerent academic disciplines, includ-
ing psychology, in addition to economics. The Good Judgment Project elicits forecasts by
experts on national security topics (Tetlock and Gardner, 2015). We find in our setting sig-
nificant parallels to their findings, including the fact that, while it is hard to identify good
forecasters based on ex ante characteristics, it is possible to do so using measures of accuracy
on a subsample of forecasts (Mellers et al., 2015).
    Related to our paper is the work on wisdom of crowds. At least since Galton (1907), social
scientists have been interested in cases in which the average of individual forecasts outperforms
nearly all of the individual forecasters (e.g. Surowiecki, 2005). We show that the wisdom-of-
crowds phenomenon does not apply to each treatment: in several of the treatments, the average
forecast is outperformed by a majority of the forecasters. It is when considering all treatments
jointly that the evidence strongly supports the wisdom of crowds.

                                                5
   Economics also has a rich tradition of studying prediction accuracy, including in macroeco-
nomics and finance (e.g., Cavallo, Cruces, and Perez-Truglia, 2016; Ben-David, Graham, and
Harvey, 2013). More closely related is the work on the value of aggregating predictions using
predictions markets (Wolfers and Zitzewitz, 2004; Snowberg, Wolfers, and Zitzewitz, 2007).
    There has been some work in economics that attempts to elicit opinions from academic
experts. For example, the IGM Economic Expert panel has academic experts forecast the
impact of policy issues or measures of future variables such as inflation or stock returns. On
a smaller scale, several papers have elicited opinions from academics. For example, Coﬀman
and Niehaus (2014) includes a survey of 7 experts on persuasion and Sanders, Mitchell, and
Chonaire (2015) ask 25 faculty and students from two universities questions on the results of 15
select experiments run by the UK Nudge Unit. Groh, Krishnan, McKenzie, and Vishwanath
(2015) elicit forecasts on the eﬀect of an RCT from audiences of 4 academic presentations.
Erev et al. (2010) ran a competition among laboratory experimenters to forecast the result
of a pre-designed laboratory experiment using learning models trained on data. These eﬀorts
suggest the need for a more systematic collection of expert beliefs about research findings.3
    We are also related to the literature on transparency in the social sciences (e.g., Simmons,
Nelson, and Simonsohn, 2011; Vivalt, 2015) and in particular to recent work on replication
in psychology and experimental economics, including the use of prediction markets to capture
beliefs about the replicability of experimental findings (Dreber et al., 2015 and Camerer et al.,
2016). We emphasize the complementarity, as our study examines diﬀerences in the informa-
tiveness of forecasts of diﬀerent experts, as well as non-experts, while the Science Prediction
Market examines the accuracy of a prediction market and the average in a survey of experts.
   The paper proceeds as follows. After presenting the design in Section 2, in Section 3 we
document the accuracy of the experts, as a group and individually. In Section 4 we present
evidence on cross-sectional diﬀerences in expertise, on non-experts and ‘superforecasters’, and
on beliefs about expertise. In Section 5 we present a simple model and in Section 6 we conclude.


2        Experiment and Survey Design
2.1        Real Eﬀort Experiment
We summarize here the design for the experiment, with additional details in DellaVigna and
Pope (2016). We designed a simple real eﬀort task on Amazon Mechanical Turk (MTurk),
varying the behavioral motivators across arms. MTurk is an online platform that allows re-
searchers and businesses to post small tasks (referred to as HITs) that require a human to
perform. Potential workers browse the postings and choose whether to complete a task for
the amount oﬀered. MTurk has become a popular platform to run experiments in market-
    3
        Banerjee, Chassang, and Snowberg (2016) provide a framework on related issues of optimal experimentation.


                                                         6
ing and psychology (Paolacci and Chandler, 2014) and is also used increasingly in economics,
such as for the study of preferences about redistribution (Kuziemko, Norton, Saez, Stantcheva,
2015). Are the results of studies run on MTurk comparable to the results in more standard
laboratory or field settings? The evidence suggests that the findings are indeed qualitatively
and quantitatively similar. For example, participants exhibit similar biases and overall results
when playing economic games online as they do in a physical laboratory (Horton, Rand, and
Zeckhauser, 2011; Amir, Rand, and Gal, 2012; Goodman, Cryder, and Cheema, 2013).
    The limited cost per subject and large available population on MTurk allow us to run
several treatments, each with a large sample size. Furthermore, the MTurk setting allows for
a simple and transparent design: the experts can sample the task and can easily compare the
diﬀerent treatments, since the instructions for the various treatments diﬀer essentially in only
one paragraph. The MTurk platform also ensures a speedy data collection eﬀort.
    We pre-registered the design of the experiment on the AEA RCT Registry as AEARCTR-
0000714 (“Response of Output to Varying Incentive Structures on Amazon Turk”). Among
the pre-registered details of the experiment, we specified the rule for the sample size and the
inclusion in the sample, as we detail in DellaVigna and Pope (2016).
    The registration also specifies the sequencing of the experiment and the survey. We ran
the experiment before seeking the forecasts in order to provide the results of three benchmark
treatments to the forecasters. To ensure that there would be no leak of any results in the
intervening period, we ourselves did not have access to the experimental results until after the
survey collection. We designed a script that monitored the sample size as well as results in the
three benchmark treatments. A research assistant ran this script and sent us daily updates so
we could monitor for potential data issues. We accessed the results of the other treatments
only at the end of September 2015, after the forecasts by the academic experts were collected.
   The task involves alternating presses of ‘a’ and ‘b’ on a computer keyboard for 10 minutes,
achieving a point for each a-b alternation, a task similar to those used in the literature (Amir
and Ariely, 2008; Berger and Pope, 2011). While the task is not meaningful per se, it does
have features that parallel clerical jobs: it involves repetition and it gets tiring, thus testing
the motivation of the workers. It is also simple to explain to both subjects and experts.
    The subjects are recruited on MTurk for a $1 pay for participating in an ‘academic study
regarding performance in a simple task.’ Subjects interested in participating sign a consent
form, enter their MTurk ID, and answer three demographic questions, at which point they
see the instructions for the task: ‘On the next page you will play a simple button-pressing
task. The object of this task is to alternately press the ‘a’ and ‘b’ buttons on your keyboard
as quickly as possible for 10 minutes. Every time you successfully press the ‘a’ and then the
‘b’ button, you will receive a point. Note that points will only be rewarded when you alternate
button pushes: just pressing the ‘a’ or ‘b’ button without alternating between the two will not
result in points. Buttons must be pressed by hand only (key-bindings or automated button-


                                                7
pushing programs/scripts cannot be used) or the task will not be approved. Feel free to score
as many points as you can.’ The participants then see a diﬀerent final paragraph (bold and
underlined) depending on the condition to which they were randomly assigned. For example,
in the benchmark 10-cent treatment, the sentence reads ‘As a bonus, you will be paid an extra
10 cents for every 100 points that you score. This bonus will be paid to your account within
24 hours.’ The key content of this paragraph for all 18 treatments is reported in Table 2.4
Subjects can try the task before moving on to the real task.
   As subjects press digits, the page shows a clock with a 10-minute countdown, the current
points, and any earnings accumulated (depending on the condition). The final sentence on the
page summarizes the condition for earning a bonus (if any) in that particular treatment. Thus,
the 18 diﬀerent treatments diﬀer in only three ways: the main paragraph in the instructions
explaining the condition, the one-line reminder on the task screen, and the rate at which
earnings (if any) accumulate on the task screen. After the 10 minutes are over, the subjects
are presented with the total points and the payout, are thanked for their participation and
given a validation code which they use to redeem their earnings.
    The experiment ran for three weeks in May 2015. The initial sample consists of 12,838
MTurk workers who started our experimental task. After applying the sample restrictions
and dropping a subsample due to a Qualtrics software glitch, the final sample includes 9,861
subjects, about 550 per treatment. As Table 2 in DellaVigna and Pope (2016) shows, the
demographics of the recruited MTurk sample matches those of the US population along gender
lines, but over-represents high-education groups and younger individuals. This is consistent
with previous literature documenting that MTurkers are quite representative of the population
of U.S. internet users (Ipeirotis, 2009; Ross et al., 2010; Paolacci et al., 2010) on characteristics
such as age, socioeconomic status, and education levels.

2.2    Forecaster Survey
Survey format. We designed the survey of experts to infer as precisely as possible the
forecasts of eﬀort in the treatments, while keeping the estimated survey duration to a maximum
of 15 minutes. The survey is also pre-registered as AEARCTR-0000731.
    The survey, formatted with the online survey platform Qualtrics, consists of two pages. On
the first and main page, the experts read a description that introduces the task: “We ran a
large, pre-registered experiment using Amazon’s Mechanical Turk (MTurk). [    ] The MTurk
participants [    ] agreed to perform a simple task that takes 10 minutes in return for a fixed
participation fee of $1.00.” The survey then described exactly what MTurkers saw: “You will
play a simple button-pressing task. The object of this task is to alternately press the ‘a’ and
   4
   For space reasons, in Table 2 we omit the sentence ‘The bonus will be paid to your account within 24 hours.’
The sentence does not appear in the time discounting treatments.



                                                      8
‘b’ buttons on your keyboard as quickly as possible for 10 minutes. Every time you successfully
press the ‘a’ and then the ‘b’ button, you will receive a point.”
    Following this introduction, the experts can experience the task by clicking on a link. They
can also see the complete screenshots viewed by the MTurk workers with another click. The
experts are then informed of a prize that depends on the accuracy of their forecasts. “As added
encouragement, five people who complete this survey will be chosen at random to be paid, and
this payment will be based on the accuracy of each of his/her predictions. Specifically, these
five individuals will each receive $1,000 - (Mean Squared Error/200), where the mean squared
error is the average of the squared diﬀerences between his/her answers and the actual scores.”5
This reward structure is incentive compatible: participants who aim to minimize the sum of
squared errors (and thus maximize their potential reward) will indicate as their forecast the
mean expected eﬀort for each treatment. We avoided a tournament payout structure (paying
the top 5 performers) which could have introduced risk-taking incentives.
    The survey then displays the mean eﬀort in the three benchmark treatments: no piece rate,
1-cent, and 10-cent piece rate (see Appendix Figure 1a). The results are displayed using the
same slider scale used for the other 15 treatments, except with a fixed scale. The experts then
see a list of the remaining 15 treatments and create a forecast by moving the slider, or typing
the forecast in a text box (though the latter method was not emphasized). The experts can
scroll back up on the page to review the instructions or the results of the benchmark treatments.
In order to test for fatigue, we randomize across experts the order of the treatments (the only
randomization in the survey). Namely, we designate six possible orders, always keeping related
interventions together, in order to minimize the burden on the experts.
    We decided ex ante the rule for the scale in the slider. We wanted the slider to include, of
course, the relevant values for all 18 treatments while at the same time minimizing the scope
for confusion. As such, we decided against a scale between 0 and 3,500 (all possible values).
Instead, we set the rule that the minimum and maximum unit would be the closest multiple of
500 that is at least 200 units away from all treatment scores. We asked the research assistant
to check this rule against the results, which led to a score between 1,000 and 2,500.6
    To summarize, in the first page of the survey the forecasters read a description of the task,
have the option to sample the task and read the detailed instructions, see the results for the
first three treatments and then make forecasts for the 15 other treatments.
    The second page of the survey, which is designed to take only 3-5 minutes, elicits a measure
of confidence in the stated forecasts (see Appendix Figure 1b). Namely, experts indicate their
   5
     It is theoretically possible for the reward for accuracy to be negative for very low accuracy (the forecast
errors need to exceed 400 points). This is rare in the sample and did not occur for the drawn individuals.
   6
     From the email chain on 6/10/2015, email to the research assistant: “We want to position [the bounds] at
least 200 away from the lowest and highest average eﬀort, and we want [...] min and max to be in multiples of
500” and response: “All of the average treatment counts are between 1,200 and 2,300”.



                                                       9
best guess as to the number of forecasts that they provided that are within 100 points of the
actual average eﬀort in a treatment. For example, a guess of 10 indicates a belief that the
expert is likely to get 10 treatments approximately right out of 15. The experts then make a
similar forecast for the average response of other groups of experts, such as the experts taken
altogether and the top-15 most cited experts. Finally, the subjects indicate whether they have
used MTurk subjects in their research and whether they are aware of MTurk, and finish oﬀ by
indicating their name. While the identities of the experts are not revealed, we use the name
to match to information on each expert and to assign the prize.7
   Sample of Experts. We use mostly objective criteria to form a starting group of be-
havioral experts (broadly construed), and then contact the academics to which we have some
connection (since we did not want to be seen as spamming researchers we did not know).
    Our initial list comprised of: (i) all authors of papers presented at the Stanford Institute of
Theoretical Economics (SITE) in Psychology and Economics and in Experimental Economics
from its inception until 2014 (for all years in which the program is online); (ii) all participants
of the Behavioral Economics Annual Meeting (BEAM) conferences from 2009 to 2014; (iii)
individuals in the program committee and keynote speakers for the Behavioral Decision Re-
search in Management Conference (BDRM) in 2010, 2012, and 2014; (iv) all invitees to the
Russell Sage Foundation 2014 Workshop on “Behavioral Labor Economics” and (v) a list of
behavioral economists compiled by ideas42. The resulting list includes experts in behavioral
and experimental economics (groups (i) and (ii)), experts in decision-making and psychology
(group (iii)), with a small set of additions (groups (iv) and (v)). We exclude graduate students
from this list and add a small number of additional experts. We then pare down this list of over
600 people to 314 researchers to whom at least one of the two authors had some connection.
   On July 10 and 11, 2015 one of the authors sent a personalized email to each of the
314 experts with subject ‘[Survey on Expert Forecasts] Invitation to Participate’. The email
provided a brief introduction to the project and task and informed the expert that an email
with a unique link to the survey would be forthcoming from Qualtrics. An automated reminder
email was sent about two weeks later to experts who had not yet completed the survey (and had
not expressed a desire to opt out from communication). Finally, one of the authors followed
up with a personalized email to the non-completers.
   Out of the 314 experts who were sent the survey, 213 completed it, for a participation rate
of 68 percent. Out of the 213 responses, 5 had missing forecasts for at least one of the 15
treatments and are not included in the main sample. Columns 1 and 2 of Table 1 document
the selection into response. Notice that the identity of the respondents is kept anonymous.
   For each expert, we code four features: academic status, citations (measures of vertical
expertise), field of expertise, and publications in an area (measures of horizontal expertise).
  7
      The survey also has a unique identifier, providing another way to check the identity of the participant.



                                                        10
Searching CVs online, we code the status as Professor, Associate Professor, Assistant Professor,
or Other (Post-doc and Research positions); we also record the year of PhD. For the citations,
we aim to record the lifetime citation impact of a researcher using Google Scholar. For experts
with a Google Scholar profile (about two thirds in our sample), we record the total citations
in the profile as of April 2015. For the experts without a profile, we sum the Google Scholar
citations for the 25 most cited papers by that expert. For individuals with more than 25 papers
retrieved by Google Scholar, we extrapolate the additional citations for papers beyond the top
25 from citations for the 16th-25th most-cited papers.
    As measures of horizontal expertise, we code field and publications in an area. For the
field, we coded experts qualitatively as belonging to one of these fields: behavioral economics
(including behavioral finance), applied microeconomics, economic theory, laboratory experi-
ments, and psychology (including behavioral decision-making). As for the publications, using
online CVs we code whether the individual, as far as we can tell, has written a paper on the
topic of a particular treatment.8
   Finally, on November 30, 2015, we provided personalized feedback, as we had promised.
Each expert received an email from one of the authors with a personalized link to a website
where they accessed a figure that included their own individual forecasts. We also randomly
drew winners and distributed the prizes as promised.9
   Other Samples. In a second round of survey collection, we also collect forecasts of a
broader group: PhD students in economics, undergraduate students, MBA students, and a
group of MTurk subjects recruited for the purpose.
    The PhD students in our sample are in Departments of Economics at eight schools. Students
at these institutions received an email from a faculty member or administrator at their school
that included a brief explanation of our project and a school-specific link for those willing to par-
ticipate. The participating PhD programs, the number of completed surveys, and the date of
the initial request are: UC Berkeley (N=36; 7/31/2015), Chicago (N=34; 8/3/2015), Harvard
(N=36; 8/4/2015), Stanford (N=5; 10/4/2015), UC San Diego (N=4; 10/7/2015), CalTech
(N=7; 10/7/2015), Carnegie Mellon (N=6; 10/8/2015), and Cornell (N=19; 10/29/2015).
   The first two waves of MBAs are students at the Booth School of Business at the University
of Chicago who took a class in Negotiations from one of the authors: Wave 1 students (N=48,
   8
     This involved some judgment calls when determining which topics counted for each treatment. For our beta-
delta treatments, we include experts who wrote a paper about beta-delta or about time preferences more broadly.
For the charitable donation treatments, we included papers about charitable giving or social preferences. Lastly,
we separately categorized experts as having worked in the area of reference dependence and/or probability
weighting rather than bunching together anyone who has worked on prospect theory into one category. For
example, if an expert had just one paper about loss aversion, this expert would have horizontal expertise for
the reference dependent framing treatments, but not for the probability weighting treatments.
   9
     Since the survey included also other participants–PhDs, undergraduates, and MBAs–two of the prizes
went to the experts. The prizes for the MTurk forecasters diﬀer and are described below.



                                                       11
7/31/2015) took a class in Winter 2015 and Wave 2 students (N=60, 2/26/2016) took a class
in Winter 2016. A third wave includes MBA students at Berkeley Haas (N=52, 4/7/2016).
    The undergraduates are students at the University of Chicago and UC Berkeley who took
at least an introductory class in economics: Wave 1 from Berkeley (N=36, 10/26/2015), Wave
2 from Berkeley (N=30, 11/17/2015), and Wave 3 from Chicago (N=92, 11/12/2015).
    All of these participants saw the same survey (with the exception of demographic questions
at the end of the survey) as the academic experts, and were incentivized in the same manner.
   On 10/4/2016, we recruited MTurk workers (who were not involved in the initial experi-
ment) to do a 10-minute task and take a 10-15 minute survey for a $1.50 fixed payment. These
participants obviously have direct experience with working on MTurk and may have a better
sense than academics or others about the priorities and interests of the MTurk population.
   Half of the subjects (N = 269) were randomly assigned to an ‘experienced’ condition and did
the 10-minute button-pressing task (in a randomly-assigned treatment) just like the MTurkers
in our initial experiment before completing the forecasting survey. The other half of the
subjects (N=235) were randomly assigned to an ‘inexperienced’ condition and did an unrelated
10-minute filler task (make a list of economic blogs) before completing the survey. Workers
in both samples were told that they would be entered into a lottery and 5 of them would
randomly win a prize based on the accuracy of their forecasts equal to $100 — Mean Squared
Error/2,000. Thus, if their forecasts were oﬀ by 100 points in each treatment, they would
receive $95 and if they were oﬀ by 300 points in each treatment, they would receive $55.
    On 2/12/2016 we recruited an additional sample of MTurk workers (N= 258) who were not
involved with any of the previous MTurk tasks. Like the ‘experienced’ MTurk sample above,
they first participated in the 10-minute button-pressing task and then took the forecasting
survey. For this sample, however, we made especially salient the value of trying hard when
making their forecasts.10 We also changed the incentives such that all participants were paid
based on the accuracy of their forecasts (as opposed to being entered into a lottery). Specifi-
cally, each participant was told they would receive $5 — Mean Squared Error/20,000. Thus, if
their forecasts were oﬀ by 100 points in each treatment, they would receive $4.50 and if they
were oﬀ by 300 points in each treatment, they would receive $.50.


3        Accuracy of Expert Forecasts: Average and Individual
How does the average eﬀort in the 15 experimental arms compare to the forecasts of the 208
academic experts? Table 2 lists the treatments, summarized by the category of the treatment
    10
    At the top of the survey portion of the task, we wrote “Important: you have the potential to increase your
earnings for this HIT substantially by doing well on this survey. So please take your time and think hard about
your answers.” We concluded the survey instructions with one final note of encouragement: “As you can see, if
you are accurate you have the potential to substantially increase your earnings for this HIT.”



                                                      12
(Column 1), the wording used (Column 2), and the sample size (Column 3). The table also
reports the average eﬀort in the treatment (Column 4) and the average forecast for that
treatment by the 208 experts (Column 5), reproduced from DellaVigna and Pope (2016).
   Figure 1 displays in graphical format the evidence on the accuracy of the average forecast.
Each of the 18 points in the scatter plot represents a treatment, with the x axis indicating
the average eﬀort exerted in the treatment (Column 4 in Table 2) and the y axis indicating
the average forecast by the experts (Column 5 of Table 2). The treatments are color-coded to
group together treatments based on similar motivators. The benchmark treatments (three red
squares) are on the 45 degree line since there was no forecast for those treatments.
    Figure 1 shows our first main result: the experts, taken altogether, do a remarkable job of
forecasting the average eﬀort. The correlation between the forecasts and the actual eﬀort is
0.77; the blue line displays the best interpolating line which has a slope of 0.527 (s.e. 0.122).
Measured otherwise, there is only one treatment for which the distance between the average
forecast and the average eﬀort is larger than 200 points, the very-low-pay treatment. Across
all 15 treatments, the average absolute error (Column 6 of Table 2) averages just 94 points,
or 5 percent of the average eﬀort across the treatments. In particular, the average expert
forecast ranks in the correct order all the six treatments with no private monetary incentives:
gift exchange, the psychology-based treatments, and the charitable-giving treatments.
   Thus, an average of forecasts across many experts does a remarkable job forecasting. But
a policy-maker, a firm, or an advisee will not typically be able to obtain forecasts for a large
number of experts. How accurate, then, is the forecast of an individual expert?
   Figure 2 and Table 3 provide information on the accuracy of individual forecasts using
several measures. For the benchmark measure (absolute error, Panel A in Table 3 and Fig-
ure 2a), we compute the absolute error in forecast by treatment, and average across the 15
treatments.11 We construct similarly the measure of squared error (Panel B in Table 3 and
Figure 2b). We also compute the correlation (Panel C in Table 3 and Figure 2c) and rank-order
correlation between the 15 forecasts and the treatments (Panel D in Table 3 and Figure 2d).
    Figure 2a displays the cumulative distribution function of the absolute error for the 208
experts. The figure also displays the wisdom-of-crowds error (vertical red line), as well as two
benchmarks for accuracy of prediction: random forecasts between 1,000 and 2,500 (dotted blue
line) and random forecasts between 1,500 and 2,200 (vertical blue line).12
   The figure shows that the accuracy of individual experts is substantially worse than the
accuracy of the average forecast: 96 percent of experts have a lower accuracy than the average
expert, and the average individual absolute error is 81 percent larger than the error of the
  11
     In this figure and throughout the paper, we show results for the negative of the absolute error and the
negative of the squared error, so as to display a measure of accuracy.
  12
     For the random benchmarks, we draw 10,000 random forecasts from a uniform distribution in the specified
range and compute the average error over the 10,000 draws.



                                                    13
average forecast (169 points vs. 93 points, Columns 1 and 2 in Table 3). This finding is known
as ‘wisdom of crowds’: the average over a crowd outperforms most individuals in the crowd.
    At the same time, there is clearly information in the individual forecasts: the large majority
of experts are more accurate than one would predict based on random choice (blue lines).
    Figures 2b, 2c, and 2d, and Panels B, C,and D of Table 3, show the findings with the three
alternative measures of accuracy. The results are parallel: the large majority of experts do not
do as well as the average expert, but they outperform random choice.
   What explains the large wisdom-of-crowds eﬀect? In particular, how many experts does it
take to achieve a level of accuracy similar to the one for the group average?
   In Figures 3a-d we plot once again the distribution of the individual and wisdom-of-crowds
accuracy, but we also plot the counterfactual accuracy of forecasts averaged over smaller groups
of N experts, with  = 5 10 20. Namely, we bootstrap 10,000 groups of N experts with
replacement from the pool, and compute for each treatment the accuracy of the average forecast
across the N forecasts. As Figure 3a shows, averaging over 5 forecasts is enough to eliminate
the right tail of high-error forecasts and achieve an average absolute error rate of 114, down
from 169 (Column 4 in Table 3). With 20 experts, the average absolute error, 99 points, is
nearly indistinguishable from the one with the full sample (93 points) (Column 5 in Table 3).
The pattern is very similar with squared error, correlation, and rank-order correlation.
   After clarifying the role of group size, we now decompose the accuracy by treatment. Figures
4a-b display two treatments in which the majority of individual forecasters outperform the
average forecast. Thus, the wisdom-of-crowds pattern does not apply in each treatment. In
the treatment with the largest deviation of the mean forecast from the actual (very-low-pay,
Figure 4c), more than 40 percent of experts do better than the wisdom-of-crowds estimate.
There are then, however, treatments in which the wisdom-of-crowds is spot on (Figure 4d),
and the large majority of experts do worse. Columns 7 and 8 of Table 2 present the expert
accuracy by treatment. Across treatments, 37 percent of subjects do better than the average.
   The critical point is that, while several experts do better than the wisdom-of-crowds in an
individual treatment, it is not typically the same experts who do well, since the errors in fore-
cast have a limited correlation across treatments. The wisdom-of-crowd estimate outperforms
individual experts by doing reasonably well throughout. We return to this point below.


4     Determinants of Forecast Accuracy
4.1   Measures of Expertise
So far we have treated the 208 experts as interchangeable, and studied the implication of
averaging expert forecasts versus following an individual expert. But clearly the experts in
our sample diﬀer in important ways. For example, the experts diﬀer in vertical expertise–


                                               14
academic rank and citations–, horizontal expertise–field of expertise and having a paper on
the topic of the treatment–, and contextual expertise–knowledge of the experimental context.
    These dimensions may be important determinants of the ability to forecast future research
findings. We may thus be able to identify the ‘right’ experts within the overall group who have
individual accuracy comparable to the accuracy of average forecasts.
    We focus this section on our benchmark measure of accuracy: the (negative of) the absolute
error rate; the results are very similar using the (negative of) squared error instead. We return
later to the results for an ordinal measure of accuracy, the rank-order correlation.
    Vertical Expertise. The first dimension of expertise which we consider is the vertical
recognition within a field. Full professors have a recognition and prerogatives, like tenure, that
most associate professors do not have, a diﬀerence a fortiori from assistant professors. In Figure
5a, we plot the distribution of the absolute error variable (averaged across the 15 treatments) by
academic rank of the experts. Surprisingly, assistant professors are more accurate, if anything,
than associate and full professors with respect to either accuracy measure.
   Figure 5a presents a further test of the vertical expertise hypothesis: to the extent that
depth of expertise matters, there should be a diﬀerence with respect to PhD students. Yet,
PhD students, if anything, do better than the associate and full professors in their forecasts.
    Table 4 provides regression-based evidence on expertise, specified as follows:

                                      =  +  +  + () +                                       (1)

An observation is a forecaster-treatment combination, and the dependent variable is a measure
of accuracy  for forecaster  and treatment  such as the negative of the absolute error in
forecast. The key regressors are the expertise variables  . The regression also includes
treatment fixed eﬀects   as well as fixed eﬀects for the order  () = 1 15 in which the
treatment is presented, to control for forecaster fatigue.13 The standard errors are clustered at
the forecaster level to allow for correlation in errors across multiple forecasts by an individual.
   Column 1 confirms the graphical findings on academic rank: associate and full professors
have a higher error rate in forecasts than assistant professors (the omitted category), and PhD
students are comparable to assistant professors.
    Academic rank is of course an imperfect measure of vertical expertise. A measure that more
directly captures the prominence of a researcher is the cumulative citation impact, which we
measure with Google Scholar citations. Citations, among other features, are very strong pre-
dictors of salaries among economists (Hilmer, Hilmer, and Ransom, 2015). Figure 5b presents
a split of the expert sample into thirds based on citations. The split has some overlap with the
academic rank, but there is plenty of independent variation. The evidence suggests a perverse
  13
    The term  () is identified because there are six possible orders of presentations of treatments. We find no
evidence of a trend of accuracy over the 15 forecasts, and the results are essentially identical if we remove the
treatment and order fixed eﬀects.


                                                       15
eﬀect of citations: the least-cited third of experts has the highest forecasting accuracy. Column
2 of Table 4 corroborates this finding: Google Scholar citations, in logs to reduce the skewness,
has a statistically significant negative eﬀect on citations.
   Thus, there is no evidence that vertical expertise improves the forecasting accuracy and
some evidence to the contrary. One interpretation of the latter result is that prominent experts
have a very high value of time and thus put less time and eﬀort into the survey. While the
regression above does not control for measures of eﬀort, we show below that two measures of
eﬀort do not predict accuracy for experts; furthermore, high-rank and high-citation experts do
not appear to be taking the survey faster or less carefully.
    Horizontal Expertise. Experts diﬀer not only vertically on prominence, but also horizon-
tally in the topics in which they have expertise. Among the ‘horizontal’ features we consider,
one is the main field of expertise. For each of the 312 experts sent a survey, we code a primary
field: behavioral economics (including behavioral finance), applied microeconomics, economic
theory, laboratory experiments, and psychology (including behavioral decision-making).14 It
is not obvious a priori which way field would aﬀect the results, but we thought that behavioral
economists may have an edge compared to standard economists given the emphasis on behav-
ioral factors in the experiment. Further, given the emphasis on quantitative forecasts, it was
possible that psychologists may be at a disadvantage.
    Figure 6a displays the results graphically, and Column 3 in Table 4 presents the regression-
based evidence. The diﬀerences between the groups, if any, are small. There is no statistically
significant evidence that behavioral economists outperform standard economists, and only
suggestive evidence of an advantage over psychologists.
   While the evidence so far has considered vertical expertise and field of expertise separately,
in Column 4 we include all the variables jointly: the point estimates remain relatively similar,
but none of the variables is statistically significant.
    Next, we turn to a more direct test of horizontal expertise. We code for each expert whether
he or she has written a paper on a topic that is covered by the treatment at hand, and create
an indicator variable for the match of treatment  with the expertise of expert  For example,
an expert with a paper on present-bias but no paper on social preferences is coded as an expert
for the treatments with delayed pay, but not for the treatments on charitable giving. We also
code whether the author has written a highly influential paper (by our assessment) on a topic.
    In the specification testing for horizontal expertise (Column 5 of Table 4), we add expert
fixed eﬀects since we are identifying expertise for a given expert. (The regressions already
include treatment fixed eﬀects.) The results indicate a null eﬀect of horizontal expertise: if
anything, having written a paper lowers the accuracy (albeit not significantly). The confidence
intervals are tight enough that we can reject that horizontal expertise increases accuracy by 8
 14
      The coding is admittedly subjective, but at least was done before the data analysis.



                                                       16
points, just 5 percent of the average absolute error. The eﬀect of writing an influential paper is
also not significantly diﬀerent from zero, though, not surprisingly, it is less precisely estimated.
   As a final measure of horizontal expertise we test whether PhD students who self-report
specializing in behavioral economics have higher accuracy. Figure 6b and Column 6 of Table
4 show that the variable has no discernible impact.
    Contextual Expertise. So far, we have focused on academic versions of expertise: aca-
demic rank, citations, expertise in a field, and having written a paper on a topic. Knowledge
of the setting, which we label contextual expertise, may play a more important role. Thus, we
elicit from the experts their knowledge of the MTurk sample.
   The survey respondents self-report whether they are aware of MTurk and whether they
have used MTurk for one of their studies. Among the experts, all but 3 report having heard
of MTurk, but the experts are equally split in terms of having used it. Thus, in Figure 7 we
compare the accuracy of the two sub-samples of experts. The experts are indistinguishable
with respect to absolute forecast error, as Column 7 of Table 4 also shows.

4.2    Non-Experts
Thus, various measures of expertise do not increase accuracy. Still, it is possible that academics
and academics in training (the PhD students) share an understanding of incentives and behav-
ioral forces which distinguish them from the non-experts. We thus compare their forecasts to
forecasts by undergraduate students, MBA students, and an online sample. These forecasters
have not received much training in formal economics, though some of them arguably have more
experience with incentives at work (the MBAs) and with the context (the online sample).
    Do non-experts make worse forecasts? Figure 8a compares the distribution of absolute
error in forecasting for experts and non-experts. The figure provides evidence of a diﬀerence
between experts and non-experts. The undergraduate students are somewhat less accurate,
MBA students are significantly less accurate, and online forecasters in the MTurk sample do
much worse. Column 1 in Table 5 shows that the diﬀerence in accuracy between the samples
is statistically significant. Furthermore, Column 2 in Table 5 shows that the diﬀerences in
accuracy between experts and non-experts replicate using, as a measure of accuracy, squared,
instead of absolute, errors. Thus, when making forecasts about magnitudes of the experimental
findings, yes, non-experts do worse than experts.
    Yet, while the above measures of accuracy were the main ones we envisioned for this study15 ,
they are not always the relevant ones. Policymakers or businesspersons may simply be looking
for a recommendation of the most eﬀective treatment, or for ways to weed out the least eﬀective
ones. From this perspective, it is not as important to get the levels right in the forecasts, as it
  15
    In our pre-registration, we mention three measures of accuracy: absolute error, squared error, and number
of correct answers within 100 points of the truth (more on this below).



                                                     17
is to get the order right. We thus revisit the results using rank-order correlation as the measure
of accuracy.16 We correlate the ranking of the 15 treatments implied by the forecasts with the
ranking implied by the actual average MTurk eﬀort.
   As Figure 8b shows, the rank-order correlation drastically changes the comparison with the
non-experts. By the rank accuracy measure, undergraduates, MBAs, and even MTurk workers
do about as well as the experts. Across all these samples, the average individual rank-order
correlation with the realized eﬀort is about 0.4 (Column 1 of Panel C, Table 3).
   We present regression-based evidence using the specification

                                           =  +  +  

Notice that the rank-order correlation measure  is defined at the level of forecaster  as
opposed to at the treatment-forecaster level. Column 3 of Table 5 shows that there is no
statistically significant diﬀerence in accuracy across the groups according to this measure.
    This evidence so far regards measures of accuracy for individual forecasters. What about
wisdom-of-crowds measures? When considering the accuracy of the mean forecast (Column 3
of Table 3), MBA students and especially MTurk workers display worse accuracy than experts
with respect to both absolute error (Panel A) and squared error (Panel B). With respect to the
rank-order measure (Panel C), though, the MTurk workers in fact do better than the experts,
displaying a stunning wisdom-of-crowds rank-order correlation of 0.95 (compared to 0.83 for
the experts). This pattern is also visible in Appendix Figure 2d, which shows just how well the
average forecast of the MTurkers ranks the treatments, despite being oﬀ in levels. With the
wisdom-of-crowds measure, MBAs do somewhat worse than experts on rank-order correlation,
though still at a high correlation of 0.71 (see also Appendix Figure 2c). Overall, though, the
wisdom-of-crowds results parallel the findings for individual accuracy.
    What explains this discrepancy between the measures of accuracy in levels and the rank-
based one? The diﬀerence occurs because non-experts, and especially the online sample, create
informed forecasts for treatments, but often center them on an incorrect guess for the average
eﬀort across the 15 forecasts. In our particular setting, the non-experts expect too low a level
of eﬀort on average. This pattern is visible in Appendix Figures 2b-d for the average forecast,
but is also displayed at the individual level in Appendix Figure 3a. A full quarter of MTurk
workers forecast an average eﬀort across the 15 treatments that is 200 points or more below the
average actual eﬀort (indicated by the red line). The other groups of non-experts–MBAs and
undergraduates–also tend to display low forecasts, though not as much as the MTurk workers.
In comparison, essentially none of the experts is oﬀ by so many points in the forecasts.
   To further document whether an oﬀset in level is a reason for the discrepancy, we explore
the simple correlation between the individual forecasts and the average results. The correlation
  16
    We thank seminar audiences and especially Katy Milkman for the suggestion to use rank-order correlation
as an additional measure of accuracy.


                                                    18
measure is based on levels, as opposed to ranks, but it does not measure whether the level
of eﬀort is matched. As such, if non-experts mainly diﬀer from experts in a level oﬀset, they
should be similar to experts according to simple correlation. Column 4 in Table 5 and Panel
D in Table 3 show that this is indeed the case.
   Thus, non-experts, while at a disadvantage to experts in forecasting the absolute level of
accuracy, do as well in ranking the performance of the treatments. This is consistent with
psychological evidence suggesting that people struggle with absolute judgments, but are better
at making relative judgments. Miller (1962) argues that memory constraints lead humans to
heavily rely on relative judgments as a heuristic in many settings. Laming (1984) further argues
that people will be especially prone to make relative (as opposed to absolute) judgments when
making magnitude estimations for a string of assignments. Diﬃculties in making absolute, ver-
sus relative, judgments matter for environmental and legal settings (e.g., Kahneman, Schkade,
and Sunstein, 1998). Thus, it is not overly surprising that non-experts do better in providing
a rank order, as opposed to an absolute measure of accuracy.
    A striking factor about this result is that non-experts do as well as experts on ranking
treatments despite spending significantly less eﬀort on the task as measured by time spent and
click-through on instruction. As Table 1 shows, undergraduates and MTurk workers (though
not MBAs) spend less time on the survey than experts, and all three non-expert samples are
much less likely than experts to click on the trial task or on the detailed instructions. We
analyze further the eﬀort measures in the next section.
   Finally, one may wonder if the rank-order correlation changes the results in the previous
section on vertical, horizontal, and contextual expertise of experts. In Appendix Figures 4a-c,
we show that this is not the case.

4.3   Other Correlates of Accuracy
So far, we found that expertise does not help much with forecasts. The fine-grained ex ante
measures of expertise do not increase forecasting accuracy, and experts as a group diﬀer from
non-experts only if the accuracy is about the levels, as opposed to the rank order, of treatments.
If expertise does not help much, are there other ways, then, to discriminate among forecasters
for accuracy? We consider measures of eﬀort, confidence, and revealed ability.
   Eﬀort. A key variable that is likely to impact the quality of the forecasts is the eﬀort
put into the survey. While eﬀort is unobservable, we collect two proxies that are likely to be
quite indicative. The first measure is the time taken from initial login to the Qualtrics survey
to survey completion. We cap this measure at 50 minutes, about the 90th percentile among
experts, since participants who took very long were likely multi-tasking, or even returned to
the survey hours or days later. The average time taken is 21 minutes among the experts, the
PhD students and the MBA students, and lower in the other samples (Table 1).


                                               19
   Second, we keep track if the forecasters clicked on the practice link to try the task, and
whether they clicked on the full experimental instructions. There is substantial heterogeneity,
with 44 percent of experts and 48 percent of PhDs clicking on the practice task, but only
11 and 16 percent among undergraduates and MBA students, and 0 percent of the MTurk
workers. The click rates on the instructions follow parallel trends but are about half the size.
   Within each major group of forecasters–experts; undergraduate, PhD, and MBA students
pooled; and MTurk workers–we display the average accuracy by decile of time taken, where
the decile thresholds are formed on the joint sample.
   Figure 9a and Columns 1-3 of Table 6 show the impact of time spent on the negative of the
absolute forecasting error for the three groups of forecasters. The patterns diﬀer significantly
by sample. For the MTurk sample, there is a clear positive relationship between time spent
and accuracy. The relationship is inverse U-shaped instead for the students and the experts.
In these two samples, accuracy generally increases quite monotonically until the 4th or 5th
decile, but then flattens or declines. In part, this may reflect the fact that individuals in the
top deciles may well have left the browser window open for the task, and returned to it later;
thus, a longer duration does not necessarily indicate more eﬀort in doing the task.
    With respect to the rank-order correlation (Figure 9b and Columns 5-7 of Table 6), there is
less evidence of a relationship. While forecasters in the bottom decile–those taking 5 minutes
or less–do worse, there is no obvious pattern for the other deciles, and in fact among the
experts the forecasters taking longer do significantly worse on rank-order correlation.
    We then turn to a second measure of eﬀort in taking the task: whether the forecasters
clicked on the trial task or on the full instructions for the task. Doing either, presumably,
indicates higher eﬀort. Figures 9c-d and Columns 1-2 and 5-6 of Table 6 show no obvious
diﬀerence in accuracy for individuals who do, or do not, click on such instructions.17
   In Table 6 we also report the eﬀect of a further proxy of eﬀort: the delay in days from
when the invitation was sent out to when it was taken. Presumably individuals that are more
enthusiastic are likely to do the survey sooner and with more eﬀort. This variable has no
obvious eﬀect.
   Overall, this evidence points to a mixed role played by eﬀort in forecasting, other than at
the very left tail (short durations). Yet, we cannot tell why some people appear to exert more
eﬀort than others. Are they more motivated? Do they have more free time? Are they just
multi-tasking and thus taking longer?
    In Figures 9e-f and in Columns 4 and 8 of Table 6 we present an attempt to instrument
for eﬀort. We run a third group of 250 MTurkers with increased incentives for accuracy in
forecasting. Namely, we pay each participant in the survey a sum up to $5 for accuracy,
computed as $5-MSE/20,000. This payment is higher than the promise to randomly pay two
  17
    We do not display the coeﬃcient on clickthrough for the MTurk sample, since no one in this sample clicked
on the additional material.


                                                     20
of the MTurk workers in the other sample an accuracy bonus up to $100. In addition, we
made the reward for accuracy more salient in the survey (see Section 2). The higher incentives
appear to have no impact on forecasting accuracy, suggesting that, at least for the sample of
MTurk workers, moral hazard in survey taking does not appear to play a major role.
   Confidence. We also consider a measure of confidence, to test whether respondents appear
to be aware of their own accuracy. On the second page of the survey, each forecaster indicated
the number of forecasts (out of 15) which they expected to get within 100 points of the correct
answer. As discussed below, each forecaster also indicated the number of forecasts that they
expected other groups of forecasters to get right.
   Figures 10a-c report the average accuracy for the same three groups–academic experts;
PhD, undergraduate, and MBA students; and MTurk workers–as a function of each of the
confidence levels from 0 to 15. We document the impact on absolute error (Figure 10a), on
the number of forecasts (out of 15) within 100 points of the actual average eﬀort (Figure 10b),
and on the rank-order correlation (Figure 10c).
    The confidence level is clearly predictive of accuracy with respect to both absolute error
and the number of correct answers. This is especially true for MTurk workers, but also holds
for the other groups. The relationship, though, is much flatter with respect to the rank-
order measure. Appendix Figure 3c shows how the two findings co-exist: higher confidence
increases the average forecast across all 15 treatments, which is too low for forecasters with low
confidence. Thus, higher confidence removes this average bias in forecasting and thus improves
the accuracy according to absolute error, but does not improve the ordering of treatments.
    The regression results in Table 7 confirm the graphical findings. An increase in the expected
number of correct answers of 5 (out of 15) reduces the average absolute error by 25 points for the
student sample and by 44 points for the MTurk sample, a highly significant relationship. For
experts instead, a similar increase in accuracy has a smaller (and not statistically significant)
impact. The table also documents a similar pattern of eﬀects on whether the forecast falls
within 100 points of the actual eﬀort (Columns 4-6), but displays a more limited relationship
of confidence with rank-order correlation (significant only for the student sample).
    Revealed Accuracy. Our third measure aims to capture an ability to make forecasts
which may not be reflected in the (coarse) eﬀort measures nor in the measure of confidence. In
particular, if there are diﬀerences in forecasting skill, forecasters who are more accurate in one
treatment are also likely to be more accurate in other treatments. We thus examine the cor-
relation of accuracy across treatments, avoiding extrapolation across very similar treatments:
the eﬀort in these treatments will presumably be correlated, inducing a correlation in accuracy.
   To start with, we consider a unique treatment within the design of the experiment: a 4-cent
piece-rate incentive. Before making any forecasts, the forecasters were informed of the average
eﬀort in three treatments with varying piece rate: (i) no piece rate, (ii) piece rate of 1 cent per
100 points, and (iii) piece rate of 10 cents per 100 points. One of the 15 treatments which they

                                                21
then predict is one with a piece rate of 4 cents per 100 points. Based on just the eﬀort in the
three benchmark treatments, as we show in DellaVigna and Pope (2016), it is possible to predict
the eﬀort in the 4 cent treatment accurately. We thus take the absolute deviation between
the forecast and realized eﬀort for the 4-cent treatment as a measure of ‘revealed accuracy’,
presumably capturing the ability to work mentally through a simple model. None of the other
treatments have this simple piece-rate property, so it is unlikely that there is a mechanical
correlation between the prediction for the 4-cent treatment and the other treatments.
    In Figures 11a-b, we plot the average accuracy for the three usual groups of forecasters, as
a function of deciles in the accuracy of forecasting the 4-cent treatment. (We omit the 4-cent
treatment in constructing the accuracy measures on the y axis, which thus refer to the other
14 treatments.) The correlation is striking: forecasters who do better in forecasting the 4c
treatment also do better in the other treatments. The association is particularly strong in the
MTurk sample. Indeed, for the top deciles there is almost no diﬀerence in accuracy between
the MTurk sample and the sample of experts and students, bridging what is instead a large
gap in accuracy of over 100 points for the bottom deciles.
   Figure 11b shows that the pattern is diﬀerent for the rank-order correlation measure of
accuracy. There is still evidence of an increase in accuracy for higher deciles for the MTurk
workers and for the students, but the evidence is less strong. As Appendix Figure 3d shows,
part of the reason is that forecasters with higher revealed accuracy produce forecasts with on
average a higher (and thus more correct) forecast, thus improving accuracy according to the
absolute error measure, but not by the rank order measure.
    Table 8 displays this evidence in a regression setting. We include in the regression also
all the other control variables: vertical expertise and field of the experts (just for the expert
regression in Columns 1 and 4), time taken to complete the survey and the confidence level.
For the absolute error measure of accuracy (Columns 1-3), even after controlling for these
variables, the 4-cent variable has remarkable explanatory power. In fact, it is the only variable
that consistently predicts accuracy for the academic experts (Column 1). An increase of 100
points in the accuracy of the 4-cent prediction increases the accuracy in the other treatments
by an average of 9.5 points for the experts. The predictability of accuracy is two to three
times larger in all the other samples: students (23.8 points for each 100 points), and MTurks
(31.4 points for each 100 points). We experimented with non-linear specifications in the 4-cent
error term, but a linear specification captures the eﬀect of the variable well. The table also
shows that introducing the revealed-accuracy control generally reduces the load on the other
predictors of accuracy, though confidence remains a significant predictor.
    Table 8 also shows that there is a relationship, if more muted, for the rank-order correlation
variable (Columns 4-6) for both students and MTurk workers. For these two groups, an increase
of 100 points in accuracy for the 4-cent treatment increases the rank-order correlation by 0.02,
a 5 percent eﬀect relative to a mean correlation of 0.4.


                                               22
    Taking this one step further, it is natural to ask whether there is something special about the
4-cent treatment when it comes to capturing ‘revealed accuracy’. Would the results diﬀer if one
constructed a variable based on one group of treatments, and then used it to predict accuracy
in the forecasts of other treatments, excluding any treatments that are mechanically related?
That is what we do in Table 9. In column 3, for example, we use the average accuracy in
forecasts of the two charity treatments to predict accuracy in the other treatments. We report
the results for the academic experts (Panel A) and for the other samples (Panel B).
    Remarkably, almost all measures are helpful to predict accuracy in other treatments. The
point estimates are not exactly comparable across columns because the diﬀerent columns omit
diﬀerent treatments, but nonetheless the predictability hovers around 5-15 units for the experts
and 20-40 units for the other samples. This result has an important implication. It does not
appear that the critical component is accuracy in forecasting a model-driven incentive (which
is a specific skill for the 4-cent treatment), but rather a general ability to form forecasts.

4.4   Superforecasters
As we have seen in Section 4.2, non-experts do as well as experts with respect to ranking
treatments, but not with regards to measures of accuracy in levels, such as the negative of the
absolute error rate. Thus, if one aims to obtain forecasts with the lowest absolute error rate,
forecasts by academic experts are preferable. Yet, academic experts are busy professionals that
are harder to reach than other samples such as students or online samples. Is there a way to
use the latter, more available samples, and yet match the accuracy of the expert sample?
    In the context of the Good Judgment Project, Mellers et al. (2015) and Tetlock and
Gardner (2015) phrase a similar question as one of finding ‘superforecasters’. Is it possible to
find non-experts (in their setting individuals who do not have access to classified information)
who nonetheless predict outcomes of national security as well as, or better than, the experts?
Mellers et al. (2015) and Tetlock and Gardner (2015) find that it is possible to do so using the
previous track record of forecasters.
    In our context, to identify superforecasters we use the variables examined so far: measures
of expertise, eﬀort, confidence, and revealed accuracy. As Section 4.3 shows, especially the
latter measure, which is in spirit of using the track record of a forecaster, is predictive of
forecasting accuracy. We thus take the same specification as in Table 8, with all these control
variables, and for each sample of experts we predict accuracy. To avoid in-sample data mining,
we use a 10-fold method to obtain out-of-sample predictions. For each subgroup, we randomly
split the forecasters into 10 equal-sized groups. We leave out the first tenth, estimate the
model with the remaining nine tenths of the data, and predict accuracy in the left-out tenth.
Then we rotate the same procedure with the next tenth of the data until we covered all the
observations. Within each group, we select the top percentile in predicted accuracy.


                                                23
   Table 10 reports the results for both individual accuracy (Column 1) and wisdom-of-crowds
average (Column 3). Panel A reports the results for the academic experts, comparing the
overall group of experts to the optimal 20% and optimal 10% of experts constructed using all
controls, as well as the optimal 20% constructed using all controls other than the revealed-
ability variable. The table shows that among the experts we are unable to select a subset of
super-experts who will do better than the overall group. The accuracy measure for the full
sample of experts and for the top 20% (constructed with all controls) overlap (Figure 12a).
    The results diﬀer for the other groups of forecasters. In the sample of PhD students, MBAs,
and undergraduates (Panel B), the optimal 20% of forecasters outperforms significantly the
academic experts both at the individual level (Figure 12a) and with the wisdom-of-crowds
measure.18 Indeed, the wisdom-of-crowds absolute error for the top 20% in this group is as
low as 73 points, compared to 95 points for the average expert, a diﬀerence that is statistically
significant (Column 4).19 Figure 12b displays the results for the wisdom-of-crowds measure
for bootstrapped samples of 20 forecasters.
   The results are equally striking for the MTurk workers. While on average MTurk workers
have a much higher individual absolute error than experts (272 points on average versus 175
points), picking the top 20% of MTurkers nearly closes the gap for individual accuracy. Further,
when using the wisdom-of-crowds measure, the selected MTurk forecasters actually outperform
the academic experts, achieving an accuracy of 73, compared to 95 for the experts, a diﬀerence
that is statistically significant. The revealed-ability variable plays an important role: the
prediction without it does not achieve the same accuracy.
    Thus, especially if it is possible to observe the track record, even with a very short history
(in this case we use just one forecast), it is possible to identify subsamples of non-expert
forecasters with accuracy that matches or surpasses the accuracy of expert samples.

4.5     Beliefs about Expertise
Our seventh and final result addresses a meta-question: Did we know all this already? Perhaps
there was a shared understanding of these main issues, that for example vertical and horizontal
expertise do not matter for the quality of forecasting.
    In the spirit of the forecasting idea, on the second page of the survey we elicited the expected
accuracy for diﬀerent groups of forecasters (Appendix Figure 1b). In order to compare the
responses to the data while keeping the forecasts simple, we asked for the expected number
of treatments that an individual from a particular group would guess within 100 points of
  18
      While omitting the 4-cent revealed-ability variable decreases the ability to identify superforecasters, the top
20% group selected using the other variables (eﬀort and confidence) already outperforms the experts.
   19
      To compute whether the wisdom-of-crowd accuracy of sample at hand is statistically significantly diﬀerent
from the one of the overall sample of experts, we bootstrap the sample 1,000 time. At each bootstrap we redraw
both the experts and the non-experts, determining a new group of superforcasters.


                                                         24
the truth. For example, the forecasters guess the average number of correct answers for the
academic experts participating in the survey. Next, they guess the average number of correct
answers for the 15-most cited academics participating in the survey. The diﬀerences between
the two guesses is a measure of belief about the impact of vertical expertise.
   Figure 13 plots the beliefs of the 208 experts compared with the actual accuracy for the
specified group of forecasters. The first cell indicates that the experts are on average accurate
about themselves, expecting to get about 6 forecasts ‘correct’, in line with the realization.
Furthermore, as the second cell shows, the experts expect other academics to do on average
somewhat better than them, at 6.7 correct forecasts. Thus, this sample of experts does not
display evidence of overconfidence (Healy and Moore, 2008), possibly because the experts were
being particularly cautious not to fall into such a trap.
    The key cells are the next ones, on the expected accuracy for other groups. The experts
expect the 15 most-cited experts to be somewhat more accurate when the opposite is true.
They also expect experts with a psychology PhD to be more accurate where, once again, the
data points if anything in the other direction. They also expect that PhD students would be
significantly less accurate, whereas the PhD students match the experts in accuracy.20 The
experts also expect that the PhD students with expertise in behavioral economics would do
better, which we do not find.21 The experts correctly anticipate that MBA students and
MTurk workers would do worse. However, they think that having experienced the task among
the MTurkers would raise noticeably the accuracy, counterfactually.
    Overall, the beliefs about the determinants of expertise are systematically oﬀ target. This
is understandable given the lack of previous evidence on the accuracy of research forecasts.


5        Model and Calibration
We presented a set of findings about forecasts of research results. Can a simple model make
sense of the key findings? We model agent  making forecasts about the results in treatments
 = 1  . Let  = (1       ) be the outcome (unknown to the agent) in the  treatments.
Given the incentives in the survey, the agent aims to minimize the squared distance between
the forecast  and the result  . We assume that agents start with a non-informative prior
and that agent  with  = 1   draws a signal  about the outcome of treatment :

                                         =  +  +  +                                        (2)
    20
      For the PhD students we report the actual accuracy including only Univeristy of Chicago and UC Berkeley
PhDs, since the survey refers only to these two groups. The results are similar (and more precisely estimated)
if we use all PhD students to compute the actual accuracy.
   21
      We did not elicit forecasts about undergraduate students since we had not decided yet whether to contact
a sample of undergraduates at the time the survey launched.



                                                     25
The deviation of the signal  from the truth  consists of three components, each i.i.d.
and independent from the other components: (i)  ∼  (0 2 ) is a deviation for treatment
                                                     ¡     ¢
 that is common to all forecasters; (ii)  ∼   2 is a deviation for forecaster  that
is common across all treatments (with a possible bias term if  6= 0); (iii)    with  ∼
 (0 1), is the idiosyncratic noise component, with heterogeneous  : more accurate forecasters
are characterized by a lower  . We assume that  is independent from  and that the
idiosyncratic variance 2 follows an inverse gamma distribution: 2 ∼ ( ).
   We assume that the agent is unaware of the systematic bias  Given this and the uninfor-
mative prior, the signal  is an agent’s best estimate (that is,  =  ), given that it minimizes
the (subjective) expected squared distance between the forecast and the result in treatment .
     The error term  captures idiosyncratic noise in the forecasts. Importantly, the forecasters
diﬀer in the extent of idiosyncratic noise, with some experts providing less noisy forecasts (lower
 ). This heterogeneity has implications for the correlation of errors across treatments. If  is
very similar across forecasters, the absolute error in one treatment will have little predictability
for the absolute error in another treatment for the same person, as the error in forecast arises
from noise that is similar across all forecasters. If some forecasters, instead, have significantly
lower  than other forecasters, there will be cross-treatment predictability: the forecasters
who do well in one treatment are likely to have low  , and thus do well in another treatment
too. Thus, heterogeneity in  can capture the results on revealed forecasting ability.
    Why do we need the additional error terms  and  ? A model with just the idiosyncratic
error term misses two important features of the data. First, some treatments appear harder
to forecast than others, as Table 2 shows. Given the large sample size of forecasters, these
cross-treatment diﬀerences are unlikely to be due to idiosyncratic error. The term  allows
for such diﬀerences, potentially capturing an incorrect common reading of the literature (or of
the context) for a particular treatment, or an unusual experimental finding.
   Second, forecasters diﬀer in the average forecast across all 15 treatments, again more than
one would expect based on idiosyncratic noise. Appendix Figure 3a shows that in particular
non-experts tend to under-forecast eﬀort, with a large heterogeneity. The term  captures an
agent  being more optimistic (or pessimistic) about the eﬀect of all treatments.
    We now document that this simple model can make sense of several qualitative features
of the data. We calibrate the five model parameters: 2   2   and  To tie down these
parameters, we use three variances, a measure of average bias, as well as the between-treatment
correlation in absolute error for one forecaster. We then use the calibrated model to check how
well we match some key features in the data. We do the calibration separately for the sample
of 208 experts and for the other samples (students and MTurks).
   As Panel A of Table 11 shows, the first moment is the variance of the forecast error,  − 
                                            ¡       ¢               ¡ ¢
which, as we show in the Appendix, equals   −  = 2 +2 + 2 . Second, we consider
the variance of the wisdom-of-crowds error, obtained by averaging  across all forecasters ,


                                                  26
                                         £        ¡   ¢¤
̄ = Σ  :  (̄ −  ) = 2 + 2 +  2 . Intuitively, the only part of the variance
that does not shrink is the treatment-specific variance. Third, we consider the variance of
                                                                                  ¡              ¢
the average
      h        error ifor forecaster  across all treatments  ̄ = Σ  :  ̄ − Σ   =
                ¡ ¢
2 + 2 +  2 . The overall variance and the treatment-specific variance are shrunk
by averaging, but the person-specific variance 2 is not. These three expressions allow one to
                             ¡ ¢
back out 2 , 2 , and  2 . To tie down the average bias in forecast  we use the overall
                 P ¡           ¢
average error,   −  ( ∗ ). Finally, to identify  and  the parameters determining
the distribution of   we use the correlation in errors across treatments.
   As Panel A shows, the experts, compared to the non-experts, have a significantly lower
            ¡       ¢                                                    ¡      ¢
variance   −  and also lower variance of the average error  ̄ −  , as Appendix
Figure 3a documents. The experts have instead a higher variance of the wisdom-of-crowd
error compared to non-experts, as one can see comparing Figure 1 (for experts) to Appendix
Figures 2a-d (non-experts). These figures also indicate that, while for experts there is only a
negligible bias  on average, there is a sizeable bias for non-experts. The final moment is the
correlation of the absolute error across treatments, which we take from Column 1 in Table 9.
    Panel B displays the implied calibrated values of the parameters. The experts have a higher
calibrated variance 2 of the treatment-specific error than non-experts, but a lower variance
                                                                                         ¡ ¢
2 of the forecaster-specific error, as well as a lower average idiosyncratic variance  2 .
     To identify the heterogeneity in 2 , we use the cross-treatment correlation in absolute error
for a forecaster. Figure 14a plots the implied correlation in absolute error across treatments as
                                                                                                ¡ ¢
we increase the heterogeneity in the variance 2 , holding constant the average variance  2
at the calibrated value.22 For low values of the (log) standard deviation of 2 (on the x axis),
the implied correlation of errors is quite low: if individuals are oﬀ in one treatment, they
are not much more likely to be oﬀ in another treatment (other than because of the realized 
term). For high values of the (log) standard deviation of 2 , instead, some forecasters are much
better than others in making forecasts. In this case, absolute error in one treatment will be
more informative of the error in another treatment. The observed correlation (.09 for experts
and .29 for non-experts, Column 1 of Table 9) pins down approximately the two parameters
of the inverse gamma distribution for experts and non-experts and thus the distribution of
2 .23 As Figure 14b shows, non-experts have on average higher idiosyncratic variance and
more heterogeneity. Given the higher heterogeneity, there are more ‘superforecasters’ (agents
with low  ) among the non-experts.
  22
     Each point reports the average correlation from 1,000 simulated samples for those parameter values. Each
sample has the same number of individuals (208 for the experts and 1,227 for the non-experts) and the same
number of treatments (15) as in the data. Within a sample, we correlated absolute error in each of 14 treatments
on absolute error on the 15th treatment (held constant) for each person. This mirrors the regressions in Table
                                                         ¡ ¢                                         ¡ ¢
8 and 9. Each calibration varies  and  so as to keep  2 constant, but vary the variance   2 
  23
     The sample of non-experts achieves an asymptote of correlation of 0.27, we thus pick a point with high
enough standard deviation. Picking a higher or lower point in the range leads to very similar results.


                                                      27
    In Panel C of Table 11 we examine whether this simple model can match some key features
of the forecasting data. We simulate 1,000 draws for the calibrated parameters, each draw with
the same number of forecasters and the same number of treatments as in the sample (15). We
display the average of the statistic examined over the 1,000 draws.
    Comparing the data (Columns 1 and 3) to the calibrated values (Columns 2 and 4), the
model matches remarkably well the average individual absolute error and the average wisdom-
of-crowd error for both experts and non-experts. It also reproduces very closely the error with
a group of 5 forecasters, implying that the model reproduces the speed of convergence in the
error due to aggregation of opinions. Figure 14c displays these patterns visually, comparing
the c.d.f. of the absolute error for the two groups, in the data versus the calibrations.
   We can also use the calibrated model to benchmark the ‘superforecasting’ results. We select
the 20% with the lowest absolute error in a fixed treatment, and examine the absolute error in
the other treatments. This selection criterion mirrors the results in Tables 8 and 9, in which
the absolute error in one treatment is the strongest predictor in determining the sample of
‘superforecasters’ (Table 10). Within the experts, the forecasters chosen in this way display
similar individual and wisdom-of-crowds absolute error as in the sample of all experts. Within
the non-experts, instead, the superforecasters outperform the overall group of non-experts both
individually and as a group. These results match the findings in the data, and reflect the wider
dispersion of both forecaster errors and idiosyncratic error among the non-experts.
   Next, we turn to the rank-order correlation in forecasts, a measure that reverses a key
result: non-experts are as good as experts in rank-ordering treatments, and are in fact better
when using a wisdom-of-crowds measure. Can our simple calibration match this fact?
    The calibration indeed predicts that experts will have a similar rank-order correlation as
non-experts, though it overstates the level of the individual-level correlation for both groups
(about 0.6 compared to 0.4 in the data). The calibration matches remarkably well the wisdom-
of-crowds rank-order correlation, reproducing not just the qualitative features, but also the
magnitudes in the data: about 0.8 for experts and 0.9 for non-experts.
   We also check whether the model matches a diﬀerent measure of strength of the wisdom of
crowds: the share of forecasters that does better than the wisdom-of-crowds forecasts. In this
respect, the calibrations match quite well the features in the data.
    Finally, we consider a key question raised initially: does the model need all the compo-
nents? In Appendix Table 1, we replicate the calibrations turning oﬀ in turn each error term
component. Calibrations without the forecaster-specific error (and bias  = 0) lead to unreal-
istic wisdom-of-crowds accuracy for the non-experts (Columns 3 and 4). Calibrations without
the treatment-specific error instead lead to unrealistic wisdom-of-the-crowds accuracy for the
experts (Columns 5 and 6). Finally, assuming no heterogeneity in the idiosyncratic error
(constant ), we cannot match the correlation of errors across treatments (Columns 7 and 8).
   Overall, this model is able to reproduce several stylized features of the data, including in-


                                              28
dividual accuracy versus the wisdom of crowds, performance of ‘superforecasters’, diﬀerences
between experts and non-experts, and diﬀerences between absolute error and rank-order cor-
relation. We should, however, be clear that this simple model should be seen just as a starting
point to understand how forecasters form their beliefs about future research findings.


6    Conclusion
When it comes to forecasting future research results, who knows what? We have attempted to
provide systematic evidence within one particular setting, taking advantage of forecasts by a
large sample of experts and of non-experts regarding 15 diﬀerent experimental treatments.
    Within this context, forecasts carry a surprising amount of information, especially if the
forecasts are aggregated to form a wisdom-of-crowds forecast. This information, however, does
not reside with traditional experts. Forecasters with higher vertical, horizontal, or contextual
expertise do not make more accurate forecasts. Furthermore, forecasts by academic experts are
more informative than forecasts by non-experts only if a measure of accuracy in ‘levels’ is used.
If forecasts are used just to rank treatments, non-experts, including even an easy-to-recruit
online sample, do just as well as experts. Thus, the answer to the who part of the question
above is intertwined with the answer to the what part.
   Even if one restricts oneself to the accuracy in ‘levels’ (absolute error and squared error), one
can select non-experts with accuracy meeting, or exceeding, that of the experts. Therefore,
the information about future experimental results is more widely distributed than one may
have thought. We presented also a simple model to organize the evidence on expertise.
    The current results, while just a first step, already draw out a number of implications
for increasing accuracy of research forecasts. Clearly, asking for multiple opinions has high
returns. Further, traditional experts may not necessarily oﬀer a more precise forecast than a
well-motivated audience, and the latter is easier to reach. One can then screen the non-experts
based on measures of eﬀort, confidence, and accuracy on a trial question.
    The results stress what we hope is a message from this paper. As academics we know so
little about the accuracy of expert forecasts that we appear to hold incorrect beliefs about
expertise and are not well calibrated in our accuracy. We conjecture that more opportunities
to make forecasts, and receive feedback, could lead to significant improvements. We hope that
this paper will be followed by other studies examining forecast accuracy.


References
 [1] Amir, On, and Dan Ariely. “Resting on Laurels: The Eﬀects of Discrete Progress Markers
     as Subgoals on Task Performance and Preferences.” Journal of Experimental Psychology:
     Learning, Memory, and Cognition. Vol. 34(5) (2008), 1158-1171.



                                                29
 [2] Amir, Ofra, David G. Rand, and Ya’akov K. Gal, 2012. “Economic games on the Internet:
     The eﬀect of $1 stakes.” PLoS ONE, 7(2), e31461.
 [3] Banerjee, Abhijit, Sylvain Chassang, and Erik Snowberg. 2016. Forthcoming. “Decision
     Theoretic Approaches to Experiment Design and External Validity”, Handbook of Field
     Experiments.
 [4] Ben-David, Itzhak, John Graham, Cam Harvey, 2013, “Managerial Miscalibration”, Quar-
     terly Journal of Economics 128 (4), 1547—1584.
 [5] Berger, Jonah, and Devin Pope. “Can Losing Lead to Winning.” Management Science
     Vol. 57(5) (2011), 817-827.
 [6] Camerer, Colin et al.. 2016. “Evaluating Replicability of Laboratory Experiments in Eco-
     nomics” Science, 10.1126.
 [7] Cavallo, Alberto, Guillermo Cruces, and Ricardo Perez-Truglia. 2016 “Inflation Expecta-
     tions, Learning and Supermarket Prices: Evidence from Survey Experiments” Working
     paper.
 [8] Coﬀman, Lucas and Paul Niehaus. 2014. “Pathways of Persuasion” Working paper.
 [9] DellaVigna, Stefano and Devin Pope. 2016. “What Motivates Eﬀort? Evidence and Expert
     Forecasts” NBER Working paper w22193.
[10] Dreber, Anna, Thomas Pfeiﬀer, Johan Almenberg, Siri Isaksson, Brad Wilson, Yiling
     Chen, Brian A. Nosek, and Magnus Johannesson. 2015. “Using prediction markets to
     estimate the reproducibility of scientific research”, PNAS, Vol. 112 no. 50, 15343—15347.
[11] Erev, Ido, Eyal Ert, Alvin E. Roth, Ernan Haruvy, Stefan M. Herzog, Robin Hau, Ralph
     Hertwig, Terrance Stewart, Robert West, and Christiane Lebiere, “A Choice Prediction
     Competition: Choices from Experience and from Description.” Journal of Behavioral
     Decision Making, 23 (2010): 15-47.
[12] Galton, Francis. 1907. “Vox Populi ” Nature, No. 1949, Vol. 75, 450-451.
[13] Goodman, Joseph K., Cynthia E. Cryder, and Amar Cheema. 2013. “Data collection in
     a flat world: The strengths and weaknesses of Mechanical Turk samples.” Journal of
     Behavioral Decision Making, 26, 213-224.
[14] Groh, Matthew, Nandini Krishnan, David McKenzie, Tara Vishwanath. 2015. “The Im-
     pact of Soft Skill Training on Female Youth Employment: Evidence from a Randomized
     Experiment in Jordan” Working paper.
[15] Moore, Don A.; Healy, Paul J. 2008. “The trouble with overconfidence.” Psychological
     Review, Vol 115(2), 502-517.
[16] Hilmer, Christiana E., Michael J. Hilmer, and Michael R. Ransom. 2015. “Fame and
     the Fortune of Academic Economists: How the Market Rewards Influential Research in
     Economics.” Southern Economic Journal, Vol. 82(2), pp. 430—452.
[17] Horton, John J. and Chilton, Lydia B. 2010. “The Labor Economics of Paid Crowdsourc-
     ing” Proceedings of the 11th ACM Conference on Electronic Commerce.
[18] Horton, John J., David Rand, and Richard Zeckhauser. 2011. “The online laboratory:
     conducting experiments in a real labor market” Experimental Economics, Vol. 14(3), pp
     399-425.

                                             30
[19] Ipeirotis, Panagiotis G. “Analyzing the Amazon Mechanical Turk Marketplace. 2010. ”
     XRDS: Crossroads, The ACM Magazine for Students Vol. 17, No. 2: 16-21.
[20] Kahneman, Daniel, David Schkade, Cass Sunstein. 1998. “Shared Outrage and Erratic
     Awards: The Psychology of Punitive Damages” Journal of Risk and Uncertainty, Vol.
     16(1), pp 49—86.
[21] Kuziemko, Ilyana, Michael I. Norton, Emmanuel Saez, and Stefanie Stantcheva. 2015.
     “How Elastic Are Preferences for Redistribution? Evidence from Randomized Survey
     Experiments.” American Economic Review 105(4): 1478-1508.
[22] Laming, Donald. 1984. ”The relativity of ‘absolute’ judgments.” British Journal of Math-
     ematical and Statistical Psychology, 37(2), 152-183.
[23] Mellers, Barbara, Eric Stone, Terry Murray, Angela Minster, Nick Rohrbaugh, Michael
     Bishop, Eva Chen, Joshua Baker, Yuan Hou, Michael Horowitz, Lyle Ungar, Philip Tet-
     lock. 2015. “Identifying and Cultivating Superforecasters as a Method of Improving Proba-
     bilistic Predictions,” Perspectives on Psychological Science May 2015 vol. 10 no. 3 267-281.
[24] Miller, George A. 1956. “The magical number seven, plus or minus two: some limits on
     our capacity for processing information.” Psychological Review, 63(2), 81-97.
[25] Open Science Collaboration. (2015). “Estimating the reproducibility of psychological sci-
     ence.” Science, 349(6251)
[26] Paolacci, Gabriele. 2010. “Running Experiments on Amazon Mechanical Turk.” Judge-
     ment and Decision Making Vol. 5, No. 5: 411-419.
[27] Paolacci, Gabriele, and Jesse Chandler. “Inside the Turk: Understanding Mechanical Turk
     as a Participant Pool.” Current Directions in Psychological Science Vol 23(3), 184-188.
[28] Ross, Joel, et al. 2010. “Who Are the Crowdworkers?: Shifting Demographics in Mechan-
     ical Turk.” In CHI ’10 Extended Abstracts on Human Factors in Computing Systems:
     2863-2872.
[29] Sanders, Michael, Freddie Mitchell, and Aisling Ni Chonaire. 2015. “Just Common Sense?
     How well do experts and lay-people do at predicting the findings of Behavioural Science
     Experiments” Working paper.
[30] Joseph P. Simmons, Leif D. Nelson and Uri Simonsohn. 2011. “False-Positive Psychology:
     Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as
     Significant”, Psychological Science, Vol. 22(11), pp. 1359-1366.
[31] Snowberg, Erik, Justin Wolfers and Erik Zitzewitz. 2007. “Partisan Impacts on the Econ-
     omy: Evidence from Prediction Markets and Close Elections.” Quarterly Journal of Eco-
     nomics 122, 2, 807-829.
[32] Surowiecki, James. 2005. The Wisdom of Crowds. Knopf Doubleday Publishing.
[33] Tetlock, Philip E., Dan Gardner. 2015 Superforecasting: The Art and Science of Predic-
     tion, Random House.
[34] Vivalt, Eva. 2016. “How Much Can We Generalize from Impact Evaluations?” Working
     paper.
[35] Wolfers, Justin, Zitzewitz, Eric. 2004. “Prediction Markets” The Journal of Economic
     Perspectives, Vol. 18 (2), pp. 107-126.

                                               31
A      Appendix A - Model and Calibration Appendix
We present here the derivation of the three variances used in the calibration. Notice first that
            ³        ´                         ³   ´2                                           ³         ´       ³       ´2          ³    ´2     ³     ´2
                  = (2                    ) − [(2  )]2 =  2 (                                       ) −  2         
                                      ³        ´             ³       ´2                         ³         ´
                          =  2 ∗ 1 −  2                                  ∗ 0 =  2 
The cross-sectional variance satisfies
                    ³             ´                                                                  ³        ´                            ³     ´
               −  =   ( ) +   ( ) +     = 2 + 2 +  2 

    The wisdom-of-crowds variance equals
                              ⎡                                                                                                                              ⎤
                         
                      1 ⎣X                             X
     (̄ −  ) = 2     ( +  +   ) + 2     ( +  +     +  +   )⎦
                      =1                             
                          1 ³ 2              ³ ´´     X
                     =     2
                             [  + 
                                       2
                                         +   2 + 2      ( ) + (   +   )
                                                         | {z } |             {z          }
                                                      
                                                                                                    2                           0

                     + (   +   ) + ( +     +   ))]
                         |                {z             }   |                             {z                         }
                                          0                                                0
                         1 h ³ 2     2
                                           ³ ´´
                                              2             2
                                                              i
                                                                    2   1 2       ³ ´
                     =        +   +     + ( − 1)    =   +   ( +   2 )
                         2                                             
    The average-bias variance equals
                              ⎡                                                                                                                              ⎤
       ³        ´        1    ⎢X     ³                 ´    X    ³                                     ´⎥
                              ⎢                                                                          ⎥
    −  =                ⎢      +  +    + 2     +  +      +  +    ⎥
                         2   ⎣
                               =1                             |              {z                     }⎦
                                                                                                                                   2
                         1 h          ³                      ³        ´´                                      i                  1 ³ 2    ³ ´´
                    =           2 + 2 +  2                             +  ( − 1) 2 = 2 +                               +  2 
                         2                                                                                                      
                                                                                   ¡       ¢
    Given these expressions, we can solve for  2 , ̂2 and ̂2 :
                                          1 ³                        ³         ´´
                                      ̂2 =   (̄ −  ) −    − 
                                         −1
                                          1 ³         ³        ´       ³        ´´
                                 ̂2 =           −  −    − 
                               ³ ´
                                         −³1      ´
                                =    −  − ̂2 − ̂2 
                                 2


  Finally, for the distribution of 2  which we assume to be inverse gamma distributed
( ) from standard properties we obtain
                                                                               £       ¤
                                                            2
                                                    = 2+     £ ¤                                                                                               (3)
                                                            2
                                                                               Ã                    £     ¤ !
                                                                 h         i            2
                                                    =              2           1+     £ ¤                                                                   (4)
                                                                                        2

                                                                               32
                     £   ¤                                        £   ¤
Given an implied  2 from the calibration, we can vary   2 through the implied values
of  and  to match the correlation of absolute error across treatments.
    Calibration. For the calibration in Table 11, we take the moments in Panel A from the
data: the overall variance in error, the variance of the wisdom-of-crowd error, the variance
of the average error in forecast, and the average bias. For each of the three variances, we
report the square root (the standard deviation). For the correlation of absolute error across
treatments, we take the coeﬃcients in Column (1) of Table 9, appropriately divided by 100
(given that in Table 9 the regressor was divided by 100): 0.09 for experts and 0.29 for non-
experts. Notice that Table 9 is not exactly the correlation of absolute error for treatment  on
absolute error for treatment  on two grounds. First, we estimate a regression and not a simple
correlation. Second, there are additional controls in the regression. However, the additional
controls have a limited impact and thus the regression coeﬃcient is close to a simple correlation
coeﬃcient, assuming that the variance of the dependent variable is similar to the variance of
the independent variable, which on average it is.
                                                                          ¡   ¢
   As we explain above, the three variances identify 2 , 2 , and  2  In Panel B we report
                                                                                          q   ¡   ¢
the square root of these terms. For the third term, notice that we are thus reporting  2 
To identify the  and  parameters for the distribution of 2  we use the correlation of absolute
errors across treatments
                   £ 2¤    described above. Specifically, as expressions (3) and (4) make clear,
for any £given
             ¤
                  (which we take at the estimated value in£ Panel   ¤
                                                                             B), an assumed value
                                                                                               £ ¤
of   2 pins down  and  In Figure 14b, we vary   2 keeping constant  2 
generating combinations of  and  For each of these combinations, we simulate 1,000 draws.
Each sample has the same number of individuals (208 for the experts and 1,227 for the non-
experts) and the same number of treatments (15) as in the data. Within a sample, we correlated
absolute error in each of 14 treatments on absolute error on the 15th treatment (held constant)
for each person. This mirrors the regressions in Table 8 and 9. For each point, the y axis
reports the average estimated regression slope. As Figure 14b shows, the larger the variance in
2  the more accuracy in one treatment predicts accuracy in another treatment, as expected.
Furthermore,
       £ ¤      the predictability is higher for non-experts than for experts for any level of
  2 because non-experts have higher variance in   and draws of  induce correlation
across treatments. From Figure 14b, we set the values of  and  We should note that the
correlation of errors for non-experts comes close, but does not quite reach 0.29 in the figure;
we thus pick a high value subject to not violating the constraint  ≥ 2. The exact value chosen
is immaterial to the results in Panel C.
     Having determined the values of all five parameters, we report in Panel C the results of
simulations of populations with the appropriate parameters. More precisely, we do 1,000 draws
of simulated populations, each of which with 15 treatments and as many subjects as appropriate
(that is, 208 for the experts and 1,227 for non-experts). Each draw simulates a realization of
our survey if the underlying parameters were the hypothesized ones. Within each draw, we
compute the relevant statistic, and we report the mean across the 1,000 draws.
     The statistics are computed as in the paper. For ‘superforecasters’, we fix one treatment
(the equivalent of the 4-cent treatment) and pick the 20% of subjects with the lowest realized
absolute error. We then compute the absolute error for these subjects, averaging across the
remaining 14 treatments. Notice that for most of the statistics, the draw of  does not matter,
as all is a function of the error  −   However, for the rank-order correlation the realized 
matter, and we take those from the data. (For example, the closer the  are to each other,
the worse the rank-order correlation will be, all else constant).




                                                33
                  Figure 1. Wisdom-of-Crowds Accuracy: Average Performance and Average Forecast by Treatment, Academic Experts




Notes: Figure 1 presents the results from the 15 treatments with forecasts and the three benchmarks also reported in Table 2. Each dot indicates a treatment, with the actual (average) effort by the
MTurk workers on the x-axis and the average forecast by the 208 academic experts on the y axis. The 3 benchmark treatments, for which there was no forecast, are reported with a red square.
Forecasts close to the 45 degree dotted line indicate cases in which the average forecast is very close to the actual average performance. The continuous line indicates the OLS line fit across the 15
points, with estimate forecast = 876 (238) + .527 (.122) * actual.


                                                                                                   34
                                              Figure 2. Distribution of Accuracy Measures for Individual Academic Experts




Notes: Unlike Figure 1, which focuses on the accuracy of average forecasts, Figure 2 presents measures of accuracy of individual forecasts by the 208 academic experts. For Figure 2a, for each of the
208 experts, we compute the absolute deviation between the forecast and the actual effort by treatment, average across the 15 treatments and take the negative, and plot the c.d.f. of this accuracy
measure. The blue lines show the counterfactual average absolute error assuming random forecasts between 1,000 and 2,500 points (dotted line) or between 1,500 and 2,200 points (continuous line).
The red line shows the absolute error for the average, as opposed to the individual, forecast. As Figure 2a shows, the average forecast outperforms 95 percent of experts. Figure 2b shows the c.d.f for
the negative of the mean squared error, Figure 2c shows the c.d.f. for the correlation between the forecast and the results, while Figure 2d shows the rank-order correlation.

                                                                                                  35
                                        Figure 3. Individual Expert Accuracy versus Aggregate (Wisdom-of-Crowds) Accuracy




Notes: Figure 3 presents the same information as in Figure 2, but in addition highlights the role of the “wisdom of crowds”, that is, the importance of averaging forecasts across experts. Namely, we
form hypothetical pools of N forecasters (with N=5, 10, 20) drawn with replacement from the 208 experts, and for each draw we take the average forecast across the N forecasters and compute the
accuracy measure for the average. The lines from these hypothetical draws show that taking the average quickly shrinks the tails of the accuracy measure and increases average accuracy.



                                                                                                 36
                    Figure 4. Individual Expert Accuracy versus Aggregate (Wisdom-of-Crowds) Accuracy, Representative Treatments




Notes: Figure 4 presents the same information as in Figure 3a for four treatments using the negative of the absolute mean error as accuracy measure.




                                                                                                37
                                     Figure 5. Impact of Vertical Expertise on Accuracy
                                                 Figure 5a. Academic Rank




                                           Figure 5b. Citations for Academic Experts




Notes: Figure 5a presents the cumulative distribution function for the negative of the mean absolute error in forecast by the academic experts
(full professors, associate professors, and assistant professors, with the “other” category omitted), as well as for the PhD students. Figure 5b
splits the 208 academic experts into thirds based on Google Scholar citations.




                                                                      38
                                   Figure 6. Impact of Horizontal Expertise on Accuracy
                                           Figure 6a. Fields of Academic Experts




                          Figure 6b. Specialization in Behavioral Economics (PhD Students)




Notes: Figure 6a presents the cumulative distribution function for the negative of the mean absolute error for the 208 academic experts split into
four main fields based on the assessment of the authors. Figure 6b splits the PhD students participating depending on whether the (self-reported)
field of specialization is Behavioral Economics or other.




                                                                       39
      Figure 7. Impact of Contextual Expertise on Accuracy: Experience with MTurk among Experts




Notes: Figure 7 splits the academic experts into whether they self-reported to have used MTurk themselves.




                                                                    40
                Figure 8. Experts versus Non-Experts (Undergraduates, MBAs, MTurk Workers)
                                  Figure 8a. (Negative of) Mean Absolute Error




                                               Figure 8b. Rank-Order Correlation




Notes: Figures 8a and 8b compare the academic experts with groups of non-experts: undergraduate students, MBA students, and MTurk workers
making forecasts. Figure 8a displays the result for the benchmark measure of accuracy—absolute error—while Figure 8b displays the results for
the rank-order correlation measure.




                                                                    41
   Figure 9. Impact of Effort in Taking Task on Accuracy
Figure 9a-b. Time Taken in Completing the Survey, Deciles




                           42
                                                               Figures 9c-d. Expert Checked Task or Full Instructions




                                                         Figures 9e-f. Effect of Stake Size on Motivation, MTurk Sample




Notes: Figures 9a-b plot the accuracy for three groups of forecasters (academic experts; undergraduate, MBA, and PhD students; and MTurkers) as a function of how long they took to complete the
survey. Specifically, the figures plot the average accuracy by deciles in the time taken for survey completion, where the decile thresholds are computed using all three groups. Figures 9c-d split two of
the groups into whether they clicked into a link for a trial of the task or the link for additional instructions. (The MTurk group is excluded because no one in the group clicked on the link) Figures 9e-f
compare two MTurk subgroups who differ in the incentives for survey accuracy. The low-stake group is informed that 5 out of the responds would be eligible for up to $100 for accuracy. The high-stake
group is informed that each respondent will receive up to $5 for accuracy of the survey responses. Both groups experienced the task before making forecasts.


                                                                                                    43
                          Figures 10a-c. Impact of Confidence in One’s Own Expertise on Accuracy, by Confidence Level (0 to 15)




Notes: Figures 10a-c plots the average accuracy for three groups of forecasters (academic experts, undergraduate/MBA/PhD students, and MTurkers) by how confident the respondent felt about the
accuracy. In particular, each survey respondent indicated how many out of 15 forecasts he or she made were going to be accurate up to 100 points relative to the truth.




                                                                                             44
            Figures 11a-b. Impact of Revealed Expertise (Forecasting of 4c Piece Rate), by Decile




Notes: Figures 11a-b plot the average accuracy for three groups of forecasters (academic experts, undergraduate/MBA/ PhD students, and
MTurkers) by decile of a revealed-accuracy measure (the decile thresholds are computed using all three groups). Namely, we take the absolute
distance between the forecast and the actual effort for the 4-cent piece rate treatment, a treatment for which the forecast should not involve
behavioral factors. For these plots the accuracy measure is computed excluding the 4-cent treatment.




                                                                     45
               Figure 12. Superforecasters: Selecting Non-Experts to Match Accuracy of Experts
                                       Figure 12a. Individual Accuracy




                        Figure 12b. Wisdom-of-Crowds Accuracy (Groups of 20 Forecasters)




Notes: Figures 12a-b compare, for each of three groups of forecasters (academic experts, undergraduate and PhD students and MTurkers), the
accuracy of the overall group versus the accuracy of the top 20% (the “superforecasters”) according to the regression in Table 8. To compute the
superforcasters, we use a 10-fold method to ensure no in-sample overfitting. Figure 12a plots the distribution of the individual-level accuracy,
while Figure 12b plots the wisdom-of-crowds accuracy for groups of sample size 20, using 1,000 bootstraps.




                                                                      46
                                                 Figure 13. Beliefs about Expertise




Notes: Figure 13 compares the average accuracy of a group with the forecasted accuracy for that group by the 208 academic experts. Namely,
the red squares report the average forecast of the number of correct answers (within 100 points of the truth) out of 15. The forecast is averaged
across the academic experts making the forecast. The yellow circle represents the actual accuracy (number of correct answers within 100 points
of the truth) for that same group. For example, for the 15-most cited experts, this takes the top-15 experts in citations and compares the average
of their individual accuracy. Notice that the sample slightly differs from the overall sample to be consistent with the question asked. For MBAs
we only include Chicago MBAs and for PhDs we only include Berkeley and Chicago PhDs since the question mentioned only those groups (see
Appendix Figure 1b).




                                                                       47
                                                                              Figure 14. Calibration of Simple Model of Expertise
                                                               Figure 14a. Predictability of Accuracy from Treatment to Treatment, Calibration
                                                                       Correlation of Absolute Errors as Function of Heterogeneity of Forecaster Variance
                                                  0.35                                                                                          Observed Correlation,
Correlation of absolute error across treatments




                                                                                                                                                Non-experts
                                                  0.30


                                                  0.25


                                                  0.20


                                                  0.15


                                                  0.10


                                                  0.05                                  Observed Correlation,
                                                                                        Experts
                                                  0.00
                                                         9.0         9.5         10.0             10.5          11.0          11.5       12.0     12.5         13.0     13.5
                                                                                                          ln(standard deviation of σ²)

                                                                                                          Experts       Non-Experts


                                                               Figure 14b. Implied Heterogeneity in Forecaster Variance, Experts vs. non-Experts




                                                                                                                       48
                Figure 14c. Distribution of Absolute Error in Forecasts, Actual versus Calibration




 Notes: Figures 14a-b present evidence on the calibration of the simple model in Section 5 to the forecasting data. Figure 14a shows the result
of simulations to pin down the calibrated value of the heterogeneity in forecaster variance. When the heterogeneity is low (corresponding to
low values of the x axis), the forecasters all have similar informativeness. Hence, the absolute error in one treatment for a forecaster has a low
correlation to the absolute error in another treatment for that same forecaster, as featured on the y axis (the correlation would be close to the
zero if it were not for forecaster random effects on the overall forecast, also inducing a correlation). As the heterogeneity in precision increases,
the model predicts a higher correlation in absolute errors, as forecasters who have lower variance are more likely to be correct in both
treatments. The points in the graph reflect simulations with 1 million draws. We use Figure 14a to set the values of the inverse gamma
distribution for experts and non-experts. Figure 14b displays the implied heterogeneity in forecaster variance for the calibrated parameters.
Experts have less heterogeneity and a lower mean; non-experts are more likely to be in either tail of the distribution. Figure 14c displays the
distribution of the average absolute error over 15 treatments, with each forecaster as unit of observation. The figure compares the actual
patterns in the data with the implied distribution from the calibration.




                                                                        49
                          Table 1. Summary Statistics, All Groups of Forecasters
                                                     Academic Academic
                                                      Experts,   Experts,          Undergra
                                                     Invited to Completed   PhD     duate     MBA     Mturk
                                                     Participate  Survey  Students Students Students Workers
                                                         (1)        (2)      (3)      (4)      (5)     (6)
Academic Rank (Academic Experts)
  Assistant Professor                                     0.26              0.36
  Associate Professor                                     0.15              0.15
  Professor                                               0.55               0.45
  Other                                                   0.04               0.04
Citations (Academic Experts)
  Google Scholar Citations                               7714               6326
Primary Field (Academic Experts)
  Behavioral Econ.                                        0.31              0.36
  Applied Micro                                           0.17               0.19
  Economic Theory                                         0.09               0.07
  Econ. Lab Exper.                                        0.17               0.16
  Social Psych or Decision Making                         0.25               0.22
  Field Behavioral Econ. (PhDs)                                                               0.24
Heard of Mturk                                                               0.98             0.73           0.25            0.31              .
Used Mturk                                                                   0.51             0.17           0.03            0.02              .
Minutes Spent (capped at 50)                                                21.21            21.46          16.06           21.86          10.09
Clicked Practice Task                                                        0.44             0.48           0.11            0.12           0.00
Clicked Instructions                                                         0.22             0.18           0.01            0.04           0.00
Days Waited Till Survey Completion                                          11.36             3.90           2.99            2.47           0.00
Confidence (Expected No. Own
Forecasts Within 100 Pts. of Actual)                                         5.77             6.53           6.32            5.66           6.81
Absolute Error in 4c Treatment                                              88.34           103.89         162.80          125.57          265.22
Observations                                              314                 208             147             158            160             762
Notes: Table 1 presents summary statistics for the samples used in the survey: the academic experts (Columns 1 and 2), the PhD students (Column 3), the
undergraduate students (Column 4), the MBA students (Column 5), and the Mturk workers (Column 6). Columns 1 and 2 compare characteristics of the
overall sample of academic experts contacted (Column 1) versus the characteristics of the experts that completed the forecast survey (Column 2).




                                                                         50
                                     Table 2. Findings by Treatment: Effort in Experiment and Expert Forecasts
                                                                                                                                                                                      Error,   Percent
                                                                                                                                                 Absolute
                                                                                                                                                                                      Indiv.   Experts
                                                                                                                             Mean        Mean     Error,
  Category                                               Treatment                                                 N                                                                 Forecast Outperfo
                                                                                                                          Effort (s.e.) Forecast  Mean
                                                                                                                                                                                      (Mean     rming
                                                                                                                                                 Forecast
                                                                                                                                                                                     and s.d.)  Mean
       (1)                                                     (2)                                                (3)            (4)                (5)               (6)               (7)      (8)
              “Your score will not affect your payment in any way."                                               540     1521 (31.22)                                    Benchmark
              "As a bonus, you will be paid an extra 1 cent for every 100
                                                                                                                  558     2029 (27.47)                                    Benchmark
              points that you score.”
  Piece Rate “As a bonus, you will be paid an extra 10 cents for every 100
                                                                                                                  566     2175 (24.29)                                    Benchmark
              points that you score.”
              “As a bonus, you will be paid an extra 4 cents for every 100                                                                                                             88.34
                                                                                                                  562     2132 (26.41)             2057                75                                 67.31
              points that you score.”                                                                                                                                                 (111.78)
 Pay Enough “As a bonus, you will be paid an extra 1 cent for every 1,000                                                                                                              284.97
                                                                                                                  538     1883 (28.61)             1657               226                                 44.23
 or Don't Pay points that you score.”                                                                                                                                                 (195.38)
             "As a bonus, the Red Cross charitable fund will be given 1                                                                                                                164.37
    Social                                                                                                        554     1907 (26.86)             1894                13                                  3.85
             cent for every 100 points that you score.”                                                                                                                               (117.97)
Preferences:
             "As a bonus, the Red Cross charitable fund will be given 10                                                                                                               182.1
   Charity                                                                                                        549     1918 (25.93)             1997                79                                 16.83
             cents for every 100 points that you score.”                                                                                                                              (107.68)
    Social
             “In appreciation to you for performing this task, you will be
Preferences:                                                                                                                                                                           164.16
             paid a bonus of 40 cents. Your score will not affect your                                            545     1602 (29.77)             1709               107                                 53.85
     Gift                                                                                                                                                                              (165.6)
             payment in any way.“
 Exchange
             "As a bonus, you will be paid an extra 1 cent for every 100
             points that you score. This bonus will be paid to your account                                       544     2004 (27.38)             1933                71          92.2 (129.4)           65.38
             two weeks from today.“
 Discounting
             "As a bonus, you will be paid an extra 1 cent for every 100
                                                                                                                                                                                       114.67
             points that you score. This bonus will be paid to your account                                       550     1970 (28.68)             1895                75                                 57.69
                                                                                                                                                                                      (137.22)
             four weeks from today.“
             "As a bonus, you will be paid an extra 40 cents if you score at                                                                                                           186.42
                                                                                                                  545     2136 (24.66)             1955               181                                 62.02
             least 2,000 points."                                                                                                                                                      (142.7)
             "As a bonus, you will be paid an extra 40 cents. However, you
Gains versus                                                                                                                                                                           167.06
             will lose this bonus (it will not be placed in your account)                                         532     2155 (23.09)             2002               153                                  60.1
   Losses                                                                                                                                                                             (126.28)
             unless you score at least 2,000 points. “
             "As a bonus, you will be paid an extra 80 cents if you score at                                                                                                            188
                                                                                                                  532     2188 (22.99)             2007               181                                 55.29
             least 2,000 points.“                                                                                                                                                     (121.38)
             "As a bonus, you will have a 1% chance of being paid an
             extra $1 for every 100 points that you score. One out of every                                                                                                            222.37
                                                                                                                  555     1896 (28.44)             1967                71                                 12.98
     Risk    100 participants who perform this task will be randomly                                                                                                                  (139.87)
Aversion and chosen to be paid this reward.“
 Probability "As a bonus, you will have a 50% chance of being paid an
 Weighting extra 2 cents for every 100 points that you score. One out of                                                                                                               131.48
                                                                                                                  568     1977 (24.73)             1941                36                                 21.15
             two participants who perform this task will be randomly                                                                                                                  (126.66)
             chosen to be paid this reward."
             “Your score will not affect your payment in any way. In a
    Social                                                                                                                                                                             177.63
             previous version of this task, many participants were able to                                        526     1848 (32.14)             1877                29                                   6.7
Comparisons                                                                                                                                                                           (114.22)
             score more than 2,000 points.”
             “Your score will not affect your payment in any way. After you
                                                                                                                                                                                       196.21
  Ranking    play, we will show you how well you did relative to other                                            543     1761 (30.63)             1850                89                                 29.81
                                                                                                                                                                                      (155.38)
             participants who have previously done this task.“
              "Your score will not affect your payment in any way. We are
    Task     interested in how fast people choose to press digits and we                                                                                                               181.3
                                                                                                                  554     1740 (28.76)             1757                17                                  5.87
Significance would like you to do your very best. So please try as hard as                                                                                                            (142.24)
             you can."
Average Across the 15 (Non-Benchmark) Treatments                                                                                1941               1900                94              169.42             37.54
Notes: The Table lists the 18 treatments in the Mturk experiment. The treatments differ just in one paragraph explaining the task and in the vizualization of the points earned. Column (2) reports the key part of the
wording of the paragraph. For brevity, we omit from the description the sentence "This bonus will be paid to your account within 24 hours" which applies to all treatments with incentives other than in the Time
Preference ones where the payment is delayed. Notice that the bolding is for the benefit of the reader of the Table. In the actual description to the MTurk workers, the whole paragraph was bolded and underlined.
Column (1) reports the conceptual grouping of the treaments, Columns (3) and (4) report the number of MTurk subjects in that treatment and the mean number of points, with the standard errors. Column (5) reports
the mean forecast among the 208 experts of the points in that treatment. Columns (1)-(5) are reproduced from DellaVigna and Pope (2016). Column (6) reports the absolute error between the average effort and the
average expert forecast (the wisdowm of crowds measure), while Column (7) reports the average and the standard error of the absolute error in forecast for the individual expert. Finally, Column (8) reports the share
of individual expert forecasts with a lower error than the wisdom-of-crowds average forecast.




                                                                                                        51
               Table 3. Accuracy of Forecasts by Group of Forecasters versus Random Guesses
                                                    Average                 Accuracy of      %                            Wisdom of Crowds: Accuracy
                                                 Accuracy (and                 Mean     Forecasters                       Using Average of Simulated
                                                     s.d.) of                Forecast   Doing Better                      Group of Forecasters, Mean
                                                   Individual               (Wisdom of Than Mean                                   (and s.d.)
                                                   Forecasts                 Crowds)     Forecast                          Group of 5     Group of 20
                                                          (1)                      (2)                   (3)                      (4)                      (5)
Panel A. Mean Absolute Error
Groups
  Academic Experts (N=208)                        169.42 (56.11)                  93.48                4.33               113.98 (23.15)           98.80 (11.68)
  PhD Students (N=147)                            171.42 (76.05)                  91.65                 8.16              117.99 (31.07)            97.78 (14.43)
  Undergraduates (N=158)                         187.84 (85.97)                  87.86                 3.16               115.46 (35.30)            94.80 (17.80)
  MBA Students (N=160)                            198.17 (86.04)                 100.72                 8.11              129.31 (34.34)           110.65 (17.05)
  Mturk Workers (N=762)                          271.57 (144.81)                 146.93                17.85              173.01 (68.21)           150.93 (39.57)
Benchmark for Comparison
  Random Guess in 1000-2500                             415.99
  Random Guess in 1500-2200                             224.63
Panel B. Mean Squared Error
Groups
  Academic Experts (N=208)                       49822 (34087)                   12606                  2.88              20046 (7894)             14438 (3234)
  PhD Students (N=147)                           53081 (50081)                   11980                  6.12              21365 (11268)             13895 (4142)
  Undergraduates (N=158)                         60271 (61112)                   9769                   2.53              19883 (12267)             12336 (4645)
  MBA Students (N=160)                           69855 (63213)                   13334                  3.90              24676 (12661)             16156 (4781)
  Mturk Workers (N=762)                         128801 (130473)                  23660                  9.71              44747 (32929)            28931 (13868)
Benchmark for Comparison
  Random Guess in 1000-2500                            249534
  Random Guess in 1500-2200                             75423
Panel C. Rank-Order Correlation Between Actual Effort and Forecasts
Groups
  Academic Experts (N=208)       0.42 (0.32)        0.83          4.81                                                       0.65 (0.18)             0.76 (0.09)
  PhD Students (N=147)           0.48 (0.30)        0.86          6.80                                                       0.70 (0.18)             0.80 (0.09)
  Undergraduates (N=158)         0.45 (0.31)        0.87          5.06                                                       0.69 (0.17)             0.80 (0.09)
  MBA Students (N=160)           0.37 (0.33)        0.71         18.52                                                       0.56 (0.21)             0.67 (0.11)
  Mturk Workers (N=762)          0.42 (0.35)        0.95          0.26                                                       0.69 (0.20)             0.87 (0.07)
Benchmark for Comparison
  Random Guess in 1000-2500                              0.00
  Random Guess in 1500-2200                              0.00
Panel D. Correlation Between Actual Effort and Forecasts
Groups
  Academic Experts (N=208)       0.45 (0.29)         0.77                                              9.41                  0.64 (0.16)             0.73 (0.09)
  PhD Students (N=147)           0.51 (0.28)         0.86                                               4.86                 0.72 (0.15)             0.82 (0.07)
  Undergraduates (N=158)         0.49 (0.30)         0.89                                              3.90                  0.72 (0.16)             0.84 (0.07)
  MBA Students (N=160)           0.42 (0.32)         0.77                                              15.11                 0.62 (0.19)             0.72 (0.09)
  Mturk Workers (N=762)          0.43 (0.35)         0.95                                              0.00                  0.70 (0.19)             0.88 (0.06)
Benchmark for Comparison
  Random Guess in 1000-2500                              0.00
  Random Guess in 1500-2200                              0.00
Notes: The Table reports evidence on the accuracy of forecasts made by the five groups of forecasters: academic experts, PhD students, undergraduates, MBA students,
and MTurk workers. Panel A presents the results for the benchmark measure (mean absolute error), Panel B presents the results on mean squared error, Panel C on the
rank-order correlation between actual average effort and the forecast, and Panel D on the corresponding correlation. Within each Panel and for reach group, the table
reports the average individual accuracy across the forecasters in the group (Column 1) versus the accuracy of the average forecast in the group (Column 2). The
difference is often referred to as "wisdom of crowds". Column 3 displays the percent of individuals in the group with an accuracy at least as high as the wisdom-of-crowd
accuracy (Column 2). In Columns 4 and 5 we present counterfactuals on how much the distribution of accuracy would shift if instead of considering individual forecasts
(Column 1) we considered the accuracy of average forecasts made by groups of 5 (Column 4) or 20 (Column 5). Random guesses are from a uniform distribution in (1000,
2500) and (1500, 2200), respectively.




                                                                                  52
  Table 4. Impact of Vertical , Horizontal , and Contextual Expertise on Forecast Accuracy
Dep. Var. (Measure of Accuracy):        (Negative of) Absolute Forecast Error in Treatment t by Forecaster i
                                            (1)        (2)      (3)      (4)      (5)      (6)         (7)
Measures of Vertical Expertise (Omitted: Assistant Professor)
  Associate Professor                    -23.86**                      -15.28                        -17.47
                                          (11.43)                     (13.07)                       (13.29)
    Full Professor                                                -16.03*                                          -6.99                                            -9.00
                                                                   (8.58)                                         (13.73)                                          (13.81)
    Other (Post-Doc or Research                                    16.08                                           13.82                                            13.16
    Scientist)                                                    (12.15)                                         (12.08)                                          (12.47)
    PhD Student                                                     -8.39
                                                                   (8.45)
    Ln (Google Scholar Citations)                  -4.36**                                                         -2.46                                            -2.26
                                                    (1.92)                                                         (3.25)                                           (3.24)
Main Field of Expertise (Omitted: Behavioral Economics)
  Applied Microeconomics                                                                             -6.47          -4.20                                            -4.23
                                                                                                    (9.25)         (9.36)                                           (9.30)
    Economic Theory                                                                                 -13.00         -12.77                                           -12.61
                                                                                                   (13.67)        (13.60)                                          (13.86)
    Laboratory Experiments                                                                           -5.27          -2.05                                            -2.21
                                                                                                   (12.57)        (12.19)                                          (12.21)
    Psychology or Behavioral Decision-                                                             -22.51*        -14.14                                            -10.89
    Making                                                                                         (11.51)        (12.13)                                          (12.91)
Measures of Horizontal Expertise
  Expert i Has Written Paper on                                                                                                    -8.61
  Topic of Treatment t                                                                                                            (8.44)
  Expert i Has Written Seminal Paper                                                                                              16.01
  on Topic of Treatment t                                                                                                        (24.36)
  Primary Field of PhD student is                                                                                                                 -6.85
  Behavioral Economics                                                                                                                           (15.78)
Measure of Contextual Expertise
  Has Used Mturk in Own Research                                                                                                                                    -6.05
  (Self-Reported)                                                                                                                                                   (8.32)
    Fixed Effects for Forecaster i :                                                                         X
    Controls:                                                      Fixed Effects for Treatment 1-15 and for Order 1-15 of Treatments
                                                                Academic
                                                                                                                               Academic
                                                                 Experts               Academic Experts              PhDs
                                                                                                                                Experts
    Sample:                                                     and PhDs
N                                                                   5325              3120           3120           3120          3120            2205              3120
R Squared                                                           0.085             0.117          0.117          0.121         0.263           0.057             0.121
Notes: The table reports the result of OLS regressions of measures of forecast accuracy on expertise measures. The dependent variable is the (negative of) the
absolute forecast error and an observation in the regression is a forecaster-treatment combination, with each forecaster providing forecasts for 15 treatments. Column
(2) uses as control variable the log of the Google Scholar citations for the researcher. Column (5) includes as horizontal measure of expertise an indicator for whether the
expert has written a paper on the topic of the relevant treatment. This specification also includes fixed effects for the expert i (unlike the other columns). All specifications
include fixed effects for the order in which the expert encountered a treatment (to control for fatigue) and fixed effects for the treatment.




                                                                                      53
                                                           Table 5. Experts versus Non-Experts
                                 (Neg.) Absolute       (Neg.) Squared                                                         Rank-Order                      Simple
Dep. Var. (Measure of           Forecast Error in   Forecast Error / 100 in                                                  Correlation for               Correlation for
Accuracy):                     Treat. t by Forec. i  Treat. t by Forec. i                                                     Forecaster i                  Forecaster i
                                         (1)                  (2)                                                                  (3)                           (4)
Indicator for Group (Omitted Category: Academic Experts)
   Undergraduate Students            -18.42**              -104.49*                                                                 0.037                          0.041
                                       (7.88)               (54.12)                                                                (0.033)                        (0.031)
   MBA Students                     -28.76***             -200.33***                                                                -0.040                         -0.025
                                       (7.85)               (55.35)                                                                (0.034)                        (0.032)
    Mturk Workers                                        -102.15***               -789.79***              0.010              -0.017
                                                           (6.54)                   (52.91)              (0.025)            (0.024)
                                                   Fixed Effects for Treatment 1-15 and for Order 1-
    Fixed Effects:                                                 15 of Treatments
    Sample:                                                Academic Experts, Undergraduate Students, MBA Students, Mturk Workers

N                                                              19320                           19320                                1288                           1288
R Squared                                                      0.061                           0.047                                0.004                          0.004
Notes: The table reports the result of OLS regressions of measures of forecast accuracy on other forms of expertise. In Columns (1)-(2) the dependent variable is the (negative of)
the absolute (respecitively, squared in Column 2) forecast error and an observation in the regression is a forecaster-treatment combination. In Columns (3)-(4), the dependent
variable is the rank-order correlation (respecitively, simple correlation in Column 4) between forecast and actual effort across the 15 treatments, and each observation is a
forecaster i . Columns (1)-(2) include fixed effects for the order in which the expert encountered a treatment (to control for fatigue) and fixed effects for the treatment.
* significant at 10%; ** significant at 5%; *** significant at 1%




                                                                                       54
                                   Table 6. Impact of Effort and Motivation on Forecast Accuracy
                                  (Negative of) Absolute Forecast Error in                                                      Rank-Order Correlation between
Dep. Var. (Measure of Accuracy):         Treatment t by Forecaster i                                                          Forecasts and Effort by Forecaster i
                                     (1)         (2)       (3)       (4)                                                       (5)        (6)         (7)      (8)
Time to Completion (Omitted 5-10 minutes)
  Survey Completion Time              .       -112.21** -61.26***                                                                 .           -0.374***        -0.308***
  0-5 Minutes                                  (52.13)   (20.83)                                                                               (0.132)          (0.046)
    Survey Completion Time                                 -11.39            5.96          33.80***                         -0.152**            -0.009           0.026
    10-15 Minutes                                         (11.22)          (12.76)         (12.21)                           (0.072)           (0.047)          (0.028)
    Survey Completion Time                                 -10.48          22.21*          42.82***                         -0.195***           -0.034           0.001
    15-25 Minutes                                         (12.11)          (12.02)         (14.48)                           (0.066)           (0.044)          (0.037)
  Survey Completion Time             -23.96**                               21.02           -22.05                          -0.294***           0.068           -0.115
  25+ Minutes                         (11.50)                              (12.90)          (33.67)                          (0.071)           (0.047)          (0.100)
Measures of Attention to Instructions
    Clicked on Practice Task                               -2.43             -2.94              .                             -0.066            0.035                .
                                                          (8.51)            (9.94)                                           (0.053)           (0.039)
    Clicked on Full Instructions                           0.81            -29.57*              .                            0.104*           -0.133**               .
                                                          (10.46)          (16.73)                                           (0.058)           (0.061)
Delay in Survey Completion
   Days Waited to Take Survey                              -0.07            -0.03               .                             0.000             0.000                .
   (Since Invitation)                                     (0.24)            (0.87)                                           (0.001)           (0.002)
Incentives for Forecast Accuracy
   Higher Incentives (up to $5)                                                                             -17.40                                                               0.017
   for Forecast Accuracy (Mturks)                                                                          (11.73)                                                              (0.026)
   Controls for Expertise:                                  X                                                                    X
                                                         Fixed Effects for Treatment 1-15 and for
    Fixed Effects:                                              Order 1-15 of Treatments
    Sample Indicators Interacted
    with Fixed Effects:                                                        X
    Indicators for Samples:                                                                                                                     X
                                                                          PhDs,                                                               PhDs,
                                                       Academic          Undergr.,            Mturk Workers                Academic          Undergr.,            Mturk Workers
    Sample:                                             Experts           MBAs                                              Experts           MBAs
N                                                          3120             6975            11430           11430              208               465              762             762
R Squared                                                  0.123            0.071           0.032           0.018             0.120             0.069            0.067           0.001
Notes: The table reports the result of OLS regressions of measures of forecast accuracy on measures of effort and motivation. In Columns (1)-(4) the dependent variable is the (negative
of) the absolute forecast error and an observation in the regression is a forecaster-treatment combination, with each forecaster providing forecasts for 15 treatments. In Columns (5)-(8),
the dependent variable is the rank-order correlation between forecast and actual effort across the 15 treatments, and each observation is a forecaster i. The specification in Columns (1)
and (5) include controls for rank and for field of expertise of the academic expert. The time of survey completion is measured between the logged opening time and the logged
submission time. Each forecaster has the option to click and open a practice task and/or to click or open the PDF with full instructions. Indicators for either are measures of forecaster
effort. A further measure of motivation is the delay in days between when the forecasters were invited and when the survey was completed. In Columns (4) and (8) we compare MTurk
workers with baseline incentives for forecast accuracy and with heightened incentives. Columns (1)-(4) include fixed effects for the order in which the expert encountered a treatment (to
control for fatigue) and fixed effects for the treatment.
* significant at 10%; ** significant at 5%; *** significant at 1%




                                                                                           55
                                                                     Table 7. Impact of Confidence on Forecast Accuracy
                                                                                  (Negative of) Absolute                             Forecast Within 100 Points of                              Rank-Order Correlation
                                                                               Forecast Error in Treatment t                         Actual Effort in Treatment t for                        between Forecasts and Effort
Dep. Var. (Measure of Accuracy):                                                     by Forecaster i                                          Forecaster i                                         by Forecaster i
                                                                                 (1)         (2)       (3)                              (4)          (5)        (6)                            (7)       (8)         (9)
Measures of Confidence
  Number of Own Forecasts Expected To Be                                          1.57      5.03***     8.78***      0.001       0.007**     0.009***                                          -0.007          0.018***            -0.002
  Within 100 Points of Actual (Out of 15)                                        (1.39)      (1.35)      (1.77)     (0.004)      (0.003)     (0.002)                                          (0.009)          (0.005)            (0.004)
  Fixed Effects:                                                                 Fixed Effects for Treatment 1-15 and for Order 1-15 of Treatments
  Sample Indictators Interacted with Fixed
  Effects:                                                                                            X                                                      X
  Indicators for Sample:                                                                                                                                                                                            X
  Indicator for Missing Confidence Variable:                                         X                X                X                  X                  X                 X                  X                 X                 X
  Controls for Time to Completion:                                                   X                X                X                  X                  X                 X                  X                 X                 X
  Controls for Expertise:                                                            X                                                    X                                                       X
                                                                                                  PhDs,                                                 PhDs,                                                  PhDs,
                                                                                                                   Mturk                                                   Mturk                                                  Mturk
                                                                              Academic           Undergr.,                          Academic           Undergr.,                            Academic          Undergr.,
                                                                                                                  Workers                                                 Workers                                                Workers
    Sample:                                                                    Experts            MBAs                               Experts            MBAs                                 Experts           MBAs
N                                                                                 3120              6975            11430               3120              6975              11430               208               465               762
R Squared                                                                         0.124             0.078           0.045               0.173             0.107             0.042              0.129             0.088             0.068
Notes: The table reports the result of OLS regressions of measures of forecast accuracy on measures of confidence. In Columns (1)-(3) the dependent variable is the (negative of) the absolute forecast error and in Columns (4)-(6) the
dependent variable is an indicator for whether the forecast falls within 100 points of the actual average effort in the treatment. In these columns, an observation in the regression is a forecaster-treatment combination, with each
forecaster providing forecasts for 15 treatments. In Columns (7)-(9), the dependent variable is the rank-order correlation between forecast and actual effort across the 15 treatments, and each observation is a forecaster i. The measure
of confidence is the forecast by the participant of the number of treatments that he/she expects to get within 100 points of the actual one. This variable varies from 0 (no confidence) to 15 (confidence in perfect forecast). All columns
include the controls for time of completion used in Table 6, as well as an indicator for the few observations in which the confidence variable is missing (in which case the confidence variable itself is seto to zero). The specifications in
Columns (1), (4), and (7) also includes controls for rank and for field of expertise of the academic experts. Columns (1) to (6) include fixed effects for the order in which the expert encountered a treatment (to control for fatigue) and
fixed effects for the treatment
* significant at 10%; ** significant at 5%; *** significant at 1%




                                                                                                                     56
                                                   Table 8. Impact of Revealed Accuracy
                                                                            (Negative of) Absolute                               Rank-Order Correlation
                                                                         Forecast Error in Treatment t                        between Forecasts and Effort
Dep. Var. (Measure of Accuracy):                                               by Forecaster i                                       by Forecaster i
                                                                           (1)         (2)       (3)                             (4)       (5)        (6)
Measures of Revealed Accuracy
  (Negative of) Absolute Error in Forecast                                 9.53**          23.83***        31.37***             0.015            0.023**         0.032***
  of 4-cent Piece Rate Treatment / 100                                     (3.72)           (3.08)          (1.89)             (0.016)           (0.009)         (0.005)
Controls for Time to Completion (Omitted 5-10 minutes)
  Survey Completion Time                         .                                          -36.88          -17.16                  .            -0.255*        -0.287***
  0-5 Minutes                                                                               (41.39)         (16.79)                              (0.146)         (0.050)
    Survey Completion Time                                                 -15.53           -13.24          18.72**           -0.171**           -0.028            0.016
    10-15 Minutes                                                         (11.95)           (10.56)          (9.46)            (0.071)           (0.048)          (0.027)
    Survey Completion Time                                                 -13.88            -7.09          19.99*            -0.215***          -0.065           -0.012
    15-25 Minutes                                                         (13.09)            (9.30)         (12.03)            (0.067)           (0.047)          (0.036)
  Survey Completion Time                                                 -29.78**            -1.65           -13.05           -0.289***           0.044            -0.105
  25+ Minutes                                                             (12.85)           (10.28)         (21.68)            (0.073)           (0.048)          (0.090)
Control for Confidence
  Number of Own Answers Expected                                          0.44       4.07***    5.49***                         -0.008          0.017***           -0.005
  Within 100 Points of Actual                                            (1.47)       (1.20)     (1.38)                        (0.009)          (0.005)           (0.004)
                                                                       Fixed Effects for Treatment 2-15
    Fixed Effects:                                                     and for Order 1-15 of Treatments
    Sample Indictators Interacted with Fixed
    Effects:                                                                                    X
    Indicators for Sample:                                                                                                                           X
    Indicator for Missing Confidence Variable:                                X                 X               X                  X                 X                X
    Controls for Expertise:                                                   X                                                    X
                                                                                           PhDs,                                                PhDs,
                                                                        Academic                     Mturk                   Academic                            Mturk
                                                                                          Undergr.,                                            Undergr.,
    Sample:                                                              Experts                    Workers                   Experts                           Workers
                                                                                           MBAs                                                 MBAs
N                                                                          2912              6510            10668               208               465              762
R Squared                                                                  0.115             0.122           0.163              0.136             0.096            0.128
Notes: The table reports the result of OLS regressions of forecast accuracy on measures of revealed forecasting accuracy. In Columns (1)-(3) the dependent variable is the
(negative of) the absolute forecast error and an observation in the regression is a forecaster-treatment combination, with each forecaster providing forecasts for 15
treatments. In Columns (4)-(6), the dependent variable is the rank-order correlation between forecast and actual effort across the 15 treatments, and each observation is a
forecaster i. These regressions examine whether being more accurate in the forecast of a (non-behavioral) treatment increases the accuracy of forecasts in other
treatments as well. The regressions also includes an indicator for missing confidence, as well as the other listed variables. The specifications in Columns (1) and (4) also
includes controls for rank and for field of expertise of the academic experts. Columns (1)-(3) include fixed effects for the order in which the expert encountered a treatment
(to control for fatigue) and fixed effects for the treatment.
* significant at 10%; ** significant at 5%; *** significant at 1%




                                                                                    57
                                                            Table 9. Impact of Revealed Accuracy by Groups of Treatments

Dep. Var. (Measure of Accuracy):                                                                 (Negative of) Absolute Forecast Error in Treatment t by Forecaster i
                                                                           4-cent                 Pay                    Gift      Discounti Gains vs.        Prob.                                                    Psychology
Group of Treatments Omitted:                                             Piece Rate             Enough     Charity    Exchange         ng       Losses      Weighting                                                  Treatments
                                                                             (1)                  (2)        (3)         (4)           (5)        (6)          (7)                                                         (8)
Panel A. Forecasts by Academic Experts
  (Negative of) Absolute Error in Forecast                                   9.53**               7.52***          18.55***              3.83                8.46**              -4.01              17.85***               9.88***
  in Relevant Treatments / 100                                               (3.72)               (2.09)            (3.55)              (3.06)               (3.70)              (2.87)              (4.50)                (3.60)
    Fixed Effects:                                                                                       Fixed Effects for Treatment 1-15 and for Order 1-15 of Treatments
    Controls for Expertise, Confidence and
    Time to Completion:                                                          X                   X                  X                   X          X                            X                    X                     X
    Sample:                                                                                                                                 Academic Experts
N                                                                             2912                 2912              2704                2912                2704                2496                 2704                  2496
R Squared                                                                     0.115                0.102             0.149               0.137               0.112               0.150                0.137                 0.153

Panel B. Forecasts by PhDs, Undergrads, MBAs, Mturks
  (Negative of) Absolute Error in Forecast   29.81***                                            28.32***          39.13***            17.20***            34.19***            28.83***             39.90***              43.33***
  in Relevant Treatments / 100                (1.66)                                              (1.77)            (1.98)              (2.04)              (1.76)              (2.50)               (2.21)                (2.81)
    Fixed Effects:                                                          Fixed Effects for Treatment 1-15 and for Order 1-15 of Treatments, interacted with the Sample indicators
    Controls for Confidence and Time to
    Completion:                                                                  X                   X                  X                   X                   X                   X                    X                     X
  Sample:                                                                                                               PhDs, Undergraduates, MBAs, Mturkers
N                                                                            17178                17178             15951      17178        15951       14724                                        15951                  14724
R Squared                                                                    0.181                0.171             0.195      0.114        0.200       0.144                                        0.197                  0.181
Notes: The table reports the result of OLS regressions of forecast accuracy on measures of revealed forecasting accuracy in other treatments. Each column reports the regression of forecaster accuracy as function of accuracy in
the identified treatments (leaving those treatments outside the sample). Thus, for example, in Column (2) we examine whether accuracy in forecasting the pay-enough-or-dont-pay-at-all treatment increases accuracy in forecast for
the other treatments. Panel A reports the results for the sample of academic experts, while Panel B reports the results for the sample of PhD students, undergraduates, MBAs, and MTurkers. The regression includes the same
controls for confidence and time to completion as in Table 8. The specification in Panel A also includes controls for rank and for field of expertise of the academic experts. All columns include fixed effects for the order in which the
expert encountered a treatment (to control for fatigue) and fixed effects for the treatment.
* significant at 10%; ** significant at 5%; *** significant at 1%




                                                                                                                   58
                                               Table 10. Accuracy of Optimal Forecasters
                                                                          Individual Accuracy                           Wisdom-of-Crowds
                                                                       Average        Difference in                  Accuracy    Difference in
                                                                    Accuracy (and      Accuracy                     (and s.d. of   Accuracy
                                                                        s.d.) of     Relative to All               bootstrap) of Relative to All
                                                                      Individual       Academic                        Mean       Academic
                                                                      Forecasts          Experts                     Forecast       Experts
                                                                          (1)                    (2)                       (3)                     (4)
Mean Absolute Error
Panel A. Academic Experts
  All Academic Experts (N=208)                                          175.21                                           94.76
                                                                        (58.37)                                          (4.14)
    Optimal 20% (4ct Control) (N=42)                                    173.11                  -2.63                    98.02
                                                                        (50.83)                (9.07)                    (7.13)
    Optimal 10% (4ct Control) (N=21)                                    167.61                  -8.45                    92.25
                                                                        (53.12)               (12.13)                     (9.6)
    Optimal 20% (No 4ct Control) (N=42)                                 174.57                  -0.8                      93.4
                                                                        (60.25)               (10.25)                    (7.86)
Panel B. PhD/Undergraduates/MBA
  All PhD/UG/MBA (N=465)                                                188.89               13.68**                      92.4                    -2.36
                                                                        (83.25)                (5.59)                    (3.06)                   (5.35)
    Optimal 20% (4ct Control) (N=93)                                    149.97               -25.24***                   69.02                 -25.74***
                                                                        (53.41)                (6.84)                    (4.41)                   (6.86)
    Optimal 10% (4ct Control) (N=47)                                    149.31               -25.90***                   73.83                  -20.93**
                                                                        (44.95)                (7.66)                    (7.37)                  (9.391)
    Optimal 20% (No 4ct Control) (N=93)                                 163.46                -11.75                     76.06                  -18.7***
                                                                        (64.83)                (7.82)                    (5.04)                   (7.05)
Panel C. Mturks
  All Mturks (N=762)                                                    272.02               96.81***                   143.82                 49.06***
                                                                       (143.23)               (6.58)                     (6.47)                 (12.58)
    Optimal 20% (4ct Control) (N=152)                                   184.04                 8.83                      66.68                 -28.08***
                                                                        (80.32)               (7.66)                     (7.43)                  (7.99)
    Optimal 10% (4ct Control) (N=76)                                    182.46                 7.25                      69.99                 -24.77***
                                                                        (75.17)               (9.49)                     (9.71)                  (9.01)
    Optimal 20% (No 4ct Control) (N=152)                                223.24               48.03***                   106.94                   12.18
                                                                       (133.34)              (11.53)                    (11.96)                 (11.31)
Notes: The table reports the absolute error at both the individual and windom-of-crowds level for different groups, including "superforecasters". Panel A depicts
the academic experts, Panel B the students, and Panel C the Mturk workers. Within each panel, we consider the overall group and three subsamples of optimal
forecasters. The subsamples are generated with a regression as in Table 8, determining with a 10-fold method the 20% or 10% predicted optimal forecasters out
of sample. The last group of optimal forecasters is generated not using the revealed-accuracy variable based on the forecast for the 4-cent treatment. In Column
(1) we report the average individual accuracy for the groups, and in Column (2) we test for difference relative to the sample of all 208 academic experts. In
Column (3) we present wisdow-of-crowd accuracy for each of the groups, computing the absolute error for the average forecast in the group. In Column (4) we
test whether this wisdom-of-crowd error differs from the wisdom-of-crowd error for the individual experts. To test for this difference, we perform 1,000 bootstraps
from the underlaying sample and use the bootstrapped samples to infer the standard error of the difference.
* significant at 10%; ** significant at 5%; *** significant at 1%




                                                                                59
                Table 11. Calibration of Model of Forecaster Expertise on Data
                                                                        Academic Experts                        Non-Experts
                                                                                   (1)                                 (2)
Panel A. Moments from Data
  S.d. of Forecast Error                                                         219.4                                301.2
    S.d. of Wisdom-of-Crowd Error                                                108.1                                67.1
    S.d. of Average Error for Forecaster                                          93.4                                178.7
    Average Bias in Forecast                                                      -41.0                              -110.0
    Correlation of Absolute Error                                                 0.09                                0.29
    Across Treatments (Table 9, Column 1)
Panel B. Implied Calibrated Model Parameters
  S.d. of Treatment-Specific Error                                               107.3                                66.6
    S.d. of Forecaster-Specific Error                                             76.9                                166.5
    Mean S.d. of Idiosyncratic Variance                                          175.2                                241.9
    Alpha in Inverse Gamma                                                         3.6                                 2.0
    Beta in Inverse Gamma                                                        79,579                              59,549
                                                                        Data         Calibration            Data        Calibration
                                                                         (1)               (2)               (3)              (4)
Panel C. Moments Implied by Calibrated Model
Absolute Error, Individual and Wisdom of Crowds:
  Average Individual Absolute Error              169.4                                    175.0             238.7            246.1
    Wisdom-of-Crowds Absolute Error                                      93.5             92.7              116.2            114.1
    Average Absolute Error with 5 Forecasters                           114.0             114.5             149.0            148.9

Superforecasters (top 20% based on one treatment):
  Average Individual Absolute Error for Superf. 173.1                                     167.8             171.1            203.2
  Wisdom-of-Crowds Absolute Error for Superf.     98                                       94.1              60.7             79.8
Rank-Order Correlation:
  Average Individual Rank-Order Correlation                             0.413            0.623              0.427            0.592
  Wisdom-of-Crowds Rank-Order Correlation                               0.828            0.819              0.929            0.908
Percent Individual Forecasters Outperforming
  Wisdom-of-Crowds Absolute Error                                        4.3               1.9               10.1             6.1
  Wisdom-of-Crowds Rank-Order Correlation                                4.8               7.8               0.7              1.4
Notes: The table reports the calibration of a simple model presented in Section 5. We do the calibration separately for the 208 experts
(Column 1) and the other forecasters, which we label non-experts (Column 2). Panel A presents the key moments that we use to calibrate
the data, and Panel B presents the implied calibrated parameter values for the model parameters, given the moments in Panel A. Panel C
then presents the predictions of the model regarding some key other moments in the data, compared to the realizations in the data. Panel C
presents the average of the featured statistic over 1,000 draws, with each draw mirroring the sample size of forecasters and the number of
forecasted treatments (15).




                                                                        60
           Appendix Figure 1. Expert Survey, Screenshots
Appendix Figure 1a. Expert Survey, Screenshots from Page 1 of Survey




                                61
                       Appendix Figure 1b. Expert Survey, Screenshot from Page 2 of Survey




Notes: Appendix Figures 1a-b show screenshots reproducing portions of the Qualtrics survey which experts used to make forecasts. Page1 of the
survey features 15 sliders, one for each treatment (given that the results for 3 treatments were provided as a benchmark). For each treatment,
the left side displays the treatment-specific wording which the subjects assigned to that treatment saw, and on the right side a slider which the
experts can move to make a forecast. Page2 of the survey included a forecast of accuracy for oneself and different groups, reproduced in Appendix
Figure 1b.




                                                                      62
Appendix Figures 2a-d. Wisdom-of-Crowds Accuracy, Other Groups
               Appendix Figure 2a. PhD Students




          Appendix Figure 2b. Undergraduate Students




                             63
                                                Appendix Figure 2c. MBA Students




                                               Appendix Figure 2d. MTurk Workers




Notes: These figures present the parallel evidence to Figure 1 for the other samples of forecasters.




                                                                       64
                                            Appendix Figures 3a-d. Average Forecast Across All 15 Treatments, Key Findings




Notes: These figures present evidence on the average forecast across the 15 treatments by forecasters for key findings. Appendix Figure 3a shows that MTurkers are much more likely to have offered
a low forecast relative to the average effort (vertical red line). Appendix Figures 3b-d show that the average forecast increases in the time taken to do the survey (Figure 3b), in the confidence (Figure
3c), and in the accuracy of forecast of the 4c treatment (Figure 3d). This explains why these variable predict the average absolute error more than they predict the rank-order correlation.



                                                                                                   65
                      Appendix Figures 4a-c. Key Findings on Vertical, Horizontal, and Contextual Expertise, Rank-Order Correlation
                      Appendix Figure 4a. Vertical Expertise: Citations              Appendix Figure 4b. Horizontal Expertise: Field




           Appendix Figure 4c. Contextual Expertise: Experience with MTurk




Notes: These figures replicate key results on vertical, horizontal, and contextual expertise using the rank-order correlation measure.


                                                                                                   66
                                                                           Appendix Table 1. Calibration of Model, Robustness
                                                                                                           Without Bias and                                                                                  Without Heterog. in
                                                                                Data and Baseline                             Without Treatment-
                                                                                                         Forecaster-Specific                                                                                    Idiosyncratic
                                                                                    Calibration                                 Specific Error
                                                                                                                Error                                                                                             Variance
                                                                                    Academic                 Non-               Academic             Non-              Academic              Non-            Academic              Non-
                                                                                     Experts                Experts              Experts            Experts             Experts             Experts           Experts             Experts
                                                                                          (1)                  (2)                   (3)                (4)                 (5)                (6)                 (7)               (8)
Panel A. Implied Calibrated Model Parameters
  S.d. of Treatment-Specific Error                                                      107.3                  66.6                107.3               66.6                   0                  0               107.3              66.6
    S.d. of Forecaster-Specific Error                                                    76.9                 166.5                    0                  0                76.9               166.5               76.9              166.5
                                                                                                                                                                                                                175.2,            241.9,
    Mean S.d. of Idiosyncratic Variance                                                 175.2                 241.9                191.3              293.7                205.4              250.9            constant          constant
    Bias in Forecast                                                                    -41.0                -110.0                    0                  0                -41.0             -110.0               -41.0            -110.0
    Alpha in Inverse Gamma                                                                3.6                  2.0                   4.3                2.0                 5.0                 2.0                n/a               n/a
    Beta in Inverse Gamma                                                              79,579                59,549               119,336            89,545              170,980             63,420                n/a               n/a
Panel B. Moments Implied by Calibrated Model                                      Data Calib.            Data Calib.                    Calibration                            Calibration                            Calibration
  Average Individual Absolute Error                                               169.4 175.0             239 246.1                171.2              216.4                174.6              244.3              178.2              256.4
    Wisdom-of-Crowds Absolute Error                                                93.5         92.7     116.2 114.1                86.2               53.9                41.2               110.5               92.4              113.4
    Average Absolute Error with 5 Forecasters                                     114.0 114.5            149.0 148.9               109.3              114.4                85.0               140.9              114.4              149.3
Superforecasters (top 20% based on one treatment):
  Average Individual Absolute Error for Superf.                                   173.1 167.8             171 203.2                165.6              189.2                165.4              197.1              176.0              236.9
  Wisdom-of-Crowds Absolute Error for Superf.                                       98          94.1      60.7      79.8            88.3               55.2                40.1                62.4               93.6              86.2
Rank-Order Correlation:
  Average Individual Rank-Order Correlation                                       0.413 0.623             0.43 0.592               0.590              0.532                0.622              0.605              0.600              0.523
    Wisdom-of-Crowds Rank-Order Correlation                                        0.83      0.819        0.93 0.908               0.816              0.910                0.989              0.996              0.817              0.908
Percent Individual Forecasters Outperforming
  Wisdom-of-Crowds Absolute Error                                                   4.3         1.9       10.1       6.1             1.0                0.0                 0.0                 6.3                0.7               0.5
  Wisdom-of-Crowds Rank-Order Correlation                                           4.8         7.8       0.7        1.4             6.3                0.7                 0.0                 0.0                5.9               0.4
Cross-Treatment Correlation of Absolute Error
  Avg. Regression Correlation of Abs. Errors                                      0.090 0.089 0.029 0.266                          0.076              0.250                0.090              0.281              0.018              0.126
Notes: The table reports the calibration of a simple model presented in Section 5. We do the calibration separately for the 208 experts and the other forecasters, which we label non-experts. Panel A presents the implied calibrated parameter
values for the model parameters under the robustness assumption, given the moments in Panel A of Table 11. Panel B then presents the predictions of the model regarding some key other moments in the data, compared to the realizations
in the data. Panel B presents the average of the featured statistic over 1,000 draws, with each draw mirroring the sample size of forecasters and the number of forecasted treatments (15).




                                                                                                                      67
