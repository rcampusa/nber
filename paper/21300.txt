                              NBER WORKING PAPER SERIES



THE EFFECTS OF EARNINGS DISCLOSURE ON COLLEGE ENROLLMENT DECISIONS

                                        Justine Hastings
                                     Christopher A. Neilson
                                      Seth D. Zimmerman

                                       Working Paper 21300
                               http://www.nber.org/papers/w21300

                     NATIONAL BUREAU OF ECONOMIC RESEARCH
                              1050 Massachusetts Avenue
                                Cambridge, MA 02138
                                     June 2015

The views expressed herein are those of the authors and do not necessarily reflect the views of the
National Bureau of Economic Research. This paper was previously titled “Student Loans, College
Choice and Information on the Returns to Higher Education.” We thank Noele Aabye, Phillip
Ross, Unika Shrestha, Anthony Thomas, and Lindsey Wilson for outstanding research assistance.
We thank Nadia Vasquez, Pablo Maino, Valeria Maino, and Jatin Patel for assistance with
locating, collecting and digitizing data records. J-PAL Latin America and in particular Elizabeth
Coble provided excellent field assistance. We thank Ivan Silva of DEMRE for help locating and
accessing data records. We thank the excellent leadership and staff at the Chilean Ministry of
Education for their invaluable support of these projects including Harald Beyer, Fernando Rojas,
Loreto Cox Alcaíno, Andrés Barrios, Fernando Claro, Francisco Lagos, Lorena Silva, Anely
Ramirez, and Rodrigo Rolando.We also thank the outstanding leadership and staff at Servicio de
Impuestos Internos for their invaluable assistance. We thank Gene Amromin, John Friedman,
Brad Larsen, Brigitte Madrian, Judith Scott-Clayton, Jesse Shapiro, Will Dobbie, and seminar
participants at the NBER Household Finance Meetings and Public Economics Meetings, Harvard
University, Princeton University, Columbia University, Stanford University, University of
Chicago Harris School, Rutgers University, University of Toulouse, Paris School of Economics,
Pontificia Universidad Catolica de Chile, Vanderbilt University, Uppsala University, University
of Memphis, University of Illinois-Chicago, International Bank of Development, and the World
Bank for helpful comments. We thank Raj Chetty and Larry Katz for helpful discussions during
the field experiment design. This project was funded by Brown University, the Population Studies
and Training Center at Brown, and NIA grant P30AG012810. Required disclosure: Information
contained herein comes from taxpayers' records obtained by the Chilean Internal Revenue Service
(Servicio de Impuestos Internos), which was collected for tax purposes. Let the record state that
the Internal Revenue Service assumes no responsibility or guarantee of any kind for the use or
application made of the aforementioned information, especially in regard to the accuracy,
currency, or integrity.

NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.

© 2015 by Justine Hastings, Christopher A. Neilson, and Seth D. Zimmerman. All rights
reserved. Short sections of text, not to exceed two paragraphs, may be quoted without explicit
permission provided that full credit, including © notice, is given to the source.
The Effects of Earnings Disclosure on College Enrollment Decisions
Justine Hastings, Christopher A. Neilson, and Seth D. Zimmerman
NBER Working Paper No. 21300
June 2015
JEL No. H0,H52,I22,I23,I24,I26,J3

                                         ABSTRACT

We use a large-scale survey and field experiment to evaluate a policy that provided information
about college- and major-specific earnings and cost outcomes to college applicants in Chile. The
intervention was administered by the Chilean government and reached 30% of student loan
applicants. We show that the low-income and low-achieving students who apply to low-earning
college degree programs overestimate earnings for past graduates by over 100%, while beliefs for
high-achieving students are correctly centered. Treatment causes low-income students to reduce
their demand for low-return degrees by 4.6%, and increases the likelihood they remain in college
for at least four years. To understand the mechanisms driving the effect of disclosure policies we
estimate a model of college demand. We find that disclosure changes college choice by reducing
uncertainty about earnings outcomes, but that its impact is limited by strong student preferences
for non-pecuniary degree attributes.

Justine Hastings                                Seth D. Zimmerman
Brown University                                Booth School of Business
Department of Economics                         University of Chicago
64 Waterman Street                              5807 S. Woodlawn Avenue
Providence, RI 02912                            Chicago, IL 60637
and NBER                                        and NBER
justine.s.hastings@gmail.com                    seth.zimmerman@chicagobooth.edu

Christopher A. Neilson
Woodrow Wilson School
Princeton University
Firestone Library, Room A2H
Princeton, NJ 08544
and NBER
cneilson@princeton.edu




A randomized controlled trials registry entry is available at
https://www.socialscienceregistry.org/trials/598/history/4492
An Online Appendix is available at
http://www.nber.org/data-appendix/w21300
1     Introduction
This paper uses a large-scale field experiment and survey to study how beliefs about earnings
and costs affect students’ choices about where to enroll in college and what to study. Our con-
text is Chile, a middle-income OECD country with a higher education market in which private
and public universities compete for students who often finance their education with federally-
subsidized student loans. Facing protests over educational debt burdens and rising rates of stu-
dent loan default, the Chilean government set out to understand how students make enrollment
decisions and whether information about earnings and costs for different colleges and majors
can help them make wiser educational investments. We assisted policymakers with the design,
implementation, and evaluation of a web-based disclosure policy that reached 30% of student
loan applicants in 2012.
    Disclosure policies are important to study for two reasons. First, college enrollment choices
have large effects on earnings but are often made with imperfect information.1 In the US, over-
all increases in loan default in the 2000s were driven by borrowers at non-selective, for-profit
institutions, where default rates in some cases approached 50% (Looney and Yannelis 2015). Stu-
dents applying to high-default programs often report difficulty accessing reliable information on
program costs and benefits.2 These challenges may be largest for the low-income students who
make up the majority of enrollees at the worst-performing programs. Second, in an effort to im-
prove transparency and help students make better-informed college enrollment decisions, many
countries have adopted disclosure policies that collect and disseminate information on earnings
and costs, including the US, Australia, Colombia, Mexico, and Peru (Neilson et al. 2016).
    Disclosure may be an effective policy solution if credible information on labor market re-
turns helps students make college choices. It may be less intrusive and faster to implement
than regulation (Loewenstein et al. 2014, Douglas-Gabriel 2017, State of California Department
of Justice 2016, United States Department of Education 2014a), and its benefits may grow over
time if it encourages suppliers to compete on earnings value added or price. However, disclosure
may be ineffective if beliefs about earnings are fixed or if information is difficult to deliver.
    This paper presents three new findings on the economics of disclosure policies and the de-
mand for loan-subsidized higher education. First, we show that students who choose the lowest-
earning degree programs overestimate earnings for past graduates of those programs by more
than 100%. In contrast, students choosing high-earning programs underestimate earnings for
past graduates. These findings are consistent with a parsimonious model of belief formation
    1 Altonjiet al. (2016) describe cross-major earnings differences in the US. Chetty et al. (2017) describe cross-
institution earnings differences. Saavedra (2008), Hastings et al. (2013), and Kirkebøen et al. (2016) provide causal
evidence on differences in earnings outcomes by major and/or selectivity in Colombia, Chile, and Norway.
   2 See United States Government Accountability Office (2010), Lewin (2011), United States Department of Education

(2013), Lederman (2009), and Lederman (2011).



                                                         1
in which students have significant uncertainty about program-specific earnings and respond by
shading their beliefs towards the overall mean across programs. Earnings overestimates are
largest for low-achieving students from low socioeconomic status (SES) households, leaving this
group vulnerable to ex post regret of education choices and potential loan default.
   Second, we show that disclosing degree-specific earnings reduces demand for the lowest-
earning programs, particularly among low-SES students. Students appear to learn from the
disclosure policy, updating their beliefs about earnings and trading off earnings against other
degree attributes when making enrollment decisions. The effects of disclosure persist over time,
with treated low-SES students staying in school longer to complete their higher-return programs.
   Third, we show that the kinds of earnings predictions that form the basis for disclosure poli-
cies track observed variation in earnings outcomes generated by quasi-random assignment of
students to degree programs. Taken together, our results indicate that degree-specific disclosure
policies are a potentially valuable part of the higher education policy toolkit.
   This project relies on extensive collaboration with Chilean policymakers. Through this col-
laboration, we were able to collect and analyze unique data on how students make enrollment
choices and how choices change in response to disclosure. We assisted with three aspects of pol-
icy design. First, we computed earnings returns by college and major. To do this, we constructed
a dataset linking administrative high school, college, student loan, and earnings records for all
Chileans from 1980 through 2015. Second, we created a web survey that measured students’
preferences and beliefs about earnings and costs. 49,166 student loan applicants completed the
survey, which was administered by the Ministry of Education as part of the student loan applica-
tion process. The survey asked students about application plans, own earnings and tuition cost
expectations, and expectations for the typical student. Third, we integrated the survey with an
intervention that provided randomly selected students with information from our database on
average monthly earnings and loan repayment costs for graduating students. For each degree
program, the intervention summarized earnings returns relative to non-attendance by project-
ing expected earnings and loan payments over the 15 year repayment period for federal student
loans. We label this summary measure the Net Value of the degree.
   In total, 30% of all student loan applicants in Chile participated in the intervention as part
of the treatment or control group. The broad sampling frame allows us to investigate how the
lower-income, less-prepared students who are most likely to enroll in non-selective, technical,
and for-profit programs make choices about where and what to study. This is an innovation in
the literature on subjective beliefs in major and institution choice, and we use it to draw new
conclusions on the role of beliefs in choice across the skill distribution. Prior work has focused
on convenience samples of students already enrolled at selective colleges. For example, Wiswall
and Zafar (2014) survey 488 students at New York University, Arcidiacono et al. (2012) and Ar-
cidiacono et al. (2014) survey 173 Duke undergraduates, and Stinebrickner and Stinebrickner


                                                 2
(2013) use survey data from 655 students enrolled at Berea College.3
   Our broader survey yields two new findings about earnings beliefs. First, we show that
students who plan to enroll in a degree program in the bottom decile of the mean earnings dis-
tribution believe that the typical graduate of that program earns 139% more than past graduates
actually earned. This helps explain why students choose low-earning degree programs, and sug-
gests the mostly low-SES, low-achieving students who choose these programs are vulnerable to
ex post regret of higher education investments. In contrast, and consistent with prior findings
for selective universities, we find that the highest-achieving students have beliefs about earnings
outcomes that are close to correct on average.
   Our second finding is that the distribution of beliefs across the full range of students and
enrollment plans is consistent with a parsimonious Bayesian model in which students with im-
precise signals about degree-specific earnings shrink their beliefs back towards the overall mean
value for college graduates. Beliefs about earnings are linear in observed values with a slope
of 0.43 and an upward bias of 17% at the mean. The Bayesian framework links our finding of
earnings overestimates at the bottom of the observed earnings distribution with more accurate
beliefs near the middle of the distribution and underestimates at the very top. Consistent with a
theoretical literature on the effects of advertising on product demand (Johnson and Myatt 2006),
it suggests that low-quality programs systematically benefit from the absence of trustworthy
degree-specific earnings information.
   Our survey findings highlight the benefits of program-specific disclosure policies relative
to policies which provide information on earnings for college graduates on average. Past re-
search has shown that providing information on average labor market outcomes can impact
students’ decisions about how much schooling to obtain (Jensen 2010, Nguyen 2010, Oreopoulos
and Dunn 2013, Bleemer and Zafar 2015, McGuigan et al. 2016).4 These interventions address
incorrect beliefs about overall averages but do not resolve degree-level uncertainty, which is
critically important due to the wide variation in past outcomes and belief errors across degrees.
   We measure the effects of program-specific disclosure using a randomized trial. We find
that disclosure does not affect whether students enroll in college, but does affect which degree
programs they choose. Overall, treatment raises predicted earnings at the programs students
choose by 1.4% of the control group mean. Effects are largest for the low-SES students with low
admissions test scores who are most likely to enroll in low-earning degrees at baseline. Treatment
raises predicted earnings in this group by 3.2% of the control mean.
   The potential gains from disclosure are large relative to other educational policy interven-
tions. The predicted 3.2% earnings gain for low-SES, low-scoring students is equal to the mean
return to an additional 0.27 years of schooling in Chile (Patrinos and Montenegro 2014), or to the
  3 Zafar (2011) also studies subjective beliefs in major choice, using a survey of Northwestern undergraduates.
  4A   number of other papers document the relationship between subjective expectations and choice of education
levels (Dominitz and Manski 1996, Kaufmann 2014).


                                                       3
effect of being assigned to a teacher two standard deviations higher in the value-added distribu-
tion for one year (Chetty et al. 2014a). Our findings hold both for measures of returns conditional
on graduation (Net Value) and for ‘value-added’ measures of earnings gains conditional on en-
rollment that control for student demographics and baseline academic achievement.
   Consistent with the survey finding that earnings overestimates are largest for low-earning
degrees, effects arise from shifts in enrollment away from degree programs at the bottom of the
returns distribution and towards programs between roughly the 50th and 90th percentiles. Treat-
ment reduces enrollment in programs in the bottom tercile of the returns distribution by 3.3%
overall, by 4.6% for low-SES students, and by 3.9% for low test score students. Disclosure does
not cause students to choose lower-cost degree programs. This result is consistent with survey
findings showing that cost beliefs are on average close to accurate across the cost distribution,
and parallels previous studies showing limited effects of interventions that provide information
on costs.5
   The disclosure treatment has lasting effects on educational outcomes. Treated low-SES stu-
dents, who choose degree programs characterized by higher returns for past students, are 2.4
percentage points (3.9%) more likely to stay in school for at least four years than control stu-
dents. In contrast, treated high-SES students, who choose degree programs with returns similar
to control students, are slightly more likely to complete their degree programs in three years or
less. Disclosure helps low-SES students choose higher-return programs and persist in those pro-
grams, and may also raise ex post returns for high-SES students by nudging them to complete
similar degree programs in less time.
   Our experimental findings show that disclosure reduces but does not eliminate demand for
low-return degree programs. To interpret these results, we estimate a discrete choice model of
college enrollment. This analysis helps distinguish between economic mechanisms with differ-
ent policy implications. One possibility is that disclosure helps students learn about earnings,
but that preferences for earnings are weak relative to preferences for other degree attributes. An-
other is that students learn from disclosure and would like to change their enrollment choices
but cannot because of capacity constraints on higher-earning programs. A third is that students
do not learn much from the policy. Our model allows disclosure to affect choice by increasing
the precision of students’ beliefs about earnings and costs. This is consistent with the Bayesian
model of belief formation that fits our descriptive findings. We use survey data to capture pref-
erence heterogeneity by major, field of study, institution, and geographic location, and we use
test score data to generate personalized choice sets for each student.
   Our results suggest that students learn from disclosure but that their preferences for non-
earnings attributes limit the impact of changes in beliefs on enrollment decisions. Treatment
   5 See Bettinger et al. (2012) and Hoxby and Turner (2013).
                                                           Interventions that include help with application logistics
show more positive effects. For more on application complexity, see Avery and Kane (2004); Hoxby and Avery (2013);
Scott-Clayton (2012); and Dynarski and Scott-Clayton (2013).


                                                           4
raises the weight students place on earnings by 32%. This corresponds to a 43% reduction in the
noise-to-signal ratio for earnings beliefs. The reductions in noise-to-signal for low-SES and high-
SES students are 71% and 25%, respectively. However, students have very strong preferences for
specific institutions and majors and this limits the role of earnings in choice. For example, we
show that the earnings elasticity of enrollment at students’ stated first choice degrees would rise
by 73% if institution-specific preferences were eliminated.
   Capacity constraints do not appear to mute disclosure effects. We simulate the effects of a
universal disclosure policy on demand at each degree program relative to a counterfactual in
which no students are treated. Treatment reduces enrollment at degrees in the bottom 10% of the
expected earnings distribution by 4.6%, and increases enrollment in degrees above the median
by just under 2%. Consistent with our reduced-form estimates, students leave the lowest-return
programs but choose a wide range of higher-earning degrees. These higher-earning degrees are
non-selective and report slack admissions capacity that exceeds the additional demand induced
by disclosure. The degrees that experience the largest declines in demand are typically run by
for-profit providers and are often in low-earning but potentially easy-to-market fields such as
cooking, sound engineering, and tourism. These are similar to programs identified in other
contexts as disproportionate producers of poor student outcomes (Looney and Yannelis 2015).
   The finding that capacity constraints do not bind suggests that experimental estimates of
treatment effects are reasonable predictors of the equilibrium effects of a fully scaled policy in
the short run. Concerns about how the effects of the policy may change with full scale-up and
the transition from researcher control to policymaker control (Muralidharan and Sundararaman
2015) are limited here because the government administered the evaluation itself. However,
the extent to which demand-side impacts on low-performing programs could incentivize in-
creased value added and efficiency in the non-selective higher education market remains to be
seen (Beyer et al. 2015).
   Our findings make two contributions to the developing literature on disclosure interventions
in higher education. First, compared to previous studies of intervention policies, we are the first
to track student outcomes. This lets us observe effects on the population of low-achieving stu-
dents that may be omitted from coarser measures. Hurwitz and Smith (2016) and Huntington-
Klein (2017) study the College Scorecard in the US, using SAT score-sending and web search
data, respectively, to measure student responsiveness to policy implementation. Our combina-
tion of data on beliefs and choices with a randomized trial lets us examine enrollment outcomes
and study students’ choice processes in detail. Students choosing low-selectivity programs may
not show up in measures of score-sending or web searches, and these studies do not find eco-
nomically meaningful changes in search and application behavior for low-achieving students.
   Second, compared to previous researcher-designed interventions, we are the first to study the
policy-relevant population of low-achieving students planning to attend low-earning programs.


                                                5
This addition is critical because these are the main targets of disclosure policies, and also the
students on whom the policies have the greatest effects. Kerr et al. (2015) conduct a random-
ized trial in Finland and find little evidence that information affects enrollment choices. In the
Finnish higher education market, college is effectively free to students, and institutions nego-
tiate with regulators to determine the number of seats available in each program. The fringe
of non-selective degree programs where we find disclosure to be most valuable does not exist.
The combination of public subsidies with lightly-regulated private provision we observe in the
Chilean market closely resembles other settings in which disclosure policies have been imple-
mented (Neilson et al. 2016). Our collaboration with the government to design and implement a
nationwide policy is also an important contribution. Because policy effects depend on the details
of design, the outcome of actual policies may differ from predictions based on studies that use
researcher-designed interventions (Duflo 2017).
   Over the long run, the effects of disclosure policies depend on how predicted earnings map to
earnings outcomes. We assess the predictive power of our observational earnings measures us-
ing random variation in degree assignment generated by discontinuous admissions rules (Hast-
ings, Neilson, and Zimmerman 2013; henceforth HNZ). We show that changes in observed
outcomes across an admissions threshold rise one-for-one with changes in predicted outcomes
based on the degrees in which above- and below-threshold students enroll. This exercise extends
a literature that uses quasi-experimental designs to validate estimates of teacher effects (Chetty
et al. 2014a, Chetty et al. 2014b) and school effects (Deming 2014) to a higher education setting.
Our results contrast with recent evidence that some students select into degree programs on the
basis of individual-specific comparative advantage (Kirkebøen et al. 2016), but may be unsur-
prising in a setting where students appear to have little information about differences in earnings
across degree programs. They are also consistent with findings from Goodman et al. (2015) that
differences in observed graduation rates for students randomized between low- and moderately-
selective US institutions track mean institution-level graduation rates. We add to a literature that
uses both discontinuous admissions rules (Hoekstra 2009, Saavedra 2008, Ockert 2010, Hastings
et al. 2013, Zimmerman 2014, Kirkebøen et al. 2016) and observational estimation approaches
(Brunner 2004, Brunner 2009, Brunner and Meller 2009, Altonji et al. 2012, Reyes et al. 2013) to
study how intensive-margin choices about where and what to study impact earnings.6
   Looking beyond college choice, we contribute to a broader literature on how imperfect infor-
mation and policies that seek to address it affect choices in settings where decisions are compli-
cated and information scarce. These settings include primary and secondary education markets
(Hastings and Weinstein 2008, Kapor et al. 2017), markets for healthcare and health insurance
(Abaluck and Gruber 2011, Kling et al. 2012, Handel and Kolstad 2015), and markets for con-
sumer financial products (Thaler and Benartzi 2004, Agarwal et al. 2014).
  6 We   note that our findings contrast with Dale and Krueger (2002) and Dale and Krueger (2014).


                                                         6
2     Higher education in Chile
As in Latin America as a whole, college enrollment in Chile expanded in the 2000s, with much
growth taking place at private higher education institutions (Ferreyra et al. 2017). By the 2010s,
the Chilean higher education system resembled the US in terms of educational attainment rates,
the role of student loans in financing higher education, and higher education market structure.
In 2010, 38% of adults between 25 and 34 years old in Chile had a tertiary degree, compared to
42% in the US (Organization for Economic Co-operation and Development 2013). 35.8% of stu-
dents enrolled in Chilean higher education institutions used state-backed student loans in 2011,
compared to 40.2% in the US.7 Public, private non-profit, and private for-profit firms provide
tertiary degrees in Chile. Vocational degree programs are run by private companies and can be
for-profit or not-for-profit. These programs tend to be less selective and shorter in duration. Uni-
versities offering Bachelor’s-equivalent degrees or higher may be public or private not-for-profit.
In practice, however, portions of some universities are owned by for-profit parent companies, in-
cluding companies like Laureate International and the Apollo Group, which also own for-profit
universities in the US.8 The combination of public funding and private provision parallels the
US market and suggests that firms may face similar incentives when making decisions about
educational and marketing inputs.
    Students in Chile apply to, take courses within, and graduate from institution-major combi-
nations, which we refer to as degree programs. Entrance exam scores are the key determinant of
admissions. In the early 2010s, roughly 200,000 students took the Prueba de Selecion Universi-
taria, or PSU, each year. Entrance exam takers complete exams in Mathematics and Language,
and may take additional exams in subjects such as Science or History. Scores are scaled to a dis-
tribution with a mean and median of 500 and standard deviation of 110. Older, more selective
universities typically have a minimum cutoff score of 475 for admission, with higher cutoffs for
more selective programs. Students admitted to less prestigious universities typically have en-
trance exam scores over 350. Most technical and vocational schools do not require an entrance
exam score for admission, though many students who have entrance exam scores enroll in such
programs. Even at the most selective degree programs, admissions outcomes typically do not
depend on qualitative inputs like extracurricular activities or essays.9 In what follows, we will
measure selectivity using the average of Math and Language scores for enrolling students.
    7 Source: Chilean statistics from Centro de Estudios MINEDUC (2012), Tables Programas, Becas y Ayudas Estudi-
antiles A.4.4 and Matricula D.2.45. US statistics from United States Department of Education (2014b) Table 3.2-A. US
statistics refer to the 2011-2012 school year.
   8 See La Tercera (2012) and Laureate International (2017). See Apollo Group (2013) for a discussion of Apollo’s

purchase of the Chilean university UNIACC. In 2009, the Apollo Group, which owns the university of Phoenix,
settled in a False Claims lawsuit for its recruiting and advertising practices in the US. See Lederman (2009).
   9 The most selective institutions admit students only on the basis of an index of grades and test scores. See HNZ

for details.



                                                         7
      The two main student loan programs in Chile are the Crédito Aval del Estado (CAE) and the
Fondo Solidario de Crédito Universitario (FSCU). 35.6% of low-SES postsecondary students used
CAE loans in 2012. Students are eligible for CAE loans if they meet a set of PSU and GPA cutoffs,
and if they have family income in the lowest four quintiles of the distribution. CAE loans may be
used at any accredited institution. 7.1% of low-SES students received FSCU loans in 2012. FSCU
loans may be used only at a set of older, selective institutions. Eligibility depends on test score
and family income. Interest rates for both types of loans were 2% for students originating loans
during the period we study over a standard repayment period of fifteen years.10 We use these
loan terms to compute measures of costs such as the monthly payment associated with paying
off debt from a given degree program.
      In early- to mid-November, students apply for FSCU, CAE, and several federal grant pro-
grams using the Formulario Único de Acreditación Socioeconómica (FUAS), a unified financial
aid form similar to the FAFSA in the US. After completing the FUAS, students face a short time-
line for college choice. They take the PSU in late November or early December, learn their PSU
scores in late December or early January, and begin to send in applications during the first two
weeks in January. Students begin to learn of admissions outcomes as early as mid-January, and
the school year begins in late February or early March depending on the year and degree pro-
gram. Table A.1 provides a timeline of the loan and college application processes in Chile during
the 2012-2013 application cycle. We nested the survey and the intervention in the FUAS applica-
tion, so students described their preferences and received information at a time when they were
thinking about college choice but had roughly two months remaining to adjust their enrollment
plans before application deadlines.


3      Data
We use educational data from administrative records and labor market outcomes from tax re-
turns to evaluate students’ beliefs against observed benchmarks, to track enrollment choices in
the years following the intervention, and to construct measures of earnings returns for all degree
programs in Chile.


3.1      High school records

We use high school records to define our sampling frame and to describe students’ socioeco-
nomic backgrounds. Our high school graduation records cover the 1995-2012 graduating co-
horts. These data include information on gender, parental education, scores on standardized
tests administered to 10th graders (known as the SIMCE), high school identifiers, and high school
    10 See   OECD (2012) and Solis (2017) for further details on eligibility rules for CAE.



                                                               8
characteristics such as school-level SES ratings. The SES rating categorizes schools from A (low-
est) through E (highest) and is based on parental income. Since we do not observe parental
income, we use high school SES as a proxy for student SES. The 41% of students in our sample
universe of Fall 2012 applicants coming from the 45% of schools with A and B ratings are cate-
gorized as low-SES. The small size of Chilean high schools (the median graduating senior class
size is 57) makes this classification more informative. Table A.2 shows how family background,
academic performance, and school characteristics vary with school poverty status, and describes
cross-validation of poverty rankings using available tax records for parents of children attending
each high school. 70% of students enrolled in schools within the low-SES category come from
the bottom income quintile of the population distribution, compared to 9% from the top two
quintiles. 4% of the low-SES students in our treatment or control groups have a parent with a
college degree.


3.2   College and college application records

We use college enrollment and graduation records to document educational backgrounds for
past students and to track enrollment outcomes for applicants. Enrollment data are available
for the years 2000 through 2016. These data follow students semester by semester, recording
major- and institution-specific enrollment and graduation. The data cover all higher education
programs, including both university-level and vocational degrees. We focus our experimental
analysis on initial enrollment choices (made in 2013), and present additional findings on per-
sistence through 2016 in Section 5.5. Application records include entrance exam scores for all
students by subject for the years 1980-2013. We use these as covariates when making earnings
predictions, as described below.


3.3   Labor market data

We link the database of student records to tax returns from the 2005-2012 earnings years. Over
99% of individuals in our data have matches in the tax records. Tax returns include wage, con-
tract, partnership, investment and retirement income. HNZ describe the tax data in detail, and
provide an example of a tax form to illustrate the components used to calculate income.


3.4   Earnings predictions

We construct two measures of earnings outcomes by degree program. We developed the first,
which we term Net Value, in collaboration with Mineduc for presentation to students as part
of the informational intervention. At the time of the intervention, Mineduc preferred to focus
on outcomes for graduates rather than enrollees, and on binned means rather than regression-



                                                9
adjusted predictions.11 Mineduc also preferred to present earnings and cost outcomes discounted
back to the year of labor market entry, which may depend on educational choice, as opposed to
the first year following high school completion.
   With these constraints in mind, we compute Net Value for degree j, NVj , as

                                                  t=15
                                         NVj =     ∑      βt (µ̂ jt − µ̂0t ) − Cj                                (1)
                                                   t =1

where µ̂ jt are mean earnings for graduates of degree j at experience year t, µ̂0t are mean earnings
for students who do not enroll in any degree program at experience year t, and Cj is the present
value of tuition costs for degree j, discounted to experience year 1. β = 1/(1 + r ), where r is
the discount rate. Net Value is the present value of earnings over the fifteen-year time horizon
for loan repayment for graduates, less the present value of fifteen years of earnings for students
who do not attend college and the present value of direct costs.
   We compute these values using graduation data for the years 2001 and 2005 through 2009,
and data on non-enrollees from the 2006 and 2007 cohorts of high school completers. Data from
other years were not available at the time of the intervention in Fall 2012. We compute mean
earnings values directly for experience years one through five. For experience years six through
fifteen, we compute predicted earnings by combining outcomes from earlier experience years
with field-specific linear slope terms. Costs are based on current tuition levels and suggested de-
gree lengths. We set the discount rate to 2%, the rate of interest on subsidized loans. We present
information to students in terms of the monthly payment that would yield a present value equal
to NVj over a fifteen year period. We also present monthly equivalents of the earnings and cost
components of NVj . See Online Appendix Section B for more details.
   These earnings measures condition on graduation and do not adjust for student characteris-
tics. This means that selection into enrollment or graduation may drive cross-degree differences
in earnings outcomes. To address this issue, we construct an alternate earnings measure that
conditions on enrollment as opposed to graduation and uses observable student characteristics
to correct for selection. We use this measure in conjunction with the Net Value measure to de-
scribe the relationship between beliefs and enrollment choices and measure the effects of the
intervention.
   To obtain earnings predictions, we estimate specifications of the form


                                    yijct = Xict β s( j) + Wijct δm( j)s( j) + vijct
                                                                                                                 (2)
                                    vijct = µ jc + eijct
  11 Mineduc eventually came to favor enrollment-based earnings measures, with the goal of incentivizing institutions

to increase earnings for all enrolled students rather than through selective graduation (Beyer et al. 2015).



                                                            10
We use data on the set of student-year observations in which students who enrolled in college
between 2000 and 2011 are between two and seven years removed from schooling completion
through either graduation or dropout. yijct is labor market earnings for student i enrolling in
institution-major combination (degree) j in entering cohort c at labor market experience year t.
Xict includes dummies for student socioeconomic status, gender, and whether a student took the
entrance exam. It also includes linear controls for entrance exam score, years of labor market
experience, interactions between labor market experience and student covariates, and tax year
dummies. Coefficients on the Xict are permitted to vary over broad area of study12 and five
selectivity tiers. s( j) denotes the interaction between broad area and selectivity tier. We allow
the effects of gender, SES, test taking and test scores, and labor market experience, denoted Wijct ,
to vary with major m( j) as well. We use a Mineduc classification system that divides programs
into 178 possible majors. We decompose the residual vijct into a degree-cohort specific mean
residual component µ jc and an idiosyncratic error eijct . The µ jc captures degree-specific intercept
shifts in earnings outcomes, which may vary by cohort. We estimate equation 2 using a two-step
process. In the first step, we control for degree-specific fixed effects when estimating β s and δms
to avoid confounding the effects of student characteristics with selection into degree programs.
In the second step we use estimates of β s and δms to obtain mean residuals.
   We use earnings predictions based on these regressions to describe choices for fall 2012 ap-
plicants. Our analysis focuses on predictions for the outcome year eight years after college ap-
plication, or approximately age 26. We choose age 26 because it allows students enough time
to complete schooling. Earnings outcomes at later ages are also of interest, but measurement is
more difficult because we observe fewer cohorts and the population of degree programs changes
over time.13 In Section 5, we discuss results in which we compute the present discounted value
of earnings through ages 30 and 50 for each degree program using more aggregated data on
selectivity- and field-specific earnings profiles. We base our predictions at each degree j on the
average of mean residuals µ jc across entering cohorts c = 2000 through c = 2005. Our predic-
tions thus correspond to the average over the age 26 cohorts in the five years leading up the
intervention year of 2013. Mean residuals do not vary much across years, so choice of cohort
does not affect our findings. See Online Appendix Section C for more details on estimation.
   These earnings predictions improve on the Net Value measure by accounting for dropout and
selection on observable characteristics. However, they will provide unreliable guides to future
outcomes for current applicants if earnings differences across degrees are driven by selection
on unobservable characteristics, or if degree effects are not stable over time. In Section 7, we
  12 We use CINE-UNESCO field classifications, which break programs into Business, Agriculture, Art and Architec-
ture, Basic Sciences, Social Sciences, Law, Education, Humanities, Health, and Technology.
  13 We are able to compute predicted age 26 earnings for 83% of students enrolling in college in 2013. As shown in

Table A.5, experimental treatment does not predict enrollment in a degree for which earnings estimates are unavail-
able.



                                                        11
benchmark our earnings predictions using quasi-random variation in degree assignment gener-
ated by admissions cutoff rules. We fail to reject the null hypothesis that earnings outcomes rise
one-for-one with predicted values. Moreover, effects appear to be stable across cohorts.


4    Survey and experimental intervention
We worked with Mineduc to implement the survey and disclosure intervention. We began by
pre-assigning students who graduated from high school in 2012 or who registered for the PSU
admissions exam to treatment and control groups. We assigned treatment status at the high
school level for current high school seniors, and at the individual level for past graduates. An
advantage of school level randomization for current students is that estimated effects will better
predict what would be observed in a universal policy rollout in the presence of school-level peer
spillovers. We stratified assignment by high school type and quality for current graduates and by
coarse PSU bins for past graduates to ensure that we would observe both treatment and control
students across the distribution of student backgrounds.14
    Upon submitting their FUAS applications, applicants received an email from Mineduc with
the subject line ”FUAS Confirmation Code.” The email asked applicants to participate in a brief
survey to help Mineduc make education policy decisions, and informed them that they would
receive a confirmation code at the end of the survey. The email also told students that their
survey responses would be anonymous, used only for research, and would not affect their FUAS
applications. We track bounce-backs, opens, and click-throughs for each email address.
    The survey link in the email directed applicants to a login page, where they were prompted
to enter their identification number and email address. After logging in, applicants viewed an
informed consent, which they could accept or reject. Conditional on acceptance, they began the
survey. The survey asked six questions, each appearing on its own page. Each question could
only be completed once: if a respondent left the survey and started again they would restart
where they left off. The survey program adapted questions to incorporate students’ prior re-
sponses. See Section B of the Online Appendix for survey materials with accompanying English
translations. This section also provides screen shots of the survey and information treatment.
    The first question asked students about their current educational status. Students indicated
whether they were applying to college for the first time or whether they were already enrolled
and considering re-applying. The second question asked students to list up to three degree
programs (institution-major combinations) to which they planned to apply, in preference order.
  14 We stratify schools based on high school type (private not-accepting-vouchers, private voucher-accepting, and

municipal), the fraction of students taking the PSU, and the average PSU score from the prior two senior cohorts.
We assign half of schools within randomization block to the treatment group. PSU registrants for the 2013 college
entering class could use old PSU scores. This was a new policy in 2012. The PSU registration list thus consisted of
those signed up to take the PSU, as well as those who had taken the PSU in prior years.



                                                        12
Students made these choices using a nested set of drop down menus that filtered results to make
list sizes manageable. Students were required to list at least one entry to proceed to the next
question. The third question asked students how certain they were of their application plans.
The fourth question asked students what they thought the annual cost of studying (tuition plus
registration fees) at each of their previously entered choices would be. Students could click an “I
do not know” button or move a slider to indicate the total annual cost.
   The fifth question asked about expected earnings upon graduation. The first part of the ques-
tion asked students to estimate what their monthly salary would be once they started in a stable,
full-time job after graduating from each of their elicited choices. The second part asked students
to estimate what a typical graduate in each degree would earn. Students could choose “I do not
know” for each sub-question or fill in earnings amounts with a slider. The sixth question asked
students about their expected PSU scores in Language and Math.
   Upon completing the final question, control subjects proceeded to a thank you page that
displayed their confirmation code. They also received a thank you email. Treated students con-
tinued to a new page which displayed information on students’ listed degree programs and two
search prompts. A table at the top of the page presented the earnings and cost components of
the Net Value measure in the first two columns, and the Net Value measure itself in the third
column. Each row of the table was one of the degrees a student listed in question 2. Accompa-
nying text explained to students that these values were derived from data on costs and earnings
for past students.
   A highlighted box below this table told students whether there were other institutions to
which they would likely be admitted that offered the same major with a higher Net Value. The
box also displayed the additional Net Value associated with a switch to the highest Net Value
degree program fitting this description. To determine the contents of this box, the web appli-
cation searched across institutions offering the same major as the first-choice degree, looking
for degree programs with similar entrance exam distributions and higher Net Values. A second
highlighted box indicated whether or not there were other degrees within the same broad field
of study as their first-choice degree that offered higher Net Value, and displayed the expected
Net Value gain from the within-field switch. To populate this box, the web application searched
across degrees with similar entrance exam score distributions to the listed first choice.15
   A link below the suggestion boxes invited treated students to view a searchable career database.
Text at the top of this page explained the underlying data and gave subjects a place to enter a
PSU score, degree level, and major. Students could fill in these fields and view tables displaying
institutions offering the specified major and serving students with scores similar to the listed
  15 The median gain in predicted earnings associated with the switch described in the first box was equal to 33% of

predicted earnings in students’ first listed choice. The median tuition change associated with the switch was 0. The
median gain in predicted earnings associated with a switch to the degree program described in the second box was
equal to 156% of predicted first choice earnings. The median tuition change was 34.2%. See Table A.3 for more details.


                                                         13
PSU value.16 The table displayed the institution name, the major, and the same earnings, costs,
and Net Value measures as the previous page. Results were sorted in descending order by Net
Value. The search results also displayed a suggested alternative major to search for with higher
Net Value in the same broad field of interest. For example, both nursing and nutrition are in the
broad field of health but earnings for nursing are generally higher. Students could log back in
at any time to compile and view up to ten comparative tables. This page also contained a thank
you message and the confirmation code. Students were not required to search.
   Table 1 describes the sample of students invited to participate in our intervention, comparing
characteristics of the invited sample to those of eventual respondents. 69% of students opened
the initial email. Of those, 73% agreed to the informed consent. 59% of students providing in-
formed consent completed the survey. In total, 30% of the original email requests from Mineduc
to the email address given by the respondent for their college and loan applications resulted in
a completed survey, with the largest attrition at the survey completion stage. We refer to survey
completers as respondents from this point forward, and focus our analysis on this group. Recall
that the intervention is identical for treatment and control groups through survey completion.
   Within our sample, students with lower baseline academic achievement, low-SES backgrounds,
and lower-educated backgrounds were harder to reach. The average PSU entrance exam score
for respondents is 28 points higher than for invitees. The fraction of invited students from low-
SES high schools was 43.7%; this falls to 35.7% for respondents. Respondents are more likely to
have parents with some tertiary education, and score substantially higher on high school stan-
dardized tests (SIMCE) than invitees. On average, degree programs respondents list as their first
choices offer Net Values of $734,948 CLP per month (just over $1,400 USD using November 2013
exchange rates) relative to not attending college. Students could raise this value by an average
of 36% ($267,566 CLP) by switching to peer institutions offering similar majors. 77.0% of respon-
dents matriculate in some degree program. The mean value of predicted earnings at age 26 for
respondents who enroll in college is $464,307 CLP each month, or USD $893.
   Column 5 shows characteristics of treated respondents. There are no statistically or economi-
cally significant differences in baseline characteristics between treatment and control students. A
test of joint significance of baseline characteristics in explaining treatment fails to reject the null
of no effect with a p-value of 0.191. The final column shows characteristics of treated students
who searched the database. 43% of treated students searched, and searchers are similar to non-
searchers in terms of observable characteristics and survey responses. Students who search have
slightly higher SIMCE scores, and the Net Value of searchers’ stated first-choice enrollment plans
is 4% higher than for non-searchers. See Tables A.4 and A.5 for further comparison of treatment
and control groups.
  16 Specifically, the web program selected all degrees in the same major for which the stated PSU score fell within

the 5th and 95th percentile for enrolling students.



                                                        14
5      Survey and experimental results
5.1     Earnings beliefs

Figures 1 and 2 and Table 2 describe students’ responses to survey questions about expected
earnings and compare them to values observed in tax data. We compare students’ stated beliefs
about earnings for a typical graduate once they have completed school and started work to
earnings we observe in the tax data for students in the first two years following graduation. The
horizontal axis in each graph is demeaned observed (log) earnings for past graduates, which
we refer to as observed earnings. Figure 1 shows the fraction of students choosing the “I don’t
know” option at their most-preferred program. Each point is a binned mean within a ventile of
the observed earnings distribution. Students who list the lowest-paying programs as their top
choice are more likely to state that they do not know earnings outcomes for typical students;
rates decline from 46% in the lowest decile to 28% in the highest. Overall, students indicate lack
of knowledge for 50% of typical earnings beliefs and 39% of own earnings beliefs. We discuss
the possible effects of non-reporting on our findings for students who do report beliefs below.
      Panel A of Figure 2 shows how beliefs about typical earnings vary with observed earnings
for past students. To facilitate comparison with demeaned true values, we subtract the observed
earnings mean from beliefs. If beliefs tracked observed values on average, the belief mean would
fall along a 45-degree line through the origin, which we plot for reference. We find that mean
beliefs are linear across the distribution of observed earnings values, but do not track the 45-
degree line. The slope is flatter, and the intercept shifted upwards. A linear regression of beliefs
on true values yields a slope of 0.427 (0.007) and an intercept of 0.173 (0.004). The implication is
that students with stated interest in low-earning degree programs believe earnings for a typical
graduate are far higher than those observed for past graduates, while students with stated in-
terest in high-earning programs think earnings for a typical graduate are somewhat lower than
observed past values. For example, students expressing interest in bottom-decile degree pro-
grams believe the typical graduate will earn 87 log points (139%) more than the observed value
for past graduates, while students expressing interest in top-decile programs have beliefs 34 log
points (41%) below past values on average. Beliefs are on average correct for programs near the
75th percentile of the distribution.
      These findings are consistent with a parsimonious model of belief formation in which ap-
plicants respond to uncertainty by shrinking degree-specific beliefs back towards a belief about
mean earnings for college graduates in general (regardless of degree). Consider the following
information structure. Applicants believe earnings values for typical graduates at degree j, Ỹj ,
are drawn from a normal distribution N (Ȳ + b, σY2 ). Ȳ is the true mean across programs, b is
a bias term reflecting applicants’ optimism about degree programs on average, and σY2 is the
variance of earnings outcomes. Each individual then receives a noisy signal Ỹij = Ỹj + b + eij


                                                15
about typical earnings outcomes at degree program j, with eij ∼ N (0, σe2 ). Beliefs for overall and
degree-specific mean earnings outcomes are biased up by b. Students’ posterior expectation of
earnings Ỹije is given by

                     Ỹije = αY Ỹij + (1 − αY )(Ȳ + b) = b + αY Ỹj + (1 − αY )(Ȳ ) + rij       (3)

                 σe−2
where αY =    σe−2 +σY−2
                           is a precision weighting and rij = αY eij .
   A qualitative implication of the model is that even mean-zero uncertainty leads to system-
atic overestimates for low-earning programs and underestimates for high-earning programs.
Low-quality programs may benefit from students’ belief errors even without marketing. These
patterns are not consistent with predictions from alternate models of belief formation in which
students have uniform biases across programs, or have upward-biased beliefs about heavily
marketed low-earning programs but accurate beliefs about others.
   This model also yields a useful quantitative interpretation of both the slope and intercept
terms in the belief graphs. The slope is equal to αY , and our estimated value of 0.427 implies
                                σe2
a noise-to-signal ratio of     = 1−αYαY = 1.34; i.e., the variance of belief errors is 34% larger than
                                σY2
the variance of the underlying distribution of observed values. This is similar to an independent
estimate of the noise-to-signal ratio we obtain by computing the variance of belief errors and the
variance of observed earnings values in the microdata. The variance of observed belief errors
(defined as the difference between reported typical student beliefs and the observed value) is
0.565 and the variance of observed outcomes is 0.353, for a ratio of 1.60. The belief bias term
corresponds to the intercept on the graph. Our estimates indicate that students overestimate
earnings by an average of 17 log points. This average across programs is second-order compared
to the large overestimates of earnings outcomes for the lowest-earning programs.
    We see similar patterns in reported beliefs about own earnings outcomes. Panel B of Fig-
ure 2 shows that own earnings beliefs closely track typical earnings beliefs across the observed
earnings distribution. On average, students believe that their own earnings conditional on grad-
uation will fall slightly below those for a typical graduate. The correlation between own- and
typical-earnings beliefs is 0.78. The slope of own earnings beliefs in observed earnings values is
0.474, implying a noise-to-signal ratio of 1.112, and the intercept is 0.119.
    Additional analyses help rule out alternate explanations for the pattern of belief errors we
observe. One concern is that the linear relationship between beliefs and observed values might
not persist or might have a different slope if non-reporters were forced to report beliefs. Several
pieces of evidence suggest that this is unlikely. First, conditional on observed earnings at their
listed preferences, students who do and do not report earnings beliefs are similar in terms of
both predetermined characteristics and subsequent enrollment patterns. Second, to the extent
differences in demographics and enrollment outcomes do exist, they suggest a linear relationship


                                                       16
with an even flatter slope for non-reporters. See Appendix D for details. Third, we observe
a linear relationship between beliefs and observed values for both own and typical graduate
beliefs, even though rates of non-report are much lower for own earnings. A straightforward
interpretation of our findings is that non-reporters tend to be more uncertain than reporters, and
would drive the slope of beliefs in observed earnings downwards if included.
   Another concern is that measurement error in our benchmark values of observed earnings
could bias estimates of αY downward, away from one. We show in Appendix D that measure-
ment error is limited and does not affect our findings. Appendix D also reports results from a
within-person analysis of the relationship between beliefs and observed earnings. The within-
person analysis is feasible because we elicit multiple beliefs from each student. We find that the
within-person relationship between earnings beliefs and observed values is very similar to the
cross-sectional relationship. This is consistent with our model of belief formation as reflecting
noisy degree-specific signals.
   Students who express preferences for the lowest-earning programs come from low-SES back-
grounds and have low admissions test scores. Panel A of Figure 3 plots low-SES share and mean
PSU scores by demeaned observed earnings. 56% of students planning to enroll in programs in
the bottom earnings decile come from low-SES backgrounds, compared to 16% in the top decile.
The average bottom-decile PSU score is 463, compared to 623 in the top decile. The median belief
errors about earnings for past graduates are higher for low-SES students (15 log points) than for
high-SES students (3 log points), and for students with below-median PSU scores (20 log points)
than for students with above-median PSU scores (0 log points). As reported in Table 2, the rela-
tionship between beliefs and observed earnings is fairly similar for low- and high-SES students.
The slope of beliefs in observed values is 0.42 for low-SES students and 0.43 for high-SES stu-
dents. However, low-SES students are much more likely to prefer low-earning programs where
this relationship leads to an overestimate.
   Students’ response to disclosure depends on their choice set, which is determined largely by
admissions test score. Panel B of Figure 3 plots non-response rates and typical earnings beliefs
by ventile of PSU score. The rate at which students claim not to know their typical earnings
values declines with test score. Belief errors are largest at the bottom of the score distribution.
For example, students scoring at the fourth ventile of the PSU distribution overestimate their
earnings by 35 log points (41%) over actual earnings. Degrees in this selectivity range serve
mostly low-SES students. Consistent with previous studies focusing on students enrolled in
selective universities, belief errors are close to zero on average for the highest-scoring students
(Wiswall and Zafar 2014, Arcidiacono et al. 2014). As shown in Panels C and D of Figure 3,
rates of belief reporting and mean errors are fairly similar for low- and high-SES students after
conditioning on test score.




                                                17
5.2     Cost beliefs

Figure 4 displays the distribution of cost beliefs. 33% of students do not report cost beliefs. As
shown in Panel A of Figure 4, the share of non-reports falls somewhat as costs rise, from 35% in
the bottom ventile of the cost distribution to 25% in the top decile. As was the case with earnings
beliefs, cost beliefs are linear in observed costs. However, as shown in Panel B of Figure 4, cost
beliefs track true values much more closely. The estimated slope of cost beliefs in observed
costs is 0.879 (0.009), and the intercept is -0.083 (0.004). Students choosing the bottom-decile
degree programs overestimate costs by an average of 3% while students choosing the top-decile
programs underestimate costs by 15%. While cost underestimates at high-cost programs are
arguably substantial, they are an order of magnitude smaller than earnings overestimates at the
lowest-earning programs. The noise-to-signal ratio corresponding to the slope term in costs is
0.138, one tenth the value for earnings. As with earnings, the relationship between cost beliefs
and observed costs is similar for low- and high-SES students.
      A concern in interpreting subjective beliefs data is that errors may result from inattention to
the survey, not lack of knowledge about actual outcomes. That students report accurate beliefs
about costs helps mitigate concerns that belief errors in earnings are the result of inattention.
Other pieces of evidence support this as well. For example, ordered logit estimates show that,
within students’ preference lists, earnings beliefs predict relative rank, but, conditional on earn-
ings beliefs, true earnings outcomes for past students do not predict relative rank. We would
expect the opposite if students knew true outcomes but provided random responses to ques-
tions about own beliefs. We present these results in Appendix Table A.6.


5.3     Beliefs and enrollment outcomes

Students’ elicited preferences are strong predictors of high-stakes enrollment choices. 45% of ma-
triculating students enroll in one of their listed preferences. 27% enroll in their first choice. This
is true even though we elicit preferences before application decisions are made, and suggests
that we are accurately recovering preference information.
      For students who enroll in degree programs for which we elicited earnings beliefs, the gap
between beliefs and values for past students follows a similar pattern to what we observe in the
analysis of all belief data. Panel A of Figure 5 shows that the slope of beliefs in observed earnings
conditional on enrollment is 0.41, and the intercept is 0.15. These are very similar to the values
of 0.43 and 0.17 that we observe in analysis of beliefs overall. Conditioning on enrollment does
not mitigate the large belief overestimates at the bottom of the observed earnings distribution:
students who enroll in bottom ventile programs believe the typical graduate of their chosen pro-
gram earns 169% more than observed values for past graduates. Panel B shows that cost beliefs
also follow a similar pattern conditional on enrollment, with a slope of 0.86 and an intercept of


                                                  18
-0.076.
      Panel C of Figure 5 shows that students with larger positive belief errors enroll in programs
with lower earnings and rates of on-time loan repayment for past students across the PSU score
distribution. The graph splits students into two groups. Students with above the median positive
belief error of 38 log points are in the large error group, while students with an error below the
median positive belief error are in the small error group. Students in the large error group enroll
in programs where their predicted earnings (as measured using predictions from equation 2) are
22 log points lower than students in the small error group.
      We take the gap in observed outcomes between students with large and small overestimates
as a comparison point for our information intervention treatment effect estimates. The intuition
is that this represents a reasonable upper bound on what an informational intervention could
accomplish. It is inclusive of the causal effect of belief errors as well as any correlation between
belief errors and other factors that push students towards low-earning programs. For example,
applicants who place low value on earnings may have large belief errors because it is not worth
it for them to seek out accurate earnings information (Hastings et al. 2016).


5.4     Experimental estimates of information treatment

Table 3 shows the impact of treatment on Net Value, predicted earnings, and cost outcomes at
the degree programs where students choose to enroll. We show results separately for the full
sample of students, and for subsamples by SES and PSU score. We estimate specifications of the
form
                               Yioutcome = β 0 + β 1 Ti + β 2 Yi0 + γg(i) + ei                        (4)

where Yioutcome is a characteristic of a student’s chosen degree program, such as Net Value, costs,
or predicted earnings. Ti is an indicator for treatment, and γg(i) is a randomization block fixed
effect. Yi0 is the characteristic of the outcome variable of interest at student i’s stated first choice
degree program. These controls reduce standard errors but do not substantively alter point
estimates. Table A.7 presents alternate estimates that drop all controls and examine the effect
of treatment on changes between outcomes at students’ stated first choices and the degrees in
which they ultimately enroll. Standard errors allow for clustering at the high school level for
students applying to college directly out of high school. The coefficient of interest is β 1 , the effect
of treatment on the characteristic of the chosen program.
      The first panel shows impacts of treatment on the decision to matriculate to any degree pro-
gram. The impact is close to zero and statistically insignificant across all subsamples. The second
panel shows the impact of treatment on Net Value and the costs and earnings components of the
Net Value measure in the applicant population. For the 23% of the sample who did not matricu-
late to any degree, these three values are by definition zero. The estimated effects, which average


                                                     19
the zero effect for non-matriculating students with intensive margin effects for matriculating stu-
dents, do not differ statistically from zero for Net Value or its earnings or cost components.
   Because the impact of treatment on matriculation is zero, we can estimate the inframarginal
impact of treatment on the earnings and cost characteristics conditional on enrollment. The third
panel shows the impact of treatment conditional on matriculating to some tertiary degree. Treat-
ment effects on Net Value and earnings outcomes are statistically significant in the full sample.
Treatment raises Net Value by $10,029 CLP, or 1.7% of the control group mean. This effect is
driven by larger gains for low-SES students, and particularly low-SES students with low PSU
scores. For low-SES students, the intensive-margin effect of treatment is to raise Net Value at
the chosen degree by $15,274 CLP. This is equal to 3.4% of control mean Net Value for low-SES
students matriculating in college, 5.3% of the average gain associated with a switch to a peer
institution, and 28.4% of the average monthly debt payment. For low-SES students with low
PSU scores, treatment raises Net Value by $18,430 CLP, equal to 5.0% of the control mean for
this group, 6.3% of the average gain, and 42% of the average monthly debt payment. Across all
samples, effects are driven by increases in earnings, not decreases in costs.
   The fourth panel of Table 3 shows how treatment affects earnings at age 26 conditional on
enrollment as opposed to graduation. This earnings measure, estimated using equation 2, re-
flects changes in degree “value added” conditional on gender, student SES, and test score. We
find positive and statistically significant treatment effects, again driven by gains for low-SES stu-
dents and students with low-PSU scores. The pooled sample gain of $6,324 CLP is equal to 1.4%
of the control mean. The gain for low-SES students of $11,759 CLP is equal to 3.0% of the control
group mean and 21.7% of the gap in predicted earnings between students with large and small
belief errors for students in this group. The gain of $11,337 for low-PSU, low-SES students is
equal to 3.2% of the control mean and 25.6% of the gap for these students.
   The effects we observe are large relative to other educational policy interventions. Chetty
et al. (2014a) show that assignment to a teacher one standard deviation higher in the value-
added distribution for one year raises earnings at age 28 by between 1.3 and 1.7 percent. This is
similar to our predicted mean effect in the full sample, and about half of the effect in the low-SES
sample. Alternatively, recent estimates suggest that the returns to a year of schooling in Chile are
about 12% (Patrinos and Montenegro 2014). Our mean effects are thus similar to the earnings
gains from attaining an additional 0.12 years of schooling in the full sample, or 0.25 years of
schooling in the low-SES sample. For comparison, the high-intensity, high-cost Perry Preschool
intervention raised educational attainment by about 0.9 years (Schweinhart 1993). We discuss
the relationship between our earnings predictions and long-run earnings outcomes in section
7.1.
   The effect of treatment is to push applicants away from very low-earning degree programs
and towards a range of higher-earning ones. Figure 6 plots the distribution of Net Value at the


                                                 20
matriculating degree by treatment status. The upper panel splits students by SES background,
and the lower panel splits students by above- and below-median admissions test scores. We
observe a shift in mass from below the median of the control group distribution to roughly
the 50th through 95th percentiles, primarily among low-SES and low-PSU students. Treatment
reduces the share of treated students choosing degrees with Net Values within the bottom tercile
of the distribution among control students by 3.3% overall, by 4.6% for low-SES students, and
by 3.9% for low-PSU students.
      The bottom panel of Table 3 measures the impact of treatment on measures of long-run earn-
ings gains. We compute the present discounted value (PDV) of post-college earnings net of direct
costs through ages 30 and 50. We extrapolate from our regression-based earnings measure using
data on field- and selectivity-specific earnings trends. (See Online Appendix Section C for a full
description of the calculation.) We find that treatment raises the PDV of post-schooling earnings
net of direct costs through age 30 by just under one million Chilean pesos, or USD $1,923. Multi-
plying by the 37,747 students in the matriculating respondent sample suggests an increase in the
PDV of aggregate earnings net of the direct costs of education of roughly USD $72 million at the
loan interest rate of 2%. Reweighting the respondent sample to match the universe of applicants
on observable characteristics (SES background, gender, age, and test score) indicates that apply-
ing the treatment to this larger group would yield gains of US $217 million through age 30. This
second calculation relies on the assumption that, conditional on observables, survey takers and
non-takers would respond similarly to treatment, which we cannot verify. However, because the
disclosure intervention is scalable and inexpensive to implement, the rate of return on treatment
is likely high.


5.5     Experimental effects over the medium run

We evaluate medium-run effects of the disclosure policy using data on matriculation records for
the population of higher education students through the 2016 academic year and graduation
records through the 2015 academic year. The typical suggested length of a bachelor’s-equivalent
degree in Chile is four or five years, and many students take longer than the suggested time to
complete. This means that we cannot follow all students to schooling completion. However, we
can observe persistence over a reasonably long time horizon.
      Our empirical approach is to estimate versions of equation 4 in which outcomes are measures
of persistence past the first year of enrollment. We first describe how treatment affects charac-
teristics of the degrees where students enroll that are related to persistence. We present these
findings in Panel A of Table 4. As in our analysis of initial enrollment choices, we focus on the
sample of students matriculating at some higher education institution in 2013.
      Overall, 78% of students choose degree programs with a suggested duration of four or more
years, corresponding to the difference between bachelor’s-level degrees and technical degrees.

                                                21
Treatment does not affect the suggested duration of the degrees students choose in the pooled
sample. However, there is some suggestive evidence that it raises suggested length for low-SES
students. For this group, treatment raises the probability students choose a program that lasts
for at least four years by 0.013, or 2.1% of the control group mean of 0.62. This effect is not
statistically significant at conventional levels (p=0.20). However, it is equal to about 18% of the
cross-SES gap in the rate of choosing a longer degree program conditional on admissions test
score. Treatment does not cause students to choose programs with higher rates of persistence
into the second year and eventual graduation for past students. If students’ medium-run enroll-
ment outcomes correspond to average outcomes for past students at the degrees they initially
choose, treatment should have a small positive effect on the rates at which low-SES students are
still in school four years later, and a zero or negative effect for high-SES students.
      Panel B of Table 4 presents the effects of treatment on observed enrollment and graduation
outcomes. In the pooled sample, there is no effect on enrollment in years one through four
following treatment or on graduation within three years of treatment. This reflects offsetting
effects for low- and high-SES students. Low-SES students are 2.4 percentage points (3.9%) more
likely to enroll in all four years following treatment, while high-SES students are 1.2 percentage
points (1.5%) less likely to have done so. Treatment in part shifts low-SES students away from
(high-SES students towards) shorter programs they otherwise would have completed. Low-SES
treated students are 1.8 percentage points less likely to graduate in three years, and high-SES
students 0.7 percentage points more likely to do so. Each of these effects is statistically significant
at the 5% level.
      It is helpful to interpret these findings in the context of our results on initial degree choice.
Treated low-SES students choose higher-return degree programs, and spend more time in school
studying at those programs. Treated high-SES students choose programs with similar returns,
but appear to complete them more quickly.


6      Information treatment and program-specific demand
6.1     Mechanisms underlying experimental estimates

Our experimental findings raise two sets of questions. The first is about the mechanisms that me-
diate experimental effects. Different mechanisms have different policy implications. If students
care a lot about earnings but learn relatively little from the disclosure intervention, alternate dis-
closure policies that convey information more effectively could have large effects. In contrast, if
students care relatively little about earnings but learn a lot from the intervention, there is likely
less scope for this kind of innovation. The second set is about how disclosure alters demand for
different kinds of degree programs, and therefore supplier incentives. This is critical for under-
standing how the effects of disclosure evolve over the long run. To address these questions, we

                                                   22
use a discrete choice framework to explore student preferences and study how treatment shifts
demand across degree programs.
   We estimate logit specifications based on the student choice problem

                 max uij = X j β 1 + Xij β 2 + π0Y Ỹj + π1Y Ỹj Ti + π0C C̃j + π1C C̃j Ti + eij    (5)
                  j∈ Ji


Each student chooses the utility-maximizing degree program j within their personalized choice
set Ji . Utility uij for student i at degree j depends on degree characteristics X j , the interaction
between i’s tastes and j’s attributes Xij , and iid logit shock eij . Treatment Ti affects choice by
changing the weights students place on observed (log) earnings and cost values Ỹj and C̃j .
   Treatment operates by causing students to upweight the information displayed in the inter-
vention. The beliefs model from Section 5.1 provides a plausible microfoundation. Assume that
students make degree choices based on expectations about their own earnings and costs, using
utility weights τY and τC respectively. Setting τC = 0 for purposes of exposition, we may then
write the choice problem as

                                  max uij = X j β 1 + Xij β 2 + τY Yije + ẽij
                                   j∈ Ji


Treatment changes students’ earnings expectations Yije by raising the precision of earnings and
cost signals αY , with αy (t) being precision of earnings beliefs for Ti = t. Treatment may also alter
intercept terms b, but these do not affect intensive-margin degree choice. We can project own
earnings beliefs Yije onto beliefs about earnings for past graduates, Ỹije , so that Yije = ρY Ỹije +
ηij = ρY αY (t)Ỹj + a0 + aij , where a0 = ρY (b + (1 − αy (t))Ȳ ) is constant across degrees and
aij = ρY rij + ηij is an idiosyncratic degree-specific shock. ρY captures the predictive power of
information about past earnings for formulation of own earnings expectations. If we impose
the distributional assumption that ẽij + aij = eij , we can write the utility weights in equation
5 as π0Y = ρY τY αY (0) and π1Y = ρY τY (αY (1) − αY (0)). The coefficients reflect belief precision
and the effect of treatment on belief precision, scaled by the weight of earnings beliefs in utility
and the relevance of past outcomes for own beliefs. Further, the fraction change in coefficients
(π1Y − π0Y )/π0Y = (αY (1) − αY (0))/αY (0) in the treated relative to untreated sample gives the
fraction increase in belief precision relative to baseline.
   Alternate microfoundations for this empirical specification are possible. For example, treat-
ment may cause students to place more utility weight on earnings and cost outcomes even if they
do not update beliefs about these outcomes. It is also possible that this empirical specification
misses some channels through which treatment systematically affects choice. We consider one
additional channel below, using a specification that allows treatment to effect choice by increas-
ing the salience of the specific degree programs recommended by the web application. Close



                                                      23
parallels between our experimental results and our model estimates suggest that our formula-
tion provides a reasonable accounting of treatment effects.
   When estimating the model, we exploit our detailed survey data to capture variation in stu-
dents’ preferences across programs. The X j consist of dummies for narrowly defined field of
study (178 categories) and institution (141 categories), as well as mean test scores and high-SES
share for admitted students. To allow the utility of degree characteristics to vary across students,
the Xij interact degree characteristics with student preferences as measured in our survey re-
sponse data.17 The Xij consist of indicator variables for whether j’s attributes match those of
degrees that i ranked Nth on the pre-survey, for N=1, 2, or 3. The attributes we consider are
narrow major, broad area (10 categories), and institution. To measure the intensity of listed pref-
erences, we interact these indicators with a dummy equal to one if i reported certainty in their
preference listing. We capture preferences over geography by including indicators equal to one
if an institution is located in i’s home region or in the same region as one of i’s listed preferences.
To capture substitution patterns across selectivity levels, we include controls for the absolute
difference between the high-SES share and mean admissions score for matriculating students at
i’s listed first choice versus those at degree j.
   We create personalized choice sets Ji for each student based on the set of degree programs to
which students with similar or lower scores on the standardized college admissions exam were
admitted. This approach addresses the potential mismeasurement of elasticities from lack of
product availability (Conlon and Mortimer 2013). It is compelling here because test scores are
the primary determinant of college admissions. Bucarey (2018) also uses test scores to define
choice sets for Chilean students making college choice.
   We report estimated coefficients from this exercise in Appendix Table A.9, and summarize our
estimates in Table 5. Panel A of Table 5 compares the utility weights that treated and untreated
students, respectively, place on earnings and costs. To obtain these values we compute predicted
utilities ûij and regress the values on log earnings and log costs. We estimate separate regressions
in the treatment and control groups. This allows our estimated earnings and cost utility weights
to incorporate the role that earnings and costs play in the formation of stated student preferences,
rather than conditional on stated preferences. The estimate for the untreated group reflects the
correlation between utility and earnings at a degree program, including any correlation between
earnings and other degree attributes that affect utility. The difference between estimates for
treated and untreated students shows how disclosure raises the correlation between earnings
for past students and utility at a degree program.
   Overall, treatment raises the utility weight students place on earnings by 32%, while raising
(in absolute value) the weight students place on costs by 4%. This is consistent with our exper-
  17 Mixed logit demand models typically draw from distributions of unobserved preferences for characteristics when

measures of preferences are not available. In contrast, we measure preferences for particular degree characteristics
directly and allow stated preferences to shift utilities.


                                                        24
imental finding that disclosure raises earnings but does not reduce cost at the programs where
students enroll. Treatment raises the utility weight placed on earnings by 72% for low-SES stu-
dents compared to 12% for high-SES students. The utility weights rise by 199% for low-scoring
students, but from a baseline close to zero. These findings are consistent with the heterogeneous
effects observed in the experimental setting. It is worth noting that, for high-PSU students,
costs are positively correlated with utility on average. This is likely due to correlations between
desirable degree attributes and some combination of high production costs and markups over
production costs in this subsample.
   In the full sample, our baseline estimate of αY (0) from the pre-treatment beliefs data is 0.427,
for a noise-to-signal ratio of 1.34. A 32% increase from αY (0) to αY (1) corresponds to a 43%
decrease in the noise-to-signal ratio from 1.34 to 0.78. The reduction in noise-to-signal ratio
for low-SES students is 1.40 to .40 (71%), and for high-SES students is 1.35 to 1.10 (25%). Our
findings are what would be expected from substantial increases in belief precision, but suggest
that uncertainty remains even following treatment.
   Even though treatment increases precision in students’ beliefs we see modest enrollment ef-
fects because students place high utility weights on degree attributes other than earnings. To
illustrate this, we calculate the elasticity of enrollment at students’ stated first choice degree pro-
gram with respect to earnings under counterfactual assumptions on student preferences. We
report results in Panel B of Table 5.
   The “Baseline” row reports observed choice elasticities. In the pooled sample, treatment
raises the earnings elasticity from 0.143 to 0.182, with larger effects for low-SES students. The
subsequent rows eliminate, one at a time, student preferences over geography, institution, and
narrow major. We obtain these preferences by zeroing out the portion of utility attributable to,
respectively, an institution’s location, a student’s stated preference over institutions, and a stu-
dent’s stated preference over narrow majors. (When eliminating preferences over narrow majors,
we allow students to maintain stated preferences over broad fields of study). These variables are
demeaned so that average utility over institutions and majors does not change in the counter-
factual setting. Eliminating geographic preferences raises earnings elasticity at the first choice
degree for treated students by 16%, to 0.210. Eliminating preferences over institutions (majors)
raises the same elasticity by 62% (77%), to 0.294 (0.321). These findings suggest that strong pref-
erences for particular majors and institutions reduce students’ willingness to substitute towards
higher-earning programs. They also suggest that survey data successfully capture preference
heterogeneity.




                                                  25
6.2     Demand shifts under hypothetical scaled treatment

With model estimates in hand, we simulate the effects of a population-scaled informational in-
tervention. We use the SES-specific logit estimates to simulate enrollment probabilities for each
survey respondent under the assumptions that either a) all applicants received treatment, or b)
no applicants receive treatment. We then assign each applicant a sampling weight based on their
high school type, gender, age, and admissions test score so that the weighted sample matches the
population of loan applicants on these characteristics. Using the weighted demand estimates, we
compute the expected change in enrollment in each degree program between the all-treated and
none-treated scenarios.
      The upper panel of Figure 7 shows mean percent change in enrollment by deciles of degree-
level mean earnings. Treatment shifts students away from the lowest-earning degree programs
and towards programs with earnings in the top 60% of the distribution. Enrollment declines
by 4.6% in the lowest-earning tenth of programs, and grows by just under 2% in the top three
deciles. Shifts away from low-earning programs are larger for students from low-SES back-
grounds, reaching 6.4% in the bottom decile. The lower panel of Figure 7 plots the same outcome
by deciles of the cost distribution. There are no systematic changes in enrollment across the dis-
tribution of program costs. Modest shifts towards low-cost programs for low-SES students are
balanced by small shifts away from those programs for high-SES students. These simulation re-
sults echo the reduced-form evidence from Figure 6. Treatment reduces demand for the degree
programs with the worst labor market outcomes, particularly among low-SES students.
      Degree programs with biggest declines are low-selectivity programs run by for-profit providers.
The majors where enrollment declines are the largest include sound engineering, cooking, and
tourism, as well as special education. These are the types of degree programs that drive high
loan default rates in our setting and also in the US (see e.g. Looney and Yannelis 2015). Ap-
pendix Table A.8 reports enrollment changes by major and institution-major combination for a
selection of degree programs where demand declines.



6.3     Capacity constraints

The analysis thus far assumes that universal treatment does not shift the set of degree programs
to which students have access. If treatment caused students to chase a small number of spots in
capacity-constrained programs, this would affect our analysis of demand effects in the short run.
To understand whether capacity constraints bind, we compare simulated changes in enrollment
to slack capacity using data on degree-specific vacancies. These data are reported each year to a




                                                 26
centralized accreditation authority.18 Figure 8 reports results. The left panel presents the mean
of scores for admitted students by decile of percentage change in demand for the program. The
distribution of changes in enrollment is relatively tight. The mean enrollment change in the
bottom (top) decile of the change distribution is -5% (5%). The upside-down V shape in the left
panel indicates that the largest changes in enrollment (both positive and negative) take place at
less-selective degrees, while changes at more-selective programs tend to be smaller. Treatment
shifts students between non-selective programs. These findings make sense because treatment
effects are largest among low-SES and low-PSU students.
      The right panel of Figure 8 presents mean admissions slack (in percentage terms) by decile of
enrollment change. The rough V shape of this graph indicates that the degrees where enrollment
changes are the largest also have the most unfilled capacity. Degrees in the top decile of the
change distribution have an average enrollment increase of 5%, but an average of 14% excess
enrollment capacity. Informational treatment does not result in students chasing a small number
of spots at capacity-constrained programs.


7      Discussion: long-run effects and psychological underpinnings
7.1     Validating earnings predictions over the long run

We design our disclosure policy and evaluate its effects primarily using earnings predictions
derived from observational data on past students. A concern about this approach and about dis-
closure policies in general is that outcomes for past students do not provide a reliable guide to
potential earnings outcomes for applicants. Regression-adjusted earnings outcomes may not re-
flect the causal effects of degree choice if students select into programs on the basis of unobserv-
able absolute or comparative advantage. Further, even if cross-degree differences in disclosed
values do reflect causal effects for past students, these effects may not apply to those affected by
the disclosure policy. This could be because skill demand changes over time, because students
who shift into degree programs in response to disclosure have different potential outcomes than
past students, or because of changes in the relative supply of skills induced by the disclosure
policy itself.
      In our setting, where students overestimate earnings at low-earning programs by a factor
of nearly three, it seems likely that information about earnings values at these programs is a
valuable input into choice even if it partially reflects selection or if earnings outcomes change
going forward. Still, understanding the relationship between earnings predictions and earnings
  18 One third of degree programs report enrolling more students than their stated number of open spots. We compute

percentage excess capacity for each program as the maximum of 0 and the difference between the logs of available
spots and observed enrollment. This underestimates true slack because a) there are implicitly more spots available
at some undersubscribed programs than there are listed vacancies, and b) it is possible that the true capacity at
(nominally) oversubscribed programs is above observed enrollment.


                                                        27
outcomes is important for two reasons. First, we evaluate outcomes using measures of predicted
earnings at the degrees students choose. If our predicted values differ from causal effects we
may over- or under-estimate the earnings effects of changes in the allocation of students across
programs. Second, earnings predictions like those we use here are frequently used as inputs
in accountability systems, in Chile and elsewhere (Beyer et al. 2015, Scott-Clayton and Minaya
2016). The motivation underlying these policies is that regression predictions of earnings capture
the causal effect of degree programs going forward.
      To test the validity of our earnings predictions, we benchmark our predicted earnings values
against causal estimates of earnings effects obtained using a regression discontinuity design. As
described in HNZ (2013), admissions outcomes at most selective degree programs in Chile are
determined by indices of student grades and test scores. The index-based approach generates
sharp cutoffs in admissions outcomes around score thresholds that are unknown at the time of
application. Students above the threshold attend their chosen program, while students below the
threshold attend a mix of next-best programs. Earnings changes across the admissions threshold
reflect gains from admission to the target program relative to this mix.
      Our benchmarking strategy is to compute regression discontinuity estimates at each selec-
tive degree program using observed earnings outcomes, and to compare the results to alternate
regression discontinuity estimates obtained using predicted earnings values from equation (2).
If observed earnings changes rise one-for-one with predicted changes, we interpret this as evi-
dence that our predicted values capture causal effects. We formalize this intuition in Section E of
the Online Appendix.
      We estimate regressions of the form

                                                               pred
                                            δ̂jobs = λ0 + λ1 δ̂j      + ej                                       (6)

where δ̂jobs is the regression discontinuity estimate for degree j obtained using earnings outcomes
            pred
and δ̂j            is the estimate obtained using predicted outcomes. To account for measurement error
                                                                         pred
in the independent variable, we shrink estimates of the δ̂j                     towards the sample mean based
on their sampling variance. We account for measurement error in the dependent variable by
                                                                                  pred
weighting using the inverse of the sampling variance for δ̂jobs . If δ̂j                 is an unbiased predictor of
δ̂jobs ,   we expect λ1 to equal one and λ0 to equal zero. When estimating this regression, we by ne-
cessity focus on selective programs (i.e., programs that reject some students), and programs for
which we can make earnings predictions. This eliminates discontinued older programs which
lack the recent enrollment data necessary to predict outcomes and non-selective programs. Be-
cause we drop the non-selective programs in which the least-informed students typically enroll
from our sample, we expect to overstate the role of selection into degree programs on the basis
of program-specific comparative advantage, which depends on knowledge of fit.


                                                         28
      We estimate these specifications using two sets of application cohorts. The first spans 2000
through 2005. These are the application cohorts used to make earnings predictions. Results from
this cohort test whether regression predictions provide reasonable approximations of causal ef-
fects. We use estimated pre-2000 mean residuals µ jc when computing predicted values. The
second is 1982 through 1993. 1982 is the earliest application cohort we observe. We choose the
1993 end date because students applying in 1993 have in almost all cases completed schooling
by 2000 and are not included in the enrollment data. Findings for the earlier cohort speak to the
stability of degree effects over time and experience.
      Table 6 and Figure 9 present results. Each point on the graphs represents the mean of the
                                                                                    pred
observed earnings gain within bins defined by decile of the distribution of δ̂j            . The binned
means track the linear fit across the distribution of predicted values. In both graphs, observed
values are linear in predicted values, have slope close to one, and pass through the origin. In the
2000-2005 application cohort, our estimate of λ1 is 0.716 with a 95% CI of (0.42, 1.1). Our estimate
of λ0 is -18.9, with a CI of (-189, 151). In the 1982-1993 cohorts, our estimate of λ1 is 0.778, with a
95% CI of (0.41, 1.14) and our estimate of λ0 is 150, with a CI of (-96, 396). We cannot reject the
hypotheses that λ1 = 1 or that λ0 = 0 at conventional levels in either specification.
      We cannot reject the hypothesis that our measures of predicted earnings succeed in captur-
ing the causal effects of enrollment in different degree programs, and that effects are relatively
stable across cohorts. This is consistent with recent research showing that value added mea-
sures of teacher and school effects provide reasonable estimates of causal effects (Kane and
Staiger 2008, Deming 2014, Chetty et al. 2014a, Chetty et al. 2014b). These papers argue that se-
lection of teachers based on student unobservables may be negligible given observed assignment
policies. In the context of higher education, selection on unobservables may be small if students
are uninformed about their own degree-specific pecuniary deviation from mean returns condi-
tional on observables, or if they weight non-pecuniary factors heavily when choosing schools.
This is consistent with our survey evidence that students do not differentiate themselves from
typical graduates and with our discrete choice analysis suggesting that students weight other
preferences more heavily than earnings when making enrollment choices.


7.2     Skill prices

Informational interventions could in principle affect future skill prices by raising or lowering the
supply of skills of different types. To understand how supply changes across broad skill types,
we aggregate enrollment changes under the full treatment counterfactual and no treatment coun-
terfactuals across broad field of study. Consistent with the strong field- and major-specific pref-
erences we observe in our discrete choice model, we find that treatment does not substantially
shift the distribution of enrollment by field. The field with the largest enrollment increase is
Technology, where enrollment rises by 1.2%. The fields with the largest declines are Education

                                                  29
and Humanities, where enrollment falls by 1.4%. and 1.2%, respectively. If skill prices are de-
termined by supply of and demand for graduates at the field-degree type level, it would require
very large elasticities of demand for the changes we observe to substantially reduce cross-field
wage gaps, which are quite large. Mean monthly earnings for students choosing technology
degrees in the no treatment counterfactual are 120% higher than earnings for students choosing
humanities degree programs despite the fact students enrolling in these fields have, on average,
similar test scores. We report these findings in Table A.8.


7.3     Psychological underpinnings of treatment effects

Understanding the mechanism through which treatment affects choice is important for design-
ing future interventions. Our treatment had two parts: it highlighted specific degree programs
that offered high returns on average for past students, and it provided information about earn-
ings and cost outcomes for programs of the students choice. We test whether effects derive from
the recommendation or from the more generalized information treatment. To do so, we augment
our baseline choice model with indicator variable equal to one for degree programs in the career
that we recommended to the student (or, for control-group students, would have recommended
had they been treated) and an interaction between this indicator and treatment. If treatment ef-
fects shift away from the interaction with degree-specific earnings and towards the interaction
with the recommendation the student received, this would suggest that our recommendation
was a driver of student choice, and that, for example, policy makers could simply advertise the
names of higher return degrees rather than work to update students earnings information and
beliefs through disclosure.
      Our findings, reported in Table A.9, suggest that adding the recommendation effects does not
reduce effects of treatment on students preference for degree mean earnings. The treatment ap-
pears to operate by raising the utility weights students place on earnings outcomes more broadly.
The coefficient on the interaction between treatment and the recommendation indicator is pos-
itive but not statistically significant. The advertising component of the intervention is not the
main driver of the effects we see.


8      Conclusion
This paper uses a survey and a randomized evaluation of a scaled disclosure policy to explore
why some college applicants enroll in degree programs where past students have had very low
earnings. The scale of our survey and intervention and a link between survey records and pop-
ulation tax data allow us to present the first description of beliefs and belief errors across the
distribution of degree programs and student characteristics, and to show how enrollment deci-
sions change when students have access to accurate information about earnings and costs.

                                                 30
   Our survey results highlight the importance of degree-specific earnings beliefs as an input
into college choice, and show that belief accuracy and the nature of belief errors vary systemati-
cally across the distribution of enrollment plans and student demographics. Students who plan
to enroll in the lowest-earning degree programs think the typical graduate will earn 139% more
than the average value for past graduates. Overall, the distribution of earnings beliefs is consis-
tent with a model in which students shrink noisy signals about degree-specific earnings at their
preferred programs towards a mean belief about outcomes for graduates, with overestimates at
the bottom of the observed earnings distribution and underestimates at the top. The result is
that low-quality programs systematically benefit from even mean-zero uncertainty on the part
of students. Belief errors are largest for low-achieving students and students from low-SES back-
grounds.
   Our experimental findings show that while the effects of disclosure fall most heavily on de-
gree programs where past students have had very low earnings, many students continue to
enroll in these programs even after receiving credible earnings information. An informational
intervention implemented by the Chilean Ministry of Education reduced the fraction of low-SES
students choosing degree programs in the bottom tercile of the earnings distribution by 4.6%,
and raised their mean predicted earnings by 21% of the gap the between students with large and
smaller earnings overestimates. Drops in student demand are largest for a set of low-earning,
non-selective programs that are often run by for-profit providers. Short-run effects of the dis-
closure intervention are likely a lower bound on long-run effects, which may grow over time as
institutions respond to changes in demand. However, the effects of the intervention are miti-
gated by students unwillingness to substitute across narrowly-defined majors and institutions.
   We conclude by noting that, as is typical in policy design, details matter (Duflo 2017). One im-
plication of our findings is that how policymakers reach out to students is important. Our inter-
vention included direct, personalized outreach from the central education authority. Even with
this recruitment strategy, we observe negative selection into treatment: the low-scoring, low-SES
students who benefit most from disclosure are less likely than other students to complete our
survey and reach the disclosure stage of randomization. A pilot survey we conducted in the pre-
vious year that included outreach but not from the central authority yielded much lower takeup
than the 30% we observe here (Hastings et al. 2016). Our results echo findings across a range of
social services on the difficulty of program takeup for those most in need (Currie 2006, Choi et
al. 2011, Amior et al. 2012). Higher education disclosure policies in most countries do not include
an outreach component: policymakers simply post a search tool online (Neilson et al. 2016). This
may limit their effects.




                                                31
References
Abaluck, Jason and Jonathan Gruber, “Choice Inconsistencies among the Elderly: Evidence
    from Plan Choice in the Medicare Part D Program,” American Economic Review, June 2011,
    101 (4), 1180–1210.

Agarwal, Sumit, Gene Amromin, Itzhak Ben-David, Souphala Chomsisengphet, and Dou-
    glas D. Evanoff, “The Effectiveness of Mandatory Mortgage Counseling: Can One Dis-
    suade Borrowers from Choosing Risky Mortgages?,” NBER Working Paper 19920, 2014.

Altonji, Joseph G., Erica Blom, and Costas Meghir, “Heterogeneity in Human Capital Invest-
    ments: High School Curriculum, College Major, and Careers,” Annual Review of Economics,
    2012, 4 (1), 185–223.

    , Peter Arcidiacono, and Arnaud Maurel, “The Analysis of Field Choice in College and
    Graduate School: Determinants and Wage Effects,” Handbook of the Economics of Education,
    2016, 5, 305 – 396.

Amior, Michael, Pedro Carnerio, Emanuela Galasso, and Rita Ginja, “Overcoming Barriers to
    the Take-Up of Social Subsidies,” World Bank Conference Paper, 2012.

Apollo Group, “Global Fact Sheet,” 2013.

Arcidiacono, Peter, V. Joseph Hotz, and Songman Kang, “Modeling college major choices using
    elicited measures of expectations and counterfactuals,” Journal of Econometrics, 2012, 166 (1),
    3–16.

Arcidiacono, Peter, V. Joseph Hotz, Arnaud Maurel, and Teresa Romano, “Recovering ex ante
    returns and preferences for occupations using subjective expectations data,” NBER Working
    Paper 20626, 2014.

Avery, Christopher and Thomas J. Kane, “Student Perceptions of College Opportunities. The
    Boston COACH Program,” in “College Choices: The Economics of Where to Go, When to
    Go, and How to Pay For It” NBER Chapters, National Bureau of Economic Research, Inc,
    October 2004, pp. 355–394.

Bettinger, Eric P., Bridget Terry Long, Philip Oreopoulos, and Lisa Sanbonmatsu, “The role
    of application assistance and information in college decisions: Results from the H&R Block
    FAFSA experiment,” The Quarterly Journal of Economics, 2012, 127 (3), 1205–1242.

Beyer, Harald, Justine Hastings, Christopher Neilson, and Seth Zimmerman, “Connecting Stu-
    dent Loans to Labor Market Outcomes: Policy Lessons from Chile,” American Economic Re-
    view: Papers & Proceedings, 2015, 105(5), 508–513.

                                               32
Bleemer, Zachary and Basit Zafar, “Intended College Attendance: Evidence from an Experi-
     ment on College Returns and Cost,” IZA Discussion Papers 9445, Institute for the Study of
     Labor (IZA) Oct 2015.

Brunner, José Joaquı́n, Oferta y Demanda de Profesionales y Técnicos en Chile. El Rol de le Información
     Pública, Universidad de Chile, Facultad de Ciencias Fı́sicas y Matemáticas, Departamento
     de Ingenierı́a Industrial, 2004.

     , Educación superior en Chile: Instituciones, Mercados y Polticas Gubernamentales (1967-2007),
     Universidad Diego Portales, 2009.

         and Patricio Meller, Educación Técnico-Profesional y Mercado Laboral en Chile: Un Reader,
     Gobierno de Chile, Minesterio de Educacion y Universidad Diego Portales y Ingeneria In-
     dustrial Universidad de Chile, 2009.

Bucarey, Alonso, “Who Pays for Free College? Crowding out on Campus’,” MIT Working Paper,
     2018.

Centro de Estudios MINEDUC, “Estadisticas de la Educacion 2012,” Technical Report, Ministe-
     rio de Educacion, Gobierno de Chile 2012.

Chetty, Raj, John Friedman, and Jonah Rockoff, “Measuring the Impact of Teachers II: Teacher
     Value-Added and Student Outcomes in Adulthood,” American Economic Review, 2014,
     104(9), 2633–2679.

     ,        , and      , “Measuring the Impacts of Teachers I: Evaluating Bias in Teacher Value-
     Added Estimates,” American Economic Review, 2014, 104(9), 2593–2632.

Chetty, Raj, John N. Friedman, Emmanuel Saez, Nicholas Turner, and Danny Yagan, “Mobility
     Report Cards: The Role of Colleges in Intergenerational Mobility,” Working Paper, January
     2017.

Choi, James, David Laibson, and Brigitte Madrian, “$100 Bills on the Sidewalk: Suboptimal
     Investment in 401(k) Plans,” The Review of Economics and Statistics, August 2011, 93 (3), 748–
     763.

Conlon, Christopher T. and Julie Holland Mortimer, “Demand estimation under incomplete
     product availability,” American Economic Journal: Microeconomics, 2013, 5 (4), 1–30.

Currie, Janet, “The Take-up of Social Benefits,” in “Public Policy and the Income Distribution.,”
     Russell Sage Foundation, 2006, chapter 3, pp. 80–148.




                                                   33
Dale, Stacy Berg and Alan B. Krueger, “Estimating the Payoff to Attending a More Selective
    College: An Application of Selection on Observables and Unobservables,” Quarterly Journal
    of Economics, 2002, 117(4), 1491–1527.

     and       , “Estimating the Effects of College Characteristics over the Career Using Admin-
    istrative Earnings Data,” Journal of Human Resources, 2014, 49(2), 323–358.

Deming, David J., “Using School Choice Lotteries to Test Measures of School Effectiveness,” The
    American Economic Review, 2014, 104 (5), 406–411.

Dominitz, Jeff and Charles F Manski, “Eliciting student expectations of the returns to school-
    ing,” The Journal of Human Resources, 1996, 31 (1), 1.

Douglas-Gabriel, Danielle, “SEC settles fraud charges against defunct for-profit college com-
    pany ITT,” 2017. Accessed 3 July 2017.

Duflo, Esther, “The Economist as Plumber,” Working Paper 23213, National Bureau of Economic
    Research March 2017.

Dynarski, Susan and Judith Scott-Clayton, “Financial Aid Policy: Lessons from Research,” Fu-
    ture of Children, 2013, 23 (1), 67–91.

Ferreyra, Marı́a Marta, Ciro Avitabile, Francisco Haimovich Paz et al., At a Crossroads: Higher
    Education in Latin America and the Caribbean, World Bank Publications, 2017.

Goodman, Joshua, Michael Hurwitz, and Jonathan Smith, “College Access, Initial College
    Choice, and Degree Completion,” NBER Working Paper 20996 2015.

Handel, Benjamin R. and Jonathan T. Kolstad, “Health Insurance for ‘Humans’: Information
    Frictions, Plan Choice, and Consumer Welfare,” American Economic Review, Aug 2015, 105
    (8), 2449–2500.

Hastings, Justine S. and Jeffrey M. Weinstein, “Information, School Choice, and Academic
    Achievement: Evidence from Two Experiments,” The Quarterly Journal of Economics,
    November 2008, 123 (4), 1373–1414.

Hastings, Justine S., Christopher A. Neilson, and Seth D. Zimmerman, “Are Some Degrees
    Worth More than Others? Evidence from College Admission Cutoffs in Chile,” NBER Work-
    ing Paper 19241, 2013.

    ,      , Anely Ramirez, and Seth D. Zimmerman, “(Un)informed college and major choice:
    Evidence from linked survey and administrative data,” Economics of Education Review, 2016,
    51 (C), 136–151.

                                                34
Hoekstra, Mark, “The Effect of Attending the Flagship State University on Earnings: A
    Discontinuity-Based Approach,” The Review of Economics and Statistics, November 2009, 91
    (4), 717–724.

Hoxby, Caroline M. and Christopher Avery, “The Missing One-Offs: The Hidden Supply of
    High-Achieving, Low-Income Students,” Brookings Papers on Economic Activity, 2013, Spring,
    1–65.

     and Sarah Turner, “Expanding College Opportunities for High-Achieving, Low Income
    Students,” Stanford Institute for Economic Policy Research, Mar 2013. Working Paper.

Huntington-Klein, Nick, “The Search: The Effect of the College Scorecard on Interest in Col-
    leges,” Working Paper, 2017.

Hurwitz, Michael and Jonathan Smith, “Student responsiveness to earnings data in the College
    Scorecard,” Available at SSRN: https://ssrn.com/abstract=2768157, 2016.

Jensen, Robert, “The (Perceived) Returns to Education and the Demand for Schooling,” The
    Quarterly Journal of Economics, 2010, 125 (2), 515–548.

Johnson, Justin P. and David P. Myatt, “On the Simple Economics of Advertising, Marketing,
    and Product Design,” American Economic Review, June 2006, 96 (3), 756–784.

Kane, Thomas J. and Douglas O. Staiger, “Estimating Teacher Impacts on Student Achieve-
    ment: An Experimental Evaluation,” NBER Working Paper 14607, 2008.

Kapor, Adam, Christopher A. Neilson, and Seth D. Zimmerman, “Heterogeneous Beliefs and
    School Choice Mechanisms,” Technical Report 2017.

Kaufmann, Katja Maria, “Understanding the income gradient in college attendance in Mexico:
    The role of heterogeneity in expected returns,” Quantitative Economics, 2014, 5 (3), 583–630.

Kerr, Sari Pekkala, Tuomas Pekkarinen, Matti Sarvimäki, and Roope Uusitalo, “Post-
    Secondary Education and Information on Labor Market Prospects: A Randomized Field
    Experiment,” 2015.

Kirkebøen, Lars J., Edwin Leuven, and Magne Mogstad, “Field of study, earnings, and self-
    selection,” The Quarterly Journal of Economics, 2016, 131 (3), 1057–1111.

Kling, Jeffrey R., Sendhil Mullainathan, Eldar Shafir, Lee C. Vermeulen, and Marian V. Wro-
    bel, “Comparison Friction: Experimental Evidence From Medicare Drug Plans,” The Quar-
    terly Journal of Economics, 2012, 127, 199–235.



                                                35
La Tercera, “‘Las 11 instituciones de Educación Superior cuestionadas por irregularidades en
    2012’,” 2012.

Laureate International, “Our Network,” 2017.

Lederman, Doug, “$78.5M Settles U. of Phoenix Case,” Inside Higher Ed., 12 2009. Accessed 14
    November 2014.

    , “For-Profits and the False Claims Act,” Inside Higher Ed., 8 2011. Accessed 13 November
    2014.

Lewin, Tamar, “For-Profit College Sued as U.S. Lays Out Wide Fraud,” The New York Times, 8
    2011. Accessed 14 November 2014.

Loewenstein, George, Cass Sunstein, and Russell Golman, “Disclosure: Psychology Changes
    Everything,” Annual Review of Economics, 2014, 6, 391–419.

Looney, Adam and Constantine Yannelis, “A Crisis in Student Loans? How Changes in the
    Characteristics of Borrowers and in the Institutions They Attended Contributed to Rising
    Loan Defaults,” Brookings Papers on Economic Activity, Sept 2015.

McGuigan, Martin, Sandra McNally, and Gill Wyness, “Student Awareness of Costs and Bene-
    fits of Educational Decisions: Effects of an Information Campaign,” Journal of Human Capital,
    2016, 10 (4), 482–519.

Muralidharan, Karthik and Venkatesh Sundararaman, “The Aggregate Effect of School Choice:
    Evidence from a two-stage experiment in India,” Quarterly Journal of Economics, Aug 2015,
    130, 1011–1066.

Neilson, Christopher, Jose Luis Flor, and David Barraza, “Rol de la informacin para la toma
    de decisiones de estudios postsecundarios,” Informe IPA 2, Innovations for Poverty Action
    Nov 2016.

Nguyen, Trang, “Information, Role Models and Perceived Returns to Education: Experimental
    Evidence from Madagascar,” MIT Working Paper 2010.

Ockert, Bjorn, “What’s the Value of an Acceptance Letter? Using Admissions Data to Estimate
    the Return to College,” Economics of Education Review, 2010, 29(4), 504–516.

OECD, “Quality Assurance in Higher Education in Chile,” 11 2012. Accessed 31 May 2013.

Oreopoulos, Philip and Ryan Dunn, “Information and College Access: Evidence from a Ran-
    domized Field Experiment,” Scandinavian Journal of Economics, 2013, 115(1), 3–26.


                                               36
Organization for Economic Co-operation and Development, “Education at a Glance: OECD
     Indicators,” 2013. Accessed 4 March 2015.

Patrinos, Harry and Claudio E Montenegro, “Comparable estimates of returns to schooling
     around the world,” World Bank Policy Research Working Paper, 2014, 7020.

Reyes, Loreto, Jorge Rodrguez, and Sergio S. Urza, “Heterogeneous Economic Returns to Post-
     secondary Degrees: Evidence from Chile,” NBER Working Papers 18817, 2013.

Saavedra, Juan E., “The Returns to College Quality: A Regression Discontinuity Analysis,” Un-
     published Manuscript, Harvard University 2008.

Schweinhart, Lawrence J., Significant Benefits: The High/Scope Perry Preschool Study through Age
     27. Monographs of the High/Scope Educational Research Foundation, No. Ten., ERIC, 1993.

Scott-Clayton, Judith, “Information Constraints and Financial Aid Policy,” NBER Working Paper
     17811, 2012.

Scott-Clayton, Judith and Veronica Minaya, “Should student employment be subsidized? Con-
     ditional counterfactuals and the outcomes of work-study participation,” Economics of Edu-
     cation Review, 2016, 52 (C), 1–18.

Solis, Alex, “Credit Access and College Enrollment,” Journal of Political Economy, 2017, 125 (2),
     562–622.

State of California Department of Justice, “Attorney General Kamala D. Harris Obtains $1.1
     Billion Judgment Against Predatory For-Profit School Operator,” Mar 2016.

Stinebrickner, Ralph and Todd Stinebrickner, “A Major in Science? Initial Beliefs and Final
     Outcomes for College Major and Dropout,” Review of Economic Studies, 2013, 81(1), 426–472.

Thaler, Richard and Shlomo Benartzi, “Save More Tomorrow (TM): Using Behavioral Eco-
     nomics to Increase Employee Saving,” Journal of Political Economy, 2004, 112(S1), S164–S187.

United States Department of Education, 2013. Accessed 14 November 2014.

     , “Obama Administration Announces Final Rules to Protect Students from Poor-Performing
     Career College Programs,” 2014. Accessed 14 November 2014.

     , “Web Tables: Student Financing of Undergraduate Education 2011-12,” 2014. NCES 2015-
     173.




                                                37
United States Government Accountability Office, “For-Profit Colleges: Undercover Testing
    Finds Colleges Encouraged Fraud and Engaged in Deceptive and Questionable Marketing
    Practices,” 2010. Hearing Before the Committee on Health, Education, Labor and Pensions,
    U.S. Senate. Statement of Gregory Kutz, Managing Director Forensics Audits and Special
    Investigations.

Wiswall, Matthew and Basit Zafar, “Determinants of College Major Choices: Identification from
    an Information Experiment,” Review of Economic Studies, 2014, 82(2), 791–824.

Zafar, Basit, “How do College Students form Expectations?,” Journal of Labor Economics, 2011,
    29.2, 311–348.

Zimmerman, Seth, “The Returns to College Admission for Academically Marginal Students,”
    Journal of Labor Economics, 2014, 32(4), 711–754.




                                                38
Figures and tables

                        Figure 1: Belief nonreports by observed earnings ventile




Horizontal axis is demeaned observed (log) earnings for past graduates at students’ elicited first choice programs.
Each point is the fraction of students who do not report a belief within a ventile of the observed earnings distribution.




                                                           39
                         Figure 2: Earnings beliefs relative to observed earnings




Horizontal axis in both panels is demeaned observed (log) earnings for past graduates at students’ stated degree
preferences. Points are binned means by ventiles of this variable. Panel A: Typical earnings beliefs by observed
earnings outcomes. Circles are mean beliefs about typical earnings, + symbols show the IQR within each bin. Thick
dashed line is a linear fit of observed belief means. “Observed” is the 45-degree line, plotted for reference. Panel
B: Means of beliefs about own earnings and typical earnings conditional on graduation and observed earnings for a
typical student. Thick dashed line is linear fit of typical earnings beliefs. “Observed” is the 45-degree line, plotted for
reference.
                                                            40
                          Figure 3: Earnings beliefs and student demographics




Panel A: Fraction low-SES students (left axis) and mean test scores (right axis) by demeaned observed earnings at
preferred degree (horizontal axis). Panel B: Horizontal axis: admissions test scores. Points reflect binned means
within ventiles of score distribution. Left axis: Fraction belief non-reports at first-choice program. Right axis: Binned
means of beliefs about typical graduate earnings at listed programs (circles) and observed values for past graduates
(line). Panel C: Belief report rates by ventile of admissions test score and SES. Panel D: mean belief error by ventile of
admissions test score and SES.

                                                           41
                                  Figure 4: Cost beliefs and observed costs




Panel A: Fraction of cost non-reports by ventile of demeaned observed cost distribution. Panel B: Binned means of
cost beliefs (vertical axis) by ventile of observed cost distribution (horizontal axis). Thick dashed line is linear fit.
“Observed” is 45 degree line plotted for reference.




                                                           42
                      Figure 5: Earnings and cost beliefs conditional on enrollment




Panel A: (Log) Beliefs about earnings for typical graduates (vertical axis) by ventile of observed log earnings distri-
bution (horizontal axis) for enrolling students. Thick dashed line is linear fit of observed values, with 45-degree line
plotted for reference. Panel B: (Log) Beliefs about costs (vertical axis) by ventile of observed log cost distribution
(horizontal axis). Thick dashed line is linear fit of observed values, with 45-degree line plotted for reference. Panel C:
Predicted earnings at the enrolled degree (vertical axis) by admissions test score (horizontal axis) and belief error type.
“Large error” is sample of students with larger than the median positive belief error on their stated most-preferred
program. “Small error” is other students.




                                                            43
                   Figure 6: Distribution of Net Value by treatment status and SES




Density of log predicted earnings distribution for matriculating students by treatment status and SES background
(upper panel) or PSU score (lower panel). Vertical lines are quantiles of control group distribution in pooled sample.
Both panels use an Epanechnikov kernel with bandwidth 75.



                                                         44
              Figure 7: Enrollment effects by deciles of log monthly income and costs




Mean percentage enrollment changes in full-treatment relative to no-treatment counterfactuals by deciles of degree-
level mean earnings distribution (upper panel) and costs distribution (lower panel). Left graphs show results for
pooled sample and right graphs split by SES. See section 6 for description of counterfactual simulations.



                                                         45
             Figure 8: Degree selectivity and admissions slack by enrollment effect size




Left panel: mean PSU admissions score (vertical axis) by decile of disclosure effect distribution (horizontal axis). Right
panel: mean admissions slack as fraction of total enrollment (vertical axis) by decile of disclosure effect distribution
(horizontal axis). Admissions slack is a measure of available spots in a degree program relative to enrolling students.
Disclosure effects reflect comparisons of the full-treatment and no-treatment counterfactuals. See section 6 for details.




                                                           46
                 Figure 9: Observed and predicted cross-threshold earnings changes




Regression discontinuity estimates of observed changes in earnings outcomes across admissions thresholds (vertical
axis) by predicted changes (horizontal axis). Points are binned means with deciles of the predicted value distribution,
and dashed lines are linear fits from estimates of equation (6). Left panel: outcome variable is observed earnings
effects for entering 2000-2005 entering cohorts. Right panel: outcome is observed earnings effects for 1982-1993
entering cohorts. See section 7.1 for details of the benchmarking exercise.




                                                          47
                       Table 1: Comparison of survey sample invitees, opened email, consenting sample, & respondents

                                                               Invited Sample    Opened     Consent    Respondents   Treated    Treated & Searched
                                                                     (1)            (2)        (3)          (4)         (5)              (6)
                      PSU Score (Ave. Language & Math)              508.4          518.8      524.5        536.3       537.0            543.4
                                                                  (155,167)     (101,736)   (72,474)     (47,568)    (23,402)         (10,339)
                      Municipal High School                        37.80%        37.00%     36.50%       33.10%      32.50%           32.40%
                                                                  (164,798)     (114,398)   (83,346)     (49,166)    (24,162)         (10,448)
                      Mother Some Tertiary Edu.                    25.80%        26.80%     26.90%       29.80%      30.30%           29.50%
                                                                  (130,324)      (85,134)   (60,616)     (40,744)    (20,041)          (8,725)
                      Father Some Tertiary Edu.                    27.20%        28.40%     28.60%       31.70%      32.00%           32.00%
                                                                  (126,082)      (82,449)   (58,722)     (39,511)    (19,439)          (8,452)
                      Low-SES School                               43.70%        43.30%     43.20%       35.70%      34.50%           34.30%
                                                                  (153,706)     (105,441)   (76,476)     (46,444)    (22,680)          (9,891)
                      Average of Math + Lang SIMCE (Z-score)        0.326         0.414      0.465        0.568       0.581             0.635
                                                                  (123,937)      (79,504)   (56,255)     (38,625)    (18,981)          (8,282)
                      Female                                       55.40%        57.30%     58.20%       57.50%      56.50%           58.90%
                                                                  (164,786)     (114,265)   (83,215)     (49,166)    (24,162)         (10,448)
                      Delayed College Entrance                     26.40%        36.40%     39.60%       24.50%      24.50%           26.50%
                                                                  (164,798)     (114,398)   (83,346)     (49,166)    (24,162)         (10,448)
                      Net Value 1st Choice Degree                                                        734,948     736,867          769,452
48




                                                                                                         (40,806)    (20,048)          (8,683)
                      Potential Institution Gains                                                        267,566     266,131          262,685
                                                                                                         (48,672)    (23,922)         (10,350)
                      Predicted Earnings at Age 26                                                       464,307     466,988          482,513
                                                                                                         (31,549)    (15,532)          (6,713)
                      Matriculation Rate                                                                  77.0%       77.2%            77.6%
                                                                                                         (49,166)    (24,162)         (10,448)
                      Total Observations                          164,798       114,398     83,346        49,166      24,162           10,448
     Calculations are based on survey responses linked to administrative data from the Chilean Ministry of Education (Mineduc). The number of observations
     for each calculation are in parentheses. The ”Invited Sample” is all November 2012 FUAS Applicants for the 2013 school year for whom we had a valid email
     address to send our survey invitation. The ”Opened” sample is the subset of our Invited Sample who opened the survey invitation email. The ”Consent”
     sample is the subset of those who opened the email and also consented to complete the survey. The ”Respondents” are those who consented to complete
     the survey, completed all 6 questions in the survey, and graduated high school between 2009-2012. The ”Treated” are those who were randomly assigned
     to be treated with degree information upon completion of the survey. The ”Treated & Searched” are those who were treated with information who also
     searched for alternative degrees after being shown information about their first choice degree and a suggested institution and degree. PSU scores are the
     most recent PSU scores on record for the student. The type of high school (municipal, private, voucher) is from the 2012 high-school graduation records.
     Mother and Father having some tertiary education is as reported by the student in the survey component of the national standardized test, SIMCE. Low-SES
     is defined as coming from a high school in one of the two highest poverty categories. SIMCE scores are standardized high school test scores administered to
     all students enrolled in the 10th grade in 2001, 2003, 2006, 2008, and 2010, normalized within each testing year. Delayed college represents those that were
     not directly coming from high school. Net-Value 1st Choice Degree is the Net Value displayed in the experiment for the student’s stated first-choice degree.
     Potential Gains from Switching Institution is the maximum gains in net value that was displayed to treatment group if they chose a different institution in
     the same major as their stated first-choice degree.
        Table 2: Elicited earnings and cost beliefs vs. observed outcomes

                              A. Typical student earnings
                            All    Low SES High SES Low PSU                        High PSU
Slope                      0.427     0.416      0.425      0.366                     0.463
                          (0.007)   (0.014)    (0.009)    (0.012)                   (0.009)
Intercept                  0.173     0.265      0.123      0.328                     0.078
                          (0.004)   (0.009)    (0.005)    (0.008)                   (0.005)
Share non-reports          0.500     0.566      0.461      0.579                     0.435
Noise-to-Signal            1.344     1.402      1.352      1.731                     1.158
N                          51415     14503      34610      19565                     31850

                                      B. Own earnings
                            All       Low SES High SES              Low PSU        High PSU
Slope                      0.474        0.450     0.464               0.392          0.498
                          (0.006)      (0.012)   (0.008)             (0.011)        (0.008)
Intercept                  0.119        0.163     0.088               0.224          0.048
                          (0.004)      (0.008)   (0.005)             (0.007)        (0.005)
Share non-reports          0.389        0.444     0.356               0.451          0.337
Noise-to-Signal            1.112        1.223     1.154               1.554          1.008
N                          62893        18593     41324               25508          37385

                                           C. Costs
                             All       Low SES High SES              Low PSU        High PSU
 Slope                      0.879        0.842       0.864             0.797          0.866
                           (0.009)      (0.017)     (0.012)           (0.016)        (0.013)
 Intercept                 -0.083       -0.081      -0.083            -0.106         -0.069
                           (0.004)      (0.008)     (0.004)           (0.007)        (0.004)
 Share non-reports          0.397        0.453       0.363             0.471          0.336
 Noise-to-Signal            0.138        0.188       0.157             0.255          0.155
 N                          59820        17474       39640             23552          36268
Table compares student beliefs about earnings and costs to observed values. Observations are
at the student-degree level. “Share non-reports” is the share of possible belief elicitations to
which a student responds “I don’t know.” “Slope” and “Intercept” are estimated slope and
intercept terms from a univariate regression of log beliefs on log observed values. Standard
errors (in parentheses) are clustered at the student level. “Noise to Signal” reports the noise-to-
signal ratio corresponding to the estimated slope. See Section 5.1 for details. Columns denote
student subsamples. Panel A presents results for beliefs about earnings for a typical student,
Panel B for beliefs about own earnings, and Panel C for beliefs about costs. See section 5.1 for
a description of belief elicitation and benchmarking procedures. Standard errors clustered at
student level are in parentheses.




                                                49
                                                          Table 3: Impact of treatment on outcome variables

                                                                       Pooled         Low-SES          High-SES        Low-PSU          High-PSU        Low-SES & Low-PSU
      Matriculation                                                     0.004           0.000            0.003          -0.005            0.008               -0.012
                                                                       (0.004)         (0.008)          (0.006)         (0.006)          (0.005)              (0.010)
      All Students
      Net Value                                                         8,270           10,749            5,427           5,071           11,059                   5,790
                                                                       (5,217)          (7,296)          (7,370)         (5,352)          (8,094)                 (6,874)
      Earnings Gains                                                    8,856           11,252            5,932           5,534           12,031                   5,817
                                                                       (5,740)          (7,973)          (8,139)         (5,812)          (8,886)                 (7,445)
      Monthly Debt                                                       267              319              34.8            385              361                    -157
                                                                        (536)            (722)            (775)           (503)            (816)                   (621)
      Conditional on Matriculation
      Net Value                                                       10,029*          15,274*            8,040          12,008*           7,545                 18,430*
                                                                      (4,230)           (7,149)          (5,435)         (5,547)          (5,455)                 (7,366)
      Earnings Gains                                                  10,971*          16,083*            9,066          13,091*           8,438                 19,288*
                                                                      (4,532)           (7,671)          (5,819)         (5,887)          (5,784)                 (7,795)
50




      Monthly Debt                                                      376               763              125           1,036*            -166                     750
                                                                       (435)             (680)            (580)           (491)            (552)                   (610)
      Degree Average Earnings at Age 26                               6,324*           11,759**           3,789           2,682           7,936*                 11,337**
                                                                      (2,814)           (4,425)          (3,771)         (3,421)          (3,949)                 (4,396)
      Monthly Payment (conditional on enrollment)                       498               824              344            1,197*            164                     933
                                                                       (459)             (758)            (568)           (578)            (546)                   (713)
      PDV of long- and short-run returns
      Returns to Degree at Age 50                                   2, 459, 579+     4,190,955*        1,458,519        1,422,974        2,735,342             3,995,739*
                                                                    (1,480,585)      (2,107,587)      (1,947,204)      (1,430,280)      (2,038,518)            (1,704,860)
      Returns to Degree at Age 30                                     999,737*       1,369,854*         778,104          434,677        1,252,546*             1,364,551**
                                                                     (399,217)        (568,630)        (526,576)         406,904         (556,829)              (490,399)
     Table reports coefficients on treatment dummy from a regression of the dependent variable (row) on treatment, the dependent variable value for the survey response first choice
     for enrollment and randomization block. Clustered standard errors are in parentheses. Regression results in the second panel combine extensive and intensive margins; values
     of the outcome variables are set to zero if the respondent didn’t matriculate anywhere. Third and fourth panels report intensive margin effects in sample of matriculants. Net
     Value, Earnings Gains, and Monthly Debt are the values for degrees as exhibited in our experiment. Degree Average Earnings at Age 26 is a regression-adjusted estimate of
     earnings for all enrolling students that conditions on student SES, test scores, and gender. LR and SR Returns are predicted earnings gains conditional on enrollment projected
     to ages 30 and 50, respectively. See section 3.4 for details. Low-SES is defined as the lowest two income quintiles as defined by Mineduc; High-SES is the highest 3 income
     quintiles. Low PSU is defined as below the median PSU in the experiment sample (median = 537), High PSU is defined as above median PSU. + p <0.10, * p < 0.05, ** p < 0.01,
     *** p < 0.001.
                     Table 4: Effect of treatment on medium-run outcomes

         Pooled Low SES            High SES                 Low PSU      High PSU       Low SES & Low PSU
A. Persistence and completion for past students
Choose deg. with duration >3 years
Effect    0.004      0.013          -0.001                    0.007        -0.001                0.014
SE       (0.005)    (0.010)         (0.006)                  (0.009)       (0.004)              (0.012)
Mean      0.781      0.616           0.880                    0.600         0.919                0.523

Persistence for past students
Effect    0.003        0.003              0.002               0.003         0.002                0.003
SE       (0.002)      (0.003)            (0.002)             (0.002)       (0.002)              (0.003)
Mean      0.746        0.720              0.766               0.706         0.780                0.697

Grad. rate for past students
Effect    0.001        0.002              0.000              -0.002         0.004                0.001
SE       (0.004)      (0.006)            (0.005)             (0.004)       (0.005)              (0.007)
Mean      0.582        0.548              0.602               0.538         0.615                0.529

B. Observed outcomes
Enrolled in each year 1-4
Effect    0.000       0.024*            -0.012*               0.007        -0.006               0.028*
SE       (0.005)     (0.010)            (0.006)              (0.009)       (0.005)              (0.012)
Mean      0.729       0.608              0.812                0.597         0.830                0.548

Graduated in 3 years or less
Effect -0.002       -0.018*              0.007*              -0.007         0.002               -0.025**
SE      (0.003)     (0.007)              (0.003)             (0.006)       (0.002)               (0.009)
Mean     0.074       0.134                0.043               0.140         0.024                 0.171


       This table reports effects of treatment on measure of schooling attainment. Samples are in
       columns, dependent variables are in rows. ‘Effect’ is point estimate of treatment effect, ‘SE’ is
       standard error of point estimate, and ‘Mean’ is the mean of the dependent variable. ‘Choose
       deg. with duration > 3’ is an indicator equal to one if a student’s initial degree choice has
       a recommended program length of more than three years. ‘Persistence for past students’ is
       the mean rate of continuation from the first year to the second year for students in the pre-
       intervention 2007-2012 at the degree programs chosen by students in the analysis sample.
       ‘Grad. rate for past students’ is the graduation rate at the student’s chosen degree for entering
       students in the 2000-2005 cohorts. ‘Enrolled in each year 1-4’ is a dummy variable equal to
       one if a student is observed enrolled in a higher education program in each of the first four
       years following treatment. ‘Graduated in 3 years or less’ is a dummy equal to one if a stu-
       dent completes a higher education degree program in 2015 or earlier. ‘Enrolled in multiple
       programs’ is a dummy equal to one if a student enrolls in more than one higher education
       program over the 2013-2016 period. Sample is treatment and control students conditional on
       2013 matriculation. See section 5.5 for details. + p <0.10, * p < 0.05, ** p < 0.01, *** p < 0.001.




                                                       51
              Table 5: Utility weights and enrollment elasticities for earnings and costs

                                     All      Low SES     High SES     Low PSU      High PSU    Low PSU, Low SES
  A. Utility weights
  Earnings
  Treated                         0.3666***   0.4844***   0.3513***    0.1419***    0.2154***        0.2192***
  Untreated                       0.2879***   0.2977***   0.3159***    0.0699***    0.1027***        0.0691***
  Costs
  Treated                        -0.6197*** -1.2305***    -0.2334***   -0.9056***   0.7124***       -1.5899***
  Untreated                      -0.6042*** -1.1262***    -0.2416***   -0.8617***   0.6714***       -1.4982***
  B. Earnings elasticities at first choice program
  Baseline
  Treated                          0.1816      0.2378      0.1748       0.0737       0.1021           0.1125
  Untreated                        0.1428      0.1462      0.1574       0.0363       0.0488           0.0354
  No geography preference.
  Treated                          0.2104      0.2842      0.1994       0.0859       0.1171           0.1325
  Untreated                        0.1655      0.1748      0.1795       0.0423       0.0560           0.0418
  No institutional preference.
  Treated                          0.2943      0.3909      0.2809       0.1166       0.1702           0.1792
  Untreated                        0.2312      0.2403      0.2527       0.0574       0.0812           0.0564
  No major preference.
  Treated                          0.3206      0.4216      0.3081       0.1238       0.1883           0.1897
  Untreated                        0.2519      0.2593      0.2772       0.0609       0.0899           0.0598

Panel A: Estimated utility weights that students place on earnings and cost outcomes. Results based on logit estimates
using survey data. See Section 6 for details and Table A.9 for additional results from logit estimation. “Treated” row
shows results for treated students, and “Untreated” row shows results for untreated students. Inference based on
standard errors that cluster at student level. + p <0.10, * p < 0.05, ** p < 0.01, *** p < 0.001. Panel B: Estimated
elasticities of enrollment probability with respect to changes in earnings at students stated first choice programs.
‘Baseline’ is elasticities estimated in data. Subsequent subpanels eliminate preferences over listed degree attributes
while keeping mean utility at each degree program fixed. See section 6 for details. Columns identify student subsam-
ples.




                                                          52
Table 6: Regressions of observed earnings effects on predicted earnings effects

                                       2000-2005         1982-1993
                      Corrected
                      Slope               0.716            0.778
                                      [0.332,1.100]    [0.415,1.142]
                      Intercept           28.90            149.9
                                     [-129.6,187.4]    [-95.82,395.6]
                      Uncorrected
                      Slope               0.466             0.180
                                      [0.324,0.608]    [0.0895,0.271]
                      Intercept           134.7             341.9
                                     [-26.77,296.2]     [116.6,567.2]
                      N cutoffs            500               501

Point estimates and 95% CIs from regressions of RD estimates of observed earnings changes
on RD estimates of predicted earnings changes as described in equation 6. Null hypothesis is
a slope of one and an intercept of zero. Observations are at degree program level with results
weighted by inverse sampling variance of dependent variable. “Corrected” panel reports re-
sults that account for sampling error in the independent variable by shrinking values towards
the sample mean. “Uncorrected” panel reports results that do not account for sampling error.
“2000-2005” column predicts outcomes for 2000 through 2005 entering cohorts. “1982-1993”
column predicts outcomes for 1982-1993 entering cohorts. See Section 7.1 for details.




                                             53
