                               NBER WORKING PAPER SERIES




    SEMIPARAMETRICALLY EFFICIENT ESTIMATION OF THE AVERAGE LINEAR
                        REGRESSION FUNCTION

                                         Bryan S. Graham
                                 Cristine Campos de Xavier Pinto

                                       Working Paper 25234
                               http://www.nber.org/papers/w25234


                      NATIONAL BUREAU OF ECONOMIC RESEARCH
                               1050 Massachusetts Avenue
                                 Cambridge, MA 02138
                                    November 2018




We thank Guido Imbens, Pat Kline, Tony Strittmatter and seminar participants at University College
London, UC Berkeley and University of St. Gallen for helpful discussion. Financial support from NSF
grant SES #1357499 is gratefully acknowledged. The initial draft of this paper was prepared in October
of 2016. All the usual disclaimers apply. The views expressed herein are those of the authors and do
not necessarily reflect the views of the National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2018 by Bryan S. Graham and Cristine Campos de Xavier Pinto. All rights reserved. Short sections
of text, not to exceed two paragraphs, may be quoted without explicit permission provided that full
credit, including © notice, is given to the source.
Semiparametrically Efficient Estimation of the Average Linear Regression Function
Bryan S. Graham and Cristine Campos de Xavier Pinto
NBER Working Paper No. 25234
November 2018
JEL No. C14,C21,C31

                                          ABSTRACT

Let Y be an outcome of interest, X a vector of treatment measures, and W a vector of pre-
treatment control variables. Here X may include (combinations of) continuous, discrete, and/or
non-mutually exclusive “treatments”. Consider the linear regression of Y onto X in a
subpopulation homogenous in W = w (formally a conditional linear predictor). Let b0 (w) be the
coefficient vector on X in this regression. We introduce a semiparametrically efficient estimate of
the average β0 = Ε [b0 (W)]. When X is binary-valued (multi-valued) our procedure recovers the
(a vector of) average treatment effect(s). When X is continuously-valued, or consists of multiple
non-exclusive treatments, our estimand coincides with the average partial effect (APE) of X on Y
when the underlying potential response function is linear in X, but otherwise heterogenous across
agents. When the potential response function takes a general nonlinear/heterogenous form, and X
is continuously-valued, our procedure recovers a weighted average of the gradient of this
response across individuals and values of X. We provide a simple, and semiparametrically
efficient, method of covariate adjustment for settings with complicated treatment regimes. Our
method generalizes familiar methods of covariate adjustment used for program evaluation as well
as methods of semiparametric regression (e.g., the partially linear regression model).


Bryan S. Graham
University of California - Berkeley
530 Evans Hall #3880
Berkeley, CA 94720-3880
and NBER
bgraham@econ.berkeley.edu

Cristine Campos de Xavier Pinto
Escola de Economia de São Paulo, FGV/SP
Rua Itapeva 474, sala 1200
São Paulo– SP, Brasil, 01332-000
cristine.pinto@fgv.br




A data appendix is available at http://www.nber.org/data-appendix/w25234
A Computer Code is available at https://github.com/bryangraham/ipt
Let Y be a scalar-valued outcome of interest, X a K × 1 vector of policy variables, and W
a J × 1 vector of additional controls. For example Y might equal hours worked, X include
the real wage rate and total unearned income (K = 2), and W be a vector of demographic
measures capturing heterogeneity in preferences for work (e.g., Pencavel, 1986, Section 4).
The goal is to summarize how Y – labor supply – covaries with X – the wage rate and
unearned income – “holding the controls W fixed”. In a second example, Y might be an
end-of-year student mathematics achievement measure, X a vector containing (i) number of
days absent from school, (ii) class size and (iii) an indicator for whether the student received
supplemental tutoring. Here the vector W might include beginning of school year joint
predictors of Y and X (e.g., prior mathematics achievement, socioeconomic background,
health indicators, and known determinants of class size and tutoring assignment used by the
school). The goal is to summarize how math achievement covaries with attendance, class
size and supplemental tutoring conditional on W (cf., Gottfried & Kirksey, 2017).
Following the prototype established by Yule (1899) over one hundred years ago, social scien-
tists typically report the coeﬃcient on X in the (long) least squares fit of Y onto a constant,
X, and W for this purpose.
When X is a scalar binary variable, the econometrician can choose from – in addition
to least squares – an ever more elaborate menu of covariate adjustment methods (see
Imbens & Rubin (2015) for a recent textbook introduction). Many of these methods ex-
tend naturally to settings where X is multi-valued (e.g., Cattaneo, 2010).
When X is continuously-valued, and/or consists of multiple distinct policy variables (K ≥ 2),
options are fewer (cf., Wooldridge, 2010, Chapter 21.6.3). The partially linear regression
(PLM) model
                         Y = X ′ β0 + h0 (W ) + U, E [U| W, X] = 0,                    (1)

represents one semiparametric generalization of (long) linear regression. Chamberlain (1986),
in an influential but never published paper, introduced an estimator for β0 in (1) (cf.,
Robinson, 1988). In later work he characterized its semiparametric eﬃciency bound (SEB)
(Chamberlain, 1992).
Partially linear regression is widely, albeit heuristically, used in empirical work. Typically
researchers proceed by (i) choosing W to be a rich vector of basis functions in the underlying
controls (e.g., a vector of polynomial or piecewise polynomial terms) and then (ii) estimate
β0 by least squares. With discretely-valued control variables a saturated specification for
h0 (W ) is possible, at least when utilizing a very large dataset (e.g., Angrist & Krueger,
1999, Section 2.3.1). A principled variant of this general approach is embodied in the E-
Estimation algorithm of Newey (1990) and Robins et al. (1992).


                                               1
In this paper we propose a diﬀerent approach to covariate adjustment. Consider a subpopu-
lation homogenous in W = w. Within this subpopulation we compute the linear regression
of Y onto a constant and X (formally a conditional linear predictor as in Wooldridge (1999)).
Let b0 (w) be the coeﬃcient on X in the conditional linear regression for the subpopulation
homogenous in W = w. We propose a method for identifying and eﬃciently estimating the
average regression coeﬃcient
                                          β0 = E [b0 (W )] .                                       (2)

The average is over the marginal distribution of controls, W .
In the absence of controls, the relationship between the linear predictor slope coeﬃcient
and the gradient of the (possibly nonlinear) conditional expectation function (CEF) of Y
given X = x is well-understood (e.g., Goldberger, 1991; Yitzhaki, 1996). In the presence of
controls, this relationship is rather more complicated (cf., Angrist, 1998; Sloczynski, 2017).
Our focus on averages of conditional linear predictor coeﬃcients allows for conditioning on
W , while also preserving the interpretative transparency of unconditional linear analyses.
That is, β0 , as we demonstrate below, is easy to interpret.
When X is binary-valued (multi-valued) β0 coincides with the (a vector of) average treat-
ment eﬀect(s); estimands familiar from the program evaluation literature (e.g., Hahn, 1998;
Imbens, 2000). These estimands have causal interpretations under certain conditions. Mod-
estly extending the analysis of Wooldridge (2004), we show that this causal interpretation
generalizes under a (i) heterogenous random coeﬃcients potential outcome structure and (ii)
an unconfoundedness-type assumption. These assumptions coincide with their program eval-
uation counterparts when X is binary- or multi-valued. Our semiparametric model includes
both the program evaluation model and the partially linear regression model as special cases.
Our work is also connected to the varying coeﬃcient model of Hastie & Tibshirani (1993).
Hastie & Tibshirani (1993) focus on pointwise estimation of b0 (w), while we focus on (eﬃ-
cient) estimation of the average β0 = E [b0 (W )].
The relationship of our work with that of Wooldridge (2004) is as follows.2 We both study
the same functional of the joint distribution of W , X and Y (see Equation (7) below).
Relative to Wooldridge (2004) we provide an average partial eﬀect interpretation of this
estimand under (i) weaker assumptions when maintaining a correlated random coeﬃcient
potential outcome structure and (ii) a new weighted average partial eﬀect interpretation
under a general potential response function structure. These are useful, but relatively mod-
est generalizations. More significantly we (i) provide distribution theory for the estimator
  2
    The Wooldridge (2004) paper remains unpublished, but a textbook treatment of the material in it can
be found in Chapter 21.6.3 of Wooldridge (2010).


                                                  2
proposed by Wooldridge (2004), (ii) characterize the semiparametric eﬃciency bound (SEB)
for β0 , and (iii) introduce a new locally eﬃcient estimator. The procedure proposed by
Wooldridge (2004) is ineﬃcient.
Another feature of our estimator is computational simplicity. Let µ̂W = N1 N
                                                                              !
                                                                                i=1 Wi be
the sample mean of W . A common approach to modeling heterogeneous eﬀects in applied
work is to compute the least squares fit of Y onto a constant, W − µ̂W , (W − µ̂W ) ⊗ X,
and X. As is well-known from textbook treatments on interaction terms in linear regression
analysis, centering the control variable vector, W , about is mean in this way ensures that the
coeﬃcient on X captures an average eﬀect. This approach essentially coincides with Oaxaca-
Blinder type methods of covariate adjustment popular in labor economics (e.g., Kline, 2014).
One variant of our procedure involves computing the exact same regression, but where X
is instead instrumented with a particular function of its conditional distribution given W
(i.e., of the “generalized” propensity score). Theorems 2 and 3 below show that this small
modification to a familiar estimation procedure delivers considerable gains.
The next section introduces our average linear regression model. We provide a statistical
definition of β0 as well as sets of assumptions under which it has a causal – average partial
eﬀect (APE) – interpretation. Section 2 presents the semiparametric eﬃciency bound for β0 .
Section 3 studies the large sample properties of the Wooldridge (2004) estimator. We also
introduce our new estimator and present its large sample properties. Finally, in Section 4,
we connect our results with prior work on eﬃcient estimation of average treatments eﬀects as
well as the partially linear semiparametric regression model. We end our paper with a small
simulation study in Section 5. All proofs are collected in the Appendix or the supplemental
materials.



1     Average linear regression model
We begin with a conventional sampling assumption.
                                                     #∞
Assumption 1. (Random Sampling) Let (Wi′ , Xi′, Yi )′ i=1 be a sequence of independent
                                   "

and identically distributed random draws from some population FW,X,Y with E [Y 2 | W = w] <
∞ and E ∥X∥2 % W = w < ∞ for all w ∈ W.
         $       %        &


The finite moment restrictions included in Assumption 1 ensure that a conditional linear
predictor (CLP) is well-defined for all w ∈ W.
Let
                                   e0 (w) = E [X| W = w]                                   (3)

                                              3
be the conditional mean of X given W = w and

                                   v0 (w) = V (X| W = w)                                   (4)

the corresponding conditional variance. We also require that X vary conditional on W = w.

Assumption 2. (Overlap) For all w ∈ W and any non-zero column vector t, t′ v0 (w) t ≥
κ > 0.

Assumption 2 ensures that the CLP is uniquely defined. In the absence of conditioning it
is equivalent to linear independence of the elements of X. When X is binary v0 (W ) =
e0 (W ) (1 − e0 (W )) with e0 (W ) = Pr ( X = 1| W ) equal to the propensity score; in this
case Assumption 2 coincides with the familiar strong overlap assumption from the program
evaluation literature. More generally Assumption 2 implies that X varies conditional on
W = w for all w ∈ W.
Under Assumptions 1 and 2 the conditional linear predictor is well-defined for all w ∈ W.
Wooldridge (1999, Section 4) provides a self-contained introduction to conditional linear
predictors. The following definition and lemma is taken from Wooldridge (1999).

Definition 1. (Conditional Linear Predictor) The mean squared error minimizing
linear predictor of Y given X conditional on W = w, henceforth the conditional linear
predictor (CLP), equals

                                              def
                           E∗ [ Y | X; W = w] ≡ a0 (w) + X ′ b0 (w)                        (5)

with

                                  def
                           a0 (w) ≡ E [Y | W = w] − e0 (w)′ b0 (w)                         (6)
                                  def
                           b0 (w) ≡ v0 (w)−1 C (X, Y | W = w) .

It is straightforward to show that the prediction error U = Y − E∗ [Y | X; W ] is conditionally
mean zero and conditionally uncorrelated with X. This property of E∗ [Y | X; W ] will prove
useful for what follows.
                                                          def
Lemma 1. Wooldridge (1999, Lemma 4.1). Let U ≡ Y − a0 (W ) − X ′ b0 (W ), then
E [ U| W = w] = 0 and E [ XU| W = w] = 0 for all w ∈ W.




                                              4
Identification of the average regression slope
We begin by presenting a convenient representation of the average slope coeﬃcient β0 =
E [b0 (W )] in terms of the joint distribution of (W ′ , X ′, Y )′ . The most direct representation
follows directly from (6):
                                β0 = E v0 (W )−1 C (X, Y | W ) .
                                        $                           &

For our purposes, however, an alternative representation of β0 is more convenient; both
for our semiparametric eﬃciency bound (SEB) analysis and for the approach to estimation
developed below. Using the law of iterated expectations and the definition of conditional
covariance we get, under Assumptions 1 and 2,

            E v0 (W )−1 (X − e0 (W )) Y = E v0 (W )−1 E [ (X − e0 (W )) Y | W ]
             $                         &    $                                  &

                                         = E v0 (W )−1 C ( X, Y | W ) .
                                            $                        &


Applying definition (6) then gives our preferred estimand representation:

                              β0 = E v0 (W )−1 (X − e0 (W )) Y .
                                    $                         &
                                                                                               (7)

Wooldridge (2004) emphasizes the coincidence between (7) and the average partial eﬀect
of X on Y associated with a particular correlated random coeﬃcients (CRC) potential out-
comes structure. This endows β0 with causal meaning. While we also develop this connection
below, we wish to initially emphasize that (7) is also just one way of representing a popu-
lation average of conditional linear predictor coeﬃcients. Under Assumptions 1 and 2 the
expectation in (7) is well-defined and β0 is simply a “statistical” estimand. We are interested
in estimating it as precisely as possible.


Causal interpretation
In this subsection we show that (7) admits a causal interpretation under a particular treat-
ment response model and selection on observables type assumption. As noted earlier, this
interpretation was previously emphasized by Wooldridge (2004), but under stronger condi-
tions than we maintain here.
Associated with each agent in the target population is an individual-specific potential re-
sponse function, Y (x), which maps counterfactual values of the input vector X into their
corresponding (potential) outcomes. The observed outcome coincides with the value of the
potential response function at the observed input level X: Y = Y (X). We assume that


                                                5
Y (x) is linear in x, but otherwise heterogeneous across individuals:

                                        Y (x) = A + x′ B,                                   (8)

where A and B are an individual-specific intercept and slope vector respectively.
Equation (8) allows for each individual to have their own potential response function, but
restricts them to be linear in X. When X is binary, or multi-valued, linearity is unrestrictive.
For example, in the binary case, we have the potential outcome under control (X = 0) and
active (X = 1) treatment equal to Y (0) = A and Y (1) = A + B. In the multi-valued
treatment setting of Imbens (2000) and Cattaneo (2010), with X a vector of treatment
indicators for K mutually exclusive treatments, we have Y (0) = A and Y (k) = A + Bk
for k = 1, . . . , K. In contrast, when X is ordered, continuously valued, or includes multiple
treatments/policies, linearity is restrictive.
Consider the following thought experiment: draw a unit at random and (exogenously) in-
crease the value of the k th component of X by one unit. The expected eﬀect of this inter-
vention is E [Bk ]. In the binary- and multi-valued treatment setting E [Bk ] corresponds to
an average treatment eﬀect (ATE)

                                   E [Bk ] = E [Y (k) − Y (0)] .

More generally E [Bk ] equals the average partial eﬀect (APE) of a unit increase in Xk . This
estimand was introduced in a panel data setting by Chamberlain (1984); general expositions,
with additional results, are available in Blundell & Powell (2003) and Wooldridge (2005).
Under the following assumption, in addition to those introduced above, we can show that β0
coincides with the APE vector, E [B].
Assumption 3. (Conditional Exogeneity) For all w ∈ W and k, l = 1, . . . , K, and
under potential responses of the form given in (8)

            C ( A, Xk | W = w) = C (B, Xk | W = w) = C ( B, Xk Xl | W = w) = 0.             (9)

Assumption 3 restricts the form of any dependence between the potential response function,
Y (x) = A + x′ B, and the treatment vector actually chosen by the respondent, X. It is a
conditional exogeneity or selection on observables type assumption. To see this observe that
when X is binary Assumption 3 coincides with the standard mean independence assumption
familiar from the program evaluation literature, implying that

                                  E[Y (x)|X, W ] = E[Y (x)|W ].

                                                 6
In the multi-valued treatment setting Assumption 3 also coincides with standard generaliza-
tions of the mean independence assumption (cf., Imbens, 2000). See also Section 4 below.
When the linearity of (8) is restrictive, as occurs when X includes continuously-valued com-
ponents, or non-mutually exclusive binary inputs, Assumption 3 is less restrictive than other
possible formulations of conditional exogeneity. For example, Wooldridge (2004, 2010) works
with the identifying restrictions

        E [ X| W, A, B] = E [ X| W ] = e0 (W ), V ( X| W, A, B) = V (X| W ) = v0 (W )   (10)

which imply (9), but are generally stronger. An even stronger notion of conditional exogene-
ity is
                    E [ A| X, W ] = E [A| W ] , E [B| X, W ] = E [ B| W ] .             (11)

Assumption 3 is (apparently) the weakest assumption necessary to equate β0 with the average
partial eﬀect of X on Y when the potential response function takes form (8). The estimator
we introduce below will remain consistent under the stronger restrictions, (10) and (11), but
will generally not be semiparametrically eﬃcient in those cases. We elaborate further on this
observation below.

Proposition 1. (Average Partial Effect Identification) Under Assumptions 1, 2
and 3 the average of the CLP coeﬃcients, β0 = E [b0 (W )], and the average partial eﬀect
(APE), E [B], coincide:
                                          β0 = E [B] .

Proof. Wooldridge (2004) demonstrates the equality under the stronger restriction (10).
Under Assumption 3, however, the proof proceeds diﬀerently. Given the linear potential
response (8) and by lemma (1), we have the 1 + K conditional moment restrictions

          E [ U| W = w] = E [ A − a0 (W )| w] + E [X ′ (B − b0 (W ))| w] = 0
        E [ XU| W = w] = E [ X (A − a0 (W ))| w] + E [XX ′ (B − b0 (W ))| w] = 0.       (12)

Under Assumption 3 conditions (12) simplify to

                          {E [ A| w] − a0 (w)} + e0 (w)′ {E [ B| w] − b0 (w)} = 0
               e0 (w) {E [ A| w] − a0 (w)} + E [XX ′ | w] {E [ B| w] − b0 (w)} = 0




                                               7
or, in matrix form,
                  '                          ()                       *       )       *
                         1     e0 (w)′            E [A| w] − a0 (w)               0
                                                                          =               .
                      e0 (w) E [ XX ′ | w]        E [B| w] − b0 (w)               0

Under the Assumption 2 the first matrix to the left of the equality is invertible for all w ∈ W.
This implies that E [ A| W = w] = a0 (w) and E [B| W = w] = b0 (w) for all w ∈ W. The
result follows by iterated expectations.


Causal interpretation under misspecification
Angrist & Krueger (1999, Section 2.3.1) and Angrist & Pischke (2009, Chapter 3.3) empha-
size that when X is a continuously-valued random variable its slope coeﬃcient in the linear
predictor of Y onto a constant, X and the vector of “saturated” controls admits a weighted
average derivative interpretation when the potential response function takes a general non-
linear form (cf., Angrist et al., 2000, Lemma 5). Angrist and Krueger’s (1999) expression is
also isomorphic to the probability limit of the E-Estimator of Newey (1990) and Robins et al.
(1992)
                                         E [Y (X − e0 (W ))]
                                  βE =                                                  (13)
                                         E [X (X − e0 (W ))]
when the partially linear regression structure, equation (1) above, is incorrect.
In this section, using similar arguments to those appearing in Angrist et al. (2000, Lemma
5) and Graham et al. (2010, Lemma A.1), we provide a representation result for β0 under a
general potential response function.
Assume that the potential response function is nonlinear and heterogeneous such that
Y (x) = h (x, U). Further assume, stronger than Assumption 3 above, that U is condi-
tionally independent of X given W = w for all w ∈ W. Blundell & Powell (2003) show
that the partial mean EW [E [ Y | W, X = x]] identifies the average structural function (ASF)
m (x) = EU [h (x, U)] when the support of W given X = x coincides with its marginal sup-
port. Newey (1994) provides an explicit partial mean estimator and derives in asymptotic
properties.
Here we show that our average regression slope estimand, β0 , can be expressed as a weighted
average of the gradient of h (X, U). This provides a causal interpretation of β0 under a
general potential response function. To present this result we replace Assumption 3 with:

Assumption 4. (Nonlinear Potential Response Function) (i) X is a continuous
scalar random variable with bounded support X = [x, x], (ii) the conditional density function

                                                   8
of X given W = w is bounded and bounded away from zero for all (w, x) ∈ W × X, (iii)
Y = h (X, U) with h (x, u) a continuously diﬀerentiable function of x for all (x, u) ∈ X × U
and h (u) = h (x, u) finite for all u ∈ U, and (iv) U is conditionally independent of X given
W = w for all w ∈ W.

Proposition 2. ( Weighted Average Derivative Representation) Under Assump-
tions 1, 2 and 4                       +                   ,
                                                 ∂h (X, U)
                                 β0 = E ω (W, X)
                                                     ∂x
where
                                                                   -                  .
                         1           E [ X − e0 (W )| W = w, X ≥ x] 1 − F X|W ( x| w)
        ω (w, x) =                                                                   . .
                   f X|W ( x| w) xx̄ E [ X − e0 (W )| W = w, X ≥ v] 1 − F X|W ( v| w) dv
                                /                                  -


Proof. See the Supplemental Web Appendix.

A key feature of the weighting function ω (w, x) is that its conditional mean,
E [ω (W, X)| W = w], equals 1 for every value of w ∈ W. Furthermore, Lemma A.1 of
Graham et al. (2010) implies that, conditional on W = w, the weight given to ∂h(X,U  ∂x
                                                                                         )
                                                                                           is
highest for those values of X near its conditional mean, E [ X| W = w], and lowest for those
at the boundary of its support, x and x.
These features of the weights appearing in Proposition 2 imply the following intuitive in-
terpretation: (i) for each value of w ∈ W compute a weighted average of ∂h(X,U
                                                                             ∂x
                                                                                 )
                                                                                   , where
the average emphasizes values of X near its conditional mean given W = w, (ii) average
these (weighted average) gradients over the marginal
                                              0        distribution
                                                         1          of W . This indicates that
                                                ∂h(X,U )
β0 only diﬀers from the unweighted average E      ∂x
                                                           due to variation in ω (W, X) within
W = w cells. The contribution of each subpopulation, defined in terms of the control, W ,
mirrors its density in the sampled population. Since W proxies for U in this set-up we are
averaging over the correct heterogeneity distribution.
More precisely, since E [ ω (W, X)| W = w] = 1, we have that, using the definition of condi-
tional covariance,
                         +          ,    + 2                      % 3,
                          ∂h (X, U)                    ∂h (X, U) %%
                   β0 − E             = E C ω (W, X) ,            %W   .                   (14)
                              ∂x                           ∂x
                    0         1
The bias of β0 for E ∂h(x,U
                        ∂x
                            )
                                is therefore solely due to conditional covariance between the
weight function and the gradient of interest within subpopulations homogenous in W .
In contrast to the one for β0 , the weight function appearing in the weighted average derivative
representation result of Angrist & Krueger (1999) or Angrist & Pischke (2009) for βE is only

                                               9
unconditionally mean zero. This implies that βE averages over the incorrect heterogeneity
distribution as well as the incorrect policy variable distribution.
                       +       ,   + 2                       % 3,
                     ∂h (X, U)                   ∂h (X, U) %%
              βE − E             =E C ω (W, X) ,             %W                                            (15)
                         ∂x                          ∂x
                                     2                     +            % ,3
                                                             ∂h (X, U) %%
                                  + C E [ω (W, X)| W ] , E              %W   .
                                                                 ∂x
                                                                       0              1
                                                                           ∂h(X,U )
If the ultimate object of interest is the average derivative E               ∂x
                                                                                       , then, relative to (15),
a focus on β0 eliminates one source of potential bias. Namely that the weight function may
over- or under-emphasize various subpopulations defined in terms of their value of the control
variable vector W . In this case E [ ω (W, X)| W ] may not equal one and the second term to
the right of the equality in (15) may be non-zero.3


Motivating β0
Our focus on averages of conditional linear predictor slope coeﬃcients is motivated by a
combination of principled and pragmatic reasons.
First, the kitchen sink long regression remains a workhorse of everyday empirical social
science research. Our model extends kitchen sink regression in an easy to understand way.
Relative to the partially linear regression model, our model allows for heterogenous responses
of Y to variation in X; a feature likely to be both empirically relevant and a priori attractive
to researchers.
Second, β0 has a causal interpretation under additional assumptions. When the potential
response function is linear, but heterogeneous across agents, it coincides with an average
partial eﬀect (APE) under a selection on observables type assumption. When X is binary-
or multi-valued, as in the program evaluation literature, it coincides with the well-known
average treatment eﬀect (ATE). Our causal model nests the usual one as a special case, but
accommodates continuous and/or multiple treatments as well (albeit under restrictions).
Third, in the presence of misspecification β0 coincides with a weighted average of the
derivative of a general non-linear potential response function. This weighted average
derivative is more interpretable than existing representation results; for example those of
Angrist & Krueger (1999) for βE .
                               √
Fourth, as we show next, β0 is N estimable (or regularly identified). This is not the case
for, say, a partial mean with a continuous policy variable (e.g., Newey, 1994). Regular
   3
    To be clear ω (W, X) are diﬀerent functions in expressions (14) and (15); for its form in the latter case
see Angrist & Krueger (1999) or Angrist & Pischke (2009).

                                                     10
identification suggests that estimation is practically feasible and we present one such feasible
estimator below.
Ultimately the balance between ease of interpretation under various population assumptions
and, as we show below, ease of estimation, provides the strongest case for focusing on β0 .



2     Semiparametric eﬃciency bound
Using the method of calculation outlined by Bickel et al. (1993) and Newey (1990), we derive
the semiparametric variance bound for β0 of,


                                I(β0 )−1 = E [Ω0 (W )] + V(b0 (W )),                         (16)

where
                  0                                                    #′ %%   1
        Ω0 (w) = E v0 (W )−1 (X − e0 (W )) UU ′ v0 (W )−1 (X − e0 (W )) % W = w .
                                               "


The corresponding eﬃcient influence function equals

     ψβeﬀ (Z, β0 , g0 (W ) , h0 (W )) =v0 (W )−1 (X − e0 (W )) (Y − a0 (W ) − X ′ b0 (W ))   (17)
                                      + (b0 (W ) − β0 )

with Z = (W ′ , X ′ , Y )′ , g (W ) = (e(W ), v(W )) and h (W ) = (a (W ) , b (W )).

Theorem 1. (Semiparametric Efficiency Bound) The eﬃcient influence function for
β0 = E [b0 (W )] in the semiparametric problem established by Definition 1 and Assumptions
1 and 2 equals (17).

Proof. See Appendix A.
We also have the following corollary, which is similar to a result for the binary case due to
Robins et al. (1994), Hahn (1998) and Chen et al. (2008). This corollary will be useful when
we discuss locally eﬃcient estimation in Section (3).

Corollary 1. (Redundancy) Let f ( x| w; φ) be a parametric family of conditional distribu-
tions for X given W with f0 ( x| w) = f ( x| w; φ) at some unique φ = φ0 . The knowledge that
f0 ( x| w) is a member of the family f ( x| w; φ) does not change the semiparametric eﬃciency
bound for β0 .

Proof. See the Supplemental Web Appendix.

                                                 11
See Frölich (2004) and Graham et al. (2016) for additional intuition about results like Corol-
lary 1.


Double robustness property of the eﬃcient influence function
Before introducing our estimator in the next section we highlight an important property of
the eﬃcient influence function for β0 .
Consider replacing h0 (W ) = (a0 (W ) , b0 (W )) in (17) with the incorrect conditional
linear predictor coeﬃcients h∗ (W ) = (a∗ (W ) , b∗ (W )).   Use the notation U∗ =
(Y − a∗ (W ) − X ′ b∗ (W )) to emphasize that U∗ is the prediction error associated with an
arbitrary conditional linear predictor (which need not be the mean squared error minimizing
one). Note that U∗ will not be conditionally mean zero or conditionally uncorrelated with X
(i.e., E [ U∗ | W ] ̸= 0 and E [ XU∗ | W ] ̸= 0). Nevertheless, as long as e0 (X) and v0 (W ) equal
the true conditional mean and variance of X given W , we have the pair of equalities, using
iterated expectations,

                           E v0 (W )−1 (X − e0 (W )) a∗ (W ) =0
                            $                                 &

                         E v0 (W )−1 (X − e0 (W )) X ′ b∗ (W ) =E [b∗ (W )]
                          $                                   &


(the second equality follows from the fact that E [ (X − e0 (W )) X ′ | W ] = v0 (W )).
Therefore (17) remains mean zero even if the nuisance functions h0 (W ) = (a0 (W ) , b0 (W ))
are replaced by arbitrary functions of W :

                               E ψβeﬀ (Z, β0 , g0 (W ) , h∗ (W )) = 0.
                                $                                &
                                                                                              (18)

One special choice of h∗ (W ) is the zero vector. This choice directly recovers the representa-
tion of β0 derived earlier (Equation (7) above). In moment condition form

                             E v0 (W )−1 (X − e0 (W )) Y − β0 = 0.
                              $                              &


Next consider replacing g0 (W ) = (e0 (W ) , v0 (W )) in (17) with the incorrect condi-
tional mean and variance functions g∗ (W ) = (e∗ (W ) , v∗ (W )). Use the notation U0 =
(Y − a0 (W ) − X ′ b0 (W )) to emphasize that U0 is the prediction error associated with the
mean squared error minimizing conditional linear prediction of Y given X conditional on W .




                                                 12
By Lemma 1 E [ U0 | W ] = 0 and E [ XU0 | W ] = 0. Therefore

E ψβeﬀ (Z, β0, g∗ (W ) , h0 (W )) =E v∗ (W )−1 (X − e∗ (W )) (Y − a0 (W ) − X ′ b0 (W ))
 $                               &  $                                                    &

                                     + E [(b0 (W ) − β0 )]
                                    =E v∗ (W )−1 E [XU0 | W ] − E v∗ (W )−1 e∗ (W ) E [U0 | W ]
                                      $                      &   $                              &

                                    =0.

Hence (17) also remains mean zero even if the nuisance functions g0 (W ) = (e0 (W ) , v0 (W ))
are replaced by arbitrary functions of W .
Moment (17) has the so-called doubly robust property of Scharfstein et al. (1999) (cf., Ruud,
1986). It is mean zero as long as one of the two nuisance functions, g (W ) or h (W ), coincides
with its population one. We exploit this property when constructing our estimator in the
next section.



3     Estimation
In this section we present a locally semiparametrically eﬃcient estimate of β0 . To motivate
the precise form of our estimator we also discuss the estimator proposed by Wooldridge
(2004). A textbook presentation of this estimator is available in Chapter 21.6.3 of Wooldridge
(2010).
For the purposes of estimation we impose a parametric restriction on the conditional distri-
bution of X given W. Since the distribution of X given W is ancillary for β0 , this parametric
restriction does not change the semiparametric eﬃciency bound (cf., Corollary 1). We call,
borrowing nomenclature from related settings (e.g., Hirano & Imbens, 2004), the resulting
model for X the generalized propensity score.

Assumption 5. (Generalized Propensity Score) f (x| w; φ) is a parametric family of
densities indexed by φ ∈ Φ ⊂ RL with (i) f0 ( x| w) = f( x| w; φ0) at some unique φ0 ∈ int (Φ),
(ii) a maximum likelihood estimate (MLE) of φ0 equal to

                                                 N
                                                 4
                                  φ̂ = arg max         ln f ( Xi | Wi ; φ)
                                           φ∈Φ
                                                 i=1

                                                                                    p
with a score vectors of Sφ ( X| W ; φ) = ∇φ f ( X| W ; φ) /f ( X| W ; φ), (iii) φ̂ → φ0 with E [Si S′i ]




                                                  13
non-singular and the asymptotically linear representation

                                                          N
                          √    5       6
                                                 ′ −1 1
                                                         4
                              N φ̂ − φ0 = E [Si Si ] √       Si + op (1)                  (19)
                                                       N i=1

where Si = Sφ ( Xi | Wi ; φ0) .

Assumption 5 corresponds to a parametric model for the propensity score when X is a binary
treatment indicator. More generally Assumption 5 requires the researcher to model the
distribution of the policy given controls. Consider a researcher interested in the relationship
between regular school attendance and student achievement. In this case Y could be a
measure of end-of-school-year achievement, X number of school days absent, and W a vector
of joint determinants of achievement and attendance (e.g., family background measures, prior
achievement, pre-existing health conditions etc.). In this case the researcher might assume
that the distribution of X given W is Poisson with

                  E [ X| W ] = exp k (W )′ φ0 , V (X| W ) = exp k (W )′ φ0 ,
                                  -          .                 -          .


where k (W ) is a known L×1 vector of functions of W . Estimation of φ0 , and hence e (W ; φ0)
and v (W ; φ0 ), is by maximum likelihood. In most cases the conditional distribution of X
given W can be conveniently modeled by, depending on the nature of X, the appropriate
generalized linear model (GLM). When X is multivariate, the outcome of censoring, or has
mixed discrete/continuous components,
                                  5 then6 specifying
                                                 5 f (6x| w; φ) may involve considerable
work. For complicated likelihoods e W ; φ̂        and v W ; φ̂ may need to be approximated
numerically or by simulation.


The Wooldridge (2004) estimator
Wooldridge (2004) introduced a two-step estimator for β0 . A textbook exposition appears
in Wooldridge (2010, Chapter 21.6.3 ). His procedure is summarized in Algorithm 1.
Wooldridge (2004) does not characterize the asymptotic sampling properties of β̂W . In this
section, we show that Wooldridge’s estimator is not eﬃcient under Assumptions 1, 2 and
5. Furthermore it requires the generalized propensity score to be correctly specified. The
structure of this ineﬃciency and lack of robustness, as well as the form of the eﬃcient
influence function derived earlier, guides the construction of our new, locally eﬃcient and
doubly robust estimator.
The second step of Algorithm 1 corresponds to finding the β̂W which solves the sample


                                                14
Algorithm 1 The Wooldridge (2004) Estimate of β0
                                                                          6     5  5     6
  1. Compute the maximum likelihood estimate of φ0 and construct e Wi , φ̂ and v Wi , φ̂
     for i = 1, . . . , N;

  2. Compute linear instrumental variables fit of Y onto X (with no constant) using
      5      6−1 5      5      66
     v W ; φ̂      X − e W ; φ̂   as the instrument for X. The coeﬃcient on X equals
      β̂.



moment
                                            N
                                         1 4 5               6
                                               ρ Zi , φ̂, β̂W = 0,                           (20)
                                         N i=1

for ρ (Z, φ, β) = v (W ; φ)−1 (X − e (W ; φ)) (Y − X ′ β) . Here φ̂ corresponds to the MLE of φ0
computed in the first step of the procedure. A mean value expansion of (20) in β̂W about β0
yields

                                                N
                                             1 4 5            6
                             β̂W = β0 +            ρ Z, φ̂, β0 + op (N −1/2 ).
                                             N i=1

Rearrangement of terms and a second mean value expansion in φ̂ about φ0 gives

                                          N
            √     5            6      1 4
                N β̂W − β0         = √        ρ (Z, φ0, β0 )
                                       N i=1
                                       7     N               .8
                                                                √ 5
                                                    -
                                         1 4 ∂ρ Z, φ̄, β0                 6
                                     +                           N φ̂ − φ0 + op (1) .
                                         N i=1        ∂φ

Observe that under Assumptions 1 and 2

     E [ ρ (Z, φ0 , β0 )| W = w] = E v (W ; φ0 )−1 (X − e (W ; φ0)) (Y − X ′ β0 )% W = w
                                    $                                            %       &

                                     = b0 (w) − β0

since E v (W ; φ0)−1 (X − e (W ; φ0 )) X ′ % W = w = IK . In integral form:
       $                                   %      &

                      9
                          ρ (z, φ0 , β0 ) f0 ( y| w, x) f (x| w; φ0) dxdy = b0 (w) − β0 .    (21)




                                                      15
Diﬀerentiating (21) through the integral with respect to φ gives:
                        +               %      ,
                       ∂ρ (Z, φ0, β0 ) %%                               ′
                     E
                            ∂φ          % W = w = −E [ ρ (Z, φ0 , β0 ) S | W = w] ,               (22)


which is a Generalized Information Matrix Equality (GIME) result (e.g., Newey, 1990, p.
104).
Using (19) and (22) we have

                                    N
                 √ 5        6   1 4
                  N β̂W − β0 = √       ρi
                                 N i=1
                                                               N
                                                         1 4−1
                                         −E [ρS′ ] E [SS′ ]
                                                        √          Si + op (1)
                                                          N i=1
                                             N
                                         1 4:                         −1
                                                                          ;
                                      = √       ρi − E [ρS′ ] E [SS′ ] Si + op (1)                (23)
                                          N i=1

for ρi = ρ (Zi , φ0, β0 ) .
Similar to the result of Wooldridge (2007) for the binary X case, this asymptotically linear
representation of β̂W implies that if practitioners ignore sampling error in φ̂, they can get
conservative confidence intervals. In addition, this expression shows that over-parameterizing
the conditional distribution of X given W will not decrease the asymptotic precision β̂W .
We show next that β̂W is ineﬃcient for β0 in the semiparametric model defined by As-
sumptions 1, 2 and 5. This demonstration of ineﬃciency usefully provides insight into how
to construct a more eﬃcient estimator. We begin by decomposing Wooldridge’s (2004)
identifying moment into the eﬃcient influence function and a remainder: ρ (Z, φ0 , β0 ) =
ψβeﬀ (Z, β0 , φ0 , h0 (W )) + r (W, X, β0 , φ0 , h0 (W )) with

    r (W, X, β0 , φ0 , h0 (W )) =v (W ; φ0 )−1 (X − e (W ; φ0)) (a0 (W ) + X ′ (b0 (W ) − β0 ))   (24)
                                   − (b0 (W ) − β0 )

Let r0 (W, X) = r (W, X, β0 , φ0 , h0 (W )) . Note that E [r0 (W, X)| W ] = 0. Note further that
S is also conditionally mean zero given W .
Now observe that for l = 1, . . . , dim (φ)

                ∂ψβeﬀ                          ∂v (W ; φ0)
                            = −v (W ; φ0)−1                v (W ; φ0 )−1 (X − e (W ; φ0)) U
                 ∂φl                               ∂φl
                                               ∂e (W ; φ0)
                               −v (W ; φ0)−1               U,
                                                   ∂φl

                                                      16
and hence that
       '        % (
         ∂ψβeﬀ %%                    ∂v (W ; φ0 )
     E          %W  = −v (W ; φ0 )−1              v (W ; φ0)−1 E [ (X − e (W ; φ0 )) U| W ] (25)
          ∂φl %                          ∂φl
                                             ∂e (W ; φ0 )
                            −v (W ; φ0 )−1                E [U| W ]
                                                 ∂φl
                        = 0

by Lemma 1 above.
Next start with the fact that
                          9
                             ψβeﬀ f0 ( y| x, w) f (x| w; φ0) f0 (w) = 0.

Diﬀerentiating through the integral gives the equality

         ∂ψβeﬀ
     9                                                  9
                                                            " eﬀ ′ #
               f0 ( y| x, w) f ( x| w; φ0) f0 (w) = −        ψβ S f0 ( y| x, w) f ( x| w; φ0 ) f0 (w)
          ∂φ′

and hence that, using the decomposition of ρ (Z, φ0 , β0 ) introduced above and equation (25),

                              E [ρS′ ] = E ψβeﬀ S′ + E [rS′ ] = E [rS′ ] .
                                          $       &


Plugging this into our influence function we get

                              N
          √ 5        6    1 4:                         −1
                                                          ;
           N β̂W − β0 = √        ρi − E [ρS′ ] E [SS′ ] Si + op (1)
                           N i=1
                              N
                          1 4 : eﬀ 0                          −1
                                                                 1;
                       = √       ψβ,i + ri − E [rS′ ] E [SS′ ] Si + op (1) ,
                           N i=1

and hence an asymptotic distribution of
                √    5        6
                                D
                    N β̂W − β0 → N 0, I (β0 )−1 + E (r − ΠrS S) (r − ΠrS S)′
                                  -                $                         &.
                                                                                                        (26)

with ΠrS = E [rS′ ] × E [SS′ ]−1 .
The form of the the limit distribution (26) is similar to that of the familiar inverse probability
weighting (IPW) estimator for binary treatments (e.g., Graham et al., 2012, Proposition
3.1). In that context it is well-known that replacing a known propensity score with an
estimated one increases precision (Hirano et al., 2003; Hitomi et al., 2008; Graham, 2011).
In principle the degree of precision increase is increasing in the complexity/richness of the

                                                   17
fitted propensity score model. Expression (26) indicates that a similar phenomena operates in
our setting. If the portion of the eﬃcient influence function that is omitted by the Wooldridge
(2004) procedure is well-approximated by a linear combination of the scores used to estimate
the propensity score, then the β̂W will be precisely determined. In practice, instead of relying
on a possibly overfitted propensity score to yield eﬃcient estimates, it is better to redesign
the estimation procedure with eﬃciency in mind at the outset.


A locally eﬃcient, doubly robust estimator
Our estimator for β0 requires a working parametric model for the CLP coeﬃcients a0 (W )
and b0 (W ). Consistency and asymptotic normality of our estimate, β̂, will not depend on
the correctness of this working model, but its limiting variance will. A convenient working
model is provided by Assumption 6.

Assumption 6. (CLP Coefficients): a0 (W ) = α0 + (W − µW )′ γ0 and b0 (W ) = β0 +
∆0 (W − µW ).

In practice these models for a0 (W ) and b0 (W ) can be made arbitrarily flexible since W can
include a rich set of basis functions (e.g., squares, cross-products etc.) in the underlying
controls.
Under Assumption 6 we have that

             E∗ [ Y | X; W ] = α0 + (W − µW )′ γ0 + X ′ (β0 + ∆0 (W − µW ))
                               = α0 + (W − µW )′ γ0 + ((W − µW ) ! X)′ δ0 + X ′ β0 ,                   (27)

where δ0 = vec (∆0 ) .
Equation (27) implies that, maintaining Assumption 6, one approach to estimating β0 would
be to compute the least squares fit of Yi onto a constant, Wi − µW , all interactions of Wi − µi
and Xi , and Xi itself. For the special case where X is a binary treatment indicator, this
estimator is familiar to labor economists as a Oaxaca-Blinder average treatment eﬀect (ATE)
estimator (e.g., Sloczynski, 2015).4 Consistency of of this estimator hinges upon Assumption
6 accurately characterizing the sampled population.
In our setting Assumption 6 plays a diﬀerent role. Unlike in the Oaxaca-Blinder procedure,
its validity is not required for consistency, but if it does accurately described the sampled
   4
    In this literature researchers typically center W around E [ W | X = 1], not the unconditional mean µW =
E [W ] as is done here. With this alternative centering the coeﬃcient on Xi will coincide with the average
treatment eﬀect on the treated (ATT).


                                                    18
Algorithm 2 Locally Efficient and Doubly Robust Estimation Of β0
                                                                  5       6       5          6
  1. Compute the maximum likelihood estimate of φ0 and construct e Wi , φ̂ and v Wi , φ̂
     for i = 1, . . . , N;

  2. Compute the sample mean µ̂W = N1 N
                                      !
                                         i=1 Wi and construct Ri (µ̂W ) for i = 1, . . . , N ;

     3. Compute the linear instrumental variables fit of Yi onto Ri (µ̂W ) and Xi using
         5       6−1 5      5       66
        v Wi ; φ̂     Xi − e Wi ; φ̂   as the excluded instrument for Xi . The coeﬃcient on
       Xi in this fit coincides with β̂.



population our estimator will be highly eﬃcient. These benefits come at the cost of assuming
that prior knowledge regarding the form of the generalized propensity score is available (i.e.,
maintaining Assumption 5).
To describe our procedure we require some additional notation. Let λ = (α, γ ′, δ ′ )′ , R (µW ) =
                                     .′
  1, (W − µW )′ , ((Wi − µW ) ! Xi )′ and
-


                            Ui (µW , λ, β) = Yi − R (µW )′ λ − Xi′ β .
                                            -                       .


When R (µW ) is evaluated at the correct population mean of W , we often simply write R.
Our estimator is based upon the (L + J + 1 + J + JK + K)×1 vector of moment conditions,
m (Zi , θ), partitioned into the three ordered sub-vectors:

                    m1 (Xi , Wi , φ) =Sφ (Xi | Wi ; φ)                                       (28)
                         L×1

                      m2 (Wi , µW ) =Wi − µW                                                 (29)
                          J×1
                                     )                                 *
                                                     Ri (µW )
              m3 (Zi , φ, µW , λ, β) =              −1                     Ui (µW , λ, β)    (30)
                  1+J+JK+K×1               v (W ; φ) (X − e (W ; φ))

where θ = (φ, µW , λ′ , β)′ with dim (θ) = L + J + 1 + J + JK + K.
Equations (28), (29) and (30) constitute a just-identified system. The corresponding method-
of-moments estimate of β0 can be computed in the three simple steps listed in Algorithm
2.
In many cases of interest Algorithm 2 is easily implemented using standard soft-
ware. Standard errors may be constructed in the usual way for GMM estimators (e.g.,
Newey & McFadden, 1994; Wooldridge, 2010) or using a bootstrap.
In step 3, if instead we let Xi serve as its own instrument, we get an “Oaxaca-Blinder” type

                                                  19
estimator.
The next theorem summarizes the large sample properties of β̂. In the statement of
                                                          ˆ If Assumption 6 addi-
the Theorem, ∆∗ denotes the limiting pseudo-true value of ∆.
tionally holds then ∆∗ = ∆0 . We also define ϵ̃ = v (W )−1
                                                        0 (X − e0 (W )) ϵ where ϵ =
{a0 (W ) + X ′ (b0 (W ) − β0 ) − R′ λ∗ } (with λ∗ denoting a pseudo-true parameter value). Fi-
nally we let Πϵ̃S = E [ϵ̃i S′ ] E [SS′ ]−1 denote the coeﬃcient matrix associated with the multi-
variate regression of ϵ̃ onto the score vector associated with φ0 (the parameter indexing the
generalized propensity score).

Theorem 2. (Large Sample Distribution) Consider the semiparametric problem es-
tablished by Definition 1 and Assumptions 1, 2, and 5. Let β̂ be the method of mo-
ments estimate of β0 based upon restrictions (28) to (30). Under regularity conditions (cf.,
Newey & McFadden, 1994, Theorem 3.4) β̂ is (i) asymptotically normal with a limiting dis-
tribution of
    √ 5       6
                D
     N β̂ − β0 → N 0, E [Ω0 (W )] + ∆∗ V (W ) ∆′∗ + E (ϵ̃ − Πϵ̃S S) (ϵ̃ − Πϵ̃S S)′ ,
                  -                                  $                            &.
                                                                                            (31)

and (ii) locally eﬃcient for β0 at Assumption 6 with
                               √    5       6
                                              D
                                   N β̂ − β0 → N 0, I(β0 )−1 .
                                                -           .
                                                                                            (32)

Proof. See Appendix A.

Part (ii) of Theorem (2) follows easily from part (i). In the proof we show that ϵ equals
the prediction error associated with the mean squared error minimizing linear prediction of
a0 (W )+X ′ (b0 (W ) − β0 ) given R (µW ). When Assumption 6 additional holds this prediction
error will be identically equal to zero and the third term in the variance expressing appearing
in part (i) drops out. Similarly when Assumption 6 holds we have ∆∗ V (W ) ∆′∗ = V (b0 (W )).
Together these two observations give part (ii).
Our eﬃciency bound calculation, Theorem 1, gives the information bound for β0 without
imposing the additional auxiliary Assumption 6. This assumption imposes restrictions on
the joint distribution of the data not implied by the baseline model. If these restrictions are
added to the prior used to calculate the eﬃciency bound, then it will generally be possible
to estimate β0 more precisely. Our estimator is not eﬃcient with respect to this augmented
model. Rather it attains the bound provided by Theorem 1 if Assumption 6 “happens to
be true” in the sampled population, but is not part of the prior restriction used to calculate
the bound. Newey (1990, p. 114) discusses the concept of local eﬃciency in detail. In what
follows we will, for brevity, say β̂ is locally eﬃcient at Assumption 6.

                                               20
Even if Assumption 6 does not hold precisely, our procedure will be “nearly” eﬃcient when
it is approximately true (in which case variability in ϵ about zero is small). A caveat to
this claim is that the third variance term in (31) may still be large in this case if v0 (w)
is nearly zero for enough values of w. This occurs when overlap is poor, or there exists a
lack of variation in the policy variable for some subpopulations defined in terms of W = w.
Graham et al. (2016) develop this observation more extensively for the special case where X
is binary, but similar issues apply in the more general setting considered here.
Our next result formalizes the above observation. It extends our local eﬃciency result to
“near” global eﬃciency. The basic argument mirrors that given by Chamberlain (1987, Propo-
sition 2) for approximately eﬃcient estimation of conditional moment problems. Presenting
this result requires defining a sequence of estimators based upon Algorithm 2.
Let L2 be the space of functions f : W → R with finite second moment E f (W )2 < ∞.
                                                                              $      &

Under Assumptions 1 and 2 the set of feasible conditional linear predictor coeﬃcients lies
within this space such that a : W → R1 and b : W → RK with E a (W )2 < ∞ and
                                                                         $       &

E ∥b (W )∥2 < ∞. Let {kj (W )}∞
  $          &
                                   j=1 be a sequence of linearly independent functions of the
control variables, each with finite variance. Similar to Chamberlain (1987) we call this
sequence complete if, (i) for any ζ > 0 and (ii) any feasible conditional linear predictor
coeﬃcients a (W ) and b (W ) in L2 , there are the real numbers α, γ1 , . . . , γJ and δk1 , . . . , δkJ
for k = 1, . . . , K such that              0<          <2 1
                                           E <δ (J) (W )< < ζ 2 ,                                  (33)

with δ (J) (W ) defined as

                                          a (W ) − α − Jj=1 (kj (W ) − µj ) γj
                                  ⎛                    !                           ⎞

                                        b1 (W ) − β01 − Jj=1 (kj (W ) − µj ) δ1j
                                   ⎜                   !                           ⎟
                      (J)
                                   ⎜                                               ⎟
                  δ         (W ) = ⎜                      ..                       ⎟.              (34)
                                                           .
                                   ⎜                                               ⎟
                                   ⎝                                               ⎠
                                                       !J
                                       bK (W ) − β0K − j=1 (kj (W ) − µj ) δKj

Let k (J) (W ) be the J ×1 vector of functions of W with j th element kj (W ). We can construct
a sequence of estimators, indexed by J, based upon Algorithm 2 with k (J) (W ) replacing W .
                       $          &
To do this let µ(J) = E k (J) (W ) and additionally define
                         5 -                  .′ --                    .′ 6′
                   R(J) = 1, k (J) (W ) − µ(J) , k (J) (W ) − µ(J) ⊗ X
                                                                  .
                                                                             .

We can then estimate β0 by Algorithm 2 with k (J) (W ), µ(J) and R(J) respectively replacing
W , µW , and R (µW ).
Consider the asymptotic precision matrix of this method of moments estimator; from The-

                                                     21
orem 2 we get
                                                                         .′
                     I (J) (β0 )−1 =E [Ω0 (W )] + ∆(J)
                                                       - (J)
                                                             (W ) ∆(J)
                                                                 .-
                                                   ∗ V k              ∗
                                         +5            65             6′ ,
                                            (J)    (J)    (J)     (J)
                                    + E ϵ̃ − Πϵ̃S S ϵ̃ − Πϵ̃S S            .


with I (J) (β0 )−1 ≥ I (β0 )−1 (here “A ≥ B” denotes “A − B is positive semi-definite”). Recall
that I (β0 ) is the semiparametric eﬃciency bound given in Theorem 1. Let β̂ (J) denote the
estimate of β0 based upon k (J) (W ).

Proposition
       :       3. (Near Efficiency) If, maintaining the Assumptions of part (i) of The-
               ;
orem 2, β̂ (J)
                 is based upon a linearly independent, complete sequence {kj (W )}∞
                                                                                  j=1 , then,
for X × W a compact subset of RK+dim(W ) ,

                                  lim I (J) (β0 )−1 = I (β0 )−1 .
                                  J→∞


Proof. See Appendix A.

The compact support assumption invoked in the statement of the theorem is used in the
proof, but does not appear to be essential.
Proposition 3 leaves unanswered important practical questions, such as how quickly J should
increase with N. More generally the question of exactly how to choose the elements of
k (J) (W ) in order to achieve good precision in practice remains unanswered. However we
expect that many insights from related settings could be applied here (e.g., Belloni et al.,
2014).
We conclude this section by demonstrating double robustness in the sense of Scharfstein et al.
(1999). If the specification of the generalized propensity score is not correct (i.e., Assumption
5 does not hold), but Assumption 6 is true, then our estimator remains consistent for β0 .
Recall that Assumption 6 was initially invoked to ensure local eﬃciency of our procedure. It
turns out that modeling the form of the conditional linear predictor coeﬃcients has the added
benefit of ensuring that our estimator remains consistent even if our generalized propensity
score model is incorrect. Double robustness results are familiar from the literature on missing
data and program evaluation (e.g., Scharfstein et al., 1999; Cattaneo, 2010; Graham, 2011).
In these settings X is binary or a vector of mutually-exclusive treatment indicators. Double
robustness in our more general setting is perhaps unsurprising, but nevertheless a new result.
To understand this result observe that step 3 of Algorithm 2 corresponds to solving the



                                               22
sample analog of
                       ')                                  *                 (
                                       R (µW )
                   E                  −1                       U (µW , λ0 , β0 ) = 0
                            v (W ; φ∗) (X − e (W ; φ∗ ))

for λ0 and β0 . Here we use the notation φ∗ to denote that our generalized propensity score
model may be miss-specified.
If Assumption 6 holds in the population, then U0 = Ui (µW , λ0 , β0 ) is a conditional
linear predictor (CLP) error and Lemma 1 above applies. Recall that R (µW ) =
                                     .′
  1, (W − µW )′ , ((Wi − µW ) ! Xi )′ ; by Lemma 1 U0 is uncorrelated with all components
-

of this vector. Likewise, because U0 is mean independent of W and conditionally uncor-
related with X, we also have that E v (W ; φ∗)−1 (X − e (W ; φ∗)) U is mean zero as well.
                                     $                             &

Hence step 3 of Algorithm 2 involves the computation of a correctly specified method-of-
moments estimator under Assumption 6; irrespective of whether Assumption 5 additionally
holds. Double robustness follows, more or less, directly.
The above discussion also clarifies why, as is sometimes true in practice, sampling variabil-
ity in our estimator can theoretically be lower than the semiparametric variance bound in
Theorem 1 when the generalized propensity score is misspecified, but the form of the CLP
coeﬃcients are not. First, recall that the variance bound is computed without making any
a priori assumptions about the form of the CLP coeﬃcients. It turns out that in our setting
such assumptions generally increase the precision with which β0 may be estimated. When
we invoke the double robustness property of our procedure to ensure consistency we are in
a situation where the veracity of Assumption 6 is central. Whereas is the setting covered by
Theorem 2, Assumption 6 “may happen to be true”, but need not be.
It is instructive to compare our estimator with the “Oaxaca-Blinder-type” one described ear-
lier. The Oaxaca-Blinder procedure necessarily maintains Assumption 6. Since this restric-
tion is part of the prior, it would not be surprising to find that, under correct specification,
that the Oaxaca-Blinder estimator is more eﬃcient than ours. For the purposes of developing
this point, additionally assume that U0 is homoscedastic in X and W (but that this is not
part of the prior), then – maintaining Assumption 6 – replacing v (W ; φ∗ )−1 (X − e (W ; φ∗))
with X in the above moment would be natural. This replacement leads the researcher to
the Oaxaca-Blinder estimator (which will also be eﬃcient in this case). Hence, when As-
sumption 6 does hold in the sampled population, our procedure will be less eﬃcient that the
Oaxaca-Blinder one (at least under homoscedasticity of U0 ). Of course, when Assumption 6
does not characterize the sampled population, our procedure remains consistent, while the
Oaxaca-Blinder one does not.


                                                 23
                                                                              p
Theorem 3. ( Double Robustness) Under Assumptions 1 and 2 , β̂ → β0 if either
Assumption 5 or 6 holds.

The proof is straightforward and omitted (see Graham et al. (2012) and Graham et al. (2016)
for proofs of related results). As a practical matter using the standard method-of-moments
sandwich variance-covariance matrix estimator associated with the moment problem defined
by (28), (29) and (30) above will support asymptotically valid inference under the conditions
of both Theorems 2 and 3.



4    Examples and special cases
In this section we demonstrate that our semiparametric regression model encompasses several
other well-known models.


Example 1: Binary Treatment Eﬀect
Following the program evaluation literature let Y0 denote the potential outcome under control
and Y1 the potential outcome under active treatment treatment. For each sampled unit we
observe either Y0 or Y1 but not both. The observed outcome, Y , therefore equals

                                   Y = XY1 + (1 − X)Y0

where X equals 1 if the unit is treated and zero otherwise. Rewriting yields a random
coeﬃcients model of
                                       Y = A + BX

with A = Y0 and B = Y1 − Y0 . The average treatment eﬀect (ATE) equals

                                  β0 = E[Y1 − Y0 ] = E[B].

Rosenbaum & Rubin (1983) show that the ATE is identified when (Y0 , Y1 )⊥X|W (uncon-
foundedness) and 0 < Pr ( X = 1| W = w) < 1 for all w ∈ W (overlap).
When X is binary our Assumption 3 implies unconfoundedness. Assumption 3 implies that
X is conditionally uncorrelated with the two potential outcomes. When X is binary this
also corresponds to mean and full conditional independence. Next observe that e0 (W ) =
Pr ( X = 1| W = w) and v0 (W ) = e0 (W ) [1 − e0 (W )]. Hence our Assumption 2 implies that
0 < κ ≤ e0 (W ) ≤ 1 − κ < 1 or so called strong overlap.

                                             24
Now consider Algorithm 2. When X is binary we have that
                  5      6−1 5     5     66             X          1−X
                 v W, φ̂      X − e W, φ̂ =           5      6−      5      6.
                                                     e W, φ̂    1 − e W, φ̂

Our ATE estimate is the coeﬃcient on X associated with the linear instrumental variables
                                                                      X
fit of Y onto a constant, (W − µ̂W ), (W − µ̂W ) · X, and X where e W,    − 1−X serves
                                                                     ( φ̂) 1−e(W,φ̂)
as an instrument for X. This estimator is similar to, but distinct from, the weighted least
squares (WLS) one introduced by Hirano & Imbens (2001).
Wooldridge (2004) shows, for X binary, that equation (7) coincides with
                                                      +                       ,
                                                           XY      (1 − X) Y
                 E v0 (W )−1 (X − e0 (W )) Y = E
                  $                         &
                                                                 −              ,
                                                          e0 (W ) 1 − e0 (W )

which is the familiar inverse probability weighting (IPW) representation of the average treat-
ment eﬀect (ATE) in, for example, Hirano et al. (2003).
The general form of the eﬃcient influence function given in Theorem 1 above corresponds to
the specialized one for the ATE when X is binary derived by, for example, Hahn (1998) and
Hirano et al. (2003). Hence our general procedure, as summarized by Algorithm 2, provides
a locally eﬃcient and double robust estimator of the ATE. To the best of our knowledge, our
proposed estimator is a new one, even in the special case where it identifies the ATE of a
binary treatment. Bang & Robins (2005) and Tsiatis (2006) provide introductions to double
robust causal inference.


Example 2: Multiple Treatment Eﬀects
Following Imbens (2000), Wooldridge (2004), and Cattaneo (2010) consider finite collection of
mutually exclusive treatments indexed by k ∈ {0, 1, 2, ..., K} with K ∈ N. Associated with
these treatments are the K + 1 potential outcomes, Y (0), Y (1), . . . , Y (K). The observed
outcome is
                                       4K
                           Y = Y (0) +    Xk {Y (k) − Y (0)}
                                          k=1

where Xk is a binary random variable that equals 1 if treatment k = 0, . . . , K is assigned
to the unit and zero otherwise. In this case, we work with the following random coeﬃcient
model:
                                                  ′
                                       Y =A+X B




                                                25
where X = (X1 , . . . , XK )′ is a K × 1 vector of treatment indicators and B a corresponding
vector of individual treatment eﬀects.
In this setup X is multinomial with a conditional mean of
                                               ⎛                 ⎞
                                                Pr ( X1 = 1| W )
                                              ⎜         ..       ⎟
                                    e0 (W ) = ⎜
                                              ⎝          .       ⎟
                                                                 ⎠
                                                Pr ( XK = 1| W )

and an inverse conditional variance of (cf., Henderson & Searle, 1981)
                                       C                                                 D
                            −1                 1                         1
                  v0 (W )        =diag                  ,...,
                                       Pr (X1 = 1| W )        Pr ( XK = 1| W )
                                                 1
                                  +                             ιK ι′K .
                                     1− K
                                       !
                                          k=1 Pr ( X K = 1| W )

A little bit of tedious algebra then gives
                                                        ⎡                                        ⎤
                                                                X1 Y                 X0 Y
                                                            Pr( X1 =1|W )
                                                                            −    Pr( X0 =1|W )
                                                                            ..
           β0 = E v0 (W )−1 (X − e0 (W )) Y = E ⎢
                 $                         &    ⎢                                                ⎥
                                                                             .                   ⎥,
                                                ⎣                                                ⎦
                                                                XK Y                 X0 Y
                                                            Pr( XK =1|W )
                                                                            −    Pr( X0 =1|W )


which corresponds to the IPW representation of the ATEs
                                           ⎛                  ⎞
                                            E [Y (1) − Y (0)]
                                          ⎜         ..        ⎟
                                     β0 = ⎜
                                          ⎝          .        ⎟,
                                                              ⎠
                                            E [Y (K) − Y (0)]

in the multiple treatment setting.
As in the case where X is binary, the general form of the eﬃcient influence function given
in Theorem 1 above corresponds to the specialized one derived by Cattaneo (2010). Hence
our general procedure also provides a locally eﬃcient and doubly robust estimate of ATEs
in the multiple, mutually exclusive, treatments setting.


Example 3: Partially linear model
Chamberlain (1986, 1992) and Robinson (1988) studied the semiparametric partially linear
regression model (PLM)
                                        Y = X ′ β0 + h0 (W ) + U


                                                   26
with E[U|W, X] = 0. This model can be represented by the random coeﬃcient model

                                        Y = A + X ′B

where E[A|W, X] = a0 (W ) = h0 (W ) and E[B|X, W ] = b0 (W ) = β0 . These assumptions are
stronger than those implied by Assumption 3.
To fit this into our framework we replace Assumption 6 with a working CLP model of

                   a0 (W ) = h0 (W ) = α0 + (W − µW )′ λ0 , b0 (W ) = β0 .

This implies a constant additive treatment eﬀect structure.
Estimation follows Algorithm 2. First compute the MLE of φ0 . Second compute the sample
means µ̂W = N1 N
                !
                   i=1 Wi . Finally compute the linear instrumental variables fit of Y onto
                                       5     6−1 5       5     66
a constant, (W − µ̂W ) and X, using v W, φ̂        X − e W, φ̂    as an instrument for X.
Because of the constant additive treatment eﬀect structure of the PLM we exclude the
interactions (W − µ̂W ) ⊗ X from the IV fit computed in the third step.
It is important to recognize that although our procedure invokes the working assumption
that the treatment eﬀect is constant in W (i.e., b0 (w) = β0 for all w ∈ W), this assumption
is not required for consistency as long as our generalized propensity score model is correct.
Put diﬀerently although our procedure incorporates the PLM structure, this structure is not
part of the maintained prior (albeit the form of the generalized propensity score is part of
the prior).
If b0 (w) = β0 for all w ∈ W and U is conditionally mean zero given both W and X (and also
has a constant variance), but these are not part of the prior restriction used to calculate the
bound, then (16) evaluates to

                                 I(β0 )−1 = σ 2 E v0 (W )−1 .
                                                 $         &


The modified PLM estimator described above, and based on our Algorithm 2, will attain
this bound when the true model is a partially linear one.
Chamberlain (1992) gives a bound for β0 – where the partially linear regression structure is
part of the prior restriction (but the homoscedasticity assumption is not) – of

                                Iplm (β0 )−1 = σ 2 E [v0 (W )]−1 .

The diﬀerence I(β0 )−1 − Iplm (β0 )−1 is positive semi-definite. This follows directly from,
for example, the Theorem in Groves & Rothenberg (1969) on the expectations of inverse

                                               27
                               Table 1: Monte Carlo Designs
                              Designs      1    2     3    4
                                α0         1    1    1.5 1.5
                                γ1         1    1     1    1
                                γ2         0    0    0.5 0.5
                                β0         2    2    2.5 2.5
                                δ1       1.22 1.26 1 1.05
                                δ2         0    0    0.5 0.5
                                φ0        0.1 0.1 0.1 0.1
                                φ1        0.5 0.5 0.5 0.5
                                φ2         0   0.1    0   0.1
Notes: We specified a0 (w) = α0 + γ1 (W − E [W ]) + γ2 (W 2 − E [W 2 ]) and b0 (w) = β0 +
δ1 (W − E [W ]) + δ2 (W 2 − E [W 2 ]) analogous to K
                                                   the formulation given in Assumption 6.
Each of the four designs are calibrated such that    I (β0 )−1 /N = 0.05 when N = 1, 000.


matrices. Hence although our approach to estimation remains consistent for β0 when the
true regression function takes a partially linear form, it will generally be less eﬃcient than
methods which exploit this structure at the outset (e.g., Robinson, 1988; Robins et al., 1992).



5    Finite sample properties
In order to assess the approximation accuracy of Theorems 2 and 3 in finite samples we
conducted a small simulation experiment, the results of which we report here. We considered
four designs. The outcome was generated according to

                                Y = a0 (W ) + b0 (W ) X + U

with W and U independent standard normal random variables and a0 (W ) and b0 (W ) either
linear (designs 1 and 2) or quadratic (designs 3 and 4) in W . The conditional distribution of
X given W was specified as Poisson with parameter exp k (W )′ φ and k (W ) = (1, W )′ in
                                                           -        .
                                          ′
designs 1 and 3 and k (W ) = (1, W, W 2 ) in designs 2 and 4. Complete details on the data
generating process are given in Table 1.
We evaluate the performance of three estimators. First we consider a simple “Oaxaca-
Blinder” type estimator. Specifically we estimate β0 by the coeﬃcient on X in the least
squares fit of Y onto a constant, W − µ̂W , (W − µ̂W ) × X and X. As in Kline (2014)
we appropriately account for the eﬀect of estimating µW when constructing standard errors
and confidence intervals. This estimator is consistent for the true average partial eﬀect in


                                              28
both designs 1 and 2. It is also, since U is Gaussian and homoscedastic, eﬃcient in these
two designs. Eﬃciency is in the semiparametric model which, in addition to Assumptions
1 and 2, maintains Assumption 6. The variance of the Oaxaca-Blinder estimate therefore
lies (weakly) below the bound given by Theorem 1 in these two designs. In designs 3 and 4,
where a0 (W ) and b0 (W ) are quadratic in W , the “Oaxaca-Blinder” estimator is inconsistent.
The second estimator is the generalized inverse probability weighting (GIPW) one due to
Wooldridge (2004, 2010). Our implementation tracks our analysis in Section 3. For estima-
tion we correctly assume that the conditional distribution of X given W is Poisson, but set
the parameter to exp k (W )′ φ with k (W ) = (1, W )′ . This is correct in designs 1 and 3,
                      -        .

but not 2 and 4. Hence the GIPW estimate of β0 is consistent in designs 1 and 3, but not
2 and 4. The GIPW is never eﬃcient. Standard errors are constructed using the sample
analog of the influence function given in (23) above.
Finally we consider the properties of our locally eﬃcient, doubly robust estimator. Imple-
menting this procedure requires assumptions on both the CLP and the generalized propensity
score. We make the same assumptions used to implement the Oaxaca-Blinder and GIPW
procedures. Consequently this last estimator is eﬃcient – in the sense of Theorem 1 – in de-
sign 1 and consistent in designs 1, 2 and 3. Like all the estimators it is inconsistent in design
4. We construct standard errors using the (sample analog) of the influence function given
in Theorem 2; consequently our intervals are conservative in design 2 (where our propensity
score model is misspecified).5
                                                  K
Each of the four designs are calibrated such that I (β0 )−1 /N = 0.05 (0.025) when N =
1, 000 (4, 000). In an asymptotic sense inference on β0 is equally hard across all the designs
considered. We focus on the N = 1, 000 experiments in our discussion (the quality of the
asymptotic approximations predictably improve in the larger sample).
Results from the four designs are reported in Table 2. As expected our DR estimator is
median unbiased across Designs 1, 2 and 3. In contrast the Oaxaca-Blinder estimator only
performs acceptably in designs 1 and 2, and the GIPW estimator in design 1 and 3. In
designs 1 and 2 the variability of the DR estimator is nearly as small as that of the Oaxaca-
Blinder one. Neither the DR, nor the GIPW, estimators are expected to be eﬃcient in design
3 but, interestingly, GIPW is more eﬃcient than DR in this case. In design 1, where the DR
estimator is locally eﬃcient, its standard deviation is substantially smaller than that of the
GIPW estimator (as expected).



  5
   We use Python 3.6 to conduct our experiments. Replication code is available in the supplemental
materials.

                                               29
                                                    Table 2: Simulation Results
                                            Panel A, N = 1, 000                          Panel B, N = 4, 000
                                   Bias    Std. Dev. Std. Err. Coverage     Bias        Std. Dev. Std. Err. Coverage
                    Design 1
                  Oaxaca-Blinder -0.0003     0.0500       0.0496     0.9480     0.0006    0.0252      0.0248     0.9438
                       GIPW       -0.0008    0.0853       0.0809     0.9438     0.0016    0.0429      0.0419     0.9494
                        DR         0.0001    0.0507       0.0499     0.9450     0.0009    0.0254      0.0250     0.9448
                    Design 2
                  Oaxaca-Blinder 0.0009      0.0504       0.0497     0.9442     0.0000    0.0251      0.0248     0.9456
                       GIPW       -0.2597    0.1331       0.1198     0.4634     -0.2613   0.0710      0.0666     0.0258
                        DR         0.0014    0.0518       0.0561     0.9620     -0.0001   0.0258      0.0283     0.9694
                    Design 3
                  Oaxaca-Blinder -0.3268     0.0993       0.0899     0.0772     -0.3319   0.0531      0.0481     0.0002




30
                       GIPW       -0.0018    0.0830       0.0794     0.9436     -0.0005   0.0428      0.0413     0.9452
                        DR        -0.0113    0.1099       0.0981     0.9276     -0.0036   0.0570      0.0529     0.9368
                    Design 4
                  Oaxaca-Blinder -0.2010     0.1571       0.1087     0.5148     -0.2391   0.1255      0.0674     0.0168
                       GIPW        0.2717    0.1897       0.1216     0.4006     0.3052    0.1335      0.0748     0.0034
                        DR         0.4123    0.2035       0.1687     0.3076     0.4362    0.1058      0.1013     0.0986
                     I (β0 )−1 /N                    0.05                                        0.0250
     Notes: The bias column reports median bias across all B = 5, 000 simulations. The Std. Dev. column reports the standard
                   K


     deviation of the point estimates across these simulations, Std. Err. the median estimated standard error, and Coverage the
     actual coverage of a nominal 95 percent confidence interval (constructed using the estimated point estimate       and standard
     error in the normal way). The standard error associated with a Monte Carlo coverage estimate is α (1 − α) /B. With
     B = 5, 000 simulations and α = 0.05, this results in a standard error of approximately 0.003 or a 95 percent confidence interval
                                                                                                              L


     of [0.944, 0.956].
Overall the simulation results track our theoretical predictions remarkably closely. Of course
exploring the performance of these estimators in the context of real world empirical appli-
cations and other, more realistic, simulation designs would be of interest.



6    Conclusion
We have introduced a locally eﬃcient, doubly robust, semiparametric method of estimating
averages of conditional linear predictor coeﬃcients. Our estimand, and semiparametric eﬃ-
ciency bound, specialize to familiar counterparts found in the program evaluation literature
(e.g. Hahn, 1998; Cattaneo, 2010). While encompassing well-known program evaluation set-
tings, our framework allows for (semiparametric) covariate adjustment in many other settings
as well (including ones with few extant alternative methods of such adjustment).
Researchers interested in estimating the average treatment eﬀect (ATE) associated with a
binary treatment can apply our methods. While we believe the precise form of our procedure
is new even to this familiar setting, it is a variant of the class of augmented inverse probability
weighting (AIPW) estimators introduced by Robins et al. (1994) in the missing data context
over 20 years ago. The real attraction of Algorithm 2, and the corresponding Theorems 2
and 3 (as well as Proposition 3), is that they apply to models beyond the “classic” program
evaluation one of Rosenbaum & Rubin (1983). Multiple, mutually exclusive treatments,
as in Imbens (2000) and Cattaneo (2010) are easily handled as a special case. Similarly,
maintaining a linear, but heterogeneous, potential response function structure, Algorithm
2 recovers average partial eﬀects (APE) for continuous treatments, multiple non-exclusive
treatments, mixtures of binary, discrete and continuous treatments and so on. A weighted
average derivative interpretation of our estimand is also available for settings where the linear
potential response function structure may not hold (Proposition 2).
We also wish to emphasize that averages of conditional linear predictor coeﬃcients represent
a natural, but substantial, generalization of linear predictor coeﬃcients as estimated by
the method of least squares. Hence Algorithm 2 also provides a method of flexible covariate
adjustment that may be of independent interest even in settings where formal causal inference
is not warranted; similar to how least squares is sometimes used for descriptive purposes.
Our work leaves several questions unanswered. First, although the flexible parametric mod-
eling embodied in Assumptions 5 and 6 closely mirrors empirical practice, it would be useful
to development methods that leave the generalized propensity score and CLP coeﬃcients
non-parametric. It seems likely that results from the binary and multiple treatments case
could be extended to apply here (e.g., Hirano et al., 2003; Cattaneo, 2010; Belloni et al.,


                                                31
2014).
In other work we have shown that first order equivalent estimators may have appreciably
diﬀerent higher order properties in program evaluation settings (Graham et al., 2012). We
expect that other locally eﬃcient, doubly robust approaches to estimation for the class of
problems considered in this paper are feasible. These approaches may exhibit superior or
inferior higher order bias.
Third, maintaining the correlated random coeﬃcient structure, diﬀerent notions of condi-
tional exogeneity will imply diﬀerent semiparametric eﬃciency bounds (when linearity is
restrictive). Our decision to work with a weak notion of exogeneity maintains a connec-
tion with conditional linear predictors. If a researcher was comfortable with the correlated
random coeﬃcient structure, then it would generally be possible to construct more eﬃcient
estimates of β0 = E [B] if she was willing to assume, for example, that (A, B ′ )′ ⊥ X % W = w
                                                                                       %

for all w ∈ W. Such estimators would likely be quite complicated and may have poor finite
sample properties.




                                             32
A       Proofs
This appendix contains proofs of the results contained in the main paper. All notation is as
defined in the main text unless explicitly noted otherwise. Equation numbering continues in
sequence with that established in the main text.


Proof of Theorem 1 (Semiparametric eﬃciency bound)
In calculating the eﬃciency bound for β0 in the semiparametric regression model defined by
Definition 1 and Assumptions 1 and 2 of the main text, we follow the approach outlined
by Newey (1990, Section 3). First, we characterize the model’s tangent space. Second, we
demonstrate pathwise diﬀerentiability of β0 . The eﬃcient influence function for β0 equals
the projection of this derivative onto the tangent space. In the present case the pathwise
derivative lies in the tangent space and hence coincides with the required projection. The
result then follows from an application of Theorem 3.1 in Newey (1990).


Step 1: Characterization of the Model Tangent Space:

The joint density function for z = (w, x, y) is given by

                                  f0 (w, x, y) = f0 ( x, y| w) f0 (w) ,

where f0 ( x, y| w) denotes the conditional density/mass of (X = x, Y = y) given W = w and
f0 (w) is the marginal density/mass of W = w.
Consider a regular parametric submodel indexed by η with f (w, x, y; η) = f0 (w, x, y) at
η = η0 . The submodel joint density equals

                               f (w, x, y; η) = f ( x, y| w; η) f (w; η) ,

with a corresponding score vector of

                             sη (w, x, y; η) = sη ( x, y| w; η) + tη (w; η)                         (35)

where

 sη (w, x, y; η) = ∇η f (w, x, y; η) , sη ( x, y| w; η) = ∇η f ( x, y| w; η) , tη (w; η) = ∇η f (w; η) .




                                                   33
By the usual (conditional) mean zero property of scores we have that

                                  E [ sη ( X, Y | W )| W ] = E [tη (W )] = 0,                            (36)

where the suppression of η in a function indicates that it is evaluated at its population value
(e.g., tη (w) = tη (w; η0)).
The model tangent set is the closed linear span of the set of all such scores. From (35) and
(36) this set evidently equals
                                            T = {s (x, y| w) + t (w)}

where s (x, y| w) and t (w) satisfy the (conditional) moment restrictions

                                      E [ s ( X, Y | W )| W ] = E [t (W )] = 0,

and also have finite variance.


Step 2: Demonstration of pathwise diﬀerentiability:

Under the parametric submodel, β (η) is identified by
                                                   9
                                         β (η) =       b (w; η) f (w; η) dw,                             (37)

where b (w; η) satisfies the conditional moment restriction
                    9 9 )         *
                              1
                                      (y − a (w; η) − x′ b (w; η)) f ( x, y| w; η) dxdy = 0.             (38)
                             x

Diﬀerentiating (37) under the integral and evaluating at η = η0 gives
                                     +              ,
                         ∂β (η0 )      ∂b (W ; η0 )
                                  =E                  + E [b (W ; η0 ) tη (W ; η0 )] .                   (39)
                          ∂η ′             ∂η ′

                                                       ∂b(w;η0 )
We can derive a close-form expression for                ∂η′
                                                                   in (39) by diﬀerentiating (38) with respect
to η (and evaluating at η = η0 ):
    9 9 )       *                                        9 9 ) ′ *
            1       ∂a (w; η0 )                                     x       ∂b (w; η0 )
−                         ′
                                f ( x, y| w; η0 ) dxdy −                                f ( x, y| w; η0) dxdy
            x          ∂η                                          xx  ′        ∂η ′
                      9 9 ) *
                                 1
                    +                  (y − a (w; η) − x′ b (w; η0 )) sη ( x, y| w; η0) f ( x, y| w; η0) dxdy = 0
                                 x

                                                          34
Using the matrix inverse
           '             %      (−1 )                                                 *
                                                ′        −1                ′       −1
               1 X′      %            1 + e0 (w)  v0 (w)    e0 (w) − e0 (w)  v0 (w)
       E                 %W = w    =
                         %
                                               −1                         −1
               X XX ′    %              −v0 (w) e0 (w)              v0 (w)

we solve to get
 )               *       )                                                    *
     ∂a(w;η0 )
       ∂η′                   1 + e0 (w)′ v0 (w)−1 e0 (w) − e0 (w)′ v0 (w)−1
                     =
     ∂b(w;η0 )
       ∂η′
                            −v0 (w)−1 e0 (w)               v0 (w)−1
                            ')                                        *                     %      (
                                  Y − a (W ; η0 ) − X ′ b (W ; η0 )                         %
                         ×E                                             sη ( X, Y | W ; η0 )% W = w .
                                                                                            %
                                X (Y − a (W ; η0 ) − X ′ b (W ; η0 ))                       %

Evaluating the second row of this expression gives

 ∂b (w; η0 )     $         −1                                ′
                                                                                         %     &
             = E   v0 (W )    (X − e0 (W )) (Y − a0 (W ) − X   b0 (W )) s η ( X, Y | W ) %W = w ,
    ∂η ′
                                                                                               (40)
which, after substituting into (39), yields

       ∂β (η0 )     $        −1                                 ′
                                                                                            &
                = E  v0 (W )    (X − e 0 (W )) (Y − a0 (W ) − X   b0 (W )) s η ( X, Y | W )
        ∂η ′
                  +E [b0 (W ) tη (W )] .                                                          (41)

To demonstrate pathwise diﬀerentiability of β, we require F (W, X, Y ) such that

                                ∂β (η0 )
                                         = E F (W, X, Y ) sη (W, X, Y )′ .
                                            $                           &
                                    ′
                                                                                                  (42)
                                 ∂η

Setting F (W, X, Y ) equal to ψβeﬀ (Z, β0 , g0 (W ) , h0 (W )), as defined in (17) of the main text,
we get E F (W, X, Y ) sη (W, X, Y )′ equal to (41) since, by Lemma 4.1 of Wooldridge (1999),
         $                           &


                         E [ (X − e0 (W )) (Y − a0 (W ) − X ′ b0 (W ))| W ] = 0

and iterated expectations (and the conditional mean zero property of the score sη ( X, Y | W ))
further implies that E [(b0 (W ) − β0 ) sη (X, Y | W )] = 0.




                                                   35
Step 3: Verification that the conjectured influence function equals the required
projection:

Observe that ψβeﬀ (Z, β0 , g (W ) , h (W )) lies in the model tangent space. Its first term
is conditionally mean zero given W and hence plays the role of s ( X, Y | W ) . Its sec-
ond term is a mean zero function of W alone and hence plays the role of t (W ). Since
ψβeﬀ (Z, β0 , g0 (W ) , h0 (W )) ∈ T , its projection onto T equals itself. Since equation (9) of
Newey (1990, p. 106) is satisfied the result follows from his Theorem 3.1.


Proof of Theorem 2 (Large sample properties of β̂ )
Recall that λ = (α, γ ′ , δ ′ )′ and
                                                                         .′
                                       = 1, (W − µW )′ , ((W − µW ) ⊗ X)′ .
                                        -
                              R
                         (1+J+JK)×1


In what follows we let λ∗ denote value of λ which solves the just-identified population mo-
ments (28), (29) and (30). If Assumption 6 additionally holds in the sampled population,
then we use λ0 to denote the population value of λ. In this case λ0 correctly specifies the
form of the CLP of Y given X conditional on W .
In the Supplemental Web Appendix we show, without maintaining Assumption 6, that

                                        −1
                         λ∗ =E [RR′ ]         E [R (Y − X ′ β0 )]                           (43)
                                         −1
                             =E [RR′ ]        E [R (a0 (W ) + X ′ (b0 (W ) − β0 ))] .

Equation (43) implies that R′ λ∗ is the mean squared error minimizing linear predictor of
a0 (W ) + X ′ (b0 (W ) − β0 ) given R. This interpretation of λ∗ is all that is required for the
first part of Theorem 2.
We will also use the notation

                                         U0 = (Y − R′ λ0 − X ′ β0 )

and
                                       U∗ = (Y − R′ λ∗ − X ′ β0 ) .

Note that under Assumption 6 U0 equals a conditional linear prediction error. However when
Assumption 6 does not hold an implication of (43) is that U∗ is still an unconditional linear
predictor error.
We also use the shorthand e0 (W ) = e (W ; φ0 ) and v0 (W ) = v (W ; φ0 ) in order to simplify

                                                       36
some of the expressions presented below. Finally we use θ0 to denote both (φ′0 , µ′W , λ′∗ , β0′ )′
and (φ′0 , µ′W , λ′0 , β0′ )′ , with the relevant case made clear by the context.
Next define the (1 + J + JK) × J and K × J matrices
                                                               ⎡                                    ⎤
                                   + :                    ,                             0
                                                   ′
                                                       ;′
                     B1         = E R γ∗ + (IJ ⊗ X) δ∗      − E⎣                        0                        (44)
                                                               ⎢                                    ⎥
                                                                                                    ⎦
                  (1+J+JK)×J
                                                                                  (IJ ⊗ X) U∗
                                     +                                                           ;′ ,
                                      -             −1                   .:                 ′
                          B2    =E        v0 (W )        (X − e0 (W ))        γ∗ + (IJ ⊗ X) δ∗          .        (45)
                          K×J


Using     this       notation        we        can         write    the         (L + J + 1 + J + JK + K)           ×
(L + J + 1 + J + JK + K) Jacobian matrix of the moment vector as
                      ⎛                                                                                     ⎞
                                           −H (φ0 )                       0         0           0
               ⎜                                                                                            ⎟
               ⎜                         0                   IJ    0        0                               ⎟
          M = −⎜                                                                                            ⎟,
               ⎜
               ⎝                         0                  −B1 E [RR ] E [RX ′ ]
                                                                     ′                                      ⎟
                                                                                                            ⎠
                           E v0 (W )−1 (X − e0 (W )) U∗ S′ −B2
                            $                             &
                                                                   0       IK

with H (φ0 ) equal to the L × L expected Hessian matrix associated with the generalized
propensity score log-likelihood. The inverse Jacobian is therefore
                       ⎛
                                                              −H (φ0 )−1
                       ⎜
             −1
                       ⎜                                0
         M        = −⎜ ⎜ −E [RR′ ]−1 E [RX ′ ] E $v (W )−1 (X − e (W )) U S′ & H (φ )−1
                       ⎝                           0             0         ∗       0
                                     $         −1                 ′
                                                                    &       −1
                                   E v0 (W ) (X − e0 (W )) U∗ S H (φ0 )
                                                                                  ⎞
                               0                     0                  0
                                                                                  ⎟
                              IJ                     0                  0         ⎟
                         ′ −1                ′         ′ −1           ′ −1
                                                                                  ⎟.
                                                                                ′ ⎟
                                                                                                                 (46)
                   E [RR ] (B1 − B2 E [RX ]) E [RR ]         −E [RR ] E [RX ] ⎠
                              B2                     0                 IK




                                                            37
Under Assumption 5 a key observation is that the expected value of (45) equals
  +                                      ;′ ,
         −1            .:            ′
                                              = E v0 (W )−1 (X − e0 (W )) γ∗′
  -                                              $-                      . &
E v0 (W ) (X − e0 (W )) γ∗ + (IJ ⊗ X) δ∗

                                                           v0 (W )−1 (X − e0 (W )) δ∗′ (IJ ⊗ X)
                                                           $-                     .             &
                                                      +E
                                                    =0 + E v0 (W )−1 (X − e0 (W ))
                                                          $-                        .
                                                        5                     61
                                                      × X ′ δ1∗ · · · X ′ δJ∗
                                                     5                6
                                                    = δ1∗ · · · δJ∗ = ∆∗ .

Using this last equality, as well as the fact that under Assumption 5 we have H (φ0 ) =
−E [SS′ ], implies that the last K rows of −M −1 √1N N
                                                    !
                                                     i=1 m (Zi , θ0 ) + op (1) equal, after some
manipulation,

                                   N
                √ 5       6   1 4"
                 N β̂ − β0 = √       v0 (Wi )−1 (Xi − e0 (Wi )) U∗i
                               N i=1
                                                                       −1
                             − E v0 (W )−1 (X − e0 (W )) U∗ S′ E [SS′ ] Si
                                 $                            &

                                 +∆∗ (Wi − µW )} + op (1) .                                 (47)

Next observe that we may decompose U∗ as

                          U∗ =Y − R′ λ∗ − X ′ β0
                             =Y − a0 (W ) − X ′ b0 (W )
                               + {a0 (W ) + X ′ (b0 (W ) − β0 ) − R′ λ∗ }
                             =U0 + ϵ.

Since E [U∗ W ] = 0 by the properties of linear predictors, E [U0 | W ] = 0 by the properties
of conditional linear predictors, and U∗ = U0 + ϵ, we have that E [ϵW ] = 0. Defining
ϵ̃ = v0 (W )−1 (X − e0 (W )) ϵ we can re-write 47 as

                                        N
                   √    5       6   1 4"
                       N β̂ − β0 = √       v0 (Wi )−1 (Xi − e0 (Wi )) U0i
                                     N i=1
                                   + (ϵ̃i − Πϵ̃S Si ) + ∆∗ (Wi − µW )} + op (1)             (48)

where Πϵ̃S = E [ϵ̃i S′ ] E [SS′ ]−1 . This gives the first implication of the Theorem. The second
implication follows from the fact that ϵ = 0 and ∆∗ V (W ) ∆′∗ = V (b0 (W )) under Assumption
6.

                                               38
Proof of Proposition 3 (Near global semiparametric eﬃciency)
Let A be an m × n matrix with ∥A∥F = Tr (A′ A)1/2 denoting the Frobenius matrix norm,
∥A∥2 the spectral norm and recall that ∥A∥2 ≤ ∥A∥F . Let a be an n × 1 vector with
Euclidean norm ∥a∥ = (a′ a)1/2 . We make use of several matrix and probability inequalities
in what follows. These are drawn from Hansen (2018, Appendices A & B) unless stated
otherwise.
Let t be a non-zero column vector. The diﬀerence in the asymptotic variance of the estimate
of the linear combination t′ β0 based upon R(J) and a corresponding semiparametrically
eﬃcient estimate is
                                                                         .′
           t′ I (J) (β0 )−1 t − t′ I (β0 )−1 t =t′ ∆(J)
                                                         - (J)
                                                               (W ) ∆(J)    t − t′ V (b0 (W )) t
                                                                   .-
                                                    ∗ V k              ∗
                                                        +5            65               6′ ,
                                                     ′     (J)    (J)    (J)       (J)
                                                 + t E ϵ̃ − Πϵ̃S S ϵ̃ − Πϵ̃S S              t

                                           ≥0.                                                      (49)

We seek to show that this variance diﬀerence is also bounded above by something that can
be made arbitrarily close to zero.
To start observe that, after some manipulation (see the Supplemental Web Appendix) we
can show that
                                                             .′
  V b0 (W ) + ∆(J)
                   - (J)
                    k (W ) − µ(J) − β0 =∆(J)
                                             - (J)
                                                   (W ) ∆(J)
   -                             .    .                .-
               ∗                         ∗ V k            ∗     − V (b0 (W ))
                                                      − 2E [(b0 (W ) − β0 )
                                                                                                  #′ 1
                                                      × b0 (W ) + ∆(J)
                                                        "               - (J)          (J)
                                                                                           .
                                                                      ∗   k   (W ) − µ       − β0

                                                                                                    (50)

Using (50) we can rewrite t′ I (J) (β0 )−1 t − t′ I (β0 )−1 t as

               t′ V b0 (W ) + ∆(J)
                   -                - (J)          (J)
                                                       .         .
                                ∗     k   (W ) − µ       −   β 0   t
                        0                                                               #′ 1
                + 2t′ E (b0 (W ) − β0 ) b0 (W ) + ∆(J)         (J)           (J)
                                        "                  -                     .
                                                       ∗     k      (W ) − µ       − β0      t
                       +5              65               6′ ,
                                  (J)            (J)
                + t′ E ϵ̃(J) − Πϵ̃S S ϵ̃(J) − Πϵ̃S S         t.                                     (51)


Consider the first term in (51). The Quadratic Inequality (QI), Expectation Inequality (EI),
and completeness of the sequence {kj (W )}∞j=1 (see equation (33)) give




                                                     39
                      t′ V b0 (W ) + ∆(J)
                                          - (J)
                                           k (W ) − µ(J) − β0 t ≤ C1 ζ 2 ,
                          -                             .    .
                                      ∗                                                       (52)

with C1 a constant.
Next consider the second term in (51). Applying the Cauchy-Schwarz inequality to this term
yields
% 0                                                           #′ 1 %%
%′                                                                                   1/2
%t E (b0 (W ) − β0 ) b0 (W ) + ∆(J)
                                    - (J)          (J)
                                                                  t% ≤V (t′ b0 (W ))
                    "                                  .
                                ∗    k    (W ) − µ       − β0

                                                                        × V (t′ {b0 (W )
                                                                                                    #.1/2
                                                                        +∆(J)
                                                                                - (J)
                                                                                 k (W ) − µ(J) − β0
                                                                                              .
                                                                           ∗


Again invoking completeness of the sequence {kj (W )}∞
                                                     j=1 , and also boundedness of the
variance of b0 (W ), we then get
            % 0                                                    #′ 1 %%
            %′                  "           (J)
                                                - (J)    (J)
                                                             .
            %t E (b0 (W ) − β0 ) b0 (W ) + ∆∗ k (W ) − µ       − β0 t% ≤ C2 ζ,                (53)

with C2 a constant (which depends on V (b0 (W ))).
Finally consider the third term in (49). To analyze this term we start by writing the linear
                                       .′ (J)
predictor approximation error of R(J) λ∗ for a0 (W ) + X ′ (b0 (W ) − β0 ) as
                                  -

                             :                                      .′   ;
                       ϵ(J) = a0 (W ) + X ′ (b0 (W ) − β0 ) − R(J) λ(J)
                                                               -
                                                                       ∗
                                                                  ′
                            =a (W ) − α∗(J) − k (J) (W ) − µ(J) γ∗(J)
                                             -                  .

                             + X ′ b0 (W ) − β0 − ∆(J)
                                                         - (J)
                                                          k (W ) − µ(J)
                                   -                                     ..
                                                      ∗

                           = (1, X ′) δ (J) (W ) ,

with the final equality following from definition (34). The EI and the fact that, for a and b
m × 1 vectors ∥ab′ ∥F = ∥a∥ ∥b∥, then gives
                 < +5             65             6′ ,< +<           <2 ,
                 <E ϵ̃(J) − Π(J) S ϵ̃(J) − Π(J) S < ≤ E <  (J) (J) <
                 <                                   <
                 <           ϵ̃S            ϵ̃S      <  <ϵ̃ − Πϵ̃S S<

with
                        ϵ̃(J) = v0 (W )−1 (X − e0 (W )) (1, X ′ ) δ (J) (W ) .
                                                       "                    #

By the norm-reducing property of projection and Schwarz Matrix Inequality (SMI) we further




                                                     40
get that
               <           < < <
               < (J)  (J) <
               <ϵ̃ − Πϵ̃S S< ≤ <ϵ̃(J) <
                             = <v0 (W )−1 (X − e0 (W )) (1, X ′) δ (J) (W ) <
                               <                        "                   #<

                             ≤ <v0 (W )−1 (X − e0 (W )) {(1, X ′)}< <δ (J) (W )< .
                               <                                  <<           <


Applying the expectation operator, invoking Assumption 2, and using the compact support
assumption for (W, X), finally gives
            +<           <2 ,   0<
                    (J) <                                        <2 <         <2 1
           E <ϵ̃ − Πϵ̃S S< ≤E <v0 (W )−1 (X − e0 (W )) {(1, X ′)}< <δ (J) (W )<
             < (J)
                                   0<          <2 1
                              ≤C3 E <δ (J) (W )<

                                  ≤C3 ζ 2                                                (54)

                                                     <2
              sup <v0 (W )−1 (X − e0 (W )) {(1, X ′)}< .
                  <
with C3 =
            w,x∈W,X
Applying the TI to (51) and using terms (52), (53) and (54) then gives the bound

                      0 ≤ t′ I (J) (β0 )−1 t − t′ I (β0 )−1 t ≤ (C1 + C3 ) ζ 2 + C2 ζ.   (55)

Since ζ is arbitrary the limit of the diﬀerence in (55) is zero.


References
Angrist, J. D. (1998). Estimating the labor market impact of voluntary military service using
 social security data on military applicants. Econometrica, 66(2), 249 – 288.

Angrist, J. D., Graddy, K., & Imbens, G. W. (2000). The interpretation of instrumental
 variables estimators in simultaneous equations models with an application to the demand
  for fish. Review of Economic Studies, 67(3), 499 – 527.

Angrist, J. D. & Krueger, A. B. (1999). Handbook of Labor Economics, volume 3, chapter
  Empirical strategies in labor economics, (pp. 1277 – 1366.). North-Holland: Amsterdam.

Angrist, J. D. & Pischke, J.-S. (2009). Mostly Harmless Econometrics. Princeton, NJ:
 Princeton University Press.

Bang, H. & Robins, J. M. (2005). Doubly robust estimation in missing data and causal
  inference models. Biometrics, 61(4), 962 – 973.

                                                    41
Belloni, A., Chernozhukov, V., & Hansen, C. (2014). Inference on treatment eﬀects after
  selection among high-dimensional controls. Review of Economic Studies, 81(2), 608 – 650.

Bickel, P. J., Klaassen, C. A. J., Ritov, Y., & Wellner, J. A. (1993). Eﬃcient and Adaptive
  Estimation for Semiparametric Models. New York: Springer-Verlag.

Blundell, R. & Powell, J. L. (2003). Advances in Economics and Econometrics: Theory and
  Applications, Eighth World Congress, volume 2, chapter Endogeneity in nonparametric
  and semiparametric regression models, (pp. 312 – 357). Cambridge University Press.

Cattaneo, M. D. (2010). Eﬃcient semiparametric estimation of multi-valued treatment eﬀects
  under ignorability. Journal of Econometrics, 155(2), 138 – 154.

Chamberlain, G. (1984). Handbook of Econometrics, volume 2, chapter Panel Data, (pp.
  1247 – 1318). North-Holland: Amsterdam.

Chamberlain, G. (1986). Notes on Semiparametric Regression. Working paper, University
  of Wisconsin - Madison.

Chamberlain, G. (1987). Asymptotic eﬃciency in estimation with conditional moment re-
  strictions. Journal of Econometrics, 34(3), 305 – 334.

Chamberlain, G. (1992). Eﬃciency bounds for semiparametric regression. Econometrica,
  60(3), 567 – 596.

Chen, X., Hong, H., & Tarozzi, A. (2008). Semiparametric eﬃciency in gmm models with
  auxiliary data. Annals of Statistics, 36(2), 808 – 843.

Frölich, M. (2004). A note on the role of the propensity score for estimating average treatment
  eﬀects. Econometric Reviews, 23(2), 167 – 174.

Goldberger, A. S. (1991). A Course in Econometrics. Cambridge, MA: Harvard University
  Press.

Gottfried, M. A. & Kirksey, J. J. (2017). “when” students miss school: the role of timing of
 absenteeism on students’ test performance. Educational Researcher, 46(3), 119 – 130.

Graham, B. S. (2011). Eﬃciency bounds for missing data models with semiparametric
  restrictions. Econometrica, 79(2), 437 – 452.

Graham, B. S., Imbens, G. W., & Ridder, G. (2010). Measuring the eﬀects of segregation in
  the presence of social spillovers: a nonparametric approach. Working Paper 16499, NBER.

                                              42
Graham, B. S., Pinto, C., & Egel, D. (2012). Inverse probability tilting for moment condition
  models with missing data. Review of Economic Studies, 79(3), 1053 – 1079.

Graham, B. S., Pinto, C., & Egel, D. (2016). Eﬃcient estimation of data combination models
  by the method of auxiliary-to-study tilting (ast). Journal of Business and Economic
  Statistics, 31(2), 288 – 301.

Groves, T. & Rothenberg, T. (1969). A note on the expected value of an inverse matrix.
  Biometrika, 56(3), 690 – 691.

Hahn, J. (1998). On the role of the propensity score in eﬃcient semiparametric estimation
  of average treatment eﬀects. Econometrica, 66(2), 315 – 331.

Hansen, B. (2018). Econometrics.

Hastie, T. & Tibshirani, R. (1993). Varying-coeﬃcient models. Journal of the Royal Statis-
  tical Society B, 55(4), 757 – 796.

Henderson, H. V. & Searle, S. R. (1981). On deriving the inverse of a sum of matrices. SIAM
  Review, 23(1), 53 – 60.

Hirano, K. & Imbens, G. W. (2001). Estimation of causal eﬀects using propensity score
  weighting: an application to data on right heart catheterization. Health Services and
  Outcomes Research Methodology, 2(3-4), 259 – 278.

Hirano, K. & Imbens, G. W. (2004). Applied Bayesian Modelling and Causal Inference from
  Missing Data Perspectives, chapter The propensity score with continuous treatments, (pp.
  73 – 84). John Wiley & Sons, Inc.: New York.

Hirano, K., Imbens, G. W., & Ridder, G. (2003). Eﬃcient estimation of average treatment
  eﬀects using the estimated propensity score. Econometrica, 71(4), 1161 – 1189.

Hitomi, K., Nishiyama, Y., & Okui, R. (2008). A puzzling phenomenon in semiparametric
  estimation problems with infinite-dimensional nuisance parameters. Econometric Theory,
  24(6), 1717 – 1728.

Imbens, G. W. (2000). The role of the propensity score in estimating dose-response functio.
  Biometrika, 87(3), 706 – 710.

Imbens, G. W. & Rubin, D. B. (2015). Causal Inference for Statistics, Social, and Biomedical
  Sciences: An Introduction. Cambridge: Cambridge University Press.


                                             43
Kline, P. (2014). A note on variance estimation for the oaxaca estimator of average treatment
  eﬀects. Economics Letters, 122(3), 428 – 431.

Newey, W. K. (1990). Semiparametric eﬃciency bounds. Journal of Applied Econometrics,
  5(2), 99 – 135.

Newey, W. K. (1994). Kernel estimation of partial means and a general variance estimator.
  Econometric Theory, 10(2), 233 – 253.

Newey, W. K. & McFadden, D. (1994). Handbook of Econometrics, volume 4, chapter Large
  sample estimation and hypothesis testing, (pp. 2111 – 2245). North-Holland: Amsterdam.

Pencavel, J. (1986). Handbook of Labor Economics, volume 1, chapter Labor supply of men:
  a survey, (pp. 3 – 102). North-Holland: Amsterdam.

Robins, J. M., Mark, S. D., & Newey, W. K. (1992). Estimating exposure eﬀects by modelling
  the expectation of exposure conditional on confounders. Biometrics, 48(2), 479 – 495.

Robins, J. M., Rotnitzky, A., & Zhao, L. P. (1994). Estimation of regression coeﬃcients when
  some regressors are not always observed. Journal of American Statistical Association,
  89(427), 846 – 866.

Robinson, P. M. (1988). Root-n-consistent semiparametric regression. Econometrica, 56(4),
  931 – 954.

Rosenbaum, P. R. & Rubin, D. B. (1983). The central role of the propensity score in
  observational studies for causal eﬀects. Biometrika, 70(1), 41 – 55.

Ruud, P. A. (1986). Consistent estimation of limited dependent variable models despite
  misspecification of distribution. Journal of Econometrics, 32(1), 157–187.

Scharfstein, D. O., Rotnitzky, A., & Robins, J. M. (1999). Adjusting for nonignorable drop-
  out using semiparametric nonresponse models: rejoinder. Journal of American Statistical
  Association, 94(448), 1135 – 1146.

Sloczynski, T. (2015). The oaxaca–blinder unexplained component as a treatment eﬀects
  estimator. Oxford Bulletin of Economics and Statistics, 77(4), 588 – 604.

Sloczynski, T. (2017). A general weighted average representation of the ordinary and two-
  stage least squares estimands. Working paper, Brandies University.

Tsiatis, A. A. (2006). Semiparametric Theory and Missing Data. New York: Springer.

                                             44
Wooldridge, J. M. (1999). Distribution-free estimation of some nonlinear panel data models.
 Journal of Econometrics, 90(1), 77 – 97.

Wooldridge, J. M. (2004). Estimating average partial eﬀects under conditional moment
  independence assumptions. Working Paper CWP03/04, CeMMAP.

Wooldridge, J. M. (2005). Identification and inference for econometric models, chapter Unob-
  served heterogeneity and the estimation of average partial eﬀects, (pp. 27 – 55). Number 3.
  Cambridge University Press: Cambridge.

Wooldridge, J. M. (2007). Inverse probability weighted estimation for general missing data
  problems. Journal of Econometrics, 141(2), 1281 – 1301.

Wooldridge, J. M. (2010). Econometric Analysis of Cross Section and Panel Data. Cam-
  bridge, MA: MIT Press, 2nd edition.

Yitzhaki, S. (1996). On using linear regressions in welfare economics. Journal of Business
  and Economic Statistics, 14(4), 478 – 486.

Yule, G. U. (1899). An investigation into the causes of changes in pauperism in england,
  chiefly during the last two intercensal decades (part i.). Journal of the Royal Statistical
  Society, 62(6), 249 – 295.




                                             45
