                              NBER WORKING PAPER SERIES




                         ESTIMATING THE ANOMALY BASE RATE

                                      Alexander M. Chinco
                                       Andreas Neuhierl
                                         Michael Weber

                                       Working Paper 26493
                               http://www.nber.org/papers/w26493


                     NATIONAL BUREAU OF ECONOMIC RESEARCH
                              1050 Massachusetts Avenue
                                Cambridge, MA 02138
                                   November 2019




We would like to thank Justin Birru, Svetlana Bryzgalova, Zhi Da, Xavier Gabaix, Niels
Gormsen, Sam Hartzmark, Christian Julliard, Ralph Koijen, Bob Korajczyk, Yan Liu, Stefan
Nagel, Walt Pohl, Jeff Pontiff, Tarun Ramadorai, Alessio Saretto, Andrea Tamoni, Julian
Thimme, Allan Timmermann, Rüdiger Weber, and Dacheng Xiu for extremely helpful comments
and suggestions. This paper has also benefited greatly from presentations at the University of
Chicago, the University of Illinois, the MFA meetings, AQR Asset-Management Institute's
Academic Symposium, the Future of Financial Information Conference, the 5th BI-SHoF
Conference, the NBER Summer Institute, the SITE Asset-Pricing Theory and Computation
Meetings, the EFA Meetings, the NFA Conference, and the SAFE Asset-Pricing Workshop.
Bianca He provided excellent research assistance. Weber also gratefully acknowledges financial
support from the University of Chicago, the Fama Research Fund, and the Fama-Miller Center.
The views expressed herein are those of the authors and do not necessarily reflect the views of the
National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.

© 2019 by Alexander M. Chinco, Andreas Neuhierl, and Michael Weber. All rights reserved.
Short sections of text, not to exceed two paragraphs, may be quoted without explicit permission
provided that full credit, including © notice, is given to the source.
Estimating The Anomaly Base Rate
Alexander M. Chinco, Andreas Neuhierl, and Michael Weber
NBER Working Paper No. 26493
November 2019
JEL No. C12,C52,G11

                                            ABSTRACT

The academic literature literally contains hundreds of variables that seem to predict the cross-
section of expected returns. This so-called "anomaly zoo" has caused many to question whether
researchers are using the right tests of statistical significance. But, here's the thing: even if
researchers use the right tests, they will still draw the wrong conclusions from their econometric
analyses if they start out with the wrong priors---i.e., if they start out with incorrect beliefs about
the ex ante probability of encountering a tradable anomaly.

So, what are the right priors? What is the correct anomaly base rate?

We develop a first way to estimate the anomaly base rate by combining two key insights: 1)
Empirical-Bayes methods capture the implicit process by which researchers form priors based on
their past experience with other variables in the anomaly zoo. 2) Under certain conditions, there is
a one-to-one mapping between these prior beliefs and the best-fit tuning parameter in a penalized
regression. We study trading-strategy performance to verify our estimation results. If you trade on
two variables with similar one-month-ahead return forecasts in different anomaly-base-rate
regimes (low vs. high), the variable in the low base-rate regime consistently underperforms the
otherwise identical variable in the high base-rate regime.

Alexander M. Chinco                                        Michael Weber
University of Illinois at Urbana-Champaign                 Booth School of Business
1206 S 6th St, Rm 343J                                     University of Chicago
Champaign, IL 61820                                        5807 South Woodlawn Avenue
AlexChinco@gmail.com                                       Chicago, IL 60637
                                                           and NBER
Andreas Neuhierl                                           michael.weber@chicagobooth.edu
University of Notre Dame
College of Business
221 Mendoza
Notre Dame, IN 46556
aneuhier@nd.edu
1     Introduction
Imagine you are a financial economist sitting down at your weekly research seminar. Today's speaker
has regressed each stock's excess return, Rn , on lagged values of a new variable, Xn :

                    Rn = µ
                         ^+^ · Xn + ^n                     for stocks n = 1, . . . , (N + 1).        (1)

^ is the mean excess return in the current month, 
µ                                                   ^ is the estimated slope coefficient, and ^n is the
residual for the nth stock. Before running this regression, the speaker standardized Xn to have zero
mean and unit variance in the cross-section. And, on the current slide, he is reporting a positive
estimated slope coefficient,   ^ > 0, that is statistically significant at the 1% level.
    Predictive regressions and trading-strategy returns are two sides of the same coin (Fama, 1976).
So, the  ^ in Equation (1) can be viewed as the realized return to a zero-cost portfolio that is long
stocks with high Xn values last month and short stocks with low Xn values last month:

                          ^ def
                            =     Cov[Rn , Xn ]
                                                  =   1
                                                          ·       n         ^) · (Xn - 0).
                                                                      (Rn - µ                        (2)
                                    Var[Xn ]          N


Thus, an estimated   ^ > 0 implies not only that high-Xn stocks last month, (Xn - 0) > 0, tended to
have high excess returns this month, (Rn - µ  ^) > 0, but also that it was profitable to trade on Xn
in the past. If you had bought high-Xn stocks last month, then you would have earned high returns
this month. Since investors should exploit and thereby eliminate such an arbitrage opportunity, this
finding might suggest a gap in our economic understanding of how financial markets work.
    However, the speaker's estimated slope coefficient is just that. . . an estimate. An estimated
 ^ > 0 implies that it was profitable to trade on Xn , but you want to know whether it will be
profitable to trade on Xn going forward. And, this change in verb tense makes a world of
difference since "many of the results being published fail to hold up in the future" (Harvey, 2017).
Predictability often turns out to be a decidedly in-sample phenomenon (McLean and Pontiff, 2016;
Linnainmaa and Roberts, 2018). Xn still might not represent a tradable anomaly even if the
seminar speaker estimates a large, positive, and statistically significant slope coefficient. His results
might just be a fluke, a chance event not warranting any change in our economic understanding.
    Your task as an audience member is to figure out Pr[ anom | signif ], the probability that Xn is
a tradable anomaly given the speaker's statistically significant results. Bayes' rule tells you how:

                                                          Pr[ signif | anom ]
                       Pr[ anom | signif ] =                  Pr[ signif ]
                                                                                × Pr[ anom ].        (3)

Multiply the ex ante probability that Xn is a tradable anomaly, Pr[ anom ], times the relative
                                                                                             | anom ]
increase in the base rate due to the speaker's statistically significant results, Pr[ signif
                                                                                      Pr[ signif ]
                                                                                                      .


                                                              1
    The current literature on cross-sectional predictability is primarily concerned with how data
                                                                                  | anom ]
mining distorts the first term on the right-hand side of Equation (3), Pr[ signif
                                                                           Pr[ signif ]
                                                                                           . Computing
this `Bayes factor' requires knowing the correct unconditional probability of a significant result,
Pr[ signif ]. And, knowing the correct unconditional probability requires using the right p-value:

     Pr[ signif ] = Pr[ signif | anom ] × Pr[ anom ] + Pr[ signif | ¬anom ] ×Pr[ ¬anom ].           (4)
                                                                   =p-value


Earlier we said that the seminar speaker reported a positive estimated slope coefficient,    ^ > 0, that
was statistically significant at the 1% level--i.e., a p-value = 0.01 and a t-statistic = 2.54. A
p-value = 0.01 means that there is only a 1/100 chance of a single variable having such an estimate
by pure chance under the null hypothesis of no predictability. But, if the seminar speaker also ran
regressions involving 99 other variables, then on average you would expect one of these 100
regressions to produce a p-value = 0.01. Thus, by ignoring the effects of pervasive data mining,
financial economists tend to think that significant results are less likely than they actually are. And,
as a result, they tend to over-react to statistically significant results when they see them.
    Using a higher t-statistic cutoff (or equivalently, a lower p-value cutoff) when assessing
statistical significance is one way of correcting this error (Harvey et al., 2016). Financial economists
can and should be doing this. But, it is also worth remembering that the Bayes factor is not the
only term on the right-hand side of Equation (3). Someone who enters the seminar room with
wildly inaccurate priors, Pr[ anom ], is going to draw the wrong conclusions from the speaker's
results even if they correct their Bayes factor for data mining. And, it seems like some of your
colleagues must have entered the seminar room today with wildly inaccurate priors. Before the talk
even started, you already knew who would leave the room skeptical regardless of what the speaker
said. You could also point to a group of your colleagues known to be much more receptive to new
cross-sectional predictors. Regardless of where you stand, it is clear that both priors cannot be right.
We are not the first to point out the importance of prior beliefs in asset-pricing tests (Shanken, 1987;
Harvey and Zhou, 1990). But, the current literature gives no guidance about which priors to use.
    So, how did you estimate an anomaly base rate for use in today's seminar? We recognize that
there is a whiff of paradox about the idea of estimating one's priors. But, the notion is not
completely absurd. Penalized-regression procedures such as the Ridge (Hoerl and Kennard, 1970)
can be viewed as something like this when they solve the optimization problem below:

                                                               2
                           min    1
                                  N
                                      ·   n        ^ -  · Xn
                                              Rn - µ               +  · 2 .                         (5)
                            


  0 is called the `tuning parameter'. And, it is well-known that this optimization problem has a
clear Bayesian interpretation when the true slope coefficients are drawn from the appropriate


                                                   2
distribution. For the Ridge, the appropriate distribution is i  Normal[0,  2 ]. Under these
conditions, the Ridge's success can be viewed as the result of incorporating one's prior beliefs about
 2 , the variance of the market conditions that lead to cross-sectional return predictability (Pástor,
2000; DeMiguel et al., 2009; Ledoit and Wolf, 2017; Kozak et al., 2018b).
     In this simple statistical setup, the best-fit tuning parameter () is inversely related to
the variance of one's prior beliefs about the market conditions for predictability ( 2 ). This
mathematical connection between the best-fit tuning parameter and prior beliefs is a promising lead.
Unfortunately, the economic meaning of this mathematical connection is not entirely clear. Suppose
 is inversely related to the variance of market conditions for return predictability. There is no
general theory of these market conditions. The variables that seem to predict the cross-section of
expected returns are constructed using a wide variety of different data sets and typically have little
to do with one another economically (Lewellen et al., 2010). So, how should we interpret the
associated prior variance,  2 ? What exactly are the corresponding prior beliefs beliefs about?
     With these difficulties in mind, this paper proposes a first way to estimate the prevailing
anomaly base rate. Here is the logic. The existence of the anomaly zoo tells us that research
practices matter. So, maybe your prior beliefs walking into the seminar room today had more to do
with your first-hand knowledge of current research practices than with the market conditions
underpinning return predictability. After all, as a well-trained financial economist, you know the
econometric tips and tricks that other well-trained financial economists use. You yourself frequently
employ these same techniques. So, your past experience with other variables in the anomaly zoo
must contain information about the relative rate at which these shared techniques produce tradable
anomalies rather than spurious predictors.
     When reframed this way, the idea of estimating one's priors no longer seems quite so paradoxical.
Financial economists are constantly evaluating their current research practices based on how well
their results hold up out of sample. This is an integral part of doing good research, and it is a
process that is perfectly captured by empirical-Bayes methods (Robbins, 1956; Efron and Morris,
1972, 1973, 1975). "The essential empirical-Bayes task [is] learning an appropriate prior distribution
from ongoing statistical experience rather than knowing it by assumption (Efron, 2013)." Or, in the
words of Efron and Hastie (2016, p. 77), "large data sets of parallel situations carry within them
their own Bayesian information" about the appropriate prior. And, what is the anomaly zoo if not a
large data set of many parallel situations, one for each variable in the academic literature
i = 1, . . . , I that seems to predict the cross-section of expected returns?
     To exploit this reframing, we model the true predictive strength of each of variable in the
anomaly zoo as being drawn from a common normal distribution:

                         iid
                     i  Normal[0,  2 ]         for each variable i = 1, . . . , I.                (6)


                                                  3
  5%
  4%
                                            2
  3%                                      v
                                          ¯t
  2%
  1%
  0%
            `80           `85          `90           `95           `00           `05          `10           `15
     7                                                                       It
     4
     1
                                   2
  Figure 1. Forecasted v          ¯t . Top Panel: Average forecasted prior variance each year. For
  each variable i = 1, . . . , It first introduced prior to month t, we estimate the in-sample
                2
  parameter v  ^i,t for all months t < t via Proposition 2.2. We then make a one-month-ahead
                            2
  forecast for month t, v ¯i,t , by fitting an AR(3) model in months {t - 60, . . . , t - 1}. Finally, we
                                                                2
  average the forecasts for all past variables to compute v    ¯t . Bottom Panel: Number of new
  variables introduced to the academic literature each year, It . Sample: June 1978 to June 2015.


The idea is to parameterize the efficacy of current research practices by the variance of this
distribution. If  2 = 0, current research practices only produce spurious predictors. As  2 increases,
it becomes more likely that a randomly selected variable in the anomaly zoo is a tradable anomaly.
    Once we view  2 as related to research methods and not market conditions, this quantity has a
clear economic meaning. It then makes sense to invert the best-fit  in univariate Ridge regressions
to estimate it. Thus, we can use empirical-Bayes logic to capture the way in which researchers
combine their past experiences with other variables to inform their priors by simply averaging the
resulting variable-specific estimates for  2 . We apply this approach to monthly data on 85 different
variables in the anomaly zoo. Let It  85 denote the number of variables discovered prior to month t.
Each month, we run separate univariate Ridge regressions of excess returns on lagged values of each
variable i = 1, . . . , It . Then, we invert the best-fit tuning parameter from each of these regressions
to produce variable-specific estimates for  2 in each month t, which we call v            2
                                                                                         ^i,t . We fit an AR(3)
model to the previous 60 months of data to make one-month-ahead forecasts of the prior variance
                                       2
associated with each predictor, v    ¯i,t . Our results are robust to using other out-of-sample forecasting
                                                                                                                2
rules. Finally, we average these out-of-sample forecasts to find the aggregate prior variance, v              ¯t  .
                                                                                                            2
    The top panel of Figure 1 shows how this prior variance has evolved over time. Note that v             ¯t  is
not just counting the number of variables recently added to the academic literature. The bottom
panel of Figure 1 shows that the anomaly base rate can be low at times when lots of new variables
are being `discovered' if these new variables tend to have poor subsequent performance--i.e., if they
                                      2
tend to be spurious predictors. v   ¯t   is also not just a stand-in for return volatility or related variables.
  2
¯t is measuring the dispersion in performance among variables in the anomaly zoo. Last but not
v


                                                        4
least, recall that our statistical approach is explicitly agnostic about the market conditions that lead
to cross-sectional return predictability. So, the variables we are calling `tradable anomalies' do not
necessarily have to be pricing errors. We are using this term as a convenient short-hand since it
reflects the original motivation for looking for such variables as described in Equation (2).
      This paper offers the first practical means of estimating the anomaly base rate. The anomaly
zoo contains a few tradable anomalies and many more spurious predictors. Our core economic
insight is that a researcher's past experience with these variables must contain information about
the efficacy of his research practices. So, in the absence of an overarching theory for cross-sectional
predictability, a researcher likely uses this information to inform the prior beliefs he uses when
evaluating new variables. Since this is the first paper pursuing this idea, we have deliberately used a
highly stylized statistical setup. But, numerical simulations show our approach is readily extensible.
      What's more, a trading-strategy performance shows that, in spite of its transparent simplicity,
our statistical approach still produces actionable estimates of the anomaly base rate. We consider
trading on two variables in different base-rate regimes (one low, the other high) with the same
one-month-ahead return forecast. Theory suggests that the variable in the low base-rate regime
should under-perform the otherwise identical variable in the high base-rate regime. And, this
is exactly what we find in the data. Let direction i be the direction of the ith variable. e.g.,
direction i = +1 for medium-term momentum while direction i = -1 for short-term reversals. And,
let   ¯i,t be the one-month-ahead return forecast for the ith variable.
      We start with a benchmark strategy that holds all variables in the anomaly zoo that have
 ¯
i,t · direction i > 1% in month t. The 1%-per-month threshold captures the idea, to be a tradable
anomaly, a variable must generate enough predictability to cover implementation costs. The specific
threshold level is not important, and our results are robust to using a wide range of thresholds.
Rather, the key thing is that, when the prior variance implied by all other variables i = i is
           2     def    1             2
small, v  ¯¬ i,t = It -1 ·          ¯i
                               i =i v   ,t  0, you should expect the ith variable to have a small amount of
predictability too, i,t  0. As a result, you should be less willing to trade on the ith variable if
 ¯i,t · direction i        threshold . Following this logic, our base-rate-adjusted strategy only holds
                                                                                                        2
variables i = 1, . . . , It that still have return forecasts in excess of 1% after adjusting for v    ¯¬  i,t .
      This base-rate-adjusted strategy delivers net excess returns of 0.74% per month and has an
annualized out-of-sample Sharpe ratio of 0.57. The strategy works by discarding variables in the
benchmark strategy that are implausibly strong predictors given the prevailing anomaly base rate.
And, we find that an alternative strategy which only invests in these discarded variables has net
returns of just 0.18% per month and an annualized out-of-sample Sharpe ratio of 0.11. This drop in
performance persists even when we control for the strength and precision of each variable's
forecasted      ¯i,t . In other words, we find that trading on the same one-month-ahead return forecast is
less profitable in months when the anomaly base rate is low just as predicted by the theory.


                                                      5
1.1     Related Literature
This paper connects to three strands of the literature on cross-sectional return predictability.
    Data Mining. There is a large financial-econometrics literature on data mining (Lo and
MacKinlay, 1990; Ferson et al., 1999; Sullivan et al., 1999; White, 2000; Barras et al., 2010;
Bajgrowicz and Scaillet, 2012; McLean and Pontiff, 2016; Harvey et al., 2016; Yan and Zheng, 2017;
Harvey and Liu, 2018a,b,c; Linnainmaa and Roberts, 2018). We want to emphasize that, while
clearly related to the topic, this is not a paper about data mining. We are doing something different.
We are proposing the first method of estimating the anomaly base rate, Pr[ anom ]. In fact, our
                                                           | anom ]
analysis specifically takes the Bayes factor, Pr[ signif
                                                    Pr[ signif ]
                                                                    , as given. You should use the above
papers to adjust the Bayes factor for data mining when interpreting a seminar speaker's results.
Then, you should apply this data-mining adjusted Bayes factor to the prevailing anomaly base rate,
which we are showing how to estimate in this paper.
    Factor Structure. Even if there are many statistically significant predictors, it still might be
possible to summarize the information in all these variables using a few well-chosen factors. Papers
on the factor structure of predictors try to simplify investors' lives by collapsing the anomaly zoo
down into a few manageable variables. Ideally, once you condition on these variables, there would be
no incremental value in considering any other variables when forecasting returns. Researchers
typically try to accomplish this goal using some form of principal-component analysis (Kelly et al.,
2017; Green et al., 2017; Kelly et al., 2018; Lettau and Pelger, 2018).
    Suppose you are managing director of a quant trading desk. One of your analysts has just
walked into your office and proposed a new candidate predictor. What is the ex ante probability that
this variable is a tradable anomaly? This is our question of interest. Later tonight you go to dinner
with one of your managing-director peers. It turns out that he also met with one of his analysts
about a new candidate predictor earlier in the day. What is the probability that the information
contained in both these variables is the same? That is the question the academic literature on the
factor structure of predictors is answering. These are interesting but logically distinct questions.
    Penalized Regressions. Many recent papers have used penalized-regression procedures to
solve asset-pricing problems (Pástor, 2000; DeMiguel et al., 2009; Bryzgalova, 2017; Feng et al.,
2017; Freyberger et al., 2017; Ledoit and Wolf, 2017; Chinco et al., 2018; Kozak et al., 2018b). We
are using these same techniques with an entirely different goal in mind: estimating the anomaly base
rate. Here is the key distinction. If there were only a handful of variables to choose from, then these
earlier papers would not need to use a penalized regression. By contrast, estimating the correct
anomaly base rate is important regardless of the size of the anomaly zoo. To our knowledge, this is
the first instance where insights from machine learning are being used to shape our understanding of
a more fundamental asset-pricing problem, a problem that exists even in a low-dimensional setting.


                                                   6
2     Statistical Approach
Imagine you are a financial economist sitting at your weekly research seminar. The seminar speaker
is showing evidence that it was profitable to trade on some new variable, Xn . You want to figure
out the probability of Xn being a tradable anomaly going forward. To do this, you need to start out
with the right anomaly base rate. Then, you need to update this prior belief based on the speaker's
empirical results. Your financial-econometrics training tells you how to do the updating. This section
describes a first statistical approach to estimating the correct anomaly base rate.

2.1     Inference Problem
We begin by defining the inference problem you face in greater detail. What precisely is the anomaly
base rate? And, what does it mean to learn about this quantity from past experience?
    Data-Generating Process. Suppose there are (N + 1) stocks indexed by n = 1, . . . , (N + 1).
Let Rn denote the excess return of the nth stock in the current month. Each stock's excess return
this month is related to the lagged value of the ith variable in the previous month, Xn,i :

                     Rn = µ + i · Xn,i + n,i         for each variable i = 1, . . . , I.              (7)

µ is the average excess return in the current month, i is the true slope coefficient associated with
                               iid
the ith variable, and n,i  Normal[0, N · se 2     i ] is the residual excess return for the nth stock for
the ith variable. In other words, n,i represents the portion of the nth stock's excess return that is
not explained by the ith variable. This residual might come from idiosyncratic shocks that are
fundamentally random or represent the effects of other variables i = i.
     We normalize the lagged values of each variable to have zero mean and unit variance in the
cross-section each month: N1       +1
                                      · n Xn,i = 0 and N   1
                                                              · n (Xn,i - 0)2 = 1. This normalization
ensures that in-sample estimates of       ^i are comparable across variables. Consider the following
example to see why this is important. Suppose that variable 1 corresponds to a portfolio that is
long/short the top/bottom deciles while variable 2 corresponds to a portfolio that is long/short the
                                                                            1            1
top/bottom quintiles. Variable 1 will have a variance of Var[Xn,1 ] = 10       · (+1)2 + 10 · (-1)2 = 1/5;
whereas, variable 2 will have a variance of Var[Xn,2 ] = 1      5
                                                                  · (+1)2 + 1
                                                                            5
                                                                              · (-1)2 = 2/5. Thus, since
 ^i def
    = Cov[Rn , Xn,i ] , we would expect that ^1 >  ^2 even if both variables have equal predictive power
        Var[Xn,i ]
simply because Var[Xn,1 ] = 1/5 < 2/5 = Var[Xn,2 ].
    Tradable Anomalies. We say that the ith variable is a tradable anomaly if the magnitude
of its true predictive power exceeds some minimum performance threshold:

                               def
                        anom i = 1 |i | > threshold          for threshold  0.                        (8)


                                                    7
You can think about this minimum performance threshold as coming from trading costs (Novy-Marx
and Velikov, 2015). Or, you can think about it more broadly as any form of implementation costs. If
you are running a trading desk, then how strong does a predictor need to be before you redeploy
scarce resources to trade on it? In our empirical analysis, we will typically set threshold = 1% per
month for a trading strategy that is long/short the top/bottom deciles. However, we show in Section
4 that using a threshold  [0.5%, 1.5%] yields qualitatively similar results.
    We take the absolute value of the true slope coefficient in Equation (8) because the sign of the
i associated with a long-short strategy is arbitrary. Fama and French (1993) could just as easily
have defined their HML factor using market-to-book rather than book-to-market ratios. This
alternative choice would have flipped the sign of the associated slope coefficient but left everything
else about the authors' analysis unchanged. Contrarian strategies, such as the long-term reversals
(De Bondt and Thaler, 1985), will have i < 0. What really matters is the magnitude of i .
    The statistical approach we describe in this section is agnostic about the market conditions
leading to cross-sectional return predictability. So, although we will refer to strong predictors as
tradable anomalies, their predictability does not necessarily have to be the result of a pricing error
(Kozak et al., 2018a). Nevertheless, we use the term `tradable anomaly' rather than `sufficiently
strong predictor' because we feel it captures the motivation behind researchers' interest in
cross-sectional predictability as we outline in the introduction.
    Research Practices. There is no single unified explanation for the origins of cross-sectional
return predictability. The variables that seem to predict the cross-section of expected returns are all
quite different. Cross-sectional predictability might be the result of any number of different
limits-to-arbitrage models (Barberis and Thaler, 2003; Gromb and Vayanos, 2010). There is no
shortage of risk-based explanations to choose from either (Fama and French, 1996). And, different
variables often use entirely different data sources. Some variables only involve past market data (e.g.,
medium-term momentum; Jegadeesh and Titman, 1993). Others use only accounting data (e.g.,
investment growth; Titman et al., 2004). Why would there be a single economic explanation for the
predictability associated with both medium-term momentum and investment growth? The only
common theme connecting these two variables is Sheridan Titman.
    There is no general theory of the market conditions leading to cross-sectional return predictability.
So, in the absence of such a theory, your prior beliefs about the likelihood of a tradable anomaly
likely have more to do with your first-hand knowledge of current research practices. Even if the
economic rationales for medium-term momentum and investment growth are completely different,
you know that both variables were discovered by financial economists using a shared set of research
practices. We have all attended the same PhD programs. We have taken the same courses. We have
all been trained by the same advisers. We have all been given access to the same data sources.
While computing power has grown over time, at any one point in time all researchers have access to

                                                   8
similar amounts of processing power. Thus, while some researchers are better at searching for new
variables than others, everyone's discovery process is constrained by the same inputs.
     We use a statistical model for the anomaly-discovery process to capture this commonality in
research practices. Specifically, we assume that the strength of each variable is drawn from a
common normal distribution as described in Equation (6). The key assumption embedded in this
equation is that the efficacy of these research practices can be encapsulated by a single parameter
 2 , which governs the range of predictability that financial economists discover. A larger value of  2
means that researchers are more likely to discover strong predictors. It is not essential to assume
normality or independence across variables as we show in Section 2.3 below. The mean-zero
assumption is also not crucial. We make these simplifying assumptions for pedagogical reasons since
this is the first paper to propose a practical method for estimating the prevailing anomaly base rate.
    Inference Problem. If the strength of the ith variable is drawn as in Equation (6), then the
parameter  2 controls the typical size of cross-sectional predictability. This assumption implies that
calibrating your expectations for what today's seminar speaker will say during the next hour is
tantamount to learning about the prevailing value for  2 .

Proposition 2.1 (Inference Problem). Suppose that the true slope coefficients are drawn as in
Equation (6), excess returns are governed by the data-generating process in Equation (7), and there
exists some minimum performance threshold as specified in Equation (8), then the anomaly base rate
is given by
                               Pr[ anom i ] = 2 · [ - threshold / ],                            (9)

where [·] represents the standard normal CDF.

      Today's seminar speaker is presenting evidence that the ith variable is a very strong predictor,
|^i |    threshold . If your past experience with (I - 1) other variables tells you that  2       0, then
                                                        2
this result should strike you as quite plausible.            0 means that it is quite common for a
researcher to draw a i far from zero. By contrast, if your past experience suggests that  2  0,
then the seminar speaker's result will strike you as highly implausible. You will rationally discount
the seminar speaker's evidence, not because he did anything wrong, but because you think his result
is very unlikely to begin with.  2  0 means that tradable anomalies are quite rare. But, your past
experience could not have dictated that  2 = 0 exactly. If it did, then you would not have even
bothered to show up to the seminar in the first place. There would have been nothing that the
seminar speaking could say to convince you that i = 0. This is known as having `dogmatic priors'.
      We propose an extremely simple model of the anomaly-discovery process. But, it is possible to
extend this simple model in various ways to make it more realistic. We view these extensions as
promising avenues for future research, and we use numerical simulations to explore many of these
possibilities in Section 2.3. However, to the best of our knowledge, this is the first paper to explicitly

                                                    9
study the anomaly-discovery process. Taking this first step is a key contribution of our paper.
    Differences in  2 represent differences in the efficacy of current research practices. When
researchers have been applying the same empirical techniques to the same collection of data sets for
a long time already,  2 will be close to zero. It is unlikely that these same techniques will suddenly
uncover a new variable after years and years of trying. But, these exact same techniques can be
given new life by the introduction of an important new data set, such as the Trade and Quote
Database (TAQ). That being said, we do not have a full explanation for variation in  2 . Our
approach is similar in spirit to the idea of estimating stock-market volatility (Andersen et al., 2003)
or an uncertainty index (Baker et al., 2016). We certainly do not think that there is a single unified
explanation for all stock-market fluctuations. And, almost by definition, uncertainty can rise for any
number of reasons. Better understanding the drivers of the anomaly-discovery process is an
important and understudied topic. It is instructive to compare and contrast how financial
econometrics treats variable search with how labor economics treats job search (Petrongolo and
Pissarides, 2001). We hope our paper can spark interest in this topic.
    Publication Bias. At first glance, the assumption of a normally distributed i seems to be
at odds with the existence of publication bias. If researchers are engaging in data mining, why are
there not peaks in the i distribution right above/below the positive/negative 5% p-value cutoffs?
After all, this is the level of statistical significance that is needed to get a paper published.
    Using a cross-sectional OLS regression like in Equation (1) to estimate i based on data
observed prior to publication does produce an empirical distribution with two large masses
immediately above/below the positive/negative 5% p-value cutoffs as shown by the red shaded region
in Figure 2. This figure is based on 104 simulations of the data-generating process from Equations
(6) and (7) with (N + 1) = 1,000 stocks and parameter values µ = 0%,  = 1%, and se i = 1%.
Each simulation starts by repeatedly sampling from this data-generating process until the realized
in-sample OLS estimate of the true slope coefficient,        ^i , has a p < 0.05. The red-shaded region
reports the empirical density of the resulting   ^i realizations conditional on statistical significance.
    But, we are not modeling the        ^i distribution in Equation (6); we are modeling the 
                                                                                                        i
distribution. The   ^i distribution, which is depicted by the red-shaded region in Figure 2, is a
function of both the i distribution and publication bias. If you are concerned about the pernicious
effects of data mining, p-hacking, publication bias, etc. . . , then you must believe that these research
practices have a big effect on the ^i distribution. In which case, there is no reason to expect the    ^i
distribution to look like the i distribution. And, this is precisely what the black-shaded region in
Figure 2 shows. This region reports the empirical density of i computed in those same 104
simulations leading to a significant    ^i estimate at the 5% level. In spite of the existence of an
extreme form of publication bias, the black-shaded region is not far from normal as depicted by the
blue curve representing a normal density with the same mean and variance.

                                                   10
            0.08
       ^i = x] 

            0.06   Pre-Publication

            0.04
    Pr[




            0.02                                        Post-
                                                      Publication
            0.00        -4                -2                  0                 2                 4
    Figure 2. Publication Bias. x-axis: value of i . y -axis: empirical density computed using
    104 simulations. Each simulation repeatedly samples from the data-generating process governed
    by Equations (6) and (7) with (N + 1) = 1,000 stocks and parameter values µ = 0%,  = 1%,
    and se i = 1% until the realized OLS estimate,   ^i , has p < 0.05. Red-shaded region reports
                                       ^
    empirical density of the resulting i estimates, which correspond to pre-publication empirical
    results. Black-shaded region reports empirical density of i values associated with each      ^i
    estimate. These values correspond to long-run average post-publications empirical results. Blue
    line is a normal distribution with same mean and variance as the empirical i distribution.


    Empirical Bayes. In many ways, the existence of the anomaly zoo is problematic. It is a sign
that financial economists have adopted bad research practices. But, if these research practices
matter so much, then they must have had an effect on your prior beliefs walking into the seminar
room today. Your past experience with other variables in the anomaly zoo--some tradable
anomalies and many more spurious predictors--must contain valuable information about how often
the variables produced by these research practices turn out to be tradable anomalies going forward.
    Thus, the cross-sectional asset-pricing literature represents a large data set of parallel past
situations for you to draw on, one for each variable you have encountered over the course of your
career i = 1, . . . , I . Empirical Bayes formalizes the process by which researchers convert past
experience with such variables into a prior for next time around. As Efron and Hastie (2016, p. 88)
writes, empirical-Bayes methods allow us to "remove the usual Bayesian scaffolding. In place of a
reassuring prior, the statistician [puts] his or her faith in the relevance of the `other' cases in a large
data set to the direct case of interest." After being introduced by Robbins (1956), the first major
work on empirical-Bayes methods was a series of papers by Efron and Morris (1972, 1973, 1975).1
    Empirical-Bayes applications all have the same basic structure (cf. Maritz, 1970, p. 13). They all
1
 There is some ambiguity about naming conventions. As Efron (2012, p. 14) writes, "Robbins reserved the name
`empirical Bayes' for situations where a genuine prior distribution was being estimated, using `compound Bayes' for
 more general parallel estimation and testing situations, but Efron and Morris (1973) hijacked `empirical Bayes' for
James and Stein-type estimators." "One might refer to Robbins's formulation as nonparametric empirical Bayes;
whereas, the formulation discussed here can be referred to as parametric empirical Bayes. (Casella, 1985)"
 Parametric empirical Bayes sometimes goes by the name of `objective Bayes' (Berger, 2006) or `hierarchical Bayes'
(Gelman et al., 2013) in the statistics literature, too.


                                                       11
start with a problem involving repeated samples from an unknown prior distribution, g[ 2 ]. This
repeated sampling yields unseen realizations, {1 , . . . , I }. Each of these unseen realizations, i , in
turn provides an observation,          ^i  f [·], drawn from a known probability family. On the basis of
                                                 i

this observed data set, {       ^1 , . . . , ^I }, a researcher wishes to solve some inference problem that
would be easy if only he knew the true prior distribution, g[ 2 ], e.g., while sitting in today's
research seminar, you are interested in estimating the true slope coefficient associated with the ith
variable, i , after seeing the speaker's in-sample OLS estimate,           ^i --a problem that would be easy if
only you knew the correct choice of prior beliefs.
     Here is how empirical Bayes suggests you solve this problem related to interpreting evidence
about the ith variable. First, convert the information about the other samples i = i into
estimated values for  2 , which we will denote v            2
                                                           ^i . Then, since each of these samples represents a
parallel situation with some noise, take the average of these (I - 1) sample-specific estimates,
  2 def 1             2
^¬
v  i = I -1 ·       ^i
               i =i v   . Finally, solve your inference problem by plugging this average value into your
                         2
prior distribution, g[^ v¬ i ]. These steps allow you to "[learn the] appropriate prior distribution from
ongoing statistical experience rather than knowing it by assumption (Efron, 2013)."
     Starting with Shanken (1987) and Harvey and Zhou (1990), numerous papers have pointed out
the importance of prior beliefs in asset-pricing tests. But, this Bayesian asset-pricing literature has
always interpreted prior beliefs as having something to do with the market conditions leading to
cross-sectional predictability. It is unclear how to interpret such priors in the absence of an unifying
economic model for predictability. And, as a result, this existing literature can offer no guidance
about which anomaly base rate to use. We resolve this paradox by pointing out that, in the absence
of such a model, researchers' priors are likely determined by their first-hand knowledge of the
anomaly-discovery process. We may not know the underlying economic model connecting variables
like medium-term momentum (Jegadeesh and Titman, 1993) and investment growth (Titman et al.,
2004), but we know a lot about the shared empirical toolkit used to discover both these variables.

2.2     Econometric Estimator
We are using  2 , the variance of i , to represent the efficacy of current research practices. We now
show how to apply univariate Ridge regressions to estimate this parameter based on the subsequent
performance of every other variable in the anomaly zoo. Since each of these variables is just one of
many parallel situations, we can then average these predictor-specific estimates for  2 to compute
the prevailing anomaly base rate. This averaging reflects the implicit process by which a researcher
uses his past experience with other variables to inform his prior for use next time around.
   Ridge Regression. We study a penalized-regression procedure known as the Ridge regression
(Hoerl and Kennard, 1970). A Ridge regression combines a standard OLS regression with a
quadratic penalty that shrinks OLS-regression estimates towards zero. Using the Ridge to compute

                                                     12
the slope coefficient, ^i [], for the ith predictor means solving the following optimization problem:

                                                                                   2
                    ^i [] def
                          = arg min            1
                                                   ·        n        ^ -  · Xn,i
                                                                Rn - µ                 +  · 2 .                   (10)
                                               N


 ·  2 represents the quadratic penalty, and   0 is known as the `tuning parameter'. While there
are a total of (N + 1) stocks, the sum-of-squares term is divided by N in Equation (10) because it
already includes the estimated mean, µ    ^, leading to a degrees-of-freedom correction.
    It is possible to analytically solve for  ^[] in a univariate setting:

                                                   ^i [] =
                                                                   1
                                                                       ·^i .                                      (11)
                                                                  1+


When  = 0, the penalty function disappears, 0 ·  2 = 0, and the results of the Ridge coincide with
those of an OLS regression, ^i = ^i [0]. But, for all  > 0, the quadratic penalty in Equation (10)
shrinks OLS-regression coefficients toward zero, with larger s resulting in more shrinkage.
    Bayesian Interpretation. There is a Bayesian interpretation for this shrinkage. You can
view it as the effect of incorporating prior beliefs about the volatility of the true slope coefficients,
i , when these coefficients are drawn from a common normal distribution as in Equation
(6). This is because the negative posterior log likelihood of i =  given the realized data,
   def                           def
R = {R1 , . . . , RN +1 } and Xi = {X1,i , . . . , XN +1,i }, can be written as follows

                                                                               2
      - log Pr[ | R, Xi ] =         1
                                         2     ·       n    Rn - µ
                                                                 ^ -  · Xn,i       +     1
                                                                                       2· 2
                                                                                              · ( - 0)2 + · · ·
                                2·(N ·se i )
                                                                                   2          2                   (12)
                                  1            1                                           se i       2
                            =        2   ·     N
                                                   ·       n         ^ -  · Xn,i
                                                                Rn - µ                 +   2
                                                                                                  ·       + ···
                                2·se i


where the "· · · " terms represent constants that do not depend on the choice of  .
   Thus, when using the appropriate tuning parameter,

                                                           i = se 2  2
                                                                  i / ,                                           (13)

estimating a univariate Ridge regression--i.e., solving the optimization problem in Equation
(10)--is equivalent to finding the most likely estimate for i given both your prior beliefs and the
observed data--i.e., minimizing the negative posterior log likelihood in Equation (12). Since this
fully Bayesian estimate will have the lowest out-of-sample prediction error on average, we can use
this connection to turn your past experience with (I - 1) other variables into (I - 1) different
variable-specific estimates for  2 by inverting the best-fit .
    Our choice of penalized-regression procedures--namely, a Ridge regression--is based on the
assumption that i is normally distributed. A different choice of prior distributions would suggest
using a different kind of penalized regression. e.g., if i were drawn from a Laplace distribution,

                                                                 13
then the most efficient way to estimate  2 would be to use the LASSO (Park and Casella, 2008).
Such an approach would result in a slightly different functional relationship between the best-fit i
and  2 than reported in Equation (13). However, the same core economic intuition would still apply.
We demonstrate this fact below using both numerical simulations (see Section 2.3) and analytical
arguments (see Appendix B).
    Standard Error. i might be large for either of two reasons. First, the anomaly base rate
might be really low. If it is very unlikely that a researcher discovers a new cross-sectional predictor,
then  2 must be tiny and you should heavily discount any empirical evidence that suggests
otherwise for the ith variable. Second, you might not be very confident in your estimated slope
coefficient for the ith variable. If the variance of your estimated slope coefficient is quite large after
seeing data on the cross-section of (N + 1) excess returns, se 2 i    0, then you should also place less
weight on any estimated |     ^i |    0.
                                    2
    So, in order to learn about  by inverting the best-fit tuning parameter for the ith variable, we
have to be able to estimate se 2    i separately for each variable. We can do this using OLS:


                             N · se 2      1
                                                   ·       Rn - µ
                                                                ^-^i · Xn,i 2 .
                                    i =   N -1         n                                            (14)

We use this OLS-based approach to estimating se 2     i when searching for the best-fit tuning parameter
associated with each variable i = 1, . . . , I . To repeat: we estimate se 2
                                                                           i separately for each variable.

    In-Sample Overfitting. What exactly does it mean to choose the `best-fit tuning parameter'
though? One approach you could take would be to choose the tuning parameter i > 0 that
best fits the data you have observed for each variable in-sample. Let Erri [] denote a Ridge
regression's expected in-sample prediction error when using a particular value of  given the realized
cross-section of excess returns in a particular month and lagged values of the ith variable:

                                                                             2
                           Err[|R, Xi ] = E
                                             def
                                                       Rn - µ
                                                            ^-^i [] · Xn,i        .                 (15)

This is called the `training error' (Hastie et al., 2001). We will typically write Err[|R, Xi ] = Erri [].
    Unfortunately, this simple approach is too simple. No matter what the true value of  2 is, the
training error will always be minimized by setting i = 0. In other words, an OLS regression will
                                                                                          2
always outperform a Ridge regression in sample according to this metric. If v            ~i denotes the
parameter estimate with the minimum in-sample prediction error for the ith variable,

                                                  Erri [se 2
                                      2def                     2
                                    ~i
                                    v   = arg min
                                              2            i /v ] ,                                 (16)
                                                   v >0

                      2
then we have that E[~vi ] =  regardless of which  2 > 0 was used to generate the data.
   Minimizing the training error requires fine-tuning the slope coefficient to explain variation in

                                                           14
excess returns coming from in-sample noise. And, this is easiest when there is no penalty for doing
so--i.e., when i = 0; or equivalently, when v       2
                                                   ~i = . Put another way, the ratio i = se 2      i /
                                                                                                       2

governs the relative likelihood that a statistically significant estimate for the slope coefficient,
|^i |   0, is due to in-sample overfitting rather than the existence of an honest-to-goodness tradable
                                         2
anomaly, i = 0. But, the estimator v   ~i  does not reflect this comparison. It only reveals whether the
in-sample fit is good; it does not tell you anything about the origins of this good performance.
    Econometric Estimator. If we want a consistent estimator for  2 , then we need to adjust
the metric used above so that it does not reward in-sample overfitting. Large values of  2 will result
in lots of in-sample overfitting, so these choices should receive a large penalty; whereas, small values
of  2 will result in little in-sample overfitting, so these choices should receive a small penalty.

Proposition 2.2 (Econometric Estimator). Let E[·] denote an expectations operator evaluated
with respect to realizations of i , and let Erri [] denote the training error as specified in Equation
           2
(15). If v
         ^i  denotes the parameter estimate with the minimum training error subject to an overfitting
penalty for the ith variable,

                                      Erri [se 2                             · se 2
                          2def                     2              1
                        ^i
                        v   = arg min
                                  2            i /v ] + 2 ·         2
                                                               1+se i /v 2        i ,               (17)
                                  v >0


then for any  2 > 0 we have that E[^2
                                   vi ] = 2.

Each variable implicated in the anomaly-zoo literature represents one of many parallel situations. So,
                        2
after estimating the v ^i    associated with each variable i = i, we can then average these (I - 1)
variable-specific estimates to form a prior for use when evaluating evidence about the ith variable.
    Note that the econometric estimator described in Proposition 2.2 follows from the same basic
intuition as many other information-theoretic model-selection criteria, such as the Akaike
information criterion (AIC; Akaike, 1974). Such procedures minimize a mean squared-error loss
function plus an additional penalty proportional to the number of degrees of freedom, df, times the
noise variance, Var[n,i ]. In fact, one way to derive the penalty function in Equation (17) is to note
that the effective degrees of freedom in a univariate Ridge regression is given by 1/(1 + ). Thus,
since Var[n,i ] = N · se 2i , the Akaike penalty function, 2 · (df /N ) × Var[n,i ], reduces to the one in
Equation (17) when i = se 2         2
                                i / . By the same logic, Stone (1977) shows that using Proposition 2.2
to estimate  2 will deliver estimates that are asymptotically equivalent to those of cross-validation.
And, we verify this claim in the simulation analysis below.
    Forecasting vs. Learning. We have just outlined a univariate approach to estimating  2 .
In essence, we are asking: `How can you separately use the information in each of the other (I - 1)
variables to learn about  2 ?' We are not looking for the best combination of variables. But, that is
not to say looking for the best combination is wrong. It just depends on what you are after. Taming

                                                   15
the factor zoo (Feng et al., 2017) will mean different things to different people.
    If all you care about is making good forecasts, then taming the factor zoo will mean looking for
the best combination of variables. You should use something like principle-component analysis to
collapse the information in all I variables down into a single forecasting variable (Kelly et al., 2017).
But, if you want to use these same variables to learn how to make actionable predictions, then you
do not want to combine variables. This would throw away valuable information and make it harder
for you to learn.
    Consider an example. Driving a car is a complicated activity; there are lots of different factors
involved. It has taken quite a while for engineers to teach a computer how to do it.2 But, it is
nevertheless easy to forecast who will be a good driver. You can collapse all the information in the
zoo of driving-related factors into a single variable: IsTeenager  {True , False } (Jonah, 1986). If
you are an insurance underwriter trying to forecast future claims, this single variable effectively
tames the factor zoo. But, if you are a Google engineer, it does not. You cannot create a self-driving
car by not installing a 17-year-old operating system.

2.3       Simulation Analysis
A researcher's past experience with other variables in the anomaly zoo contains information about
the efficacy of financial economists' research practices. So, it is possible to turn these many parallel
signals about methodological efficacy into an anomaly base rate for use when evaluating the next
candidate predictor. We apply empirical-Bayes methods to formalize this reasoning. We then encode
the efficacy of our shared research practices as a variance,  2 , and use the Bayesian interpretation of
the Ridge regression to extract information about this variance from other variables.
    This is the first paper to propose a way of estimating the anomaly base rate. We are not
developing empirical-Bayes methods from scratch or pointing out the Bayesian interpretation of
univariate Ridge regressions for the first time. Instead, our core insight has to do with how to use
these ideas to choose the right priors. We have simplified our statistical approach as much as
possible to highlight the economic intuition guiding our analysis. Modeling assumptions, like having
the true slope coefficients being drawn iid normal, are there for pedagogical reasons. They are not
central to our analysis. And, we now run a variety of numerical simulations to demonstrate the
robustness of our statistical approach.
      Simulation Setup. We simulate data from the following generalization of Equation (6):

                                                iid
                                            i  Dist[µ,  2 ].                                       (18)

In the analysis below, we vary both the distribution function--i.e., the functional form of
2
    Wired. 12/13/2018. The Wired Guide to Self-Driving Cars.


                                                      16
Dist[·]--as well as the mean and variance of the i distribution--i.e., the parameter values µ and
 2 . In addition, we also verify that our results using a regularized maximum-likelihood approach as
outlined in Proposition 2.2 closely match the results when using a cross-validation approach.
     Unless otherwise stated, all simulations will use the data-generating process in Equation (7) for
a cross-section of (N + 1) = 1,000 excess returns where µ = 0% per month and se i = 1% per
month. Everything will be the same as in our main statistical framework except for the fact that the
true slope coefficients will be drawn as in Equation (18). When studying the effects of changing
                                                               2
either µ or Dist[·], we will compare the average estimated v ^i  in 104 simulations to the true  2 used
to generate the data. When studying the effects of changing  2 , we will compute the average fit of
the econometric estimator outlined in Proposition 2.2 across the entire range of choices for v 2 :

                           Fiti [v 2 ] = Erri [se 2                            · se 2
                                     def              2             1
                                                  i /v ] + 2 ·        2
                                                                 1+se i /v 2        i.           (19)

Note that in each simulation we will be separately estimating se 2
                                                                 i every time we estimate v
                                                                                          ^i2
                                                                                              , just
like we will have to when studying real-world data.
    Benchmark Simulation. We begin with a benchmark simulation to confirm that our
statistical approach recovers the correct value of  2 when we simulate data drawn from our
hypothesized anomaly-discovery process in Equation (6), Dist[·] = Normal[·] and µ = 0% per
month. We study three different regimes,  2  {0.25%, 1.00%, 4.00%}.  2 = 4.00% denotes a
regime where tradable anomalies are likely;  2 = 0.25% denotes a regime where tradable anomalies
are unlikely; and,  2 = 1.00% denotes a regime somewhere in between. Each panel in Figure 3
reports the average fit at a range of input values v 2  (0, 16] when the data are generated using a
particular prior variance,  2 . The figure confirms that, when we simulate data using a particular
value of  2 , the objective function in Proposition 2.2 is minimized at that value.
    Non-Zero Mean. Next, we explore the sensitivity of our statistical approach to the
assumption that µ = 0% per month. We do this by running simulations where Dist[·] = Normal[·],
 2 = 1%2 , and µ  [-0.3%, 0.3%]. The top panel of Figure 4 shows the results. Each black dot
                               2
reports the average value of v^i produced by our statistical approach in 104 simulations using a
particular choice for µ. We look at values of µ ranging from -0.3% per month to 0.3% per month.
The true prior variance in all these simulations is  2 = 1% per month, which is depicted by the
dashed red line in Figure 4. And, for any choice of µ  [-0.3%, 0.3%], our results are quite close to
this theoretical target.
    The inset figure shows that, if we extend the range of µ to ±1% per month, then the
econometric estimator in Proposition 2.2 will start to over-estimate  2 . The black dots are
the same as in the main figure. The white dots represent analogous calculations made using
µ  [-1.0%, - 0.3%)  (0.3%, 1.0%]. The tendency of our econometric estimator to over-estimate


                                                     17
                            2 = 0.25                 2 = 1.00                     2 = 4.00
               0.6                         0.5                         3
 Average Fiti [v 2 ] 

                                           0.4
               0.4                                                     2
                                           0.3
                                           0.2
               0.2                                                     1
                                           0.1
               0.0                  v2     0.0                         0
                        0.25   1.00 4.00         0.25    1.00   4.00         0.25    1.00    4.00
  Figure 3. Benchmark Simulation. Results for 104 simulations of a market with
  (N + 1) = 1,000 stocks. For each simulation, we generate a new realization of the cross-section of
                                                    iid                         iid
  excess returns using the parameters µ = 0, i  Normal[0,  2 ], and n,i  Normal[0, N · 1%2 ].
  y -axis: average Fiti [v 2 ] at each value of v 2  (0, 16]. x-axis: v 2 on a log scale. Each panel
  reports results for data simulated using  2  {0.25%, 1.00%, 4.00%}. Large dot: v 2 value that
  minimize each curve, which corresponds to estimate defined by Proposition 2.2.


 2 when µ = 0 makes intuitive sense. The econometric estimator outlined in Proposition 2.2
assumes that µ = 0. So, it reads any deviation of i from zero as evidence of high prior variance,
even if the deviations are due to a non-zero mean.
    Using an econometric estimator that is based on a misguided assumption about the true
data-generating process is not innocuous; however, the top panel in Figure 4 suggests that the
anomaly-discovery process would have to be implausibly efficient to materially affect our estimation
results. If µ = 0.30% per month, then the long-short portfolio associated with a randomly selected
variable i = 1, . . . , I would have excess returns of 30 bps per month. This is too large an amount of
out-of-sample cross-sectional return predictability for the average variable (Harvey et al., 2016;
McLean and Pontiff, 2016; Linnainmaa and Roberts, 2018), especially since the sign of i can be
both positive and negative.
    Non-Gaussian Prior. Section 2.2 shows how it is possible to use a univariate Ridge
regression to learn about  2 when the true slope coefficients are drawn from a normal distribution.
We just saw that allowing this normal distribution to have a non-zero mean is unlikely to affect our
statistical approach in real-world settings. But, what if the true slope coefficients are not even being
drawn from a normal distribution to begin with? In particular, if the vast anomaly zoo only
contains a small handful of tradable anomalies, then you might expect the i distribution to have
fat tails. A few variables have |i |       0 while the rest have |i |  0.
    We address this concern in the bottom panel of Figure 4 by showing that our statistical
approach is still able to recover the correct value of  2 when this parameter is the variance
of a non-Gaussian prior distribution. In other words, this simulation exercise varies the i


                                                    18
         1.5
            




         1.0
          2
        ^i




                                                                                                      2 = 1.0
Average v




                          2.0
                          1.5
         0.5              1.0
                          0.5
                          0.0
                                -1.0     -0.3 0.0 0.3       1.0
         0.0                                                                                                       µ
                   -0.3                -0.2             -0.1            0.0              0.1       0.2             0.3


                                Normal                             Laplace                       Student's t
               4
               2
          2
            




               1
        ^i
Average v




           0.5                                                                                                     i
                                        i =  · i                          i =  · i                       i =  ·    
                                                                                                                   3
        0.25               i  Normal[0, 1]                        i  Laplace[1/ 2]
                                                                                      
                                                                                                 i  StudentT[3]

          2           0.25 0.5          1     2         4      0.25 0.5       1   2      4     0.25 0.5    1   2       4

Figure 4. Estimator Robustness. Top Panel: Robustness to having a non-zero mean.
                                                     2
Each dot represents the average estimated v        ^i  across 104 simulations of a market with
(N + 1) = 1,000 stocks. For each simulation, we generate a new realization of the cross-section
                                                                           iid
of excess returns as in Equation (7) using the parameters µ = 0, i  Normal[µ, 1%2 ], and
       iid
n,i  Normal[0, N · 1%2 ]. x-axis: average strength of true slope coefficients µ  [-0.3%, 0.3%]
per month in Equation (18). Dashed red line is true prior variance used to generate the
data,  2 = 1%2 . Inset: identical figure for µ  [-1%, 1%]. Bottom Panel: Robustness
                                                                                     2
to having a non-Gaussian prior. Each dot represents the average estimated v         ^i across 104
simulations of a market with (N + 1) = 1,000 stocks. For each simulation, we generate a new
realization of the cross-section of excess returns as in Equation (7) using the parameters µ = 0,
     iid                  iid
i  Dist[·], and n,i  Normal[0, N · 1%2 ]. x-axis: prior variance used to simulate the
data,  2 , on a log scale. Left: true slope coefficients drawn from a normal distribution,
   iid
i  Normal[0, 1] and i =  · i . Middle: true slope coefficients drawn from a Laplace
                  iid             
distribution, i  Laplace[1/ 2] and i =  · i . Right: true slope coefficients drawn from a
                              iid
Student's-t distribution, i  StudentT[3] and i =  · i . If statistical approach is successful,
black dots sit on 45 dashed red line.




                                                                   19
distribution used in Equation (18). Each panel reports results for a different prior distribution,
Dist[·]  {Normal, Laplace, Student's-t}.
    The left chart shows results when data are generated using Dist[·] = Normal[·] just like in
our primary analysis. Each black dot represents an average taken over 104 simulations using
(N + 1) = 1,000, µ = 0% per month, and  2  [0.25, 4.00]. The x-axis represents the prior variance
used to simulate the data,  2 , while the y -axis reports the average estimated v 2
                                                                                 ^i . Thus, if our
statistical approach is successful, all the black dots should organize themselves along the 45
dashed red line. And, this is exactly what happens, providing further confirmation of the earlier
benchmark-simulation results in Figure 3 at a wider range of input values.
    The middle chart is different. Rather than drawing i from a normal distribution, this chart
shows analogous results when i is sampled from a Laplace distribution:

                                        i =  · i
                                           iid                                                  (20)
                                         i  Laplace[1/ 2].

Using a Laplace distribution for i is one way to capture the idea that the distribution of true slope
                                                                                        
coefficients is fat tailed. A Laplace distribution with shape parameter equal to 1/ 2 has unit
variance. So, to make sure that i has the appropriate variance when simulating the data, we first
           iid           
sample i  Laplace[1/ 2] and then multiply the result by the desired prior volatility, i =  · i .
Just like with the left chart, the middle chart depicts black dots that neatly line up along the 45
dashed red line, which confirms that our statistical approach is able to recover the correct prior
variance even when i is drawn from a non-Gaussian distribution.
    An alternative way of capturing fat tails is to use a Student's-t distribution:

                                        i =  · i
                                            iid
                                                                                                (21)
                                         i  StudentT[3].

A Student's-t distribution with 3 degrees of freedom has unit variance. So, to make sure that i has
                                                                             iid
the appropriate variance when simulating the data, we first sample i  StudentT[3] and then
multiply the result by the desired prior volatility, i =  · i . Just as before, the right chart in the
bottom panel of Figure 4 depicts black dots that neatly line up along the 45 red dashed line.
    In this paper, we are assuming that the i distribution is Gaussian for pedagogical reasons.
Every financial economist is familiar with this distribution, and the assumption makes a clear
prediction about how to we should infer  2 from the best-fit tuning parameter in a Ridge regression.
The bottom panel of Figure 4 shows that this statistical approach is robust to mis-specifying the
actual i distribution. However, if you strongly believe that i is drawn from a non-Gaussian
distribution, it is possible to improve the efficiency of the econometric estimator in Proposition 2.2


                                                  20
                            2 = 0.25                 2 = 1.00                             2 = 4.00
               0.6                         0.5                                    3
 Average Fiti [v 2 ] 

                                           0.4




                                                                              d
               0.4




                                                                          ate
                                           0.3                                    2




                                                                      lid

                                                                        d
                                                                    ize
                                                                 -Va
                                           0.2
               0.2




                                                                lar
                                                                                  1




                                                             oss
                                                             gu
                                           0.1




                                                          Cr
                                                          Re
               0.0                  v2     0.0                                    0
                        0.25   1.00 4.00         0.25    1.00      4.00               0.25   1.00   4.00
   Figure 5. Cross-Validation. Results for 104 simulations of a market with (N + 1) = 1,000
   stocks. For each simulation, we generate a new realization of the cross-section of excess returns
                                         iid                      iid
   using the parameters µ = 0, i  Normal[0,  2 ], and n,i  Normal[0, N · 1%2 ]. Solid black
   line: average regularized Fiti [v 2 ] at each value of v 2  (0, 16]. Dashed red line: average
   cross-validated Fiti [v 2 ] at each value of v 2  (0, 16]. x-axis: v 2 on a log scale. Each panel
   reports results for data simulated using  2  {0.25%, 1.00%, 4.00%}. Large black dot: v 2 value
   that minimize each regularized curve, which corresponds to estimate defined by Proposition 2.2.
   Large red/white triangle: v 2 value that minimize each cross-validated curve. The cross-validated
   analogue to Fiti [v 2 ] is the test-sample mean squared error (MSE) of   ^i [] where  is chosen to
   have the lowest training-sample MSE. The training/test samples are chosen via a 10-fold scheme.


by adjusting the penalty function (see Appendix B). e.g., Park and Casella (2008) show that using
the LASSO is tantamount to adopting a Laplace prior for i . But, the basic insight underpinning
both empirical and objective Bayesian thinking is the same.
    This connection suggests an alternative economic interpretation of the results in this paper. It is
now common for financial economists to employ various kinds of penalized-regression procedures.
These tools are also widely used by execution desks. In both cases, the OLS-regression procedure is
being modified in order to control for false positives, data mining, p-hacking, etc. . . The standard
view is that penalized-regression procedures are a statistical tool for dealing with the proliferation of
variables. And, both empirical researchers and execution desks choose the penalty function that best
deals with the anomaly zoo in the setting they are currently facing. But, "any sensible estimator is
Bayesian for some prior (Diaconis and Skyrms, 2017)." So, if an empirical researcher or execution
desk comes to the conclusion that it is best to use a Ridge regression or the LASSO when pruning
away weak predictors, this says something important about the economics of anomaly discovery,
either in academia or at the research desk.
    Cross-Validation. As we point out above, Stone (1977) shows that using a regularized
maximum-likelihood estimator, such as the one in Proposition 2.2, should be asymptotically
equivalent to cross-validation in our setting. And, Figure 5 verifies that this is indeed the case. The
solid black line in each panel corresponds to the results from Figure 3, which reports the average


                                                    21
Fiti [v 2 ] at each value of v 2  (0, 16] computed across 104 simulations. The dashed red line then
replicates this analysis using a cross-validated analogue to Fiti [v 2 ]. The cross-validated analogue to
Fiti [v 2 ] is the test-sample mean squared error (MSE) of  ^i [] where  is chosen to have the lowest
training-sample MSE. The training/test samples are chosen via a 10-fold scheme. The red triangle
corresponds to the value of v 2 that minimizes the cross-validated fit. The fact that this red triangle
sits right on top of the value produced by the regularized econometric estimator in Proposition 2.2
shows that these two kinds of estimators produce identical results.


3     Estimation Results
Having described our statistical approach to estimating the anomaly base rate, we next apply it to
recover  2 each month. We use data on a collection of 85 different variables that were published in
the academic literature some time after May 1973.

3.1     Data Description
We begin by describing the data and variables we use in our analysis.
     Data Sources. We study the cross-section of monthly returns from May 1973 to June 2015
for each U.S. stock traded on either the NYSE, Amex, or NASDAQ. These data come from the
Center for Research in Security Prices (CRSP) monthly stock file. To make sure that our results are
not being driven by small illiquid stocks, we exclude any stock with a price below $1 at the end of
the previous month. However, our empirical results are robust to filtering the data based on other
filters as well (see Appendix C, Table C1). We use balance-sheet data from the Standard and Poor's
Compustat database. All items are taken from the fiscal year ending in calendar year (y - 1) for
estimation starting in June of year y until May of year (y + 1) predicting returns from July of year
y until June of year (y + 1). To alleviate a potential survivorship bias due to back-filling, we require
that a firm has at least two years of Compustat data for it to be included in our sample. Let
(Nt + 1) denote the number of stocks in our sample in month t.
    Return Predictors. We use a collection of 85 different variables that were first documented
in the academic literature sometime on or after May 1973. We list each variable along with its
publication date in Tables 1a, 1b, and 1c. Let It denote the set of variables discovered prior to
month t:
                        def
                     It = i  I : publication date for ith variable < t .                     (22)
            def
And, let It = |It | denote the number of variables discovered prior to month t. So, looking at the
first four rows of Table 1a, we have IMay73 = 0, IJun73 = 3, . . . , IJun77 = 3, and IJul77 = 4.
    Slope Coefficients. Each month t, we compute the realized returns to a zero-cost strategy



                                                   22
         Name                  P. Date     Description
   1.    Beta                  1973-05     Rolling CAPM beta
   2.    BetaSq                1973-05     Rolling CAPM beta, squared
   3.    IdioVolCAPM           1973-05     Idiosyncratic volatility, CAPM
   4.    Earn/Share            1977-06     Earnings per share
   5.    Debt/Price            1979-06     Debt to price
   6.    Divd/Price            1979-06     Dividend to price
   7.    Mcap                  1981-03     Market capitalization, prev. fiscal year
   8.    Earn/Price            1982-08     Earnings to price
   9.    Ret, 36-12            1985-07     Cum. return, months [-36, - 12)
  10.    AvgSpread             1986-12     Mean bid-ask spread
  11.    Assets/Mcap           1988-06     Assets to market cap
  12.    Levrg                 1988-06     Leverage
  13.    Levrg/Price           1988-06     Leverage to price
  14.    Sales
             /Cash             1989-11     Sales to cash
  15.    LtCF                  1989-11     Long-term cash flow
  16.    CurrRatio             1989-11     Current ratio
  17.    %CurrRatio            1989-11     Perc. change in current ratio
  18.    %QuickRatio           1989-11     Perc. change in quick ratio
  19.    %[Sales/Invtry]       1989-11     Perc. change in sales to inventory
  20.    QuickRatio            1989-11     Quick ratio
  21.    Sales
             /Invtry           1989-11     Sales to inventory
  22.    Sales
             /Recv             1989-11     Sales to receivables
  23.    Ret, 1-0              1990-07     Return, month [-1, 0)
  24.    Ret, 12-1             1990-07     Cum. return, months [-12, - 1)
  25.    BkVal                 1992-06     Book value
  26.    MonthlyMcap           1992-06     Market cap, prev. month
  27.    Sales
             /Price            1992-06     Sales to price
  28.    %[Deprc/PP&E]         1992-09     Perc. change in depreciation to PP&E
  29.    D&A  /Assets          1992-09     D&A to assets

Table 1a. List of Variables. List of variables documented in the academic literature
sometime on or after May 1973. Variables are constructed using data from CRSP and
Compustat. Name: The name for the variable used throughout this paper. P. Date: The month
of publication for the first academic paper about each variable. Description: A description of
how variable is constructed for each stock.




                                             23
         Name                  P. Date     Description
  30.    Deprc
             /PP&E             1992-09     Depreciation to PP&E
  31.    Ret, 6-1              1993-03     Cum. return, months [-6, - 1)
  32.    %Sales                1994-12     Perc. change in sales
  33.    OpAccr                1996-07     Operating accruals
  34.    CapitalTOver          1996-07     Capital turnover
  35.    RetOnEquity           1996-07     Return on equity
  36.    KaplanZingales        1997-02     Kaplan-Zingales index
  37.    %[Sales/Invtry]       1997-04     Perc. change in Sales to Inventory
  38.    %[Sales/Recv]         1997-04     Perc. change in Sales to Receivables
  39.    %[Sales/XG&A]         1997-04     Perc. change in Sales to XG&A
  40.    %[GrMgn/Sales]        1998-01     Perc. change in Gross margin to Sales
  41.    LagTOver              1998-08     Lagged turnover
  42.    Adj[BkVal/Mcap]       2000-02     Ind. adjusted book-to-market ratio
  43.    AdjMcap               2000-02     Ind. adjusted market cap
  44.    SdTOver               2001-01     Std. deviation of daily turnover
  45.    AdvertRate/Ret        2001-12     Advertising expense rate to returns
  46.    R&D /Mcap             2001-12     R&D to market cap
  47.    R&D /Sales            2001-12     R&D to sales
  48.    Advert/Mcap           2001-12     Advertising expense to market cap
  49.    Invtry /Assets        2002-06     Inventory changes to assets
  50.    OpCF /Price           2004-04     Operating cash flow to price
  51.    Invmt/Lag[AvgInvmt]   2004-12     Investment to trailing 3 years average
  52.    NetOpAssets /Sales    2004-12     Net operating assets to lagged sales
  53.    %BkVal                2005-09     Perc. change in book value
  54.    %LtDebt               2005-09     Perc. change in long-term debt
  55.    Price-52WkHi          2005-11     Closeness to previous 52-week high
  56.    IdioVolFF93           2006-02     Idiosyncratic volatility, FF93
  57.    TotVol                2006-02     Total volatility

Table 1b. List of Variables, Ctd. List of variables documented in the academic literature
sometime on or after May 1973. Variables are constructed using data from CRSP and
Compustat. Name: The name for the variable used throughout this paper. P. Date: The month
of publication for the first academic paper about each variable. Description: A description of
how variable is constructed for each stock.




                                             24
         Name                  P. Date     Description
  58.    %Mcap                 2006-08     Residual perc. change in market cap
  59.    NetExtnlFin/Assets    2006-10     Net external financing to assets
  60.    DailyBeta             2006-11     Daily rolling CAPM beta
  61.    NetPO/Price           2007-04     Net payouts to price
  62.    PO/Price              2007-04     Payouts to price
  63.    NetPO                 2007-04     Net payout ratio
  64.    RetOnInvstCap         2007-06     Return on invested capital
  65.    %Shares               2008-04     Perc. change in shares outstanding
  66.    ProfMgn               2008-05     Profit margin
  67.    AdjProfMgn            2008-05     Ind. adjusted profit margin
  68.    RetOnOpAssets         2008-05     Return on net operating assets
  69.    AssetTOver            2008-05     Asset turnover
  70.    AdjAssets             2008-05     Ind. adjusted total assets
  71.    %InvmtX               2008-07     Perc. change in investments (Xing)
  72.    %Invmt                2008-08     Perc. change in investments
  73.    AdjShares             2008-08     Change in split-adjusted shares outstanding
  74.    RetOnCash             2009-01     Return on cash
  75.    Tangibility           2009-04     Asset tangibility
  76.    AdjTOver              2009-10     Change in market-adjusted turnover
  77.    UnexplVlm             2009-10     Standardized unexplained volume
  78.    RetOnAssets           2010-05     Return on assets
  79.    OpLevrg               2011-01     Operating leverage
  80.    MaxRet                2011-02     Max monthly return during prev. year
  81.    FreeCF                2011-05     Free cash flow
  82.    R&Dcapital            2011-09     R&D capital
  83.    %Invtry               2012-01     Perc. change in inventory
  84.    Ret, 12-6             2012-03     Cum. return, months [-12, - 6)
  85.    CashHldgs             2012-04     Cash holdings

Table 1c. List of Variables, Ctd. List of variables documented in the academic literature
sometime on or after May 1973. Variables are constructed using data from CRSP and
Compustat. Name: The name for the variable used throughout this paper. P. Date: The month
of publication for the first academic paper about each variable. Description: A description of
how variable is constructed for each stock.




                                             25
  85

  64
                                                                          It
  43

  22

   1
       `75         `80         `85         `90         `95          `00          `05         `10      `15
  Figure 6. Number of Variables. Number of variables discovered prior to month t,
  It = |It |, as defined in Equation (22). Sample Period: June 1973 to June 2015.


based on each variable i  It by running a separate cross-sectional OLS-regression:

               Rn,t = µ
                      ^t + ^i,t · Xn,i,t-1 + ^n,i,t        for each variable i = 1, . . . , It .       (23)

Rn,t is the excess return of the nth stock in month t, µ    ^t is the cross-sectional average excess return
for all stocks in our sample during month t, Xn,i,t-1 is the value of the ith variable for stock n in
the previous month normalized to have mean zero and unit variance in the cross-section,           ^i,t is the
OLS-regression coefficient for the ith variable in month t, and           ^n,i,t is the regression residual.
      Table 2 provides summary statistics describing these realized returns. There are two things
about this table that are worth pointing out. First, contrarian strategies, such as the long-term
reversals captured by the variable `Ret, 36-13' in row five, will result in estimated values that are
negative on average. This is consistent with the modeling assumption that the true i,t values are
drawn from a mean-zero normal distribution. When we incorporate these sorts of variables in a
trading strategy, we will always trade in the appropriate direction. However, the sign of the slope
coefficient associated with a cross-sectional long-short strategy is arbitrary.
      Second, our estimates for each variable's      ^i,t tend to be smaller than the ones reported in
the original papers. This is because researchers typically report the excess returns to sorted
high-minus-low portfolios. This standard practice is tantamount to running a cross-sectional
regression where Var[Xn,i,t-1 ] < 1. For example, going long the top 10% of stocks and short the
                                                               1             1
bottom 10% of stocks corresponds to Var[Xn,i,t-1 ] = 10           · (+1)2 + 10   · (-1)2 = 1/5. And, since
 ^i,t = Cov[Rn,t , Xn,i,t-1 ]/Var[Xn,i,t-1 ], this approach would result in point estimates that are five
times larger than ours. We will adjust for this difference in the variance of the right-hand-side
variables when we consider the effects of implementation costs before combining variables into a
single strategy in Section 4 below.



                                                      26
                                    Avg       Sd                                     Avg      Sd                                Avg     Sd
     Beta                           0.00     2.21 Deprc/PP&E                         0.22 1.76 %Mcap                            0.16   1.22
     BetaSq                         0.01     2.21 Ret, 6-1                          -0.16 2.36 NetExtnlFin/Assets               0.00   0.52
     IdioVolCAPM                    0.70     2.88 %Sales                            -0.29 0.84 DailyBeta                        0.06   1.79
     Earn/Share                    -0.31     2.15 OpAccr                             0.01 0.97 NetPO/Price                     -0.15   1.17
     Debt/Price                     0.10     1.48 CapitalTOver                      -0.17 0.67 PO/Price                        -0.16   0.99
     Divd/Price                    -0.14     1.74 RetOnEquity                       -0.44 1.94 NetPO                           -0.01   0.53
     Mcap                          -0.55     1.69 KaplanZingales                    -0.03 0.94 RetOnInvstCap                   -0.28   1.04
     Earn/Price                    -0.21     1.91 %[         Sales/Invtry]           0.07 0.49 %Shares                          0.14   0.36
     Ret, 36-12                    -0.56     1.63 %[Sales/Recv]                      0.11 0.52 ProfMgn                          0.07   0.52
     AvgSpread                      0.59     1.68 %[Sales/XG&A]                     -0.10 0.56 AdjProfMgn                      -0.49   1.34
     Assets/Mcap                    0.25     1.87 %[GrMgn/Sales]                    -0.03 0.59 RetOnOpAssets                   -0.01   1.02
     Levrg                         -0.05     1.42 LagTOver                          -0.04 2.44 AssetTOver                      -0.15   0.67
     Levrg/Price                    0.05     1.63 Adj[BkVal/Mcap]                    0.39 0.90 AdjAssets                        0.17   0.97
     Sales
         /Cash                      0.14     1.97 AdjMcap                           -0.31 1.43 %InvmtX                         -0.17   0.47
     LtCF                          -0.05     1.37 SdTOver                            0.04 1.73 %Invmt                           0.09   0.95
     CurrRatio                     -0.20     0.57 AdvertRate/Ret                     0.31 1.02 AdjShares                       -0.41   0.85
27




     %CurrRatio                    -0.20     0.58    R&D   /Mcap                     0.47 1.69 RetOnCash                       -0.05   0.87
     %QuickRatio                    0.08     0.45 R&D/Sales                          0.12 2.00 Tangibility                      0.00   0.79
     %[Sales/Invtry]               -0.03     1.62 Advert/Mcap                        0.64 1.20 AdjTOver                         0.14   0.39
     QuickRatio                    -0.01     1.45       Invtry/Assets               -0.15 0.52 UnexplVlm                        0.24   0.55
     Sales
         /Invtry                   -0.08     0.85 OpCF/Price                         0.01 1.01 RetOnAssets                     -0.25   0.96
     Sales
         /Recv                      0.08     0.72 Invmt/Lag[AvgInvmt]               -0.08 0.43 OpLevrg                          0.15   0.55
     Ret, 1-0                      -0.45     2.04 NetOpAssets/Sales                 -0.07 0.89 MaxRet                           0.07   1.06
     Ret, 12-1                     -0.17     2.40 %BkVal                            -0.26 0.70 FreeCF                          -0.10   0.89
     BkVal                          0.39     1.45 %LtDebt                           -0.11 0.40 R&Dcapital                       0.31   1.30
     MonthlyMcap                   -0.82     1.93 Price-52WkHi                      -0.46 2.33 %Invtry                         -0.07   0.42
     Sales
         /Price                     0.38     1.56 IdioVolFF93                        0.31 1.66 Ret, 12-6                       -0.10   0.69
     %[Deprc/PP&E]                  0.00     0.46 TotVol                             0.28 1.83 CashHldgs                        0.10   0.99
     D&A  /Assets                   0.32     1.15
                       Table 2. Estimated        ^i,t . Summary statistics describing the realized returns to a zero-cost
                       trading strategy based on each variable i = 1, . . . , It . These realized returns correspond to the
                       estimated  ^i,t in Equation (23). Sample Period: June 1973 to June 2015. Units: % per month.
                       Sparkline Plots: time series for each variable on a common scale, -3% <          ^i,t < 3%. All plots
                       end in June 2015. Variables discovered later have shorter plots. Red indicated negative returns.
                                     Avg      Sd                                   Avg      Sd                                 Avg     Sd
                                                    Deprc                                            %Mcap
     Beta                            3.40    4.79       /PP&E                      2.07    3.97                                1.41   2.46
     BetaSq                          3.40    4.79   Ret, 6-1                       2.32    4.22   NetExtnlFin/Assets           0.23   0.52
     IdioVolCAPM                     4.12    5.30   %Sales                         0.74    1.45   DailyBeta                    2.15   3.73
     Earn/Share                      2.85    4.32   OpAccr                         0.87    1.80   NetPO/Price                  1.32   2.75
     Debt/Price                      1.71    3.34   CapitalTOver                   0.42    1.10   PO/Price                     0.93   1.79
     Divd/Price                      2.40    3.80   RetOnEquity                    2.40    4.19   NetPO                        0.22   0.50
     Mcap                            2.34    3.87   KaplanZingales                 0.78    1.82   RetOnInvstCap                1.10   2.27
     Earn/Price                      2.35    3.92   %[Sales/Invtry]                0.20    0.60   %Shares                      0.11   0.18
     Ret, 36-12                      1.98    3.78   %[Sales/Recv]                  0.24    0.85   ProfMgn                      0.22   0.37
     AvgSpread                       2.27    3.83   %[Sales/XG&A]                  0.27    0.45   AdjProfMgn                   1.92   3.18
     Assets/Mcap                     2.63    4.37   %[GrMgn/Sales]                 0.30    0.56   RetOnOpAssets                0.88   2.19
     Levrg                           1.54    3.19   LagTOver                       3.54    4.91   AssetTOver                   0.41   0.84
     Levrg/Price                     2.00    3.68   Adj[BkVal/Mcap]                0.92    1.74   AdjAssets                    0.84   2.00
     Sales
         /Cash                       2.39    4.16   AdjMcap                        1.27    2.95   %InvmtX                      0.21   0.32
     LtCF                            1.42    2.94   SdTOver                        2.10    3.45   %Invmt                       0.85   1.51
     CurrRatio                       0.31    0.84   AdvertRate/Ret                 0.85    2.08   AdjShares                    0.82   2.04
28




     %CurrRatio                      0.32    0.89   R&D /Mcap                      2.16    3.81   RetOnCash                    0.69   1.28
     %QuickRatio                     0.16    0.37   R&D /Sales                     2.53    4.25   Tangibility                  0.56   1.50
     %[Sales/Invtry]                 1.85    3.77   Advert/Mcap                    1.64    3.30   AdjTOver                     0.14   0.35
     QuickRatio                      1.62    3.45   Invtry /Assets                 0.25    0.49   UnexplVlm                    0.32   0.72
     Sales
         /Invtry                     0.63    1.72   OpCF /Price                    0.94    2.08   RetOnAssets                  0.93   1.91
     Sales
         /Recv                       0.48    1.02   Invmt/Lag[AvgInvmt]            0.15    0.30   OpLevrg                      0.28   0.45
     Ret, 1-0                        2.19    4.22   NetOpAssets /Sales             0.72    1.75   MaxRet                       1.07   1.75
     Ret, 12-1                       2.70    4.42   %BkVal                         0.51    1.67   FreeCF                       0.74   1.61
     BkVal                           1.84    3.34   %LtDebt                        0.13    0.24   R&Dcapital                   1.46   2.72
     MonthlyMcap                     2.70    4.41   Price-52WkHi                   2.93    4.55   %Invtry                      0.14   0.23
     Sales
         /Price                      2.00    3.69   IdioVolFF93                    1.95    3.53   Ret, 12-6                    0.43   0.74
     %[Deprc/PP&E]                   0.17    0.57   TotVol                         2.35    3.87   CashHldgs                    0.93   1.53
     D&A  /Assets                    1.29    2.69
                                                  2
                       Table 3. Estimated v     ^i,t . Summary statistics describing the variable-specific estimates for
                                                            2
                       the prior variance. We estimate v   ^i,t separately for each variable i = 1, . . . , It in month t as
                       described in Proposition 2.2. Sample Period: June 1973 to June 2015. Units: %2 per month.
                       Sparkline Plots: time series for each variable on a common scale, 0% < v  ^i,t < 6%. All plots end
                       in June 2015. Variables discovered later have shorter plots.
3.2     Anomaly Base Rate
Here is how we use this data to estimate the anomaly base rate. Our goal is to mirror the process
by which researchers implicitly convert their past experience with other variables in the anomaly zoo
into a prior for use next time around.
    Variable-Specific Estimates. We start by creating separate variable-specific estimates for
 2
 using each variable in the anomaly zoo as of month t. For each variable i = 1, . . . , It , we solve
the optimization problem outlined in Proposition 2.2 in each month t such that i  It . Table 3
                                                                                                           2
provides summary statistics describing these variable-specific estimates for the prior variance, v        ^i,t .
The sparkline plots represent the time series for each variable i = 1, . . . , 85 on a common scale,
0% < v  ^i,t < 6%. Every time series ends in June 2015, so shorter sparkline plots correspond to
variables that were discovered later in our sample period. The sparkline plots give evidence
that predictor strengths were drawn from a common distribution. Variables in our sample are
constructed in very different ways using entirely different datasets--e.g., compare Deprec/PP &E and
Ret , 6-1 in the top two rows of the center panel of Tables 2 and 3. Nevertheless, notice that these
                                                              2
two variables generate similar point estimates for v       ^i,t each month. And, this is true even though,
when you look at Table 2, the      ^i,t time series for each of these variables is very different. Trading on
Deprec/PP &E and Ret , 6-1 clearly yields a very different pattern of realized returns, and yet the
                  2
estimated v    ^i,t implied by each variable is quite similar.
    We next use the time series of prior-variance estimates associated with each variable to generate
out-of-sample one-month-ahead forecasts. After all, you want to convert your past experience into a
prior for future use. So, to make a forecast for the ith variable in month t, we study the 60-month
immediately prior to month t, {t - 60, . . . , t - 1}. We denote the resulting one-month-ahead
                                      2
forecasted value for month t by v   ¯i,t  to distinguish it from the in-sample estimates. We begin making
forecasts of the prior volatility implied by the ith variable only after we have observed 60 months of
post-publication data. e.g., for the rolling-CAPM-beta (row 1 in Table 1a), the first forecast is for
month t = Jun1978. We do this for the reasons related to publication bias discussed above. We
know that prior to publication the estimated              ^i 's are a combination of both the true slope
coefficient i and the effects of publication bias. By only using post-publication observations to
             2
forecast v  ¯i,t , we avoid this problem.
    To make each forecast, we fit an AR(3) model to these 60 observations prior to month t:

             2
           ^i,t  i +
                =a         3         ·v 2
                                      ^i,t              for months t = (t - 60), . . . , (t - 1).
           v                =1 bi,         - +e
                                              i,t                                                         (24)

i and {
a        bi, } =1,2,3 denote the coefficients associated with the ith variable, and ei,t represents the
regression residual in month t . Note that these coefficients will be different for each forecast date t;
we have just suppressed the t subscripts for clarity. We then compute one-month-ahead forecasts for

                                                      29
                       #Obs        Avg      Sd               1%             25%    50%    75%    99%
                         446      2.19      1.43            0.57            1.16   1.82   2.69   7.30
                              2
 Table 4. Aggregate v       ¯t  . Summary statistics describing the sample average of the rolling
                                                   2
 one-month-ahead forecast of prior variance, v   ¯t  . We separately compute in-sample estimates for
       2
 the v
     ^i,t associated with each variable i = 1, . . . , It in each month as described in Proposition 2.2.
                                                                     2
 Then, we make an out-of-sample one-month-ahead forecast, v         ¯i,t , by fitting an AR(3) model to
                                                                                         2
 months {t - 60, . . . , t - 1}. Finally, we average these forecasts to compute v      ¯t  as in Equation
                 2
 (26). Units: % per month. Sample Period: June 1978 to June 2015. Sparkline Plot: aggregate
 time series corresponding to Figure 1 in miniature.


the prior variance in month t by applying these estimated coefficients to the final three months of
data prior to month t:
                             v 2 def
                             ¯i,t = Et-1 [^2
                                          vi,t   i + 3=1 
                                               ]=a        bi, · v 2
                                                                ^i,t - .                      (25)

We use an AR(3) forecasting model in our primary analysis for convenience. Note that our results
are robust to using different numbers of lags as shown in Table C2 of Appendix C. The precise
choice of forecasting model is not key to our findings.
    Combining Results. The anomaly zoo represents a large data set of parallel situations, one
for each variable in the anomaly zoo as of month t, i = 1, . . . , It . The idea behind empirical-Bayes
methods is that you can combine the signal about  2 produced by each of these variables to
generate a more precise forecast:

                                            2def   1                 2
                                          ¯t
                                          v   =    It
                                                        ·    iIt   ¯i,t
                                                                   v    .                               (26)

                            2
This combined forecast, v  ¯t , represents your best guess about which prior variance of predictor
strengths to use when evaluating variables in month t--i.e., your best guess about the true value  2
to use in Equation (6)--based on your past experience with It other variables. So, as the number of
variables in the anomaly zoo grows, we will be getting more and more signals about the anomaly
base rate that we should be using going forward. By analogy, a seasoned researcher who has seen
many variables come and go will have more signals with which to inform his beliefs than a junior
researcher with less experience.
     To give a sense of the prior beliefs suggested by the data, Table 4 provides summary statistics
                                                   2                            2
describing this combined forecast each month, v  ¯t  , and Figure 1 plots the v
                                                                              ¯t  time-series. The yearly
average forecasted value for the prior variance peaked in the year 2000 at 12      1
                                                                                     · Dec  `00 2
                                                                                                ¯t = 5.89,
                                                                                         Jan`00 v
                                          1    Dec`15 2
and it reached its low point in 2015 at 12 · Jan`15 v  ¯t = 0.60. While Figure 1 reveals a large spike in
  2
¯t
v   at the time of the DotCom crash, this variable is not just a crash indicator. For example, note
                                                                                             2
that there is no spike in prior variance around the 1987 crash. We estimate that v         ¯Oct87  = 1.53,


                                                        30
                         VIX t       RVol t      RMkt,t      dGDP t        tSpd t     Rcsn t
                          (1)         (2)         (3)          (4)          (5)        (6)
               Avg      19.89       14.47         1.02         1.41         1.81       0.13
                Sd       7.59        8.70         4.48         0.89         1.15       0.33
            Start       Jan`90      Jun`78      Jun`78       Jun`78        Jan`82    Jun`78
            #Obs         306         446         446          446           402       446

 Table 5. Macroeconomic Variables. Summary statistics for macroeconomic forecasting
 variables. VIX t : level of the VIX index; Jan 1990 to Jun 2015. RVol t : realized volatility on
 value-weighted market index; Jun 1978 to Jun 2015. RMkt,t : return on the CRSP value-weighted
 market index; Jun 1978 to Jun 2015. dGDP t : log growth rate of seasonally adjusted GDP; Jun
 1978 to Jun 2015. tSpd t : the term spread; Jan 1982 to Jun 2015. Rcsn t : NBER indicator for
 whether a recession is taking place; Jun 1978 to Jun 2015.


which is much lower than the average value over the entire sample period, 2.19.
    Relaxing the independence assumption will only affect the precision of our estimates for  2 . To
                                                     2
see why, note that Proposition 2.2 says that E[^    vi ] =  2 for any i  It . The most extreme way to
violate the assumption of independent draws for i in Equation (6) would be to assume that the
realized values of i are the same for all i  It --i.e., to assume that all the draws were the same.
In this extreme case, we would effectively only have one signal about  2 . But, we could still use this
             2     2           2
lone draw, v^1 =v^2  = ··· = v
                             ^I t
                                  , as a signal about  2 . It would just be a much noisier signal. When
we analyze trading-strategy performance in the following section, we will focus our attention on the
sample period starting in January 1990 to minimize this concern.
    Macroeconomic Correlations. Of course, there are numerous macroeconomic variables
that forecast returns. And, in particular, there is a well-known relationship between expected
returns and both realized variance and uncertainty. So, it is important to make sure that, when we
            2
estimate v
         ¯t   , we are not just repackaging and rebranding some existing variable. We consider data
on six different alternative variables: the level of the VIX index, realized volatility, log growth rate
of seasonally adjusted GDP, the term spread, the NBER recession indicator, and the value-weighted
market return. Table 5 provides summary statistics for each of these macroeconomic variables. Note
that we do not have data on the VIX or the term spread for our entire sample period. In particular,
our data on the VIX begins in January 1990.
                                  2                               2       2           2
    We regress our forecasted v  ¯t in month t on lagged values v
                                                                ¯t -1 , v
                                                                        ¯t -2 , and v
                                                                                    ¯t -3 as well as values
of these other macroeconomic variables using the following specification:

                                 ¯t2
                                     =a
                                      ^+      3   ^   ·v
                                                       ¯t2
                                                             ^ Zt + e
                                 v             =1 b       - +c      ^t .                             (27)



                                                      31
                           2
     Dependent Variable: v
                         ¯t
              (1)      (2)          (3)       (4)       (5)       (6)       (7)       (8)       (9)      (10)      (11)      (12)
      Const     0.37      0.35      1.38      1.82      2.18     2.00      2.05      2.15       0.79      0.74      0.37      0.33
                (0.08)    (0.10)    (0.26)    (0.13)   (0.07)    (0.13)    (0.14)    (0.07)    (0.40)    (0.47)    (0.25)    (0.29)
          2
        ¯t
        v  -1   0.65      0.69                                                                                      0.68      0.68
                (0.05)    (0.06)                                                                                   (0.06)    (0.06)
          2
        ¯t
        v  -2   0.08      0.04                                                                                      0.04      0.04
                (0.06)    (0.07)                                                                                   (0.07)    (0.07)
          2
        ¯t
        v  -3   0.10      0.10                                                                                      0.11      0.11
                (0.05)    (0.06)                                                                                   (0.06)    (0.06)

      VIX t                         0.04                                                                  0.01                0.00
                                    (0.01)                                                               (0.03)              (0.02)

      RVol t                                  0.02                                              0.05      0.05      0.01      0.01
                                              (0.01)                                           (0.01)    (0.02)    (0.01)    (0.01)

      RMkt,t                                            0.00                                    0.02      0.02      0.02      0.02
                                                       (0.02)                                  (0.02)    (0.02)    (0.01)    (0.01)
32




     dGDP t                                                      0.13                           0.44      0.44    -0.10     -0.10
                                                                 (0.08)                        (0.19)    (0.19)    (0.12)    (0.12)

      tSpd t                                                               0.05               -0.01     -0.01     -0.07     -0.07
                                                                           (0.06)              (0.08)    (0.08)    (0.04)    (0.05)

      Rcsn t                                                                         0.26       0.13      0.11    -0.21     -0.22
                                                                                     (0.20)    (0.37)    (0.37)    (0.23)    (0.23)

     Adj R2     61.02% 62.89%       2.42%     2.04% -0.22%       0.37% -0.13%        0.15%      4.40%     4.09% 63.35% 63.23%
      Start Jun`78       Jan`90    Jan`90    Jun`78 Jun`78 Jun`78         Jan`82    Jun`78    Jan`90    Jan`90    Jan`90    Jan`90
      #Obs   443          306       306       443    443    443            402       443       306       306       306       306
                                                                                                                 2
                   Table 6. Macroeconomic Correlations. Relationship between the forecasted v                   ¯t in
                   month t and macroeconomic variables. VIX t : level of the VIX index; Jan 1990 to Jun 2015.
                   RVol t : realized volatility on value-weighted market index; Jun 1978 to Jun 2015. RMkt,t : return
                   on the CRSP value-weighted market index; Jun 1978 to Jun 2015. dGDP t : log growth rate of
                   seasonally adjusted GDP; Jun 1978 to Jun 2015. tSpd t : the term spread; Jan 1982 to Jun 2015.
                   Rcsn t : NBER indicator for whether a recession is taking place; Jun 1978 to Jun 2015. Each
                   column represents results of a separate regression as described in Equation (27). Numbers in
                   parentheses are Newey-West standard errors. Significance: = 10%, = 5%, and                  = 1%.
Zt denotes a vector of macroeconomic variables in month t. Table 6 reports the results of these
regression specifications. Each column reports the results of a separate regression. The table reveals
that, while our forecasts for the prior variance are sometimes related to well-known macroeconomic
                                                                                                        2
variables, such as the VIX, they are certainly not subsumed by them. e.g., the three lags of v         ¯t
explain 62.89% of the variation in column (2) whereas the VIX explains only 2.42% of the variation
in column (3). Moreover, once other macroeconomic variables are included, the VIX is no longer a
                                                           2
significant predictor of prior variance. In other words, v
                                                         ¯t  is not just a proxy for the VIX. In the next
section, we are going to be looking at volatility-managed portfolios. But, instead of looking at the
volatility of returns like in Moreira and Muir (2017), we are going to be looking at the volatility of
prior beliefs. The results in Table 6 demonstrate that these are two very different things.


4     Trading Strategy
Financial economists implicitly turn their past experience with other variables in the anomaly zoo
into prior beliefs for use next time around based on their knowledge of the research process. To
make use of this insight, we employ a simple statistical framework that combines empirical-Bayes
methods and the Bayesian interpretation of penalized-regression procedures. In this section, we
study trading-strategy performance to demonstrate that this first statistical approach to computing
the anomaly base rate still produces actionable estimates in spite of its transparent simplicity.

4.1     Portfolio Construction
We start with a benchmark strategy that explicitly does not account for the anomaly base rate,
implicitly assuming that any amount of predictability is equally likely ex ante,  2 = . We then
adjust this benchmark strategy to account for the anomaly base rate, discarding implausibly strong
predictors given the forecasted prior variance.
   Variable-Specific Returns. The realized return for to a zero-cost long-short portfolio
associated with the ith variable in month t is just the estimate for ^i,t in that month:

                                  def
                             Ri,t =   ^i,t =   1
                                                    ·   n           ^t ) · Xn,i,t-1 .
                                                            (Rn,t - µ                               (28)
                                               Nt


Ri,t is the realized return to a zero-cost portfolio that is long stocks which had high Xn,i,t-1 values
and short stocks which had low Xn,i,t-1 values. In Equation (28), µ   ^t denotes the mean excess return
of all stocks in month t, (Nt + 1) denotes the number of stocks in month t, and the ith variable has
been normalized to have zero mean and unit variance, 0 = E[Xn,i,t-1 ] = Nt1       +1
                                                                                     · n Xn,i,t-1 and
                       1                               2
1 = Var[Xn,i,t-1 ] = Nt · n (Xn,i,t-1 - E[Xn,i,t-1 ]) . We compute variable-specific returns this way
so that our empirical analysis exactly matches our statistical framework. However, all of our results
are robust to computing Ri,t by ranking stocks based on Xn,i,t-1 and forming top/bottom 30%


                                                        33
long-short portfolios (cf. Fama and French, 1993; Jegadeesh and Titman, 1993).
    To account for the fact that some variable represent contrarian strategies--e.g., `Ret, 1-0 ',
which represents short-run reversals and delivers negative returns on average--we define an
indicator variable direction i  {-1, + 1} that flips the sign of the benchmark portfolio's holdings
for all contrarian strategies. For example, since there are return reversals at the one-month
horizon and momentum at the 12-month horizon, we have that direction Ret, 1-0 = -1 while
direction Ret, 12-1 = +1. Note that this direction indicator is defined only once at the time the
variable is discovered.
    To make a one-month-ahead forecasts for these returns, we first fit an AR(3) model to the
previous five years of monthly data on     ^i,t :

          ^i,t = a
                 i +     3         ·^i,t - + e
                                             i,t     for months t = (t - 60), . . . , (t - 1).   (29)
                          =1 bi,


i and {
a        bi, } =1,2,3 denote estimated coefficients, and e
                                                         i,t represents the regression residual in
month t . Note that these coefficients will be different for each forecast date t; we have just
suppressed the t subscripts for clarity. We then apply these estimated coefficients to the final three
months of data prior to month t:

                              ¯i,t def
                                   = Et-1 [^i,t ] = a
                                                    i +   3         ·^i,t- .                     (30)
                                                           =1 bi,


If the resulting forecast is very different from zero, |¯i,t | 0, then we say that the ith variable is
a strong signal. By contrast, if |    ¯i,t |  0, then we say it is a weak signal. Just like with our
one-month-ahead forecasts for prior variance, we are using an AR(3) forecasting model out of
convenience. Our results are robust to using alternative number of lags as shown in Table C2 of
Appendix C. The choice of forecasting model is not essential to our results.
    Benchmark Strategy. Our benchmark strategy only uses these one-month-ahead forecasts
to decide whether or not to invest in the ith variable in month t. It explicitly does not take into
consideration any information about the anomaly base rate. It only looks at the data; it does not
consider the ex-ante probability that the ith variable is a tradable anomaly. Let At denote the set
of `active' predictors for the benchmark strategy in month t:

                                      def
                                   At =     i  It : ¯i,t > threshold .                           (31)

This is the collection of previously discovered anomalies whose past performance exceeds some
minimum threshold. In the analysis below, we are going to set the minimum performance threshold
to 1% per month. And, the solid blue line in the left panel of Figure 6 reports the number of active
predictors each month when using this 1%-per-month threshold level.


                                                    34
  The benchmark strategy holds an equal-weighted position in all active predictors, i  At , in
month t. Its raw returns are given by:

                                           def    1
                                  RAt ,t =       |At |
                                                         ·   iAt   Ri,t · direction i .                    (32)

If you want these returns to reflect the minimum performance threshold, you can also compute an
analogous `net' return:

                               1
                              |At |
                                      ·   iAt    (Ri,t · direction i - threshold ).                        (33)

We will study both kinds of returns in various contexts in the analysis below. If a strategy has
positive raw returns but negative net returns, then it is not something that is tradable. The strategy
is generating phantom returns that will likely disappear once you start trading.
    Base-Rate Adjustment. Imagine that you forecasted a large positive realized return in
month t for the ith variable, |  ¯i,t |     0. If the analogous forecasted value, v
                                                                                  ¯i2
                                                                                      ,t , was small for all
other variables i = i in month t and the realized returns for all variables i = 1, . . . , It were drawn
from a common distribution, then the ith variable's true magnitude is likely small as well, |i,t |  0.
So, even though your forecasted value represents a strong signal, you should still be reluctant to
trade on it.
    But, exactly how reluctant? We use the statistical framework outlined in Section 2 to answer
this question. Your forecasted return for the ith variable is a noisy signal about the true realized
return next month,    ¯i,t  Normal[ , N · se 2 ], where se 2 > 0 represents the typical size of your
                                        i,t         i          i
forecast error for the ith variable. So, if realized returns for each variable are drawn from a normal
                                                                 iid
distribution each month with prior variance  2 > 0, i,t  Normal[0,  2 ], then your best guess
about the ith variable's realized return in month t would be:

                                                                                -1
                             E[Ri,t |¯i,t , se 2 ,  2 ] = 1 + se 2 / 2               ×¯i,t .               (34)
                                               i                 i


This formula is just an application Bayesian-normal learning. It is the same as the formula for the
Ridge-regression slope coefficient in Equation (11). And, you could use it to revise your forecast for
next month's realized return if you also had forecasts for se 2  i and  .
                                                                          2

   Forecasting se 2
                  i in month t is easy enough. You can fit an AR(3) model to the squared residuals
          ^
from the i,t forecasting regression in Equation (29):

          2              3   ^     2
                                  ·e                           for months t = (t - 60), . . . , (t - 1).
          e i,t = ^i +    =1 i,     i,t - + ^ i,t                                                          (35)

^i and {
        ^i, } =1,2,3 denote coefficients associated with the ith variable when forecasting the value


                                                             35
for month t, and  ^ i,t represents the regression residual in month t . Note that these coefficients will
be different for each forecast date t; we have just suppressed the t subscripts for clarity. You can
then forecast the standard error in the following month by applying these estimated coefficients to
the final three months of data prior to month t:

                              ¯2
                              se
                                   def
                                             e2              3   ^     2
                                                                      ·e
                                 i,t = Et-1 [ i,t ] = ^i +    =1 i,      i,t- .                     (36)

Again, forecasts are robust to using different numbers of lags and alternative forecasting models.
    Computing a forecast of prior variance for use when evaluating the ith variable in month t
requires a bit more subtlety. You cannot use data about the ith variable to inform the prior you use
when evaluating the ith variable. That would be circular logic. So, when adjusting the forecast of
the ith variable, we compute the average prior-variance forecast using all other variables in the
anomaly zoo besides, i = i:
                                        2     def  1           2
                                      ¯¬
                                      v   i,t = It -1 ·      ¯i
                                                        i =i v   ,t .                           (37)

This means that the forecasted return associated with each variable i will be adjusted by a slightly
different prior variance:
                                  ¯2 def                    -1
                                 ¯v           ¯2      2
                                                               ×¯i,t .
                                  i,t = 1 + se      ¯¬
                                               i,t /v   i,t                                    (38)

It also means that you cannot calculate the base-rate adjustment unless you have past experience
with at least two other variables. In other words, you need It  3. Ideally though, you would have
many more than two other variables. For this reason, even though our data sample starts in May
1973, all of the trading-strategy results in this section will compare the benchmark strategy to a
base-rate-adjusted strategy using data starting in January 1990 when there are at least It  20
variables in the anomaly zoo as shown in Table 1a.
    Economic Interpretation. The benchmark strategy in Equation (32) does not explicitly
use any information about  2 . But, that does not mean it is agnostic about this quantity. In fact,
the benchmark strategy implicitly makes a very strong assumption about the anomaly base
rate--namely, it assumes any level of cross-sectional predictability is just as likely as the next ex
ante. The way this assumption shows up mathematically is as follows:

                                             lim ¯2 = 
                                                      ¯i,t .                                        (39)
                                             2    i,t
                                             


Put differently, in order for it to make sense to completely ignore your prior beliefs and only pay
attention to the data presented by today's seminar speaker, your prior beliefs must have been
completely uninformative. You have to take a stand on your prior beliefs in order to form posteriors.
You cannot punt on this issue. Ignoring the problem is taking a really strong position.
    By contrast, if your past experience tells you that there are never any tradable anomalies, then

                                                    36
                   Active Predictors                                          Discarded Predictors
  40                                                         40
                                 At
  30                                                         30
                                                                                                     2
  20                                                         20                               At \Av
                                                                                                   ¯
                                                                                                   t
  10                           2                             10
                            Av
                             ¯
                             t
   `90       `95      `00     `05         `10             `15 `90           `95         `00   `05   `10   `15
                                                                                    2
 Figure 7. Number of Active Predictors. Black region, Av            ¯
                                                                    t : number of variables included
 in the base-rate-adjusted strategy in month t. Blue line, At : number of variables included in the
                                                                   ¯2
 benchmark strategy in month t. Red shaded region, At \ Av        t : number of variables in the
 benchmark strategy that were discarded due to base-rate considerations in month t. Height of
 this region is the same in both panels. Sample Period: Jan 1990 to Jun 2015.


you should discard all evidence to the contrary:

                                                  lim ¯2 = 0.                                             (40)
                                                       i,t
                                                   2 0


If  2 = 0 exactly, then every past candidate predictor must have turned out to have no out-of-sample
predictive power. But, if you really believed that  2 = 0, then you should not have even bothered
showing up to the seminar speaker's talk today. Nothing he could have said would change your
mind. The talk would be a wasted hour for you. This is the sense in which believing that  2 = 0 is
equivalent to having dogmatic priors.
    Base-Rate-Adjusted Strategy. We modify the benchmark trading strategy to account for
changes in the prevailing anomaly base rate by choosing which variables to invest in based on     ¯v
                                                                                                   ¯2
                                                                                                   i,t
rather than ¯i,t . Let Av
                        ¯2
                        t denote the set of active predictors used by the base-rate-adjusted strategy
in month t:
                                ¯2 def             ¯2
                                                  ¯v
                              Avt =     i  It :    i,t > threshold .                             (41)

The solid black region in the left panel of Figure 7 reports the number of active predictors each
month for the base-rate-adjusted strategy when the threshold is set to 1% per month. Because
the base-rate-adjusted strategy is revising the benchmark return forecasts toward zero, the
base-rate-adjusted strategy will never include more active predictors than the benchmark strategy.
Let RAv ¯2
         ,t
            denote the gross realized return of the base-rate-adjusted strategy at time t:
         t


                                        def     1
                              RAv
                                ¯2,t
                                     =          ¯2
                                              |Av
                                                      ·    iAv
                                                             ¯  2   Ri,t · direction i .                  (42)
                                    t           t |          t




                                                           37
We also calculate an analogous net return for this strategy:

                             1
                             ¯2
                           |Av
                                   ·   iAv
                                         ¯  2   (Ri,t · direction i - threshold ).              (43)
                             t |         t


                2
   Let At \ Av
             ¯
             t denote the set of variables discarded due to base-rate considerations:

                                   2 def                               ¯2
                        At \ Av
                              ¯
                                           i  It : ¯i,t > threshold > ¯v
                              t =                                      i,t           .          (44)

We plot the number of variables in this discarded set in the right panel of Figure 7. This discarded
set of variables represents the cleanest laboratory for studying the effects of incorporating the
anomaly base rate. These variables look like strong signals when considered only on their own
merits, but they should turn out to be bad investment ideas after incorporating the right prior
beliefs. We will return to this idea when we evaluate trading-strategy performance below.
    Variable Selection. But, before we get to these results, it is helpful to get a sense of which
variables get included in the benchmark strategy, which get included in the base-rate-adjusted
strategy, and which get discarded by the base-rate-adjusted strategy. Table 7 presents a detailed
account of variable-specific usage rates. There are three panels. Within each panel, a row
corresponds to a single variable just like in Tables 2 and 3. The column labeled At reports the
fraction of all post-discovery months that the ith variable was held in the benchmark strategy. The
                     ¯2
column labeled Av    t reports the same statistic but for the base-rate-adjusted strategy.
    The sparkline plots then report precisely which months each variable was held by each strategy.
The x-axis in each plot represents time in months. Every time series ends in June 2015. So, variables
discovered later in our sample period will have shorter sparkline plots. Months in which the ith
variable was held by the benchmark strategy are denoted by vertical blue bars on the bottom half of
each sparkline plot. Months in which the ith variable was held by the base-rate-adjusted strategy
are denoted by vertical black bars on the top half of each sparkline plot. And, months in which the
ith variable was discarded by the base-rate-adjusted strategy are denoted by vertical red bars on the
top half of each sparkline plot. The color scheme matches the one used in Figure 7.
    These sparkline plots show more detailed patterns in variable usage. For example, short-run
reversals (Ret, 1-0; left panel near bottom) was selected by the benchmark strategy in 77% of
months. And, as a result, the bottom half of its sparkline plot is mostly blue. The base-rate-adjusted
strategy only selected this variable in 44% of months, though. So, the top half of the sparkline plot
is split almost evenly between black bars (included) and red bars (discarded). What's more,
the sparkline plot further indicates that this variable was more likely to be discarded by the
base-rate-adjusted strategy later in our sample period.




                                                       38
                                                2                                           2                                        2
                                     At      Av¯
                                               t                                 At      Av¯
                                                                                           t                                At     Av¯
                                                                                                                                     t
     Beta                            0.40    0.11   Deprc
                                                        /PP&E                    0.47    0.27   %Mcap                       0.46   0.30
     BetaSq                          0.41    0.12   Ret, 6-1                     0.16    0.06   NetExtnlFin/Assets          0.03   0.03
     IdioVolCAPM                     0.10    0.04   %Sales                       0.69    0.53   DailyBeta                   0.36   0.09
     Earn/Share                      0.67    0.26   OpAccr                       0.29    0.10   NetPO/Price                 0.47   0.21
     Debt/Price                      0.37    0.23   CapitalTOver                 0.51    0.40   PO/Price                    0.48   0.30
     Divd/Price                      0.54    0.28   RetOnEquity                  0.73    0.45   NetPO                       0.09   0.04
     Mcap                            0.77    0.59   KaplanZingales               0.24    0.18   RetOnInvstCap               0.51   0.29
     Earn/Price                      0.61    0.31   %[Sales/Invtry]              0.15    0.11   %Shares                     0.11   0.08
     Ret, 36-12                      0.75    0.56   %[Sales/Recv]                0.29    0.23   ProfMgn                     0.10   0.07
     AvgSpread                       0.79    0.55   %[Sales/XG&A]                0.23    0.21   AdjProfMgn                  0.14   0.06
     Assets/Mcap                     0.19    0.10   %[GrMgn/Sales]               0.18    0.13   RetOnOpAssets               0.29   0.15
     Levrg                           0.22    0.12   LagTOver                     0.41    0.16   AssetTOver                  0.42   0.35
     Levrg/Price                     0.37    0.23   Adj[BkVal/Mcap]              0.74    0.66   AdjAssets                   0.64   0.27
     Sales
         /Cash                       0.42    0.22   AdjMcap                      0.68    0.59   %InvmtX                     0.05   0.01
     LtCF                            0.32    0.15   SdTOver                      0.41    0.06   %Invmt                      0.28   0.18
     CurrRatio                       0.37    0.33   AdvertRate/Ret               0.72    0.49   AdjShares                   0.78   0.65
39




     %CurrRatio                      0.34    0.29   R&D /Mcap                    0.06    0.02   RetOnCash                   0.26   0.17
     %QuickRatio                     0.14    0.12   R&D /Sales                   0.23    0.03   Tangibility                 0.17   0.07
     %[Sales/Invtry]                 0.29    0.15   Advert/Mcap                  0.84    0.77   AdjTOver                    0.16   0.02
     QuickRatio                      0.26    0.09   Invtry /Assets               0.46    0.35   UnexplVlm                   0.88   0.83
     Sales
         /Invtry                     0.25    0.17   OpCF /Price                  0.20    0.05   RetOnAssets                 0.68   0.44
     Sales
         /Recv                       0.34    0.23   Invmt/Lag[AvgInvmt]          0.20    0.09   OpLevrg                     0.72   0.46
     Ret, 1-0                        0.77    0.44   NetOpAssets /Sales           0.17    0.06   MaxRet                      0.53   0.25
     Ret, 12-1                       0.34    0.14   %BkVal                       0.68    0.54   FreeCF                      0.68   0.34
     BkVal                           0.11    0.08   %LtDebt                      0.36    0.25   R&Dcapital                  0.22   0.02
     MonthlyMcap                     0.85    0.64   Price-52WkHi                 0.61    0.31   %Invtry                     0.00   0.00
     Sales
         /Price                      0.71    0.48   IdioVolFF93                  0.16    0.05   Ret, 12-6                   0.15   0.05
     %[Deprc/PP&E]                   0.05    0.04   TotVol                       0.15    0.08   CashHldgs                   0.41   0.15
     D&A  /Assets                    0.68    0.49
                       Table 7. Variable Selection. At : fraction of all post-discovery months that a variable was
                                                        ¯2
                       held by benchmark strategy. Av   t : same statistic for the base-rate-adjusted strategy. Sparkline
                       plots: x-axis is time in months. All time series end in June 2015. Variables discovered later have
                       shorter sparkline plots. Blue bar, bottom half: variable is held by benchmark strategy. Black bar,
                       top half: variable is held by base-rate-adjusted strategy. Red bar, top half: variable was
                       discarded by base-rate-adjusted strategy. Sample period: January 1990 to June 2015.
4.2     Realized Returns
We now examine the realized returns of these trading strategies and find that adjusting the
 benchmark strategy for the prevailing anomaly base rate significantly boosts its performance. Our
 aim is to demonstrate that our simple statistical framework is able to estimate an economically
 important quantity--namely, the anomaly base rate--with an actionable degree of precision.
 Showing that it is possible to trade on this information is one way of doing this, which is also of
 great practical importance. Just think about hitting Ctrl+H and replacing `seminar speaker' with
`quant researcher' on the first two pages.
    Active Predictors. We begin by comparing the performance of the active predictors that
were selected by each strategy to the inactive predictors that were not. We do this by regressing
each variable's realized returns in month t, Ri,t · direction i , on an indicator variable for whether the
ith variable was included in a given strategy. For example, for the benchmark strategy, this means
running the following regression:

                                             ^+^
                        Ri,t · direction i = a                   ¯2
                                                             ^ · se
                                               b · 1 i  At + c      i,t + e
                                                                          ^i,t .                     (45)

The regression uses data on each variable in the anomaly zoo i = 1, . . . , It as of month
t = Jan`90, . . . , Jun`15. We start in January 1990 to ensure that the anomaly zoo is sufficiently
                                                2
large for us to form an accurate estimate for v¯¬ i,t . Our results are robust to using alternative start
dates. Column (1) in Table 8 shows that active predictors which were selected by the benchmark
strategy have 1.56% per month higher excess returns than inactive predictors.
    Column (2) displays analogous results for the base-rate-adjusted strategy:

                                                        ¯2
                                            ^+^
                       Ri,t · direction i = a b · 1 i  Av
                                                        t   ^ · se
                                                           +c   ¯2 i,t + e
                                                                         ^i,t .                      (46)

Active predictors selected by the base-rate-adjusted strategy have 1.83% per month higher excess
returns. But, this is not really a fair comparison. The base-rate-adjusted strategy is trading on
return forecasts that have been shrunk toward zero. So, to be included in the base-rate-adjusted
strategy, a variable had to start out with a more extreme return forecast. To account for this fact, we
add the magnitude of each variable's return forecast, | ¯i,t |, to the previous regression specification:

                                                  ¯2
                                      ^+^
                 Ri,t · direction i = a b · 1 i  Av
                                                  t       ¯2
                                                      ^ · se
                                                     +c            ^ ¯
                                                             i,t + d · |i,t | + e
                                                                                ^i,t .               (47)

Column (3) reports an even larger increase in performance, ^   b = 2.02% per month, after controlling
for the scale of the ith variable's return forecast. Thus, the boost in performance that comes with
adjusting for the prevailing anomaly base rate cannot be explained by the base-rate-adjusted
strategy simply selecting more extreme predictors.

                                                   40
  Dependent Variable: Ri,t · directioni
                                   (1)         (2)             (3)         (4)           (5)           (6)
                     Const       -0.03         0.13            0.23        0.07         0.22          0.20
                                  (0.18)       (0.18)         (0.13)      (0.14)        (0.13)        (0.13)

              1 i  At              1.56
                                  (0.23)
                          2
            1 i  Av
                  ¯
                  t                            1.83            2.02        2.20
                                               (0.25)         (0.32)      (0.34)
                          2
      1 i  At \ Av
                 ¯
                 t                                                         0.85
                                                                          (0.17)
                        v2 ]
                      E[¯
          1 i  At                                                                       1.67
                                                                                        (0.30)
             2          v2 ]
                      E[¯
  1 i  Av
        ¯
        t  At                                                                                         2.05
                                                                                                      (0.33)
              2         v2 ]
                      E[¯
  1 i  Av
        ¯
        t \ At                                                                                        2.18
                                                                                                      (0.83)
              v2 ]
            E[¯           2
  1 i  At            \ Av
                        ¯
                        t                                                                             0.48
                                                                                                      (0.27)

                        ¯2
                        se i,t     0.01        0.02            0.03        0.02         0.03          0.03
                                  (0.01)       (0.01)         (0.01)      (0.01)        (0.01)        (0.01)

                       |¯i,t |                               -0.08      -0.08         -0.06         -0.08
                                                              (0.07)     (0.07)        (0.07)        (0.07)

                  Adj . R2        0.98%       1.05%           1.12%      1.26%         0.86%         1.13%
                     #Obs        15,913      15,913          15,913      15,913        15,913       15,913

Table 8. Active Predictors. Relationship between the realized returns to a zero-cost
long-short portfolio based on the ith variable, Ri,t · direction i , and portfolio inclusion.
Each column reports results for a separate regression involving 15,913 variable×month
observations--one observation for each variable in the anomaly zoo i = 1, . . . , It as of
month t = Jan`90, . . . , Jun`15. 1[i  At ]: indicator variable for the ith variable's inclusion in
                                                      ¯2
the benchmark strategy in month t. 1[i  Av           t ]: indicator variable for the ith variable's
                                                                                      ¯2
inclusion in the base-rate-adjusted strategy in month t. 1[i  At \ Av                 t ]: indicator variable
for the ith variable's inclusion in the benchmark strategy but not the base-rate-adjusted
                                   v2 ]
                                 E[¯
strategy in month t. 1[i  At ]: indicator variable for the ith variable's inclusion in the
                                                                  ¯2          v2 ]
                                                                            E[¯
average base-rate-adjusted strategy in month t. 1[i  Av           t  At            ]: indicator variable for
the ith variable's inclusion in both the true base-rate-adjusted strategy and the average
                                                         ¯2       v2 ]
                                                                E[¯
base-rate-adjusted strategy in month t. 1[i  Av          t \ At        ]: indicator variable for the ith
variable's inclusion in the true base-rate-adjusted strategy but not the average base-rate-adjusted
                                 v2 ]
                               E[¯       ¯2
strategy in month t. 1[i  At          \ Av
                                         t ]: indicator variable for the ith variable's inclusion in the
average base-rate-adjusted strategy but not the true base-rate-adjusted strategy in month t.
|¯i,t |: absolute value of one-month-ahead return forecast for ith variable in month t. se               ¯2i,t :
variance of one-month-ahead return forecast for ith variable in month t. Numbers in parentheses
are standard errors clustered by variable. Significance: = 10%, = 5%, and                             = 1%.


                                                        41
    Our ideal experiment would involve comparing two variables with the same out-of-sample return
forecasts in two different base-rate regimes, one with  2   0 and the other with  2  0. In this
situation, we should find that the variable in the  2  0 regime consistently under-performs the
otherwise identical variable in the  2      0 regime. We approximate this ideal experiment by
estimating the following regression specification:

                                               ¯2
                                  ^+^
             Ri,t · direction i = a b1 · 1 i  Av
                                               t
                                                                                                      (48)
                                                      ¯2
                                     +^
                                      b2 · 1 i  At \ Av
                                                      t       ¯2
                                                          ^ · se
                                                         +c            ^ ¯
                                                                 i,t + d · |i,t | + e
                                                                                    ^i,t .

This specification controls for the magnitude and precision of the one-month-ahead return forecasts,
|¯i,t | and se
            ¯2                                    ¯2
                                                  v
               i,t . So, the coefficient on 1 i  At  captures the excess returns to trading on a strong
                                                                                                        2
cross-sectional predictor in the high base-rate regime. Whereas, the coefficient on 1 i  At \ Av        ¯
                                                                                                        t
reflects the excess returns to trading on that exact same cross-sectional predictor in the low
base-rate regime--i.e., in a regime where  2  0 leading to lots of shrinkage.
      Column (4) displays the results for this regression specification. We find that, for a given level of
in-sample signal strength, the ith variable is a much better forecasting tool during the high
base-rate regime: ^ b1 = 2.20% per month vs. ^   b2 = 0.85% per month. The difference between these
coefficient estimates is statistically significant at the 1% level. And, they suggest an economically
large difference in trading-strategy performance. Suppose that the ith variable has a forecasted
return of   ¯i,t = 2% and the average level of precision over the entire sample, se   ¯ i,t = 1.76%. If the
prevailing anomaly base rate in month t was high enough such that              ¯v
                                                                                ¯ 2
                                                                                i,t > 1%, then the point
estimates in column (4) of Table 8 suggest that trading on the ith variable would result in a
monthly return of 2.18%. By contrast, if the prevailing anomaly base rate in month t was such that
 ¯v
  ¯2
  i,t < 1%, then trading on the ith variable would only result in a monthly return of 0.83%. While
still positive, note that this predicted value of 0.83% per month is no longer large enough to cover
the 1%-per-month minimum return threshold. Thus, we have clear evidence that trading on the
same variable in two different base-rate regimes leads to two very different outcomes.
    Base Rate Or Just Shrinkage? The base-rate-adjusted strategy improves performance by
acting on return forecasts that have been shrunk toward zero. We motivated this shrinkage in
Section 2 as a way of incorporating information about the prevailing anomaly base rate. But, so far
we have not seen any evidence of this link in the data. Put differently, perhaps any sort of shrinkage
will boost performance regardless of how it is motivated? Column (5) in Table 8 shows results for a
version of the regression specification in Equation (47) using the average level of prior variance,
    2                                                 2
E[¯
  vt  ] = 2.19, rather than the prevailing value of v
                                                    ¯t  to construct the base-rate-adjusted portfolio:

                                                   v2 ]
                                                 E[¯
                                     ^+^
                Ri,t · direction i = a b · 1 i  At           ¯2
                                                         ^ · se
                                                        +c            ^ ¯
                                                                i,t + d · |i,t | + e
                                                                                   ^i,t .             (49)


                                                    42
Using the average level of prior variance mutes the effect of adjusting for the anomaly base rate,
^
b = 1.67% per month rather than 1.83% as in column (3), which suggests that there is more going
on than just shrinkage.
    But, it is possible to construct a sharper test based on this average level of prior variance. If it is
the anomaly base rate that is driving the base-rate adjusted portfolio's improved returns, then
time-series fluctuations in this base rate should matter. When the anomaly base rate is higher
than its sample average, there are going to be variables that are strong enough to be included
in the true base-rate-adjusted portfolio but not in the average base-rate-adjusted portfolio,
         ¯2       v2 ]
                E[¯
1 i  Av  t \ At        . The subsequent returns to trading on these variables should look like the
subsequent returns to trading on any other active predictor in the base-rate-adjusted strategy.
Conversely, when the anomaly base rate is lower than its sample average, there are going to be
variables that are strong enough to be included in the average base-rate-adjusted portfolio but
                                                             v2 ]
                                                           E[¯       ¯2
not in the true base-rate-adjusted portfolio, 1 i  At             \ Av
                                                                     t  . These variables should have
subsequent returns that look very different. And, when we run a version of the regression in
Equation (47) where we split the active set along these lines,

                                            ¯2    v2 ]
                                                E[¯
                               ^+^
          Ri,t · direction i = a b1 · 1 i  Av
                                            t  At
                                              ¯2     v2 ]
                                                   E[¯
                                  +^
                                   b2 · 1 i  Av
                                              t \ At                                                      (50)
                                                        v2 ]
                                                      E[¯         2
                                     +^
                                      b3 · 1 i  At             \ Av
                                                                  ¯
                                                                  t    ^ · se
                                                                      +c   ¯2       ^ ¯
                                                                              i,t + d · |i,t | + e
                                                                                                 ^i,t ,

this is exactly what we find. Column (6) in Table 8 shows that ^ b2 = 2.18% per month while
^
b3 = 0.48% per month. This difference in coefficient estimates is statistically significant and
economically large, confirming that the success of the base-rate-adjusted strategy cannot be
replicated by a simple shrinkage-based approach.
    Cumulative Returns. Having shown that incorporating information about the prevailing
anomaly base rate improves variable selection, we now combine these selections into a single trading
strategy. We start by computing the cumulative returns to investing $1 in either the benchmark
strategy or the base-rate-adjusted strategy starting in January 1990. Figure 8 reports the total
amount of money that you would have in your account in month t if you followed either strategy,
continually reinvesting any capital gains along the way. The left panel reports cumulative gross
                                                                                                    ¯2
returns. By June 2015, a portfolio investing in the base-rate-adjusted strategy (solid black line; Av
                                                                                                    t )
would have been worth $137.42 while a portfolio investing in the benchmark strategy (solid blue line;
At ) would only have been worth $78.67. What's more, a portfolio that only invested in the variables
                                                                      ¯2
discarded by the base-rate-adjusted strategy (red dotted line; At \ Avt ) would only have been worth
$20.59. The right panel in Figure 8 shows analogous results for cumulative returns net of the
1%-per-month performance threshold.

                                                   43
                Cum Return (Gross)                                     Cum Return (Net)
  $100                            2                     $100
                              Av
                               ¯
                               t
                                         At
   $10                                        2          $10
                                      At \Av
                                           ¯
                                           t

     $1                                                   $1
      `90      `95     `00     `05      `10       `15      `90       `95   `00   `05     `10      `15
                                                                 2
  Figure 8. Cumulative Returns. Solid black line, Av           ¯
                                                               t : current value of a portfolio that
  invested $1 in the base-rate-adjusted strategy on January 1st, 1990. Solid blue line, At : current
  value of a portfolio that invested $1 in the benchmark strategy on January 1st, 1990. Dotted red
               ¯2
  line, At \ Avt : current value of a portfolio that invested $1 in the discarded-variables strategy
  starting on January 1st, 1990. Left panel: cumulative gross returns. Right panel: cumulative
  returns net of 1% per month performance threshold. Sample Period: January 1990 to June 2015.


    We recognize that some variables are noisier signals than others. And, with this idea in mind,
practitioners often use volatility weights to combine signals (e.g., see Moskowitz et al., 2012). We
want to emphasize that these volatility weights are a different phenomenon from what we
                                                            2
are studying in the current paper. The prior variance v   ¯¬  i,t measures the ex ante likelihood of
encountering a tradable signal. These volatility weights measure the precision of a tradable signal
once it has been found. What's more, our approach to adjusting for the anomaly base rate is
accounting for this precision by dividing through by the squared forecasted standard error, se   ¯2i,t ,
                 ¯
when adjusting i,t for the prevailing anomaly base rate in Equation (38). Thus, the success of the
base-rate-adjusted strategy is not just coming from taking large bets on the most volatile predictors.
    Performance Metrics. In Table 9, we explore the difference in performance between the
benchmark and base-rate-adjusted strategies in more detail. We report both the mean and standard
deviation of each strategy's monthly returns as well as their skewness, kurtosis, and annualized
Sharpe ratio. The first column, middle panel, reveals that, after accounting for implementation costs,
the annualized Sharpe ratio of the benchmark strategy is only 0.43 during our sample period. By
contrast, when we look at the second column, we can see that the base-rate-adjusted strategy has
an annualized Sharpe ratio of 0.57. And, as you would expect, this difference is due to the
base-rate-adjusted strategy systematically dropping variables from the benchmark portfolio that
only seem to have strong predictive power in-sample. The third column shows that the net
returns of a trading strategy that only invests in the variables that are held by the benchmark
strategy but not by the benchmark-adjusted strategy are only 0.18% per month. Moreover, this
discarded-variables strategy has an annualized Sharpe ratio of only 0.11. The top and bottom panels


                                                   44
                                                                       2                       2
                                                  At                Av¯
                                                                      t              At \ Av
                                                                                           ¯
                                                                                           t
                threshold = 0.50%                (1)                (2)                (3)
         Gross:              E[Rs,t ]            1.37               1.46                0.77
                            E[Rs,t ]            0.87                0.96               0.32
        Net of
                           Sd[Rs,t ]            3.73                3.77               6.37
     minimum
  performance            Skew[Rs,t ]            1.88                1.77               1.99
    threshold:           Kurt[Rs,t ]           11.16                9.38              17.81
                              SRs               0.80                0.88               0.17

                             1.00%
         Gross:              E[Rs,t ]            1.52               1.72                1.15
                            E[Rs,t ]             0.53               0.74                0.18
        Net of
                           Sd[Rs,t ]             4.27               4.50                5.66
     minimum
  performance            Skew[Rs,t ]             1.31               0.94                0.92
    threshold:           Kurt[Rs,t ]             7.75               5.79                8.87
                              SRs                0.43               0.57                0.11

                             1.50%
         Gross:              E[Rs,t ]            1.74               2.06                1.35
                            E[Rs,t ]             0.26               0.68              -0.10
        Net of
                           Sd[Rs,t ]             4.76               5.09               5.73
     minimum
  performance            Skew[Rs,t ]             1.22               0.74               0.42
    threshold:           Kurt[Rs,t ]             6.64               5.16               6.61
                              SRs                0.19               0.46              -0.06

Table 9. Performance Metrics. Performance statistics for the excess returns to three
different trading strategies. Column (1): benchmark strategy, At . Column (2): base-rate-adjusted
             ¯2
strategy, Avt . Column (3): strategy that invests in variables held by the benchmark strategy but
                                               ¯2
not the base-rate-adjusted strategy, At \ Av   t . All return statistics quoted in % per month.
E[Rs,t ]: mean monthly return. E[Rs,t ]: mean net monthly return. Sd[Rs,t ]: standard deviation of
net monthly returns. Skew[Rs,t ]: skewness of net monthly returns. Kurt[Rs,t ]: kurtosis of net
month returns. SRs : annualized Sharpe ratio using net monthly returns. Top panel: threshold of
0.50% per month. Middle panel: threshold of 1.00% per month. Bottom panel: threshold of
1.50% per month. Sample period: January 1990 to June 2015.




                                               45
                                   (a) Summary Statistics
                                                  Avg           Sd            SR
                                   RMkt,t         0.62        4.33          0.50

             (b) Regression Results
             Dependent Variable: Rs,t
                                                                     2                          2
                                    At                           Av
                                                                  ¯
                                                                  t                      At \ Av
                                                                                               ¯
                                                                                               t
                            (1)           (2)             (3)             (4)        (5)        (6)
              Const       0.53           0.44            0.74            0.68      0.18        0.04
                          (0.34)         (0.32)      (0.34)              (0.32)    (0.32)     (0.29)

               RMkt,t                    0.15                            0.09                  0.22
                                         (0.05)                          (0.08)               (0.10)

             Adj . R2                    1.86%                        0.47%                   2.62%
               #Obs        306           306             306             306        306        306

 Table 10. Abnormal Returns. Net abnormal returns relative to the market for three
                                                                                               ¯2
 different trading strategies: benchmark strategy, At ; base-rate-adjusted strategy, Av        t ;
 set-difference strategy that invests in variables held by the benchmark strategy but not the
                                        ¯2
 base-rate-adjusted strategy, At \ Av  t . RMkt,t : excess return on the value-weighted market
 portfolio. (a) Summary Statistics. Mean and standard deviation of the excess return on
 the market in units of % per month as well as the annualized Sharpe ratio. (b) Regression
 Results. Each column reports the results of a separate time-series regression with the net
 excess returns of a particular strategy as the left-hand side variable. Const has units of % per
 month, and the slope coefficients are dimensionless. Numbers in parentheses are Newey-West
 standard errors. Statistical significance: = 10%, = 5%, and               = 1%. Sample period:
 January 1990 to June 2015. All regressions involve 306 monthly observations.


show that these conclusions are robust to varying the threshold for implementation costs. In
addition, all our results are robust to estimating the anomaly base rate using cross-validation rather
than the regularized estimator in Proposition 2.2 as shown in Table C3 of Appendix C.
    Abnormal Returns. We next show that the performance of the base-rate-adjusted strategy
is not just the result of exposure to market risk. We run a time-series regression of the net excess
returns to the base-rate-adjusted strategy on the excess returns to the value-weighted market:

                                      RAv
                                        ¯2,t
                                              ^+^
                                             =a b · RMkt,t + e
                                                             ^t .
                                            t



In the equation above, RMkt,t is the excess return on the market in month t, a     ^ is the abnormal
return to the base-rate-adjusted strategy, ^
                                           b is the slope coefficient from this time-series regression,




                                                         46
                   Dependent Variable: Ri,t0 (i)+120 · direction i
                                                    (1)            (2)         (3)           (4)
                                     Const          2.53           3.20        4.62         3.77
                                                   (3.07)         (2.73)      (2.80)        (3.14)

                             1 i  At0 (i)           7.70
                                                   (4.10)
                                        2
                             1 i  Av
                                   ¯
                                   t0 (i)                        10.37       14.81         15.56
                                                                  (4.77)      (5.33)        (5.50)
                                        2
                   1 i  At0 (i) \ Av
                                   ¯
                                   t0 (i)                                                   3.12
                                                                                            (5.13)

                                    se
                                    ¯ i,t0 (i)    -0.66          -0.51         0.01       -0.01
                                                   (0.62)         (0.62)      (0.68)       (0.69)

                                    |¯i,t (i) |                             -1.52         -1.49
                                         0
                                                                             (0.87)        (0.88)

                                   Adj . R2        4.64%          6.42%      9.59%         8.59%
                                     #Obs           62             62          62            62

    Table 11. Newly Discovered Variables. Evidence that the anomaly base rate is helpful
    when evaluating variables at the moment they are added to the academic literature. Each
    column reports results of a separate cross-sectional regression with 62 observations, one for
    each variable i discovered after January 1990. Ri,t0 (i)+120 · direction i : annualized return to
    zero-cost long-short portfolio associated with ith variable defined in Equation (28) during
    10 years immediately following publication. Const : intercept term; units of % per year.
    1[i  At0 (i) ]: indicator variable for ith variable's inclusion in the benchmark strategy in the
                                                                               ¯2
    month following its discovery, t0 (i); units of % per year. 1[i  Av        t0 (i) ]: indicator variable for ith
    variable's inclusion in the base-rate-adjusted strategy in month t0 (i); units of % per year.
                      ¯2
    1[i  At0 (i) \ Av t0 (i) ]: indicator variable for ith variable's inclusion in the benchmark strategy but
    not the base-rate-adjusted strategy in month t0 (i); units of % per year. Numbers in parentheses
    are Newey-West standard errors. Statistical significance: = 10%, = 5%, and                              = 1%.


and e ^t is the regression residual. These data come from Kenneth French's website.3 Column (4) in
Table 10 shows that market-risk exposure does not account for the base-rate-adjusted trading
strategy's good performance. The average net return of the base-rate-adjusted strategy is 0.74% per
month; and, after accounting for market-risk exposure, the net abnormal return to this strategy is
0.67% per month. There is hardly any difference.4 The columns (2) and (6) in Table 10 replicate the
same analysis using the net returns of the benchmark and discarded-variables strategies as the
left-hand-side variable. Neither of these alternative strategies has significant net excess returns after
adjusting for exposure to the market.
3
    See http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html.
4
    These results are not unexpected to the extent that the firm characteristics entered our sample because they had
    anomalous returns relative to a factor model, often the CAPM, during some period of time.



                                                            47
    New Variables. Finally, we motivated this paper by discussing the problem faced by a
researcher who is trying to evaluate statistical evidence concerning a new cross-sectional predictor.
However, the trading strategies discussed so far have held positions in both brand new and
previously discovered variables. e.g., it might invest based on a firm's investment growth at any time
on or after December 2004 when Titman et al. (2004) published the paper documenting the variable.
So, maybe the improved performance of the base-rate-adjusted strategy relative to the benchmark
strategy is only due to different positions in previously discovered variables? In the last part of our
analysis, we show this is not the case.
    First, we compute the realized returns of each variable-specific trading strategy held by the
benchmark strategy in the 10 years immediately after its publication. Let Ri,t0 (i)+120 denote the
annualized return to the variable-specific strategy associated with the ith variable defined in
Equation (28) during the 10 years immediately following its publication date as given in Tables 1a,
1b, and 1c. And, let t0 (i) denote the month immediately following the publication of the ith
variable, e.g., for investment growth as defined in Titman et al. (2004), t0 (i) = Jan2005. There are
62 variables discovered sometime after January 1990 for which we can forecast prior variance.
    Then, we regress the post-publication returns of each variable on an indicator for whether the
variable would have been held by the benchmark strategy:

                                               ^+^
                 Ri,t0 (i)+120 · direction i = a                        ¯2
                                                                    ^ · se
                                                 b · 1 i  At0 (i) + c      i,t0 (i) + e
                                                                                      ^i .       (51)

Column (1) of Table 11 reports the results of this cross-sectional regression, which contains one
observation for each of the 62 variables discovered following January 1990. We estimate that
^
b = 7.70% per year for the benchmark strategy. By contrast, when we estimate the same regression
using an indicator variable for inclusion in the base-rate-adjusted strategy,

                                                           ¯2
                                               ^+^
                 Ri,t0 (i)+120 · direction i = a b · 1 i  Av            ¯2
                                                                    ^ · se
                                                           t0 (i) + c      i,t0 (i) + e
                                                                                      ^i ,       (52)

we get a ^
         b = 10.37% per year in column (2). Column (3) indicates that adjusting for the amount of
predictive power associated with the variable at the time of publication does not explain this
difference. And, column (4) provides further evidence that a researcher should discount empirical
results suggesting the existence of a strong predictor during low anomaly-base-rate regimes.
Variables that would be included in the benchmark strategy at the time of discovery but not in the
base-rate-adjusted strategy have average returns of only 3.12% per year over the next decade.


5     Conclusion
Evaluating evidence of cross-sectional predictability is hard. Today's seminar speaker is presenting
evidence about how lagged values of some new variable, Xn , are positively related to subsequent

                                                    48
 excess returns at the 1% level of statistical significance. In order to form the correct posterior beliefs
 based on this evidence, you need to start out with the correct prior beliefs about Xn being a
 tradable anomaly and then appropriately revise this prior based on the speaker's statistically
 significant results for  ^ > 0.
      The existing academic literature contains hundreds of other variables that also seem to predict
 the cross-section of expected returns. These variables are often based on entirely different data
 sources and have little in common with one another economically. The existence of this so-called
`anomaly zoo' represents something of a crisis in the field of empirical asset pricing. Specifically, it
 suggests that financial economists are not taking the effects of data mining into account when
 revising their prior beliefs about each new variable Xn they encounter. One way to address this
 problem is to demand a higher t-statistic cutoff when assessing statistical significance. Researchers
 can and should be doing this when evaluating evidence of cross-sectional predictability.
      But, that being said, it is important to remember that there is more to Bayesian inference than
 choosing the right t-statistic. Anyone who walked into today's seminar room with wildly inaccurate
 priors is going to walk out of the seminar room with wildly inaccurate posteriors even if the speaker
 correctly adjusts his empirical results for data mining. There is a fast growing literature on how to
 adjust econometric techniques in light of widespread data mining. Yet, empirical asset pricing offers
 no useful suggestions about which prior beliefs to use when evaluating the next candidate predictor
you see. And, this is true in spite of the fact that, in practice, talks about new variables are often
 dominated the priors of particular audience members.
      In this paper, we follow Winston Churchill's advice about how one should "never let a good
 crisis go to waste." "Large data sets of parallel situations carry within them their own Bayesian
 information (Efron and Hastie, 2016, p. 77)" about the appropriate prior. And, what is the anomaly
 zoo if not a large data set of many parallel situations, one for each variable in the academic
 literature that seems to predict the cross-section of expected returns? Your past experience with
 other variables in the anomaly zoo (a few tradable anomalies and many more spurious predictors)
 contains information about the efficacy of financial economists' current research practices. And, in
 the absence of an overarching theory of the market conditions responsible for cross-sectional return
 predictability, you must have been using this information to inform your priors when walking into
 the seminar room today.
      We provide a simple statistical approach to operationalizing this insight. Then, we use
 trading-strategy performances to show that the anomaly-base-rate estimates produced by our simple
 statistical approach are economically meaningful. We use numerical simulations to demonstrate that
 our statistical approach can be extended in various ways. So, our hope is that, by clearly outlining
 this important economic problem and providing a general statistical framework for solving it, we can
 encourage future research on how to even better estimate the anomaly base rate.


                                                    49
References
Akaike, H. (1974). A new look at statistical model identification. IEEE Transactions on Automatic
 Control .

Andersen, T., T. Bollerslev, F. Diebold, and P. Labys (2003). Modeling and forecasting realized
 volatility. Econometrica .

Bajgrowicz, P. and O. Scaillet (2012). Technical trading revisited: False discoveries, persistence tests,
  and transaction costs. Journal of Financial Economics .

Baker, S., N. Bloom, and S. Davis (2016). Measuring economic-policy uncertainty. Quarterly
 Journal of Economics .

Barberis, N. and R. Thaler (2003). A survey of behavioral finance. Handbook of Financial
 Economics .

Barras, L., O. Scaillet, and R. Wermers (2010). False discoveries in mutual-fund performance:
  Measuring luck in estimated alphas. Journal of Finance .

Berger, J. (2006). The case for objective Bayesian analysis. Bayesian Analysis .

Bryzgalova, S. (2017). Spurious factors in linear asset-pricing models. Working Paper .

Casella, G. (1985). An introduction to empirical-Bayes data analysis. American Statistician .

Chinco, A., A. Clark-Joseph, and M. Ye (2018). Sparse signals in the cross-section of returns.
 Journal of Finance .

De Bondt, W. and R. Thaler (1985). Does the stock market overreact? Journal of Finance .

DeMiguel, V., L. Garlappi, F. Nogales, and R. Uppal (2009). A generalized approach to portfolio
  optimization: Improving performance by constraining portfolio norms. Management Science .

Diaconis, P. and B. Skyrms (2017). Ten great ideas about chance. Princeton University Press.

Efron, B. (2012). Large-scale inference: empirical-Bayes methods for estimation, testing, and
  prediction. Cambridge University Press.

Efron, B. (2013). Empirical-Bayes modeling, computation and accuracy. Division of Biostatistics,
  Stanford University.

Efron, B. and T. Hastie (2016). Computer age statistical inference. Cambridge University Press.

Efron, B. and C. Morris (1972). Limiting the risk of Bayes and empirical-Bayes estimators--part ii:
  The empirical-Bayes case. Journal of the American Statistical Association .

Efron, B. and C. Morris (1973). Stein's estimation rule and its competitors--an empirical Bayes
  approach. Journal of the American Statistical Association .



                                                  50
Efron, B. and C. Morris (1975). Data analysis using Stein's estimator and its generalizations.
  Journal of the American Statistical Association .

Fama, E. (1976). Foundations of finance: Portfolio decisions and securities prices. Basic Books (AZ).

Fama, E. and K. French (1993). Common risk factors in the returns on stocks and bonds. Journal
  of Financial Economics .

Fama, E. and K. French (1996). Multi-factor explanations of asset-pricing anomalies. Journal of
  Finance .

Feng, G., S. Giglio, and D. Xiu (2017). Taming the factor zoo. Working Paper .

Ferson, W., S. Sarkissian, and T. Simin (1999). The alpha-factor asset-pricing model: A parable.
  Journal of Financial Markets .

Freyberger, J., A. Neuhierl, and M. Weber (2017). Dissecting characteristics non-parametrically.
  Working Paper .

Gelman, A., J. Carlin, H. Stern, D. Dunson, A. Vehtari, and D. Rubin (2013). Bayesian data
 analysis. Chapman and Hall/CRC.

Green, J., J. Hand, and F. Zhang (2017). The characteristics that provide independent information
  about average US monthly stock returns. Review of Financial Studies .

Gromb, D. and D. Vayanos (2010). Limits of arbitrage. Annual Review of Financial Economics .

Harvey, C. (2017). Presidential address: The scientific outlook in financial economics. Journal of
  Finance .

Harvey, C. and Y. Liu (2018a). Detecting repeatable performance. Review of Financial Studies .

Harvey, C. and Y. Liu (2018b). False (and missed) discoveries in financial economics. Working
 Paper .

Harvey, C. and Y. Liu (2018c). Lucky factors. Working Paper .

Harvey, C., Y. Liu, and H. Zhu (2016). . . . and the cross-section of expected returns. Review of
 Financial Studies .

Harvey, C. and G. Zhou (1990). Bayesian inference in asset-pricing tests. Journal of Financial
 Economics .

Hastie, T., R. Tibshirani, and J. Friedman (2001). The elements of statistical learning: Data mining,
  inference, and prediction. Springer Science & Business Media.

Hoerl, A. and R. Kennard (1970). Ridge regression: Applications to non-orthogonal problems.
 Technometrics .

James, W. and C. Stein (1961). Estimation with quadratic loss. In Proceedings of the Fourth
  Berkeley Symposium on Mathematical Statistics and Probability.

                                                 51
Jegadeesh, N. and S. Titman (1993). Returns to buying winners and selling losers: Implications for
  stock-market efficiency. Journal of Finance .

Jonah, B. (1986). Accident risk and risk-taking behaviour among young drivers. Accident Analysis
  & Prevention .

Kelly, B., S. Pruitt, and Y. Su (2017). Instrumented principal-component analysis. Working Paper .

Kelly, B., S. Pruitt, and Y. Su (2018). Characteristics are covariances: A unified model of risk and
  return. Journal of Financial Economics .

Kozak, S., S. Nagel, and S. Santosh (2018a). Interpreting factor models. Journal of Finance .

Kozak, S., S. Nagel, and S. Santosh (2018b). Shrinking the cross-section. Journal of Financial
 Economics .

Ledoit, O. and M. Wolf (2017). Non-linear shrinkage of the covariance matrix for portfolio selection:
  Markowitz meets Goldilocks. Review of Financial Studies .

Lettau, M. and M. Pelger (2018). Estimating latent asset-pricing factors. Working Paper .

Lewellen, J., S. Nagel, and J. Shanken (2010). A skeptical appraisal of asset-pricing tests. Journal
  of Financial Economics .

Linnainmaa, J. and M. Roberts (2018). The history of the cross-section of stock returns. Review of
  Financial Studies .

Lo, A. and C. MacKinlay (1990). Data-snooping biases in tests of financial asset-pricing models.
  Review of Financial Studies .

Maritz, J. (1970). Empirical Bayes methods. Methuen.

McLean, D. and J. Pontiff (2016). Does academic research destroy stock-return predictability?
 Journal of Finance .

Moreira, A. and T. Muir (2017). Volatility-managed portfolios. Journal of Finance .

Moskowitz, T., Y. Ooi, and L. Pedersen (2012). Time-series momentum. Journal of Financial
 Economics .

Novy-Marx, R. and M. Velikov (2015). A taxonomy of anomalies and their trading costs. Review of
  Financial Studies .

Park, T. and G. Casella (2008). The Bayesian LASSO. Journal of the American Statistical
  Association .

Pástor, L. (2000). Portfolio selection and asset-pricing models. Journal of Finance .

Petrongolo, B. and C. Pissarides (2001). Looking into the black box: A survey of the matching
  function. Journal of Economic Literature .


                                                 52
Robbins, H. (1956). An empirical Bayes approach to statistics. In Proceedings of the Third Berkeley
  Symposium on Mathematical and Statistical Probability.

Shanken, J. (1987). A Bayesian approach to testing portfolio efficiency. Journal of Financial
  Economics .

Stone, M. (1977). An asymptotic equivalence of choice of model by cross-validation and Akaike's
  criterion. Journal of the Royal Statistical Society .

Sullivan, R., A. Timmermann, and H. White (1999). Data-snooping, technical trading-rule
  performance, and the bootstrap. Journal of Finance .

Tibshirani, R. (1996). Regression shrinkage and selection via the LASSO. Journal of the Royal
  Statistical Society .

Titman, S., J. Wei, and F. Xie (2004). Capital investments and stock returns. Journal of Financial
  and Quantitative Analysis .

White, H. (2000). A reality check for data snooping. Econometrica .

Yan, X. and L. Zheng (2017). Fundamental analysis and the cross-section of stock returns: A
  data-mining approach. Review of Financial Studies .

Zou, H., T. Hastie, and R. Tibshirani (2007). On the "degrees of freedom" of the LASSO. Annals of
  Statistics .




                                                53
A      Technical Appendix
Proof (Proposition 2.1). The result follows from properties of the normal distribution. If
z  Normal[0,  2 ], then for any  > 0 we have that:

                                           Pr[z >  ] = Pr[z < - ]
                                                     = [-/ ].

Thus, Pr[ anom i ] = Pr[ |i | > threshold ] = 2 · Pr[ i < -threshold ] = 2 · [ -threshold/ ].
Derivation (Equation 11). Optimizing Equation (10) results in the first-order condition:
                                  1
                     0 = -2 ·     N
                                       ·    n     Rn - µ
                                                       ^ -  · Xn,i · Xn,i + 2 ·  · 
                           1                                             1              2
                        =-N   · n (Rn - µ^) · Xn,i +  ·                  N
                                                                             ·     n   Xn,i +·
                        = -^i + (1 + ) · .

Solving for  yields the desired result.
Derivation (Equation 16). Let Si [v 2 ] denote the Ridge shrinkage when i = se 2   2
                                                                               i /v :

                                               Si [v 2 ] = v 2 /(v 2 + se 2
                                                                          i ).

The Ridge optimization problem can then be expressed as:
                                                                                       2
                              min        ^ - Si [ v 2 ] · 
                                  E Rn - µ                ^i · Xn,i                        .
                              2
                              v >0

This optimization problem results in the following first-order condition:

                               ^ - Si [v 2 ] · 
                0 = 2 · E Rn - µ               ^i · Xn,i · S [v 2 ] · ^i · Xn,i
                                                            i

                  = 2 · Si [ v 2 ] · ^i · E[(Rn - µ
                                                  ^) · Xn,i ] - Si [v 2 ] · E[(^i · Xn,i )2 ]

                  = 2 · Si [ v 2 ] · ^2 - Si [v 2 ] · ^2
                                      i                i

                  = 2 · Si [v 2 ] · (1 - Si [v 2 ]) · ^2
                                                       i
                                4
                  =2·        se i
                                  2    ·^2 .
                         (v +se i )3
                           2             i


Taking the expectation with respect to realizations of the true slope coefficient yields:
                                               4                            4
                       0=E 2·               se i
                                                      ·^2 = 2 ·          se i
                                                                                   · ( 2 + se 2
                                                 2
                                        (v +se i )3
                                          2             i              2
                                                                     (v +se i )3
                                                                              2               i ).


The only way to satisfy this first-order condition is to choose v 2 = .
Proof (Proposition 2.2). Suppose we add a correction term, Ci [v 2 ], to the training error in




                                                            54
Equation (15) to undo this in-sample overfitting. The objective function would then become:
                                                                                            2
                   ^i
                   v 2
                       = arg min        ^ - Si [v 2 ] · 
                                 E Rn - µ               ^i · Xn,i                               + Ci [ v 2 ] .
                             2  v >0


Our goal is to find a functional form for Ci [v 2 ] that yields an unbiased estimate of E[^2
                                                                                          vi ] = 2.
   Note that this corrected optimization problem yields the following first-order condition:

                               ^ - Si [v 2 ] · 
                0 = 2 · E Rn - µ               ^i · Xn,i · S [v 2 ] · ^i · Xn,i - C [v 2 ]
                                                            i                      i

                  = 2 · Si [v 2 ] · (1 - Si [v 2 ]) · ^2 - C [v 2 ]
                                                       i    i
                                4
                  =2·         se i
                                   2     ·^2 - C [v 2 ].
                         (v 2 +se i )3     i    i


And, taking the expectation of this corrected first-order condition with respect to realizations of the
true slope coefficient yields:
                                 4                                                4
              0=E 2·          se i
                                          ·^2 - C [v 2 ] = 2 ·                 se i
                                                                                         · ( 2 + se 2          2
                                   2
                          (v +se i )3
                            2               i    i                           2
                                                                           (v +se i )3
                                                                                    2               i ) - Ci [v ].


By inspection, we see that choosing any Ci [v 2 ] with the following first derivative,
                                                                  4
                                                                se i
                                         Ci [v 2 ] = 2 ·             2     · (v 2 + se 2
                                                                                       i)
                                                           (v 2 +se i )3                    ,
                                                                                  -2
                                                 =2·       se 4
                                                              i · (v
                                                                       2
                                                                           + se 2
                                                                                i)

will result in a minimum at v 2 =  2 . Thus, by appropriately choosing the constant of integration,
we can arrive at the desired result:

                                            Ci [v 2 ] = 2 ·         1
                                                                      2         · se 2
                                                                                     i.
                                                                 1+se i /v 2




                                                                55
B      Distributional Assumptions
The statistical approach described in Section 2 models the anomaly-discovery process as independent
draws from a normal distribution. The key assumption is that the strength of cross-sectional
predictors is drawn from a common distribution. The assumption of normality is not essential.
   To see why, consider an alternative setting where the true slope coefficients are drawn from a
Laplace distribution:                                 
                                           iid
                                       i  Laplace[ 2 / ].
                                                                                                              
                                                                                                              2
The probability density function of this Laplace distribution is given by pdf[ ] = ·1     · e-  ·| | ,
                                                                                          2
which implies that the mean and variance of the resulting draws are the same as in the original
normally distributed case: E[i ] = 0 and Var[i ] =  2 . We now show that, even though the true
slope coefficients are being drawn from a different prior distribution, you can apply the exact same
logic to estimate the anomaly base rate.
    If the true slope coefficients are drawn from a Laplace distribution, then the functional form of
our inference problem will change slightly. Now, the negative log likelihood of the true slope
coefficient taking on a particular value, i =  , given the realized cross-section of returns and
lagged values will correspond to
                                                                             2       
                                   1                                                 2
      - log Pr[ |R, Xi ] =              2     ·       n   Rn - µ
                                                               ^ -  · Xn,i       +   
                                                                                     × | | + · · ·
                               2·(N ·se i )
                                                                                 2          2
                           =     1
                                    2   ·     1
                                              N
                                                  ·       n        ^ -  · Xn,i
                                                              Rn - µ               + 8 · se
                                                                                          
                                                                                            i
                                                                                              × | |   + ···
                               2·se i


where the "· · · " represents constants that do not depend on the choice of  . This inference problem
suggests using a different penalized-regression procedure than before--i.e., a procedure with an
absolute-value penalty rather than a quadratic penalty like a Ridge regression.
   The least absolute shrinkage and selection operator (the LASSO; Tibshirani, 1996) is just such a
penalized-regression procedure. Estimating the LASSO involves solving the optimization problem
below:
                                                                    2
                      ^i [] def
                            = arg min N  1
                                           · n Rn - µ  ^ -  · Xn,i +  · | | .
                                    

Note that this is just the optimization problem given in Equation (5) when replacing  2 with | |.
What is more, when there is only one variable that has been standardized to have zero mean and
unit variance, it is possible to characterize the solution to this optimization problem analytically:
                                        ^i [] = Sign[
                                                     ^i ] · (|^i | - )+ .

Thus, as pointed out in Park and Casella (2008), the LASSO's absolute-value penalty can be
interpreted as the effect of imposing Laplace priors on an inference problem when the tuning
parameter is chosen as follows:             
                                       i = 8 · se 2i /.

    The proposition below shows that, if the true slope coefficients are drawn from a Laplace
distribution instead of a normal distribution, then we can learn about the anomaly base rate by
studying the best-fit tuning parameter in the LASSO instead of a Ridge regression. Different prior
distribution. Different penalized regression. Same underlying approach.


                                                               56
Proposition B (Econometric Estimator, The LASSO). Let E[·] denote an expectations operator
                                                                                      2
evaluated with respect to realizations of i drawn from a Laplace distribution. If v ^i  denotes the
parameter estimate with the minimum in-sample prediction error subject to an overfitting penalty for
the ith variable,
                                                                 
                    2 def
                                Erri se 2    2
                                               + 2 · 1 |^i | >      8 · se 2         2
                  ^i
                  v   = arg min
                            2           i /v                               i /v · se i ,
                            v >0


then for all  2 > 0 we have that E[^
                                   vi2
                                       ] = 2.
                                          
Proof (Proposition B). The 2 · 1 | ^i | > 8 · se 2 /v · se 2 term in Proposition B is an information-
                                                 i         i
criterion penalty. This sort of penalty takes the form 2 · (df /N ) × Var[n,i ] where df represents the
estimator's degrees of freedom. Zou et al. (2007) proves that the number of non-zero slope
coefficients is an unbiased estimator for the degrees of freedom when using the LASSO:

                                        Pr |^i | >  = df[].

Thus, since Var[n,i
                    ] = N · se 2
                               i , the generalized information-criterion penalty reduces to the one
                         2
above when i = 8 · se i / .




                                                  57
C     Additional Tables




                                               30/30 LS             $1 filter          $5 filter
                                                  (1)                 (2)                (3)
         Gross:              E[Rs,t ]             1.99                1.72                2.48
                            E[Rs,t ]              1.13                0.74               1.48
    Net of 1.0%
                           Sd[Rs,t ]              6.17                4.50               4.02
       minimum
                         Skew[Rs,t ]              1.26                0.94               1.90
    performance
      threshold:         Kurt[Rs,t ]              7.59                5.79              13.13
                              SRs                 0.63                0.57               1.28

Table C1. Performance Metrics, Data Sample Robustness of performance statistics
for the excess returns of base-rate adjust strategy. Column (1): Statistics for portfolio going long
30% of stocks with highest-predicted returns and going short 30% of stocks with lowest predicted
returns. Column (2): $1 price filter for small and illiquid stocks. Column (3): $5 price filter for
small and illiquid stocks. All return statistics quoted in % per month. E[Rs,t ]: mean monthly
return. E[Rs,t ]: mean net monthly return. Sd[Rs,t ]: standard deviation of net monthly returns.
Skew[Rs,t ]: skewness of net monthly returns. Kurt[Rs,t ]: kurtosis of net month returns. SRs :
annualized Sharpe ratio using net monthly returns. Sample period: January 1990 to June 2015.




                                                58
                                                 AR(1)             AR(3)            AR(5)
                                                  (1)               (2)              (3)
            Gross:              E[Rs,t ]            2.06            1.72             1.59
                               E[Rs,t ]             1.08            0.74             0.59
      Net of 1.0%
                              Sd[Rs,t ]             5.11            4.50             4.46
         minimum
                            Skew[Rs,t ]             0.79            0.94             1.13
      performance
        threshold:          Kurt[Rs,t ]             4.72            5.79             6.40
                                 SRs                0.73            0.57             0.46

Table C2. Performance Metrics, Autoregressive Order. Robustness of performance
statistics to variations in the autoregressive order to forecast the out-of-sample base rate.
Column (1): autoregressive process of order 1. Column (2): autoregressive process of order 3.
Column (3): autoregressive process of order 5. All return statistics quoted in % per month.
E[Rs,t ]: mean monthly return. E[Rs,t ]: mean net monthly return. Sd[Rs,t ]: standard deviation of
net monthly returns. Skew[Rs,t ]: skewness of net monthly returns. Kurt[Rs,t ]: kurtosis of net
month returns. SRs : annualized Sharpe ratio using net monthly returns. Sample period: January
1990 to June 2015.




                                               59
                                                   Regularized         Cross-Validated
                                                      (1)                    (2)
            Gross:             E[Rs,t ]               1.72                   1.68
                               E[Rs,t ]               0.74                   0.69
      Net of 1.0%
                              Sd[Rs,t ]               4.50                   4.25
         minimum
                            Skew[Rs,t ]               0.94                   0.98
      performance
        threshold:          Kurt[Rs,t ]               5.79                   7.47
                                 SRs                  0.57                   0.70

Table C3. Performance Metrics, Cross-Validated. Robustness of performance statis-
                     2
tics to estimating v^i,t using cross-validation rather than the regularized estimator from
Proposition 2.2. Column (1): baseline estimates using regularized estimator. Column (2):
estimates using 10-fold cross-validation procedure as used in simulation analysis. See Figure 5
and surrounding discussion on page 21 for more details. All return statistics quoted in % per
month. E[Rs,t ]: mean monthly return. E[Rs,t ]: mean net monthly return. Sd[Rs,t ]: standard
deviation of net monthly returns. Skew[Rs,t ]: skewness of net monthly returns. Kurt[Rs,t ]:
kurtosis of net month returns. SRs : annualized Sharpe ratio using net monthly returns. Sample
period: January 1990 to June 2015.




                                              60
