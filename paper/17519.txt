                                NBER WORKING PAPER SERIES




      IDENTIFICATION AND INFERENCE WITH MANY INVALID INSTRUMENTS

                                           Michal Kolesár
                                             Raj Chetty
                                          John N. Friedman
                                          Edward L. Glaeser
                                          Guido W. Imbens

                                        Working Paper 17519
                                http://www.nber.org/papers/w17519


                      NATIONAL BUREAU OF ECONOMIC RESEARCH
                               1050 Massachusetts Avenue
                                 Cambridge, MA 02138
                                     October 2011




We thank the National Science Foundation for financial support. The views expressed herein are those
of the authors and do not necessarily reflect the views of the National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2011 by Michal Kolesár, Raj Chetty, John N. Friedman, Edward L. Glaeser, and Guido W. Imbens.
All rights reserved. Short sections of text, not to exceed two paragraphs, may be quoted without explicit
permission provided that full credit, including © notice, is given to the source.
Identification and Inference with Many Invalid Instruments
Michal Kolesár, Raj Chetty, John N. Friedman, Edward L. Glaeser, and Guido W. Imbens
NBER Working Paper No. 17519
October 2011
JEL No. C01,C2,C26,C36

                                              ABSTRACT

We analyze linear models with a single endogenous regressor in the presence of many instrumental
variables. We weaken a key assumption typically made in this literature by allowing all the instruments
to have direct effects on the outcome. We consider restrictions on these direct effects that allow for
point identification of the effect of interest. The setup leads to new insights concerning the properties
of conventional estimators, novel identification strategies, and new estimators to exploit those strategies.
A key assumption underlying the main identification strategy is that the product of the direct effects
of the instruments on the outcome and the effects of the instruments on the endogenous regressor has
expectation zero. We argue in the context of two specific examples with a group structure that this
assumption has substantive content.


Michal Kolesár                                       Edward L. Glaeser
Department of Economics                              Department of Economics
Harvard University                                   315A Littauer Center
1805 Cambridge St.                                   Harvard University
Cambridge, MA 02138                                  Cambridge, MA 02138
kolesarmi@gmail.com                                  and NBER
                                                     eglaeser@harvard.edu
Raj Chetty
Department of Economics                              Guido W. Imbens
Harvard University                                   Department of Economics
1805 Cambridge St.                                   Littauer Center
Cambridge, MA 02138                                  Harvard University
and NBER                                             1805 Cambridge Street
chetty@fas.harvard.edu                               Cambridge, MA 02138
                                                     and NBER
John N. Friedman                                     imbens@fas.harvard.edu
Harvard Kennedy School
Taubman 356
79 JFK St.
Cambridge, MA 02138
and NBER
john_friedman@harvard.edu
1    Introduction
A key condition underlying identification of the causal effect in instrumental variable models
is the assumption that the instruments only affect the outcome of interest through their
effect on the endogenous variable. However, in many empirical applications, there is a
concern that instruments may also affect the outcome directly. To address this concern, this
paper establishes conditions under which the effects of interest are identified in settings with
direct effects of instruments on the outcome. Following Kunitomo (1980), Morimune (1983),
Bekker (1994), Hahn (2002), Chamberlain and Imbens (2004), Chao and Swanson (2005),
Hansen, Hausman and Newey (2008), Chioda and Jansson (2009), Anderson, Kunitomo and
Matsushita (2010), and others, we focus on the case with many instruments where each
individual instrument is weak in the Staiger and Stock (1997) sense but collectively the
instruments have substantial predictive power.
    In the absence of direct effects of the instruments the limited-information-maximum-
likelihood (liml) estimator is consistent (Bekker, 1994) and efficient (Chioda and Jansson,
2009; Anderson et al., 2010) under the Bekker many-instrument asymptotic sequence given
homoscedasticity. The two-stage-least-squares (tsls) estimator is inconsistent (Kunitomo,
1980; Bekker, 1994), but a bias-corrected version, known as the bias-corrected-two-stage-
least-squares (btsls) (Donald and Newey, 2001), estimator remains consistent. Another
consistent estimator in this setting is the jackknife-instrumental-variables-estimator (jive)
(Phillips and Hale, 1977; Angrist, Imbens and Krueger, 1999). Motivated by our leading
examples, and as in Anatolyev (2011), we also allow the number of exogenous covariates to
increase in proportion with the sample size. This requires some minor modification of the
btsls and jive estimators (denoted by mbtsls and mjive), but does not affect the consistency
of liml.
    We examine the robustness of these five estimators (liml, btsls, jive, mbtsls, and mjive)
to the presence of direct effects in this many-instrument setting. We show that liml loses
consistency if direct effects are present. The intuition is that the liml estimator attempts to
impose proportionality of all the reduced form coefficients. This explains the efficiency of
liml in the absence of direct effects, but because the reduced form coefficients are no longer
proportional when direct effects are present, it makes liml sensitive to their presence. On the
other hand, under the assumption that the product of the direct effects of the instruments
on the outcome and the direct effects on the endogenous regressor has expectation zero,
the btsls and jive estimators (in the case with a fixed number of exogenous variables) or
their many-exogenous-variables modifications mbtsls and mjive (in general) remain consis-

                                              [1]
tent. We argue through some examples and a link with the clustering literature that this
identifying assumption, although not innocuous, substantively weakens existing identifica-
tion conditions. The intuition for the robustness compared to liml is that the btsls, jive,
mbtsls, and mjive estimators, like the tsls estimator, can be thought of as two-stage estima-
tors. In the first stage a single instrument is constructed as a function of only instruments
and endogenous regressors, not involving the outcome variable. This constructed instrument
is then used in the second stage to estimate the parameter of interest using methods for
just-identified settings. Identification only requires validity of the constructed instrument,
not of all the individual instruments.
    We then study in detail two leading cases that motivate the set up and illustrate the
range and applicability of our new identifying assumptions. Both cases have a clustering
structure where the instruments are related to the cluster indicators. Such settings are
often the reason for the presence of many instruments. The many-instrument asymptotic
approximation implies that, as is common in clustering settings, large sample approximations
are based on the number of clusters growing with the sample size while the number of sampled
units from each cluster remains fixed.
    The first of the two special cases arises when the instruments are cluster indicators. For
example, Fryer (2011) and Levitt, List, Neckermann and Sadoff (2011) conducted a series
of experiments where students in randomly selected schools were given varying financial
incentives to improve achievement on test scores. Suppose we are interested in the effect
of test score achievement on outcomes later in life as in Chetty, Friedman, Hilger, Saez,
Schanzenbach and Yagan (2011). One could use the school indicators study as instruments
to capture the fact that the incentives varied between schools. However, one might be
concerned that schools also affect outcomes directly, not just through test scores. Our results
suggest that a sufficient, and, because of the randomization, substantially weaker condition
for identification is that the direct effects of the school on the outcomes are uncorrelated
with the effects of the school on test scores.
    In another example within this class, Aizer and Doyle, Jr. (2011) and Nagin and Snod-
grass (2011) study the effect of incarceration on subsequent outcomes. Defendants are ran-
domly assigned to one of a relatively large number of judges. Judges vary in their propensity
to sentence individuals to jail terms. The judge assignment is used as an instrument. One
might be concerned that judges have direct effects on outcomes beyond those mediated
through the effect on incarceration. Our critical identification assumption is that these di-
rect effects of the judges are uncorrelated with the judges’ propensity to incarcerate. This is


                                              [2]
a substantive assumption that may or may not hold in practice, but shifts the discussion of
the validity of inference away from the substantially stronger assumption that judges have
no direct effect on outcomes whatsoever.
    In the second case we have a small number of basic instruments. These basic instruments
are interacted with cluster indicators to generate a large number of instruments. Here the
number of exogenous regressors (which includes the cluster indicators) increases proportional
to the number of instruments. This case is motivated by the Angrist and Krueger (1991,
AK from hereon) study where the basic instruments, four quarter of birth indicators, are
interacted with year and state of birth indicators to generate additional instruments. In the
context of this set up our approach suggests new identification strategies that allow for direct
effects of the instruments on the outcome. In the first of these identification strategies, the
average direct effect of the instruments on the outcome is zero. In the second identification
strategy, the average direct effect (equal to the direct effect of the basic instrument) is
unrestricted, but the direct effects are uncorrelated with the effect of the instruments on the
endogenous regressor. Again these are not innocuous assumptions, but they substantively
weaken the assumption that all instruments are valid.
    The results in this paper contribute to two strands of literature. First, we contribute
to the recent many-instrument literature that has extended the earlier work by Kunitomo
(1980), Morimune (1983), Bekker (1994), and Chao and Swanson (2005). In recent work
Anatolyev (2011) also relaxes the assumption of fixed number of exogenous regressors. Haus-
man, Newey, Woutersen, Chao and Swanson (2009); Chao, Swanson, Hausman, Newey and
Woutersen (2010) and Ackerberg and Devereux (2009) relax the assumption of homoscedas-
ticity. Hansen et al. (2008), Belloni, Chen, Chernozhukov and Hansen (2011) and Gautier
and Tsybakov (2011) allow the first stage to be estimated non-parametrically. This paper
takes a complementary approach: we relax the assumption of no direct effects, but keep the
rest of the model simple to maintain tractability.
    Second, we contribute to the literature studying properties of instrumental variables
methods allowing for direct effects in settings with a fixed number of instruments. The
focus of this literature has been on correcting size distortions of tests, biases of estimators,
sensitivity analyses, and bounds in the presence of direct effects. Fisher (1961, 1966, 1967),
Caner (2007); Berkowitz, Caner and Fang (2008) and Guggenberger (2010) analyze the
implications of local (small) violations of exogeneity assumption. Hahn and Hausman (2005)
compare biases for different estimators in the presence of direct effects. Conley, Hansen and
Rossi (2007); Ashley (2009) and Kraay (2008) propose sensitivity analyses in the presence of


                                              [3]
possibly invalid instruments. Nevo and Rosen (2010) consider assumptions about the sign of
the direct effects of the instruments on the outcome to derive bounds on the parameters of
interest. Reinhold and Woutersen (2011) and Flores and Flores-Lagunes (2010) also derive
bounds allowing for direct effects of the instruments on the outcome. The current paper is
the first to derive (point) identification results in the presence of non-local departures from
the no-direct-effects assumption or exclusion restriction.
    The rest of the paper is organized as follows. In Section 2 we introduce the set up and
the notation. In Section 3 we introduce the estimators. In Section 4 we present the main
formal results allowing for direct effects of the instruments. In Section 5 we discuss in detail
two leading cases with a clustering structure. We apply the methods developed in this paper
to the data analyzed by AK in Section 6. Section 7 concludes.


2     Set Up
We consider the following instrumental variables model:

       Yi = Xi β + Wi0 δ + Zi0 γ + i .
                                                                                            (2.1)
      Xi = Zi0 π12 + Wi0 π22 + νi .

The first equation relates a scalar outcome Yi , i = 1, . . . , N , to a potentially endogenous
scalar regressor Xi . Wi is a vector of exogenous regressors with dimension LN , and Zi is
a vector of instruments with dimension KN . The second equation relates the endogenous
regressor Xi to the exogenous regressors Wi the instruments Zi . The object of interest is the
coefficient β on the endogenous regressor in the outcome equation.
    The model (2.1) modifies the conventional many-instruments model (e.g., Bekker (1994))
in two ways. First, we allow γ to be non-zero, thus allowing for direct effects of the instrument
on the outcome. If we restrict γ = 0, then the exclusion restriction holds, and the instruments
are valid. If we leave γ unrestricted, then β, the coefficient of interest, is not identified. In
this paper, we will be concerned with determining assumptions on γ that are weaker than
γ = 0, but that still allow us to identify β. Second, like Anatolyev (2011), we allow the
number of exogenous regressors, LN , to change with the sample size. The main motivation
for this extension is that often the presence of a large number of instruments is the result
of interacting a few basic instruments with many exogenous covariates. We discuss such an
example in detail in Section 5.2.
    Because the number of instruments and the number of exogenous variables changes with

                                               [4]
the sample size, the distribution of some of the random variable also changes with the sample
size. To be precise, we should therefore index the random variables and parameters by the
sample size N . For ease of notation we drop this index.
    We assume that the pairs of structural errors (i , νi ) are mutually independent, and
conditionally homoscedastic:
         "        !         !0                                     #
             i        i
     E                           Z1 , . . . , ZN , W1 , . . . , WN = Σ
             νi        νi

Recent papers by Chao et al. (2010) and Hausman et al. (2009) investigate the implications
of heteroscedasticity in the setting with many valid instruments, and show that liml loses
some of its attractive properties in that case. Our results complement theirs in the sense
that our results highlight a different potential concern with liml. To simplify the derivation
of distributional results, we will assume in addition that the structural errors Normally
distributed. We do not require Normality for consistency arguments.
     In the remainder of this section we introduce some additional notation. Let Y be the
N -component vector with ith element Yi , X the N -component vector with ith element Xi ,
 the N -component vector with ith element i , ν the N -component vector with ith element
νi , W the N × LN matrix with ith row equal to Wi0 , and Z the N × KN matrix with ith row
equal to Zi0 . Let X = (X, W) be the full matrix of endogenous and exogenous regressors,
let Y = (Y, X) be the full matrix of endogenous variables, and let Z = (Z, W) be the full
matrix of exogenous variables. Define for an arbitrary N × J matrix S the following four
N × N matrices, the projection matrix PS , the matrix MS that projects on the orthogonal
complement of S, the diagonal matrix DS with diagonal elements equal to those of the
projection matrix, and the product of MS and (1 − DS )−1 :
                  −1                                   −1
PS = (S (S0 S)         S0 ,      MS = I − (S (S0 S)         S0 ,       DS = Diag(PS ),       and HS = MS (1 − DS )−1 .

We use the subscript ⊥ as shorthand for taking residuals after regression on the exogenous
regressors W, so Z⊥ = MW Z, X⊥ = MW X, Y⊥ = MW Y, and Y⊥ = MW Y. We also
denote by ιN and N -dimensional vector of ones.
   Define the augmented concentration parameter, the two by two matrix ΛN :
                                       !
                      ΛN,11 ΛN,12                         0                       
     ΛN =                                  =       γ π12        Z0⊥ Z⊥       γ π12       .                    (2.2)
                      ΛN,12 ΛN,22

The (2, 2) element of ΛN , denoted by ΛN,22 is a key measure of the strength of the instru-

                                                                [5]
ments. The (1,1) element, ΛN,11 , measures the degree of misspecification. In the case with
valid instruments, γ = 0, ΛN,11 = ΛN,12 = 0 and the only non-zero element of ΛN is ΛN,22 .
The (2, 2) element ΛN,22 is closely related to the conventional concentration parameter (Mar-
iano, 1973; Rothenberg, 1984), defined as ΛN,22 /Σ22 . Here, following Andrews, Moreira and
Stock (2006), we use the version without dividing by the structural variance Σ22 because
that will simplify the discussion later.


3    Estimators
In this section we introduce the five estimators for β whose properties we shall study. All
five have asymptotically equivalent in the setting with a fixed number of valid instruments
and a fixed number of exogenous regressors. Four of these estimators have been introduced
previously, and the fifth is a minor modification of a previously proposed estimator. The
first three estimators fit into the k-class (Nagar, 1959; Theil, 1961, 1971; Davidson and
MacKinnon, 1993). Given a scalar k, a k-class estimator for (β, δ) is given by:
             !
        β̂k      0             −1  0              
               = X (I − kMZ )X       X (I − kMZ )Y .
         δ̂k

We are primarily interested in the estimator for β, which can be written using the ⊥ notation
as

                                      −1
     β̂k = (X0⊥ (I − kMZ⊥ )X⊥ )            (X0⊥ (I − kMZ⊥ )Y⊥ ) .                       (3.1)

A prominent member of the k-class is the two-stage-least-squares (tsls Basmann, 1957; Theil,
1961) estimator, with k̂tsls = 1. This estimator has been shown to be inconsistent under
many-instrument asymptotics, see Kunitomo (1980) and Bekker (1994). We therefore do not
further investigate its properties under the various generalizations of the many-instrument
setup here. Instead we consider a bias-corrected version of the tsls estimator. Nagar (1959)
suggested the correction k̂nagar = 1 + (KN − 2)/N, but the first of the five estimators we
focus on is a slightly different version suggested by Donald and Newey (2001), with

                       1
     k̂btsls =                    .
                 1 − (KN − 2)/N

Although in samples with a moderate number of instruments the difference between the
Nagar and Donald-Newey estimators is small, this difference does not go away under many-

                                                     [6]
instruments asymptotics with KN /N → αK > 0, and only the Donald-Newey version is
consistent. As we will show in the next section, once we allow LN to increase with sample
size, btsls also loses consistency. To address this issue, the second estimator we consider is a
further modification of the Donald-Newey bias-corrected estimator that achieves consistency
even when LN /N → αL > 0:
                       1 − LN /N
      k̂mbtsls =                     .
                   1 − KN /N − LN /N
This estimator is also considered in Anatolyev (2011).
    The third estimator we consider is the limited-information-maximum-likelihood estimator
(liml, Anderson and Rubin, 1949), with

                       (Y − Xβ)0 MW (Y − Xβ)
      k̂liml = min                           .
                   β   (Y − Xβ)0 MZ (Y − Xβ)
This estimator has been shown to be asymptotically efficient under many-instrument asymp-
totics (Chioda and Jansson, 2009; Anderson et al., 2010).
    The fourth estimator we study in the current paper is the jackknife-instrumental-variables
estimator (jive Phillips and Hale, 1977; Angrist et al., 1999):

                                        −1
      β̂jive = (X0⊥ (MW − HZ ) X⊥ )          (X0⊥ (MW − HZ ) Y⊥ ) .                                  (3.2)

Ackerberg and Devereux (2009) present simulation evidence that this estimator is biased
when the number of exogenous regressors is large, and suggest a bias-corrected version. We
study a new version of the jackknife estimator, closely related to the Ackerberg-Devereux
estimator, which we refer to as the modified jive estimator, or mjive:

                                                          −1
      β̂mjive = (X0⊥ (MW − (1 − LN /N )HZ ) X⊥ )               (X0⊥ (MW − (1 − LN /N )HZ ) Y⊥ ) . (3.3)

We will show that unlike the original jive estimator, this estimator remains consistent even
if LN /N → αL > 0.
     The focus of the current paper is on the properties of these five estimators, that is,
β̂btsls , β̂mbtsls , β̂liml , β̂jive , and β̂mjive , under various assumptions about the rates at which the
number of instruments and exogenous regressors increase with the sample size, KN , LN , and
the assumptions about the parameters governing the misspecification, γ.




                                                    [7]
4     Many Invalid Instruments
In this section we look at the properties of the five estimators allowing for many exogenous
covariates (LN /N → αL > 0), and allowing for direct effects of the instruments (γ 6= 0). If
we fix αL = 0 and γ = 0, we are in the many instrument case studied in the literature (e.g.
Bekker, 1994; Morimune, 1983; Hahn, 2002; Chao and Swanson, 2005). If we also restrict
αK = 0, we are back in the case with conventional instrumental variables asymptotics
discussed in most textbooks (e.g. Wooldridge, 2002; Angrist and Pischke, 2009).
    We make the following assumptions.
Assumption 1.(Instruments and exogenous variables)
  (i) Zi ∈ RKN , Wi ∈ RLN , i ∈ R, νi ∈ R, for i = 1, . . . , N , N = 1, . . . are triangular arrays
      of random variables with (Zi , Wi , i , νi ), i = 1, . . . , N exchangeable.
 (ii) Z is full column rank with probability one.
(iii) (PZ )ii < c for some c < 1 for all i = 1, . . . , N with probability one.
                           √
(iv) maxi≤N |(Z⊥ )0i π12 |/ N → 0 and;
                   P                                        P
 (v) supN supi≥1 j |(PZ⊥ )ij | < C and supN supi≥1 j |(PW )ij | < C for some C < ∞ with
      probability one

The first two parts of this assumption are standard, with a minor adaption to allow for many
exogenous variables. The remaining three parts are technical assumptions we use to deal
with the jive and mjive estimators.
Assumption 2.(Model)
 (i) (i , νi )0 | Z, W are iid with mean zero, positive definite covariance matrix Σ, and finite
     fourth moments;
(ii) The distribution of (i , νi )0 | Z, W is Normal.

For consistency we only use the first part of this assumption. For the distributional results we
use Normality to highlight the specific modifications to the asymptotic distributions coming
from the direct effects of the instruments.
Assumption 3.(Number of instruments and exogenous regressors)
For some 0 ≤ αK < 1 and 0 ≤ αL < 1,

      KN /N = αK + o(N −1/2 ),         and      LN /N = αL + o(N −1/2 ).

The first part of this assumption is standard in the many-instrument literature. The second
part is identical to the corresponding assumption in Anatolyev (2011).


                                                 [8]
Assumption 4.(Concentration parameter)
For some positive semi-definite Λ with Λ22 > 0,
                p
      ΛN /N → Λ,            and E [ΛN /N ] → Λ.

The first part of assumption 4 is a natural extension of the assumption underlying the
Bekker many-instrument asymptotics. The second part of the assumption strengthens this
slightly by also requiring the expectation of the concentration parameter to converge to its
probability limit.
    The first main result establishes the probability limit of the estimators.
Theorem 1.(Consistency with Many Invalid Instruments)
Suppose Assumptions 1(i)–(iii), 2(i), 3 and 4 hold. Then:
                      p              1−αL            Λ22
 (i) (k-class) if k̂ −→ k with k < 1−α K −αL
                                             + Σ22 (1−α K −αL )
                                                                , then:

                        p              Λ12 + (1 − αL − (1 − αK − αL )k)Σ12
                    β̂k̂ −→ βk = β +                                       ,
                                       Λ22 + (1 − αL − (1 − αK − αL )k)Σ22

(ii) (liml) Suppose min eig(Σ−1 Λ) < Λ22 /Σ22 . Then:

                               Λ12 − min eig(Σ−1 Λ)Σ12                    1 − αL     min eig(Σ−1 Λ)
                βliml = β +                            ,     kliml =               +                ,
                               Λ22 − min eig(Σ−1 Λ)Σ22                 1 − αK − αL   1 − αK − αL

(iii) (btsls)

                               Λ12 + {αK αL /(1 − αK )} Σ12                      1
                βbtsls = β +                                ,      kbtsls =          ,
                               Λ22 + {αK αL /(1 − αK )} Σ22                   1 − αK

(iv) (mbtsls)

                               Λ12                    1 − αL
            βmbtsls = β +          ,   kmbtsls =               ,
                               Λ22                 1 − αK − αL

 (v) (jive) Suppose αL < Λ22 /Σ22 . Then:

                               Λ12 − αL Σ21
                βjive = β +                 ,
                               Λ22 − αL Σ22




                                                      [9]
(vi) (mjive)

                            Λ12
             βmjive = β +       ,
                            Λ22

If we impose Λ11 = 0 (implying Λ12 = 0) and αL = 0, the condition for consistency of β̂k̂
is the same as in Chao and Swanson (2005), namely that k̂ → 1/(1 − αK ). Having many
exogenous regressors changes the condition on k̂ to k̂ → (1 − αL )/(1 − αK − αL ).
    A key finding is the robustness of the mbtsls and mjive estimators relative to the liml
estimator. Specifically, if Λ12 is equal to zero, then mbtsls and mjive are consistent even if
Λ11 differs from zero. If the number of exogenous variables is fixed, then btsls and jive are
also consistent if Λ12 = 0. In order for liml to be consistent for all values of Σ, then it has
to be the case that Λ11 is equal to zero (and that immediately implies that Λ12 = 0). To
provide some intuition, consider the reduced-form based on the model (2.1):

      Yi = Zi0 (π12 β + γ) + Wi0 (δ + π22 β) + (νi β + i ),
      Xi = Zi0 π12 + Wi0 π22 + νi .

If the instruments are valid, so that γ = 0, then the vector of reduced-form coefficients
on Zi in the first equation is proportional to π12 , the vector of reduced-form coefficients in
the second equation. The liml estimator tries to impose this proportionality. This leads
to efficiency if proportionality holds (Chioda and Jansson, 2009; Anderson et al., 2010).
However, if γ 6= 0, then the proportionality does not hold in the population, and liml
loses consistency. On the other hand, mbtsls and mjive, like tsls, can be thought of as
two stage estimators. In the first stage composite instruments are constructed, one for
each regressor (endogenous or exogenous) based on the data on the endogenous regressor,
the exogenous variables, and the instruments alone. These instruments are then used to
estimate the parameters of interest using a method for just-identified settings, possibly with
some adjustment. In this procedure proportionality of the reduced forms is never exploited.
This explains why Λ12 = 0 is a sufficient condition for consistency, although it results in
efficiency loss relative to liml when proportionality does hold.
    Next we consider large sample approximations to the distribution of the estimators.
We make use of Assumption 2(ii), which puts Normality on the error terms. If instead of
Normality we only assumed finite fourth moments (Assumption 2 (i)), then the asymptotic
variance terms would depend on the third and fourth moments of the error terms (Hansen
et al., 2008; van Hasselt, 2010). Assuming Normality leads to simpler asymptotic formulae

                                                 [10]
that will allow us to better focus on the effect of relaxing the standard assumptions that
γ = 0 and αL = 0 and highlight the substantive differences. To put the main results for the
case with direct effects in perspective we first present the distributional results for the case
with γ = 0, but αL possibly positive. See Anatolyev (2011) for asymptotic variances for liml
and mbtsls without normality.
Theorem 2.(Asymptotic Normality with Many Exogenous Regressors)
Suppose Assumptions 1–4 hold. Suppose in addition that γ = 0. Then:
  (i) (liml)
             √ 
                                                                                      
                          
                                d         −2              αK (1 − αL )             2
                                                                                      
              N β̂liml − β | Z → N 0, Λ22 · Σ11 Λ22 +                   Σ11 Σ22 − Σ12
                                                         1 − αK − αL
 (ii) (mbtsls)
             √ 
                                                                                       
                            
                                  d         −2             αK (1 − αL )             2
                                                                                       
              N β̂mbtsls − β | Z → N 0, Λ22 Σ11 Λ22 +                   Σ11 Σ22 + Σ12
                                                          1 − α K − α L
(iii) (mjive) Suppose in addition that N −1 i 1−(D1 )ii → τ
                                           P
                                                      Z


            √              
                                   d
                N β̂mjive − β | Z →

                      N 0, Λ−2                                                 2
                                                                                     
                            22 Σ11 Λ22 + (1 − αL ) ((1 − αL )τ − 1) Σ11 Σ22 + Σ12          . (4.1)

The presence of many exogenous variables increases the asymptotic variance of liml and
mbtsls since (1 − αK )αK /(1 − αL − αK ) > αK /(1 − αK ) if αL > 0, but the conclusion that
liml is more efficient than mbtsls does not change. Also, by Jensen’s inequality τ ≥ 1−αK1 −αL ,
so that mbtsls has smaller asymptotic variance than mjive.
    If we want to determine the asymptotic distribution when γ is allowed to differ from
zero, it no longer suffices to simply condition on Z and treat the sequence of parameters γ
as constant. The reason is because the stochastic behaviour of the estimators now depends
on ΛN,12 . Even if the limit Λ12 = 0, if γ differs from zero (and thus Λ11 > 0) it will generally
be the case that ΛN,12 differs from zero for finite N . The stochastic behavior of ΛN,12 affects
the large sample distribution of the estimators, and we need to put sufficient structure on it
to be able to determine this distribution.
    The assumption below puts a random effects structure on the direct effects of the in-
strument on the outcome and the endogenous regressor similar to that in Chamberlain and
Imbens (2004). This provides a natural way of determining the stochastic behaviour of ΛN,12 ,
although it is not necessarily the only way of doing so.
    First we redefine the parameters by orthogonalizing them with respect to Z⊥ as
                                            
                            0    1/2
         γ̃ π̃12 = (αK Z⊥ Z⊥ )         γ π12 .

                                              [11]
Then we consider the following assumption
Assumption 5.(Incidental parameters)
The pairs (γ̃k , π̃12,k ), for k = 1, 2, . . . , KN , are iid with distribution
                !                              ! !
          γ̃k                           µγ
                     Z, W ∼ N                     ,Ξ .
        π̃12,k                          µπ
A key implication of Assumption 5 is that
                                              !                 !
                                           γ0
                          
                      ΛN             1                       
       Λ = plim           = plim            0
                                                Z0⊥ Z⊥ γ π12
                      N             N π12
                         KN
                                    !                !        !       !0
                       1 X     γ̃k
                                                        µ γ     µ γ
         = plim                         γ̃k π̃12,k     =                 + Ξ.
                      KN k=1 π̃12,k                       µπ      µπ

Hence, if we rule out the knife-edge case Ξ12 = −µγ µπ , then under Assumption 5, the
identification condition Λ12 = 0 is equivalent to µγ = 0 and Ξ12 = 0. This equivalence
will be useful in determining more primitive conditions that imply the condition Λ12 = 0.
We defer further discussion of this assumption, and in particular the motivation for making
the independent random effects assumption in terms of (γ̃k , π̃12,k ) (instead of in terms of
(γk , π12,k )) to the next section where we consider two special cases. Next we present the
large sample distribution theory for the case with γ 6= 0.
Theorem 3.(Asymptotic Normality with Many Invalid Instruments)
Suppose that Assumptions 1(i)–(iii), 2–5 hold. Suppose in addition that µγ = Ξ12 = 0. Then:
 (i) (mbtsls)

             √                
                                 d
                 N β̂mbtsls − β →
                                                                             
                          −2         αK (1 − αL )            2
                                                                           Λ22
                  N 0, Λ22 Σ11 Λ22 +              Σ11 Σ22 + Σ12 + Λ11 Σ22 +         ,
                                     1 − αK − αL                            αK

(ii) (mjive) Suppose that in addition N −1                 1
                                                   P
                                                      i 1−(DZ )ii   → τ . Then:

             √            
                             d
              N β̂mjive − β →
                                                                                     
                   −2                                                2
                                                                                   Λ22
             N 0, Λ22 Σ11 Λ22 + (1 − αL )((1 − αL )τ − 1) Σ11 Σ22 + Σ12 + Λ11 Σ22 +         ,
                                                                                    αK

Compared to Theorem 2 (ii)–(iii), allowing for direct effects leads to an additional term in
the asymptotic variance which is proportional to Λ11 , which measures the extent of mis-

                                                    [12]
specification. If Λ11 = 0, then the asymptotic variance of mbtsls and mjive reduces to that
in Theorem 2(ii)–(iii). Note that the extra term decreases in the number of instruments.
The intuition is that as the number of instruments increases, we are better able to deal
with the presence of direct effects, as the product of the direct effects and the effects of the
instruments on the endogenous variable gets averaged out to identify β.


5     Two Special Cases
In this section we consider two special cases with additional structure on the data generating
process. In both cases each unit i belongs to a subpopulation or cluster, with cluster indicator
Gi ∈ {1, 2, . . . , GN }. These clusters are closely related to the instruments. We are interested
in large sample approximations where the number of units sample from each subpopulation is
finite, and the number of subpopulations increases proportional to the sample size, leading to
the many-instruments setting. Let the number of units in group g be Ng , with N = G
                                                                                         P N
                                                                                           g=1 Ng .
For convenience, let us assume that the number of unit sampled from each subpopulation is
the same for all subpopulations, Ng = N/GN for all g.


5.1    Special Case I: Clustering
To focus on the conceptual issues, let us assume there are no exogenous regressors beyond
the intercept, LN = 1. In the first special case the instruments are the cluster indicators,
Zik = 1Gi =k , for k = 1, . . . , GN − 1, so that the number of instruments is the number of
clusters minus one, KN = GN − 1. The general model in (2.1) can now be written as

                            N −1
                           GX
      Yi = δ + βXi +               γk 1Gi =k + i ,                                          (5.1)
                           k=1
                    N −1
                   GX
      Xi = π22 +           π12,k 1Gi =k + νi .                                               (5.2)
                   k=1


Exploiting the special structure here, in combination with the equal cluster size, the aug-
mented concentration parameter can be written as the sample covariance matrix of (γk , π12,k ):

              GN −1
                                                                   !
           N X          (γk − γ)2          (γk − γ) (π12,k − π 12 )
      ΛN =                                                           ,
           GN k=1 (γk − γ) (π12,k − π 12 )     (π12,k − π 12 )2



                                                      [13]
where
             GN −1                                   GN −1
           1 X                                     1 X
       γ=          γk ,        and       π 12   =          π12,k .
          GN k=1                                  GN k=1

Now let us consider Assumption 5 and interpret it in this context. Suppose we have a
large population of clusters. Let µY,g and µX,g be the population means of Yi − βXi and
Xi in cluster g, and let µY and µX be the overall population means. In terms of the
original parametrization, we have: π22 = µX,GN , π12,k = µX,k − µX,GN , δ = µY,GN and
γk = µY,k − µY,GN .
    The natural way to impose a random effects structure on the parameters would be to
assume that the cluster means (µY,k , µX,k ) are independent and
                 !                !     !
          µY,k               µY
                     ∼N               ,Φ .                                                                            (5.3)
          µX,k               µX

This implies
                                     
                r           µY,1 µX,1
                    GN − 1                                                       √
                             ..   .. 
                                                                                                                               
                                                                              1−1/ GN
    γ̃ π̃12 =             B  .    . ,              B=         IGN −1 −       GN −1
                                                                                      ιGN −1 ι0GN −1             − √G1 N ιGN −1
                     GN    
                            µY,1 µX,1


where the (GN − 1) × GN matrix B satisfies BιGN = 0, and BB 0 = IGN −1 . Thus, a ran-
dom effects specification on (µY,k , µX,k ) as in (5.3) implies a random effects specification on
(γ̃, π̃12 ), namely
                    !                 ! !
               γ̃k                 0                             GN − 1
                      Z, W ∼ N          ,Ξ ,         with Ξ =           · Φ.
             π̃12,k                0                               GN

On the other hand, because (γk , π12,k ) measure the effect relative to the last group, GN ,
assuming independence of (γk , π12,k ) of (γl , π12,l ) is not attractive. The random effects as-
sumption on (γ̃k , π̃12,k ) is therefore more reasonable than a random effects assumption on
(γk , π12,k ) would be. Moreover, the augmented concentration parameter can be expressed as
a sample covariance matrix of (γ̃k , π̃12,k ):

               GN                          2           1
                                                             PGN                                       !
            N X                   γ̃k − γ̃             GN        k=1    γ̃k − γ̃       π̃12,k − π̃ 12
       ΛN =                                                                           2                   ,
            GN k=1        γ̃k − γ̃ π̃12,k − π̃ 12                      π̃12,k − π̃ 12

                                                      [14]
where γ̃ = G1N G
                P N                    1
                                         PGN
                  g=1 γ̃k and π̃ 12 = GN   g=1 π̃12,k .
    There is an alternative representation to the set up in (5.1)–(5.2) that ties it in more
closely to the clustering literature. In this alternative representation the demeaned direct
effects µY,g − µY are viewed as random effects reflecting clustering. Let us write the outcome
equation (5.1) as
                                                         
      Yi = µY + βXi + ηi ,          where ηi = µY,Gi − µY + i ,

is the composite residual. The cluster-specific component is equal to the direct effect of the
instrument. Hence, we can think of the residuals ηi having a clustering structure associated
with the instruments
                                                
                                                 Σ + Φ11 if i = j,
                                                 11
                                                
                                                
      E [ηi | Z] = 0             E [ηi ηj | Z] = Φ11        if Gi = Gj , i 6= j,
                                                
                                                
                                                
                                                0          otherwise.

Analogously we can write the second equation with a clustering structure:
                                             
     Xi = µX + ζi      where ζi = µX,Gi − µX + νi ,

and
                                                      
                                                       Σ + Φ22   if i = j,
                                                       22
                                                      
                                                      
      E [ζi | Z] = 0                   E [ζi ζj | Z] = Φ22        if Gi = Gj , i 6= j,
                                                      
                                                      
                                                      
                                                      0          otherwise.

In addition, let Φ12 = E [ζi ηj |Gi = Gj ]. The critical assumption that Λ12 is equal to zero
(equivalent to Φ12 = 0) in this representation amounts to assuming that the cluster compo-
nent in the outcome equation is uncorrelated with the cluster component in the first stage.
This assumption is not innocuous, but assumptions about zero correlations for cluster com-
ponents are often made in clustering settings. It is obviously substantively weaker than
assuming the absence of clustering effects in the outcome equation, or Φ11 = 0.
   In this case with the instruments equal to the group dummies the original jive estimator
has an interesting form. The predicted value for Xi underlying the tsls estimator is the
average value of Xj for all units in the cluster,

      b tsls = 1
                        X
      X i                         Xj
              NGi      j:Gj =Gi


                                                  [15]
The jive estimator modifies that to the average over all units in the cluster, excluding unit
i itself:

        b jive =   1         X
       X  i                           Xj .
                 NGi − 1 j:G =G ,j6=i
                                j     i


With a finite number of units per cluster omitting unit i can make a substantial difference,
and this is reflected in the inconsistency of tsls in this setting.
    The properties of the previously discussed estimators liml, btsls, mbtsls, jive, and mjive
follow as a special case of Theorems 1-3, specializing it to the case with LN = 1 so that
αL = 0. In this case there is no difference asymptotically between jive and mjive and
between btsls and mbtsls because the number of exogenous variables is fixed.


5.2    Special Case II: Clusters with Interactions
In the second case we maintain the cluster structure with cluster indicator Gi ∈ {1, 2, . . . , GN }.
For each unit there is a binary indicator Qi that serves as the basic instrument. More
generally we could have a number of basic instruments, and allow these to be discrete or
continuous. This special case is motivated by the Angrist-Krueger analysis where the basic
instruments are quarter of birth indicators. We generate additional instruments by inter-
acting the cluster indicator with this binary instrument. We include the cluster indicators
as exogenous covariates, Wi,k = 1Gi =k , so that again KN = LN = GN . Again for ease of
exposition let us assume that the clusters are all equal size, Ng = N/GN for all g, and that
                                                              P
the fraction of Qi = 1 units in each cluster is equal to q = i Qi · 1Gi =g /Ng for all g. The
model can now be written as
                     KN
                     X                    KN
                                          X
      Yi = βXi +           δk Wik +             γk Zik + i ,                                 (5.4)
                     k=1                  k=1




             KN
             X                      KN
                                    X
      Xi =         π12,k Zik +            π22,k Wik + νi .                                    (5.5)
             k=1                    k=1


In this case the limit of the augmented concentration parameter is
                    !            !0
              µγ           µγ
      Λ=                              + Ξ.                                                    (5.6)
              µπ           µπ


                                                             [16]
We can directly apply the results from Section 4, which imply that mjive and mbtsls are
consistent and asymptotically normally distributed if Λ12 is equal to zero. In this case
Λ12 = 0 is not necessarily an attractive assumption. It would require that µγ · µπ + Ξ12 = 0,
which essentially requires that both µγ and Ξ12 are zero. We can in fact relax the sufficient
conditions for identification in this special setting. We consider two specific alternatives.
First, we assume that µγ = 0, allowing Ξ12 to be different from zero. Second, we consider
the assumption that Ξ12 = 0, allowing µγ to be different from zero. In both cases Λ12 6= 0,
yet the parameter of interest is identified.
   Under the first assumption, µγ = 0, we can simply use the Wald estimator with Qi as
the single instrument and Wi = 1 as a single exogenous regressor:
                 1                          1
                    P                              P
                Nq   i : Qi =1 Yi    −   N (1−q)       i : Qi =0   Yi
     β̂wald =   1
                   P                        1
                                                   P
                Nq  i : Qi =1 Xi     −   N (1−q)       i : Qi =0   Xi

   Adding the interactions of the type Qi · 1Gi =k as additional instruments would lead to
inconsistency if we use the liml, btsls, mbtsls, jive or mjive estimators.
Theorem 4.(Zero Mean)
Suppose the model in (5.4)-(5.5) holds. Suppose also that Assumptions 1–3 and 5 hold.
Suppose that in addition µγ = 0 and µπ 6= 0. Then β̂wald is consistent for β and satisfies
     √              
                       d
         N β̂wald − β → N (0, (Ξ11 /αK + Σ11 )/µ2π )

In the second case with µγ 6= 0 and Ξ12 = 0, again using all interactions as instruments does
not lead to consistency whether we use liml, btsls, mbtsls, jive or mjive. However, in this
case we can base a consistent estimator on a strategy where we treat Qi as an exogenous
regressor instead of an instrument, and only use the remaining KN − 1 interactions of the
type Qi · 1Gi =k as instruments with the mbtsls or mjive estimators:

                               KN
                               X
      Y i = X i β + Q i δ0 +         Wik δk + i                                       (5.7a)
                               i=1
                        KN
                        X                      N −1
                                              KX
     Xi = Qi π22,0 +          Wik π22,k +               π12,k Qi Wik + νi             (5.7b)
                        k=1                    k=1


This allows for a direct (common) effect of the original basic instrument, but rules out
interaction effects.


                                                             [17]
Theorem 5.(Interactions)
Suppose that the model (5.4)–(5.5) holds. Suppose also that Assumptions 1–5 hold and that
Ξ12 = 0. Then the mbtsls and mjive estimators based on the model (5.7) are consistent for
β. Moreover, under those assumptions:

      √                      d
          N (β̂mbtsls − β) →
                                                                                     
                         −2                                (1 − αK )αK            2
                                                                                     
                N 0, Ξ22 Ξ11 Ξ22 /αK + Ξ11 Σ22 + Ξ22 Σ11 +             Σ11 Σ22 + Σ12
                                                            (1 − 2αK )

and

      √                      d
          N (β̂mjive − β) →
      N 0, Ξ−2                                                                        2
                                                                                               
            22 Ξ11 Ξ22 /αK + Ξ11 Σ22 + Ξ22 Σ11 + (1 − αK )((1 − αK )τ − 1) Σ11 Σ22 + Σ12


               q2         (1−q)2
where τ =    q−αK
                     +   1−q−αK
                                   is the probability limit of tr((I − DZ )−1 /N ).


6     An Application
We apply some of the methods to a subset of the Angrist and Krueger (1991) data. We
use individuals born in the first and fourth quarter (so we have a single binary basic in-
strument, although this is not essential), dropping observations from Alaska because there
are some years birth quarters with no observations, leaving us with observations on 162,487
individuals.
    Let Wik , for k = 1, . . . , GN be the cluster indicators, corresponding to year of birth times
state of birth interactions, so that GN = 500, and let Qi be the binary quarter of birth
indicator. The general model we consider is

                      KN
                      X                KN
                                       X
      Yi = βXi +            δk Wik +         γk Qi Wik + i ,                                (6.1)
                      k=1              k=1




             KN
             X                       KN
                                     X
      Xi =          π12,k Qi Wik +         π22,k Wik + νi .                                  (6.2)
              k=1                    k=1




                                                         [18]
                 !                     !    !
          γk                      µγ
                     Z, W ∼ N              ,Ξ .
         π12,k                    µπ

    We look at six estimators, the five studied in this paper and the two-stage-least-squares
(tsls) estimator. We consider three sets of instruments and exogenous variables.
    In the first setting, we use a single binary instrument, an indicator for being born in
the fourth quarter, Zi = Qi . There are no exogenous covariates beyond the intercept. The
properties of this estimator are captured by Theorem 4. In particular, in this just-identified
case the iv estimator is valid here if the average direct effect of the instruments is zero,
µγ = 0.
    In the second case we interact the qob dummy with state of year times year of birth
dummies, for a total of 500 instruments, and 500 exogenous regressors. Here Theorems 1
and 3 contain the relevant results. In this case liml is not consistent unless Ξ11 , Ξ12 and µγ
are zero. The mjive and mbtsls estimators are consistent under the weaker condition that
the linear combination Λ12 = Ξ12 + µγ · µπ is equal to zero. Within the context of the model
this setting requires the strongest conditions.
    In the third case we only use the interactions as instruments and treat the basic quarter
of birth dummy as an exogenous variable rather than as an excluded instrument. We also
include the year of birth times quarter of birth dummies as exogenous covariates. For this
case Theorem 5 has the appropriate results. Here liml is not consistent unless both Ξ11 and
Ξ12 are equal to zero. The mbtsls and mjive estimators are consistent under the weaker
condition that Ξ12 = 0.
    Table 1 presents the estimates and standard errors under various assumptions. Liml,
mbtsls, and mjive yield similar point estimates, irrespective of the set of instruments. Jive
yields smaller point estimates under the designs which include many exogenous regressors,
which is consistent with Theorem 1. On the other hand, the bias of btsls under these designs
appears small.
    The standard errors are quite different though for the different estimators when we use a
large number of instruments. Taking into account the large number of exogenous variables
does not appear to matter very much. Neither does taking into account non-zero values for
µγ , Ξ12 or Ξ11 . In this specific case this appears to be due to the fact that point estimates
for Λ11 conditional on Λ12 = 0 are close to zero: for this data set there is little evidence for
direct effects of the instruments, consistent with the validity of the instruments.




                                                [19]
7     Conclusion
In this paper we analyze linear models with a single endogenous and many instruments.
Departing from the current literature we allow for direct effects of the instruments on the
outcome. Such direct effects have very different impacts on standard estimators. The liml
estimator, efficient in the many-valid-instrument case, is inconsistent in the presence of such
effects. The btsls and jive estimators are consistent if the direct effects are uncorrelated with
the effects of the instruments on the endogenous regressor. This condition is not innocuous.
In many cases direct effects of the instruments on the outcome may well be correlated with
effects on the endogenous regressor. However, it does shift the discussion of identification
issues in instrumental variables away from the focus on the requirement that none of the
instruments have any direct effects whatsoever, which in cases with many instruments may be
unrealistic, and as this paper shows, unnecessarily restrictive. The results in the paper also
suggest a re-assesment of the merits of liml versus other estimators in the many-instrument
setting.




                                              [20]
Appendices
We first define some additional notation. Write the reduced-form based on Equations (2.1)
as:
                                   
                          π11 π12
                                       + Vi0
              
       Yi Xi = Zi Wi
                            π21 π22

where π11 = γ + π12 β and π21 = δ + π12 β, and Vi = (i + νi β, νi )0 , and let V be the N by
2 matrix with ith row equal to Vi0 . Denote the upper KN × 2 submatrix of the matrix of
reduced-form coefficients by Π1 = (π11 , π12 ). Let:
                 
             1 0
     Γ=
            −β 1

Let Ω = E[Vi Vi0 ] denote the reduced-form covariance matrix. Then:
                                                             
           −1 0     −1     Σ11 + 2Σ12 β + Σ22 β 2 Σ12 + Σ22 β
     Ω = Γ ΣΓ =
                                Σ21 + Σ22 β           Σ22

Let Wd (f, V, V −1 M ) denote a d-dimensional non-central Wishart distribution with f de-
grees of freedom, scale parameter V , and non-centrality parameter M . Let S1/2 denote the
symmetric square root of a symmetric positive semi-definite matrix S.


Appendix A                Auxilliary Lemmata
Lemma A.1.
Consider the quadratic form Q = (M + U )0 C(M + U ), where M ∈ RN ×S , C ∈ RN ×N are
non-stochastic, C is symmetric, and U = (u1 , . . . , uN )0 , with ui ∼ [0, Ω] iid. Let a ∈ RS be a
non-stochastic vector. Assume ui has finite fourth moments. Denote dC = diag(C). Then:
 (i) (Lemma 1, Bekker and van der Ploeg, 2005)

               E[Q | C] = M 0 CM + tr(C)Ω
            var(Qa | C) = a0 ΩaM 0 C 2 M + a0 M 0 C 2 M aΩ + Ωaa0 M 0 C 2 M + M C 2 M aa0 Ω
                             + tr(C 2 )(a0 ΩaΩ + Ωaa0 Ω)
                             + d0C dC [E(a0 u)2 uu0 − a0 Ωaa0 Ω − a0 ΩaΩ] + 2d0C CM aE[(a0 u)uu0 ]
                             + M 0 CdC E[(a0 u)2 u0 ] + E[(a0 u)2 u]d0C CM

      If the distribution of ui is Normal, the last two lines of the variance expression equals
      zero.




                                               [21]
(ii) Suppose that the distribution of ui is Normal, and that, as N → ∞:

                 M 0 C 2 M/N → QCM                                tr(C 2 )/N → τC 2
                                                                                     √
                           is of C may depend on N . Suppose also that maxi≤N kmis k/ N →
       where the elements cP
       0 and supN maxi≤N N    j=1 |cij | = DC < ∞. Then:

                 √                             d
                      N (Qa/N − EQa/N ) → N (0, V ) ,

       where

                 V = a0 ΩaQCM + a0 QCM aΩ + Ωaa0 QCM + QCM aa0 Ω + τC 2 (a0 ΩaΩ + Ωaa0 Ω).

Proof. We only prove Part (ii). We follow the arguments in van Hasselt (2010), who proves
asymptotic Normality of Qa/N when ui are non-normal, but imposes slightly stronger regularity
conditions. By the Cramér-Wold device, it suffices to prove that for any vector b ∈ RS :
                              d
       N −1/2 b0 Qa − E b0 Qa → N 0, b0 V b .
                                          
                                                                                                            (A.1)

Let mb = M b be an N -vector with the ith element equal to Ss=1 mis bs , and similarly for ma , ub
                                                                      P
and ua . Let also Ωp,r = p0 Ωr, for p, r ∈ {a, b}. Then the left-hand side of (A.1) can be written as:
                               X X                                             X                   X a,b
     N −1/2 b0 Qa − E b0 Qa =               cij (uai mbi + mai ubi + uai ubi ) −   cii Ωa,b = N −1/2
                        
                                                                                                      Di ,
                                         i     j                                 i                      i

where, using the fact that cij = cji :
        a,b
                                          X               X               X               X
     DN,i   = cii (uai ubi − Ωa,b ) + ubi   cij uaj + uai   cij ubj + ubi   cij maj + uai   cij mbj .       (A.2)
                                         j<i            j<i              j                 j

              a,b
{N −1/2 DN,i      , 1 ≤ i ≤ N } is a martingale-difference sequence with respect to the filtration FN,i =
σ(u1 , . . . , ui ). To apply a martingale central limit theorem, we need to verify that:
                N      h                i p
                          a,b 2
                X
           −1
       N              E (DN,i ) | FN,i−1 → b0 V b                                                           (A.3)
                i=1




                                                      [22]
Expanding the expression yields:
         X h a,b             i       X                                       XXX
 N −1     E (DN,i )2 | Fn,i−1 = N −1   c2ii (Ωa,a Ωb,b + Ω2a,b ) + Ωb,b N −1     cij cik uaj uak
          i                                                i                                                       i   j<i k<i
                                                                   XXX                                                 XXX
                                            + Ωa,a N −1                             cij cik ubj ubk + 2Ωa,b N −1                          cij cik uaj ubk
                                                                   i j<i k<i                                            i       j<i k<i
                                                       0       0   2                       0        2                            0    2
                                            + Ωb,b a M C M a/N + Ωa,a b M C M b/N + 2Ωa,b b M C M a/N
                                                         XXX                                  XXX
                                            + 2Ωb,b N −1         cij cik mak uaj + 2Ωa,b N −1     cij cik mbk uaj
                                                                   i   j<i     k                                            i    j<i   k
                                                                   XXX                                                  XXX
                                            + 2Ωa,b N −1                             cij cik mak ubj + 2Ωa,a N −1                           cij cik mbk ubj
                                                                   i   j<i     k                                            i    j<i   k
                                                                                                                                                    (A.4)

The last four terms are op (1) since their variance converges to zero. This follows from writing them
as:
                                                                   

                      cij cik mpk urj =                 cij cjk mpk  uri
           XXX                          X        X   X
      N −1                                N −1                           p, r ∈ {a, b}
               i    j<i      k                     i                     j>i    k

and noting that
                                      2                                                                     2
                                                                   √                                                           √
                          cij cjk mpk  ≤ (max mpi / N )2 N −1                                               cik  ≤ (max mpi / N )2 CM
X             XX                                                                    X         X          X
     N −1                                                                                        cij                                4
                                                                                                                                        →0
                                                 i≤N                                                                            i≤N
 i            j>i    k                                                                i        j         k



Now consider the terms of the form:

                    cij cik upj urk = N −1    c2ij upj urj + N −1     cij cik (upj urk + cij cik urj upk )
         XXX                               XX                     XXX
    N −1
               i    j<i k<i                                    i   j<i                             i     j<i k<j
                              
                1 X X 2   1 2 p r                                              1 X 2 p r
                                                 cij cik (upj urk + urj upk ) −
                                             XXX
              =       cij + cii uj uj + N −1                                        cii ui ui
                N          2                                                    2N
                         j       i>j                                           i     j<i k<j                                                 i
               1              1 X 2 p r
              = τC 2 p0 Ωr −     cii ui ui + op (1)
               2             2N
                                             i




                                                                         [23]
The last line follows from applying Chebyshev inequality to the first two terms, and noting that:
                                                                                                           2
                  P P                                                                             1
                                           + 12 c2ii upj urj = var(upj urj ) · N −2
                                                                                         X    X
     var       1
               N        j
                                      2
                                 i>j cij
                                                                                               c2ij + c2ii 
                                                                                                      2
                                                                                         j     i>j

                                                               ≤   var(upj urj )N −2 tC 2 DC
                                                                    →0                     2
                                                                              2
                                              
                                           p r
                                                                X X  X
                                                    −2 0
                1
                                                      p Ωpr0 Ωp        cij cik  ≤ O(N −2 DC
                                                                                           4
                       P P P
      var       N       i j<i k<j cij cik uj uk = N
                                                                                            )→0
                                                                                     j   k<j    i>j

Pulling together the results yields:
                            h                      i
                                  a,b 2
                X
           −1
      N                 E       (DN,i )    | Fn,i−1 = b0 V b+
                   i
                                                  X
                                           N −1        c2ii (Ωa,a b0 Ωb + (Ωa,b )2 − Ωa,a ubi ubi /2 − b0 Ωbuai uai /2 − Ωa,b uai ubi )
                                                   i

This establishes (A.3), since the second term is op (1) as maxi c2ii /N → 0.
                                                        a,b 4
   Secondly, it is possible to show that N −2 i E(DN,i
                                               P
                                                           ) → 0, so that the Lindeberg condition
holds. Hence, a martingale central limit theorem applies, which yields the result.              
Lemma A.2.
Consider a sequence of random matrices {XN }∞                                −1
                                            N =1 such that XN ∼ WS (JN , Ω, Ω ΞN ).
Suppose that ΞN /N → Ξ, and that JN /N = α + o(N −1/2 ), α > 0. Then, for any vector
a ∈ RS

      N −1/2 (XN a/N − (ΞN /N + αΩ)a)
                                           d
                                           → N (0, (a0 ΩaΞ + a0 ΞaΩ + Ωaa0 Ξ + Ξaa0 Ω) + α(a0 ΩaΩ + Ωaa0 Ω))
Proof. By definition of a non-central Wishart distribution, we can decompose XN = (U +M )0 (U +
M ), where U = (u1 , . . . , uJN )0 , uj ∼ N (0, Ω) iid, M 0 M = ΞN , and ΞN /JN → Ξ/α. Hence, we can
apply Lemma A.1 (ii) with C = IJN to get:

       −1/2
      JN           (XN a − (ΞN + JN Ω)a)
                                               d
                                               → N 0, α−1 (a0 ΩaΞ + a0 ΞaΩ + Ωaa0 Ξ + Ξaa0 Ω) + a0 ΩaΩ + Ωaa0 Ω
                                                                                                               


which yields the result.                                                                                                                  
Lemma A.3.
Suppose Assumptions 1, 2(i), 3 and 4 hold. Then:
                    0                p
                Y⊥ Y⊥ /N → Ψ + (1 − αL )Ω                                                                                       (A.5a)
       0                             p
      Y⊥ PZ⊥ Y⊥ /N                  → Ψ + αK Ω                                                                                  (A.5b)
         0                           p
      Y⊥ HZ Y⊥ /N                   →Ω                                                                                          (A.5c)

                                                                        [24]
where
                                              
            Λ11 + 2Λ12 β + Λ22 β 2 Λ12 + Λ22 β
        Ψ=                                                                                             (A.6)
                 Λ12 + Λ22 β           Λ22

These probability limits also hold conditional on Z.

Proof. First we establish the probability limit of V0 PZ⊥ V/N . By Lemma A.1 (i):

        E[V0 PZ⊥ V/N | Z⊥ ] = (KN /N )Ω                                                                (A.7)

Fix a ∈ R2 . Since PZ⊥ is a projection matrix, 0 ≤ (PZ⊥ )ii ≤ 1. Hence,             2
                                                                          P                  P
                                                                            i (PZ⊥ )ii   ≤       i (PZ⊥ )ii   ≤
KN . Therefore:

        var(V0 PZ⊥ Va/N ) = E var(V0 PZ⊥ Va/N | PZ⊥ )
                          = E tr(PZ⊥ /N 2 ) (a0 ΩaΩ + Ωaa0 Ω)
                                          

                              + E N −2 i (PZ⊥ )2ii [E(a0 Vi )2 Vi Vi0 − a0 Ωaa0 Ω − a0 ΩaΩ]
                                       P          
                                                                                                       (A.8)
                            KN                       KN
                          ≤ 2 (a0 ΩaΩ + Ωaa0 Ω) + 2 [E(a0 vi )2 vi vi0 − a0 Ωaa0 Ω − a0 ΩaΩ]
                            N                        N
                                      2
                          = O(KN /N )

Combining Equations (A.7) and (A.8) with Assumption 3 yields :
                        p
        V0 PZ⊥ V/N → αK Ω                                                                              (A.9)

By similar arguments:
                        p
        V0 MW V/N → (1 − αL )Ω                                                                        (A.10)

Next, by Assumption 2 (i), E[Π01 Z0⊥ V/N | Z⊥ ] = 0, so that:

      var Π01 Z0⊥ Va/N = E var Π01 Z0⊥ Va/N | Z⊥ = (a0 Ωa)E Π01 Z0⊥ Z⊥ Π1 /N 2
                                                                          
                                      0 
                        = (a0 Ωa)Γ−1 E ΛN /N 2 Γ−1 = O(1/N )
                                                 

where the last equality follows by Assumption 4. Consequently:
                    p
        Π1 Z0⊥ V/N → 0                                                                                (A.11)

Combining the representation Y⊥ = Z⊥ Π1 + V⊥ with the limits in Equations (A.10) and (A.11),
and Assumption 4 establishes (A.5a):
          0
        Y⊥ Y⊥ /N = Π01 Z0⊥ Z⊥ Π1 /N + Π01 Z⊥ V/N + V0 Z⊥ Π1 /N + V0 MW V/N
                   = Γ−1 ΛN Γ−1 /N + (1 − αL )Ω + op (1)
                   = Ψ + (1 − αL )Ω




                                                 [25]
Claim (A.5b) follows by similar arguments from Equations (A.9) and (A.11):
          0
      Y⊥ PZ⊥ Y⊥ /N = Π01 Z0⊥ Z⊥ Π1 /N + Π01 Z⊥ V/N + V0 Z⊥ Π1 /N + V0 PZ⊥ V/N
                          p
                          → Ψ + αK Ω

Next we prove (A.5c). As an intermediate step, we need to find the probability limit of V0 HZ V.
Because HZ is symmetric, we can apply Lemma A.1 (i), so that:

      E[V0 HZ V/N ] = E tr(HZ /N )Ω = Ω
                                     2 ), we have t = tr(M (I − D )−2 ) = tr((I − D )−1 ) ≤
since tr(HZ ) = N . Denoting t = tr(HZ                                                                                N
                                                          Z      Z                 Z                                 1−c
by Assumption 1. Moreover, i (HZ )2ii = i 12 = N . Hence, for any a ∈ RG+1 :
                             P            P

      var(V0 HZ Va/N ) = E var(V0 HZ Va/N | Z)
                                                                   "                #
                                                                    X
                                           0        0
                              = E[t] · (a ΩaΩ + Ωaa Ω)/N + E   2
                                                                         (HZ )2ii       · [E(a0 vi )2 vi vi0 − a0 Ωaa0 Ω − a0 ΩaΩ]/N 2
                                                                     i
                                 1
                              ≤     (a0 ΩaΩ + Ωaa0 Ω)/N + [E(a0 vi )2 vi vi0 − a0 Ωaa0 Ω − a0 ΩaΩ]/N
                                1−c
                              = O(N −1 )

Therefore, by Chebyshev’s inequality:
          0                            p
      Y HZ Y/N = V0 HZ V/N → Ω                                                                                   (A.12)

Finally, the same calculations go through even if we condition on Z, so that the probability limits
hold also conditional on Z.                                                                      
Lemma A.4.
                                     p                                     1−αL               Λ22 /Σ22
Consider a k-class estimator with k̂ → k subject to k <                  1−αL −αK
                                                                                         +   1−αL −αK
                                                                                                       .   Then under
Assumptions 1, 2 (i), 3 and 4:

              p       Λ12 + (1 − αL − (1 − αK − αL )k)Σ12
      β̂k̂ → β +
                      Λ22 + (1 − αL − (1 − αK − αL )k)Σ22

Proof. Combining Lemma A.3 with the condition k̂ = k + op (1) yields:
                  0               0
     (1 − k̂)Y⊥ Y⊥ /N + k̂Y⊥ PZ⊥ Y⊥ /N = (1 − k)(Ψ + (1 − αL )Ω) + k(Ψ + αK Ω) + op (1)
                                                                                                                 (A.13)
                                                = Ψ + (1 − αL − (1 − αK − αL )k)Ω + op (1)

The (2,2) element of (A.13) is given by:

      (1 − k̂)X0⊥ X⊥ /N + k̂X0⊥ PZ⊥ X⊥ /N = Λ22 + (1 − αL − (1 − αK − αL )k)Σ22 + op (1)

becausee Σ22 = Ω22 . By the condition on k, Λ22 + (1 − αL − (1 − αK − αL )k)Σ22 > 0, so that:
                                                −1
         (1 − k̂)X0⊥ X⊥ /N + k̂X0⊥ PZ⊥ X⊥ /N            = (Λ22 + (1 − αL − (1 − αK − αL )k)Σ22 )−1 + op (1)

                                                        [26]
                                                                                                        (A.14)

The (1,2) element in Equation (A.13) is given by:

       (1 − k̂)X0⊥ Y⊥ /N + k̂X0⊥ PZ⊥ Y⊥ /N = Λ12 + Λ22 β + (1 − αL − (1 − αK − αL )k)Ω12 + op (1)
       = Λ12 + (1 − αL − (1 − αK − αL )k)Σ12 + (1 − αL − (1 − αK − αL )k)Σ22 β + Λ22 β + op (1)
                                                                                          (A.15)

Applying Equations (A.14) and (A.15) to β̂k̂ :

               (1 − k̂)X0⊥ Y⊥ /N + k̂x0⊥ PZ⊥ Y⊥                  Λ12 + ((1 − k)(1 − αL ) + αK k)Σ12
   β̂k̂ =                                                = β+                                       + op (1). 
            (1 −   k̂)X0⊥ X⊥ /N    +   k̂X0⊥ PZ⊥ X⊥ /N           Λ22 + ((1 − k)(1 − αL ) + αK k)Σ22



Appendix B                         Proofs of Theorems
Proof of Theorem 1. The results for a general k-class estimator, btsls and mbtsls follows di-
rectly from Lemma A.4. We therefore just need to derive the results for liml, jive and mjive.
    First, we establish the result for liml. Define
                               0
                          φ0 Y⊥ Y⊥ /N φ
       Q̂N (φ) =           0                .
                        φ0 Y⊥ MZ⊥ Y⊥ /N φ
Then
                                       0
                           (1, −β̃)Y⊥ Y⊥ /N (1, −β̃)0
       k̂liml = min                0                      = min Q̂N (φ)
                   β̃    (1, −β̃)Y⊥ MZ⊥ Y⊥ /N (1, −β̃)0         φ∈S 1


where S 1 denotes the unit circle in R2 . Applying Lemma A.3 yields:

                   p    φ0 (Ψ + (1 − αL )Ω)φ   φ0 T φ
       Q̂N (φ) →                             ≡         ≡ Q(φ)
                        (1 − αL − αK )φ0 Ωφ    φ0 T⊥ φ

where we define T = Ψ + (1 − αL )Ω and T⊥ = (1 − αL − αK )Ω. Assumption 2 (i) guarantees that
the denominator is non-zero for any value of φ. The minimum of Q(φ) is achieved at:

                             1 − αL           1          φ0 Ψφ
       min Q(φ) =                     +              min 0
       φ∈S 1              1 − αK − αL 1 − αL − αK φ∈S 1 φ Ωφ
                             1 − αL     min eig(Σ−1 Λ)
                        =             +                = kliml
                          1 − αK − αL   1 − αK − αL

where the last line follows since the eigenvalues of Ω−1 Ψ correspond to the eigenvalues of Σ−1 Λ.
The minimand φliml is given by the eigenvector corresponding to the smallest eigenvalue of the
matrix:
               1
                           Ω−1 (Ψ + (1 − αL )Ω)
       1 − αK − αL

                                                         [27]
We now need to show that:
                                                   p
      k̂liml − kliml = min Q̂N (φ) − Q(φliml ) → 0                                                (A.1)
                       φ∈S 1

To this end, we first show that the convergence of the objective function is uniform:
                               p
      sup |Q̂N (φ) − Q(φ)| → 0                                                                    (A.2)
      φ∈S 1

Fix φ ∈ S 1 . By triangle inequality:

                                   1                0                      0
|Q̂N (φ) − Q(φ)| ≤         0                 φ0 Y⊥ Y⊥ φ/N − Q(φ)φ0 Y⊥ MZ⊥ Y⊥ φ/N
                      |φ0 Y⊥ MZ⊥ Y⊥ φ/N |
                                   1              0
                                                                         
                                                                           0
                                                                                           
                  =        0                 φ0 (Y⊥ Y⊥ /N − T )φ − Q(φ)φ0 Y⊥ MZ⊥ Y⊥ /N − T⊥ φ
                      |φ0 Y⊥ MZ⊥ Y⊥ φ/N |
                               1          
                                                       0
                                                                               
                                                                                 0
                                                                                                  
                  ≤        0                      φ0 (Y⊥ Y⊥ /N − T )φ + Q(φ) φ0 Y⊥ MZ⊥ Y⊥ /N − T⊥ φ
                      |φ0 Y⊥ MZ⊥ Y⊥ φ/N |
                                                                                                  (A.3)

We now need to bound all three terms in the expression uniformly in φ. Because the trace operator
is the inner product under Frobenius norm, by Cauchy-Schwarz inequality:
                                                                 
             0                             0
       |φ0 (Y⊥ MZ⊥ Y⊥ /N − T⊥ )φ| = tr (Y⊥ MZ⊥ Y⊥ /N − T⊥ )φφ0
                                    p               0
                                  ≤ tr((φφ0 )2 )k(Y⊥ MZ⊥ Y⊥ /N − T⊥ )kF
                                             0
                                       = k(Y⊥ MZ⊥ Y⊥ /N − T⊥ kF
                                       = op (1)
                                                                                    0         p
where the third line follows since kφk2 = 1, and the last line follows since Y⊥ MZ⊥ Y⊥ /N → T⊥ by
Lemma A.3. By similar argument
              0
      |φ0 (Y⊥ Y⊥ /N − T )φ| = op (1)
                                                         0             p                  0
Finally, we bound the denominator. Because Y⊥ MZ⊥ Y⊥ /N → T⊥ > 0, φ0 Y⊥ MZ⊥ Y⊥ φ/N > 0
                          0
wpa1, so that wpa1 |φ0 Y⊥ MZ⊥ Y⊥ φ/N | < C for some C < ∞. Applying these bounds and the
fact that Q(φ) is bounded implies that the right-hand side in (A.3) is op (1), which implies (A.2).
    Next, denote the argmin of Q̂N (φ) by φ̂. Note that k̂liml and hence φ̂ exists wpa1. We can now
establish (A.1), using the uniform convergence result (A.2):

      Q(φliml ) ≤ Q(φ̂) = Q̂N (φ̂) + (Q(φ̂) − Q̂N (φ̂)) ≤ Q̂(φliml ) + (Q(φ̂) − Q̂n (φ̂))
                          = Q(φliml ) + (Q̂N (φliml ) − Q(φliml )) + (Q(φ̂) − Q̂N (φ̂))
                          = Q(φliml ) + op (1)

The probability limit for liml then follows by Lemma A.4.



                                                        [28]
   It remains to establish the results for jive and mjive. Applying Lemma A.3, we get:
                           0                  p
                         Y (MW − HZ )Y/N → Ψ − αL Ω                                                (A.4)
        0                                     p
      Y (MW − (1 − LN /N )HZ )Y/N → Ψ                                                              (A.5)

Because Λ22 > αL Σ22 , it follows from the (2,2) element of (A.4) that:

                         (X0 (MW − HZ )X)−1 = (Λ22 − αL Σ22 ) + op (1)
      (X0 (MW − (1 − LN /N )HZ )X)−1 = Λ22 + op (1)

Combining these with an expansion of the (2,1) element in (A.4) and (A.5) yields the results for
jive and mjive.                                                                               

Proof of Theorem 2. All probability statements are conditional on Z. We omit the condition-
ing for ease of notation.
    Proof of part (i) The liml estimator is given by the minimand of the objective function:

                          (Y⊥ − X⊥ β̃)0 (Y⊥ − X⊥ β̃)
      Q̂N (β̃) =
                        (Y⊥ − X⊥ β̃)0 MZ⊥ (Y⊥ − X⊥ β̃)

The associated first-order condition is proportional to ĝN (β̂liml ) = 0, where

                         1 0                 Q̂N (β̃) 0
      ĝN (β̃) = −         X⊥ (Y⊥ − X⊥ β̃) +         X⊥ MZ⊥ (Y⊥ − X⊥ β̃)
                         N                     N
The derivative of the first-order condition is given by:

     0          X0⊥ X⊥                                   2ĝN (β̃)
   ĝN (β̃) =          − Q̂N (β̃)X0⊥ MZ⊥ X⊥ +                               X0⊥ MZ⊥ (Y⊥ − X⊥ β̃)
                  N                                       0
                                              (Y⊥ − X⊥ β̃) MZ⊥ (Y⊥ − X⊥ β̃)
                                                     p
We will show that for any estimator β̂ with β̂ → β:
        0       p
      ĝN (β̂) → Λ22                                                                               (A.6)

Secondly, we will show that at the true value:
     √
                                                                 
                  d                  αK (1 − αL )            2
                                                                
       N ĝN (β) → N 0, Σ12 Λ22 +                 Σ11 Σ22 − Σ12                                    (A.7)
                                    1 − αK − αL

                       0 (β̂) does not depend on β and it is positive, and since β̂    p
Because the limit of ĝN                                                           liml → β is consistent
by Theorem 1, assertion ((i)) the theorem will follow (see Newey and McFadden, 1994)
   We first prove (A.6). Let φ = (1, −β). By Lemma A.3 and consistency of β̂:

                  φ0 (Ψ + (1 − αL )Ω)φ
                    p                          1 − αL
      Q̂N (β̂) →                   0
                                         =               ≡ kliml
                  (1 − αL − αK )φ Ωφ        1 − αL − αK
                p                       1 − αL
       ĝN (β̂) → −(1 − αL )Σ12 +                 (1 − αK − αL )Σ12 = 0
                                     1 − αL − αK


                                                     [29]
where we use the fact that Λ11 = Λ12 = 0 since γ = 0. Hence:
        0      p
      ĝN (β̂) → Λ22 + (1 − αL )Σ22 − kΣ22 + 0 = Λ22

which proves (A.6). It remains to show that ĝN (βliml ) satisfies a central limit theorem. Let ν̃ =
ν − %, where % = Σ12 /Σ11 be a projection of ν onto space orthogonal to . We have:
       √                               0 MW 
                                                          
                       −1/2      0                  0
         N ĝN (β) = N       ν MZ  0           − X MW 
                                        MZ 
                                       0 MW 
                                                                    
                   = N −1/2 ν̃ 0 MZ  0         − (Z⊥ π12 + ν̃)0 MW 
                                        MZ 
                   = N −1/2 ν̃ 0 MZ  · kliml − (Z⊥ π12 + ν̃)0 MW  + op (1)
                                                                   

                                   0
where the third line follows since 0M W
                                      MZ  = kliml +op (1) by arguments in Lemma A.3, and N
                                                                                            −1/2 ν̃ 0 M 
                                                                                                       Z
is Op (1). Therefore, we can write:
       √
         N ĝN (β) = N −1/2 (Z⊥ π12 + ν̃)0 kliml MZ − MW 
                                                             


This expression is the (2,1) element of the quadratic form:
                            0
      N −1/2  Z⊥ π12 + ν̃ C  Z⊥ π12 + ν̃
                                              


where C = kliml MZ −MW . To establish (A.7), we need to check the assumptions of Lemma A.1(ii).
We have:
                                                                αK (1 − αL )
      tr(C) = o(N −1/2 )                                 τC 2 =                                  (A.8a)
                                                                1 − αL − αK
                                                                                
                0 0                                      i       Σ11        0
      QCM =                                         cov       =                                  (A.8b)
                0 Λ22                                    ν˜i       0 Σ22 − Σ212 /Σ11

Applying Lemma A.1 (ii) then yields (A.7).
   Proof of part (ii) We can write:
      √                                           −1                              
          N β̂mbtsls − β = X0 (MW − k̂mbtsls MZ )X/N     N −1/2 X0 (MW − k̂mbtsls MZ )

By Lemma A.3, we have:
                                      −1
          X0 (MW − k̂mbtsls MZ )X/N          = Λ22 + op (1)                                       (A.9)

The second term is a (2,1) element of the quadratic form:
                           0
      N −1/2  Z⊥ π12 + ν C  Z⊥ π12 + ν
                                             


where C = (MW − k̂mbtsls MZ ). Applying Lemma A.1 (ii) with tr(C), τC 2 and QCM given by




                                                     [30]
Equation (A.8), and cov(i , νi ) = Σ then yields:
                                                                                  
       −1/2
            
               0
                                       
                                         d            αK (1 − αL )             2
     N        X (MW − k̂mbtsls MZ ) → N 0, Σ11 Λ22 +              (Σ11 Σ22 + Σ12 )
                                                      1 − αL − αK

Combining this result with (A.9) yields part ((ii)) in the Theorem.
  Proof of Part (iii) Write the estimator as:
    √              
      N β̂mjive − β = (X0 (MW − (1 − LN /N )HZ )X/N )−1 N −1/2 X0 (MW − (1 − LN /N )HZ ).

By Lemma A.3, the first term satisfies:

      (X0 (MW − (1 − LN /N )HZ )X/N )−1 = Λ−1
                                           22 + op (1)                                                           (A.10)

The second term is the (2,1) element of:
                           0
      N −1/2  Z⊥ π12 + ν C  Z⊥ π12 + ν
                                         


where C = MW − (1 − LN /N )HZ . Because tr(HZ (I − DZ )−1 ) = tr((I − DZ )−1 ), we have:

           tr(C) = N − LN − (1 − LN /N )N = 0
      tr(C 2 /N ) = (LN /N − 1) + (1 − LN /N )2 tr((I − DZ )−1 /N )
                       p
                      → (αL − 1) + (1 − αL )2 τ

QCM is given by Equation (A.8), and cov(i , νi ) = Σ. Moreover, by Assumption 1:
                    N
                    X                      N
                                           X
      sup max         |cij | ≤ 1 + sup max   |(PW )ij |
       N   i≤N                       N       i≤N
                    j=1                            j=1
                                                                  N
                                                                  X
                                 + (1 − LN /N ) sup max             |(Iij − (PW )ij − (PZ⊥ )ij )||((I − DZ )−1 )jj |
                                                     N    i≤N
                                                                  j=1
                                             CP
                                ≤ 1 + CP +        <∞
                                           1 − CD

Applying Lemma A.1 (ii) and combining it with (A.10) then yields the result.                                           

Proof of Theorem 3. Under Assumption 2, we have:

                   (Z0⊥ Z⊥ )−1/2 Z0⊥ Y
                                                                                          
      √                                                       π̃12 β + γ̃
          αK                                 Z∼N                                , αK Ω ⊗ IKN
                   (Z0⊥ Z⊥ )−1/2 Z0⊥ X                            π̃12
                            0
                           Y⊥ MZ⊥ Y⊥ | Z ∼ W2 (N − KN − LN , Ω)

Moreover, these two statistics are independent. Let b = (1, −β)0 and a = (β, 1). Assumption 5




                                                                 [31]
then implies that unconditionally:
                                                               −1
         0                      0                 0
        Y⊥ PZ⊥ Y⊥ ∼ W2 (KN , Γ−1 ΞΓ−1 /αK + Ω, Γ−1 ΞΓ−1 /αK + Ω     KN aa0 µ2π /αK )
            0
        Y⊥ MZ⊥ Y⊥ ∼ W2 (N − KN − LN , Ω)

with the independence property preserved. Applying Lemma A.2 then after some algebra yields:
                                                    d
        N 1/2 X0⊥ MZ⊥ Y⊥ b/N − (1 − αK − αL )Σ12 → N (0, (1 − αK − αL )VΣ )                               (A.11a)
                                                    d
                  N 1/2 X0⊥ PZ⊥ Y⊥ /N b − (αK Σ12 ) → N (0, αK VΣ + VΞ )                                  (A.11b)

where

        VΣ = Σ22 Σ11 + Σ212
                                  −1
        VΞ = Λ22 Σ11 + Λ11 Σ22 + αK  Λ22 Λ11

Equations (A.11) imply:
                                                                                                          
                                                                               d           αK (1 − αL )
            1/2
                    X0⊥ PZ⊥ Y⊥ /N            kmbtsls )X0⊥ MZ⊥ Y⊥ /N
                                                                         
        N                           + (1 −                                   b → N 0, VΞ +              VΣ
                                                                                           1 − αK − αL
                                                                                          p
Because, by Lemma A.3, (X0⊥ PZ⊥ X⊥ N + (1 − kmbtsls )X0⊥ MZ⊥ X⊥ /N )−1 → Λ−1
                                                                          22 + op (1), this yields
the claim in the theorem.
    Now consider mjive. Write the estimator as:
 √              
  N β̂mjive − β = (X0 (MW −(1−LN /N )HZ )X/N )−1 N −1/2 X0 (MW −(1−LN /N )HZ )(Z⊥ γ+).

By Lemma A.3, the first term satisfies:

        (X0 (MW − (1 − LN /N )HZ )X/N )−1 = Λ−1
                                             22 + op (1)                                                   (A.12)

Let where ˜ = (Z0⊥ Z⊥ )−1/2 Z0⊥  and ν̃ = (Z0⊥ Z⊥ )−1/2 Z0⊥ ν. The second term can be rewritten as:

 N −1/2 X0 (MW − (1 − LN /N )HZ )(Z⊥ γ + )
                = N −1/2 π12
                          0
                             Z0⊥ Z⊥ γ + π12
                                         0
                                            Z0⊥  + γ 0 Z0⊥ ν + ν 0 (MW − (1 − LN /N )HZ )
                                                                                           
                                                                                                     
                            −1/2             −1/2
                = N −1/2 (αK π̃12 + ν̃)0 (αK π̃12 + ˜) + ν 0 ((I − (1 − LN /N )(I − DZ )−1 )MW MZ⊥ )


Because (˜, ν̃) is independent of MZ⊥ (, ν), the two terms are independent. The distribution of the
first term is given by the (2,1) element of a random variable with distribution
                                                                  
                            −1              −1         0     0
        W2 KN , Ω +        αK  Ξ, (Ω   +   αK  Ξ)−1        KN 2
                                                       0   αK µ π




                                                           [32]
so that by Lemma A.2:
                                                    
               −1/2            −1/2
      N −1/2 (αK π̃12 + ν̃)0 (αK π̃12 + ˜) − KN Ω12
                                         d
                                         → N 0, Σ11 Λ22 + αK (Σ11 Σ22 + Σ212 ) + Λ11 (Σ11 + Λ22 /αK )
                                                                                                     



Applying Lemma A.1(ii) to the second term yields:

      N −1/2 ν 0 ((I − (1 − LN /N )(I − DZ )−1 )MW MZ⊥ ) + KN Ω12
                                                                               

                                                 d
                                                 → N 0, (αL − 1 − αK + (1 − αL )2 τ )(Σ11 Σ22 + Σ212 )
                                                                                                      



Adding the variances of these limit distributions yields the result.                                      
                                                p                       p
Proof of Theorem 4. Because γ̃k =                q(1 − q)γk and π̃12,k = q(1 − q)π12,k , we can write
the estimator as:
                                               P                                         
                       1
                                      q(1 − q) N1q i : Qi =1 i − N (1−q)
                                                                     1
                           P        p                                     P
                      KN      k γ̃k +                                       i : Qi =0  i
      β̂wald = β +                               P                                         
                      1                          1                     1
                          P          p                                     P
                     KN      π̃
                            k 12,k +   q(1 − q)  Nq           ν
                                                     i : Qi =1 i − N (1−q)               ν
                                                                               i : Qi =0 i

By law of large numbers, we have:
                                                          
       1 X            p           1  X          1     X      p
              π̃12,k + q(1 − q)       νi −             νi  → µπ
      KN                          Nq        N (1 − q)
            k                              i : Qi =1               i : Qi =0

Therefore, because µπ 6= 0:
                                                               √
                          p                                                        
  √ 
                                        p
                      1  N/KN  X       (1 − q)  X               q        X
    N β̂wald − β =          √      γ̃k + √              i − p                   i  + op (1)
                      µπ      KN           N q i : Q =1       N (1 − q)
                                 k                                 i    i : Q =0                i


All three terms are Normally distributed and mutually independent. Adding up the variances yields
the result.                                                                                    

Proof of Theorem 5. Denote the matrices of instruments and exogenous regressors in the
model (5.7) by ∼, so that W̃ = [Q, W], where Q is an N -vector of basic instruments, Z̃ is
the matrix of first KN − 1 columns of Z, and Z̃⊥ = MW̃ Z̃. Then PW̃ = PW + PQ⊥ , where
           (Q −q)(Qj −q)
(PQ⊥ )ij = iN q(1−q)     . Note that Z remains the same.
   Let ν̄k = KNN i : Gi =k νi denote group averages, let ν̄1,k = K
                  P                                                   P
                                                                          i : Qi =1,Gi =k νi denote group
                                                                    N
                                                                  qN
                                                        KN P
averages for individuals with Qi = 1, and let ν̄0,k = (1−q)N i : Qi =0,Gi =k νi denote group averages




                                                       [33]
for individuals with Qi = 0. Define:
                      KN     X                                                                             KN     X
      Σ̂12,k =                         νi i − ν̄k ¯k                                     Σ̂22,k =                         νi2 − ν̄k2
                      N                                                                                    N
                           i : Gi =k                                                                            i : Gi =k
                   KN                Q −q
                                    p i                                                      sγ,
                             X                    p
         10,k   =                            i = q(1 − q)(¯
                                                            1,k − ¯0,k )                    k = γ̃k − µγ + 10,k
                   N                 (1 − q)q
                           i : G =k
                               i


Some tedious algebra shows that the mbtsls estimator is given by:

                        (1 − k̂mbtsls )X0 MW̃ Y + k̂mbtsls X0 PZ̃⊥ Y
         β̂mbtsls =
                      (1 − k̂mbtsls )X0 MW̃ X + k̂mbtsls X0 PZ̃⊥ X
                                                                                             
                            1 P          γ, π12 ,ν                                                     P P π12 ,ν γ,
                          KN      k    s k ks       + (1 − k̂ mbtsls )(Σ̂ 12,k −     ν
                                                                                  10,k 10,k )   − K12    k     l sl    sk
                                                                                                    N
                     =β+                                                               
                              1 P           π12 ,ν 2                               2 ) − 1
                                                                                                  P P π12 ,ν π12 ,ν
                             KN      k (sk        ) + (1 − k̂mbtsls )(Σ̂22,k − ν10,k           K2     k   l sl      sk
                                                                                                       N


By the weak law of large numbers, we have:
         1 X         p                                                       1 X 2      p
              Σ̂22,k → (1 − αK )Σ22                                               ν10,k → αK Σ22                                  (A.13a)
        KN                                                                 KN
            k                                                                  k
       1 X π12 ,ν 2 p                                                       1 X π12 ,ν p
           (sk ) → Ξ22 + αK Σ22                                                  sk     →0                                       (A.13b)
      KN                                                                   KN
                 k                                                                  k

Hence:
   P                                                                                                −1
  1
              (sπk 12 ,ν )2 + (1 − k̂mbtsls )(Σ̂22,k − ν10,k
                                                        2 ) −         1                π12 ,ν π12 ,ν
                                                                           P P
 KN       k                                                          KN2        k   l sl     sk              = Ξ22 +op (1) (A.14)

The nominator can be written as:

       1 X  γ, π12 ,ν                                           1 X X π12 ,ν γ,
            sk sk       + (1 − k̂mbtsls )(Σ̂12,k − 10,k ν10,k ) − 2      sl    sk =
      KN                                                          KN
         k                                                            k l
                          1 X                     1 X γ, π12 ,ν      1 X
                                  Dk,k̂mbtsls − 2          sk sk   =      Dk,k̂mbtsls + Op (1/KN )
                         KN                      KN                  KN
                                               k                     k                           k



where:
                                                                                      1 X π12 ,ν γ,
      Dk,k̂mbtsls = sγ, π12 ,ν
                                                                                                sk + sπk 12 ,ν sγ,
                                                                                                                    
                     k sk       + (1 − k̂mbtsls )(Σ̂12,k − 10,k ν10,k ) −                sl                    l
                                                                                     KN
                                                                                           l<k

                                                                         −1/2
Note that under the Assumption that Ξ12 = 0, {KN Dk,k̂mbtsls }k≥1 is a martingale difference
sequence with respect to the filtration Fk = σ(γk , π12,k , {i : Gi = k}, {νi : Gi = k}).




                                                              [34]
    The next step is to show that:
       √                                                                                   
        NX             d                                        (1 − αK )αK            2
                                                                                          
           Dk,k̂mbtsls → N 0, Ξ11 Ξ22 /αK + Ξ11 Σ22 + Ξ22 Σ11 +             Σ11 Σ22 + Σ12
       KN                                                        (1 − 2αK )
                  k
                                                                                                                      (A.15)

by applying the martingale central limit theorem. The claim of the theorem for mbtsls will then
follow by combining (A.15) with (A.14). To show (A.15), we first need to check that:

        KN                                                                            2
      1 X     2                      p                                      (1 − αK )αK
                                                                                        Σ11 Σ22 + Σ212
                                                                                                       
           E[Dk,          | FN,k−1 ] → Ξ11 Ξ22 + αK (Ξ11 Σ22 + Ξ22 Σ 11 ) +
     KN         k̂ mbtsls                                                    (1 − 2αK )
            k=1
                                                                                                                      (A.16)

Expanding the left-hand side yields:

   2                         KN             KN             KN      1 X X π12 ,ν γ,
E[Dk,k̂
                | FN,k−1 ] = Ξ11 Ξ22 +
                                Ξ11 Σ22 +      Ξ22 Σ11 + 2     Σ12 2         sl sm
       mbtsls                N               N             N      KN
                                                                     l<k m<k
                KN       1 X X π12 ,ν π12 ,ν          KN        1 X X γ, γ,
       + (Ξ11 +    Σ11 ) 2      sl   sm + (Ξ22 +          Σ22 ) 2        sl sm
                N       KN                             N       KN
                                       l<k m<k                                             l<k m<k
                                                               2
          + 2k̂mbtsls (1 − k̂mbtsls )E10,k ν10,k Σ̂12,k +   k̂mbtsls (KN /N )2 (Σ11 Σ22   + 2Σ212 )   + (1 − k̂mbtsls )2 EΣ̂212,k
                                                                                                                      (A.17)

where the expectations in the last line equal
                                  KN       KN                  KN 2
                      EΣ̂212,k =     (1 −      )Σ11 Σ22 + (1 −   )Σ12
                                  N         N                  N
                                  KN 2
       10,k ν10,k Σ12,k
      Eˆ                        =      Σ11 Σ22 + αΣ212
                                  N
We can therefore write:
          KN
        1 X     2                                 KN
             E[Dk,k̂mbtsls
                           | FN,k−1 ] = Ξ11 Ξ22 +    Ξ11 Σ22
       KN                                         N
              k=1
         KN           (1 − KN /N )(KN /N )2                     KN      1 X X X π12 ,ν γ,
                                             Σ11 Σ22 + Σ212 + 2
                                                           
       +    Ξ22 Σ11 +                   2
                                                                   Σ12 3            sl  sm
         N               (1 − 2(KN /N ))                        N      KN
                                                                          k l<k m<k
                   KN        1 X X X π12 ,ν π12 ,ν             KN       1 X X X γ, γ,
         + (Ξ11 +     Σ11 ) 3             sl   sm + (Ξ22 +        Σ22 ) 3           sl sm
                    N      KN                                   N      KN
                                          k   l<k m<k                                              k     l<k m<k



Now, for a, b ∈ {(γ, ), (π12 , ν)}, note that:
        1 XX X b a    1 X                  1 X          X
         3    sl sm = 3  (KN − l)sbl sal + 3   (KN − l)   (sbl sam + sbm sal ) = op (1)
       KN            KN                   KN
               k      l<k m<k                 l                            l               m<l


                                                             [35]
                                                                                         −2             P         4
Therefore, the last three terms are op (1), which proves (A.16). One can also show that KN                  k   EDk,k̂
                                                                                                                              →
                                                                                                                     mbtsls
0, which implies (A.15). Next consider the mjive estimator. Let

                  KN     X                               KN     X
      Σ̂112,k =                    Qi vi i   Σ012,k =                    (1 − Qi )vi i
                  N                                      N
                       i : Gi =k                              i : Gi =k
                  q                                   1−q                                    
      t12
       k =                Σ̂112,k − qν̄1,k ¯1,k +                 Σ̂012,k − (1 − q)ν̄0,k ¯0,k
              q − KN /N                            1 − q − KN /N

Then we can write the mjive estimator as:
                                                        −1
                                 KN
               X MW̃ Y − 1 − N X0 MZ I − DZ
                0                                              Y
     β̂mjive =                       
                                                            −1
               X0 MW̃ X − 1 − KNN X0 MZ I − DZ
                                                          
                                                               X
                                                                              
                    1 P      γ, π12 ,ν                               KN 12          −2 P P γ, π,ν
                   KN   k   sk ks       + Σ̂ 12,k −     ν
                                                     10,k 10,k − (1 −  N  )t k   − KN     k     l sk sl
             =β+                                                           
                                 π12 ,ν 2                                         −2 P P π,ν π,ν
                      1 P
                     KN    k (sk
                                                       2
                                       ) + Σ̂22,k − ν10,k   − (1 − KNN )t22
                                                                         k     − KN     k   l sk sl

                                                                                               p
Using (A.13) and the fact that by the weak law of large numbers t22
                                                                 k → Σ22 , we get:

                                                                       !−1
           1 X     π12 ,ν 2        2           KN 22      −2
                                                             X X π,ν π,ν
                 (sk ) + Σ̂22,k − ν10,k − (1 −   )t    − KN     sk sl        = Ξ22 + op (1)
          KN                                   N k
              k                                                                        k       l
                                                                                                            (A.18)

We can rewrite the nominator as:
                                                     
 1 X γ, π12 ,ν                                 KN 12   −2
                                                           X X γ, π,ν  1 X           −1
         sk sk    + Σ̂12,k − 10,k ν10,k − (1 −   )tk −KN     sk sl =       D̃k +op (KN  )
KN                                              N                      KN
      k                                                                            k       l        k


where:
                               KN 12
      D̃k = Dk,0 − (1 −          )t
                               N k
                                                                                       −1/2
Like in the case of mbtsls, under the Assumption that Ξ12 = 0, {KN Dk,k̂mbtsls }k≥1 is a martingale
difference sequence with respect to the filtration Fk = σ(γk , π12,k , {i : Gi = k}, {νi : Gi = k}). To
prove the claim of the theorem for mjive, it therefore remains to check that:

         KN
       1 X                    p
            E[D̃k2 | FN,k−1 ] →
      KN
            k=1
             Ξ11 Ξ22 + αK (Ξ11 Σ22 + Ξ22 Σ11 ) + αK (1 − αK )((1 − αK )τ − 1) Σ11 Σ22 + Σ212
                                                                                                        
                                                                                                            (A.19)




                                                          [36]
and that:
             X
      −2
     KN          ED̃k4 → 0                                                                      (A.20)
             k

                                                                                     −1         P
These two conditions will allow us to apply the martingale central limit theorem to KN           k   D̃k .
We first establish (A.19). Using the expansion in (A.17), we get that:
     KN
   1 X
        E[D̃k2 | FN,k−1 ] = Ξ11 Ξ22 + αK (Ξ11 Σ22 + Ξ22 Σ11 ) + αK (1 − αK )Σ11 Σ22 + (1 − αK )Σ212
  KN
       k=1
                                + (1 − KN /N )2 E[t12 12                         12
                                                   k tk ] − 2(1 − KN /N )E[Dk,0 tk ] + op (1)

The remaining expectations are given by:
                               KN
      E[Dk,0 t12
              k | FN,k−1 ] =       Σ11 Σ22 + Σ212
                                N
                                     q2           (1 − q)2
                                                            
                                                               KN
       E[t12 12
          k tk    | FN,k−1 ] =              +                     (Σ11 Σ22 + Σ212 ) + Σ212
                                 1 − KN /N     1 − q − KN /N   N

Substituting them in the expansion above yields (A.19). It can also be shown that (A.20) holds,
which proves the result.                                                                     




                                                 [37]
References
Ackerberg, D. A. and Devereux, P. J. (2009). Improved Jive estimators for overidentified
  linear models with and without heteroskedasticity. Review of Economics and Statistics, 91 (2),
  351–362.

Aizer, A. and Doyle, Jr., J. J. (2011). Effects of Juvenile Incarceration: Evidence from
  Randomly-Assigned Judges, unpublished manuscript.

Anatolyev, S. (2011). Instrumental variables estimation and inference in the presence of many
  exogenous regressors, unpublished manuscript.

Anderson, T. W., Kunitomo, N. and Matsushita, Y. (2010). On the asymptotic optimality of
  the LIML estimator with possibly many instruments. Journal of Econometrics, 157 (2), 191–204.

— and Rubin, H. (1949). Estimation of the Parameters of a Single Equation in a Complete System
  of Stochastic Equations. The Annals of Mathematical Statistics, 20 (1), 46–63.

Andrews, D. W. K., Moreira, M. J. and Stock, J. H. (2006). Optimal Two-Sided Invariant
  Similar Tests for Instrumental Variables Regression. Econometrica, 74 (3), 715–752.

Angrist, J. D., Imbens, G. W. and Krueger, A. B. (1999). Jackknife instrumental variables
  estimation. Journal of Applied Econometrics, 14 (1), 57–67.

— and Krueger, A. B. (1991). Does compulsory school attendance affect schooling and earnings?
  The Quarterly Journal of Economics, 106 (4), 979–1014.

— and Pischke, J.-S. (2009). Mostly Harmless Econometrics: An Empiricist’s Companion.
  Princeton: Princeton University Press.

Ashley, R. (2009). Assessing the credibility of instrumental variables inference with imperfect
  instruments via sensitivity analysis. Journal of Applied Econometrics, 24 (2), 325–337.

Basmann, R. L. (1957). A generalized classical method of linear estimation of coefficients in a
  structural equation. Econometrica, 25 (1), 77–83.

Bekker, P. A. (1994). Alternative Approximations to the Distributions of Instrumental Variable
  Estimators. Econometrica, 62 (3), 657–681.

— and van der Ploeg, J. (2005). Instrumental variable estimation based on grouped data.
  Statistica Neerlandica, 59 (3), 239–267.



                                               [38]
Belloni, A., Chen, D., Chernozhukov, V. and Hansen, C. B. (2011). Sparse models
  and methods for optimal instruments with an application to eminent domain, unpublished
  manuscript.

Berkowitz, D., Caner, M. and Fang, Y. (2008). Are “Nearly Exogenous Instruments” reliable?
  Economics Letters, 101 (1), 20–23.

Caner, M. (2007). Near Exogeneity and Weak Identification in Generalized Empirical Likelihood
  Estimators: Many Moment Asymptotics, unpublished manuscript.

Chamberlain, G. and Imbens, G. W. (2004). Random Effects Estimators with Many Instru-
  mental Variables. Econometrica, 72 (1), 295–306.

Chao, J. C. and Swanson, N. R. (2005). Consistent estimation with a large number of weak
  instruments. Econometrica, 73 (5), 1673–1692.

—, —, Hausman, J. A., Newey, W. K. and Woutersen, T. (2010). Asymptotic Distribu-
  tion of JIVE in a Heteroskedastic IV Regression with Many Instruments. Econometric Theory,
  (forthcoming).

Chetty, R., Friedman, J. N., Hilger, N., Saez, E., Schanzenbach, D. W. and Yagan,
  D. (2011). How does your kindergarten classroom affect your earnings? Quarterly Journal of
  Economics, (forthcoming).

Chioda, L. and Jansson, M. (2009). Optimal Invariant Inference When the Number of Instru-
  ments Is Large. Econometric Theory, 25 (3), 793–805.

Conley, T. G., Hansen, C. B. and Rossi, P. E. (2007). Plausibly Exogenous, unpublished
  manuscript.

Davidson, R. and MacKinnon, J. G. (1993). Estimation and Inference in Econometrics. Oxford:
  Oxford University Press.

Donald, S. G. and Newey, W. K. (2001). Choosing the Number of Instruments. Econometrica,
  69 (5), 1161–1191.

Fisher, F. M. (1961). On the cost of approximate specification in simultaneous equation estima-
  tion. Econometrica, 29 (2), 139–170.

— (1966). The relative sensitivity to specification error of different k-class estimators. Journal of
  the American Statistical Association, 61 (314), 345–356.



                                                [39]
— (1967). Approximate Specification and the Choice of a k-Class Estimator. Journal of the Amer-
  ican Statistical Association, 62 (320), 1265–1276.

Flores, C. A. and Flores-Lagunes, A. (2010). Partial Identification of Local Average Treat-
  ment Effects with an Invalid Instrument, unpublished manuscript.

Fryer, R. G. (2011). Financial Incentives and Student Achievement: Evidence from Randomized
  Trials. Quarterly Journal of Economics, (forthcoming).

Gautier, E. and Tsybakov, A. B. (2011). High-dimensional instrumental variables regression
  and confidence sets, unpublished manuscript.

Guggenberger, P. (2010). On the Asymptotic Size Distortion of Tests When Instruments Locally
  Violate the Exogeneity Assumption, unpublished manuscript.

Hahn, J. (2002). Optimal inference with many instruments. Econometric Theory, 18 (1), 140–168.

— and Hausman, J. A. (2005). IV Estimation with Valid and Invalid Instruments. Annales
  d’Économie et de Statistique, (79/80), 25–57.

Hansen, C. B., Hausman, J. A. and Newey, W. K. (2008). Estimation With Many Instrumental
  Variables. Journal of Business and Economic Statistics, 26 (4), 398–422.

Hausman, J. A., Newey, W. K., Woutersen, T., Chao, J. C. and Swanson, N. R. (2009).
  Instrumental Variable Estimation with Heteroskedasticity and Many Instruments, unpublished
  manuscript.

Kraay, A. (2008). Instrumental Variables Regressions with Honestly Uncertain Exclusion Restric-
  tions, unpublished manuscript.

Kunitomo, N. (1980). Asymptotic expansions of the distributions of estimators in a linear func-
  tional relationship and simultaneous equations. Journal of the American Statistical Association,
  75 (371), 693–700.

Levitt, S. D., List, J. A., Neckermann, S. and Sadoff, S. (2011). The Impact of Short-term
  Incentives on Student Performance, unpublished manuscript.

Mariano, R. S. (1973). Approximations to the Distribution Functions of Theil’s k-Class Estima-
  tors. Econometrica, 41 (4), 715–721.

Morimune, K. (1983). Approximate distributions of k-class estimators when the degree of overi-
  dentifiability is large compared with the sample size. Econometrica, 51 (3), 821–841.


                                               [40]
Nagar, A. L. (1959). The bias and moment matrix of the general k-class estimators of the pa-
  rameters in simultaneous equations. Econometrica, 27 (4), 575–595.

Nagin, D. and Snodgrass, M. G. (2011). The Effect of Incarceration on Offending: Evidence
  from a Natural Experiment in Pennsylvania, unpublished manuscript.

Nevo, A. and Rosen, A. M. (2010). Identification with Imperfect Instruments. Review of Eco-
  nomics and Statistics, (forthcoming).

Newey, W. K. and McFadden, D. L. (1994). Large sample estimation and hypothesis testing. In
  R. F. Engle and D. L. McFadden (eds.), Handbook of Econometrics, vol. 4, Chapter 36, Elsevier,
  pp. 2111–2245.

Phillips, G. D. A. and Hale, C. (1977). The Bias of Instrumental Variable Estimators of Si-
  multaneous Equation Systems. International Economic Review, 18 (1), 219–228.

Reinhold, S. and Woutersen, T. (2011). Endogeneity and Imperfect Instruments in Applied
  Work : Deriving Bounds in a Semiparametric Model, unpublished manuscript.

Rothenberg, T. J. (1984). Approximating the distributions of econometric estimators and test
  statistics. In Z. Griliches and M. D. Intriligator (eds.), Handbook of econometrics, vol. 2, Chapter
  15, Elsevier, pp. 881–935.

Staiger, D. and Stock, J. H. (1997). Instrumental Variables Regression with Weak Instruments.
  Econometrica, 65 (3), 557–586.

Theil, H. (1961). Economic Forecasts and Policy. Amsterdam: Horth-Holland, 2nd edn.

— (1971). Principles of Econometrics. New York: John Wiley & Sons.

van Hasselt, M. (2010). Many Instruments Asymptotic Approximations Under Nonnormal Error
  Distributions. Econometric Theory, 26 (02), 633–645.

Wooldridge, J. M. (2002). Econometric Analysis of Cross Section and Panel Data. Cambridge,
  MA: MIT Press.




                                                [41]
Table 1: Estimates for Angrist-Krueger Data (N = 162, 487)

                                           Standard Error
    Estimator       β̂    classic    bekker   many exo      Λ11 > 0
 single qob dummy
     tsls      0.089 (0.021)
     liml      0.089 (0.021) (0.021)            (0.021)
     btsls     0.089 (0.021) (0.021)
     mbtsls    0.089 (0.021) (0.021)            (0.021)     (0.021)
     jive      0.090 (0.021) (0.021)
     mjive     0.089 (0.021) (0.021)            (0.021)     (0.021)

 qob interacted   with year and state of birth
    tsls          0.073 (0.017)
    liml          0.095 (0.017) (0.042)        (0.042)
    btsls         0.097 (0.017) (0.039)
    mbtsls        0.098 (0.017) (0.040)        (0.040)      (0.039)
    jive          0.056 (0.017) (0.053)
    mjive         0.096 (0.017) (0.054)        (0.040)      (0.040)

 qob interacted   with year and state of birth, qob exogenous variable
    tsls          0.069 (0.033)
    liml          0.093 (0.034) (0.128)        (0.128)
    btsls         0.099 (0.034) (0.131)
    mbtsls        0.102 (0.034) (0.132)        (0.132)   (0.132)
    jive          0.064 (0.033) (0.180)
    mjive         0.096 (0.034) (0.184)        (0.133)   (0.133)




                                    [42]
