                                NBER WORKING PAPER SERIES




  THE ROLE OF BELIEFS IN INFERENCE FOR RATIONAL EXPECTATIONS MODELS

                                          Bruce N. Lehmann

                                        Working Paper 11758
                                http://www.nber.org/papers/w11758


                      NATIONAL BUREAU OF ECONOMIC RESEARCH
                               1050 Massachusetts Avenue
                                 Cambridge, MA 02138
                                    November 2005




This paper had an unusually long gestation period. I want to thank Lars Hansen for a long conversation
I had with him sometime in the last ten years (!) and to apologize to those with whom I have had helpful
conversations that I have forgotten. I also want to thank seminar participants at the Federal Reserve
Bank of New York, Hong Kong University of Science and Technology, the London School of Economics,
Southern Methodist University, Syracuse University, the University of Alberta, the University of Arizona,
the University of California at San Diego, the University of Houston, and Yale University, students
at the 2002 SSRC Summer Workshop in Applied Economics: Risk and Uncertainty, and conference
participants at The First Symposium on Econometric Theory and Application. The views expressed
herein are those of the author(s) and do not necessarily reflect the views of the National Bureau of
Economic Research.

 2005 by Bruce N. Lehmann. All rights reserved. Short sections of text, not to exceed two paragraphs,
may be quoted without explicit permission provided that full credit, including notice, is given to the
source.
The Role of Beliefs in Inference for Rational Expectations Models
Bruce N. Lehmann
NBER Working Paper No. 11758
November 2005, October 2007
JEL No. C1,C2,C3,C4,C5

                                             ABSTRACT

This paper discusses inference for rational expectations models estimated via minimum distance methods
by characterizing the probability beliefs regarding the data generating process (DGP) that are compatible
with given moment conditions. The null hypothesis is taken to be rational expectations and the alternative
hypothesis to be distorted beliefs. This distorted beliefs alternative is analyzed from the perspective
of a hypothetical semiparametric Bayesian who believes the model and uses it to learn about the DGP.
This interpretation provides a different perspective on estimates, test statistics, and confidence regions
in large samples, particularly regarding the economic significance of rejections in rational expectations
models.


Bruce N. Lehmann
University of California, San Diego
IR/PS
1415 Robinson Building Complex
La Jolla, CA 92093-0519
and NBER
blehmann@ucsd.edu
1. Introduction

           A somewhat longer version of the following question appeared on the finance field exam

at Columbia in 1991: Consider the excess return on a market index R mt − R ft . How would you

test the null hypothesis that E[R mt − R ft | I t-1 ] = 0 by examining p sample moment conditions

gT = T1 ∑ t g t ; g t = ( R mt − R ft ) z t-1 ; z t-1 ∈ I t-1 for some conditioning information set It–1? Suppose

it is certain that E[R mt − R ft | I t-1 ] = 0 . How would you interpret large values of the test statistic?1

           The answer is deceptively simple. The null hypothesis can be tested with the generalized

method of moments (GMM) overidentifying restrictions test statistic TgT′ ST−1 gT → χ 2 (p) where

ST = T1 ∑ t g t g′t ; see Hansen (1982). Now if the null model E[g t | I t-1 ] = 0 is a maintained

hypothesis, rational expectations becomes the null hypothesis and the alternative is then that

expectations were not rational. Even if gT reliably differed from zero in a statistical sense, there

might be beliefs implicit in a rejection region that seem plausible given the historical record.

This possibility of assessing the economic significance of statistical rejections makes the

distorted beliefs alternative a natural one in rational expectations models.

           The distorted beliefs alternative arises when an econometrician specifies an economic

model of the relations among a set of observables xt of the form E P0 [g(x t , θP0 )] = 0 , where θP0 is

an unknown parameter vector and P0 is the data generating process if expectations are rational

and the model is correct. The econometrician estimates P0 using minimum distance methods via

inf | P − Pˆ | where |•| is a measure of the distance between the empirical distribution P̂ and P,
    P


which satisfies the a priori moment restrictions. What is missing is the link between the

econometrician’s estimate and other ex post beliefs that might seem plausible.

1
    Unsurprisingly, nobody ever answers the questions I put on field exams.


                                                           1
       This gap is closed by supposing the econometrician asks the following question: what

beliefs might a hypothetical expected utility maximizer have after looking at the same data? The

answer is not entirely straightforward because a semiparametric Bayesian would need to specify

priors over the space of probability measures that satisfy E P [g(x t , θP )] = 0 . The circumstances

in which the archetype’s beliefs would converge to those of the econometrician can be quite

delicate because a prior that places too little mass in the neighborhood of P0 or too much outside

of such neighborhoods can lead to inconsistent posteriors. Fortunately, there are weak sufficient

conditions under which the archetype’s beliefs will converge to those of the econometrician.

       This paper is related to the extensive literature on empirical likelihood and related

minimum divergence estimators; see Owen (2001) and Kitamura (2006) for recent surveys. It is

most closely related to Back and Brown (1992,1993), who discuss GMM estimation of

probability distributions, and to Zellner (1994,1997), Kim (2002), Lazar (2003), and Schennach

(2005), who discuss Bayesian inference in GMM settings.            However, the paper is almost

orthogonal to the latter, in which probabilities are nuisance parameters and interest centers on

inference for θ.    Here probability measures are not nuisance parameters to be profiled or

integrated out but rather are the focus of the analysis.

       The paper is laid out as follows. The next section describes the a posteriori beliefs of a

hypothetical semiparametric Bayesian and discusses circumstances in which they will converge

to the probability measure estimated by a GMM econometrician. The penultimate section

suggests some of the ways in which this insight can inform the interpretation of estimates, test

statistics, and confidence regions in large samples. A brief conclusion rounds out the paper.

2. A Portrait of a Semiparametric Bayesian

       This section constructs a semiparametric Bayesian archetype – one who believes in a



                                                  2
model comprised of moment conditions with otherwise general preferences and constraints –

with a view to finding weak sufficient conditions under which posterior beliefs converge to the

corresponding probability model estimated by a GMM econometrician. After setting the stage in

Section 2A, Section 2B shows that the archetype will want beliefs that converge weakly and why

convergence can obtain with relatively unrestricted priors in an iid setting. Section 2C proves

that convergence still obtains on the subspace of discrete measures and 2D discusses the

corresponding subspace for the GMM econometrician, which differs only in that each discrete

approximation satisfies the moment conditions. This seemingly trivial modification results in a

very simple structure that provides insight into the distorted beliefs alternative.

A. Preliminaries

        To fix the setting, let x be a random variable taking values on a sample space X ⊆                      d
                                                                                                                    , let

BX be the Borel σ-algebra of X , and let g(x, θ ) = {g j ∈ F : X × Θ →                    ∀ θ ∈Θ ⊂       q
                                                                                                             , j ≤ p} ,

where F is the space of all bounded real-valued uniformly continuous functions. Let P θ be a

nonempty set of probability measures P on (X , BX ) that satisfy E P [g(x, θP )] = 0 , where

rank ( E P [gθ (x, θP )]) = p and E P [g(x t , θ )] = 0 ⇒ θ = θ P since θ can differ across P θ . Since X is

a complete separable metric space, P θ is metrizable and can be equipped with its Borel

σ-algebra BP θ ; see Theorem 6.2 of Parthasarathy (1967). Finally, let P0 denote the measure

governing the realizations of x and θP0 its associated parameter value.2 To avoid the notational

clutter associated with atoms and P-continuity sets, each P ∈ P θ is taken to be dominated by a



2
 Some Bayesians prefer to think of P0 as being drawn randomly from P θ . Alternatively, one can view the analysis
as conditional on P0 being true under the null with the understanding that there can be a separate modeling exercise
under the alternative hypothesis. On this interpretation, the semiparametric Bayesian would possess priors over this
model class and assign the remaining prior probability to all remaining model classes. This Bayesian would view P0
as the measure that minimizes the Kullback-Leibler divergence between it and the truth under the alternative.


                                                         3
σ-finite measure μ with density given by the Radon-Nikodym derivative p = dP/dμ.

B. On the Beliefs of a Semiparametric Bayesian

         The hypothetical semiparametric Bayesian is taken to maximize expected utility (or to

minimize expected expenditure or cost for a given level of utility or production) taking account

of the uncertainty in both the random variables that impinge on this maximum problem and the

probability law generating them. Many such problems can be cast in the following form:

         A1: The archetype chooses actions a ∈ A ⊆                  k
                                                                            to maximize the conditional expectation

of a bounded utility function V : X × A →                  ∪ {−∞} , where V is upper semicontinuous for

almost all x ∈ X , based on the information in the sub-σ-algebra F ⊂ BX .

         A2: The archetype formulates a prior Π(P) which satisfies                         ∫Pθ
                                                                                                 Π(dP) = 1 where the

propriety of the prior ensures that integrals over P θ converge.

         In these circumstances, the Bayesian archetype will solve the maximum problem:

         sup E P [V(x,a) | F ] = sup ∫ V(x,a)P(dx | F ) = ∫ V(x,a) ∫ θ P(dx | F )Π(dP | F )                          (1)
          a∈A                       a∈A   X                             X           P



where Π(P | F ) is the posterior probability that P = P0 and P(dx | F ) is the predictive

distribution for the next realization of x.3

         Something definitive can be said about predictive and posterior distribution asymptotics

under two additional assumptions, one about the measures in P θ and one about priors over P θ .

                          iid
         A3: Each x t ~ P0 and there is a random sample XT = {x1, x2,…, xT} with FT = X T .4

         Given A3, the predictive distribution given XT is given by:

3
  As is readily apparent, additional random variables y ∈ Y ⊆ m can impinge on the stochastic program as long as
they can be integrated out. This would be the case if x is taken to be weakly exogenous with respect to y in the
language of Engle et al. (1983) and if the prior distribution is constructed to insure that the conditional distributions
P(y|x,F) are conditionally independent of the distributions P(x|F) in P θ both a priori and a posteriori.
4
  FT = X T can be replaced with X T ∈ FT , which would make P(x T +1 | X T ) = E[P(x T +1 | FT ) | X T ] .


                                                           4
         P ( x T +1 X   T
                            )=∫           P(x T +1 )Π(P | X   T
                                                                  )=
                                                                     ∫
                                                                     Pθ
                                                                          P(x T +1 )P(X T )Π(dP)
                                Pθ
                                                                          ∫   Pθ
                                                                                   P(X T )Π(dP)
                                                                                                                      (2)
                                ∫     θ
                                          P(x T +1 )∏ t ≤ T p(x t )Π(dP)
                            =     P

                                          ∫ ∏
                                           Pθ     t ≤T
                                                         p(x t )Π(dP)

where independence is only used in the passage from the first line to the second.5

         A4. (Schwartz (1965)): The prior of the archetype satisfies Π ⎡⎣ K ε (P0 ) ⎤∀
                                                                                    ⎦ ε > 0 where

           { ∫ pln(p / q)dμ < ε} .
K ε (P) = Q :                                     6




         Note that the archetype’s prior is defined over P θ , not over θ and a nuisance parameter

that defines the measures compatible with the moment conditions for each θ. In contrast to most

semiparametric settings where interest centers on θ and not on nuisance parameters like

probabilities, the archetype is interested in the model only for forecasting and, hence, would

naturally form priors over probability measures, not parameter values.7

         A1 – A4 suffice for weak convergence of the posterior and predictive distributions for P.

         Theorem 1 (Theorem 6.1 of Schwartz (1965) and Theorem 4.4.2 of Ghosh and

Ramamoorthi (2003): Let U be any weak neighborhood of P0 and let U c = P θ \ U . Under

assumptions A1 – A4, Π(U c | X T ) → 0 a.s. P0 .

         The idea of the proof is as follows. The posterior probability that P is in Uc is given by:

5
 This serves to make it clear that there is nothing in the Bayesian calculus that makes it difficult to accommodate
heterogeneity and dependence. The difficulty lies in the curse of dimensionality, the need to replace p(xt) with
p(xt|Ft-1) throughout. A formal way to handle heterogeneity and dependence is to rewrite P(XT) as:
                       P(X T )
            P(X T ) =            ∏ t ≤ T p(x t ) ≡ Λ(X T )∏ t ≤ T p(x t )
                      ∏ t≤T t
                           p(x )
                                                                                          iid
where Λ(X T ) is the likelihood ratio statistic for the hypothesis that x t ~ P . Heterogeneity and dependence can be
integrated out if Λ(•) is distributed independently of p(•) both a priori and a posteriori. The elucidation of the
circumstances in which P θ has the required structure is beyond the scope of this paper.
6
   Kε(P) is termed a Kullback-Leibler neighborhood of P and A4 is taken to mean that P0 is in the Kullback-Leibler
support of the prior.
7
  In addition, priors over probability measures are invariant with respect to reparameterizations of the form φ = f (θ ) .


                                                                          5
                              ∏
                          ) ∫
                                             p(x t )Π(dP)
            (
         Π P ∈ Uc XT =         Uc     t ≤T
                                                                                                                 (3)
                              ∫ ∏
                               Pθ     t ≤T
                                           p(x t )Π(dP)

The denominator can be shown to go to infinity when A4 holds and the numerator can be shown

to converge to zero at an exponential rate because the likelihood ratio statistic for testing the null

P = P0 against the alternative hypothesis that P∈Uc is uniformly consistent.

        Theorem 2 (Proposition 4.2.1 of Ghosh and Ramamoorthi (2003)): Under the conditions

of Theorem 1, P(x T +1 | X T ) ⇒ P0 (x T +1 ) a.s. P0, where ⇒ denotes weak convergence.

        The essence of the proof is that           ∫     PΠ ( P X T ) Π(dP) − P0 ≤ ∫     P − P0 Π ( P X T ) Π(dP)
                                                    Pθ                              Pθ



by Jensen’s inequality and the right hand side converges to zero.

        How is weak convergence of the predictive distribution relevant to the Bayesian

archetype? A partial answer is given by the following theorem.

        Theorem 3 (Propositions 2.6–2.14 of Berger and Salinetti (1995); Theorem 1 of Zervos

(1999)): Suppose that V satisfies A1 and let PN be any sequence for which PN ⇒ P0 . Then:

         sup ∫ V(x,a)PN (dx) → sup ∫ V(x,a)P0 (dx) a.s.                                                          (4)
         a∈A    X                     a∈A     X



Theorem 2 of Zervos (1999) obtains this result under less restrictive conditions than A1.

        Theorem 3 suggests that the relevance of the weak convergence criterion depends on the

use to which the predictive distribution is being put. If the purpose is to learn P0 with high

posterior probability, weak convergence is too weak: distributions in weak neighborhoods of P0

can look quite different from it. Dramatic examples can be found in Freedman (1963, 1965),

Freedman and Diaconis (1983, 1986a,b), and Stinchcombe (2004).8 Measures in P θ that are


8
 The fact that posterior convergence can fail even if the prior assigns positive mass to weak neighborhoods of P0 led
Freedman (1965) to conclude that “for essentially any pair of Bayesians, each thinks the other is crazy” and
Stinchcombe (2004) to say that such Bayesians engage in “erratic, wildly inconsistent, fickle, or faddish” behavior.


                                                             6
close to P0 in the Prohorov, dual bounded Lipshitz, or other weak metric can be far from P0 in a

Kullback-Leibler sense and relative entropy is what is relevant for likelihood ratios.

         However, this semiparametric Bayesian is using the model to solve a problem like (4).

By the Portmanteau Theorem (Theorem 6.1 of Parthasarathy (1967) and Theorem 11.1.1 of

Dudley (1989)), weak convergence implies that        ∫ fdPN   → ∫ fdP ∀ f ∈ F . If ∫ gdPN → ∫ gdP = 0


and    ∫ V dP
         a   N   → ∫ Va dP = 0 , where Va is the set of Euler equations from (4), the archetype will

learn the optimal decision rule asymptotically. To be sure, the archetype’s prior might reflect a

priori beliefs about other aspects of the measures in P θ , particularly with respect to their

smoothness, but the archetype would want to ensure that the resultant prior would not interfere

with weak convergence to the optimal decision rule. Achieving weak convergence when possible

would appear to be a minimal condition for an inductive learning scheme to be deemed rational.

         Moreover, the proviso that         ∫ fdPN   → ∫ fdP ∀ f ∈ F     is surely relevant for the

econometrician as well. The econometrician is assuming that the semiparametric Bayesian is

solving an optimization problem based on beliefs codified in the moment conditions but is

making no assumptions regarding the functional form of the archetype’s preferences. Consistent

estimation of the probability measure by the econometrician is asymptotically equivalent to

learning aspects of the beliefs of this semiparametric Bayesian that are relevant for optimal

decisions irrespective of the specifics of the utility function in these circumstances.

C. Multinomial Approximation of Semiparametric Bayesian Beliefs

         The weak topology is appropriate when interest centers on probability measures or

distributions, not on probability densities; that is, weak convergence is equivalent to

lim PN ( A ) = P( A ) for all Borel sets A that have boundaries with P-measure zero and the natural
N →∞




                                                     7
collection of Borel sets to contemplate is the partition of X induced by sampling. The fact that

the predictive distribution converges weakly to the associated countable cell multinomial links

the asymptotic beliefs of the semiparametric Bayesian archetype to the probability measures

estimated by the GMM econometrician.9 The purpose of this subsection is to make some

connections that will prove useful in the sequel.

        As is well-known, the set of discrete measures is dense in the space of all Borel

probability measures on X ; see, for example, Theorem 6.3 of Parthasarathy (1967) and Lemma

11.7.3 of Dudley (1989). A sequence of multinomial approximations can be constructed in the

following manner. For each N, partition X into a countable collection of Borel sets of the form:

                                          1
         X = ∪ n X nN ; sup y − z ≤         ∀ n; X mN ∩ X nN = ∅ ∀ m ≠ n                                      (5)
                         y,z∈X N
                               n
                                          N

where     •   is the usual Euclidean metric.                 For each n, choose any x Nn ∈ X nN and set

P(x Nn ) = P(X nN ) so that PN = ∑ n P(x Nn )δx N , where δx N is the Dirac measure at x Nn , and the
                                                  n               n




corresponding probability distribution is FN (x) = ∑ n P(x Nn )1x N <x . The error in approximating
                                                                            n




any f ∈ F on X nN by f (x Nn ) is at most ζ nN = sup( f | X nN ) − inf( f | X nN ) . Weak convergence


                                                      ∫ fdP − ∫ fdP ≤ sup ζ         → 0 as N→∞.
                                                                                N
obtains by the Portmanteau Theorem since                  N                     n
                                                                        n



        The likelihood for each PN is given by PN (X T ) = ∏ t ∏ n P(X nN ) x t ∈X n . If, in addition,
                                                                                         1   N




N     T , each set X nN will contain at most one observation from XT. Letting X ntN denote the cell

with x t ∈ X nN , PN (X T ) = ∏ t ≤T P(X ntN ) . Thus the posterior and predictive distributions can be

approximated on (5) by:


9
  Chamberlain (1987) used multinomial approximation to study semiparametric efficiency but not on the weak
topology, forming a neighborhood base for P0 with measurable and integrable, not bounded or continuous, functions.


                                                         8
                                    ∫ ∏ P(X )Π(dP) ⇒ Π P ∈Q X
                                                              N

         Π N ( P ∈Q X           )                       (        )
                                                t ≤T          nt
                            T
                                  =  Q                                                  T

                                    ∫ ∏ P(X )Π(dP)                                                                                          (6)
                                                              N
                                                t ≤T          nt
                                     Pθ

             (
         PN x T +1 ∈ X nN       X ) = ∫ P(x ∈ X )Π ( P X ) Π(dP) ⇒ P (x
                                 T
                                           Pθ
                                                       T +1
                                                                           N
                                                                           n    N
                                                                                    T           0
                                                                                                       T +1   ∈ X nN )

with convergence following from the Portmanteau Theorem and Theorems 1 and 2, respectively.10

         For comparability with the GMM econometrician, it makes sense to consolidate the

countable partition (5) into a smaller set of partitions centered on the observations in the sample

XT. To be concrete, aggregate (5) into an ‘asymptotic’ Voronoi tessellation:

                                                                   {
         X = ∪ t Xt T ; x t ∈ Xt T ; Xt T = ∪ n X nN : x Nn − x t ≤ x Nn − x s ∀ n, s ≠ t                          }                        (7)

for sufficiently large N.11 The associated multinomial probabilities over (7) can be taken to be

PN ( Xt T ) = P(X ntN ) . Alternatively, the semiparametric Bayesian can group within cells and set

PN ( Xt T ) = P   (∪ X ) = ∑
                    n
                        N
                        n            n
                                          P(X nN ) for all X nN allocated to Xt T .12 Either way,                      ∏   t ≤T
                                                                                                                                  PN ( Xt T ) is

an approximate likelihood function for each P ∈ P θ and the aggregated multinomial probabilities

live on the associated standard T-simplex STN : PN ( Xt T ) > 0,           {            ∑P  t   N   ( Xt T ) = 1 .}
         The resulting approximate posterior and predictive distributions are given by:

                                     ∏ P ( X )Π(dP)
                                )= ∫
                                                                       T

         Π N ( P ∈Q X                                  ⇒ Π ( P ∈Q X )
                                                t ≤T   N       t
                            T        Q                                                      T

                                   ∫  ∏
                                     Pθ
                                          P ( X )Π(dP)
                                                t ≤T   N           t
                                                                       T
                                                                                                                                            (8)
             (
         PN x T +1 ∈ X tT       X ) = ∫ P(x ∈ X )Π ( P X ) Π(dP) ⇒ P (x
                                 T
                                           Pθ
                                                       T +1                t
                                                                            T
                                                                                N
                                                                                    T           0
                                                                                                       T +1   )

which converge weakly as well. It could be that (6) and (8) approximate their continuous

analogues. It could be that the semiparametric Bayesian approximates the “true” posterior in this


10
   See also Theorem 4.1 of Diaconis and Freedman (1986b) for a related multinomial approximation in a Bayesian
context based on discretization of both the sample space and the space of probability measures.
11
   This is not a standard Voronoi tessellation. The word “asymptotic’ and the large N requirement arise because
each X nN is allocated to only one Xt T and there will be points in X nN closer to some other Xs T because the diameter
of X nN is 1/N. When N is large, any such tie-breaking rule will suffice.
12
   Grouping is a coarse way of smoothing but is consistent with multinomial approximation.


                                                                                9
fashion. In either case, the archetype consistently estimates the probability measure of xt.

D. Multinomial Approximation and the GMM Econometrician

            Now consider a second set of T-cell multinomial distributions on (7) given by

{S   G
     T   : PtT (θ ) > 0,   ∑   t ≤T
                                      PtT (θ ) = 1,   ∑   t ≤T                                            }
                                                                 PtT (θ )g(x t , θ ) = 0, θ ∈ Θ , where the requirement that the

moment conditions hold for each T makes STN differ from STG . However, STN ⇒ STG – that is,

PN ( Xt T ) ⇒ PtT (θP ) ∀ P ∈ P θ – as the cell diameters shrink to zero. This is the large sample link

between probability models of the archetype and the GMM econometrician.13

            There is a very simple theorem that provides considerable insight into the structure of

STG . Let gT (θ ) = T1 ∑ t ≤ T g(x t , θ ) and VT (θ ) =                 1
                                                                         T   ∑   t ≤T
                                                                                        [g(x t , θ ) − gT (θ )][g(x t , θ ) − gT (θ )]′ . Then:

            Theorem 4: Let STG (θ ) be the subset of STG for a given θ ∈ Θ .                                                             Then each

{P1T (θ ), P2T (θ ),…, PTT (θ )} ∈ STG (θ ) satisfies:

            PtT (θ ) = T1 − T1 gT (θ )′VT (θ ) −1 [g(x t , θ ) − gT (θ )] + εt (P)
                                                                                                                                                      (9)
            ∑ t ≤T ( PtT (θ ) − T1 ) = T1 gT (θ )′VT (θ )−1 gT (θ ) + Tσ ε2T (P); σ ε2T (P) = T1 ∑ t ≤T εt (P)2
                                    2




where the residuals satisfy                T −1   < gT (θ )′VT (θ ) −1 [g t (θ ) − gT (θ )] − εt (P) <               1   so that PtT (θ ) > 0 .14
                                            T                                                                        T


            Proof:     Trivial application of the normal equations of multiple regression with an

intercept.

            This is an arithmetic result: all multinomial probabilities based on the same value of θ

13
     A discrete prior can be formed over the T-simplex STG (or, for that matter, on the N-simplex over (5) constrained
                                                   Diam( STG ) ≤ 2 but the probabilities are O(T-1) and so Diam( ST ) = O(√T),
                                                                                                                  G
to satisfy the moment conditions).
bounding STG by the positive orthants of spheres of the form                       ∑p    t
                                                                                             2
                                                                                             t   = O(T −1 ) . Letting H 2 ( x, y ) = ∑ t ( √ x t − √ y t )
be the squared Hellinger metric, STG can be covered by {PmN : H 2 ( PkT , PmT ) ≥ δ > 0 ∀ k ≠ m, m = 1,…, M} and an
approximate prior is given by Π T (δ ) = ∑ m Π Tm PmT where Π Tm > 0 and                          ∑   m
                                                                                                          Π Tm = 1 . Π T (δ ) satisfies A4 because the
Hellinger distance bounds relative entropy. See Ghosh and Ramamoorthi (2003) for a version of Π T (δ ) with a
prior over a random number of cells. See also Schennach (2005).
14
   The awkward notation ε t (P) arises because there will generally be many P ∈ P θ for each value of θ.


                                                                         10
have the same fitted value        1
                                  T   − T1 gT (θ )′VT (θ ) −1 [g t (θ ) − gT (θ )] , termed implied probabilities by

Back and Brown (1993), where the residuals εt (P) are identically zero if the implied probabilities

are all positive.      In large samples, gT (θ )′VT (θ ) −1 [g t (θ ) − gT (θ )] = o p (1) for values of θ in

shrinking neighborhoods of θP0 . For such values of θ and for P in shrinking neighborhoods of

P0, nonzero values of εt (P) are a small sample event because E P0 [g(x t , θP0 )] = 0 . The fact that

(9) holds for any numbers           {p : ∑ p
                                       t      t   t                  }
                                                      = 1, ∑ t p t g t = 0 – that is, pt need not be positive –

suggests that this regression structure can be useful for interpreting their sample analogues.15

         The regression sum of squares can be interpreted along similar lines. The sum of squared

differences between PtT (θ ) and 1/T is proportional to what Owen (1991) termed Euclidean

likelihood. In large samples, it is proportional to the φ- or f-divergences introduced by Csiszár

(1967), making for a connection with a rich literature on estimation and testing based on the

minimization of empirical divergences.                  These divergences are defined by the discrepancy

functions φ( qp ) ≡ φ(z) > 0 where p and q are two densities defined on the same sample space and

where φ(•) is continuous, convex, and twice differentiable and normalized so that φ(1) = φ′(1) = 0

and φ′′(1) = 1 . The term discrepancy serves as a reminder that φ(•) need not possess either the

symmetry or triangle inequality properties of a metric.16

         The scaled divergence between discrete measures with probabilities pt and qt is measured

by DφT (z) = 2E q [φ(z)] = 2∑ t q t φ(z t ) and a Taylor series expansion yields:

15
   The regression structure of multinomial probabilities can also be used in prior construction if the zero covariance
between g t (θ ) and ε t (P) is strengthened to E[ε t (P) | g(x t ,θ )] = 0 .
16
   The smoothness assumption rules out weak metrics such as the Kolmogorov, Levy, Prohorov, and dual bounded
Lipschitz; Donoho and Liu (1988) discuss how such metrics can produce poorly behaved minimum distance
estimates. It contains the convex members of the Cressie-Read (1988) power divergence family for which φ(z) is an
affine function of zα including the likelihood divergence, entropy or Kullback-Leibler information, the Hellinger
metric, and Pearson’s and Neyman’s modified χ2.


                                                           11
                                                                           1
        DTφ (z) = 2∑ t q t φ(z t ) = 2∑ n q t [φ(1) + φ′(1)(z t − 1) + φ′′(ξ t )(z t − 1) 2 ]
                                                                           2                             (10)
                = ∑ t q t (z t − 1) 2 + ∑ t q t [φ′′(ξ t ) − 1](z t − 1) 2

where ξ t is between 1 and zt. When p t = PtT (θ ) and q t =                1   , (10) takes the form:
                                                                            T


        DTφ (z) = T ∑ t ⎡⎣ PtT (θ ) − T1 ⎤⎦ + T ∑ t [φ′′(ξ t ) − 1] ⎡⎣ PtT (θ ) − T1 ⎤⎦
                                           2                                              2
                                                                                                         (11)

for all {θ , STG (θ )} . If sup t PtT (θ ) − T1 = o p (1) and if φ″(z) is bounded in the neighborhood of

unity, the second term in (11) converges to zero uniformly. The leading term is proportional to

Pearson’s χ2 divergence, which suggests that the decomposition of the regression sum of squares

in the second line of (9) can provide insight into sample analogues as well.

3. The Distorted Beliefs Interpretation of Hypothesis Tests and Confidence Regions

       Section 2 provided a framework for interpreting estimates of a probability measure that

satisfies given moment conditions along the lines of Back and Brown (1992). Confronting

                                                              iid
uncertainty in the form of random variables x t ~ P0 , a generic expected utility maximizer forms a

prior distribution over P θ because this semiparametric Bayesian believes P0 ∈ P θ . In this

setting, the archetype’s predictive distribution converges weakly to P0 when P0 ∈ P θ under the

sole condition that the prior assigns positive probability to all Kullback-Leibler neighborhoods of

P0. Moreover, discrete approximations to the predictive distribution converge weakly as well

and, as a consequence, converge to the GMM estimate of P0. The restrictions on the preferences

and prior beliefs of this hypothetical semiparametric Bayesian would appear to be quite weak.

       However, the archetype is a construct, a hypothetical Bayesian econometrician looking at

the same data as the GMM econometrician. It is the large sample connection between the two

that forms the framework proposed here: the notion that STN ↑ P θ and that STN ⇒ STG . One way

to exploit this insight is to actually do the work of the semiparametric Bayesian and replicate the


                                                              12
multinomial construction in 2C on (5) or (7) or on some other appropriate partition of the sample

space. Such an analysis would require much more than the characterizations in 2B and 2C; it

would necessitate formulating priors that satisfied Assumption A4 (or some analogue of it)

without placing additional substantive restrictions. While it is possible to do so along the lines of

footnotes 13 and 15, this sort of analysis is beyond the scope of the present paper.

           This section is devoted to a discussion of the alternative hypothesis that motivated the

paper:         that plausible differences between the a posteriori beliefs of a hypothetical

semiparametric Bayesian and a GMM econometrician can inform estimation and inference in

GMM settings. The next subsection provides a distorted beliefs interpretation of confidence

regions and goodness-of-fit statistics based on the regression sum of squares in (9). The final

subsection discusses some of the uses of the corresponding residuals.

A. Test Statistics and Confidence Regions

                          {                           }
           Let ẑ φT = θˆTφ ,{Pˆ tT (θˆTφ ), t ≤ T} = arg min D(z φT ) and note that TD(zˆ φT ) is given by:
                                                               {θ ,{P( θ ),t ≤ T}}



                                      (                )                                            (
                                                               + T 2 ∑ t ≤ T {φ′′[ξ t (zˆ φT ) − 1} Pˆ tT (θˆTφ ) − T1   )
                                                           2                                                                 2
           TD(zˆ Tφ ) = T 2 ∑ t Pˆ tT (θˆTφ ) − T1
                                                                                                                                 (
                         = TgT (θˆTφ )′VT (θˆTφ ) −1 gT (θˆTφ ) + T 3σ ε2T (Pˆ tT ) + T 2 ∑ t ≤T {φ′′[ξ t (zˆ φT )] − 1} Pˆ tT (θˆTφ ) − T1   )
                                                                                                                                                  2
                                                                                                                                                      (12)
                         → Tg (θˆ φ )′V (θˆ φ ) −1 g (θˆ φ ) ∼ χ 2 (p − q)
                                 T    T      T   T         T       T


where the second line involves the substitution of (9) into (12) and the convergence to a χ2(p–q)

random variable obtains if the moment conditions are valid because ξ t = 1 + op (1) and

T 3σ ε2T (Pˆ tT ) = o p (1) .    Hence, TD(zˆ φT ) differs from the GMM overidentifying restrictions test

statistic in the presence of these two op(1) terms and in the choice of estimator and covariance

matrix – arg min gT (θ )′ST (θ ) −1 gT (θ ) instead of θˆTφ and ST (θT ) = T1 ∑ t ≤ T g(x t , θT )g(x t , θT )′ in
                     θ



place of VT (θˆTφ ) , where θT is any √T consistent estimator of θ.



                                                                                13
           This large sample χ2 test statistic can be used to test the null hypothesis and to construct

confidence regions for θ under the null. Conventional practice is to select a significance level α

and an associated critical value c α that solves Pr( χ p2− q ≥ c α ) = α . The null hypothesis is rejected

if TD(zˆ φT ) > c α while the statistic fails to reject the null if TD(zˆ φT ) ≤ c α . As is typically the case

in likelihood-based inference, the rejection region can be viewed as the complement of the 1–α

per cent confidence region given by {θ ,{PtT (θ ), t ≤ T}: TD(z φT ) ≤ c α } .17

           The link between STN and STG provides for an economic interpretation of rejections in

this inference framework. The rejection region {{PtT (θ ), t ≤ T} ∈ STG : TD(zˆ φT ) > c α } is a subset

of the T-cell multinomials in STG and STN ⇒ STG . The question at hand is simple: are there

beliefs implicit in the rejection region that the econometrician would think that the archetype

might reasonably possess a posteriori? Put differently, might the beliefs of such a Bayesian

make a seemingly sharp rejection appear instead to be compatible with the data? Might there be

plausible beliefs outside the associated 1–α per cent confidence region?

           This then is the main point of the paper. If the answer to these questions is “yes,” the

econometrician could reasonably declare that the test statistic provided a statistically significant

rejection at level α that should be thought of as economically insignificant. A similar statement

applies to economically plausible beliefs that lie outside the confidence region that is the

complement of the rejection region.                  An econometrician who did not want to draw sharp

conclusions about economic as opposed to statistical significance could simply report summary

17
     The empirical likelihood ratio statistic – that is, TD(zˆ φT ) with φ(z) = ln(z) – is Bartlett correctable; see Chen and
                                                                                                      −1      −2
Cui (2006) for the moment condition version of this result. Its mean is E{TD(zˆ log  T )} = q(1 + Bc T ) + O(T ) in large
                                                                           α    ˆ −1
                                                                   T ) ≤ c (1 + Bc T )] = α + O(T ) , where B̂c can be
                                                                                                   −2
samples and the Bartlett correction takes the form Pr[TD(zˆ log
obtained from the bootstrap. There is a subtle issue here; the prior also influences second order inference in this
setting. Under suitable regularity conditions, the Bartlett-corrected empirical likelihood rejection region would be
the appropriate object of inference if the prior was sufficiently flat in the neighborhood of the optimum.


                                                              14
statistics describing the beliefs that seem to be sufficiently compatible with the data.

          One such summary statistic involves the comparison of the sample relative entropy

1
T   ∑ ln Pˆ
      t       t
               T
                   (θˆTφ ) − T1 ln T1 based on the estimate θˆTφ with that of a distribution that is more easily

interpreted. McCulloch (1989) suggested one such calibration: compare the sample relative

entropy with that from a hypothetical binomial experiment in which the null success probability

is ½ and the sample success probability is q with q selected so that:

          1
          T   ∑ ln Pˆ
                    t        t
                              T
                                  (θˆTφ ) = ½[ln½ − ln(1 − q)] + ½[ln½ − ln q] = ½ ln½ − ½ ln[q(1 − q)] − T1 ln T1                    (13)

The presumption is that values of q close to ½ suggest that a sample entropy that is statistically

significant at level α is small in this alternative metric.

          A similar calibration can be based on the multivariate normal distribution for which the

entropy is         d ln 2πe + ln | ∑ |      where ∑ is the covariance matrix. Hence:
                   2

          1
          T   ∑     t ≤T
                           ln Pˆ tT (θˆTφ ) = d2 ln 2πe + ln | ∑ |                                                                    (14)

                                                                                         ˆ | from:
can be solved for | ∑ | , which, in turn, can be compared with the restricted estimate | ∑

          ˆ =
          ∑   ∑ t ≤T PˆtT (θˆTφ )(x t − μˆ )(x t − μˆ )′                                                                              (15)

where μˆ = ∑ t ≤ T Pˆ tT (θˆTφ )x t is the restricted estimate of the mean. Here, too, sufficiently small

                                ˆ | suggest that the difference between the two is “reasonably
differences between | ∑ | and | ∑

small” in this alternative metric.

B. Residual Analysis

          Reasonable a posteriori probability beliefs can be assessed via relations (9) and (12).

The relative contributions of the fitted values                      1
                                                                     T   − T1 gT (θˆTφ )′VT (θˆTφ ) −1 [g(x t , θˆTφ ) − gT (θˆTφ )] and the

residuals εˆtT (θˆTφ ) are given in (9) and values of either that are large in absolute value have a


                                                                     15
disproportionate impact on the P̂tT (θˆTφ ) estimates and their associated sample entropy. Large

residuals may be especially informative since the residuals are identically zero if the implied

probabilities lie between zero and one. The incremental impact of alternative divergences can be

examined via the observed covariance between the excess curvature φ′′[ξ t (zˆ φT )] − 1 and

(Pˆ tT (θˆTφ ) − T1 ) 2 as codified in (12).18

         The whole probability simplex STG can be investigated in this fashion. Plausible values of

θ might be suggested by theory or introspection or might be obtained by bootstrapping, which is

rigorously justifiable under A3. For each θ, STG (θ ) can be explored by enumerating sets of

residuals εt (P) that sum to zero, are orthogonal to g(x t , θ ) , and satisfy the lower and upper

bound constraints, which can then be examined with the regression diagnostics. Conditioning on

θ is the best way to explore STG if E[ εt (P)g(x t , θ )] = 0 is strengthened to E[ εt (P) | g(x t , θ )] = 0 .

         Implicit in this discussion is a particular concern for the effect of outliers on probabilities,

which play a special role in models that incorporate expectations. As Back and Brown (1993)

emphasized, outliers in this setting represent data that are not representative of the underlying

population when the moment conditions are true. In rational expectations models, data that are

underrepresented – that is, those for which PtT (θ ) − T1 is large – are often thought to represent

peso problems, events that were expected to happen but that did not eventuate or that did not

18
  There is a suggestive interpretation of alternative divergences that is hard to make rigorous without taking a stand
on priors. Priors generated via ex ante maximization of the expected distance, such as the Kullback-Leibler or χ2
divergence, between prior and posterior over the sample space are called reference or default priors; see Bernardo
(2005) for a survey and Kuboki (1998) for an application to parametric prediction. Maximizing this distance is
analogous to minimizing the distance between PT and the true distribution P0. The empirical distribution P̂ ⇒ P 0
and P̂(θˆφ ) ⇒ P even under the alternative. In this heuristic sense, cells for which φ (zˆ ) is small are ones for which
        T       T                                                                         t

the data dominate the prior in the distance as measured by φ(•) while those for which φ (zˆ t ) is large are ones for
which the apparent impact of the prior remains sizeable. It is a considerable leap to go beyond these heuristics to an
actual reference prior for semiparametric Bayesian prediction and a corresponding assessment of the impact of the
prior in a given sample. The ideas in footnotes 13 and 15 are one place to start.


                                                          16
occur as frequently as expected. For example, the Great Depression might represent a recurrent

rare event or one that will succumb to the law of large numbers. Accordingly, we might

reasonably expect the prior predictive probability PN ( x T +1 ∈ X tT ) = ∫ θ P(x T +1 ∈ X tT )Π(dP) – that
                                                                            P



is, the predictive distribution for xT+1 in the absence of sample information – for some such X tT to

be much larger than the empirical probability      1   , resulting in a seemingly large value of PtT (θ ) .
                                                   T


Note also that PN ( • ) is the posterior predictive probability outside the convex hull of the data.

4. Conclusion

        This paper was based on a simple intuition. What can we learn from probability

statements about sample moment conditions in rational expectations models under the

maintained hypothesis that the moment conditions are true? The answer is straightforward:

modulo sampling error, sample moments reflect biases in the expectations of the relevant

economic actors in these circumstances. This distorted beliefs alternative would appear to be an

interesting one, if only because it provides one dimension in which to distinguish between

economic and statistical significance. All that is needed is a way to measure the attributes of

expectations compatible with the moment conditions.

        The attainment of this goal required a detour down the path of Bayesian semiparametrics.

Models based on moment conditions do not deliver likelihoods and the strict application of the

Bayesian calculus requires their specification. Moreover, the formation of prior beliefs is more

challenging in such settings because the data need not swamp the prior when priors are over

spaces of probability measures. Finally, the literature on priors for semiparametric models is thin

and a broad set of priors would appear to be necessary when seeking to characterize the extent to

which the expectations compatible with a given set of moment conditions are “nearly rational.”




                                                    17
       Two attributes of the archetypical semiparametric Bayesian constructed in Section 2

eliminated these problems. The first was the presumption that the archetype was a consumer of

economic theory who used the model based on moment conditions solely for forecasting. The

second was the shift from densities that respect the moment conditions to discrete measures that

do so. The resulting predictive distribution based on a countable set of multinomial

approximations proves to be consistent under the weak restriction that the prior assigns positive

probability to all Kullback-Leibler neighborhoods of the true distribution. While this observation

is hardly surprising in finite-dimensional parametric settings, it is somewhat more remarkable in

this semiparametric setting in which the typical requirement is far more stringent.

       The result is a semiparametric Bayesian interpretation of probability estimates provided

by empirical likelihood and related minimum divergence methods. From this perspective,

rejection and confidence regions are comprised of probability beliefs, not parameter values,

beliefs an econometrician can examine for their plausibility. This association of plausible beliefs

with such regions yields a framework for assessing the economic significance of distorted beliefs.

       Let me conclude by suggesting three ways in which research along these lines can

proceed. First, it would be useful to have additional analytical tools beyond those described in

Section 3. Second, statistics other than omnibus goodness-of-fit tests can be examined in this

fashion but the difference between the Bayesian and frequentist treatment of nuisance parameters

might make it more difficult to equate the beliefs of the archetype and the GMM econometrician.

Finally, a more interesting archetype might be one with the same objectives but whose decisions

affect sample outcomes as is the case in rational expectations models with learning or in Kurz’s

(1997) rational beliefs equilibria. Here, too, it might well be substantially more challenging to

equate the beliefs of a semiparametric Bayesian and the GMM econometrician.




                                                18
                                          References

Back, K. and Brown, D. P., 1993. “Implied probabilities in GMM estimators,” Econometrica 61,
       971-975.

––––––, 1992. “GMM, maximum likelihood, and nonparametric efficiency,” Economics Letters
      39, 23-28.

Berger, J. O., and G. Salinetti, 1995. “Approximations of Bayes decision problems:          the
       epigraphical approach,” Annals of Operations Research 56, 1-13.

Bernardo, J. M., 2005. “Reference analysis,” in Dey, D. K. and C. R. Rao (eds.). Handbook of
       Statistics, Volume 25: Bayesian Thinking, Modeling, and Computation. Amsterdam:
       Elsevier, 17-90.

Chamberlain, G., 1987. “Asymptotic efficiency in estimation with conditional moment
     restrictions,” Journal of Econometrics 34, 305-334.

Csiszár, I., 1967. “Information-type measures of difference of probability distributions and
       indirect observations,” Studia Scientifica Materia Hungary 2, 299-318.

Diaconis, P. and D. Freedman, 1983. “On Inconsistent Bayes Estimates in the Discrete Case,"
      Annals of Statistics 11, 1109-1118.

––––––, 1986a. “On the Consistency of Bayes Estimates," Annals of Statistics 14, 1-26.

––––––, 1986b. “On Inconsistent Bayes Estimates of Location,” Annals of Statistics 14, 68-87.

Donoho, D. L., and Liu, R. C., 1988. “Pathologies of some minimum distance estimators,”
      Annals of Statistics 16, 587-608.

Dudley, R. M., 1989, Real Analysis and Probability, (New York: Chapman and Hall).

Engle, R. F., D. F. Hendry, and J.-F. Richard, 1983. “Exogeneity,” Econometrica 51, 277-304.

Freedman, D., 1963. “On the Asymptotic Behavior of Bayes Estimates in the Discrete Case I,”
      Annals of Mathematical Statistics 34, 1386-1403.

––––––, 1965. “On the Asymptotic Behavior of Bayes Estimates in the Discrete Case II,”
      Annals of Mathematical Statistics 36, 454-456.

Ghosh, J. K. and R. V. Ramamoorthi, 2003. Bayesian Nonparametrics. Springer, New York.

Hansen, L. P., 1982. “Large Sample Properties of Generalized Method of Moments Estimators,”
      Econometrica 50, 1029 1054.




                                              19
Kim, J.-Y., 2002. “Limited information likelihood and Bayesian Analysis,” Journal of
      Econometrics 107, 175-193.

Kitamura, Y., 2006. “Empirical Likelihood Methods in Econometrics: Theory and Practice,”
      Cowles Foundation Discussion Paper No. 1569, Department of Economics, Yale
      University.

Kuboki, H., 1998. “Reference priors for prediction,” Journal of Statistical Planning and
      Inference 69, 295-317.

Kurz, M., (ed.), 1997. Endogenous Economic Fluctuations: Studies in the Theory of Rational
      Beliefs. Springer, New York.

Lazar, N., 2003. “Bayesian empirical likelihood,” Biometrika 90, 319–26.

McCulloch, R. E., 1989. “Local Model Influence,” Journal of the American Statistical
      Association, 84, 473-478.

Owen, A., 1991. “Empirical Likelihood for Linear Models,” Annals of Statistics 19, 1725-1747.

––––––, 2001. Empirical Likelihood (New York: Chapman and Hall).

Parthasarathy, K. R., 1967, Probability Measures on Metric Spaces, (New York: Academic
       Press).

Read, T. R. C., and N. A. C. Cressie, 1988. Goodness-of-fit statistics for discrete multivariate
       data (New York: Springer-Verlag).

Schennach, S. M., 2005. “Bayesian exponentially tilted empirical likelihood,” Biometrika 92,
      31–46.

Schwartz, L., 1965. “On Bayes procedures,” Z. Wahrsch. Verw. Gebiete 4, 10-26.

Stinchcombe, M., 2004. “The unbearable flightiness of Bayesians: generically erratic updating,”
       working paper, Department of Economics, University of Texas at Austin.

Zellner, A., 1994. “Model, prior information and Bayesian analysis,” Journal of Econometrics
       75, 51–68.

––––––, 1997. “The Bayesian method of moments (BMOM): theory and application,” in Fomby,
      T., and Hill, R. C. (eds.). Advances in Econometrics. Cambridge University Press.

Zervos, M., 1999. “On the Epiconvergence of Stochastic Optimization Problems,” Mathematics
       of Operations Research 24, 495-508.




                                              20
