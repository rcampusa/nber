                               NBER WORKING PAPER SERIES




                   UPDATING BELIEFS WITH AMBIGUOUS EVIDENCE:
                         IMPLICATIONS FOR POLARIZATION

                                       Roland G. Fryer, Jr.
                                         Philipp Harms
                                       Matthew O. Jackson

                                       Working Paper 19114
                               http://www.nber.org/papers/w19114


                     NATIONAL BUREAU OF ECONOMIC RESEARCH
                              1050 Massachusetts Avenue
                                Cambridge, MA 02138
                                     June 2013




This project benefited greatly from extensive discussions with Richard Holden. We are also grateful
to Ben Golub, Lawrence Katz, Matthew Rabin, and Andrei Shleifer for helpful comments and suggestions.
We gratefully acknowledge financial support from the NSF grants SES-0961481 and SES-1155302,
and ARO MURI Award No. W911NF-12-1-0509. The views expressed herein are those of the authors
and do not necessarily reflect the views of the National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2013 by Roland G. Fryer, Jr., Philipp Harms, and Matthew O. Jackson. All rights reserved. Short
sections of text, not to exceed two paragraphs, may be quoted without explicit permission provided
that full credit, including © notice, is given to the source.
Updating Beliefs with Ambiguous Evidence: Implications for Polarization
Roland G. Fryer, Jr., Philipp Harms, and Matthew O. Jackson
NBER Working Paper No. 19114
June 2013, Revised July 2013
JEL No. D03,J01

                                             ABSTRACT

We introduce and analyze a model in which agents observe sequences of signals about the state of
the world, some of which are ambiguous and open to interpretation. Instead of using Bayes' rule on
the whole sequence, our decision makers use Bayes' rule in an iterative way: first to interpret each
signal and then to form a posterior on the whole sequence of interpreted signals. This technique is
computationally efficient, but loses some information since only the interpretation of the signals is
retained and not the full signal. We show that such rules are optimal if agents sufficiently discount
the future; while if they are very patient then a time-varying random interpretation rule becomes optimal.
One of our main contributions is showing that the model provides a formal foundation for why agents
who observe exactly the same stream of information can end up becoming increasingly polarized in
their posteriors.


Roland G. Fryer, Jr.                                Matthew O. Jackson
Department of Economics                             Department of Economics
Harvard University                                  Stanford University
Littauer Center 208                                 Stanford, CA 94305-6072
Cambridge, MA 02138                                 and CIFAR
and NBER                                            and also external faculty of the Santa Fe Institute
rfryer@fas.harvard.edu                              jacksonm@stanford.edu

Philipp Harms
EdLabs
44 Brattle Street
Cambridge MA 02138
pharms@edlabs.harvard.edu
        “It ain’t what you don’t know that gets you into trouble. It’s what you know for
        sure that just ain’t so. – Mark Twain


1       Introduction
Some argue that the world is becoming more polarized (Sunstein 2009, Brooks 2012). Con-
sider, for instance, Americans’ views on global warming. In a 2003 Gallup poll, 68 percent of
self-identified Democrats believed that temperature changes over the past century could be
attributed to human activity, relative to 52 percent of Republicans (Saad 2013). By 2013,
these percentages had diverged to 78 percent and 39 percent.1 In a similar vein, there were
large racial differences in reactions to the O.J. Simpson murder trial. Four days after the
verdict was announced, 73 percent of whites but only 27 percent of blacks surveyed in a
Gallup/CNN/USA Today poll believed that Simpson was guilty of murdering Nicole Brown
Simpson and Ronald Goldman (Urschel 1995).
    Psychologists have long recognized humans’ propensity to over-interpret evidence that
reinforces their beliefs while disregarding contradictory information.2 A particularly striking
example is Darley and Gross (1983). Subjects in their experiment are asked to grade an essay
written by a fictional student. Before reading the essay, however, graders view one of two
videos of the student playing in his or her home. The first video shows the student in a poor,
inner-city neighborhood, while the second depicts a middle-class suburban environment.
Subjects shown the first video give the essay significantly lower grades. However, a subset of
each group was also shown an identical video of the student in class, providing both correct
and incorrect answers to a teacher’s questions. Respondents who saw this second video
diverged even further in their grading of the essay. Subjects who viewed the low-quality
(high-quality) neighborhood and the class video gave the essay a lower (higher) grade than
those who just saw the neighborhood video. The showing of the same video to the subjects
caused them to further diverge in their grades.
    The importance of polarizing beliefs in part derives from the conflict they induce. In
influential papers, Esteban and Ray (1994) and Duclos, Esteban, and Ray (2004) derive
measures of societal polarization and methods to estimate them.3 Later work has linked
increased polarization to conflict both theoretically (Esteban and Ray 2011) and empirically
(Esteban, Mayoral, and Ray 2013). These papers motivate understanding the mechanisms

    1
      Beliefs that the effects of global warming “have already begun to happen” and that “global warming
will pose a serious threat to you or your way of life in your lifetime” show similar trajectories for both groups
(Saad 2013).
    2
      See, e.g., Lord, Ross, and Lepper (1979). A summary of many of these studies can be found in Table
1, and we review some of this evidence in more detail below.
    3
      Relatedly, Jensen et al. (2012) derive measures of political polarization.


                                                       2
that can lead opinions to diverge even in the face of common information, since such diver-
gence can result in society-level disruptions, intolerance, and discrimination.
    In this paper, we introduce a model that provides a simple foundation for why the above-
described polarizations in beliefs should be observed from rational agents. In our model,
agents interpret information according to Bayes’ rule as it is received and store the inter-
preted signal in memory rather than the full information, and this simple modification of
Bayesian updating can lead to increasing and extreme polarization. In particular, there are
two possible states of nature A, B and an agent observes a series of signals a, b that are cor-
related with the state of nature. Some of the signals are ambiguous and come as ab and must
be interpreted. The difference from standard Bayesian agents with unbounded memory, is
that we assume that an agent does not store a sequence such as a, b, ab, ab, a, ab, b, b . . . in
memory, but, due to limited memory, interprets the ab signals as they are seen according
to Bayes’ rule (or some other rule), and then stores the interpretation in memory. So, if
the agent started by believing that A was more likely, then the sequence would be inter-
preted and stored in memory as a, b, a, a, a, a, b, b . . .. By storing the full ambiguous sequence
a, b, ab, ab, a, ab, b, b . . . the agent would actually see more evidence for b than a, while inter-
preting signals according to Bayes’ rule as they are received and then storing the interpre-
tation leads to a, b, a, a, a, a, b, b . . . and more evidence for a than b.
    This can lead people with slightly different priors to end up with increasingly different
posteriors and they interpret the same string of ab’s very differently, which then reinforces
their differences in beliefs. As we show, if a nontrivial fraction of experiences are open to
interpretation, then two agents who have differing priors and who see exactly the same se-
quence of evidence can, with positive probability, end up polarized with one agent’s posterior
tending to place probability one on A and the other tending to place probability one on B.
    With the lens of our model, the two neighborhood videos in Darley and Gross (1983)
created different priors on the students’ ability. This then affected the way that respondents
interpreted the second video of students answering questions. A student with a lower prior
on the talent of the student would put more emphasis on the incorrect answers in the second
video, and a student with a higher prior would put more emphasis on the correct answers.
This then leads to the observed polarization of beliefs when grading the essays. Bayes’ rule
with unbounded memory would predict the opposite – seeing the same second video should
lead beliefs to get closer if anything. With respect to the O.J. Simpson murder trial, people
interpreted the same trial as either another example of an unfair legal system or another
example of murder. The data are open to interpretation, and based on their differing past
experiences and beliefs prior to the trial people reached different conclusions.
    The deviation of our model from fully Bayesian updating is minimal: Bayes’ rule is still
used at all steps and agents are rational, they are simply constrained by limited memory and


                                                 3
must interpret ambiguous signals before storing them and then base their posteriors on the
memories rather than all of the original data. Some of our results examine optimal ways to
interpret ambiguous signals. We show that the rule above, based on iterative use of Bayes’
rule is optimal if the agent is not too patient, and so cares enough about making correct
decisions in the short run (or else begins with a strong prior). In contrast, if the agent is very
patient, then the agent may wish to randomize interpretations and may do so in a manner
that depends on the current belief.
    The paper proceeds as follows. Section 2 provides a brief literature review. Section
3 describes a framework for updating beliefs when evidence is unclear and section 4 uses
this insight to understand potential mechanisms that drive polarization. The final section
concludes. An appendix contains the proofs of all formal results as well as additional technical
details.


2        A Brief Review of the Literature
Rabin and Schrag (1999) provide a first decision-making model of confirmation bias.4 In
each period agents observe a noisy signal about the state of the world, at which point they
update their beliefs. Agents’ perceptions are clouded by bias. In their model, signals that
are believed to be less likely are misinterpreted with an exogenous probability. Given the
exogenous mistakes introduced into the model, agents can converge to incorrect beliefs if
they misinterpret contradictory evidence sufficiently frequently. The model does not clarify
the mechanism behind the misinterpretation. Our model provides a foundation for the
interpretation and storage of ambiguous information that can be thought of as providing a
“why” behind the polarization.
    In an important paper, Hellman and Cover (1970) introduced a model of limited memory
that provides insight into how restrictions on the updating process motivated by the psy-
chology literature can yield biases in beliefs (see also Wilson (2003), Mullainathan (2002),
Gennaioli and Shleifer (2010), and Glaeser and Sunstein (2013)). Agents are endowed with
a finite number of “memory states,” and the updating process is a set of rules for how to
transition between states after observing a period’s signal. The agent’s goal is to know the
correct state of the world when the sequence randomly terminates. The optimal transition
rule turns out to be fairly simple. First, agents rank their memory states from 1 to N ,
with higher numbers indicating an increased likelihood that the “good” state of the world is
correct. Following a good (bad) signal, the agent shifts to the next higher (lower) state until
she reaches one of the extremes states (n = 1 or n = N ). Hellman and Cover show that
optimizing agents with sufficient patience (probability of continuation) will leave the extreme
    4
        See also Dandekar, Goel and Lee (2013) for a network model of a confirmatory bias.


                                                      4
state with very low probability. Thus, with some probability, agents will reach a state in
which they ignore large amounts of seemingly contradictory information. As Wilson (2003)
points out, this can yield polarization under several scenarios. For instance, two people who
receive the same set of signals but start at different memory states can end up in different
places for long periods of time, as they can get temporarily “stuck” at one of the extreme
states. More generally, Wilson proves that as long as agents do not have identical priors and
do not start at one of the extreme states, their beliefs can differ for long periods of time with
positive probability.
    Baliga, Hanany and Klibanoff (2013) provide a very different explanation for polarization
based on ambiguity aversion. In their model, agents with different prior beliefs may update
in different directions after observing some intermediate signals due to a “hedging effect” -
as agents wish to make predictions that are immune to uncertainty, and they may be averse
to different directions of ambiguity given their differing priors.
    Our model is based on a completely orthogonal approach to both of those above. Im-
portantly, ours is the first model providing foundations for increasing, extreme, and perma-
nent polarization even when agents observe a common sequence of information that would
lead a fully rational Bayesian agent to an arbitrarily accurate posterior. In Hellman and
Cover’s model, as detailed by Wilson (2003), beliefs are ergodic: agents follow a similar ir-
reducible and aperiodic Markov chain, simply starting in different states, and their limiting
distributions would coincide. In the Baliga, Hanany and Klibanoff model, with increasing
information over time, agents’ posteriors would eventually become closer as they become
increasingly certain of the state. In our model, agents become increasingly polarized and
increasingly certain that they are each correct despite their disagreement, after seeing ar-
bitrarily informative sequences. Thus, our results are quite complementary to those based
on models of finite memory or ambiguity aversion. The simplicity and transparency of our
model mean that it is also easily extendable to alternative settings.


3     A Model of Updating Beliefs when Experiences are
      Open to Interpretation
There are two possible states of nature: ω ∈ {A, B}.
     An agent observes a sequence of signals, st , one at each date t ∈ {1, 2, . . .}. The signals
lie in the set {a, b, ab}. A signal a is evidence that the state is A, a signal b is evidence that
the state is B, and the signals ab are ambiguous and open to interpretation.
     In particular, signals are independent over time, conditional upon the state. With a
probability π < 1 the signal is st = ab independent of the state. With probability 1 − π the


                                                5
signal st is either a or b. If the state is ω = A and the signal is not ab, then the probability
that st = a is p > 1/2. Likewise, if the state is ω = B and the signal is not ab, then the
probability that st = b is p > 1/2.
    For example, A might be a world with many racists and B a world with few racists.
In A a fraction p > 1/2 of interactions are racist, while in state B a fraction p > 1/2 of
interactions are non-racist. Or it might be that there are a sequence of studies regarding the
impact of climate change or the efficacy of the death penalty that come out, p of which are
valid and 1 − p of which are corrupt or flawed in state A and the reverse in state B.
    λ0 ∈ (0, 1) is the agent’s prior that ω = A.
    Throughout our analysis we assume that the prior, λ0 , is not 0 or 1, as otherwise learning
is precluded. Similarly, we maintain the assumption that π < 1 as otherwise no information
is ever revealed.
    It follows directly from a standard law of large numbers argument (e.g., Doob’s (1949)
consistency theorem), that the agent’s posterior beliefs converge to place weight one on the
true state ω almost surely.

Observation 1 A Bayesian-updating agent who forms beliefs conditional upon the full se-
quence of signals observed through each time has a posterior that converges to place probability
1 on the correct state almost surely.

    The signals ab are uninformative as they occur with a frequency π regardless of the state.
Thus, a Bayesian updater who remembers all of the signals in their entirety effectively ignores
interactions that are open to interpretation.


3.1    Interpreting Signals and Limited Memory
An important aspect of the example described in the introduction is that agents have limited
memory. Otherwise, there would be no need to interpret signals – agents would store the
full “scene” of every interaction in memory and be fully Bayesian.
    A key element of our model is that at each date an agent can only store one bit of
data. When the agent sees an ambiguous signal st = ab, the agent must interpret and store
it as either a or b. An agent has limited memory and cannot remember all the possible
interpretations of all social interactions and thus, must store in memory a strict subset of
the information involved in the social interaction. The interpretation of ambiguous signals
st = ab as clear signals a or b is based on the agent’s experiences through time t.
    Our limited memory assumption warrants some discussion. In a world with unlimited
memory, agents would store an nxt matrix – n arbitrarily large and t growing with time
– where the first dimension n captures the complexity of the signal in terms of “bytes”


                                               6
(a, b, ab, . . .) and the t dimension captures how many experiences she has stored. Our model
reduces this matrix to 1xt, making the signal agents are able to store one byte of the data
but still allowing the sequence of bytes to be arbitrarily long. If we were to make agents
have limited memory on both dimensions – say they can only store mxT for some fixed m
and T where m is less than the complexity of the signals5 and T is finite – the results would
still hold qualitatively.
     Our agent faces two related limitations. First, she interprets ambiguous signals and stores
the interpreted signals in memory as if they were observed as interpreted. The agent does
this in a “rational” manner, according to their maximum likelihood. As we show below,
this will be an optimal storage technique if the agent is faced with making decisions in the
short run that depend on how they interpret the situations they face. In addition, she also
“forgets” that she has interpreted these signals. If the agent could remember the sequence
of dates on which she interpreted signals, then she could undo the memory storage and
would end up with the fully rational posterior.6 It is important that the agent is not able
to completely invert the sequence of stored signals: at least some of the interpreted signals
are eventually treated as if that was the observed signal. This seems both plausible and
consistent with the evidence discussed in Section 4.1.

3.1.1     An Example of Polarized Beliefs

To illustrate this process and the polarization that it induces, let us consider a specific
example. For instance, suppose that the true state is ω = A, the probability that an
unambiguous signal matches the state is p = 2/3, and the fraction of ambiguous signals is
π = 1/2. In such a case, the sequence of signals might look like:

                                  a, ab, ab, b, a, ab, a, ab, ab, b, ab, ab, . . .

    In particular, consider a case such that the agent interprets st = ab based on what is
most likely under her current belief λt−1 : the agent interprets st = ab as a if λt−1 > 1/2 and
as b if λt−1 < 1/2.
    Suppose that the agent’s prior is λ0 = 3/4. When faced with s1 = a, the agent’s posterior
λ1 becomes 6/7. So, the agent updates beliefs according to Bayes’ rule given the interpreted
(remembered) signals.7 Then when seeing s2 = ab the agent stores the signal as a, and ends



    5
      See Fryer and Jackson (2008) for discussion of coarse storage of signals.
    6
      Even if she knew in which way she interpreted signals and how frequently (e.g., knowing that she did
it 40 percent of the time and storing ab’s as a’s) she could undo the effect.
    7
      In this case λt = P (A|st = a, λt−1 ) = 2λt−1 /(1 + λt−1 ), and λt = P (A|st = b, λt−1 ) = λt−1 /(2 − λt−1 )


                                                         7
up with a posterior of λ2 = 12/13. In this case, the agent would store the sequence as

                                     a, a, a, b, a, a, a, a, a, b, a, a, . . .

and the posterior at the end of this sequence would already be very close to 1.
   In contrast, consider another agent whose prior is λ0 = 1/4. When faced with s1 = a, the
agent’s posterior λ1 becomes 2/5. Then when seeing s2 = ab the agent stores the signal as b,
and ends up with a posterior of λ2 = 1/4. In this case, the agent would store the sequence
as
                                a, b, b, b, a, b, a, b, b, b, b, b, . . .

and the posterior at the end of this sequence would be very close to 0.
   Two agents observing exactly the same sequence with different (and non-extreme) priors,
come to have increasingly different posteriors. Their posteriors are

3/4, 6/7, 12/13, 24/25, 12/13, 24/25, 48/49, 96/97, 192/193, 384/385, 192/193, 192/193 . . .
1/4, 2/5, 1/4,      1/7,    1/13, 1/7,        1/13, 1/7,          1/13,          1/25,   1/49,   1/97 . . .


3.2     Optimal Interpretations in the Face of Choosing an Action
In the above example, we considered situations where the agent interprets signals according
to which state is more likely, which we refer to as maximum likelihood storage. There is a
tradeoff: if an agent stores a signal in the way that is viewed as most likely given the current
beliefs that can help in reacting to the current situation at hand, but then the agent can
bias long-term learning.
    To explore this tradeoff, consider a situation in which the agent must take an action
based on the current interaction. For instance, in the context of our racist example, at each
date the agent must react appropriately to either being faced with a racist or not.8 In the
case of an ambiguous encounter, the agent must take an action. The key assumption is that
if the agent decides that it is best to respond to st = ab at time t in a manner consistent
with a signal of a, then the agent must store the signal as an a. The agent cannot treat the
signal in one manner for actions and then remember it differently. This is in line with our
limited memory assumption.
    To be more explicit, suppose the agent has to call out her interpretation a or b of the
unclear signal she receives. She gets payoff of ut = 1 if the current situation is correctly


   8
    Note that it is the particular situation/signal that must be appropriately interpreted, not the state.
The probability of the state is useful in making a decision, but it is the particular encounters that the agent
must make decisions about.


                                                        8
identified and ut = 0 when a mistake is made.9 In particular, if the agent calls out a, then
ut = 1 with probability p if the state is A and probability (1 − p) if the state is B. Similarly,
if the signal is ambiguous and the agent calls out b, then ut = 1 with probability (1 − p) if
the state is A and probability p if the state is B.
     To represent the full set of possible strategies that an agent might have for making
interpretations (including strategies for agents with unbounded memories), we first define
histories. Let a history ht = (s1 , i1 ; . . . ; st , it ) ∈ {{a, b, ab} × {a, b}}t be a list of raw signals
and their interpretations seen through time t. Let Ht = {{a, b, ab} × {a, b}}t be all the
histories of length t and H = ∪t Ht be the set of all finite histories.
     A strategy for the agent is a function σ that can depend on the history and the agent’s
beliefs and generates a probability that the current signal is interpreted as a: σ : H ×[0, 1] →
[0, 1].10 In particular, σ(ht−1 , λ0 ) is the probability that the agent interprets an ambiguous
signal st = ab as a conditional upon the history ht−1 and the initial prior belief λ0 .
     A limited-memory strategy is a strategy that depends only on interpreted and not on raw
signals.11
     An agent’s expected payoffs can be written as:
                                                   ∞
                                                                                     !
                                                   X
                             U (σ, δ, λ0 ) = E           δ t ut (σ(ht−1 , λ0 )) λ0       .
                                                   t=1


     The optimal strategy in the case of unconstrained memory is that of a fully rational
Bayesian. It can be written solely as a function of the posterior belief, as the history that
led to the posterior is irrelevant.
     Our limited-memory strategies exhibit interesting history dependencies. For instance,
a sequence can be reshuffled and will not affect Bayesian updating with unbounded mem-
ory. However, with bounded memory, the order of observation becomes important. See-
ing a sequence where the a’s all appear early tilts the prior towards the state A which
then effects the interpretation of ab’s towards a’s. In contrast, reshuffling a sequence to-
wards having the b’s early has the opposite effect. For instance, in our example in Sec-
tion a, ab, ab, b, a, ab, a, ab, ab, b, ab, ab, was interpreted as a, a, a, b, a, a, a, a, a, b, a, a, . . . by
anyone starting with a prior above 1/2. Suppose we reorder that original sequence to be
b, b, ab, ab, ab, ab, ab, ab, a, a, ab, a,. With the same prior in favor of A, but sufficiently close to
1/2, the interpretation would instead flip to be b, b, b, b, b, b, b, b, a, a, b, a,, and end up push-
ing the beliefs towards B. So, the order in which a sequence of signals appear can now be
    9
      Given the binary setting, the normalization is without loss of generality.
   10
      Given that we allow the strategy to depend on the history, it is irrelevant whether we allow it to depend
on the current posterior or original prior, as either can be deduced from the other given the history.
   11
      A strategy is limited-memory if σ(ht ) = σ(h0t ) whenever the even entries (the interpreted signals, it ’s)
of ht and h0t coincide.


                                                         9
(enormously) consequential.
    An optimal limited-memory strategy is one that adjusts the probability of interpreting a
signal with the posterior, but appears difficult to derive in a closed form. Nonetheless, we can
find strategies that approximate the optimal strategy when the agent is sufficiently patient
or impatient. To this aim, we will compare several classes of strategies: (a) the optimal
strategy, (b) ones that depend only on the time, and (c) ones that involve randomizing in a
simple manner based on the posterior.
    In the latter class of strategies, the agent randomizes in interpreting ambiguous signals,
but with a fixed probability that leans towards the posterior. Let γ ∈ [0, 1] be a parameter
such that the agent follows the posterior with probability γ and goes against the posterior
with probability 1 − γ. Under such a strategy, at time t with posterior λt−1 , the agent
interprets the unclear signal as a with probability γ1λt−1 ≥.5 + (1 − γ)1λt−1 <.5 and b with the
remaining probability.12 We denote this strategy by σ γ .
    The special case of γ = 1 corresponds to maximum likelihood interpretation. As we show
now, maximum likelihood is an approximately optimal method of interpretation if the agent
cares relatively more about the short run than the long run.

Proposition 1 If the agent’s discount factor is small enough or the prior belief is close
enough to either 0 or 1, then the maximum likelihood strategy is approximately optimal. That
is, for any λ0 and ε > 0, there exist δ such that if δ < δ, then U (σ 1 , δ, λ0 ) ≥ U (σ, δ, λ0 )−ε for
all strategies σ. Moreover, the same statement holds for any δ for λ0 that are close enough
to 0 or 1.

    All proofs appear in the appendix.
    The intuition is straightforward. The underlying tension is between correctly calling the
state in the short run, and interpreting signals over the long run. If the decision maker is
sufficiently impatient then it is best to make correct decisions in the short run and not worry
about long-run learning. The last part of the proposition shows that even if the agent is
very patient, if the agent begins with a strong prior in one direction or the other (λ0 near
either 0 or 1), then maximum likelihood is again optimal as the agent does not expect to
learn much.
    At the other extreme, by setting γ = 1/2, then it is clear that the agent will learn
the state with probability one in the long run. However, that is at the expense of making
incorrect decisions at many dates. More generally, we can state the following proposition.
    Let us say that there is long-run learning under the randomized-interpretation strategy
  γ
σ for some γ if the resulting beliefs λt converge to the true state almost surely.
  12
    It is straightforward to make the updating function more continuous around .5, with no qualitative
impact on the results.


                                                  10
Proposition 2 If π < p−1/2p
                              , then ambiguous signals are infrequent enough so that there is
long-run learning under the randomized-interpretation strategy σ γ for any γ. So, consider
the case in which π > p−1/2
                        p
                            .

(a) If γ < p−1/2+π(1−p)
                   π
                        then there is long run learning under the randomized-interpretation
    strategy σ γ .

(b) If γ > p−1/2+π(1−p)
                π
                        then there is a positive probability that beliefs λt converge to the wrong
    state under the randomized-interpretation strategy σ γ .


3.3    Approximately Optimal Strategies: Two-Step Rules

    Proposition 2 shows that long run learning under a randomized-interpretation strategy
only occurs if the randomization is sufficiently high (γ is sufficiently low). This can be very
costly in the long run, as although the posterior converges, it happens because the agent is
randomly interpreting ambiguous signals, even when the agent is almost certain of how they
should be interpreted.
    The optimal strategy should adjust the randomization with the belief: as the agent be-
comes increasingly sure of the true state, the agent should become increasingly confident
in categorizing ambiguous signals, and use less randomization. The fully optimal strategy
is difficult to characterize as it is the solution to an infinitely nested set of dynamic equa-
tions and we have not found a closed-form. Nonetheless, we can easily find a strategy that
approximates the optimal strategy when the agent is sufficiently patient.
    Consider a T -period two-step rule, defined as follows. For T periods the agent uses
γ = 1/2, and after T periods the agent uses γ = 1. Let σ T denote such a strategy. We will
show that with sufficient patience, such a strategy is approximately optimal.
    As a strong benchmark, let σ F I denote the full-information strategy, where the agent
actually knows whether the state is A or B, and so when seeing ab then always calls a if
ω = A (or b if ω = B). In this case, the expected utility is independent of λ0 and can be
written as a function of the strategy and the discount factor alone: U (σ F I , δ). This is a very
stringent benchmark as it presumes information that the agent would never know even under
the best circumstances. Even so, we can show that a simple two-step rule can approximate
this benchmark.

Proposition 3 Sufficiently patient agents can get arbitrarily close to the full information
payoff by using a variation on the two-step rule. That is, for any  and λ0 , there exist T and
δ such that if δ > δ, then U (σ T , δ, λ0 ) > U (σ F I , δ) − .



                                               11
    Thus, with sufficient patience a very simple strategy approaches the full information
benchmark.13
    Putting these two results together, how a subjective degree of belief should change “ra-
tionally” to account for evidence which is open to interpretation depends, in important
ways, on how patient a decision maker is. If they are sufficiently patient, a strategy that
entails randomization in the presence of unclear evidence for finite time and then following
their maximum likelihood estimate thereafter approximates the full information Bayesian
outcome.14
    In stark contrast, if agents sufficiently discount the future (or, equivalently, don’t expect
a large number of similar interactions in the future), they interpret ambiguous evidence as
the state which has the highest maximum likelihood, given their prior belief. This leads to
more informed (and optimal, though possibly mistaken) decisions in the short-run, but the
potential of not learning over the longer-run.
    An important remark is that we have specified two-step rules in terms of time. An
alternative method is to do the following. If an agent’s beliefs λt place at least weight ε on
both states (λt ∈ [ε, 1 − ε]), then randomize with equal probability on interpretations, and
otherwise use the maximum likelihood method. This strategy does not require any attention
to calendar time, and also will approximate the full information strategy for small enough ε.


4        Implications for Polarization of Beliefs
As we have already seen in Section 3.1.1, two agents can have very different interpreta-
tions of exactly the same events and that can lead them to update differently, and become
increasingly polarized in their beliefs. The next result generalizes that observation.

Proposition 4 Suppose that a nontrivial fraction of experiences are open to interpretation
so that π > p−1/2
                p
                   . Consider two interpretative agents 1 and 2 who both use the maximum
likelihood rule but have differing priors: agent 1’s prior is that A is more likely (so 1 has a
prior λ0 > 1/2) and agent 2’s prior is that B is more likely (so 2 has a prior λ0 < 1/2).
Let the two agents see exactly the same sequence of signals. With a positive probability that
tends to 1 in π the two agents will end up polarized with 1’s posterior tending to 1 and 2’s
posterior tending to 0. With a positive probability tending to 0 in π the two agents will end
up with the same (possibly incorrect) posterior tending to either 0 or 1.

    13
      Proposition 3 also clearly holds for a slightly different strategy: σ x which is defined by setting γt = 1/2
until either λt > 1 − x or λt < x, and then setting γt = 1 forever after. Instead of holding for a large enough
T and δ, the proposition then holds for a small enough x and large enough δ.
   14
      This result highlights a familiar tradeoff between exploitation (being correct in the short run) and
experimentation (long-run learning) typical in the multi-armed bandit literature (Berry and Fristedt 1985).


                                                       12
    The proof is based on the observation that when π = 1 and λ0 > 1/2, then all signals are
interpreted as a under the maximum likelihood storage rule. Moreover, the law of the belief
process depends continuously on π.


4.1    Evidence Consistent with Polarization of Beliefs
Table 6 summarizes a variety of studies in which identical information given to subjects in
a variety of experimental settings resulted in increased polarization of beliefs – individuals
expressing more confidence in their initial beliefs. The seminal paper in this literature is
Lord, Ross, and Lepper (1979), who provided experimental subjects with two articles on
the deterrent effects of capital punishment. The first article offered evidence that capital
punishment has a deterrent effect on crime, while the second argued there was no relationship.
The authors observe both biased assimilation (subjects rate the article reinforcing their
viewpoint as more convincing) and polarization (subjects express greater confidence in their
original beliefs).
    Several other studies have produced similar results using opinions of nuclear power
(Plous 1991), homosexual stereotypes (Munro and Ditto 1997), perceptions of fictional
brands (Russo, Meloy, and Medvec 1998), theories of the assassination of John F. Kennedy
(McHoskey 2002), the perceived safety of nanotechnology (Kahan et al 2007), and the accu-
racy of statements made by contemporary politicians (Nyahn and Reifler 2010).
    Nyhan, Reifler, and Ubel (2013) provide a recent example relating to political beliefs
regarding health care reform. They conduct an experiment to determine if more aggressive
media fact-checking can correct the (false) belief that the Affordable Care Act would create
“death panels.” Participants from an opt-in Internet panel were randomly assigned to either
a control group in which they read an article on Sarah Palin’s claims about “death panels” or
a treatment group in which the article also contained corrective information refuting Palin.
    Consistent with the maximum likelihood storage rule, Nyhan, Reifler, and Ubel (2013)
find that the treatment reduced belief in death panels and strong opposition to the Affordable
Care Act among those who viewed Palin unfavorably and those who view her favorably but
have low political knowledge. However, identical information served to strengthen beliefs in
death panels among politically knowledgeable Palin supporters.


5     Conclusion
Polarization of beliefs has been documented by both economists and psychologists. To date,
however, there has been little understanding of the underlying mechanisms that lead to
such polarization, and especially why increasing polarization occurs even though agents are


                                             13
faced with access to the same information. To fill this void, we illuminate a simple idea:
when evidence is open to interpretation, then a straightforward – and constrained optimal
– iterative application of Bayes’ rule can lead individuals to polarize when their information
sets are identical.
    As an additional important prediction, it follows directly from our model that agents who
are forced to crystalize their beliefs (through deliberate action or social interactions) in the
face of signals that are open to interpretation will polarize faster (and in more situations)
than those who only infrequently have to react to signals that are open to interpretation.
This provides a testable empirical prediction for future experiments.
    Beyond polarization, our adaptation of Bayes’ rule has potentially important implications
for other information-based models such as discrimination. For instance, using our updating
rule, it is straightforward to show that statistical discrimination can persists in a world with
infinite signals. This is not true in standard models the rely on a single signal (e.g. Coate
and Loury 1993). Moreover, policies designed to counteract discrimination likely have to
be more complicated in an environment in which employers are forced to interpret unclear
signals before hiring decisions are made.
    Bayes’ rule is one of the most often used results in the applied sciences – expressing
how a belief should change, rationally, to account for evidence. In many applications, from
economics to ecology, evidence is open to interpretation and our model can be used to form
posteriors.


References
[1] Baliga, Sandeep, Eran Hanany, and Peter Klibanoff “Polarization and Ambiguity,” forth-
    coming: American Economic Review.

[2] Berry, Donald A. and Bert Fristedt. 1985. Bandit Problems: Sequential Allocation of
    Experiments. Monographs on Statistics and Applied Probability. London; New York.
    Chapman/Hall.

[3] Brooks, David. 2012, June 1. “The Segmentation Century.” The New York
    Times, p. A27. Retrieved from http://www.nytimes.com/2012/06/01/opinion/
    brooks-the-segmentation-century.html?_r=0

[4] Coate, Steven, and Glenn Loury. 1993. “Will Affirmative Action Policies Eliminate Neg-
    ative Stereotypes.” American Economic Review, 83(5): 1220-40




                                              14
[5] Dandekar, Pranav, Ashish Goel, and David T. Lee. (2013) “Biased assimilation, ho-
    mophily, and the dynamics of polarization,” Proceedings of the National Academy of
    Sciences, www.pnas.org/cgi/doi/10.1073/pnas.1217220110

[6] Darley, John M. and Paget H. Gross. 1983. “A Hypothesis-Confirming Bias in Labeling
    Effects.” Journal of Personality and Social Psychology, 44(1): 20-33.

[7] Doob, J.L. “Application of the theory of martingales.” 1949. In Le Calcul des Proba-
    bilités et ses Applications, Colloques Internationaux du Centre National de la Recherche
    Scientifique, 13: 2327.

[8] Duclos, Jean-Yves, Joan Esteban, and Debraj Ray. 2004. “Polarization: Concepts, Mea-
    surement, Estimation,” Econometrica 72(6): 1737-1772.

[9] Esteban, Joan, Laura Mayoral, and Debraj Ray. 2013. “Ethnicity and Conflict: An
    Empirical Study.” American Economic Review, forthcoming.

[10] Esteban, Joan, and Debraj Ray. 1994. “On the Measurement of Polarization.” Econo-
    metrica, 62(4): 819-851.

[11] Esteban, Joan, and Debraj Ray. 2011. “Linking Conflict to Inequality and Polarization.”
    American Economic Review, 101(4): 1345-74.

[12] Fryer, Roland, and Matthew O. Jackson. 2008. “A Categorical Model of Cognition and
    Biased Decision Making,” The B.E. Journal of Theoretical Economics, Volume 8, Issue
    1, Article 6.

[13] Gennaioli, Nicola and Andrei Shleifer. 2010. “What Comes to Mind,” The Quarterly
    Journal of Economics, 125 (4): 1399-1433.

[14] Glaeser, Edward, and Cass Sunstein. “Why Does Balanced News Produce Unbalanced
    Views?” NBER Working Paper No. 18975.

[15] Hellman, Martin A. and Thomas M. Cover. 1970. “Learning with Finite Memory,” The
    Annals of Mathematical Statistics, Vol. 41, No. 3 (June), pp. 765 - 782.

[16] Hoeffding, Wassily. 1963. “Probability Inequalities for Sums of Bounded Random Vari-
    ables.” Journal of the American Statistical Association, 58(301): 13-30.

[17] Jacod, Jean, and Albert Shiryaev. 2003. Limit Theorems for Stochastic Processes. Berlin:
    Springer-Verlag.




                                             15
[18] Jensen, Jacob, Ethan Kaplan, Suresh Naidu, and Laurence Wilse-Samson. 2012. “Po-
    litical Polarization and the Dynamics of Political Language: Evidence from 130 years of
    Partisan Speech.” Brookings Papers on Economic Activity, Fall.

[19] Lord, Charles, Lee Ross and Mark Lepper. 1979. “Biased Assimilation and Attitude Po-
    larization: The Effects of Prior Theories on Subsequently Considered Evidence.” Journal
    of Personality and Social Psychology, 37(11): 2098-2109.

[20] Kahan, Dan M., Paul Slovic, Donald Braman, John Gastil, and Geoffrey L. Cohen. 2007.
    “Affect , Values, and Nanotechnology Risk Perceptions: An Experimental Investigation.”
    GWU Legal Studies Research Paper No. 261; Yale Law School, Public Law Working
    Paper No. 155; GWU Law School Public Law Research Paper No. 261; 2nd Annual
    Conference on Empirical Legal Studies Paper. Available at SSRN: http://ssrn.com/
    abstract=968652

[21] McHoskey, John W. 2002. “Case Closed? On the John F. Kennedy Assassination:
    Biased Assimilation of Evidence and Attitude Polarization.” Basic and Applied Social
    Psychology, 17(3): 395-409.

[22] Mullainathan, Sendhil. 2002. “A Memory-Based Model of Bounded Rationality.” Quar-
    terly Journal of Economics, 117(3): 735-774.

[23] Munro, Geoffrey D. and Peter H. Ditto. 1997. “Biased Assimilation, Attitude Polariza-
    tion, and Affect in Reactions to Stereotype-Relevant Scientific Information.” Personality
    and Social Psychology Bulletin, 23(6): 636-653.

[24] Nyhan, Brendan and Jason Reifler. 2010. “When Corrections Fail: The Persistence of
    Political Misperceptions.” Political Behavior, 32:303-330.

[25] Nyhan, Brendan, Jason Reifler, and Peter Ubel. “The Hazards of Correcting Myths
    about Health Care Reform.” Medical Care 51(2): 127-132.

[26] Plous, Scott. 1991. “Biases in the Assimilation of Technological Breakdowns: Do Acci-
    dents Make Us Safer?” Journal of Applied Social Psychology, 21(13): 1058-1082.

[27] Russo, J. Edward, Margaret G. Meloy, and Victoria Husted Medvec. 1998. “Predeci-
    sional Distortion of Product Information.” Journal of Marketing Research, 35(4): 438-
    452.

[28] Saad,   Lydia. 2013,   April 9. “Republican Skepticism Toward Global
    Warming    Eases.”  Gallup   Politics, http://www.gallup.com/poll/161714/
    republican-skepticism-global-warming-eases.aspx. Retrieved April 27, 2013.

                                             16
[29] Sunstein, Cass. 2009. Going to Extremes: How Like Minds Unite and Divide. Oxford
    University Press.

[30] Urschel, Joe. 1995, October 9. “Poll: A Nation More Divided.” USA Today, p. 5A.
    Retrieved from LexisNexis Academic database, April 22, 2013.

[31] Wilson, Andrea. 2003. “Bounded Memory and Biases in Information Processing.” Un-
    published manuscript, Princeton University.


6       Appendix
Proof of proposition 1.
    Let us first show for any λ0 and ε > 0, there exist δ such that if δ ≤ δ, then U (σ 1 , δ, λ0 ) ≥
U (σ, δ, λ0 ) − ε for all strategies σ. Recall that

                                                 ∞
                                                                                   !
                                                 X
                           U (σ, δ, λ0 ) = E           δ t ut (σ(ht−1 , λ0 )) λ0       .
                                                 t=1


Write
                                                                ∞
                                                                                                  !
                                                                X
               U (σ, δ, λ0 ) = E (u1 (σ(∅, λ0 ))|λ0 ) + E             δ t ut (σ(ht−1 , λ0 )) λ0       .
                                                                t=2

The basic idea is that as δ → 0, the future does not matter and the decision maker only
needs to maximize the current period’s payoff which amounts to choosing the most likely
interpretation. Note that E ( ∞        t                                                 δ     δ
                                P
                                  t=2 δ ut (σ(ht−1 , λ0 ))|λ0 ) lies in the interval [− 1−δ , 1−δ ] lies
                           ε
within [−ε, ε] if δ ≤ δ = 1+ε . Thus, if δ ≤ δ, then

          U (σ 1 , δ, λ0 ) − U (σ, δ, λ0 ) ≥ E u1 (σ 1 (∅, λ0 )) λ0 − E (u1 (σ(∅, λ0 ))|λ0 ) − ε.
                                                                   
                                                                                                          (1)

Next, note that

                  E (u1 (σ(∅, λ0 ))|λ0 ) = E[p Pr[i1 = ω] + (1 − p) Pr[i1 6= ω]|λ0 ].

Since p > 1/2, the maximizing solution is to set i1 to match the most likely state ω given
λ0 , and so σ 1 is optimal for the first period optimization. This implies that

                            E u1 (σ 1 (∅, λ0 )) λ0 ≥ E (u1 (σ(∅, λ0 ))|λ0 ) .
                                                  


Thus, from (3) it follows that if δ ≤ δ, than

                                    U (σ 1 , δ, λ0 ) ≥ U (σ, δ, λ0 ) − ε.                                 (2)

                                                       17
    Next, let us now show that it is possible to choose δ such that it approaches 1 as λ0
approaches 0 or 1. For any δ, there exists T (δ) such that the expected sum of discounted
utilities past time T (δ) amounts to less than ε/2 and so the utility is captured in the first
T (δ) periods:15                                                          
                                           T (δ)
                                           X
                       U (σ, δ, λ0 ) ≥ E        δ t ut (σ(ht−1 , λ0 )) λ0  − ε/2.
                                                    t=1

Next, note that the expression
                                                                               
                                                 T (δ)
                                                 X
                                          E             δ t ut (σ 1 (ht−1 , λ0 ))
                                                 t=1


is continuous in λ0 including the extreme points of λ0 ∈ {0, 1} for any given δ. Note also
that σ 1 is the optimal strategy if λ0 = 1, since then the expected payoff in any given period
(independent of the history) is simply the probability that the interpretation is A times p
plus the probability that the interpretation is the interpretation is B times 1 − p. This is
maximized by setting the interpretation to A. Similarly if If λ0 = 0 the optimal strategy is
to interpret things as B in any given period. Thus, maximum likelihood storage rule, σ 1 ,
is optimal for λ0 ∈ {0, 1}. Given the continuity, it is within ε/2 optimal for any λ0 close
enough to 1 or 0. So, for any δ we can find λ0 close enough to 1 or 0 for which

                                         U (σ 1 , δ, λ0 ) ≥ U (σ, δ, λ0 ) − ε.            (3)

which completes the proof.

Proof of Proposition 2. We first state an auxiliary result, from Hoeffding (1963), that
is useful in proving Proposition 2.

Lemma 1 (Hoeffding’s inequality) If X1 , . . . , Xt are independent and ai ≤ Xi ≤ bi for
i = 1, 2, . . . , t, then for δ > 0,
                              t
                                                        !
                              X                                2 2
                                                                   Pt
                                        Xi − E(Xi ) ≥ t ≤ e−2t  / i=1 (bi −ai ) .
                                                   
                          P
                                  i=1


   Let n(λ) be the number of b interpreted signals minus the number of a interpreted signals




  15                               δT
       An upper bound is to set   1−δ   = ε/2.


                                                              18
needed to reach the frontier where λt = 1/2 starting from λ0 = λ, i.e.,
                                                     
                                                   λ
                                             log 1−λ 
                                                      
                                     n(λ) =       .
                                                   p
                                              log 1−p

    The b·c reflects starting from a prior below 1/2, and otherwise it would be rounded up.
    The process nt = n(λt ) is a random walk in the integers such that nt is increased by
1 every time there is an interpreted a signal and decreased by 1 every time there is an
interpreted b signal. The conditional laws given the states A and B are denoted by PA and
PB , respectively, and EA and EB are the corresponding expectations.

(a) First, note that if (1 − π)p + π(1 − γ) > 1/2 (which is rewritten as γ < 1/2−(1−p)(1−π)
                                                                                       π
                                                                                              ),
    then even if all of the unclear signals are incorrectly interpreted, the majority of signals
    will still match the true state. Therefore, if the true state is A, then the increments
    ∆nt = nt+1 − nt are positive in expectation, i.e., EA (∆nt ) > 0. Moreover, they have
    bounded first and second moments. It follows from the strong law of large numbers that
    (nt −EA (nt ))/t converges to zero PA -a.s., which implies that nt → ∞ PA -a.s. and λt → 1
    PA -a.s. The PB -a.s. convergence of λt to zero is proven in a similar same way.
    Note that this is automatically satisfied if π < (p − 1/2)/p for any γ, which establishes
    the first sentence of the proposition.

(b) Now suppose that (1−π)p+π(1−γ) < 1/2 and assume that the true state is B. We claim
    that PB assigns positive probability to the event λt → 1, which coincides with the event
    nt → ∞. First, we note that nt reaches any preset level with positive probability if t is
    large enough. Therefore, it is sufficient to prove the proposition for n0 large. Whenever
    nt is positive, it is more likely to increase than to decrease, i.e., PB (∆nt = 1) ≡ z > 1/2.
    As long as this is the case, EB (∆nt ) = 2z − 1 > 0 and Hoeffding’s inequality states that
    for any  > 0,
                                                                     2
                                PB nt − n0 ≤ (2z − 1 − )t ≤ e−t /2 .
                                                            

    Setting  = (2z − 1)/2 leads to the bound

                                                                         2
                 PB nt ≤ (z − /2)t ≤ PB nt − n0 ≤ (z − 1/2)t ≤ e−t(z−1/2) /2 .
                                                           


    When n0 is large, nt cannot immediately fall below (z − 1/2)t. More specifically, this is
    impossible for t ≤ bn0 /(z + 1/2)c. It follows that

                                                                              2
                                                                  X
                                                                    e−t(z−1/2) /2 .
                                                          
      PB ∀t : nt > (z − 1/2)t = 1 − PB ∃t : nt ≤ (z − 1/2)t ≥ 1 −
                                                                        t>bn0 /(z+1/2)c



                                               19
    The last expression is positive if n0 is large enough. This proves that
                                                              
                                  PB lim λt = 1 = PB lim nt = ∞ > 0.
                                        t→∞                         t→∞


    It can be shown in a similar way that PA assigns positive probability to the event λt → 0.



Proof of Proposition 3. In the limit, the belief process places a.s. weight 1 on the true
state of Nature when the γ = 1/2 rule is used, as shown in proposition 2 (and could also be
deduced from Levy’s 0-1 law and Martingale convergence of beliefs). Therefore, the belief
process remains eventually on the correct side of the frontier. Formally, under the γ = 1/2
rule, the random time
                           S = inf{t ≥ 0 : ∀s ≥ t : 1λt >1/2 = ω}

is Pω -a.s. finite for any ω ∈ {A, B}. Therefore, for any T ,

                               ∞
                                                                !                   ∞
                                                                                                            !
                               X                                                    X
  U (σ T , δ, λ) ≥ E   1S≤T          δ t ut (σ T (ht−1 , λt−1 ))    =E       1S≤T          δ t ut (σ F I (ω))
                               t=T                                                   t=T
                       ∞
                                              !                     ∞
                                                                                            !
                       X                                            X
               >E            δ t ut (σ F I (ω))   − /2 > E               δ t ut (σ F I (ω))    −  = U (σ F I , δ) − .
                       t=T                                          t=0


In the above equation, the first relation holds because some non-negative terms are dropped,
the second relation holds because the agent calls out the right state past time S, the third
relation holds for large enough T because P (S ≤ T ) → 1 as T → ∞, and the fourth relation
holds for δ close enough to 1 because then the first T stages do not matter relative to the
rest.

Proof of Proposition 4. The argument in the proof of proposition 2b shows that when
π < 1, then the values 0 and 1 occur with positive probability as the limit of the belief
process λt as t → ∞. It remains to show that λt → 1 with probability tending to one as
π → 1 if λ0 > 1/2. So let us assume λ0 > 1/2. Obviously, in the extreme case that π = 1,
all signals are interpreted as a and therefore, the probability that λt → 1 equals 1. This
probability depends continuously on the parameter π by Lemma 2.

Lemma 2 Under any strategy, the belief process Λ is continuous in the total variation norm
with respect to the parameters p and π.

Proof. It is equivalent to show the proposition for the point process nt defined in the proof
of Proposition 2 instead of the process λt . Let pk → p, π k → π, and let P k and P be the

                                                           20
corresponding laws of the process nt . Furthermore, let Ft be the sigma algebra generated
by n0 , . . . , nt and Ptk the restriction of P k to Ft . It follows directly from the Chapman-
Kolmogorov equations or from Jacod and Shiryaev (2003, corollary V.4.39a) applied to the
point process (nt − n0 + t)/2 that the total variation of the signed measure Ptk − Pt tends to
zero, i.e.,

   kPtk − Pt k = sup{|Ptk (φ) − Pt (φ)| : φ Ft -measurable function on Ω with |φ| ≤ 1} → 0.

The restriction that φ is Ft -measurable can be removed by an approximation argument: for
any Ft -measurable function φt , one has

          |P k (φ) − P (φ)| ≤ |P k (φ) − P k (φt )| + |P k (φt ) − P (φt )| + |P (φt ) − P (φ)|
                                                                                                  (4)
                            ≤ |P k (φ) − P k (φt )| + kP k − P k + |P (φt ) − P (φ)|.

Setting k large enough, kPtk − Pt k can be made smaller than /3. Then φt can be set equal
to the Ft -conditional expectation of φ under the measure (P k + P )/2. It follows that φt → φ
a.s. under P k and P . By the dominated convergence theorem, the first and third term in
the right-hand side of equation (4) are smaller than /3 when t is large enough. It follows
that (4) is arbitrarily small for large enough values of k. Thus P k converges to P in the total
variation norm.




                                                  21
                           Table 1: Summary of Belief Divergence Results
Lord, Ross, and Lep-        Experimental subjects were provided with evidence for and against
per (1979)                  the detterant effect of the death penalty. Subjects of all beliefs re-
                            port that the article matching their baseline is more convincing, and
                            students became more confident in their original position.

Darley   and    Gross       Subjects were asked to grade an identical essay after hearing the child
(1983)                      described as from a poor, inner-city neighborhood or a middle-class
                            suburban neighborhood. Subjects gave lower grades to the inner-
                            city author, despite no change in the text of the essay. Subjects who
                            viewed a video of the child answering quiz questions before grading
                            the test displayed even greater divergence.

Plous (1991)                Subjects with varying opinions on nuclear energy and deterrence
                            were provided with articles on the Three Mile Island disaster and
                            a narrowly-averted accidental missile launch. Subjects of all view-
                            points expressed increased confidence in their original viewpoints af-
                            ter reading the articles.

Munro    and    Ditto       Subjects with high and low levels of prejudice towards homosexuals
(1997)                      were presented with two fictional studies on the empirical prevalence
                            of a homosexual stereotype. Follow-up interviews revealed evidence
                            of both biased assimilation and attitude polarization.

Russo, Meloy,        and    Experimenters sequentially provided subjects with information on
Medvec (1998)               two fictional brands. In later stages, once participants have formed
                            preferences, neutral information causes subjects to identify more
                            strongly with their preferred brand.

McHoskey (2002)             Students were randomly selected to review either information sup-
                            porting the claim that Lee Harvey Oswald acted alone in assassi-
                            nating John F. Kennedy and or information pointing to a larger
                            conspiracy. Students with extreme opinions intensify their positions
                            when presenting with information supporting their beliefs and relax
                            their beliefs to a lesser degree when confronted with contradictory
                            evidence.

Kahan et al (2007)          Subjects were surveyed on their beliefs about the safety of nanotech-
                            nology after half were randomly provided with factual information
                            about risks and benefits. Those who were exposed to information
                            displayed greater polarization than those who were not.

Nyhan and Reifler           These studies detail a series of five experiments in which participants
(2010), Nyhan, Rei-         are asked to assess the validity of a false or misleading statement
fler, and Ubel (2013)       by a politician. In each case, the additional information leads the
                            most-committed members of the targeted subgroup to intensify their
                            misperceptions, rather than weakening them.


                                                 22
