                                 NBER WORKING PAPER SERIES




                   ULTRA HIGH FREQUENCY VOLATILITY ESTIMATION
                      WITH DEPENDENT MICROSTRUCTURE NOISE

                                           Yacine Aït-Sahalia
                                            Per A. Mykland
                                              Lan Zhang

                                         Working Paper 11380
                                 http://www.nber.org/papers/w11380


                       NATIONAL BUREAU OF ECONOMIC RESEARCH
                                1050 Massachusetts Avenue
                                  Cambridge, MA 02138
                                       May 2005




We are grateful to Joel Hasbrouck for comments and help with the sequencing of trades. Financial support
from the NSF under grants SBR-0350772 (Aït-Sahalia), DMS-0204639 (Mykland and Zhang) and the NIH
under grant RO1 AG023141-01 (Zhang) is also gratefully acknowledged. The views expressed herein are
those of the author(s) and do not necessarily reflect the views of the National Bureau of Economic Research.

©2005 by Yacine Aït-Sahalia, Per A. Mykland, and Lan Zhang. All rights reserved. Short sections of text,
not to exceed two paragraphs, may be quoted without explicit permission provided that full credit, including
© notice, is given to the source.
Ultra High Frequency Volatility Estimation with Dependent Microstructure Noise
Yacine Aït-Sahalia, Per A. Mykland, and Lan Zhang
NBER Working Paper No. 11380
May 2005
JEL No. G12, C22

                                            ABSTRACT

We analyze the impact of time series dependence in market microstructure noise on the properties
of estimators of the integrated volatility of an asset price based on data sampled at frequencies high
enough for that noise to be a dominant consideration. We show that combining two time scales for
that purpose will work even when the noise exhibits time series dependence, analyze in that context
a refinement of this approach based on multiple time scales, and compare empirically our different
estimators to the standard realized volatility.

Yacine Aït-Sahalia
Department of Economics
Fisher Hall
Princeton University
Princeton, NJ 08544-1021
and NBER
yacine@princeton.edu

Per A. Mykland
The University of Chicago
mykland@galton.uchicago.edu

Lan Zhang
Carnegie Mellon University
lzhang@stat.cmu.edu
1      Introduction
When studying financial data, the notion that noise plays an essential role is an accepted fact of life, whether at
the high frequency typical of transactions data or at the lower frequencies more commonly used in asset pricing.
That this is a central issue is perhaps best demonstrated by the fact that two recent presidential addresses
to the American Finance Association have been entitled “noise” (Black (1986)) and “frictions” (Stoll (2000))
respectively. So we work under the assumption that the observed log-price Y (either transaction or quoted) in
high frequency financial data is the unobservable eﬃcient log-price X plus some noise component         due to the
imperfections of the trading process,

                                                   Yt = Xt + t .                                              (1.1)

Since X is defined implicitly (as opposed to explicitly, such as the sum of expected discounted dividends for
instance) the maintained identifying assumption is that       is independent of the X process.
     We are interested in the implications of such a data generating process for the estimation of the volatility
of the eﬃcient log-price process

                                               dXt = µt dt + σt dWt                                           (1.2)

using discretely sampled data on the transaction price process at time intervals of length ∆. By ultra high
frequency, we mean that we are in a situation where the data available are such that ∆ will be measured
in seconds rather than minutes or hours. Under these circumstances, the drift is of course irrelevant, both
economically and statistically, and so we shall focus on functionals of the σt process and set µt = 0. It is
the case that transactions and quotes data series in finance are often observed at random time intervals (see
Aït-Sahalia and Mykland (2003) for inference under these circumstances) but, throughout this paper, we will
assume for simplicity that ∆ is constant. We make essentially no assumptions on the σt process: its driving
process can of course be correlated with the Brownian motion Wt in (1.2), and it need not even have continuous
sample paths.
     The noise term summarizes a diverse array of market microstructure eﬀects, which can be roughly divided
into three groups. First, represents the frictions inherent in the trading process: bid-ask bounces, discreteness
of price changes and rounding, trades occurring on diﬀerent markets or networks, etc. Second,             captures
informational eﬀects: diﬀerences in trade sizes or informational content of price changes, gradual response of
prices to a block trade, the strategic component of the order flow, inventory control eﬀects, etc. Third,
encompasses measurement or data recording errors such as prices entered as zero, misplaced decimal points,
etc., which are surprisingly prevalent in these types of data. As is clear from the laundry list of potential
sources of noise, the data generating process for        is likely to be quite involved. Therefore, robustness to
departures from any assumptions on       is desirable.
     If σt is modelled parametrically, we showed in Aït-Sahalia, Mykland, and Zhang (2005) that incorporating
    explicitly in the likelihood of the observed log-returns Y provides consistent and asymptotically normal
estimators of the parameters. But what distributional assumption to use for ? Surprisingly, we found that


                                                          1
misspecifying the marginal distribution of       has no adverse consequences.
    In the nonparametric case where σt is an unrestricted stochastic process, the object of interest is the
                                                                      RT
integrated volatility or quadratic variation of the process, hX, XiT = 0 σt2 dt, over a fixed interval T, typically
one day in empirical applications. This quantity can then be used to hedge a derivatives’ portfolio, forecast
                                                                                                        (all)
the next day’s integrated volatility, etc. Without noise, the realized volatility (RV) estimator [Y, Y ]T =
Pn                  2
  i=1 (Yti+1 − Yti ) provides an estimate of the quantity hX, XiT , and asymptotic theory would lead one to
                                                                                                               (all)
sample as often as possible, or use all the data available, hence the “all” superscript. The sum [Y, Y ]T
converges to the integral hX, XiT , with a known distribution, a result which dates back to Jacod (1994)
and Jacod and Protter (1998); see also e.g., Barndorﬀ-Nielsen and Shephard (2002) and Mykland and Zhang
(2002).
   In Aït-Sahalia, Mykland, and Zhang (2005) and Zhang, Mykland, and Aït-Sahalia (2002), we studied the
corresponding problem when a relatively simple type of market microstructure noise, iid, is present and we
refer to these two papers for a review of the literature to date. We showed there that the situation changes
radically in the presence of market microstructure noise. In particular, computing RV using all the data
available (say every second) leads to an estimate of the variance of the noise, not the quadratic variation that
                              (all)
one seeks to estimate: [Y, Y ]T       has bias 2nE[ 2 ], which is an order of magnitude larger than the object we
seek to estimate, hX, XiT . The divergence of the RV estimator as the number of observations n increases is
illustrated in Figure 1, which shows the behavior of the RV estimator as a function of the sampling interval
∆ = T /n : as predicted by our theory, the plot shows divergence proportional to 1/n. The RV estimator in
the figure is computed for an average of the 30 Dow Jones Industrial Average stocks, averaged again over the
last ten trading days in April 2004; the objective of the double averaging is to reduce the variability of the
estimator in order to display its bias.
                                                           £ ¤
    Equivalently, since our theory predicts that RV ≈ 2nE 2 asymptotically in n, we expect that ln RV ≈
      £ ¤
ln(2E 2 ) + ln n so that a regression of ln RV on ln n should have slope coeﬃcient close to 1. Figure 2 shows
the result: the estimated slope coeﬃcient is 1.02 and the null value of 1 is not rejected, with a t−statistic of
0.46. Note from the equation above that an estimate of E[ 2 ] can be constructed using the intercept in that
regression (more on that later).
   While a formal analysis of this phenomenon originated in our work cited above, the empirical message that
emerges from this has long been known: do not compute RV at too high a frequency. This in fact formed the
rationale for the recommendation in the literature to sample sparsely at some lower frequency. A sampling
interval ∆sparse is picked in the range from 5 to 30 minutes: see e.g., Andersen, Bollerslev, Diebold, and Labys
(2001), Barndorﬀ-Nielsen and Shephard (2002) and Gençay, Ballocchi, Dacorogna, Olsen, and Pictet (2002).
                                                                           (sparse)
We denote the RV estimator corresponding to ∆sparse = T /nsparse as [Y, Y ]T        .
   If one insists upon sampling sparsely, we then showed in our earlier papers how to determine the optimal
sparse frequency, instead of selecting it arbitrarily. But even if sampling sparsely at our optimally-determined
frequency, one is still throwing away a large amount of data. For example, if T = 1 NYSE day and transactions
occur every ∆ = 1 second, the original sample size is n = T /∆ = 23, 400. Sampling sparsely even at the highest
frequency used by empirical researchers (once every 5 minutes) entails throwing away 299 out of every 300



                                                          2
observations: the sample size used is only nsparse = 78. This violates one of the most basic principles of
statistics, and our objective when starting this research project was to propose a solution which made use of
the full data sample, despite the fact that ultra high frequency data can be extremely noisy.
    Our approach to estimating the volatility is to use Two Scales Realized Volatility (TSRV). By evaluating
the quadratic variation at two diﬀerent frequencies, averaging the results over the entire sampling, and taking
a suitable linear combination of the result at the two frequencies, one obtains a consistent and asymptotically
unbiased estimator of hX, XiT . We start by briefly reviewing the rationale behind the TSRV estimator in
Section 2.
    In our earlier paper, however, we made the assumption that the noise term was iid. In Section 3, we
document that dependence in the noise can be important in some empirical situations. So our main purpose
in the following will be to propose a version of the TSRV estimator which can deal with such serial dependence
in market microstructure noise.
    Just like the marginal distribution of the noise is likely to be unknown, its degree of dependence is also
likely to be unknown and so our approach will be nonparametric in nature. We develop the theory for our new,
serial-dependence-robust, TSRV estimator in Section 4. In a nutshell, we will continue combining two diﬀerent
time scales, but rather than starting with the fastest possible time scale as our starting point, one now needs
to be somewhat more subtle and adjust how fast the fast time scale is. Next, we analyze in Section 5 the
                                                                                                    (all)             (sparse)
impact of serial dependence in the noise on the distribution of the RV estimators, [Y, Y ]T                 and [Y, Y ]T         .
We then discuss in Section 6 a further refinement to this approach, called Multiple Scales Realized Volatility
(MSRV), which achieves further asymptotic eﬃciency gains over TSRV (see Zhang (2004)), and as we did for
TSRV and RV, we analyze the impact of serial dependence in the noise on that estimator. Finally, we provide
in Section 7 an empirical study of the TSRV and MSRV estimators, and compare them to RV. We examine
in particular the robustness of TSRV to the choice of the two time scales, contrast it with RV’s divergence as
sampling gets more frequent and with RV’s variability in empirical samples, and study the dependence of the
estimators on various ways of pre-processing the raw high frequency data. Section 8 concludes.


2     The TSRV Estimator with IID Noise
Before showing how to extend TSRV to account for serial dependence in market microstructure noise, we
first review the basic TSRV construction under iid noise. The TSRV estimator is based on subsampling,
averaging and bias-correction. The idea is to partition the original grid of observation times, G = {t0 , ..., tn }
into subsamples, G (k) , k = 1, ..., K where n/K → ∞ as n → ∞. For example, for G (1) start at the first
observation and take an observation every 5 minutes; for G (2) , start at the second observation and take an
observation every 5 minutes, etc. Then we average the estimators obtained on the subsamples. The idea is
                                                   (sparse)             (all)
that the benefit of sampling sparsely, as in [Y, Y ]T         vs. [Y, Y ]T      , can now be retained, while the variation
of the estimator can be lessened by the averaging and the use of the full data sample.




                                                          3
   Subsampling and averaging together gives rise to the estimator

                                                                        1 X
                                                                          K
                                                          (avg)                    (sparse,k)
                                                 [Y, Y ]T          =        [Y, Y ]T
                                                                        K
                                                                          k=1

                                                               (sparse,k)
constructed by averaging the estimators [Y, Y ]T                              obtained by sampling sparsely on each of the K grids
of average size n̄.
                         (avg)
   Unfortunately, [Y, Y ]T       remains a biased estimator of the quadratic variation hX, XiT of the true return
process, although its bias 2n̄E[ 2 ] now increases with the average size n̄ of the subsamples, instead of the full
                                                                                    (all)
sample size n as in 2nE[ 2 ]. But E[ 2 ] can be consistently approximated by [Y, Y ]T :

                                                          [  2] =
                                                                          1        (all)
                                                          E[                [Y, Y ]T .                                        (2.1)
                                                                         2n

Thus a bias-adjusted estimator for hX, XiT can be constructed as

                                      \
                                                 (tsrv)                   (avg)            n̄         (all)
                                     hX, XiT              =        [Y, Y ]           −        [Y, Y ]                         (2.2)
                                                                   | {zT }                 n | {zT }
                                                                slow time scale              fast time scale

and this is the TSRV estimator. Figure 3 summarizes this construction.
   If the number of subsamples is optimally selected as K ∗ = cn2/3 , then TSRV has the following distribution:
                                                                                                     Z
                   \
                        (tsrv)       L                              1           8     2 2        4T T 4
                  hX, XiT        ≈          hX, Xi             +          [       E[   ]   +   c          σt dt ]1/2 Ztotal   (2.3)
                                            | {z T}                n1/6        c2
                                                                               | {z }             3   0
                                         ob ject of interest                                   |      {z       }
                                                                              due to noise   due to discretization
                                                                              |               {z                 }
                                                                                      total variance

and the constant c can be set to minimize the total asymptotic variance above.
   Unlike all the previously considered ones, this estimator is now correctly centered, and to the best of our
knowledge is the first consistent estimator for hX, XiT in the presence of market microstructure noise. A small
sample refinement to hX,\  Xi can be constructed as follows
                                 T

                                                     (tsrv,adj)     ³   n̄ ´−1 \ (tsrv)
                                            \
                                           hX, XiT                 = 1−       hX, XiT   .                                     (2.4)
                                                                        n

The diﬀerence from the estimator in (2.2) is of order Op (n̄/n) = Op (K −1 ), and thus the two estimators behave
identically to the asymptotic order that we consider. The estimator (2.4), however, has the appeal of being
unbiased to higher order.
   Following our work, Barndorﬀ-Nielsen, Hansen, Lunde, and Shephard (2004) have shown that our TSRV
estimator can be viewed as a form of kernel based estimator. However, all kernel-based estimators are inconsis-
tent estimators of hX, XiT under the presence of market microstructure noise. When viewed as a kernel-based
estimator, TSRV owes its consistency to its automatic selection of end eﬀects which must be added “manually”
to a kernel estimator to make it match TSRV. Optimizing over the kernel weights leads to an estimator with
the same properties as MSRV in Section 6 below, although the optimal kernel weights will have to be found
numerically, whereas the optimal weights for MSRV will be explicit (see Zhang (2004)). With optimal weights,



                                                                          4
the rate of convergence can be improved from n−1/6 for TSRV to n−1/4 for MSRV as the cost of the higher
complexity involved in combining O(n1/2 ) time scales instead of just two as in (2.2). In the fully parametric
case we studied in Aït-Sahalia, Mykland, and Zhang (2005), we showed that when σt = σ is constant, the MLE
for σ2 converges for T fixed and ∆ → 0 at rate ∆1/4 /T 1/4 = n−1/4 (see equation (31) p. 369 in Aït-Sahalia,
Mykland, and Zhang (2005)). This establishes n−1/4 as the best possible asymptotic rate improvement over
(2.3).


3        Time Series Dependence in High Frequency Market Microstruc-
         ture Noise
We now turn to examining empirically whether there is a need to relax the assumption that the market
microstructure noise     is iid. In other words, is it the case that every time a new price is observed, one
observes it with an error that is independent of the previous one, no matter how close together those two
successive prices might be?


3.1      The Data
Our data consist of transactions and quotes from the NYSE’s TAQ database for the 30 Dow Jones Industrials
Average (DJIA) stocks, over the last ten trading days of April 2004 (April 19-23 and 26-30). To save space,
we will focus on four of the thirty stocks: 3M Inc. (trading symbol: MMM), American International Group
(trading symbol: AIG), Intel (trading symbol: INTC) and Microsoft (trading symbol: MSFT). Of these, the
first two are traded on the NYSE while the latter two are traded on the Nasdaq. Table 1 reports the basic
summary statistics on these four stocks’ transactions.
    In our earlier paper where we introduced the TSRV estimator, we assumed that microstructure noise was
iid. In that case, log-returns
                                                         Z   τi
                                        Yτi − Yτi−1 =             σt dWt +   τi   −   τi−1                    (3.1)
                                                          τi−1

                                                  R τi
follow an MA(1) process since the increments       τi−1
                                                          σt dWt are uncorrelated, ⊥ W and therefore, in the simple
case where σt is nonrandom (but possibly time varying),
                                                ⎧ Rτ              £ ¤
                                                ⎪
                                                ⎪
                                                     i
                                                       σ2 dt + 2E 2      if j = i
                     £¡          ¢¡          ¢¤ ⎨  τi−1 t
                                                             £ 2¤
                    E Yτj − Yτj−1 Yτi − Yτi−1 =         −E            if j = i + 1                            (3.2)
                                                ⎪
                                                ⎪
                                                ⎩
                                                            0         if j > i + 1

Under the simple iid noise assumption, log-returns are therefore (negatively) autocorrelated at the first order.
We will examine below whether this is compatible with what we observe in the data, but for now note that this
is consistent with the predictions of many simple reduced form market microstructure models. For instance,
in the Roll (1984) model,     t   = (s/2)Qt where s is the bid/ask spread and Qt , the order flow indicator, is a
binomial variable that takes the values +1 and −1 with equal probability, generating first order autocorrelation


                                                                  5
in returns. French and Roll (1986) proposed to adjust variance estimates to control for such autocorrelation
and Harris (1990) studied the resulting estimators. Zhou (1996) proposed a bias correcting approach based
on the first order autocovariances; see also Hansen and Lunde (2004).
     We now turn to confronting this model to the data. Figure 4 reports the autocorrelogram computed for
the 3M and AIG transactions, respectively. The plot show a good agreement with the prediction of the iid
noise model, namely the MA(1) structure in (3.2) for 3M and AIG.
     However, Figure 5 shows the corresponding result for Intel and Microsoft. It is clear that the MA(1) model,
and consequently the iid noise model, does not fit those data well for these two stocks. Both stocks were added
to the DJIA on November 1, 1999, becoming the first two companies traded on the Nasdaq to be included in
the DJIA.
     It is important to note however, that the diﬀerence between the two figures does not appear to be driven
by the diﬀerent market structures on the NYSE (a specialist market structure) compared to the Nasdaq (a
dealers’ market). In fact, the autocorrelogram pattern for the other 26 DJIA stocks is closer to that of Intel
and Microsoft, not that of 3M and AIG. Table 2 reports the results of a cross-sectional OLS regressions of
the autocorrelation coeﬃcients of order 2-5 on the average time between transactions used as a measure of
the liquidity of the stock, for the 30 DJIA stocks. These autocorrelation coeﬃcients of order greater than 1
would be zero if the noise term were serially uncorrelated, as in (3.2). The table shows that the lower the time
between successive transactions, the higher the observed autocorrelation in absolute value (the coeﬃcients
alternate signs because the autocorrelation coeﬃcients do, as in Figure 5). In other words, based on these
data, the more liquid the stock, the more likely we are to face departures from the iid assumption.


3.2      Example: A Simple Model to Capture the Noise Dependence
A simple model to capture the higher order dependence that we just documented in INTC and MSFT trades
is

                                                  ti   = Uti + Vti                                         (3.3)

where U is iid, V is AR(1) with first order coeﬃcient ρ, |ρ| < 1, and U ⊥ V. Under this model, we have
                                     ⎧ Rτ              £ ¤               £ ¤
                                     ⎪
                                     ⎪
                                          i
                                            σ2 dt + 2E U 2 + 2 (1 − ρ) E V 2    if j = i
          £¡          ¢¡          ¢¤ ⎨  τi−1 t
                                                  £ 2¤              £ 2¤
                                                                2
         E Yτj − Yτj−1 Yτi − Yτi−1 =         −E U − (1 − ρ) E V              if j = i + 1                  (3.4)
                                     ⎪
                                     ⎪                            £   ¤
                                     ⎩                        2
                                               −ρj−i−1 (1 − ρ) E V 2         if j > i + 1

     This model can easily be fitted to the data by the generalized method of moments. We use the first twenty
autocovariances of the log-returns as moment functions, in order to estimate the three parameters E[U 2 ],
E[V 2 ] and ρ. Their estimated values are 4.2 10−8 , 3.5 10−8 and −0.68 for INTC and 2.9 10−8 , 4.3 10−8 and
−0.70 for MSFT. Figure 6 shows the sample autocorrelogram and the corresponding one fitted by the model
above.
     Let us stress, however, that, while this simple model seems to capture fairly well the dependence in the
stock data that we have examined, our theory is not tied to this particular specification. of . It applies to


                                                          6
fairly general dependence structures, as can be seen from Assumption 1 below.


3.3    Transactions or Quotes?
The model (3.3) for the microstructure noise describes well a situation where the primary source of the noise
beyond order one consists of further bid-ask bounces. In such a situation, the fact that a transaction is on the
bid or ask side has little predictive power for the next transaction, or at least not enough to predict that two
successive transactions are on the same side with very high probability (although Choi, Salandro, and Shastri
(1988) have argued that serial correlation in the transaction type can be a component of the bid-ask spread,
and extended the model of Roll (1984) to allow for it).
    Figure 5 and the estimates just reported (ρ = −0.7) are evidence of negative autocorrelation at horizons of
up to about 15 transactions. In trying to assess the source of the higher order dependence in the log-returns, a
natural hypothesis is that this is due to the trade reversals: in transactions data and an orderly liquid market,
one might expect that in most cases successive transactions of the same sign (buy or sell orders) will not move
the price. The next recorded price move is then, more likely than not, going to be caused by a transaction
that occurs on the other side of the bid-ask spread, and so we observed these reversals when the data consist
of the transactions that lead to a price change.
    To examine this hypothesis, we turn to quotes data, also from the TAQ database. The results are reported
in Figure 7 and suggest that an important source for the AR(1) pattern with negative autocorrelation (the
term V in (3.3)) will be trade reversals. The remaining autocorrelation exhibited in the quotes data can also
be captured by model (3.3), but with a positive autocorrelation in the V term. This can capture eﬀects such
as the gradual adjustment of prices in response to a shock such as a large trade.


4     Extending the TSRV Estimator for Dependent Noise
In the previous section, we found that there are empirical situations (such as Intel or Microsoft transactions)
where the assumption of iid market microstructure noise could be problematic. We now proceed to suitably
extending the TSRV estimator to make it robust to departures from the iid noise assumption. The idea is to
somewhat slow the fast time scale to reduce the degree of dependence that is induced by the noise.


4.1    The Setup
As above, we let Y be the logarithm of the transaction price, which is observed at times 0 = t0 , t1 , ... ,
tn = T . We assume that at these times, Y is related to a latent true price X (also in logarithmic scale)
through equation (1.1). The latent price X is given by (1.2).

Assumption 1. We assume that the noise process         ti   is independent of the Xt process, and that it is (when
viewed as a process in index i) stationary and strong mixing with the mixing coeﬃcients decaying exponentially.
                                          4+κ
We also suppose that for some κ > 0, E          < ∞.




                                                        7
   Definitions of mixing concepts can be found e.g., in Hall and Heyde (1980), p. 132. Note that by Theorem
A.6 (p. 278) of Hall and Heyde (1980), there is a constant ρ < 1 so that, for all i,
                                          ¯                      ¯
                                          ¯Cov(     ti , ti+l )
                                                                 ¯        ≤ ρl Var( )                                            (4.1)

   For the moment, we focus on determining the integrated volatility of X for one time period [0, T ]. This is
also known as the continuous quadratic variation hX, Xi of X. In other words,
                                                                      Z   T
                                                   hX, XiT =                  σt2 dt.                                            (4.2)
                                                                      0

   Our volatility estimators can be described by considering subsamples of the total set of observations. A
realized volatility based on every j’th observation, and starting with observation number r, is given as

                                     (j,r)
                                                          X
                               [Y, Y ]T      =                            (Ytji+r − Ytj(i−1)+r )2 .
                                                 0≤j(i−1)≤n−r−j


Under most assumptions, this estimator violates the suﬃciency principle, whence we define the average lag j
realized volatility as
                                                                J−1
                                                 (J)          1 X
                                          [Y, Y ]T =                [Y, Y ](J,r)
                                                                           T
                                                              J r=0
                                                                n−J
                                                              1 X
                                                          =         (Yti+J − Yti )2                                              (4.3)
                                                              J i=0

   A generalization of TSRV can be defined for 1 ≤ J < K ≤ n as

                                 \
                                          (tsrv)            (K)   n̄K         (J )
                                hX, XiT            = [Y, Y ]T   −     [Y, Y ]      ,                                             (4.4)
                                                     | {z }       n̄J | {z T }
                                                     slow time scale                    fast time scale

thereby combining the two time scales J and K. Here n̄K = (n − K + 1)/K and similarly for n̄J .
   We will continue to call this estimator the TSRV estimator, noting that the estimator we proposed in
Zhang, Mykland, and Aït-Sahalia (2002) is the special case where J = 1 and K → ∞ as n → ∞. The original
TSRV produces a consistent estimator in the case where the                       ti   are iid. For the optimal choice K = O(n2/3 ),
                                                 (tsrv)
                                       \
                                      hX, XiT             − hX, XiT = Op (n−1/6 ).

The problem with which we are concerned here is that these assumptions on the noise                       ti   may be too restrictive.
We shall see that in the case where J is allowed to be larger than 1, the problem of dependence of the                            ti ’s

will be eliminated and the generalized TSRV estimator given in (4.4) will be consistent for suitable choices of
(J, K).


4.2       A Signal-Noise Decomposition
We have the following.



                                                                  8
Lemma 1. Under the assumptions above, let n → ∞, and let j = jn be any sequence. Then
                                         n−j
                                         X
                                               (Xti+j − Xti )(           ti+j      −   ti )   = Op (j 1/2 ).
                                         i=0

   The lemma is important because it gives rise to the sum of squares decomposition

                                               (J)                 (J)                 (J)
                                        [Y, Y ]T = [X, X]T + [ , ]T + Op (J −1/2 ).

Thus, if we look at linear combinations of the form (4.4), one obtains

                  \
                       (tsrv)                 n̄K                        n̄K
                 hX, XiT        = [X, X](K)
                                        T   −     [X, X](J)
                                                        T   + [ , ](K)
                                                                   T   −     [ , ](J)
                                                                                  T   + Op (K −1/2 ),
                                              n̄J                        n̄J
                                  |          {z           }   |         {z          }
                                           signal term                                          n oise term


so long as

                                                 1≤J ≤K                  and K = o(n),                                (4.5)

both of which will be assumed throughout.


4.3       Analysis of the Noise Term
It can be seen that when the ’s are independent E[noise term] = 0, so that the linear combination used in
(4.4) is exactly what is needed to remove the bias due to noise. To analyze the more general case, and to
obtain approximate distribution of the noise term, note that

                                                            1 X (J)
                                                               n
                                                                                       2 X
                                                                                         n−J
                                                (J )                          2
                                          [ , ]T =               c            ti   −               ti ti+J ,          (4.6)
                                                            J i=0 i                    J i=0

          (J)
where ci        = 2 for J ≤ i ≤ n − J, and = 1 for other i. By construction
                                                               X
                                                                   c(J
                                                                    i
                                                                       )
                                                                         = 2J n̄J ,                                   (4.7)
                                                               i

so that
                                                           µ                                                      ¶
                                                   1   2             n̄K 1
                           E |noise term| ≤ 2E       (n − K + 1)ρK +       (n − J + 1)ρJ
                                                  K                  n̄J J
                                              ³n            ´
                                          =O     (ρK + ρJ )                                                           (4.8)
                                               K

and, in the regular case where Cov(        t0 , tk )       = o(Cov(      t0 , tJ ))


                                                                    2    n
                                      E [noise term] = 2E                  Cov(         t0 , tJ )(1    + o(1)).
                                                                         K

If J → ∞ at even a quite slow rate when n → ∞, the bias is negligible. Also, in the case of m-dependent s,
the bias becomes zero for finite J. We obtain:



                                                                          9
Proposition 1. Under assumption (4.13) below,

                                  K                                  L
                                  1/2
                                      (noise term − E [noise term]) −→ ξZno is e                                       (4.9)
                                 n

as n → ∞, where Zno ise is standard normal. Further, in the case where both J and K go to infinity with n,
we have that ξ 2 = ξ∞
                    2
                      , where
                                                                       ∞
                                                                       X
                                       2
                                      ξ∞ = 16 Var( )2 + 32                    Cov(            2
                                                                                     t0 , ti ) .                      (4.10)
                                                                       i=1

In the case where J does not go to infinity (the m-dependent case, say), then
                                     ∞
                                     X                                         ∞
                                                                               X
                       ξ 2 = ξ∞
                              2
                                +8          Cov(   ti−J ,
                                                                  2
                                                            ti+J ) + 8               Cum(    t0 , ti , tJ , ti+J ),   (4.11)
                                     i=−∞                                     i=−∞

                                                                      2
   Note that even when J → ∞, one may be better oﬀ using (4.11) than ξ∞ since the former is closer to the
small sample variance, and since J → ∞ quite slowly. (By contrast, K → ∞ much more quickly, as we shall
see).


4.4     Analysis of the Signal Term
As for the “signal term”, we obtain that

                                                            (K)
                                                   [X, X]T        → hX, XiT                                           (4.12)

                                                                               \
in probability as n → ∞, provided K = o(n). Obviously, for the signal term in hX, XiT − hX, XiT to be
scalable to be consistent (see equation (4.17) below), we need

                                                                  J
                                                     lim sup        < 1,                                              (4.13)
                                                       n→∞        K

which is easily satisfied. In fact, as we shall see, one would normally take

                                                                  J
                                                     lim sup        = 0.                                              (4.14)
                                                       n→∞        K

   Specifically, we have in the case of dependent noise:

Proposition 2. Under (4.5),
              µ       µ      ¶¶−1/2 µ                               ¶    √
                  K       J3               (K) n̄K       (J)          L
                       1+2 3         [X, X]T −     [X, X]T − hX, XiT −→ η T Zd iscrete ,                              (4.15)
                  n       K                    n̄J

where, Zdiscrete is standard normal, and where in general, η2 is given as the limit in Theorem 3 in Zhang,
Mykland, and Aït-Sahalia (2002) (i.e., the discretization variance η2 has the same expression as when the
noise is iid). In the special case where observations are equidistant,
                                                                  Z    T
                                                             4
                                                     η2 =                  σt4 dt.                                    (4.16)
                                                             3     0



                                                                  10
    The convergence in law is stable (see Chapter 3 of Hall and Heyde (1980)), the most important consequence
of which is that Zdiscrete is independent of η.


4.5     The Combined Estimator
Consider the adjusted estimator
                                                                        µ            ¶−1
                                                  \
                                                       (tsrv,ad j)        n̄K               \
                                                                                                    (tsrv)
                                                 hX, XiT             = 1−                  hX, XiT                                            (4.17)
                                                                          n̄J

In the iid case of Zhang, Mykland, and Aït-Sahalia (2002), this adjustment was introduced from small sample
considerations (Section 4.2). Here, we also see that in the case where (4.13) is satisfied but (4.14) is not, this
adjustment is needed for consistency. In the following, we analyze this estimator, and for the case when (4.14)
                                                        (tsrv)
                                                  \
holds, the same analysis applies to the original hX, Xi        .             T

    We obtain from equations (4.9) and (4.15) that
                           Ã                                        µ µ      ¶¶1/2               !
               (tsrv,adj)
                               2 n                   n1/2            K    J3        √
         \
        hX, XiT           = 2E     Cov( t0 , tJ ) +       ξZnoise +    1+2 3       η T Zdiscrete
                                 K                    K              n    K
                           µ       ¶−1
                               n̄K
                          × 1−         (1 + op (1)) ,                                              (4.18)
                               n̄J

where Zn oise and Zdiscrete are asymptotically standard normal, and asymptotically independent.
    It is easy to see that the optimal trade-oﬀ between the two variance terms results in a choice of K = O(n2/3 ).
The worst thing that can then happen to the bias term is then that this is of the order of (n/K)ρJ = n1/3 ρJ .
Thus the bias is of small order relative to the variance provided one chooses n1/3 ρJ = o(n−1/6 ), i.e., ρJ =
o(n−1/2 ). Thus, one can safely assume that J/K ∼ 0 (i.e., (4.14)), and it follows that
                                                                                             (tsrv,ad j)
                                                         \
Proposition 3. The asymptotic behavior of the estimator hX, XiT                                            is given by
                                Ã                                                        µ       ¶1/2                    !
              (tsrv,ad j)         ¡    ¢ n                        n1/2                       K             √
     \
    hX, XiT                 =    2 E 2     Cov(       t0 , tJ ) +      ξZn oise +                       η T Zd iscrete       (1 + op (1)) .   (4.19)
                                         K                         K                         n

    Thus the optimal K is as given above, and one chooses, ultimately, J so that

                                                           Cov(      t0 , tJ )   = o(n−1/2 ).                                                 (4.20)

Obviously, when             is m-dependent, one can simply choose J = m + 1.


4.6     A Further Adjustment to the TSRV Estimator
                                                                 (K)
It should be noted that when K is large, [X, X]T                        may be a slight underestimate of hX, XiT . To consider the
                                                                                                               (K)
issue, if   σt2   is constant,      σt2   = σ , one gets that hX, XiT = σ 2 T , whereas [X, X]T
                                             2
                                                                                                                     ≈ σ2 T (n − K + 1)/n (the




                                                                            11
approximation here is loose, but, for example, it is an equality in expectation when σt2 is constant). Thus
                                                           µ                         ¶
                                    n̄K                      n − K + 1 n̄K n − J + 1
                       [X, X](K)
                             T   −      [X, X]
                                               (J)
                                               T   ≈ σ 2
                                                         T               −
                                    n̄J                           n        n̄J  n
                                                           (K −  J)n̄K
                                                   = σ2 T              .                                 (4.21)
                                                                n

A further modification of our estimator is thus the area adjusted quantity

                                               \
                                                     (tsrv,aa)            n       \
                                                                                       (tsrv)
                                              hX, XiT             =              hX, XiT      .                                                 (4.22)
                                                                      (K − J)n̄K

Since, by (4.5),
                                                                           µ                 ¶−1
                                                         n                        n̄K
                                                                ∼              1−                                                               (4.23)
                                                     (K − J)n̄K                   n̄J

we have:
                                                 (tsrv,a a)                                                         (tsrv,ad j)
                              \
Proposition 4. The estimator hX, XiT                                                       \
                                                              has the same asymptotics as hX, XiT                                 given in Proposition
3.

     In conclusion, the TSRV estimator that is robust to serial dependence in the noise, behaves as follows:
                                                                                                        Z
                  \
                             (tsrv,aa)    L                           1             1 2            4T T 4
                 hX, XiT                  ≈    hX, XiT        +            [           ξ      + c             σ dt ]1/2 Ztotal                  (4.24)
                                                                  n1/6             c 2
                                                                                   | {z }           3 0 t
                                                                                                 |        {z       }
                                                                               due to   noise  du e to d iscretization
                                                                               |                {z                   }
                                                                                            total varian ce

                                                 (tsrv,aa)                 (tsrv,ad j)
                                 \
whether it is taken in the form hX, XiT                           \
                                                              or hX, XiT                 . Here K ∼ cn2/3 , and ξ is given by (4.10), or,
more generally, (4.11).


5      RV Under Serial Dependence in the Noise
We now turn to an analysis of the standard RV estimator when the noise is serially dependent. First, we
have than sparse sampling at a given nsparse results in the same asymptotic distribution as when the noise is
serially uncorrelated. Second, however, we find that dependence in the noise impacts the asymptotic variance
(but not the bias) of the RV estimator when all the data (all n) are used. Specifically:
                                                                      (sparse)
Proposition 5. The traditional RV estimator, [Y, Y ]T                              , computed at a sparse sampling frequency ∆spa rse =
T /nspa rse , has the following behavior:
                                                                                                      Z T
                 (spa rse)      L                                                             2T
           [Y, Y ]T             ≈        hX, XiT +     2ns pa rs e E 2 + [4nsparse E 4 +                   σ 4 dt ]1/2 Ztotal .                  (5.1)
                                                       |     {z      }    |     {z      }  nsparse 0 t
                                                     bias du e to n oise   due to no ise  |          {z          }
                                                                                          du e to d iscretiz atio n
                                                                          |               {z                      }
                                                                                                    to ta l va ria nce

which is unaﬀected by serial dependence in the noise.


                                                                          12
   The reason why this last expression is as in the iid case is as follows. Essentially, the asymptotic variance
  P
of i ti ti+J behaves as if the quantities were uncorrelated if J goes to infinity, and so when we have nsp arse =
                                                                                                           (sparse)
n/J go to infinity with n eﬀectively the log-returns involved in [Y, Y ]T      are separated by enough time
                                                            P     ¡          £       ¤¢
interval for the dependence in not not to matter. Then i ti ti+J − E ti+J |Fi is a martingale, with
          P     h  ¡         £       ¤¢2 i                                   £ ¤2
variance i E 2ti ti+J − E ti+J |Fi         , which is approximately nsparse E 2 under exponential mixing,
                          P    ¡            £        ¤¢
while the remainder term i ti ti+J − E ti+J |Fi becomes negligible under exponential mixing.
                                                                                           (all)
    By contrast, when all the observations are used, as in [Y, Y ]T                                , the asymptotic variance of RV is influenced
                                                                                   (all)
by the dependence of the noise. The asymptotics of [Y, Y                          ]T        are, to first order, like that of [ , ]. The mean
of the latter is 2nE 2 . As for the asymptotic variance, from standard formulas for mixing sums, we have
                                        ∙     µ              ¶¸
                                          √     [ , ]
                           Ω∞ = AVAR        n         − 2E 2
                                                  n
                                                       ∞
                                                       X
                                = Var(( 1 − 0 )2 ) + 2    Cov(( 1 − 0 )2 , ( i+1 − i )2 ).              (5.2)
                                                                          i=1

This gives:
                                                                            l)
Proposition 6. By contrast, the RV estimator using all the data, [Y, Y ](al
                                                                        T      , computed at the highest sampling
frequency ∆ = T /n, has the following behavior:
                                                                                                                    Z     T
                           (a ll)   L                                2                               2T
              [Y, Y       ]T        ≈     hX, XiT +           |2nE{z }        + [ 4n Ω∞ +                             σt4 dt ]1/2 Zto ta l .                  (5.3)
                                                                                    | {z }            n               0
                                                      bia s    d ue to no ise    d ue to no is e    |              {z      }
                                                                                                 d ue to         discretization
                                                                                 |                 {z                         }
                                                                                                     tota l va rian ce

                      4
where Ω∞ = E               when the noise is iid; otherwise, dependency in                           gives rise to Ω∞ in (5.2).

    An alternate expression for Ω∞ can be obtained by noting that

                             2
      Cov((   1   −       0 ) , ( i+1   − i )2 ) = 2 Cov(   1   −   0 , i+1     − i )2 + Cum(          1   −   0, 1       −     0 , i+1   − i,   i+1   − i)

(and similarly for the variance).


6     MSRV: Multiple Scales Realized Volatility
We have seen that TSRV provides the first consistent and asymptotic (mixed) normal estimator of the quadratic
variation hX, XiT , that it can be made to work even if market microstructure noise is serially dependent, and
that it has the rate of convergence n−1/6 . At the cost of higher complexity, it is possible to generalize TSRV to
multiple time scales, by averaging not on two time scales but on multiple time scales. The resulting estimator,
multiple scale realized volatility (MSRV), has the form of
                                        (msrv)                  XM
                                \
                               hX, XiT           =                        ai [Y, Y ]T(Ki )             +        2             d2
                                                                                                                              E       .                       (6.1)
                                                                    i=1                                                       |{z}
                                                                |          {z           }                        fast time scale
                                                      weighted sum of M slow tim e scales


      d2 is given as before in (2.1).
where E

                                                                           13
                                                          (m srv)
                                         \
   For suitably selected weights ai ’s, hX, XiT                     converges to the true hX, XiT at rate n−1/4 . In what
follows, we recall the construction of the MSRV estimator in the iid case (see Zhang (2004)), before making
it robust to time series dependence in market microstructure noise of the same type we assumed in the above
analysis of TSRV (see Assumption 1).


6.1     A Signal-Noise Decomposition
To describe the selection of the weights ai ’s, we start with a special case, where we restrict attention to weights
that satisfy the two conditions
                                                            XM
                                                                         ai = 1,                                                   (6.2)
                                                                   i=1


                                                           XM            ai
                                                                            = 0.                                                   (6.3)
                                                               i=1       Ki

Then under (6.3), we have the decomposition

                          (msrv)
                                       M
                                       X                   (Ki )
                                                                             M
                                                                             X                              M
                                                                                                            X             (Ki )
                    \
                   hX, XiT         =          ai [X, X]T            +            ai Un,Ki         + 2              ai [X, ]T
                                       |i=1      {z           }              |i=1 {z          }            | i=1    {z         }
                                                sign al                              n oise           signal-n oise interaction
                                       M
                                       X
                                                                    2
                                   +          ai En,Ki + 2E                  +       Op (n−1/2 ),                                  (6.4)
                                       i=1
                                       |            {z              }
                                           end points of noise


where

                                                              2 X
                                                                 n
                                                  Un,K     =−                    ti ti−K ,
                                                              K i=K

and

                                                          1 X
                                                            K−1                          n
                                                                                         X
                                                                        2        1                  2
                                       En,K = −                         tj   −                      tj .
                                                          K j=0                  K   j=n−K+1


Condition (6.2) ensures that the first term in (6.4) will be asymptotically unbiased for hX, XiT .


6.2     Analysis of the Noise Term
Consider the noise term in (6.4)

                                                                   M
                                                                   X
                                                          ζ=             ai Un,Ki                                                  (6.5)
                                                                   i=1

Since Un,Ki and Un,Kl are uncorrelated zero-mean martingales, under conditions (6.2)-(6.3),

                                                                                        2
                                                      Var(ζ) ≈ γ2 n(E 2 ) ,                                                        (6.6)


                                                                     14
for Ki << n, where
                                                                M µ
                                                                X      ¶2
                                                         2          ai
                                                     γ =4                              .
                                                                             Ki
                                                                 i=1

   Subject to conditions (6.2)-(6.3), one can find the optimal weights ai , i = 1, ..., M, so that (6.6) is minimized.
In the special case where Ki = i, the optimal weight a∗i is given by
                                                    ¡                ¢
                                          ∗       i Mi − 12 − 2M   1
                                         ai = 12 2      ¡         ¢ .
                                                M        1 − M1 2

And the resulting variance of the noise term is minimized at

                                                                 48n          2
                                               Var∗ (ζ) =          2
                                                                        (E 2 )                                     (6.7)
                                                               M(M − 1)

6.3    Analysis of the Signal Term
The signal term in (6.4) is
                                                              M
                                                              X
                                                     φ=             ai [X, X](i)
                                                                             T .                                   (6.8)
                                                              i=1

                                                   (J)
Let the lag J discretization error be [X, X]T − hX, XiT . Unlike the noise term, the signals at diﬀerent lags
are highly correlated. In particular, the covariance between the lag J and lag K discretization error is,
                                                          µ                  ¶
                                      T                 2      min(J, K) + 1
                            ΥJ,K = (min(J, K) − 1)          3−                 η2                         (6.9)
                                      n                 3        max(J, K)

where, when the ti are equidistant, and under regular allocation of points to subgrids,
                                                                    Z    T
                                                         η2 = 2              σt4 dt.                             (6.10)
                                                                     0

   It is straightforward from (6.9) that

                                                                      −1/2
                                              φ − hX, XiT = Op ((n/M )     ).                                    (6.11)
                       PM            (Ki )
The interaction term    i=1 ai [X,           ]T and the end points of noise are of order M −1/2 . Hence, after balancing
the terms in (6.4), one obtains the optimal M is

                                                             M = O(n1/2 ).                                       (6.12)

   Therefore, the rate of convergence for the overall error for MSRV is
                                               (msrv)
                                      \
                                     hX, XiT             −     hX, XiT            = Op (n−1/4 ),                 (6.13)

which is an improvement over TSRV’s
                                                (tsrv)
                                      \
                                     hX, XiT             −     hX, XiT            = Op (n−1/6 ),                 (6.14)


                                                                    15
at the cost of course of the greater complexity of MSRV over TSRV. As discussed above, from our earlier
analysis of the parametric case, the rate n−1/4 is optimal.


6.4      Noise-Optimal Weights
Consider now the larger class of weights
                                                         µ ¶            µ ¶
                                                     i    i     1 i 0 i
                                           ai =        h     −        h     ,                                                (6.15)
                                                    M2    M    2M 2 M    M

where h is a continuously diﬀerentiable real-value function with derivative h0 , and satisfying the following two
conditions:
                                                           Z    1
                                                                    xh(x)dx = 1,                                             (6.16)
                                                            0


                                                            Z      1
                                                                       h(x)dx = 0.                                           (6.17)
                                                               0

This class of weights encompassed the earlier class (6.2)-(6.3).
   When the weights are chosen optimally, h takes the form
                                                      µ      ¶
                                                           1
                                          h∗ (x) = 12 x −                                                                    (6.18)
                                                           2

and the general optimal weights are then
                                                               µ         ¶                 µ        ¶
                                                   i                i           i              i
                                           a∗i   = 2 h∗                      −      h∗0                  .                   (6.19)
                                                  M                 M          2M 3            M
                                                                                        (msrv)
                                                           \
Using the optimal choice h∗ , the asymptotic variance for hX, XiT                                   is

                                                                2                   52
                          Υ
                         |{z}          = 48c−3 (E 2 )                    +             cT η2                                 (6.20)
                                         |   {z      }                              35
                                                                                    | {z }
                     total variance              due to noise
                                                                               d ue to discretization
                                                   12 −1                                   48 −1 2
                                       +              c Var( 2 )               +              c E hX, XiT
                                                   |5   {z    }                            |5    {z     }
                                                 due to noise endp oints           d ue to interaction b etween X and


where c is the proportionality constant in M = cn1/2 .
   To summarize, when computed the optimal weights (6.19), the MSRV estimator has the following distrib-
ution:
                                      (msrv)      L                                    1       h             i1/2
                            \
                           hX, XiT                ≈     hX, XiT               +                     Υ               Ztotal   (6.21)
                                                                                     n1/4          |{z}
                                                                                            total variance

when using the optimal weights (6.18). We can then select the value of the constant c to minimize the
asymptotic variance in the expression (6.21). Note finally that this class of weights is optimal for the purpose
of minimizing the part of the asymptotic distribution that is due to the presence of the noise. There is room


                                                                         16
for achieving further small asymptotic gains by minimizing over the full asymptotic variance (only at the level
of the constant and not the rate, which is already optimal here) but at the cost of even greater complexity
resulting in the loss of the simple, explicit, selection rule (6.19) for the weights.


6.5      The AVAR of MSRV When the Noise is Serially Dependent
We now study the MSRV estimator under the (dependence) Assumption 1 from Section 4.1. The only extra
bias (due to dependency) in equation (6.4) comes from the Un,Ki . Under our mixing assumption,
                                                             n
                                                             X
                                                    2                     2        ρKi
                              |E[Un,Ki ]|       ≤     Var( )   ρi ≤         Var( )
                                                    K                     K        1−ρ
                                                             j=Ki


Thus, when the ai follow (6.15), the absolute value of the extra bias in (6.4) becomes
                   ¯ "M            #¯              ¯      ¯
                   ¯   X            ¯      XM
                                                2 ¯¯ i ¯¯ ρi
                   ¯                ¯
                   ¯E       ai Un,i ¯ ≤          2 ¯
                                                     h( )¯        + corresponding h0 term
                   ¯                ¯          M       M    1 − ρ
                        i=1                i=1

                                              = O(M −1 )                                                            (6.22)

To the extent that the MSRV estimator converges at the rate Op (M −1/2 ), the bias induced by the dependence
of the ’s is therefore irrelevant asymptotically.
    An inspection of the terms in (6.4) shows that the rate of convergence does, indeed, remain of order
Op (M −1/2 ) = Op (n−1/4 ) under Assumption 1. As in the TSRV case, however, the asymptotic (random) vari-
ance now changes due to the dependence of the ’s. To compute that variance when the market microstructure
noise is serially dependent, note first that the four terms in (6.4) are asymptotically independent. This is by
the same methods as we use in the following. Also, the behavior of the signal term is, obviously, unchanged.
    We compute the covariances of the individual terms, and obtain:
                                                                            (m s rv)
                                                            \
Proposition 7. The overall (random) asymptotic variance of hX, XiT                     − hX, XiT is given, in the presence
of (possibly) serially correlated noise with autocorrelation function γ(l) = Cov( 0 ,               tl ),   by:
       Z 1 Z x
 4
M T η2    dx    h(y)h(x)y 2 (3x − y)dy
 3      0     0

   1 XX
       M M                         XK
                J    K
+ 4          h( )h( ) hX, XiT          (γ(l) + γ(l + J − K) − γ(l − K) − γ(l + J))(min(l + J, K) − max(0, l))
 M J=1 K=1 M         M
                                             l=−J

    4
           M
         M X
         X              J    K
                                    n−J
                                    X
+                  h(     )h( )              (min(n, n + l) − max(J + l, K) + 1)+ Cov(        t0 t−J , tl tl−K )    (6.23)
    M4   J=1 K=1
                        M    M
                                  l=−(n−K)
         M M                        K−1
     2 XX        J   K              X
+              h( )h( )                      (min(J + l, K) − max(0, l) + 1)+ Cov(     2    2
                                                                                       t0 , tl ).
    M 4 J=1 K=1 M    M
                                  l=−(J−1)


Under our mixing assumptions, the expression (6.23) is of order Op (n−1/2 ) as n → ∞, when M = O(n1/2 ).




                                                             17
    For example, in the special case of model (3.3), we have
                                      ⎧    £ 2¤               £ 2¤
                                      ⎪
                                      ⎪ 2E  U   + 2 (1 − ρ) E  V    if l = 0
                                      ⎨     £ 2¤              £ 2¤
                                                          2
                               γ(l) =   −E U − (1 − ρ) E V         if |l| = 1                                  (6.24)
                                      ⎪
                                      ⎪
                                      ⎩ −ρ|l|−1 (1 − ρ)2 E £V 2 ¤  if |l| > 1

to be inserted in (6.23). It can also be noted that, if instead
                                               ⎧    £ 2¤
                                               ⎪
                                               ⎪ 2E  U    if l = 0
                                               ⎨    £ 2¤
                                        γ(l) =   −E U    if |l| = 1                                            (6.25)
                                               ⎪
                                               ⎪
                                               ⎩    0    if |l| > 1

then the expression (6.23) reduces to the asymptotic variance of MSRV in the iid case, that is Υ in (6.20).
    We conclude our analysis of MSRV with two remarks:

Remark 1. Recall that for TSRV we replaced (2.2) with (4.4), thereby “jumping” to frequencies (J, K) over
the very fastest one (1, K) at which the serial dependence in the noise manifests itself. By letting both J
and K go to infinity with n, we were eﬀectively able to eliminate the serial dependence within each subgrid.
However, the asymptotic variance of TSRV is aﬀected by the serial dependence across subgrids coming from
                                                                  (K)
the averaging over RV from diﬀerent subgrids in both [Y, Y ]T           and [Y, Y ](J)
                                                                                   T   in (4.4), hence the asymptotic
variance in Proposition 3, also in (4.24), which is diﬀerent than in the iid noise case.

Remark 2. The asymptotic distribution of MSRV is also aﬀected by the dependence of the noise. Unlike the
TSRV case, there is no benefit to adjusting the MSRV estimator in the presence of serial dependence in the
noise. This is because the weights a∗i in (6.19) already assign most of the mass on the interval [1, M ] with
M = O(n1/2 ) to subintervals of the form [cM, M] where c is a positive constant. Therefore, the very fastest
frequencies of observations (those close to m = 1) already play a small role in MSRV even under iid noise.


7     Empirical Analysis
With these theoretical results in hand, we now turn to a comparison of the empirical performance of the RV,
TSRV and MSRV estimators, study the impact of the selection of the fast and slow time scales on the TSRV
estimators and the improvement due to MSRV relative to TSRV in the context of transactions data for Intel
and Microsoft in the last ten days of April 2004.


7.1    Comparison of the RV and TSRV Estimators
In our empirical analysis of the diﬀerent estimators, we start by comparing our TSRV estimator to the
traditional RV estimator. In particular, we establish that TSRV solves the two main problems associated with
RV, namely the divergence of RV as the sampling interval gets small and the variability of RV. The comparison
is reported for Intel in Figure 8 and for Microsoft in Figure 9, where we compare RV computed at diﬀerent
sampling frequencies with TSRV.


                                                       18
   Besides the well-known divergence of RV as ∆ → 0, the two figures also demonstrate the large diﬀerence in
variability of both estimates. Without the benefits of the double averaging in Figure 1, what these two series
of plots show is that computing RV at, say, 4mn as opposed to 5mn or 6mn can result in substantially diﬀerent
daily estimates. And the computation of day-by-day estimates is how RV is actually used. In existing empirical
applications, RV has typically been employed in the empirical literature at an arbitrary sparse frequency: in
light of the variability of RV as a function of the sparse sampling interval ∆sparse , whatever particular choice
is made can matter.


7.2    Robustness of TSRV to the Choice of Slow Time Scale
Both RV and TSRV require that the econometrician make a choice. In RV, one needs to select the sparse
sampling frequency at which to compute the estimator. In TSRV, one needs to select the number of subgrids
K over which to average the slow time scales sum of squares. In Zhang, Mykland, and Aït-Sahalia (2002),
we showed how to compute an optimal value K ∗ for the slow time scale parameter K when the noise term
is assumed to be iid, but in practical application it would be beneficial to be able to dispense with that
computation. For that, we would need to establish that the TSRV estimator is empirically robust to departures
from the optimal K ∗ . So, we now examine and compare the robustness of the two estimators to the selection
of their respective free parameter.
   The left panels in Figure 10 show RV, computed for Intel and Microsoft, as an average of the RV values
over the last ten days in April 2004 for diﬀerent sparse sampling frequency (which is the choice parameter
for RV). The right panels report the robustness of TSRV to the choice of K. The right panels show that the
estimator is numerically very robust to a large range of choices of K. In other words, the value of the TSRV
estimator is largely unaﬀected by choice of K within a reasonable range.


7.3    Robustness of TSRV to the Choice of Slow and Fast Time Scales
When the noise is serially dependent, the TSRV estimator defined in (4.4) depends on the choice of both the
slow time scale (K ) and the fast time scale (J). We find that the time-dependent TSRV is quite robust to the
choice of (J, K). Figure 11 shows that the value of the estimator is essentially identical within the reasonable
range of values considered.


7.4    The Improvement in MSRV over TSRV
It is possible to improve upon TSRV by considering MSRV. Both are consistent estimators of hX, XiT , but
MSRV has the faster convergence rate n−1/4 vs. n−1/6 for TSRV. The trade-oﬀ involves the additional
computational burden, since the number of slow time scales to be computed for MSRV is M = O(n1/2 ) and
n can be large in empirical applications: for instance n = 23, 400 for a stock that trades on average once a
second for a full day.
   We now examine the diﬀerence between TSRV and MSRV in the context of our empirical application.
Figure 12 shows that both methods produce close .estimates, especially when compared to the diﬀerences


                                                       19
exhibited earlier between RV and TSRV. As in the case of TSRV shown in 11, the MSRV estimator is not
sensitive to the specific choice of M within a reasonable range.


7.5    Robustness to Data Cleaning Procedures
One aspect that is sometimes briefly mentioned, but often not emphasized, in empirical papers using high
frequency financial data is the fact that the raw data is typically pre-processed to eliminate data errors,
outliers, etc. In addition, empirical applications of RV can involve pre-filtering of the data of various types,
but we focus here on the impact of data cleaning procedures that typically take place before any actual RV
computation is performed.
    It turns out that the impact of the specific data-cleaning procedures used to pre-process the raw data can
have a large impact on RV estimators. We illustrate this eﬀect by considering diﬀerent cutoﬀs to determine
which outliers to eliminate before calculating the RV estimator. First, we eliminate the obvious data errors
(such as a transaction price reported as zero, transaction times that are out of order, etc.).
    Second, we seek to eliminate outliers of various sizes. This is where things get trickier. For that purpose,
an outlier is a “bounceback”: a log-return from one transaction to the next that is both greater in magnitude
than an arbitrary cutoﬀ, and is followed immediately by a log-return of the same magnitude but of the opposite
sign, so that the price returns to its starting level before that particular transaction. Certainly, we do not
expect such large “roundtrips” to represent meaningful transactions. The question is how large is large, and
so we are led to study the dependence of the RV and TSRV estimators on three diﬀerent cutoﬀs that could
conceivably be adopted, 0.1% and 1% respectively in log-returns terms, and no cutoﬀ (no raw bounceback
return is larger than 2% in our sample, so that any cutoﬀ larger than this would make no diﬀerence). The
analysis reported above is all based on the intermediary cutoﬀ of 1%.
    The left panels in Figure 13 show the large impact of the cutoﬀ on the RV estimator. As shown in the
right panel, where all three curves are close together, TSRV is much less sensitive to the specific cutoﬀ used.
This is due to the structure of TSRV as a diﬀerence of two estimators: large returns in the data are part of
the slow time scale calculation, but then subtracted out in the fast time scale one.
    Since the cutoﬀ level is essentially arbitrary, we can view such outliers as a form of market microstructure
noise, and the robustness of TSRV to diﬀerent ways of pre-processing the data is therefore a desirable property.


8     Conclusions
We documented that there are instances where the market microstructure noise contained in high frequency
financial data can exhibit serial correlation. We showed that combining two or more time scales for the purpose
of estimating integrated volatility will work even in the situation where the microstructure noise exhibits time
series dependence.
    In most data error situations, one might expect that progress will lead the issue to become somehow
less salient over time. But in this instance, the measurement errors we face in ultra high frequency data
are compounded by the institutional evolution of the equity markets. While changes such as the passage to


                                                       20
decimalization contribute to reducing the amount of noise in the data, by reducing the rounding errors, the
emergence of competing electronic networks means that multiple transactions can be executed (and ultimately
reported in our database) on diﬀerent exchanges at the same time, thereby increasing the potential for slight
time reporting mismatches and other forms of data error.
   Indeed, the data generated by the individual market venues find their way to the public in various ways.
The principal Electronic Communication Networks (ECNs), such as INET and its precursors and Archipelago,
have high speed dissemination directly to their subscribers. These dissemination systems run on a telecom-
munications protocol known as “frame relay”, which is quite fast. Most other market data, however, reaches
the trading public (and ultimately us econometricians) either through Nasdaq’s dissemination or CTS/CQS.
The Consolidated Tape Association administers the CTS (the consolidated trade system) and CQS (the con-
solidated quote system). Virtually all US trades are reported to CTS, but the path may be indirect. Island
(not INET) may report a trade to the National (formerly Cincinnati) Stock Exchange, which will then report
it to CTS, which then broadcasts it to us. The general problem is that trading activity is fast relative to the
CTS speed of collection and dissemination.
   Furthermore, Nasdaq has had long-standing issues with late and delayed trade reports. In principle, a
Nasdaq member has up to 30 (in the past, 90) seconds to report a trade and anecdotal evidence suggests
that some dealers were/are using this leeway to its greatest extent. Since this practice was not uniform across
dealers and across time, the sequencing can be suspect. And the sequencing across exchanges may be unreliable
over very short time intervals: a trade on one exchange followed (and time-stamped to the same second as) a
trade in the same stock on a diﬀerent exchange, may not in fact have occurred in that order.
   While the consolidated tape feed (which we see on TAQ) is probably the best source of data available, we
may not be seeing the trades in the order in which they occurred, and the emergence and further development
of alternative networks on which to trade the same stocks makes the issue of market microstructure noise
in the data an increasing, not decreasing, one. ECNs represent over 30% of Nasdaq trading volume and are
increasing their market share in NYSE-listed issues as well (see e.g., Barclay, Hendershott, and McCormick
(2003)). Clearly, the decentralization of trading, combined with the increased frequency of trading, create
challenges for the data collection which ultimately aﬀect the estimation of a quantity as basic as the daily
integrated volatility of the price. So there are reasons to believe that the issue of controlling for market
microstructure noise in high frequency financial econometrics will be with us for some time.




                                                      21
References
Aït-Sahalia, Y., and P. A. Mykland (2003): “The Eﬀects of Random and Discrete Sampling When
 Estimating Continuous-Time Diﬀusions,” Econometrica, 71, 483—549.
Aït-Sahalia, Y., P. A. Mykland, and L. Zhang (2005): “How Often to Sample a Continuous-Time
 Process in the Presence of Market Microstructure Noise,” Review of Financial Studies, 18, 351—416.
Andersen, T. G., T. Bollerslev, F. X. Diebold, and P. Labys (2001): “The Distribution of Exchange
 Rate Realized Volatility,” Journal of the American Statistical Association, 96, 42—55.
Barclay, M. J., T. Hendershott, and D. T. McCormick (2003): “Competition among Trading Venues:
 Information and Trading on Electronic Communications Networks,” Journal of Finance, 58, 2637—2665.
Barndorff-Nielsen, O. E., P. R. Hansen, A. Lunde, and N. Shephard (2004): “Regular and Modified
 Kernel-Based Estimators of Integrated Variance: The Case with Independent Noise,” Discussion paper,
 Department of Mathematical Sciences, University of Aarhus.
Barndorff-Nielsen, O. E., and N. Shephard (2002): “Econometric Analysis of Realized Volatility and
 Its Use in Estimating Stochastic Volatility Models,” Journal of the Royal Statistical Society, B, 64, 253—280.
Black, F. (1986): “Noise,” Journal of Finance, 41, 529—543.
Choi, J. Y., D. Salandro, and K. Shastri (1988): “On the Estimation of Bid-Ask Spreads: Theory and
 Evidence,” The Journal of Financial and Quantitative Analysis, 23, 219—230.
French, K., and R. Roll (1986): “Stock Return Variances: The Arrival of Information and the Reaction
 of Traders,” Journal of Financial Economics, 17, 5—26.
Gençay, R., G. Ballocchi, M. Dacorogna, R. Olsen, and O. Pictet (2002): “Real-Time Trading
 Models and the Statistical Properties of Foreign Exchange Rates,” International Economic Review, 43, 463—
 491.
Hall, P., and C. C. Heyde (1980): Martingale Limit Theory and Its Application. Academic Press, Boston.
Hansen, P. R., and A. Lunde (2004): “An Unbiased Measure of Realized Variance,” Discussion paper,
 Stanford University, Department of Economics.
Harris, L. (1990): “Statistical Properties of the Roll Serial Covariance Bid/Ask Spread Estimator,” Journal
 of Finance, 45, 579—590.
Jacod, J. (1994): “Limit of Random Measures Associated with the Increments of a Brownian Semimartin-
 gale,” Discussion paper, Université de Paris VI.
Jacod, J., and P. Protter (1998): “Asymptotic Error Distributions for the Euler Method for Stochastic
 Diﬀerential Equations,” Annals of Probability, 26, 267—307.
Mykland, P. A., and L. Zhang (2002): “ANOVA for Diﬀusions,” Discussion paper, The University of
 Chicago, Department of Statistics.
Roll, R. (1984): “A Simple Model of the Implicit Bid-Ask Spread in an Eﬃcient Market,” Journal of Finance,
 39, 1127—1139.
Stoll, H. (2000): “Friction,” Journal of Finance, 55, 1479—1514.
Zhang, L. (2004): “Eﬃcient Estimation of Stochastic Volatility Using Noisy Observations: A Multi-Scale
 Approach,” Discussion paper, Carnegie-Mellon University.
Zhang, L., P. A. Mykland, and Y. Aït-Sahalia (2002): “A Tale of Two Time Scales: Determining
 Integrated Volatility with Noisy High-Frequency Data,” forthcoming in the Journal of the American Statistical
 Association.

                                                      22
Zhou, B. (1996): “High-Frequency Data and Volatility in Foreign-Exchange Rates,” Journal of Business &
 Economic Statistics, 14, 45—52.




                                                 23
                                               Appendix: Proofs

A        Proof of Lemma 1
First,
                                 n−J
                                 X                                                     n
                                                                                       X
                                        (Xti+J − Xti )(         ti+J   −    ti )   =     (−ci+J + ci )   ti
                                  i=0                                                  i=0

where ci = Xti − Xti−J for J ≤ i ≤ n, and = 0 otherwise. Second,
    ⎛Ã                     !2 ⎞
         n
         X                               Xn
  E⎝       (−ci+J + ci ) ti |X ⎠ = Var( (−ci+J + ci ) ti |X)
             i=0                                    i=0
                                                    ⎛                                                     ⎞
                                                 X                   X X
                                          ≤ E 2 ⎝ (−ci+J + ci )2 + 2  ρl | (−ci+J + ci )(−ci+J+l + ci+l )|⎠
                                                            i                                l≥1   i
                                               X                      X
                                          ≤E 2  (−ci+J + ci )2 (1 + 2   ρl )
                                                        i                                    l≥1

                                          ≤   4J[X, X](J) 2
                                                      T E (1                + 2ρ/(1 − ρ))                                  (A.1)

where the last two transitions follow by the Cauchy-Schwarz Inequality. The lemma follows by the Markov
inequality. This finishes the proof.


B        Proof of Proposition 1
By (4.7),
         Ã                                         !2            Ã                                     !
             1 X (K)
                n
                              n̄K 1 X (J)
                                     n
                                                                       1 X (K)
                                                                          n
                                                                                    n̄K 1 X (J) 2
                                                                                            n
                       2                      2                                          2
    E             c    ti   −          c      ti        = Var               c    −       ti     c
             K i=0 i          n̄J J i=0 i                              K i=0 i      n̄J J i=0 i ti
                                                          Xn µ                   ¶2
                                                                (K)    n̄K (J)
                                                        ≤      ci −        ci        Var( 2 )
                                                          i=0
                                                                       n̄J

                                                           X   n−l µ
                                                             n X                       ¶µ                    ¶
                                                                             n̄K (J)                n̄K (J )
                                                        +2          c(K)
                                                                     i    −       ci      c  (K)
                                                                                             i+l  −      c            2   2
                                                                                                          i+l Cov( ti , ti+l )
                                                           l=1 i=0
                                                                              n̄J                   n̄J
                                                                                     Ã                                  !
                                                          X µ (K) n̄K (J) ¶2
                                                           n                                         X n
                                                                                              2                2    2
                                                        ≤      ci −        c           Var( ) + 2         Cov( t0 , tl )
                                                          i=0
                                                                       n̄J i                         l=0
                                                            Ã n µ                       !
                                                              X (K) n̄K (J ) ¶2
                                                        =O         ci −          c         .                                 (B.1)
                                                              i=0
                                                                            n̄J i

where the second to last transition is due to the Cauchy-Schwarz inequality, and the final one follows from our
moment and mixing assumptions, again in view of Theorem A.6 (p. 278) of Hall and Heyde (1980). Under
(4.5) and by tedious calculation, one obtains that the r.h.s. of (B.1) is no larger than the order O(J/n). (In
fact, this is the exact order under the condition (4.13) below.) Thus
                                                                                Ãµ ¶1/2 !
                                     1 X
                                        n−K
                                                         n̄K 1 X
                                                               n−J
                                                                                    J
                     noise term = −2         ti ti+K + 2           ti ti+J + Op             ,
                                     K i=0               n̄J J i=0                  n




                                                                       24
whence, finally,
                                                        n−K                                      n−J
              K                                       1 X                                     1 X
                  (noise term − E [noise term]) = −2 √                          ti ti+K   + 2√            ti ti+J    + op (1)
             n1/2                                      n i=0                                   n i=0
                                                            L
                                                           −→ ξZnoise                                                                    (B.2)
as n → ∞, where Znoise is standard normal, by the same methods as in Chapter 5 of Hall and Heyde (1980)
(we here have a triangular array of sums, but the arguments go through nonetheless). In the case where both
J and K go to infinity with n, standard computations show that ξ 2 = ξ∞2
                                                                         . In the case where J does not go
to infinity (the m-dependent case, say),
                                          ∞
                                          X                                   ∞
                                                                              X
                            ξ 2 = ξ∞
                                   2
                                     +8          Cov(   ti−J , ti+J )    +8          Cum(   t0 , ti , tJ , ti+J ),
                                          i=−∞                                i=−∞

in obvious notation.


C     Proof of Proposition 7
Consider first the behavior of the [X, ](K) term. The conditional covariance behaves as follows:
     ³                        ¯          ´      n
                                             1 XX
                                                    n
            (J)         (K) ¯
Cov [X, ]         , [X, ]     ¯ X process =            (Xtj − Xtj−J )(Xtk − Xtk−K )Cov(                        tj    −   tj−J , tk   −   tk−K )
                                            JK j=J k=K

                                                  1 XX
                                                     n  n
                                            =             (Xtj − Xtj−J )(Xtk − Xtk−K )
                                                 JK j=J
                                                           k=K

                                              × (γ(j − k) + γ(j − k − (J − K)) − γ(j − k + K) − γ(j − k − J))
                                               1 XX
                                                   n   n
                                            ≈             (hX, Ximin(tj ,tk ) − hX, Ximax(tj−J ,tk−K ) )+
                                              JK j=J
                                                           k=K

                                                 × (γ(j − k) + γ(j − k − (J − K)) − γ(j − k + K) − γ(j − k − J))
so that
        ³                    ¯          ´       K
                                            1 X X
                                                    n
                             ¯
     Cov [X, ](J) , [X, ](K) ¯ X process =             (hX, Ximin(tk−l ,tk ) − hX, Ximax(tk−l−J ,tk−K ) )+
                                           JK l=−J k=K
                                                        × (γ(l) + γ(l + J − K) − γ(l − K) − γ(l + J))
                                                         1 X
                                                           K
                                                   =         (γ(l) + γ(l + J − K) − γ(l − K) − γ(l + J))
                                                        JK
                                                             l=−J
                                                         n
                                                         X
                                                    ×         (hX, Ximin(tk−l ,tk ) − hX, Ximax(tk−l−J ,tk−K ) )+ .                      (C.1)
                                                        k=K

For the final summation in (C.1), note that this is a telescope sum, of the form (where a and b depend on J,
K, and l, and where one can take a ≤ b)
                            n
                            X                                        n−a
                                                                     X                       n−b
                                                                                             X
                                (hX, Xitk−a − hX, Xitk−b ) =                  hX, Xitk −            hX, Xitk
                        k=K                                         k=K−a                   k=K−b
                                                                        n−a
                                                                        X                    K−a−1
                                                                                              X
                                                                =              hX, Xitk −              hX, Xitk
                                                                    k=n−b+1                  k=K−b

                                                                ≈ (b − a) hX, XiT                                                        (C.2)


                                                                    25
since hX, Xi0 = 0. Specifically, it is easy to see that a = max(0, l) while b = min(l + J, K). Thus

       ³                     ¯          ´           1 X
                                                        K
                             ¯
    Cov [X, ](J ) , [X, ](K) ¯ X process ≈ hX, XiT         (γ(l) + γ(l + J − K) − γ(l − K) − γ(l + J))
                                                   JK l=−J
                                                                       × (min(l + J, K) − max(0, l))                                 (C.3)

   At the same time,
                                        X     n                  n
                                                                 X
                                 4
        Cov(Un,J , Un,K ) =        Cov(            tj   tj−J ,         tk tk−K )
                                JK     j=J                       k=K

                                 4 XX
                                    n  n
                            =            Cov(              tj tj−J , tk tk−K )
                                JK j=J
                                       k=K
                                     n X
                                     X  n
                                 4
                            =                     Cov(     t0 t−J , tk−j tk−j−K )
                                JK   j=J k=K
                                         n−J
                                         X
                                 4
                            =                     (min(n, n + l) − max(J + l, K) + 1)+ Cov(                 t0 t−J , tl tl−K ),      (C.4)
                                JK
                                     l=−(n−K)

where

             Cov(   t0 t−J , tl tl−K )   = γ(l)γ(l − (J − K)) + γ(l − K)γ(l + J) + Cum(                     t0 , t−J , tl , tl−K )

Finally,
                                               µX                               XK−1          ¶
                                         2       J −1
                                                                         2               2
                    Cov(En,J , En,K ) ≈    Cov                           tj ,            tk
                                        JK       j=0                               k=0
                                                        K−1
                                                        X
                                               2
                                          =                       (min(J + l, K) − max(0, l) + 1)+ Cov(           2    2
                                                                                                                  t0 , tl )          (C.5)
                                              JK
                                                   l=−(J−1)

As before,
                                               2    2
                                      Cov(     t0 , tl )    = 2γ(l)2 + Cum(          t0 , t0 , tl , tl ).

Following (6.9) and (C.3)-(C.5), we obtain the combined expression given in equation (6.23). Note that in the
special case of model (3.3), with Gaussian U and V , the fourth cumulant above is zero, and γ(l) is given by
(6.24).




                                                                        26
                  Descriptive Statistics                        3M          AIG          Intel     Microsoft


                       Transactions

         Average number of transactions per day                2, 820      3, 435       13, 018       14, 299
       Average time between transactions (seconds)               8.3         6.8          1.8           1.6
            Min log-return from transactions                   −0.019      −0.028       −0.044        −0.083
                     Max log-return                             0.019       0.028        0.044         0.082
         Average daily first order autocorrelation             −0.41       −0.40        −0.60         −0.63
        Average daily second order autocorrelation              0.017       0.08          0.21         0.25
        Average daily third order autocorrelation               0.009      −0.01        −0.12         −0.17


                           Quotes

        Average number of quote revisions per day              12, 824     13, 507      22, 275       22, 661
          Average time between quote revisions                   1.8         1.7          1.1           1.0
           Min log-return from quote revisions                 −0.031      −0.044       −0.016        −0.013
                     Max log-return                             0.034       0.044        0.016         0.013
         Average daily first order autocorrelation             −0.49       −0.49        −0.24         −0.23
        Average daily second order autocorrelation              0.001       0.001         0.07         0.02
        Average daily third order autocorrelation               0.005       0.004         0.03         0.02




                                        Table 1: Descriptive Statistics


For the purpose of counting transactions, only transactions leading to a price change are counted. Identical quotes are
counted as a single one when reporting the number of quote revisions. Log-returns from quotes are computed using
a bid-ask midpoint, weighted by the respective depth of the two sides. Autocorrelations of log-returns are reported
in transaction time and quote time, respectively. Averages are computed over the last ten trading days in April 2004
(April 19-23 and 26-30). Minima and maxima are computed over the full ten day sample. All descriptive statistics for
the transactions data are reported prior to any data processing, except for the removal of obvious data errors such as
prices or quotes reported as zero. The estimates to be computed in the rest of the paper from transaction prices are
based on data cleaned to remove any price “bounceback”, defined as a price jump of size greater than a cutoﬀ of 1%,
immediately followed by a jump of equal magnitude but opposite sign (see Section 7.5 below). The raw quotes data are
pre-processed to remove any sets of quotes whose bid or ask price deviate from the closest transaction price recorded
by more than 5% (except in instances where the transaction price itself moves by that amount). The data are from
the TAQ database.




                                                          27
   Autocorrelation Order                    Constant                Avg Time Between Transactions                R2


                2                               0.25                                 −0.015                     0.35
                                                (8.0)                                (−3.9)

                3                              −0.16                                  0.012                     0.39
                                               (−6.5)                                 (4.2)

                4                               0.11                                 −0.009                     0.46
                                                (7.1)                                (−4.9)

                5                              −0.08                                  0.008                     0.49
                                               (−6.7)                                 (5.2)




            Table 2: Regressions of Higher Order Autocorrelations on Stock Liquidity


This table reports the results of cross-sectional OLS regressions of the autocorrelation coeﬃcients of order 2-5 on the
average time between transactions used as a measure of the liquidity of the stock, for the 30 DJIA stocks. These
autocorrelation coeﬃcients would be zero if the noise term were serially uncorrelated. The autocorrelation coeﬃcients
are computed for each stock as the average of the daily autocorrelations over the last ten trading days in April 2004.
t-statistics are in parentheses.




                                                          28
          0.0009


          0.0008


          0.0007


          0.0006


          0.0005


          0.0004


          0.0003


          0.0002

                   5sec 30sec    1mn             2mn              3mn             4mn             5mn


Figure 1: This figure shows the RV estimator [Y, Y ](all)
                                                    T     plotted against the sampling interval ∆. Since
                                                                                                   £ ¤ ∆ = T /n,
the plot illustrates the divergence of RV as n → ∞ predicted by our theory: RV has bias 2nE ε2 .




                                                       29
                             -5



                             -6
           RV in log−scale




                             -7



                             -8



                             -9




                                  30sec   15sec              5sec             2sec        1sec
                                             sampling interval in log−scale


Figure 2: This figure shows a regression of ln RV against ln n, plotted in log-log scale. Each data point in
the plot represents a triplet (one stock, one day,j) from the 30 DJIA stocks, the last 10 trading days in April
2004, and j = 1 or 2 depending upon whether all the observations are used on one out of two. For ease of
interpretation, the sample size n on the x-axis is translated into an average sampling interval on the basis of
1 trading day = 6.5 hours = 23, 400 one-second time intervals.




                                                        30
                         full dataset:
                     sampling every second                           use sum of squared
fast                                                                     log-returns
time                                                                       for bias
scale                                                                     correction



                       sparse sampling:
                          every 5 mn
                                                                         sum of squared
                                                                        log-returns = RV
slow
time                                                                     subsampling
scale                                                                         and
                                                                         averaging of
                                                                       sums of squared
                                                                          log-returns




             TSRV = subsampling and averaging, then
        bias-correcting using ultra high frequency data

        Figure 3: This figure describes the construction of the TSRV estimator.




                                          31
      1                                      AIG Transactions


                                                                                    Sample autocorrelation
    0.8



    0.6



    0.4



    0.2


               1

           0   1   2    3   4    5   6   7    8   9        10   11   12   13   14   15   16   17   18   19   20

                                                                                                   transaction
   -0.2                                                                                            lag




   -0.4


      1                                       MMM Transactions


    0.8



    0.6



    0.4



    0.2


               1

           0   1   2    3   4    5   6   7    8   9        10   11   12   13   14   15   16   17   18   19   20
                                                                                                   transaction
                                                                                                   lag
   -0.2



   -0.4




Figure 4: Log-return autocorrelogram from transactions on the stocks of American International Group, Inc.
(trading symbol: AIG) and 3M Co. (trading symbol: MMM), last ten trading days in April 2004.


                                                      32
       1                                    INTC Transactions


                                                                                Sample autocorrelation
    0.75



     0.5



    0.25


               1        3       5       7        9         11

           0   1   2   3    4   5   6        8        10        12   13   14   15   16   17   18   19   20
                                                                                               transaction
                                                                                               lag
   -0.25



    -0.5




       1                                    MSFT Transactions


    0.75



     0.5



    0.25


               1       3        5       7        9         11

           0   1   2   3    4   5   6        8        10        12   13   14   15   16   17   18   19   20
                                                                                              transaction
                                                                                              lag
   -0.25



    -0.5




Figure 5: Log-return autocorrelogram from transactions for Intel (trading symbol: INTC) and Microsoft
(trading symbol: MSFT), last ten trading days in April 2004.


                                                 33
       1                                        INTC Transactions


                                                                                  Sample autocorrelation
    0.75
                                                                                  Fitted autocorrelation



      0.5



    0.25


                 1        3        5        7        9             11

            0   1    2    3   4        6         8            10        12   13     14   15   16   17   18   19   20
                                                                                                        transaction
                                                                                                        lag
   -0.25



    -0.5




       1                                        MSFT Transactions


    0.75



     0.5



    0.25


                1        3        5        7         9             11
            0   1    2   3    4   5    6        8             10        12   13    14    15   16   17   18   19   20
                                                                                                         transaction
                                                                                                         lag
   -0.25



    -0.5




Figure 6: Log-return autocorrelogram from transactions for Intel and Microsoft, last ten trading days in April
2004, superimposed with the autocorrelogram fitted from the basic iid plus AR(1) model for the noise.


                                                         34
      1                                       INTC Ask Quotes

                                                                                     Sample autocorrelation
    0.8




    0.6




    0.4




    0.2



               1

           0   1   2    3   4    5   6    7   8    9        10   11   12   13   14    15   16   17   18   19   20
                                                                                                     quotes lag

   -0.2




      1                                       MSFT Ask Quotes



    0.8




    0.6




    0.4




    0.2



               1

           0   1   2    3   4    5   6    7   8    9        10   11   12   13   14    15   16   17   18   19   20
                                                                                                      quotes lag

   -0.2




Figure 7: Log-return autocorrelogram from quote revisions for Intel and Microsoft, last ten trading days in
April 2004.


                                                       35
       RV and TSRV for INTC on April 19, 2004                      RV and TSRV for INTC on April 20, 2004
  0.0003                                                     0.000275
0.000275                                                      0.00025
 0.00025                                                     0.000225
0.000225                                                       0.0002
  0.0002                                                     0.000175
0.000175                                                      0.00015
 0.00015                                                     0.000125

           15s1mn 2mn 3mn 4mn 5mn 6mn 7mn 8mn 9mn10mn                   15s1mn 2mn 3mn 4mn 5mn 6mn 7mn 8mn 9mn10mn


      RV and TSRV for INTC on April 21, 2004                       RV and TSRV for INTC on April 22, 2004

 0.00035                                                     0.000475
                                                              0.00045
0.000325
                                                             0.000425
  0.0003
                                                               0.0004
0.000275
                                                             0.000375
 0.00025
                                                              0.00035
0.000225
                                                             0.000325
  0.0002
                                                               0.0003
           15s1mn 2mn 3mn 4mn 5mn 6mn 7mn 8mn 9mn10mn                   15s1mn 2mn 3mn 4mn 5mn 6mn 7mn 8mn 9mn10mn


      RV and TSRV for INTC on April 23, 2004                       RV and TSRV for INTC on April 26, 2004

                                                             0.00022
 0.00035
0.000325                                                      0.0002

  0.0003                                                     0.00018
0.000275                                                     0.00016
 0.00025
                                                             0.00014
0.000225
                                                             0.00012
  0.0002
           15s1mn 2mn 3mn 4mn 5mn 6mn 7mn 8mn 9mn10mn                  15s1mn 2mn 3mn 4mn 5mn 6mn 7mn 8mn 9mn10mn



    Figure 8: Comparison of the RV and TSRV estimators for Intel, computed on a daily basis.




                                                        36
       RV and TSRV for MSFT on April 19, 2004                 RV and TSRV for MSFT on April 20, 2004
0.00018                                                  0.0002

0.00016                                                 0.00018

0.00014                                                 0.00016

                                                        0.00014
0.00012
                                                        0.00012
0.0001
                                                        0.0001

          15s1mn 2mn 3mn 4mn 5mn 6mn 7mn 8mn 9mn                  15s1mn 2mn 3mn 4mn 5mn 6mn 7mn 8mn 9mn

      RV and TSRV for MSFT on April 21, 2004                  RV and TSRV for MSFT on April 22, 2004

0.00025                                                 0.00028
                                                        0.00026
 0.0002                                                 0.00024
                                                        0.00022
0.00015
                                                         0.0002

0.0001                                                  0.00018
                                                        0.00016

          15s1mn 2mn 3mn 4mn 5mn 6mn 7mn 8mn 9mn                  15s1mn 2mn 3mn 4mn 5mn 6mn 7mn 8mn 9mn

      RV and TSRV for MSFT on April 23, 2004                   RV and TSRV for MSFT on April 26, 2004
                                                        0.00016
 0.0004
0.00035                                                 0.00014
 0.0003
                                                        0.00012
0.00025
 0.0002
                                                        0.0001
0.00015
0.0001                                                  0.00008
          15s1mn 2mn 3mn 4mn 5mn 6mn 7mn 8mn 9mn                  15s1mn 2mn 3mn 4mn 5mn 6mn 7mn 8mn 9mn



  Figure 9: Comparison of the RV and TSRV estimators for Microsoft, computed on a daily basis.




                                                   37
                      Robustness of RV for INTC                                   Robustness of TSRV for INTC
         0.0007                                                          0.0004

                                                                        0.00035
         0.0006
                                                                         0.0003
         0.0005




                                                                TSRV
    RV




                                                                        0.00025
         0.0004
                                                                         0.0002
         0.0003
                                                                        0.00015
         0.0002

                                                                                    averaging frequency HKL
                  15s1mn2mn3mn4mn5mn6mn7mn8mn9mn10mn                          2mn 3mn 4mn 5mn 6mn 7mn 8mn 9mn 10mn
                           sparse frequency

                      Robustness of RV for MSFT                                   Robustness of TSRV for MSFT
         0.0007                                                         0.00025

         0.0006
                                                                         0.0002
         0.0005
                                                                 TSRV
    RV




         0.0004                                                         0.00015

         0.0003
                                                                         0.0001
         0.0002

         0.0001

                                                                                    averaging frequency HKL
                  15s1mn 2mn 3mn 4mn 5mn 6mn 7mn 8mn 9mn                      2mn 3mn 4mn 5mn 6mn 7mn 8mn 9mn
                            sparse frequency



Figure 10: Comparison of the RV and TSRV estimators for Intel and Microsoft, averaged over the last ten
trading days of April 2004. The left panels demonstrate the dependence of RV as a function of the sparse
sampling interval, while the right panels study the robustness of TSRV with respect to the choice of the
averaging frequency as represented by the number of subgrids K.




                                                           38
                     Robustness of TSRV with Time−Dependent Noise for INTC




      5mn                                                                                                  0.00025

                   6mn

                                7mn                                                               1mn30s

                                                8mn                                             1mn
                                                                                                       J
                                            K
                                                             9mn                          30s

                                                                                    5s
                                                                            10mn



                     Robustness of TSRV with Time−Dependent Noise for MSFT




                                                                                                       0.00015

      5mn
                                                                                                      2mn

                     6mn
                                                                                                1mn30s

                                      7mn
                                                                                          1mn
                                                                                                 J

                                            K         8mn                           30s

                                                                    9mn        5s




Figure 11: Robustness of the TSRV estimator for Intel and Microsoft over the choice of the two time scales J
(fast) and K (slow), averaged over the last 10 trading days of April 2004.




                                                      39
                                           TSRV and MSRV for INTC
   0.0004




   0.0003




   0.0002




   0.0001




              4ê19    4ê20     4ê21     4ê22       4ê23    4ê26     4ê27   4ê28    4ê29     4ê30



                                               TSRV and MSRV for MSFT
    0.0003




   0.00025




    0.0002




   0.00015




    0.0001




   0.00005




              4ê19     4ê20    4ê21     4ê22       4ê23     4ê26    4ê27   4ê28    4ê29     4ê30




Figure 12: Comparison of the TSRV and TSRV estimators for Intel and Microsoft, for each of the last ten
trading days of April 2004.

                                                    40
                       Robustness of RV for INTC                                  Robustness of TSRV for INTC
          0.0007                                                        0.0004

                                                                        0.00035
          0.0006
                                                                         0.0003
          0.0005




                                                                 TSRV
     RV




                                                                        0.00025
          0.0004
                                                                         0.0002
          0.0003
                                                                        0.00015
          0.0002
                   5s 1mn2mn3mn4mn5mn6mn7mn8mn9mn10mn                         2mn 3mn 4mn 5mn 6mn 7mn 8mn 9mn 10mn
                            sparse frequency                                        averaging frequency HKL

                       Robustness of RV for MSFT                                  Robustness of TSRV for MSFT
          0.0007                                                        0.00025

          0.0006
                                                                         0.0002
          0.0005
                                                                 TSRV
     RV




          0.0004                                                        0.00015

          0.0003
                                                                         0.0001
          0.0002

          0.0001
                   5s 1mn 2mn 3mn 4mn 5mn 6mn 7mn 8mn 9mn                     2mn 3mn 4mn 5mn 6mn 7mn 8mn 9mn
                             sparse frequency                                       averaging frequency HKL



Figure 13: Dependence of the RV and TSRV estimators for Intel and Microsoft, averaged over the last ten
trading days of April 2004 on the degree of pre-processing of the raw data. In each panel, the three curves
correspond respectively to the raw data (solid line), the data where immediate price bouncebacks of 1% or more
are eliminated (large dashes) and the data where immediate price bouncebacks of 0.1% or more are eliminated
(short dashes). In the case of TSRV, the results for the raw data and the elimination of 1% bouncebacks are
virtually indistinguishable.




                                                            41
