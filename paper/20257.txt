                               NBER WORKING PAPER SERIES




     EXTREMAL QUANTILE REGRESSIONS FOR SELECTION MODELS AND THE
                       BLACK-WHITE WAGE GAP

                                      Xavier D'Haultfoeuille
                                         Arnaud Maurel
                                         Yichong Zhang

                                       Working Paper 20257
                               http://www.nber.org/papers/w20257


                     NATIONAL BUREAU OF ECONOMIC RESEARCH
                              1050 Massachusetts Avenue
                                Cambridge, MA 02138
                                     June 2014




We are grateful to Derek Neal for useful suggestions and providing us with the sample used in his
1996 JPE article. We also thank Peter Arcidiacono, Victor Chernozhukov, Ivan Fernandez-Val, Shakeeb
Khan, Pat Kline, Adam Rosen, Lowell Taylor, Ed Vytlacil, and participants at various seminars and
conferences for useful comments and suggestions. The views expressed herein are those of the authors
and do not necessarily reflect the views of the National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2014 by Xavier D'Haultfoeuille, Arnaud Maurel, and Yichong Zhang. All rights reserved. Short
sections of text, not to exceed two paragraphs, may be quoted without explicit permission provided
that full credit, including © notice, is given to the source.
Extremal Quantile Regressions for Selection Models and the Black-White Wage Gap
Xavier D'Haultfoeuille, Arnaud Maurel, and Yichong Zhang
NBER Working Paper No. 20257
June 2014
JEL No. C21,C24,J31

                                             ABSTRACT

We consider the estimation of a semiparametric location-scale model subject to endogenous selection,
in the absence of an instrument or a large support regressor. Identification relies on the independence
between the covariates and selection, for arbitrarily large values of the outcome. In this context, we
propose a simple estimator, which combines extremal quantile regressions with minimum distance.
We establish the asymptotic normality of this estimator by extending previous results on extremal
quantile regressions to allow for selection. Finally, we apply our method to estimate the black-white
wage gap among males from the NLSY79 and NLSY97. We find that premarket factors such as AFQT
and family background characteristics play a key role in explaining the level and evolution of the black-
white wage gap.


Xavier D'Haultfoeuille                              Yichong Zhang
CREST                                               Duke University
15 Bd Gabriel Peri                                  Department of Economics
92245 Malakoff Cedex                                213 Social Sciences Building
FRANCE                                              Box 90097
xavier.dhaultfoeuille@ensae.fr                      Durham, NC 27708
                                                    yichong.zhang@duke.edu
Arnaud Maurel
Department of Economics
Duke University
213 Social Sciences Building
Box 90097
Durham, NC 27708
and NBER
apm16@duke.edu
1    Introduction
Endogenous selection has been recognized as one of the key methodological issues arising in
the analysis of microeconomic data since the seminal articles of Gronau (1974) and Heckman
(1974). The most common strategy to deal with selection is to rely on instruments that
determine selection but not the potential outcome (see, among many others, Heckman, 1974,
1979, 1990, Ahn & Powell, 1993, Das et al. , 2003, and Vella, 1998 for a survey). However, in
practice, valid instruments are generally difficult to find. Identification at infinity has been
proposed in the literature as an alternative solution to the endogenous selection problem, in
situations where one is primarily interested in estimating the effects of some covariates on a
potential outcome. In particular, Chamberlain (1986) showed that if some individuals face an
arbitrarily large probability of selection and the outcome equation is linear, then one can use
these individuals to identify the effects of the covariates on the outcome of interest. Lewbel
(2007) generalized this result by proving that identification can be achieved in the context of
moment equality models, provided that a special regressor has a support which includes that
of the error term from the selection equation (see Lewbel, 2014, for an overview of the special
regressor method). Again, in many applications, such a regressor is hard to come by. In a
recent article, D’Haultfoeuille & Maurel (2013a) have shown that identification in the absence
of an instrument is in fact possible without such a covariate. The starting intuition is that,
if selection is truly endogenous, then one can expect the effect of the outcome on selection to
dominate those of the covariates for sufficiently large values of the outcome. Following this
idea, they proved identification under the key condition that selection becomes independent
of the covariates at the limit, i.e., when the outcome tends to the upper bound of its support.

This paper builds on this insight and develops a novel inference method for a class of semi-
parametric location-scale models subject to endogenous selection. Unlike prior estimation
methods for sample selection models, we propose a distribution-free estimator that does not
require an instrument for selection or a large support regressor. While D’Haultfoeuille & Mau-
rel (2013a) prove identification for a nonparametric location-scale outcome equation, we rely
instead on a semiparametric specification in order to obtain faster convergence rates. This is
crucial for the practical usefulness of our method. Importantly, the model is left unrestricted
otherwise. In particular, we do not restrict the selection process, apart from the indepen-
dence at the limit condition mentioned above. In this paper, we interpret this condition in
the context of standard selection models, and show that it translates into a restriction on
the copula between the error terms of the outcome and selection equation. This restriction is
mild provided that selection is endogenous, and holds in particular for all Gaussian copulas


                                               2
with positive dependence. In this context, we show that linear quantile regressions, for large
values of the quantile indices, allow us to recover some linear combinations of the covariates
effects on the location and scale of the outcome. Those parameters can then be estimated in a
second step by a simple minimum distance estimator, which combines the previous estimators
for a range of quantile indices. This insight is important for at least two reasons. First, our
estimator is simple to implement. In particular, unlike most of the existing semiparametric
estimators for sample selection models, our estimator is not based on a nonparametric first
step. Second, the asymptotic properties of extremal quantile regressions, that is quantile
regressions applied to the tails, have been thoroughly studied in the case without selection
in an important paper by Chernozhukov (2005).1 This provides a very natural starting point
to develop asymptotic inference in our setting. The estimators of the location and scale pa-
rameters can then be used to construct bounds on the quantile effects. We characterize the
sharp bounds and further derive simpler outer bounds on which one can conduct inference
using the methodology developed by Chernozhukov et al. (2013). It is worth noting that,
while we use quantile regressions as a tool to circumvent the selection issue, we assume a
linear location-scale specification for the potential outcome. This is dfferent in spirit from the
methods proposed in the literature to estimate quantile regression models in the presence of
sample selection (see notably Buchinsky, 1998, and Arellano & Bonhomme, 2011).

The main difficulty in establishing the asymptotic properties of our estimator is that because
of selection, extremal conditional quantiles are not exactly linear here, but only equivalent
to a linear form as the quantile index τn tends to zero. Hence, we face a bias-variance
trade-off that is typical in non- or semiparametric analysis. Choosing a moderately small
quantile index decreases the variance of the estimator, but this comes at the price of a higher
bias. Conversely, choosing a very small quantile index mitigates the bias, but increases the
variance. In the paper, we provide sufficient conditions under which both bias and variance
vanish asymptotically, resulting in asymptotically normal and unbiased estimators. As in
the case without selection examined by Chernozhukov (2005), the convergence rates are not
standard, and depend on the tail behavior of the error term from the outcome equation.
This is broadly similar to the convergence rates discussed in Andrews & Schafgans (1998),
Schafgans & Zinde-Walsh (2002) and Khan & Tamer (2010), the main difference being that
   1
     Formally, denoting by n the sample size and τn the quantile index, extremal quantile regressions correspond
to τn -quantile regressions where τn tends to zero as the sample size n grows to infinity. In this paper, we focus on
the intermediate order case, which corresponds to situations where τn × n tends to infinity. See Chernozhukov
& Du (2008) for a review of extremal quantile regressions. See also related work by Altonji et al. (2008),
who derive the asymptotic properties of a nonparametric extremal quantile regression estimator. While their
framework is very general, it cannot be readily extended to the case where the outcome is subject to sample
selection.



                                                         3
the tail behavior of the outcome is going to play a key role here, rather than that of the
covariates. Importantly, though, our asymptotic results suggest a rate-adaptive approach for
inference, as in Khan & Tamer (2010) and Chernozhukov & Fernandez-Val (2011).

Asymptotic normality and unbiasedness of our estimators requires an appropriate choice of
the quantile index, similarly to nonparametric kernel regressions that require an appropriate
bandwidth choice. But contrary to the latter case, admissible rates of convergence towards
zero for the quantile index depend in a complicated way on the data-generating process.
An analogous issue arises in the estimation at infinity of the intercept of sample selection
models (see Andrews & Schafgans, 1998 and Schafgans & Zinde-Walsh, 2002), as well as in
the estimation of extreme value indices (see Drees & Kaufmann, 1998 and Danielsson et al. ,
2001). This is a difficult problem. In the paper, we propose a heuristic data-driven procedure
that selects the quantile index minimizing a criterion function capturing the trade-off between
bias and variance. In particular, we use subsampling combined with a minimum distance
estimator to proxy the bias term, which, in this setting, cannot be simply estimated. Monte
Carlo simulation results show that the sampling distributions of our estimators are fairly well
approximated by the asymptotic normal distributions, suggesting that our procedure performs
well in practice.

Finally, we apply our method to the estimation of the black-white wage gap among males
from the 1979 and 1997 cohorts of the National Longitudinal Survey of Youth (NLSY79 and
NLSY97). Following Neal & Johnson (1996), we focus on the residual portion of the wage
gap that remains after controlling for premarket factors. To the extent that black males
are more likely to dropout from the labor market than white males, as was first pointed
out in the influential work of Butler & Heckman (1977), correcting for selection is crucial
for consistently estimating the black-white differential in terms of potential wages. Besides,
evidence that the black-white employment gap has substantially widened over time (see, e.g.,
Juhn, 2003 and Neal & Rick, 2014) stresses the importance of dealing with selection in order
to draw valid conclusions regarding the across-cohort evolution of the black-white wage gap.
In this context, finding a valid instrument that affects selection but not potential wages is
particularly challenging, making it desirable to use an estimation method that does not require
such an instrument. For the NLSY79 cohort, we find a smaller residual wage gap (10.1%)
than the one obtained using the imputation method of Neal & Johnson (1996) and Johnson
et al. (2000), which is consistent with our approach being based on a weaker identifying
restriction.2 Overall, our estimates strengthen the key takeaway of Neal & Johnson (1996) by
   2
     Other noteworthy papers analyzing the black-white wage gap while using imputation methods to correct
for selection into the workforce include Brown (1984), Smith & Welch (1989), Juhn (2003), Neal (2004), Neal


                                                    4
providing evidence of an even more important role played by the black-white AFQT gap.

Turning to the evolution across the 1979 and 1997 cohorts, we find that there has been a
slow convergence in the raw male black-white wage gap between 1990 and 2007 (-4.6 pp), and
an even slower convergence in the residual portion of the wage gap that remains after con-
trolling for premarket factors such as AFQT and family background (-1.2 pp). Interestingly,
this provides evidence that premarket skills are a key component of the level as well as the
evolution of the black-white wage gap. Besides, the fact that the wage gap which remains
after accounting for differences in premarket factors is essentially stable after almost 20 years
suggests that this residual portion of the wage gap is an important factor behind the slow
convergence of the wages of blacks and whites.

The remainder of the paper is organized as follows. Section 2 presents the set-up and discusses
the identification results. Section 3 defines the estimators and establishes the main asymptotic
normality results. Section 4 discusses some Monte Carlo simulation results. Section 5 applies
our method to the estimation of the black-white wage gap among males. Finally, Section
6 concludes. Additional details on the estimation procedure and the data, along with the
proofs, are collected in the appendix.


2     The set-up and identification

2.1    Model and main result

Before presenting the model, let us introduce some notations and definitions. For any random
variable U , we denote by FU and SU its cumulative distribution function (cdf.) and survival
function, while QU denotes its quantile function, QU (u) = inf{u : FU (u) ≥ τ }. For more
general increasing functions G, we let G← (u) = inf{v : G(v) ≥ u}, with the convention that
inf ∅ = +∞, denote its generalized inverse. Finally, we use in the following some notions
from extreme value theory. A function F is regularly varying at x ∈ {0, +∞} with index
α ∈ [−∞, +∞], and we write F ∈ RVα (x), if for any t > 0, limu→x F (tu)/F (u) = tα ,
with the understanding that t∞ = ∞ if t > 1 and = 0 if 1 > t > 0 (and similarly for
α = −∞). F is slowly varying at x if F ∈ RV0 (x). We also say that a given cdf. F belongs to
the domain of attraction of generalized extreme value distributions if there exists sequences
(an )n∈N and (bn )n∈N and a cdf. G such that for any independent draws (U1 , ..., Un ) from F ,
b−1
 n (max(U1 , ..., Un ) − an ) converges in distribution to G. In such a case, G belongs to the
family of generalized extreme value distributions.
(2006) and Neal & Rick (2014).



                                               5
Let Y ∗ denote the outcome of interest and X ∈ Rd denote a vector of covariates, excluding
the constant. We suppose that Y ∗ and X are related through the location-scale model

                                        Y ∗ = X 0 β + (1 + X 0 δ)ε,                                   (2.1)

where we suppose, without loss of generality, that 1+X 0 δ > 0. Because we do not standardize
ε, we can always suppose that there is no intercept and fix the constant in the multiplier of
ε to one. Our focus throughout the paper is on the parameters β, δ, along with the quantile
effects of Xj on Y ∗ . These quantile effects correspond to the effect on Y ∗ of an exogenous,
infinitesimal, change of Xj , or a change from Xj = 0 to Xj = 1 if Xj is binary, for individuals
at a given conditional quantile of Y ∗ . We do not seek to recover the distribution of ε, and
in particular the intercept E(ε), though this distribution will play an important role in our
analysis.

We face a sample selection issue here as we only observe (D, Y = DY ∗ , X), where D denotes
the selection dummy. Importantly, we do not assume to have access to an instrument affecting
D but not Y ∗ , nor do we require one of the covariates to have a large support. Instead,
identification is achieved under the following conditions.

Assumption 1. (Exogeneity) X ⊥⊥ ε.

Assumption 2. (Covariates) X has a compact support Supp(X). Let X = [1, X 0 ]0 , QX =
       0
E(XX ) is full rank.

Assumption 3. (Tail and regularity of the residual) (i) sup(Supp(ε)) = ∞, (ii) Sexp(ε) is not
slowly varying at infinity, (iii) Sε is in the domain of attraction of generalized extreme value
distributions and (iv) the distribution of (X, ε) conditional on D = 1 is dominated by a product
measure. We denote by fε|D=1,X and fY |D=1,X the corresponding conditional densities.

Assumption 4. (Independence at infinity) There exists h > 0 such that for all x ∈ Supp(X),

                                  lim P (D = 1|X = x, Y ∗ = y) = h.
                                 y→∞




Assumption 1 is restrictive but commonly made in the context of selection models.3 It is also
weaker than the exogeneity assumption imposed, for instance, by Chamberlain (1986) or Ahn
   3
    Notable exceptions include Das et al. (2003) and Lewbel (2007), who allow for endogenous regressors.
However, the estimators proposed in these papers require an instrument for selection or a special regressor,
respectively.


                                                     6
& Powell (1993), since we allow for heteroskedasticity here. The compact support condition
in Assumption 2 is not required for identification but will be needed when using extremal
quantile regression techniques. We maintain this assumption here for simplicity. Assumption
3-(ii) is satisfied if, for instance, E(exp(bε)) < ∞ for some b > 0. Note that this tail condition
is fairly mild. For example, in the context of a wage equation where Y ∗ corresponds to the
logarithm of the wage w, it is satisfied as long as E(wb ) < ∞ for a given b > 0. It follows that
this condition holds even if wages exhibit very fat tails, for instance Pareto-like. Conditions
(i) and (iii) are not necessary for identification but will be used subsequently. Condition
(iii) is mild and satisfied by most of the standard continuous cdf., including the normal one.
Condition (iv), which is very mild, is not needed for the identification of β and δ. It is
only required to define the bounds on the quantile effects. Finally, Assumption 4 is our key
identifying condition. We require selection to become independent of the covariates at infinity,
that is conditional on having arbitrarily large outcomes. The underlying intuition is that, if
selection is indeed endogenous, then one can expect the effect of the outcome on selection to
dominate those of the covariates for sufficiently large values of the outcome. We come back
to this condition in the following section, by discussing in detail several examples where this
condition holds.

The first part of Theorem 2.1 below follows from Theorem 2.1 in D’Haultfoeuille & Maurel
(2013a). They proved, in a nonparametric framework where X 0 β and X 0 δ are replaced by
two unrestricted functions ψ(X) and σ(X), that these functions are identified. Assumption
2 then ensures that β and δ are identified. The second part of Theorem 2.1 provides the
sharp bounds on the quantile effects, which were not considered in D’Haultfoeuille & Maurel
(2013a).

Theorem 2.1. Under Assumptions 1-4, β and δ are identified. Moreover, the quantile effect
∆jτ = ∂QY ∗ |X /∂Xj (τ |X)4 satisfies ∆jτ ∈ [∆jτ , ∆jτ ], with

                         ∆jτ   = βj + δj F ←
                                           ε [τ − 1{δj > 0}(1 − F ε (+∞))] ,

                         ∆jτ   = βj + δj F ←
                                           ε [τ − 1{δj < 0}(1 − F ε (+∞))] ,


and where
                 Z   v
                                    P (D = 1|X = x)(1 + x0 δ)fY |D=1,X (x0 β + (1 + x0 δ)u|x) du.
                                                                                            
     F ε (v) =           sup
                 −∞ x∈Supp(X)

   4
     When Xj is binary, ∆jτ should rather be defined by ∆jτ = QY ∗ |X−j ,Xj =1 − QY ∗ |X−j ,Xj =0 , where X−j
denotes all components of X except Xj . Under Assumption 1, both definitions coincide and are equal to
βj + δj Qε (τ ).


                                                     7
Finally, suppose that there exists K > 0 such that for all (x, x0 ) ∈ Supp(X)2 and all y large
enough,

             |P (D = 1|X = x, Y ∗ = y) − P (D = 1|X = x0 , Y ∗ = y)| ≤ Kkx − x0 k,                (2.2)

where kxk denotes the euclidian norm of x. Then the bounds ∆jτ and ∆jτ are sharp.

The underlying intuition of the identification result for (β, δ) is that under Assumption 4,
the right tail of Y and Y ∗ are equivalent up to a multiplicative constant. It follows that we
can use the conditional survival function of Y given X to uniquely recover the location and
scale parameter, provided the tail of the residual is not too fat (Assumption 3-(ii)). Point
identification of β and δ does not necessarily entail point identification of the quantile effects
∆jτ , because of potential heteroskedasticity. Specifically, ∆jτ is point identified under partial
homoskedasticity (δj = 0), but only partially identified otherwise. This is due to the fact that
∆jτ = βj + δj Qε (τ ). Because of the missing data issue, the quantile Qε (τ ) cannot be point
identified in general.

An important limitation of the sharp bounds is that, to the best of our knowledge, no existing
method can be readily used to conduct inference on them. Kitagawa (2010) provides some
useful results, but in a simpler framework where X is discrete and without the need to estimate
β and δ in a first step. On the other hand, we derive in Appendix C.2 the following outer
bounds on ∆jτ :
                                       h                                    i
                                                                      o
                    ∆ojτ      = βj + δj 1{δj > 0}Qoε (τ ) + 1{δj < 0}Qε (τ )
                                       h                                    i ,                   (2.3)
                         o                         o
                    ∆jτ       = βj + δj 1{δj > 0}Qε (τ ) + 1{δj < 0}Qoε (τ )

with
                                                                                
                                                                τ −P (D=0|X=x)
                                              QY |D=1,X=x         P (D=1|X=x)        − x0 β
                    Qoε (τ ) =      sup                                                       ,
                                  x∈Supp(X)                     1 + x0 δ
                                                                          
                     o
                                              QY |D=1,X=x          τ
                                                             P (D=1|X=x)       − x0 β
                    Qε (τ )   =      inf                                                .
                                  x∈Supp(X)                 1 + x0 δ

The outer bounds take a more convenient form for inference than the sharp bounds. Besides,
they actually coincide with these sharp bounds when the family of functions e 7→ P (D =
1|X = x)(1 + x0 δ)fY |D=1,X (x0 β + (1 + x0 δ)e|x) indexed by x do not intersect.




                                                     8
2.2    The independence at infinity condition

Point identification of β and δ relies mostly on the independence at infinity assumption. To
get a better sense of this condition, we discuss it below in the context of two common selection
models. The first one is a threshold crossing model described in Assumption 5.

Assumption 5. (i) D = 1{φ(X) − η ≥ 0} with (ε, η) ⊥⊥ X, (ii) inf x∈Supp(X) Fη (φ(x)) =
v > 0, (iii) Fε and Fη are continuous and strictly increasing and the copula C of (−ε, η) is
differentiable with respect to its first argument.

The first condition defines the selection model as a standard threshold crossing model. Impor-
tantly however, we do not add any instrument in this selection equation. The second condition
ensures that x 7→ P (D = 1|X = x) is bounded below by a positive number. Note that this
condition will typically hold if none of the covariates has a large support, which is precisely the
type of situation we are interested in. In this context, Proposition 2.1 provides a restriction on
C ensuring that Assumption 4 is satisfied. Hereafter, let fC (τ ) = supu≤τ,v∈[v,1] |∂1 C(u, v) − 1|,
where ∂1 C denotes the partial derivative of C with respect to its first argument.

Proposition 2.1. Suppose that Assumptions 2, 3 and 5 hold, and

                                             lim fC (τ ) = 0.                                        (2.4)
                                             τ →0


Then Assumption 4 is satisfied, and therefore β and δ are identified.

The key idea is that selection becomes independent of the covariates for large values of the out-
come if selection is endogenous enough, in the sense that (−ε, η) satisfies (2.4). To understand
this condition better, it is useful to consider two extreme cases. In the perfect dependence
case such that η = −ε, then ∂1 C(u, v) = 1 for all u < v, so that (2.4) actually holds exactly
for small values of τ . On the other hand, when η and −ε are independent, ∂1 C(u, v) = v, and
fC (τ ) = 1 − v, which is positive except in the degenerate case where D = 1 almost surely.
In between these two extreme cases, Table 1 provides examples of copulas that satisfy this
constraint. Importantly, it holds for all Gaussian copulas with positive dependence.5 It also
holds for Archimedean copulas under a restriction on the behavior of the generator Ψ around
0. This restriction holds for instance for the Clayton copula, for which Ψ(u) = (u−θ − 1)/θ,
provided that θ > 0. The Gumbel family is another popular Archimedean family of copulas
that does not satisfy the restriction on Ψ, since Ψ is slowly varying at 0. However, Condition
(2.4) still holds for some parameters of this family.
   5
    Note that what is important here is the strength, but not the sign, of the dependence between η and −ε.
The case of negative dependence could be addressed by replacing D by 1 − D (see D’Haultfoeuille & Maurel,
2013a for a discussion on this point).


                                                    9
                                                                            Restriction
    Copula family                                                          ensuring (2.4)

    Gaussian C(u, v; ρ)                                                        ρ>0
    Archimedean C(u, v; Ψ) = Ψ−1 (Ψ(u) + Ψ(v))                           limu→0 Ψ(u) = +∞
                                                             Ψ is C 1 and RVα (0) with α ∈ (0, +∞]
    Gumbel Ψ(u; θ) = (− log(u))θ                                               θ>1


                           Table 1: Examples of copulas satisfying (2.4).


Proposition 2.1 suggests that our identification strategy is natural in the context of Roy’s
model of self-selection. Following-up on this idea, we consider a generalized Roy model (see
Heckman & Vytlacil, 2007, Eisenhauer et al. , 2013) with two or more sectors. Let Ys∗ denote
the potential outcome corresponding to sector s ∈ {1, ..., S}, we suppose that

                                      Ys∗ = X 0 βs + (1 + X 0 δs )εs .                                 (2.5)

The utility Us associated with sector s is supposed to satisfy

                                        Us = Ys∗ + Gs (X) + ηs .                                       (2.6)

The pure Roy model would correspond to Gs (X) = ηs = 0. Here we also allow for the
deterministic and random factors Gs (X) and ηs to affect Us . Individuals choose the sector
that maximizes their utility,
                                          D = arg      max        Us .                                 (2.7)
                                                    s∈{1,...,S}

We further assume a factor structure for the unobservables εs and ηs .

Assumption 6. (i) εs = λ0s,1 π + νs,1 and ηs = λ0s,2 π + νs,2 , where π is a vector of common
factors, (ii) π has a compact support, (iii) (X, π, ν1,1 , ..., νS,2 ) are mutually independent, (iv)
νs = supx∈Supp(X) maxi6=s ((1 + x0 δi )νi,1 + νi,2 − νs,2 ) satisfies the condition of Assumption 3,
and (v) Gs (X) has compact support.

The assumption of a linear factor model on the error terms is quite common in the context
of generalized Roy models, see e.g. Carneiro et al. (2003) or Cunha & Heckman (2007).
Proposition 2.2 below shows that, under these conditions, our identification strategy can be
used to identify (βs , δs ) without any exclusion restriction or large support regressor.6
   6
     D’Haultfoeuille & Maurel (2013a, 2013b) derived related results, but for a different class of Roy models
with only two sectors and in the absence of heteroskedasticity in the outcome equation.


                                                     10
Proposition 2.2. Suppose that Equations (2.5)-(2.7) and Assumptions 2 and 6 hold. Then
for all s ∈ {1, ..., S}, Ds = 1{D = s} satisfies Assumption 4, and (βs , δs ) are identified.


3     Estimation

3.1   Definition of the estimators

We start by defining our estimators before establishing their asymptotic properties in the
next subsection. Suppose we have a sample (Di , Yi , Xi )i=1...n of n i.i.d. random variables
distributed as (D, Y, X). The starting point for identification is that under Assumptions 1
and 4, we have, as y → −∞,

                           F−Y |X (y|x) ∼ h F−ε ((y + x0 β)/(1 + x0 δ))                         (3.1)

Now, the key insight for estimating (β, δ) is that if one also imposes Assumption 3, then it
is possible to invert both sides and maintain the equivalence. It follows that the quantile
regression of −Y on X is asymptotically linear. This result is going to play an important role
in our estimation procedure.

Lemma 3.1. Under Assumptions 1-4, as τ → 0,

                                  Q−Y |X (τ |x) ∼ γ(τ ) + x0 β(τ )                              (3.2)

where γ(τ ) = Q−ε (τ /h) and β(τ ) = −β + γ(τ )δ.

Lemma 3.1 provides the intuition that it might be possible to use quantile regressions in the
tails to consistently estimate (γ(τ ), β(τ )), for small values of the quantile index τ . The main
difficulty in formalizing this intuition though, comes from the fact that (3.2) is an equivalence
and not an equality, which gives rise to a bias term that needs to be controlled. We define

                                                   n
                                                   X
                         (b
                          γ (τ ), β(τ
                                  b )) = arg min         ρτ (−Yi − γ − Xi0 β),
                                             γ,β
                                                   i=1


where ρτ (u) = (τ − 1{u < 0})u is the check function used in quantile regressions. Then one
can simply use the following relationships to estimate the parameters of interest β and δ:

                                         β(lτ ) − β(τ )
                                     δ =                ,
                                         γ(lτ ) − γ(τ )
                                     β = −β(τ ) + γ(τ )δ,



                                                   11
where the first equality holds provided that γ(lτ ) − γ(τ ) 6= 0.7 We basically follow this
route in the paper, except that for an efficiency matter we estimate δ using J reduced form
estimators (β(l1 τ ), ..., β(lJ τ )) rather than just two, where (l1 , . . . , lJ ) is a vector of positive
spacing parameters such that lj 6= 1 for all j ∈ {1, . . . , J}. Let us consider
                                                                                   
                                           b 1 τ ) − β(τ
                                           β(l       b ) − (b   γ (l1 τ ) − γ
                                                                            b(τ ))δ
                                                            ...
                                                                                   
                                gn (δ) = 
                                         
                                                                                    
                                                                                    
                                          β(lJ τ ) − β(τ ) − (b
                                           b         b          γ (lJ τ ) − γ
                                                                            b(τ ))δ

and let Wn be a Jd × Jd positive definite symmetric matrix. We estimate δ using a minimum
distance procedure:
                                           δb = arg min gn (δ)0 Wn gn (δ).                                     (3.3)
                                                      δ

Finally, we estimate β by averaging across the quantile indices:

                                                      J
                                                1 X b
                                       βb =        −β(lj τ ) + γ
                                                               b(lj τ )δ,
                                                                       b
                                              J +1
                                                     j=0


with l0 = 1. We do not simultaneously estimate β and δ since the corresponding estimators of
β and δ would have different rates of convergence, thus implying that the standard asymptotic
theory of minimum distance estimators would not apply in this context. In particular, this
framework would lead to a singular optimal weighting matrix. Intuitively, only the terms with
the slowest rate of convergence would be weighted positively, since the other terms would not
matter asymptotically. We would then lose consistency of the estimators.

Our estimators depend on a choice of τ , (l1 , .., lJ ) and Wn . We derive in the following section
the optimal weighting matrix, which can be consistently estimated. Regarding the quantile
indices, while the choice of the constants (l1 , .., lJ ) does not appear to matter much in practice,
an appropriate choice of τ is crucial to balance bias and variance in such a way that guaran-
tees our estimators to be asymptotically normal with zero mean. We propose a data-driven
procedure for that purpose in Section 3.3.

We now turn to the estimation of the quantile effects. ∆jτ is point identified when δj = 0 and
partially identified otherwise. Moreover, as shown in Section 3.2, it is possible to estimate βj
more precisely in the case where δj = 0. We consider these two cases separately, noting that
the restriction δj = 0 can be tested using the asymptotic distribution of δbj provided in the
following section.
   7
       Assumptions 3-(i) and 8 below ensure that the latter condition is satisfied for any l 6= 1 and τ small enough.



                                                           12
                                                                                  β      d
Suppose first that the model is partially homoskedastic, in the sense that {δjk }k=1 are equal to
                                                                                                 β     d
zero for some d ≥ dβ ≥ 1 and {j1 , ..., jdβ } ⊂ {1, ..., d}. Then the quantile effects of {Xjk }k=1
correspond to the average effects of the corresponding covariates, which are identified and
                d
                β
equal to {βjk }k=1 . Let Ψ be a dβ × d matrix that picks out the corresponding subvector of β,
i.e. β 1 = Ψβ and let us consider
                                                                                0
                    g1n (β 1 ) = Ψβ(τ
                                  b ) − β 1 , Ψβ(l
                                               b 1 τ ) − β 1 , ..., Ψβ(l
                                                                     b J τ ) − β1 .


We then propose to estimate β 1 by

                                  βb1 = arg min g1n (β 1 )0 W1n g1n (β 1 ),
                                              β


for some positive definite matrix W1n . We also estimate the subvector δ 1 of nonzero compo-
                    e denote the matrix such that δ 1 = Ψδ
nents of δ. Letting Ψ                                   e and

             h               i                                  h               i                        0
g2n (δ 1 ) = Ψ  b 1 τ ) − β(τ
             e β(l        b ) − [b           b(τ )] δ 1 , ..., Ψ
                                 γ (l1 τ ) − γ                     b J τ ) − β(τ
                                                               e β(l         b ) − [b           b(τ )] δ 1 ,
                                                                                    γ (lJ τ ) − γ

we estimate δ 1 by
                                  δb1 = arg min g2n (δ 1 )0 W2n g2n (δ 1 ),
                                               δ

for some positive definite matrix W2n .

Finally, if we reject partial homoskedasticity so that the quantile effects are only partially
identified, one possibility would be to estimate the sharp bounds given in Theorem 2.1. As
mentioned previously, however, to the best of our knowledge one cannot conduct inference on
these bounds using available methods. Instead, we propose to use the simpler outer bounds
given by (2.3). These bounds can be consistently estimated using plug-in estimators, replacing
(β, δ) by their estimators (β,
                            b δ)
                               b and (P (D = 1|X = x), QY |D=1,X=x ) by any given consistent
nonparametric estimator, e.g., kernel or local polynomial estimators.


3.2    Asymptotic properties

We now turn to the asymptotic properties of (β,
                                             b δ).
                                                b We rely for that purpose on the asymptotic
properties of extremal quantile regressions, established by Chernozhukov (2005). As already
discussed, an important difference is that (3.2) is an equivalence rather than an equality. This
implies that a bias term comes into play, which needs to be controlled for.

In addition to the previous Assumptions 1-4, our asymptotic analysis relies on the three



                                                     13
conditions discussed below. In the following, we let
                                 "                                       #
                      f (γ) = E sup |h − P (D = 1|X, −ε = u)| × kXk .
                                     u≤γ


Assumption 7. (i.i.d. sampling) (Di , Yi , Xi )i=1...n are independent, with the same distribu-
tion as (D, Y, X).

Assumption 8. (Monotone densities) There exists A < 0 such that almost surely, F−ε and
F−ε|D=1,X are differentiable with increasing derivatives on (−∞, A).

Assumption 9. (Rate of convergence of the quantile index) τn satisfies, as n → ∞, (i)
                               √
τn → 0, (ii) τn n → ∞ and (iii) τn nf (γ(τn )) → 0, where γ(τn ) = Q−ε (τn /h).

Assumption 8 rules out erratic behavior of the densities in the tail. It is very mild and satisfied
by all standard distributions. Assumption 9 is an important condition that restricts the rate
of convergence of the tail index τn . Conditions (i) and (ii) basically ensure that the number
of observations that are useful for inference, which is proportional to τn n, tends to infinity,
but at a slower rate than the sample size. Thus, following the standard terminology in order
statistics theory, our estimators are based on quantile regressions where τn is an intermediate
order sequence, which we will refer to as intermediate order quantile regressions. The reason
why we use intermediate order instead of extreme order sequences, where τn n tends to a non-
zero constant, is that in the latter case, δ,
                                           b and thus β,
                                                      b are not consistent. Intuitively, this is
due to the fact that only a finite number of observations are useful in the extreme order case.
Intermediate order quantile theory also has the nice feature that it guarantees asymptotic
normality rather than convergence towards a non-standard, data-dependent, distribution (see
Chernozhukov, 2005 and Chernozhukov & Fernandez-Val, 2011, in the absence of sample
selection). Finally, Condition (iii) is specific to our context. This is an undersmoothing
condition, which ensures that the bias arising because (3.2) is an equivalence rather than an
equality vanishes quickly enough.

Importantly, under Assumption 4, there always exists a τn satisfying Assumption 9. Specif-
ically, for any α ∈ (0, 1) define G(γ) = F−ε (γ)f (γ)2(1−α) , where f (.) was introduced at the
beginning of the section. By construction, f is increasing. Because F−ε is strictly increasing
on (−∞, A) by Assumption 8, G is also strictly increasing on (−∞, A). Then define, for n
large enough,
                                       τn∗ = hF−ε ◦ G−1 (1/n).                               (3.4)




                                                 14
Under Assumption 4, limγ→−∞ f (γ) = 0.8 Thus, limγ→−∞ G(γ) = 0. This implies that
limn→∞ G−1 (1/n) = −∞, ensuring that τn∗ satisfies Condition (i). Moreover, it follows from
                               G(γ)
the equality F−ε (γ) =     f 2(1−α) (γ)
                                          that

                                                              h/n
                                           τn∗ =                          ,
                                                   f 2(1−α)   ◦ G−1 (1/n)

which implies that Condition (ii) holds as well. Finally, by using this expression again and
noting that γ(τn∗ ) = G−1 (1/n), we get
                                              √
                                                h × f ◦ G−1 (1/n) √ α
                       nτn∗ f (γ(τn∗ ))                           = hf ◦ G−1 (1/n),
                   p
                                          =
                                              f (1−α) ◦ G−1 (1/n)

so that Condition (iii) is also satisfied. An obvious issue is that such a τn∗ depends on F−ε
and f , both of which are unknown to the researcher. We shall come back to the issue of the
practical choice of τn in Section 3.3.

The main result of this section is stated in Theorem 3.1 below, which shows that the estimators
of β and δ are consistent and asymptotically normal, and characterizes their asymptotic
variances. We first need to introduce several matrices. First, let L be the matrix of typical
            l   ∧l
term Li,j = √i−1 j−1 for (i, j) ∈ {1, ..., J +1}2 . Second, let ∆ = [−δ, Id ], where Id denotes the
              li−1 lj−1
                                                        √         √
identity matrix of size d. Define Γ = [−ιJ , diag(1/ l1 , ..., 1/ lJ )] ⊗ Id+1 , where ιJ denotes
the column vector of ones of size J and, for any vector v, diag(v) denotes the diagonal matrix
                                                                          h                  i
                                                                                0
with diagonal v. Finally, let G = (log(l1 ), ..., log(lJ ))0 ⊗ Id , QH = E Xi Xi /(1 + Xi0 δ) and
Ω0 = Q−1    −1
      H QX QH .
                                                                              p
Theorem 3.1. Under Assumptions 1-4 and 7-9, and if Wn −→ W symmetric positive definite
and nonstochastic,

                                       √               d
                                         τn n(δb − δ) −→ N (0, Ωδ )
                                      √
                                        τn n b         d
                                             (β − β) −→ N (0, Ωδ )
                                      γ
                                      b(τn )

where Ωδ = (G0 W G)−1 G0 W (IJ ⊗∆)Γ(L⊗Ω0 )Γ0 (IJ ⊗∆0 )W G(G0 W G)−1 . The optimal weight-
ing matrix is Wδ∗ = ((IJ ⊗ ∆)Γ(L ⊗ Ω0 )Γ0 (IJ ⊗ ∆0 ))−1 and the corresponding asymptotic
variance is Ω∗δ = (G0 Wδ∗ G)−1 . Finally, there exists τn0 satisfying Assumption 9 such that
- βb is consistent if, for some a > 1, f (u) = o(|u|−a ) as u → −∞.
- The rates of convergence of δb and βb are polynomial if for some a > 0, f (u) = o(F−ε (u)a )
  8
    To see this, note that for any x, supu≤γ |h − P (D = 1|X = x, −ε = u)| tends to zero by Assumption 4.
Because this term is bounded by 2, f tends to zero by the dominated convergence theorem.


                                                         15
as u → −∞.9

A consistent estimator of the asymptotic variance Ωδ can be obtained by replacing W by Wn ,
∆ by ∆
     b = [−δ,b Id ] and Ω0 by Ω     b−1 Q
                               b0 = Q       b−1 , with
                                         bX Q
                                                      H       H

                                           n                          n
                           bX = 1        0                bH = 1        0
                                  X                              X
                           Q        Xi Xi ,               Q        Xi Xi /(1 + Xi0 δ).
                                                                                   b
                                n                              n
                                          i=1                        i=1


Similarly, one can consistently estimate Wδ∗ , and thus obtain a two-step estimator that is
optimal in the class of estimators considered here.

Theorem 3.1 shows that δb converges more quickly than βb towards the true value, since
|γ(τn )| → ∞ as τn → 0. Actually, even though one can always construct asymptotically
valid confidence intervals on β, βb may not be consistent. Consistency is secured, however, if
f decreases to zero quickly enough. To understand what this condition means, it is useful to
discuss it in the context of the sample selection model defined by Assumption 5. In such a
case, letting x = [1, x0 ]0 , we have

       f (γ(τ )) ≤     sup       ||x||          sup          |P (D = 1|X = x, −ε = u) − 1|
                     x∈Supp(X)           x∈Supp(X),u≤γ(τ )

                =      sup       ||x||          sup          |P (η ≤ φ(x)| − ε = u) − 1|
                     x∈Supp(X)           x∈Supp(X),u≤γ(τ )

                =      sup       ||x||          sup          |P (Fη (η) ≤ Fη (φ(x))|F−ε (−ε) = F−ε (u)) − 1|
                     x∈Supp(X)           x∈Supp(X),u≤γ(τ )

                ≤      sup       ||x|| fC (τ ),
                     x∈Supp(X)


where the second inequality follows from fC (τ ) = supu≤τ,v∈[v,1] |∂1 C(u, v) − 1|. Hence, con-
sistency of βb is achieved if fC (τ ) = o(|γ(τ )|−a ) for some a > 1. Similarly, if, for some b > 0,

                                                      fC (τ ) = o(τ b ),                                  (3.5)

then a polynomial rate of convergence, faster than n(b−ζ)/(2b+1) for any b > ζ > 0, is possible.
Table 2 below provides examples of copulas of (η, −ε) satisfying the latter condition (see Ap-
pendix C.7 for its verification in each case), and therefore copulas for which βb is consistent.10
It is worth noting that for the last two copulas considered in the table, we actually establish
that fC (τ ) tends to zero exponentially fast in τ . In such situations, (3.5) holds for all b, and
   9
     Assumption 3 implies that for all a > 0, F−ε (u) = o(|u|−a ). Thus, the condition f (u) = o(F−ε (u)a ) is
stronger than the one ensuring consistency of β,  b as expected.
  10
     See, e.g., Nelsen (2006) for a detailed review of copulas and their properties.



                                                              16
it is possible to achieve a rate of convergence for δb and βb that is faster than n1/2−ζ for any
ζ > 0. In other words, an adequate choice of τn can make the rate of convergence arbitrarily
close to the standard parametric root-n rate.

                                                                                        Restriction
 Copula family                                                                         ensuring (3.5)

 Gaussian C(u, v; ρ)                                                                          ρ>0
 Clayton C(u, v; θ) = max ([u−θ + v −θ − 1]−1/θ , 0)                                          θ>0
 Rotated Gumbel-Barnett C(u, v; θ) = u − u(1 − v) exp(−θ log(u) log(1 − v))               θ ∈ (0, 1]
 C(u, v; θ) = (1 + [(u−1 − 1)θ + (v −1 − 1)θ ]1/θ )−1                                         θ>1
 C(u, v; θ) = (1 + [(u−1/θ − 1)θ + (v −1/θ − 1)θ ]1/θ )−θ                                     θ≥1
 C(u, v; θ) = θ/ log(exp(θ/u) + exp(θ/v) − exp(θ))                                            θ>0
 C(u, v; θ) = [log(exp(u−θ ) + exp(v −θ ) − e)]−1/θ                                           θ>0



          Table 2: Examples of copulas leading to a polynomial rate of convergence.


In order to conduct inference on the quantile effects, we need to distinguish between the
partially homoskedastic case and the heteroskedastic case. This involves (pre)testing the
restriction δj = 0. Valid inference requires that the critical value of the corresponding t-test
depend on the sample size n, so that the level of the test tends to zero while the power tends
to one. A possibility is to choose the critical values cn so that cn → ∞, but slowly enough so
         √                                                       p
that cn / n → 0. In practice, we use in our application cn = log(n), which is advocated in
different contexts by Andrews (1999) and Andrews & Soares (2010).

We first consider the partially homoskedastic case. As before, we let Gδ = (log(l1 ), ..., log(lJ ))0 ⊗
                                                               √            √
Id−dβ , Gβ = −ιJ+1 ⊗ Idβ , Γ2 = (0, Id ) and Γ3 = diag(1, 1/ l1 , · · · , 1/ lJ ). Finally, we let
λ
bn = γ               γ (mτn ) − γ
      b(τn ) log(m)/(b          b(τn )), where m 6= 1 denotes an arbitrary positive constant.
                                                   β        d                             p
Theorem 3.2. Under Assumptions 1-4 and 7-9, {δik }k=1 are zeros and if W1n −→ W1 and
      p
W2n −→ W2 , where W1 and W2 are symmetric positive definite and nonstochastic, then

                                  √                   d
                                    τn n(δb1 − δ 1 ) −→ N (0, Ωδ1 )
                                 √
                                   τn n b1            d
                                        (β − β 1 ) −→ N 0, Ωβ 1 ,
                                                                  
                              λn
                              b
                                 γ
                                 b(τn )




                                                 17
where

                                 −1
                     G0δ W2 Gδ         G0δ W2 (IJ ⊗ Ψ∆)Γ(L                   e 0 )W2 Gδ G0 W2 Gδ −1 ,
                                                           ⊗ Ω0 )Γ0 (IJ ⊗ ∆0 Ψ
                                                                                              
      Ωδ 1   =                                      e
                                                                                         δ

      Ωβ 1   = (G0β W1 Gβ )−1 G0β W1 (Γ3 ⊗ ΨΓ2 )(L ⊗ Ω0 )(Γ03 ⊗ Γ02 Ψ0 )W1 Gβ (G0β W1 Gβ )−1 .

                                                              h                                   i−1
The optimal weighting matrices for δb1 and βb1 are then Wδ∗1 = (IJ ⊗ Ψ∆)Γ(L
                                                                     e      ⊗ Ω0 )Γ0 (IJ ⊗ ∆0 Ψ
                                                                                              e 0)
and Wβ∗1 = [(Γ3 ⊗ ΨΓ2 )(L ⊗ Ω0 )(Γ03 ⊗ Γ02 Ψ0 )]−1 , and the corresponding asymptotic variances
                                                                      p
are given by (G0 W ∗1 Gδ )−1 and (G0 W ∗1 Gβ )−1 . Finally, |λ
                      δ   δ                    β   β
                                                                bn | −→  ∞, so that the rate of
convergence of βb1 is faster than the one of the unconstrained estimator β.
                                                                         b

Finally, in the case where we reject δj = 0, we propose to construct a confidence interval
on the quantile effect ∆jτ based on the outer bounds given by (2.3). Importantly, because
                                       o
the quantities Qoε (τ ) and Qε (τ ) are defined as supremum and infimum of functions that have
to be estimated, we can apply the methodology developed by Chernozhukov et al. (2013)
to conduct inference on intersection methods. More details on the construction of these
confidence intervals in our context are provided in Appendix A.1.


3.3     Choice of the quantile index

The estimators of β and δ are asymptotically normal with zero mean provided that they are
based on a sequence of quantile indices τn satisfying the bias-variance trade-off of Assumption
9. Though there always exists a sequence τn satisfying Assumption 9 under Assumption 4,
admissible rates of convergence towards 0 for τn are unknown, since they depend on f (γ(τn )),
which is itself unknown. A related issue arises in the estimation at infinity of the intercept
of sample selection models (see Andrews & Schafgans, 1998 and Schafgans & Zinde-Walsh,
2002) or in the estimation of extreme value index (see Drees & Kaufmann, 1998 and Danielsson
et al. , 2001). We propose in the following a heuristic data-driven method, which consists of
selecting τn as the minimizer of a criterion function that represents the trade-off between bias
and variance. The innovative idea here is to combine a subsampling method with a minimum
distance estimator to produce a proxy of the bias.

Specifically, let us consider the J test statistic

                                               log(m)2 τ n        b ))0 W
                                                                        c ∗ gn (δ(τ
                              TJ (τ ) =                       gn (δ(τ     δ
                                                                                b )),
                                                      b(τ ))2
                                            γ (mτ ) − γ
                                           (b

for some arbitrary fixed m > 1. Here δ̂(τ ) is the estimator obtained using the quantile index
τ and Wc ∗ is an estimator of W ∗ using δ̂(τ ). We prove in Appendix A.2 that if τn satisfies
         δ                      δ



                                                         18
Assumption 9, TJ (τn ) converges to a chi-square distribution with (J − 1)d degrees of freedom
as n grows to infinity. We also show that otherwise, the asymptotic distribution of the J
test statistic includes an additional term. Heuristically, this suggests in particular that if the
median of the J test statistic is close enough to the median of a chi-square distribution with
(J − 1)d degrees of freedom, denoted by M(J−1)d , then the bias term should be small. Our
data-driven procedure builds on this idea.

In practice, we propose to estimate the difference between the two medians using subsampling.
For each subsample and each quantile index τ within a grid defined below, we compute TJ (τ ).
Then, letting Ms (τ ) denote the median of these test statistics over the different subsamples
and for a given τ , we compute

                                               Ms (τ ) − M(J−1)d
                                 diff
                                 c n (τ ) =           √          ,
                                                        bn τ

where bn denotes the subsample size.

Similarly, the asymptotic variance is estimated by the variance of the subsampling point
estimates of δ multiplied by the normalizing factor bn /n. We call this estimator Var
                                                                                  d n (τ ). At
the end, we select the quantile index as follows:

                                 τbn = arg min Var
                                               d n (τ ) + diff
                                                          c n (τ ).
                                           τ


We thus base our procedure on the trade-off between the variance and our proxy of the bias. It
follows that we achieve undersmoothing in comparison with a more standard trade-off between
variance and squared bias. Note that, similarly to the case of nonparametric regressions, this
is needed to control the asymptotic bias that would otherwise affect the limiting distribution
of our estimator.

We implement this method by searching over a grid of τ on an interval. In practice, we set
the upper bound of this interval to 0.3 and the lower bound to 80/bn . This lower bound is
motivated by the fact that if the effective subsampling size τ bn becomes too small, then the
intermediate order asymptotic theory is likely to be a poor approximation (see Chernozhukov
& Fernandez-Val, 2011 for a related discussion). Finally, we select the quantile indices for
estimating β and δ in the partially homoskedastic case in the same manner.




                                                 19
4    Simulations
In this section, we investigate the finite-sample performances of our estimation procedure by
simulating the following model for four different sample sizes (n = 250, n = 500, n = 1, 000
and n = 2, 000):

                   Y ∗ = β1 X1 + β2 X2 + β3 X3 + (1 + δ1 X1 + δ2 X2 + δ3 X3 )ε
                    D = 1 0.6 + Y ∗ + 0.3X1 + 0.2X2 + X32 + η ≥ 0 .
                           


X1 and X2 are two mutually exclusive binary variables, such that X1 = 1{U ≤ 0.3} and
X2 = 1{U ≥ 0.8}, with U uniformly distributed over [0, 1]. X3 is drawn from a truncated
normal distribution with support [−1.8, 1.8], mean 0 and standard deviation! 1. (ε, η) are
                                                                        1 0.2
jointly normally distributed, with mean zero and covariance matrix               . Finally, the
                                                                       0.2 1
true values of the parameters are given by: β1 = 0.2, β2 = 0.4, β3 = 0.5, δ1 = 0, δ2 = 0.1 and
δ3 = −0.3.

We report in Table 3 below, for each sample size, the bias and standard deviation for nine
different estimators. Namely, we first estimate (δ1 , δ2 , δ3 ) and (β1 , β2 , β3 ) without imposing
δ1 = 0. Then we impose δ1 = 0 and reestimate β1 along with (δ2 , δ3 ) under this partial
homoskedasticity constraint. As shown in Section 3.2 (Theorem 3.2), β1 is estimated at a
faster convergence rate in the latter case. In both cases, we use the two-step, asymptotically
optimal estimators of β and δ. We also document the severity of the selection bias in this
context by reporting the bias and standard deviation of a naive OLS estimator of β1 only
using the observations such that D = 1. Throughout this section we pay special attention to
the performances of our constrained estimator of β1 since, in our application, the black-white
wage gap will be estimated similarly.

The vector of spacing parameters lj used in minimum distance estimation is set equal to (0.65,
0.85, 1.15, 1.45). Intuitively, these parameters have to differ sufficiently to provide enough
variation. At the same time, they should not be too large, otherwise the corresponding
quantile indices τn lj might escape from the extremal quantiles region. However, in practice,
our estimates do not appear to be meaningfully sensitive to the choice of l. The choice of the
quantile index τn is more critical. We choose this parameter using the data-driven method
discussed in Section 3.3, with subsample sizes (150, 300, 500, 600) corresponding to the four
total sample sizes (250, 500, 1, 000 and 2, 000) and 500 subsamples in each case. We report
in Table 3 below the average quantile indices computed across all simulations.



                                                20
                                 Heteroskedastic                      Homoskedastic (δ1 = 0)        OLS
                  δ1      δ2       δ3     β1         β2       β3       δ2     δ3       β1            β1
  True Value      0       0.1     -0.3    0.2        0.4      0.5      0.1   -0.3      0.2          0.2
      n=250
        Bias     0.070   0.104    0.086   -0.018   -0.053    -0.054   0.066 0.065        0.021     -0.075
     Std dev     0.305   0.395    0.148    0.252   0.318      0.099   0.430 0.154        0.187      0.152
  Average τn             0.256                      0.256                 0.236          0.207
      n=500
        Bias     0.073   0.074    0.064   -0.041   -0.051    -0.040   0.012 0.053        0.012     -0.076
     Std dev     0.260   0.358    0.128    0.208   0.283      0.098   0.334 0.124        0.137      0.102
  Average τn             0.220                      0.220                 0.209          0.201
    n=1,000
        Bias     0.023   0.025    0.031   -0.018   -0.013    -0.019   0.004 0.032       -0.010     -0.078
     Std dev     0.192   0.230    0.082    0.176   0.211      0.069   0.241 0.083       0.089       0.072
  Average τn             0.203                      0.203                 0.201         0.208
    n=2,000
        Bias     0.020   0.045    0.020   -0.009   -0.035    -0.015   0.008 0.011        0.000     -0.077
     Std dev     0.134   0.192    0.064    0.126   0.171      0.055   0.175 0.051        0.062      0.054
  Average τn             0.191                      0.191                 0.185          0.203
Note: Results were obtained using 300 simulations for each sample size.

                                   Table 3: Monte Carlo simulations


Importantly, for each sample size, the bias-standard deviation ratio for each estimator is
much smaller than 1, consistent with our data-driven choice of τn leading to undersmoothing.
Besides, the standard deviations of our estimators as well as the average τn generally decrease
with the sample size, as expected given the consistency of our estimators and the bias-variance
tradeoff underlying the choice of τn .11 In practice, our estimators exhibit a fairly small bias
for sample sizes larger than n = 500. Note also that the constrained version of the estimator
of β1 , which makes use of the partial homoskedasticity constraint δ1 = 0, is much more precise
and yields a smaller bias (except for n = 250) than the unconstrained estimator. The OLS
estimator of β1 , on the other hand, displays a large bias, which remains stable across all
sample sizes.

We report in Figure 1 below the QQ-plots of β1 , after imposing homoskedasticity (δ1 = 0),
for all four sample sizes. The plots are generally close to the diagonal line, which shows
that the estimator of β1 is approximately normally distributed, even for small sample sizes.
Importantly, this provides evidence that it is in practice reasonable, even in small samples, to
conduct inference based on the asymptotic distributions of our estimators.
  11
     An exception to the overall decrease in τn with the sample size is the slight increase of τn between n = 500
and n = 1, 000 for the constrained estimator of β1 . τn is essentially stable across all sample sizes for this
estimator.




                                                       21
                               n = 250                                    n = 500
               4                                            4


               2                                            2


               0                                            0


              −2                                           −2


              −4                                           −4
               −4        −2       0        2      4         −4     −2       0        2        4

                               n = 1,000                                 n = 2,000
               4                                            4


               2                                            2


               0                                            0


              −2                                           −2


              −4                                           −4
               −4        −2       0        2      4         −4     −2       0        2        4
                   Note: The crosses represent the quantiles of the standard normal
                   distribution, against the quantiles of the empirical distribution of the
                   studentized estimates of β1 .

                         Figure 1: QQ-plots (constrained estimator of β1 )


Figure 2 below shows the evolution of the Mean Squared Error (MSE) of the constrained
estimator of β1 , with respect to the quantile index τn . The vertical line corresponds to
the average τn (across simulations) chosen based on our data-driven method. The plots
corresponding to n = 1, 000 and n = 2, 000 exhibit a U-shaped relationship between the MSE
and the quantile index. This pattern reflects a bias-variance tradeoff with respect to the
choice of τn . When the quantile index is small, the bias is small but the variance is large, and
vice versa. On the other hand, the relationship between MSE and τn is mostly decreasing for
n = 250 and n = 500. This is consistent with the variance term dominating the bias term for
τn < 0.3 and such small sample sizes. For all sample sizes, the average quantile index selected
with our method is generally smaller than the one yielding the smallest MSE, consistent with
our data-driven method tending to undersmooth. However, for sample sizes larger than 500,
it is worth noting that the MSE evaluated at the average selected quantile index is close to
the minimum (even equal for n = 2, 000).




                                                      22
                              n = 250                                              n = 500
       0.04                                             0.025


      0.035
                                                            0.02

       0.03

                                                        0.015
      0.025


       0.02                                                 0.01
          0.1          0.15      0.2      0.25   0.3           0.1          0.15      0.2      0.25   0.3


              x 10
                  −3          n = 1,000                            x 10
                                                                       −3          n = 2,000
        12                                                    7

        11
                                                              6
        10

         9                                                    5

         8
                                                              4
         7

         6                                               3
         0.1      0.15      0.2      0.25      0.3        0.1      0.15    0.2     0.25    0.3
          Note: The solid vertical line is the average quantile index produced by our
          data-driven method. The solid curve plots the MSE of our estimator as a function
          of the quantile index τn .

Figure 2: Relationship between MSE (Y-axis) and τn (X-axis), constrained estimator of β1




                                                       23
                           n = 250                                         n = 500
           1                                                1

         0.95                                             0.95

          0.9                                              0.9

         0.85                                             0.85

          0.8                                              0.8

         0.75                                             0.75
            0.1     0.15      0.2      0.25    0.3           0.1   0.15      0.2      0.25     0.3

                           n = 1,000                                      n = 2,000
           1                                                1

         0.95                                             0.95

          0.9                                              0.9

         0.85                                             0.85

          0.8                                              0.8

         0.75                                             0.75
            0.1      0.15       0.2      0.25     0.3        0.1      0.15     0.2     0.25    0.3
              Note: The solid vertical line is the average quantile index produced by our
              data-driven method. The horizontal dashed and solid lines represent the 97.5%
              and 95% nominal coverage rates, respectively. The dashed and solid curves
              represent the coverages of the 95% and 97.5% confidence intervals, as a function
              of the quantile index τn .

Figure 3: Relationship between coverage (Y-axis) and τn (X-axis), constrained estimator of
β1


Finally, we examine in Figure 3 the relationship between the coverage of the 95% and 97.5%
confidence intervals constructed with our constrained estimator of β1 and the quantile index
τn . The confidence intervals are constructed using the asymptotic variance of our estimator
together with normal critical values.12 The coverage gets reasonably close to the nominal rates
for n = 500, and remarkably close for larger sample sizes, for values of τn in the neighborhood
of the average quantile index obtained with our data-driven method. The sharp decline in
coverage for large values of the quantile index for n = 1, 000 and n = 2, 000 reflects the
existence of a nonvanishing bias. For smaller sample sizes, in particular for n = 500, the
coverage decreases for small values of the quantile index. This may be due to the fact that
the quantile index falls in the extreme, rather than intermediate order region, in which case
the finite sample distribution of our estimator cannot be approximated well by a standard
  12
     We also computed confidence intervals based on percentile bootstrap or subsampling. Overall, the method
based on normal critical values performed best in terms of coverage.


                                                     24
normal distribution.As sample size increases, however, the extreme order region moves even
closer to the origin, and for all the values of τn we consider the sampling distribution remains
close enough to a normal distribution. Importantly, for all sample sizes except n = 250,
the quantile index obtained from our data-driven procedure appears to be relatively close to
optimal, in the sense of minimizing the discrepancy between the actual and nominal coverages.
For the smallest sample size (n = 250) though, our procedure yields a value of the quantile
index that appears to be suboptimal, both in terms of coverage and MSE (see Figure 2 above).
This is presumably due to the fact that, in this context, the variance term strongly dominates
the bias term. In any case, the results reported in Table 3 are quite encouraging since they
show that, even with such a small sample, the bias and variance of our constrained estimator
of β1 remain reasonable.


5         Application to the black-white wage gap
We apply our method to the estimation of the black-white wage gap among young males
for two groups of cohorts, using data from the National Longitudinal Survey of Youth 1979
(NLSY79) and National Longitudinal Survey of Youth 1997 (NLSY97). Individuals surveyed
in the NLSY79 were 14 to 22 years old in 1979, while individuals from the NLSY97 were
12 to 16 years old in 1997. In the following, we are interested in estimating the black-white
wage gap for these two groups of individuals as of 1990-1991 and 2007-2008, respectively. As
noted in early articles by Butler & Heckman (1977) and Brown (1984), and documented more
recently by Juhn (2003), among males, blacks are significantly more likely to dropout from
the labor market. To the extent that those dropouts tend to have lower potential wages, it
follows that failure to control for endogenous labor market participation is likely to result
in underestimating the black-white wage differential. It is worth noting that finding a valid
instrument for selection is particularly difficult in the context of male labor force participation.
As a result, most of the attempts to deal with selection have consisted of imputing wages for
non-workers (see, among others, Brown, 1984, Smith & Welch, 1989, Neal & Johnson, 1996,
Juhn, 2003, Neal, 2004, Neal, 2006 and Neal & Rick, 2014).

Importantly, since across-cohort changes in selection into the workforce is also different for
blacks and for whites, adequately dealing with selection is needed to obtain credible estimates
of the across-cohort evolution of the black-white wage gap. Altonji & Blank (1999) stress the
importance of correcting for changes in race differential selection into work, and review some
of the empirical literature addressing this issue.13
    13
         As the authors put it, “Comparisons of average or median wages of persons with jobs do not provide an



                                                        25
5.1    Evidence from the NLSY79

We first use our method to estimate the black-white wage gap among young males from the
NLSY79, revisiting the influential work of Neal & Johnson (1996) on this question. We use
the same sample as Neal & Johnson (1996) in our analysis, and consider as they did that an
individual is a nonparticipant if he did not work in 1990 nor in 1991. The total sample size
is n = 1, 674, with an overall labor force participation rate over the period of interest (1990-
1991) equal to 95%. We refer the reader to Neal & Johnson (1996) for a detailed discussion
on the data.

We start by replicating the results of Neal & Johnson (1996) in Table 4 below by running four
regressions on the log of hourly wages on a set of observable characteristics, namely black,
Hispanic dummies and age (specifications (1) and (3)), together with AFQT and AFQT
squared (specifications (2) and (4)). The first two columns contain the results of simple OLS
regressions, replicating Columns (1) and (3) in Table 1 of Neal & Johnson (1996) (p.875),
while in the last two columns we replicate their Table 4 (p.883) by imputing a zero log-
wage for nonparticipants and running a median log-wage regression. As discussed in Neal
& Johnson (1996) and more extensively in Johnson et al. (2000), this imputation method
yields consistent estimates under the assumption that, conditional on the set of observable
characteristics included in the regression, the potential wage for any individual who did not
work neither in 1990 nor in 1991 lies below the median. It is important to note that the
identifying condition of independence at infinity used in our paper (Assumption 4) relaxes
this assumption by replacing the median with some extremal quantile of the conditional wage
distribution.14 As is put forward by Neal & Johnson (1996), Columns (1) and (2) show that
the estimated black-white wage gap drops sharply, from 24.4% to 7.1%, after adding controls
for ability, namely AFQT and AFQT squared. It is also worth noting that the estimated
black-white wage differential changes substantially, increasing (in absolute value) by as much
as 6.4 points, after addressing the selection issue with the imputation method proposed in
Neal & Johnson (1996) (see Columns (2) and (4)).
accurate picture of changes in the offer distributions faced by black and by white workers” (pp. 3240). See
also Juhn (2003), who provides evidence that the evolution over the period 1969-1998 of the black-white wage
gap is severely biased if one does not take into account the decline in work participation rates of black men
relative to white men. In recent work, Neal & Rick (2014) show that the growth in prison populations in the
last decades is an important factor behind the evolution of differential workforce participation of blacks and
whites.
  14
     Our identifying condition is also weaker in the sense that h does not need to be equal to 1.




                                                     26
                                        (1)        (2)         (3)         (4)
                         Black        -0.244     -0.071      -0.356      -0.135
                                     (0.026)    (0.027)     (0.028)     (0.034)
                       Hispanic       -0.114      0.005      -0.181      -0.013
                                     (0.030)    (0.030)     (0.033)     (0.038)
                          Age          0.048      0.040       0.068       0.055
                                     (0.014)    (0.013)     (0.016)     (0.017)
                        AFQT                      0.173                   0.206
                                       —–                     —–
                                                (0.012)                 (0.015)
                        AFQT2                    -0.013                   0.010
                                       —–                     —–
                                                (0.011)                 (0.014)
                      Note: Standard errors are reported in parentheses.

              Table 4: OLS and median log-wage regression results (NLSY79)

We now investigate how the above results are changed when we use our estimation method
and implement the two-step asymptotically optimal estimators of δ and β. Table 5 presents
the estimation results for the heteroskedasticity parameters δ and the parameters β. Since we
fail to reject homoskedasticity for all the covariates with the exception of age, we report both
the corresponding unconstrained (“Heteroskedastic”) and constrained (“Homoskedastic”) es-
timates of β. In the discussion below we focus on our preferred constrained estimates, which
have a structural interpretation in terms of average effects.

                                      Heteroskedastic        Homoskedastic
                                          δ       β                 β
                          Black         0.019  -0.215            -0.101
                                      (0.029) (0.326)           (0.010)
                        Hispanic        0.005   0.014             0.020
                                      (0.030) (0.337)           (0.012)
                           Age         -0.029  0.215
                                                                    —–
                                      (0.003) (0.032)
                         AFQT          -0.005  0.238               0.215
                                      (0.012) (0.134)             (0.005)
                         AFQT2         -0.007  0.030               0.000
                                      (0.011) (0.121)             (0.004)
                        Note: Standard errors are reported in parentheses.
                       We perform the homoskedasticity
                                            p               tests using the crit-
                       ical values cn =       log(n), where n is the sample size
                       (n = 1, 674 here). The vector of spacing parame-
                       ters lj used in minimum distance estimation is equal to
                       (0.65, 0.85, 1.15, 1.45). The quantile index τn is chosen
                       based on the data-driven procedure discussed in Section
                       3.3, using 500 subsamples of size 550.

                   Table 5: Extremal quantile regression results (NLSY79)


                                                  27
The estimation results from our extremal quantile method show that the size of the black-white
wage gap (10.1%) is smaller than the estimated gap obtained under the imputation method
proposed by Neal & Johnson (1996) (13.5%), but larger than the gap estimated using simple
OLS (7.1%). The fact that our preferred estimate of the black-white wage gap is smaller than
the one obtained with the imputation method is consistent with our estimator being based on
a weaker identifying assumption. While Neal & Johnson (1996) assume that, conditional on
observed characteristics, those individuals who do not participate to the labor market have a
potential wage below the median, a sufficient condition to apply our method is to rule out the
possibility that non-participants have arbitrarily large potential wages. Intuitively it follows
that our approach results in a milder form of selection correction, which is consistent with
our findings.

Finally, it is worth stressing that our results are in line with the key takeaway of Neal &
Johnson (1996), namely that premarket factors, as measured here by AFQT, account for most
of the black-white wage differential. In fact, our results point to an even more important role
played by AFQT, since the estimated wage gap drops from close to the median regression
estimate (around 35%) to 10.1% after adding AFQT.15


5.2    Across-cohort evolution

We now examine the evolution across the NLSY79 and NLSY97 cohorts of the black-white
wage gap. To do so, we apply our method to estimate the wage gap using hourly wages
measured in 1990-1991 for the NLSY79 sample and in 2007-2008 for the NLSY97 sample.
We follow Altonji et al. (2012) by using a modified version of the AFQT variable, which
corrects for the across-cohort changes in the ASVAB test format as well as in the age ranges
at which the test was taken. This age correction procedure is based on an equipercentile
mapping. To the extent that the rank within the AFQT distribution may vary with the age
of the respondent at the time of the test, we further restrict the samples to the respondents
who took the test when they were 16 or 17. Besides this age restriction, we constructed the
NLSY97 sample so as to match as closely as possible the sample selection rules used by Neal &
Johnson (1996) for the NLSY79. Consistent with prior evidence, we find that the labor force
participation rate of black men has fallen over time relative to white men (see Appendix B for
more details on the data). The baseline estimation results are reported in Table 6 below. The
resulting sample sizes are equal to 1, 077 and 1, 123 for the NLSY79 and NLSY97 cohorts,
respectively.
 15
    Estimation results from our method without controlling for AFQT are not reported here to save space.
They are available from the authors upon request.


                                                  28
                                  NLSY79                                NLSY97
                        Extremal Quantile         Median      Extremal Quantile        Median
             Black            -0.122               -0.145           -0.140              -0.167
                             (0.001)              (0.039)          (0.050)             (0.058)
           Hispanic            0.029               -0.017           -0.054              -0.089
                             (0.002)              (0.056)          (0.050)             (0.050)
            AFQT               0.185                0.180            0.153               0.111
                             (0.001)              (0.019)          (0.022)             (0.026)
            AFQT2              0.007                0.008            0.002              -0.023
                             (0.001)              (0.017)          (0.022)             (0.020)
           Notes: Estimations also include linear control for age. Standard errors are reported in
          parentheses. In the column “Extremal Quantile”, we report the results corresponding
          to our preferred constrained specification, since we fail to reject homoskedasticity for
          all of the covariates with the p
                                         exception of age. We perform the homoskedasticity tests
          using the critical values cn = log(n), where n is the sample size. The vector of spacing
          parameters lj used in minimum distance estimation is equal to (0.65, 0.85, 1.15, 1.45).
          The quantile index τn is chosen based on the data-driven procedure discussed in Section
          3.3, using 500 subsamples of size 500.

       Table 6: Extremal quantile and median regression results (NLSY79-NLSY97)


The estimation results obtained with our method (“Extremal Quantile” columns) provide
evidence of a wider black-white wage gap for the 1997 cohort relative to the 1979 cohort, with
an increase in the estimated gap from 12.2% to 14%. It is also interesting to note that, while
the estimated levels do differ across both methods, the results from the median regression
of Neal & Johnson (1996) (“Median” columns) imply an across-cohort increase of a similar
magnitude (from 14.5% to 16.7%).

It is important to step back and try and understand what these results really mean. Specif-
ically, do they suggest that labor market discrimination against blacks has actually gotten
worse over the last two decades? Or does the estimated increase in the black-white wage gap
reflect the fact that the AFQT score only captures a fraction of all the premarket factors that
matter on the labor market, which may have changed over time? In particular, the results
reported in Table 6 provide clear evidence of a decline across cohorts in the wage returns to
AFQT, consistent with the latter story. Recent work by Castex & Dechter (2014) also provides
evidence from the NLSY79 and NLSY97 that the wage returns to AFQT have decreased over
time (see also Beaudry et al. , 2013, who argue that there has been a decline in the demand for
cognitive skills in the U.S. since 2000). While providing a definite answer to those questions
is particularly challenging, we attempt to shed light on this issue by controlling for additional
premarket factors, namely parental education and household structure (as measured by the
presence of both biological parents at age 14). Bringing those characteristics into the analysis


                                                    29
is important since differences in family environment have been found to account for most of
the black-white gap in noncognitive skills (see, e.g., Carneiro et al. , 2005).
Table 7 below reports the estimated black-white wage gap for the 1979 and 1997 cohorts,
using our extremal quantile method and the median regression of Neal & Johnson, for three
different specifications. The first specification (“No premarket factors”) only controls for age
and the Hispanic dummy, the second specification (“AFQT only”) also controls for AFQT
and AFQT squared, while the third specification (“Preferred”) further controls for parental
education and household structure.
                                               NLSY79                               NLSY97
                                     Extremal Quantile        Median      Extremal Quantile       Median
 Black (No premarket factor)               -0.342              -0.349           -0.296             -0.311
                                          (0.014)             (0.032)          (0.003)            (0.051)
      Black (AFQT only)                    -0.122              -0.145           -0.140             -0.167
                                          (0.001)             (0.039)          (0.050)            (0.058)
       Black (Preferred)                   -0.099              -0.123           -0.087             -0.135
                                          (0.017)             (0.042)          (0.043)            (0.064)
 Notes: Standard errors are reported in parentheses. The “preferred” specification includes AFQT, parental
education and household structure. For that case, the sample is then restricted to the individuals with non-
missing parental education and household structure, resulting in sample sizes equal to 1, 016 for the NLSY79
and 1, 071 for the NLSY97. In the column “Extremal Quantile”, we report the results corresponding to our
preferred constrained specification, since we fail to reject homoskedasticity
                                                             p                for the black dummy. We perform
the homoskedasticity tests using the critical values cn = log(n), where n is the sample size. The vector of
spacing parameters lj used in minimum distance estimation is equal to (0.65, 0.85, 1.15, 1.45). The quantile
index τn is chosen based on the data-driven procedure discussed in Section 3.3, using 500 subsamples of size
500.

    Table 7: Black-white wage gap with age restriction and additional premarket factors

Without controlling for premarket factors, our estimation results show that the black-white
wage gap has decreased by 4.6 points across the 1979 and 1997 cohorts. This result provides
evidence of a very slow black-white wage convergence between 1990 and 2007. While most of
the available evidence in the literature relates to the evolution of the black-white wage gap
before 2000, it is interesting to note that our results are of the same order of magnitude as the
estimates obtained by Neal & Rick (2014) using different datasets (namely the Census Long
Form for the year 1990 and the American Community Survey for the year 2007). In their
paper, Neal & Rick address the issue of differential selection into the workforce by examining
the sensitivity of the median black-white wage gap to various imputation rules, which vary
based on the fraction of (missing) wages supposed to fall below the median of the potential
wage distribution. This type of sensitivity analysis cannot be used after adding controls for
premarket factors, since in that case knowing the fraction of wages falling below or above the
median is not sufficient to estimate the median wage gap.

                                                    30
While we find that the black-white wage gap increases over time after controlling for AFQT,
Table 7 shows that the direction of the change is overturned when including other premar-
ket factors in addition to the AFQT. Using our estimation method, the black-white wage
gap is found to be fairly stable across cohorts, declining by only 1.2 points (from 9.9% to
8.7%) between 1990 and 2007. This result suggests that the across-cohort increase in the
wage gap conditional on AFQT is actually attributable to the premarket factors other than
AFQT, thus reflecting a time-varying omitted variable bias based on these family environment
characteristics. Interestingly, one can understand this result as extending the key finding of
Neal & Johnson (1996) to the across-cohort change in the wage gap. Premarket factors are a
dominant component of the black-white wage gap, not only in level but also in evolution.

In sum, our estimation results provide evidence of (i) a slow convergence in the raw male
black-white wage gap between 1990 and 2007 (Specification without premarket factors), and
(ii) an even slower convergence in the residual portion of the black-white wage gap, which
remains after controlling for premarket factors (Preferred specification). While we do find
that differences in premarket factors are a key component of the black-white wage gap and,
as such, should be a major focus from a policy standpoint, the fact that its residual portion
remains virtually stable after almost 20 years is also concerning.

We conclude this section by examining whether one could alternatively estimate the across-
cohort evolution of the black-white wage gap by applying the inverse density weighting scheme
of Lewbel (2007), treating AFQT as a special regressor. Note that, in this context, AFQT
appears to be the only potential candidate as a special regressor, thus ruling out the possible
use of the special regressor method in the absence of controls for premarket skills. The
large support condition would require the employment probability to be arbitrarily small for
some values of the AFQT. Although there is some variation, we found that the conditional
employment probability, estimated via nonparametric regression, remains very far from 0,
specifically above 0.63 for both NLSY cohorts. This clearly indicates that this method could
not be used in this context.


6    Concluding remarks
In this paper, we develop a new semiparametric inference method for location-scale models
in the presence of sample selection. A key feature of our method is that it can be used in
situations where one does not have access to an instrument for selection, nor to a large support
regressor. Instead, the main identifying condition is based on selection being independent of
the covariates for large values of the outcome. We show that this condition is typically mild

                                              31
provided that selection is endogenous. Building on this identification strategy, we propose
a simple estimation procedure, which combines quantile regressions in the tails, or extremal
quantile regressions, with minimum distance. We establish the consistency and asymptotic
normality of our estimators by extending the analysis of Chernozhukov (2005) to a setting with
sample selection. The choice of an appropriate quantile index is important in this context, and
we derive a data-driven procedure for this purpose. Importantly for the practical usefulness
of our method, we show that our estimation procedure performs well even with fairly small
samples.

Finally, we apply our method to the estimation of the black-white wage gap among males
from the NLSY79 and NLSY97 cohorts. Correcting for selection into the workforce is key in
this context since black males are more likely to dropout from the labor market than white
males, and this difference has increased over time. Our estimation results show that premarket
factors play a major role in explaining the magnitude of the black-white wage gap, as well as
its evolution over time.




                                              32
References
Ahn, H., & Powell, J. L. 1993. Semiparametric Estimation of Censored Selection Models With
  a Nonparametric Selection Mechanism. Journal of Econometrics, 58(1-2), 3–29.

Altonji, J.G., & Blank, R.M. 1999. Race and Gender in the Labor Market. In: Ashenfelter,
  O., & Card, D. (eds), Handbook of Labor Economics, vol. 3. Elsevier.

Altonji, J.G., Ichimura, H., & Otsu, T. 2008. Nonparametric Intermediate Order Regression
  Quantiles. Working paper.

Altonji, J.G., Bharadwaj, P., & Lange, F. 2012. Changes in the Characteristics of American
  Youth: Implications for Adult Outcomes. Journal of Labor Economics, 30, 783–828.

Andrews, D. K., & Schafgans, M. 1998. Semiparametric Estimation of the Intercept of a
  Sample Selection Model. Review of Economic Studies, 65, 497–517.

Andrews, D.W.K. 1999. Consistent Moment Selection Procedures for Generalized Method of
  Moments Estimation. Econometrica, 67(3), 543–564.

Andrews, D.W.K., & Soares, G. 2010. Inference for Parameters Defined by Moment Inequal-
  ities Using Generalized Moment Selection. Econometrica, 78(1), 119–157.

Arellano, M., & Bonhomme, S. 2011. Quantile Selection Models. Working paper.

Beaudry, P., Green, D.A., & Sand, B.M. 2013. The Great Reversal in the Demand for Skill
  and Cognitive Tasks. NBER Working Paper No. 18901.

Brown, C. 1984. Black-White Earnings Ratios Since the Civil Rights Act of 1964: The
  Importance of Labor Market Dropouts. Quarterly Journal of Economics, 99, 31–44.

Buchinsky, M. 1998. The Dynamics of Changes in The Female Wage Distribution in the USA:
  a Quantile Regression Approach. Journal of Applied Econometrics, 13, 1–30.

Butler, R., & Heckman, J.J. 1977. The Government’s Impact on the Labor Market Status
  of Black Americans: A Critical Review. In: et al., F. E. Bloch (ed), Equal Rights and
  Industrial Relations. Madison, Wis.: Indus. Relations Res. Assoc.

Carneiro, P., Heckman, J.J., & Masterov, D.V. 2005. Labor Market Discrimination and Racial
  Differences in Premarket Factors. Journal of Law and Economics, 48(1), 1–39.




                                            33
Carneiro, P.M., Hansen, K.T., & Heckman, J.J. 2003. Estimating Distributions of Treatment
  Effects with an Application to the Returns to Schooling and Measurement of the Effects of
  Uncertainty on College Choice. International Economic Review, 44, 361–422.

Castex, G., & Dechter, E. 2014. The Changing Roles of Education and Ability in Wage
  Determination. Forthcoming in the Journal of Labor Economics, 32(4), –.

Chamberlain, G. 1986. Asymptotic Efficiency in Semiparametric Model with Censoring. Jour-
  nal of Econometrics, 32, 189–218.

Chernozhukov, V. 2005. Extremal Quantile Regression. The Annals of Statistics, 33(2), pp.
  806–839.

Chernozhukov, V., & Du, S. 2008. Extremal quantiles and value-at-risk. In: Durlauf, S., &
  Blume, L. (eds), The New Palgrave Dictionary of Economics. London: Palgrave Macmillan
  Press.

Chernozhukov, V., & Fernandez-Val, I. 2011. Inference for Extremal Conditional Quantile
  Models, with an Application to Market and Birthweight Risks. Review of Economic Studies,
  78, 559–589.

Chernozhukov, V., S.Lee, & Rosen, A.M. 2013. Intersection Bounds: Estimation and Infer-
  ence. Econometrica, 81(2), 667–737.

Cunha, F., & Heckman, J.J. 2007. Identifying and Estimating the Distributions of Ex Post
  and Ex Ante Returns to Schooling. Labour Economics, 14, 870–893.

Danielsson, J., de Haan, L., Peng, L., & de Vries, C. G. 2001. Using a Bootstrap Method to
  Choose the Sample Fraction in Tail Index Estimation. Journal of Multivariate Analysis,
  76, 226–248.

Das, M., Newey, W., & Vella, F. 2003. Nonparametric Estimation of Sample Selection Models.
  Review of Economic Studies, 70, 33–58.

D’Haultfoeuille, X., & Maurel, A. 2013a. Another Look at the Identification at Infinity of
  Sample Selection Models. Econometric Theory, 29(1), 213–224.

D’Haultfoeuille, X., & Maurel, A. 2013b. Inference on an Extended Roy Model, with an
  Application to Schooling Decisions in France. Journal of Econometrics, 174, 95–106.

Drees, H., & Kaufmann, E. 1998. Selection of the optimal sample fraction in univariate
  extreme value estimation. Stochastic Processes and their Applications, 75, 149–172.

                                            34
Eisenhauer, P., Heckman, J.J., & Vytlacil, E.J. 2013. The Generalized Roy Model and the
  Cost-Benefit Analysis of Social Programs. Working paper.

Gronau, R. 1974. Wage Comparison - a Selectivity Bias. Journal of Political Economy, 82,
  1119–43.

Heckman, J. J. 1974. Shadow Prices, Market Wages, and Labor Supply. Econometrica, 42,
  679–694.

Heckman, J. J. 1979. Sample Selection Bias as a Specification Error. Econometrica, 47,
  153–161.

Heckman, J. J. 1990. Varieties of Selection Bias. The American Economic Review, 80, 313–
  318.

Heckman, J.J., & Vytlacil, E.J. 2007. Econometric evaluation of social programs, Part II. In:
  Heckman, J.J., & Leamer, E.E. (eds), Handbook of Econometrics, vol. 6B. Elsevier.

Johnson, W., Kitamura, Y., & Neal, D. 2000. Evaluating a Simple Method for Estimating
  Black-White Gaps in Median Wages. American Economic Review, 90, 339–343.

Juhn, C. 2003. Labor Market Dropouts and Trends in the Wages of Black and White Men.
  Industrial and Labor Relations Review, 56(4), 643–662.

Khan, S., & Tamer, E. 2010. Irregular Identification, Support Conditions and Inverse Weight
  Estimation. Econometrica, 78, 2021–2042.

Kitagawa, T. 2010. Testing for Instrument Independence in the Selection Model. Working
  paper.

Lewbel, A. 2007. Endogenous Selection or Treatment Model Estimation. Journal of Econo-
  metrics, 141, 777–806.

Lewbel, A. 2014. An Overview of the Special Regressor Method. In: Racine, J., Su, L.,
  & Ullah, A. (eds), The Oxford Handbook of Applied Nonparametric and Semiparametric
  Econometrics and Statistics. Oxford University Press.

Neal, D. 2004. The Measured Black-White Wage Gap among Women Is Too Small. Journal
  of Political Economy, 112, S1–S28.

Neal, D. 2006. Why Has Black-White Skill Convergence Stopped?          In: Hanushek, E., &
  Welch, F. (eds), Handbook of the Economics of Education, vol. 1. Elsevier.

                                             35
Neal, D., & Johnson, W.R. 1996. The Role of Premarket Factors in Black-White Wage
  Differences. Journal of Political Economy, 104(5), pp. 869–895.

Neal, D., & Rick, A. 2014. The Prison Boom and the Lack of Black Progress since Smith and
  Welch. Working paper.

Nelsen, Roger B. 2006. An Introduction to Copulas. 2nd ed. edn. New York: Springer,.

Pollard, D. 1991. Asymptotics for Least Absolute Deviation Regression Estimators. Econo-
  metric Theory, 7(2), pp. 186–199.

Resnick, S. 1987. Extreme values, regular variation, and point processes. New York: Springer-
  Verlag.

Schafgans, M., & Zinde-Walsh, V. 2002. On intercept estimation in the sample selection
  model. Econometric Theory, 18, 40–50.

Smith, J., & Welch, F. 1989. Black Economic Progress After Myrdal. Journal of Economic
  Literature, 27(2), 519–564.

Vella, F. 1998. Estimating Models with Sample Selection Bias: a Survey. Journal of Human
  Resources, 33, 127–169.

Wooldridge, J. 2002. Econometric Analysis of Cross Section and Panel Data. MIT Press.




                                             36
A      Additional details on inference

A.1     Confidence intervals on quantile effects when δj 6= 0

The endpoints of the confidence intervals on ∆jτ can be estimated by applying the method-
ology of Chernozhukov et al. (2013) to the outer bounds (2.3). We illustrate the procedure
by focusing on the upper bound of the confidence interval, when δj > 0. The lower bound of
the interval and the case δj < 0 can be treated similarly. Let

                                                                    τ
                                                 QY |D=1,X=x ( P (D=1|X=x) ) − x0 β
                           θj (x) = βj + δj                                                    ,
                                                              1 + x0 δ
           o
so that ∆jτ = inf x∈Supp(X) θj (x). One can estimate θj (.) by

                                                 Q
                                                 b Y |D=1,X=x (     τ
                                                                           )     − x0 βb
                                                               P (D=1|X=x)
                                                                b
                           θbj (x) = βbj + δbj                                                 ,
                                                              1 + x0 δb

where Q
      b Y |D=1,X=x and Pb(D = 1|X = x) are nonparametric (for instance kernel) estimators of
QY |D=1,X=x and P (D = 1|X = x), respectively. By Theorem 3.1, the rates of convergence of δb
and βb are (τn n)−1/2 and γ(τn )(τn n)−1/2 , respectively. The rates of convergence of Q
                                                                                       b Y |D=1,X=x
and Pb(D = 1|X = x) depend on the number of continuous components of X, on the degree
of smoothness of x 7→ QY |D=1,X=x and x 7→ P (D = 1|X = x) as well as on the choice of
the tuning parameters. In any case, it is always possible to choose τn so that the rate of
convergence of βb will be slower than the rates of convergence of δ,
                                                                  b Qb Y |D=1,X=x and Pb(D =
1|X = x).16 In this case,
                                                                                      
                                                                              γ(τn )
                             θj (x) − θbj (x) = fj (x)(βb − β) + oP           √            ,
                                                                                τn n

                          δbj x0
where fj (x) = −e0j +              and ej is a vector of Rd whose jth coordinator equals 1 and others
                         1+x0 δb
equal 0. One can then apply the inference procedure discussed in Section 4.1 of Chernozhukov
et al. (2013) to construct the upper bound of the confidence interval. Note that although
                              √                                                        √
the rate of convergence is not n here, their Theorem 4.1 still applies, after replacing n by
  16
    This is the case for any τn satisfying Assumption 9 if all the components of X are discrete. If one component
of X is continuous, the rate of convergence for QY |D=1,X=x and P (D = 1|X = x) will typically be n−2/5 . Then
one has to impose, in addition to Assumption 9, that τn = o(n−1/5 ). Under these conditions, one can show
that such a τn always exist by a simple monotonicity argument.




                                                         37
√
    τn n/γ(τn ).17 Specifically, let us define

                              γ(τn )                                                  b 1/2
                                                                                fj (x)Ω
                     sn (x) = √            b 1/2
                                     fj (x)Ω             ,   Zn? (x) =                  δ
                                                                                                  Nd .
                                τn n         δ
                                                     2                                b 1/2
                                                                                fj (x)Ω δ
                                                                                              2


where k.k2 denotes the Euclidean norm, Ω
                                       b δ is the consistent estimator of the asymptotic vari-
ance matrix Ωδ described after Theorem 3.1 and Nd is a d-dimensional standard normal vector
generated independently from the data. Then one can compute, typically by simulations,

                            K1n = Qsupx∈Supp(X) Zn? (x)|data (1 − 0.1/ log(n)).

Now, constructing Xbn as
                                                                                                        
            Xbn = x ∈ Supp(X) : θbj (x) ≤ 2K1n sn (x) +                   inf        θbj (e
                                                                                          x) + K1n sn (e
                                                                                                       x) .
                                                                      e∈Supp(X)
                                                                      x


one can compute
                                    K2n (τ ) = Qsupx∈Xb           ? (x)|data (τ ).
                                                                 Zn
                                                             n


                         θj,1−α of a confidence interval on ∆jτ of nominal coverage 1 − α is
Finally, the upper bound b
defined by
                             θj,1−α =
                             b             inf      θbj (x) + K2n (1 − α)sn (x).
                                        x∈Supp(X)


A.2      Details on the data-driven τn

We provide in this section a rationale for the construction of the data-driven τn detailed in
Section 3.3. We study for that purpose the asymptotic behavior of δb for sequences τn0 that do
not satisfy Assumption 9 (iii), but only τn0 nf (γ(τn0 )) = O(1). We show that in this case,
                                         p

δb has an asymptotic bias. Then we relate this bias with the asymptotic behavior of the J
test statistic TJ (τn0 ), and show how this can be used to select a quantile index for which the
asymptotic bias is small.

First, let us define

            E (τ − 1{−Y ≤ γ(τ ) + X 0 β(τ )) X     E (τ − 1{−Dε ≤ Qεe(τ /h)(1 + X 0 δ)) X
                                                                                       
    µ(τ ) ≡                                      =                                          .
                          τ                                        τ
  17
     For that purpose, we need to assume their Condition V, which is a mild regularity condition (see Cher-
nozhukov et al. , 2013, p.691, for a discussion). Then, using the proof of Theorem 3.1, we can check that under
                                                                      √      √
Assumptions 1-4 and 7-9, their Conditions P-(ii)-(v) hold (replacing n by τn n/γ(τn )). Although Condition
P-(i) does not hold, we can still prove their Lemma 4 using the fact that the nonparametric part of θ(.) does
not play any role in the asymptotic distribution of θ(.).
                                                     b



                                                      38
As shown in the proof of Theorem 3.1, µ(τ ) is the core component of the bias induced by the
fact that (3.2) is an equivalence instead of an equality. f (γ(τn )) in Assumption 9 can then be
                                                              √
viewed as an envelope of µ(τ ). Under Assumption 9-(iii), τn nµ(τn ) → 0, meaning that the
asymptotic bias vanishes. In what follows, we derive the asymptotic bias of our estimator δb
as a function of µ(τ ) and propose a subsampling method to approximate this bias.

From (C.10) and the linear representation of Zbn (1) below (C.13) in the proof of Theorem 3.1,
we have, for any sequence τn that satisfies Assumption 9,

                  √
                      τn n(δb − δ) = log(m)(G0 Wδ∗ G)−1 G0 Wδ∗ αn (τn )gn (δ) + oP (1),

                  √
where αn (τ ) =       τn n/(γ(mτ ) − γ(τ )). We also show in the proof of Theorem 3.1 that

                                    αn (τn )gn (δ) = (I ⊗ ∆)ΓZbn (τn ),

where Zbn (τn ) is asymptotically normal with mean 0 when the asymptotic bias of δb is zero. In
order to analyze situations where a sequence τn0 only satisfies τn0 nf (γ(τn0 )) = O(1), consider
                                                               p

                                                                 √
                                 Z̃n (τ ) ≡ log(m)Zbn (τ ) + Q−1
                                                              H    τ nb(τ ),

                    √                    √
with b(τ ) = (µ(τ ), l1 µ(l1 τ ), · · · , lJ µ(lJ τ ))0 . Then one can show that

        τn0 n(δb − δ) =(G0 Wδ∗ G)−1 G0 Wδ∗ (IJ ⊗ ∆)ΓZ̃n (τn0 )
       p

                            − (G0 Wδ∗ G)−1 G0 Wδ∗ (IJ ⊗ ∆)Γ(IJ+1 ⊗ Q−1    τn0 nb(τn0 ) + oP (1).
                                                                        p
                                                                    H  )

Z̃n (τn0 ) is asymptotically normal with mean 0 by definition of b(τ ). Hence, the second term is
the asymptotic bias of δ. b We seek to approximate the norm of this bias. In order to do so, we
consider the minimum distance statistic, which is commonly used to conduct a specification
test in the context of minimum distance estimation. By plugging in the minimum distance
estimator δ,
           b we obtain

                                                                              h                                 i
                  ∗1/2                       ∗1/2                     ∗1/2
log(m)αn (τn0 )Wδ                                   G(G0 Wδ∗ G)−1 G0 Wδ      ) (I ⊗ ∆)ΓZ̃n (τn0 ) − τn0 nB(τn0 )
                                                                                                   p
                             b =(IJd − W
                         gn (δ)          δ

                                 + oP (1),

where B(τ ) = (I ⊗ ∆)Γ(IJ+1 ⊗ Q−1
                               H )b(τ ). B(τ ) is the bias associated with the choice of
quantile index τ . It follows that the J-statistic defined in Section 3.3 can be written as




                                                         39
                                                    ∗1/2                        ∗1/2
       TJ (τn0 ) =Z̃n (τn0 )0 Γ0 (I ⊗ ∆0 )(IJd − Wδ        G(G0 Wδ∗ G)−1 G0 Wδ
                                                                         )(I ⊗ ∆)ΓZ̃n (τn0 )
                                                     ∗1/2                ∗1/2 p
                  + 2Z̃n (τn0 )0 Γ0 (I ⊗ ∆0 )(IJd − Wδ G(G0 Wδ∗ G)−1 G0 Wδ ) τn0 nB(τn0 )
                                            ∗1/2                        ∗1/2
                  + τn0 nB(τn0 )0 (IJd − Wδ        G(G0 Wδ∗ G)−1 G0 Wδ         )B(τn0 ) + oP (1).

This equation shows that the J-statistic on the left-hand side converges to a chi square dis-
tribution with (J − 1)d degrees of freedom, plus a bias term. If τn0 nB(τn0 ) → 0, then the
                                                                  p

median of the J-statistic is asymptotically the median M(J−1)d of a χ2 ((J −1)d). On the other
hand, if the asymptotic bias τn0 nB(τn0 ) does not vanish, the difference between the median
                               p

of the J-statistic and M(J−1)d will generally be asymptotically different from zero. Following
this idea, we estimate the difference between the two medians and use it as a proxy for the
asymptotic bias of δ.
                   b As indicated in the text, we rely for that purpose on subsampling.


B    Data appendix
We construct our NLSY97 dataset based on the interviews that were conducted during the
years 2007 and 2008, using data on males from the cross-sectional sample and the oversample
of blacks and Hispanics of the NLSY97. Our sample consists of the respondents who reported
wages for at least one of these two years, along with the respondents who reported not working
in either year (nonparticipants). Respondents with a missing AFQT score are excluded from
the analysis. For the individuals working in both years, the wage variable is defined as the
average of the hourly wages corresponding to the main job at the time of the interview.
For those working during one year only, we define the wage variable as the hourly wage
corresponding to the main job at the time of the interview in that year. Finally, we trim the
data by dropping the wage observations below 1 dollar and above 118.95 dollars (corresponding
to 75 dollars in 1991). We report in Table 8 below some descriptives corresponding to our
NLSY79 and NLSY97 samples restricted to the respondents who took the ASVAB test when
they were 16 or 17. Table 9 reports the labor force participation rates for the NLSY79 and
NLSY97 samples, separately for blacks and whites.




                                                      40
                                                  NLSY79                               NLSY97
                                        Blacks    Hispanics     Whites       Blacks    Hispanics      Whites
            AFQT                        -0.716       -0.314       0.387      -0.726       -0.279        0.373
           Std.dev.                    (0.812)      (0.935)     (0.966)     (1.037)      (0.942)      (0.923)
  Highest grade completed               11.638       11.419      12.859      11.239       11.502       12.849
           Std.dev.                    (3.927)      (3.953)     (3.691)     (4.811)      (4.529)      (4.726)
 Mother high school graduate             0.447        0.243       0.715       0.707        0.536        0.829
 Father high school graduate             0.368        0.284       0.665       0.518        0.396        0.758
   Mother college graduate               0.040        0.027       0.093       0.107        0.094        0.217
   Father college graduate               0.046        0.050       0.188       0.086        0.068        0.245
   Both parents at age 14                0.486        0.599       0.760       0.264        0.540        0.597
Note: Samples restricted to males. Blacks account for 31% (25%) of the NLSY79 (NLSY97) sample, while
Hispanics account for 20% (21%) of the NLSY79 (NLSY97) samples.

              Table 8: Descriptive statistics for the subsample with restricted age


                                                                Blacks     Whites
                             NLSY79 full sample                 91.02%     97.52%
                          NLSY79 with age restriction           90.58%     98.10%
                          NLSY97 with age restriction           81.43%     93.09%

                          Table 9: Labor force participation rates (males)




C      Proofs of the results
In the following, we let, for any random variable U and with a slight abuse of notations,
SU← = 1 − FU← . We also let Ũ = −U and define ε̆ = Ỹ + X 0 β. Finally, we take the convention
that intervals [a, b] refer to [b, a] when b < a, and similarly for open or semi-open intervals.


C.1     Proof of Theorem 2.1

By Theorem 2.1 in D’Haultfoeuille & Maurel (2013a), x 7→ x0 β and x 7→ x0 δ are identified.18
Identification of β and δ then follows from Assumption 2. Turning to ∆jτ , remark first that
by independence between X and ε,

                                           ∆jτ = βj + δj Qε (τ ).                                       (C.1)
  18
     In D’Haultfoeuille & Maurel (2013a) we use E(exp(λε)) < +∞ for some λ > 0 instead of the weaker
condition that Sexp(ε) slowly varying. An inspection of the proof reveals however that the proof only relies on
this latter condition.




                                                      41
It suffices therefore to obtain bounds on Qε (τ ). We suppose hereafter that δj > 0. The
reasoning is similar for δj < 0, while ∆j is identified from (C.1) if δj = 0. First, we have

         fε|X (u|x) = P (D = 1|X = x)fε|D=1,X (u|x) + P (D = 0|X = x)fε|D=0,X (u|x).

Thus, for all x in the support of X,

                            fε|X (u|x) ≥ P (D = 1|X = x)fε|D=1,X (u|x).

By independence between X and ε,

            fε (u) =     sup       fε|X (u|x) ≥     sup       P (D = 1|X = x)fε|D=1,X (u|x).      (C.2)
                       x∈Supp(X)                  x∈Supp(X)


Integrating (C.2) between −∞ and v implies that Fε (v) ≥ F ε (v). Hence, Qε (τ ) ≤ F ←
                                                                                     ε (τ ).
This yields the upper bound on ∆jτ . Now, integrating (C.2) between v and +∞ implies that
                                        "                                           #
                               Z   +∞
               1 − Fε (v) ≥                 sup     P (D = 1|X = x)fε|D=1,X (u|x) du.
                               v        x∈Supp(X)


Hence, Fε (v) ≤ 1 − F ε (+∞) + F ε (v). As a result,

                 Qε (τ ) ≥ [1 − F ε (+∞) + F ε ]← (τ ) = F ←
                                                           ε (τ − (1 − F ε (+∞))).


The lower bound on ∆jτ follows.

Now, let us show that these bounds are sharp under (2.2). For that purpose, we exhibit
conditional cdfs Feε|D=0,X (.|.), different in general from the true ones, which rationalize the
bounds and satisfy the restrictions imposed by Assumptions 1, 3, 4 and (2.2). Note that the
other conditions (Assumptions 2 and 3-(iv)) only depend on the observed data and therefore
need not be verified. Note also that we can restrict to the case where P (D = 0|X = x) > 0
for almost all x. For in the case where P (D = 0|X = x) = 0, Inequality (C.2) is actually an
equality, and the two bounds coincide. The bounds then correspond to the true model, and
are therefore sharp.

Now, consider the upper bound. Let u0 be such that F ε (u0 ) > τ and suppose that

                    F (u) − P (ε ≤ u, D = 1|X = x)
   Feε|D=0,X (u|x) = ε                             1{u < u0 } + Fε|D=0,X (u|x)1{u ≥ u0 }.
                           P (D = 0|X = x)

Let us first show that for all x, Feε|D=0,X (.|x) is indeed a cdf. It suffices to show that its limit at

                                                      42
−∞ is zero, that it is increasing and right-continuous on (−∞, u0 ) and limu↑u0 Feε|D=0,X (u|x) ≤
Fε|D=0,X (u0 |x). The first point holds because limu→−∞ F ε (u) = limu→−∞ P (ε ≤ u, D =
1|X = x) = 0. The second point follows by remarking that
                                         Z   v   
                                                                       fε|D=1,X (u|x0 )P (D = 1|X = x0 )
                                                                                                       
    F ε (v) − P (ε ≤ v, D = 1|X = x) =                    sup
                                            −∞       x0 ∈Supp(X)
                                                                                    
                                                     − fε|D=1,X (u|x)P (D = 1|X = x) du.


The integral form implies that Feε|D=0,X (.|x) is right-continuous. Because the term in braces
is positive, Feε|D=0,X (.|x) is also increasing. Finally, the third point follows because for any u,

    F ε (u) − P (ε ≤ u, D = 1|X = x)   Fε (u) − P (ε ≤ u, D = 1|X = x)
                                     ≤                                 = Fε|D=0,X (u|x).
             P (D = 0|X = x)                   P (D = 0|X = x)

Now, let us prove that the conditional cdfs Feε|D=0,X (.|.) rationalize the bounds and satisfy
the restrictions of the model. First,

            Feε|X (u|x) = Feε|D=0,X (u|x)P (D = 0|X = x) + P (ε ≤ u, D = 1|X = x)
                        = F ε (u)1{u < u0 } + Fε (u)1{u ≥ u0 }.                                        (C.3)

The right-hand side does not depend on x. Therefore, Feε|D=0,X satisfies Assumption 1. (C.3)
also implies that for any τ 0 ≤ τ ,
                                        Feε← (τ 0 ) = F ←   0
                                                        ε (τ ).

Therefore, the conditional cdfs Feε|D=0,X (.|.) rationalize ∆j (τ ). Now, because feε (u) is equal
to the true fε (u) for u large enough, the conditional cdfs Feε|D=0,X (.|.) satisfy Assumption 3.
Similarly, by Bayes’ theorem, we have, for y large enough,

                                            fY |D=1,X=x (y|x)P (D = 1|X = x)
            Pe(D = 1|X = x, Y ∗ = y) =
                                                       feY ∗ |X (y|x)
                                            (1 + x0 δ)fY |D=1,X=x (y|x)P (D = 1|X = x)
                                        =
                                                       feε [(y − x0 β)/(1 + x0 δ)]
                                            (1 + x0 δ)fY |D=1,X=x (y|x)P (D = 1|X = x)
                                        =
                                                       fε [(y − x0 β)/(1 + x0 δ)]
                                        = P (D = 1|X = x, Y ∗ = y),

and therefore, Assumption 4 is satisfied. This equality also ensures that (2.2) is satisfied.
Hence, the upper bound is sharp.


                                                     43
Now, let us turn to the lower bound. Let u1 be such that F ε (u1 ) < τ and consider

                                            1 − F ε (+∞) + F ε (u) − P (ε ≤ u, D = 1|X = x)
Feε|D=0,X (u|x) = Fε|D=0,X (u|x)1{u < u1 }+                                                 1{u ≥ u1 }.
                                                           P (D = 0|X = x)

As previously, Feε|D=0,X (.|x) is indeed a cdf and

               Feε|X (u|x) = Fε (u)1{u < u1 } + [1 − F ε (+∞) + F ε (u)] 1{u ≥ u1 },

so that Assumption 1 holds and Feε|D=0,X (.|.) rationalizes the lower bound. We now check
Assumption 3. For u large enough,

              feε (u) =     sup       P (D = 1|X = x)fε|D=1,X (u|x)
                          x∈Supp(X)

                    =       sup       P (D = 1|X = x, ε = u)fε|X (u|x)
                          x∈Supp(X)
                               "                                                   #
                                                                    ∗     0    0
                    = fε (u)          sup      P (D = 1|X = x, Y = x β + (1 + x δ)u) .     (C.4)
                                   x∈Supp(X)


We now prove that feε (u) ∼ hfε (u) as u → ∞. Fix η > 0. Because Supp(X) is compact, there
exists (x1 , ..., xk ) ∈ Supp(X)k such that for all x ∈ Supp(X), minj=1...k kx − xj k < η. There
exists also y0 such that for all y ≥ y0 ,

                              max |P (D = 1|X = xj , Y ∗ = y) − h| < η.                    (C.5)
                              j=1...k


By compacity of Supp(X) once more, there exists u0 such that for all u ≥ u0 ,

                                         inf      x0 β + (1 + x0 δ)u ≥ y0 .                (C.6)
                                      x∈Supp(X)


Then, for all x ∈ Supp(X), and all u ≥ u0 ,

  |P (D = 1|X = x, Y ∗ = x0 β + (1 + x0 δ)u) − h|
   ≤ |P (D = 1|X = x, Y ∗ = x0 β + (1 + x0 δ)u) − P (D = 1|X = xj , Y ∗ = x0 β + (1 + x0 δ)u)|
     + |P (D = 1|X = xj , Y ∗ = x0 β + (1 + x0 δ)u) − h|
   ≤ Kkx − xj k + η,

where the second inequality follows by (2.2), (C.5) and (C.6). Choosing j such that kx−xj k <




                                                       44
η finally yields

                   sup      |P (D = 1|X = x, Y ∗ = x0 β + (1 + x0 δ)u) − h| < (K + 1)η.
               x∈Supp(X)


As a result,


                    lim        sup    P (D = 1|X = x, Y ∗ = x0 β + (1 + x0 δ)u) = h.
                    u→∞ x∈Supp(X)


Hence, by (C.4), as u → ∞,
                                               feε (u) ∼ hfε (u).                            (C.7)

This implies that Assumption 3-(i) holds. Now, suppose that Seexp(ε) is slowly varying. Then
for all l > 0, Seexp(ε) (lu)/Seexp(ε) (u) → 1. Now, (C.7) also implies that for any l > 0,

                                         Sexp(ε) (lu)   Seexp(ε) (lu)
                                                      ∼               .
                                         Sexp(ε) (u)     Seexp(ε) (u)

This implies that Sexp(ε) is also slowly varying, a contradiction. Thus, Assumption 3-(ii) is
satisfied. By (C.7) once more, there exists η > 0 arbitrarily small such that for all u large
enough,
                                  (h − η)Sε (u) ≤ Seε (u) ≤ (h + η)Sε (u).

Now, fix τ small enough and let u = Seε← (τ ). Seε (u) ≥ τ implies Sε (u) ≥ τ /(h + η), which
yields in turn u ≤ Sε← (τ /(h + η)). Hence, we obtain

                               Seε← (τ ) ≤ Sε← (τ /(h + η)) = −Qεe(τ /(h + η)).

Now, let u0 > u, so that Seε (u0 ) ≤ τ . Then u0 ≥ Sε← (τ /(h − η)). Letting u0 tend to u yields

                               Seε← (τ ) ≥ Sε← (τ /(h − η)) = −Qεe(τ /(h − η)).

As a result, for any fixed m > 1 and letting e = exp(1),

                         Seε← (mτ ) − Seε← (τ )   Q (mτ /(h − η)) − Qεe(τ /(h + η))
                                                 ≤ εe                                 .
                          Seε← (eτ ) − Seε← (τ )   Qεe(eτ /(h + η)) − Qεe(τ /(h − η))

By Lemma D.2 in Appendix D, the right-hand side converges to log(m(h+η)/(h−η))/ log(e(h−




                                                      45
η)/(h + η)). Reasoning similarly on the lower bound, we obtain,

                                                                        Seε← (mτ ) − Seε← (τ )
            log(m(h − η)/(h + η))/ log(e(h + η)/(h − η)) ≤ lim inf
                                                                   τ →0 Seε← (eτ ) − Seε← (τ )
                    Seε← (mτ ) − Seε← (τ )
        ≤ lim sup                          ≤ log(m(h + η)/(h − η))/ log(e(h − η)/(h + η)).
               τ →0 Seε← (eτ ) − Seε← (τ )

Because η was arbitrary, we can make it tend to zero, thus obtaining

                                     Seε← (mτ ) − Seε← (τ )
                                lim                         = log(m).
                                τ →0 Se← (eτ ) − Se← (τ )
                                      ε           ε


This proves (see Resnick, 1987, Proposition 0.10) that Seε belongs to the domain of attraction
of the Gumbel distribution. Hence, Assumption 3-(iii) holds.

Turning to Assumption 4, we reason as for the upper bound:

                                           (1 + x0 δ)fY |D=1,X=x (y|x)P (D = 1|X = x)
            Pe(D = 1|X = x, Y ∗ = y) =
                                                      feε [(y − x0 β)/(1 + x0 δ)]
                                         (1 + x0 δ)fY |D=1,X=x (y|x)P (D = 1|X = x)
                                       ∼
                                                   hfε [(y − x0 β)/(1 + x0 δ)]
                                         P (D = 1|X = x, Y ∗ = y)
                                       ∼                             .
                                                       h

Therefore, the conditional cdfs Feε|D=0,X satisfy Assumption 4, with a limit equal to 1 instead
of h. The result follows.

Finally, let us check (2.2). We have by what precedes, for y large enough

                                                    P (D = 1|X = x, Y ∗ = y)
              Pe(D = 1|X = x, Y ∗ = y) =                                              .
                                            supx0 ∈Supp(X) P (D = 1|X = x0 , Y ∗ = y)

Moreover, we have proved that the denominator tends to h as y → ∞. Therefore, because the
true distribution satisfies (2.2), we have, for all (x, x0 ) ∈ Supp(X)2 and all y large enough,

                                                                           K
            |Pe(D = 1|X = x0 , Y ∗ = y) − Pe(D = 1|X = x, Y ∗ = y)| ≤         kx − x0 k,
                                                                          h−η

for some 0 < η < h. This ensures that Feε|D=0,X satisfies (2.2), and thus that the lower bound
is sharp.




                                                 46
C.2    Derivation of the outer bounds (2.3)

We only consider the case where δj > 0, the case δj < 0 being similar. Note first that
P (D = 1|X) > 0 almost surely, because P (D = 1|X = x) = E [P (D = 1|X = x, Y ∗ )|X = x],
and P (D = 1|X = x, Y ∗ = y) is bounded from below by h/2 > 0 for y large enough. Now,
consider the lower bound. We have for all (u, x), by independence between ε and X,

  P (ε ≤ u) = P (ε ≤ u|X = x)
               ≤ P (ε ≤ u, D = 1|X = x) + P (D = 0|X = x)
               ≤ P (Y ≤ x0 β + (1 + x0 δ)u|D = 1, X = x)P (D = 1|X = x) + P (D = 0|X = x).

Taking u = Qε (τ ), using Fε (Qε (τ )) ≥ τ and the definition of the quantiles of Y |D = 1, X = x,
we obtain, for all x in the support of X,
                                                                                                     
                   0               0                                  τ − P (D = 0|X = x)
                  x β + (1 + x δ)Qε (τ ) ≥ QY |D=1,X=x                                                    .
                                                                        P (D = 1|X = x)

As a result,                                                                      
                                                                  τ −P (D=0|X=x)
                                                QY |D=1,X=x         P (D=1|X=x)        − x0 β
                       Qε (τ ) ≥        sup                                                       .
                                   x∈Supp(X)                      1 + x0 δ

The outer lower bound of ∆jτ follows from ∆jτ = βj + δj Qε (τ ).

Now let us turn to the outer upper bound. Reasoning as before, we have, for all x in the
support of X and u < Qε (τ ),

          τ ≥ P (ε ≤ u) ≥ P (Y ≤ x0 β + (1 + x0 δ)u, D = 1|X = x)P (D = 1|X = x).

The definition of the quantiles of Y |D = 1, X = x then yields
                                                                                         
                         0               0                                   τ
                       x β + (1 + x δ)u ≤ QY |D=1,X=x                                         .
                                                                      P (D = 1|X = x)

Letting u tend to Qε (τ ) and taking the infimum over x then yields
                                                                                
                                                   QY |D=1,X=x           τ
                                                                   P (D=1|X=x)       − x0 β
                        Qε (τ ) ≤         inf                                                 .
                                       x∈Supp(X)                  1 + x0 δ

The upper bound follows.




                                                        47
C.3     Proof of Proposition 2.1

We verify Assumption 4 with h = 1. By Assumption 5 and because fC (γ) → 0, we have, as
y → ∞,
                                                                                   0       
                                                                                    xβ−y
      |P (D = 1|X = x, Y ∗ = y) − 1| = P         Fη (η) ≤ Fη (φ(x))|Fεe(e ε) = Fεe              −1
                                                                                    1 + x0 δ
                                                   0                       
                                                        xβ−y
                                          = ∂1 C Fεe               , Fη (φ(x)) − 1
                                                        1 + x0 δ
                                                          0            
                                                              xβ−y
                                          ≤ sup ∂1 C Fεe                   ,v − 1
                                           v∈[v,1]             1 + x0 δ

                                        −→ 0.

C.4     Proof of Proposition 2.2
                                       PJ          ∗
Let Ds = 1{D = s} and Y =                 s=1 Ds Ys .    By considering the dataset (Ds , Ds Y, X), we
are back to the binary model. Then Theorem 2.1 is directly applicable if one can verify
Assumption 4. For a given x in the support of X, we have

  1 − P (Ds = 1|X = x, Ys∗ = y)
=P (y + Gs (x) + λ0s,2 π + νs,2 ≤ maxi6=s x0 βi + (1 + x0 δi )(λ0i,1 π + νi,1 ) + Gi (x) + λ0i,2 π + νi,2 )
                                                                                                              

≤P (νs ≥ y + G(x)),

where

           νs =      sup      max((1 + x0 δi )νi,1 + νi,2 − νs,2 ),
                  x∈Supp(X) i6=s

                           (Gs (x) + λ0s,2 p − max x0 βi + (1 + x0 δi )λ0i,1 p + Gi (x) + λ0i,2 p) .
                                                                                                  
        G(x) =       inf
                  p∈Supp(π)                      i6=s


By Assumption 6, G(x) > −∞. Therefore, as y → ∞, P (νs ≥ y + G(x)) → 0. Thus,
Assumption 4 holds (with h = 1).


C.5     Proof of Lemma 3.1

Let Ux (y) ≡ 1/P (Y > y|X = x), Vx (y) ≡ 1/hSε ((y − xβ)/(1 + x0 δ)). Then from Equation
(3.1), Ux (y) ∼ Vx (y). We want to show the equivalence Ux← (τ ) ∼ Vx← (τ ). For that purpose,
we suppose that there exists ε0 > 0 and a sequence (ym )m∈N tending to infinity such that

                                       Vx← (ym )/Ux← (ym ) ≥ 1 + ε0 ,                                  (C.8)



                                                        48
and shows that this leads to a contradiction. The reasoning is similar for the other inequality
(Vx← (ym )/Ux← (ym ) ≤ 1 − ε0 ).

First, by Lemma D.1 in Appendix D, Sε is in the domain of attraction of Type I extreme
value distribution. This implies that V ≡ 1/Sε is Γ-varying (see Resnick (1987) Propo-
                                        V (z+tf (z))
sition 0.10), i.e. limz→∞                   V (z)    = et   for some auxiliary function f . Define fx (y) =
f [(y −   x0 β)/(1   +   x0 δ)]   ×   (1 + x0 δ). Then
                                                            h                         i
                                                                z−x0 β              0β
                                  Vx (z + tfx (z))     V        1+x0 δ   + tf z−x   0
                                                                                 1+x δ
                                                   =                     h   0
                                                                                 i        → et
                                       Vx (z)                        V    z−x β
                                                                          1+x0 δ


as z → ∞. Thus Vx (z) is Γ-varying with auxiliary function fx . Furthermore, Ux (z) ∼ Vx (z)
and z + tfx (z) → ∞, which implies

                      Ux (z + tfx (z))   Ux (z + tfx (z)) Vx (z) Vx (z + tfx (z))
                                       =                                          → et .
                           Ux (z)        Vx (z + tfx (z)) Ux (z)      Vx (z)

Hence, Ux is also Γ-varying with the same auxiliary function. fx also satisfies (see Resnick,
1987, Ex. 0.4.3.10)
                                                            fx (z)
                                                     lim           → 0.                               (C.9)
                                                     z→∞      z

Combining (C.8) and (C.9), we obtain that for m large enough,

                                            Vx← (ym )           fx (Ux← (ym ))
                                                      ≥ 1 + ε 0                .
                                            Ux← (ym )             Ux← (ym )

Now, because y ∼ Vx (Vx← (y)) and y ∼ Ux (Ux← (y)) (see Resnick, 1987, page 28), for any
ε1 > 0, there exists m large enough such that

                           ym (1 + ε1 ) ≥ Vx (Vx← (ym ))
                                             ≥ Vx (Ux← (ym ) + ε0 fx (Ux← (ym )))
                                             ≥ (1 − ε1 )Ux (Ux← (ym ) + ε0 fx (Ux← (ym )))
                                             = (1 − ε1 )2 eε0 Ux (Ux← (ym ))
                                             ≥ (1 − ε1 )3 eε0 ym .

                     (1−ε1 )3 ε0
Therefore, 1 ≥        1+ε1 e .         Letting ε1 tend to zero leads to a contradiction.




                                                                49
C.6    Proof of Theorem 3.1

First let us introduce additional notations. For any τ , let θ(τ ) = (γ(τ ), β(τ )0 )0 . Let us also
                        b n ) − θ(τn )), with
define Zbn (l) = αn (l)(θ(τ
                                          √                     √
                                         lτn n                    lτn n
                         αn (l) =                    =
                                  γ(mlτn ) − γ(lτn )   Qεe(mlτ /h) − Qεe(lτ /h)

for some arbitrary fixed m > 1 and αn ≡ αn (1). Let also
                                                                                         0
                            Zbn (l1 , · · · , lJ ) = Zbn0 (1), Zbn0 (l1 ), ..., Zbn0 (lJ ) .

Finally, let us define                                          
                                              b(l1 τn ) − γ
                                              γ           b(τn )
                                    ∂gn (δ)           ..        
                             Gn ≡ −        =           .         ⊗ Id ,
                                      ∂δ                        
                                             γb(lJ τn ) − γ
                                                          b(τn )

e n = Gn /(γ(mτn ) − γ(τn )) and
G
                                                                
                                              b 1 τn ) − β(τ
                                              β(l         b n)
                                                      ..        
                                       B
                                       bn = 
                                                       .        .
                                                                 
                                              β(lJ τn ) − β(τn )
                                              b           b


The main part of the proof is devoted to the asymptotic normality of δ.  b The asymptotic
normality of βb and the second part of the theorem follows quite easily.
                                 e n and Zbn (l1 , · · · , lJ ). To see this, note that the first order
The behavior of δb is related to G
condition of (3.3) writes
                                           G0n Wn Gn δb = G0n Wn B
                                                                 bn .

Remarking that δ = [G0n Wn Gn ]−1 G0n Wn Gn δ and B
                                                  bn − Gn δ = gn (δ), we obtain

                           √                 h
                                               e 0 Wn G
                                                         i−1
                                                             e 0 Wn [αn gn (δ)] .
                               τn n(δb − δ) = G  n
                                                      en     G n


Moreover, some algebra shows that

                                   αn gn (δ) = (IJ ⊗ ∆)ΓZbn (l1 , · · · , lJ ).




                                                           50
We thus obtain

                    √                 h
                                        e 0 Wn G
                                                  i−1
                                                      e 0 Wn (IJ ⊗ ∆) ΓZbn (l1 , · · · , lJ ).
                        τn n(δb − δ) = G  n
                                               en     G n                                                   (C.10)


The first step of the proof shows that Zbn (l1 , · · · , lJ ) is asymptotically normal. The proof of
this part is related to the proof of Theorem 5.1 in Chernozhukov (2005), but we have to take
into account that (3.2) is an equivalence, not an equality as in his framework. The second step
                      p
                 e n −→
establishes that G        G/ log(m). Both steps, combined with (C.10), prove the asymptotic
normality of δ.
             b We then show in the third step the main asymptotic result on β.     b Finally,
Step 4 establishes the consistency of βb and the fact that the rate of convergence of δb and βb
can be polynomial under some additional conditions on f (.).

                           d
1. Zbn (l1 , · · · , lJ ) −→ N (0, log(m)−2 L ⊗ Ω0 ).

We prove the result for Zbn (1) only, the multivariate generalization being straightforward
but notationally cumbersome. Similarly to Chernozhukov (2005), Equation (9.43), Zbn (1)
minimizes
                                          Ψn (z, τn ) = Wn (τn )0 z + Λn (z, τn ),

with, for any τ ,

                                                   n
                                  −1 X
                        Wn (τ ) = √    (τ − 1{(Ỹi − γ(τ ) − Xi0 β(τ ) ≤ 0)})X i                            (C.11)
                                    τn
                                               i=1


and for any z = (z1 , z20 )0 ∈ R × Rd ,

                    n     Z     (z1 +Xi0 z2 )/αn
             αn X
Λn (z, τ ) = √                                     1{Ỹi − γ(τ ) − Xi0 β(τ ) ≤ s} − 1{Ỹi − γ(τ ) − Xi0 β(τ ) ≤ 0}ds.
              τn            0
                    i=1
                                                                                                            (C.12)
Λn (z, τn ) is convex in z because the integrands are increasing in s. Moreover, by Lemma D.4
in Appendix D, Λn (z, τn ) →             1
                                         2   log(m)z 0 QH z. We shall now prove that

                                                              d
                                                   Wn (τn ) −→ N (0, QX ).                                  (C.13)

By applying the convexity lemma and the same arguments as in the end of the proof of
Theorem 1 in Pollard (1991), Condition (C.13) implies Zbn (1) + log(m)−1 Q−1
                                                                          H Wn (τn ) = oP (1)
                  d            −2
                                    
and thus Zbn (1) −→ N 0, log(m) Ω0 .




                                                               51
                                          −1
                                                   − 1{Ỹi − γ(τ ) − Xi0 β(τ ) ≤ 0})X i −
                                                                                            p
To establish (C.13), let Mn,i (τ ) =      √
                                           τn
                                              (τ                                                τ /nµ(τ ), with

                                          h                                 i
                                      E        τ − 1{Ỹ ≤ γ(τ ) + X 0 β(τ )} X
                            µ(τ ) ≡                                                   .
                                                                τ

Then
                                                  n
                                                  X                   √
                                    Wn (τ ) =           Mn,i (τ ) +       nτ µ(τ ).                     (C.14)
                                                  i=1

By Lemma 9.6 of Chernozhukov (2005), we have

                                        n
                                                            d
                                        X
                                               Mn,i (τn ) −→ N (0, QX ).                                (C.15)
                                        i=1

Besides,

                              1                                                
                   kµ(τ )k =    kE (hFεe(γ(τ )) − P (D = 1, εe ≤ γ(τ )|X)) X k
                              τ     ( Z                                           )
                                          γ(τ )
                              1
                            =    E X            [h − P (D = 1|X, εe = e)] dFεe(e)
                              τ          −∞
                                  (                                       )
                              1
                            ≤ E kXk sup |h − P (D = 1|X, εe = e)| Fεe(γ(τ ))
                              τ         e≤γ(τ )
                                1
                            =     f (γ(τ )).
                                h
                       √
By Assumption 9,           nτn f (γ(τn )) = o(1). Combined with (C.14) and (C.15), this proves
(C.13).

        p
   e n −→
2. G      G/ log(m) and asymptotic normality of δ.
                                                b
First,

          γb(lτn ) − γ
                     b(τn )   b(lτn ) − γ(lτn )
                              γ                   γ(lτn ) − γ(τn )   γ(τn ) − γ
                                                                              b(τn )
                            =                   +                  +                 .                  (C.16)
          γ(mτn ) − γ(τn )    γ(mτn ) − γ(τn ) γ(mτn ) − γ(τn ) γ(mτn ) − γ(τn )

                               b and Zbn (l), Step 1 of the proof and because τn n → ∞ by
Besides, by definition of αn , θ(.)
Assumption 9,

                   b(lτn ) − γ(lτn )
                   γ                  e0 αn (θ(lτ
                                             b n ) − θ(lτn ))  e0 Zbn (l)
                                     = 1       √              = 1         = oP (1),
                   γ(mτn ) − γ(τn )               τn n            τn n

where e1 = (1, 0, ..., 0)0 . Similarly, the third term of (C.16) also tends to zero in probability.




                                                          52
Now, by Lemma D.3 in Appendix D,

                       γ(lτn ) − γ(τn )  Q (lτn /h) − Qεe(τn /h)      log(l)
                                        = εe                      −→         .
                       γ(mτn ) − γ(τn )  Qεe(mτn /h) − Qεe(τn /h)    log(m)

Hence,
                                          b(mτn ) − γ
                                          γ         b(τn ) p log(l)
                                                           −→        ,
                                          γ(mτn ) − γ(τn )    log(m)
                                    p
                               e n −→
which in turn establishes that G      G/ log(m). Combined with Step 1 and (C.10), this
shows that
                                          √                 d
                                              τn n(δb − δ) −→ N (0, Ωδ ),

where Ωδ = (G0 W G)−1 G0 W (IJ ⊗∆)Γ(L⊗Ω0 )Γ0 (IJ ⊗∆0 )W G(G0 W G)−1 . The optimal weight-
ing matrix is then (see, e.g., Wooldridge, 2002, Problem 8.5)

                                                                    −1
                               Wδ∗ = (IJ ⊗ ∆)Γ(L ⊗ Ω0 )Γ0 (IJ ⊗ ∆0 )
                                    
                                                                        ,

and the corresponding asymptotic variance is Ω∗δ = (G0 Wδ∗ G)−1 .

3. Asymptotic normality of β.
                           b

Consider first βbj = −β(l        b(lj τn )δb for j ∈ {0, ..., J}. We have
                      b j τn ) + γ

               √                                                         
                 τn n b          γ(lj τn ) √         b(lj τn ) − γ(lj τn )
                                                     γ                           √
                      (βj − β) =             τn n                            δ̂ + τn n(δb − δ)
               γ(τn )             γ(τn )                   γ(lj τn )
                                              √                               
                                                τn n b 
                                           −             β(lj τn ) − β(lj τn ) .                           (C.17)
                                             γ(lj τn )

By Lemma D.1 in Appendix D, γ(.) ∈ RV0 (0). Thus, the first ratio on the right-hand side
tends to one. We now show that the first and third term in the brackets are oP (1). We have
                                                                   
       √        b(lj τn ) − γ(lj τn )
                γ                             γ(mτn ) − γ(lj τn )                                     
           τn n                       =                                  e01 αn (θ(l
                                                                                 b j τn ) − θ(lj τn )) .   (C.18)
                      γ(lj τn )                   γ(lj τn )

The second term of the right-hand side is e01 Z(l
                                              b j ) and is therefore bounded in probability
uniformly over j. Because γ(.) ∈ RV0 (0), the first term converges to 0. Thus, the first term
in the brackets of the right-hand side of (C.17) is a oP (1). The same reasoning applies to the
third term in the brackets in (C.17).

Hence,                            √
                                    τn n b         √
                                         (βj − β) = τn n(δb − δ) + oP (1).
                                  γ(τn )


                                                          53
Now, because βb = Jj=0 βbj /(J + 1), we obtain
                 P

                                    √
                                      τn n b        d
                                           (β − β) −→ N (0, Ωδ ).
                                    γ(τn )

                                                                                     \                    p
It also follows from the fact that the left-hand side of (C.18) converges to 0 that γ(l j τn )/γ(lj τn ) −→
                       [              p                                                        [
1, and in particular γ(τ  n )/γ(τn ) −→ 1. This implies that γ(τn ) can be replaced by γ(τn ) in

the equation above.

4. Consistency of βb and polynomial rates of convergence.

First, suppose that f (u) = o(|u|−a ) as u → −∞, for some a > 1. Fix α ∈ (0, 1) such
that a(1 − α) > 1 and let τn = τn∗ be defined by (3.4). As shown in the discussion before
Theorem 3.1, such a τn satisfies Assumption 9. Moreover, since γ(τn ) = G−1 (1/n) (with
G(γ) = Fεe(γ)f (γ)2(1−α) ),
                                √               √
                                  τn n            h
                                       = 1−α −1                .
                                γ(τn )  f   (G (1/n))G−1 (1/n)

Because f 1−α (u)u → 0 as u → −∞ and G−1 (1/n) → −∞, we get
                                                  √
                                                    τn n
                                            lim          = −∞.
                                          n→∞     γ(τn )

Thus, βb is consistent with such a choice of τn .

Now, suppose that f (u) = o(Fεe(u)a ) for some a > 0. Consider in this case τn = n−1/(2a+1) .
Then τn → 0 and nτn → ∞. Because f (γ(τ )) = o(τ a ), we also have

                        √                                         
                            τn nf (γ(τn )) = na/(2a+1) o n−a/(2a+1) = o(1).

Hence, this choice of τn satisfies Assumption 9. Besides, γ(.) ∈ RV0 (0). This implies that for
any α > 0, |γ(τn )| < τn−α for n large enough. Choose 0 < α < a. Then, for n large enough,
                                        √
                                           τn n
                                                 > n(a−α)/(2a+1) .
                                        |γ(τn )|

This ensures that βb has a polynomial rate of convergence. With such a τn , the rate of
convergence of δb is na/(2a+1) , which is also polynomial. This concludes the proof of Theorem
3.1.




                                                      54
C.7    Verification of (2.4) and (3.5) for several copulas

Case 1: Gaussian copula with ρ > 0. We just check (3.5), which is stronger than (2.4). We
have, after some algebra,
                                       Z   Φ−1 (v)
                             1                                1
1 − ∂1 Cρ (u, v) = 1 −                                             exp(−(Φ−1 (u)2 − 2ρΦ−1 (u)s + s2 )/[2(1 − ρ2 )])ds
                         ϕ(Φ−1 (u))
                                                          p
                                          −∞         2π    1 − ρ2
                                 2   −1    2         2    Z Φ−1 (v)
                    e−(1−ρ )Φ (u) /[2(1−ρ )]                             1
                                                                                 exp −(s − ρΦ−1 (u))2 /[2(1 − ρ2 )] ds
                                                                                                                    
                =1−     √                                           p
                          2πϕ(Φ−1 (u))                     −∞         (2π)1 − ρ2
                                       !
                    ρΦ−1 (u) − Φ−1 (v)
                =Φ       p               .
                           1 − ρ2

Thus, because ρ > 0,
                                                                                          !
                                                                  ρΦ−1 (τ ) − Φ−1 (v)
                          sup        1 − ∂1 Cρ (u, v) = Φ             p                       .
                       v∈[v,1],u≤τ                                       1 − ρ2

Now, as x → −∞, we have Φ(x) ∼ −ϕ(x)/x. Because for any K > 0, exp(−Kx2 ) ≤ −1/x ≤ 1
for x small enough, we have ϕ(x/σ) ≤ Φ(x) ≤ ϕ(x) for any 0 < σ < 1. This also implies that
Φ−1 (τ ) ≤ σϕ−1 (τ ), for τ small enough and with ϕ−1 the inverse of ϕ on (−∞, 0]. Similarly,
for any m > 0, there exists σ > 1 such that for any x small enough, ϕ(x + m) ≤ ϕ(x/σ).
                                                       p
Combining these inequalities, we obtain, for any K < ρ/ 1 − ρ2 ,

                                                                       2    √        K 2 −1 K 2
                    fC (τ ) ≤ ϕ(Kϕ−1 (τ )) = K 0 ϕ(Φ−1 (τ ))K ≤                 2π        τ       .

The result follows.
Case 2: Archimedean copulas with limu→0 Ψ(u) = +∞ and Ψ ∈ RVα (0) with α ∈ (0, +∞].
Because Ψ is decreasing, we have, by Proposition 0.8 of Resnick (1987), Ψ−1 ∈ RV1/α (∞).
As a result, for all v ∈ [v, 1],

                  u ≥ C(u, v) ≥ Ψ−1 (Ψ(u) + Ψ(v)) ∼ Ψ−1 (Ψ(u)) = u as u → 0.

In other words,
                                       lim sup |C(u, v)/u − 1| = 0.
                                       u→0 v∈[v,1]

This implies that
                                           Ψ0 (u)          Ψ0 (u)
                              sup                   − 1 =            −1 .                             (C.19)
                             v∈[v,1]   Ψ0 (C(u, v))       Ψ0 (l(u)u)

for some function l(.) tending to one as u → 0. Now, by Proposition 0.7 of Resnick (1987),

                                                          55
Ψ0 ∈ RVα−1 (0). This implies that the left-hand side of (C.19) tends to 0. (2.4) follows by
remarking that ∂1 C(u, v) = Ψ0 (u)/Ψ0 ◦ C(u, v).

Case 3: Gumbel copulas with θ > 1. Some algebra yields

                                                  1          C(u, v) log C(u, v)
                          ∂1 C(u, v) =                                           .
                                         1 + Ψ(v; θ)/Ψ(u; θ)       u log u

Now, by the fact that x log(x) is decreasing when x is close to 0 and C(u, v) ≤ u, we have
                                           C(u,v) log C(u,v)
C(u, v) log C(u, v) ≥ u log(u), i.e.            u log u        ≤ 1. Because v 7→ C(u, v) is increasing,
C(u, v) log C(u, v) ≤ C(u, v) log C(u, v). Furthermore, 0 ≤ Ψ(v, θ) ≤ Ψ(v, θ). Therefore, we
have

          sup |∂1 C(u, v) − 1|
         v∈[v,1]
                                                                            
                  C(u, v) log C(u, v)                    C(u, v) log C(u, v)
       ≤ sup                          − 1 + ∂1 C(u, v) −
        v∈[v,1]         u log u                                u log u
                                                                   
                    C(u, v) log C(u, v)                  Ψ(v, θ)        C(u, v) log C(u, v)
       ≤ sup 1 −                          + sup
        v∈[v,1]             u log u         v∈[v,1] Ψ(v, θ) + Ψ(u, θ)         u log u
               C(u, v) log C(u, v)         Ψ(v, θ)
       ≤(1 −                       )+
                     u log u          Ψ(v, θ) + Ψ(u, θ)

Ψ(u, θ) → ∞ as u → 0, so the second term also converges 0. Therefore, to prove (2.4), it
suffices to show that C(u, v) ∼ u. We have, for θ > 1,
                                                               1/θ 
                                                 θ             θ
                       C(u, v) = exp − (− log u) + (− log v)
                                                                  
                                                          θ !1/θ
                                                   − log v
                               = exp log u 1 +                    
                                                   − log u

                                               (− log v)θ
                                                                              
                                                                        1
                               = exp log u +                +o
                                             θ(− log u)θ−1         (− log u)θ−1
                              ∼ u.


Case 4: Clayton copula with θ > 0. We obtain in this case
                                                                            
                                                                     1
                                   1 − ∂1 C(u, v; θ) ≤ Kuθ              −1
                                                                     vθ
                                                        
Hence, fC (τ ) ≤ K 0 τ θ , where K 0 = K         1
                                                 vθ
                                                      − 1 . (3.5) follows.




                                                         56
Case 5: Rotated Gumbel-Barnett copula with θ ∈ (0, 1]. We have
                                                                               
 1−∂1 C(u, v; θ) = (1−v) exp(−θ log(u) log(1−v))(1−θ log(1−P )) ≤ O u−θ log(1−v) (C.20)

It follows that (3.5) holds.

Case 6: C(u, v; θ) = (1 + [(u−1 − 1)θ + (v −1 − 1)θ ]1/θ )−1 with θ > 1 . In this case,

                                                                    2 "                       θ #1/θ−1
                                                                                    v −1 − 1
                                                                               
                                             1
 1 − ∂1 C(u, v; θ) = 1 −                                                   1+                              ≤ Ku.
                               u + [(1 − u) + uθ (v −1 − 1)θ ]1/θ
                                           θ                                        u−1 − 1

(3.5) follows.

Case 7: C(u, v; θ) = (1 + [(u−1/θ − 1)θ + (v −1/θ − 1)θ ]1/θ )−θ with θ ≥ 1. We have
                                                                                
                                                                                                1
                                                                                                     !θ 1/θ−1
                                                                     −θ−1               v− θ − 1
  ∂1 C(u, v; θ) = 1 − u1/θ + [(1 − u1/θ )θ + u(v −1/θ − 1)θ ]1/θ                1 +            1
                                                                                                           
                                                                                          u− θ − 1

                 ≤ Ku1/θ

which implies (3.5).

Case 8: C(u, v; θ) = θ/ log(exp(θ/u) + exp(θ/v) − exp(θ)) with θ > 0. We have

                                                                                                        1
1 − ∂1 C(u, v; θ) = 1 − 1/(1 + log(1 + (exp(θ/v) − exp(θ)) exp(−θ/u)))2
                                                                                        1 + (exp(θ/v) − exp(θ)) exp(−θ/u)
                  ≤ K exp(−θ/u)

Thus Condition (3.5) is easily satisfied. In this case, any polynomial rate slower than the
parametric rate is in fact possible.

Case 9: C(u, v; θ) = [log(exp(u−θ ) + exp(v −θ ) − e)]−1/θ with θ > 0. Start from

                                                                 −1/θ−1
                                                  exp(v −θ ) − e
                                           
                                        θ                                       1
         1 − ∂1 C(u, v; θ) = 1 − 1 + u log 1 +              −θ                      −θ )−e
                                                     exp(u )              1 + exp(v
                                                                               exp(u−θ )
                                                                     
                           ≤ K1 uθ log 1 + [exp(v −θ ) − e] exp(−u−θ ) + K2 exp(−u−θ )

                           ≤ K exp(−u−θ )

Therefore, Condition (3.5) is easily satisfied and once more, any polynomial rate slower than
parametric rate is possible.




                                                    57
C.8       Proof of Theorem 3.2

We use the notations of the proof of Theorem 3.1 (see Section C.6) along with the notations
introduced in the text before Theorem 3.2. First consider δ 1 . Let
                                                                         
                                                       b(l1 τn ) − γ
                                                       γ           b(τn )
                                        ∂g2n (δ 1 )            ..        
                              G2n    =−            =            .         ⊗ Id .
                                          ∂δ 1                                δ

                                                       b(lJ τn ) − γ
                                                       γ           b(τn )

Reasoning as in the proof of Theorem 3.1, above Equation (C.10),

                 √                                   −1 0
                     τn n(δb1 − δ 1 ) = G02n W2n G2n
                                       
                                                        G2n W2n (IJ ⊗ Ψ∆)Γ
                                                                      e   Zbn (l1 , ..., lJ ).

                                            p
Besides, one can show that G2n −→ Gδ / log(m) as in the proof of Theorem 3.1. Combined
with the asymptotic normality of Zbn (τ ), this implies that

√                    d
                                    −1 0                                                       
                                                                              e 0 )W2 Gδ G0 W2 Gδ −1 .
    τn n(δb1 −δ 1 ) −→ N 0, G0δ W2 Gδ                       ⊗ Ω0 )Γ0 (IJ ⊗ ∆0 Ψ
                                                                                        
                                         Gδ W2 (IJ ⊗ Ψ∆)Γ(L
                                                     e
                                                                                          δ


The optimal weighting matrix is then
                                    h                                    i−1
                              Wδ∗1 = (IJ ⊗ Ψ∆)Γ(L
                                           e      ⊗ Ω0 )Γ0 (IJ ⊗ ∆0 Ψ
                                                                    e 0)


and the corresponding asymptotic variance is (G0δ Wδ∗1 Gδ )−1 .

Next, we derive the asymptotic properties of βb1 . Reasoning as previously, we have

         log(m)αn (βb1 − β 1 ) = (G0β W1 Gβ )−1 G0β W1 (Γ3 ⊗ ΨΓ2 ) log(m)Zbn (l1 , ..., lJ ) + oP (1),

                                 √
Remark that log(m)αn =     nτn λn /γ(τn ), with λn = γ(τn ) log(m)/[γ(mτn ) − γ(τn )]. Then the
asymptotic normality of Zn (l1 , ..., lJ ) yields
                        b
     √
       τn n b1 1 d
            (β −β ) −→ N 0, (G0β W1 Gβ )−1 G0β W1 (Γ3 ⊗ ΨΓ2 )(L ⊗ Ω0 )(Γ03 ⊗ Γ02 Ψ0 )W1 Gβ (G0β W1 Gβ )−1 .
                                                                                                         
λn
     γ(τn )

                                                        p
                                                bn /λn −→        [              p
It follows from the proof of Theorem 3.1 that λ            1 and γ(τn )/γ(τn ) −→ 1. Therefore,

we can replace λn by λ                  [
                       bn and γ(τn ) by γ(τ n ) in the previous equation. Finally, the optimal

matrix is
                                                                         −1
                               Wβ∗1 = (Γ3 ⊗ ΨΓ2 )(L ⊗ Ω0 )(Γ03 ⊗ Γ02 Ψ0 )
                                     


and the corresponding asymptotic variance is (G0β Wβ∗1 Gβ )−1 .


                                                       58
D     Technical lemmas
Lemma D.1. If Assumption 3 (ii)-(iii) hold, then Sε is rapidly varying at +∞, i.e. its
extreme value index is 0. Moreover, Qεe ∈ RV0 (0).

Proof. Because sup(Supp(ε)) = ∞, Sε is not in the attraction domain of type III extreme
value distributions (see Resnick, 1987, Proposition 1.13). Suppose Sε is not rapidly varying.
Then, Sε is not either in the attraction domain of type I extreme value distribution (See
Resnick, 1987, Exercise 1.1.9). So Sε is in the attraction domain of type II extreme value
distribution, i.e. Sε ∈ RV−ξ−1 (+∞) with extreme value index ξ > 0. We also have

                                    Sexp(ε) (tx)   Sε (u(x) log(x))
                                                 =                                       (D.1)
                                    Sexp(ε) (x)       Sε (log(x))

               log(t)+log(x)
where u(x) =       log(x)      → 1 as x → +∞. Because Sε ∈ RV−ξ−1 (+∞), the right-hand side
of Equation (D.1) converges to 1. This implies that Sexp(ε) is slowly varying, a contradiction.
Thus, Sε is rapidly varying at +∞.

To prove the second result, note that 1/Sε is nondecreasing, rapidly varying at +∞ and
satisfies 1/Sε (+∞) = +∞. Thus, by Proposition 0.8 of Resnick (1987), (1/Sε )← ∈ RV0 (∞).
Remark that (1/Sε )← (1/τ ) = −Qεe(τ ). Hence, Qεe ∈ RV0 (0).

Lemma D.2. Suppose that Assumptions 3 (ii)- (iii) and 8 hold. Then Qεe(eτ ) − Qεe(τ ) ∈
RV0 (0), Q0εe ∈ RV−1 (0) and for any positive (l, m),

                                        Qεe(lτ ) − Qεe(τ )    log(l)
                                   lim                     =         .
                                   τ →0 Qεe(mτ ) − Qεe(τ )   log(m)

Proof. We first prove the last point. By Lemma D.1, Fε is in the attraction domain of type
I distribution. Then by Proposition 0.10 in Resnick (1987), τ 7→ −Qε̃ (τ ) is Π-varying with
auxiliary function τ 7→ Qεe(eτ ) − Qεe(τ ), namely

                                            Qεe(lτ ) − Qεe(τ )
                                     lim                       = log(l)                  (D.2)
                                     τ →0   Qεe(eτ ) − Qεe(τ )

for all l > 0. Then

             Qεe(lτ ) − Qεe(τ )  Q (lτ ) − Qεe(τ ) . Qεe(mτ ) − Qεe(τ )    log(l)
                                = εe                                    →         .
             Qεe(mτ ) − Qεe(τ )  Qεe(eτ ) − Qεe(τ ) Qεe(eτ ) − Qεe(τ )    log(m)




                                                     59
Turning to the first point, we have, by (D.2),

     Qεe(exτ ) − Qεe(xτ )  Q (exτ ) − Qεe(τ ) Qεe(xτ ) − Qεe(τ )
                          = εe                 −                    → log(ex) − log(x) = 1.
      Qεe(eτ ) − Qεe(τ )    Qεe(eτ ) − Qεe(τ )   Qεe(eτ ) − Qεe(τ )

Finally, let us prove the second point. By monotonicity of Q0εe,

                   Q0εe(bτ )τ (b − a)  Q (bτ ) − Qεe(aτ )   Q0 (aτ )τ (b − a)
                                      ≥ εe                 ≥ εe                ,
                   Qεe(eτ ) − Qεe(τ )   Qεe(eτ ) − Qεe(τ )  Qεe(eτ ) − Qεe(τ )

for any b > a > 0. Therefore, using (D.2),

                                          Q0εe(aτ )τ       log(b) − log(a)
                           lim sup                       ≤                 .
                             τ →0     Qεe(eτ ) − Qεe(τ )        b−a

Letting b ↓ a, we obtain
                                                   Q0εe(aτ )τ      1
                                    lim sup                       ≤ ,
                                      τ →0     Qεe(eτ ) − Qεe(τ )  a
for any a > 0. Similarly, we obtain from the other inequality

                                                   Q0εe(bτ )τ      1
                                     lim inf                      ≥ ,
                                      τ →0     Qεe(eτ ) − Qεe(τ )  b

for any b > 0. By letting a = b = 1, we obtain

                                                    Qεe(eτ ) − Qεe(τ )
                                       Q0εe(τ ) ∼                                             (D.3)
                                                            τ

This, combined with Qεe(eτ ) − Qεe(τ ) ∈ RV0 (0), shows the second point.

Lemma D.3. Suppose that Assumptions 1-4 and 8 hold. Then, for all x ∈ Supp(X),

                                    Qε̆|X (τ |x) − Qεe(τ /h)(1 + x0 δ)
                            lim                                        = 0,
                            τ →0            Qεe(mτ ) − Qεe(τ )

                           fε̆|X Qε̆|X (τ |x) ∼ hfε̃ (Qε̃ (τ /h))/(1 + x0 δ).
                                             
                                                                                              (D.4)

Proof. For the first point, fix ∆ ∈ (0, h) and remark first that by Lemma D.2,
                                                                                               
       Q (τ /(h + ∆)) − Qεe(τ /h)               Qεe(τ /(h + ∆)) − Qεe(τ ) Qεe(τ /h) − Qεe(τ )
  lim εe                                = lim                             −
  τ →0     Qεe(mτ ) − Qεe(τ )             τ →0      Qεe(mτ ) − Qεe(τ )       Qεe(mτ ) − Qεe(τ )
                                          − log(h + ∆)       log(h)    log[h/(h + ∆)]
                                        →                +          =                         (D.5)
                                              log(m)        log(m)         log(m)

and the same holds replacing ∆ by −∆.


                                                        60
Besides, by definition of the quantiles of ε̆|X = x, we have, for all τ small enough,

                    τ   ≤ P (ε̆ ≤ Qε̆|X (τ |x)|X = x)
                        = P (Y ≥ x0 β − Qε̆|X (τ |x)|X = x)
                        = P (Y ∗ ≥ x0 β − Qε̆|X (τ |x), D = 1|X = x)
                          Z ∞
                                                                     ∗
                        =                P (D = 1|Y ∗ = y, X = x)dP Y |X=x (y).
                             x0 β−Qε̆|X (τ |x)


For τ small enough, P (D = 1|Y ∗ = y, X = x) ∈ [h − ∆, h + ∆] for all y > x0 β − Qε̆|X (τ |x).
Thus,
                              τ ≤ (h + ∆)P εe(1 + x0 δ) ≥ Qε̆|X (τ |x) .
                                                                     


Similarly, using τ ≥ (h − ∆)P (ε̆ < Qε̆|X (τ |x)|X = x),

                              τ ≥ (h − ∆)P εe(1 + x0 δ) ≥ Qε̆|X (τ |x) .
                                                                     


Then, by definition of the quantiles of εe,

               (1 + x0 δ)Qεe(τ /(h + ∆)) ≤ Qε̆|X (τ |x) ≤ (1 + x0 δ)Qεe(τ /(h − ∆)).

This, together with Equation (D.5)

                    Qε̆|X (τ |x) − Qεe(τ /h)(1 + x0 δ)
          lim sup
            τ               Qεe(mτ ) − Qεe(τ )
                              max (Qεe(τ /(h − ∆)) − Qεe(τ /h), Qεe(τ /h) − Qεe(τ /(h + ∆)))
        ≤(1 + x0 δ) lim sup
                        τ                             Qεe(mτ ) − Qεe(τ )
                    max (log(h/(h − ∆)), log((h + ∆)/h))
        ≤(1 + x0 δ)                                            .
                                      log(m)

By letting ∆ tend to 0, the left-hand side tends to zero. The first result follows.

Now let us turn to the second result. We first show that for any fixed x, Qε̆|X (τ |x) is
Π−varying. We have

Qε̆|X (mτ |x) − Qε̆|X (τ |x) Qε̆|X (mτ |x) − (1 + x0 δ)Qεe(mτ /h) Qε̆|X (τ |x) − (1 + x0 δ)Qεe(τ /h)
                            =                                        −
     Qεe(eτ ) − Qεe(τ )                Qεe(eτ ) − Qεe(τ )                  Qεe(eτ ) − Qεe(τ )
                                          Q  (mτ  /h)  −  Q   (τ /h)
                              + (1 + x0 δ) εe               εe
                                              Qεe(eτ ) − Qεe(τ )

By Lemma D.2, τ 7→ Qεe(eτ ) − Qεe(τ ) is slowly varying. Thus, by the first result of this
lemma, the first and second term converge to zero. Since Qεe(τ ) is Π−varying, the third term


                                                 61
converges to (1 + x0 δ) log(m). Therefore

                          Qε̆|X (mτ |x) − Qε̆|X (τ |x)
                                                       ∼ (1 + x0 δ) log(m).
                               Qεe(eτ ) − Qεe(τ )

Then

  Qε̆|X (mτ |x) − Qε̆|X (τ |x)   Qε̆|X (mτ |x) − Qε̆|X (τ |x)      Qεe(eτ ) − Qεe(τ )
                               =                                                           → log(m),
  Qε̆|X (eτ |x) − Qε̆|X (τ |x)        Qεe(eτ ) − Qεe(τ )      Qε̆|X (eτ |x) − Qε̆|X (τ |x)

which proves that Qε̆|X (.|x) is Π−varying. Now, remark that for y small enough,

           P (ε̆ ≤ y|X = x) = P (Ỹ + X 0 β ≤ y|X = x)
                               = P (Y ∗ ≥ −y + x0 β, D = 1|X = x)
                                         y − x0 β
                                                               
                               = P ε̃ ≤            |D = 1, X = x P (D = 1|X = x).
                                          1 + x0 δ

This equality, combined with Assumption 8 and the fact that X is bounded, ensures that the
cdf of ε̆|X is increasing. As a result, Q0ε̆|X (.|x) is decreasing at the lower tail and we have, by
the same reasoning as in Lemma D.2,

                           Qε̆|X (τ |x)0 ∼ τ (Qε̆|X (mτ |x) − Qε̆|X (τ |x)).                       (D.6)

Combining Equations (D.3) and (D.6), we obtain

                    Qεe(τ /h)0           (Qεe(mτ ) − Qεe(τ ))             1
                                  ∼                                 ∼
                    Qε̆|X (τ |x)0   h(Qε̆|X (mτ |x) − Qε̆|X (τ |x))   h(1 + x0 δ)

This proves the second result of the lemma.

Lemma D.4. Suppose that Assumptions 1 - 9 hold and let Λn (z, τ ) be defined as in (C.12).
Then
                                                  p   1
                                  Λn (z, τn ) −→        log(m)z 0 QX z.
                                                      2
Proof. By Lemma 9.6 in Chernozhukov (2005), the variance of Λn (z, τ ) converges to 0. Thus
it suffices to prove that E[Λn (z, τn )] →   1
                                             2   log(m)z 0 QH z. Let us define, for any (s, t) ∈ R2 ,

                                                  1        if 0 < s ≤ t,
                                  m(s, t) =       −1 if t ≤ s < 0,
                                                  0        otherwise.




                                                      62
We have

 E [Λn (z, τn )]
             "Z                                                                                        #
                   (z1 +X 0 z2 )/αn
  αn                                                    0                                 0
=√      nE                          1{Ỹ − γ(τn ) − X β(τn ) ≤ s} − 1{Ỹ − γ(τn ) − X β(τn ) ≤ 0}ds
   τn n          0
           "Z                                                                                        #
                z1 +X 0 z2
   n
=√      E                    1{Ỹ − γ(τn ) − X 0 β(τn ) ≤ s/αn } − 1{Ỹ − γ(τn ) − X 0 β(τn ) ≤ 0}ds
   τn n       0
           "Z                                                                                            #
                z1 +X 0 z2
   n
=√      E                    1{ε̆ − (1 + X 0 δ)Qεe(τn /h) ≤ s/αn } − 1{ε̆ − (1 + X 0 δ)Qεe(τn /h) ≤ 0}ds
   τn n       0
    "Z                                                                                         #
          z1 +X 0 z2 F       ((1  + X 0 δ)Q (τ /h) + s/α ) − F         ((1 + X 0 δ)Q (τ /h))
                        ε̆|X               ε  n             n     ε̆|X              ε  n
=nE                                                     √                                    ds
                                           e                                        e

        0                                                 τn n
                                     nfε̆|X [(1 + X 0 δ)Qεe(τn /h) + Vs ]
   Z +∞                                                                    
                                0
=E           m(s, z1 + X z2 )s                        √                   ds ,                       (D.7)
      −∞                                          αn τn n

where for each s, Vs is a random variable satisfying Vs ∈ [0, s/αn ]. Let

                                                  nfε̆|X [(1 + X 0 δ)Qεe(τn /h) + Vs ]
                   Un (s) = m(s, z1 + X 0 z2 )s                    √                   .
                                                               αn τn n

We first show that
                                       p.s.   m(s, z1 + X 0 z2 )s log(m)
                                Un (s) −→                                .                         (D.8)
                                                      1 + X 0δ
Since 1/αn = o(Qεe(mτn )−Qεe(τn )), we have Vs = o(Qεe(mτn )−Qεe(τn ))). Moreover, by Lemma
D.3,
                    Qε̆|X (τn |x) − Qεe(τn /h)(1 + x0 δ) = o(Qεe(mτn ) − Qεe(τn )).

Then, following the same argument as Chernozhukov (2005) after his Equation (9.57),

                               fε̆|X [(1 + X 0 δ)Qεe(τn /h) + Vs ] p
                                                                 −→ 1.                            (D.9)
                                       fε̆|X Qε̆|X (τn |x)

Besides, by Lemma D.3,

                          fε̆|X Qε̆|X (τn |x) ∼ hfε̃ (Qε̃ (τn /h))/(1 + x0 δ).
                                             
                                                                                                  (D.10)




                                                    63
Now, by definition of αn and because Q0ε̃ ∈ RV−1 (0) by Lemma D.2,

                 nhfε̃ (Qε̃ (τn /h))   h(Qεe(mτn /h) − Qεe(τn /h))fεe(Qεe(τn /h))
                        √            =
                    αn τn n                                τn
                                       Z m 0                 Z m 
                                             Qεe(sτn /h)              ds
                                     =          0        ds →             = log(m),                                  (D.11)
                                         1   Qεe(τn /h)          1    s

                                                                                                              Q0εe(sτn /h)
where the second last convergence is because, by Proposition 0.5 of Resnick (1987),                           Q0εe(τn /h)
                                                                                                                             →
1                        1
s   locally uniformly.   s   is bounded over [1, m], so dominated convergence theorem can be ap-
plied. Combining (D.9), (D.10) and (D.11) proves that (D.8) holds.

Next, we prove that for n large enough,
                                                                  Z     ∞             
                             |Un (s)| ≤ U (s),       with     E              U (s)ds       < ∞.                      (D.12)
                                                                     −∞


Together with (D.8), this will allow us to use the dominated convergence theorem on the
right-hand side of (D.7). We bound |Un (s)| for |s| ≤ |z1 + X 0 z2 |, since m(s, z1 + X 0 z2 ) = 0
otherwise.

First, because X is bounded, supx∈Supp(X) γ(τn ) + x0 β(τn ) → −∞. Thus, for any |s| ≤
|z1 + X 0 z2 |, we have, for n large enough, γ(τn ) + X 0 β(τn ) < 0 and γ(τn ) + X 0 β(τn ) + s/αn < 0.
Hence, by definition of Ye and Y ∗ ,

{Ye ∈ (γ(τn )+X 0 β(τn ), γ(τn )+X 0 β(τn )+s/αn ]} ⊂ {−Y ∗ ∈ (γ(τn )+X 0 β(τn ), γ(τn )+X 0 β(τn )+s/αn ]}.

Taking conditional expectations, this implies that for any |s| ≤ |z1 +X 0 z2 | and n large enough,
                                                                         
     |s|            0
                                                                s
         f     (1 + X δ)Qεe(τn /h) + Vs ≤ Fεe Qεe(τn /h) +                  − Fεe (Qεe(τn /h)) .
     αn ε̆|X                                               αn (1 + X 0 δ)

By the mean value theorem,

                                                                                  fεe (Qεe(τn /h) + Vs0 )
                                              
                                    s
         Fεe Qεe(τn /h) +                          − Fεe (Qεe(τn /h)) = |s|                               ,          (D.13)
                              αn (1 + X 0 δ)                                           αn (1 + X 0 δ)

where Vs0 ∈ [0, s/(αn (1 + X 0 δ))]. Because s/(1 + x0 δ) is bounded for all |s| ≤ |z1 + x0 z2 | and
all x ∈ Supp(X), |Vs0 | ≤ K/αn . Now, by Lemma D.2, we have, for any η > 0,

                                                              √    Qεe((1 + η)τn /h) − Qεe(τn /h)
             αn [Qεe((1 + η)τn /h) − Qεe(τn /h)] =                τn n
                                                                      Qεe(mτn /h) − Qεe(τn /h)
                                                             √     log(1 + η)
                                                            ∼ τn n            → ∞.
                                                                     log(m)

                                                            64
Hence, for n large enough,

                                                           K
                    Qεe((1 + η)τn /h) ≥ Qεe(τn /h) +          ≥ Qεe(τn /h) + Vs0 .
                                                           αn

Plugging this inequality in (D.13) and using monotonicity of fεe, we obtain

                 |s|
                     fε̆|X (1 + X 0 δ)Qεe(τn /h) + Vs ≤ fεe (Qεe((1 + η)τn /h)) .
                                                    
                 αn

Because 1 + X 0 δ is bounded from below, we finally get

                                                           nfεe (Qεe((1 + η)τn /h))
                  Un (s) ≤ K|s|1{|s| ≤ |z1 + X 0 z2 |}                √             .
                                                                   αn τn n
                                                                √
We have shown in (D.11) that the sequence nfεe (Qεe(τn /h)) /(αn τn n) admits a finite limit.
It is therefore bounded. Hence, we finally have, for n large enough, |Un (s)| ≤ U (s) with
U (s) = K|s|1{|s| ≤ |z1 + X 0 z2 |}. Thus, (D.12) holds and by the dominated convergence
theorem applied to (D.7).
                                  "                                 #
                                             Z    (z1 +X 0 z2 )
                                  log(m)                                  1
              E [Gn (z, τn )] → E                                 sds =     log(m)z 0 QH z.
                                  1 + X 0δ    0                           2




                                                 65
