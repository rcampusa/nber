                                 NBER WORKING PAPER SERIES




            FORECASTS IN A SLIGHTLY MISSPECIFIED FINITE ORDER VAR

                                           Ulrich K. Müller
                                           James H. Stock

                                         Working Paper 16714
                                 http://www.nber.org/papers/w16714


                      NATIONAL BUREAU OF ECONOMIC RESEARCH
                               1050 Massachusetts Avenue
                                 Cambridge, MA 02138
                                     January 2011




This research was funded in part by NSF grant SBR-0617811 (Stock). We thank participants of workshops
at Columbia, Harvard, and Montreal, and at the Greater New York Econometrics Colloquium for helpful
comments, and Adam Clark-Joseph for excellent research assistance. The views expressed herein are
those of the authors and do not necessarily reflect the views of the National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2011 by Ulrich K. Müller and James H. Stock. All rights reserved. Short sections of text, not to
exceed two paragraphs, may be quoted without explicit permission provided that full credit, including
© notice, is given to the source.
Forecasts in a Slightly Misspecified Finite Order VAR
Ulrich K. Müller and James H. Stock
NBER Working Paper No. 16714
January 2011
JEL No. C11,C22,C32

                                            ABSTRACT

We propose a Bayesian procedure for exploiting small, possibly long-lag linear predictability in the
innovations of a finite order autoregression. We model the innovations as having a log-spectral density
that is a continuous mean-zero Gaussian process of order 1/√T. This local embedding makes the problem
asymptotically a normal-normal Bayes problem, resulting in closed-form solutions for the best forecast.
When applied to data on 132 U.S. monthly macroeconomic time series, the method is found to improve
upon autoregressive forecasts by an amount consistent with the theoretical and Monte Carlo calculations.


Ulrich K. Müller
Department of Economics
Princeton University
Princeton, NJ 08544-1013
umueller@princeton.edu

James H. Stock
Department of Economics
Harvard University
Littauer Center M27
Cambridge, MA 02138
and NBER
James_Stock@harvard.edu
1       Introduction
Low-order autoregressions provide good benchmark forecasts for many economic time series,
yet there is reason to suspect that some small amount of linear predictability remains be-
yond the initial autoregressive approximation. One indication of this residual predictability
is the diﬀerence between autoregressive lag lengths estimated by the Akaike Information
Criterion (AIC) and the Bayes Information Criterion (BIC). In empirical applications with
macroeconomic data, this diﬀerence is substantially larger than what asymptotic theory
would lead one to expect for an exact pth-order autoregression (AR(p)). For example, in
Section 4 we examine a data set, taken from Stock and Watson (2005), which contains 132
monthly macroeconomic time series for the United States from 1959:1 — 2003:12. The first
two columns in Table 1 report the empirical distribution of the diﬀerence between AIC-
and BIC-selected lags, computed using a maximum lag length (pmax ) of 18, along with the
asymptotic distribution of this diﬀerence computed for white noise.1 In theory, AIC should
exceed BIC by 3 or more 12.0% of the time; empirically, AIC exceeds BIC by 3 or more
61.4% of the time. It thus appears that there is some small amount of predictability in the
BIC residuals that is pushing AIC to include substantially more lags. Yet, as we report
in Section 4, this predictability is suﬃciently small that the mean squared forecast error
(estimated using the pseudo out-of-sample method described in Section 4) of AIC exceeds
BIC, at least at short horizons. Thus there appears to be residual predictability beyond the
BIC-selected autoregression, but the estimation error introduced by longer autoregressions
overwhelms this small residual predictability and degrades the forecasts. Framed in terms
of AIC/BIC asymptotics for stationary time series, this suggests the presence of residual
                                √
predictability that lies in a 1/ T neighborhood of no predictability.
    This paper proposes a structure for exploiting this small residual linear predictability.
Consider the univariate Gaussian AR(p),

                               β(L)yt = α + ut                                                           (1)
                                         = α + σet ,          t = 1, . . . , T,

where β(z) = 1 − β 1 z − · · · − β p z p has its roots outside the unit circle, σ > 0 and et is a
    1
    If the process is an AR(p0 ) and the information criteria are used to choose a lag up to pmax ≥ p0 , then
BIC estimates p0 consistently and AIC overestimates p0 by ∆p; the overestimation ∆p is asymptotically
                                          P
distributed as ∆p ∼ arg max0≤d≤pmax −p0 dj=1 (Zj2 − 2), where Zj are independent standard normals. This
distribution depends only weakly on pmax −p0 if pmax −p0 is large. Unreported results show the small sample
distribution for T = 510 and Gaussian errors to closely match the asymptotic approximation, at least for an
AR(0).

                                                     1
mean-zero stationary Gaussian process. We model et as having a small amount of residual
predictability by supposing that the spectral density of et , fe (ω), is local-to-flat, specifically,
                                                    1 G(ω)/√T
                                        fe (ω) =      e                                            (2)
                                                   2π
         Rπ
where −π G(ω)dω = 0. This setup captures the notion that after a prewhitening with a
                                                                              √
parametric model, there remains residual predictability of order 1/ T . On the one hand,
by Kolmogorov’s formula (Brockwell and Davis (1991, p. 191)),           ³Rthe optimal one-step
                                                                                            ´ ahead
                                                                           π
linear predictor of et has variance V [et |et−1 , et−2 , . . .] = 2π exp −π ln fe (ω)dω/2π = 1. On
the other hand, ignoring this ³local predictability yields an       ´ error variance ofR approximately
          Rπ               1
                             Rπ              √         1                            1    π
V [et ] = −π fe (ω)dω ≈ 2π −π 1 + G(ω)/ T + 2 G(ω) /T dω = 1 + T −1 4π
                                                                 2
                                                                                        −π
                                                                                           G(ω)2 dω, so
that the increase in the mean square forecast error is of order 1/T , the same order that arises
                     √                                               √
from standard 1/ T parameter uncertainty. Also, the 1/ T rate in (2) ensures contiguity
to the model with white noise ut (Dzhaparidze (1986), p. 66); this implies in particular that
any consistent lag-selection rule for p (such as BIC) remains asymptotically unaﬀected by
the slight misspecification of the AR(p).
    The local perturbation G cannot be consistently estimated. Instead we model G as an
unobserved continuous Gaussian process on [−π, π] with a known covariance kernel. The
optimal forecast of yT +1 under quadratic loss thus becomes a Bayes problem of computing
the posterior mean of yT +1 under such a prior in the spectral domain. We show that the
local-to-flat spectral density assumption provides substantial computational simplifications.
In particular, the posterior mean of uT +1 is approximately a linear function of the autoco-
variances of ut , and the log-likelihood is approximately quadratic in these autocovariances.
    Asymptotically, the posterior mean of uT +1 thus becomes a standard normal-normal
Bayes problem with a straightforward closed form solution. In the special case where p =
0 and G is a constant c times demeaned Brownian motion on [−π, π], the approximate
posterior mean for the jth autocovariance is simply the jth sample autocovariance of ut
with a shrinkage factor of c2 /(c2 + 2π2 j 2 ). When p > 0, β(L) needs to be estimated, so
only the OLS residuals ût are directly observed. The appropriate shrinkage of the sample
autocovariances of ût then involves an additional linear regression step. Formally, we show
that with a sample size independent, non-dogmatic prior over β(L), and an independent
Gaussian process prior on G, this approximation to the posterior mean of yT +1 is within
op (T −1/2 ) of the exact posterior mean.
    This result is particularly useful because computation of the exact posterior mean is
challenging. Carter and Kohn (1997), Liseo, Marinucci, and Petrella (2001), Choudhuri,
Ghosal, and Roy (2004a), McCoy and Stephens (2004), and Rosen, Stoﬀer, and Wood (2009),

                                                   2
among others, consider Bayesian inference in time series models with priors in the spectral
domain. They resort to computationally intensive Markov Chain Monte Carlo techniques to
obtain posteriors. What is more, their samplers are all based on the Whittle (1957, 1962)
approximation of the likelihood. The pseudo-posteriors obtained in this fashion thus contain
an approximation error that could be as large as Op (1).2
    Alternatively, one might approximate the spectral density prior (2) by a corresponding
prior on the coeﬃcients of a long-lag AR(q) with q À p. Under the demeaned Wiener
process specification for G, the resulting prior on the AR coeﬃcients j = p + 1, p + 2, . . . is
approximately independent mean-zero Gaussian with variance proportional to 1/(T j 2 ). This
rate of decay corresponds to the rate of decay the ’Minnesota’ prior (cf. Doan, Litterman,
and Sims (1984)) imposes for j = 1, 2, . . .. For G a demeaned Wiener process, our forecast
might thus be interpreted as the approximate posterior under a "local" Minnesota prior on
prewhitened data, which is implemented without estimation of a long-lag AR(q).
    If the baseline AR model (1) for yt is locally misspecified for low frequencies, then the
function G in (2) has most of its variation close to zero.3 A large literature considers good
forecasting rules in the presence of low frequency phenomena, such as structural breaks or
time varying means (cf. Chernoﬀ and Zacks (1964), Clements and Hendry (1998), Pesaran,
Pettenuzzo, and Timmermann (2006), for example). Such concerns are seamlessly accommo-
dated in our framework by picking a prior for G with more variation close to the origin. The
shrinkage of the sample autocovariances of ut is then performed in a way that the resulting
forecast tracks low frequency movements in the mean of yt .
    From a decision theoretic perspective, a Bayesian approach to forecasting is entirely
natural, as the resulting forecast is eﬃcient relative to the prior and thus admissible by
construction. In contrast, in the literature on optimal lag selection for forecasting (Shibata
(1980), Ing and Wei (2005), Schorfheide (2005), among others) attention is restricted to the
class of OLS forecasts, which might be dominated by another forecasting function.4 From
a more technical perspective, the quadratic approximations to the log-likelihood underlying
our results are similar to those employed by Müller and Petalas (2010) in the context of
   2
     In the light of the results of Choudhuri, Ghosal, and Roy (2004b), one would expect that the Whittle
approximation induces errors in posterior means of autocovariances no larger than Op (T −1/2 ) for suﬃciently
smooth spectral densities, but the approximation error for forecasts computed from the Whittle likelihood
is Op (1) in general.
   3
                                                                                   √
     The spectral density of yt in (1) is fy (ω) = σ 2 fe (ω)/|β(eiω )|2 , with i = −1. Thus, G in (2) equivalently
represents a local misspecification in the log spectral density of yt .
   4
     For instance, Ing and Wei (2005, Section 4) provide evidence that AIC based OLS forecasts are not in
general admissible under squared loss.


                                                        3
                                                           √
the estimation of parameter time variation of order 1/ T . Finally, there is an interesting
connection to the recent paper by Golubev, Nussbaum, and Zhou (2010): These authors
establish a general but non-constructive equivalence between spectral density estimation
and estimation of a nonparametric function on the unit interval, observed with Gaussian
noise. In the local-to-flat spectrum framework, this link becomes quite explicit: The as-
ymptotically normal-normal Bayes problem for the autocovariances of et corresponds to a
Gaussian functional estimation problem with Gaussian prior, and the optimal forecast of
                                                               Rπ
eT +1 in (2) under   the belief that  G is equal to  Ĝ (where      Ĝ(ω)dω = 0) has variance
Rπ                √
           −Ĝ(ω)/ T             −1 1
                                      Rπ                 2
                                                                −π

 −π e
     f (ω)e          dω ≈ 1 + T 4π −π (G(ω) − Ĝ(ω)) dω, so that the impact on the mean
squared forecast error of estimation error in G becomes asymptotically proportional to the
L2 -norm of the estimation error of G.
    A heuristic derivation, including the simplifying steps arising from the local-to-flat spec-
trum assumption, details on the suggested forecast and the formal theoretical result are
given in Section 2. The result covers a general VAR model, with unknown intercept and
error variance, and -step ahead forecasts of yT + . Under a mixture prior on G (for example,
we consider the case that G is c times demeaned Brownian motion and multiple values of
c are used), it is possible to combine forecasts by Bayesian model averaging (BMA), and
we provide a simple expression for the model averaging weights. Our main result is that
diﬀerence between the approximate posterior mean of yT + and the exact posterior mean
(which is the optimal forecast under quadratic loss) is op (T −1/2 ) for all fixed horizons .
    Section 3 reports a Monte Carlo simulation which confirms the main features of the
theory. The simulations focus on the case that the process G is c times a demeaned Brownian
motion. The cases that the prior is correctly specified are seen to map out an asymptotic
eﬃciency envelope. The BMA forecast comes reasonably close to this envelope. Even if the
prior is misspecified, for example if ut is a finite-order moving average with coeﬃcients of
         √
order 1/ T , the forecasts based on the scaled Brownian motion prior capture much of the
additional predictability in the process. Moreover, when there is no predictability, the cost
of using the BMA forecasts is found to be small. These findings suggest that the demeaned
Brownian motion prior provides a flexible way to exploit small remaining predictability in
autoregressive residuals for general unspecified forms of that predictability.
    Section 4 assesses the performance of the smooth-spectrum forecasts via a pseudo out-
of-sample forecasting experiment using the U.S. monthly macro data set examined in Table
1. Table 1 suggested that there is a small additional linear predictability, at least in some
series. The final 6 columns of Table 1 report the asymptotic distribution of the diﬀerence
between AIC- and BIC-selected lags in model (2) with G a demeaned Brownian motion of

                                               4
scale c, for a true lag length of p0 = 0 and p0 = 3.5 At least qualitatively, these diﬀerences
match the empirical results in the first column quite closely. Accordingly, the approximately
optimal forecasts computed from this model with c = 20 have median improvements of the
mean squared forecast error, relative to BIC, of 1.2% at the one month horizon. For some
series, the improvement is substantially greater and for only a few series do we find that
this procedure imposes much of a cost, relative to using BIC: the 10% and 90% percentiles
of relative mean squared forecast errors, relative to BIC, are 0.964 and 1.014, respectively.
The magnitudes of these improvements are also in line with the theoretical expressions in
Section 2 and the Monte Carlo results of Section 3.


2       Theoretical Results
This section provides an informal derivation of the approximately best forecast, first for
the case of no autoregressive component, then with an autoregressive component. We then
provide a detailed description of the suggested forecast in a VAR with unknown mean and
scale. The section concludes with a statement of the main theorem, which is proven in the
appendix.


2.1     Heuristic Derivation
We begin by obtaining an expression for approximate posterior mean of yT +1 in the case
that β(L) = 1, so that yt = ut and ut is observed. For simplicity, we assume α = 0 and
σ = 1 known (so that ut = et ), and that T is odd.
   Given G, the jth autocovariance of ut is
                       Z π                Z π
                            iωj
       E[ut ut−j |G] =     e fe (ω)dω = 2      cos(ωj)fe (ω)dω,  j = 0, ±1, ±2, . . . (3)
                            −π                      0
        √                                           √
with i = −1. Define the autocovariances, scaled by T ,
                           ( √
                               T (E[u2t |G] − 1)   for j = 0
                  γ j (G) = √
                               T E[ut ut−j |G]    for j = ±1, ±2, . . .
    5
    The asymptotic distribution of ∆p described in footnote 1 then changes to ∆p ∼
                   Pd
arg max0≤d≤pmax −p0 j=1 (κj+p0 Zj2 − 2), where Zj are independent standard normals and κj = 1 + c2 /2π2 j 2 .




                                                        5
Let γ(G) = (γ 1 (G), . . . , γ T (G))0 , and
                                    ⎛                                                   ⎞
                                          γ 0 (G) γ 1 (G) · · · γ T −1 (G)
                                    ⎜                                 ..                ⎟
                                    ⎜ γ 1 (G) γ 0 (G) · · ·            .                ⎟
                        Γ(G) = ⎜    ⎜                                                   ⎟
                                              ..      ..    ...                         ⎟
                                    ⎝          .       .          γ 1 (G)               ⎠
                                         γ T −1 (G) · · · γ 1 (G) γ 0 (G)

so that with UT the (reversed) vector of observations on ut , UT = (uT , . . . , u1 )0 , we have

                              V (G) = E[UT UT0 ] = I + T −1/2 Γ(G).

The dependence of γ(G), Γ(G) and V (G) on T is suppressed to simplify notation.
     Given G, the optimal forecast of uT +1 is uT +1|T (G) = T −1/2 γ(G)0 V (G)−1 UT . Thus
u∗T +1|T = T −1/2 E[γ(G)0 V (G)−1 UT |UT ] is the exact posterior mean of uT +1 , where the expec-
tation is taken over the posterior of G (or, equivalently, over the posterior of (γ(G), V (G))).
We are interested in obtaining a simple approximate expression upT +1|T ≈ u∗T +1|T .
     The derivation proceeds in four steps. The first is to approximate uT +1|T (G). Under the
local model (2),
                                       1 T −1/2 G(ω)    1 ¡               ¢
                           fe (ω) =      e           ≈     1 + T −1/2 G(ω)
                                      2π               2π
so that                                              Z
                                                   1 π
                              γ j (G) ≈ γ̃ j (G) =       cos(ωj)G(ω)dω.                        (4)
                                                   π 0
Thus V (G) ≈ I, so uT +1|T (G) = T −1/2 γ(G)0 V (G)−1 UT ≈ T −1/2 γ(G)0 UT and the posterior
mean is approximately
                                  u∗T +1|T ≈ T −1/2 γ ∗0 UT                              (5)
where γ ∗ is the posterior mean of γ(G).
   The second step is to approximate the exact Gaussian likelihood by the Whittle (1957,
1962) likelihood. Let zj be the jth element of the periodogram,
                ¯                   ¯2
                ¯ 1 X  T            ¯
                ¯                   ¯
           zj = ¯ √       ut eiωj t ¯ ,       where ω j = 2πj/T, j = 1, · · · , (T − 1)/2.
                ¯ 2πT t=1           ¯

The Whittle log-likelihood is
                              (T −1)/2                    (T −1)/2               T /2
                               X                              X         zj       X
            ln p(YT |G) ≈ −              ln fe (ω j ) −                        =     lj (T −1/2 δ j (G))   (6)
                                j=1                           j=1
                                                                     fe (ω j )   j=1


                                                          6
                  √                                                2πz
where δ j (G) = T (2πfe (ωj ) − 1) and lj (x) = − ln(1 + x) − 1+xj + ln 2π. As noted in
the introduction, the Whittle approximation is often employed to facilitate computation of
posteriors in models with priors in the spectral domain. At the same time, direct use of
(6) for inference about fe (ω j ) amounts to a non-parametric regression problem with log-chi
squared distributed errors, which requires non-Gaussian posterior sampler techniques (cf.
Carter and Kohn (1997)). What is more, the Whittle likelihood approximation potentially
induces non-trivial errors in posterior means.
    The third and key step of our approximation exploits the local embedding of fe (ω j ) to
address both these issues. On the one hand, for nearly flat fe (ω), the approximation (6)
becomes highly accurate. On the other hand, since δ j (G) ≈ G(ω j ) is O(1) for fixed G, we
can further approximate lj by a second order Taylor series
             T /2                    T /2
             X                       X    ¡                                                             ¢
                    lj (δ j (G)) ≈          lj (0) + T −1/2 lj0 (0)δ j (G) + 12 T −1 lj00 (0)δ j (G)2
             j=1                     j=1
                                 T /2 ³                                                       ´
                                 X               −1/2
                               =       lj (0) + Tj z̄j δ j (G) − 12 T −1 (1 + 2z̄j ) δ j (G)2               (7)
                                     j=1

                                                             P /2                P /2
where z̄j ≡ 2πzj − 1. Because G(ω) is continuous and T1 Tj=1       z̄j ≈ 0, T1 Tj=1    z̄j δ j (G)2 ≈
1
  P T /2           2                                                              2
T   j=1 z̄j G(ω j ) ≈ 0 (see Müller and Petalas (2010)). Thus the term z̄j δ j (G) in (7) vanishes
and, upon completing the square, we have
                                           T /2
                                           X    ¡                    ¢2
                     ln p(YT |G) ≈ − 12          z̄j − T −1/2 δ j (G) + function of {z̄j }.                 (8)
                                           j=1


The sample information about the spectral density of ut (or, equivalently, G) is thus captured
asymptotically by the Gaussian pseudo-model

                                        Z̄|δ(G) ∼ N (T −1/2 δ(G), I),                                       (9)

where Z̄ = (z̄1 , . . . , z̄(T −1)/2 )0 and δ(G) = (δ 1 (G), . . . , δ (T −1)/2 (G))0 . Note, however, that (9)
is only an accurate approximation to the likelihood as a function of δ(G)–the sampling dis-
tribution of the centred periodogram ordinates z̄j does not, of course, become approximately
Gaussian, even asymptotically.
    The fourth step in the approximation is to use the pseudo-model in the frequency do-
main to obtain a corresponding time domain model and hence an approximate posterior of
                                         Rπ                             PT /2
γ(G). From (3), γ j (G) ≈ π1 0 cos(ωj)G(ω)dω ≈ T2 l=1 cos(ω l j)δ l (G) = T −1/2 qj0 δ(G),

                                                          7
where qj = 2T −1/2 (cos(ω1 j), · · · , cos(ω(T −1)/2 j))0 .          Thus γ(G) ≈ T −1/2 Q0 δ(G), where
Q = (q1 , . . . , q(T −1)/2 ). Similarly, let
                                                                         T /2
                                           X
                                           T                             X
                                    −1/2                          −1/2
                          ŝj = T                  ut ut−j = 2T                 cos(ω l j)z̄l
                                           t=j+1                         l=1
                                                         √
so the vector of sample autocovariances of ut , scaled by T , is ŝ = Q0 Z̄. From (9) and
Q0 Q = I, we therefore have the approximate pseudo-model

                                            ŝ|γ(G) ∼ N (γ(G), I).                                (10)

The Gaussian prior for G implies via (4) an approximately Gaussian prior for γ(G),
                                             Z Z
                                          1 π π
           γ(G) ≈ N (0, Σ), where Σjl = 2           cos(rj)kG (r, s) cos(sl)drds                  (11)
                                          π 0 0
and kG (r, s) = E[G(r)G(s)] is the covariance kernel of the stochastic process G. Note that
if                                                 Z
                                                 1 π
                                G(s) = J(s) −           J(r)dr,                            (12)
                                                π 0
that is G is the demeaned version of the stochastic process J, then Σjl in (11) can be
                                                                                Rπ
alternatively computed with kG (r, s) replaced by kJ (r, s) = E[J(r)J(s)], since 0 cos(sj)ds =
0 for j = 1, 2, . . ..
    Combining (10) and (11), the approximate posterior mean of γ is

                                           γ ∗ ≈ γ p = Σ(I + Σ)−1 ŝ                              (13)

which can be seen as a generalized shrinkage estimator of γ. With (5), the approximate
posterior mean of uT +1 is thus

                       u∗T +1|T ≈ upT +1|T = T −1/2 UT0 γ p = T −1/2 Σ(I + Σ)−1 ŝ.

      We now present explicit expressions for Σ and for the approximate posterior mean γ p =
(γ p1 , · · · , γ pT )0 in three cases.
Demeaned Brownian motion. Let J(ω) = cW (ω), where W is Brownian motion on [0, π]
with E[W (π)2 ] = 1, and via (12), G is distributed as a demeaned Brownian motation
                                          Rπ                              2
G(s) = cW μ (s) = cW (s) − πc 0 W (r)dr. Then kJ (r, s) = cπ min(r, s) for r, s ≥ 0, and direct
evaluation of Σjl yields
                          (
                            0, j 6= l               p       c2
                Σjl =          c2           , and γ j =              ŝj (Brownian motion prior). (14)
                             2π 2 j 2
                                      , j=l             c2 + 2π2 j 2

                                                         8
Thus the approximate posterior mean of uT +1 , upT |T +1 , is computed using shrinkage estimates
of the autocovariances of ut , with the shrinkage factor given in (14).
Demeaned integrated Brownian Bridge.               Let J(ω) = cIB(ω), where IB(ω) =
  R ¡
1 ω            s
                        ¢
π 0
      B(s) − π B(π) ds, with W as above. Then
          (
            0, j 6= l               p       c2
    Σjl =      c2           , and γ j =               ŝj (integrated Brownian Bridge prior).
             2π 4 j 4
                      , j=l             c2 + 2π 4 j 4

Demeaned Brownian motion restricted to a frequency band. A focus on deviations from the
flat spectral density in a particular frequency band [ω, ω̄], 0 ≤ ω < ω̄ ≤ π, is obtained by G
that are constant for ω ∈/ [ω, ω̄]. To be specific, suppose the baseline model is assumed to be
misspecified for frequencies lower than ω̄ (i.e. ω = 0). One suitable process is then given by
                                       (
                                          √c W (ω) for 0 ≤ ω < ω̄
                                            ω̄
                             J(ω) =
                                          √c W (ω̄) otherwise
                                            ω̄
                                                                                            √
where W is a standard Wiener process with E[W (π)2 ] = 1. The scaling by 1/ ω̄ ensures
that the total variation in J is comparable to the demeaned Brownian motion case, since
E[J(1)2 ] = c2 . The covariance kernel of J is kJ (r, s) = c2 min(r, s, ω̄)/ω̄ for r, s ≥ 0, and Σ
has j, lth element
    ( 2
         c j cos(ω̄j) sin(ω̄i)−l cos(ω̄l) sin(ω̄l)
        ω̄π 2              l3 j−lj 3
                                                   for j 6= l
         c2   2ω̄j−sin(2ω̄j)                                  (Brownian motion prior on [0, ω̄]). (15)
        ω̄π 2      4j 3        for   j =  j

    We now discuss the extension to a baseline AR(p) model, but maintain α = 0 and σ = 1
known. Suppose that β(L) is estimated as β̂(L) by OLS for a given lag length p, which is
asymptotically equivalent to the posterior mean of a Bayes estimation with a non-dogmatic
prior on β(L). The task then is to forecast ûT +1 , where ût = β̂(L)yt , so ût is the AR(p)
residual for t = 1, . . . , T .
    The approximate posterior mean of ûT +1 , ûpT +1|T , is obtained following the steps for the
case β(L) = 1, except using the spectral density for the residuals. Because ût = β̂(L)yt , the
spectral density of ût is,
                              ¯       ¯               ¯       ¯
                              ¯ iω ¯2                 ¯ iω ¯2
                              ¯β̂(e )¯             1 ¯β̂(e )¯ G(ω)/√T
                   fû (ω) =             fe (ω) =               e
                              |β(eiω )|2          2π |β(eiω )|2
                               1
                            ≈     [1 + T −1/2 μ(ω)0 b + T −1/2 G(ω)] + O(T −1 )
                              2π

                                                  9
where b = T 1/2 (β − β̂) is Op (1) and
                                                   ¯       ¯ ¯
                                                   ¯ iω ¯2 ¯¯
                                                 ∂ ¯β̂(e )¯ ¯
                                         μ(ω) =               ¯           .
                                                ∂β |β(eiω )|2 ¯
                                                              ¯
                                                                   β=β̂
                              √
    As before, set δ j (G) = T (2πfû (ω j )−1), so δ j (G) ≈ μ(ω j )0 b+G(ω j ). With this redefined
notation for δ, the argument leading to (8) applies directly (because δ j remains Op (1), so
after prewhitening, the relevant spectral density remains local-to-flat). Thus ŝ|γ û (G, b) ∼
                                                                                    √
N (γ û (G, b), I), where ŝ now collects the autocovariances of ût scaled by T , γ û (G, b) =
T −1/2 Q0 δ(G) = mb + γ(G), m = T −1/2 Q0 μ and μ = (μ(ω 1 ), . . . , μ(ω(T −1)/2 ))0 . The T × p
matrix m has a simple form. Let Ψ(L) = β(L)−1 and Ψ̂(L) = β̂(L)−1 , so Ψ̂0 = 1 and Ψ̂j is
the jth term in the MA representation of β(L). Then
                                ⎛                                        ⎞
                                      1       0       0     ···    0
                                ⎜                                        ⎟
                                ⎜ Ψ̂1         1       0     ···    0 ⎟
                                ⎜                                        ⎟
                            m=⎜ ⎜ Ψ̂2        Ψ̂1      1     ···    0 ⎟   ⎟.
                                ⎜ ..          ..       ..   ...     .. ⎟
                                ⎝ .            .        .            . ⎠
                                   Ψ̂T −1 Ψ̂T −2 Ψ̂T −3 · · · Ψ̂T −p

   With an approximate N (0, Σ) prior on γ(G), where Σ is given in (11), γ û (G, b) = mb +
γ(G) implies γ û (G, b)|b ∼ N (mb, Σ). Thus ŝ|γ û (G, b) ∼ N (γ û (G, b), I) yields

                                            ŝ|b ∼ N (mb, I + Σ),                                           (16)

so E[γ û (G, b)|ŝ, b] = mb + Σ(I + Σ)−1 (ŝ − mb). With a continuous prior on β, the prior on b
is asymptotically flat, so that the posterior for b simply reflects the shape of the integrated
likelihood (16), E[b|ŝ] = (m0 (I + Σ)−1 m)−1 m0 (I + Σ)−1 ŝ. Combining these expressions and
using the identity I − Σ(I + Σ)−1 = (I + Σ)−1 , we obtain the approximate posterior mean,

             γ ∗û ≈ γ pû = Σ(I + Σ)−1 ŝ + (I + Σ)−1 m(m0 (I + Σ)−1 m)−1 m0 (I + Σ)−1 ŝ.

The approximate posterior mean ûpT +1|T of ûT +1 may then be computed analogously to (13),
that is, ûpT +1|T = T −1/2 ÛT0 γ pû , where ÛT is the vector of AR(p) residuals, ÛT = (ûT , . . . , û1 )0 ,
and the approximate posterior mean of yT +1 is then given by yTp +1|T = β̂ 1 yT +. . .+ β̂ p yT −p+1 +
ûpT +1|T .



                                                       10
Conditional heteroskedasticity
   In the presence of conditional heteroskedasticity, the sampling distribution of ŝ has an
asymptotic covariance matrix that is no longer proportional to the identity matrix. In
                                         P
general, we have T Var[ŝj ] = Var[T −1/2 Tt=j+1 ut ut−j ] ≈ E[u2t u2t−j ] = dj , so that

                                            ŝ|γ ∼ N (γ, D)                                           (17)

with D = diag(d1 , d2 , · · · , dT ). The sampling distribution of ŝ thus diﬀers from the pseudo-
model (10). In analogy to results of Müller (2009) and Müller and Petalas (2010), one would
therefore expect that one obtains better forecasts by employing the pseudo-model (17) that
reflects the actual sample information about γ. Proceeding as above, in the case where
β(L) = 1, we obtain with ŝD = D−1/2 ŝ, γ D (G) = D−1/2 γ(G) and ΣD = D−1/2 ΣD−1/2 that
γ pD = ΣD (I + ΣD )−1 ŝD , so that now

                               γ p = D1/2 γ pD = D1/2 ΣD (I + ΣD )−1 ŝD .

Similarly, with a baseline AR(p) model and mD = D−1/2 m, we have instead of (16) that

                                   ŝD |b ∼ N (T −1/2 mD b, I + ΣD ),

so that now

γ pû = D1/2 γ pD,û
      = D1/2 ΣD (I + ΣD )−1 ŝD + D1/2 (I + ΣD )−1 mD (m0D (I + ΣD )−1 mD )−1 m0D (I + ΣD )−1 ŝD .

The approximate posterior mean γ pû is thus D1/2 γ pD,û , where γ pD,û is obtained by comput-
ing the posterior mean as before with ŝ, Σ and m replaced by ŝD , ΣD and mD . The
                                                                               P
matrix D is usually unknown, but can be estimated via dˆj = T −1 Tt=j+1 u2t u2t−j and
D̂ = diag(dˆ1 , dˆ2 , · · · , dˆT ).


2.2      Suggested Forecast and Formal Results
Consider the VAR generalization of (1)

                          yt = α + β 1 yt−1 + . . . + β p yt−p + ut
                              = α + β 1 yt−1 + . . . + β p yt−p + P et , t ≥ 1                        (18)

where {yt }Tt=−p+1 are observed k × 1 vectors, {yt }0t=−p+1 are fixed values independent of
θ = (α, β 1 , · · · , β p ), β j ∈ Rk×k , j = 1, · · · , p, P is a full rank lower-triangular k × k matrix,

                                                    11
and {et } is a zero mean stationary Gaussian process. Suppose the spectral density of {et } is
                                               1
                                   fe (ω) =      exp[T −1/2 G(ω)]
                                              2π
where G is a fixed Hermitian k×k matrix valued function on [−π, π] with G(−ω) = G(ω)0 and
Rπ                                                                             Rπ
 −π
    G(ω)dω = 0, so that the jth autocovariance of et is given by E[et e0t−j ] = −π eijω fe (ω)dω.
    Let A∗ be the conjugate transpose of a complex matrix A. We consider the following
class of priors for G.

Condition 1 Under the prior measure,
    (a) G(ω) is a k × k Hermitian matrix for all ω ∈ [−π, π], and G(−ω) = G(ω)0 a.s.;
        Rπ
    (b) −π G(ω)dω = 0 a.s.;
    (c) vec(G(ω)) is a k2 × 1 mean-zero complex Gaussian process on [−π, π], with a.s.
bounded sample paths and covariance kernel kG (r, s) = E[vec(G(s)) vec(G(r))∗ ], r, s ∈
[−π, π];
    (d) k (r, s) = ∂ 2 kG (r, s)/∂r∂s exists and is continuous for all s 6= r,
Rπ Rπ 2 ∗
         ϕ(r) k2 (r, s)ϕ(s)dsdr < ∞, k∆ (s) = lim →0 [∂kG (r, s)/∂r|r=s− − ∂kG (r, s)/∂r|r=s+ ]
 −π −π      Rπ
exists and −π ϕ(s)∗ k∆ (s)ϕ(s)ds < ∞, for all k2 × 1 complex valued functions ϕ with
Rπ
 −π
    ϕ(s)∗ ϕ(r)ds = 1.

    Parts (a) and (b) ensure that with prior probability one, et has a well defined spectral
density, and V [et |et−1 , et−2 , . . .] = Ik . Part (c) imposes a mean-zero Gaussian process prior
for G. Loosely speaking, the diﬀerentiability assumptions on the covariance kernel kG in
part (d) ensure that under the prior, sample paths of G are at least as smooth as a Brownian
motion. Note that a bounded modulus of the elements in k2 (r, s) and k∆ (s) over r, s ∈ [−π, π]
is suﬃcient (but not necessary) for the existence of the integrals in part (d).
    For computational reasons, it makes sense to avoid large matrix inversions. Since all
                                                                             1
                                                                               R π ijs
priors satisfying Condition 1 imply an eventual decay in γ̃ j (G) = 2π          −π
                                                                                   e G(s)ds → 0 as
j → ∞ almost surely, treating γ̃ j (G) = 0 for j ≥ N under the prior only induces a very
minor additional approximation error for large enough N. Thus, define the Nk2 × Nk2
matrix Σ with j,lth k2 × k2 block equal to
                                           Z πZ π
                                        1
                                                    eijs kG (r, s)e−ilr drds                   (19)
                                      4π2 −π −π

so that Σ is the prior covariance matrix of vec(γ̃ 1 (G), γ̃ 2 (G), · · · , γ̃ N (G)) (cf. (4) of the
heuristic discussion).

                                                 12
     Condition 1 allows for a wide range of priors on the local-to-flat spectral density of
et . Smoothness assumptions can be expressed continuously by letting G be a (demeaned)
fractional Brownian with Hurst parameter H ≥ 1/2 (with the Brownian motion case H = 1/2
the least smooth choice allowable under Condition 1 (d)), or integrated fractional Brownian
motions. Condition 1 also covers the three priors for which Σ is worked out in Section 2.1.
     When yt is a vector, one can choose to treat all k2 elements of the autocovariance
function of et symmetrically by letting the k2 elements of G be i.i.d. copies of the real
scalar Gaussian process Ḡ under the prior. With kḠ (r, s) = E[Ḡ(r)Ḡ(s)], this leads to
kG (r, s) = kḠ (r, s) ⊗ Ik2 , and Σ has a corresponding Kronecker structure Σ = Σ̄ ⊗ Ik2 , where
Σ̄ is a N × N matrix with i, jth element constructed from kḠ as in (11). For example,
with a demeaned Wiener process prior for Ḡ, one obtains the diagonal Nk2 × Nk2 matrix
Σ = (c2 /2π 2 ) diag(1/12 , 1/22 , · · · , 1/N 2 ) ⊗ Ik2 .
     Alternatively, one might want to allow for relatively greater non-flatness in the k diagonal
elements of the spectrum, as residual cross-equation correlations might be expected to be
relatively smaller than residual autocorrelations in the individual series in et . It might also
make sense in some applications to treat the individual series in et asymmetrically, based on
assumptions about frequencies and magnitudes of cross equation correlations, or diﬀerent
smoothness properties of individual and cross spectra.
     The approximately best steps ahead forecast of yt in the VAR system (18) under a
Condition 1 prior is computed as follows: Let α̂ and β̂ be the OLS estimates of α and β from
a regression of yt on a constant and p lagged values, and denote by ût the OLS residuals. Let
                                                                        P
P̂ the k ×k lower diagonal matrix that satisfies P̂ P̂ 0 = T −1 Tt=1 ût û0t , set b̂     et = P̂ −1 ût , and let
Ψ̂j , j = 1, 2, . . . be k × k matrices satisfying (Ik − β̂ 1 x − · · · − β̂ p xp )−1 = Ik + Ψ̂1 x + Ψ̂2 x2 + . . ..
Define the Nk × pk matrix
                                   ⎛                                               ⎞
                                          Ik        0      0     ···       0
                                   ⎜                                               ⎟
                                   ⎜ Ψ̂01          Ik      0     ···       0 ⎟
                                   ⎜                                               ⎟
                              Ψ̂ = ⎜
                                   ⎜     Ψ̂ 0
                                            2      Ψ̂  0
                                                       1   I k   · · ·     0       ⎟,
                                                                                   ⎟
                                   ⎜ ..             ..      ..   ...        .. ⎟
                                   ⎝ .               .       .               . ⎠
                                       Ψ̂0N−1 Ψ̂0N−2 Ψ̂0N−3 · · · Ψ̂0N −k
the Nk2 × pk2 matrix m̂ = Ψ̂ ⊗ Ik , and the Nk2 × Nk 2 block diagonal matrix D̂ with jth
                              P            0        0
k2 × k2 block equal to T −1 Tt=j+1 b̂    et−j ⊗ b̂
                                     et−jb̂        et . Further, define the Nk2 × 1 vector b̂
                                                etb̂                                        s,
                                                                               P          0
which is the vec of the k × Nk matrix with jth k × k block equal to T −1/2 t=j+1 b̂
                                                                                 T
                                                                                      etb̂
                                                                                        et−j ,
                            2
j = 1, · · · , N, and the Nk × 1 vector
             γ p = Σ(D̂ + Σ)−1b̂
                              s + D̂(D̂ + Σ)−1 m̂(m̂0 (D̂ + Σ)−1 m̂)−1 m̂0 (D̂ + Σ)−1b̂
                                                                                     s.

                                                        13
Let γ pj , j = 1, · · · , N be the k × k matrices such that vec(γ p1 , γ p2 , · · · , γ pN ) = γ p and define

                                                         X
                                                         N−l
                            ûpT +l|T   =T   −1/2
                                                    P̂         γ pt+lb̂
                                                                     eT −t ,   l = 1, · · · , .
                                                         t=0

The approximately best forecast of yT + , yTp + |T , then is obtained by iterating the VAR (18)
forward by periods, using the OLS estimates for α and β(L), with future disturbances set
equal to ûpT +l|T .
   These calculations are quite insensitive to N, as long as N is not chosen too small. A
reasonable default is N = bT 3/4 c. If conditional heteroskedasticity is not a concern, then D̂
can be set equal to INk2 . Also, if P is known (or partially known), then P̂ can be replaced
by the true P (or any other consistent estimator).
   The main result of the paper is as follows.

Theorem 1 Suppose that {yt }Tt=1 is generated from (18) with G any fixed and bounded
function, and θ = θ0 so that (18) is causal. Assume further that the prior on G satisfies
Condition 1, and the prior on θ = (α, β 1 , · · · , β p ) is a sample size independent, bounded
                                                    R
Lebesgue probability density w satisfying supt≥1 Eθ [||yt ||2 ]w(θ)dθ < ∞ when G = 0. Let
yT∗ + |T be the posterior mean for yT + under this prior, assuming knowledge of P . If w(θ0 ) > 0
and w is continouus at θ0 , and N → ∞ with N/T → 0, then
                                                                               p
                                         T 1/2 ||yT∗ + |T − yTp + |T || → 0.

    Moreover, if the prior is a mixture of n Gaussian processes as above, with kernels

                             kG(i) (r, s) = E[(vec G(i) (r))(vec G(i) (s))∗ ],
                                              P
i = 1, · · · , n and mixture weights pi ≥ 0, ni=1 pi = 1, then the same results holds with yTp + |T
a corresponding convex combination of n versions of yTp + |T , computed using Σ(i) (with j, lth
block as in (19) with kG replaced by kG(i) ) instead of Σ, with weights proportional to

  pi det(D̂ + Σ(i) )−1/2 det(m̂0 (D̂ + Σ(i) )−1 m̂)−1/2
                    0                      0
       × exp[− 12 b̂
                  s (D̂ + Σ(i) )−1b̂
                                  s + 12 b̂
                                         s (D̂ + Σ(i) )−1 m̂(m̂0 (D̂ + Σ(i) )−1 m̂)−1 m̂0 (D̂ + Σ(i) )−1b̂
                                                                                                        s].


3     Monte Carlo Results
This section reports selected results from a Monte Carlo study of the forecasting performance
of the approximate posterior mean in the univariate case. We report the results of three

                                                               14
experiments. In all three experiments, the results for multi-step ahead forecasts are for
cumulative values, that is, forecasts of yT +1 + · · · + yT + .
    The first experiment checks the results of the theorem in a special case by quantifying the
discrepancy between the exact (yT∗ +1|T ) and approximate (yTp +1|T ) posterior mean of yT +1 in
the case that the prior G is correctly specified demeaned Brownian motion. The data were
generated as in (1) and (2), with β(L) = 1. The estimators were implemented including an
intercept and a fixed number p of autoregressive lags, p = 0, . . . , 6. In all the Monte Carlo
work the approximate posterior mean yTp +1|T was computed as described in Section 2.2 using
the demeaned Brownian motion prior (so Σ is given by (14)), and D̂ = IN .
    The results of this first experiment are reported in Table 2. The entries are
T (MSFEposterior mean — MSFEAR )/MSFEAR , that is, the scaled relative increase of the mean
squared forecast error of the approximate and exact posterior mean, relative to the MSFE
of the simple AR forecast with the same lag length. A value less than zero indicates an
improvement upon the AR forecast. The scaling by T is such that the entries of Table 2
stabilize as T → ∞. To get some sense for the magnitude of the entries, note that estimat-
ing an AR(p) by OLS for forecasting an exact AR(p0 ) (i.e. G = 0) with p0 < p leads to an
asymptotic scaled relative deterioration of p−p0 over a forecast using the more parsimonious
AR(p0 ) model. Roughly speaking, the entries of Table 2 (and Figures 1-4 below) are thus in
units of "unnecessarily estimated parameters".
    As can be seen in Table 2, consistent with Theorem 1, the discrepancy between the
scaled MSFEs for the exact and approximate posterior means tends to zero as T increases.
In addition, the smaller c and the larger p and T , the more accurate the approximate
posterior mean. Intuitively, estimating β(L) soaks up some of the variability of G, making
the quadratic approximation of the log-likelihood more accurate. The exact posterior mean
is only a substantially better forecast for large c and very small p, which is unlikely to be a
much of a concern in practice, as highly variable realizations of G would lead one to include
some AR lags in small samples.
    The second experiment examines the performance of the posterior mean forecast using
the prior G ∼ cWμ and some lag-length selection for the AR(p) under a data generating
process where β(L) = 1 and G ∼ c0 Wμ . Specifically, for a given Monte Carlo draw, yTp + |T
was computed with either no autoregressive component, or with an AR(p) with BIC-selected
lag length (AR(BIC)), with 0 ≤ p ≤ 4), for c on a unit grid of 0 to 20 and for = 1, 2, 3,
and 4. In addition, the Bayes model averaging (BMA) forecast was computed using an equal
weighted prior over the grid of c, weighted by the normalized Bayes factor given in Theorem
1. The unadjusted -step ahead iterated OLS AR(BIC) forecast was also retained. This

                                              15
exercise was repeated on a grid of c0 and evaluated for T = 200, with 20,000 Monte Carlo
repetitions.
     Figures 1 and 2 plot T (MSFEf — MSFEBIC)/MSFEBIC, where MSFEf is a candidate
forecast, similar to the entries of Table 2. Each line in the plot corresponds to the relative
MSFE of a given forecast (a given value of c) as a function of the true DGP c0 . The lower
envelope of these lines maps out the forecast gains possible using the Bayes procedure with
correct c0 (up to our asymptotic approximation), and each forecasting procedure achieves this
envelope when c = c0 . Figure 1 shows the results when no AR component is estimated, Figure
2 shows the results when an AR(BIC) component is estimated. Evidently improvements over
the benchmark linear forecast are obtained for a wide range of c 6= c0 . The main case in which
yTp + |T is worse than BIC is when c0 is very small (recall that c0 = 0 corresponds to no residual
predictability), especially when c is large. This is not surprising since larger values of c yield
less shrinkage and introduce more estimation error which, when c0 = 0, simply adds noise to
the forecast. The BMA forecast is never very far from the envelope and is only worse than the
AR forecast for c0 ≤ 2.5. These conclusions hold whether p = 0 (no AR term is estimated;
Figure 1) or an AR(BIC) is estimated (Figure 2), although not surprisingly the magnitude
of the gains is less when an AR term is estimated. The magnitude of the forecasting gains
depend on the true amount of residual predictability; when an AR(BIC) term is estimated,
a typical value for the relative scaled MSFE gains is —1.5, which corresponds to percentage
MSFE improvements of 1.5% with T = 100 or 0.75% with T = 200.
     The third experiment examines the performance using the prior G = cWμ under a
non-stochastic local-to-flat spectral density. Specifically, data are generated according to
                                                         √          √
a MA(1), with MA coeﬃcients on a grid from −4/ T to 4/ T . The AR(BIC), approxi-
mate yTp + |T , BMA, and AR(AIC) forecasts ( = 1 and 4 are reported here) were computed,
where the maximum AIC and BIC lag lengths were 0 ≤ p ≤ 4(T /100)1/3 . This experiment
was repeated for T = 50, 100, 200, and 400, with 20,000 Monte Carlo repetitions each.
     The results are summarized in Figures 3 and 4, which (like Figures 1 and 2) plot T (MSFEf
— MSFEBIC)/MSFEBIC, where MSFEf is the MSFE of the candidate forecast. Consistent
with the theory, if there is no residual predictability then the Bayes procedures simply add
noise. For moderate amounts of residual predictability, the Bayes procedures improve upon
the AR(BIC) forecast. The BMA forecasts improve upon AR(BIC) for all values of the MA
coeﬃcient except for those very close to zero, in which case the BMA procedure produces
only a very small deterioration of the forecast. AR(AIC) improves upon AR(BIC) for larger
amounts of predictability, but BMA uniformly improves upon AR(AIC).



                                               16
4    Empirical Analysis
This section reports the results of an empirical comparison of the pseudo out-of-sample fore-
casting performance of the univariate posterior mean forecasts yTp + |T , relative to unadjusted
AR forecasts. The data set consists of monthly data on 132 U.S. monthly macroeconomic
time series, including data on real output, employment, wages and prices, monetary variables,
interest rates and exchange rates, and miscellaneous other indicators of monthly economic
activity from 1959:1 — 2003:12. The data set and data transformations are taken from Stock
and Watson (2005) and the reader is referred to that article for details. As in Stock and
Watson (2005), for variables transformed to growth rates or changes, -step ahead forecasts
are forecasts of cumulative changes, for example, of cumulative employment growth over the
next months, or of the cumulative change in the rate of price inflation over the next
months. Nominal series, such as prices, are modeled in changes of inflation, and the -step
ahead forecasts are of average inflation over the next months.
    Forecasts were computed recursively with the first forecast date being the earliest date
that 198 observations on the transformed variable were observed (so for real variables the
first forecast date is 1975:7). The final forecast date is 2003:12 — , and forecasts were
computed for horizons up to 12 months ahead. At each date, the AR forecasts and AR
component of the posterior mean forecasts were computed using lag length estimated by
BIC, where 0 ≤ p ≤ 18. We consider two types of priors on G for the posterior mean
forecasts: First, a demeaned Brownian motion of scale c (cf. (14) of Section 2.1 above), and
second, a sum of an independent demeaned Brownian motion of scale c, and a (demeaned)
truncated Brownian motion that varies only over frequencies below ω̄ = 2π/96 of fixed scale
20 (cf. (??) and (15) of Section 2.2). The latter is motivated by an expectation that the
baseline AR model exhibits relatively more pronounced misspecification below business cycle
frequencies (=cycles with periods of 96 months or more). The posterior mean forecasts were
computed for c fixed and equal to c = 0, 10, 20, 30, and two BMA forecasts for 0 ≤ c ≤ 20
and for 0 ≤ c ≤ 40.
    The results for the full data set are summarized in Table 3. All posterior mean procedures
provide improvements over the AR(BIC) forecasts, with mean and median relative MSFE
over the 132 series below unity at all horizons. The improvements are rather insensitive to
the choice of c and to the use of BMA averaging instead of a fixed c. Imposing additional
misspecification below business cycle frequencies leads to further improvements in forecast
performance at longer horizons. The mean relative reduction in the MSFE in the one month
ahead BMA (0 ≤ c ≤ 40) forecast is 0.011. Because of the recursive design, the sample size


                                              17
varies, but the average sample size is approximately 350. The improvement of 0.011 thus
corresponds roughly to a scaled improvement of 350×0.011 = 3.8 in the units of Table 2 and
Figures 1-4, and is at the upper end of the range of improvements reported there.
    For some series, the improvements are even greater and, in the context of the literature
that has used this data set, are in fact quite substantial: 10% of the series have relative
reductions in MSFEs of 6% or more for most of the posterior mean forecasting procedures
at the six month horizon. At the same time, the cost from using the procedure is small,
for example the 90% percentile of relative MSFEs is at most 1.011 at all horizons reported
for all the demeaned Wiener process BMA procedures. At longer horizons, also AR(AIC)
outperforms AR(BIC), with very large improvements at the 10% percentile. At the same
time, the cost associated with using AIC is also large in the sense that for some series
using the additional lags results in a marked deterioration of the AR(AIC) forecasts, with
90% percentiles exceeding 1.077 at the four reported horizons. Interestingly, in results not
reported here, it appears that the residual predictability found in Table 3 is particularly
pronounced in the nominal series.


5    Conclusion
This paper develops a framework to study slight misspecifications of finite order VARs by
modeling driving disturbances with a local-to-flat spectral density. We focus on the impact of
the misspecification on forecasts, and derive a computationally straightforward modification
of standard VAR based forecasts that eﬀectively exploits this residual predictability in large
samples. Monte Carlo and empirical evidence suggests that the large sample results provide
meaningful approximations for practically relevant sample sizes.
    The suggested framework and some of our theoretical results could also be applied to
study other issues involving slightly misspecified VARs. For instance, our likelihood approx-
imations naturally lead to computationally straightforward tests of the null hypothesis of
correct specification that maximize weighted average power in large samples. One could also
consider the problem of the estimation of the spectral density, or the impact of the slight
misspecification on standard large-sample inference about the VAR parameters, in analogy
to Müller and Petalas (2010) and Li and Müller (2009). A potentially diﬃcult but interesting
extension could be a combination of locally varying VAR parameters as in Stock and Watson
(1996) with the slight misspecification in the spectral domain considered here.




                                             18
A        Appendix: Proof of Theorem 1
                                              √
Notation and Preliminaries: Let |A| = tr A∗ A, and denote by ||A||2 the largest eigenvalue of
A∗ A, so that ||·|| is a submultiplicative matrix norm, and ||A|| ≤ |A|. Recall the following identities
for conformable matrices A, B, C, D:

                                 (A ⊗ B)(C ⊗ D) = AC ⊗ BD                                               (20)
                                                                  0
                                         vec(ABC) = (C ⊗ A) vec B                                       (21)
                                           ||A ⊗ B|| = ||A|| · ||B||                                    (22)
                                           ||A − B|| ≤ |A − B| = || vec(A − B)||.                       (23)

    Let Γ(G) be the kT × kT symmetric block Toeplitz matrix with jth k × k block in the lower
block diagonals equal to
                                    Z
                               T 1/2 π iωj
                     γ j (G) =        e (exp[T −1/2 G(ω)] − Ik )dω, j ≥ 0                (24)
                                2π −π

and γ(G) is the k2 (T + ) × 1 vector that is the vec of the k × (T + )k matrix with blocks γ j (G),
j = 1, · · · , T + . Let V (G) = IkT + T −1/2 Γ(G), so that e = (e01 , e02 , · · · , e0T )0 ∼ N (0, V (G)),
conditional on G.
    Similarly, let Γ̃(G) be the kT × kT symmetric block Toeplitz matrix with jth k × k block in
the lower block diagonals equal to
                                               Z π
                                             1
                                 γ̃ j (G) =        eiωj G(ω)dω, j ≥ 0                                  (25)
                                            2π −π

and γ̃(G) is the k2 N ×1 vector that is the vec of the k ×N k matrix with blocks γ̃ j (G), j = 1, · · · , N .
                            Rπ
Note that γ̃ 0 (G) = 0 from −π G(ω)dω = 0. Also, let DΓ (G) = Γ(G) − Γ̃(G).
   Let Λ be the N k2 × N k2 diagonal matrix diag(1, 1/2, 1/3, · · · , 1/N ) ⊗ Ik2 . Note that by (22)
and Lemma 4 (ii), ||Λ−1 ΣΛ−1 || = O(1). Define the T × (kp + 1) matrix
                                    ⎛                                 ⎞
                                        1 y00        0
                                                    y−1          0
                                                          · · · y−p+1
                                    ⎜ 1 y0           y00         0
                                                          · · · y−p+2 ⎟
                                    ⎜           1                     ⎟
                               X̄ = ⎜ .       ..      ..  ..       .. ⎟
                                    ⎝ ..       .       .      .     . ⎠
                                                  1 yT0 −1 yT0 −2 · · ·      yT0 −p

and note that with θ0 = (α0 , β 01 , β 02 , · · · , β 0p ) = (α0 , β 0 ),

                                  P −1 (y1 , · · · , yT ) = P −1 θ0 X̄ 0 + (e1 , · · · , eT )
                                       (IT ⊗ P −1 )y = (X̄ ⊗ P −1 ) vec θ0 + e
                                                          = X vec θ0 + e

where y = vec(y1 , · · · , yT ) and X = X̄ ⊗ P −1 .
   Let μ(θ) = limt→∞ Eθ [yt ], and denote by wμ : R(k+1)×p 7→ R the prior density of (μ(θ), β)
induced by w. Let μ0 = μ(θ0 ). It is clear that the posterior mean of yT + is equal to the sum

                                                               19
of μ0 and the posterior mean of yT + − μ0 given data {yt − μ0 }Tt=−p+1 , where the latter posterior
is computed using the prior w0 on θ such that the prior wμ0 on (μ(θ), β) induced by w0 satisfies
wμ0 ((μ, β)) = wμ ((μ + μ0 , β)), for all (μ, β). Also note that yTp + |T is equivariant to translations of
{yt }Tt=−p+1 . It thus suﬃces to show the result for yT + − μ0 given data {yt − μ0 }Tt=−p+1 and using
prior w0 . Noting that w0 ((0, β 0 )) = w((α0 , β 0 )), this amounts to showing the result for α0 = 0
and prior w0 . Thus, from now on, we will assume α0 = 0 and replace w with w0 . Note that
θ 7→ (μ(θ), β) is a continuous and invertible function at θ0 , so that w0 (θ) is continuous and positive
at (0, β 0 ), since w is continuous and positive at θ0 by assumption.
     All subsequent convergences and expectations with respect to the data are for the model where
(IT ⊗ P −1 )y ∼ N (X vec θ0 , IT k ). Corresponding convergences in probability under the data gen-
erating process assumed in the Theorem follow from contiguity, which is shown for a fixed G in
Dzhaparidze (1986), Theorem 4, page 64 for the univariate case. For brevity, we omit the concep-
tually straightforward extension of the contiguity proof.
     Let h = (a0 , b0 )0 = T 1/2 vec(θ − θ0 ), where a is k × 1. The log-likelihood ratio of the model where
(IT ⊗ P −1 )y ∼ N (X vec θ, V (G)1/2 ) to the model where (IT ⊗ P −1 )y ∼ N (X vec θ0 , IT k ), evaluated
at (IT ⊗ P −1 )y = X vec θ0 + e ∼ N (X vec θ0 , IT k ), is

ln LT (G, h) = − 12 ln det V (G) − 12 (e − T −1/2 Xh)0 V (G)−1 (e − T −1/2 Xh) + 12 e0 e
              = − 12 ln det V (G) − 12 e0 (V (G)−1 − IkT )e + T −1/2 e0 V (G)−1 Xh − 12 T −1 h0 X 0 V (G)−1 Xh.

Let A(b) be the pk × pk companion matrix of             the VAR with parameter β satisfying vec β =
vec β 0 + T −1/2 b,               ⎛                                      ⎞
                                    β1 β2                · · · β p−1 β p
                                  ⎜ Ik 0                 ···     0   0 ⎟
                                  ⎜                                      ⎟
                                  ⎜                                  0 ⎟
                           A(b) = ⎜ 0 Ik                 ···     0       ⎟
                                  ⎜ ..   ..              ..       ..  .. ⎟
                                  ⎝ .     .                  .     .   . ⎠
                                    0 0                  · · · Ik    0
define YT = vec(yT , yT −1 , · · · , yT −p+1 ) and J1 as the first k columns of Ipk . Note that yT + can be
written as
                                   X
                       yT + =           J10 A(b) −l J1 (P eT +l + T −1/2 a) + J10 A(b) YT .
                                l=1

Let R be the T k × T k matrix such that vec(eT , eT −1 , · · · , e1 ) = Re. Note that E[Ree0 R0 ] =
RV (G)R0 6= V (G), but R−1 = R0 = R, and ||R|| = 1. Since
    µ       ¶       µ µ                                                                                 ¶¶
      eT +l                      Ik + T −1/2 γ 0 (G)             T −1/2 (γ l (G), · · · , γ T +l−1 (G))
               ∼ N 0,                                                                                      ,
       Re                T −1/2 (γ l (G), · · · , γ T +l−1 (G))0              RV (G)R

we have, using (21)

                    E[eT +l |e] = T −1/2 (γ l (G), · · · , γ T +l−1 (G))(RV (G)R)−1 Re
                                 = T −1/2 ((RV (G)R)−1 Re ⊗ Ik )0 (∆l γ(G))
                                 = T −1/2 eT +l|T (G)

                                                    20
where ∆l is the T k2 ×(T + )k2 matrix such that ∆l γ(G) = vec(γ l (G), · · · , γ T +l−1 (G)). Conditional
on (h, G, e), the diﬀerence between the posterior mean of yT + and J10 A(0) YT , multiplied by T 1/2 ,
is
                                                                 X
      fT (G, h) = T 1/2 J10 (A(b) − A(0) )YT +                         J10 A(b)   −l
                                                                                       J1 (a + P eT +l|T (G))
                                                                 l=1
                   = T 1/2 J10 (A(b) − A(0) )YT
                                     X
                                 +    (J1 (a + P ((RV (G)R)−1 Re ⊗ Ik )0 (∆l γ(G))) ⊗ J1 )0 vec(A(b)                              −l
                                                                                                                                       )
                                     l=1

where the second equality uses (21). Let ê = e − T −1/2 X ĥ, where

                                           ĥ = (â0 , b̂0 )0 = (T −1 X 0 X)−1 T −1/2 X 0 e.

We will use the approximation

                                                                       X
          f˜T (G, h) = T 1/2 J10 (A(b̂) − A(0) )YT +                    ((A(b̂)l−1 ỸT )0 ⊗ J10 A(b̂)         −l
                                                                                                                   J1 )(b − b̂)
                                                                       l=1
                                 X
                             +                                  ˜ l γ̃(G)) ⊗ J1 )0 vec(A(b̂)
                                       J1 (a + P (R̃ê ⊗ Ik )0 (∆                                    −l
                                                                                                          )
                                 l=1

where the N k ×kT matrix R̃ satisfies R̃ê = vec(êT , êT −1 , · · · , êT −N+1 ), ∆  ˜ l is the N k2 ×N k 2 matrix
such that ∆                                                                  ˜l = ∆
          ˜ l γ̃(G) = vec(γ̃ l (G), · · · , γ̃ N (G), 0, · · · , 0) (so that ∆    ¯ l ⊗Ik2 with ∆ ¯ l a N ×N matrix
with ones on the (l − 1)th upper diagonal, and zero elsewhere), and
                                                                         N−1
                                                                         X
                                                          0       −1/2
                          ỸT = YT − (1, · · · , 1) ⊗ (T                        Ψ̂t a) − A(b̂)N YT −N
                                                                          t=0

with YT −N = vec(yT −N , yT −N−1 , · · · , yT −N−p+1 ). Note that f˜T (G, h) is linear in γ̃(G) and
                                                                                                 P h.
   Let ŝ be the N k2 ×1 vector which is the vec of the k×N k matrix with blocks ŝj = T −1 t êt ê0t−j ,
j = 1, · · · , N . Define m to be the N k2 × pk2 matrix with k2 × pk2 blocks equal to

                                     mj = (P 0 Ψ0j−1 , P 0 Ψ0j−2 , · · · , P 0 Ψ0j−p ) ⊗ P −1
                                                                          P∞
for j = 1, · · · , N, where (Ik − β 01 x − · · · − β 0p xp )−1 =              j=−∞ Ψj x
                                                                                       j      (so that Ψj = 0 for j < 0, and
Ψ0 = Ik ). Note that
                         ⎛         ⎛                ⎞⎛                 ⎞0 ⎞
                                       Ψj−1 P              Ψj−1 P
                   ⎜XN ⎜
                                       Ψj−2 P       ⎟⎜     Ψj−2 P      ⎟⎟
                   ⎜   ⎜                            ⎟⎜                 ⎟⎟
            m0 m = ⎜   ⎜                 ..         ⎟⎜       ..        ⎟ ⎟ ⊗ (P −10 P −1 )                                                 (26)
                   ⎝   ⎝                  .         ⎠⎝        .        ⎠⎠
                             j=1
                                       Ψj−p P              Ψj−p P


                                                                 21
                                      ⎛                                                                 ⎞
                                            E[yt yt0 ]               0 ]
                                                              E[yt yt−1                        0
                                                                                  · · · E[yt yt−p+1 ]
                                     ⎜     E[yt−1 yt0 ]        E[yt yt0 ]                      0
                                                                                  · · · E[yt yt−p+2 ]   ⎟
                        N→∞          ⎜                                                                  ⎟
                           →     lim ⎜         ..                 ..                          ..        ⎟ ⊗ Ω−1 .
                                t→∞ ⎝           .                  .                           .        ⎠
                                          E[yt−p+1 yt0 ] E[yt−p+2 yt0 ] · · ·              E[yt yt0 ]
                                                                                  p                                        p
where Ω = P P 0 , and it is not diﬃcult to see that m̂0 m̂−m0 m → 0. Also, by Lemma 8 (ii), m̂0 ŝ → 0.
Define M̂ = (I + Σ)−1 m̂ and Ŝ = (I + Σ)−1 ŝ.
   Note that by Lemma 5 (i),

                                                   X
  f˜T (G, h) = T 1/2 J10 (A(b̂) − A(0) )YT +              J10 A(b̂)   −l                           ˜ l (γ̃(G) + m̂(b − b̂))].
                                                                           J1 [a + P (R̃ê ⊗ Ik )0 ∆
                                                    l=1

      The approximation of the likelihood proceeds as follows

ln LT (G, h) = − 12 ln det V (G) − 12 e0 (V (G)−1 − I)e + T −1/2 e0 V (G)−1 Xh − 12 T −1 h0 X 0 V (G)−1 Xh
                    1 −1/2 0
               ≈    2T    e Γ̃(G)e − 12 T −1/2 tr Γ̃(G) − 14 T −1 tr Γ̃(G)2              + T −1/2 e0 Xh
                                                                −1 0
                                                                      −T        e Γ̃(G)Xh − 12 T −1 h0 X 0 Xh
                    1 −1/2 0
               ≈    2T    ê Γ̃(G)ê − 12 T −1/2 tr Γ̃(G) − 14 T −1 tr Γ̃(G)2            − T −1 (h − ĥ)0 X 0 Γ̃(G)e
                                                                  −1 0 0
                                                                      +T        ĥ X Xh − 12 T −1 h0 X 0 Xh
               ≈ ŝ0 γ̃(G) − 12 γ̃(G)0 γ̃(G) − (b − b̂)0 m̂0 γ̃(G) + b̂0 m̂0 m̂b − 12 b0 m̂0 m̂b + â0 Ω−1 a − 12 a0 Ω−1 a
               ≈ − 12 ||ŝ − γ̃(G) − m̂(b − b̂)||2 − 12 (a − â)0 Ω−1 (a − â) + 12 ||ŝ||2 + 12 b̂0 m̂0 m̂b̂ + 12 â0 Ω−1 â
               = ln L̃T (G, h).

so that the approximate log-likelihood ln L̃T (G, h) is quadratic in (h, γ̃(G)).
    Define ST (G) = 1[sup−π≤ω≤π ||G(ω)|| ≤ T κ ] for some 0 < κ < 1/6. Let Ey stand for inte-
gration over the sampling distribution of e (and y, X, etc.) in the model where (IT ⊗ P −1 )y ∼
N (X vec θ0 , IT k ) and e ∼ N (0, IT k ), EG integration over G, Eh integration over N (ĥ, CT−1 Ipk2 +k ),
where CT is defined in Lemma 7. Also, let wc0 (h) = w0 (θ), where vec θ = vec(θ0 ) + T −1/2 h.

      Proof of Theorem 1:
      We start with the first claim. We show
                   R                             R
                EG wc0 (h)fT (G, h)LT (G, h)dh EG wc0 (0)f˜T (G, h)L̃T (G, h)dh p
                       R                      −      R                          →0                                         (27)
                    EG wc0 (h)LT (G, h)dh         EG wc0 (0)L̃T (G, h)dh

via
                           Z
                                                                                                          p
                      EG       ||wc0 (h)fT (G, h)LT (G, h) − wc0 (0)f˜T (G, h)L̃T (G, h)||dh → 0                           (28)
                           Z
                                                                                   p
                      EG       |wc0 (h)LT (G, h) − wc0 (0)L̃T (G, h)|dh → 0.                                               (29)




                                                             22
                                                                       R
These imply (27) if we can show that d˜−1                   ˜                                 ∗
                                            T = Op (1) with dT = EG L̃T (G, h)dh. Write Eh for the
                                  2 +k
integration over the ball in R pk      of volume 1 and centered at zero, so that d˜T ≥ Eh EG L̃T (G, h).
                                                                                        ∗

Then
                                          1
                          d˜−1
                            T ≤      ∗
                                                   ≤ exp[−Eh∗ EG ln L̃T (G, h)]
                                  Eh EG L̃T (G, h)
where the second inequality follows from Jensen, and

            −2Eh∗ EG ln L̃T (G, h) = EG γ̃(G)0 γ̃(G) + Eh∗ (b0 m̂0 m̂b) + Eh∗ (a0 Ω−1 a)
                                      = tr Σ + tr(m̂0 m̂Eh∗ [hh0 ]) + tr(Ω−1 Eh∗ (aa0 )) = Op (1)

using Lemma 4 (i). We are thus left to show (28) and (29). We focus on (28), which is strictly
more diﬃcult.
    Now
                  Z
                    EG ||wc0 (h)fT (G, h)LT (G, h) − wc0 (0)f˜T (G, h)L̃T (G, h)||dh
               Z
              ≤ EG ||ST (G)wc0 (h)fT (G, h)LT (G, h) − wc0 (0)f˜T (G, h)L̃T (G, h)||dh
                         Z
                      + EG (1 − ST (G))wc0 (h)||fT (G, h)||LT (G, h)dh.

The last term converges in probability to zero by Lemma 2 (ii). By Hoelder, Lemma 7 (ii), Jensen
and Minkowksi, we obtain
         Z
            EG ||ST (G)wc0 (h)fT (G, h)LT (G, h) − wc0 (0)f˜T (G, h)L̃T (G, h)||dh
         Z
                                                                   LT (G, h)
     ≤      (EG (L̃T (G, h)4 ))1/4 · (EG ||ST (G)wc0 (h)fT (G, h)             − wc0 (0)f˜T (G, h)||4/3 )3/4 dh
                                                                   L̃T (G, h)
                                              L T (G, h)
     = ζ T Eh (EG ||ST (G)wc0 (h)fT (G, h)               − wc0 (0)f˜T (G, h)||4/3 )3/4
                                              L̃T (G, h)
                                              LT (G, h)
     ≤ ζ T (Eh EG ||ST (G)wc0 (h)fT (G, h)               − wc0 (0)f˜T (G, h)||4/3 )3/4
                                              L̃T (G, h)
                                              LT (G, h)
     ≤ ζ T (Eh EG ||ST (G)wc0 (h)fT (G, h)               − wc0 (0)ST (G)f˜T (G, h)||4/3 )3/4
                                              L̃T (G, h)
         +ζ w0 (0)(EG Eh ||(1 − ST (G))f˜T (G, h)||4/3 )3/4
             T   c

where ζ T = Op (1), and by Hoelder and Lemmas 2 (i) and 6 (i),
                                                                                                    p
     (EG Eh ||(1 − ST (G))f˜T (G, h)||4/3 )3/4 ≤ (EG (1 − ST (G)))1/4 · (Eh EG ||f˜T (G, h)||2 )1/2 → 0.

   Now apply Lemma 1 with Ec = Eh EG , x1 = ST (G)UT (h)fT (G, h), x̃1 = ST (G)UT (h)f˜T (G, h),
x2 = wc0 (0), x̃2 = wc0 (h), x̃j = ST (G) for j > 2 and xj various products in ST (G)LT (G, h)/L̃T (G, h)
and x̃j = 1, so that Lemmas 3 and 6 imply
                                                      LT (G, h)                           p
                     Eh EG ST (G)||wc0 (h)fT (G, h)              − wc0 (0)f˜T (G, h)||4/3 → 0
                                                      L̃T (G, h)

                                                        23
as was left to be shown. We therefore established (27).
    Now by Lemma 9,
                         R
                      EG wc0 (0)f˜T (G, h)L̃T (G, h)dh
                             R                         − T 1/2 J10 (A(b̂) − A(0) )YT
                          EG wc0 (0)L̃T (G, h)dh
                            X
                    =             J10 A(b̂)   −l                          ˜ l (γ p + m̂bp )]
                                                   J1 [â + P (R̃ê ⊗ I)0 ∆
                            l=1
                            X
                    =             J10 A(b̂)   −l                          ˜ l (ŝ − Ŝ + M̂ (m̂0 M̂ )−1 m̂0 Ŝ)]
                                                   J1 [â + P (R̃ê ⊗ I)0 ∆
                            l=1

and the first claim follows from Lemma 5 (ii).
   The second claim follows from the same arguments as in the proof of Theorem 4 of Müller and
Petalas (2010), using (29) and the last claim of Lemma 9.

Lemma 1 (i) Let x1 and x̃1 be sequences (in T ) of random vectors, and let xj and x̃j , j = 2, · · · , l
be sequences (in T ) of scalar random variables, and denote by Ec a conditional expectation.
                                                    p
    (i) If Ec ||x̃1 ||2 = Op (1), Ec ||x1 − x̃1 ||2 → 0, and for all K ∈ N and j ≥ 2, Ec |x̃j |K = Op (1) and
                 p                Q             Q             p
Ec |xj − x̃j |K → 0, then Ec || lj=1 xj − lj=1 x̃j ||4/3 → 0.
                                               p                                                   p
    (ii) For all j ≥ 2, if Ec [x̃j K ] → 1 for all K ∈ N, then Ec |x̃j − 1|K → 0 for all K ∈ N.

    Proof. (i) Define zj (0) = ||x̃j || and zj (1) = ||xj − x̃j ||. Then
                                               l
                                               Y            l
                                                            Y                 l
                                                                             XY
                                          ||         xj −         x̃j || ≤              zj (vj )
                                               j=1          j=1              v∈Sv j=1

almost surely, where Sv is the set of all vectors v = (v1 , · · · , vl )0 ∈ Rl with elements vj equal to zero
or one, and v 0 v ≥ 1. Let #Sv be the number of elements in Sv . Then, for any p > 1, by convexity
                                         l                 l
                                      1 XY          p   1 XY
                                   (       zj (vj )) ≤       zj (vj )p
                                     #Sv               #Sv
                                              v∈Sv j=1                           v∈Sv j=1

Furthermore, by Hoelder,
                            l
                            Y            l
                                         Y                                     l ³
                                                                              XY                    ´1/pj
                    Ec ||         xj −         x̃j ||4/3 ≤ (#Sv )4/3              Ec zj (vj )4pj /3
                            j=1          j=1                                 v∈Sv j=1

                                           P
for any pj > 1,j = 1, · · · , l such that lj=1 1/pj = 1. The result now follow from setting p1 = 3/2.
    (ii) By Jensen,
                PKit suﬃces       to consider even K. The result then follows from applying the premise
                                j
to (x̃j − 1) = j=0 bK,j x̃j (−1)K−j , where bK,j are the binomial coeﬃcients.
            K




                                                                    24
Lemma 2 (i) T K EG (1 − ST (G)) → 0 for all finite K.
           R                                           p
  (ii) EG (1 − ST (G))wc0 (h)||fT (G, h)||LT (G, h)dh → 0.
  (iii) ST (G)||DΓ (G)|| ≤ 2T −1+κ a.s. and ST (G)T −1 tr DΓ (G)2 ≤ 4kT −2+4κ a.s.
  (iv) ST (G)||Γ̃(G)|| ≤ T κ a.s. and ST (G)||Γ(G)|| ≤ 2T κ a.s.
  (v) ST (G)||V (G)−1 || ≤ 1 + 4T κ−1/2 a.s. for T > 64.
  (vi) ST (G)||γ(G) − γ̃(G)|| ≤ 2kT −1+2κ a.s.
  (vii) ST (G)||γ(G)||        κ
                   PT ≤ 2kT a.s. 2
  (viii) EG ST (G) j=N+1 |γ j (G)| → 0.

    Proof. Define STc (G) = 1 − ST (G), and dT (G, ω) = T 1/2 exp[T −1/2 G(ω)] − T 1/2 Ik − G(ω).
    (i) By the Isoperimetric Inequality, the tail of the random variable sup−π≤ω≤π ||G(ω)|| is dom-
inated by a normal with suﬃciently large mean and variance, so that EG STc (G) → 0 exponentially
fast.
    (ii) For any ε > 0, by Markov
       Z                                                          Z
P (EG ST (G)wc (h)||fT (G, h)||LT (G, h)dh > ε) ≤ ε Ey EG STc (G)wc0 (h)||fT (G, h)||LT (G, h)dh
            c     0                                       −1

                                                        Z
                                               = EG STc (G)wc0 (h)Ey|G,h ||fT (G, h)||dh

where Ey|G,h denotes integration over (e, y, X) in the model (IT ⊗ P −1 )y ∼ N (X vec θ, V (G)),
where vec θ = vec θ0 + T −1/2 h.
    Define FT (G, h) = T −1/2 fT (G, h) + J10 A(0) YT , which is the conditional mean of yT +
given (y, h, G) by definition of fT (G, h). Therefore Ey|G,h [||FT (G, h)||2 ] ≤ Ey|G,h [||yT + ||2 ] ≤
supt≥1 Ey|G,h [||yt ||2 ]. Furthermore, Ey|G,h [||J10 A(0) YT ||2 ] can also be be bounded in terms of
supt≥1 Ey|G,h [||yt ||2 ].
    Now in model (18), supt≥1 Ey|G,h [||yt ||2 ] ≤ supt≥1 Eθ [||yt ||2 ] · sup−π≤ω≤π || exp[T −1/2 G(ω)]||.
Furthermore, sup−π≤ω≤π || exp[T −1/2 G(ω)]|| ≤ sup−π≤ω≤π exp[T −1/2 ||G(ω)||] and by Cauchy-
Schwarz
    Z                                                              µ                                ¶1/2
         c                       −1/2                    c                    −1/2
 EG ST (G) sup || exp[T               G(ω)]||dh ≤ (EG ST (G)) · EG exp[2T           sup ||G(ω)||]        .
               −π≤ω≤π                                                              −π≤ω≤π


But EG exp[2T −1/2 sup−π≤ω≤π ||G(ω)||] = O(1), so that
     Z                                                      Z
         c     0                                    k/2+1/2
 EG ST (G)wc (h)Ey|G,h ||fT (G, h)||dh ≤ (C1 + C2 T           sup Eθ [||yt ||2 ]w(θ)dθ) · EG STc (G)
                                                                         t≥1

for some large enough C1 and C2 , and the result follows from the result of part (i), since ε > 0 was
arbitrary.                                 P
    (iii) By definition, exp[T −1/2 G(ω)] = ∞
                                            j=0 (T
                                                   −1/2 G(ω))j /j!. Thus,


                                                         ∞
                                                         X (T κ−1/2 )j
                          sup    ST (G)||dT (G, ω)|| ≤                    ≤ 2T −1+2κ                  (30)
                        −π≤ω≤π                                  j!
                                                         j=2


                                                    25
almost surely, and the first result follows from Lemma 3.1 (i)R π of Davis (1973). 2Furthermore, by
                                                            1
Lemma 3.1 (iii) of Davies (1973), ST (G)T −1 tr DΓ (G)2 ≤ 2π   −π  |ST (G)dT (G, ω)| dω ≤ 4kT −2+4κ .

   (iv) Follows from Lemma 3.1 (i) of Davis (1973), the norm inequality, and part (iii).
   (v) Follows from ||V (G)−1 || ≤ 1/(1 − T −1/2 ||Γ(G)||)
                                                  R π ijω and part (iv).
                                               1
   (vi) By construction, γ j (G) − γ̃(G)j = 2π        e dT (G, ω)dω. Thus, by Lemma 3.1 (iii) of
                                             1
                                               R π −π
Davies (1973), ST (G)||γ(G) − γ̃(G)|| ≤ 2π −π |ST (G)dT (G, ω)|2 dω ≤ 4kT −2+4κ a.s., as in the
                                       2

proof of part (iii).
   (vii) Follows fromP the norm inequality and part (vi).
                                                        P
   (viii) EG ST (G) Tj=N+1 |γ j (G)|2 ≤ 4kT −2+4κ +EG Tj=N+1 |γ̃ j (G)|2 = 4kT −2+4κ +tr ΣNT → 0,
where ΣNT is defined in the proof of Lemma 3 (xi).

Lemma 3 For any fixed K > 0,
                                                                  p
                                  Eh EG ST (G) exp[Kξ T (G, h)] → 1

where ξ T (G, h) is one of the following expressions:
   (i) ln det V (G) − tr Γ(G) + 12 tr Γ(G)2
   (ii) e0 (V (G)−1 − I)e − e0 (−T −1/2 Γ(G) + T −1 Γ(G)2 )e
   (iii) T −1/2 e0 V (G)−1 Xh − (T −1/2 e0 Xh − T −1 e0 Γ(G)Xh)
   (iv) T −1 h0 X 0 V (G)−1 Xh − T −1 h0 X 0 Xh
   (v) T −1 e0 Γ(G)2 e − T −1 tr Γ(G)2
   (vi) T −1/2 e0 DΓ (G)e − T −1/2 tr DΓ (G)
   (vii) T −1 tr Γ(G)2 − T −1 tr Γ̃(G)2
   (viii) T −1 e0 DΓ (G)Xh
   (ix) T −3/2 ĥ0 X 0 Γ̃(G)Xh
   (x) T −3/2 ĥ0 X 0 Γ̃(G)X ĥ
   (xi) T −1 12 tr Γ̃(G)2 − γ̃(G)0 γ̃(G)
   (xii) T −1 h0 X 0 Xh − a0 Ω−1 a − b0 m̂0 m̂b
   (xiii) T −1 ĥ0 X 0 Xh − â0 Ω−1 a − b̂0 m̂0 m̂b
   (xiv) T −1 (h − ĥ)0 X 0 Γ̃(G)e − (b − b̂)0 m̂0 γ̃(G)
   (xv) ŝ0 m̂(b − b̂)
   (xvi) ln wc0 (h) − ln wc0 (0)

   Proof. Note that by Lemma 2 (i)

        Eh EG exp[KST (G)ξ T (G, h)] − Eh EG ST (G) exp[Kξ T (G, h)] = EG (1 − ST (G)) → 0

so it suﬃces to show the claim for Eh EG exp[KST (G)ξ T (G, h)]. We repeatedly rely on ||T −1/2 e|| =
Op (1), ||T −1/2 X|| = Op (1) and ||ĥ|| = Op (1). Also, note that it suﬃces to show that

                                 Ey Eh EG exp[KST (G)ξ T (G, h)] → 1                               (31)

for any K, since by Jensen, Ey (Eh EG exp[K 0 ST (G)ξ T (G, h)])2 ≤ Ey Eh EG exp[2K 0 ST (G)ξ T (G, h)],
so that (31) with K = K 0 , 2K 0 implies Eh EG exp[KST (G)ξ T (G, h)] → 1 in quadratic mean.



                                                  26
   (i) By Lemma A1.1 of Dzhaparidze (1986), for any matrix A with ||A|| < 1,
                                                                            2                  3
                                                             1 ||A|| · (tr A )       1 T ||A||
                   | ln det(I + A) − tr A + 12 tr A2 | ≤     3 (1 − ||A||)3      ≤   3 (1 − ||A||)3


where the second inequality follows from tr A2                   ≤    T ||A||2 .       Since by Lemma 2 (iv),
ST (G)||KT −1/2 Γ(G)|| → 0, for large enough T ,

                      ST (G)K| ln det(I + T −1/2 Γ(G)) − T −1/2 tr Γ(G) + 12 T −1 tr Γ(G)2 |
                                               ||Γ(G)||3
                  ≤ K 13 ST (G)T −1/2                           →0
                                         (1 − T −1/2 ||Γ(G)||)3

uniformly in G, where the convergence follows from Lemma 2 (iv).
    (ii) With V (G)−1 = I − T −1/2 Γ(G) + T −1 Γ(G)2 V (G)−1 , we find

          |ST (G)Ke0 (V (G)−1 − I + T −1/2 Γ(G) − T −1 Γ(G)2 )e|
                                                                                                            p
      = |ST (G)T −3/2 Ke0 Γ(G)3 V (G)−1 e| ≤ T −1/2 ||T −1/2 e||2 ST (G)K||V (G)−1 || · ||Γ(G)||3 → 0

uniformly in G, where the convergence follows from Lemma 2 (iv) and (v).
    (iii)

      Eh exp[ST (G)KT −1/2 e0 (V (G)−1 − I + T −1/2 Γ(G))Xh]
  = Eh exp[ST (G)KT −3/2 e0 Γ(G)2 V (G)−1 Xh]
  = exp[ 12 ST (G)T −3 CT−1 K 2 ||e0 Γ(G)2 V (G)−1 X||2 + ST (G)KT −3/2 e0 Γ(G)2 V (G)−1 X ĥ]
  ≤ exp[ 12 ST (G)CT−1 K 2 ||T −1/2 e||2 ||T −1/2 X||2 ||V (G)−1 ||2 T −1 ||Γ(G)2 ||
                                                                                                                p
                                 +ST (G)KT −1/2 ||T −1/2 e|| · ||Γ(G)2 || · ||V (G)−1 || · ||T −1/2 X|| · ||ĥ||] → 1

uniformly in G, where the convergence follows from Lemma 2 (iv) and (v).
    (iv)

                           Eh ST (G) exp[KT −1 h0 X 0 (V (G)−1 − I)Xh]
                      ≤ ST (G)Eh exp[||V (G)−1 − I||KT −1 h0 X 0 Xh]
                                                                                               p
                      = ST (G) det(Ipk2 +k − CT ||V (G)−1 − I||KT −1 X 0 X)−1/2 → 1

uniformly in G, since ST (G)||V (G)−1 − I|| ≤ 1/(1 − T −1/2 ST (G)||Γ(G)||)−1 − 1 → 0 uniformly in
G from Lemma 2 (iv).
    (v)

                       Ey exp[ 12 ST (G)KT −1 e0 Γ(G)2 e − 12 ST (G)KT −1 tr Γ(G)2 ]
                   = exp[− 12 ln det(I − ST (G)KT −1 Γ(G)2 ) − 12 ST (G)KT −1 tr Γ(G)2 ]

and for large enough T ,

ST (G)KT −1 tr Γ(G)2 −ST (G)K 2 T −2 tr Γ(G)4 ≤ ln det(I−ST (G)KT −1 Γ(G)2 ) ≤ ST (G)KT −1 tr Γ(G)2

                                                        27
uniformly in G, because ||ST (G)KT −1 Γ(G)2 || → 0 by Lemma 2 (iv), and for |x| < 1/2, −x − x2 ≤
ln(1 − x) ≤ −x. With tr A4 ≤ ||A||2 tr A2 ≤ T ||A||4 , T −2 ST (G) tr Γ(G)4 ≤ ST (G)T −1 ||Γ(G)||4 → 0
uniformly in G by Lemma 2 (iv).
    (vi)

       Ey exp[ST (G)KT −1/2 (e0 DΓ (G)e − tr DΓ (G))]
   = exp[−ST (G)KT −1/2 tr DΓ (G)] det(I − ST (G)2KT −1/2 DΓ (G))−1/2
   ≤ exp[−ST (G)KT −1/2 tr DΓ (G)] exp[ 12 (ST (G)2KT −1/2 tr DΓ (G) + ST (G)4K 2 T −1 tr DΓ (G)2 ]
   = exp[ST (G)4K 2 T −1 tr DΓ (G)2 ] → 1

uniformly in G by Lemma 2 (iii), where the inequality follows for suﬃciently large T , because
||ST (G)2KT −1/2 DΓ (G)|| → 0, and for |x| < 1/2, ln(1 − x) ≥ −x − x2 .
    (vii)

ST (G)T −1 | tr Γ(G)2 − tr Γ̃(G)2 | ≤ ST (G)T −1 tr DΓ (G)2 + 2ST (G)T −1 | tr DΓ (G)Γ̃(G)|
                                      ≤ ST (G)T −1 tr DΓ (G)2 + 2ST (G)(T −1 tr DΓ (G)2 )1/2 ||T −1/2 Γ̃(G)|| → 0

uniformly in G by Lemma 2 (iii) and (iv).
    (viii)

                Eh exp[ST (G)KT −1 e0 DΓ (G)Xh]
           = exp[ 12 ST (G)CT−1 K 2 T −2 e0 DΓ (G)XX 0 DΓ (G)e + ST (G)KT −1 e0 DΓ (G)X ĥ]
           ≤ exp[ 12 CT−1 K 2 ||T −1/2 e||2 ||T −1/2 X||2 ST (G)||DΓ (G)||2 ]
                                                                                                       p
                                     × exp[K||T −1/2 e|| · ST (G)||DΓ (G)|| · ||T −1/2 X|| · ||ĥ||] → 1

uniformly in G by Lemma 2 (iii).
    (ix)

      Eh exp[ST (G)KT −3/2 ĥ0 X 0 Γ̃(G)h]
  = exp[ 12 ST (G)CT−1 K 2 T −3 ĥ0 X 0 Γ̃(G)2 X ĥ + ST (G)KT −3/2 ĥ0 X 0 Γ̃(G)ĥ]
                                                                                                              p
  ≤ exp[CT−1 K 2 ST (G)||T −1/2 Γ̃(G)||2 ||T −1/2 X||2 ||ĥ||2 + ST (G)KT −1 ||ĥ||2 ||T −1/2 X|| · ||Γ̃(G)||] → 1

uniformly in G by Lemma 2 (iv).
    (x)
                                                                                                       p
        exp[ST (G)KT −3/2 ĥ0 X 0 Γ̃(G)X ĥ] ≤ exp[ST (G)||T −1/2 Γ̃(G)|| · ||T −1/2 X||2 ||ĥ||2 ] → 1

uniformly in G by Lemma 2.             P                                     Rπ
    (xi) By Lemma 3.1 of Davies (1973), ∞              2
                                        j=−∞ |γ̃ j (G)| =
                                                                         1
                                                                        2π   −π   |G(ω)|2 dω < ∞ a.s. Thus

                                                          N
                                                          X                       T
                                                                                  X
            |γ̃(G)0 γ̃(G) − 12 T −1 tr Γ̃(G)2 | ≤ T −1          j|γ̃ j (G)|2 +           |γ̃ j (G)|2
                                                          j=1                    j=N+1



                                                       28
                                                                  N              T
                                                                NX            2
                                                                                 X
                                                         ≤          |γ̃ j (G)| +   |γ̃ j (G)|2 =: εγ (G)
                                                                T
                                                                     j=1                 j=N+1

and a straightforward argument shows εγ (G) → 0 for all G with square summable |γ̃ j (G)|. Fur-
thermore,
                                                     N                                T
                                                   NX                                 X
        EG exp[2Kεγ (G)] ≤ (EG exp[4K                  |γ̃ j (G)|2 ])1/2 · (EG exp[4K   |γ̃ j (G)|2 ])1/2 .
                                                   T
                                                         j=1                                        j=N+1

                                     PN
It is easy to see that EG exp[4K N T
                                                      2
                                        j=1 |γ̃ j (G)| ] = O(1). Since tr Σ = O(1) by Lemma 4 (i),
tr ΣNT → 0, where ΣNT is the prior covariance matrix of γ̃ NT (G) = vec(γ̃ N+1 (G), · · · , γ̃ T (G)).
Therefore also ||ΣNT || → 0, and for large enough T ,
                                          T
                                          X
                          EG exp[4K              |γ̃ j (G)|2 ] = det(I − 8KΣNT )−1/2
                                         j=N+1

                                                                     = exp[− 12 ln det(I − 8KΣNT )]
                                                                     ≤ exp[ 12 16K tr ΣNT ] → 1

where ln det(I − 8KΣNT ) ≥ 16K tr ΣNT because for 0 ≤ x < 1/2, ln(1 − x) ≥ −2x.
Thus exp[Kεγ (G)] has uniformly bounded second moments, so that εγ (G) → 0 implies
EG exp[Kεγ (G)] → 1.
                              p
   (xii) Clearly, m̂0 m̂−m0 m → 0, so that from a law of large numbers, with D̂X = diag(Ω−1 , m̂0 m̂),
                                                                               p
                                                   T −1 X 0 X − D̂X → 0.

Thus,
                                                                                                            p
             Eh exp[ 12 Kh0 (T −1 X 0 X − D̂X )h] ≤ det(I − CT K(T −1 X 0 X − D̂X ))−1/2 → 1.
    (xiii) In the notation of the proof of part (xii)
                                                                                                                    p
Eh exp[Kh0 (T −1 X 0 X − D̂X )ĥ] = exp[ 12 CT−1 K 2 ĥ0 (T −1 X 0 X − D̂X )2 ĥ + K ĥ0 (T −1 X 0 X − D̂X )ĥ] → 1.

    (xiv) Note that for any K 0 ,

     Eh EG exp[K 0 (b − b̂)0 (m̂ − m)0 γ̃(G)] = Eh exp[K 02 (b − b̂)0 (m̂ − m)0 Σ(m̂ − m)(b − b̂)]
                                                         ≤ Eh exp[K 02 ||Λ−1 ΣΛ−1 || · ||(m̂ − m)0 Λ2 (m̂ − m)||]
                                     p                           p
and ||(m̂ − m)0 Λ2 (m̂ − m)|| → 0 follows from θ̂ → θ0 , and ||Λ−1 ΣΛ−1 | = O(1) by Lemma 4 (ii). The
                                                                  p
convergence of Eh exp[K 02 ||Λ−1 ΣΛ−1 ||·||(m̂−m)0 Λ2 (m̂−m)||] → 1 now follows as in the proof of part
(xii). By Cauchy-Schwarz, it thus suﬃces to show the claim for T −1 (h−ĥ)0 X 0 Γ̃(G)e−(b− b̂)0 m0 γ̃(G).
    The first k × 1 block of T −1 X 0 Γ̃(G)e is equal to
            T
            X −1          T
                          X                              T
                                                         X −1             T
                                                                          X
                   T −1           P −10 γ̃ j (G)et−j +          T −1           P −10 γ̃ j (G)0 et
            j=1           t=j+1                          j=1           t=j+1


                                                                     29
                        ⎛                                 ⎞                                    ⎛                              ⎞
               T
               X −1                 T
                                    X                                                T
                                                                                     X −1               T
                                                                                                        X                        ¡          ¢
        =               ⎝T −1            e0t−j ⊗ P −10 ⎠ vec(γ̃ j (G)) +                       ⎝T −1            e0t ⊗ P −10 ⎠ vec γ̃ j (G)0
               j=1               t=j+1                                                   j=1            t=j+1
                        ⎛                                                                 ⎞0
               T
               X −1                 T
                                    X
        =               ⎝T −1           (et−j ⊗ P −1 + Ckk (et ⊗ P −1 ))⎠ vec(γ̃ j (G))
               j=1               t=j+1
                        0
        = ξ̄(j) vec(γ̃ j (G))
                                                                              ¡       ¢
                                                           0 vec(γ̃ (G)) = vec γ̃ (G)0 , and the following
where Ckk is the commutation matrix that satisfies Ckk             j             j
k2 × 1 blocks of T −1 X 0 Γ̃(G)e are given by, i = 1, · · · , p
        T
        X −1                T
                            X                                        T
                                                                     X −1                T
                                                                                         X
                   −1                        −10                                −1
               T                [yt−i ⊗ P          ]γ̃ j (G)et−j +          T                  [yt−j−i ⊗ P −10 ]γ̃ j (G)0 et
        j=1             t=j+1                                        j=1             t=j+1
                    ⎛                                            ⎞                        ⎛                                           ⎞
        T
        X −1                                      T
                                                  X                         T
                                                                            X −1                                          T
                                                                                                                          X
  =            vec ⎝P −10 γ̃ j (G)T −1                        0 ⎠
                                                        et−j yt−i +                vec ⎝P −10 γ̃ j (G)0 T −1                      0
                                                                                                                              et yt−j−i ⎠
        j=1                                    t=j+1                        j=1                                      t=j+1
               ⎛                                            ⎞                                    ⎛                                          ⎞
        T
        X −1                 T
                             X                                                            T
                                                                                          X −1             T
                                                                                                           X                                  ¡          ¢
  =            ⎝(T −1                     0
                                    et−j yt−i )0 ⊗ P −10 ⎠ vec(γ̃ j (G)) +                       ⎝(T −1                 0
                                                                                                                    et yt−j−i )0 ⊗ P −10 ⎠ vec γ̃ j (G)0
        j=1                 t=j+1                                                         j=1             t=j+1
               ⎛                                                                                   ⎞0
        T
        X −1                T
                            X
  =            ⎝T −1                     0
                                  (et−j yt−i ⊗ P −1 + Ckk (et yt−j−i
                                                               0
                                                                     ⊗ P −1 ))⎠ vec(γ̃ j (G))
        j=1                 t=j+1

  =     m̄(i)0j    vec(γ̃ j (G)).
Thus                                                            µ       0                  0                    ¶
                                             −1     0                 ξ̄ γ̃(G) + ξ̄ NT γ̃ NT (G)
                                         T        X Γ̃(G)e =
                                                                     m̄0 γ̃(G) + m̄0NT γ̃ NT (G)
where γ̃ NT (G) = vec(γ̃ N+1 (G)0 , · · · , γ̃ T (G)0 ), the jth k2 × k block of ξ̄ and ξ̄ NT equals ξ̄(j) and
ξ̄(j + 1), respectively, and the jth k2 × k 2 p block of m̄ and m̄NT equals m̄j = (m̄(1)0j , · · · , m̄(p)0j )0
and m̄N+j = (m̄(1)0N+j , · · · , m̄(p)0N+j )0 , respectively.
     Thus
                                                                                           0                          0
      T −1 (h − ĥ)0 X 0 Γ̃(G)e − (b − b̂)0 m0 γ̃(G) = (a − â)0 ξ̄ γ̃(G) + (a − â)0 ξ̄ NT γ̃ NT (G)
                                                                            +(b − b̂)0 m̄0NT γ̃ N T (G) + (b − b̂)0 (m̄ − m)0 γ̃(G)
and by repeated applications of Cauchy-Schwarz and Eh exp[xT |||ĥ−h||] = Op (1) for all xT = Op (1),
                         0      p       0                p                     p                          p
it suﬃces to show that ξ̄ Λ2 ξ̄ → 0, ξ̄ T N Λ2T N ξ̄ T N → 0, m̄0NT Λ2T N m̄NT → 0, and (m̄−m)0 Λ2 (m̄−m) →
0, where ΛT N = diag((N + 1)−1 , · · · , T −1 ) ⊗ Ik2 . These claims follow from Markov’s inequality
after noting that Ey |ξ̄(j)|2 → 0 and Ey |m̄j − mj |2 → 0 for all j, and supj≥N |mj | → 0.
                                                                                     p
    (xv) Eh exp[K ŝ0 m̂(b − b̂)] ≤ exp[ 12 K 2 CT ŝ0 m̂m̂0 ŝ] → 1 by Lemma 8 (ii).
    (xvi) Immediate from dominated convergence.
Lemma 4 (i) tr Σ = O(1).
  (ii) ||Λ−1 ΣΛ−1 || = O(1).

                                                                        30
    Proof. (i) Let kG,l (r, s) be the lth diagonal element of kG (r, s). Note that kG,l (r, s) is a non-
negative definite kernel, so that by by Mercer’s Theorem, there  P exist orthonormal ∗functions ϕl,j ∈
L2 [−π, π] and nonnegative numbers ν l,j such that kG,l (r, s) = ∞     ν ϕ (r)ϕl,j (s) (where ν l,j are
                                                                    R π l,j l,j
                                                                   j=1                 P
the eigenvalues and ϕl,j are the eigenfunctions of kG,l (r, s)), and −π kG,l (s, s)ds = ∞j=1 ν√ l,j . Note
     Rπ                        Rπ
that −π ϕl,j (s)ds = 0, since −π G(s)ds = 0 a.s.. Since the functions {φj (ω)}j=0 = {e / 2π}∞
                                                                                   ∞      ijω
                                                                                                       j=0
form a basis of L2 [−π, π], we have

                 2π tr Σ ≤ 2π lim tr Σ
                                         N→∞
                                    ∞ Z π
                                 k2 X
                                 X                  Z   π
                           =                                φj (s)kG,l (s, r)φj (r)dsdr
                                 l=1 j=1       −π     −π

                                   ∞ Z
                                 k X 2              Z                 Ã∞                          !
                                 X              π       π              X
                           =                                φj (s)             ν l,i ϕl,i (r)ϕl,i (s) φj (r)dsdr
                                 l=1 j=1       −π     −π                 i=1
                                 k X
                                 X
                                     2
                                   ∞ X
                                     ∞                     Z    π
                           =                        ν l,i (         φj (s)ϕl,i (s)ds)2
                                 l=1 j=1 i=1                −π

                                 k X
                                 X ∞ 2                      Z   π
                           =               ν l,i = tr               kG (s, s)ds.
                                 l=1 i=1                       −π


    (ii) The j, lth k × k block of Λ−1 ΣΛ−1 is equal to
                                       Z πZ π
                                    jl
                                               eirj e−isl kG (r, s)drds.
                                   4π 2 −π −π

Let k1 (r, s) = ∂kG (r, s)/∂r. By integration by parts,
                             Z π                    Z π
                                  irj
                           j     e kG (r, s)dr = −i     eirj k1 (r, s)dr =: gj (s)
                                −π                                    −π

where
                                         µZ s                      Z π               ¶
                      dgj (s)         d          irj                     irj
                                 = −i           e k1 (r, s)dr +        e k1 (r, s)dr
                        ds            ds    −π                      s
                                      Z π
                                 = −i     eirj k2 (r, s)dr − ieisj k∆ (s).
                                               −π

Thus, by another application of integration by parts,
             Z π                    Z π
                  −isl                         dgj (s)
           l     e gj (s)ds = i          e−isl         ds
              −π                                 ds
                                   Z π−πZ π                     Z                             π
                                              irj −isl
                              =             e e k2 (r, s)drds +                                   eisj e−isl k∆ (s)ds.
                                           −π        −π                                      −π




                                                                    31
Now let v = (v10 , · · · , vN
                            0 )0 be such that v 0 v = 1, where v is k 2 × 1. Then
                                                                j

                              N      µZ π Z π                                 Z π                    ¶
      0   −1    −1        1 X 0                     irj −isl                        isj −isl
  2πv Λ        ΣΛ    v =          vj              e e k2 (r, s)drds +              e e k∆ (s)ds vl
                         2π             −π −π                                  −π
                            j,l=1
                                   ⎛                     ⎞0          ⎛                     ⎞
                         Z πZ π            X N                                XN
                       =           ⎝ √1          vj eirj ⎠ k2 (r, s) ⎝ √
                                                                          1
                                                                                  vj e−isj ⎠ drds
                          −π −π        2π j=1                             2π j=1
                                                         ⎛                    ⎞0        ⎛                    ⎞
                                                   Z π            XN                            XN
                                                +        ⎝ √1         vj eisj ⎠ k∆ (s) ⎝ √
                                                                                             1
                                                                                                    vj e−isj ⎠ ds
                                                      −π     2π j=1                          2π j=1
                         Z πZ π                                  Z π
                       =           ϕ(r)∗ k2 (r, s)ϕ(s)drds +          ϕ(s)∗ k∆ (s)ϕ(s)ds
                               −π     −π                                  −π
                      PN
with ϕ(s) =     √1               −isj ,
                 2π      j=1 vj e          and the result follows.

                P                                                  P                              ˜ l m̂(b − b̂).
Lemma 5 (i) l=1 ((A(b̂)l−1 ỸT )0 ⊗ J10 A(b̂) −l J1 )(b − b̂) = l=1 J10 A(b̂) −l J1 P (R̃ê ⊗ I)0 ∆
                   0
   (ii)P (R̃ê ⊗ I) ∆˜ l (ŝ − Ŝ + M̂ (m̂ M̂ ) m̂ Ŝ) = P (R̃ê ⊗ Ik ) ∆
                                          0    −1 0                    0 ˜ l [(IN ⊗ P P̂
                                                                                     0   −10    −1
                                                                                             ⊗ P P̂ )][Σ(D̂ +
           ĉ     ĉ                                               ĉ                        b̂
Σ)−1b̂
    s + D̂M (m̂0 M )−1 m̂0 Ŝ] + op (1) for l = 1, · · · , , where M = (D̂ + Σ)−1 m̂ and S = (D̂ + Σ)−1b̂     s.

    Proof. (i) Define μ̂j = (P 0 Ψ̂0j−1 , P 0 Ψ̂0j−2 , · · · , P 0 Ψ̂0j−p ), and μ̂ the N k × pk matrix with block
                                                                                              P
rows equal to μ̂j , j = 1, · · · , N . Note that m̂ = μ̂ ⊗ P −1 . From yT −l = N−1               t=0 Ψ̂t (P êT −t−l +
T −1/2        0      N
       â) + J1 A(b̂) YT −N −l , we obtain
                                            ⎛     PN                      ⎞
                                                PN+1t=0 Ψ̂t P êT −t
                                          ⎜                               ⎟
                                          ⎜       t=0 Ψ̂t−1 P êT −t      ⎟    ¯ 1 μ̂)0 R̃ê
                                    ỸT = ⎜              ..               ⎟ = (∆
                                          ⎝               .               ⎠
                                                PN+p
                                                 t=0 Ψ̂t−p+1 P êT −t

so that A(b̂)l−1 ỸT = (∆                      ¯ l is the N k × N k matrix such that ∆
                          ¯ l μ̂)0 R̃ê, where ∆                                          ¯ l μ̂ has tth block row
            0  0        0   0                0
equal to (P Ψ̂t+l−2 , P Ψ̂t+l−3 , · · · , P Ψ̂t+l−p−1 ), t = 1, · · · , T . Now for l = 1, · · · , , using (21),

                                      ((A(b̂)l−1 ỸT )0 ⊗ J10 A(b̂) −l J1 )(b − b̂)
                                    = (((∆¯ l μ̂)0 R̃ê)0 ⊗ J10 A(b̂) −l J1 )(b − b̂)
                                    = J10 A(b̂) −l J1 P P −1 (b − b̂)(∆   ¯ l μ̂)0 R̃ê
                                    = J10 A(b̂)   −l      ¯ l μ̂)0 R̃ê ⊗ P −10 )0 vec(b − b̂)
                                                   J1 P ((∆
                                                          ˜ l m̂)0 (R̃ê ⊗ I)]0 (b − b̂)
                                    = J10 A(b̂) −l J1 P [(∆
                                    = J10 A(b̂) −l J1 P (R̃ê ⊗ I)0 ∆ ˜ l m̂(b − b̂).

                                               b̂
   (ii) Note that ŝ− Ŝ = Σ(I +Σ)−1 ŝ, ŝ− D̂S = Σ(D̂+Σ)−1b̂
                                                            s and (IN ⊗P ⊗P )ŝ =P(IN ⊗ P̂ ⊗ P̂ )b̂
                                                                                                 s, so
that b̂
     s = (IN ⊗ P̂ P ⊗ P̂ P )ŝ. Furthermore, ||D̂ −I|| ≤ |D̂ − I|, and |D̂ − I| = N
                 −1       −1                                                   2
                                                                                   i=1 |dˆj − Ik2 |2 =


                                                               32
PN        ˆ − Ik2 )0 vec(dˆj − Ik2 ). By standard arguments, maxj E|dˆj − Ik2 |2 = O(T −1 ), so that
  i=1 vec(dj
E[|D̂ − I|2 ]= O(N/T ) = o(1), and thus ||D̂ − I|| = op (1). Therefore, also ||D̂|| = Op (1) and
    −1                        p
||D̂ || ≤ 1/(1 − ||D̂ − I||) → 1.
    Further, note that

                                                            ||Λŝ|| = Op (1)                                 (32)
                                                                   2
                                                      ||ΛR̃ê||        = Op (1)                              (33)
                                        −1            −1      −1
                                     ||Λ     Σ(I + Σ)       Λ      || = O(1)                                 (34)
                                                         −1
                                                      ||Λ       m̂|| = Op (1)                                (35)
                                            0       −1        −1
                                      ||(m̂ (I + Σ)      m̂)       || = Op (1)                               (36)

where (32) follows from ||Λŝ||2 = ŝ0 Λ2 ŝ = Op (1) by Lemma 8 (i); with ê = e−T −1/2 X ĥ, (33) follows
from (a + b)0 A(a + b) ≤ a0 Aa + b0 Ab + 2[(a0 Aa)(b0 Ab)]1/2 , Ey e0 R̃0 Λ2 R̃0 e = tr[R̃(Ey ee0 )R̃0 Λ2 ] = O(1)
and T −1 ĥXΛ2 X ĥ ≤ ||ĥ||2 T −1 ||X||2 = Op (1); (34) follows from ||Λ−1 (IN − (IN + Σ)−1 )Λ−1 || ≤
||Λ−1 ΣΛ−1 || = O(1) by Lemma 4 (ii), since for any eigenvalue λi of Σ, the corresponding eigenvalue
of IN − (IN + Σ)−1 is given by λi /(1 + λi ) ≤ λi ; (35) follows, since with probability converging
to one, θ̂ is such that ||Ψ̂j || decays exponentially fast in j; (36) follows from v0 m̂0 (I + Σ)−1 m̂v ≥
v 0 m̂0 m̂v/(1 + ||Σ||) ≥ ||(m̂0 m̂)−1 ||−1 /(1 + ||Σ||) for any any pk2 × 1 vector v with ||v|| = 1, and
                   p                                 p
||(m̂0 m̂)−1 ||−1 → ||(m0 m)−1 ||−1 > 0 via m̂0 m̂ → m0 m.
      Moreover, also

                Σ(D̂ + Σ)−1 − Σ(I + Σ)−1 = (I + Σ)−1 − D̂(D̂ + Σ)−1
                                                = (I + Σ)−1 [D̂ + Σ − (I + Σ)D̂](D̂ + Σ)−1
                                                = (I + Σ)−1 Σ(I − D̂)(D̂ + Σ)−1

so that

          Λ−1 [Σ(D̂ + Σ)−1 − Σ(I + Σ)−1 ]Λ−1 = [Λ−1 (I + Σ)−1 ΣΛ−1 ][I − D̂][Λ(D̂ + Σ)−1 Λ−1 ]

and

                      Λ(D̂ + Σ)−1 Λ−1 = Λ(D̂ + Σ)−1 D̂Λ−1 D̂−1
                                             = D̂−1 − [Λ(D̂ + Σ)−1 Λ][Λ−1 ΣΛ−1 ]D̂−1
                                        p                                   p
so that by ||(D̂ + Σ)−1 || ≤ ||D̂−1 || → 1, ||Λ|| = O(1), ||I − D̂|| → 0, ||D̂−1 || = Op (1) and by Lemma
4 (ii),

                                                   ||Λ(D̂ + Σ)−1 Λ−1 || = Op (1)
                           ||Λ−1 (Σ(D̂ + Σ)−1 − Σ(I + Σ)−1 )Λ−1 || = op (1).
                                                              p                   p
    The result now follows from (22), ||P P̂ −1 −Ik || → 0, ||P −1 P̂ −Ik || → 0 and repeated applications
of the norm inequality.

Lemma 6 (i) Eh EG ||f˜T (G, h)||2 = Op (1).
                                               p
  (ii) Eh EG ST (G)||fT (G, h) − f˜T (G, h)||2 → 0.

                                                         33
   Proof. By definition and Lemma 5 (i)

                                                                    X
 f˜T (G, h) = T 1/2 J10 (A(b̂) − A(0) )YT +                                                 ˜ l γ̃(G))] ⊗ J1 )0 vec(A(b̂)
                                                                     (J1 [a + P (R̃e ⊗ I)0 (∆                                                     −l
                                                                                                                                                       )
                                                                    l=1
                            X
                        +    ((A(b̂)l−1 ỸT )0 ⊗ J10 A(b̂)              −l
                                                                             J1 )(b − b̂)
                            l=1
                                                                    X
              = T 1/2 J10 (A(b̂) − A(0) )YT +                                               ˜ l γ̃(G))] ⊗ J1 )0 vec(A(b̂)
                                                                     (J1 [a + P (R̃e ⊗ I)0 (∆                                                     −l
                                                                                                                                                       )
                                                                    l=1
                            X
                        +         J10 A(b̂)     −l
                                                     J1 (b − b̂)A(b̂)l−1 ỸT
                            l=1
                                                                    X
              = T 1/2 J10 (A(b̂) − A(0) )YT +                                J10 A(b̂)       −l                         ˜ l [γ̃(G) + m̂(b − b̂)]].
                                                                                                  J1 [a + P (R̃ê ⊗ I)0 ∆
                                                                    l=1

By Minkowki’s inequality, it suﬃces to consider each summand in f˜T (G, h) and ST (G)fT (G, h) −
ST (G)f˜T (G, h) separately.
    Note that for any positive integer q, by a direct calculation, (cf. Lütkepohl (2005), page 96)
                                                                  q−1
                                               ∂ vec A(b)q X
                                                          = (A(b)0 )q−1−i ⊗ (A(b)i J1 ),
                                                   ∂b0
                                                                  i=0

                                  2 p2
and with gA : R 7→ Rk                        defined as gA (λ) = vec(A(b̂ + λ(b − b̂))q ) for some positive integer q, we
have

                     vec(A(b)q ) − vec(A(b̂)q )
                     Z 1
                         gA (λ)
                   =            dλ
                      0    dλ
                            Z 1Xq−1
                       −1/2
                   = T              (A(b̂ + λ(b − b̂))0 )q−1−i ⊗ A(b̂ + λ(b − b̂))i J1 )dλ · (b − b̂).                                                 (37)
                                         0    i=0

   (i) Let Ā0 = sup0≤λ≤1 ||A(λb̂)|| = Op (1). Then
                                                                                             Z     −1
                                                                                                  1X
          ||T 1/2 J10 (A(b̂)        − A(0) )YT ||          2
                                                               = ||(YT ⊗ J1 )            0
                                                                                                        (A(λb̂)0 )      −1−i
                                                                                                                               ⊗ A(λb̂)i J1 )dλ · b̂||2
                                                                                              0     i=0
                                                                                                      −1
                                                                                                      X
                                                               ≤ ||YT ||2 ||J1 ||4 ||b̂||           2
                                                                                                            Ā20   −2
                                                                                                                        = Op (1)
                                                                                                      i=0
              Eh ||J10 A(b̂)        −l
                                             J1 (a − â)||2 ≤ Ā02       −2l
                                                                               ||J1 ||4 Eh ||a − â||4
  Eh ||J10 A(b̂)   −l
                        J1 (b − b̂)A(b̂)l−1 YT ||2 ≤ Ā02                −2l
                                                                               ||J1 ||4 ||YT ||2 Eh ||b − b̂||2 = Op (1)


                                                                             34
Furthermore, ||J10 A(b̂)   −l J P ||
                               1       ≤ ||J1 ||2 ||P ||Ā0−l = Op (1), ê = e − T −1/2 X ĥ, and

        EG T −1 ||(R̃X b̂ ⊗ I)0 ∆ ˜ l γ̃(G)||2 ≤ ||T −1/2 X||2 ||b̂||2 ||∆  ˜ l ||2 EG ||γ̃(G)||2 = Op (1)
            Ey EG ||(R̃e ⊗ I)0 ∆  ˜ l γ̃(G)||2 ≤ ||∆
                                                   ˜ l ||2 EG ||γ̃(G)||2 = O(1).                                     (38)
                             ˜ l m̂(b − b̂)||2 ≤ ||T −1/2 X||2 ||b̂||2 Eh ||b − b̂||2 ||∆
     Eh T −1 ||(R̃X b̂ ⊗ I)0 ∆                                                              ˜ l ||2 ||m̂||2 = Op (1)
        Ey Eh ||(R̃e ⊗ I)0 ∆ ˜ l m̂(b − b̂)||2 ≤ Eh ||b − b̂||2 ||∆˜ l ||2 ||Λ−1 m̂||2 Ey ||Λ(R̃e ⊗ I)|| = O(1).

(ii) Define the N k × (T + )k matrix ∆0l such that ∆0l γ(G) = vec(γ l (G), · · · , γ N (G), 0, · · · , 0). By
                                                              p
Minkowski, it suﬃces to show that Eh EG ST (G)||ξ T (G, h)||2 → 0 for ξ T (G, h) equal to any of

                              J10 [A(b)   −l
                                               − A(b̂)   −l
                                                       ]J1 a,
                              J1 A(b) J1 P (((RV (G)R)−1 − I)Re ⊗ I)0 (∆l γ(G)),
                                0       −l

                              J10 A(b) −l J1 P (e ⊗ I)0 [(R̃ ⊗ I)0 ∆0l − (R ⊗ I)0 ∆l ]γ(G),
                              J10 A(b) −l J1 P (R̃e ⊗ I)0 [∆0l γ(G) − ∆    ˜ l γ̃(G)],
                              J10 [A(b) −l − A(b̂) −l ]J1 P (R̃e ⊗ I)0 (∆   ˜ l γ̃(G)),
                              J10 A(b̂) −l J1 P (R̃(e − ê) ⊗ I)0 (∆˜ l γ̃(G)),
                              J10 A(b̂) −l J1 (b − b̂)A(b̂)l−1 (ỸT − YT ),

for l = 1, · · · , , as well as for ξ T (G, h) equal to

                                                                   X
         T 1/2 (YT ⊗ J1 )0 (vec(A(b) ) − vec(A(b̂) )) −             ((A(b̂)l−1 YT )0 ⊗ J10 A(b̂)   −l
                                                                                                        J1 )(b − b̂).   (39)
                                                                   l=1

    Let Ā1,q = Eh [||A(b)||q ], and Ā2,q = supλ∈[0,1] Eh [||A(b̂ + λ(b − b̂))||q ], which are Op (1) for any
fixed q, since ||A|| ≤ |A|, and multivariate normal distributions posses all moments. By Cauchy-
Schwarz
                 Eh [||J10 [A(b)q − A(b̂)q ]J1 a||2 ]2 ≤ ||J1 ||4 Eh [||A(b)q − A(b̂)q ||4 ]Eh [||a||4 ]
where clearly Eh [||a||4 ] = Op (1), and using (23), (37), Minkowski and Cauchy-Schwarz, we find

Eh ||A(b)q − A(b̂)q ||4 ≤ Eh || vec(A(b)q − A(b̂)q )||4                                                    (40)
                                     Z 1X
                                        q−1
                        ≤ T −1 Eh ||        (A(b̂ + λ(b − b̂))0 )q−1−i ⊗ A(b̂ + λ(b − b̂))i J1 )dλ||4 ||b − b̂||4
                                               0   i=0
                                                                    q−1
                                                                    X                                p
                           ≤ T −1 ||J1 ||4 (Eh ||b − b̂||8 )1/2         (Ā2,16q−16−16i Ā2,16i )1/4 → 0.
                                                                    i=0

Furthermore,

                 ST (G)Ey (∆l γ(G))0 [(RV (G)R)−1 − I)Ree0 R(RV (G)R)−1 − I) ⊗ I](∆l γ(G))
            = ST (G)(∆l γ(G))0 [(RV (G)R)−1 − I)(RV (G)R)−1 − I) ⊗ I](∆l γ(G))
            = ST (G)(∆l γ(G))0 [R(V (G)−1 − I)2 R ⊗ I](∆l γ(G))

                                                              35
                = ST (G)T −1 (∆l γ(G))0 [RΓ(G)V (G)−2 Γ(G)0 R ⊗ I](∆l γ(G))
                ≤ ST (G)T −1 ||Γ(G)||2 ||V (G)−2 || · ||γ(G)||2 → 0

uniformly in G by Lemma 2 (iv), (v) and (vii). Also

                 EG ST (G)Ey γ(G)[(R̃ ⊗ I)0 ∆0l − (R ⊗ I)0 ∆l ](ee0 ⊗ I)[(R̃ ⊗ I)0 ∆0l − (R ⊗ I)0 ∆l ]γ(G)
                                     T
                                     X
           ≤ EG ST (G)                    |γ j (G)|2 → 0
                                  j=N+1

by Lemma 2 (viii). Also
                           ˜ l γ̃(G)]0 (R̃eeR̃0 ⊗ I)0 [∆0 γ(G) − ∆
      ST (G)Ey [∆0l γ(G) − ∆                                     ˜ l γ̃(G)] ≤ ST (G)||γ(G) − γ̃(G)|| → 0
                                                        l

uniformly in G by Lemma 2 (vi), and

                            Eh EG ST (G)||J10 [A(b)          −l
                                                       − A(b̂) −l ]J1 P (R̃e ⊗ I)0 (∆  ˜ l γ̃(G))||2
                                                                                                          p
                                                          ˜ l γ̃(G)||2 ) · (Eh ||A(b) −l − A(b̂) −l ||2 ) →
                      ≤ ||J1 ||4 ||P ||2 (EG ||(R̃e ⊗ I)0 ∆                                                 0
                               ˜ l γ̃(G)||2 = Op (1) as above. Furthermore,
since by (38), EG ||(R̃e ⊗ I)0 ∆

   EG Eh ||J10 [A(b)       −l
                                − A(b̂)   −l                     ˜ l γ̃(G))||2 ≤
                                               ]J1 P (R̃e ⊗ I)0 (∆
                                                                                                                      ˜ l γ̃(G))||2 → 0 p
                                                       ||J1 ||3 Eh ||A(b)      −l
                                                                                    − A(b̂)   −l 2
                                                                                                || EG ||P (R̃e ⊗ I)0 (∆
                              p
                                                                                  ˜ l γ̃(G)||2 = Op (1).
since Eh ||A(b)q − A(b̂)q ||2 → 0 from (40) and Jensen, and again EG ||(R̃e ⊗ I)0 ∆
    Furthermore,

 EG ||J10 A(b̂)     −l                           ˜ l γ̃(G))||2 ≤ ||J10 A(b̂)
                         J1 P (R̃(e − ê) ⊗ I)0 (∆                                      −l                                     ˜ l γ̃(G))||2
                                                                                             J1 P ||2 T −1 EG ||(R̃X ĥ ⊗ I)0 (∆

and
                                         ˜ l γ̃(G))||2 ≤ tr(R̃X ĥĥ0 X 0 R̃0 ⊗ I)∆
                     EG ||(R̃X ĥ ⊗ I)0 (∆                                        ˜ l Σ∆ ˜0
                                                                                          l
                                                       ≤ ||ĥ||2 tr(R̃XX 0 R̃0 ⊗ I)∆ ˜ l Σ∆ ˜ 0 = Op (1)
                                                                                              l

since, using Lemma 4 (ii),
                                     ˜ l Σ∆
                Ey tr(R̃XX 0 R̃0 ⊗ I)∆    ˜ 0 ≤ ||Λ−1 Σ∆
                                                       ˜ 0 Λ−1 || tr Λ(R̃Ey [XX 0 ]R̃0 ⊗ I)Λ = O(1).
                                            l            l

Moreover
                                                                                                                                  p
          Eh ||J10 A(b̂)     −l
                                  J1 (b − b̂)A(b̂)l−1 (ỸT − YT )||2 ≤ Ā20           −2
                                                                                           ||J1 ||4 ||YT − ỸT ||2 Eh ||b − b̂||2 → 0
                    p        P                                          p                                p
since ||YT − ỸT || → 0 from ∞ t=0 ||Ψ̂l+t−1 || = Op (1), T
                                                            −1/2 ||â|| → 0 and ||A(b̂)N || · ||YT −N || → 0.
    Finally, using (37), the spectral norm of (39) is bounded above by
                                     Z   1X
||YT || · ||J1 ||2 ||b − b̂|| · ||             [(A(b̂ + λ0 (b − b̂))0 )   −l
                                                                               ⊗ A(b̂ + λ0 (b − b̂))l−1 − (A(b̂)0 )    −l
                                                                                                                            ⊗ A(b̂)l−1 ]dλ0 ||.
                                     0   l=1


                                                                          36
For any fixed integer q and λ0 ∈ [0, 1], from (23) and (37),
                                            Z    q−1
                                                1X
||A(b̂+λ0 (b−b̂))q −A(b̂)q || ≤ T −1/2 ||              (A(b̂+λ0 λ(b−b̂))0 )q−1−i ⊗A(b̂+λ0 λ(b−b̂))i J1 )dλ||·||b−b̂||
                                            0    i=0

                                                                                                                   p
so that from the same argument as employed for (40), Eh supλ0 ∈[0,1] ||A(b̂+λ0 (b−b̂))q −A(b̂)q ||K → 0
for any integer K. But for conformable matrices A, B, C, D, ||(A ⊗ B) − (C ⊗ D)|| ≤ ||A − C|| ·
||B|| + ||A|| · ||B − D||, so that this result, along with Minkowski and Cauchy-Schwarz, establishes
                             p
that also Eh ||ξ T (G, h)||2 → 0 with ξ T (G, h) equal to (39).

Lemma 7 (i) EG exp[K ŝ0 γ̃(G)] = Op (1) for any fixed K ∈ R.
   (ii) EG L̃T (G, h)4 ≤ ζ̃ T exp[− 12 4CT ||h − ĥ||2 ] uniformly in h, where CT ,CT−1 and ζ̃ T are Op (1)
and do not depend on h.

   Proof. (i) EG exp[K ŝ0 γ̃(G)] = exp[K 2 ŝ0 Σŝ] ≤ exp[K 2 ||Λ−1 ΣΛ−1 || · ŝ0 Λ2 ŝ], so the result follows
from Lemmata 4 (ii) and 8 (i).
   (ii) By Cauchy-Schwarz

                                    EG L̃T (G, h)4 ≤ (EG exp[8ŝ0 γ̃(G)])1/2
           ·(EG exp[−8 12 γ̃(G)0 γ̃(G) − 8(h − ĥ)0 m̂0 γ̃(G) − 12 8(h − ĥ)0 m̂0 m̂(h − ĥ) + 12 8ĥ0 m̂0 m̂ĥ])1/2

and a direct calculation

                           EG exp[−8(h − ĥ)0 m̂0 γ̃(G) − 8 12 γ̃(G)0 γ̃(G)]
                      = det(I + 8Σ)−1/2 exp[ 12 8(h − ĥ)0 m̂0 (I + (8Σ)−1 )−1 m̂(h − ĥ)]
                      ≤ exp[ 12 8(h − ĥ)0 m̂0 (I + (8Σ)−1 )−1 m̂(h − ĥ)].

Since I − (I + (8Σ)−1 )−1 = (I + 8Σ)−1 , we obtain

             EG exp[−8 12 γ̃(G)0 γ̃(G) − 8(h − ĥ)0 m̂0 γ̃(G) − 12 8(h − ĥ)0 m̂0 m̂(h − ĥ) + 12 8ĥ0 m̂0 m̂ĥ]
         ≤ exp[− 12 8(h − ĥ)0 m̂0 (I + 8Σ)−1 m̂(h − ĥ)] ≤ exp[− 12 4CT ||h − ĥ||2 ]

where CT is the smallest eigenvalue of 2m̂0 (I + 8Σ)−1 m̂, and

                                                                           2||(m̂0 m̂)−1 ||−1
                           CT   = 2||(m̂0 (I + 8Σ)−1 m̂)−1 ||−1 ≥
                                                                              1 + 8||Σ||
                           CT   ≤ 2||m̂||2 .
                       p
From m̂0 m̂ − m0 m → 0 and (26), we have CT = Op (1) and CT−1 = Op (1). Also, by part (i),
EG exp[8ŝ0 γ̃(G)] = Op (1). Thus

                                  EG L̃T (G, h)4 ≤ ζ̃ T exp[− 12 4CT ||h − ĥ||2 ]

where CT and ζ̃ T = Op (1).

                                                            37
Lemma 8 (i) ŝ0 Λ2 ŝ = Op (1).
              p
  (ii) m̂0 ŝ → 0.

    Proof. (i) Define s just as ŝ, but with êt replaced by et . Then Ey s0 Λ2 s = tr[(Ey ss0 )Λ2 ] = O(1),
because the elements of Ey ss0 are uniformly bounded. Furthermore, recall that êt = et − T −1/2 Xt0 ĥ,
where Xt0 = (1, yt−1  0 , · · · , y 0 ) ⊗ P −1 , so that the jth k 2 × 1 block of s − ŝ is given by
                                   t−p
                P
    T −1 vec t [et ĥ0 Xt+j + Xt0 ĥet+j + T −1/2 Xt0 ĥĥ0 Xt+j ] =
                                          P
                                     T −1 t [(Xt+j     0   ⊗ et )ĥ + (e0t+j ⊗ Xt )ĥ + T −1/2 (Xt+j0    ⊗ Xt0 ) vec(ĥĥ0 )].
                                                                                                                              (41)
Thus, letting dXe , deX and dXX be N k 2P                × (p + 1)k2 , N k 2 ×P(p + 1)k2 and Nk 2 × (p +P        1)2 k4 matrices
with jth k 2 block of rows equal to T −1 (Xt+j                0   ⊗ et ), T −1 (e0t+j ⊗ Xt ) and T −3/2 (Xt+j          0  ⊗ Xt0 ),
respectively, we have ŝ = s+dXe ĥ+deX ĥ+dXX vec(ĥĥ0 ). It thus suﬃces to show s0 Λ2 s = tr ss0 Λ2 =
Op (1), ĥ0 d0Xe Λ2 dXe ĥ ≤ ||ĥ||2 tr dXe d0Xe Λ2 = Op (1), ĥ0 d0eX Λ2 deX ĥ ≤ ||ĥ|| tr deX d0eX Λ2 = Op (1) and
vec(ĥĥ0 )0 d0XX Λ2 dXX vec(ĥĥ0 ) ≤ || vec(ĥĥ0 )||2 tr dXX d0XX Λ2 = Op (1). These follow from Markov’s
inequality, since the diagonal elements of Ey ss0 , Ey dXe d0Xe , Ey deX d0eX and Ey dXX d0XX are readily
seen to be uniformly bounded, and tr Λ2 = O(1).
    (ii) Note that with ût defined as ût = 0 for t ≤ 0,
                         ⎛                                    ⎞          ⎛                  P                            ⎞
                      N          vec(P −10 ŝ0j P 0 Ψ̂0j )            N      vec(Ω−1 T −1/2 Tt=1 ût û0t−j Ψ̂0j−1 )
                    X⎜                      ..                ⎟ X⎜                             ..                        ⎟
       m̂0 ŝ =          ⎝                   .                ⎠=         ⎝                      .                        ⎠
                    j=1               −10 0    0   0                 j=1           −1  −1/2
                                                                                            PT              0     0
                             vec(P ŝj P Ψ̂j−p+1 )                           vec(Ω T              t=1 ût ût−j Ψ̂j−p )
                    ⎛                        P   T        P  N                 ⎞
                        vec(Ω−1 T −1/2 t=1 ût j=1 û0t−j Ψ̂0j−1 )
                    ⎜                               ..                         ⎟
                = ⎝                                  .                         ⎠.
                                 −1   −1/2
                                             PT           PN       0     0
                        vec(Ω T                  t=1 ût     j=1 ût−j Ψ̂j−p )

Furthermore, with Yt−1 = A(b̂)−1 (Yt − T −1/2 (1, · · · , 1)0 ⊗ â) for t = 0, −1, −2, . . .,
                                       N
                                       X                             N
                                                                     X −l
                           yt−l =            Ψ̂j−l ût−j + T −1/2           Ψ̂j â + J10 A(b̂)N−l+1 Yt−N+l−1
                                       j=1                           j=0

we have
             T
             X           N
                         X                              T
                                                        X                          N−l
                                                                                   X
    T −1/2         ût         û0t−j Ψ̂0j−l = T −1/2         ût (yt−l − T −1/2         Ψ̂j â − J10 A(b̂)N−l+1 Yt−N +l−1 )0   (42)
             t=1         j=1                            t=1                        j=0
      P                     P                                                                        p
Now Tt=1 ût yt−l0 = 0 and Tt=1 ût = 0 by the OLS first order condition, and T −1/2 ||A(b̂)N−l+1 || →
0, since with probability converging to one, ||A(b̂)|| < 1 − εA /2 < 1, where εA = 1 − ||A(0)||. Thus
(42) is op (1), and the result follows.

Lemma 9
                                               R
                                   p         EG bL̃T (G, h)dh
                                 b     =       R              = (m̂0 M̂ )−1 m̂0 Ŝ
                                             EG L̃T (G, h)dh

                                                                     38
                                     R
                         p        EG γ̃(G)L̃T (G, h)dh
                     γ       =          R                = ŝ − Ŝ + (M̂ − m̂)bp
                                    EG L̃T (G, h)dh
                                     R
                                  EG aL̃T (G, h)dh
                     ap =             R             = â
                                  EG L̃T (G, h)dh
         R
      EG L̃T (G, h)dh
       R                     = det(I + Σ)−1/2 det(m̂0 M̂ )−1/2 exp[− 12 ŝ0 Ŝ + 12 Ŝ 0 m̂(m̂0 M̂ )−1 m̂0 Ŝ]
         L̃T (0, h)dh

where Ŝ = (I + Σ)−1 ŝ and M̂ = (I + Σ)−1 m̂.

   Proof. Follows from "completing the squares", as ln L̃T (G, h) is quadratic in h and γ̃(G).




                                                        39
References
Brockwell, P. J., and R. A. Davis (1991): Time Series: Theory and Methods. Springer,
  New York, second edn.

Carter, C. K., and R. Kohn (1997): “Semiparametric Bayesian Inference for Time Series
  with Mixed Spectra,” Journal of the Royal Statistical Society Series B, 59, 255—268.

Chernoff, H., and S. Zacks (1964): “Estimating the Current Mean of a Normal Dis-
  tribution Which is Subject to Changes in Time,” Annals of Mathematical Statistics, 35,
  999—1028.

Choudhuri, N., S. Ghosal, and A. Roy (2004a): “Bayesian Estimation of the Spectral
  Density of a Time Series,” Journal of the American Statistical Association, 99, 1050—1059.

         (2004b): “Contiguity of the Whittle Measure for a Gaussian Time Series,” Bio-
  metrika, 91, 211—218.

Clements, M. P., and D. F. Hendry (1998): Forecasting Economic Time Series. Cam-
  bridge University Press, Cambridge, UK.

Davies, R. B. (1973): “Asymptotic Inference in Stationary Gaussian Time-Series,” Ad-
  vances in Applied Probability, 5, 469—497.

Doan, T., R. Litterman, and C. Sims (1984): “Forecasting and Conditional Projection
  Using Realistic Prior Distributions,” Econometric Reviews, 3, 1—100.

Dzhaparidze, K. (1986): Parameter Estimation and Hypothesis Testing in Spectral Analy-
  sis of Stationary Time Series. Springer, New York.

Golubev, G. K., M. Nussbaum, and H. H. Zhou (2010): “Asymptotic Equivalence of
  Spectral Density Estimation and Gaussian White Noise,” The Annals of Statistics, 38,
  181—214.

Ing, C., and C. Wei (2005): “Order Selection for Same-Realization Predictions in Autore-
  gressive Processes,” The Annals of Statistics, 33, 2423—2474.



                                               40
Li, H., and U. K. Müller (2009): “Valid Inference in Partially Unstable General Method
  of Moment Models,” Review of Economic Studies, 76, 343—365.

Liseo, B., D. Marinucci, and L. Petrella (2001): “Bayesian Semiparametric Inference
  on Long-Range Dependence,” Biometrika, 88, 1089—1104.

Lütkepohl, H. (2005): New Introduction to Multiple Time Series Analysis. Springer,
  Berlin.

McCoy, E. J., and D. A. Stephens (2004): “Bayesian Time Series Analysis of Periodic
  Behaviour and Spectral Structure,” International Journal of Forecasting, 20, 713—730.

Müller, U. K. (2009): “Risk of Bayesian Inference in Misspecified Models, and the Sand-
  wich Covariance Matrix,” Working paper, Princeton University.

Müller, U. K., and P. Petalas (2010): “Eﬃcient Estimation of the Parameter Path in
  Unstable Time Series Models,” Review of Economic Studies, 77, 1508—1539.

Pesaran, M. H., D. Pettenuzzo, and A. Timmermann (2006): “Forecasting Time
  Series Subject to Multiple Structural Breaks,” Review of Economic Studies, 73, 1057—
  1084.

Rosen, O., D. Stoffer, and S. Wood (2009): “Local Spectral Analysis Via a Bayesian
  Mixture of Smoothing Splines,” Journal of the American Statistical Association, 104, 249—
  262.

Schorfheide, F. (2005): “VAR Forecasting under Misspecification,” Journal of Econo-
  metrics, 128, 99—136.

Shibata, R. (1980): “Asymptotically Eﬃcient Selection of the Order of the Model for
  Estimating Parameters of a Linear Process,” The Annals of Statistics, 8, 147—164.

Stock, J. H., and M. W. Watson (1996): “Evidence on Structural Instability in Macro-
  economic Time Series Relations,” Journal of Business and Economic Statistics, 14, 11—30.

            (2005): “Implications of Dynamic Factor Models for VAR Analysis,” manuscript,
  Harvard University.

                                             41
Whittle, P. (1957): “Curve and Periodogram Smoothing,” Journal of the Royal Statistical
  Society Series B, 19, 38—63.

         (1962): “Gaussian Estimation in Stationary Time Series,” Bulletin of the Interna-
  tional Statistical Institute, 39, 105—129.




                                               42
                                             Table 1
                 Distributions of Estimated Lag Differences, AIC – BIC:
     Empirical (132 monthly U.S. macroeconomic time series) and Asymptotic


       pˆ AIC – pˆ BIC   Empirical                       Asymptotic
                                                     p0=0                    p0=3
                                     AR(0)   c=10    c=20    c=40    c=10    c=20    c=40
            0             0.205      0.714   0.462   0.185   0.021   0.646   0.482   0.169
            1             0.098      0.114   0.196   0.175   0.043   0.132   0.157   0.116
            2             0.083      0.056   0.114   0.157   0.070   0.071   0.101   0.112
            3             0.098      0.034   0.071   0.122   0.089   0.044   0.070   0.103
            4             0.068      0.023   0.045   0.093   0.100   0.030   0.049   0.089
            5             0.030      0.016   0.031   0.069   0.100   0.021   0.035   0.079
            6             0.038      0.011   0.022   0.051   0.094   0.015   0.027   0.067
            7             0.045      0.008   0.016   0.037   0.086   0.011   0.020   0.057
            8             0.045      0.006   0.011   0.028   0.075   0.008   0.015   0.046
            9             0.030      0.005   0.008   0.021   0.064   0.006   0.012   0.039
           10             0.015      0.003   0.006   0.016   0.054   0.005   0.009   0.032
           11             0.053      0.003   0.005   0.012   0.046   0.004   0.008   0.026
           ≥12            0.189      0.008   0.014   0.035   0.157   0.008   0.016   0.065


Notes: Entries are the probability of observing the indicated difference between AIC and
BIC. The empirical distribution is computed from the data set described in Section 4
consisting of 132 U.S. monthly macroeconomic time series, 1959:1 – 2003:12, with the
regressions all run on a balanced panel with T = 510 observations, and 0 ≤ p ≤ pmax=18
for both AIC and BIC. The “Asymptotic AR(0)” reports the asymptotic distribution of
 pˆ AIC – pˆ BIC in model (1) with white noise ut, and the remaining columns report the
asymptotic distribution in model (1) and (2) with G distributed as c times a demeaned
Brownian motion.




                                                43
                            Table 2
            Mean Square Forecast Errors of
Approximate and Small Sample Exact Bayes Procedures

                            p=0    p=1      P=2    p=3    p=6
                           c=5, T=100
    Exact Bayes yT* +1|T     -1.9    -0.4   -0.1   -0.1   -0.2
      Approx. yTp+1|T       -1.8   -0.4     -0.1   -0.1    0.0
        Difference          -0.1    0.0     -0.1   -0.1   -0.2
                           c=5, T=200
    Exact Bayes yT* +1|T     -2.2    -0.3   -0.1   -0.1   -0.1
                  p
      Approx. y   T +1|T    -2.2   -0.3     -0.1   0.0    -0.1
        Difference           0.0    0.0      0.0   0.0    -0.1
                           c=5, T=400
    Exact Bayes yT* +1|T     -2.6    -0.3   -0.1   0.0    0.0
                  p
      Approx. y   T +1|T     -2.6   -0.3    -0.1   0.0    0.0
        Difference            0.0    0.0     0.0   0.0    0.0
                           c=10, T=100
    Exact Bayes yT* +1|T    -10.5    -3.3   -1.0   -0.5   -0.5
      Approx. yTp+1|T        -7.5   -3.0    -0.9   -0.4   -0.2
        Difference           -2.9   -0.3    -0.1   -0.1   -0.3
                           c=10, T=200
    Exact Bayes yT* +1|T    -12.1    -2.7   -0.9   -0.8   -0.3
      Approx. yTp+1|T       -10.8   -2.5    -0.9   -0.8   -0.2
        Difference           -1.3   -0.2     0.0    0.0   -0.1
                           c=10, T=400
    Exact Bayes yT* +1|T    -13.8    -2.8   -0.8   -0.4   -0.1
                  p
      Approx. y   T +1|T    -12.9   -2.8    -0.9   -0.4   -0.1
        Difference           -0.9    0.0     0.0    0.0    0.0
                           c=20, T=100
    Exact Bayes yT* +1|T    -44.2   -19.7   -8.6   -5.6   -1.6
                  p
      Approx. y   T +1|T     19.8   -9.2    -6.4   -4.2   -1.2
        Difference          -64.0 -10.5     -2.2   -1.3   -0.4
                           c=20, T=200
    Exact Bayes yT* +1|T    -50.0   -18.1   -9.6   -3.8   -0.8
      Approx. yTp+1|T       -18.4 -13.6     -8.7   -3.4   -0.7
        Difference          -31.6   -4.4    -1.0   -0.4   -0.1
                           c=20, T=400
    Exact Bayes yT* +1|T    -58.0   -18.2   -6.1   -5.5   -1.3
      Approx. yTp+1|T       -38.6   -15.6   -6.0   -5.3   -1.4
        Difference          -19.4    -2.6    0.0   -0.1    0.1




                               44
Notes to Table 2: Entries are Monte Carlo estimates of T(MSFEposterior mean –
MSFEAR)/MSFEAR, where MSFEAR is the one-step ahead mean square forecast error of
the AR(p) model estimated by OLS (where p is given in the column heading), and
MSFEposterior mean is the small sample exact ( yT* +1|T ) and approximate ( yTp+1|T ) posterior
mean of yT+1|T, respectively (computed using AR(p) residuals, for p given in the column
heading), along with the difference of these two numbers. For the exact Bayes posterior,
the prior on the AR(p) coefficients and the constant is N(0, Ip+1), independent of G. Based
on 1000 Monte Carlo draws, with analytical integration over yT+1 - yT* +1|T . The exact
Bayes procedure is implemented via importance sampling over G (discretely
approximated with 2,000 steps) using the exact (not Whittle) likelihood, with the prior as
proposal, using up to 1,000,000 draws.




                                               45
                                           Table 3
 Pseudo Out-of-Sample Mean Square Forecast Errors of Univariate Approximate
  Posterior Mean Forecasts, Relative to BIC, for 132 Monthly U.S. Macro Time
                            Series, 1959:1 – 2003:12

           A. Demeaned Brownian motion prior               B. Demeaned integrated
                                                            Brownian motion prior
  ℓ     c =10    c = 20    c = 30    BMA,    BMA,      c = 20 c = 30    BMA,      AIC
                                    0≤c≤20 0≤c≤40                      0≤c≤40
                                          Mean
 1      0.991     0.993    0.998     0.992   0.992      0.993    0.992     0.994    1.040
 3      0.987     0.986    0.990     0.987   0.986      0.990    0.988     0.990    1.033
 6      0.983     0.978    0.980     0.983   0.980      0.987    0.984     0.988    1.007
 12     0.986     0.981    0.983     0.985   0.983      0.989    0.985     0.989    1.007
                                         Median
 1      0.991     0.992    0.996     0.991   0.992      0.993    0.991     0.993    1.010
 3      0.992     0.991    0.994     0.993   0.993      0.994    0.994     0.995    1.011
 6      0.989     0.983    0.984     0.989   0.989      0.992    0.990     0.993    1.001
 12     0.991     0.988    0.991     0.992   0.991      0.992    0.990     0.993    1.002
                                     10% Percentile
 1      0.976     0.964    0.959     0.975   0.974      0.979    0.977     0.981    0.974
 3      0.963     0.944    0.940     0.965   0.959      0.969    0.959     0.970    0.943
 6      0.957     0.936    0.932     0.960   0.949      0.963    0.951     0.962    0.929
 12     0.957     0.935    0.930     0.955   0.946      0.963    0.957     0.966    0.935
                                     90% Percentile
 1      1.011     1.025    1.038     1.010   1.014      1.009    1.010     1.007    1.144
 3      1.009     1.021    1.036     1.007   1.011      1.006    1.009     1.005    1.160
 6      1.005     1.017    1.026     1.005   1.009      1.004    1.005     1.002    1.092
 12     1.006     1.019    1.033     1.007   1.012      1.004    1.005     1.003    1.127

Notes: Entries are the relative mean square forecast error of cumulative ℓ-step ahead
approximate posterior mean forecasts ( yTp+1|T + … + yTp+l|T ) for real series, and of average
inflation over the next ℓ months for nominal series, relative to the BIC forecast with the
same lag length The posterior mean forecast in Panel A was computed using the value of
c (or BMA-weighted average over c) given in the column heading, using the demeaned
Brownian motion prior; Panel B uses the prior corresponding to the sum of demeaned
Brownian motion of scale c plus a demeaned truncated Brownian motion with variation
below frequencies corresponding to cycles of 96 months with fixed scale c = 20. Lag
lengths were chosen by BIC, with 0 ≤ p ≤ 18. MSFEs were computed using recursive
forecasts, with the first forecast made when there were 198 observations on the
transformed variable (for real series,1975:7) and the final forecast made in 2003:12 – ℓ.




                                              46
                                   Figure 1
Monte Carlo estimates of T (MSFEposterior mean – MSFEBIC)/MSFEBIC for cumulative
                             forecasts, ℓ = 1,…, 4
Experiment 2: Estimator prior cWμ, DGP prior c0Wμ for c, c0 = 0,…, 20, T = 200
                          No estimated AR component
     Dashed lines: MSFEs for fixed-c estimators; Solid line: BMA forecast




                                      47
                                   Figure 2
Monte Carlo estimates of T(MSFEposterior mean – MSFEBIC)/MSFEBIC for cumulative
                             forecasts, ℓ = 1,…, 4
Experiment 2: Estimator prior cWμ, DGP prior c0Wμ for c, c0 = 0,…, 20, T = 200
                         AR(BIC) component, 0 ≤ p ≤ 4
     Dashed lines: MSFEs for fixed-c estimators; Solid line: BMA forecast




                                      48
                                 Figure 3
Monte Carlo estimates of T(MSFEposterior mean – MSFEBIC)/MSFEBIC for ℓ = 1
      Experiment 3: MA(1) DGP; estimator prior cWμ, c = 0,…, 20
    AR(BIC) component, 0 ≤ p ≤ 3, 4, 5, 6 for T = 50, 100, 200, 400
 Dashed lines: fixed-c estimators; Solid line: BMA; Heavy dash-dot: AIC




                                   49
                                          Figure 4
   Monte Carlo estimates of T(MSFEposterior mean – MSFEBIC)/MSFEBIC for cumulative
                                      forecasts, ℓ = 4
              Experiment 3: MA(1) DGP; estimator prior cWμ, c = 0,…, 20
            AR(BIC) component, 0 ≤ p ≤ 3, 4, 5, 6 for T = 50, 100, 200, 400
Dashed lines: fixed-c estimators; Solid line: BMA; Heavy dash-dot: AIC




                                         50
