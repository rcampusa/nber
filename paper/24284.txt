                               NBER WORKING PAPER SERIES




                           HUMAN JUDGMENT AND AI PRICING

                                         Ajay K. Agrawal
                                          Joshua S. Gans
                                           Avi Goldfarb

                                       Working Paper 24284
                               http://www.nber.org/papers/w24284


                     NATIONAL BUREAU OF ECONOMIC RESEARCH
                              1050 Massachusetts Avenue
                                Cambridge, MA 02138
                                    February 2018




Thanks to Miguel Villas-Boas for helpful comments. The authors are associated with the Creative
Destruction Lab at the University of Toronto that involves artificial intelligence start-ups. The
views expressed herein are those of the authors and do not necessarily reflect the views of the
National Bureau of Economic Research.

At least one co-author has disclosed a financial relationship of potential relevance for this
research. Further information is available online at http://www.nber.org/papers/w24284.ack

NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.

© 2018 by Ajay K. Agrawal, Joshua S. Gans, and Avi Goldfarb. All rights reserved. Short
sections of text, not to exceed two paragraphs, may be quoted without explicit permission
provided that full credit, including © notice, is given to the source.
Human Judgment and AI Pricing
Ajay K. Agrawal, Joshua S. Gans, and Avi Goldfarb
NBER Working Paper No. 24284
February 2018
JEL No. D81,L12,O33

                                           ABSTRACT

Recent artificial intelligence advances can be seen as improvements in prediction. We examine
how such predictions should be priced. We model two inputs into decisions: a prediction of the
state and the payoff or utility from different actions in that state. The payoff is unknown, and can
only be learned through experiencing a state. It is possible to learn that there is a dominant action
across all states, in which case the prediction has little value. Therefore, if predictions cannot be
credibly contracted upfront, the seller cannot extract the full value, and instead charges the same
price to all buyers.

Ajay K. Agrawal                                   Avi Goldfarb
Rotman School of Management                       Rotman School of Management
University of Toronto                             University of Toronto
105 St. George Street                             105 St. George Street
Toronto, ON M5S 3E6                               Toronto, ON M5S 3E6
CANADA                                            CANADA
and NBER                                          and NBER
ajay.agrawal@rotman.utoronto.ca                   agoldfarb@rotman.utoronto.ca

Joshua S. Gans
Rotman School of Management
University of Toronto
105 St. George Street
Toronto ON M5S 3E6
CANADA
and NBER
joshua.gans@gmail.com
                                                                                                                                   2




                                                             I. Introduction

  Artificial intelligence (AI) is undergoing a renaissance. Thanks to developments in machine
learning – particularly, deep learning and reinforcement learning – there has been an explosion in
the applications of AI in many settings. In actuality, however, far from providing new forms of
machine intelligence in a general fashion, what AI has been able to do has been to reduce the cost
of higher quality predictions in a drastic way (Agrawal et.al., 2018). As deep learning pioneer
Geoffrey Hinton put it, “Take any old problem where you have to predict something and you have
a lot of data, and deep learning is probably going to make it work better than the existing
techniques.” (Hinton 2016) Thus, when they are able to utilize AI, decision-makers know more
about their environment including about future states of the world.
  These developments have brought about discussion as to the role of humans in that decision-
making process. The view we take here (see also Agrawal et.al., 2017) is that humans still play a
critical role in determining the reward functions in decisions. That is, if the decision can be
formulated as a problem of choosing an action (x), in the face of uncertainty about the state of the
world (q) with probability distribution function F(q), in an ideal setting, an AI can transform that
problem from 𝑚𝑎𝑥$ ∫ 𝑢(𝑥, 𝜃)𝑑𝐹(𝜃) into 𝑚𝑎𝑥$ 𝑢(𝑥, 𝜃) with actions being made in a state-
contingent manner. However, this transformation relies on someone knowing the utility function,
𝑢(𝑥, 𝜃). We claim that, at present, only a human can develop this knowledge.1
  That said, the value to understanding the utility function in all of its nuance is enhanced when
the decision-maker knows that they will have accurate predictions of the state of the world. This
is especially true when it comes to states that are unlikely to arise or as applied to decision-making
in complex environments.
  Here we develop a model of utility function discovery in the presence of AI. In so doing, we
choose to emphasize experiences as the means by which decision-makers come to know that
function. Our goal here is to understand what this implies for the demand for AI and, in particular,
how suppliers of AI services should go about pricing their services. We show that learning leads
to some interesting dilemmas in setting AI pricing. In particular, learning may lead decision-



  1
      Over time, machines may learn to predict the utility function by observing human decisions. We leave this for future work.
                                                                                                       3


makers to discover they have dominant actions and so do not need AI for prediction at all. This
presents challenges for the long-term pricing of AI services. The mechanism driving this result is
related to the price discrimination literature on the strategic effects of firms gaining information
about consumers (e.g. Hart and Tirole 1988; Villas-Boas 2004; Acquisti and Varian 2005; Zhang
2011; Fudenberg and Villas-Boas 2012).

                                          II. Model Set-Up

  Our baseline model is drawn from Agrawal et.al. (2017); itself inspired by Bolton and Faure-
Grimaud (2009). The decision-maker faces uncertainty over two states of the world, {𝜃. , 𝜃/ } with
equal prior probabilities. There are two possible actions: a state independent action with known
payoff of S (safe) and a state dependent action with unknown payoff, R or r as the case may be
(risky). The agent does not know the payoff from the risky action in each state and must apply
judgment to determine that payoff. We assume that there are only two possible payoffs from the
risky action, R and r, where 𝑅 > 𝑆 > 𝑟. In the absence of judgment, the ex ante expectation that
the risky action is optimal in state 𝜃5 is 𝑣; common across states. That is, 𝑣 is the probability in
state 𝜃5 that the risky payoff is R rather than r. This is a statement about the payoff, given the state.
  In the absence of knowledge regarding the specific payoffs from the risky action, a decision can
only be made on the basis of prior probabilities. In this case, the expected payoff from the risky
action is 𝜌 ≡ 𝑣𝑅 + (1 − 𝑣)𝑟. We make the following assumptions:
  A1 (Safe Default) 𝜌 ≤ 𝑆
  A2 (Judgment Insufficient) =>(𝑅 + 𝑟) ≤ 𝑆
(A1) states that, in the absence of judgment, the safe action is the default in each state. (A2) states
that, if the agent knows the payoffs in each state, judgment alone will not change that default.
  If an AI is deployed to assist in this decision-making, what it does is provide an ex ante prediction
of the state. To keep things simple, we assume that prediction is perfect and so, with an AI, the
decision-maker knows the state with certainty. By (A2), without judgment, having an AI does not
change the decision or payoff. With both judgment and a prediction, optimal state-contingent
decision-making is possible and the decision-maker’s expected payoff is 𝜌∗ ≡ 𝑣𝑅 + (1 − 𝑣)𝑆 in
each period.
                                                                                                     4


                                 III. Judgment Through Experience

  Judgment does not come for free. In Agrawal et.al. (2017), we assume that it takes thought (at
the cost of time). By contrast, here we assume that judgment arises from experience. Specifically,
an agent must actually experience a given state in order to, potentially, learn the payoffs from that
state. If they do not know the state, they cannot learn about payoffs.
  Decision-makers discount with factor d < 1. If a state arises, they may gather enough experience
to determine the optimal action and can make a choice based on that judgment. Otherwise, they
can make a decision in the absence of that judgment. Importantly, they cannot learn the payoff
associated with the state if they take the default action. Ignorance remains and their per period
payoff is S.
  The timing of the game is as follows:
1. (Prediction) The decision-maker is informed by the AI of the state that period.
2. (Judgment) With probability, 1 − 𝜆, the decision-maker does not learn the payoff for the risky
    action in that state. With probability 𝜆, the decision-maker gains this knowledge and retains it
    into the future.
3. (Action) Based on these outcomes, the decision-maker takes an action and payoffs are realized
    and the time period ends.
There are three phases to experience: (i) Full experience: when the agent has learned payoffs in
                                                                   =
both states, resulting in a discounted payoff from this point of: =AB 𝜌∗ . (ii) Partial experience: Let
𝜋5 denote the expected present discounted value if the agent already knows what the optimal action
is in 𝜃5 . Then:
                             .               .                           .
                       𝜋5 = /(𝜌∗ + 𝛿𝜋5 ) + / F(1 − 𝜆)(𝑆 + 𝛿𝜋5 ) + 𝜆.GH 𝜌∗ I

                                                     L
                                                 (.K=AB)M ∗K(.GN)O
                                      ⟹ 𝜋5 =              =
                                                   /(.GP.G>NQH)

And finally, (iii), no experience with expected discounted payoff of:
                                 Π = 𝜆(𝜌∗ + 𝛿𝜋5 ) + (1 − 𝜆)(𝑆 + 𝛿Π)

                                            (.GH)(.GN)/OK(/GH)NM ∗
                                    ⟹Π=        (.GH)(/G(/GN)H)
                                                                                                   5


Thus, there is a learning period of uncertain length followed by a period whereby the agent can
apply full experience to decisions into the future earning 𝜌∗ on average. As l increases, so does Π,
showing that prediction and judgment are complements in this model.

                                   IV. Pricing AI as a Service

  Without any judgment or experience, the net present discounted value earned by the agent would
    =
be =AB 𝑆. Without initial access to an AI, the agent cannot apply judgment and gain experience to
improve upon this. This suggests that a monopolist provider of AI could charge a fixed sum of
     =
Π − =AB 𝑆. Moreover, as Π is increasing in l, that provider would want to target agents with
judgment ability (or ease) as high as possible first before moving on to worse judges.
  There are several challenges to pricing AI with a once off payment. First, such algorithms often
are run by the provider and not hosted as a distinct app by the user. Therefore, there are on-going
costs to be recouped and users may be reluctant to pay up front for such a service. Second,
algorithms hosted by the provider may improve at a more rapid rate. The provider may then want
a means of monetizing those improvements.
  For these reasons, we consider pricing of AI as an ongoing service with a subscription fee of p
per period. If the AI provider does not have knowledge of the experience level – and indeed, the
experience – of each agent, this is a non-trivial pricing problem.
  To see this, let us consider the purchase decisions of fully experienced agents who know their
payoff function. For some of these agents, they would have found that neither the safe nor risky
action is dominated and their per period expected payoff is =>(𝑅 + 𝑆). They can realize these
payoffs with prediction but in the absence of prediction, they earn S per period (by A1). Thus, their
willingness to pay for prediction is =>(𝑅 − 𝑆). For other agents, their experience has shown them
that one of the actions is dominated. Those agents either earn R or S per period but do not need
prediction to do so. What this means is that the long-term market for prediction is at most a share
2𝑣(1 − 𝑣) of the original market; that is, prediction is only valuable to those who have found
neither action to be dominated. To keep things simple, we now assume that 𝑣 = =>. A fully
experienced agent will continue to purchase AI if =>(𝑅 − 𝑆) ≥ 𝑝. If the provider, charges a price
based on this, they will earn =V(𝑅 − 𝑆) per period.
                                                                                                    6


  What determines whether a partially experienced agent pays for the AI service? If they have
learned that the risky action is optimal in one state, their expected discounted payoff is 𝜋W where:

                            .             .                       (XWKOG/Y)
                       𝜋W = /(𝑅 + 𝛿𝜋W ) + / F(1 − 𝜆)(𝑆 + 𝛿𝜋W ) + 𝜆 Z(.GH) I − 𝑝

                                                     L
                                                WKV(=AB)(XWKOG/Y)K(.GN)OG/Y
                                   ⟹ 𝜋W =                         =
                                                           /(.GP.G>NQH)

  If this agent did not have access to an AI after this point, their expected discounted payoff would
                  .
be:     =
       =AB
             max {Z (3𝑅 + 𝑟), 𝑆}. On the other hand, if a partially experienced agent learned the safe

action was optimal in one state, their expected discounted payoff is 𝜋O where:

                              .             .                             (WKXOG/Y)
                        𝜋O = /(𝑆 + 𝛿𝜋O ) + / F(1 − 𝜆)(𝑆 + 𝛿𝜋O ) + 𝜆         Z(.GH)
                                                                                    I   −𝑝

                                                     L
                                                OKV(=AB)(WKXOG/Y)K(.GN)OG/Y
                                    ⟹ 𝜋O =                        =
                                                           /(.GP.G>NQH)

  If this agent did not have access to an AI after this point, their expected discounted payoff would
     =
be: =AB 𝑆. These two options differ both in terms of the payoffs they generate while learning as well
as what the potential upside is from moving to full experience. If the agent has learned that the
risky action is optimal, this upside is `V𝑅 + =>𝑆 − 𝑝 while otherwise it is =>𝑅 + `V𝑆 − 𝑝. Thus, 𝜋W >
𝜋O .
  This leads to a pricing dilemma on the part of an AI provider. They have two pricing options:
                                                   .
                                  =
they can set p so that min {𝜋W − =AB max cZ (3𝑅 + 𝑟), 𝑆d , 𝜋O − =AB
                                                                 =
                                                                    𝑆} ≥ 0 thereby, selling to the
                                                                               .
                                                             =
entire market or price above this level so that either 𝜋W ≥ =AB max cZ (3𝑅 + 𝑟), 𝑆d or 𝜋O ≥ =AB
                                                                                             =
                                                                                                𝑆

and sell to half of the market. The following proposition demonstrates, however, that, for a far-
sighted AI provider, servicing the entire market is the more profitable approach; however, the AI
provider does not extract the full value of the prediction despite having perfect knowledge of the
state.
Proposition 1. For d sufficiently high, the AI provider will maximize profits by covering the entire
market with a price equal to:
                                                       N
                                          𝑝 = /(NKZ(.GH))(𝑅 − 𝑆).
                                                                                                       7


PROOF: We first examine the prices in the proposition that would result in full inclusion. The
                           =
prices are such that 𝜋W ≥ =AB 𝑚𝑎𝑥g=V(3𝑅 + 𝑟), 𝑆h and 𝜋O ≥ =AB
                                                           =
                                                              𝑆 so there is full inclusion with
                                                                            =
partially experienced agents. Specifically, note that the price where 𝜋W = =AB 𝑆, 𝑝 =
(V(=AB)i`L)
>PLiV(=AB)Q
           (𝑅 − 𝑆) > >PLiV(=AB)Q
                          L
                                (𝑅 − 𝑆) which is the price where 𝜋O = =AB
                                                                       =
                                                                          𝑆. It is useful to examine

whether these prices will result in inclusion with fully experienced agents. Note also that the price
              =
where 𝜋W = V(=AB)(3𝑅 + 𝑟), 𝑝 = >(>jAkAl)(=AB)iL(kAjiB(>jAkAl))
                                          >PLiV(=AB)Q
                                                                      L
                                                               > >PLiV(=AB)Q(𝑅 − 𝑆). Thus, the price

in the proposition is the only price that will support full inclusion at the partial experience phase.
    Will this price also support inclusion at the full experience stage; that is, is 𝑝 ≤ =>(𝑅 − 𝑆)? Note
              N                                                 (V(=AB)i`L)
that /(NKZ(.GH)) < => so this is satisfied. Note, however, that >PLiV(=AB)Q (𝑅 − 𝑆) > =>(𝑅 − 𝑆) and that,
                  /(/OGWGo)(.GH)KN(WGOKH(/OGWGo))       (WGOKH(/OGWGo))
as 𝛿 → 1,                   /PNKZ(.GH)Q
                                                    →          /
                                                                          > =>(𝑅 − 𝑆). Thus, under these

conditions, setting a price that excludes some agents at the partial experience phase, causes future
demand by fully experienced agents to fall to 0.
    When we examine pricing to agents with no experience, note that:

                                .
                           Π = 𝜆/(𝑅 + 𝛿𝜋W + 𝑆 + 𝛿𝜋O ) + (1 − 𝜆)(𝑆 + 𝛿Π) − 𝑝


                                     𝜆./(𝑅 + 𝛿𝜋W + 𝑆 + 𝛿𝜋O ) + (1 − 𝜆)𝑆 − 𝑝
                             ⟹Π=
                                                 1 − (1 − 𝜆)𝛿
The issue is whether an AI provider can charge a price that extracts the maximal rents at this phase;
                   =
i.e., so that Π = =AB 𝑆. If this could be done, p will be: 𝑝 = 𝜆=>P𝑅 + 𝛿𝜋W + 𝑆 + 𝛿𝜋O − =AB
                                                                                        >
                                                                                           𝑆Q.
Substituting and solving for p we have:

                                             (/GH)(.G(.GN)H)
                                    𝑝=𝜆                            (𝑅 − 𝑆)
                                          ZGHPpGN(qKN)GH(ZGqN)Q
                                                       =
    However, it easy to check that at this price 𝜋O < =AB 𝑆, so this would not result in full inclusion
beyond that phase. Moreover, as 𝛿 → 1, this price becomes (𝑅 − 𝑆). Therefore, the price in the
proposition is the only fully inclusive price resulting in a long-run per period payoff of more than
=
>
          L
 𝑝 = V(LiV(=AB))(𝑅 − 𝑆) as the provider always serves half of the fully experienced agents.
                                                                                                                                                     8


   We have also shown that for d sufficiently high, any candidate exclusionary price will result in
prices that exceed =>(𝑅 − 𝑆). Thus, for d sufficiently high, the AI provider will not find it profitable
to exclude agents at any stage.


   Intuitively, when some initial judgment is complete, there is either good news (in that the risky
strategy is optimal) or bad news (in which it is not). An inclusion strategy requires price to be low
enough that following bad news, learning still occurs. However, while the upside potential for the
user following good news is higher than that following bad news, the value of prediction after full
experience is gained is the same. Thus, the AI provider has no mechanism by which they can share
in the upside. Given this, they choose to price low and not exclude any users at this stage. Half of
the users eventually opt out when they find that either the safe or risky action is dominant.
   What this means is that an AI provider, who cannot implement upfront pricing, is restricted in
the value they can appropriate. While learning can yield good or bad news to the decision-maker,
good news may cause prediction to lose its value as the decision-maker discovers the risky action
is dominant. Thus, the AI provider must sacrifice rents in order to ensure that they can capture
some rents as the decision-maker gains experience.
   Can versioning – selling an AI product which has lower performance – improve this outcome
for the AI provider? The intuition would be that until they are fully experienced, users will
purchase the lower performing product allowing the AI provider to charge more in the long-term.
The downside is a lower performing product may slow the gathering of experience and push that
long-term out further. The details of this are left to future work.2

                                          V. Judgment Through Experimentation

   Using an experience frame to understand judgment suggests an alternative way of ‘learning’ the
reward function: experimentation. In particular, when coupled with prediction, a decision-maker
could, by choosing the risky action, evaluate whether that is the right action for that state. The
expected cost would be 𝑆 − 𝜌. In this conception, we have the following:




   2
     Also left for future work is what happens if the discount factor is low. We conjecture that the provider may still be able to sell AI to facilitate
judgment and the identification of dominated actions. However, in the case, the demand from a given customer may be so short-term that it may be
better to sell AI as a once off and lose advantages that come from providing it as a service.
                                                                                                      9


                                      .               .            .
                                𝜋5 = /(𝜌∗ + 𝛿𝜋5 ) + / r𝜌 + 𝛿 .GH 𝜌∗ s
                                                    .
                                                   .GH
                                                       𝜌∗
                                                       +𝜌
                                          ⟹ 𝜋5 =
                                                     2−𝛿
Thus, the expected present discount payoff prior to any experience is:
                                             Π = 𝜌 + 𝛿𝜋5
                                            .                  H
                                  ⟹ Π = /GH r(3 − 𝛿)𝜌 + .GH 𝜌∗ s

The convenient property of this frame is that it relates the cost of judgment explicitly to the
expected cost of experimentation. In particular, as r decreases, experimentation becomes more
costly.
  These calculations presume that the decision-maker finds it worthwhile to experiment. If no
                                                            =
experiment is undertaken, the present discounted payoff is =AB 𝑆. Thus, it may be the case that there
is no value for an AI as the cost of experimentation may be too high.
  Even if this were not the case, in assessing the demand for AI under experimentation, we need
to consider the fact that decision-makers can use experimentation to discover whether they have
dominated actions or not. Depending on v, by running repeated experiments, even in the absence
of knowledge of which state has arisen, the decision-maker can potentially infer whether the risky
or safe action is preferred in both states. In this case, as we noted earlier, there would be no further
demand for an AI.
  Working out the full equilibrium outcome under experimentation is beyond the scope of our
analysis in this short paper. However, we believe that, in some environments, this could prove to
be an interesting driver of the demand for AI and how it evolves.

                                             Conclusion

  Recent analysis (Brynjolfsson et.al., 2018, Felten et al., 2018) has viewed the introduction of AI
as substituting for human labor in a given task. Our approach decomposes the task and shows that
AI complements human judgment. In particular, we show that AI facilitates the use of experience
to understand utility functions. This is something we would expect to be stronger in newer and
more complex decisions. In macroeconomic models that examine the creation of new tasks
(Acemoglu & Restrepo, 2017), AI drives that creation by freeing up labor. Here, we posit a direct
role for AI in those new tasks. Moreover, we show that this has a non-straightforward impact on
                                                                                                10


the demand for AI and how it is priced. Thus, the impact of AI is likely to be more complex than
the analyses based on straight substitution imply.

                                         REFERENCES

Acquisti, A., and H. Varian (2005), “Conditioning Prices on Purchase History,” Marketing Science
  24(3), 367-381.
Acemoglu, D. and P. Restrepo (2017), “Artificial Intelligence, Automation and Work,” in The
  Economics of Artificial Intelligence, A. Agrawal et.al. (eds), University of Chicago Press:
  Chicago (forthcoming).
Agrawal, A., J.S. Gans and A. Goldfarb (2017), “Prediction Machines, Judgment and
  Complexity,” in The Economics of Artificial Intelligence, A. Agrawal et.al. (eds), University of
  Chicago Press: Chicago (forthcoming).
Agrawal, A., J.S. Gans and A. Goldfarb (2018), Prediction Machines: The Simple Economics of
  Artificial Intelligence, Harvard Business Review Press: Boston.
Bolton, P. and A. Faure-Grimaud (2009), “Thinking Ahead: The Decision Problem,” Review of
  Economic Studies, 76: 1205-1238.
Brynjolfsson, E., T. Mitchell and D. Rock (2018), “What can Machines Learn and What does it
  mean for Occupations and Industries,” AEA Papers and Proceedings, forthcoming.
Felten, E.W., M. Raj and R. Seamans (2018), “Linking Advances in Artificial Intelligence to
  Skills, Occupations and Industries,” AEA Papers and Proceedings, forthcoming.
Fudenberg, D., and J. M. Villas-Boas (2012), “Price Discrimination in the Digital Economy.”
  Chapter 10 in Oxford Handbook of the Digital Economy, eds. M. Peitz and J. Waldfogel.
Hinton, G, (2016), “Geoff Hinton: On Radiology” Panel discussion, October 27,
  https://www.youtube.com/watch?v=2HMPRXstSvQ.
Villas-Boas, J. M. (2004), “Price cycles in markets with customer recognition,” RAND Journal of
  Economics 35(3), 486-501.
Zhang, Juanjuan (2011), “The Perils of Behavior-Based Personalization,” Marketing Science
  30(1), 170-18.
