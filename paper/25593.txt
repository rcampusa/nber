                              NBER WORKING PAPER SERIES




          BEST LINEAR APPROXIMATIONS TO SET IDENTIFIED FUNCTIONS:
                WITH AN APPLICATION TO THE GENDER WAGE GAP

                                     Arun G. Chandrasekhar
                                      Victor Chernozhukov
                                       Francesca Molinari
                                         Paul Schrimpf

                                      Working Paper 25593
                              http://www.nber.org/papers/w25593


                     NATIONAL BUREAU OF ECONOMIC RESEARCH
                              1050 Massachusetts Avenue
                                Cambridge, MA 02138
                                    February 2019




We are grateful to Casey Mulligan and Yona Rubinstein for sharing their data, and Denis
Chetverikov and Hiroaki Kaido for comments. We thank seminar participants at Bern, Collegio
Carlo Alberto, Penn State, Toronto, Toulouse, UCL, Yale, the 2012 Winter Meetings of the
Econometric Society, and the 2013 Cowles Econometrics Conference for comments. Financial
support from the NSF through grants SES-0922330 and SES-1824448 (Molinari) is gratefully
acknowledged. Chernozhukov acknowledges NSF support. The views expressed herein are those
of the authors and do not necessarily reflect the views of the National Science Foundation and the
National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.

© 2019 by Arun G. Chandrasekhar, Victor Chernozhukov, Francesca Molinari, and Paul
Schrimpf. All rights reserved. Short sections of text, not to exceed two paragraphs, may be quoted
without explicit permission provided that full credit, including © notice, is given to the source.
Best Linear Approximations to Set Identified Functions: With an Application to the Gender
Wage Gap
Arun G. Chandrasekhar, Victor Chernozhukov, Francesca Molinari, and Paul Schrimpf
NBER Working Paper No. 25593
February 2019
JEL No. C13,C31

                                           ABSTRACT

This paper provides inference methods for best linear approximations to functions which are
known to lie within a band. It extends the partial identification literature by allowing the upper
and lower functions defining the band to carry an index, and to be unknown but parametrically or
non-parametrically estimable functions. The identification region of the parameters of the best
linear approximation is characterized via its support function, and limit theory is developed for
the latter. We prove that the support function can be approximated by a Gaussian process and
establish validity of the Bayesian bootstrap for inference. Because the bounds may carry an
index, the approach covers many canonical examples in the partial identification literature arising
in the presence of interval valued outcome and/or regressor data: not only mean regression, but
also quantile and distribution regression, including sample selection problems, as well as mean,
quantile, and distribution treatment effects. In addition, the framework can account for the
availability of instruments. An application is carried out, studying female labor force participation
using data from Mulligan and Rubinstein (2008) and insights from Blundell, Gosling, Ichimura,
and Meghir (2007). Our results yield robust evidence of a gender wage gap, both in the 1970s and
1990s, at quantiles of the wage distribution up to the 0.4, while allowing for completely
unrestricted selection into the labor force. Under the assumption that the median wage offer of the
employed is larger than that of individuals that do not work, the evidence of a gender wage gap
extends to quantiles up to the 0.7. When the assumption is further strengthened to require
stochastic dominance, the evidence of a gender wage gap extends to all quantiles, and there is
some evidence at the 0.8 and higher quantiles that the gender wage gap decreased between the
1970s and 1990s.

Arun G. Chandrasekhar                             Francesca Molinari
Department of Economics                           Department of Economics
Stanford University                               Cornell University
579 Serra Mall                                    Ithaca, NY
Stanford, CA 94305                                fm72@cornell.edu
and NBER
arungc@stanford.edu                               Paul Schrimpf
                                                  Vancouver School of Economics
Victor Chernozhukov                               University of British Columbia
Department of Economics                           997 - 1873 East Mall Vancouver,
Massachusetts Institute of Technology             BC V6T1Z1
77 Massachusetts Avenue Cambridge,                schrimpf@mail.ubc.ca
Mass. 02139
vchern@mit.edu
                  BEST LINEAR APPROXIMATIONS TO SET IDENTIFIED FUNCTIONS                                      1


                                            1. Introduction

   This paper contributes to the partial identification literature by providing estimation
and inference results for best linear approximations to set identified functions. Specifically,
we work with a family of functions f (x, α) indexed by some parameter α ∈ A, that is
known to satisfy θ0 (x, α) ≤ f (x, α) ≤ θ1 (x, α), x − a.s., with x ∈ Rd a vector of covariates.
Econometric frameworks yielding such restriction are ubiquitous in economics and in the
social sciences, as illustrated by Manski (2003, 2007). Cases explicitly analyzed in this paper
include: (1) mean regression; (2) quantile regression; and (3) distribution and duration
regression, in the presence of interval valued outcome and/or covariate data, including
hazard models with interval-valued failure times; (4) sample selection problems; (5) mean
treatment effects; (6) quantile treatment effects; and (7) distribution treatment effects, see
Section 3 for details.1 Yet, the methodology that we propose can be applied to virtually any
of the frameworks discussed in Manski (2003, 2007). In fact, our results below also allow
for exclusion restrictions that yield intersection bounds of the form supv∈V θ0 (x, v, α) ≡
θ0 (x, α) ≤ f (x, α) ≤ θ1 (x, α) ≡ inf v∈V θ1 (x, v, α) x − a.s., with v an instrumental variable
taking values in a finite set V. The bounding functions θ0 (x, α) and θ1 (x, α) may be indexed
by a parameter α ∈ A and may be any estimable function of x.
  When the restriction θ0 (x, α) ≤ f (x, α) ≤ θ1 (x, α) x−a.s. summarizes all the information
available to the researcher, the identification region for f (·, α) is given by the set of functions
                     F (α) = {φ(·, α) : θ0 (x, α) ≤ φ(x, α) ≤ θ1 (x, α) x − a.s.}                         (1.1)
The set F (α) , while sharp, can be difficult to interpret and report, especially when x
is multi-dimensional. Similar considerations apply to related sets, e.g. the set of marginal
effects of components of x on f (x, α) . Consequently, in this paper we focus on the sharp set
of parameters characterizing best linear approximations to the functions comprising F (α).
This set, denoted B (α) in what follows, is of great interest in empirical work because of its
tractability. In particular, it can be computed extremely rapidly using standard statistical
packages such as Stata or R.2
   Our method appears to be the first and currently only method available in the literature
for performing inference on the set B (α) and its elements when the bounding functions
θ0 (·, α) and θ1 (·, α) need to be estimated. This estimation may be carried out both para-
metrically as well as non-parametrically via series methods. Previous closely related con-
tributions by Beresteanu and Molinari (2008, BM henceforth) and Bontemps, Magnac, and
Maurin (2012) provided inference methods for best linear approximations to conditional
expectations in the presence of interval outcome data. In that environment, the bounding
functions do not need to be estimated, as the set of best linear approximations can be
   1For example, one may be interested in the α-conditional quantile of a random variable y given x,
denoted Qy (α|x) , but only observe interval data [y0 , y1 ] which contain y with probability one. In this case,
f (x, α) ≡ Qy (α|x) and θ` (x, α) ≡ Q` (α|x) , ` = 0, 1, the conditional quantiles of properly specified random
variables.
   2An implementation in R with non-parametrically estimated functions θ (·, α), θ (·, α) can be found
                                                                                   0       1
at https://bitbucket.org/paulschrimpf/mulligan-rubinstein-bounds.An implementation in Stata with
observed θ0 (·, α), θ1 (·, α) can be found at https://molinari.economics.cornell.edu/programs/Stata_
SetBLP.zip.
2                  CHANDRASEKHAR, CHERNOZHUKOV, MOLINARI, AND SCHRIMPF


characterized directly through functions of moments of the observable variables. Hence,
our paper builds upon and significantly generalizes their results. These generalizations are
our main contribution and are important for many empirically relevant applications.
   As we discuss in Section 2, the set B (α) is convex and hence can be fully characterized
by its support function.3 We therefore use this function to carry out inference for B (α)
and its elements. An application of results in BM and Bontemps, Magnac, and Maurin
(2012) gives that the support function of B(α) is equal to the expectation of a function
s (θ0 (x, α), θ1 (x, α), x, E (xx0 )) . Hence, the analogy principle suggests to estimate the sup-
port function of B (α) through a sample average of the same function s, where θ0 (x, α) and
θ1 (x, α) are replaced by parametric or non-parametric estimators, and E (xx0 ) is replaced
by its sample analog. We show that the resulting estimator is consistent for the support
function of B (α), uniformly over (q, α) ∈ S d−1 × A, where S d−1 := {q ∈ Rd : kqk = 1}
is the range of directions for which the support function needs to be evaluated to fully
characterize B (α). We then provide a methodology for inference. In doing so, our paper
overcomes significant technical complications, thereby making contributions of independent
interest.
   First, when θ0 (·, α) and θ1 (·, α) are non-parametrically estimated through series methods,
we show that the support function process is strongly approximated by a Gaussian process
on S d−1 ×A that may not necessarily converge as the number of series functions increases to
infinity. To solve this difficulty, we show that each subsequence has a further subsequence
converging to a tight Gaussian process on S d−1 × A with a uniformly equicontinuous and
non-degenerate covariance function. We show how to conduct inference using properties
of this covariance function, and we provide a consistent Bayesian bootstrap procedure to
estimate quantiles of functions of the Gaussian process.
   Second, we allow for the possibility that some of the regressors in x have a discrete dis-
tribution. In order to conduct test of hypothesis and make confidence statements, both BM
and Bontemps, Magnac, and Maurin (2012) had explicitly ruled out discrete regressors, as
their presence greatly complicates the derivation of the limiting distribution of the support
function process. By using a simple data-jittering technique, we show that these compli-
cations completely disappear, albeit at the cost of basing statistical inference on a slightly
conservative confidence set.4
  Third the function f (·, α) may be set identified via intersection of bounds of the form
supv∈V θ0 (x, v, α) ≤ f (x, α) ≤ inf v∈V θ1 (x, v, α) x − a.s., with V a finite set. In this case
we show that the set of best linear approximations to the intersected bands of functions is
equal to the intersection of the sets of best linear approximations obtained for each v ∈ V
corresponding to the band θ0 (x, v, α) ≤ f (x, α) ≤ θ1 (x, v, α). We then propose an extremely

    3“The support function (of a nonempty closed convex set B in direction q) σ (q) is the signed distance of
the support plane to B with exterior normal vector q from the origin; the distance is negative if and only if
q points into the open half space containing the origin,” Schneider (1993, page 37). See Rockafellar (1970,
Chapter 13) or Schneider (1993, Section 1.7) for a thorough discussion of the support function of a closed
convex set and its properties.
   4More recently, Fang and Santos (2018) propose an alternative resampling scheme, which does not require
data jittering, and is valid in our context. Their procedure, however, requires the estimation of the directional
derivative of the support function.
               BEST LINEAR APPROXIMATIONS TO SET IDENTIFIED FUNCTIONS                        3


straightforward, albeit (mildly) conservative, procedure to conduct inference. More pow-
erful inference can be achieved by employing either the precision correction procedure of
Chernozhukov, Lee, and Rosen (2013) or the generalized moment selection approach of
Andrews and Shi (2013).
   Because the support function process is characterized on the entire S d−1 × A, our func-
tional asymptotic results also allow us to perform inference on statistics that involve a
continuum of values for q and/or for α. The latter is a substantial advancement compared
to the related literature using support function for inference in partially identified models.
For example, for best linear approximations to conditional quantile functions with interval
outcome data, we are able to test whether a given regressor xj has a positive coefficient in
the best linear approximation for all α ∈ A. When the conditional quantile is in fact linear,
rejection of this assumption implies that xj has a non-positive effect for some α ∈ A.
   To illustrate the use of our estimator, we revisit the analysis of Mulligan and Rubinstein
(2008) in light of the bounding approach of Blundell, Gosling, Ichimura, and Meghir (2007).
The literature studying female labor force participation has argued that the gender wage gap
has shrunk between 1975 and 2001. Mulligan and Rubinstein (2008) suggest that women’s
wages may have grown less than men’s wages between 1975 and 2001, had their behavior
been held constant, but a selection effect induces the data to show the gender wage gap
contracting. They point out that a growing wage inequality within gender induces women to
invest more in their market productivity. In turn, this would differentially pull high skilled
women into the workplace and the selection effect may make it appear as if cross-gender
wage inequality had declined. To test this conjecture they employ a Heckman selection
model to correct married women’s conditional mean wages for selectivity and investment
biases. Using CPS repeated cross-sections from 1975-2001 they argue that the selection of
women into the labor market has changed sign, from negative to positive, or at least that
positive selectivity bias has come to overwhelm investment bias. Specifically, they find that
the gender wage gap measured by OLS decreased from -0.419 in 1975-1979 to -0.256 in 1995-
1999. After correcting for selection using the classic Heckman selection model, they find
that the wage gap was -0.379 in 1975-1979 and -0.358 in 1995-1999, thereby concluding that
correcting for selection, the gender wage gap may have not shrunk at all. Because it is well
known that without a strong exclusion restriction results of the normal selection model can
be unreliable, Mulligan and Rubinstein conduct a sensitivity analysis which corroborates
their findings.
   We provide an alternative approach. We use our method to estimate bounds on the
quantile gender wage gap for the 1970s and the 1990s, without assuming a parametric form
of selection or a strong exclusion restriction. When selection into the labor force if left
completely unrestricted, we are able to document a gender wage gap for quantiles up to the
0.4. We then augment our analysis with additional assumptions, following the identification
analysis earlier put forward by Blundell, Gosling, Ichimura, and Meghir (2007) to study the
gender wage gap in the UK. Under the assumption that the median wage offer of the
employed is larger than that of individuals that do not work, the evidence of a gender wage
gap extends to quantiles up to the 0.7. When the assumption is further strengthened to
require stochastic dominance, the evidence of a gender wage gap extends to all quantiles, and
4               CHANDRASEKHAR, CHERNOZHUKOV, MOLINARI, AND SCHRIMPF


there is some evidence at the 0.8 and higher quantiles that the gender wage gap decreased
between the 1970s and 1990s.
Related Literature. This paper contributes to a growing literature on inference on set-
identified parameters. Important examples in the literature include, among others, Andrews
and Barwick (2012), Andrews and Shi (2013), Andrews and Soares (2010), BM, Bontemps,
Magnac, and Maurin (2012), Bugni (2010), Canay (2010), Chernozhukov, Hong, and Tamer
(2007), Chernozhukov, Lee, and Rosen (2013), Galichon and Henry (2009), Kaido (2016),
Kaido and Santos (2014), Romano and Shaikh (2008), Romano and Shaikh (2010), and
Rosen (2008). BM propose an approach for estimation and inference in models where the
identification region is equal to the Aumann expectation of a properly defined random set
that can be constructed from observable random variables, and can therefore be estimated
via Minkowski averages of sample counterparts. Building on the fundamental insight in
random set theory that convex compact sets can be represented via their support func-
tions to leverage limit theorems for stochastic processes (Artstein and Vitale (1975)), BM
propose a support function based framework for inference in partially identified models.
They use best linear prediction as their main illustration. Bontemps, Magnac, and Maurin
(2012) extend the results of BM in important directions, by allowing for incomplete linear
moment restrictions where the number of restrictions exceeds the number of parameters to
be estimated, and extend the familiar Sargan test for overidentifying restrictions to par-
tially identified models. Kaido (2016) establishes a duality between the criterion function
approach proposed by Chernozhukov, Hong, and Tamer (2007), and the support function
approach proposed by BM. Kaido and Santos (2014) establish that support function based
estimators for identified sets resulting from convex moment inequalities are asymptotically
efficient, including the estimator proposed by BM.
   Closely related to the application of our method to the sample selection example, is
the work of Kline and Santos (2013). They study the sensitivity of empirical conclusions
about conditional quantile functions to the presence of missing outcome data, when the
Kolmogorov-Smirnov distance between the conditional distribution of observed outcomes
and the conditional distribution of missing outcomes is bounded by some constant k across
all values of the covariates. Under these assumptions, Kline and Santos show that the
conditional quantile function is sandwiched between a lower and an upper band, indexed
by the level of the quantile and the constant k. They conduct inference pointiwise in α,
under the assumptions that: (i) the support of the covariates is finite, so that the lower and
upper bands can be estimated at parametric rates; and (ii) the distribution of x is known,
thereby obtaining that E (xx0 ) is known. Each of these assumptions significantly simplifies
the derivation of the asymptotic distribution, but is often not warranted in applications.
The former assumption rules out the presence of any continuous regressor, while the latter
introduces a strong element of arbitrariness in the analysis. In sharp contrast, our results
are derived without imposing any of these assumptions, hold uniformly in α, and allow the
researcher to utilize instruments. While technically challenging, allowing for non-parametric
estimates of the bounding functions and for intersection bounds considerably expands the
domain of applicability of our approach, while allowing for an unknown population distri-
bution on x eliminates a significant element of arbitrariness from the analysis.
                BEST LINEAR APPROXIMATIONS TO SET IDENTIFIED FUNCTIONS                             5


Structure of the Paper. Section 2 develops our framework, and Section 3 demonstrates
its versatility by applying it to quantile regression, distribution regression, sample selection
problems, and treatment effects. Section 4 provides an overview of our theoretical results
and describes the estimation and inference procedures. Section 5 reports results for our
empirical application. Section 6 concludes. All proofs are in the Appendix.

                                  2. The General Framework

   We propose a method to carry out inference for best linear approximations to the set of
functions F (α) defined in equation (1.1). We let x denote a (column) vector in Rd , and
α ∈ A some index with A a compact set. For example, in quantile regression α denotes a
quantile; in duration regression α denotes a failure time. We assume that for each x the
bounding functions θ0 (x, α) and θ1 (x, α) are absolutely integrable. We note that even in
the special case where f (·, α) is linear in x, the bounding functions θ0 (·, α), θ1 (·, α) need not
be linear in x and economic theory may provide little guidance on their functional form.
Therefore, our asymptotic results allow for non-parametric estimation of these functions via
series methods.
   If the true function of interest f (·, α) were point identified, we could approximate it with
a linear function by choosing coefficients β(α) to minimize the expected squared prediction
error E[(f (x, α) − x0 β (α))2 ] under the assumption that E[xx0 ] is full rank. Because f (·, α) is
only known to lie in F (α) , performing this operation for each admissible function φ(·, α) ∈
F (α) yields a set of observationally equivalent parameter vectors, denoted B (α):
  B(α) = {β ∈ Rd : β = arg min E[(φ(x, α) − x0 b)2 ], P (θ0 (x, α) ≤ φ(x, α) ≤ θ1 (x, α)) = 1}
                              b
        = {β ∈ R : β = E[xx0 ]−1 E[xφ(x, α)], P (θ0 (x, α) ≤ φ(x, α) ≤ θ1 (x, α)) = 1}. (2.1)
                  d

It is easy to see that the set B (α) is almost surely non-empty, compact, and convex valued,
because it is obtained by applying linear operators to the (random) almost surely non-
empty interval [θ0 (x, α), θ1 (x, α)] , see BM (Section 4) for a discussion. Hence, B (α) can be
characterized quite easily through its support function
                                   σ (q, α) :=     sup       q 0 β (α) ,
                                                 β(α)∈B(α)

which takes on almost surely finite values ∀q ∈ S d−1 . In fact,
                                             b : q 0 b ≤ σ (q, α) ,
                                       T 
                           B(α) =                                                             (2.2)
                                        q∈S d−1

see Rockafellar (1970, Chapter 13). The support function provides a convenient way to
compute projections of the identified set. These can be used to report upper and lower
bounds on individual coefficients and draw two-dimensional identification regions for pairs of
coefficients. For example, the bound for the kth component of β(α) is [−σ(−ek , α), σ(ek , α)],
where ek is the kth standard basis vector. Similarly, the bound for a linear combination of
the coefficients, q 0 β(α), is [−σ(−q, α), σ(q, α)]. Figure 2.1 provides an illustration. In this
example, β is three dimensional. The left panel shows the entire identified set. The right
panel shows the joint identification region for β1 and β2 . The identified intervals for β1 and
β2 are also marked in red on the right panel.
6                            CHANDRASEKHAR, CHERNOZHUKOV, MOLINARI, AND SCHRIMPF



                                    Figure 2.1. Identification region and its projections


    1.25

     1.2
                                                                                    1.25
    1.15
                                                                                     1.2
     1.1
                                                                                    1.15
    1.05

      1                                                                              1.1

    0.95                                                                            1.05

     0.9
                                                                                       1
    0.85
                                                                                    0.95
     0.8
     1.3
                                                                                     0.9
           1.2                                                                1.3
                 1.1                                                    1.2         0.85

                         1                                       1.1
                                                          1                          0.8
                              0.9
                                                   0.9
                                      0.8    0.8                                    0.75
                                                                                       0.75   0.8   0.85   0.9   0.95   1   1.05   1.1   1.15   1.2   1.25




    More generally, if the criterion for “best” linear approximation is to minimize E[(f (x, α)−
x0 b(α))z̃ 0 W z̃(f (x, α) − x0 b(α))], where W is a j × j weight matrix and z̃ a j × 1 vector of
instruments, then we have
B(α) = {β ∈ Rd : β = E[xz̃ 0 W z̃x0 ]−1 E[xz̃ 0 W z̃φ(x, α)], P (θ0 (x, α) ≤ φ(x, α) ≤ θ1 (x, α)) = 1}.

  As in Bontemps, Magnac, and Maurin (2012), Magnac and Maurin (2008), and BM (p.
807) the support function of B(α) can be shown to be5
                                                              σ(q, α) = E[zq wq ]
where
                                             z = xz̃ 0 W z̃, zq = q 0 E[xz 0 ]−1 z,
                                            wq = θ1 (x, α)1(zq > 0) + θ0 (x, α)1(zq ≤ 0).
We estimate the support function by plugging in estimates of θ` (x, α), ` = 0, 1, and taking
empirical expectations:
                    h             −1                                                 i
       b(q, α) = En q 0 En xi zi0
                          
       σ                              zi θb1 (xi , α)1(b                        ziq ≤ 0) ,
                                                       ziq > 0) + θb0 (xi , α)1(b

where En denotes the empirical expectation, zbiq = q 0 (En [xi zi0 ])−1 zi , and θb` (x, α), ` = 0, 1,
are the estimators of θ` (x, α).
      5To further illustrate, suppose that z = x = [1; x ], with x a scalar random variable, so β(α) =
                                                      1         1
 β0 (α) β1 (α) . In most applications, β1 (α) is the primary object of interest. Setting q = [0 ± 1], BM and
Bontemps, Magnac, and Maurin (2012) give explicit formulae for the upper and lower bound of β1 (α):
                                          cov(x1i , fi )   E [(x1i − E[x1i ]) (θ1i 1 {x1i < E[x1i ]} + θ0i 1 {x1i > E[x1i ]})]
            β 1 (α) =        inf                         =
                        fi ∈[θi0 ,θi1 ]    var(xi1 )                                 E[x21i ] − E[x1i ]2
                                          cov(x1i , fi )   E [(x1i − E[x1i ]) (θ1i 1 {x1i > E[x1i ]} + θ0i 1 {x1i < E[x1i ]})]
            β 1 (α) =        sup                         =
                        fi ∈[θi0 ,θi1 ]    var(xi1 )                                 E[x21i ] − E[x1i ]2
where θ0i = θ0 (xi , α) and θ1i = θ1 (xi , α), see also Stoye (2007).
                 BEST LINEAR APPROXIMATIONS TO SET IDENTIFIED FUNCTIONS                                 7


                                    3. Motivating Examples

3.1. Interval valued data. Analysis of regression with interval valued data has become a
canonical example in the partial identification literature due to the widespread presence of
such data. The Health and Retirement Study is one of the first instances where income data
is collected from respondents in the form of brackets, with degenerate (singleton) intervals
for individuals who opt to fully reveal their income (see, e.g. Juster and Suzman (1995)).
The Occupational Employment Statistics (OES) program at the Bureau of Labor Statistics
collects wage data from employers as intervals, and uses these data to construct estimates
for wage and salary workers in 22 major occupational groups and 801 detailed occupations.
Due to concerns for privacy, public use tax data are recorded as the number of tax payers
which belong to each of a finite number of cells, see e.g. Picketty (2005).6

3.1.1. Interval valued y. BM and Bontemps, Magnac, and Maurin (2012), among others,
have focused on estimation of best linear approximations to conditional expectation func-
tions with interval outcome data. Our framework covers the conditional expectation case,
as well as an extension to quantile regression wherein we set identify β(α) across all quan-
tiles α ∈ A. To avoid redundancy with the related literature, here we describe the setup for
quantile regression. Let the α-th conditional quantile of y|x be denoted Qy (α|x). We are
interested in a linear approximation x0 β(α) to this function. However, we do not observe
y. Instead we observe y0 and y1 , with P (y0 ≤ y ≤ y1 ) = 1. It is immediate that
                            Qy0 (α|x) ≤ Qy (α|x) ≤ Qy1 (α|x) x − a.s.,
where Qy` (α|x) is the α-th conditional quantile of y` |x, ` = 0, 1. Hence, the identification
region B(α) is as in equation (2.1), with θ` (x, α) = Qy` (α|x).

3.1.2. Interval valued x. Consider now inference on functionals of P (y|x), when y is per-
fectly observed but x is only learned to lie in the interval [x0 , x1 ], as P (x0 ≤ x ≤ x1 ) = 1,
with x0 , x1 observed. Our methodology applies to the framework of Manski and Tamer
(2002), extended to the case of quantile regression.7 Following Manski and Tamer, we as-
sume that the conditional expectation (respectively, quantile) of y|x is weakly monotonic
in x, say nondecreasing, and mean independent of x0 , x1 conditional on x (respectively,
Qy (α|x, x0 , x1 ) = Qy (α|x)). Manski and Tamer show that
                           sup E (y|x0 , x1 ) ≤ E (y|x) ≤ inf E (y|x0 , x1 ) ,                      (3.1)
                          x1 ≤x                             x0 ≥x

and similar reasoning yields
                        sup Qy (α|x0 , x1 ) ≤ Qy (α|x) ≤ inf Qy (α|x0 , x1 ) .                      (3.2)
                        x1 ≤x                               x0 ≥x


  6See Manski and Tamer (2002) and Bontemps, Magnac, and Maurin (2012) for more examples.
  7Our approach also applies to the framework of Magnac and Maurin (2008), who study identification
in semi-parametric binary regression models with regressors that are either discrete or measured by inter-
vals, under an uncorrelated error assumption, a conditional independence assumption between error and
interval/discrete valued regressor, and a finite support assumption.
8                CHANDRASEKHAR, CHERNOZHUKOV, MOLINARI, AND SCHRIMPF


Hence, the identification region B(α) corresponding to (3.1) is as in equation (2.1), with
θ0 (x, α) = supx1 ≤x E (y|x0 , x1 ) and θ1 (x, α) = inf x0 ≥x E (y|x0 , x1 ), while the the identifica-
tion region B(α) corresponding to (3.2) is as in equation (2.1), with θ0 (x, α) = supx1 ≤x Qy (α|x0 , x1 )
and θ1 (x, α) = inf x0 ≥x Qy (α|x0 , x1 ) .

3.2. Distribution and duration regression with interval outcome data. Distribu-
tion regression with interval valued data is another application of our method. We consider
models in which the conditional distribution of y|x is given by
                            P (y ≤ α|x) ≡ Fy|x (α|x) = Φ (f (x, α))
where Φ (.) is a known one-to-one link function. A special case of this class of models is
the duration model, wherein we have f (α, x) = g(α) + γ (x), where g (.) is a monotonic
function. As in the quantile regression example, assume that we observe (y0 , y1 , x) with
P (y0 ≤ y ≤ y1 ) = 1. Then
                      Φ−1 Fy1 |x (α|x) ≤ f (x, α) ≤ Φ−1 Fy0 |x (α|x) .
                                                                   

Hence, B(α) is as in equation (2.1), with θ` (x, α) = Fy1−` |x (α|x), ` = 0, 1 and the operator
Φ−1 applied to the bounding functions. A leading example, following Han and Hausman
(1990) and Foresi and Peracchi (1995), takes this form with Φ a probit or logit link function.

3.3. Sample Selection. Sample selection is a commonplace problem in the empirical anal-
ysis of important economic phenomena, including labor force participation, skill composition
of immigrants, returns to education, program evaluation, productivity estimation, insur-
ance, models with occupational choice and financial intermediation (for recent examples,
see respectively Mulligan and Rubinstein (2008), Jasso and Rosenzweig (2008), Card (1999),
Imbens and Wooldridge (2009), Olley and Pakes (1996), Einav, Finkelstein, Ryan, Schrimpf,
and Cullen (2013), Townsend and Urzua (2009)). In Section 5 we use our methodology in
conjunction with the bounding approach of Blundell, Gosling, Ichimura, and Meghir (2007)
to revisit the analysis of Mulligan and Rubinstein (2008), who confront selection in the
context of female labor supply.
  Consider a standard sample selection model. We are interested in the behavior of y
conditional on x; however, we only observe y when u = 1. Manski (1994) shows that the
sharp bounds on the conditional quantile function of y are
                           (                           
                             Qy α−P(u=0|x)
                                   P(u=1|x)    x, u =  1     if α ≥ P(u = 0|x)
              Q0 (α|x) =
                             y0                              otherwise
                           (                        
                                     α
                             Qy P(u=1|x)    x, u = 1      if α ≤ P(u = 1|x)
              Q1 (α|x) =
                             y1                           otherwise
where y0 is the smallest possible value that y can take (possibly −∞) and y1 is the largest
possible value that y can take (possibly +∞). Thus, we obtain
                               Q0 (α|x) ≤ Qy (α|x) ≤ Q1 (α|x) .
and the corresponding set of coefficients of linear approximations to Qy (α|x) is as in equa-
tion (2.1), with θ` (x, α) = Q` (α|x) , ` = 0, 1.
                BEST LINEAR APPROXIMATIONS TO SET IDENTIFIED FUNCTIONS                            9


3.3.1. Alternative Form for the Bounds. As written above, the expressions for Q0 (α|x) and
Q1 (α|x) involve the propensity score, P (u|x) and several different conditional quantiles
of y|u = 1. Estimating these objects might be computationally intensive. We provide a
simplification by working with
             ỹ0 = y1 {u = 1} + y0 1 {u = 0} ,        ỹ1 = y1 {u = 1} + y1 1 {u = 0} .       (3.3)
One can easily verify that Qỹ` (α|x) = Q` (α|x), ` = 0, 1, hence the bounds on the conditional
quantile function can be obtained without calculating the propensity score.

3.3.2. Sample Selection with an Exclusion Restriction. Often when facing selection prob-
lems researchers impose exclusion restrictions. That is, they assume that there are some
components of x that affect P(u = 1|x), but not F (y|x). Availability of such an instrument,
denoted v, can help shrink the bounds on Qy (α|x). For concreteness, we replace x with
(x, v) we assume that F (y|x, v) = F (y|x) ∀v ∈ Vx , with Vx denoting the support of v given
x, and we assume that Vx is finite x − a.s. To simplify notation, we assume Vx = V for all
x, but we note that our approach is valid in the absence of this restriction. Using again
Manski (1994), for each v ∈ V the bounds on the conditional quantile function are:
                           Q0 (α|x, v) ≤ Qy (α|x) ≤ Q1 (α|x, v) ∀v ∈ V,
and therefore
                           sup Q0 (α|x, v) ≤ Qy (α|x) ≤ inf Q1 (α|x, v),
                           v∈V                              v∈V

where Q` (α|x, v), ` = 0, 1, are defined similarly to the previous section with x replaced
by (x, v), and as before we can avoid computing the propensity score by constructing the
variables ỹ` , ` = 0, 1 as in equation (3.3). Then Qỹ` (α|x, v) = Q` (α|x, v), and the set of
coefficients of linear approximations to Qy (α|x) is as in equation (2.1), with θ0 (x, α) =
supv∈V Qỹ0 (α|x, v) and θ1 (x, α) = inf v∈V Qỹ1 (α|x, v).

3.4. Average, Quantile, and Distribution Treatment Effects. Researchers are often
interested in mean, quantile, and distributional treatment effects. Our framework easily
accommodates these examples. Let yiC denote the outcome for person i if she does not
receive treatment, and yiT denote the outcome for person i if she receives treatment. The
methods discussed in the preceding section yield bounds on the conditional quantiles of these
outcomes. In turn, these bounds can be used to obtain bounds on the quantile treatment
effect as follows:
sup QT0 (α|x, v) − inf QC                                           T                 C
                        1 (α|x, v) ≤ Qy T (α|x) − Qy C (α|x) ≤ inf Q1 (α|x, v) − sup Q0 (α|x, v),
v∈V               v∈V                                              v∈V                 v∈Vx

and again the corresponding set of coefficients of linear approximations to QyT (α|x) −
QyC (α|x) is as in equation (2.1), with θ0 (x, α) = supv∈V QT0 (α|x, v) − inf v∈V QC
                                                                                   1 (α|x, v),
and θ1 (x, α) = inf v∈V QT1 (α|x, v) − supv∈Vx QC
                                                0 (α|x, v).
   Analogous bounds apply for the distribution treatment effect and the mean treatment
effect.8
  8Interval regressors can also be accommodated, by adapting the results in Section 3.1.
10                  CHANDRASEKHAR, CHERNOZHUKOV, MOLINARI, AND SCHRIMPF


                                  4. Estimation and Inference

4.1. Overview of the Results. This section provides an overview of our results, and
explains how these can be applied in practice. As described in section 2, our goal is to
estimate the support function, σ(q, α).

4.1.1. Use of asymptotic results. We develop limit theory that allows us to (1) derive the
asymptotic distribution of the support function process; (2) provide inferential procedures;
and (3) establish validity of the Bayesian bootstrap. Bootstrapping is especially important
for practitioners, because of the potential complexity of the covariance functions involved
in the limiting distributions.
                                                                                  √
Limit theory and bootstrap. We show that the support function process Sn (t) := n (b  σ (t) − σ0 (t)),
with t ∈ T := S d−1 × A, is strongly approximated by a Gaussian process on T :
                                       Sn (t) = G [hk (t)] + oP (1)
in   `∞ (T ) ,
            where     `∞ (T )
                           denotes the set of all uniformly bounded real functions on T, k
denotes the number of series terms in our non-parametric estimator of θ` (x, α), ` = 0, 1, and
hk (t) denotes a stochastic process carefully defined in Section 4.3. Here, G [hk (t)] is a tight
P-Brownian bridge with covariance function Ωk (t, t0 ) = E [hk (t) hk (t0 )]−E [hk (t)] E [hk (t0 )].
We show that while the sequence of processes G[hk (t)] may not necessarily converge weakly
when k → ∞, each subsequence has a further subsequence converging to a tight Gaussian
process in `∞ (T ) with a non-degenerate covariance function.
   We show how to conduct inference using the quantiles of the sequence G [hk (t)]. Specif-
ically, if we have a continuous function f that satisfies certain (non-restrictive) conditions
detailed in Section 4.3,9 and b cn (1 − τ ) = cn (1 − τ ) + oP (1) is a consistent estimator of the
(1 − τ )-quantile of f (G [hk (t)]), given by cn (1 − τ ), then
                                    P{f (Sn ) ≤ b
                                                cn (1 − τ )} → 1 − τ.
  Finally, we consider the limiting distribution
                                            √     of the Bayesian bootstrap version of the
support function process, denoted S̃n (t) := n (σ̃(t) − σ
                                                        b(t)) , and show that, conditional on
the data, it admits an approximation
                                      S̃n (t) = G^
                                                 [hk (t)] + oPe (1)
where G^[hk (t)] has the same distribution as G [hk (t)] and is independent of G [hk (t)], and
Pe denotes the probability measure conditional on the data. Since the bootstrap distribution
is asymptotically close to the true distribution of interest, this allows us to perform many
standard and some less standard inferential tasks.
Pointwise asymptotics. Suppose we want to form a confidence interval for q 0 β(α) for some
fixed q and α. Since our estimator can be approximated by a sequence of Gaussian processes,
we know that
                      √
                                                   
                           −bσ (−q, α) + σ0 (−q, α)
                        n                             ≈d N (0, Ωk (q, α)) .
                              b(q, α) − σ0 (q, α)
                              σ
     9For example, functions yielding test statistics based on the directed Hausdorff distance and on the
Hausdorff distance (see, e.g., BM) satisfy these conditions.
                 BEST LINEAR APPROXIMATIONS TO SET IDENTIFIED FUNCTIONS                                   11


To form a confidence interval that covers the bound on q 0 β(α) with probability 1 − τ we
can take
            −bσ (−q, α) + n1/2 C bτ /2 (q, α) ≤ q 0 β(α) ≤ σ b(q, α) + n−1/2 Cb1−τ /2 (q, α)
                                              b1−τ /2 (q, α), are such that if x1 x2 0 ∼ N (0, Ωk (q, α)),
                                                                                        
where the critical values, C
                           bτ /2 (q, α) and C
then                                                             
                   P x1 ≥ C  bτ /2 (q, α) , x2 ≤ C  b1−τ /2 (q, α) = 1 − τ + op (1)

If we had a consistent estimate of Ωk (q, α), we would be able to set
                                       !                    √
                                                         −1 ( 1 − τ )
                                                                      
                         Cbτ /2 (q, α)      1/2       −Φ    √
                        b1−τ /2 (q, α) = Ωk (q, α) Φ−1 ( 1 − τ )
                                          b
                        C
where Φ−1 (·) is the inverse normal distribution function. However, the formula for Ωk (q, α)
is complicated and it can be difficult to estimate. Therefore, we recommend using a Bayesian
bootstrap procedure to estimate the critical values and we provide theoretical justification
for it. See section 4.1.3 for details.10
Functional asymptotics. Because our asymptotic results yield a functional approximation
by Gaussian processes of Sn (q, α) on the entire S d−1 × A, we can also perform inference on
statistics that involve a continuum of values of q and/or α. For example, in our application
to quantile regression with selectively observed data, we might be interested in whether
covariate j has an effect on the outcome distribution. When translated to our best linear
approximation setting, the hypothesis becomes
                              H0 : 0 ∈ [−σ0 (−q, α), σ0 (q, α)] ∀α ∈ A,
with q = ej . When the conditional quantile is in fact linear, rejection of this assumption
implies that xj has a non-zero effect for some α ∈ A. A natural family of test statistics is
          √
    Tn = n sup (1{−b  σ (−q, α) > 0}|bσ (−q, α)|ρ(−q, α) ∨ 1{b
                                                             σ (q, α) < 0}|b
                                                                           σ (q, α)|ρ(q, α))
               α∈A

where ρ(q, α) ≥ 0 is some weighting function which can be chosen to maximize weighted
power against some family of alternatives. There are many values of σ0 (q, α) consis-
tent with the null hypothesis, but the one for which it will√be hardest to control size is
−σ0 (−q, ·) = σ0 (q, ·) = 0. In this case, we know that Sn (t) = nb σ (t), t = (q, α) ∈ T, is well
approximated by the Gaussian process G[hk (t)]. Moreover, the quantiles of any functional
of Sn (t) converge to the quantiles of the same functional applied to G[hk (t)]. Thus, we
could calculate a τ critical value for Tn by repeatedly simulating a realization of G[hk (q, ·)],
computing Tn (G[hk (q, ·)]), and then taking the (1 − τ )-quantile of the simulated values of
Tn (G[hk (q, ·)]). Simulating G[hk (t)], however, requires estimating the covariance function.
As stated above, the formula for this function is complicated and it can be difficult to
estimate. Therefore, we recommend using the Bayesian bootstrap to compute the critical
values. Theorem 4 proves that this bootstrap procedure yields consistent inference. Section
4.1.3 gives a more detailed outline of how to implement this bootstrap. Similar reasoning
  10Instead, if one believes there is some true value q 0 β (α) in the identified set, and one wants to cover
                                                           0
this true value (uniformly) with asymptotic probability 1 − τ , then one can apply the procedures of Imbens
and Manski (2004) and Stoye (2009), as adapted by Bontemps, Magnac, and Maurin (2012).
12              CHANDRASEKHAR, CHERNOZHUKOV, MOLINARI, AND SCHRIMPF


can be used to test hypotheses involving a set of values of q and construct confidence sets
that are uniform in q and/or α.

4.1.2. Estimation. The first step in estimating the support function is to estimate θ0 (x, α)
and θ1 (x, α). Since economic theory often provides even less guidance about the functional
form of these bounding functions than it might about the function of interest, our asymptotic
results are written to accommodate non-parametric estimates of θ0 (x, α) and θ1 (x, α). In
particular, we allow for series estimators of these functions. In this section we briefly review
this approach. Parametric estimation follows as a special case where the number of series
terms is fixed. Note that while the method of series estimation described here satisfies the
conditions of theorems 1 and 2 below, there might be other suitable methods of estimation
for the bounding functions.
  In each of the examples in section 3, except for the case of intersection bounds (e.g.
sample selection with an exclusion restriction), series estimates of the bounding functions
can be formed as follows. Suppose there is a known function of the data for observation i,
denoted yi` , and a known function m(y, θ(x, α), α) such that
                          θ` (·, α) = arg min E [m (yi` , θ(xi , α), α)] ,
                                     θ∈L2 (X,P)

where X denotes    the support of x and L2 (X, P) denotes the space of real-valued functions
g such that X |g(x)|2 dP(x) < ∞. Then we can form an estimate of the function θ` (·, α) by
             R
replacing it with its series expansion and taking the empirical expectation in the equation
above. That is, obtaining the coefficients
                          ϑbk,` (α) = arg min En m yi` , pk (xi )0 ϑ, α ,
                                                                      
                                        ϑ
and setting
                                   θbl (xi , α) = pk (xi )0 ϑbk,` (α).
Here, pk (xi ) is a k × 1 vector of series functions evaluated at xi . These could be any set
of functions that span the space in which θ` (x, α) is contained. Typical examples include
polynomials, splines, and trigonometric functions, see Chen (2007). Both the properties of
m(·) and the choice of approximating functions affect the rate at which k can grow. We
discuss this issue in more detail after stating our regularity conditions in section 4.2.
    With intersection bounds, for concreteness discussed here in the case of sample selec-
tion with an exclusion restriction, one can proceed as follows. First, estimate Qỹ` (α|x, v),
` = 0, 1, using the method described above. Next, set θb1 (xi , α) = minv∈V Qỹ1 (α|x, v) and
θb0 (xi , α) = maxv∈V Qỹ0 (α|x, v), and proceed as in the previous case. We show below, how-
ever, that the set of best linear approximations to the band {φ(·, α) : maxv∈V Qỹ0 (α|x, v) ≤
φ(x, α) ≤ minv∈V Qỹ1 (α|x, v) x − a.s.}, is equal to the intersection over v ∈ V of the sets of
best linear approximations to the bands {φ(·, α) : Qỹ0 (α|x, v) ≤ φ(x, α) ≤ Qỹ1 (α|x, v) x −
a.s.}. Establishing this equivalence allows us to provide a valid and extremely straightfor-
ward, albeit (mildly) conservative, procedure for inference also in this case.

4.1.3. Bayesian Bootstrap. We suggest using the Bayesian Bootstrap to conduct inference.
In particular, we propose the following algorithm.
                 BEST LINEAR APPROXIMATIONS TO SET IDENTIFIED FUNCTIONS                                13


Procedure for Bayesian Bootstrap Estimation of Critical Values.
    (1) Simulate each bootstrap draw of σ̃(q, α) :
         (a) Draw ei ∼ exp(1), i = 1, ..., n, ē = En [ei ]
        (b) Estimate:
                                            he                          i
                                                i
                  ϑ̃k,` (α) = arg min En          m yi` , pk (xi )0 ϑ, α ,
                                  ϑ            ē
                    θ̃` (x, α) = pk (x)0 ϑ̃k,` (α),
                                         he          i−1
                                            i
                              Σ̃ = En         xi zi0     ,
                                           ē
                        w̃i,q0 Σ̃ = θ̃1 (x, α)1(q 0 Σ̃z > 0) + θ̃0 (x, α)1(q 0 Σ̃z ≤ 0),
                                         he                   i
                                            i 0
                     σ̃(q, α) = En            q Σ̃zi w̃i,q0 Σ̃ .
                                           ē
                                                                          (b)  √
    (2) Denote the bootstrap draws as σ̃ (b) , b = 1, ..., B, and let S̃n = n(σ̃ (b) − σ  b). To
        estimate the 1 − τ quantile of f (Sn ), where f is a continuous function determining
        the test statistic of interest as detailed in Section 4.3, use the empirical 1−τ quantile
                            (b)
        of the sample f (S̃n ), b = 1, ..., B
    (3) Confidence intervals for linear combinations of coefficients can be obtained as out-
        lined in Section 4.1.1. Inference on statistics that involve a continuum of values of
        q and/or α can be obtained as outlined in Section 4.1.1.

4.2. Regularity Conditions. In what follows, we state the assumptions that we maintain
to obtain our main results. We then discuss these conditions, and verify them for the
examples in Section 3.
C1 (Smoothness of Covariate Distribution). The covariates zi have a sufficiently smooth
distribution, namely for some 0 < m ≤ 1, we have that P (|q 0 Σzi /kzi k| < δ) /δ m . 1 as
δ & 0 uniformly in q ∈ S d−1 , with d the dimension of x. The matrix Σ = (E[xi zi0 ])−1 is
finite and invertible.
C2 (Linearization for the Estimator of Bounding Functions ). Let θ̄ denote either the
unweighted estimator θb or the weighted estimator θ̃, and let vi = 1 for the case of the
unweighted estimator, and vi = ei for the case of the weighted estimator. We assume that
for each ` = 0, 1 the estimator θ̄` admits a linearization of the form:
             √
                n θ̄` (x, α) − θ` (x, α) = pk (x)0 J`−1 (α)Gn [vi pi ϕi` (α)] + R̄` (x, α)
                                        
                                                                                           (4.1)
where pi = pk (xi ), supα∈A kR̄` (xi , α)kPn ,2 →P 0, and (xi , zi , ϕi` ) are i.i.d. random elements.
C3 (Design Conditions). The score function ϕi` (α) is mean zero conditional on xi , zi and
has uniformly bounded fourth moment conditional on xi , zi . The score function is smooth
                                   h                        i1/2
in mean-quartic sense: E (ϕi` (α) − ϕi` (α̃))4 |xi , zi          ≤ C kα − α̃kγϕ for some constants
C and γϕ > 0. Matrices J` (α) exist and are uniformly Lipschitz over α ∈ A, a bounded
and compact subset of Rl , and supα∈A kJ`−1 (α)k as well as the operator norms of matrices
       0          0                0 2                               E[kzi k6 ] and E[kxi k6 ] are finite.
   i zi ], E[zi pi6], and E[kpi pi k ] are uniformly bounded in k.
E[z
E kθ` (xi , α)k is uniformly bounded in α, and E[|ϕi` (α)|4 xi , zi ] is uniformly bounded in α,
14                CHANDRASEKHAR, CHERNOZHUKOV, MOLINARI, AND SCHRIMPF

                                                                                               γθ
x, and z. The functions θ` (x, α) are smooth, namely |θ` (x,
                                                           α) −4 θ` (x, α̃)| ≤ L(x) kα − α̃k
for some constant γθ > 0 and some function L(x) with E L(x) bounded.
C4 (Growth Restrictions). When k → ∞, supx∈X kpk (x)k ≤ ξk , and the following growth
condition holds on the number of series terms:
                                                 r
   log2 n n−m/4 + (k/n) · log n · max kzi k ∧ ξk
                    p
                                                    max F14 →P 0, ξk2 log2 n/n → 0,
                                             i                   i≤n,

where m is defined in Condition C1 above and F1 is defined in Condition C5 below.
C5 (Complexity of Relevant Function Classes). The function set F1 = {ϕi` (α), α ∈ A, ` =
0, 1} has a square P-integrable envelope F1 and has a uniform covering L2 entropy equivalent
to that of a VC class. The function class F2 ⊇ {θi` (α), α ∈ A, ` = 0, 1} has a square P-
integrable envelope F2 for the case of fixed k and bounded envelope F2 for the case of
increasing k, and has a uniform covering L2 entropy equivalent to that of a VC class.

4.2.1. Discussion and verification of conditions. Assumptions C3 and C5 are common reg-
ularity conditions and they can be verified using standard arguments.
   Condition C1 requires that the covariates zi be continuously distributed, which in turn
assures that the support function is everywhere differentiable in q ∈ S d−1 , see BM (Lemma
A.8) and Lemma 3 in the Appendix. The assumption fails in the presence of discrete
covariates. In this case, the identified set has exposed faces and therefore its support
function is not differentiable in directions q orthogonal to these exposed faces, see e.g.,
Bontemps, Magnac, and Maurin (2012, Section 3.1). When discrete covariates are present,
Condition C1 can be met by adding to each discrete covariate a small amount of smoothly
distributed noise as shown in Appendix B.4, i.e. by using a data jittering method. Adding
noise gives “curvature” to the exposed faces, thereby guaranteeing that the identified set
intersects its supporting hyperplane in a given direction at only one point, and is therefore
differentiable, see Schneider (1993, Corollary 1.7.3). Lemma 8 in the Appendix shows that
the distance between the true identified set and the set resulting from jittered covariates
can be made arbitrarily small. In particular, it can be made less than or equal to a chosen
constant δ > 0 by setting the variance of the smoothly distributed noise as a function of
δ and of the moments of θ` (x, α), ` = 0, 1. One can then obtain a valid confidence set for
the original (non data-jittered) set B(α) by taking the confidence set based on the jittered
support function, and enlarging it by δ. Lemma 8 shows that the conservative bias of this
enlarged confidence set is a function of δ that can be estimated, so that inference can be
made arbitrarily slightly conservative.
   Condition C2 requires the estimates of the bounding functions to be asymptotically linear.
In addition, it requires that the number of series terms grows fast enough for the remainder
term to disappear. This requirement must be reconciled with Condition C4, which limits
the rate at which the number of series terms can increase. We show below how to verify
these two conditions in each of the examples of Section 3.
Example (Mean regression, continued). We begin with the simplest case of mean regression
with interval valued outcome data. In this case, the bounding functions θ` (·) do not depend
on α, and we have θb` (·) = pk (·)0 ϑbk,` with ϑbk,` = (P 0 P )−1 P 0 y` and P = [pk (x1 ), ..., pk (xn )]0 .
                 BEST LINEAR APPROXIMATIONS TO SET IDENTIFIED FUNCTIONS                              15


Let ϑk,` be the coefficients of a projection of E[y` |xi ] on P , or pseudo-true values, so that
ϑk,` = (P 0 P )−1 P 0 E[y` |xi ]. We then have the following linearization for θb` (·)
     √                      √                                      √
       n θb` (x) − θ` (x) = npk (x)(P 0 P )−1 P 0 (y` − E[y` |x]) + n pk (x)0 ϑk,` − θ` (x) .
                                                                                           

                                          0
√ is in 0the form of (4.1) with J` = P P , ϕi` = (yi` − E[y` |xi ]), and R` (x, α) = R` (x) =
This
  n (pk (x) ϑk,` − θ` (x)). The remainder term is simply approximation error. Many results
on the rate of approximation error are available in the literature. This rate depends on the
choice of approximating functions, smoothness of θ` (x), and dimension of x. When using
polynomials as approximating function, if θ` (x) = E[y` |xi ] is s times differentiable with
respect to x, and x is d dimensional, then (see e.g. Newey (1997) or Lorentz (1986))
                               sup |pk (x)0 ϑk,` − θ` (x)| = O(k −s/d ).
                                 x
                                                                                      d
In this case C2 requires that n1/2 k −s/d → 0, or that k grows faster than n 2s . Assumption
C4 limits the rate at which k can grow. This assumption involves ξk and supi |ϕi` |. The
behavior of these terms depends on the choice of approximating functions and some auxiliary
assumptions. With polynomials as approximating functions and the support of x compact
with density bounded away from zero, ξk = O(k). If yi` − E[y` |xi ] has exponential tails,
then supi, |ϕi` | = O(2(log n)1/2 ). In this case, a sufficient condition to meet C4 is that
k = o(n1/3 log−6 n). Thus, we can satisfy both C2 and C4 by setting k ∝ nγ for any
       d 1
γ ∈ 2s   , 3 under the assumption that 3d ≤ 2s. Notice that as usual in semiparametric
problems, we require undersmoothing compared to the rate that minimizes mean-squared
                      d
error, which is γ = d+2s . Also, our assumption requires increasing amounts of smoothness
as the dimension of x increases.

  We now discuss how to satisfy assumptions C2 and C4 more generally. Recall that in our
examples, the series estimates of the bounding functions solve
                         θb` (·, α) = arg min En [m(yi` , θ` (xi , α), α)]
                                       θ` ∈L2 (X,P)

or θb` (·, α) = pk (·)0 ϑbk,` with ϑbk,` = arg minϑ En [m(yi` , pk (xi )0 ϑ, α)] . As above, let ϑk,` be
the solution to ϑk,` = arg minϑ E [m(yi` , pk (xi )0 ϑ, α)] . We show that the linearization in C2
holds by writing
     √                           √                      √
        n θb` (x, α) − θ` (x, α) = npk (x) ϑbk,` − ϑk,` + n pk (x)0 ϑk,` − θ` (x, α) .
                                                                                             
                                                                                                   (4.2)
The first term in (4.2) is estimation error. We can use the results of He and Shao (2000) to
show that                              
                            ϑbk,` − ϑk,` = En J`−1 pi ψi + op (n−1/2 ),
                                                       

where ψ denotes the derivative of m(yi` , pk (xi )0 ϑ, α) with respect to ϑ.
   The second term in (4.2) is approximation error. Standard results from approximation
theory as stated in e.g. Chen (2007) or Newey (1997) give the rate at which the error from the
best L2 -approximation to θ` disappears. When m is a least squares objective function, these
results can be applied directly. In other cases, such as quantile or distribution regression,
further work must be done.
16                  CHANDRASEKHAR, CHERNOZHUKOV, MOLINARI, AND SCHRIMPF


Example (Quantile regression with interval valued data, continued). The results of Belloni,
Chernozhukov, Chetverikov, and Fernandez-Val (2018) can be used to verify our conditions
for quantile regression. The conditions required to apply their results are as follows.
 (Q.1) The data {(yi0 , yi1 , xi ), 1 ≤ i ≤ n} are an i.i.d. sequence of real (2 + d)-vectors, and
       x ∈ X ≡ [0, 1]d .
 (Q.2) For ` ∈ {0, 1}, the conditional density of y` given x is bounded above by f uniformly
       over y` ∈ Y and x ∈ X ; its derivative is continuous and bounded in absolute
                                   0
       value from above by   f uniformly in y` ∈ Y and x ∈ X . The conditional density
       fy` |X Qy` |x (α|x)|x is bounded away from zero uniformly in α ∈ A and x ∈ X .
 (Q.3) For all k, the eigenvalues of E[pi p0i ] are uniformly bounded above and away from
       zero.
 (Q.4) For some C > 0, Qy` |x belongs to the Hölder ball Ω(s, C, X ) for all α ∈ A.11 If the
       approximating functions are polynomials, we require s > d; if the approximating
       functions are B-splines of order s0 , we require (s ∧ s0 ) > d.
 (Q.5) For some a > 0 ξk = O(k a ); k is chosen such that k 3 ξk2 = o(n1− ) and k −b+1 = o(n− )
       for some constant  > 0 and with b = s/d − 1 for polynomials and b = (s ∧ s0 )/d for
       B-splines.
  Under these assumptions, Lemma 1 in Belloni, Chernozhukov, Chetverikov, and Fernandez-
Val (2018) shows that the approximation error satisfies
                                   sup     pk (x)0 ϑk (α) − θ` (x, α) . k −b .
                                x∈X ,α∈A

Theorem 2 of Belloni, Chernozhukov, Chetverikov, and Fernandez-Val (2018) then shows
that C2 holds. Condition C4 also holds because for quantile regression ψi is bounded, so
C4 only requires k 1/2+a (log n)5/2 = o(n1/2 ) (with a = 1 for polynomials and a = 1/2 for
splines), which is implied by Q.5.
Example (Distribution regression, continued). In this example, θ` (x, α) = Fy1−` |x (α|x),
` = 0, 1. As described above, the estimator solves ϑbk` = arg minϑ En [m(yi` , pk (xi )0 ϑ, α)],
with
   m(yi` , pk (xi )0 ϑ, α) = −1{yi` < α} log Φ pk (xi )0 ϑ − 1{yi` ≥ α} log 1 − Φ pk (xi )0 ϑ
                                                                                             

for some known distribution function Φ. We must show that (i) the estimation error,
ϑbk,` − ϑk,` , can be linearized; and that (ii) the approximation error, pk (x)ϑk,` − θ` (x, α),
vanishes at rate o(n−1/2 ). We accomplish this by verifying the conditions of He and Shao
(2000) to show that (ϑbk,` −ϑk,` ) can be linearized, and by verifying the conditions of Hirano,
Imbens, and Ridder (2003) to show that the approximation bias is of the correct order. We
let A0 ⊂ A denote the set of values for α that the researcher is interested in. The conditions
required to apply their results are as follows.
 (D.1) The distribution function Φ is smooth; its associated probability density function φ
       has a bounded derivative.
 (D.2) The data {(yi0 , yi1 , xi ), 1 ≤ i ≤ n} are an i.i.d. sequence of real (2 + d)-vectors.
     11See Belloni, Chernozhukov, Chetverikov, and Fernandez-Val (2018, p. 14) for a formal definition of the
Hölder ball.
                 BEST LINEAR APPROXIMATIONS TO SET IDENTIFIED FUNCTIONS                                 17


 (D.3) X = [0, 1]d . The density function of x is uniformly bounded away from zero on X .
 (D.4) For all k, the eigenvalues of E[pi p0i ] are uniformly bounded above and away from
       zero. For some a > 0, ξk = O(k a ) and k = o((n/ log n)1/2 ).
 (D.5) For each ` ∈ {0, 1} and for some C > 0, θ` (x, α) belongs to the Hölder ball Ω(s, C, X )
       for all α ∈ A, with s/d ≥ 4.
 (D.6) For each ` ∈ {0, 1}, θ` (x, α) is bounded away from 0 and from 1, uniformly in
       α ∈ A0 .
   Adopting the notation of He and Shao (2000), in this example we have that the derivative
of m(yi` , pk (xi )0 ϑ, α) with respect to ϑ is
                                                                   
                                   1{yi` < α}       1{yi` ≥ α}
                                                                      φ pk (xi )0 ϑ pk (xi ).
                                                                                   
            ψ(yi` , xi , ϑ) = −              0
                                                −               0
                                  Φ (pk (xi ) ϑ) 1 − Φ (pk (xi ) ϑ)
Because for each ` ∈ {0, 1} m(yi` , pk (xi )0 ϑ, α) is a smooth function of ϑ, En ψ(yi` , xi , ϑbk,` ) =
0, and conditions C.0 and C.2 in He and Shao (2000) hold. By condition D.1, ψ is Lipschitz
in ϑ, and we have the bound
                                    kηi` (ϑ, τ )k2 . kpk (xi )k2 kτ − ϑk2 ,
where ηi` (ϑ, τ ) = ψ(yi` , xi , ϑ) − ψ(yi` , xi , τ ) − Eψ(yi` , xi , ϑ) + Eψ(yi` , xi , τ ). By condition
D.4, which is satisfied by polynomials with a = 1 or by splines with a = 1/2, condition C.1
in He and Shao (2000) holds. Differentiability of φ and our assumption C3 are sufficient
for C.3 in He and Shao (2000). Finally, conditions C.4 and C.5 in He and Shao (2000) hold
with A(n, k) = k because
                                                   2               2
                                  s0 ηi` (ϑ, τ )       . s0 pk (xi ) kτ − ϑk2 ,
and E |s0 pk (xi )|2 is uniformly bounded for s ∈ S k−1 := {q ∈ Rk : kqk = 1} for all k when
                   

the series functions are orthonormal. Applying Theorem       2.2 of He and Shao (2000), we
                                                    1/2
                                                        
obtain the desired linearization if k = o (n/ log n)      .
  The results of Hirano, Imbens, and Ridder (2003) can be used to show that the approx-
imation bias is sufficiently small. All conditions required by Lemma 1 in Hirano, Imbens,
and Ridder (2003) are satisfied by our set of conditions D.1-D.6, and therefore for Φ given
by the logistic distribution we have for each ` ∈ {0, 1}
                        sup      |Φ (pk (x)ϑk,` ) − Φ (θ` (x, α))| =O(k −s/(2d) ξk ),
                      x∈X ,α∈A

which implies that
                              sup     |pk (x)ϑk,` − θ` (x, α)| =O(k −s/(2d) ξk ).
                          x∈X ,α∈A

This result is only for the logistic link function, but it can easily be adapted for any link
function with first derivative bounded from above and second derivative bounded away from
zero. We need the approximation error to be o(n−1/2 ). For this, it suffices to have
                                           k −s/(2d) ξk n1/2 = o(1).
                                                                         d
Given condition D.4, it suffices to have k ∝ nγ for γ >                s−2ad .
18                    CHANDRASEKHAR, CHERNOZHUKOV, MOLINARI, AND SCHRIMPF
                                                                                                             
                                                                                                    d     1
     To summarize, condition C2 can be met by having k ∝ nγ for any γ ∈                           s−2ad , 2       , and, as
                                                                                                                       1
in the mean and quantile regression examples above, condition C4 will be met if γ <                                  1+2a .

4.3. Theoretical Results. In order to state the result we define
                         hik (t) :=         q 0 ΣE[zi p0i 1{q 0 Σzi > 0}]J1−1 (α)pi ϕi1 (α)
                                        + q 0 ΣE[zi p0i 1{q 0 Σzi < 0}]J0−1 (α)pi ϕi0 (α)
                                        − q 0 Σxi zi0 ΣE zi wi,q0 Σ (α)
                                                                      

                                        + q 0 Σzi wi,q0 Σ (α).
We Pbegin by providing limit theory results for the support function process. Below, Gn [hk (t)] :=
√1    n
  n   i=1 (hik (t) − Ehk (t)).

Theorem 1 (Limit Theory     for Support Function Process). The support function process
           √
Sn (t) = n σ    bθ,
                 bΣ b − σθ,Σ (t), where t = (q, α) ∈ T , admits the approximation Sn (t) =
Gn [hk (t)]+oP (1) in `∞ (T ). Moreover, the support function process admits an approximation
                                      Sn (t) = G[hk (t)] + oP (1) in `∞ (T ),
where the process G[hk (t)] is a tight P-Brownian bridge in `∞ (T ) with covariance function
Ωk (t, t0 ) = E[hk (t)hk (t0 )] − E[hk (t)]E[hk (t0 )] that is uniformly Holder on T × T uniformly
in k, and is uniformly non-degenerate in k. These bridges are stochastically equicontinuous
with respect to the L2 pseudo-metric ρ2 (t, t0 ) = [E[h(t) − h(t0 )]2 ]1/2 . kt − t0 kc for some
c > 0 uniformly in k. The sequence G[hk (t)] does not necessarily converge weakly under
k → ∞; however, each subsequence has a further convergent subsequence converging to a
tight Gaussian process in `∞ (T ) with a non-degenerate covariance function. Furthermore,
the canonical distance between the law of the support function process Sn (t) and the law of
G[hk (t)] in `∞ (T ) approaches zero, namely supg∈BL1 (`∞ (T ),[0,1]) |E[g(Sn )]−E[g(G[hk ])]| → 0.

  Our second step is to provide inference results for various statistics based on the support
function process. Formally, we consider these statistics as mappings f : `∞ (T ) → R from
the possible values s of the support function process Sn to the real line. Examples include:
        •   a   support function evaluated at t ∈ T, f (s) = s(t),
        •   a   Kolmogorov statistic, f (s) = supt∈T0 |s(t)|/$(t),
        •   a                                        R supt∈T0 {−s(t)}+ /$(t),
                directed Kolmogorov statistic, f (s) =
        •   a   Cramer-Von-Mises statistic, f (s) = T s2 (t)/$(t)dν(t),
where T0 is a subset of T , $ is a continuous and uniformly positive weighting function, and ν
is a probability measure over T whose support is T .12 Allowing for a weighting function $ is
important because, as shown in Chernozhukov, Kocatulum, and Menzel (2015), it enforces
either exact or first-order equivariance of the statistics to transformations of parameters,
yielding more powerful inference. More generally we can consider any continuous function
f such that f (Z) (a) has a continuous distribution function when Z is a tight Gaussian
process with non-degenerate covariance function and (b) f (ζn + c) − f (ζn ) = o(1) for any
     12Observe that test statistics based on the (directed) Hausdorff distance (see, e.g., BM) are special cases
of the (directed) Kolmogorov statistics above.
                BEST LINEAR APPROXIMATIONS TO SET IDENTIFIED FUNCTIONS                           19


c = o(1) and any kζn k = O(1). Denote the class of such functions Fc and note that the
examples mentioned above belong to this class by the results of Davydov, Lifshits, and
Smorodina (1998).
Theorem 2 (Limit Inference on Support Function Process). For any b
                                                                 cn = cn + oP (1) and
cn = OP (1) and f ∈ Fc we have
                            P{f (Sn ) ≤ b
                                        cn } − P{f (G[hk ]) ≤ cn } → 0.
If cn (1 − τ ) is the (1 − τ )-quantile of f (G[hk ]) and b
                                                          cn (1 − τ ) = cn (1 − τ ) + oP (1) is any
consistent estimate of this quantile, then
                                 P{f (Sn ) ≤ b
                                             cn (1 − τ )} → 1 − τ.

  Next we provide limit theory results for the bootstrap support function process. Let Pe
denote the probability measure conditional on the data, eoi = ei − 1, and hok = hk − E[hk ].
Theorem 3 (Limit Theory for the Bootstrap Support Function Process). The bootstrap
                                   √
support function process S̃n (t) = n(e  σθ,
                                         eΣ e − σ
                                                bθ,
                                                  bΣb )(t), where t = (q, α) ∈ T , admits the
following approximation conditional on the data: Sen (t) = Gn [eoi hok (t)] + oPe (1) in `∞ (T )
in probability P. Moreover, the bootstrap support function process admits an approximation
conditional on the data:
                                ^                      ∞
                     Sen (t) = G[hk (t)] + oPe (1) in ` (T ), in probability P,

where G[h^                                                    ∞
            k ] is a sequence of tight P-Brownian bridges in ` (T ) with the same distribu-
tions as the processes G[hk ] defined in Theorem 1, and independent of G[hk ]. Further-
more, the canonical distance between the law of the bootstrap support function process
Sen (t) conditional on the data and the law of G[hk (t)] in `∞ (T ) approaches zero, namely
supg∈BL1 (`∞ (T ),[0,1]) |EPe [g(Sen )] − E[g(G[hk ])]| →P 0.

  We conclude by establishing that inference can be carried out using critical values esti-
mated using our proposed Bayesian bootstrap procedure.
Theorem 4 (Bootstrap Inference on the Support Function Process). For any cn = OP (1)
and f ∈ Fc we have
                     P{f (Sn ) ≤ cn } − Pe {f (Sen ) ≤ cn } →P 0.
                  cn (1 − τ ) is the (1 − τ )-quantile of f (Sen ) under Pe , then
In particular, if e
                                P{f (Sn ) ≤ e
                                            cn (1 − τ )} →P 1 − τ.

4.4. Intersection bounds. Suppose we are interested in a function θ : X → R with bounds
of the form
                           θ0 (x, α, v) ≤ θ(x, α) ≤ θ1 (x, α, v) ∀v ∈ Vx .
Our example of quantile regression with selection and an exclusion restriction has this form.
Our results can easily be applied to this case.
20                     CHANDRASEKHAR, CHERNOZHUKOV, MOLINARI, AND SCHRIMPF


  First, note that taking the intersection over v and then forming the set of best linear
approximations is the same as forming the set of best linear approximations for each v, and
then intersecting the sets of best linear approximations. To see this, let
                                                                                            
                                     0
B(α) = β ∈ arg min E[φ(x, α) − x b] : θ0 (x, α, v) ≤ φ(x, α) ≤ θ1 (x, α, v) x − a.s. ∀v ∈ Vx
                  b
                                                                                            
                                     0
      = β ∈ arg min E[φ(x, α) − x b] : sup θ0 (x, v, α) ≤ φ(x, α) ≤ inf θ1 (x, v, α) x − a.s. ,
                        b                       v∈Vx                               v∈Vx
                                                                                                                   (4.3)
and let
                                                                                                                  
                                                 0
     B(α, v) =        β ∈ arg min E[φ(x, α) − x b] : θ0 (x, α, v) ≤ φ(x, α) ≤ θ1 (x, α, v) x − a.s. ,
                              b

Recall that we are assuming without loss of generality that Vx = V for all x,13 and that V
is finite. It is then trivial to verify that B(α) = ∩v∈V B(α, v) because all sets B(α, v) are
parallel shifts of each other. Additionally, if σ(q, α) is the support function of B(α) and
σ(q, α, v) is the support function of B(α, v), then
                                                                 
                                                   P
                             σ(q, α) = P inf          σ(qv , α, v) ,
                                                v∈V
                                                      qv =q   v∈V

see Rockafellar (1970, Corollary 16.4.1).
  To apply theorems 1-4, assume conditions C1-C5 hold for each v ∈ V, with (x, v) ∈ X × V
replacing x ∈ X in each
                      √ P  condition. Then denoting by qV := [qv , v ∈ V], theorems 1-4 apply
to Sn (qV , α, V) := n        v∈V (bσ (qv , α, v) − σ(qv , α, v)) . On the other hand, the support
function process for σ  b(q, α) is
                              √
                                                                                    
                                             P                       P
                 Sn (q, α) = n        inf        b(qv , α, v) − inf
                                                 σ                       σ(qv , α, v) ,
                                       qV ∈QV v∈V                    qV ∈QV v∈V
                                  P
where QV := {qv , v ∈ V : v∈V qv = q}. Note that
        √ P                                                                        √
                                                                                                                                
                                                                                               P
− sup     n       σ (qv , α, v) − σ(qv , α, v)) ≤ Sn (q, α) ≤ sup
                 (b                                                                    n              σ (qv , α, v) − σ(qv , α, v))
                                                                                                     (b
     qV ∈QV       v∈V                                                     qV ∈QV               v∈V
This inequality along with theorems 2 and 4 allows for conservative inference for many
statistics of interest. For example,
   (                      )     (                                                      )
                                            √ P
                                                                                 
 P     sup |Sn (t)| ≤ cn ≥ P        sup sup  n       σ (qv , α, v) − σ(qv , α, v)) ≤ cn ,
                                                    (b
       (q,α)∈T0                          (q,α)∈T0 qV ∈QV            v∈V

and since f : `∞ (T × V) → R defined by f (Sn ) = sup(q,α)∈T0 supqV ∈QV |Sn (qV , α, V)| is in
Fc , theorems 2 and 4 apply.
   This procedure will generally be conservative because it treats uncertainty in σ
                                                                                  b(q, v, α)
for v far from the minimum as though it is as relevant as uncertainty in σ  b(q, v, α) near
its minimum. More powerful inference can be achieved by employing either the precision
     13This is without loss of generality because we can assign θ (x, v) = −∞ and θ (x, v) = ∞ for v ∈
                                                                                                     / Vx ,
                                                                 0                 1
and this change does not affect B(α).
               BEST LINEAR APPROXIMATIONS TO SET IDENTIFIED FUNCTIONS                        21


correction procedure of Chernozhukov, Lee, and Rosen (2013) or the generalized moment
selection approach of Andrews and Shi (2013).
   In the special case that v0∗ is the unique maximizer of θ0 (x, v, α) and v1∗ is the unique
minimizer of θ1 (x, v, α) over v ∈ V, we show in Lemma 7 in the Appendix that condition
C2 is satisfied by an estimator of the bounding functions equal to the maximum (minimum)
of pk (x, v)0 ϑbxv
                k,` over v ∈ V. We then have that all our conditions apply, and exact inference
can be carried out working with the set B(α) as defined in (4.3).

               5. Application: the gender wage gap and selection

   An important question in labor economics is whether the gender wage gap is shrink-
ing over time. Blau and Kahn (1997) and Card and DiNardo (2002), among others, have
noted the coincidence between a rise in within-gender inequality and a fall in the gender
wage gap over the last 40 years. Mulligan and Rubinstein (2008) observe that the growing
wage inequality within gender should induce females to invest more in productivity. In
turn, able females should differentially be pulled into the workforce. Motivated by this
observation, they use Heckman’s two-step estimator on repeated Current Population Sur-
vey cross-sections in order to compute relative wages for women since 1970, holding skill
composition constant. They find that in the 1970s selection into the female workforce was
negative, while in the 1990s it was positive. Moreover, they argue that the majority of
the reduction in the gender gap can be attributed to the changes in the female workforce
composition. In particular, the OLS estimates of the log-wage gap suggest that it has fallen
from -0.419 in the 1970s to -0.256 in the 1990s, while the Heckman two step estimates
suggest that once one controls for skill composition, the wage gap is -0.379 in the 1970s
and -0.358 in the 1990s. Based on these results, Mulligan and Rubinstein (2008) conclude
that the wage gap has not shrunk over the last 40 years. Rather, the behavior of the OLS
estimates can be explained by a switch from negative to positive selection into female labor
force participation. See Blau and Kahn (2017) for a thorough review of this literature.
   In what follows, we address the same question as Mulligan and Rubinstein (2008), using
the same data, the same variables and the same instruments as in their original study.
However, we use our method to estimate bounds on the quantile gender wage gap without
assuming a parametric form of selection or a strong exclusion restriction. We compare
conditional quantiles that ignore the selection effect, with the bounds on these quantiles
that one obtains when taking selection into account. Our results indicate the presence of a
marked gender wage gap at quantiles below the 0.4, especially for the subsample of lower
educated and of married individuals. However, we are unable to reject that the gender wage
gap declined over the period in question. This suggests that the instruments may not be
sufficiently strong to yield tight bounds and that there may not be enough information in
the data to conclude that the gender gap has or has not declined from 1975 to 1999 without
strong functional form assumptions.

5.1. Setup. The Mulligan and Rubinstein (2008) setup relates log-wage to covariates in a
linear model as follows:
                                 log w = x0 β + ε,
22                CHANDRASEKHAR, CHERNOZHUKOV, MOLINARI, AND SCHRIMPF

                              Table 1. Gender wage gap estimates

                               OLS    2-step QR(0.5)   Low    High
                   1975-1979  -0.408 -0.360   -0.522  -1.242  0.588
                             (0.003) (0.013) (0.003) (0.016) (0.061)
                   1995-1999 -0.268 -0.379    -0.355  -0.623  0.014
                             (0.003) (0.013) (0.003) (0.012) (0.010)
Estimates of the gender wage gap (female − male) conditional on having average characteristics. Column
1: OLS estimates of the average gender gap. Column 2: Heckman two-step estimates. Column 3: quantile
regression estimates of the median gender wage gap ignoring selection. Columns 4-5: estimates of bounds on
the median wage gap that account for selection. Standard errors are shown in parentheses (for the bounds,
these were calculated using the Bayesian bootstrap described in Section 4.1.3).


wherein x includes marital status, years of education, potential experience, potential ex-
perience squared, and region dummies, as well as their interactions with an indicator for
gender which takes the value 1 if the individual is female, and zero otherwise. They model
selection as in the following equation:
                                    u = 1 z0γ + η > 0 ,
                                         

where z = [x z̃], z̃ is marital status interacted with indicators for having zero, one, two, or
more than two children, and η is the unobservable.
  For each quantile, we estimate bounds for the gender wage gap utilizing our method.
The bound equations we use are given by θ` (x, α, v) = pk (x, v)0 ϑxvk,` (α), where pk (x, v) =
          
 x v w , v are indicators for the number of children, and w consists of years of education
squared, potential experience cubed, and education × potential experience, and v interacted
with marital status. In our data, the maximizer (minimizer) over x ∈ V of the lower (upper)
bounding function is unique, therefore exact inference can be carried out as described at
the end of Section 4.4. Hence, after taking the intersection of the bounds over the excluded
variables v, our estimated bounding functions are simply the minimum or maximum over
v of pk (x, v)0 ϑxv
                 k,` (α).

5.2. Results. Let x̄f be a female with average (unconditional on gender or year) character-
istics and x̄m be a male with average (unconditional on gender or year) characteristics. In
what follows, we report the predicted gender wage gap for someone with average character-
istics, (x̄f − x̄m ) β(α). The first two columns of Table 1 reproduce the results of Mulligan
and Rubinstein (2008). The first column shows the gender wage gap estimated by ordinary
least squares. The second column shows estimates from Heckman’s two-step selection cor-
rection. The OLS estimates show a decrease in the wage gap, while the Heckman selection
estimates show no change. The third column shows estimates of the median gender wage
gap from quantile regression. Like OLS, quantile regression shows a decrease in the gender
wage gap. The final two columns show bounds on the median gender wage gap that account
for selection. The bounds are wide, especially in the 1970s, and cover zero, indicating that
the data alone do not contain sufficient information to sign the median gender wage gap.
   Figure 5.1 shows the estimated quantile gender wage gaps in the 1970s and 1990s. The
solid black line shows the quantile gender wage gap when selection is ignored. In both the
                                   BEST LINEAR APPROXIMATIONS TO SET IDENTIFIED FUNCTIONS                                                                          23



                                           Figure 5.1. Bounds at Quantiles for full sample
                                         1975-1979                                  1995-1999
                1.5                                                                               1.5
                                                                  QR                                                                              QR
                                                                 Low                                                                             Low
                  1                                             High                                1                                           High
                                                             Low CB                                                                          Low CB
                                                             High CB                                                                         High CB
                0.5                                                                               0.5
 log wage gap




                                                                                   log wage gap
                  0                                                                                 0

                -0.5                                                                              -0.5

                 -1                                                                                -1

                -1.5                                                                              -1.5

                 -2                                                                                -2

                       0.1   0.2   0.3     0.4   0.5   0.6    0.7      0.8   0.9                         0.1   0.2   0.3   0.4   0.5   0.6    0.7      0.8   0.9
                                                 τ                                                                               τ


Estimated quantile gender wage gap (female − male) conditional on having average characteristics. The solid
black line shows the quantile gender wage gap when selection is ignored. The blue and red lines with upward
and downward pointing triangles show upper and lower bounds that account for employment selection for
females. The dashed lines represent a uniform 90% confidence region for the bounds.



1970s and 1990s, the gender wage gap is larger for lower quantiles. At all quantiles the
gap in the 1990s is about 40% smaller than in the 1970s. However, this result should be
interpreted with caution because it ignores selection into the labor force.
   The blue line with downward pointing triangles and the red line with upward pointing
triangles show our estimated bounds on the gender wage gap after accounting for selection.
The dashed lines represent a uniform 90% confidence region. In both the 1970s and 1990s,
the upper bound lies below zero for low quantiles, roughly up to the 0.4. This means that
the low quantiles of the distribution of wages conditional on having average characteristics
are lower for women than for men. This difference exists even if we allow for the most
extreme form of selection (subject to our exclusion restriction) into the labor force for
women, thereby yielding a very robust result. For quantiles at or above the median, our
estimated upper bound lies above zero and our lower bound lies below zero. Thus, high
quantiles of the distribution of wages conditional on average characteristics could be either
higher or lower for women than for men, depending on the true pattern of selection.
  The bounds in Figure 5.1 are tighter for the 1990s than for the 1970s. This reflects higher
female labor force participation in the 1990s. For all quantiles, there is considerable overlap
between the identification region in the 1970s and in the 1990s. Therefore, we conclude
that the data does not contain enough information to sign the changes in the gender wage
gap over time without additional assumptions.
  To further explore what can be learned about the change over time in the gender wage
gap of specific demographic groups, Figures C.1-C.4 in Appendix C show the estimated
quantile bounds conditional on being in these subgroups. That is, rather than reporting
the gender wage gap for someone with average characteristics, these figures show the gender
24              CHANDRASEKHAR, CHERNOZHUKOV, MOLINARI, AND SCHRIMPF


wage gap for someone with average subgroup characteristics (e.g., unconditional on gender
or year, but conditional on subgroup: marital status and education level). To generate these
figures, the entire model was re-estimated using only observations within each subgroup.
   Figures C.1 and C.2 show the results for singles and people with at least 16 years of
education. The results are broadly similar to the results for the full sample. There is robust
evidence of a gap at low quantiles, although it is only marginally significant for the highly
educated in the 1990s. As expected, the bounds are tighter than the full sample bounds
because these subgroups have higher labor force participation. For comparison, Figure C.3
shows the results for people with no more than a high school degree, and Figure C.4 shows
results for singles with at least a college degree. Across all subgroups, the bounds on the
gender wage gap at higher quantiles and on the change in the gap over time continue to be
wide, suggesting the need to augment the model with additional assumptions.

5.2.1. With restrictions on selection. Blundell, Gosling, Ichimura, and Meghir (2007) previ-
ously studied changes in the distribution of wages in the UK. They perform partial identifi-
cation analysis by estimating quantile bounds that account for selection. Similarly to what
we have reported, Blundell, Gosling, Ichimura, and Meghir (2007) find that the estimated
bounds are quite wide. As a result, they explore various restrictions to tighten the bounds.
One such restriction posits that the distribution of wages for the employed stochastically
dominates the distribution of wages for those not working. This implies that the observed
quantiles of wages conditional on employment are an upper bound for the quantiles of wages
not conditional on employment.
   Figure 5.2 shows the results for the full sample of our approach imposing stochastic
dominance, while Figure C.5 in Appendix C shows the results for the subsample of highly
educated singles. Stochastic dominance implies that the upper bound on each quantile
coincides with the corresponding quantile regression estimate that ignores selection. The
assumption has substantial identification power in our data. It yields strong evidence of
a gender wage gap at all quantiles in both the 1970s and 1990s, for both the full sample
and the subsample of highly educated singles. Moreover, the assumption has sufficient
identifying power when combined with our data to yield informative results on the change
in the gender wage gap over time. Figure 5.3 shows the estimated change, both for the full
sample and for the subsample of highly educated singles. For the full sample, the estimated
bounds include zero at low and moderate quantiles. At the 0.8 and higher quantiles, there is
evidence that the gender wage gap decreased by approximately 0.15 log dollars. For highly
educated singles, there is evidence that the gender wage gap did not increase at the 0.7 and
higher quantiles, although –in part due to the smaller size of the subgroup– the confidence
bands are relatively wide and hence consistent with an increase.
   While these results allow one to draw some strong conclusions, the assumption of positive
selection into employment is not innocuous. It may be violated if there is a strong positive
correlation between potential wages and reservation wages. This may be the case if there
is positive assortative matching in the marriage market. Women with high potential wages
could marry men with high wages, making these high potential wage women less likely to
work. Also, the conclusion of Mulligan and Rubinstein (2008) that there was a switch from
adverse selection into the labor market in the 1970s to advantageous selection in the 1990s
                                   BEST LINEAR APPROXIMATIONS TO SET IDENTIFIED FUNCTIONS                                                                        25



                   Figure 5.2. Quantile bounds for full sample imposing stochastic dominance
                            1975-1979                                   1995-1999
                1.5                                                                             1.5
                                                                QR                                                                              QR
                                                               Low                                                                             Low
                  1                                           High                                1                                           High
                                                           Low CB                                                                          Low CB
                                                           High CB                                                                         High CB
                0.5                                                                             0.5
 log wage gap




                                                                                 log wage gap
                  0                                                                               0

                -0.5                                                                            -0.5

                 -1                                                                              -1

                -1.5                                                                            -1.5

                 -2                                                                              -2

                       0.1   0.2   0.3   0.4   0.5   0.6    0.7      0.8   0.9                         0.1   0.2   0.3   0.4   0.5   0.6    0.7      0.8   0.9
                                               τ                                                                               τ


Estimated quantile gender wage (female − male) conditional on average characteristics. The solid black line
shows the quantile gender wage when selection is ignored. The blue and red lines with upward and downward
pointing triangles show upper and lower bounds that account for employment selection for females. The
dashed lines represent a uniform 90% confidence region for the bounds.


                        Figure 5.3. Quantile bounds for the change in the gender wage gap impos-
                        ing stochastic dominance
                                 Full sample                    Single and ≥ 16 years education
                  2                                             QR                                2                                             QR
                                                               Low                                                                             Low
                                                              High                                                                            High
                1.5                                        Low CB                               1.5                                        Low CB
                                                           High CB                                                                         High CB
                  1                                                                               1

                0.5                                                                             0.5
 log wage gap




                                                                                 log wage gap




                  0                                                                               0

                -0.5                                                                            -0.5

                 -1                                                                              -1

                -1.5                                                                            -1.5

                 -2                                                                              -2

                       0.1   0.2   0.3   0.4   0.5   0.6    0.7      0.8   0.9                         0.1   0.2   0.3   0.4   0.5   0.6    0.7      0.8   0.9
                                               τ                                                                               τ


Estimated change (1990s − 1970s) in the quantile gender wage gap (female − male) conditional on having
average characteristics. The solid black line shows the quantile gender wage gap when selection is ignored.
The blue and red lines with upward and downward pointing triangles show upper and lower bounds that
account for employment selection for females. The dashed lines represent a uniform 90% confidence region
for the bounds.
26              CHANDRASEKHAR, CHERNOZHUKOV, MOLINARI, AND SCHRIMPF


implies that stochastic dominance did not hold in the 1970s. Accordingly, we also explore
some weaker restrictions. Blundell, Gosling, Ichimura, and Meghir (2007) propose a median
restriction — that the median wage offer for those not working is less than or equal to the
median observed wage. This restriction implies the following bounds on the distribution of
wages
            F (y|x, u = 1)P(u = 1|x) + 1{y ≥ Qy (0.5|x, u = 1)}0.5P(u = 0|x) ≤
                             ≤ F (y|x) ≤ F (y|x, u = 1)P(u = 1|x) + P(u = 0|x),
where y is wage and u = 1 indicates employment. Transforming these into bounds on the
conditional quantiles yields
                              Q0 (α|x) ≤ Qy (α|x) ≤ Q1 (α|x) ,
where
                            (                     
                                α−P(u=0|x)
                             Qy  P(u=1|x)  x, u = 1         if α ≥ P(u = 0|x),
               Q0 (α|x) =
                             y0                             otherwise,
and
                                        
                            α
                    Q
                    y  P(u=1|x)
                   
                                 x, u = 1              if α < 0.5 & α ≤ P(u = 1|x),
                                             
         Q1 (α|x) = Qy α−0.5P(u=0|x) x, u = 1           if α ≥ 0.5 & α ≤   1+P(u=1|x)
                                                                                      ,
                          P(u=1|x)                                            2
                   
                    y1                                  otherwise.
                   

As above, we can also express Q0 (α|x) and Q1 (α|x) as the α conditional quantiles of ỹ0 and
ỹ1 where
                                  ỹ0 =y1 {u = 1} + y0 1 {u = 0}
and
                (
                 y1 {u = 1} + y1 1 {u = 0}                     with probability 0.5,
          ỹ1 =
                 y1 {u = 1} + Qy (0.5|x, u = 1)1 {u = 0}       with probability 0.5.
We can easily generalize this median restriction by assuming the α1 quantile of wages
conditional on working is greater than or equal to the α0 quantile of wages conditional on
not working. In that case, the bounds can still be expressed as α conditional quantiles of
ỹ0 and ỹ1 with ỹ0 as defined above and
                (
                 y1 {u = 1} + y1 1 {u = 0}               with probability (1 − α0 ),
          ỹ1 =
                 y1 {u = 1} + Qy (α1 |x, u = 1)1 {u = 0} with probability α0 .
We can even impose a set of these restrictions for (α1 , α0 ) ∈ R ⊆ A × A. Stochastic
dominance is equivalent to imposing this restriction for α1 = α0 for all α1 ∈ [0, 1].
   Figure 5.4 shows estimates of the gender wage gap with the median restriction for the
full sample. Figure C.6 in Appendix C reports the results for the subsample of highly
educated singles. The median restriction has substantial identifying power for the presence
of a gender wage gap both in the 1970s and in the 1990s, yielding strong evidence of a
gender wage gap up to the 0.7 (or slightly higher) quantiles in both the 1970s and 1990s.
In comparison, in the absence of this restriction we have strong evidence of a gender wage
                                   BEST LINEAR APPROXIMATIONS TO SET IDENTIFIED FUNCTIONS                                                                        27



                        Figure 5.4. Quantile bounds for full sample imposing the median restric-
                        tion
                                1975-1979                                   1995-1999
                1.5                                                                             1.5
                                                                QR                                                                              QR
                                                               Low                                                                             Low
                  1                                           High                                1                                           High
                                                           Low CB                                                                          Low CB
                                                           High CB                                                                         High CB
                0.5                                                                             0.5
 log wage gap




                                                                                 log wage gap
                  0                                                                               0

                -0.5                                                                            -0.5

                 -1                                                                              -1

                -1.5                                                                            -1.5

                 -2                                                                              -2

                       0.1   0.2   0.3   0.4   0.5   0.6    0.7      0.8   0.9                         0.1   0.2   0.3   0.4   0.5   0.6    0.7      0.8   0.9
                                               τ                                                                               τ


Estimated quantile gender wage (female − male) conditional on average characteristics. The solid black line
shows the quantile gender wage when selection is ignored. The blue and red lines with upward and downward
pointing triangles show upper and lower bounds that account for employment selection for females. The
dashed lines represent a uniform 90% confidence region for the bounds.


gap only up to the 0.4 quantiles. However, under this weaker restriction there is substantial
overlap in the identification regions between the two periods, hence it is not possible to
draw strong conclusions about the change in the gender wage gap.

                                                                     6. Conclusion

    This paper provides a novel method for inference on best linear approximations to func-
tions which are known to lie within a band. It advances the literature by allowing for
bounding functions that may be estimated parametrically or non-parametrically by series
estimators, and that may carry an index. Our focus on best linear approximations is moti-
vated by the difficulty to work directly with the sharp identification region of the functions
of interest, especially when the analysis is conditioned upon a large number of covariates.
By contrast, best linear approximations are tractable and easy to interpret. The sharp iden-
tification region of the parameters characterizing the best linear approximation is convex,
hence can be equivalently described via its support function. The latter can be estimated
with a plug-in method that replaces moments of the data with their sample analogs and
the bounding functions with their estimators. We show that the support function process is
strongly approximated by a Gaussian process. While this process may not converge weakly
as the number of series terms increases to infinity, each subsequence contains a further sub-
sequence that converges weakly to a tight Gaussian process with a uniformly equicontinuous
and non-degenerate covariance function. We establish validity of the Bayesian bootstrap
for practical inference, and verify our regularity conditions for a large number of empirically
relevant problems.
28                CHANDRASEKHAR, CHERNOZHUKOV, MOLINARI, AND SCHRIMPF


                                         Appendix A. Notation

                              n                o
                S d−1    :   = q ∈ Rd : kqk = 1 ;
                   T     :   = S d−1 × A
                                    n
                             = √1n
                                   P
            Gn [h(t)]    :            (hi (t) − Eh(t));
                                     i=1
               ^
       G[hk ], G[hk]     :   = P-Brownian bridge processes, independent of each other, and
                               with identical distributions;
                                                Z                   
                                                           2
           L2 (X, P)     :   = g : X −→ R s.t.       |g(x)| dP(x) < ∞ ;
                                                          X
            `∞ (T )      : set of all uniformly bounded real functions on T ;
BL1 (`∞ (T ), [0, 1])    : set of real functions on `∞ (T ) with Lipschitz norm bounded by 1;
                         . left side bounded by a constant times the right side;
                  fo     : = f − Ef.

                              Appendix B. Proof of the Results

     Throughout this Appendix, we impose Conditions C1-C5.

B.1. Proof of Theorems 1 and 2.
Step 1. We can write the difference between the estimated and true support function as
the sum of three differences.
                                                             
               σ
               bθ,
                 bΣb − σ θ,Σ =   σ
                                 bθ,
                                   bΣb − σ
                                         b θ,Σ
                                             b  +  σ
                                                   b θ,Σ
                                                       b − σ
                                                           b        σθ,Σ − σθ,Σ )
                                                             θ,Σ + (b

where t ∈ T := S d−1 × A. Let µ := q 0 Σ and
                        wi,µ (α) := (θ0 (x, α)1(µzi < 0) + θ1 (x, α)1(µzi ≥ 0)) .
We define                    h                 i
                  bθ,Σb := En q 0 Σz                bθ,Σ := En q 0 Σzi wi,q0 Σ (α) .
                                                                                 
                  σ               b i w 0 b (α) and σ
                                       i,q Σ
By Lemma 1 uniformly in t ∈ T
     √              
      n σbθ,
          bΣ    bθ,Σb (t) = q 0 ΣE[zi p0i 1{q 0 Σzi > 0}]J1−1 (α)Gn [pi ϕi1 (α)]
             b −σ

                                    + q 0 ΣE[zi p0i 1{q 0 Σzi < 0}]J0−1 (α)Gn [pi ϕi0 (α)] + oP (1).
By Lemma 2 uniformly in t ∈ T
           √                     √ 0                             
            n σ bθ,Σb − σ
                        bθ,Σ (t) =  nq Σ   b − Σ E zi wi,q0 Σ (α) + oP (1)

                                 = −q 0 ΣG
                                        b n [xi z 0 ]ΣE zi wi,q0 Σ (α) + oP (1)
                                                                     
                                                 i
                                 = −q 0 ΣGn [xi zi0 ]ΣE zi wi,q0 Σ (α) + oP (1).
                                                                     

By definition                 √
                                     σθ,Σ − σθ,Σ ) (t) = Gn [q 0 Σzi wi,q0 Σ (u)].
                                  n (b
                BEST LINEAR APPROXIMATIONS TO SET IDENTIFIED FUNCTIONS                     29


Putting all the terms together uniformly in t ∈ T
                         √
                           n(b
                             σθ,
                               bΣb − σθ,Σ )(t) = Gn [hk (t)] + oP (1),

where for t := (q, α) ∈ T = S d−1 × A
                    hki (t) :=       q 0 ΣE[zi p0i 1{q 0 Σzi > 0}]J1−1 (α)pi ϕi1 (α)
                                 + q 0 ΣE[zi p0i 1{q 0 Σzi < 0}]J0−1 (α)pi ϕi0 (α)
                                 − q 0 Σxi zi0 ΣE zi wi,q0 Σ (α)
                                                               

                              + q 0 Σzi wi,q0 Σ (α)
                           :=   hk1i (t) + hk2i (t) + hk3i (t) + hk4i (t),             (B.1)
where k indexes the number of series terms. To simplify notation, we omit the subscript k
below.
   Step 2. (Finite k). This case follows from H = {hi (t), t ∈ T } being a Donsker class with
square-integrable envelopes. Indeed, H is formed as finite products and sums of VC classes
or entropically equivalent classes, so we can apply Lemma 9. The result
                                 Gn [hk (t)] ⇒ G[hk (t)] in `∞ (T ),
follows, and the assertion that
                           Gn [hk (t)] =d G[hk (t)] + oP (1) in `∞ (T )
follows from e.g., the Skorohod-Dudley-Whichura construction. (The =d can be replaced
by = as in Step 3, in which case G[hk (t)] is a sequence of Gaussian processes indexed by n
and identically distributed for each n.)
   Step 3. (Case with growing k.) This case is considerably more difficult. The main
issue here is that the uniform covering entropy of Hl = {hli (t), t ∈ T }, l = 0, 1, grows
without bound, albeit at a very slow rate log n. The envelope Hl of this class also grows in
general, and so we can not rely on the usual uniform entropy-based arguments; for similar
reasons we can not rely on the bracketing-based entropy arguments. Instead, we rely on a
strong approximation argument, using ideas in Chernozhukov, Lee, and Rosen (2013) and
Belloni and Chernozhukov (2009a), to show that Gn [hk (t)] can be approximated by a tight
sequence of Gaussian processes G[h(t)], implicitly indexed by k, where the latter sequence is
very well-behaved. Even though it may not converge as k → ∞, for every subsequence of k
there is a further subsequence along which the Gaussian process converges to a well-behaved
Gaussian process. The latter is sufficient for carrying out the usual inference.
  Lemma 4 below establishes that
                             Gn [h(t)] = G[h(t)] + oP (1) in `∞ (T ),
where G[h] is a sequence of P-Brownian bridges with the covariance function E[h(t)h(t0 )] −
E[h(t)]E[h(t0 )]. Lemma 6 below establishes that for some 0 < c ≤ 1/2
                                                        1/2                       c
                  ρ2 (h(t), h(t0 )) = E[h(t) − h(t0 )]2      . ρ(t, t0 ) := t − t0 ,
and the function E[h(t)h(t0 )]−E[h(t)]E[h(t0 )] is equi-continuous on T ×T uniformly in k. By
assumption C3 we have that inf t∈T var[h(t)] > C > 0, with Lemma 6 providing a sufficient
condition for this.
30                 CHANDRASEKHAR, CHERNOZHUKOV, MOLINARI, AND SCHRIMPF


  An immediate consequence of the above result is that we also obtain the convergence in
the bounded Lipschitz metric
             sup             |E [g(Gn [h])] − E [g(G[h])]| ≤ E sup |Gn [h(t)] − G[h(t)]| ∧ 1 → 0.
     g∈BL1 (`∞ (T ),[0,1])                                    t∈T

   Step 4. Let’s recognize the fact that h depends on k by using the notation hk in this step
of the proof. Note that k itself is implicitly indexed by n. Let Fk (c) := P{f (G[hk ]) ≤ c}
and observe that by Step 3 and f ∈ Fc
                       |P{f (Sn ) ≤ cn + op (1)} − P{f (G[hk ]) ≤ cn }|
                     ≤ |P{f (G[hk ]) ≤ cn + op (1)} − P{f (G[hk ]) ≤ cn }|
                     ≤ δn (op (1)) →P 0, for δn () := sup |Fk (c + ) − Fk (c)|,
                                                             c∈R

where the last step follows by the Extended Continuous Mapping Theorem (Theorem 18.11
in van der Vaart (2000)) provided that we can show that for any n & 0, δn (n ) → 0. Suppose
otherwise, then there is a subsequence along which δn (n ) → δ 6= 0. We can select a further
subsequence say {nj } along which the covariance function of Gn [hk ], denoted Ωnk (t, t0 )
converges to a covariance function Ω0 (t, t0 ) uniformly on T ×T . We can do so by the Arzelà-
Ascoli theorem in view of the uniform equicontinuity in k of the sequence of the covariance
functions Ωnk (t, t0 ) on T × T . Moreover, inf t∈T Ω0 (t, t) > C > 0 by our assumption on
Ωnk (t, t0 ). But along this subsequence G[hk ] converges in `∞ (T ) in probability to a tight
Gaussian process, say Z0 . The latter happens because G[hk ] converges to Z0 marginally
by Gaussianity and by Ωnk (t, t0 ) → Ω0 (t, t0 ) uniformly and hence pointwise on T × T and
because G[hk ] is asymptotically equicontinuous as shown in the proof of Lemma 4. Thus,
along this subsequence we have that
                       Fk (c) → F0 (c) = P{f (Z0 ) ≤ c}, uniformly in c ∈ R,
because we have pointwise convergence that implies uniform convergence by Polya’s theo-
rem, since F0 is continuous by f ∈ Fc and by inf t∈T Ω0 (t, t) > C > 0. This implies that
along this subsequence δnj (nj ) → 0, which gives a contradiction.
  Step 5. Finally, we observe that c(1 − τ ) = O(1) holds by supt∈T kG[hk (t)]k = OP (1) as
shown in the proof of Lemma 4, and the second part of Theorem 2 follows.                 

B.2. Proof of Theorems 3 and 4.
Step 1. We can write the difference between a bootstrap and true support function as the
sum of three differences.
                                                         
               σeθ,
                 eΣ e − σθ,Σ = σeθ,
                                 eΣ e −σ
                                       eθ,Σe + σeθ,Σe − σ       σθ,Σ − σθ,Σ )
                                                        eθ,Σ + (e

where for
                       wi,µ (α) =: (θ0 (x, α)1(µzi < 0) + θ1 (x, α)1(µzi ≥ 0))
we define
                    h                            i
         eθ,Σe := En (ei /ē)q 0 Σ
                                 e 0 zi w 0 e (α) and σ
                                                      eθ,Σ =: En (ei /ē)q 0 Σ0 zi wi,q0 Σ (α) ,
                                                                                             
         σ                               i,q Σ

where ē = En ei →P 1.
                 BEST LINEAR APPROXIMATIONS TO SET IDENTIFIED FUNCTIONS                                  31


  By Lemma 1 uniformly in t ∈ T
     √              
      n σeθ,
          eΣ    eθ,Σe (t) = q 0 ΣE[zi p0i 1{q 0 Σzi > 0}]J1−1 (α)Gn [ei pi ϕi1 (α)]
             e −σ

                                + q 0 ΣE[zi p0i 1{q 0 Σzi < 0}]J0−1 (α)Gn [ei pi ϕi0 (α)] + oP (1).
By Lemma 2 uniformly in t ∈ T
        √                     √ 0                                
            eθ,Σe − σ
          n σ       eθ,Σ (t) =      nq Σ e − Σ E zi wi,q0 Σ (α) + oP (1)

                              = q 0 ΣG
                                    e n [(ei /ē)(xi z 0 )o ]ΣE zi wi,q0 Σ (α) + oP (1)
                                                                             
                                                        i
                              = q 0 ΣGn [ei (xi zi0 )o ]ΣE zi wi,q0 Σ (α) + oP (1).
                                                                         

By definition
  √                                                  o                                o
       σθ,Σ − σθ,Σ ) (t) = Gn [ei q 0 Σzi wi,q0 Σ (α) ]/ē = Gn [ei q 0 Σzi wi,q0 Σ (α) ](1 + op (1)).
    n (e
Putting all the terms together uniformly in t ∈ T
                         √                               o
                           n(e
                             σθ,
                               eΣe − σθ,Σ )(t) = Gn [ei hi (t)] + oP (1).

  Step 2. Combining conclusions of Theorems 1 and Step 1 above we obtain:
                                    √
                         S̃n (t) = n(e  σθ,
                                          eΣe −σ bθ,
                                                  bΣ b )(t)
                         √                         √
                     =     n(e σθ,
                                 eΣe − σθ,Σ )(t) − n(b    σθ,
                                                            bΣb − σθ,Σ )(t)

                            = Gn [ei hoi (t)] − Gn [h(t)] + oP (1)
                            = Gn [eoi hoi (t)] + oP (1).
Observe that the bootstrap process Gn [eoi hoi (t)] has the unconditional covariance function
                                   E[h(t)h(t0 )] − E[h(t)]E[h(t0 )],
which is equal to the covariance function of the original process Gn [hi ]. Conditional on data
the covariance function of this process is
                                 En [h(t)h(t0 )] − En [h(t)]En [h(t0 )].
Comment B.1. Note that if a bootstrap random element Zn taking values in a normed
space (E, k · k) converges in probability P unconditionally, that is Zn = oP (1), then Zn =
oPe (1) in L1 (P ) sense and hence probability P, where Pe denotes the probability measure
conditional on the data. In other words, Zn also converges in probability conditionally on the
data. This follows because EP |Pe {kZn k > }| = P{kZn k > } → 0, so that Pe {kZn k > } →
0 in L1 (P ) sense and hence in probability P. Similarly, if Zn = OP (1), then Zn = OPe (1)
in probability P.

   Step 3. (Finite k). This case follows from H = {hi (t), t ∈ T } being a Donsker class with
square-integrable envelopes. Indeed, H is formed as a Lipschitz composition of VC classes or
entropically equivalent classes. Then by the Donsker theorem for exchangeable bootstraps,
see e.g., van der Vaart and Wellner (1996), we have weak convergence conditional on the
data
                                      ^ under Pe in `∞ (T ) in probability P,
                Gn [eoi hoi (t)]/ē ⇒ G[h(t)]
32                    CHANDRASEKHAR, CHERNOZHUKOV, MOLINARI, AND SCHRIMPF


where G[h]
        g is a sequence of P-Brownian bridges independent of G[h] and with the same dis-
                                                             g is E[h(t)h(t0 )]−E[h(t)]E[h(t0 )].
tribution as G[h]. In particular, the covariance function of G[h]
Since ē →Pe 1, the above implies

                                             ^ under Pe in `∞ (T ) in probability P.
                          Gn [eoi hoi (t)] ⇒ G[h(t)]

The latter statement simply means

                                        sup           |EPe [g(Gn [h])] − E[g(G[h])]|
                                                                             g →P 0.
                              g∈BL1 (`∞ (T ),[0,1])

This statement can be strengthened to a coupling statement as in Step 4.
  Step 4. (Growing k.) By Lemma 4 below we can show that (on a suitably extended
                                                                 ^ such that
probability space) there exists a sequence of Gaussian processes G[h(t)]

                                                        ^ + oP (1) in `∞ (T ),
                                     Gn [eoi hoi (t)] = G[h(t)]

which implies by Remark B.1 that

                                              ^ + oPe (1) in `∞ (T ) in probability.
                           Gn [eoi hoi (t)] = G[h(t)]

Here, as above, G[h]
                 g is a sequence of P-Brownian bridges independent of G[h] and with the
                                                                       g is E[h(t)h(t0 )] −
same distribution as G[h]. In particular, the covariance function of G[h]
            0
E[h(t)]E[h(t )]. Lemma 6 describes the properties of this covariance function, which in turn
define the properties of this Gaussian process.
  An immediate consequence of the above result is the convergence in bounded Lipschitz
metric

        sup               EPe [g(Gn [eoi hoi (t)])] − EPe [g(G[h])]
                                                             g      ≤ EPe sup |Gn [eoi hoi (t)]−G[h(t)]|∧1
                                                                                                ^          →P 0.
g∈BL1   (`∞ (T ),[0,1])                                                     t∈T


Note that EPe [g(G[h])]
                 g = EP [g(G[h])],
                           g since the covariance function of G[h]
                                                              g does not depend
on the data. Therefore

                                  sup             |EPe [g(Gn [eoi hoi (t)])] − EP [g(G[h])]|
                                                                                     g →P 0.
                          g∈BL1 (`∞ (T ),[0,1])


   Step 5. Let us recognize the fact that h depends on k by using the notation hk in
this step of the proof. Note that k itself is implicitly indexed by n. By the previous
steps and Theorem 1 there exist n & 0 such that π1 = Pe {|f (Sen ) − f (G[h
                                                                         ^  k ])| > n } and
π2 = P{|f (Sen ) − f (G[hk ])| > n } obey E[π1 ] →P 0 and π2 → 0. Let

                                                    ^               e
                  F (c) := P{f (G[hk ]) ≤ c} = P{f (G[hk ]) ≤ c} = P {f (G[hk ]) ≤ c},

                                              ^
where the equality holds because G[hk ] and G[h   k ] are P-Brownian bridges with the same
covariance kernel, which in the case of the bootstrap does not depend on the data.
                BEST LINEAR APPROXIMATIONS TO SET IDENTIFIED FUNCTIONS                               33


  For any cn which is a measurable function of the data,

                   E|Pe {f (Sen ) ≤ cn } − P{f (Sn ) ≤ cn }|
               ≤ E[Pe {f (G[h
                          ^   k ]) ≤ cn + n } − P{f (G[hk ]) ≤ cn − n } + π1 + π2 ]
               = EF (cn + n ) − EF (cn − n ) + o(1)
               ≤ sup |F (c + n ) − F (c − n )| + o(1) = o(1),
                   c∈R

where the last step follows from the proof of Theorem 1. This proves the first claim of
Theorem 4 by the Chebyshev inequality. The second claim of Theorem 4 follows similarly
to Step 5 in the proof of Theorems 1-2.                                              

B.3. Main Lemmas for the Proofs of Theorems 1 and 2.
Lemma 1 (Linearization). 1. (Sample) We have that uniformly in t ∈ T
     √                
                              0 b√
                                                                  n
                                                                        0b
                                                                                  o
       n σbθ,
           bΣ b − σ
                  bθ,Σ
                     b   =  q  Σ   n   E z
                                        n i   θ
                                              b 1,i (α) − θ 1,i (α) 1 q  Σz i > 0
                                 √                                n            o
                         + q0Σ  b n En zi θb0,i (α) − θ0,i (α) 1 q 0 Σz  b i<0

                               = q 0 ΣE[zi p0i 1{q 0 Σzi > 0}]J1−1 (α)Gn [pi ϕi1 (α)]
                               + q 0 ΣE[zi p0i 1{q 0 Σzi < 0}]J0−1 (α)Gn [pi ϕi0 (α)] + oP (1).
2. (Bootstrap) We have that uniformly in t ∈ T
     √                 
                                  0 e√
                                                                               n
                                                                                     0e
                                                                                              o
       n σeθ,
            eΣe − σ
                  e θ,Σ
                      e   (t) = q  Σ   n   E (e
                                            n i /ē)z i   θ
                                                          e 1,i (α) − θ 1,i (α)  1 q  Σz i > 0
                                     √                                         n           o
                              + q0Σ e n En (ei /ē)zi θe0,i (α) − θ0,i (α) 1 q 0 Σz   e i<0

                               = q 0 ΣE[zi p0i 1{q 0 Σzi > 0}]J1−1 (α)Gn [ei pi ϕi1 (α)]
                               + q 0 ΣE[zi p0i 1{q 0 Σzi < 0}]J0−1 (α)Gn [ei pi ϕi0 (α)] + oP (1).

   Proof of Lemma 1. In order to cover both cases with one proof, we will use θ̄ to mean
either the unweighted estimator θb or the weighted estimator θ̃, and vi to mean either 1 in
the case of the unweighted estimator or exponential weights ei in the case of the weighted
estimator. We also observe that Σ̄ →P Σ by the law of large numbers and the continuous
mapping theorem.
  Step 1. It will suffice to show that
                             √
                       q 0 Σ̄ n En (vi /v̄)zi θ̄1,i (α) − θ1,i (α) 1 q 0 Σ̄zi > 0
                                                                                

                    = q 0 ΣE[zi p0i 1{qΣzi > 0}]J1−1 (α)Gn [vi pi ϕi1 (α)] + oP (1)
and that
                               √
                         q 0 Σ̄ n En (vi /v̄)zi θ̄0,i (α) − θ0,i (α) 1 q 0 Σ̄zi < 0
                                                                                  

                    = q 0 ΣE[zi p0i 1{qΣzi < 0}]J0−1 (α)Gn [vi pi ϕi0 (α)] + oP (1).
34                  CHANDRASEKHAR, CHERNOZHUKOV, MOLINARI, AND SCHRIMPF


We show the argument for the first part; the argument for the second part is identical. We
also drop the index ` = 1 to ease notation. By Assumption C2 we write
      √
q 0 Σ̄ nEn (vi /v̄)zi θ̄i − θi 1 q 0 Σ̄zi > 0 = q 0 Σ̄En [(vi zi p0i 1{q 0 Σ̄zi > 0}]J −1 (α)Gn [vi pϕi (α)]
                                              

                                                       +q 0 Σ̄En [vi zi R̄i (α)1{q 0 Σ̄zi > 0}] /v̄
                                                     = : (a(α) + b(α))/(1 + oP (1)).
We have from the assumptions of the theorem
     sup |b(α)| ≤ kq 0 Σ̄k · kkzi kkPn ,2 kkvi kkPn ,2 · sup kR̄i (α)kPn ,2 = OP (1)OP (1)oP (1) = oP (1).
     α∈A                                                α∈A
Write a(α) = c(α) + d(α), where
                   c(α) :=      q 0 ΣE[zi p0i 1{qΣzi > 0}]J −1 (α)Gn [vi pi ϕi (α)]
                  d(α) :=       µ̄0 J −1 (α)Gn [vi pi ϕi (α)]
                    µ̄0 :=      q 0 Σ̄En [vi zi p0i 1{q 0 Σ̄zi > 0}] − q 0 ΣE[zi p0i 1{q 0 Σzi > 0}]   (B.2)
The claim follows after showing that supα∈A |d(α)| = oP (1), which is shown in subsequent
steps below.
   Step 2. (Special case, with k fixed). This is the parametric case, which is trivial. In
this step we have to show supα∈A |d(α)| = oP (1). We can write
               d(α) = Gn [µ̄0 fα ],   fα := (fαj , j = 1, ..., k) , fαj := J −1 (α)vi pij ϕi (α)
and define the function class F := {fαj , α ∈ A, j = 1, ..., k}. Since k is finite, and given the
assumptions on F1 = {ϕ(α), α ∈ A}, application of Lemmas 9 and 10-2(a) yields
                               sup log N (kF kQ,2 , F, L2 (Q)) . log(1/).
                                Q

and the envelope is P-square integrable. Therefore, F is P-Donsker and
                                              sup |Gn [fα ]| .P 1
                                              α∈A
and supα∈A |d(α)| .P kkµ̄k →P 0.
  Step 3. (General case, with k → ∞). In this step we have to show supα∈A |d(α)| = oP (1).
The case of k → ∞ is much more difficult if we want to impose rather weak conditions on
the number of series terms. We can write
                             d(α) = Gn [fαn ],       fαn := µ̄0 J −1 (α)vi pi ϕi (α)
and define the function class F3 := {fαn , α ∈ A}, see equation (B.11) below. By Lemma 10
the random entropy of this function class obeys
                           log N (kF3 kPn ,2 , F3 , L2 (Pn )) .P log n + log(1/).
Therefore by Lemma 12, conditional on Xn = (xi , zi , i = 1, ..., n), for each δ > 0 there exists
a constant Kδ , that does not depend on n, such that for all n:
                                                                           
                               p
           P sup |d(α)| ≥ Kδ log n sup kfαn kPn ,2 ∨ sup kfαn kP|Xn ,2           ≤ δ,
                   α∈A                            α∈A                   α∈A
where P|Xn denotes the probability
                       √           measure conditional on Xn . The conclusion follows if
we can demonstrate that log n supα∈A kfαn kPn ,2 ∨ supα∈A kfαn kP|Xn ,2 →P 0. Note that
                  BEST LINEAR APPROXIMATIONS TO SET IDENTIFIED FUNCTIONS                                      35




           sup kfαn kPn ,2 ≤ kµ̄k sup kJ −1 (α)kkEn pi p0i k · sup vi        sup      |ϕi (α)| →P 0,
           α∈A                      α∈A                          i≤n        i≤n,α∈A

where the convergence to zero in probability follows because
         kµ̄k .P n−m/4 + (k/n) · log n · (log n max kzi k) ∧ ξk ,
                          p
                                                                                   sup vi .P log n
                                                            i                       i≤n

by Step 4 below, supα∈A kJ −1 (α)k . 1 by assumption C3, kEn pi p0i k .P 1 by Lemma 11,
and                                                     
                   −m/4
             2
                          p
          log n n       + (k/n) · log n · max kzi k ∧ ξk   sup |ϕi (α)| →P 0
                                                      i                 i≤n,α∈A

by assumption C4. Also note that
                                                                                                       1/2
sup kfαn kP|Xn ,2 ≤ kµ̄k sup kJ −1 (α)kkEn pi p0i k · (E[vi2 ])1/2 ·                E[ϕ2i (u)|xi , zi ]
                                                                                   
                                                                        sup                                 →P 0,
α∈A                        α∈A                                         i≤n,α∈A

by the preceding argument and E[ϕ2i (u)|xi , zi ] uniformly bounded in α and i by assumption
C3.
  Step 4. In this step we show that
                 kµ̄k .P n−m/4 + (k/n) · log n · (log n max kzi k) ∧ ξk .
                                 p
                                                                        i

We can bound
                    kµ̄k ≤ kΣ − Σ̄kkE zi pi 1{q 0 Σzi > 0} k + kΣ̄kµ1 + kΣ̄kµ2 ,
                                                         

where
                    µ1 = kEn vi zi p0i 1{q 0 Σzi > 0} − E zi p0i 1{q 0 Σzi > 0} k
                                                                            

                    µ2 = kEn [vi zi p0i {1{q 0 Σzi > 0} − 1{q 0 Σ̄zi > 0}}]k.
By Lemma 11, kΣ − Σ̄k = oP (1), and from Assumption C3 kE [zi pi 1{q 0 Σzi > 0}] k . 1.
  By elementary inequalities
        µ̄22 ≤ En kvi k2 En kzi k2 kEn [pi p0i ]kEn [{1{q 0 Σzi > 0} − 1{q 0 Σ̄zi > 0}}2 ] .P n−m/2 ,
where we used the Chebyshev inequality along with Ekvi k2 = 1 and E[kzi k2 ] < ∞, kEn [pi p0i ]k .P
1 by Lemma 11, and En [{1{q 0 Σzi > 0} − 1{q 0 Σ̄zi > 0}}2 ] .P n−m/2 by Step 5 below.
  We can write µ1 = supg∈G |En g − Eg|, where G := {vi γ 0 zi p0i η1{q 0 Σzi > 0}, kγk = 1, kηk =
1}. The function class G obeys
           sup log N (kGkQ,2 , G, L2 (Q)) . (dim(zi ) + dim(pi )) log(1/) . k log(1/)
            Q

for the envelope Gi = vi kzki · ξk that obeys maxi log Gi .P log n by E|vi |p < ∞ for any
p > 0, Ekzi k2 < ∞ and log ξk .P log n. Invoking Lemma 12 we obtain
                            p
                     µ1 .P (k/n) · log n × sup kgkPn ,2 ∨ sup kgkP,2 ,
                                                      g∈G              g∈G
36                  CHANDRASEKHAR, CHERNOZHUKOV, MOLINARI, AND SCHRIMPF


where
                                                                
                                                            0
                                                                                1/2 
                             .P max kzi k max vi · kEn [pi pi ]k ∧ En kvi zi k2
                                                                    
              sup kgkPn ,2                                                          ξk
              g∈G                         i           i

                       .P (max vi max kzi k) ∧ ξk .P (log n max kzi k) ∧ ξk
                                  i             i                          i

         k2
by Ekzi < ∞ and by           En [pi p0i ]     .P 1, maxi vi .P log n and supg∈G kgkP,2 = kEzi p0i k . 1 by
Assumption C3. Thus
                                              p
                              µ1 .P            (k/n) · log n(max kzi k log n) ∧ ξk ,
                                                                i
and the claim of the step follows.
     Step 5. Here we show
                                      h                                   2 i
                       sup En             1(q 0 Σzi < 0) − 1(q 0 Σ̄zi < 0)     .P n−m/2 .
                      q∈S d−1
                                     2
Note 1(q 0 Σzi < 0) − 1(q 0 Σ̄zi < 0)      = 1(q 0 Σzi < 0 < q 0 Σ̄zi ) + 1(q 0 Σzi > 0 > q 0 Σ̄zi ). The
set
       n                                                                                            o
    F = 1(q 0 Σzi < 0 < q 0 Σ̃zi ) + 1(q 0 Σzi > 0 > q 0 Σ̃zi ), q ∈ S d−1 , kΣk ≤ M, kΣ̃k ≤ M

is P-Donsker because it is a VC class with a constant envelope. Therefore, |En f − Ef | .P
n−1/2 uniformly on f ∈ F. Hence uniformly in q ∈ S d−1 , En [(1(q 0 Σzi < 0)−1(q 0 Σ̄0 zi < 0))2 ]
is equal to
                                                                                        
                E 1(q 0 Σzi < 0 < q 0 Σ̄0 zi ) + 1(q 0 Σzi > 0 > q 0 Σ̄0 zi ) + OP n−1/2
                                                                            
                                                                
            = P q 0 Σzi < q 0 Σ − Σ̄ zi + OP n−1/2
                                             
                                            
            ≤ kΣ − Σ̄km + OP n−1/2 .P n−m/2 + n−1/2 .P n−m/2
where we are using that for 0 < m ≤ 1
       P q 0 Σzi < q 0 Σ − Σ̄ zi ≤ P |q 0 Σzi /kzi k| < kqkkΣ − Σ̄k . kΣ̄ − Σkm ,
                                                                 

where the last inequality holds by Assumption C1, which gives that P (|q 0 Σzi /kzi k| < δ) /δ m .
1. 
Lemma 2. Let wi,µ (α) =: (θ0 (x, α)1(µzi < 0) + θ1 (x, α)1(µzi ≥ 0)). 1. (Sample) Then
uniformly in t ∈ T
               √                   √           
                           bθ,Σ (t) = nq 0 Σ
                                                                  
                   bθ,Σb − σ
                 n σ                        b − Σ E zi wi,q0 Σ (α) + oP (1)

2. (Bootstrap) Then uniformly in t ∈ T
               √                     √           
                 n σ̃θ,Σ̃ − σ̃θ,Σ (t) = nq 0 Σ̃ − Σ E zi wi,q0 Σ (α) + oP (1)
                                                                    


   Proof of Lemma 2. In order to cover both cases with one proof, we will use θ̄ to mean
either the unweighted estimator θb or the weighted estimator θ̃ and so on, and vi to mean
either 1 in the case of the unweighted estimator or exponential weights ei in the case of the
                  BEST LINEAR APPROXIMATIONS TO SET IDENTIFIED FUNCTIONS                               37


weighted estimator. We also observe that Σ̄ →P Σ by the law of large numbers (Lemma
11) and the continuous mapping theorem.
  Step 1. Define F = q 0 Σzi wi,q0 Σ (t) : t ∈ T, kΣk ≤ C . We have that for f¯i (t) = q 0 Σ̄zi wi,q0 Σ̄ (t)
                             

and fi (t) = q 0 Σzi wi,q0 Σ (t) by definition
     √                               √
                                       nEn [(vi /v̄)(f¯i (t) − fi (t))]
                         
        n σ̄θ,Σ̄ − σ̄θ,Σ (t) =
                                     √
                                 = ( nE[f¯i (t) − fi (t)] + Gn [vi (f¯i (t) − fi (t))o ])/(1 + oP (1)).
By intermediate value expansion and Lemma 3, uniformly in α ∈ A and q ∈ S d−1
√                       √                                       √
  n E[f¯i (t) − fi (t)] = n(q 0 Σ̄−q 0 Σ)E[zi wi,q0 Σ∗ (t) (t)] = n(q 0 Σ̄−q 0 Σ)E[zi wi,q0 Σ (t)]+oP (1),
for qΣ∗ (t) on the line connecting q 0 Σ and q 0 Σ̄, where the last step follows by the uniform
continuity of the mapping (α, q 0 Σ) 7→ E[zi wi,q0 Σ (t)] and q 0 Σ̄ − q 0 Σ →P 0. Furthermore
supt∈T |Gn [vi (f¯i (t) − fi (t))o ]| →P 0 by Step 2 below, proving the claim of the Lemma.
                                                                                              o
    Step 2. It suffices to show that for any t ∈ T , we have that Gn [vi f¯i (t) − fi (t) ] →P 0.
                                                                                    
                                                                                             o
By Lemma 19.24 from van der Vaart (2000) it follows that if vi f¯i (t) − fi (t) ∈ G =
                                                                                  

vi ((F − F)o ) is such that
                                            1/2                                1/2
               E (vi (f¯i (t) − fi (t))o )2       ≤ 2 E (vi (f¯i (t) − fi (t)))2
                                                      
                                                                                       →P 0,
and G is P-Donsker, then Gn [vi (f¯i (t) − fi (t))o ] →P 0. Here G is P-Donsker because F is a P-
Donsker class formed by taking products of F2 ⊇ {θi` (α) : α ∈ A, ` = 0, 1} , which possess
a square-integrable envelope, with bounded VC classes {1(q 0 Σzi > 0), q ∈ S d−1 , kΣk ≤
C} and {1(q 0 Σzi ≤ 0), q ∈ S d−1 , kΣk ≤ C} and then summing followed by demeaning.
The difference (F − F)o is also P-Donsker, and its product with the independent square-
integrable variable vi is still a P-Donsker class with a square-integrable envelope. The
functions class has a square-integrable envelope. Note that
                                                                                                    2
                                      (q 0 Σ̄ − q 0 Σ)zi θ0i (α)1(q 0 Σ̄0 zi < 0)1  (q 0 Σzi < 0)
                               
                                     +(q 0 Σ̄ − q 0 Σ)zi θ1i (α)1 q 0 Σ̄z                0
                                                                                  
      E[f¯i (t) − fi (t)]2 = E                                            i > 00 1 (q Σzi >0 0)  
                                                                                                   
                                        0                  0
                                + q Σ̄zi θ0i (α) − q Σzi θi1 (α) 1 q Σ̄zi < 0 < q Σzi 
                                 + q 0 Σ̄zi θ1i (α) − q 0 Σzi θ0i (α) 1 q 0 Σ̄zi > 0 > q 0 Σzi
                                                                                                 

  .     kΣ̄ − Σk2   P,2
                          · kzi k2         max
                                     P,2 α∈A,`∈{0,1}
                                                         2
                                                       kθ`i (α)kP,2
                                                                                                   1/2
      + kΣ̄k2P ∨ kΣk2 · kkzi k2 kP,2                       2
                                                              (α)kP,2 · sup P q 0 Σzi < q 0 Σ̄ − Σ z
                                                                            
                                              max        kθ`i
                                          α∈A,`∈{0,1}                   q∈S d−1
                                                                1/2
              − Σk2 + sup P q 0 Σzi /kzi k < kΣ̄ − Σk
                                 
  .   P kΣ̄                                                            → 0,
                      q∈S d−1

where we invoked the moment and smoothness assumptions.                                                
Lemma 3 (A Uniform Derivative). Let σi,µ (α) = µzi (θ0i (α)1(µzi < 0) + θ1i (α)1(µzi > 0)).
Uniformly in µ ∈ M = {q 0 Σ : q ∈ S d−1 , kΣk ≤ C} and α ∈ A
                                       ∂E[σi,µ (α)]
                                                    = E[zi wi,µ (α)],
                                          ∂µ
where the right hand side is uniformly continuous in µ and α.
38                 CHANDRASEKHAR, CHERNOZHUKOV, MOLINARI, AND SCHRIMPF


  Proof: The continuity of the mapping (µ, α) 7→ E[zi wi,µ (u)] follows by an application of
the dominated convergence theorem and stated assumptions on the envelopes.
     Note that for any kδk → 0
          E[(µ + δ)zi wi,µ+δ (α)] − E[µzi wi,µ (α)]    δ                    1
                                                    =     E[zi wi,µ (α)] +     E[Ri (δ, µ, α)],
                            kδk                       kδk                  kδk
where
              Ri (δ, µ, α) :=      (µ + δ) zi (θ1i (α) − θ0i (α)) 1 (µzi < 0 < (µ + δ) zi )
                                 + (µ + δ) zi (θ0i (α) − θ1i (α)) 1 (µzi > 0 > (µ + δ) zi ) .
By Cauchy-Schwarz and the maintained assumptions
        sup    E|Ri (δ, µ, α)| . kδzkP,2 ·          sup         kθ`i (α)kP,2     sup   [P (|µz| < |δz|)]1/2
     µ∈M,α∈A                                    α∈A,`∈{0,1}                    µ∈M,α∈A
                                                      m/2
                                  . kδzkP,2 · 1 · δ         .
Therefore, as kδk → 0
                   1                             1
            sup      |E[Ri (δ, µ, u)]| ≤ sup        E|Ri (δ, µ, α)| . δ m/2 → 0.                  
         µ∈M,α∈A kδk                    µ∈M,α∈A kδk


Lemma 4 (Coupling Lemma). 1. (Sample) We have that
                                   Gn [h(t)] = G[h(t)] + oP (1) in `∞ (T ),
where G is a P-Brownian bridge with covariance function E[h(t)h(t0 )] − E[h(t)]E[h(t0 )].
     2. (Bootstrap). We have that
                                                  ^ + oP (1) in `∞ (T ),
                                 Gn [eo ho (t)] = G[h(t)]
      e is a P-Brownian bridge with covariance function E[h(t)h(t0 )] − E[h(t)]E[h(t0 )].
where G

Proof. The proof can be accomplished by using a single common notation. Specifically it
will suffice to show that for either the case gi = 1 or gi = ei − 1
                                  Gn [gho ] = Gg [h(t)] + oP (1) in `∞ (T ),
where G is a P-Brownian bridge with covariance function E[h(t)h(t0 )] − E[h(t)]E[h(t0 )]. The
process Gg for the case of gi = 1 is different (in fact independent) of the process Gg for the
case of gi = ei − 1, but they both have identical distributions. Once we understand this, we
can drop the index g for the process.
     Within this proof, it will be convenient to define:
                                Sn (t) := Gn [gho (t)] and Zn (t) := G[h(t)].
  Let Bjk , j = 1, ..., p be a partition of T into sets of diameter at most j −1 . We need at
most
                                      p . j d , d = dim(T )
such partition sets. Choose tjk as arbitrary points in Bjk , for all j = 1, ..., p. We define the
sequence of projections πj : T → T , j = 0, 1, 2, . . . , ∞ by πj (t) = tjk if t ∈ Bjk .
                BEST LINEAR APPROXIMATIONS TO SET IDENTIFIED FUNCTIONS                               39


   In what follows, given a process Z in `∞ (T ) and its projection Z ◦ πj , whose paths are
constant over the partition set, we shall identify the process Z ◦ πj with a random vector
Z ◦ πj in Rp , when convenient. Analogously, given a random vector Z in Rp we identify
it with a process Z in `∞ (T ), whose paths are constant over the elements of the partition
sets.
  The result follows from the following relations proven below:
  1. Finite-Dimensional Approximation. As j/ log n → ∞, then ∆1 = supt∈T kSn (t)−
Sn ◦ πj (t)k →P 0.
  2. Coupling with a Normal Vector. There exists Nnj =d N (0, var[Sn ◦ πj ]) such
that, if p5 ξk2 /n → 0, then ∆2 = supj |Nnj − Sn ◦ πj | →P 0.
  3. Embedding a Normal Vector into a Gaussian Process. There exists a Gaussian
process Zn with the properties stated in the lemma such that Nnj = Zn ◦ πj almost surely.
   4. Infinite-Dimensional Approximation. if j → ∞, then ∆3 = supt∈T |Zn (t) − Zn ◦
πj (t)| →P 0.
   We can select the sequence j = log2 n such that the conditions on j stated in relations
(1)-(4) hold. We then conclude using the triangle inequality that
                           sup |Sn (t) − Zn (t)| ≤ ∆1 + ∆2 + ∆3 →P 0.
                           t∈T

  Relation 1 follows from
               ∆1 = sup |Sn (t) − Sn ◦ πj (t)| ≤                 sup     |Sn (t) − Sn (t0 )| →P 0,
                     t∈T                                  kt−t0 k≤j −1

where the last inequality holds by Lemma 5.
    Relation 2 follows from the use of Yurinskii’s coupling (Pollard (2002,P page   244)):
                                                                                         Let
ζ1 , . . . , ζn be independent p-vectors with Eζi = 0 for each i, and κ := i E kζi k3 finite.
Let S = ζ1 + · + ζn . For each δ > 0 there exists a random vector T with a N (0, var(S))
distribution such that
                                                            
                                                 | log(1/B)|
                   P{kS − T k > 3δ} ≤ C0 B 1 +                 where B := κpδ −3 ,
                                                       p
for some universal constant C0 .
  In order to apply the coupling, we collapse Sn ◦ πj to a p-vector, and we let
                           ζi = ζ1i + ... + ζ4i ∈ Rp , ζli = gi holi ◦ π ∈ Rp ,
                                                                                    √
where hli , l = 1, ..., 4 are defined in (B.1), so that Sn ◦ πj = ni=1 ζi / n. Now note that
                                                                          P
since E[kζi k3 ] . max1≤l≤4 E[kζli k3 ] and
                                      p
                                                            !3/2            p
                                                                                                  !
                                   1 X                                    1 X
          Ekζli k3 = p3/2 E               |gi holi (tkj )|2      ≤ p3/2 E       |gi holi (tkj )|3
                                   p                                      p
                                      k=1                                          k=1
                           3/2
                    ≤ p          sup E|holi (tkj )|3 E|gi |3 ,
                                 t∈T
40                 CHANDRASEKHAR, CHERNOZHUKOV, MOLINARI, AND SCHRIMPF


where we use the independence of gi , we have that
                               E[kζi k3 ] . p3/2 max sup E|holi (t)|3 E|gi |3 .
                                                         1≤l≤4 t∈T

     Next we bound the right side of the display above for each l.                                         First, for A(t) :=
                           −1
q 0 ΣE[z   0    0
        i pi 1{q Σzi > 0}]J1 (α)

sup E|ho1i (t))|3 = sup E |A(t)pi ϕi1 (α)|3 ≤ sup kA(t)k3 · sup E|δ 0 pi |3                                 sup     E[|ϕi (α)|3 |xi = x]
t∈T                    t∈T                                        t∈T                 kδk=1               α∈A,x∈X

                   .   sup E|δ 0 pi |3            sup        E[|ϕi (α)|3 |xi = x]
                       kδk=1               α∈A,x∈X
                                       0
                   . ξk sup E|δ pi |2                  sup        E[|ϕi (α)|3 |xi = x] . ξk ,
                         kδk=1                    α∈A,x∈X

where we used the assumption that supα∈A,x∈X E[|ϕi (α)|3 |xi = x] . 1, kEpi pi k . 1, and
that
            sup kA(t)k ≤ sup [E[zi0 δ]2 ]1/2 sup [E[p0i δ]2 ]1/2 sup kJ −1 (α)k . 1,
                 t∈T             kδk=1                           kδk=1                  α∈A

where the last bound is true by assumption. Similarly                              E|ho2i (t))|3    . ξk . Next
                                          o
     sup E|ho3i (t)|3 = sup E|q 0 Σ xi zi0 ΣE[zi , wi,q0 Σ (α)]|                      3
     t∈T                 t∈T
                                  o
                      . Ek xi zi0 k3 sup k E[zi wi,q0 Σ (α)] k3
                                                             
                                                       t∈T
                                                                         3                3/2        h           i3/2
                                                   3
                                            0
                                                       + E xi zi0                  Ekzi k2          sup E |θli (α)|2
                                              
                       .         E     xi z i
                                                                                                    α∈A
                       . 1,                                                                                                 (B.3)
where the last bound follows from assumptions C3. Finally,
                         1/3                               1/3
        sup E|ho4i (t)|3      = sup E|q 0 Σzi wi,q0 Σ (α)|3      + sup |Eq 0 Σzi wi,q0 Σ (α)|
                                  
           t∈T                             t∈T                                                t∈T
                                                             0      6 1/6
                                                                                                  1/6
                                                                                E|wi,q0 Σ (α)|6
                                                                            
                                     ≤ 2 sup[E|q Σzi | ]
                                            t∈T

                                     . [E|zi |6 ]1/6 sup E[|θli (α)|6 ]1/6 . 1,
                                                                 α∈A
where the last line follows from assumption C3.
  Therefore, by Yurinskii’s coupling, observing that in our case by the above arguments
     3/2 ξ n
κ = p(√n) k                       5 2
           3 , for each δ > 0 if p ξk /n → 0,

                       Pn
                                                npp3/2 ξk   p5/2 ξ
                                              
                               ζi
                    P      i=1
                           √      − Nn,j ≥ 3δ .   √ 3 = 3 1/2k → 0.
                             n                  (δ n)      (δ n )
This verifies relation (2).
   Relation (3) follows from the a.s. embedding of a finite-dimensional random normal
vector into a path of a Gaussian process whose paths are continuous with respect to the
standard metric ρ2 , defined in Lemma 6, which is shown e.g., in Belloni and Chernozhukov
(2009b). Moreover, since ρ2 is continuous with respect to the Euclidian metric on T , as
                 BEST LINEAR APPROXIMATIONS TO SET IDENTIFIED FUNCTIONS                                            41


shown in part 2 of Lemma 6, the paths of the process are continuous with respect to the
Euclidian metric as well.
  Relation (4) follows from the inequality
    ∆3 = sup |Zn (t) − Zn ◦ πj (t)| ≤                 sup         |Zn (t) − Zn (t0 )| .P (1/j)c log(1/j)c → 0,
           t∈T                                     kt−t0 k≤j −1

where 0 < c ≤ 1/2 is defined in Lemma 6. This inequality follows from the entropy
inequality for Gaussian processes (Corollary 2.2.8 of van der Vaart and Wellner (1996))
                                                                      Rδ p
                          E      sup         Zn (t) − Zn (t00 ) ≤         log N (, T, ρ2 )d
                              ρ2 (t,t0 )≤δ                            0

and parts 2 and 3 of Lemma 6. From part 2 of Lemma 6 we first conclude that
                                              log N (, T, ρ2 ) . log(1/),
and second that kt − t0 k ≤ (1/j) implies ρ2 (t, t0 ) ≤ (1/j)c , so that
                     E      sup        |Zn (t) − Zn (t0 )| ≤ (1/j)c log(1/j)c as j → ∞.
                         kt−t0 k≤1/j

The claimed inequality then follows by Markov inequality.                                                          
Lemma 5 (Bounded Oscillations). Let c be as in Lemma 6.
  1. (Sample) For n = o((log n)−1/(2c) ), we have that
                                             sup     |Gn [h(t) − h(t0 )]| →P 0.
                                        kt−t0 k≤n

  2. (Bootstrap). For n = o((log n)−1/(2c) ), we have that
                                   sup       |Gn [(ei − 1)(ho (t) − ho (t0 ))]| →P 0.
                               kt−t0 k≤n

   Proof. To show both statements, it will suffice to show that for either the case gi = 1
or gi = ei − 1, we have that
                                       sup       |Gn [gi (ho (t) − ho (t0 ))]| →P 0.
                                    kt−t0 k≤n

  Step 1. Since
             sup       |Gn [gi (ho (t) − ho (t0 ))]| . max                sup   |Gn [gi (ho` (t) − ho` (t0 ))]|,
          kt−t0 k≤n                                        1≤`≤4 kt−t0 k≤n

we bound the latter for each `. Using the results in Lemma 10 that bound the random
entropy of H1 and H2 and the results in Lemma 12 we have that for ` = 1 and 2
 ∆n` = sup |Gn [gi (ho` (t) − ho` (t0 ))]| .P log n sup  max kgi (ho` (t) − ho` (t0 ))kP,2 .
                                             p
        kt−t0 k≤n                                                     kt−t0 k≤n P∈{P,Pn }

By Lemma 10 that bounds the entropy of gi (H`o − H`o )2 and Lemma 12 we have that for
` = 1 or ` = 2,
                                                                  r
                o      o 0                  o      o 0              log n
   sup    kgi (h` (t)−h` (t ))kPn ,2 −kgi (h` (t)−h` (t ))kP,2 .P         sup max kgi2 ho2
                                                                                        ` (t)kP,2 .
    0
kt−t k≤n                                                             n   t∈T P∈{P,Pn }
42                 CHANDRASEKHAR, CHERNOZHUKOV, MOLINARI, AND SCHRIMPF


By Step 2 below we have
                                                            r                              r
          sup max         kg 2 ho2
                                ` (t)kP,2         .P            ξk2 max F14 max |gi |4   .P ξk2 max F14 (log n)4 ,
          t∈T P∈{P,Pn }                                             i≤n      i                      i≤n

and by Lemma 6, kg(h` (t) − h` (t0 ))kP,2 . kt − t0 kc . Putting the terms together we conclude
                                          r                            !
                                            log n
                        p                            r
              ∆n` .P log n cn +                  ξk max F14 (log n)2 →P 0,
                                              n          i≤n

by assumption and the choice of n .
     For ` = 3 and ` = 4, by Lemma 10, g(H3o − H3o ) and g(H4o − H4o ) are P-Donsker, so that
                              ∆n` ≤           sup           |Gn [g(ho` (t) − ho` (t0 ))]| →P 0. 
                                          ρ2 (t,t0n )≤cn


   Step 2. Since kg 2 ho2               2 2             2   o     2
                        ` (t)kP,2 ≤ 2kg h` (t)kP,2 + 2kg E[h` (t)] kP,2 , for P ∈ {P, Pn }, it
suffices to bound each term separately.
     Uniformly in t ∈ T for ` = 1, 2
      En [gh` (t)]4 ≤ max gi4 · kΣk4 kEn [zi pi 1{q 0 Σzi < 0}]kkJ0−1 (α)k · sup En [[δ 0 pi ]4 ϕ4i` (α)]
                          i                                                                    kδk=1

                     .P (log n) · 1 · 4
                                                ξk2 kEn [pi p0i ]k max F14     .P ξk2 max F14 (log n)4 ,
                                                                   i≤n                i≤n

where we used assumptions C3 and C5 and the fact that kEn [zi pi 1{q 0 Σzi < 0}]k .P 1 and
En [pi p0i ] .P 1 as shown in the proof of Lemma 1.
     Uniformly in t ∈ T for ` = 1, 2
        E[gh` (t)]4 ≤ E[g 4 ]kΣk4 kE[zi pi 1{q 0 Σzi < 0}]kkJ0−1 (α)k · sup E[[δ 0 pi ]4 ϕ4i` (α)]
                                                                                              kδk=1

                .P             1·   ξk2 kE[pi p0i ]k        sup      E[ϕ4i` (α)|xi   = x] . ξk2 ,
                                                       x∈X,α∈A

where we used assumption C3.
     Uniformly in t ∈ T for ` = 1, 2
                       En [g 4 E[ho` (t)]4 ] ≤ En g 4 E[ho` (t)]4 .P 1 · E[ho2    2
                                                                            ` (t)] . 1,

and
                              E[g 4 E[ho` (t)]4 ] ≤ Eg 4 E[ho` (t)]4 . 1 · E[ho2    2
                                                                              ` (t)] . 1.

where the bound in E[ho2    2
                      ` (t)] follows from calculations given in the proof of Lemma 6. 

Lemma 6 (Covariance Properties).                 1. For some 0 < c ≤ 1/2
                                                           1/2                                           c
                     ρ2 (h(t), h(t0 )) = E[h(t) − h(t0 )]2      . ρ(t, t0 ) := t − t0
       2. The covariance function E[h(t)h(t0 )] − E[h(t)]E[h(t0 )] is equi-continuous on T × T
          uniformly in k.
                     BEST LINEAR APPROXIMATIONS TO SET IDENTIFIED FUNCTIONS                                 43


     3. A sufficient condition for the variance function to be bounded away from zero,
        inf t∈T var(h(t)) ≥ L > 0, uniformly in k is that the following
                                                                     matrices have minimal
                                                                                           
                                                                                      0
        eigenvalues bounded away from zero uniformly in k: var ϕi1 (α) ϕi0 (α) |xi , zi
        E[pi p0i ], J0−1 (α), J1−1 (α), b00 b0 , and b01 b1 , where b1 = E[zi p0i 1{q 0 Σzi > 0}] and
        b0 = E[zi p0i 1{q 0 Σzi < 0}].

Comment B.2. We emphasize that claim 3 only gives sufficient conditions for var (h(t))
to be bounded away from zero. In particular, the assumption that
                                                0     
                      mineig var ϕi1 (α) ϕi0 (α) |xi , zi    ≥L

is not necessary, and does not hold in all relevant situations. For example, when the upper
and lower bounds have first-order equivalent asymptotics, which can occur in the point-
identified and local to point-identified cases, this condition fails. However, the result still
follows from equation (B.4) under the assumption that

                                var (ϕi1 (α)|xi , zi ) = var (ϕi0 (α)|xi , zi ) ≥ L

                                                                    
Proof. Claim 1. Observe that ρ2 h(t), h(t̃) . maxj ρ2 hj (t), hj (t̃) . We will bound each
of these four terms. For the first term, we have
                            "                                                              2 #1/2
                                 q 0 ΣE [zi p0i 1 {q 0 Σzi > 0}] J1−1 (α) pi ϕi1 (α) −
  ρ2 (h1 (t), h1 (t̃)) =E
                                 −q̃ 0 ΣE [zi p0i 1 {q̃ 0 Σzi > 0}] J1−1 (α̃) pi ϕi1 (α̃)
                            h                                                        2 i1/2
                             (q − q̃)0 ΣE zi p0i 1 q 0 Σzi > 0 J1−1 (α) pi ϕi1 (α)
                                                                
                       ≤E                                                                    +
                              "                 0 1 {q 0 Σz > 0}] −
                                                                                          2 #1/2
                                         E  [z  p
                          + E q̃ 0 Σ           i i           i
                                                                        J1−1 (α) pi ϕi1 (α)         +
                                         −E [zi p0i 1 {q̃ 0 Σzi > 0}]
                              h                                 −1                             2 i1/2
                          + E q̃ 0 ΣE zi p0i 1 q̃ 0 Σzi > 0      J1 (α) − J1−1 (α̃) pi ϕi1 (α)
                                                                                   
                                                                                                        +
                              h                                                                2 i1/2
                          + E q̃ 0 ΣE zi p0i 1 q̃ 0 Σzi > 0 J1−1 (α̃) pi (ϕi1 (α) − ϕi1 (α̃))
                                                             


For the first term we have
            h                                                      2 i1/2
          E (q − q̃)0 ΣE zi p0i 1 q 0 Σzi > 0 J1−1 (α) pi ϕi1 (α)
                                              
                                                                           ≤
                                                             2
            ≤ kq − q̃k kΣk E zi p0i       J1−1 (α) E[ pi p0i ]1/2 sup E[ϕi` (α)4 |xi , zi ]1/2
                                     
                                                                             xi ,zi


By assumption C3, kE [zi p0i ]k, J1−1 (α) , E[kpi p0i k2 ], and supxi ,zi E[ϕi1 (α)4 |xi , zi ] are bounded
uniformly in k and α. Therefore,
                 h                                                         2 i1/2
                     (q − q̃)0 ΣE zi p0i 1 q 0 Σzi > 0 J1−1 (α) pi ϕi1 (α)
                                                    
             E                                                                     . kq − q̃k
44                  CHANDRASEKHAR, CHERNOZHUKOV, MOLINARI, AND SCHRIMPF


The same conditions give the following bound on the second term.

                        "            0 1 {q 0 Σz > 0}] −
                                                                                  2 #1/2
                               E [z  p
                    E q̃ 0 Σ        i i            i
                                                               J1−1 (α) pi ϕi1 (α)
                                −E [zi p0i 1 {q̃ 0 Σzi > 0}]
                     . E 1 q 0 Σzi > 0 − 1 q̃ 0 Σzi > 0
                                                              
                          h                                     2 i1/2
                     . E 1 q 0 Σzi > 0 − 1 q̃ 0 Σzi > 0
                                                     




As in step 5 of the proof of Lemma 1, the assumption that P (|q 0 Σzi /kzi k| < δ) /δ m . 1
implies

                             h                                  2 i1/2
                                 1 q 0 Σzi > 0 − 1 q̃ 0 Σzi > 0         . kq − q̃km/2
                                                 
                         E


     Similarly, the third term is bounded as follows:

     h                             −1       −1
                                                              2 i1/2
          0
                                                                      . J1−1 (α) − J1−1 (α̃) .
               0  0                              
 E       q̃ ΣE zi pi 1 q̃ Σzi > 0  J1 (α) − J1 (α̃) pi ϕi1 (α)


Note that J1−1 (α) is uniformly Lipschitz in α ∈ A by assumption C3, so J1−1 (α) − J1−1 (α̃) .
kα − α̃k . Finally, the fourth term is bounded by

                    h                                                                  2 i1/2
                        q̃ 0 ΣE zi p0i 1 q̃ 0 Σzi > 0 J1−1 (α̃) pi (ϕi1 (α) − ϕi1 (α̃))
                                                    
                E                                                                              .
                                   h                             i1/2
                        . sup E (ϕi1 (α) − ϕi1 (α̃))4 |xi , zi
                          xi ,zi
                        . kα − α̃kγϕ

                                    h                              i1/2
where we used the assumption that E (ϕi1 (α) − ϕi1 (α̃))4 |xi , zi      is uniformly γϕ -Hölder
continuous in α. Combining, we have


                 ρ2 (h1 (t), h1 (t̃)) . kq − q̃k + kq − q̃km/2 + kα − α̃k + kα − α̃kγϕ
                                                   1∧m/2∧γϕ
                                        . t − t0


     An identical argument shows that ρ2 (h2 (t), h2 (t0 )) . kt − t0 k1∧m/2∧γϕ .
                         BEST LINEAR APPROXIMATIONS TO SET IDENTIFIED FUNCTIONS                                    45


   The third and fourth components of h(t) can be bounded using similar arguments. For
h3 (t), we have

                                      h                                                             2 i1/2
                                   q 0 Σxi zi0 ΣE zi wi,q0 Σ (α) − q̃ 0 Σxi zi0 ΣE zi wi,q̃0 Σ (α̃)
                                                                               
ρ2 h3 (t), h3 (t̃)            = E
                                 h
                                              0    0
                                                                       2 i1/2
                              . E (q − q̃) Σxi zi ΣE zi wi,q Σ (α)
                                                                 0               +
                                   h                                                      2 i1/2
                                +E q̃ 0 Σxi zi0 Σ E zi wi,q0 Σ (α) − E zi wi,q̃0 Σ (α)
                                                                          
                                                                                                       +
                                   h                                                      2 i1/2
                                +E q̃ 0 Σxi zi0 Σ E zi wi,q̃0 Σ (α) − E zi wi,q̃0 Σ (α̃)
                                                                          

                                                                              1/2
                              . kq − q̃k kΣk E[zi0 zi max θ` (xi , α)2 ]            +
                                                         `∈{0,1}
                                           h                                                                  i1/2
                                    + kΣk E zi0 zi (θ1 (xi , α) − θ0 (xi , α))2 1{ (q − q̃)0 Σzi ≥ |q 0 Σzi |}     +
                                                                                       1/2
                                              0                                       2
                                    + kΣk E zi zi max (θ` (xi , α) − θ` (xi , α̃))
                                                    `∈{0,1}


                                       h      i              1/2
By assumption, E zi0 zi θ` (xi , α)2 ≤ E kzi k4 E θ` (xi , α)4
                                   
                                                                     is bounded uniformly in
α. Also,
 h                                                                    i
E zi0 zi (θ1 (xi , α) − θ0 (xi , α))2 1{|q 0 Σzi |/ kzi k ≤ kq − q̃k} .
          h        i1/2               1/2                    1/2    0                          1/2
   . E kzi k4            E θ1 (xi , α)4       + E θ0 (xi , α)4
                                                     
                                                                       E 1 |q Σzi |/ kzi k ≤ kq − q̃k

    . kq − q̃km/2


where we have used the smoothness condition (C1) and the fact that E[kzi k4 ] < ∞ and
E[θ` (xi , α)4 ] < ∞ uniformly in α.
  By assumption, θ` (x, α) are Hölder continuous in α with coefficient L(x), so

                                                             1/2     h       i1/2 
                                                          2                                  1/2
       E       zi0 zi   max (θ` (xi , α) − θ` (xi , α̃))             .E kzi k4     E L(xi )4      kα − α̃kγθ
                        `∈{0,1}
                                                                     . kα − α̃kγθ


  Thus,

                                  ρ2 (h3 (t), h3 (t̃)) . kq − q̃k + kq − q̃km/2 + kα − α̃kγθ
                                                                1∧m/2∧γθ
                                                   . t − t0
46                CHANDRASEKHAR, CHERNOZHUKOV, MOLINARI, AND SCHRIMPF


     For h4 , we have
                                       h                                           2 i1/2
                 ρ2 h4 (t), h4 (t̃) =E q 0 Σzi wi,q0 Σ (α) − q̃ 0 Σzi wi,q̃0 Σ (α̃)
                                   

                                       h                           2 i1/2
                                     ≤E (q − q̃)0 Σzi wi,q0 Σ (α)            +
                                         h                                        2 i1/2
                                      + E q̃ 0 Σzi wi,q0 Σ (α) − wi,q̃0 Σ (α)              +
                                         h                                        2 i1/2
                                      + E q̃ 0 Σzi wi,q̃0 Σ (α) − wi,q̃0 Σ (α̃)

                                      . kq − q̃k + kq − q̃km/2 + kα − α̃kγθ
by the exact same arguments used for h3 .
  Claim 2. It suffices to show that E [hj (t)] for j = 1, ..., 4 and E [hj (t)hk (t0 )] for j = 1, ..., 4
and k = 1, ..., 4 are uniformly equicontinuous. Hölder continuity implies equicontinuity, so
we show that each of these functions are uniformly Hölder continuous.
     Jensen’s inequality and the result in Part 1 show that E[hj (t)] are uniformly Hölder.
                                              h                  2 i1/2          c
                   E[hj (t)] − E[hj (t0 )] ≤ E hj (t) − hj (t0 )         . t − t0

Given this, a simple calculation shows that E [hj (t1 )hk (t2 )] are uniformly Hölder as well.
                                                     (hj (t1 ) − hj (t01 )) hk (t2 )+
                                                                                      
                                  0       0
                                            
        E hj (t1 )hk (t2 ) − hj (t1 )hk (t2 ) = E
                                                     +hj (t01 ) (hk (t2 ) − hk (t02 ))
                                                 h                     2 i1/2              1/2
                                               ≤E hj (t1 ) − hj (t01 )         E[hk (t2 )2 ] +
                                                                1/2
                                                                      h                      2 i1/2
                                                + E[hj (t01 )2 ] E hk (t2 ) − hk (t02 )
                                                              c                c
                                                 . t1 − t01       ∨ t2 − t02

     Claim 3. By the law of total variance,
                        var(h(t)) = E [var (h(t)|xi , zi )] + var (E [h(t)|xi , zi ]) .
Note that h3 (t) and h4 (t) are constant conditional on xi , zi , so
         var (h(t)|xi , zi ) =var (h1 (t) + h2 (t)|xi , zi )
                                                                     0
                               q ΣE[zi p0i 1{q 0 Σzi > 0}]J1−1 (α)pi
                               0                                                           
                                                                              ϕi1 (α)
                             = 0                                        var            |xi , zi ×
                               q ΣE[zi p0i 1{q 0 Σzi < 0}]J0−1 (α)pi          ϕi0 (α)
                                   q ΣE[zi p0i 1{q 0 Σzi > 0}]J1−1 (α)pi
                                  0                                     
                              × 0                                                                 (B.4)
                                   q ΣE[zi p0i 1{q 0 Σzi < 0}]J0−1 (α)pi
Recall that b1 = E[zi p0i 1{q 0 Σzi > 0}] and b0 = E[zi p0i 1{q 0 Σzi < 0}]. Let γ` = q 0 Σb` , and
mineig(M ) denote the minimal eigenvalue of any matrix M . By assumption,
                                                    0          
                         mineig var ϕi1 (α) ϕi0 (α) |xi , zi          > L,
                BEST LINEAR APPROXIMATIONS TO SET IDENTIFIED FUNCTIONS                                  47


so
                                             h                                           2i
                E [var (h(t)|xi , zi )] &E          γ1 J1−1 (α)pi γ0 J0−1 (α)pi
                                             h                             i
                                                                         2 2
                                      &E                 ∨ γ0 J0−1 (α)pi
                                                  γ1 J1−1 (α)pi
                                        h                i    h                                     i
                                                       2                                        2
                                      &E γ1 J1−1 (α)pi     ∨ E γ0 J0−1 (α)pi

Repeated use of the inequality kxyk2 ≥ mineig(yy 0 ) kxk2 yields for l = 0, 1,
                h                i
                               2                                       2
             E γ` J`−1 (α)pi       ≥ mineig E pi p0i mineig J`−1 (α) kγ` k2
                                                   

                                                                               2
                                      & mineig(b0` b` ) q 0 Σ
                                      & b0` b`
where the last line follows from the fact that b0` b` is a scalar. We now show that b0` b` > 0.
Let f1i = zi 1{q 0 Σzi > 0} and f0i = zi 1{q 0 Σzi < 0}. Observe that zi = f1i + f0i and
    0 f ] = 0, so
E [f1i 0i
                               0          0      1 
                                              f0i ≥ E zi0 zi > 0
                                                               
                            E f1i f1i ∨ E f0i
                                                       2
By the completeness of our series functions, we can represent f1i and f0i in terms of the
series functions. Let
                                       ∞
                                       X               X∞
                                f1i =    c1j pji f0i =     c0j pji
                                          j=1                          j=1
Without loss of generality, assume the series functions are orthonormal. Then
                                          ∞                 ∞
                                0      X   2
                                                  0      X
                              E f1i f1i =   c1j E f0i f0i =   c20j
                                                 j=1                               j=1

Also,
                                                              k
                                                              X
                                                 b0` b`   =         c2`j
                                                              j=1
Thus,
                     E [var (h(t)|xi , zi )] & mineig(b01 b1 ) ∨ mineig(b00 b0 ) > 0
                                                                                                        
Lemma 7. Suppose Conditions C1-C5 hold, with x replaced by (x, v) and the supremum
over x ∈ X replaced by the supremum over (x, v) ∈ X × V. Let V be a finite set. Let
ϑbk` = arg minϑ En [m(yi` , pk (xi , vi )0 ϑ, α)] . To simplify notation, let ` = 1 so that we focus
on the upper bounding function. Let
                                      v1∗ = arg min θ1 (x, v, α)
                                                          v∈V

and assume that   v1∗   is the unique minimizer of θ1 (x, v, α). Let
                                    vb1n ∈ arg min pk (xi , vi )0 ϑbk1 .
                                                     v∈V
48                 CHANDRASEKHAR, CHERNOZHUKOV, MOLINARI, AND SCHRIMPF


Let ϑk` = arg minϑ E [m(yi` , pk (xi , vi )0 ϑ, α)] and assume that
                                 sup          |pk (x, v, α)ϑk1 − θ1 (x, v, α)| = O(k −r )                       (B.5)
                           (x,v)∈X×V,α∈A

for some r > 0.14 Then the estimator pk (xi , vb1n )0 ϑbk1 satisfies Condition C2.

                           √                                        
Proof. We show that          n pk (xi , vb1n )0 ϑb1 − θ1 (x, v1∗ , α) satisfies equation (4.1). To simplify
notation, omit dependence of ϑ,       b ϑ on k in what follows. Observe that:
  √                                         
    n pk (xi , vb1n )0 ϑb1 − θ1 (x, v1∗ , α)
  √                                           √
 = n pk (xi , vb1n )0 ϑb1 − pk (xi , v1∗ )0 ϑ1 + n pk (xi , v1∗ )0 ϑ1 − θ1 (x, v1∗ , α)
                                                                                          

  √                                                             
 = n pk (xi , vb1n )0 ϑb1 − pk (xi , v1∗ )0 ϑ1 + O n1/2 k −r
  √                                          √                                                        
 = n pk (xi , v1∗ )0 ϑb1 − pk (xi , v1∗ )0 ϑ1 + n pk (xi , vb1n )0 ϑb1 − pk (xi , v1∗ )0 ϑb1 + O n1/2 k −r ,
                                                                                                        (B.6)
    √                                          
and n pk (xi , v1∗ )0 ϑb1 − pk (xi , v1∗ )0 ϑ1 satisfies Condition C2 by assumption. We are left to
show that the second term in equation (B.6) converges to zero in probability as n → ∞.
Recall that vb1n is a minimizer of pk (xi , vi )0 ϑb1 and v1∗ is the unique minimizer of θ1 (x, v, α),
and therefore
       √                                             
0 ≥      n pk (xi , vb1n )0 ϑb1 − pk (xi , v1∗ )0 ϑb1                                                     (B.7)
       √                                               √
         n pk (xi , vb1n )0 ϑb1 − pk (xi , vb1n )0 ϑ1 + n pk (xi , vb1n )0 ϑ1 − pk (xi , v1∗ )0 ϑ1
                                                                                                   
   =
         √                                            
       + n pk (xi , v1∗ )0 ϑ1 − pk (xi , v1∗ )0 ϑb1
       √                                               √                                                  
   =     n pk (xi , vb1n )0 ϑb1 − pk (xi , vb1n )0 ϑ1 + n (θ1 (x, vb1n , α) − θ1 (x, v1∗ , α)) + O n1/2 k −r
         √                                            
       − n pk (xi , v1∗ )0 ϑb1 − pk (xi , v1∗ )0 ϑ1
       √                                                                
   ≥     n (pk (xi , vb1n ) − pk (xi , v1∗ ))0 ϑb1 − ϑ1 + O n1/2 k −r
     = op (1) .                                                                                                  (B.8)
To establish the last equality in (B.8), let V = v 1 , ..., v J
                                                                 
                                                                               and notice that
                                J
                                X
                       0
           pk (xi , ṽ1 ) ϑ =         pk (xi , vj )0 ϑ1 (ṽ1 = vj ) ,   ϑ = ϑb1 , ϑ1 , and ṽ1 = vb1n , v1∗ .
                                j=1


   14As stated in the examples, when using either polynomials or splines, this condition is met for mean
regression with r = ds , for quantile regression with r = s−ad
                                                            d
                                                               , and for distribution regression with r = s−2ad
                                                                                                            2d
                                                                                                                ,
where s is the smoothness of θ` , d is the dimension of x, and a = 1 for polynomials or a = 1/2 for splines.
Notice that n1/2 k−r → 0 for our motivating examples under the conditions derived in Section 4.2.
                   BEST LINEAR APPROXIMATIONS TO SET IDENTIFIED FUNCTIONS                                         49


Hence,
                                                       J
√                                                 √ X                        
    n (pk (xi , vb1n ) − pk (xi , v1∗ ))0 ϑb1 − ϑ1 = n   pk (xi , vj )0 ϑb1 − ϑ1 [1 (b
                                                                                     v1n = vj ) − 1 (v1∗ = vj )] .
                                                           j=1

Standard results give
                                   J                              √ 
                                √ X                        
                                 n   pk (xi , vj )0 ϑb1 − ϑ1 = Op   k .                                       (B.9)
                                     j=1

Next, observe that
                                                                                              
            v1n = v1∗ } = P pk (xi , v1∗ )0 ϑb1 ≤ pk (xi , v j )0 ϑb1 ∀v j ∈ V : v j 6= v ∗
         P {b
                                                                                                 !
              pk (xi , v1∗ )0 ϑb1 − θ1 (x, v j , α) − θ1 (x, v ∗ , α) ≤       j            j   ∗
    =    P                                                                 ∀v ∈ V : v 6= v
              ≤ pk (xi , v j )0 ϑb1 − θ1 (x, v j , α) − θ1 (x, v ∗ , α)
                                                                                                                      !
                pk (xi , v1∗ )0 ϑb1 − θ1 (x, v ∗ , α) − pk (xi , v j )0 ϑb1 − θ1 (x, v j , α) ≤
    =    P                                                                                          ∀v j ∈ V : v j 6= v ∗
              ≤ θ1 (x, v j , α) − θ1 (x, v ∗ , α)
                                                                                                                        !
                 pk (xi , v1∗ )0 ϑb1 − θ1 (x, v ∗ , α) − pk (xi , v j )0 ϑb1 − θ1 (x, v j , α) ≤
    ≥    P                                                                                            ∀v j ∈ V : v j 6= v ∗
                             j
              ≤ θ1 (x, v , α) − θ1 (x, v , α)    ∗
                                                                                                                       !
               pk (xi , v1∗ )0 ϑb1 − θ1 (x, v ∗ , α) + pk (xi , v j )0 ϑb1 − θ1 (x, v j , α) ≤      j         j      ∗
    ≥    P                                                                                        ∀v ∈ V : v 6= v
              ≤ θ1 (x, v j , α) − θ1 (x, v ∗ , α)
                   (                   )
                         k 1/2 −r
    &p   1 − max                    ,k      ,
                         n

where the first inequality follows from the assumption that θ1 is uniquely minimized at v ∗
and the last line follows from (B.5) and (B.9). This yields
                                                          (          )!
                                       ∗                    k 1/2 −r
                      v1n = vj ) − 1 (v1 = vj )] = Op max
                  [1 (b                                           ,k
                                                            n

and hence as long as k ∝ nγ for any γ < 12 , the claim follows. Note that this condition
is consistent with the requirements on the growth of k that we obtained in verifying our
assumptions for the motivating examples in Section 4.2.                                

B.4. Conservative Inference with Discrete Covariates. Let Θ(x, α) = [θ0 (x, α), θ1 (x, α)] ,
and to simplify notation suppress the dependence of Θ and θ` on (x, α) and let the in-
struments coincide with x = [x1 x2 ]0 , with x1 = 1 and x2 ∈ Rd−1 . Let Σ =E(xx0 )−1 ,
z = x + σ [0 η]0 , with η ∼ N (0, I) and independent of x and θ` , ` = 0, 1, where I denotes
the identity matrix. Note that E(xx0 ) =E(zx0 ) , and define

                                    B = ΣE (xΘ) ,            B̃ = ΣE (zΘ) ,
50                       CHANDRASEKHAR, CHERNOZHUKOV, MOLINARI, AND SCHRIMPF


where E (·) denotes the Aumann expectation of the random set in parenthesis, see Molchanov
(2005, Chapter 2). Denote by B b̃ the estimator of B̃ (the unique convex set correspond-
ing to the estimated support function) and by Bbcn(1−τ ) a ball in Rd centered at zero
and with radius b                  cn(1−τ ) the Bayesian bootstrap estimate of the 1 − τ quan-
                   cn(1−τ ) , with b
tile of f (G [hk (t)]) , with f (s(t)) = supt∈T {−s (t)}+ , see Section 4.3. Following argu-
ments in BM (Section 2.3), one can construct a (convex) confidence set CSn such that
supα∈A σCSn (q) − σ b̃ (q, α) = b    cn(1−τ ) for all q ∈ S d−1 , where σA (·) denotes the support
                        B
function of the set A. It then follows that
                                                                       !
                           lim P           sup       σB̃ (q, α) − σCSn (q)   +
                                                                                 =0       = 1 − τ.
                          n→∞        q,α∈S d−1 ×A
                                                                   0
Lemma 8. For a givenδ > 0,  one can jitter x via z = x + σδ [0 η] , so as to obtain a set
B̃ such that supα∈A ρH B̃, B ≤ δ and
                                                                    !
        1 − γ(δ) ≥ lim P                   sup      |σB (q, α) − (σCSn (q) + δ)|+ = 0          ≥ 1 − τ,        (B.10)
                          n→∞        q,α∈S d−1 ×A
                                                    
where γ(δ) = P supt∈T {−G [hk (t)]}+ > cn(1−τ ) + 2δ .
                             
Proof. Observe that ρH B̃, B = ρH (ΣE (zΘ) , ΣE (xΘ)) . By the properties of the Au-
mann expectation (see, e.g., Molchanov (2005, Theorem 2.1.17)),
                             ρH (ΣE (zΘ) , ΣE (xΘ)) ≤ E [ρH (Σ (zΘ) , Σ (xΘ))] .
In turn,
         sup E [ρH (Σ (zΘ) , Σ (xΘ))]
         α∈A
               "                                                                  #
     = sup E              sup       sup (v1 + z2 v2 ) θ̃ − sup (v1 + x2 v2 ) θ
         α∈A         v=Σ0 q:kvk=1 θ∈Θ                       θ∈Θ
                 "
     = sup E              sup       (v1 + x2 v2 + σηv2 ) (θ0 1 (v1 + x2 v2 + σηv2 < 0) + θ1 1 (v1 + x2 v2 + σηv2 > 0))
         α∈A         v=Σ0 q:kvk=1
                                                                                      
         −(v1 + x2 v2 ) (θ0 1 (v1 + x2 v2 < 0) + θ1 1 (v1 + x2 v2 > 0))
              "                                                                                                    #
     ≤ sup E              sup       |σηv2 (θ0 1 (v1 + x2 v2 + σηv2 < 0) + θ1 1 (v1 + x2 v2 + σηv2 > 0))|
         α∈A         v=Σ0 q:kvk=1
                     "                                                                                                               #
         + sup E             sup        |(v1 + x2 v2 ) (θ1 − θ0 ) (1 (0 < − (v1 + x2 v2 ) < σηv2 ) − 1 (0 < v1 + x2 v2 < −σηv2 ))|
           α∈A           v=Σ0 q:kvk=1
                                                                                  
     ≤ σE |η| sup E |θ0 (x, α)| + sup E |θ1 (x, α)| + sup E |θ1 (x, α) − θ0 (x, α)| .
                     α∈A                     α∈A                   α∈A

                                                                              δ
     Hence, we can choose σδ =              E|η|(supα∈A E|θ0 (x,α)|+supα∈A E|θ1 (x,α)|+supα∈A E|θ1 (x,α)−θ0 (x,α)|)
                                                                                                                    .
               BEST LINEAR APPROXIMATIONS TO SET IDENTIFIED FUNCTIONS                         51
                                           
   Now observe that because supα∈A ρH B̃, B ≤ δ, we have B(α) ⊆ B̃(α) ⊕ Bδ for all
α ∈ A, where “⊕” denotes Minkowski set summation and Bδ is a ball of radius δ centered
at the origin. Therefore
                                 ≤ 0 ∀ q ∈ S d−1
                               
      sup σB̃ (q, α) − σCSn (q)
     α∈A
                                     =⇒ sup (σB (q, α) − (σCSn (q) + δ)) ≤ 0 ∀ q ∈ S d−1 ,
                                           α∈A

from which the second inequality in (B.10) follows. Notice also that B̃(α) ⊆ B(α) ⊕ Bδ for
all α ∈ A, and therefore
  sup (σB (q, α) − (σCSn (q) + δ))     ≤    0 ∀ q ∈ S d−1
  α∈A
                                      =⇒ sup σB̃ (q, α) − (σCSn (q) + 2δ) ≤ 0 ∀ q ∈ S d−1 ,
                                                                         
                                            α∈A

from which the first inequality in (B.10) follows. Because δ > 0 is chosen by the researcher,
inference is arbitrarily slightly conservative. Note that a similar argument applies if one
uses a Kolmogorov statistic rather than a directed Kolmogorov statistic. Moreover, the
Hausdorff distance among convex compact sets is larger than the Lp distance among them
(see, e.g., Vitale (1985, Theorem 1)), and therefore a similar conclusion applies for Cramer-
Von-Mises statistics.                                                                      

B.5. Lemmas on Entropy Bounds. We collect frequently used facts in the following
lemma.
Lemma 9. Let Q be any probability measure whose support concentrates on a finite set.
   (1) Let F be a measurable VC class with a finite VC index k or any other class whose
       entropy is bounded above by that of such a VC class, then its entropy obeys
                         log N (kF kQ,2 , F, L2 (Q)) . 1 + k log(1/)
        Examples include e.g., linear functions F = {α0 wi , α ∈ Rk , kαk ≤ C} and their
        indicators F = {1{α0 wi > 0}, α ∈ Rk , kαk ≤ C}.

   (2) Entropies obey the following rules for sets created by addition, multiplication, and
       unions of measurable function sets F and F 0 :
               log N (kF + F 0 kQ,2 , F + F 0 , L2 (Q)) ≤ B
               log N (kF · F 0 kQ,2 , F · F 0 , L2 (Q)) ≤ B
               log N (kF ∨ F 0 kQ,2 , F ∪ F 0 , L2 (Q)) ≤ B
                                                                                   
               B = log N      kF kQ,2 , F, L2 (Q) + log N       kF 0 kQ,2 , F 0 , L2 (Q) .
                            2                                 2
   (3) Entropies are preserved by multiplying a measurable function class F with a random
       variable gi :
               log N (k|g|F kQ,2 , gF, L2 (Q)) . log N /2kF kQ,2 , F, L2 (Q)
                                                                                      
52                CHANDRASEKHAR, CHERNOZHUKOV, MOLINARI, AND SCHRIMPF


      (4) Entropies are preserved by integration or taking expectation: for f ∗ (x) :=
                                                                                         R
                                                                                             f (x, y)dµ(y)
          where µ is some probability measure,
                    log N (kF kQ,2 , F ∗ , L2 (Q)) ≤ log N kF kQ,2 , F, L2 (Q)
                                                                                


  Proof. For the proof of (1)-(3) see e.g., Andrews (1994). For the proof of (4), see e.g.,
Ghosal, Sen, and van der Vaart (2000, Lemma A2).                                         
     Next consider function classes and their envelops
           H1 = {q 0 ΣE[zi p0i 1{q 0 Σzi < 0}]J0−1 (α)pi ϕi0 (α), t ∈ T }, H1 . kzi kξk F1
           H2 = {q 0 ΣE[zi p0i 1{q 0 Σzi > 0}]J1−1 (α)pi ϕi1 (α), t ∈ T }, H2 . kzi kξk F1
           H3 = {q 0 Σxi zi0 ΣE zi wi,q0 Σ (α) , t ∈ T }, H3 . kxi kkzi k
                                             

           H4 = {q 0 Σzi wi,q0 Σ (α), t ∈ T }, H4 . kzi kF2
            F3 = {µ̄0 J −1 (α)pi ϕi (α), α ∈ A}, F3 . ξk F1 ,                                  (B.11)
where µ̄0 is defined in equation (B.2).
Lemma 10. 1. (a) The following bounds on the empirical entropy apply
                          log N (kH1 kPn ,2 , H1 , L2 (Pn )) .P log n + log(1/)
                          log N (kH2 kPn ,2 , H2 , L2 (Pn )) .P log n + log(1/)
                          log N (kF3 kPn ,2 , F3 , L2 (Pn )) .P log n + log(1/)
(b) Moreover similar bounds apply to function classes gi (Hlo − Hlo ) with the envelopes given
by |gi |4H`, where gi is a random variable.
     2. (a) The following bounds on the uniform entropy apply
                             sup log N (kH1 kQ,2 , H1 , L2 (Q)) . k log(1/)
                              Q

                             sup log N (kH2 kQ,2 , H2 , L2 (Q)) . k log(1/)
                              Q

                             sup log N (kF3 kQ,2 , F3 , L2 (Q)) . k log(1/)
                              Q

                             sup log N (kH3 kQ,2 , H3 , L2 (Q)) . log(1/)
                              Q

                             sup log N (kH4 kQ,2 , H4 , L2 (Q)) . log(1/).
                              Q

(b) Moreover similar bounds apply to function classes gi (Hlo − Hlo ) with the envelopes given
by |gi |4H`, where gi is a random variable.

  Proof. Part 1 (a). Case of H1 and H2 . We shall detail the proof for this case, while
providing shorter arguments for others, as they are simpler or similar.
   Note that H1 ⊆ M1 · M2 · F1 , where M1 = {q 0 Σzi , q ∈ S d−1 } with envelope M1 = kzi k
is VC with index dim(zi ) + dim(xi ), and M2 = {γ(q)J0−1 (α)pi , (q, α) ∈ S d−1 × A} with
envelope M2 . kξk k, F1 = {ϕi0 (α), α ∈ A} with envelope F1 , where γ(q) is uniformly
                BEST LINEAR APPROXIMATIONS TO SET IDENTIFIED FUNCTIONS                          53


Holder in q ∈ S d−1 by Lemma 3. Elementary bounds yield
                        km2 (t) − m2 (t̃)kPn ,2 ≤ L1n kα − α̃k + L2n kq − q̃k,
                        L1n . sup kJ −1 (α)kkξk k L2n . kEn [pi p0i ]k,
                               α∈A
                        log L1n .P log n and log L2n .P 1.
Note that log ξk . log n by assumption, supα∈A kJ −1 (α)k . 1 by assumption, kEn [pi p0i ]k .P
1 by Lemma 11. The sets S d−1 and A are compact subsets of Euclidian space of fixed
dimension, and so can be covered by a constant times 1/c balls of radius  for some
constant c > 0. Therefore, we can conclude
                     log N (kM2 kPn ,2 , M2 , L2 (Pn )) .P log n + log(1/).
Repeated application of Lemma 9 yields the conclusion, given the assumption on the func-
tion class F1 . The case for H2 is very similar.
  Case of F3 . Note that F3 ⊂ M2 · F1 and kµ̄k = oP (1) by Step 4 in the proof of Lemma
1. Repeated application of Lemma 9 yields the conclusion, given the assumption on the
function class F1 .
  Part 1 (b). Note that Ho = H − E[Ho ], so it is created by integration and summation.
Hence repeated application of Lemma 9 yields the conclusion.
   Part 2. (a) Case of H1 , H2 , and F3 . Note that all of these classes are subsets of
{µ0 pi , kµk ≤ C} · F1 with envelope ξk F1 . The claim follows from repeated application
of Lemma 9.
   Case of H3 . Note that H3 ⊂ {q 0 Σxi zi0 µ, kµk ≤ C} with envelope kxi kkzi k. The claim
follows from repeated application of Lemma 9.
   Case of H4 . Note that H4 is a subset of a function class created from taking the class
F2 multiplying it with indicator function class 1{q 0 Σzi > 0, q ∈ S d−1 } and with function
class {q 0 Σzi , q ∈ S d−1 } and then adding the resulting class to itself. The claim follows from
repeated application of Lemma 9.
  Part 2 (b). Note that Ho = H − E[Ho ], so it is created by integration and summation.
Hence repeated application of Lemma 9 yields the conclusion.
                                                                                                

B.6. Auxiliary Maximal and Random Matrix Inequalities. We repeatedly use the
following matrix LLN.
Lemma 11 (Matrix LLN). Let Q1 , ..., Qn be i.i.d. symmetric non-negative matrices such
that Q = EQi and kQi k ≤ M , then for Q      b = En Qi
                                               r
                                EkQb − Qk . M (1 + kQk) log k .
                                                       n
                           0
In particular, if Qi = pi pi , with kpi k ≤ ξk , then
                                                s
                                                    2
                                EkQb − Qk . ξk (1 + kQk) log k .
                                                       n
54                CHANDRASEKHAR, CHERNOZHUKOV, MOLINARI, AND SCHRIMPF


Proof. This is a variant of a result from Rudelson (1999). By the symmetrization lemma,
                                    b − Q ≤ 2EE kEn [i Qi ]k
                             ∆ := E Q
where i are Rademacher random variables. The Khintchine inequality for matrices, which
was shown by Rudelson (1999) to follow from the results of Lust-Piquard and Pisier (1991),
states that                                r
                                             log k            1/2
                        E kEn [i Qi ]k .           En [Q2i ]     .
                                               n
Since (remember that k·k is the operator norm)
                            1/2                  1/2
                 E En [Q2i ]      = E En [Q2i ]        ≤ [M E kEn Qi k]1/2 ,
and
                                     kEn Qi k ≤ ∆ + kQk ,
one has
                                      r
                                          M log k
                                ∆≤2               [∆ + kQk]1/2 .
                                            n
Solving for ∆ gives
                            s                                 2
                                4M kQk log k         M log k            M log k
                       ∆≤                    +                      +           ,
                                    n                  n                  n
                                                     M log k
which implies the result stated in the lemma if        n       < 1.                           
     We also use the following maximal inequality.
Lemma 12. Consider a separable empirical process Gn (f ) = n−1/2 ni=1 {f (Zi )−E[f (Zi )]},
                                                                        P
where Z1 , . . . , Zn is an underlying independent data sequence on the space (Ω, G, P), defined
over the function class F, with an envelope function F ≥ 1 such that log[maxi≤n kF k] .P
log n and                                           
                      log N ε kF kPn ,2 , F, L2 (Pn ) ≤ υm log(κ/), 0 <  < 1,
with some constants 0 < log κ . log n, m potentially depending on n, and 1 < υ . 1. For
any δ ∈ (0, 1), there is a constant Kδ large enough, such that for n sufficiently large
       (                                  (                                    ))
                            p
     P sup |Gn (f )| ≤ Kδ m log n max         sup kf (Zi )kP,2 , sup kf kPn ,2    ≥ 1 − δ.
           f ∈F                              i≤n,f ∈F                    f ∈F

Proof. This is a restatement of Lemma 19 from Belloni and Chernozhukov (2009b).               
                                     BEST LINEAR APPROXIMATIONS TO SET IDENTIFIED FUNCTIONS                                                                          55


                             Appendix C. Additional Results for the Empirical Application



                                                 Figure C.1. Quantile bounds for singles
                                           1975-1979                                 1995-1999
                1.5                                                                                 1.5
                                                                    QR                                                                              QR
                                                                   Low                                                                             Low
                  1                                               High                                1                                           High
                                                               Low CB                                                                          Low CB
                                                               High CB                                                                         High CB
                0.5                                                                                 0.5
 log wage gap




                                                                                     log wage gap
                  0                                                                                   0

                -0.5                                                                                -0.5

                 -1                                                                                  -1

                -1.5                                                                                -1.5

                 -2                                                                                  -2

                       0.1     0.2   0.3     0.4   0.5   0.6    0.7      0.8   0.9                         0.1   0.2   0.3   0.4   0.5   0.6    0.7      0.8   0.9
                                                   τ                                                                               τ


Estimated quantile gender wage gap (female − male) conditional on being single with average characteristics.
The solid black line shows the quantile gender wage gap when selection is ignored. The blue and red lines
with upward and downward pointing triangles show upper and lower bounds that account for employment
selection for females. The dashed lines represent a uniform 90% confidence region for the bounds.




                                     Figure C.2. Quantile bounds for ≥ 16 years of education
                                      1975-1979                                  1995-1999
                1.5                                                                                 1.5
                                                                    QR                                                                              QR
                                                                   Low                                                                             Low
                  1                                               High                                1                                           High
                                                               Low CB                                                                          Low CB
                                                               High CB                                                                         High CB
                0.5                                                                                 0.5
 log wage gap




                                                                                     log wage gap




                  0                                                                                   0

                -0.5                                                                                -0.5

                 -1                                                                                  -1

                -1.5                                                                                -1.5

                 -2                                                                                  -2

                       0.1     0.2   0.3     0.4   0.5   0.6    0.7      0.8   0.9                         0.1   0.2   0.3   0.4   0.5   0.6    0.7      0.8   0.9
                                                   τ                                                                               τ


Estimated quantile gender wage gap (female − male) conditional on having at least 16 years of education
with average characteristics. The solid black line shows the quantile gender wage gap when selection is
ignored. The blue and red lines with upward and downward pointing triangles show upper and lower bounds
that account for employment selection for females. The dashed lines represent a uniform 90% confidence
region for the bounds.
56                                 CHANDRASEKHAR, CHERNOZHUKOV, MOLINARI, AND SCHRIMPF



                                   Figure C.3. Quantile bounds for ≤ 12 years of education
                                    1975-1979                                  1995-1999
                1.5                                                                             1.5
                                                                QR                                                                              QR
                                                               Low                                                                             Low
                  1                                           High                                1                                           High
                                                           Low CB                                                                          Low CB
                                                           High CB                                                                         High CB
                0.5                                                                             0.5
 log wage gap




                                                                                 log wage gap
                  0                                                                               0

                -0.5                                                                            -0.5

                 -1                                                                              -1

                -1.5                                                                            -1.5

                 -2                                                                              -2

                       0.1   0.2   0.3   0.4   0.5   0.6    0.7      0.8   0.9                         0.1   0.2   0.3   0.4   0.5   0.6    0.7      0.8   0.9
                                               τ                                                                               τ


Estimated quantile gender wage gap (female − male) conditional on having 12 or fewer years of education
with average characteristics. The solid black line shows the quantile gender wage gap when selection is
ignored. The blue and red lines with upward and downward pointing triangles show upper and lower bounds
that account for employment selection for females. The dashed lines represent a uniform 90% confidence
region for the bounds.




                             Figure C.4. Quantile bounds for ≥ 16 years of education and single
                                  1975-1979                                   1995-1999
                1.5                                                                             1.5
                                                                QR                                                                              QR
                                                               Low                                                                             Low
                  1                                           High                                1                                           High
                                                           Low CB                                                                          Low CB
                                                           High CB                                                                         High CB
                0.5                                                                             0.5
 log wage gap




                                                                                 log wage gap




                  0                                                                               0

                -0.5                                                                            -0.5

                 -1                                                                              -1

                -1.5                                                                            -1.5

                 -2                                                                              -2

                       0.1   0.2   0.3   0.4   0.5   0.6    0.7      0.8   0.9                         0.1   0.2   0.3   0.4   0.5   0.6    0.7      0.8   0.9
                                               τ                                                                               τ


Estimated quantile gender wage gap (female − male) conditional on being single with at least 16 years
of education and average characteristics. The solid black line shows the quantile gender wage gap when
selection is ignored. The blue and red lines with upward and downward pointing triangles show upper and
lower bounds that account for employment selection for females. The dashed lines represent a uniform 90%
confidence region for the bounds.
                                   BEST LINEAR APPROXIMATIONS TO SET IDENTIFIED FUNCTIONS                                                                        57



                        Figure C.5. Quantile bounds for single and ≥ 16 years of education im-
                        posing stochastic dominance
                                 1975-1979                                1995-1999
                1.5                                                                             1.5
                                                                QR                                                                              QR
                                                               Low                                                                             Low
                  1                                           High                                1                                           High
                                                           Low CB                                                                          Low CB
                                                           High CB                                                                         High CB
                0.5                                                                             0.5
 log wage gap




                                                                                 log wage gap
                  0                                                                               0

                -0.5                                                                            -0.5

                 -1                                                                              -1

                -1.5                                                                            -1.5

                 -2                                                                              -2

                       0.1   0.2   0.3   0.4   0.5   0.6    0.7      0.8   0.9                         0.1   0.2   0.3   0.4   0.5   0.6    0.7      0.8   0.9
                                               τ                                                                               τ


Estimated quantile gender wage (female − male) conditional on average characteristics. The solid black line
shows the quantile gender wage when selection is ignored. The blue and red lines with upward and downward
pointing triangles show upper and lower bounds that account for employment selection for females. The
dashed lines represent a uniform 90% confidence region for the bounds.




                        Figure C.6. Quantile bounds for single and ≥ 16 years of education im-
                        posing the median restriction
                                 1975-1979                                1995-1999
                1.5                                                                             1.5
                                                                QR                                                                              QR
                                                               Low                                                                             Low
                  1                                           High                                1                                           High
                                                           Low CB                                                                          Low CB
                                                           High CB                                                                         High CB
                0.5                                                                             0.5
 log wage gap




                                                                                 log wage gap




                  0                                                                               0

                -0.5                                                                            -0.5

                 -1                                                                              -1

                -1.5                                                                            -1.5

                 -2                                                                              -2

                       0.1   0.2   0.3   0.4   0.5   0.6    0.7      0.8   0.9                         0.1   0.2   0.3   0.4   0.5   0.6    0.7      0.8   0.9
                                               τ                                                                               τ




Estimated quantile gender wage (female − male) conditional on average characteristics. The solid black line
shows the quantile gender wage when selection is ignored. The blue and red lines with upward and downward
pointing triangles show upper and lower bounds that account for employment selection for females. The
dashed lines represent a uniform 90% confidence region for the bounds.
58                CHANDRASEKHAR, CHERNOZHUKOV, MOLINARI, AND SCHRIMPF


                                            References
Andrews, D. W. K. (1994): Empirical Process Methods in Econometricsvol. IV of Handbook of Economet-
 rics, pp. 2248–2294.
Andrews, D. W. K., and P. J. Barwick (2012): “Inference for Parameters Defined by Moment Inequal-
 ities: A Recommended Moment Selection Procedure,” Econometrica, 80(6), 2805–2826.
Andrews, D. W. K., and X. Shi (2013): “Inference Based on Conditional Moment Inequalities,” Econo-
 metrica, 81, 609–666.
Andrews, D. W. K., and G. Soares (2010): “Inference for Parameters Defined by Moment Inequalities
 Using Generalized Moment Selection,” Econometrica, 78, 119–157.
Artstein, Z., and R. A. Vitale (1975): “A Strong Law of Large Numbers for Random Compact Sets,”
 The Annals of Probability, 3(5), 879–882.
Belloni, A., and V. Chernozhukov (2009a): “On the computational complexity of MCMC-based esti-
 mators in large samples,” The Annals of Statistics, 37(4), 2011–2055.
            (2009b): “Post-L1-Penalized Estimators in High-Dimensional Linear Regression Models,”
 arXiv:1001.0188.
Belloni, A., V. Chernozhukov, D. Chetverikov, and I. Fernandez-Val (2018): “Conditional Quan-
 tile Processes based on Series or Many Regressors,” Journal of Econometrics, forthcoming.
Beresteanu, A., and F. Molinari (2008): “Asymptotic Properties for a Class of Partially Identified
 Models,” Econometrica, 76(4), 763–814.
Blau, F. D., and L. M. Kahn (1997): “Swimming Upstream: Trends in the Gender Wage Differential in
 the 1980s,” Journal of Labor Economics, 15, 1–42.
          (2017): “The Gender Wage Gap: Extent, Trends, and Explanations,” Journal of Economic Liter-
 ature, 55, 789–865.
Blundell, R., A. Gosling, H. Ichimura, and C. Meghir (2007): “Changes in the Distribution of
 Male and Female Wages Accounting for Employment Composition Using Bounds,” Econometrica, 75(2),
 323–363.
Bontemps, C., T. Magnac, and E. Maurin (2012): “Set Identified Linear Models,” Econometrica, 80,
 1129–1155.
Bugni, F. A. (2010): “Bootstrap Inference in Partially Identified Models Defined by Moment Inequalities:
 Coverage of the Identified Set,” Econometrica, 78, 735–753.
Canay, I. A. (2010): “EL Inference for Partially Identified Models: Large Deviations Optimality and
 Bootstrap Validity,” Journal of Econometrics, 156, 408–425.
Card, D. (1999): The Causal Effect of Education on Earningschap. 30, pp. 1801–1863. Elsevier.
Card, D., and J. E. DiNardo (2002): “Skill Biased Technological Change and Rising Wage Inequality:
 Some Problems and Puzzles,” Journal of Labor Economics, 20, 733–783.
Chen, X. (2007): “Large Sample Sieve Estimation of Semi-Nonparametric Models,” Handbook of Econo-
 metrics, 6.
Chernozhukov, V., H. Hong, and E. Tamer (2007): “Estimation and Confidence Regions for Parameter
 Sets in Econometric Models,” Econometrica, 75(5), 1243–1284.
Chernozhukov, V., E. Kocatulum, and K. Menzel (2015): “Inference on sets in finance,” Quantitative
 Economics, 6, 309–358.
Chernozhukov, V., S. Lee, and A. Rosen (2013): “Intersection Bounds: Estimation and Inference,”
 Econometrica, 81, 667–737.
Davydov, Y. A., M. A. Lifshits, and N. V. Smorodina (1998): Local Properties of Distribuions of
 Stochastic Functionals. American Mathematical Society.
Einav, L., A. Finkelstein, S. P. Ryan, P. Schrimpf, and M. R. Cullen (2013): “Selection on Moral
 Hazard in Health Insurance,” American Economic Review, 103(1), 178–219.
Fang, Z., and A. Santos (2018): “Inference on Directionally Differentiable Functions,” Review of Eco-
 nomic Studies, forthcoming.
Foresi, S., and F. Peracchi (1995): “The Conditional Distribution of Excess Returns: An Empirical
 Analysis,” Journal of the American Statistical Association, 90, 451–466.
                 BEST LINEAR APPROXIMATIONS TO SET IDENTIFIED FUNCTIONS                                   59


Galichon, A., and M. Henry (2009): “A test of non-identifying restrictions and confidence regions for
 partially identified parameters,” Journal of Econometrics, 152(2), 186–196.
Ghosal, S., A. Sen, and A. W. van der Vaart (2000): “Testing Monotonicity of Regression,” Annals
 of Statistics, 28, 1054–1082.
Han, A., and J. A. Hausman (1990): “Flexible Parametric Estimation of Duration and Competing Risk
 Models,” Journal of Applied Econometrics, 5, 1–28.
He, X., and Q.-M. Shao (2000): “On Parameters of Increasing Dimensions,” Journal of Multivariate
 Analysis, 73(1), 120–135.
Hirano, K., G. Imbens, and G. Ridder (2003): “Efficient estimation of average treatment effects using
 the estimated propensity score,” Econometrica, 71(4), 1161–1189.
Imbens, G. W., and C. F. Manski (2004): “Confidence Intervals for Partially Identified Parameters,”
 Econometrica, 72, 1845–1857.
Imbens, G. W., and J. M. Wooldridge (2009): “Recent developments in the econometrics of program
 evaluation,” Journal of Economic Literature, 47, 5–86.
Jasso, G., and M. R. Rosenzweig (2008): “Selection Criteria and the Skill Composition of Immigrants:
 A Comparative Analysis of Australian and U.S. Employment Immigration,” DP 3564, IZA.
Juster, F. T., and R. Suzman (1995): “An Overview of the Health and Retirement Study,” Journal of
 Human Resources, 30 (Supplement), S7–S56.
Kaido, H. (2016): “A Dual Approach to Inference for Partially Identified Econometric Models,” Journal of
 Econometrics, 192, 269–290.
Kaido, H., and A. Santos (2014): “Asymptotically Efficient Estimation of Models Defined by Convex
 Moment Inequalities,” Econometrica, 82, 387–413.
Kline, P., and A. Santos (2013): “Sensitivity to Missing Data Assumptions: Theory and An Evaluation
 of the U.S. Wage Structure,” Quantitative Economics, 4, 231–267.
Lorentz, G. G. (1986): Approximation of Functions. Chelsea.
Lust-Piquard, F., and G. Pisier (1991): “Non commutative Khintchine and Paley inequalities,” Arkiv
 för Matematik, 29(1), 241–260.
Magnac, T., and E. Maurin (2008): “Partial Identification in Monotone Binary Models: Discrete Re-
 gressors and Interval Data,” Review of Economic Studies, 75(3), 835–864.
Manski, C. F. (1994): The selection problemchap. 4, pp. 143–170, Advances in Econometrics, Sixth World
 Congress. Cambridge University Press.
          (2003): Partial Identification of Probability Distributions. Springer Verlag, New York.
          (2007): Identification for Prediction and Decision. Harvard University Press, Cambridge, MA.
Manski, C. F., and E. Tamer (2002): “Inference on Regressions with Interval Data on a Regressor or
 Outcome,” Econometrica, 70(2), 519–546, ArticleType: primary article / Full publication date: Mar., 2002
 / Copyright c 2002 The Econometric Society.
Molchanov, I. (2005): Theory of Random Sets. Springer Verlag, London.
Mulligan, C. B., and Y. Rubinstein (2008): “Selection, Investment, and Women’s Relative Wages Over
 Time,” Quarterly Journal of Economics, 123(3), 1061–1110.
Newey, W. K. (1997): “Convergence rates and asymptotic normality for series estimators,” Journal of
 Econometrics, 79(1), 147–168.
Olley, G. S., and A. Pakes (1996): “The Dynamics of Productivity in the Telecommunications Equipment
 Industry,” Econometrica, 64(6), 1263–1297, ArticleType: primary article / Full publication date: Nov.,
 1996 / Copyright 1996 The Econometric Society.
Picketty, T. (2005): “Top Income Shares in the Long Run: An Overview,” Journal of the European
 Economic Association, 3, 382–392.
Pollard, D. (2002): A User’s Guide to Measure Theoretic Probability. Cambridge.
Rockafellar, R. (1970): Convex Analysis. Princeton University Press.
Romano, J. P., and A. M. Shaikh (2008): “Inference for identifiable parameters in partially identified
 econometric models,” Journal of Statistical Planning and Inference, 138(9), 2786–2807.
         (2010): “Inference for the Identified Set in Partially Identified Econometric Models,” Econometrica,
 78, 169–211.
60                CHANDRASEKHAR, CHERNOZHUKOV, MOLINARI, AND SCHRIMPF


Rosen, A. M. (2008): “Confidence sets for partially identified parameters that satisfy a finite number of
 moment inequalities,” Journal of Econometrics, 146(1), 107–117.
Rudelson, M. (1999): “Random vectors in the isotropic position,” Journal of Functional Analysis, 164(1),
 60–72.
Schneider, R. (1993): Convex Bodies: The Brunn-Minkowski Theory, Encyclopedia of Mathematics and
 Its Applications. Cambridge University Press, Cambridge, UK.
Stoye, J. (2007): “Bounds on Generalized Linear Predictors with Partially Identified Outcomes,” Reliable
 Computing, 13, 293–302.
         (2009): “More on Confidence Intervals for Partially Identified Parameters,” Econometrica, 77,
 1299–1315.
Townsend, R. M., and S. S. Urzua (2009): “Measuring the Impact of Financial Intermediation: Linking
 Contract Theory to Econometric Policy Evaluation,” Macroeconomic Dynamics, 13(Supplement S2), 268–
 316.
van der Vaart, A. W. (2000): Asymptotic statistics. Cambridge University Press.
van der Vaart, A. W., and J. Wellner (1996): Weak Convergence and Empirical Processes: With
 Applications to Statistics. Springer.
Vitale, R. A. (1985): “Lp Metric for Compact, Convex Sets,” Journal of Approximation Theory, 45,
 280–287.

     Department of Economics, Stanford University & Microsoft Research New England

     Department of Economics, M. I. T.

     Department of Economics, Cornell University

     Vancouver School of Economics, University of British Columbia
