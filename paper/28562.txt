                              NBER WORKING PAPER SERIES




                    UNCERTAINTY AND RISK-TAKING IN SCIENCE:
                    MEANING, MEASUREMENT AND MANAGEMENT

                                        Chiara Franzoni
                                         Paula Stephan

                                      Working Paper 28562
                              http://www.nber.org/papers/w28562


                    NATIONAL BUREAU OF ECONOMIC RESEARCH
                             1050 Massachusetts Avenue
                               Cambridge, MA 02138
                                   March 2021




The authors wish to thank Erin Baker, Luigi Buzzacchi, Enrico Cagno, Renato Frey,
Massimiliano Guerini, Francis Halzen, Chris Hesselbein, Greg Petsko, Henry Sauermann, Viola
Schiaffonati, Massimo Tavoni, Paolo Trucco, Reinhilde Veugelers, Roberta Zappasodi for
fruitful discussions. The work contained herein has greatly benefited from discussions with
scholars and professionals who participated in the Science of Science Funding annual meetings at
the NBER Summer Institute and with seminar participants at the Kellogg School of Management,
Northwestern. The views expressed herein are those of the authors and do not necessarily
reflect the views of the National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.

© 2021 by Chiara Franzoni and Paula Stephan. All rights reserved. Short sections of text, not to
exceed two paragraphs, may be quoted without explicit permission provided that full credit,
including © notice, is given to the source.
Uncertainty and Risk-Taking in Science: Meaning, Measurement and Management
Chiara Franzoni and Paula Stephan
NBER Working Paper No. 28562
March 2021
JEL No. H41,I23,I28,J18,O3,O31,O33,O38

                                          ABSTRACT

An underlying rationale for public support of science is that private companies underinvest in
research of a risky nature. Yet, risk in science is a poorly understood concept. This paper sets out
the foundations for understanding, measuring and managing risk in science. We review insights
offered from existing fields that study risk. These contributions, combined with knowledge
gained from studies of science, are used to build a conceptual model of risk in science. The model
is illustrated with examples drawn from the development of the IceCube Neutrino Observatory. It
disentangles different components that determine risk and is used to operationalize an expert-
based risk metric, potentially useful in peer review. Moreover, we review emerging empirical
work on risk-taking in science, most of which suggests that the current reward structure of
science discourages risky research. We develop and outline strategies for hedging and
encouraging risk taking. We conclude by proposing a rich agenda for future studies, which is both
intellectually challenging and critical for the future of science.


Chiara Franzoni
Department of Management, Economics
and Industrial Engineering
Politecnico di Milano
20133 Milan
Italy
chiara.franzoni@polimi.it

Paula Stephan
Department of Economics
Andrew Young School of Policy Studies
Georgia State University
Box 3992
Atlanta, GA 30302-3992
and NBER
pstephan@gsu.edu
         1. Introduction

        An underlying rationale for public support of science is that private companies
underinvest in research of a risky nature (Arrow 1962; Nelson 1959). Both spillovers and the
time horizon necessary to recoup the investment discourage the private sector from investing
in risky research. Yet risky research is essential for shifting the knowledge frontier. To the
extent this was the case in the past, in the days of Bell Labs and other large private research
labs, it is more so today, when private companies, taking a short-run view, engage in less
basic, risky research (Arora, Belenzon, and Patacconi 2018; Budish, Roin, and Williams
2015; Fleming et al. 2019). Yet, among researchers today there is concern that risk-taking in
science is declining not only among those in the private sector but among those working in
the non-profit sector (Edwards et al. 2011; Fedorov, Müller, and Knapp 2010). Many put the
onerous on funding agencies, focusing on ways in which science is funded and grants
awarded as well as tight budgets faced by agencies (Azoulay, Graff Zivin, and Manso 2011;
OECD 2018; Petsko 2011). James Rothman, for example, who shared the Nobel Prize in
Physiology or Medicine in 2013, told an interviewer the day after he received the prize, that
"he was grateful he started work in the early 1970s when the federal government was willing
to take much bigger risks in handing out funding to young scientists." Rothman went on to
say "I had five years of failure, really, before I had the first initial sign of success. And I'd
like to think that that kind of support existed today, but I think there's less of it. And it's
actually becoming a pressing national issue, if not an international issue." (Harris 2013).
There is also evidence that the rewards to science not only discourage risk-taking on the part
of scientists but increasingly do so (Foster, Rzhetsky, and Evans 2015; Stephan, Veugelers,
and Wang 2017; Wang, Veugelers, and Stephan 2017).

        This discussion often occurs in the absence of well-defined and developed concepts of
what uncertainty and risk-taking mean in science.1 This paper sets out to address this void.
The core contributions of the paper are twofold. First, we lay out a conceptual model of risk
in science, which can also be used to obtain expert-based metrics of risk. Second, based on
the model and studies of risk taking, we discuss how the current academic environment




1
  Science is not the only domain in which the term risk is ambiguous or difficult to define. The problems posed
by defining risk have been extensively debated in various fields. For a review see e.g., Hansson (1989) and
Aven (2012).

                                                                                                                  2
affects risk-taking in science, address ways in which risk can be hedged or encouraged and
sketch-out a rich research agenda for future investigation.

         The structure of the paper is as follows. Section 2 clarifies concepts and assumptions
used in the discussion of risk with the goal of eliminating potential sources of confusion and
common misconceptions. Section 3 reviews insights about risk and uncertainty provided by
adjacent literatures. Section 4 provides a full conceptual model of risk in science, using the
IceCube Neutrino Observatory as an example to illustrate risk components. In Section 5, we
explain how the model can be used to obtain an expert-based assessment of risk in science
and review alternative approaches to measuring risk. In Section 6 we review studies of risk
taking in science and conclude that the current reward structure of science discourages risk-
taking. We discuss ways in which risk can be hedged in Section 7 and encouraged in Section
8. We close in Section 9 by outlining directions for future research.




         2. Definitions and potential pitfalls

         The road to the study of risk in science is paved with misconceptions and prone to
misunderstanding. We thus preface this essay by clarifying concepts and assumptions
underlying the discussion.

         First, the terms risk and uncertainty are ambiguous and prone to generate confusion.
Many scholars of economics of science are acquainted with the work of Frank Knight
(Knight 1921), who stressed the measurability of the probability of an event in order to
distinguish risk from uncertainty. This distinction, however, is rarely found in contemporary
economics (Feduzi, Runde, and Zappia 2014). A more recent approach, following Ramsey
and De Finetti, stresses the ubiquitous imperfect information that surrounds agents' decisions
in real life (Marinacci 2015).2 Following this approach, it makes sense only to talk about
subjective probabilities, i.e. degrees of beliefs expressed with more or less confidence.
Subjective probabilities are `measurable' by asking agents their willingness to bet on their
beliefs. This view renders the risk-uncertainty distinction void of substantive meaning.
Recognizing and subscribing to this view means that we should talk only of uncertainty in


2
  In this view, uncertainty is a state of imperfect knowledge that relates to all aspects of a decision, including the
states of the world (e.g., what might happen, which consequences might derive), and not just to the probability
of each state.


                                                                                                                    3
science. However, given that risk is the term most commonly used (albeit not in the
Knightian sense) by scientists, policy makers and granting agencies to speak about the
uncertainty of research, we use the term risk as a synonym for uncertainty in this essay.

       Second, research is not free (Stephan 2012). From the researcher's point of view,
time spent on research has an opportunity cost; from a university and/or foundation's point of
view, the materials, equipment and space devoted to research come with a cost. These costs
should be viewed as participation costs and are equivalent to those incurred when purchasing
a lottery ticket in which one can gain or lose. In this case, participation costs are those
incurred by funders or researchers who `buy' a chance to discover the outcome of a potential
course of action. are fundamentally and conceptually different from the gains and losses that
can result from the course of action. Although participation costs can be one element to
consider when evaluating the yield from doing a research project, they should not be
confused with losses.

       Third, the value associated with successfully accomplishing proposed research and the
prospects of doing so varies across projects. Some projects have a high prospects of
achieving their research goals, others have lower prospects of accomplishing their goals.
Some projects have a high value if successful; others do not. Individuals or organizations that
pursue high value research with low prospects of success can be described as being risk-
takers; those that do not can be described as being risk-averse.

       The discussion of risk in science often conjures up comparisons of risk associated
with financial investments. Such a comparison is beneficial in the sense that the concept of
risk in finance, especially after the dramatic financial crises of 2008 and 2020, are well
known by the public. However, the comparison has drawbacks in the sense that risk and
volatility differ between finance and science in several important respects. First, in finance,
the outcomes of investments vary both in the negative and positive spectrum (as many of
Bernie Madoff's investors learned all too well). In science, instead, volatility is rarely
associated with strictly negative outcomes (losses), unless in cases where research involves
harm to a patient, a researcher or significant damage or loss of resources and reputation.
More commonly, volatility in science refers to the distribution of research findings that can
be thought of as either on the upside or as the status quo, in the sense that, although the
research produces no loss, it does not substantially advance the area of study. Second, in
finance, risk has both a speculative meaning, which focuses on the expected gains associated


                                                                                                  4
with an investment, and a preventive meaning, which focuses on avoiding the losses
associated with an investment. Science funding organizations that embrace the goal of high
risk/high gain, such as the ERC, clearly subscribe to the speculative concept of risk focused
on expected gain. The preventive meaning is also present in science, but not with the goal of
insuring organizations or people against losses, but rather of insuring organizations against
funding research that comes up "empty." One could argue that it is this "empty basket"
outcome that review panels insure against, focusing their review efforts on reasons that the
research will not yield results rather than focusing on the value of the research if it were to
succeed.3 Third, in finance, volatility is a popular construct because past volatility (or lack
thereof) can provide the basis for prediction and calculation of risk associated with an
investment. For example, it is common to calculate the alpha associated with an investment,
as well as other measures such as the beta, standard deviation, R-squared and Sharpe ratio,
based on price fluctuation history. The reason is that past volatility is easy to calculate and
can predict future volatility. Instead, in the case of science, past volatility is not necessarily
predictive of future volatility.4 Moreover, even if volatility were predictable, it would be
both difficult to observe, because many unsuccessful projects go unpublished (Fanelli 2010;
Rosenthal 1979), and to compute, because the value of research findings is not fully
understood, especially in the short term.

        Before commencing, we address some extreme simplifications and misconceptions
that can hijack the discourse on risk and uncertainty in science. One extreme simplification
is that "scientific research is always risky." This blunt approximation fails to recognize a
considerable degree of nuance, including that, in certain areas of research, risk is limited or
mitigated by the character of the research. First, a non-negligible share of research involves
virtually zero risks. Consider, by way of example, the Cochrane reviews that consist of
collecting, coding and jointly-testing the results of multiple studies of the same medical
treatment. Cochrane reviews require rigorous methods and provide valuable results to



3
  A possible reason review panels may focus on the "empty basket" is fear of wasting the limited money they
have. This is an empirical question that deserves investigation. Today insurance against some undesirable
outcomes is assumed at the institutional level, for example via the actions of the IRB and GPRD on ethical and
privacy matters, or via the requirements of funding agencies that the institution certifies access to needed
equipment and resources. As a result, while, in the past panels concerned themselves with insuring against some
of these problems, today panels arguably focus on insuring against coming up empty handed.
4
  An exception is when the routinization of aspects of research provide scientists with increased confidence in
predictions. By way of example, protein structure determination was greatly facilitated by developing an
"automated pipeline for protein production and structure determination" which included the development of
robots that could grow and screen crystals (Stephan 2012: 93).

                                                                                                             5
scholars and practitioners. Yet, the researcher who chooses to perform a Cochrane review
has no doubt that the review can be accomplished and published; the volatility involved in the
outcome is practically nonexistent. Second, some research is bound to produce non-zero
outcomes, although the value of what is found may be uncertain. For example, many
archeological excavations are done after a site has been identified. In these cases, findings are
guaranteed, but there is uncertainty with respect to what they will be and their importance.
Archival studies, large statistical analyses of galaxies, or research on the collateral effects of
approved drugs are other examples of similar situations where risk is very small. Third, some
research projects have predictable outcomes, although there is uncertainty concerning the
time or costs required to obtain them. An example is the Human Genome Project, which was
formally launched in 1990, with the aim of sequencing the 3 billion base pairs of the human
genome. At the time the project was started, the set of techniques readily available was
sufficient to guarantee eventual success. However, there was uncertainty concerning whether
the project could be accomplished in the 15 years that it aspired to. In the end, a working
draft of the genome was obtained within ten years, thanks largely to improvements in the
technology (Stephan 2012: 88). These three cases suggest that a non-negligible part of
research involves projects that are virtually certain to lead to an outcome. In these cases, risk
is confined to the uncertainty associated with the value of the outcomes or to the time and
resources needed to achieve the research, but not to whether an outcome will be forthcoming.
In the next sections we expand the discussion by considering the array of components that
coalesce to determine risk in science.

       Another misconception concerning risk in science is that engaging in a new line of
research is always the riskier course of action, while continuing along an existing research
path is the play-safe alternative. Consider, for example, the case of James P. Allison. Allison
had spent most of his career studying the use of antibodies blocking the immune inhibitory
molecule CTLA-4, as a strategy to unleash the immune response to cancer. In 1995, he
understood that CTLA-4, a T-cell surface receptor, served to dampen T-cells responses and
could be used as a target for cancer immunotherapy (Krummel and Allison 1995). In the
same years, other cancer immunotherapy approaches were being investigated, including
cancer vaccines and agonist antibodies activating immune stimulatory receptors. However,
cancer vaccines showed very limited efficacy, as an influential NIH review pointed-out in
2004 (Rosenberg, Yang, and Restifo 2004), and several agonist antibodies (e.g., targeting
CD28, CD40, or 4-1BB) were found to cause serious adverse effects, which in some cases


                                                                                                     6
were life-threating for healthy patients (Suntharalingam et al. 2006). As a result, the clinical
community was skeptical of Cancer Immunotherapy in general and pharmaceutical
companies were uninterested in further development of these approaches. In spite of this,
Allison engaged with a small biotech company to develop a CTLA-4 blocking antibody for
clinical use to test in cancer patients, called Ipilimumab. The clinical trial was eventually
successful and Ipilimumab became the first immune checkpoint inhibitor drug to receive
FDA approval for cancer treatment in 2011 (Wolchok et al. 2013). Immune checkpoint
inhibitors have since become one of the most promising frontiers of cancer treatment
research. Allison shared the Nobel Prize for Medicine in 2018 (Dobosz and Dziecitkowski
2019). As the example illustrates, in this case the choice to persist could arguably be
described as risky behavior. Thus, undertaking a new line of research is not always the risky
behavior and continuing a line of research does not always imply risk avoidance. Kuhn
refers to this choice as an "essential tension" (Kuhn 1991), noting that both alternatives, not
just the former, are hazardous. Working in a new area of research often requires formulating
new theory and using new methods, both of which arguably involve risk. On the other hand,
persisting along a line of research can often provide a more predictable path, albeit one with
diminishing returns. But it can also lead to a dead-end with no results, perceived as risky
behavior by peers. If we want to understand risk, we should look at the uncertainty of
prospective results and refrain from taking shortcuts that assume identity between risk and
any given observed behavior.

         Finally, a further misconception is that some scientists are prone to take risks and
others are conservative and risk-averse, as if the attitude of scientists towards risk were a
fixed individual characteristic. While recent empirical work has established with reasonable
certainty that attitudes towards risk are stable traits, specific to each individual (Frey et al.
2017; Mata et al. 2018), individual risk preference explains only about half of the predicted
variance in the measures of risk-taking. The rest depends on erratic behavior and on domain-
specific attitudes that individuals show towards various facets of risks encountered in life.5
Although few if any studies have looked specifically at risk-attitude and risk-taking in
scientific research, we would expect that, as for humans in general, the attitude of scientists
for embracing risk in research varies at any given point in time, depending on their role or the
situation that they face. For example, prior correlational studies suggest that scientists who


5
  Domain-specific factors relate, for example, to preferences for risks exhibited in investments, attitude for
seeking thrill and adventure, disinhibition in health risks, et cetera.

                                                                                                                 7
have yet to get tenure, may eschew risk; those who have a stable career position may embrace
risk (Franzoni and Rossi-Lamastra 2017). Other commentators have proposed that scientists
who have obtained success in early-career are less likely to be at risk of losing their
reputation and hence more likely to engage in risky research (Henrickson and Altshuler
2012). Regardless of career stage or past success, we expect scientists to share the trait of
overconfidence about their ability to achieve success consistently found in individuals'
assessment of risk (Camerer and Lovallo 1999; Weinstein 1980).




       3. The Meaning of Risk in Adjacent Literatures

       We frequently use the word risk in everyday life. As such, risk at first appears to be a
rather intuitive concept. However, when making risk a subject of scholarly investigation, we
quickly realize that we lack a shared, let alone precise, understanding of the meaning of risk.
Scholars of risk, recognizing this, have offered extensive discussion on the topic, and have
noted a number of different meanings of risk in different literatures (Althaus 2005; Aven and
Renn 2009; Hansson 2002, 2018). Thus, a pre-condition to holding productive discussions is
to develop a sound conceptual understanding of risk in science, bringing together different
pieces of relevant theory which provide insightful concepts and tools. Failure to do so not
only undermines our ability to make conscious decisions regarding risk, but also leads to a
Tower of Babel in which scientists from different backgrounds implicitly use notions
germane to their discipline, but alien to others. In this section we draw insights from four
main approaches. All see risk as a manifestation of uncertainty, but each focuses on specific
aspects of uncertainty. We explain each in this section. Together, they constitute the building
blocks with which we set about defining the meaning of risk in science, presented in the next
section.

       Risk Analysis. A primary focus in engineering studies of risk regards the occurrence
and impact of potential events. The intent of this literature is fundamentally utilitarian. The
main focus is on representing and quantifying the risks involved in a situation in order to
facilitate making decisions. The quantification of risk in Risk Analysis is sometimes called
`Technical assessment' (Renn 1998) or `Risk metric' (Johansen and Rausand 2014). This is a
pragmatic approach that disentangles complex problems into a number of simple pieces, such
that each item can be quantified in isolation and then combined. The standard model implies
three items (Kaplan and Garrick 1981): the scenarios (e.g. what can happen?), the probability

                                                                                                  8
associated with each scenario (e.g., how likely is this to happen?) and the consequences
associated with each scenario (e.g., what loss/gain would this lead to?). More sophisticated
models can further disentangle more fine-grained items (Aven 2011). In applied risk
analyses, each item is analyzed by one or more experts and the combined outcome is simply
calculated as probability times consequence (Kasperson et al. 1988). This approach for
quantifying risk is commonly used in insurance, where the primary focus is on unwanted
events, i.e. events that deserve special consideration because of their negative impact.6
Mirroring this view, in the insurance literature it is common to distinguish two families of
strategies of risk reduction: reducing the probability of a loss, called `protection', and
reducing the size of the loss (the consequences), called `insurance' (Ehrlich and Becker
1972).

         Return and volatility. In Finance, risk refers to the uncertainty concerning the return
on the capital that the entrepreneur or the shareholders bears. This view of risk is well
represented by the volatility of returns. The uncertainty is in both the upside -the profits- and
the downside ­ the losses. The quantification of risk is needed in order to evaluate the assets.
The standard approach is to estimate the probability distribution of future returns of the asset
and measure the level of dispersion in terms of variance (Markowitz 1952; Tobin 1958), or
its square root, the standard deviation, commonly called volatility. Because prices of traded
assets are widely available and because measures of volatility over time are self-correlated,
the volatility of asset prices observed in the past is commonly used to forecast volatility of
the asset prices in the future. This is done with econometric methods, which assign
diminishing weights to the increasingly distant past.7 The ability to measure and predict asset
volatility and the correlation of volatility has led to the emergence of portfolio diversification
as a common strategy of risk-coping in finance, that is: investing in a portfolio of assets, such
that the average portfolio volatility is stabilized at a level deemed desirable by the investor,
conditional on the desired returns.

         Probability and Ambiguity. In Probability and Decision Theory, risk and uncertainty
refer to knowledge regarding the likelihood of contingencies (Marinacci 2015). There are
situations in which the cause of uncertainty is rather well-known. For example, when


6
  E.g., fire, theft, injury, et cetera. This view of risk as danger places the uncertainty in the area of losses: from
zero -the status quo- downwards.
7
  The most popular are the Autoregressive Conditional Heteroskedasticity (Engle 1982) and the Generalized
Autoregressive Conditional Heteroskedasticity (Bollerslev 1986).

                                                                                                                         9
throwing a dice, the physical properties of the dice -it having 6 equal-shape faces- determine
the possible realizations -6 states with equal probability. In this case, the model that generates
uncertainty can be understood and described as an object, and risk can be expressed
numerically in terms of objective probabilities. Although such situations are not common in
real life, objective probability can be applied, with some degree of simplification to other -
more common- situations, in which past occurrences, recorded as frequencies, are revealing
of the model that causes the uncertainty. For example, the mechanism that causes an illness to
be fatal may not be fully known. However, it may be reasonable to assume that the mortality
rate observed among people with the illness describes some objective property of this
mechanism. The relative frequencies can thus be used to shed light on the risk of death and
interpreted as objective probability in this case (Hájek 2019). Such calculations have been
common during the COVID-19 pandemic. Conversely, when the model that generates
uncertainty is unknown and frequencies are unavailable or non-informative, we have a
condition of epistemic uncertainty, in which objective probability is inapplicable and we can
only resort to subjective probability. These situations are common in science. For example, a
scientist who prepares an experiment often has no prior observations, because the experiment
is new, and has only incomplete theories that suggest potential results. The scientist can of
course express her best belief concerning the outcome, based on the scant knowledge that she
has and on personal experience. This subjective probability can be thought of in Bayesian
terms as the degree of confidence in the scientist's belief. Situations like this are also called
deep uncertainty or ambiguity (Ellsberg 1961).8

           Recent theoretical works in decision theory have concentrated on modeling decisions
under ambiguity (Klibanoff, Marinacci, and Mukerji 2005). Empirical work has shown that
humans are not only risk-averse, that is they prefer a sure thing over a gamble of equal
expected value but are also ambiguity-averse. That is, they prefer situations in which they
face objective probabilities as opposed to situations in which they face subjective
probabilities (Ellsberg 1961; Tversky and Fox 1995). For example, they prefer to draw a
marble from an urn that they know has half reds and half blacks, than to draw from an urn
that one expert says is all reds and another expert says is all blacks. (Berger and Bosetti 2020;
Ellsberg 1961). In situations where there is considerable scientific disagreement, such as
models that predict the relationship between levels of CO2 abatement and consequent climate



8
    Different scholars have used different terms (see Camerer and Weber 1992: 326).

                                                                                                    10
change, it appears that policy makers prefer options that reduce not only climate change
impact, but also the uncertainty/disagreement among the models (Berger, Emmerling, and
Tavoni 2016). This leads us to discuss a fourth important stream of contributions.

           Human Cognition of Risk. In Social and Cognitive Psychology, risk is examined
from a human perspective. Scholars generally agree that the human mind understands risk
both through analytical thinking, and through intuition (Epstein 1994; Evans and Stanovich
2013; Sloman 1996). That is, humans are capable of reasoning about risk in a logical and
rational way, for example when they consider probabilities. But they also hold instinctive
reactions when confronted by risk, for example when they feel danger (Loewenstein et al.
2001; Slovic et al. 2005, 2010).9 A classical stream of research, known as the psychometric
paradigm, has conducted extensive empirical research to explain risk perception and to
identify the kinds of hazardous situations associated with feelings (emotional intuitions) of
risk. The conclusion, which is widely accepted, is that there are two kinds of risks that
prompt the strongest emotional reactions in people. These are: "dread risk", i.e. the
possibility that something uncontrollable, irreversible or catastrophic will occur, and
"unknown risk", the exposure to new, unforeseen or delayed harms (Fischhoff et al. 1978;
Slovic 1987). This can explain, for example, the skepticism and consequently slow progress
surrounding several streams of research, such as power generation from nuclear fusion,
where it is easy to imagine catastrophic scenarios and uncontrollable unknown events.

           A second important focus of psychological research on risk is how people behave and
make decisions in conditions of uncertainty. The main contributions in this respect is that
human understanding of risk and probability is biased in systematically predictable ways
(Tversky and Kahneman 1974). In particular, people systematically undervalue perspective
gains, while they exaggerate the magnitude of perspective losses (Kahneman and Tversky
1979). Furthermore, the overvaluing of losses is larger in magnitude than the undervaluing of
gains (Kahneman and Tversky 1979), a condition called loss aversion (Tversky and
Kahneman 1991, 1992). Consequently, when people have to make decisions that involve
uncertain gains or losses, their decisions depart systematically from what rational behavior
would predict and their behavior is inconsistent and opposite in the spectrum of gains and
losses (reflection effect) (Kahneman and Tversky 1979). To be more specific, when
individuals face decisions that involve prospects of gains, they are risk-averse, that is, they


9
    Note that the instinctive reaction is solely associated with potential danger, and thus to a negative view of risk.

                                                                                                                    11
would give away gains for more certainty (prefer gambles with greater certainty even if they
involve smaller expected gains). When they face decisions that involve prospects of losses,
they are risk-seeking. That is, they would accept greater losses as long as they are less certain
(prefer gambles with less certainty even if they involve a greater expected loss over a smaller
but sure loss) (Kahneman and Tversky 1979).

       In the following sections, we draw on these four perspectives from distinct disciplines
to frame the discussion concerning risk and risk-taking in science in a conceptually-sound
way.




       4. Components of a Model of Risk in Science

       In this section we outline the components of a model for representing risk in science.
To facilitate our understanding, we take as an example the IceCube Neutrino Observatory at
the Amundsen Scott Station at the South Pole. The project was initially proposed in 1987 by
Francis Halzen (University of Wisconsin) in a co-authored paper that he presented at a
cosmic ray conference in Lodz, Poland (Halzen and Learned 1988) that discussed the
possibility of using deep polar ice as a detector.

       Neutrinos are subatomic particles of nearly zero mass that have very little interaction
with other masses and hence travel undisturbed across matter in outer space. The observation
of neutrinos can thus shed light on astrophysical phenomena originating outside our solar
system, such as the formation of supermassive black holes; more generally their observation
can shed light on the origins of the universe. Although scientists have explored ways to
detect neutrinos since the late 1950s, constructing, for example, detectors in mines and lakes,
at the time Halzen proposed placing a detector at the South Pole no detection device had
successfully observed neutrinos from outside the solar system. The research challenge was
thus to build an instrument capable of detecting such neutrinos and determine the direction
from which they came and examine "the relevant optical properties of deep Antarctic ice."
(Halzen and Learned 1988).




                                                                                               12
        In 1988 Halzen and colleagues were awarded $50,000 from NSF to study the optical
quality of ice.10 The research team at that time knew little about ice or the challenges
associated with drilling in ice, which was necessary in order to embed the sensors. This
exploratory project evolved into the proof-of concept project AMANDA (Antarctic Muon
and Neutrino Detector Array), supported by NSF with additional funding from other
foundations and countries. IceCube, which incorporates the AMANDA arrays, received its
initial funding from NSF in 2000. At completion of construction in 2010, the project had
placed 5,584 digital optical modules in a series of 88 holes drilled into a cubic kilometer of
ice, lying 1.5 kilometers below the surface at the South Pole. The ice lying above the sensors
shields the sensors from radiation at the earth's surface. The basic principle behind this
design is that when a neutrino collides with a nucleon it produces a muon through inverse
beta decay. When this occurs, a pale blue light known as Cherenkov radiation is emitted,
which can be detected. Importantly, the light bounces back in the exact same direction from
which the neutrino came. As a result, the position of the cosmic object from which the
neutrino originated can be inferred. Some data are sent to the IceCube Project at the
University of Wisconsin by satellite. The balance of the data is stored on hard drives and
sent once a year to the researchers.

        Let us now examine the risks involved in the IceCube Neutrino Observatory. For
simplicity, let us assume that the project sought funding all at once and let us take the point of
view of a scientist or panel member who is tasked with evaluation of the project. There are
sufficiently large numbers of uncertainties involved that it would be difficult to judge the
overall risk of the project without a conceptual framework that first identifies and analyses
multiple components of risk in isolation and then combines the components into a model of
risk. In order to build a suitable framework, we adopt the approach of risk metric used in risk
analysis and identify a set of questions concerning risk components. We discuss how the
components should be combined in the next section.

        A first question to consider is `what can be found'. The answer normally entails a
range of options. The range can be especially large in exploratory research, the goal of which
is to shed light on unknown domains of the natural world (e.g. space exploration, deep ocean
exploration). In the case of the IceCube Neutrino Observatory, for example, the goal was to



10
   The funding was obtained in the form of a Small Grant for Exploratory Research (SGER), which did not
require external review. (Bowen: 138-139).

                                                                                                          13
explore the cosmos using ice to detect neutrinos and the direction from which the neutrinos
came. In empirical research aiming at the test of formal hypotheses, the range is usually
narrower.

        There are two important things to note concerning what can be found. The first is that
uncertainty concerning what can be found cannot be represented by probability. Uncertainty
here regards outlining the possible states of the world and captures one essential feature of
research: its being an open-ended quest (Nelson 1959). The second is that this component is
represented by a range of alternative scenarios. How many scenarios are appropriate is a
question of practical relevance that should be addressed with a degree of pragmatism.
Research that involves one formal hypothesis may be represented by two scenarios -
hypothesis rejected or not-. Other research may require three or more hypotheses. For the
sake of clarity, this risk component should outline only the range of alternative primary
outcomes and leave aside secondary outcomes of research that may be a byproduct of the
research agenda. We return to this point later in this section, when discussing the possibility
of secondary outcomes (what else).

        Questions two and three conceive uncertainty in terms of the probability of each
scenario happening and focus respectively on methodological and natural risk.
Methodological risk--question two-- can be spelled out as: `how likely is the proposed
approach to work'. Uncertainty here concerns (subjective) probability and is both epistemic
and technical. `Epistemic' because the theory behind the design could be fallacious, or there
is uncertainty regarding the scientific knowledge on which a method is grounded. For
example, in the IceCube project the strategy for detecting neutrino bursts accompanying the
formation of black holes was drawn in part from theoretical work by Shi and Fuller (1998).
`Technical' uncertainty because several details of the execution are typically not yet resolved
at the stage of conception. For example, in the IceCube project it was not clear how deep one
would have to drill to find bubble-free ice.11 The original proposal submitted to NSF
assumed that one would only have to go to a depth of around 500 meters.12 This turned out to
be off by 1000 meters (Bowen 2017:129-30). There was also uncertainty that "optical
attenuation of deep ice in the mid UV range ... characteristic of the Cerenkov light emitted


11
   Bubbles cause light to bounce in all directions, which means that if a Cherenkov cone were detected, it would
not travel in a straight line.
12
   This assumption was based in part on a conversation with Prof. Edward Zeller (University of Kansas), who
thought that "we will obtain good optical clarity below about 150 meters near the pole." (Bowen 2017:130).

                                                                                                             14
by high energy muons, had not been directly measured," (Halzen and Learned 1988, p.2).
Uncertainty also existed concerning the difficulty of ice drilling and the time required to drill
each hole, as well as potential interferences with signal detection and the frequency of events.
In such situations, judgments concerning the risk involved in the method is a matter of
subjective beliefs made with little confidence. To quote Halzen: "it's pretty clear we had no
idea what we were doing, and so this was real research, right?" Halzen goes on to say that "if
we really had [known] what we were doing we would probably not have done it. And, in
fact, it turns out that a lot of things we should have known turned out not to be true" (Bowen
2017:147). Situations of deep uncertainty are the norm in projects that demand new and
highly-creative methods. In such situations, evaluation often involves assessing whether the
team proposing the research is in the best possible position to make it work. Conversely,
projects that employ standard or well-known methods do not confront this type of problem
and judgments can readily be based on prior experiences or data from past research.

       The third question to consider is: `if all works, how likely is it that the outcome will
be found?', i.e. the odds that the expected primary finding happens within the observation
spectrum. This can be thought of as natural risk. Note that this question applies only to
observational/experimental research and not to theoretical research. It depends on the relative
rarity of the phenomenon in nature, and consequently on the probability of capturing it within
the spectrum of observation. We can think of this as the natural risk of observing a
phenomenon, given the size and conditions of the observations. In our example, assuming
that cosmic neutrinos form as theorized, and that the experiment is well executed and capable
of detecting neutrinos, the probability that a cosmic event actually happens and is detected
poses an element of uncertainty. This uncertainty is caused partly by chance and partly by
nature. In many cases, past experiments in adjacent areas or data collected in prior works can
provide a basis for computing frequency-based probability. In the case of the IceCube, the
probability that a supermassive object collapses was modeled by an equation, in which the
key parameters were taken from data collected from previous experiments,

       The fourth question to consider concerns `what else can be found' and comprises
evaluations of secondary outcomes that the project might produce, beyond the stated primary
outcome. There are several common situations in research that induce secondary outcomes.
First, new instruments, designed with specific goals in mind, are an especially effective
source of secondary outcome (Franzoni 2009). Galileo's telescope, for example, intended for
navigation, resulted in the discovery of the moons of Jupiter; the radio telescope used by

                                                                                                  15
Jansky, intended to study noise that could interfere in radio transmission, ended-up detecting
radio galaxies. Second, because science is an open-ended quest, it is acceptable for a
scientist to be flexible and shift the goals driving a project (Nelson 1959). Flexibility can
provide backup or recovery plans when a planned task is known to pose problems. For
example, when the IceCube team found that bubble-free ice did not exist until a considerably
greater depth than they had initially thought, they realized they had a problem. They had,
"'goofed up' by placing their instrument in shallow ice." (Bowen 2017:175). Halzen
reportedly began to look for "some way to make this disaster look good." (Bowen 2017:188).
The answer was supernova. They realized they had "by far the most sensitive supernova
detector on the planet." Their disaster "had a mission." (Bowen 2017:189). A third reason
why secondary outcomes may be found is that in many scientific projects the need to solve
practical and theoretical problems induces learning in ways that often branch-out from the
main line of investigation. This learning inevitably suggests directions of additional research
that could not have been anticipated at the time of project conception. By way of example,
the drilling of ice needed to place the IceCube Neutrino detectors lead to important
discoveries concerning the physical properties of deep-ice.

       It should be noted that secondary outcomes may exist with or without primary
outcomes. Moreover, some types of research are more likely to produce secondary outcomes
than others. While it is difficult to think of secondary outcomes arising from a Cochrane
review, research that involves exploration of nature and activities never performed before is
more likely to have secondary outcomes. More generally, basic research provides, by
definition, insurance against coming up empty handed in the sense that non-findings are, in
their own way, findings. To quote Bowen, the physicist who chronicled the IceCube project,
"There is no such thing as a disaster in basic research. Whatever happens, you learn from it."
(Bowen 2017:163). The same cannot be said for research that is extremely path dependent,
with few opportunities for secondary discoveries. Protein structure determination is a case in
point; it "is either a complete success or complete failure." There are "no intermediate results
to publish along the way or to fall back upon if you fail, unlike other fields." (Petsko
interview July 3, 2020).

       The fifth question to consider is `how much is the finding worth,' and concerns the
importance of what is found in terms of scientific gains and societal benefits. For example, if
cosmic neutrinos detect a cataclysmic astronomical event, what would be the value of this
new piece of knowledge? The uncertainty in this case relates to the magnitude of the impact.

                                                                                                16
There are two important things to note regarding the impact. First, a finding that falsifies a
theory may provide informative content, just as (if not more so than) a finding that complies
to the theory (Popper 1959). Thus, all scenarios have a non-zero value. Second, the value of a
finding in science depends on the context, in the sense that the same finding could be more-
or-less valuable depending on the conditions and actions of other scientists in the same field.
If the research area is crowded by many competing teams, it is possible that the same finding
will be reported independently by multiple researchers and will thus be of lower value to
science and society. This is not to say that additional independent reports do not add value
by confirming the result, but they do not have the same value as that of the first discovery
(Merton 1957; Stephan 1996). Consequently, the ex-ante appreciation of the prospective
value of a discovery requires speculation concerning the actions of competing groups.13

        So far, we have discussed the five questions that help identify risk components from a
general point of view. However, if we assume the point of view of the Principle Investigator
(PI) there is one additional aspect of risk to consider: personal consequences and/or
consequences related to the PI's team or lab, germane to the fifth component discussed
above. Stated differently, the fifth component concerning how much a project is worth can be
disentangled into two sub-components: `how much is the finding worth for science and
society' and `how much is the finding worth to the PI'. One or both apply, depending on who
is doing the assessment.

        Research outcomes have many possible consequences for a PI. Here we consider the
primary ones: i) career, ii) reputation, and iii) future funding (Stephan 2012). Career
consequences depend in part on the position and career stage of the PI. Tenure, for example,
shelters PIs from the negative consequences of research that has disappointing results
(Franzoni and Rossi-Lamastra 2017). A tenured scholar is arguably less damaged by a non-
result than a tenure-track scholar. And a tenure-track scholar arguably benefits more than
does a tenured one from a major success. Even among PIs in the same academic rank, the
career implications of a non-result may differ. For example, the severity of a non-result for
tenure-track scholars is more of a problem for those without other successful projects than for


13
  To illustrate, in 2018 the IceCube team announced that they had detected a cosmic neutrino from a blazar,
laying 4-light years away from Earth (Collaboration 2018). Prior to this, the only other identified sources of
cosmic neutrinos were limited to the Sun and to a supernova identified in 1987. The fact that the finding was
reported only by IceCube, made it more valuable than if the finding were independently reported by other
observatories.


                                                                                                                 17
those with past successes. Regardless of academic rank, scholars in more demanding
institutions may face more severe consequences from failed research.

        Outcomes affect one's reputation. Being the lead PI of a successful research project
can lead to increased visibility and enhanced reputation. However, if the research results are
particularly disappointing, it can also jeopardize the scholar's reputation. This reputational
effect is magnified by working in crowded areas, where bibliometric indicators are more
granular and thus respond more directly to small changes. Researchers also worry that a
proposed idea may appear to be sufficiently on the "wild side" to tarnish their reputation.
Halzen had such concerns about the idea of proposing a neutrino observatory in the ice. One
reason he chose to propose the idea publicly in Poland, at a relatively small conference, was
to shelter himself from such a reputational loss. (Halzen 2010).

        Accomplishments (or lack thereof) also have implications for future funding,
necessary in most fields and countries for today's scientists.14 Uncertainty surrounds funding
in a variety of dimensions. For example, if the project brings no results or produces results
deemed insignificant, funds are unlikely to be forthcoming for further work, while, if it
produces results of sufficient importance, future requests are much more likely to be funded.
Halzen and colleagues were acutely aware of this and knew that "any type of failure will
hamper future funding." (Bowen 2017:175). Risk associated with future funding can
discourage scientists from moving in new directions. One eminent scientist who made a
major change in his research agenda at age 55 reported that while in making the change he
never considered the risk associated with not finding anything, the risk he did consider was
"the personal cost: leaving a field in which I was a leader for one in which I was completely
unknown, the difficulty in getting funding, the difficulty in getting people to work with me.
And indeed, those were the major problems I faced, for years."

        Consideration of the value for science and society and for the PI differ in the direction
of variability of risk they imply. When we consider consequences from the point of view of
science and society, the how much has a downward limit of zero, in the sense that it cannot
lead to a direct loss. When we consider the point of view of a PI, the how much implies a
variability that takes the full spectrum from gains to losses. That is, the personal



14
   Although small, "table-top" science exists, research is generally expensive and requires external support.
IceCube is at the expensive end of the spectrum, but there are other projects, such as LIGO and the LHC that
cost far more.

                                                                                                                18
consequences related to research may imply a direct gain, but also a direct loss in reputation,
career status, placement or future funding.

        The prior discussion disentangled five components and related questions that must be
addressed in a meaningful discussion of risk in science: i) what, ii) how likely, iii) if, iv) what
else and v) how much. They can easily be expanded to include additional components arising
in special cases. For example, in the relatively rare case in which research could involve
immediate danger to researchers (e.g., developing new explosives), human or animal subjects
(e.g., testing new surgical methods), the environment (e.g., research on nuclear power), or it
could involve ethical concerns (e.g., cloning of animals), the model can be expanded by
adding components regarding potential losses.15 IceCube, for example, confronted this type
of risk concerning the safety of the South Pole team. Indeed, one of the drillers was seriously
injured while working on the project. Academe has developed provisions to address and cope
with such risks. They include ethical and safety protocols and procedures like the IRB that
aim at minimizing such risks.

        More generally, one could also think of sub-questions, that detail specific aspects. For
example, one can ask separately the value of secondary outcomes from the likelihood of
finding secondary outcomes. All in all, however, the above questions capture the essential
components of risk involved in the majority of scientific research projects. Table 1
summarizes the questions, along with their applicability, focus, and source of uncertainty.
This conceptualization is useful for representing research uncertainty and can be used to
compare projects. The comparison also reminds us that there are basic differences regarding
the risks involved in different types of research. Observational studies and experimental
research is subject to natural risk (the if), whereas theoretical research is not. In this sense,
experimental research faces one more challenge that can lead the research to come-up empty
handed. Although basic experimental research may involve a small likelihood of success, due
to both methodological and natural uncertainty, in the sense of coming up empty handed, it is
not necessarily at high risk of failure compared to applied experimental research. This is
because the probability of secondary findings is more favorable than is the probability of
secondary findings in applied experimental research.




15
  This can include asking the classical triplets of questions encountered in risk analysis: what harm can happen,
how likely is this and, in that case, what would be the consequences (Kaplan and Garrick 1981)

                                                                                                              19
          Finally, it should be noted that the framework outlined in Table 1 also highlights that
evaluations of risks differ systematically depending on who is doing the evaluation, because
the consequences that accrue to science and society are different from those that accrue to the
PI.




          Table 1. Framework for representing the components of risk in science

Question                          Applies                                Focus                        Source of uncertainty

What can be found?                Always                                 Primary outcome              Nature
                                                                                                      State of scientific knowledge


How likely is it that the         Always                                 Method                       Nature
proposed approach works?                                                 Qualifications of the team   State of scientific knowledge
                                                                                                      Technology

If all works, how likely is the   Experimental research                  Rarity of event              Nature
outcome to happen?


What else can be found?           Always.                                Secondary outcome            Nature
                                  More likely in natural explorations,                                State of scientific knowledge
                                  activities never performed before,
                                  new scientific instruments,
                                  and basic research in general


How much is the finding worth     Always                                 Science                      Scientific advance in own field
for science and society?                                                 Scientific community         Size of field
                                                                         Societal benefit             Competing results
                                                                                                      Scientific advance in other fields
                                                                                                      Size of other fields


How much is the finding worth     Principle Investigator                 Career stage                 Personal value of findings
for PI?                                                                  Professional environment     Reputation
                                                                                                      Career status
                                                                                                      Placement
                                                                                                      Funding




          5. Measuring risk

          Model of Expert-based Metric of Risk. The framework set out in Section 4 outlines
multiple components of risk in research. In some cases, however, describing risk in its
multiple components is not sufficient and there is need for a comprehensive measure of risk.
This is the case, for example, in the context of peer review evaluation in which experts are
asked to assess the risk of research of proposed research. We thus need a model that


                                                                                                                              20
combines components to assess risk. Drawing on models of risk metrics (Budnitz et al. 1998;
Haimes 2009; Kaplan and Garrick 1981), we adopt the most simplistic approach and use the
question of what and what else to outline the scenarios of possible outcomes (states of the
world) --both primary and secondary-- to provide an example of such a model.

        The questions how likely and if, when applicable, shall assess the likelihood of each
scenario, intended as subjective probabilities, and expressed in the range 0-1. The questions
how much shall assess the impact for science and society (ui); they could also have a separate
estimate concerning the value for the scientist. The how much can be quantified with scores,
e.g., in the 0-100 range. Table 2 illustrates an operational application of the framework using
a simple functional form. It assumes the point of view of a granting agency and so the
question how much considers only the value for science and society, and not the value for the
scientist involved.




        Table 2. Example of risk measure for expert-based assessment

 Primary/secondary outcome          Likelihood            Value for science and       Expected value
                                                                 society          for science and society
                                                                                       (i-th scenario)

          What /             How     If       Total            How much
         What else                         likelihood


                             Prob. Prob.     Prob.                Score                    Score
                             (0-1) (0-1)     (0-1)               (0-100)                  (0-100)
         Scenario i
                              p1i   q1i    P1i= p1i q1i             u1i                Vi1 = u1i P1i
    Primary outcome (1)
         Scenario i
                              p2i   q2i    P2i= p1i q2i             u2i                V2i = u2i P2i
   Secondary outcome (2)
         Scenario i
                                                               Ui= u1i + u2i           Vi = V1i + V2i
           Total
       Scenario (i+1)
    Primary outcome (1)
       Scenario (i+1)
   Secondary outcome (2)
       Scenario (i+1)
                                                                   Ui+1                    Vi+1
           Total
            ...




        The main outcomes of the framework are pairs of the value (for science/society) and
the likelihood (ui ; Pi), for each i-th scenario of primary and secondary outcome, where the



                                                                                                            21
likelihood Pi is the product of methodological (pi) and natural (qi) risk for each scenario.16
The expected value of the i-th scenario (Vi) can be computed as the sum of the expected
values of primary (V1i) and secondary outcomes (V2i), given the respective likelihoods. The
number of scenarios considered (N) can vary, depending on the case, and on intended uses of
the analysis. The pairs can be ordered in descending order of Vi , and eventually plotted and
fitted by a risk curve, a standard representation of risk analysis (Kaplan and Garrick 1981).
One can, of course, compute the average expected value of all scenarios (                              ), but

the result would have little meaning. More informative for the purpose of science would be
the range of maximum and minimum values for science/society (min Ui ; max Ui) across the
scenarios and the range of related expected values (min Vi ; max Vi). High-risk high-gain
projects would stand out for having at the same time a high max Ui , a large range between
max Ui and min Ui, and a low max Vi . Conversely, low-risk, low-gain projects, like
Cochrane reviews, would have a low max Vi, but also a low and not-so-variable max Ui.
Likewise, the metric can detect projects that are High-risk (low max Vi), but Low-gain (low
and not variable max Ui).

        In the case of IceCube Neutrino project, for example, there was large uncertainty
concerning the findings, project feasibility (the how) and the if the sought events would
actually occur. The project almost certainly involved secondary outcomes. Thus, it could
have led to a large number of scenarios, in which the value of a single potential scenario
could be very high, and variable. By way of comparison, Cochrane reviews can lead to a
maximum of two/three scenarios (e.g., absolute effect is significant, not significant;
mixed/unclear), with no uncertainty in the how, no secondary findings and consequently a
small and not very variable, value for science/society. The testing of Ipilimumab also
involved a limited number of scenarios (e.g., the drug is effective; not effective; mixed), with
no uncertainty in the method and no or limited secondary findings. However, the value of
discovering an immunotherapy treatment for cancer was very high, given alternative
treatments available at the time, and the chances of success (if), were small, based on prior
experience. Thus, Ipilimumab was at higher risk of coming-up empty-handed (low
probability, no secondary finding), although the methodology involved was not risky and the
expected value, if successful, was high.



16
   Assumed here as independent from one another. In case the probabilities are not independent, the formula
should be edited to include conditional probabilities.

                                                                                                              22
       As the example shows, the approach of risk-analysis outlined above can in principle
be adopted to evaluate risks involved in research that has yet to be implemented. As such, it
potentially could be deployed by funding agencies, in peer-review assessment for evaluating
research proposals and could be especially useful in those granting schemas where the high-
risk high-gain evaluation is a key criterion. It is important to stress that the application of this
method to project evaluation would be an exercise of forecasting, as it involves asking
experts to gauge actions and events that could happen in the future and are not pre-
determined (Mellers et al. 2015; Tetlock and Gardner 2015). Thus, the opinions would
necessarily imply a large margin of uncertainty. In other areas where experts' forecasting is
essential, such as climate change and risk analysis, behavioral and decision scientists have
elaborated a set of best practices known as `expert elicitation techniques' to elicit,
characterize and treat quantitative expert judgments in the form of subjective probability
(Morgan 2014; Raiffa 1997). These techniques are meant to make the best possible use of
the information that experts have, including eliciting more than one point of the subjective
probability distribution. The approach also takes into account the potential interference of
human biases, such as overconfidence and anchoring (Clemen and Winkler 1999; Fischhoff
2015; Fischhoff and Davis 2014; Morgan 2014; Winkler et al. 2019).

       In practice, the application of the model we propose is rather laborious. While the
approach has much to recommend it, its adoption would undoubtedly increase the burden of
review boards that manage large numbers of proposals. At present, we do not know if the
potential gains of using a risk metric in grant peer-review would be justified in a cost-benefit
logic. This is probably an empirical question that warrants future work. At a minimum the
concepts and procedures underlying expert elicitation protocols could be used to help educate
panelists regarding aspects of risk.

       Text-based correlates of risk. Scholars of the economics of science often wish to
measure risk associated with research in large samples and often over long periods of time.
The expert-based metric outlined above is clearly too time and resource-consuming in such
instances. For these purposes, methods have been developed that can be computed for large
samples. These methods do not measure risk directly, but instead infer it from correlates
associated with risk that can be obtained from the codified products of research, such as texts
of research proposals and published papers.




                                                                                                  23
       Many text-based correlates of risk rely on the assumption that research that deviates
considerably from past research can lead to breakthroughs but holds risk in that it has yet to
be explored by others and can fail. One way to measure the extent to which research deviates
from past research is to think of past knowledge as a set of building blocks which can be
combined to produce new knowledge. Research that combines past knowledge in well
understood ways is referred to as exploitative; research that combines past knowledge in new
ways is referred to as explorative. Explorative research arguably is more likely to lead to
breakthroughs than exploitative research (Romer 1994; Varian 2009) and to carry higher
potential impact (March 1991; Simonton 2003).

       Wang, Veugelers and Stephan (2017) draw on this approach to measure novelty, the
extent to which a published paper draws on references that have not been jointly referred to
in previous research. To be more specific, they retrieve references for each paper published
in 2001, and construct co-cited journal pairs for each. They then check to see if the pair has
previously been made in the last 20 years, identifying pairs (if any) that are novel--that is not
previously made and compute the "difficulty" of making each novel pair by examining
whether it has "common friends" in the sense of past journal co-citations. The novel score
for the paper is the sum of the novel score for each reference combination. The authors find
that 89% of the articles contain no novel combinations. Among the 11% that do make a
novel combination, they distinguish between highly novel, being in the top 1%, and
moderately novel. Importantly, for our perspective, they find that, compared to non-novel
papers, the citation distribution associated with highly novel papers has a higher variance and
higher mean value, characteristics that we expect in risky research, suggesting that the novel
measure correlates with risk.

       Uzzi and coauthors (2013) adopt an alternative measure of knowledge recombination,
called atypicality. For each pair of references found in a paper, they compute a z-score,
comparing the probability of making the combination to making the combination by chance.
Z-scores for reference pairings greater than zero indicate that the pairing occurred more likely
than by chance; those below zero indicate less likely than by chance.  For each paper, they
then take the lowest 10th percentile z-score of its series of z-scores as an indication of how
novel the paper is and the median z-score as an indication of the paper's "conventionality."
The authors find that the "vast majority of papers displays a high propensity for
conventionality." (Uzzi et al. 2013: 469).



                                                                                                 24
        Responses of corresponding authors to the GlobeSci survey (Franzoni, Scellato, and
Stephan 2012) concerning a paper published in 2009 are consistent with these findings.
Authors were asked to report "with regard to the area of research, this paper is: in a high-risk
(of failure), high-reward (if successful) area of research."17 The variance of citations to
papers authors reported as high-risk was greater than the variance of citations to papers
reported as low risk as was the mean number of citations. Moreover, the authors' assessment
of risk correlated with Uzzi's measure of atypicality (Franzoni, Scellato, and Stephan
2018).18  

        Other approaches to measuring risk rely on words or findings reported in the text to
measure the extent to which the current research deviates from past research. Foster et al.
(2015), for example, examine the extent to which biochemists introduce novel chemicals and
chemical relationships, using abstracts from publications indexed in Medline. The authors
distinguish between three types of papers based on the chemical relationships described in the
work. Research that makes a jump explores previously unexplored chemical relationships -
jumping beyond current knowledge. Such research arguably is more likely to fail but, if the
research succeeds, it is more likely to make a breakthrough. Research that explores
relationships between previously studied entities, and is thus more likely to succeed, is
subdivided into research that tests a new relationship, not published before, or research that
repeats an analysis of a previously studied relationship. Again, and important for our
perspective, the authors find that the citation distribution associated with jump papers and
new papers has a higher variance and higher mean value than that of repeat papers,
characteristics that we expect in risky research, suggesting their measures correlate with risk.
The authors also find that papers based on repeat strategies were six times more likely to be
published than that those that used new or jump strategies during the period 1983-2008
(Foster et al. 2015: 886). Krieger, Li and Papanikolaou (2019) develop a related measure of
novelty which compares the chemical structure of new drugs going up for FDA approval to
that of previous drug candidates.




17
   Approximately 8% said they strongly agreed that it was high risk, high gain. Approximately the same percent
said they strongly disagreed; one third neither agreed or disagreed, while almost a quarter said they agreed--but
not strongly-- with the statement and a bit more disagreed with it.  
18
   The atypicality analysis was only available for a limited subset of respondents. See Franzoni, Scellato and
Stephan (2018).

                                                                                                              25
           Another measure used as a proxy for the degree to which research deviates from past
work is the average age of keywords associated with a publication, measured by the year a
keywork first appeared in the scientific literature. The approach works best if applied within a
single domain. For example Azoulay et al. (2011) use this method for Medical Subject
Headings (MeSH). They also compute a Herfindal index of keyword diversity. Boudreau and
coauthors (2014) use the percent of keywords never used before to calculate the novelty of a
proposal. A different approach, based on content analysis, measures the degree to which a
document employs words that suggest vagueness, probability or unsureness (e.g., uncertainty,
weak modals and negative words), as opposed to certainty (Sauermann et al. 2019). Word
lists and algorithms exist that can readily provide this metric e.g., for the English language
(Loughran and McDonald 2013).

           Open issues on risk measures It is important to note that limitations and pitfalls exist
in the use of these text-based metrics of risk. First, there is the file drawer problem. A large
share of failed research remains unpublished (Franco, Malhotra, and Simonovits 2014).
Moreover, published articles are formulated to provide an impression of order and fulfilment
of the expectations which conceals the trial-and-errors involved (Bourdieu 2001). In this
respect, looking at research proposals--which are upstream--addresses part of this concern,
although even research proposals do not include intentions and/or ideas that are censored
after early-testing. One possible way to reduce the file drawer problem is to look at log-files
and lab-notes that scholars deposit in digital repositories when research is in progress or
being proposed (Franzoni and Sauermann 2014; Sauermann, Franzoni, and Shafi 2019). Pre-
registration of experiments can serve the same purpose.19

           A second problem relates to measures based on knowledge recombination, like
novelty and atypicality. By construction, they see new research which deviates considerably
from past research as riskier. While such measures capture risky research, they do not
identify all risky research, such as the risk associated with continuing a line of research that
has proven fruitless for some time. For example, they would not have detected risk in the
case of the first immunotherapy cancer treatment made by Allison, described earlier in this
essay. Third, many of the available metrics rely on citations, which have well-known
drawbacks and are but one measure of the extent to which research is successful.
Furthermore, some of the riskiest research may not have a measurable effect for many years.


19
     E.g. https://www.cos.io/initiatives/prereg. Accessed November 23, 2020.

                                                                                                   26
Last, but certainly not least, these measures provide a single assessment of risk that relates to
a full project or completed research, and do not discern risk components within projects, the
importance of which we have stressed in Section 3. Developing complementary measures or
metrics that rely on alternative assumptions could thus prove valuable. It is possible that
textual analysis and other AI-based metrics will offer new opportunities in this direction.




         6. The Relationship of Rewards to Risk Taking; Extent to Which Funding
             Agencies are Risk Averse

        This paper began with the concern that risk taking in science is on the decline. Many
put the onus on funding agencies and changes in the way in which science is funded and
grants awarded. Others stress that the rewards to doing science not only discourage risk-
taking, but increasingly do so. In this section we examine the extent to which the rewards to
science discourage risk taking and whether there is evidence to suggest that these trends, to
the extent they exist, are increasing. We also examine the extent to which funding agencies
are risk averse.

        Reputation plays an important role in science although by no means is it the only
reward to doing science (Merton, Stephan and Levin).20 But reputation is key. It affects
hiring and promotion opportunities, as well as funding decisions and plays a major role in the
acquisition of the position and resources necessary to engage in a research. Measures of
reputation in recent years rely increasingly on bibiometrics, where citation counts and their
derivatives (e.g., the H-Index, the Journal Impact Factor), play a prominent role.

        The heavy emphasis on reputation, particularly when measured with bibliometric
indicators, arguably discourages risk taking on the part of scientists. This is not obvious;
examples of scientists who have taken a risky course receiving a Nobel Prize, for example,
are readily available 21 and there is research suggesting that prestigious prizes can encourage
risk taking (Rzhetsky et al. 2015). But overall, the citation-premium for doing risky-research,



20
    Rewards also include the satisfaction derived from puzzle solving, and financial gain that often accompanies
a successful research career (Stephan 2012; Stephan and Levin 1992). Cohen, Sauermann and Stephan (2020)
also show that scientists are strongly motivated by an interest in contributing to society.
21
   Jim Allison is but one case in point.

                                                                                                              27
compared to that of doing not-so-risky research, is arguably insufficient to encourage risk
taking. To illustrate, Foster and colleagues (2015), using the method described earlier, find
that "jump" papers reporting highly innovative chemical combinations receive 52% more
citations on average than "repeat" papers reporting known combinations, while "new" papers
reporting moderately-innovative combinations enjoy 30% more citations than those
reporting known combinations. Their results suggest that taking the risk associated with
"jump" and "new" research makes it more likely to achieve high impact, but the additional
rewards are small and arguably do not compensate for the possibly of failing. Stephan (2019)
has called this the "quad" effect, referring to the fact that competitive female figure skaters
attempt fewer quadruple jumps, arguably because the incremental score they can earn for
completing a quad, compared to successfully completing a triple jump, is insufficient to
compensate for the risk of failing to complete the quad jump. For male figure skaters,
scoring is different: the incremental score is larger and provides sufficient incentive to
attempt the quad. The work of Uzzi et al. (2013) is consistent with the findings of Foster et
al. (2015), and shows that "The highest-impact science is primarily grounded in exceptional
conventional combinations of prior work yet simultaneously features an intrusion of unusual
combinations." Stated differently, a little bit of risk adds spice to the research; but
conventionality is the dominant characteristic of highly cited papers.22

        Wang, Veugelers and Stephan (2019) find that in the short run highly novel papers are
less likely to be top-cited (1%) than moderately novel or non-novel papers but over time
highly novel papers are significantly more likely to be top-cited. The authors also find that
highly novel papers are less likely to be published in High Impact Journals. Causality of
these findings cannot, of course, be determined but the results are consistent with the idea
that in the short run the rewards to science are biased against risk-taking. This can
discourage risky research, given that universities make crucial career decisions, such as
hiring, third-year review and tenure, using evaluations based on relatively short-time
windows. While the tenure-track system has long been common in the US and Canada, it has
only recently been introduced in several continental European countries, such as Germany,
Sweden and Italy, suggesting that the rewards for risk-taking have shrunk globally.




22
   Papers characterized as having high medium conventionality coupled with a high tail "novelty" have a hit rate
in the top 5 percent 9.2 times out of 100.

                                                                                                             28
        The role that reputation plays in discouraging risk taking arguably is growing not only
because of an extension of the tenure-track system to an increasing number of countries but,
and related, because citation counts are becoming increasingly important and readily
available. Sixty-five years ago, there was no ready way to measure citations to published
work. As late as the early 1990s the only way to count citations was to laboriously look in
the volumes published by the Institute of Scientific Information. Today citation counts are
readily available and arguably affect the direction of the PI's research. Consider, for
example, what the computational chemist Richard Catlow said upon becoming Foreign
Secretary of the Royal Society: "I was lucky. When I began my scientific career in the
1970s, I had no real sense of how my work was cited. [...] If I had been citation-driven, I
might have abandoned a field that is now central [...]. By the 1990s, when citation data
became prominent, I was already a full professor." 23 The work of Foster et al. (2015) is
consistent with Catlow's view, finding that papers which focus on already established
relationships have been growing over time, consistent with the idea that high risk in research
is on the decline.

        Although we lack systematic studies of the relationship of risk taking to funding
success, the work that does exist suggests that reviewers and panels are risk averse. In an
experiment conducted at the Harvard Medical School, Boudreau and coauthors (2014), for
example, find that more novel research proposals, as measured by the percent of keywords
not previously used, receive more negative evaluations during peer-review. The result is
driven by proposals with particularly high levels of novelty. Their preferred explanation for
this finding rests on bounded rationality of reviewers. To quote the authors: "experts
extrapolating beyond the knowledge frontier to comprehend novel proposals are prone to
systematic errors, misconstruing novel work. This implies that, rather than receiving
unbiased assessments (with zero mean errors), novel proposals are discounted relative to their
true merit, quality and potential." (Boudreau et al. 2014: 2779). Veugelers and coauthors
(2019) find that applicants to the ERC Starting Grant program, with a history of highly novel
publications, are significantly less likely to receive funding than those without such a history.
The major effect comes during stage one, when panel members screen a large number of
applicants based entirely on a five-page summary of the proposed research and a CV. The
finding suggests that reviewers rely on bibliometrics, which, as we have seen, are biased


23
 https://media.nature.com/original/magazine-assets/d41586-017-08289-z/d41586-017-08289-z.pdf. Accessed
August 6, 2020.

                                                                                                     29
against risk taking in the short run, in making decisions. Lanöe (2019), using a measure of
novelty, finds evidence that funding decisions made by French National Research Agency are
biased against risk-taking. Wagner and Alexander (2013) evaluate the SGER NSF program
designed to support high risk, high reward research that ran from 1990 to 2006. Funding
decisions were made entirely by program officers with no external review. The authors find
that program officers routinely used but a small percent of available funds. The findings
suggest that either officers were averse to funding risky research, despite the number of
funded proposals that had transformative results or, that risk taking was not rewarded within
NSF. Conversely, Sauermann, Franzoni and Shafi (2019), using speculative words as a
measure of risk, find no evidence to suggest that funding made by citizens who pledge money
on the research crowdfunding platform Experiment.com either favor or disfavor risky
research.




         7. Hedging Risk
         Financial instruments such as futures are designed to hedge risk in markets where
volatility exists, as does the ability to assemble a portfolio of assets with various degrees of
risks. Insurance reduces the size of losses in cases of a negative event. A question of interest
is the extent to which strategies exist in science for hedging risk, either at the individual or
institutional level. If so, understanding such risk hedging strategies has the potential to help
policy makers and administrators advance risk taking in science.

         Individual level At the individual level, a strategy for hedging risk that is not publicly
advertised, but sufficiently common to inspire a cartoon,24 is to pursue the early stage of
research with funding drawn from an earlier grant. In some instances, research begun on the
back-burner may eventually be proposed for funding, having been "de-risked" by the back-
burner treatment. Such strategies are only available to scientists who are sufficiently senior to
have established a funding pipeline. Scientists also attempt to manage risk by seeking
funding with a sufficient time horizon to allow them to recoup from possible failures that
may occur along the way.25 Scientists also hedge their bets by pursuing research with
potential secondary objectives, as we have seen in the case of IceCube.



24
  http://phdcomics.com/comics/archive.php?comicid=1431. Accessed October 27, 2020.
25
  HHMI funding is highly prized not only for its status and size but also for the fact that the funding is for 7
years.

                                                                                                                   30
         Another important strategy for hedging risk at the individual level consists in
outsourcing parts of the research process to others in exchange for payment. For example,
ten years ago it could take a postdoctoral fellow a year to try to create a transgenic mouse in
a PI's lab. At the end of the year there was a reasonable possibility that the postdoctoral
fellow would come up empty-handed and the lab would be "mouseless." The availability of
outsourcing the creation of such a mouse to a company has de-risked the activity. While the
company may experience some failure along the way, by preserving steps of the process it is
able to guarantee a mouse in a reasonable period and, if it fails completely, the PI will get a
refund.26 Similar strategies of risk transfer via outsourcing are possible in several other
domains including experimental psychology.

         University level At the institutional level, hiring policies help universities insure
against investing in scientists who are not productive. "Soft money" positions, for example,
come with no salary guarantee, but instead salary is funded (or almost fully funded ) from
grants for which the researcher is responsible. Such arrangements put faculty under
considerable pressure to produce results. The bio-physicist Stephen Quake called this
situation "funding or famine."27 If their research is not deemed fundable or comes up empty
handed, the university can cut its losses and hire another individual into the position. It is
notable that soft money positions have been on the rise in recent years. In the US, for
example, the majority of basic medical faculty are hired in soft money positions and are
responsible for bringing in most of their own salary (Stephan 2012). Soft-money positions
also are common outside of medical institutions. Stephan documents that during the years
when the NIH budget doubled, the majority of new hires were made into soft money
positions (Stephan 2007). Soft money positions not only transfer risk to the faculty; they also
discourage risk taking on the part of the faculty given the importance of continued funding.
A second way universities insure against risk is hiring faculty into "tenure-track" positions
and/or implementing third-year review during the tenure-track period. Such practices mean
that the university can cut its losses if it views performance to be inadequate. Finally,




26
   Cyagen, for example, which at the date of this writing had delivered over 50,000 animal models, states that
"we will fully refund the client's service fee if animals with the specified genotype are not generated (except for
genetic modifications severely affecting viability, morbidity, or fertility.)
https://www.cyagen.com/us/en/service/transgenic-
mice.html?gclid=EAIaIQobChMI5aCYqo_46gIVDvDACh2PpgnmEAAYAiAAEgK7VfD_BwE. Accessed
August 6, 2020.
27
   https://opinionator.blogs.nytimes.com/2009/02/10/guest-column-letting-scientists-off-the-leash/

                                                                                                                31
universities also recruit faculty with strong funding streams rather than hire individuals
without funds28, again a strategy for minimizing risk.

        Universalities and institutions also insure against the risk that research could result in
serious harm or damage by means of policies that mandate pre-approval for certain types of
potentially harmful research. For example, most universities require pre-approval by the
Institutional Review Board (IRB) for all research that involve human subjects. Moreover,
they often engage external organizations to train and certify faculty in compliance regarding
the conduct of research, thereby minimizing their own risk in so doing.

        Granting institution level Funding institutions which award money to support
research have several ways of managing risk. Perhaps the most straightforward is to favor
proposals that include preliminary findings, thereby insuring that the research is feasible.29
Another is to adopt a portfolio approach. There are in principle several ways to do so. One
widely used in practice is to have separate calls with separate budgets for risky and `regular'
research, limiting the budget of the former. In principle, the approach assumes the ability of
panel members to assess risk involved in a proposal and select accordingly. In practice, panel
committees are given little guidance on how to discern risk and even less on how to select
accordingly.

        A third strategy granting institutions use to hedge risk is to fund in stages. In the first
stage, a pool of projects is selected for initial short-term funding. In the second stage, an
interim evaluation is conducted. Funds for less promising projects are curtailed; funds for
more promising ones are continued. Although this strategy is used by DARPA,30 it is
infrequently used among most funding agencies. A stage-funding approach is, by way of
contrast, common in the Venture Capital industry for funding entrepreneurial projects, where
it is called the `spray and pray' strategy (Lerner and Nanda 2020). Although the two domains
are fundamentally different, the parallel provides some interesting insights. The interim
evaluation is especially useful when the initial estimate is unreliable, but can be quickly
updated with initial funding (Vilkkumaa et al. 2015). It is thus especially suitable for research



28
   During the NIH doubling, universities recruited senior faculty with more than one grant. Post-doctoral fellows
in the US are reportedly more likely to get a faculty position if they have secured a K-99 grant from NIH. More
generally, getting ERC funding as a starter is seen as a path to obtaining faculty status.
29
   When the National Institute of General Medical Sciences at NIH was funding protein structure projects, the
mantra was "no crystal, no grant," code for the requirement of preliminary results
30
   https://fas.org/sgp/crs/natsec/R45088.pdf Accessed August 30, 2020.

                                                                                                              32
that can make substantial steps forwards in a relatively short period of time and does not
require large fixed-costs to be started (Ewens, Nanda, and Rhodes-Kropf 2018).31 Some
research fields meet these conditions, but the conditions are more the exception than the norm
in the natural sciences, where the share of research that requires expensive equipment and
substantial effort is large (Stephan 2010).

        A fourth strategy granting institutions use to hedge risk relies on portfolio
diversification. It applies specifically to challenge grants, that is grants designed to address a
specific challenge or set of pre-defined goals, such as the full sequencing of the human
genome, or the development of a vaccine for Covid-19. In these special cases, the desired
outcomes are known, but the way to achieve them is unknown and/or there is uncertainty
concerning which approach is more likely to be successful, or more efficient or quicker. Risk
in this case can be managed by funding a pool of projects that take diverse routes to reach the
same outcome and thus the risks are non-correlated. By way of example, Operation Warp
Speed, launched in the spring 2020 by the US government to advance a COVID-19 vaccine,
aimed at having at least one approved vaccine available by the end of 2020, a record-time
given that normal vaccine development takes about 10 years and has a success rate of 6%
(Mullard 2020). The approach of the US administration was to select a certain number of
candidates to maximize the odds that at least one would make it to the finish line. In May
2020, the task force identified 14 vaccine candidates out of more than 100 that existed at a
pre-clinical stage. Of the 14 candidates, 8 were projected to reach early stage small clinical
trials, 3-to-5 were projected to reach large scale clinical trials.32 The selection also aimed at
vaccine candidates based on different technologies and vaccinal strategies in an attempt to
accelerate the time frame and increase the odds of having a successful vaccine in the near
future. By way of example, the AZD1222 developed by University of Oxford and
AstraZeneca, is an adenovirus-based vaccine. Its strategy is to train the immune system to
recognize the spike protein typical of the SARS-CoV-2 surface, by carrying DNA for the
spike antigen in host cells though a vector. In the case of AZD1222 the vector is a
genetically-engineered adenovirus of chimps. The Moderna's mRNA-1273 aims at the same
training strategy, but with a nucleotide-based vaccine. In this case, a synthetic lipid



31
  In the VC industry, this has largely favored IT and digital companies.
32
  https://www.hhs.gov/about/news/2020/05/15/trump-administration-announces-framework-and-leadership-for-
operation-warp-speed.html. Accessed November 2020.


                                                                                                     33
nanoparticle is engineered to carry mRNA templates. Each approach may have drawbacks:
for example, some patients have pre-existing immunity to adenovirus and the antigens
encoded by mRNA may not confer sufficient protection against pathogens. Indeed, no
vaccine based on either adenovirus or nucleotides had ever been approved in the USA or
Europe (Mullard 2020) at the time of this writing. A different and possibly more dependable
strategy is the protein subunit approach taken by Sanofi-GlaxoSmithKline. In this case the
candidate is the spike antigen itself, combined with an immunogenic adjuvant, to trigger an
immune response.

           A question of considerable interest is whether a portfolio diversification strategy that
resembles financial portfolio management can be used to manage risk in science. This would
consist of choosing a mix of research projects with levels of expected value and volatility that
lead to a desired average future value with a desired risk-exposure. As before, the feasibility
of this approach depends on being able to forecast the outcomes of research projects with
reasonable accuracy. Moreover, and to complicate things, in this case one would need not
only to assess projects in isolation, but to also estimate the covariance among the outcomes of
different projects. A `bare bones' approach is to ask reviewers to classify proposals by levels
of risk involved, ask if any two pairs of projects have prospective outcomes which are
correlated,33 then fund projects in each level of risk, keeping the correlation below a
maximum threshold. It is possible that future experimental studies on protocols for risk
assessment in peer review will test this approach.




           8. Encouraging Risk

           Seed-funding Universities, for example, can promote risky research with the
potential of high payoff by providing seed funding to faculty. The California Institute of
Technology, by way of example, had such a program whereby faculty could submit a short
proposal to the Vice Provost for Research and get a decision in a matter of days. Funds
ranged from $25,000 to $250,000 a year for a period of two years. The idea was to give
faculty the wherewithal to engage in early-stage risky research that, given the risk aversion of
granting agencies, was deemed not yet ready for submission. If the initial findings looked
promising, and produced enough preliminary data, the faculty would then submit a full grant


33
     Prospective outcomes can be correlated, for example, if projects follow a similar approach and methodology.

                                                                                                              34
proposal. Universities also provide "bridge funding" to keep labs afloat between grants or if
there is a lapse in funding. For example, soon after IceCube was funded, George W. Bush
became president and declared "no-new-starts" which restricted NSF from funding any new
capital projects in the coming year. The University of Wisconsin responded by loaning
IceCube $4.5 million to keep it afloat (Bowen 2017:288).

       Block-funding Several scholars have stressed the role of block-funding--the practice
of assigning resources to scientists with no strings attached and without the need to commit to
a project--as a way to encourage risk taking. (Heinze 2017; Laudel 2017). Block funds can
be provided by universities, the employing institution or the national research systems to all
staff, irrespective of achievement, or can require some minimum level of research activity to
be funded. The practice is not without its critics, who point out that block funding allows
scientists to sit on resources and does not hold scientists accountable in case of misuse, thus
requiring more monitoring than does competitive funding. However, such a system arguably
encourages longer-term research trajectories and shelters scientists from the negative
consequences of early failure, in contrast to competitive research-funding. Wang, Lee and
Walsh (2018) compare the novelty of research funded under the two systems in Japan. They
find that research performed under block funding was more novel than that performed under
competitive funding for low-status investigators (e.g., junior and female investigators), but
the reverse was true for high-status scholars, where competitive funding was associated with
more novelty. The latter finding is consistent with work of Veugelers et al. (2019), which
finds that junior applicants to the ERC are penalized for having a history of novel research,
but senior applicants are not penalized.

        Grants for a Longer Duration of Time The Howard Hughes Medical Institute
(HHMI), as noted above, funds successful applicants for seven years, rather than for three to
five years, as is common for most other funding organizations. Furthermore, it does not
demand early results nor does it penalize researchers for early failure. Azoulay et al (2011)
compare the research output of HHMI investigators to a group of similarly accomplished
NIH investigators using propensity scoring. They find that HHMI investigators use more
novel keywords and produce more hits and more flops, compared to the NIH investigators.
Although it is not clear whether the results depend upon the longer duration of grants and the




                                                                                                35
practice of HHMI to not demand early results nor penalize researchers for early failure or
other variables,34 the results suggest that these practices encourage risk taking.

        Special High-Risk High-Gain funding initiatives Competitive funding can also be
directed specifically to High Risk High Gain science in ways that encourage risk-taking. We
have noted before that some institutions have research programs especially targeted to risky
science. Examples include the grants of the ERC, the Director's Awards for High-Risk, High-
Reward Research program of NIH, the, IDEAS Factory of the Engineering and Physical
Sciences Research Council of the UK and the Early-Concept Grants for Exploratory
Research (EAGER) program of NSF that replaced SGER grants.

        Pro-Risk Peer Review Design Targeting funds to support high risk high gain science
assumes that high-risk high-gain research can be identified and supported by reviewers. The
lack of understanding and suitable methods to assess risk, however, has been a critical
obstacle to the implementation of such a strategy. As an officer of a private foundation said
when speaking about the topic: "One of the challenges is we don't have any measures of
riskiness. We don't know when a project is high risk, high reward." (Michelson 2020:142).
Furthermore, many practices used in traditional peer review may discourage funding risky
research rather than promote the funding of risky research.35 One such practice is that of
requiring strong panel consensus to grant. Assuming that risky research is more uncertain and
sparks more disagreement than non-risky research (Linton 2016), the requirement of
consensus may work against risky endeavors.

        At present we have scant knowledge concerning which practices of peer review
design encourage or discourage the selection of risky projects. Amid a lack of scientific
understanding, some granting institutions are using creative ways to design peer-review
assessment that encourage risk taking. For example, the Audacious Project hosted by the
TED organization places emphasis on goals and type of reviewers involved. They choose to
focus on research with strong potential global impact, and to employ reviewers who are "not
typical experts in the relevant fields" (Price 2019:317). Other organizations place emphasis
on rules of deliberation that do not stress consensus. One example is the provision of a



34
   Despite the authors' efforts to match the HHMI sample with comparable NIH investigators, selection is still a
concern.
35
   For a humorous account, see Petsko, 2012. https://genomebiology.biomedcentral.com/articles/10.1186/gb-
2012-13-5-155. Accessed August 6, 2020.

                                                                                                             36
golden-ticket to each panel member that provides immediate selection, regardless of the
opinion of other panelists (Sinkjaer 2018). This system has been used by the Villum
Foundation; a similar system is used by the Melinda and Bill Gates Foundation. HHMI takes
a different perspective and selects people, instead of projects, encouraging researchers to ask
"tough questions in science, even at the risk of failure."36 Leslie Voshall, a highly productive
mosquito researcher at Rockefeller University, for example, is on record saying that her
application to HHMI, which was funded by the institute, involved doing something "bold and
new" and was supported with no preliminary data.37 The Chan Zuckerberg Biohub program
follows a somewhat similar philosophy, awarding fellowships to researchers for projects that
are based on "bold ideas that lack preliminary evidence" (Maxmen 2017). The Open
Philanthropy Project takes a somewhat similar approach, supporting projects that have "high
odds of failure" and often have been turned down by other organizations, such as NIH, on the
grounds that they are too risky.38

        Although these approaches are interesting, at the present time we lack both a scientific
understanding of peer review design and empirical evidence concerning the supposed
efficacy of the various approaches. Future research is certainly needed in this area.




        9. Conclusions

        We began this paper by asserting that a scholarly understanding of risk taking in
research was underdeveloped, yet critical given the key role that risk plays in advancing the
knowledge frontier. We set out to address this void by reviewing insights offered from other
fields that study risk. These contributions, combined with knowledge gained from studies of
science, led us to propose a conceptual model of risk in science that we hope can frame and
accelerate future research on risk and inform the related policy debate. The model we
developed disentangles different components that determine risk. It can also be used to
operationalize an expert-based risk metric, potentially useful in future peer reviewers'
evaluations. We also reviewed various text-based metrics of risk currently employed in
statistical analyses of large samples of research. Most of the studies that employ these metrics



36
   https://www.hhmi.org/programs/biomedical-research/investigator-program. Accessed August 6, 2020.
37
   https://podcasts.apple.com/us/podcast/the-inner-scientist/id1419667345. Accessed October 27, 2020.
38
   https://www.nature.com/articles/d41586-017-08795-0. Accessed October 27, 2020.


                                                                                                        37
suggest that the current reward structure of science discourages risk taking. There is also
evidence to suggest that this trend towards "play-it-safe science' is increasing. This led to a
review and discussion of strategies for hedging and for encouraging risk taking.

       Although this paper advances our understanding of risk-taking in science, there is
much that we do not know. The research agenda going forward is challenging but rich. We
encourage others to take up this important subject. Key issues to be addressed include, but
are not limited to, the following.

       First, there is a need to develop alternative measures of risk in science. We have
proposed a new metric, which awaits testing. The pluses are that it elicits different
components of risk. But the cost in terms of implementation could be quite high. Text-based
correlates of risk are much easier to implement, but, as we have outlined in Section 5, come
with serious pitfalls and limitations. Some of these can and should be addressed in future
research. Second, evidence suggests that selection by panels, which takes place in many
granting agencies, is biased against risky research, even in cases where risky research is a
stated priority. Although several funding organizations have started to experiment with
creative ways to select risky research, the reality is that none of these approaches are based
on a scientific understanding of which designs lead to more accurate assessments and are
more appropriate when supporting high-risk science is a priority. Moving forward, there is a
strong need for theorical and empirical investigations to build a scientific understanding of
peer review design. We hope that the notion of risk in science that we have offered will pave
the way to more scientific inquiries. Some of the questions that need investigation include the
following. Which experts or pool of experts are more accurate in assessing risky research?
Do experts eschew risk individually? Do they do so when they meet to discuss proposals in
consensus meetings? Do bolder peer review approaches lead to more polarized views,
sparking more disagreement? Can alternative deliberation rules, such as golden tickets,
promote risk? More generally, peer review is an understudied subject that invites research
into whether the way in which it is currently organized, and consensus formed works against
risky science.

       Third, is the need to consider how workload associated with large funding initiatives
affects panel decisions regarding risk. Likewise, there is the question of whether small scale
promotes risk taking. Is HHMI's apparent willingness to fund researchers who take risk
related to the small number of awards it makes and therefore its ability to look more closely


                                                                                                  38
at the applicant and their research? If so, could such a model be altered to situations where
scale is an issue? One possibility, for example, might be to allot a certain share of the budget
to risky proposals, then ask reviewers to identify risky proposals and choose among them,
using some random method such as a lottery, rather than try to score each individually on
risk. A related question is what should be an appropriate target budget for risk? 10%, 25%,
50%? Clearly there is no one answer to this question. The answer depends upon the funding
agency and the source of its funds. But in almost all cases the answer is greater than zero and
among public funders--whose mission is to fund research the private sector eschews-- it is
considerably larger.

       Fourth, risk and rewards are correlated in financial markets because there is a market
equilibrium that works through asset price adjustments. To what extent does science operate
in a parallel universe, inducing individuals to enter research areas where the reward--but also
the failure rate--is high? More generally, the issue of risk taking in science seems ripe for
study by financial economists. A promising and understudied line of inquiry is whether
portfolio approaches similar to those adopted in finance can be adopted by granting agencies.

       Fifth, there are virtually no studies of the attitudes of scientists towards risk, even
though numerous scholars study risk in other groups and conditions. It would be interesting
to study the risk attitude of scientists in different subfields and see how these relate to the
pace at which subfields advance. A related question is whether PhD training and early
experience of scientists exert a long-term effect on attitudes towards risk.

       Sixth, career conditions and the incentive systems appear to affect scientists'
willingness to take on a risky research agenda. But much more work needs to be done in this
area. Would less emphasis on bibliometrics in hiring, promotion and funding decisions have
the expected effect of encouraging risk taking? Would fewer soft money positions encourage
risk-taking? Would publishing failed research enhance risk-taking?

       Seventh, we need to know more about how risk correlates across projects. We lack
knowledge, for example, concerning how outcomes of research projects that share similar
research approaches are correlated. For example, if two projects on protein structure
determination use the same methodology, is the success/failure rate of projects not
correlated? Or, if a group of Alzheimer's studies focus on the same root cause, are findings
not likely to either miss the mark or coalescence on a treatment? This question also arises at
the more macro level. By way of example, mice constitute 90% of all animal models used in

                                                                                                  39
medical research (Stephan 2012). Yet only one out of nine drugs that work on animals work
on people. Why? Is the mouse a poor model? Have researchers, in an effort to standardize
experiments, controlled mouse environments to such an extent that the mouse is no longer a
useful model? Can the use of other animal models improve the success rates of drugs? The
question raises the possibility that while the progress of science is hindered by being risk
averse at the micro level, it is harmed by assuming too much risk at the macro level in terms
of highly correlated research models. This is an area that is ripe for research.

       To conclude, we set out to provide a conceptual foundation to inform and advance
discourse concerning risk in science. As this discussion of key issues shows, a rich agenda for
future studies, which is both intellectually challenging and critical for the future of science,
awaits. We call on scholars of science and adjacent disciplines to join in this effort.




Bibliography

Althaus, Catherine E. 2005. "A Disciplinary Perspective on the Epistemological Status of
     Risk." Risk Analysis 25(3):567­88.

Arora, Ashish, Sharon Belenzon, and Andrea Patacconi. 2018. "The Decline of Science in
     Corporate R&D." Strategic Management Journal 39:3­32.

Arrow, Kenneth J. 1962. "Economic Welfare and the Allocation of Resources for Invention."
     Pp. 609­26 in The Rate and Direction of Inventive Activity: Economic and Social
     Factors. Princeton University Press.

Aven, Terje. 2011. "On Some Recent Definitions and Analysis Frameworks for Risk,
     Vulnerability, and Resilience." Risk Analysis 31(4):515­22.

Aven, Terje. 2012. "The Risk Concept-Historical and Recent Development Trends."
     Reliability Engineering and System Safety 99(0951):33­44.

Aven, Terje and Ortwin Renn. 2009. "On Risk Defined as an Event Where the Outcome Is
     Uncertain." Journal of Risk Research 12(1):1­11.

Azoulay, Pierre, J. Graff Zivin, and Gustavo Manso. 2011. "Incentives and Creativity:
     Evidence from the Howard Hughes Medical Investigator Program." The Rand Journal
     of Economics 42(3):527­54.

                                                                                                   40
Berger, Loïc and Valentina Bosetti. 2020. "Are Policymakers Ambiguity Averse?" The
     Economic Journal 130(626):331­55.

Berger, Loïc, Johannes Emmerling, and Massimo Tavoni. 2016. "Managing Catastrophic
     Climate Risks Under Model Uncertainty Aversion." Management Science 63(3):587­
     900.

Bollerslev, Tim. 1986. "Generalized Autoregressive Conditional Heteroskedasticity." Journal
     of Econometrics 31(3):307­27.

Boudreau, Kevin, Eva Guinan, Karim R. Lakhani, and Christoph Riedl. 2014. "Looking
     Across and Looking Beyond the Knowledge Frontier: Intellectual Distance and
     Resource Allocation in Science." Management Science 62(10):2765­2783.

Bourdieu, Pierre. 2001. Science de La Science et Réflexivité. Ital. vers. Paris: Raisons D'Agir.

Bowen, Mark. 2017. The Telescope in the Ice. Inventing a New Astronomy at the South Pole.
     New York: San Martin Press.

Budish, Eric, Benjamin N. Roin, and Heidi Williams. 2015. "Do Firms Underinvest in Long-
     Term Research? Evidence from Cancer Clinical Trials." American Economic Review
     105(7):2044­85.

Budnitz, Robert J., George Apostolakis, David M. Boore, Lloyd S. Cluff, Kevin J.
     Coppersmith, C. Allin Cornell, and Peter A. Morris. 1998. "Use of Technical Expert
     Panels: Applications to Probabilistic Seismic Hazard Analysis." Risk Analysis
     18(4):463­69.

Camerer, Colin and Dan Lovallo. 1999. "Overconfidence and Excess Entry: An Experimental
     Approach." 89(1):306­18.

Camerer, Colin and Martin Weber. 1992. "Recent Developments in Modeling Preferences:
     Uncertainty and Ambiguity." Journal of Risk and Uncertainty 5:325­70.

Clemen, Robert T. and Robert L. Winkler. 1999. "Combining Probability Distributions From
     Experts in Risk Analysis." Risk Analysis 19(2):187­203.

Cohen, Wesley M., Henry Sauermann, and Paula Stephan. 2020. "Not in the Job Description:
     The Commercial Activities of Academic Scientists and Engineers." Management


                                                                                             41
    Science 66(9):4108­4117.

Collaboration, IceCube. 2018. "Neutrino Emission from the Direction of the Blazar TXS
    0506+056 Prior to the IceCube-170922A Alert." Science 361(6398):147­51.

Dobosz, Paula and Tomasz Dziecitkowski. 2019. "The Intriguing History of Cancer
    Immunotherapy." Fronteer Immunology 10:2965.

Edwards, Aled M., Ruth Isserlin, Gary D. Bader, Stephen V. Frye, Timothy M. Willson, and
    Frank H. Yu. 2011. "Too Many Roads Not Taken." Nature 470(7333):163­65.

Ehrlich, Isaac and Gary S. Becker. 1972. "Market Insurance , Self-Insurance, and Self-
    Protection." Journal of Political Economy 80(4):623­48.

Ellsberg, Daniel. 1961. "Risk, Ambiguity, and the Savage Axions." Quarterly Journal of
    Economics 75:643­69.

Engle, Robert F. 1982. "Autoregressive Conditional Heteroskedasticity with Estimates of the
    Variance of U.K. Inflation." Econometrica 50(4):987­1008.

Epstein, Seymour. 1994. "Integration of the Cognitive and the Psychodynamic Unconscious."
    American Psychologist 49(8):709­24.

Evans, Jonathan St B. T. and Keith E. Stanovich. 2013. "Dual-Process Theories of Higher
    Cognition: Advancing the Debate." Perspectives on Psychological Science 8(3):223­41.

Ewens, Michael, Ramana Nanda, and Matthew Rhodes-Kropf. 2018. "Cost of
    Experimentation and the Evolution of Venture Capital." Journal of Financial Economics
    128(3):422­42.

Fanelli, Daniele. 2010. "Do Pressures to Publish Increase Scientists' Bias? An Empirical
    Support from US States Data." PLoS ONE 5(4):e10271.

Fedorov, Oleg, Susanne Müller, and Stefan Knapp. 2010. "The (Un)Targeted Cancer
    Kinome." Nature Chemical Biology 6:166.

Feduzi, Alberto, Jochen Runde, and Carlo Zappia. 2014. "De Finetti on Uncertainty."
    Cambridge Journal of Economics 38(1):1­21.

Fischhoff, Baruch. 2015. "The Realities of Risk-Cost-Benefit Analysis." Science 350(6260).



                                                                                           42
Fischhoff, Baruch and Alex L. Davis. 2014. "Communicating Scientific Uncertainty."
    Proceedings of the National Academy of Sciences 111(3):13664­71.

Fischhoff, Baruch, Paul Slovic, Sarah Lichtenstein, Stephen Read, and Barbara Combs. 1978.
    "How Safe Is Safe Enough? A Psychometric Study of Attitudes Technological Risk and
    Benefit." The Perception of Risk Policy Sci(9):127­52.

Fleming, L., H. Greene, G. Li, M. Marx, and D. Yao. 2019. "Government-Funded Research
    Increasingly Fuels Innovation." Science 364(6446):1139­41.

Foster, Jacob G., Andrey Rzhetsky, and James A. Evans. 2015. "Tradition and Innovation in
    Scientists' Research Strategies." American Sociological Review 80(5):875­908.

Franco, Annie, Neil Malhotra, and Gabor Simonovits. 2014. "Publication Bias in the Social
    Sciences: Unlocking the File Drawer." Science 345(6203):1502­5.

Franzoni, Chiara. 2009. "Do Scientists Get Fundamental Research Ideas by Solving Practical
    Problems?" Industrial and Corporate Change 18(4):671­99.

Franzoni, Chiara and Cristina Rossi-Lamastra. 2017. "Academic Tenure, Risk-Taking and
    the Diversification of Scientific Research." Industry and Innovation 24(7):691­712.

Franzoni, Chiara and Henry Sauermann. 2014. "Crowd Science: The Organization of
    Scientific Research in Open Collaborative Projects." Research Policy 43(1):1­20.

Franzoni, Chiara, Giuseppe Scellato, and Paula Stephan. 2012. "Foreign-Born Scientists:
    Mobility Patterns for 16 Countries." Nature Biotechnology 30(12):1250­53.

Franzoni, Chiara, Giuseppe Scellato, and Paula Stephan. 2018. "Context Factors and the
    Performance of Mobile Individuals in Research Teams." Journal of Management
    Studies 55(1):27­59.

Frey, Renato, Andreas Pedroni, Rui Mata, Jörg Rieskamp, and Ralph Hertwig. 2017. "Risk
    Preference Shares the Psychometric Structure of Major Psychological Traits." Science
    Advances 3(10):1­13.

Gilbert, Nigel G. and Michael Mulkay. 2003. Opening Pandora's Box. A Sociological
    Analysis of Scientists' Discourse. Cambridge: Cambridge University Press.

Haimes, Yacov Y. 2009. "On the Complex Definition of Risk: A Systems-Based Approach."


                                                                                           43
    Risk Analysis 29(12):1647­54.

Hájek, Alan. 2019. "Interpretations of Probability." The Stanford Encyclopedia of
    Philosophy. Edward N. Zalta (Ed.). Retrieved
    (https://plato.stanford.edu/archives/fall2019/entries/probability-interpret/).

Halzen, Francis. 2010. "The IceCube Project."

Halzen, Francis and J. G. Learned. 1988. "High-Energy Neutrino Detection in Deep Polar
    Ice." in Proceedings of the 5th International Symposium on Very High-Energy Cosmic
    Ray Interactions.

Hansson, Sven Ove. 1989. "Dimensions of Risk." Risk Analysis 9(107­112).

Hansson, Sven Ove. 2002. "Philosophical Perspectives on Risk." Keynote Address at the
    Conference Research in Ethics and Engineering (25 April 2002).

Hansson, Sven Ove. 2018. "Risk." in The Stanford Encyclopedia of Philosophy, edited by E.
    N. Zalta. Stanford University.

Harris, Richard. 2013. "Scientists Win Nobel For Work On How Cells Communicate."
    Retrieved (https://www.npr.org/templates/story/story.php?storyId=230192033).

Heinze, Thomas. 2017. "How to Sponsor Ground-Breaking Research: A Comparison of
    Funding Schemes." Science and Public Policy 35(5):302­18.

Henrickson, Sarah and David Altshuler. 2012. "Risk and Return for the Clinician-
    Investigator." Science Translational Medicine 4(135):135cm6 LP-135cm6.

Johansen, Inger Lise and Marvin Rausand. 2014. "Foundations and Choice of Risk Metrics."
    62:386­99.

Kahneman, Daniel and Amos Tversky. 1979. "Prospect Theory: An Analysis of Decision
    under Risk." Econometrica 47(2):263­92.

Kaplan, Stanley and B. John Garrick. 1981. "On The Quantitative Definition of Risk." Risk
    Analysis 1(1):11­27.

Kasperson, Roger E., Ortwin Renn, Paul Slovic, Halina S. Brown, Jacque Emel, Robert
    Goble, Jeanne X. Kasperson, and Samuel Ratick. 1988. "A Social Amplification of
    Risk: A Conceptual Framework." Risk Analysis 8(2):177­87.

                                                                                            44
Klibanoff, Peter, Massimo Marinacci, and Sujoy Mukerji. 2005. "A Smooth Model of
    Decision Making under Ambiguity." Econometrica 73(6):1849­92.

Knight, Frank. 1921. Risk, Uncertainty, and Profit. Boston, MA: Houghton Mifflin.

Krieger, Joshua L., Danielle Li, and Dimitris Papanikolaou. 2019. Nber Working Paper
    Series Missing Novelty in Drug Development.

Krummel, Matthew F. and James P. Allison. 1995. "CD28 and CTLA-4 Have Opposing
    Effects on the Response of T Cells to Stimulation M F Krummel 1, J P Allison." Journal
    of Experimental Medicine 182(2):459­65.

Lanoë, Marianne. 2019. "The Evaluation of Competitive Research Funding: An Application
    to French Programs." L'Universite de Bordeaux.

Laudel, Grit. 2017. "How Do National Career Systems Promote or Hinder the Emergence of
    New Research Lines?" Minerva 55(3):341­69.

Lerner, Josh and Ramana Nanda. 2020. "Venture Capital's Role in Financing Innovation:
    What We Know and How Much We Still Need to Learn." Journal of Economic
    Perspectives 34(3):237­61.

Linton, J. D. 2016. "Improving the Peer Review Process: Capturing More Information and
    Enabling High-Risk/High-Return Research." Research Policy 45(9):1936­38.

Loewenstein, George F., Chrisopher K. Hsee, Elke U. Weber, and Ned Welch. 2001. "Risk as
    Feelings." Psychological Bulletin 127(2):267­86.

Loughran, Tim and Bill McDonald. 2013. "IPO First-Day Returns, Offer Price Revisions,
    Volatility, and Form S-1 Language." Journal of Financial Economics 109(2):307­26.

March, James G. 1991. "Exploration and Exploitation in Organizational Learning."
    Organization Science 2(1):71­87.

Marinacci, Massimo. 2015. "Model Uncertainty." Journal of the European Economic
    Association 13(6):998­1076.

Markowitz, Harry M. 1952. "Portfolio Selection." Journal of Finance 7(1):77­91.

Mata, Rui, Renato Frey, D. Richter, J. Schupp, and Ralph Hertwig. 2018. "Risk Preference:
    A View from Psychology." Journal of Economic Perspectives 32(2):155­72.

                                                                                         45
Maxmen, Amy. 2017. "`Riskiest Ideas' Win $50 Million from Chan Zuckerberg Biohub."
    Nature 542:280­281.

Mellers, Barbara, Eric Stone, Terry Murray, Angela Minster, Nick Rohrbaugh, Michael
    Bishop, Eva Chen, Joshua Baker, Yuan Hou, Michael Horowitz, Lyle Ungar, and Philip
    Tetlock. 2015. "Identifying and Cultivating Superforecasters as a Method of Improving
    Probabilistic Predictions." Perspectives on Psychological Science 10(3):267­81.

Merton, Robert K. 1957. "Priorities in Scientific Discovery: A Chapter in the Sociology of
    Science." American Sociological Review 22(6):635­59.

Michelson, Evan S. 2020. Philanthropy and the Future of Science and Technology. Abigdon,
    Oxon (UK) and New York (NY): Routledge.

Morgan, M. Granger. 2014. "Use (and Abuse) of Expert Elicitation in Support of Decision
    Making for Public Policy." Proceedings of the National Academy of Sciences
    111(20):7176­84.

Mullard, Asher. 2020. "COVID-19 Vaccine Development Pipeline Gears Up." Lancet
    395(10239):1751­52.

Nelson, Richard R. 1959. "The Simple Economics of Basic Scientific Research." Journal of
    Political Economy 67(3):297­306.

OECD. 2018. "Effective Operation of Competitive Research Funding Systems." OECD
    Science, Technology and Industry Policy Papers (57).

Petsko, Gregory. 2011. "Risky Business." Genome Biology 12(119).

Popper, Karl. 1959. The Logic of Scientific Discovery. London: Hutchinson & Co.

Price, Michael. 2019. "TED Offers Funding Path, Not Just Stage, for `audacious' Ideas."
    Science 364(6438):317.

Raiffa, Howard. 1997. Decision Analysis: Introductory Lectures on Choices Under
    Uncertainty. McGraw-Hill College.

Renn, Ortwin. 1998. "Three Decades of Risk Research: Accomplishments and New
    Challenges." Journal of Risk Research 1(1):49­71.

Romer, Paul. 1994. "The Origins of Endogenous Growth." Journal of Economic Perspectives

                                                                                             46
    8(1):3­22.

Rosenberg, Steven A., James C. Yang, and Nicholas P. Restifo. 2004. "Cancer
    Immunotherapy: Moving beyond Current Vaccines." Nature Medicine 10(9):909­15.

Rosenthal, R. 1979. "The File Drawer Problem and Tolerance for Null Results."
    Psychological Bulletin 86:638­641.

Rzhetsky, Andrey, Jacob G. Foster, Ian T. Foster, and James A. Evans. 2015. "Choosing
    Experiments to Accelerate Collective Discovery." Proceedings of the National Academy
    of Sciences 112(47):14569­74.

Sauermann, Henry, Chiara Franzoni, and Kourosh Shafi. 2019. "Crowdfunding Scientific
    Research: Descriptive Insights and Correlates of Funding Success." PLoS ONE
    14(1):e0208384.

Simonton, Dean Keith. 2003. "Scientific Creativity as Constrained Stochastic Behavior: The
    Integration of Product, Person, and Process Perspectives." Psychological Bulletin
    129(4):475­94.

Sinkjaer, Thomas. 2018. "Fund Ideas, Not Pedigree, to Find Fresh Insight." Nature 555:143.

Sloman, Steven A. 1996. "The Empirical Case for Two Systems of Reasoning."
    Psychological Bulletin 119(1):3­22.

Slovic, Paul. 1987. "Perception of Risk." Science 236:280­85.

Slovic, Paul, Melissa L. Finucane, Ellen Peters, and Donald G. MacGregor. 2010. "Risk as
    Analysis and Risk as Feelings: Some Thoughts about Affect, Reason, Risk and
    Rationality." Pp. 21­36 in The Feeling of Risk, edited by P. Slovic. Abingdon, Oxon:
    Earthscan.

Slovic, Paul, Ellen Peters, Melissa L. Finucane, and Donald G. MacGregor. 2005. "Affect,
    Risk, and Decision Making." Health Psychology 24(4 SUPPL.):35­40.

Stephan, Paula. 2007. "Early Careers for Biomedical Scientists: Doubling (and Troubling)
    Outcomes Presentation Harvard University." in Presentation Harvard University.
    Boston, MA.

Stephan, Paula. 2012. How Economics Shapes Science. Cambridge, MA: Harvard University


                                                                                           47
    Press.

Stephan, Paula. 2019. "Practices and Attitudes Regarding Risky Research." Stanford
    University: Metascience Symposium.

Stephan, Paula E. 1996. "The Economics of Science." Journal of Economic Literature
    34(3):1199­1235.

Stephan, Paula E. 2010. "The Economics of Science." Pp. 217­73 in Handbook of Economics
    of Innovation, edited by B. H. . Hall and N. Rosenberg. Amsterdam, NL: North Holland.

Stephan, Paula and Sharon Levin. 1992. Striking the Mother Lode in Science: The
    Importance of Age, Place, and Time. Oxford, UK: Oxford University Press.

Stephan, Paula, Reinhilde Veugelers, and Jian Wang. 2017. "Blinkered by Bibliometrics."
    Nature 544(7651):411­12.

Suntharalingam, Ganesh, Meghan R. Perry, Stephen Ward, Stephen J. Brett, Andrew
    Castello-Cortes, Michael D. Brunner, and Nicki Panoskaltsis. 2006. "Cytokine Storm in
    a Phase 1 Trial of the Anti-CD28 Monoclonal Antibody TGN1412." New England
    Journal of Medicine 355:1018­28.

Tetlock, Philippe and Dan Gardner. 2015. Super-Forecasting. The Art and Science of
    Prediction. London: Penguin Random House.

Tobin, James. 1958. "Liquidity Preference as Behavior towards Risk." Review of Economic
    Studies 25(2):65­86.

Tversky, Amos and Craig R. Fox. 1995. "Weighing Risk and Uncertainty Tversky Fox
    1995.Pdf." Psychological Review 102(2):269.

Tversky, Amos and Daniel Kahneman. 1974. "Judgment under Uncertainty: Heuristics and
    Biases." Science 185(4157):1124­31.

Tversky, Amos and Daniel Kahneman. 1991. "Loss Aversion in Riskless Choice: A
    Reference-Dependent Model." Quarterly Journal of Economics 106(4):1039­61.

Tversky, Amos and Daniel Kahneman. 1992. "Advances in Prospect Theory: Cumulative
    Representation of Uncertainty." Journal of Risk and Uncertainty 5(4):297­323.

Uzzi, Brian, Satyam Mukherjee, Michael Stringer, and Ben Jones. 2013. "Atypical

                                                                                          48
    Combinations and Scientific Impact." Science 342(6157):468­72.

Varian, Hall R. 2009. "The Economics of Combinatorial Innovation. Guglielmo Marconi
    Lecture June 16, 2009."

Veugelers, Reinhilde, Paula Stephan, and Jian Wang. 2019. "Excess Risk-Aversion at ERC."
    Presented at the ASSOF Meeting.

Vilkkumaa, Eeva, Ahti Salo, Juuso Liesiö, and Afzal Siddiqui. 2015. "Fostering
    Breakthrough Technologies - How Do Optimal Funding Decisions Depend on
    Evaluation Accuracy?" Technological Forecasting and Social Change 96:173­90.

Wang, Jian, You Na Lee, and John P. Walsh. 2018. "Funding Model and Creativity in
    Science: Competitive versus Block Funding and Status Contingency Effects." Research
    Policy 47(6):1070­83.

Wang, Jian, Reinhilde Veugelers, and Paula Stephan. 2017. "Bias against Novelty in Science:
    A Cautionary Tale for Users of Bibliometric Indicators." Research Policy 46(8):1416­
    36.

Weinstein, Neil D. 1980. "Unrealisitc Optimism about Future Life Events." Journal of
    Personality and Social Psychology 39(5):806­20.

Winkler, Robert L., Yael Grushka-Cockayne, C. Lichtendahl Jr, and Richmond R. Jose.
    2019. "Probability Forecasts and Their Combination: A Research Perspective."
    Decision Analysis 16(4):239­333.

Wolchok, Jedd D., F. Stephen Hodi, Jeffrey S. Weber, James P. Allison, Walter J. Urba,
    Caroline Robert, Steven J. O'Day, Axel Hoos, Rachel Humphrey, David M. Berman,
    Nils Lonberg, and Alan J. Korman. 2013. "Development of Ipilimumab: A Novel
    Immunotherapeutic Approach for the Treatment of Advanced Melanoma." Annals of the
    New York Academy of Sciences 1291(1):1­13.




                                                                                         49
