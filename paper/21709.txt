                             NBER WORKING PAPER SERIES




                                 DISCRETION IN HIRING

                                      Mitchell Hoffman
                                       Lisa B. Kahn
                                         Danielle Li

                                     Working Paper 21709
                             http://www.nber.org/papers/w21709


                    NATIONAL BUREAU OF ECONOMIC RESEARCH
                             1050 Massachusetts Avenue
                               Cambridge, MA 02138
                           November 2015, September 2017




We are grateful to Jason Abaluck, Ajay Agrawal, Ricardo Alonso, Pol Antras, Ian Ball, David
Berger, Arthur Campbell, David Deming, Alex Frankel, Avi Goldfarb, Lawrence Katz, Harry
Krashinsky, Peter Landry, Jin Li, Liz Lyons, Steve Malliaris, Mike Powell, Kathryn Shaw, Steve
Tadelis, numerous seminar participants, and anonymous referees for helpful comments. We are
grateful to the anonymous data provider for providing access to proprietary data. Ho man
acknowledges financial support from the Social Science and Humanities Research Council of
Canada. All errors are our own. The views expressed herein are those of the authors and do not
necessarily reflect the views of the National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.

Â© 2015 by Mitchell Hoffman, Lisa B. Kahn, and Danielle Li. All rights reserved. Short sections
of text, not to exceed two paragraphs, may be quoted without explicit permission provided that
full credit, including Â© notice, is given to the source.
Discretion in Hiring
Mitchell Hoffman, Lisa B. Kahn, and Danielle Li
NBER Working Paper No. 21709
November 2015, September 2017
JEL No. J24,M51

                                        ABSTRACT

Job testing technologies enable firms to rely less on human judgement when making hiring
decisions. Placing more weight on test scores may improve hiring decisions by reducing the
influence of human bias or mistakes but may also lead firms to forgo the potentially valuable
private information of their managers. We study the introduction of job testing across 15 firms
employing low-skilled service sector workers. When faced with similar applicant pools, we find
that managers who appear to hire against test recommendations end up with worse average hires.
This suggests that managers often overrule test recommendations because they are biased or
mistaken, not only because they have superior private information.


Mitchell Hoffman                               Danielle Li
Rotman School of Management                    Harvard Business School
University of Toronto                          211 Rock Center
105 St. George Street                          Soldiers Field
Toronto, ON M5S 3E6                            Boston, MA 02163
CANADA                                         and NBER
and NBER                                       dli@hbs.edu
mitchell.hoffman@rotman.utoronto.ca

Lisa B. Kahn
Yale University
School of Management
P. O. Box 208200
New Haven, CT 06520
and NBER
lisa.kahn@yale.edu
I Introduction
Hiring the right workers is one of the most important and dicult problems that a rm

faces.      Resumes, interviews, and other screening tools are often limited in their ability to

reveal whether a worker has the right skills or will be a good t.                   Further, the managers

that rms employ to gather and interpret this information may have poor judgement or

preferences that are imperfectly aligned with rm objectives.
                                                                              1   Firms may thus face both

information and agency problems when making hiring decisions.

    The increasing adoption of workforce analytics and job testing has provided rms with

new hiring tools.
                         2     Job testing has the potential to both improve information about the

quality of candidates and to reduce agency problems between rms and human resource (HR)

managers. As with interviews, job tests provide an additional signal of a worker's quality.

Yet, unlike interviews and other subjective assessments, job testing provides information

about worker quality that is directly veriable by the rm.

    What is the impact of job testing on the quality of hires and how should rms use job

tests? In the absence of agency problems, rms should allow managers discretion to weigh

job tests alongside interviews and other private signals when deciding whom to hire. Yet,

if managers are biased or if their judgment is otherwise awed, rms may prefer to limit

discretion and place more weight on test results, even if this means ignoring the private

information of the manager. Firms may have diculty evaluating this trade o because they

cannot tell whether a manager hires a candidate with poor test scores because of private

evidence to the contrary, or because he or she is biased or simply mistaken.

    In this paper, we evaluate the introduction of a job test and analyze the consequences

of making hiring decisions that deviate from test score recommendations. We use a unique

personnel dataset consisting of 15 rms who employ workers in the same low-skilled service

sector. Prior to the introduction of testing, rms employed HR managers who were involved

in hiring new workers. After the introduction of testing, HR managers were also given access

   1 For example, a manager could have preferences over demographics or family background that do not
maximize productivity. In a case study of elite professional services rms, Riviera (2014) shows that one of
the most important determinants of hiring is the presence of shared leisure activities.
   2 See,    for   instance,    Forbes :   http://www.forbes.com/sites/joshbersin/2013/02/17/bigdata-in-human-
resources-talent-analytics-comes-of-age/.




                                                           1
to a test score for each applicant: green (high potential candidate), yellow (moderate poten-

tial candidate), or red (lowest rating). Managers were encouraged to factor the test into their

hiring decisions, but were not required to hire strictly according to test recommendations.

   We rst estimate the impact of introducing the job test on the quality of hired workers.

Exploiting the staggered introduction of job testing across sample locations, we show that

cohorts of workers hired with job testing have substantially longer tenures than cohorts of

workers hired without testing, holding constant a variety of time-varying location and rm

variables. In our setting, job tenure is a key measure of quality because turnover is costly and

workers already spend a substantial fraction of their tenure in paid training. This nding

suggests that this job test contains useful information about the quality of candidates.

   Next, we examine how managers use job test information. We propose a model in which

rms rely on potentially biased HR managers who observe a private signal of worker quality

in addition to the publicly observable job test. Managers can decide to hire workers with

the best test scores or make exceptions by hiring against the test recommendation.         In

the absence of bias, managers make exceptions only when they have additional information,

resulting in better hires for the rm.    However, biased managers are also more likely to

make exceptions, and these exceptions lead to worse hires on average.         This model thus

provides intuition for why the observed relationship between a manager's propensity to

make exceptions and worker outcomes can be informative about the role of bias in hiring: a

positive relationship suggests that managers primarily make exceptions when they are better

informed, while a negative relationship suggests the presence of bias or mistaken beliefs.

   Our data, which includes information on applicants as well as hired workers, allows us

to explore this relationship empirically.   We dene an exception as hiring an applicant

with a yellow test score when one with a green score had also applied but is not hired (or

similarly, when a red applicant is hired while a yellow or green is not). Across a variety

of specications, we nd that exceptions are strongly correlated with worse outcomes. Even

controlling for the test scores of the applicant pools they hire from, managers who appear

to make more exceptions systematically bring in workers who leave their jobs more quickly.

This result suggests that managers make exceptions not only when they are better informed

but also because they are biased or mistaken.




                                               2
   Finally, we show that our results are unlikely to be driven by the possibility that managers

sacrice job tenure in search of workers who have higher quality on other dimensions. If this

were the case, limiting discretion may improve worker durations, but at the expense of other

quality measures. To examine this possibility, we examine the relationship between hiring

exceptions and a direct measure of individual productivity, daily output per hour, which

we observe for a subset of rms in our sample.     In this supplemental analysis, we nd no

evidence that exceptions are related to increased productivity; this makes it unlikely that

managers trade o duration for productivity.

   Our empirical approach diers from an experiment in which discretion is granted to

some managers and not others. Rather, our analysis exploits dierences across managers in

the extent to which they appear to make exceptions by overruling test recommendations.

Our approach uses this non-random variation in willingness to exercise discretion to infer

whether discretion facilitates better hires. If managers use discretion only when they have

better information, then managers who make more exceptions should have better outcomes

than managers who do not. If exceptions are instead associated with worse outcomes, then

it is likely that managers are also biased or mistaken.

   The validity of this approach relies on two key assumptions.       First, we must be able

to isolate variation in exceptions that is reective of managerial choices, and not driven

by lower yield rates for higher quality applicants.   A weakness of our data is that we do

not observe job oers; because of this, managers who hire yellow or red workers only after

green applicants have turned down job oers will mistakenly look as though they made

more exceptions. Second, it must also be true that the unobserved quality of applicants are

similar across low- and high-exception cohorts. For example, we want to rule out cases where

managers make exceptions precisely because the pool of green applicants is idiosyncratically

weak. We discuss both of these assumptions in more detail throughout the text and estimate

specications that either directly address or limit these concerns.

   As data analytics is more frequently applied to human resource management decisions,

it becomes increasingly important to understand how these new technologies impact the

organizational structure of the rm and the eciency of worker-rm matching.          While a

large theoretical literature has studied how rms should allocate authority, and a smaller




                                               3
empirical literature has examined discretion and rule-making in other settings, empirical

evidence on discretion in hiring is scant.
                                               3   Our paper provides a rst step towards an empirical

understanding of the potential benets of discretion in hiring. Our ndings provide evidence

that screening technologies may improve information symmetry between rms and managers.

In this spirit, our paper is related to the classic Baker and Hubbard (2004) analysis of the

adoption of on board computers in the trucking industry.

    Our work is most closely related to Autor and Scarborough (2008), the rst paper in

economics to provide an estimate of the impact of job testing on worker performance.
                                                                                                           4

The authors evaluate the introduction of a job test in retail trade, with a particular focus

on whether testing will have a disparate impact on minority hiring. We also nd positive

impacts of testing, and, from there, focus on the complementary question of the consequences

of overruling the job test. Our results are broadly aligned with ndings in psychology and

behavioral economics that emphasize the potential of machine-based algorithms to mitigate

errors and biases in human judgement across a variety of domains.
                                                                               5

    The remainder of this paper proceeds as follows. Section II describes the setting and data.

Section III evaluates the impact of testing on job duration. Section IV presents a model of

hiring with potentially biased managers.             Motivated by the model, Section V empirically

assesses whether managers use their discretion to improve hires. Section VI concludes. All

appendix material can be found in the Online Appendix.

   3 For theoretical work, see the canonical Aghion and Tirole (1997), the Bolton and Dewatripont (2012)
survey, and Dessein (2002) and Alonso and Matouschek (2008) for particularly relevant instances.         For
empirical work, see for example, Paravisini and Schoar (2012) and Wang (2014) for analyses of loan ocers,
Li (2017) on grant committees, Kuziemko (2013) on parole boards, and Diamond and Persson (2016) on
teacher grading.
   4 We also contribute to the broader literatures on screening technologies (e.g., Autor (2001), Stanton and
Thomas (2015), Horton (2017), Brown, Setren, and Topa (2016), Burks et al. (2015), and Pallais and Sands
(2016)) and employer learning (Farber and Gibbons (1996), Altonji and Pierret (2001), and Kahn and Lange
(2014)).
   5 See Kuncel et.   al.   (2013) for a meta-analysis of this literature, Kahneman (2011) for a behavioral
economics perspective, and Kleinberg at al. (2018) for empirical evidence that machine-based algorithms
outperform judges in deciding which arrestees to detain pre-trail.




                                                       4
II Setting and Data
Firms have increasingly incorporated testing into their hiring practices. One explanation for

this shift is that the rising power of data analytics has made it easier to look for regularities

that predict worker performance. We obtain data from an anonymous job testing provider

that follows such a model. We hereafter term this rm the data rm. In this section, we

summarize the key features of our setting and dataset. More detail about both the job test

and our sample can be found in Section A of the Online Appendix.




II.A Job test and testing adoption
Our data rm oers a test designed to predict performance for a particular job in the low-

skilled service sector. We are unable to reveal the exact nature of the job, but it is similar

to jobs such as data entry work, standardized test grading, and call center work (and is not

a retail store job). The data rm sells its services to clients (hereafter, client rms) that

wish to ll these types of positions. We have 15 such client rms in our dataset.

   Across locations, the workers in our data are engaged in a fairly uniform job and perform

essentially a single task. For example, one should think of our data as comprised entirely

of data entry jobs, entirely of standardized test grader jobs, or entirely of call center jobs.

Workers generally do not have other major job tasks to perform. As with data entry, grading,

or call center work, workers in our sample engage in individual production: they do not work

in teams to create output nor does the pace of their output directly impact others.

   The job test provided by our data rm consists of an online questionnaire comprising a

large battery of questions, including those on computer/technical skills, personality, cognitive

skills, t for the job, and various job scenarios. The data rm matches applicant responses

with subsequent performance in order to identify the various questions that are the most

predictive of future workplace success in this setting.      Drawing on these correlations, a

proprietary algorithm delivers a green-yellow-red job test score.       In our sample, 48% of

applicants receive a green score, 32% score yellow, and 20% score red. See Section A.1 of

the Online Appendix for more detail on the test itself.




                                               5
    Job testing was gradually rolled out across locations (establishments) within a given client

rm. We observe the date at which test scores appear in our data, but not all workers are

tested immediately. Our preferred measure denes test-adoption as the date at which the

modal hire had a test score. See Online Appendix A.2 for more discussion and robustness

to other denitions.

    The HR managers in our data are referred to as recruiters by our data provider and

are unlikely to manage day-to-day production.               Prior to the introduction of job testing,

our client rms gave their HR managers discretion to make hiring recommendations based

on interviews and resumes.
                                6   After adopting this job test, rms made applicant test scores

available to managers and encouraged them to factor scores into hiring recommendations,

but managers were still permitted to hire their preferred candidate.
                                                                                  7



II.B Applicant and Worker Data
Our data contain information on hired workers, including hire and termination dates, job

function, and worker location.          This information is collected by client rms and shared

with the data rm.       Once a partnership with the data rm forms, we observe additional

information, including applicant test scores, application date, and an identier for the HR

manager responsible for a given applicant.

    Table I provides sample characteristics. We observe nearly 266,000 hires; two-thirds are

observed before testing was introduced and one-third after. Our post-testing sample consists

of 400,000 applicants and 91,000 hires assigned to 445 managers.
                                                                              8

    Our primary worker outcome is job duration.                We focus on turnover for three main

reasons. Foremost, turnover is a perennial challenge for rms employing low skilled service

sector workers.     Hence, tenure is an important measure of worker quality for our sample

   6 Other managers may take part in hiring decisions as well. For example, in one rm, recruiters typi-
cally endorse a candidate to another manager (e.g., a manager in operations one rank above the frontline
supervisor) who will make a nal call.
   7 We do not directly observe managerial authority in our data. However information provided to us by
the data rm indicates that managers at client rms were not required to hire strictly by the test, and we
see in our data that many workers with low test scores are hired. Also, some client rms had other forms
of job testing before partnering with our data rm (see Online Appendix A.3 for details and robustness to
restricting the sample to client rms that likely did not have pre-sample testing.).
   8 See Section A.7 of the Online Appendix for sample restrictions.




                                                      6
rms.   To illustrate this concern, Figure I shows a histogram of job tenure for completed

spells (79% of the spells in our data) among employees in our sample. The median worker

(solid red line) stays only 99 days, or just over 3 months. One in six workers leave after only

a month. Despite these short tenures, hired workers in our sample spend the rst several

weeks of their employment in paid training.
                                                     9   Both our data rm and its client rms are

aware of these concerns: in its marketing materials, our data rm emphasizes the ability

of its job test to reduce turnover.        Second, in addition to its importance for our sample

rms, in many canonical models of job search (e.g., Jovanovic 1979), worker tenures can be

thought of as a proxy for match quality. As such, job duration is a commonly used measure

of worker quality. For example, it is the primary worker quality measure used by Autor and

Scarborough (2008), who also study the impact job testing in a low-skilled service sector

setting (retail). Finally, job duration is available for all workers in our sample.

    For a subset of our client rms, we also observe a direct measure of worker productivity:

output per hour.
                   10   Again, we are not able to reveal the exact nature of the job. That said,

output per hour measures the number of primary tasks that an individual worker is able

to complete. For example, this would be number of words entered per hour in data entry,

number of tests graded in test grading, and number of calls handled in call centers. Recall

that in our setting, individuals perform essentially one major task and engage in individual

production. Because of the discretized nature of the work, output per hour is a very common

performance metric for the type of job we study, and is easily measured. However, our data

rm was only able to provide us with this measure for a subset of client rms (roughly a

quarter of hired workers). We report these ndings separately when we discuss alternative

explanations.

    The middle panel of Table I provides summary statistics for duration and output per

hour. Job durations are censored for the 21% of hired workers who were still employed at

the time our data was collected. In our analysis, we take censoring into account by estimating

censored normal regressions whenever we use duration as an outcome measure.

   9 Reported lengths of paid training vary considerably, from around 1-2 weeks to around a couple months
or more, but is provided by all client rms in our sample.
  10 A similar productivity measure was used in Lazear, Shaw, and Stanton (2015) to evaluate the value of
bosses in a comparable setting to ours.




                                                     7
      Table I shows that both censored and uncensored job durations increase in color score. For

example, among those with completed spells, greens stay 12 days (11%) longer than yellows

who stay 18 days (20%) longer than reds. These dierences are statistically signicant and

provide initial evidence that test scores are predictive of worker performance.                  Further, if

managers hire red and yellow applicants only when their unobserved quality is high, then

tenure dierences in the overall applicant population should be even larger.                   There is no

dierence across color score in the share of observations that are censored.
                                                                                         11

      Average output per hour in our dataset is 8.4 and is fairly similar across color.                  Red

workers have somewhat higher productivity along this metric, although these dierences are

not signicant; also, controlling for client rm xed eects removes any dierence in output

per hour for red workers. Finally, the bottom panel of Table I shows that scores are predictive

of hiring: greens are more likely to be hired than yellows, who are in turn substantially more

likely to be hired than reds.




III The Impact of Testing
III.A Empirical Strategy
We rst evaluate the impact of introducing testing itself. This analysis helps us understand

whether the test has useful information that at least some managers take advantage of. To

do so, we exploit the gradual roll-out of testing across locations and over time, and examine

its impact on worker quality, as measured by tenure.




(1)             Log(Duration)ilt     = Î±0 + Î±1 Testinglt + Î´l + Î³t + Positioni Î² + ilt


Equation (1) compares outcomes for workers hired with and without job testing. We estimate

censored normal regressions with individual-specic truncation points to account for the fact

that not all workers are observed through the end of their employment spell. We regress log

  11 One thing to note in our table is that, somewhat counterintuitively, job durations are longer for workers
hired before testing than afterwards. The main reason for this is mechanical: on average, pre-testing periods
are earlier in the sample (by about 16 months), allowing hired workers more time to accrue more tenure.
Hire cohort xed eects account for this eect in our regression analysis.




                                                      8
duration (Log(Duration)ilt ) for a worker i, hired to a location l , at time t, on an indicator for

whether the location,     l   had testing at time   t   (Testinglt ). Recall, we assign the test adoption

date as the rst date in which the modal hire at a location had a test score.                    After that

point, the location is always assigned to the testing regime. We choose to dene testing at

the location, rather than individual, level in order to avoid the possibility that whether an

individual worker is tested may depend on observed personal characteristics (Table A1 of

the Online Appendix shows that our results are robust to dening testing at the individual

level or at the rst date in which any worker is tested).

    All regressions include location     (Î´l ) and month-by-year of hire (Î³t ) xed eects to control
for time-invariant dierences across locations within our client rms, and for cohort and

macroeconomic eects that may impact job duration or censoring probability. We also always

include position-type xed eects (the vector, Positioni , and associated coecients                 Î²)   that

adjust for small dierences in job function across individuals.
                                                                          12    In some specications, we

also include additional controls, which we describe alongside the results. In all specications,

standard errors are clustered at the location level to account for correlated observations

within a location over time.

    Section A.3 of the Online Appendix discusses sample coverage of locations over time and

shows robustness to using a more balanced panel; Section A.4 explores the timing of testing

and assesses whether early testing locations look dierent on observable characteristics.




III.B Results
Table II reports regression results.        Column 1 presents results with controls for location,

cohort, and position. In the subsequent columns, we cumulatively add controls. Column 2

adds client rm-by-year xed eects, to control for the implementation of any new strategies

and HR policies that rms may have adopted along with testing.
                                                                               13   The column 2 coecient

of 0.24 means that employees hired with the assistance of job testing stay, on average, 0.24 log

points, longer. Column 3 adds location unemployment rate controls to account for the fact

  12 For example, in data entry, xed eects would distinguish workers who enter textual data from those
who transcribe auditory data, and those who enter data regarding images; in test grading, individuals may
grade science or math tests; in call centers, individuals may engage in customer service or sales.
  13 Our data rm has indicated that it was not aware of other client-specic policy changes, though they
acknowledge they would not have had full knowledge of whether such changes may have occurred.



                                                        9
that outside job options will impact turnover. In practice, we use education-specic state-

level unemployment rates measured at an annual frequency, obtained from the American

Community Survey.
                         14   Finally, Column 4 adds location-specic time trends to account for

the possibility that the timing of the introduction of testing is related to trends at the

location level, for example, that testing was introduced rst to locations that were on an

upward (or downward) trajectory.

    With full controls, we nd that testing improves completed job tenures by 0.23 log points,

or just over 25%. These results are broadly consistent with previous estimates from Autor

and Scarborough (2008).
                               15   These estimates reect the treatment on the treated eect for

the sample of rms that select into receiving the sort of test we study. Given that rms often

select into receiving technologies based on their expected returns (e.g., Griliches (1957)), it

is quite possible that other rms (e.g., those that are less open to new technologies) might

experience less of a return.

    In addition to log duration, we also examine whether testing impacts the probability

that hires reach particular tenure milestones: staying at least three, six, or twelve months.

For these samples, we restrict to workers hired three, six, or twelve months, respectively,

before the data end date. We estimate OLS models because censoring for these variables is

based only on start date and not survival time. The top panel of Online Appendix Table C1

reports results using these milestone measures. We nd a positive impact of testing for all

these variables, with the most pronounced eects at longer durations. For example, using

our full set of controls, we nd that testing increases the probability of workers surviving at

least 6 months by 6 percentage points (13%) and one year by 7.5 percentage points (23%).

  14 For the 25% of locations that are international, we use aggregated (i.e., non-education-specic), annual,
national unemployment rates obtained from the World Bank. For a small set of location identiers in our data
where state cannot be easily assigned (e.g., because workers typically work o-site in dierent US states),
we use national education-specic unemployment rates from the Current Population Survey.          We include
one set of variables for education-specic unemployment rates (either national or state) and one variable for
international unemployment rates. Values are replaced by zeros when missing because of location type and
location xed eects indicate type. Our results are robust to restricting to the 70% of locations with US
state-level data.
  15 Although our estimates are larger, we nd signicant eects on the order of 14% when estimating the
impact of testing on the length of    completed   job spells, which is similar to what Autor and Scarborough
(2008) nd using that same outcome. Further, the Autor and Scarborough 12% estimate is inside the range
of our 95% condence interval with full controls. We also estimated Cox proportional hazard models, and
obtained coecients a bit smaller in magnitude than those from censored normal models, but that were
qualitatively similar.




                                                       10
    Figure II plots the accompanying event studies. The treatment eect of testing appears

to grow over time, suggesting that HR managers and other participants might take some

time to learn how to use the test eectively.
                                                            16

    Our results in this section indicate that job testing increases job durations relative to the

sample rms' initial hiring practices. In the remainder of the paper, we focus on analyzing

the consequences of overruling job test recommendations.




IV Model
In this section, we formalize a model in which a rm makes hiring decisions with the help of an

HR manager and a job test. As in our sample rms, managers in this model observe job test

recommendations, but are not required to hire strictly by the test. The main purpose of this

model is to highlight the tradeos involved in granting discretion to managers: discretion

enables HR managers to take advantage of their private information but also gives them

scope to make hires based on biases or incorrect beliefs that are not in the interest of the

rm.    The model also provides intuition for how we might empirically assess the roles of

information and bias/mistakes in hiring. All proofs are in Section B of the Online Appendix.




IV.A Setup
A mass one of applicants apply for job openings within a rm. The rm's payo of hiring

worker   i   is given by   ai .   We assume that       ai   is drawn from a distribution which depends on a

worker's type,    ti â {G, Y };        a share of workers        pG   are type   G,   a share   1 â pG   are type   Y,   and

a|t â¼ N (Âµt , Ïa2 )   with   ÂµG > ÂµY       and   Ïa2 â (0, â).        This worker-quality distribution enables us

to naturally incorporate the discrete test score into the hiring environment. We do so by

assuming that the test publicly reveals              t.17
  16 Figure II includes all controls except location time trends so that any pre-trends will be apparent in
the gure. Estimates are especially large and noisy 10 quarters after testing, reecting only a few locations
that can be observed to that point. Online Appendix Figure A3 in the Online Appendix replicates Figure
II while restricting to a balanced panel of locations that hire in each of the four quarters before and after
testing. Impacts there are smaller, but are qualitatively similar.
  17 The values of    G and Y     in the model correspond to test scores green and yellow, respectively, in our data.
We assume binary outcomes for simplicity, even though in our data the signal can take three possible values.
This is without loss of generality for the mechanics of the model.




                                                             11
    The rm's objective is to hire a proportion,                     W,   of workers that maximizes expected

quality,   E[a|Hire].18       For simplicity, we also assume           W < pG .19
    To hire workers, the rm must employ HR managers whose interests are imperfectly

aligned with those of the rm. A manager's payo for hiring worker                                i   is given by:




                                                   Ui = (1 â k)ai + kbi .


In addition to valuing the rm's payo, managers also receive an idiosyncratic payo                                          bi ,
which they value with a weight              k â [0, 1].      We assume that     b   is independent of            (a, t).
    Managers may value the rm's payo,                     a, because they are directly incentivized to, because
they risk termination or desire promotion, or because they are simply altruistic.
                                                                                                                       20   The

additional quality,      b,   can be thought of in two ways.                   First, it may capture managerial

preferences for certain workers (e.g.                 for certain demographic groups or those with shared

interests). Second,      b   can represent manager mistakes such as overcondence that lead them

to prefer the wrong candidates.
                                            21

    The parameter        k    measures the manager's bias, i.e., the degree to which the manager's

incentives are misaligned with those of the rm or the degree to which the manager is

mistaken. An unbiased manager has                     k = 0,    while a manager who makes decisions entirely

based on bias or the wrong characteristics corresponds to                      k = 1.
    The manager privately observes information about                      ai   and   bi .   For simplicity, we assume

that the manager directly observes                  bi ,   which is distributed in the population by                  N (0, Ïb2 )
  18 In theory, rms should hire all workers whose expected value is greater than their cost. However, one
explanation for the hire share rule is that a threshold rule is not contractible because                   ai   is unobservable.
Nonetheless, a rm with rational expectations will know the typical share               W       of applicants that are worth
hiring, and   W   itself is contractible.        Assuming a xed hiring share is also consistent with the previous
literature, for example, Autor and Scarborough (2008).
  19 This implies that a manager could always ll a hired cohort with type                  G   applicants. In our sample of
applicant pools (see Table I), the average share of applicants who are green is 48%, the average share of
green or yellow applicants who are green is 59%, and the average hiring rate is 19%. Thus,                         W < pG    will
hold for the typical pool.
  20 We do not have systematic data on manager incentives. However, a manager at the data rm told us
that HR managers often face some targets and/or incentives. See Online Appendix A.8 for more detail.
  21 For example, a manager may genuinely have the same preferences as the rm but draw incorrect in-
ferences from his or her interview.         Indeed, work in psychology (e.g., Dana, Dawes, and Peterson, 2013)
shows that interviewers are often overcondent about their ability to read candidates. Such mistakes t our
assumed form for manager utility because we can always separate the posterior belief over worker ability
into a component related to true ability, and an orthogonal component resulting from their error.




                                                               12
with   Ïb2 â R+ .   Second, we assume the manager observes a noisy signal of                                  ai :


                                                      si = ai + i


where   i â¼ N (0, Ï2 )   is independent of       ai , ti ,   and   bi ,    and attributes of applicants are inde-

pendent across workers,          i.   The parameter     Ï2 â R+       measures the manager's information. A

manager with perfect information on              ai   has   Ï2 = 0;        as   Ï2   approaches         â,   a manager tends

towards having no private information. This private information of managers can be thought

of as their assessments of interviews or the worker's overall resume, etc. Unlike the job test,

these subjective signals cannot be veriably communicated to the rm.

    Let   M    denote the set of managers in a rm.                       For a given manager,                  m â M,   his or

                                      2
her type is dened by the pair (k, 1/Ï ), corresponding to the bias and precision of private

information, respectively. These have implied subscripts,                              m,   which we suppress for ease of

notation. We assume rms do not observe manager type,                              si ,     or   bi .22
    Managers form a posterior expectation of worker quality and hire a worker if and only if

E(Ui |si , bi , ti ) exceeds some threshold.      Managers thus wield discretion because they choose

how to weigh various signals about an applicant when making hiring decisions. We denote

the quality of hires for a given manager under this policy as                          E[a|Hire] (where an m subscript
is implied).




IV.B Model Discussion
This model illustrates the fundamental trade-o inherent in allowing managers discretion

over hiring decisions:          a manager's private information may be valuable to the rm, but

worker quality is hurt by his or her bias.

    In practice, rms cannot directly observe a manager's bias or information. However, rms

generally do observe the hiring choices of managers and the quality of workers that are hired.

We say that when a manager chooses to overrule the test by hiring a                                       Y    worker over a   G
one, the manager is making an exception. The frequency of such exceptions is increasing

  22 We assume that managers observe the same distribution of other qualities, b, but value them dierently,
based on the parameter,    k.   In contrast, the distribution of     si     will vary across managers, based on the value
of their private information.




                                                            13
in both a manager's bias and the precision of their private information (Proposition 1 in

Online Appendix B).        That is, managers are more likely to make exceptions both when

their preferences dier from those of the rm and when they have information which is not

captured by the test.

   To assess whether bias plays a role in driving exceptions, we examine the relationship

between a manager's propensity to make exceptions and the realized quality of his or her

hires. In the absence of bias, managers only make exceptions when they are better informed,

resulting in better hires.     By contrast, bias reduces the quality of hires (Proposition 2 in

Online Appendix B). Finding a negative relationship between exceptions and the quality of

hires would therefore mean that managers make exceptions not only when they are better

informed but also because they are biased or mistaken.

   The presence of bias raises the possibility that rms may be able to improve outcomes

by placing some limits on managerial discretion. To illustrate an example, Proposition 3 (in

Online Appendix B) summarizes theoretical conditions under which a rm would prefer no

discretion to full discretion. In general, greater bias pushes the rm to prefer no discretion,

while better information pushes it towards allowing discretion. These extreme cases need

not be optimal and rms may also wish to consider intermediate policies such as limiting

the number of exceptions that managers can make.
                                                            23




V Empirical Analysis on Discretion
In our data, we observe job applicants, hires, and the outcomes of hired workers. This al-

lows us to assess how eectively managers exercise discretion, by examining whether worker

outcomes are better when managers follow test recommendations more closely, or when they

choose to hire more workers as exceptions. If we nd that managers who make many excep-

tions tend to do worse than managers who make few exceptions (holding all else constant),

then this suggests that managers are biased. Firms in our setting may then want to consider

limiting discretion, at least for the managers that make such frequent exceptions.

  23 See Frankel (2017) for follow up theoretical work providing a discussion of optimal hiring in a related
model. In particular, the author shows that there are conditions under which the optimal rm response is
to cap the number of exceptions that managers are permitted.




                                                    14
   To examine the consequences of overruling test recommendations, we must rst dene

an exception rate that corresponds to a manager's choices, rather than his or her circum-

stances. For example, two managers with the same information and bias may nonetheless

make dierent numbers of exceptions if they face dierent applicant pools or need to hire

dierent numbers of workers. Our metric should also adjust for applicant pool character-

istics that make exceptions mechanically more likely, for example, in pools with few green

applicants relative to slots.

   Second, we must compare outcomes for managers who have dierent exception rates

despite facing similar applicant pools in similar labor market conditions. These potentially

unobservable factors may also separately impact worker tenure, making it dicult to learn

about the relationship between exceptions and worker outcomes. For example, we need to

address the concern that because we do not observe job oers, applicant pools in which more

green workers turn down oers may wrongly appear to have a higher exception rate.

   We discuss how we address these issues in the next two subsections. We rst dene an

exception rate that takes into account observable dierences in applicant pools. Second, we

discuss a range of empirical specications that help deal with unobserved dierences across

applicant pools (i.e., dierences within color or across locations).




V.A Dening Exceptions
To construct an empirical analogue to the exception rate, we use data on the test scores

of applicants and hires in the post-testing period. First, we dene an applicant pool as a

group of applicants being considered by the same manager for jobs at the same location in

the same month.
                   24

   We then measure how often managers overrule the recommendation of the test by either

1) hiring a yellow when a green had applied and is not hired, or 2) hiring a red when a yellow

or green had applied and is not hired. We dene the exception rate, for a manager                    m   at a

  24 An applicant is under consideration if he or she applied in the last 4 months and had not yet been hired.
Over 90% of workers are hired within 4 months of the date they rst submitted an application.




                                                     15
location   l   in a month    t,   as follows:


                                                      Nyh â Ngnh + Nrh â (Ngnh + Nynh )
(2)                       Exception Ratemlt       =                                        ,
                                                       Maximum # of Exceptions


          h              nh
where    Ncolor   and   Ncolor    are the number of hired and not hire applicants, respectively. These

variables are dened at the pool level (m, l, t) though subscripts have been suppressed for

notational ease.

      The numerator of Exception Ratemlt counts the number of exceptions (or order viola-

tions) a manager makes when hiring, i.e., the number of times a yellow is hired for each

green that goes unhired plus the number of times a red is hired for each yellow or green that

goes unhired. This denition assigns a higher exception rate to a manager when he or she

hires a yellow applicant from a pool of 100 green applicants and 1 yellow applicant, than

from a pool of 1 green applicant and 100 yellow applicants.

      However, the total number of order violations in a pool depends on both the manager's

choices and on factors related to the applicant pool, such as size and color composition.

For example, if a pool has only green applicants, it is impossible to make any exceptions.

Similarly, if the manager needs to hire all available applicants, then there can also be no

exceptions.       These variations were implicitly held constant in our model, but need to be

accounted for in the empirics.            To control for pool characteristics that may mechanically

impact the number of exceptions, we normalize the number of order violations by the maxi-

mum number of violations that could occur, given the applicant pool that the recruiter faces

and the number of hires required.
                                            25   This results in an exception rate that ranges from 0 if

the manager never made any exceptions, to 1, if the manager made all possible exceptions.

Importantly, although the propositions in Section IV are derived for the probability of an

exception, their proofs hold equally for this denition of an exception rate. In Online Ap-

  25 That is, we count the number of order violations that would occur if the manager rst hired all available
reds, then, if there are still positions to ll, all available yellows. Specically,

                                      ï£±
                                         h    A     A        h      A
                                      ï£²N (Ng + Ny ) if N â¤ Nr
                                      ï£´
        Maximum # of     Exceptions =  N h NgA + NrA (NyA â (N h â NrA )) if NrA < N h â¤ NyA + NrA
                                      ï£´
                                      ï£³ A
                                       (Nr + NyA )(NgA â (N h â NrA â NyA )) if NyA + NrA < N h

         h
where   Ncolor   is the number of applicants of a given color and   Nh   is the total number of hires.




                                                          16
pendix A.6 we show that our results are also robust to alternative denitions of exception

rates.

   As described in Table I, we observe nearly 3,700 applicant pools consisting of, on average,

260 applicants.
                  26   On average, 19% of workers in a given pool are hired and this proportion

is increasing in the score of the applicant. Despite this, exceptions are common: the average

exception rate across applicant pools is 22%.

   There is substantial variation in exception rates across both applicant pools and man-

agers. Figure III shows histograms of the exception rate at the application pool level in the

top panel. The left graph shows the unweighted distribution, while the right graph shows

the distribution weighted by the number of hires. In either case, the median exception rate

is about 20% of the maximal number of possible exceptions, and the standard deviation is

about 15 percentage points.

   We then aggregate exception rates to the manager level by averaging over all pools a

manager hired in, weighting by the number of hires in the pool. The middle panels of Figure

III show histograms of manager-level exception rates: these have the same same mean and a

slightly smaller standard deviation of 10 percentage points. This means that managers very

frequently make exceptions, and some managers consistently make more exceptions than

others. The bottom panels of Figure III aggregate exception rates to the location level and

show that there is also systematic variation in exception rates across locations.

   When we examine the relationship between manager-level exception rates and worker

outcomes, we require that variation in exception rates reect dierences in manager choices,

driven by their information and biases.         However, it is possible that other factors such

as unobserved dierences in applicant quality also inuence exceptions rates. In the next

section, we describe how our empirical analysis seeks to control for such confounders.




V.B Empirical Specications
Post-testing Correlation Between Exception Rates and Outcomes.           We rst examine the

relationship between the manager-level exception rate and the realized durations of hires in

  26 This excludes months in which no hires were made.




                                                  17
the post-testing period:




(3)       Log(Duration)imlt      = a0 + a1 Exception      Ratem   + Î´l + Î³t + Positioni Î² + imlt


Our variation comes from dierences in manager-level exception rates for managers employed

at the same location. In our data, 99.1% of workers are hired at locations with more than one

manager. The average location in our sample has nearly 7 managers and the average worker

is at a location with 11 managers. The coecient of interest is              a1 .   A negative coecient,

a1 < 0,    indicates that the quality of hires is decreasing in the manager's exception rate.

Such a nding suggests that managers may be making exceptions because they are biased or

mistaken, and not solely because they have useful private information. We cluster standard

errors at the location level, again to take into account any correlation in observations within

a location over time.
                         27

      We face two key concerns in interpreting         a1 .   First, exception rates may be driven by

omitted variables that separately impact worker durations.                For example, some locations

may be inherently less desirable places that both attract more managers with biases or

bad judgement and retain fewer workers. This would drive a negative correlation between

exception rates and outcomes that is unrelated to discretion.

      Second, as discussed in the introduction, we observe only hires and not oers. This means

that we cannot tell the dierence between a yellow that is hired even when a green applicant

is available or a yellow that is hired after all green applicants have turned down the oer.

One concern is that we may observe more false exceptions when green workers have better

outside options. In such cases, we may also see lower durations simply because the manager

was forced to hire second choice workers.

      In both cases, accounting for controls may alleviate some concerns. For example, location

xed eects control for xed dierences across locations in unobserved applicant quality;

location-specic time trends further control for smooth changes in these characteristics.

Controlling for local labor market conditions reduces the likelihood that our results are

driven by false exceptions, because such exceptions may be more common when green

  27 If we instead cluster by manager, the level of variation underlying our key right hand side variable, we
get slightly smaller standard errors.




                                                     18
workers have better outside options.         Our full set of controls includes location, time and

position type xed eects, client-year xed eects, local labor market variables, location-

specic time trends, and detailed controls for the quality and number of applicants in an

application pool (xed eects for each decile of: the number of applicants, hire rate, share

of applicants that are green, and share that are yellow).
                                                                 28

      In addition to these controls, examining manager-level exception rates has the benet

of smoothing idiosyncratic variation across individual pools that may drive both exception

rates and outcomes. For example, in some pools, green applicants may be atypically weak. In

this case, managers may optimally hire yellow applicants, but their hires would still have low

average durations relative to workers the location is usually able to attract. Similarly, some

pools may have more false exceptions because of an idiosyncratically low yield rate for green

applicants. Averaging to the manager level (the average manager hires in 18 applicant pools)

reduces the extent to which our measure of exceptions is driven by such sources of variation.

Given our controls, this same concern would only apply if some managers systematically face

idiosyncratically weaker pools or lower yield rates than other managers at the same location

facing observably similar pools.


      Dierential Impact of Testing, by Exception Rates. We also consider how the impact of

testing diers across exception rates. If managers exercise discretion because they are biased

or misinformed, then we may expect the benets of testing that we document in Section

III.B to be lower for high exception managers. We estimate this using a similar specication

to that described in Section III.A:




(4)         Log(Duration)imlt      = b0 + b1 Testinglt Ã Exception       Ratel   + b2 Testinglt

                                       +Î´l + Î³t + Positioni Î² + imlt


      Equation (4) includes the main eect of testing but allows testing to interact with the

location-specic exception rate.       We consider location-level variation in this case because

we do not observe manager identiers in the pre-period. The coecient of interest,                b1 ,   thus

estimates how the impact of testing diers at locations that subsequently make more or fewer

  28 We have also explored controls for the number of hires made in the several preceding months to take
into account that applicant pools may be depleted over time. Results are very similar with these controls.



                                                    19
exceptions.   b1 < 0   indicates that exceptions attenuate gains from testing. Unlike Equation

(3), which can only be estimated on post-testing data, this specication uses our full dataset,

making it possible for us to more precisely identify location xed eects and other controls.




V.C Results
Figure IV examines the correlation between exception rates and durations for hired workers

after the introduction of testing. We divide managers into 20 equally sized bins based on their

hire-weighted exception rate (x-axis) and regress duration measures on an exhaustive set of

indicators for each bin, plus base controls. We plot the coecients on these exception rate

bin indicators (y -axis) against the average exception rate in each bin (x-axis). The top left

panel summarizes the log duration regression, which adjusts for censoring with a censored-

normal regression. The remaining panels plot the milestone measures for the probability that

a worker stays at least 3, 6, or 12 months. For all outcomes, we see a negative relationship:

job durations are shorter for workers hired by managers with higher exception rates.

   Table III presents the accompanying regression analysis. In these regressions, we stan-

dardize exception rates to be mean 0 and standard deviation 1 so that the units are easier

to interpret.   Column 1 contains our base specication and indicates that a one standard

deviation increase in the exception rate is associated with a 7% reduction in job durations,

signicant at the 5% level. Adding controls reduces the size of the standard error and the co-

ecient slightly. In our full-controls specication, a one standard deviation higher exception

rate is associated with 6% shorter durations, still signicant at the 5% level. This says that

even when we analyze managers at the same location hiring the same number of workers out

of pools that have the same share of red, yellow, and green applicants, we continue to nd

that managers who makes more exceptions do worse. The middle panel of Online Appendix

Table C1 summarizes regressions for the milestone measures, where we also nd signicant

negative relationships.

   Next, Table IV examines how the impact of testing varies by the extent to which locations

make exceptions.       Estimates are based on Equation (4).   Including the full set of controls

(Column 4), we nd that at the mean exception rate (recall that we standardize exception

rates), testing increases durations by 0.23 log points, but that this eect is substantially oset



                                               20
(by 0.14 log points) for each standard deviation increase in the exception rate, signicant at

the 5% level.
                29   The bottom panel of Online Appendix Table C1 shows that these results are

robust to OLS estimates using milestone measures as dependent variables.

   Figure V illustrates how the impact of testing varies for locations with dierent average

exception rates, using base controls.
                                         30   For all tenure outcomes (log(duration) and milestones)

we nd a negative relationship that does not appear to be driven by any particular exception-

rate bin.

   Across a variety of specications, we consistently nd that worker tenure is lower for

managers who made more exceptions to test recommendations.                        The magnitude of this

estimate implies that a rm made up of managers at the 10th percentile of the exception

distribution would have approximately 15% longer worker durations in the post testing

period, relative to a rm made up of 90th percentile managers (higher exception rates are

worse).     Further, locations at the 90th percentile of the exception distribution experience

duration improvements with the adoption of testing that are multiple times larger than

improvements at 10th percentile locations.

   Viewed in light of our theoretical predictions, these results suggest that managers often

make exceptions because they are either biased or misinformed.                    Even if high exception

managers were well informed about worker quality, the fact that their hiring outcomes are

worse suggests that their biases lead them to make choices that do not maximize quality.




V.D Additional Robustness Checks
In this section we address several alternative explanations for our ndings.


   Quality of Passed Over Workers. There are some scenarios under which we may nd

a negative correlation between worker outcomes and exception rates, even when managerial

discretion improves hiring. For example, as discussed earlier, a manager may tend to make

more exceptions because he or she sees idiosyncratically weak green applicants, relative to the

  29 Here, we do not include controls for applicant pool quality because it is unavailable pre-testing.
  30 To construct this, we divide locations into 20 hire-weighted bins based on their average location-level
exception rate post testing and augment Equation (4) with indicators for the interaction of exception rate
bins and the post-testing dummy. We then plot the bin-specic impact of testing coecient on the       y -axis
and the average exception rate in each bin on the   x-axis.   For the Log(duration) outcome (top left panel),
we adjust for censoring with censored-normal regressions.



                                                     21
typical applicants at the location. As another example, locations with high exception rates

may benet less from the test because its managers always had better private information.

    In these and other similar scenarios, it should still be the case that individual exceptions

are correct: a yellow hired as an exception should perform better than a green who is not

hired. To examine this, we would ideally compare the counterfactual duration of applicants

who are not hired with the actual durations of those who were. While this is not generally

possible, we can, in some cases, approximate such a comparison by exploiting the timing

of hires. Specically, we compare the tenure of yellow workers hired as exceptions to green

workers from the same applicant pool who are not hired that month, but who subsequently

begin working in a later month. If managers make exceptions when they have better infor-

mation, then exception yellows should have longer tenures than passed over greens.

    Table V shows that is not the case. The rst panel compares durations of workers who

are exception yellows (the omitted group) to greens whose application was active in the same

month, but were hired only in a later month. Because these workers are hired at dierent

times, all regressions control for hire month xed eects to account for mechanical dierences

in duration.    In Column 2, which includes applicant pool xed eects, the coecient on

passed over green compares this group to yellow applicants from the same applicant pool

who were hired before them.
                                 31   The second panel of Table V repeats this exercise, comparing

exception reds (the omitted group), to passed over yellows and greens.
                                                                                     32

    Both panels show that workers hired as exceptions have shorter tenures.                     Passed over

greens stay 4% longer than yellows hired before them from the same pool (Column 2, top

panel), though this estimate is noisy. We estimate a more precise relationship for exception-

red workers: passed over greens and yellows stay roughly 14% and 12% longer, respectively.

These results suggest it is unlikely that exceptions are driven by better information: high

scoring workers who are initially passed over outperform low scoring workers chosen rst.
                                                                                                         33

  31 The applicant pool xed eect is at the location-manager-date level, where the date is the month in
which both applications were active, the yellow was hired, and the green was hired only later. These xed
eects thus subsume a number of controls from our full specication from Table III.
  32 We restrict observations in both panels to pools in which there was both an exception and a passed over
applicant (92% and 59% of hires in the top and bottom panels, respectively). To identify control variables,
we further restrict to locations and pools that have at least 10 and 5 observations, respectively.
  33 An alternative explanation is that the applicants with higher test scores were not initially passed up,
but were instead initially unavailable, for example because they were engaged in on-the-job search. However,
Online Appendix Table C2 shows that delays are not correlated with worker quality.



                                                     22
   False Exceptions.   As mentioned, one may be concerned that we do not observe job

oers and thus cannot distinguish between cases in which yellow applicants are hired as

true exceptions, or when they are hired because green applicants turned down oers.         By

analyzing manager or location-level exception rates, we aggregate over some of the idiosyn-

cratic variation that may generate false exceptions, and our controls for local labor market

conditions may help absorb some of the time-varying drivers of such exceptions.

   As an additional test, Online Appendix Table C3 shows that our results hold when

restricting to pools with at least as many green applicants as the total number of hires (84%

of hires came from such a pool). In such pools, it is less likely that a yellow or red worker

was hired because all green applicants received an oer and turned it down.


   Heterogeneity Across Locations. Another possible concern is that the relevance of the test

varies across locations and that this drives the negative correlation between exception rates

and worker outcomes. For example, in very undesirable locations, green applicants might

have better outside options and be more dicult to retain. In these locations, a manager

attempting to avoid costly retraining may optimally decide to make exceptions in order to

hire workers with lower outside options.

   In Online Appendix A.5 we provide evidence that the apparent usefulness of the test

does not systematically vary by location characteristics. There we explore the relationship

between color score and job duration as a function of a wide range of location characteristics,

such as exception rates and average durations. We robustly nd that color score is predictive

of worker quality, regardless of the location's characteristics on each of these dimensions.


   Productivity. Our results show that high-exception managers hire workers with lower job

duration.   These exceptions may still benet the rm if such workers are better on other

dimensions. For example, managers may optimally hire workers who are more likely to turn

over if their private signals indicate that those workers might be more productive while they

are employed.

   Our nal set of results provides evidence that this is unlikely to be the case. For a subset

of client rms, we observe a direct measure of worker productivity: output per hour. Recall

that in our setting, individuals perform essentially one major task and engage in individual

production. Some examples may include: the number of data items entered per hour, the


                                              23
number of standardized tests graded per hour, and the number of phone calls completed per

hour. As in these examples, output per hour is an important measure of productivity for

the fairly homogenous task we study.

    We dene a worker-level output per hour metric as a worker's output per hour averaged

over all the days where such a metric is observed for that worker. Across all workers with an

available measure, output per hour has an average of 8.4 with a standard deviation of 4.7.

There is thus a wide range of performance outcomes.
                                                                 34   From Table I, average output per

hour is slightly higher in the post-testing sample period and varies slightly by color score,

though the dierences are not signicant.

    This measure is available for 62,427 workers (one-quarter of all hires) in 6 client rms.

The primary reason for missing output is that the metric is not made available to us for

many locations, time periods, and end clients (i.e., the ones purchasing services from the

client rms).
                35   In addition, its availability depends on workers completing their training and

being permitted to perform the job task: this is the period in which workers become valuable

to client rms."

    Relative to our main sample, the set of workers with output data is positively selected

on duration. Despite this, output remains positively correlated with job duration. Online

Appendix Figure C1 presents a binned scatter of output per hour for 20-evenly sized bins of

Log(Duration).
                     36   Except for one outlier for workers with very low tenure, there is a strong

positive relationship: workers with longer job durations have higher output per hour.

    Table VI summarizes our main analyses using output per hour as the dependent variable.

Columns 1-2 document the post-testing correlation between manager-level exceptions and

output per hour. For both our base and full sets of controls, we obtain negative coecients

that are not signicant.         For example in Column 2, the estimate is -0.11 with a standard

error of 0.095. Recall the manager-level exception rates are standardized to be mean 0 and

standard deviation 1 in the full post-testing sample. The estimate therefore implies that a

  34 We also control for the number of tasks in a day that are used to measure a worker's output per hour.
We aggregate this to the worker level by averaging indicators for count decile across all observations for a
worker.
  35 We can account for half of the variation in whether an output measure is available for an individual
worker with location, time, and position controls. According to the data rm, certain lines of business within
a rm do not make their productivity data available.
  36 We control for location xed eects to account for dierences in average output per hour across locations.



                                                     24
one standard deviation higher exception rate manager hires workers who perform 0.11 fewer

units of output per hour, on average. This is 2.3% of the standard deviation for output per

hour (4.7, mentioned above). Based on the standard error, we can rule out positive eects

beyond 1.7% and negative eects beyond -6.4% of a standard deviation with 95% condence.

   Columns 3-4 examine the dierential impact of testing by location-level exception rate.

The coecient on testing gives the impact of testing for locations with the mean exception

rate (based on the full sample). In the baseline specication, testing improves output per

hour by 0.34 or 7% of a standard deviation. This eect is small in magnitude and is not

statistically signicant.   We also nd modestly sized and insignicant coecients for the

interaction term. Coecients are similar in magnitude but opposite in sign across base and

full controls.   With full controls, the point estimate of -0.137 implies that the impact of

testing in a location with a one standard deviation higher exception rate is oset by 0.137

output per hour units, or about 2.9% of a standard deviation.      We can rule out positive

eects outside of 6.3% and negative eects outside of -12% with 95% condence.

   Although noisy, these ndings taken together suggest that output per hour does not

appear to be strongly related to exception rates. We do not nd evidence of a large negative

association between exceptions and output per hour, as we did with job durations. However,

in all cases we nd no evidence that managerial exceptions improve output per hour by any

sizeable amount. This is inconsistent with a model in which managers optimally sacrice

job tenure in favor of workers who perform better on other quality dimensions.




VI Conclusion
We evaluate the introduction of a hiring test for a low-skilled service sector job. Exploiting

variation in the timing of adoption across locations within rms, we show that testing signif-

icantly increases the durations of hired workers. We then document substantial variation in

how managers appear to use job test recommendations: some tend to hire applicants with

the best test scores while others appear to make many more exceptions. Across a range of

specications, we show that hiring against test recommendations is associated with worse

outcomes.




                                              25
    Firms in our setting may nd this result useful as they decide how much discretion to

grant their managers, and how much to rely on job tests or other signals of worker quality.

For example, in cases where high-exception managers are associated with worse outcomes,

rms may wish to decrease the rate of exceptions either by limiting managerial discretion

(particularly for high exception managers) or by nding new managers who are less biased.

More broadly, rms may be able to improve outcomes by adopting policies to inuence

manager behavior such as increasing feedback about the quality of hires or tying pay more

closely to performance. Such policies may encourage managers to nd ways to complement

the test as they continue to learn.

    There are several caveats one must keep in mind in interpreting our results.                      First,

while our results suggest that high-exception managers make decisions with bias, limiting

managerial discretion could have unintended consequences (such as demoralizing managers),

and could be bad for some managers who do have valuable private information.                       Second,

we emphasize that our ndings may not apply to all rms.                    We focus on workers who

perform low-skilled service sector tasks without a teamwork component. A manager's private

signals of worker quality may be more valuable in higher skilled settings with more complex

tasks.
         37   Further, managers may have more opportunities to correct their mistaken beliefs in

settings where they regularly interact with applicants on the job. The HR managers we study

generally do not supervise applicants after they are hired, which also limits the scope for a

manager-employee match component that might make discretion more useful. An additional

contribution of our paper is that we present a way to assess the consequences of discretion

using only data that would readily be available for many rms using workforce analytics.

    More broadly, our ndings highlight the role that technology can play in reducing the

impact of managerial mistakes or biases by changing how decision-making is structured

within the rm. As workforce analytics becomes an increasingly important part of human

resource management, more work needs to be done to understand how such technologies

  37 In fact, Frederiksen, Kahn, and Lange (2017) show that managerial discretion over performance man-
agement can be valuable in the context of a high-skilled service profession. Li and Agha (2015) show that the
judgement of human reviewers provides valuable information about the quality of scientic proposals that is
not available from CVs and other quantitative metrics. Homan and Tadelis (2017) show that subordinates
provide subjective assessments of managers that correlate with hard outcomes in another high skilled setting.




                                                     26
interact with organizational structure and the allocation of decisions rights within the rm.

This paper makes an important step towards understanding and quantifying these issues.




UNIVERSITY OF TORONTO & NBER

YALE UNIVERSITY & NBER

MIT & NBER




                                             27
References
[1] Aghion, Philippe and Jean Tirole, Formal and Real Authority in Organizations, The

   Journal of Political Economy, 105 (1997), 1-29.


[2] Altonji, Joseph and Charles Pierret, Employer Learning and Statistical Discrimina-

   tion, Quarterly Journal of Economics, 113 (2001), 79-119.



[3] Alonso, Ricardo and Niko Matouschek, Optimal Delegation, Review of Economic Stud-

   ies, 75 (2008), 259-3.


[4] Autor, David, Why Do Temporary Help Firms Provide Free General Skills Training?,

   Quarterly Journal of Economics, 116 (2001), 1409-1448.


[5] Autor, David and David Scarborough, Does Job Testing Harm Minority Workers?

   Evidence from Retail Establishments, Quarterly Journal of Economics, 123 (2008),

   219-277.



[6] Baker, George and Thomas Hubbard, Contractibility and Asset Ownership: On-Board

   Computers and Governance in U.S. Trucking, Quarterly Journal of Economics, 119

   (2004), 1443-1479.



[7] Bolton, Patrick and Mathias Dewatripont Authority in Organizations. in Robert Gib-

   bons and John Roberts (eds.), The Handbook of Organizational Economics, (Princeton,

   NJ: Princeton University Press, 2010).



[8] Brown, Meta, Elizabeth Setren, and Giorgio Topa,Do Informal Referrals Lead to Bet-

   ter Matches?   Evidence from a Firm's Employee Referral System, Journal of Labor

   Economics, 34 (2016), 161-209.


[9] Burks, Stephen, Bo Cowgill, Mitchell Homan, and Michael Housman, The Value of

   Hiring through Employee Referrals, Quarterly Journal of Economics, 130 (2015), 805-

   839.




                                            28
[10] Dana, Jason, Robyn Dawes, and Nathaniel Peterson, Belief in the Unstructured In-

    terview:   The Persistence of an Illusion, Judgment and Decision Making, 8 (2013),

    512-520.


[11] Dessein, Wouter, Authority and Communication in Organizations, Review of Eco-

    nomic Studies, 69 (2002), 811-838.


[12] Diamond, Rebecca and Petra Persson, The Long-Term Consequences of Teacher Dis-

    cretion in Grading of High-Stakes Tests, mimeo Stanford University, 2016.


[13] Farber, Henry and Robert Gibbons, Learning and Wage Dynamics, Quarterly Journal

    of Economics, 111 (1996), 1007-1047.


[14] Frankel, Alexander, Selecting Applicants, mimeo University of Chicago, 2017.


[15] Frederiksen, Anders, Lisa B. Kahn, and Fabian Lange, Supervisors and Performance

    Management Systems, NBER Working Paper #23351, 2017.


[16] Griliches, Zvi, Hybrid Corn:   An Exploration in the Economics of Technological

    Change, Econometrica, 25 (1957), 501-522.


[17] Homan, Mitchell and Steven Tadelis, How Do Managers Matter?         Evidence from

    Performance Metrics and Employee Surveys in a Firm, working paper, University of

    Toronto, 2017.


[18] Horton, John, The eects of algorithmic labor market recommendations:      Evidence

    from a eld experiment, Journal of Labor Economics, 35 (2017), 345-385.


[19] Jovanovic, Boyan, "Job Matching and the Theory of Turnover," The Journal of Political

    Economy, 87 (1979), 972-90.


[20] Kahn, Lisa B. and Fabian Lange, Employer Learning, Productivity and the Earnings

    Distribution: Evidence from Performance Measures, Review of Economic Studies, 81

    (2014), 1575-1613.


[21] Kahneman, Daniel. Thinking Fast and Slow. (New York: Farrar, Strauss, and Giroux,

    2011).


                                            29
[22] Kleinberg, Jon, Himabindu Lakkaraju, Jure Leskovec, Jens Ludwig, and Sendhil Mul-

    lainathan, Human Decisions and Machine Predictions, Quarterly Jounral of Eco-

    nomics, 2018, forthcoming.


[23] Kuncel, Nathan, David Klieger, Brian Connelly, and Deniz Ones, Mechanical Versus

    Clinical Data Combination in Selection and Admissions Decisions: A Meta-Analysis,

    Journal of Applied Psychology. 98 (2013), 10601072.


[24] Kuziemko, Ilyana, How Should Inmates Be Released from Prison? an Assessment of

    Parole Versus Fixed Sentence Regimes, Quarterly Journal of Economics. 128 (2013),

    371-424.



[25] Lazear, Edward, Kathryn Shaw, and Christopher Stanton, The Value of Bosses, Jour-

    nal of Labor Economics, 33 (2015), 823-861.


[26] Li, Danielle, Expertise and Bias in Evaluation:   Evidence from the NIH American

    Economic Journal: Applied Economics, 9 (2017), 60-92.


[27] Li, Danielle and Leila Agha, Big Names or Big Ideas: Do Peer Review Panels Select

    the Best Science Proposals? Science, 348 (2016), 434-438.



[28] Pallais, Amanda and Emily Sands, Why the Referential Treatment?      Evidence from

    Field Experiments on Referrals, Journal of Political Economy, 124 (2016), 1793-1828.



[29] Paravisini, Daniel and Antoinette Schoar, The Incentive Eect of IT: Randomized

    Evidence from Credit Committees NBER Working Paper #19303, 2013.



[30] Riviera, Lauren, Hiring as Cultural Matching: The Case of Elite Professional Service

    Firms, American Sociological Review, 77 (2014), 999-1022



[31] Stanton, Christopher and Catherine Thomas, Landing The First Job: The Value of

    Intermediaries in Online Hiring, Review of Economic Studies, 83 (2015), 810-854.



[32] Wang, James, Why Hire Loan Ocers? Examining Delegated Expertise, mimeo Uni-

    versity of Michigan, 2014.




                                           30
                  Figure I: Distribution of Length of Completed Job Spells




           .008
           .006
        Density
         .004
           .002
           0




                    0            200            400            600                  800              1000
                                                 Days of Tenure
                   Red solid line = Mean, Black dashed line = Median

Notes:     Figure I plots the distribution of completed job spells at the individual level. For legibility, this

histogram (though not the computed mean or median) omits 3% of observations with durations over 1000

days.




                                                       31
                               Figure II: Event Study of Duration Outcomes




                               Log(Duration)                                                   Survived 3 Months




                                                                              .4
              2
              1.5




                                                                              .2
              1
              .5




                                                                              0
              0
              -.5




                                                                              -.2
Coefficient




                     -10      -5         0             5        10                    -10          -5          0        5        10


                           Survived 6 Months                                                  Survived 12 Months
              .6




                                                                              .6
              .4




                                                                              .4
              .2




                                                                              .2
              0




                                                                              0
              -.2




                                                                              -.2




                     -10      -5         0             5        10                    -10          -5          0        5        10

                                        Quarters from Introduction of Testing

          Notes: These gures plot the impact of testing on worker durations as a function of event-time (in quarters)
          relative to testing adoption, adjusting for base controls.          The underlying estimating equation is given by
                                time since testing
          Outcomeilt   = Î±0 + Ilt                    Î±1 + controls + ilt ,   where   I time since testing   is a vector of event-time

          dummies in quarters, with the omitted category, -1, indicated with the vertical red line. Controls include

          location, hire year-month, position, and client-by-year xed eects, as well as local labor market variables.

          The top left panel is estimated using censored normal regression while the others are estimated using OLS

          for the sample of workers hired at least 3, 6, or 12 months before the data end date (measured for each of

          the 15 rms by the latest termination date or latest hire date in our data, whichever is later). Dashed lines

          indicate the 95% condence interval. Online Appendix Figure A3 replicates this gure while restricting to a

          balanced panel of locations that hire in each of the four quarters before and after testing.




                                                                     32
         Figure III: Distributions of Application Pool Exception Rates



                     Pool Level, Unweighted                              Pool Level, Weighted by # Hires
           8




                                                              4
           6




                                                              3
           4




                                                              2
           2




                                                              1
           0




                                                              0
               0      .2        .4   .6        .8   1              0         .2        .4   .6        .8     1


                   Manager Level, Unweighted                           Manager Level, Weighted by # Hires




                                                              8
           4




                                                              6
           3




                                                              4
           2




                                                              2
           1
           0




                                                              0
               0      .2        .4   .6        .8   1              0         .2        .4   .6        .8     1


                    Location Level, Unweighted                         Location Level, Weighted by # Hires

                                                              15
           8
           6




                                                              10
           4




                                                              5
           2
           0




                                                              0




               0           .2             .4        .6             0              .2             .4          .6

          Red solid line = mean, black dashed line = median

Notes:   These gures plot the distribution of the exception rate, as dened by Equation (2) in Section V.

The top panel presents results at the applicant pool level (dened to be a managerlocationmonth). The

middle (bottom) panel aggregates these data to the manager (location) level.                 Figures on the left dene

the distribution giving applicant pools equal weight while gures on the right weight by number of hires.

Exception rates are only dened for the post-testing sample.




                                                         33
                   Figure IV: Manager-Level Exception Rates and Post-Testing Job
                                                                                Durations




                                                    Log(Duration)                                                          Survived 3 Months
                                   5.8




                                                                                            .56
                                   5.7




                                                                                            .54
     Average of Duration Measure
                                   5.6




                                                                                            .52
                                   5.5




                                                                                            .5
                                   5.4




                                                                                            .48
                                          .1        .2         .3         .4       .5                                .15    .2   .25   .3   .35   .4


                                               Survived 6 Months                                                           Survived 12 Months
                                   .38




                                                                                            .17 .18 .19 .2 .21 .22
                                   .36
                                   .34
                                   .32
                                   .3




                                         .15   .2        .25        .3    .35     .4                                 .15    .2   .25   .3   .35   .4

                                                                         Average Exception Rate

Notes: We plot average durations (post-testing) and exceptions rates within 20 equally sized bins, weighted
by number of hires, based on the average manager-level exception rate. The x-axis represents the average

exception rate within each bin. The y-axis is the mean duration outcome in the specied bin. We control for

location, hire month, and position xed eects. Means for the top left panel are estimated using censored

normal regression while the others are estimated using OLS for the sample of workers hired at least 3, 6, or

12 months before the data end date for each of the 15 rms.




                                                                                       34
 Figure V: Location-Level Exception Rates and the Impact of Testing on
                                                             Job Durations




                                            Log(Duration)                           Survived 3 Months
                           1




                                                                         .2
                           .5




                                                                         0
                           0




                                                                         -.2
     Impact of Testing
                           -.5




                                                                         -.4
                                 .1    .2        .3         .4     .5          .1    .2       .3       .4       .5


                                      Survived 6 Months                             Survived 12 Months
                           .3




                                                                         .4
                                                                         .3
                           .2




                                                                         .2
                           .1




                                                                         .1
                           0




                                                                         0
                           -.1




                                                                         -.1




                                 .1    .2        .3         .4     .5          .1    .2       .3       .4       .5

                                                        Average Exception Rate

Notes:                   We plot the impact of testing within 20 equally sized bins, based on the location-level exception

rate, on the average exception rate in each bin. Estimates include base controls: location, hire month, and

position xed eects. The top left graph is estimated with censored-normal regressions, while the others are

estimated using OLS for the sample of workers hired at least 3, 6, or 12 months before before the data end

date for each of the 15 rms.




                                                                   35
                                  Table I: Summary Statistics



                                                                  Sample Coverage

                                                                         All     Pre-testing Post-testing

  Sample Coverage

  # Locations                                                           127          113          97

  # Hired Workers                                                     265,648     174,329       91,319

  # Applicants                                                                                 403,006

  # HR Managers                                                                                   445

  # Pools                                                                                        3,698

  # Applicants/Pool                                                                               260
                                                         Worker Characteristics mean
                                                                              (st dev)
                                         Pre-testing Post-testing      Green        Yellow       Red

  Duration of Completed Spell (Days)        252           116           122          110           92
  (N=209,808)                              (323)         (138)         (143)        (130)        (121)


  Duration of Censored Spell (Days)         807           252           265          235          223
  (N=55,840)                               (510)         (245)         (252)        (232)        (223)


  Share Censored                            0.19          0.25          0.24         0.26        0.25
                                           (0.39)        (0.43)        (0.43)       (0.44)      (0.43)

  Output per Hour                           8.35          8.44         8.39          8.32        9.16
  (N=62,427)                               (4.66)        (5.16)       (5.01)        (5.11)      (6.08)

                                                         Applicant Pool Characteristics

                                                      Post-testing     Green        Yellow       Red

  Share Applicants                                                     0.48          0.32        0.20

  Hire Probability                                        0.19         0.23          0.18        0.08


Notes:    Post-testing is dened at the location-month level as the rst month in which 50% of hires had

test scores, and all months thereafter. An applicant pool is dened at the manager-location-month level and

includes all applicants that had applied within four months of the current month and not yet hired. Number

of applicants reects the total number of applicants across all pools.    Applicant pool characteristics are

unweighted averages across pools, and are calculating using individuals with test scores in the post-testing

sample.




                                                    36
                           Table II: Impact of job testing on job durations



                                      Dependent Variable: Log(Duration)

                                               (1)              (2)              (3)              (4)


  Post-Testing                               0.368***         0.244**         0.248***         0.233***
                                             (0.120)          (0.113)         (0.0754)         (0.0637)

  N                                          265,648         265,648           265,648          265,648


  Year-Month FEs                                X                X                X                X

  Location FEs                                  X                X                X                X

  Position Type FEs                             X                X                X                X

  Client Firm X Year FEs                                         X                X                X

  Local Unemployment Controls                                                     X                X

  Location Time Trends                                                                             X
  *** p<0.1, ** p<0.05, * p<0.1

Notes: We regress log durations on an indicator for testing availability (this equals 1 in the rst month in
which the modal hire at a location was tested, and in all months thereafter for that location) and the controls

indicated. We use censored-normal regressions with individual-specic truncation points (using cnreg in

Stata) to account for the fact that 21% of hired workers had not yet left their job at the end of our data

collection. Standard errors are in parentheses and are clustered at the location level.




                                                        37
                     Table III: Exception Rates and Post-Testing Duration




                                    Dependent Variable: Log(Duration)

                                       (1)               (2)        (3)            (4)           (5)


 Manager Exception Rate             -0.0682**      -0.0658**     -0.0661**     -0.0607**      -0.0557**
                                    (0.0346)       (0.0321)      (0.0322)      (0.0292)       (0.0283)

 N                                   91,319         91,319        91,319         91,319        91,319


 Year-Month FEs                         X                X           X              X             X

 Location FEs                           X                X           X              X             X

 Position Type FEs                      X                X           X              X             X

 Client Firm X Year FEs                                  X           X              X             X

 Local Unemployment Controls                                         X              X             X

 Location Time Trends                                                               X             X

 Applicant Pool Controls                                                                          X
 *** p<0.1, ** p<0.05, * p<0.1



Notes: This table reports censored normal regressions of manager-level exception rates and tenure outcomes
restricted to the post-testing sample. The exception rate is dened as the number of times a yellow is hired

above a green plus the number of times a red is hired above a green or yellow, divided by the maximum

exceptions possible in that applicant pool. It is then aggregated to the manager level and standardized to

be mean zero and standard deviation one. Applicant pool controls include xed eects for deciles of each of

the following variables: number of applicants, hire rate, share of applicants that are green, and share that

are yellow. Standard errors are clustered by location.




                                                     38
                    Table IV: The Impact of Testing by Exception Rate




                                 Dependent Variable: Log(Duration)

                                      (1)                 (2)               (3)                 (4)

                                   0.366***            0.234**           0.242***           0.234***
 Post-Testing
                                   (0.119)             (0.107)           (0.0696)           (0.0589)

 Location Exception                -0.142**           -0.150**           -0.152**           -0.142**
 Rate* Post-Testing                (0.0652)           (0.0679)           (0.0655)           (0.0573)


 N                                 265,648            265,648            265,648             265,648

 Year-Month FEs                        X                  X                  X                   X

 Location FEs                          X                  X                  X                   X

 Position Type FEs                     X                  X                  X                   X

 Client Firm X Year FEs                                   X                  X                   X

 Local Unemployment Controls                                                 X                   X

 Location Time Trends                                                                            X
 *** p<0.1, ** p<0.05, * p<0.1


Notes:     This table reports censored normal regressions of the dierential impact of testing-adoption, by

location-level exception rate. We use the same sample as dened by the notes to Tables II. The exception

rate is dened as the number of times a yellow is hired above a green plus the number of times a red is

hired above a green or yellow, divided by the maximum exceptions possible in that applicant pool. It is then

aggregated to the location level and standardized to be mean zero and standard deviation one.




                                                     39
           Table V: Tenure of Exceptions vs. Passed Over Applicants



                                              Dependent Variable: Log(Duration)

                                                                  (1)               (2)

                            Quality of Yellow Exceptions vs. Passed over Greens


                                                              0.0402*              0.0449
              Passed Over Greens
                                                             (0.0220)             (0.0357)



              N                                               53,166              53,166

                    Quality of Red Exceptions vs. Passed over Greens and Yellows


                                                             0.159***              0.143**
              Passed Over Greens
                                                             (0.0543)             (0.0634)


                                                             0.143***              0.121**
              Passed Over Yellows
                                                             (0.0546)             (0.0597)



              N                                               25,782              25,782

              Base Controls                                       X                  X
              Comparison Pool FEs                                                    X
              *** p<0.1, ** p<0.05, * p<0.1


Notes: Regressions are restricted to the post-testing sample, adjust for censoring, and standard errors are
clustered at the location level.         The top (bottom) panel compares yellow (red) exceptions  the omitted

category  to passed over greens (and yellows) who were available at the same time but hired in a later

month. Observations are restricted to pools with at least one exception and one passed over worker, and

are further restricted to locations and pools with at least 10 and 5 observations, respectively. Base controls

are location, hire month, and position type xed eects. Comparison pool xed eects are dened by the

manager-location-month for the applicant pool in which candidates were considered together.




                                                             40
              Table VI: Testing, Exception Rates, and Output Per Hour



                                     Dependent Variable: Output per Hour

                                               (1)             (2)             (3)             (4)

                                              Post-Testing Sample           Introduction of Testing

                                                                              0.343           0.156
     Post-Testing
                                                                             (0.366)         (0.327)

                                             -0.0659         -0.111           0.137           -0.137
     Exception Rate*Post-Testing
                                             (0.134)        (0.0953)         (0.153)         (0.217)

     N                                        28,858         28,858          62,421          62,421

     Base Controls                              X               X               X               X

     Full Controls                                              X                               X
     *** p<0.1, ** p<0.05, * p<0.1


Notes: See notes to Tables III and IV. The dependent variable in this case is output per hour and regressions
are estimated with OLS. In columns 1 and 2, we examine the post-testing correlation between the manager-

level exception rate and output per hour. In columns 3 and 4, we examine the dierential impact of testing

as a function of location-level exception rates. Base controls include location, hire month, and position xed

eects. Full controls add client-by-year eects, local unemployment rates, and location-specic time trends.

For the post-testing sample regressions (columns 1 and 2), full controls also include applicant pool controls.




                                                       41
         Discretion in Hiring: Online Appendix

 Mitchell Homan                   Lisa B. Kahn                 Danielle Li
 University of Toronto        Yale University & NBER           MIT & NBER

       & NBER




   Section A is our Data Appendix.    Section B is our Theory Appendix, accompanying

Section IV in the main text. Section C contains supplemental tables and gures.




                                            1
A Data Appendix
The 15 client rms in our sample each obtained testing services from our data provider. In

this section, we describe the introduction of testing across locations within these client rms.

We rst provide some details about the test itself. We then discuss how we assign the date at

which testing is introduced to a location and demonstrate the robustness of our main results

to this denition. We also describe sample coverage over time across client rms and show

robustness to using more balanced panels. We then explore the characteristics of locations

that adopt testing early vs. later. We further provide a discussion of heterogeneity in test

accuracy across locations. Finally, we provide details on sample restrictions and additional

information about the data set.




A.1 The Job Test
The test is designed to take around 30-60 minutes, though its intended length varies by

rm (e.g., according to whether the test covers multiple positions) and consists of several

sections. Applicants generally take the test in addition to submitting standard application

information (such as a resume). The test includes an introductory section describing the job

and work environment, and asks the applicant if he/she thinks they are well-suited for the

job and about eligibility. Following this section, there are questions on many dimensions,

including those on work experience, computer/technical skills, personality traits, cognitive

skills, hypothetical job scenarios, and workplace simulations. The hypothetical job scenarios

reect issues that may arise in performing the specic task we study: for example, if this were

a data entry job, it may ask what the employee would do if she were unable to understand

the data entry interface. In the workplace simulations, applicants are asked to perform part

of the job itself. For example, if this were a data entry job, the applicant may be asked to

read an input le and enter the relevant data.

   Our data rm uses a proprietary algorithm based on candidates' responses to generate

test scores.   This algorithm varies somewhat by client rm, but there are commonalities,

and the algorithm is updated over time as more data arrives.        The algorithm used by a

given rm will include data from that particular rm, as well as data from other rms.

Correlations are analyzed between various questions and employee attrition (a key outcome),

as well as between the various questions and other outcomes (depending on the client rms),

particularly output per hour, as well as output quality. In its promotional materials as well

as in its conversations with us, our data provider has stressed the importance of attrition as

a key outcome.




                                               2
    The central output of the test is a Red/Yellow/Green score (or scores if the test covers

multiple positions) for each applicant. Recruiters observe overall job test scores, but do not

observe underlying information on data such as cognitive skills, personality, or how applicants

would handle various job scenarios.
                                           1



A.2 Assigning Testing Adoption Dates to Locations
We observe the date at which test scores appear in our data, but not all workers are tested

immediately. Our preferred denition assigns testing to begin at a location when the modal

hire in a cohort has a test score. At this point, testing turns on for the location for the

remainder of our sample period.

    Within locations, testing appears to be adopted quickly. Appendix Figure A1 plots the

share of hires who are tested as a function of time relative to when the modal hire at that

location is tested. This shows that testing ramps up very quickly within a location, reaching

roughly 80% coverage almost immediately and continuing to increase to nearly 100% by the

end of our sample period.
                              2    This supports our dening test-adoption as the rst date in which

the modal hire at a location is tested.

    Appendix Table A1 shows that our results are robust to this testing denition. Column

1 replicates our base specications from Tables II and IV in the main text for the intro-

duction of testing (top panel) and the dierential impact of testing across exception rates

(bottom panel).
                   3   These results are very similar when the alternative testing denitions used

in Columns 2 and 3. Column 2 denes testing adoption as the rst date in which any hire

is tested, while column 3 assigns testing at the individual level.

   1 Beyond the central Red/Yellow/Green score (or scores), recruiters observe information on typing speed
and accuracy, and, for some rms and time periods, information on an additional job-related skill, but these
do not enter into the scoring of Red/Yellow/Green. Results are robust to controlling for typing variables
where available, which accounts for the possibility that some locations may have had typing threshold hiring
rules.   In addition, recruiters could observe information on responses submitted during the introductory
section (e.g., whether applicants may have a work schedule issue).      Further, recruiters had the option to
observe several performance prediction scores that go into the nal Red/Yellow/Green score; however, these
also represent overall job test scores (as opposed to underlying information on data such as cognitive skills,
personality, and job scenarios).
   2 According to the data rm, non-tested individuals are primarily those hired from job fairs. Also, our
data contain a small number of non-frontline workers (such as managers and professionals) who are not
tested. These workers are distinguished in our position controls. Last, it is possible that testing could be
rolled out to hiring for particular end clients within a location (but not for others).
   3 Results from Table III from the main text on the correlation between manager-level exception rates and
outcomes of hires do not rely on a comparison of pre and post-testing data so are not included.




                                                       3
A.3 Sample Coverage within Locations over Time
Based on our preferred denition of testing, 97 out of 127 locations receive testing at some

point during our sample period; 83 locations are observed both before and after testing.

Locations observed only before or after testing are included in our regressions and help

identify coecients on controls. However, Column 4 of Appendix Table A1 shows that our

results on the impact of testing are robust to restricting to a balanced panel of locations that

are observed both before and after testing.

   Appendix Figure A2 provides a summary of our sample coverage over time for all loca-

tions. We collect locations by client rm on the            y -axis and plot a dot for each month the
location hires in, with calendar time indicated            on the x-axis. Hollow circles indicate that

testing had not yet been introduced to the location, based on our preferred measure; lled in

circles indicate the post-testing period. A gap between circles indicates no hires were made

in that month.

   This gure indicates that we observe cohorts of workers for many periods both before and

after testing for most locations. Specically, among the 83 locations that hire both before

and after testing, the average observation window post-testing is 15 months and the average

pre-testing observation window is 3.5 years (worker weighted). Furthermore, 90% of hires

in this sample are to a location that can be observed for at least 6 months before and after

testing, 60% are hired to locations with at least a 1 year window around testing. Of course,

the panel is highly unbalanced and there is a range of observation windows for clients and

locations.
             4

   From the gure, locations also appear to hire in most months during their observation

window.      In fact, of the locations that can be observed for at least a full year before and

after testing, three-quarters (worker weighted) hire in every single quarter in that window.

Column 5 of Appendix Table A1 shows that results on the impact of testing are robust to

restricting to this very balanced panel of locations. Results are similar across a wide range

of balanced panels.

   Furthermore, Appendix Figure A3, replicates the event study for the impact of testing,

restricting to locations that hire in each of the four quarters before and after testing, and

shows a very similar picture to Figure II of the main text.

   Finally, as noted in the main text, the data rm informed us that a number of client

rms had some other form of testing before the introduction of our data rm's test. While

information about whether a client rm had testing before our data provider is not part

of our dataset, we asked our data provider to collect information about this on our behalf

by surveying managers and executives at the data rm. From this, the data rm reported

  4 For example, client #13 has no pre-testing data.



                                                       4
that 5 rms had pre-sample testing (and not just in one part of its business), 1 rm had

pre-sample testing in one part of its business, 1 rm was believed to have pre-sample testing

(but our data rm was not certain), and 8 rms were regarded as either not having testing

or believed not to have testing.

   This survey does not provide certainty for all 15 client rms in our data.                However,

column 6 of Appendix Table A1 shows that key coecients are larger on the sample of rms

who likely did not have pre-sample testing.          This is consistent with testing being more of

an improvement for rms that had no alternative test in the pre-period, as well as it being

more useful for managers to follow test recommendations rather than make exceptions at

these rms.




A.4 Timing of Testing and Location Observables
Appendix Figure A4 describes how testing enters our sample across both client rms and

locations.   Circles indicate the date at which testing is adopted for the 97 locations that

ever receive testing during our sample (x-axis). Locations are collected by client rm and

lined up on the    y -axis   in the order of their specic test adoption date.       The size of each

circle reects the location's    size.
                                       5   Among client rms with more than one location (11 out

of 15 locations, accounting for 94% of hires in our data), 80% adopt testing across all their

locations in under 2 years, 50% in under one year. There does not appear to be a systematic

relationship between the size of a location and the time at which it receives testing.

   In Section III of the main paper, we exploited this gradual roll-out of testing across

locations within client rms to estimate the impact of testing on job durations, while con-

trolling for location and hire date xed eects.         Naturally, one may be concerned about

factors leading clients to introduce testing in some locations before others. However, based

on qualitative and quantitative information, we see no evidence that the timing of this roll

out would bias our results.

   On the qualitative side, we had discussions involving dierent individuals from our data

provider (including one person who worked closely with dierent rms on rolling out testing),

as well as managers from a large client rm in our dataset. Representatives mentioned several

possible drivers of testing adoption, including the availability/bandwith of managers to

oversee the adoption of testing, considerations of geography, the openness of end clients

(i.e., the ones paying for the services provided by our client rms) to testing, and whether

a location had historically high attrition.         Importantly, representatives did not say that

  5 We dene the size of the location as the number of workers currently employed in July 2013. For one
location we must use July 2012 instead. This snapshot date avoids overweighting locations that have high
churn.




                                                    5
rms may have adopted testing in ways that reect time-varying dierences in a location's

attrition risk.   For example, no one mentioned bringing in testing to a location that was

recently experiencing or expecting a retention problem.

    On the quantitative side, we have examined the correlation between location-level observ-

ables and the timing of testing adoption. For example, Appendix Figure A5 plots location

characteristics as a function of testing adoption date for several key variables. Circles and

the tted regression line are again weighted by location size, and durations are censoring

adjusted.

    The top panels show relationships for pre-testing characteristics at the location level. In

the top left panel, we nd no systematic relationship between a location's average pre-testing

duration (censoring adjusted) and the date at which it adopts testing. The top middle panel

considers a location-specic time trend in censoring-adjusted durations pre-testing.
                                                                                               6   This

gradient is also quite at: testing does not arrive earlier or later for locations that are on

a stronger or weaker trend in worker duration. Finally, the top left panel plots the average

unemployment rate among workers with exactly a High School Diploma pre-testing. Here,

there is again no relationship between the testing date and local labor market conditions.

We choose the state-level unemployment rate for the education group most representative of

workers in our sample (a high school diploma), but the graph looks similar for unemployment

rates for other groups.
                          7

    The bottom panel of Appendix Figure A5 focuses on variables that are available only after

testing: the share of applicants with a green test score, the average number of applicants per

month, and the average exception rate across HR managers at that location (see Equation

2). Again, we do not nd a discernible pattern for any of these dependent variables.

    We also point out that the linear relationships in these graphs tend to be statistically

insignicant and small in magnitude. For example, we can rule out a plus or minus 1.5%

change in pre-testing average durations with each month that testing is delayed with 95%

condence. We can similarly rule out a plus or minus 0.2% change in the share of applicants

that are green. We can also rule out a plus or minus 0.004 variation in the location exception

rate.   We have examined a wide range of location characteristics and similarly nd little

systematic or robust relationships with timing of testing. Notably, these include pre-testing

averages for the share of months that the location is active in hiring and the location-specic

churn rate.

   6 Specically, we estimate a censored normal regression of job durations on location xed eects and
location-specic time trends for the pre-testing sample.
   7 The graph also looks similar when using aggregated unemployment rates for the 25% of international
locations and when using U.S.-level unemployment rates for each education group for the non-standard
location identiers where state cannot be easily assigned.




                                                     6
A.5 The Accuracy of Test Scores Across Locations
One may be worried that the test does not predict worker quality equally well across lo-

cations. For example, worse establishments may be especially undesirable for more skilled

workers, resulting in lower durations among greens.

    Appendix Figure A6 plots the relationship between manager-level exception rates and

worker duration, separately by color.
                                            8   There are two main patterns to notice. First, greens

perform better than yellows who in turn perform better than reds across all exception rate

bins. Second, the overall quality of hired yellows and greens is broadly stable across exception

rates. This means that, among workers a manager is able to hire, color score is predictive of

performance, regardless of the manager's exception rate.

    We do see some evidence that reds hired by managers who make many exceptions appear

worse than reds hired by managers who make few exceptions. This could be because reds in

these locations are worse or because managers with high exception rates are especially bad

at picking out reds. In either case, this reinforces the point that, in high exception locations,

managers may do better by hiring more greens and yellows, relative to reds: the greens and

yellows they are able to hire are broadly comparable to the quality of greens and yellows in

low exception locations, while the reds they hire appear somewhat worse.

    Furthermore, Appendix Figure A7 provides more information along these lines.                        Here

we plot the relationship between color score and job duration as a function of the same

set of location-level characteristics reported above in Appendix Figure A5. Specically, we

divide locations into 20 equally sized bins (based on number of hires post-testing). We then

estimate censored normal regressions of job duration on an exhaustive set of 20 indicators

for bin, controlling for hire month and position type xed eects, separately by color score.
                                                                                                            9

All observations are restricted to the post-testing period when we observe color score.

    For each characteristic reported in Appendix Figure A7, we nd that color score is

strongly predictive of job durations, regardless of which bin the location falls in. For example,

the top left panel plots the relationship for the average duration of the location pre-testing

and shows three upward sloping parallel lines. This means that average job durations are

increasing in the average quality of the location pre-testing, naturally.               However, the gap

between job durations by color is roughly constant across locations.                 This is indicated by

the fact that the lines do not intersect and, for each bin, average job durations are generally

stacked in order by color score.

   8 We estimate censored normal regressions of log duration on indicators for each of 20 equally sized (based
on number of hires) exception rate bins and base controls (location, hire month, and position xed eects),
separately by color.
   9 Location xed eects are not included as they are collinear with the location characteristics.




                                                      7
   We reach a similar conclusion regardless of which location characteristic we examine: we

cannot reject that color score is equally predictive of worker duration across all the location

characteristics we examine.




A.6 Alternative exception rate denitions
In Appendix Table A2, we examine the robustness of our main results in Tables III and IV

from the main text to alternative ways of dening an exception rate. Recall that we construct

our exception rate by counting the number of order violations (the number of greens that

are passed over by each hired yellow, plus the number of greens and yellows that are passed

over by each hired red) and normalizing by the maximum number possible, given the same

color composition of applicants and total number of hires.

   First, we consider an alternative normalization:           the number of order violations that

would occur if managers hired at random. The random benchmark is interesting because

this is the number of exceptions that would occur if managers ignored the test and the test

were uninformative for quality. In our data, 86% of workers are hired from application pools

in which this exception rate is less than 1, indicating that, in the vast majority of pools,

managers' decisions align with test recommendations to some extent. Next, we consider a

dierent way of conceptualizing the exception rate, using the idea of a score rather than

a violation: 2 points for every green hire, 1 point for every yellow hire, and no points for

red hires.   We count up scores per applicant pool and normalize by either the maximum

possible score, or the score that would obtain under random hiring.                The score measure

diers conceptually from the order violation approach because it is less sensitive to the

number of unhired applicants. For example, the score is the same if a single yellow worker

is hired over 20 greens, or over only one green.
                                                       10   We negate the score metrics so that a

larger number means more exceptions, to align with the order violation measure. All three

of these measures are aggregated to the manager and location-levels and then standardized.

Appendix Table A2 shows that all of these metrics tell similar stories. Results are robust

quantitatively and generally in terms of statistical signicance as well.




A.7 Sample Restrictions
For the post-testing period, we make the following restrictions:

 10 The maximum score for one hire is 2 in both cases, but the random score will dier.




                                                   8
   1. We drop roughly one third of applicants because they have a missing identier for their

      HR manager.
                       11


   2. We drop 2% of hires that are part of pools with less than 3 applicants.


   3. We drop locations that do not have at least two managers because part of our exception

      rate analysis (Equation (3)) relies on within-location variation in manager-level excep-

      tion rates. This drops 2% of remaining managers associated with 0.9% of remaining

      hires.


   4. We drop pools that hire only exceptions because we worry that an idiosyncratic shock

      drives the lack of matriculation of higher scoring applicants. This reects 8% of the

      remaining pools associated with 0.6% of remaining hires.


   5. We drop managers that hire in only 1 pool to clean out some noise in the manager-level

      exception rates. This reects 16% of the remaining managers associated with 0.55%

      of remaining hires.


   6. We drop observations with missing manager-level exception rates, which occur when

      all pools a manager hires to have a value of 0 for the maximum number of possible ex-

      ceptions. This reects 1.5% of the remaining pools associated with 0.06% of remaining

      hires.


    We implement these restrictions for the post-testing period in all analyses, even those that

do not use exception rates, to keep the sample consistent.                However, results from Section

III on the impact of testing (which do not use exception rates) are similar without the

restrictions. We do not exclude observations in the pre-testing period on the basis of being

associated with locations that do not meet our post-testing criteria, as these observations

help identify cohort, client, and position controls. Further, for all analyses, we drop the four

locations (reecting 0.04% of remaining hires) with less than 50 hires over the sample period.

Section A.8 below describes a few further sample restrictions.




A.8 Further Information on Setting and Data
Firms in the Data.          The data were assembled for us by the data rm from records of the

individual client rms. The client rms in our sample employ workers who are engaged in

  11 To assess the possibility of selection bias, we regressed whether HR manager is missing on duration (or
log duration), a dummy for being censored, location controls, month-year of hire dummies, and position
dummies, using the full sample of tested hires. In the two regressions, the coecients on duration and log
duration are statistically insignicant, suggesting that selection bias is not a main concern for our analysis.




                                                      9
the same job, but there are some dierences across the rms along various dimensions. For

example, at one rm, workers engage in a relatively high-skilled version of the job we study.
                                                                                                           12

At a second rm, the data rm provides assistance with recruiting (beyond providing the job

test). Our baseline key results are similar when individual rms are excluded one by one.
                                                                                                           13

    Pre-testing Data.          In the pre-testing data at some client rms there is information

not only on new hires, but also on incumbent workers. This may generate a survivor bias

for incumbent workers, relative to new workers. For example, consider a rm that provided

pre-testing data on new hires going back to Jan. 2010. For this rm, we would observe the

full set of workers hired at each date after Jan. 2010, but for those hired before, we would

only observe the subset who survived to a later date. We do not explicitly observe the date

at which the rm began providing information on new hires; instead, we conservatively proxy

this date using the date of rst recorded termination. We label all workers hired before this

date as stock sampled because we cannot be sure that we observe their full entry cohort.

We drop these workers from our primary sample, but have experimented with including them

along with exible controls for being stock sampled in our regressions.


    Productivity.      In addition to the information on job durations, some client rms provide

data on output per hour. This is available for about a quarter of hired workers in our sample,

and is mentioned by our data rm in its advertising, alongside duration. We trim instances

where average transaction time in a given day is less than 60 seconds.
                                                                                      14

    Test Scores.      As described in the text, applicants are scored as Red, Yellow, or Green.

Applicants may receive multiple scores (e.g., if they are being considered for multiple roles).

In these cases, we assign applicants to the maximum of their scores.
                                                                                 15

    For candidates in our data with at least one Red/Yellow/Green score, roughly one quarter

have one score, roughly half have two scores, and roughly one quarter have more than two

scores in our data.       Among candidates with multiple scores, the scores are very highly

correlated with one another. For example, scores for the two most common positions have a

  12 As such, the work performed at this rm is fairly dierent compared to our other rms.
  13 Specically, we estimated base specications of Tables 2, 3, and 4 from the main text excluding each
rm one by one.
  14 This is about one percent of transactions. Some other productivity variables are also shared with our
data provider, but each variable is only available for an even smaller share of workers than is output per
hour. Such variables would likely face signicant statistical power issues if subjected to the analyses in the
paper (which involve clustering standard errors at the location level).
  15 For 1 of the 15 rms, the Red/Yellow/Green score is missing for non-hired applicants in the dataset
provided for this project.   Our conclusions are substantively unchanged if that rm is removed from the
data.




                                                     10
correlation coecient of 0.88 (for Red=0, Yellow=1, Green=2).
                                                                         16   Our focus on the maximum

of scores thus seems without much loss of generality.
                                                              17

    HR Manager.            The HR managers we study are referred to as recruiters by our data

provider. We do not have data for this project on the characteristics of HR managers (we

only see an individual identier).

    Other managers may take part in hiring decisions as well.                 As noted in footnote 6 of

the main text, one rm said that its HR managers will typically endorse candidates to an

additional manager (e.g., a manager in operations one rank above the frontline supervisor)

to make a nal call.        That said, HR managers play a critical role in deciding who gets

hired. For low-skilled jobs of the type we study, past work suggests that HR managers play

an active role in hiring; for example, in a detailed study by sociologists of a call-center at a

bank, Fernandez, Castilla, and Moore (2000) report that HR managers played an important

role in the recruiting process, even though there was a second interview that was done by

line managers during their study period. In fact, the importance of HR managers at this

particular rm happened to grow after the study: HR managers were granted authority to

make hiring decisions on their own.

    Also, applicants may interact with more than one HR manager during the recruitment

process. In such cases, we assign an applicant to the HR manager with whom they have the

most interactions.
                      18   Most managers are primarily associated with one location, but some

are associated with multiple locations.

    We do not observe manager incentives in our data. However, a manager from our data

provider informed us that recruiters in our setting often receive a nancial incentive to meet

or exceed several targets (while pointing out that such pay structures are highly variable by

rm). He said that recruiters always have targets with respect to ll rate (e.g., a requisition

of 20 new hires to begin work on March 1st), and often have targets with respect to short-

term tenure (e.g., a certain share of people graduating training, or of staying some length

of time, such as 90 days) or activities (e.g., conducting X interviews or reaching out to Y

candidates).


    Race, Gender, Age.            Data on race, sex, and age are not available for this project.

However, Autor and Scarborough (2008) show that job testing does not seem to aect worker

  16 This is the correlation in the raw data before imposing data restrictions.
  17 Applicants may be considered for multiple positions so it would be dicult to discern which is the most
relevant score for a given applicant.
  18 This excludes interactions where information on the HR manager is missing. If there is a tie for most
interactions, we assign an applicant to only one manager. Our main results are also qualitatively robust to
setting the HR manager identier to missing in cases of ties for most interactions.




                                                    11
race, suggesting that changes in worker demographics such as race are not the mechanism

by which job testing improves durations.


    Location Identiers.         In our dataset, we do not have a common identier for workplace

location for workers hired in the pre-testing period and applicants applying post-testing.

Consequently, we develop a crosswalk between anonymized location names (used for workers

in the pre-testing period) and the location IDs in the post-testing period. We drop workers

from our sample where the merge did not yield a clean location variable.
                                                                                      19

    As explained in the main text, there are a small number of non-standard location iden-

tiers (e.g., those where workers generally work o-site in dierent states). We assign these

locations to unemployment rates using education-specic, US national unemployment data.

We do so even though a small share of workers associated with non-standard location iden-

tiers may be outside the US.


    Hiring Practices Information.              For several client rms, our data rm surveyed its

account managers (who interact closely with the client rms regarding job testing matters),

asking them to provide us with information on hiring practices once testing was adopted.

The survey indicated that rms encouraged managers to hire workers with higher scores (and

some rms had policies on not hiring low-scored candidates), but left substantial leeway for

managers to overrule testing recommendations. Information from this survey is referenced

in footnote 7 of the main text.


    Job Oers.       As discussed in the main text, our data for this project do not include

information on the receipt of job oers, only on realized job matches. The data rm has a

small amount of information on oers received, but is only available for a few rms and a

small share of the total applicants in our sample, so it would likely be of little use for this

project.


    Position Controls.         Position is measured in our data using the last position that a

worker held when the data le was created.




  19 This includes some locations in the pre-testing data where testing is never later introduced. Our sample
is not all locations within the rms.



                                                     12
Appendix Figure A1: Share of hired workers tested by time since assigned
                                         test-adoption date


             1
             .8
       Share tested
       .4    .2
             0   .6




                      -36          -18                0                  18                         36
                                      Months since assigned testing date


Notes: Figure A1 plots the share of hired workers with a test score as a function of time since the location-
specic assigned testing date, averaged across locations. The testing date is dened at the location-month

level as the rst month in which the modal hire is tested. This graph is restricted to locations that receive

testing. For gure clarity, we further restrict to the 89% of workers hired within 3 years of the introduction

of testing.




                                                     13
                                     Appendix Figure A2: Location Coverage by Date



                    1415
                    13
                    10 1112
                    9
                    7 8
      Client Firm
                    6
                    5
                    4
                    3
                    2
                    1




                                 2007m7                2009m1               2010m7      2012m1   2013m7
                                                                            Hire Date
                              Hollow = pre-testing, Filled = post-testing


Notes: Locations are lined up on the y-axis, grouped by client rm. Dots indicate that the location hired
in a given month, while a gap means no hires were made that month. Filled circles refer to periods after

testing is adopted, using our denition (the modal hire was tested), while hollow circles refer to periods

before testing. Dates are restricted to a 3 year window around testing adoption, covering 89% of hires. All

dots are hollow for Firm 14 because it does not have a location meeting our denition of testing.




                                                                            14
              Appendix Figure A3: Event Study of Duration Outcomes, Balanced Panel




                                Log(Duration)                                     Survived 3 Months




                                                                     .6
               1 1.5 2




                                                                     .4
                                                                     .2
               .5




                                                                     0
               0
               -.5




                                                                     -.2
Coefficient




                         -8     -4     0        4       8                    -8       -4       0        4       8


                              Survived 6 Months                                   Survived 12 Months
               .6




                                                                     .6
                                                                     .4
               .4




                                                                     .2
               .2




                                                                     0
               0




                                                                     -.2
               -.2




                         -8     -4     0        4       8                    -8       -4       0        4       8

                                      Quarters since Introduction of Testing

          Notes: See notes to Figure II of the main text.   The sample is restricted to locations with observations in

          each quarter from 4 lags before testing to 4 leads after (indicated with vertical dashed lines). The graph

          window is restricted to 10 quarters before and after testing.    Dashed lines indicate the 95% condence

          interval.




                                                              15
  Appendix Figure A4: Date of Location Testing Adoption, by Client Firm



                       15
                       10
         Client Firm
                       5
                       0




                       2008m7             2009m7          2010m7           2011m7         2012m7            2013m7
                                                                    Test Date



Notes:                 Figure A4 plots location-specic assigned testing dates on the   x-axis,   organized by client rm on

the   y -axis.              Circles are weighted by location size, as dened by the number of workers currently employed in

our data on July, 2013. As noted in Figure A2, Firm 14 does not appear on the graph because it does not

have a location that meets our denition of testing.




                                                                    16
 Appendix Figure A5: Location Observables and Date of Testing Adoption



                       Job Duration                               Job Duration Trend                                HS Unemployment Rate
                     Pre-Testing Average                                  Pre-Testing                                  Pre-Testing Average




                                                                                                          .2
                                                       1
       8




                                                       0




                                                                                                          .15
       6




                                                       -1
       4




                                                                                                          .1
                                                       -2
       2




                                                                                                          .05
                                                       -3
       0




         2010m1   2011m1   2012m1    2013m1   2014m1    2010m1   2011m1     2012m1      2013m1   2014m1    2010m1   2011m1   2012m1    2013m1   2014m1



                  Share Green Applicants                         Number of Applicants                                   Exception Rate
                     Post-Testing Average                           Post-Testing Average                               Post-Testing Average
                                                       2500




                                                                                                          .6
       .8




                                                       2000




                                                                                                          .5
       .6




                                                       1500




                                                                                                          .4
                                                       1000




                                                                                                          .3
       .4




                                                                                                          .2
                                                       500




                                                                                                          .1
       .2




                                                       0




         2010m1   2011m1   2012m1    2013m1   2014m1    2010m1   2011m1     2012m1      2013m1   2014m1    2010m1   2011m1   2012m1    2013m1   2014m1



                                                  Location Test-Adoption Date

Notes:      Figure A5 plots the relationship between various location-level variables (y -axis) and date of test

adoption (x-axis). Circles and tted lines are weighted by location size. In the top left panel, pre-testing

durations are obtained from a censored normal regression of log durations on an exhaustive set of location

xed eects estimated on the pre-testing sample. The top middle panel plots location-specic time trends

estimated from a censored normal regression of log durations on location xed eects and location-specic

time trends in the pre-testing sample. The remaining variables are raw averages at the location-level either

pre- (top right) or post- (bottom panels) testing.




                                                                           17
    Figure A6: Manager-Level Exception Rates and the Color Score-Job
                                      Duration Relationship

             6.5
             6
      Log(Duration)
             5.5
             5
             4.5




                      .1               .2                   .3                  .4                   .5
                                            Manager-Level Exception Rate

                                                  Green       Yellow
                                                  Red



Notes: This graph shows the relationship between color score and job duration for 20 equally sized manager-
level exception rate bins. Specically, we estimate censored normal regressions of log duration on 20 exhaus-

tive indicators for exception rate bin and location, hire month, and position xed eects, separately by color

score. We plot the coecients on the exception rate bins as well as the line of best t.




                                                     18
    Figure A7: Location Observables and The Color Score-Job Duration
                                                                              Relationship




                                                                      6.5




                                                                                                                         14
                      7




                                                                                                                         12
                                                                      6
                      6




                                                                                                                         10
                                                                      5.5




                                                                                                                         8
                                                                      5
                      5




                                                                                                                         6
                                                                      4.5
     Log(Duration)




                                                                                                                         4
                      4




                                                                      4
                            4         5         6         7       8            -.3    -.2     -.1     0     .1      .2      .05        .1          .15          .2
                                Job Duration, Pre-Testing Average                 Job Duration Trend, Pre-Testing          HS Unemployment Rate, Pre-Testing Average
                      6.5




                                                                      6.5




                                                                                                                         6.5
                                                                                                                         6
                                                                      6
                      6




                                                                                                                         5.5
                                                                      5.5
                      5.5




                                                                                                                         5
                                                                                                                         4.5
                                                                      5
                      5




                                                                                                                         4
                                                                      4.5
                      4.5




                         .2         .4          .6           .8              0     500    1000 1500 2000 2500                  .1        .2       .3        .4       .5
                       Share Green Applicants, Post-Testing Average         Number of Applicants, Post-Testing Average           Exception Rate, Post-Testing Average


                                                                  Location-Level Characteristic

Notes:               See notes to Figure A6 of the main text. This graph shows the relationship between color score

and job duration for 20 equally sized bins based on the location-level characteristic specied on the                                                                     x-axis.
Specically, we estimate censored normal regressions of log duration on 20 exhaustive indicators for the

location characteristic bin and hire month and position xed eects, separately by color score. (We exclude

location xed eects from these regressions because they are collinear with the location characteristics.) We

plot the coecients on the bins as well as the best linear t.




                                                                                          19
            Appendix Table A1: Robustness for Results on the Impact of Testing



                                                       Dependent Variable: Log(Duration)

                                                        (1)            (2)             (3)         (4)        (5)        (6)

                                                                 Impact of Testing

                                                      0.368***       0.316***        0.316***   0.296**    0.261**     0.516**
Post-Testing
                                                      (0.120)        (0.121)         (0.119)    (0.150)    (0.117)     (0.245)

                                               Differential Impact of Testing by Exception Rates
                                                      0.366***      0.321***     0.320***        0.299**   0.294***    0.495***
Post-Testing
                                                       (0.119)       (0.120)      (0.119)        (0.151)    (0.110)    (0.161)
                                                      -0.142**      -0.151**     -0.146**        -0.127*   -0.178***   -0.444**
Exception Rate*Post-Testing                           (0.0652)      (0.0696)     (0.0687)       (0.0663)   (0.0556)    (0.202)

N                                                     265,648        265,648         265,648    216,676     96,273     83,910

Base Controls                                            X              X               X          X          X           X
Testing Definition:
                                Modal Worker Tested      X                                         X          X           X

                                 Any Worker Tested                      X

                        Individual Worker Tested                                        X
Location Restrictions:
            Observed Both Pre/Post Testing                                                         X          X

Observed in Balanced 4 Quarter Window                                                                         X

        Client Had No Pre-Sample Testing                                                                                  X
*** p<0.1, ** p<0.05, * p<0.1



    Notes: This table reports censored normal regressions with standard errors clustered at the location level.
    The top panel provides estimates of the impact of testing on log durations (see Table II), while the bottom

    panel estimates the dierential impact of testing by location-level exception rates (see Table IV). Column 1

    reproduces baseline specications from the preceding tables. Column 2 denes the test adoption date as the

    rst time a hire is observed with a test score at a location. Column 3 denes test adoption as whether the

    individual hire has a test score. Column 4 restricts to the 83 locations that are observed both before and

    after testing. Column 5 further restricts to locations that are observed in each of the four quarters prior and

    post testing. Column 6 restricts to locations that likely did not have job testing before partnering with our

    data rm. Base controls include location, hire month, and position xed eects.




                                                                       20
         Appendix Table A2: Robustness to Alternative Exception Rates



                                              Dependent Variable: Log(Duration)

                                                        (1)             (2)         (3)            (4)

                                              # Exceptions Relative to Random

                                                      Post-Testing Sample         Introduction of Testing

                                                                                  0.359***      0.232***
           Post-Testing
                                                                                  (0.119)       (0.0588)

                                                     -0.0730**       -0.0635**    -0.157**      -0.125**
           Exception Rate*Post-Testing
                                                     (0.0327)        (0.0258)     (0.0713)      (0.0556)

                                            Exception Score Relative to Max Score

                                                      Post-Testing Sample         Introduction of Testing

                                                                                  0.356***      0.230***
           Post-Testing
                                                                                  (0.120)       (0.0656)

                                                      -0.0237        -0.0707***   -0.190**       0.0420
           Exception Rate*Post-Testing
                                                     (0.0261)         (0.0190)    (0.0933)      (0.0676)

                                          Exception Score Relative to Random Score

                                                      Post-Testing Sample         Introduction of Testing

                                                                                  0.353***      0.222***
           Post-Testing
                                                                                  (0.117)       (0.0609)

                                                      -0.0585         -0.0149     -0.155**      -0.0999**
           Exception Rate*Post-Testing
                                                     (0.0364)        (0.0241)     (0.0763)      (0.0498)

           N                                          91,319          91,319      265,648       265,648

           Base Controls                                X                X           X              X

           Full Controls                                                 X                          X
          *** p<0.1, ** p<0.05, * p<0.1



Notes:   Columns 1 and 2 estimate the post-testing correlation between manager-level exception rates and

log duration (see Table III). Columns 3 and 4 estimate the dierential impact of testing by location-level

exception rates (see Table IV). The top panel denes the exception rate as the number of order violations

divided by the number of order violations under random hiring. The next panels use an exception score (1

point for yellow and 2 points for green hires) divided by the maximum possible score (middle panel) or the

score under random hiring (bottom panel). Base controls include location, hire month, and position xed

eects. Full controls add client-by-year eects, local unemployment rates, and location-specic time trends

(and applicant pool controls in columns 1 and 2).




                                                                21
B Theory Appendix
B.1 Preliminaries
We rst provide more detail on the rm's hiring problem, to help with the proofs that follow.

    Under Discretion, the manager hires all workers for whom                 E[Ui |ti , si , bi ] = (1âk)E[a|si , ti ]+
kbi > u   where     u   is chosen so that the total hire rate is         xed at W .

    We assume       bi      is perfectly observable, that    ai |ti â¼ N (Âµt , Ïa2 ),   and that   si = ai + i   where

i â¼ N (0, Ï2 )   and is independent of        ai , bi , and ti .
    Dene  Ut â¡ E[Ui |ti , si , bi ]|ti . Based on standard projection formulas for the Gaussian
                                                                        2 2  2 2       2    Ï4
distribution, we know that Ut â¼ N ((1âk)Âµt , Î£), where Î£ = (1âk) Ï +k Ïb and Ï = 2 a 2 .
                                                                                          Ï +Ï                   a   

    Let   Î¦   and   Ï    denote the standard normal cdf and pdf, respectively. Therefore, the hire

probability and         u   are pinned down by equation B1:



(B1)                              W = pG (1 â Î¦(zG )) + (1 â pG )(1 â Î¦(zY ))

               uâ(1âk)Âµt
where   zt =      â
                    Î£
                         .

    The rm's payo under Discretion is                 E[a|Hire],     i.e., the expected quality conditional

on being hired.          By using the properties of the bivariate normal distribution, this can be

expressed as follows, where           Î»(Â·)   is the inverse Mills ratio of the standard normal.


                                                         (1 â k)Ï 2
                                                                         
(B2)                W â E[a|Hire] = pG (1 â Î¦(zG )) ÂµG +    â       Î»(zG )
                                                              Î£
                                                         (1 â k)Ï 2
                                                                          
                              (1 â pG )(1 â Î¦(zY )) ÂµY +    â       Î»(zY )
                                                              Î£

    Under No Discretion, the rm hires based solely on the test. Since we assume there are

plenty of type      G       applicants, the rm will hire among type           G    applicants at random. Thus,

the expected quality of hires equals            ÂµG .


B.2 Propositions
Propositions 1 and 2, formalized below, provide intuition for our empirical analysis. Propo-

sition 1 states that the exception rate (the probability that a                 Y   is hired above a   G applicant)
is increasing in both the precision of a manager's private information and his or her bias.

Proposition 2 says that the quality of hired workers,                   E[a|Hire],      is decreasing in manager

bias. It also shows that absent bias, the quality of hires is increasing in the precision of a

manger's private information.




                                                           22
Proposition 1         The exception rate is increasing in managerial bias,                       k,   as well as weakly

increasing in the precision of the manager's private information,                      1/Ï2 .


Proof      Because the hiring rate is xed at               W , E[Hire|Y ]      is a sucient statistic for the

probability that an applicant with               t = Y     is hired over an applicant with                t = G,   i.e., an

exception is made.

   Recall from above that        Ut is normally distributed with mean                     (1 â k)Âµt and variance
               2 2      2 2
Î£ = (1 â k) Ï        + k Ïb . A manager will hire all applicants for whom                 Ut is above u where the
latter is chosen to keep the hire rate xed at               W.
   Consider the dierence in expected utility across                  G and Y   types. If    ÂµG âÂµY        were smaller,

more   Y   types would be hired, while fewer            G    types would be hired. This is because, at any

given quantile of       UG ,   there would be      more Y    types above that threshold.
                                        Ut
   Let us now dene            UÌt =    â
                                          Î£
                                            . This transformation is still normally distributed but now
              (1âk)Âµt
has mean        â
                 Î£
                      and variance         1.    Under this rescaling, it will still be the case that the

probability of an exception is decreasing in the dierence in expected utilities across                            UËG   and
              (1âk)(ÂµG âÂµY )
UËY : âU =         â
                     Î£
                             .

                                                           ââU       âk(ÂµG âÂµY )Ïb2
   One can show (with some algebra) that
                                                            âk
                                                                 =       Î£3/2
                                                                                    , which is clearly negative for

k 6= 0.    When   k   is larger, the expected gap in utility between a               G   and a        Y   narrows so the

probability of hiring a         Y   increases.
                                            ââU        (1âk)3 (ÂµG âÂµY )(Ïa2 )2
   Similarly, one can show that
                                            âÏ2
                                                   =      2Î£3/2 (Ï2 +Ïa2 )2
                                                                               , which is clearly positive for      k<1
     ââU
(and
     âÏ2
             =0   for   k = 1).     The gap in expected utility between            G   and   Y    widens when man-

agers have less information. It thus narrows when managers have better private information,

as does the probability of an exception.               

Proposition 2         Holding constant information, the quality of hires is decreasing in manage-

rial bias,   k.   When    k = 0,       the expected quality of hires for a given manager,                  E[a|Hire],     is

increasing in the precision of the manager's private information,                      1/Ï2 .


Proof      The expected quality of hires,           E[a|Hire],       is given in equation B2, above, and is a

function of manager type. With some messy algebra, we obtain that:

                                                "                               #
                                                                              2
                          âE [a|Hire]     kÏb2           k 2 2
                                                            Ï b B (ÂµG â Âµ Y )
(B3)                                  =â          AÏ 2 +
                              âk         W Î£3/2                    AÎ£

and

                                  "                                                           #
                                                                        2 2 2               2
            âE [a|Hire]    1âk                                  (1 â k)  k Ï b B (ÂµG â Âµ Y )
                                    A (1 â k)2 Ï 2 + 2k 2 Ïb2 â
                                                             
(B4)                    =
                âÏ 2      2W Î£3/2                                           AÎ£

                                                            23
where   A â¡ pG Ï (zÌG ) + (1 â pG ) Ï (zÌY )   and    B â¡ pG (1 â pG ) Ï (zÌG ) Ï (zÌY ).
   First, note that the derivative wrt           k   is negative; expected quality of hires is strictly

decreasing in bias. Second, note that when setting  k = 0, the derivative wrt Ï 2 is positive.
                   2                                  2
Also, recall that Ï moves in the same direction as 1/Ï . Therefore, expected quality of hires

is strictly increasing in the precision of private information, when the manager is unbiased.

   We next provide intuition for these results by summarizing the logic for why these in-

equalities should at least weakly hold.

   Since the family of normal distributions is Blackwell-ordered by precision, Blackwell's

Theorem tells us that an increase in the precision of information must weakly increase the

manager's utility. For     k = 0,   the manager maximizes            U = E[a|Hire],     which is a function

of the precision of the manager's private information. Therefore, absent bias, the expected

quality of hires is increasing in manager information.

   For   k,   consider two managers, with bias          kH
                                                   k L , respectively, where k L < k H . Each
                                                             and
                                                                                        H H
                                                                                            
manager chooses a group of hires that maximizes (1âk)E[a|Hire]+kE[b|Hire]. Let a , b

denote the realized expectation of a and b, conditional on being hired by the manager
                          L L
                               
with high bias, and let a , b    denote the same for the manager with low bias. Because
                   
 aH , bH and aL , bL are chosen optimally, we have the following incentive compatibility
(IC) constraints:



                            1 â k H aH + k H bH â¥             1 â k H aL + k H bL
                                                                    

                              1 â k L aL + k L bL â¥           1 â k L aH + k L bH
                                                                    


   We would like to prove that       aH â¤ aL .       Suppose to the contrary that       aH > aL .
                                                                                                               
   First, note that it cannot be that          aH > aL       and   bH > bL   because the choice,     aL , bL       ,

would violate the IC of the low-bias manager.

   Second, consider where        aH > aL        and  bH â¤ bL . In this circumstance,          the high-bias

manager is choosing candidates with            higher a and lower b than the low-bias         manager. We

can rearrange and sum the IC constraints to show that they imply the following:



                              k H â k L ((bH â bL ) â (aH â aL )) â¥ 0
                                       


   However, this expression is false because, by assumption, we have                    k H > k L , aH > aL ,
and   bH â¤ bL .
   Therefore, by contradiction, we have shown that                 aH â¤ aL . 




                                                        24
   Discussion on Proportions 1 and 2.                From Propositions 1 and 2, we observe that if

high-exception managers achieve worse outcomes than low-exception managers, this must

be because high-exception managers are biased or mistaken.

   Formally, consider two managers, manager 1 and manager 2.                    The two managers have

type   (ki , hi ), where ki        hi is the precision (i.e., inverse variance) of each manager's
                              is bias and

private information, for i â {1, 2}. The two managers have exception rates, Ri , and quality

of hires, ai , for i â {1, 2}. We claim that if R1 > R2 and a1 < a2 , then it must be that

k1 > 0.
   To see this, suppose to the contrary that             k1 = 0.   Then by Proposition 1, it must be

that   h1 > h2 .   That is, if manager 1 is weakly less biased than manager 2 but still has more

exceptions, manager 1 must have more precise private information. Now consider a third

manager with bias,          k3 = 0,                       h3 = h2 , and outcomes a3 . That
                                      precision of private information

is, manager 3 has no bias and the same information as manager 2. By Proposition 2, it must

be that a1 > a3 > a2 .
                       20 But this is a contradiction.


   Having discussed how Propositions 1 and 2 help frame our empirical work, we now present

Proposition 3. Proposition 3 illustrates the fundamental tradeo rms face when allocating

authority: managers have private information, but they are also biased. Greater bias pushes

the rm to prefer No Discretion, while better information tends to push it towards Discretion.

Specically, the rst nding states that when bias,          k , is low, rms prefer to grant discretion,
and when bias is high, rms prefer No Discretion. Part 2 states that for any level of bias,

there is a precision of private information small enough that rms prefer No Discretion.

Uninformed managers would at best follow test recommendations and, at worst deviate

because they are mistaken or biased. Finally, part 3 states that, for any xed information

precision threshold, there exists an accompanying bias threshold such that if managerial

information is greater and bias is smaller, rms prefer to grant discretion.                    Put simply,

Discretion beats out No Discretion when a manager has very precise information, but only

if the manager is not too biased.



Proposition 3        We formalize conditions under which the rm will prefer Discretion or No

Discretion.


   1. For any given precision of private information,          1/Ï2 > 0,   there exists a   k 0 â (0, 1)   such
                        0
       that ifk < k worker            quality is higher under Discretion than No Discretion and the
                        0
       opposite if k > k .

  20 Manager 1 should have better outcomes than manager 3 because they both have no bias but manager 1
has better information. Manager 3 should have weakly better outcomes than manager 2 because they have
the same information but manager 3 is unbiased (so therefore weakly less biased than   2).


                                                       25
  2. For any given bias,             k > 0,   there exists   Ï   such that when    1/Ï2 < Ï,   i.e., when preci-

      sion of private information is low, worker quality is higher under No Discretion than

      Discretion.


  3. For any precision of information              Ï â (0, â),     there exists a bias,   k 00 â (0, 1),   such that

      if   k < k 00   and   1/Ï2 > Ï,   i.e., high precision of private information and low bias, worker

      quality is higher under Discretion than No Discretion.


   We next prove each item of Proposition 3:


  1. For any given precision of private information,                 1/Ï2 > 0,   there exists a   k 0 â (0, 1)   such

      that ifk < k 0 worker            quality is higher under Discretion than No Discretion and the
                       0
      opposite if k > k .



Proof      Whenk = 1, the manager hires based only on b, which is independent of a. So
E[a|Hire] = pG ÂµG + (1 â pG )ÂµY . The rm would do better under No Discretion (where
quality of hires equals ÂµG ).

   When       k = 0,    under No Discretion, the quality of hires remains equal to                       ÂµG .   Write

Ï=   1/Ï2 for the precision of the manager's information, and write   Âµ(k, Ï) for average quality
of hire under Discretion for a manager with bias k and precision Ï. Consider a precision

Ï0 with 0 < Ï0 < Ï. By Blackwell's Theorem, it must be that ÂµG â¤ Âµ(0, Ï0 ). That is, an
                             0
unbiased manager with Ï > 0 will do better than hiring by the test score alone. Further,
                                                                                      0
since quality of hire is increasing in precision (Proposition 2), we know that Âµ(0, Ï ) < Âµ(0, Ï).

By transitivity, ÂµG < Âµ(0, Ï), and the rm would do better under Discretion.

   Thus, Discretion is better than No Discretion for                 k = 0 and the opposite is true for k = 1.
Proposition 2 shows that the rm's payo is                  strictly decreasing in k . There must therefore

be a single cutpoint,         k0,   where, below that point, the rm's payo for Discretion is larger

than that for No Discretion, and above that point, the opposite is true.                    


  2. For any given bias,             k > 0,   there exists   Ï   such that when    1/Ï2 < Ï,   i.e., when preci-

      sion of private information is low, worker quality is higher under No Discretion than

      Discretion.



Proof      Fix bias   k > 0.   One can show that as      1/Ï2    approaches 0 and therefore        Ï2   approaches

0, the expected quality of hires (equation (B2)) is strictly less than                   ÂµG .   To see this, note

that equation (B2) can be rearranged to obtain:




                                                         26
(B5)                  W â E[a|Hire] = pG (1 â Î¦(zG ))ÂµG + (1 â pG )(1 â Î¦(zY ))ÂµY
                                             (1 â k)Ï 2
                                           + â          [pG Ï(zG ) + (1 â pG )Ï(zY )]
                                                  Î£

      AsÏ 2 approaches 0, the second term vanishes. This term is bounded a way from ÂµG                                     since
                               (1âk)(ÂµG âÂµY )
the   gap zG â zY approaches                  , which is bounded away from â.
                                    kÏb
      Thus, as precision of private information tends towards                            â,    the rm will prefer No Dis-

cretion (which has a payo of                      ÂµG )   to Discretion.

      We also point out that the rm's payo under Discretion, expressed above in equation

(B2), is clearly continuous in                    Ï    (which is continuous in      1/Ï2 ).
      Thus, as the manager tends towards no information, the rm prefers No Discretion and

the rm's payo under Discretion is continuous in the manager's information.                                           Therefore

there must be a point                  Ï   such that, for precision of manager information below that point,

the rm prefers No Discretion to Discretion.                          


     3. For any precision of information                      Ï â (0, â),      there exists a bias,   k 00 â (0, 1),   such that

          if   k < k 00   and   1/Ï2 > Ï,        i.e., high precision of private information and low bias, worker

          quality is higher under Discretion than No Discretion.



Proof          Dene      â(Ï2 , k)   as the dierence in quality of hires under Discretion, compared to

No Discretion, for xed manager type                           (Ï2 , k).   Since the rm's payo under Discretion is

continuous in both              k   and    Ï2   (see Equation (B2) above),          â   must also be continuous in these

variables. By Proposition 2, the payo of Discretion is strictly decreasing in the bias                                   k,   and

so â(Ï2 , k) is strictly decreasing in k . Moreover, when k = 0, Discretion is strictly preferable
                          2                                                 2               2
to No Discretion for Ï < â (see proof to Proposition 3 part 1), so â(Ï , k) > 0 for Ï < â.

Finally, Proposition 2 shows that when k = 0, the rm's payo under Discretion is increasing
    1             2                        2
in 2 , i.e., â(Ï , 0) is decreasing in Ï .
   Ï  

      Fix anyÏ â (0, â) and                      letÏ2 = 1/Ï. We           seek to show that there exists      k 00   such that
                                                              00
â(Ï , k) > 0 for all Ï2 < Ï2
   2
                                                 and all k < k .

      Let      y = â(Ï2 , 0) > 0.         This is the dierence in the rm's payo for Discretion compared

to No Discretion for an unbiased manager, with some minimal amount of information,                                             Ï2 .
We know           y > 0       because for an unbiased manager, Discretion strictly improves upon No

Discretion (see part 1 of Proposition 3).

      Since      â(Ï2 , 0)   is decreasing in         Ï2 , it holds that â(Ï2 , 0) > y for all Ï2 < Ï2 . Therefore,
                                                           00               2            2                        00
it suces to show that there                      exists k such that â(Ï , 0) â â(Ï , k) < y for all k < k and



                                                                     27
Ï2 < Ï2 .   That is, for bias less than   k 00   and info more precise than the minimal case          Ï2 ,   we

want to show that there is some small enough bias, such that removing this bias improves the

rm's payo by only a small amount, relative to the case of no bias and minimal information,

y.   If this is true, then the bias amount is not enough to make the rm prefer No Discretion,

because     y > 0.
      Let   d(k) = max â(Ï2 , 0) â â(Ï2 , k).       We know       d(k)   exists because   â()   is continuous
                     Ï2 â[0,ÏÂ¯2 ]
wrt Ï2 and the interval over which we take the maximum is compact. In words, d(k) nds the
largest possible improvement from eliminating bias, for bias k , across all values of information

in the range.

      It now suces to show that there exists          k 00
                                                       d(k) â¤ y for all k < k 00 . This holds
                                                              such that

because d(0) = 0 (by denition) and y > 0, and because d(k) is continuous in k (since â is).






                                                        28
C Supplemental Tables and Figures
               9    8.5   Appendix Figure C1: Output per hour and Job Durations
      Output per Hour
      8        7.5




                             3            4                5                    6                  7
                                                    Log(Duration)

Notes:       Figure C1 plots average output per hour within 20 evenly sized bins, based on log(duration). It

controls for location xed eects to account for dierences in average output per hour across locations.




                                                      29
                                  Appendix Table C1: Testing and job durations
                                                     Additional outcomes



                                                >3 Months                      >6 Months              >12 Months
                                           (Mean=0.62; SD=0.49)           (Mean=0.46; SD=0.50)    (Mean=0.32; SD=0.47)

                                             (1)            (2)              (3)         (4)         (5)          (6)

                                                        Introduction of Testing

                                           0.0427*        0.0259          0.0919**    0.0597***   0.106***     0.0750***
  Post-Testing
                                          (0.0220)       (0.0200)         (0.0371)    (0.0228)    (0.0369)     (0.0198)

  N                                        256,641       256,641          243,580     243,580      217,514     217,514

                                                       Post-Testing Correlations

                                          -0.0261***    -0.0171**         -0.0158**    -0.0101*    -0.00471    -0.0127**
  Manager Exception Rate
                                          (0.00940)     (0.00780)         (0.00638)   (0.00602)   (0.00496)    (0.00483)

  N                                        82,365         82,365           71,388      71,388      56,436       56,436

                                   Differential Impact of Testing by Location-Level Exception Rate
                                           0.0420*        0.0258          0.0912**    0.0581***   0.101***     0.0738***
  Post-Testing                            (0.0215)       (0.0187)         (0.0369)    (0.0218)    (0.0350)     (0.0191)


  Location Exception                      -0.0271*       -0.0372**         -0.0297    -0.0318*    -0.0548***    -0.0248
  Rate*Post-Testing                       (0.0160)       (0.0173)         (0.0206)    (0.0179)     (0.0196)    (0.0172)

  N                                        256,641       256,641          243,580     243,580      217,514     217,514
  Base Controls                               X             X                X           X            X           X
  Full Controls                                             X                            X                        X
  *** p<0.1, ** p<0.05, * p<0.1



Notes:       See notes to Tables II, III, and IV of the main text. The dependent variables are the probability

that a worker survives 3, 6, or 12 months, respectively, among those who are not right-censored, i.e., those

hired at least that many months before the data end date for each of the 15 rms. We use OLS regressions.

Base controls include location, hire month, and position xed eects. Full controls add client-by-year eects,

local unemployment rates, and location-specic time trends. Full controls in the middle panel also include

applicant pool characteristics. The top panel provides estimates of the impact of testing on job duration

outcomes. The middle panel estimates the post-testing correlation between job duration and manager-level

exception rates. The bottom panel estimates the dierential impact of testing by location-level exception

rates.




                                                                     30
       Appendix Table C2: Job Duration of Workers, by Length of Time in
                                                  Applicant Pool



                                             Dependent Variable: Log(Duration)

                                    (1)             (2)           (3)            (4)     (5)             (6)
                                     Green Workers                Yellow Workers           Red Workers

                                  0.00545         -0.0276       -0.0271      -0.0139    -0.0338        -0.0449
  Waited 1 Month
                                  (0.0281)       (0.0263)      (0.0320)     (0.0242)   (0.0622)       (0.0752)

                                   -0.0352        -0.0714       -0.0204      -0.0542   0.00713         0.0467
  Waited 2 Months
                                  (0.0586)       (0.0632)      (0.0647)     (0.0663)   (0.144)         (0.174)

                                  0.00486         -0.0941        0.112        0.120    0.0338          0.0493
  Waited 3 Months
                                  (0.0673)       (0.0851)      (0.0855)     (0.0867)   (0.220)         (0.242)


  N                               47,809          47,809         24,496      24,496     4,098           4,098
  Base Controls                      X               X             X             X        X               X
  Initial Applicant Pool FEs                         X                           X                        X
  *** p<0.1, ** p<0.05, * p<0.1




Notes:       Regressions are restricted to the post-testing sample, adjust for censoring, and cluster standard

errors at the location level. Each panel compares applicants who started working in the month they applied

(omitted category) to those who started 1, 2, or 3 months later, separately by color.             Panels restrict to

applicant pools (location-recruiter-initial application month) with variation in wait time, and further restrict

to locations and pools with at least 10 and 5 observations, respectively. Base controls are location, hire month,

and position type xed eects. Initial applicant pool xed eects are dened by the manager-location-month

for the pool when candidates rst applied.




                                                            31
             Appendix Table C3: Exception Rates and Duration Outcomes
 Applicant Pools with at Least as Many Green Applicants as Total Hires



                                    Dependent Variable: Log(Duration)

                                          (1)                (2)               (3)                (4)

                                          Post-Testing Sample                 Introduction of Testing

                                                                            0.323***          0.262***
 Post-Testing
                                                                            (0.117)           (0.0609)

                                       -0.112***          -0.111***         -0.243**           -0.116
 Exception Rate*Post-Testing
                                       (0.0355)           (0.0303)          (0.0949)          (0.0774)


 N                                      76,425             76,425           250,754            250,754

 Base Controls                             X                 X                  X                 X

 Full Controls                                               X                                    X
 *** p<0.1, ** p<0.05, * p<0.1


Notes:     Columns 1 and 2 estimate the post-testing correlation between manager-level exception rates and

log duration (see Table III). Columns 3 and 4 estimate the dierential impact of testing by location-level

exception rates (see Table IV). Columns 1 and 2 include only hires from applicant pools with at least as many

green applicants as total hires, in the post-testing sample. Columns 3 and 4 add all pre-testing observations.

Base controls include location, hire month, and position xed eects. Full controls add client-by-year eects,

local unemployment rates, and location-specic time trends (and applicant pool controls in columns 1 and

2). In order to identify these controls, we must further restrict this subsample to locations that hire in at

least 2 months in the post-testing period (all but 0.2% of observations).




                                                     32
References
[1] Autor, David and David Scarborough, Does Job Testing Harm Minority Workers?

   Evidence from Retail Establishments, Quarterly Journal of Economics, 123 (2008),

   219-277.


[2] Fernandez, Roberto M., Emilio J. Castilla, and Paul Moore, Social Capital at Work:

   Networks and Employment at a Phone Center, American Journal of Sociology, 105

   (2000), 1288-1356.




                                         33
