                                 NBER WORKING PAPER SERIES




  A NEW CONTROL FUNCTION APPROACH FOR NON-PARAMETRIC REGRESSIONS
                    WITH ENDOGENOUS VARIABLES

                                              Kyoo il Kim
                                              Amil Petrin

                                         Working Paper 16679
                                 http://www.nber.org/papers/w16679


                       NATIONAL BUREAU OF ECONOMIC RESEARCH
                                1050 Massachusetts Avenue
                                  Cambridge, MA 02138
                                      January 2011




This paper previously circulated as "Revisiting Instrumental Variables and the Classic Control Function
Approach, with Implications for Parametric and Non-Parametric Regressions." The views expressed
herein are those of the authors and do not necessarily reflect the views of the National Bureau of Economic
Research.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2011 by Kyoo il Kim and Amil Petrin. All rights reserved. Short sections of text, not to exceed two
paragraphs, may be quoted without explicit permission provided that full credit, including © notice,
is given to the source.
A New Control Function Approach for Non-Parametric Regressions with Endogenous Variables
Kyoo il Kim and Amil Petrin
NBER Working Paper No. 16679
January 2011, Revised April 2013
JEL No. C14

                                              ABSTRACT

When the endogenous variable enters the structural equation non-parametrically the linear Instrumental
Variable (IV) estimator is no longer consistent. Non-parametric IV (NPIV) can be used but it requires
one to impose restrictions during estimation to make the problem well-posed. The non-parametric
control function estimator of Newey, Powell, and Vella (1999) (NPV-CF) is an alternative approach
that uses the residuals from the conditional mean decomposition of the endogenous variable as controls
in the structural equation. While computationally simple identification relies upon independence between
the instruments and the expected value of the structural error conditional on the controls, which is
hard to motivate in many economic settings including estimation of returns to education, production
functions, and demand or supply elasticities. We develop an estimator for non-linear and non-parametric
regressions that maintains the simplicity of the NPV-CF estimator but allows the conditional expectation
of the structural error to depend on both the control variables and the instruments. Our approach combines
the conditional moment restrictions (CMRs) from NPIV with the controls from NPV-CF setting. We
show that the CMRs place shape restrictions on the conditional expectation of the error given instruments
and controls that are sufficient for identification. When sieves are used to approximate both the structural
function and the control function our estimator reduces to a series of Least Squares regressions. Our
monte carlos are based on the economic settings suggested above and illustrate that our new estimator
performs well when the NPV-CF estimator is biased. Our empirical example replicates NPV-CF and
we reject the maintained assumption of the independence of the instruments and the expected value
of the structural error conditional on the controls in their setting.


Kyoo il Kim
Department of Economics
University of Minnesota
4-129 Hanson Hall
1925 4th Street South
Minneapolis, MN 55455
kyookim@umn.edu

Amil Petrin
Department of Economics
University of Minnesota
4-101 Hanson Hall
Minneapolis, MN 55455
and NBER
petrin@umn.edu
1        Introduction

        The problem of endogenous regressors in simultaneous equations models has a long history in
econometrics and empirical studies. The econometric problem is further complicated when the en-
dogenous variables enter non-parametrically in the structural equation because linear Instrumental
Variable approaches are no longer valid.
        There are currently two approaches to identification for this case. The non-parametric instru-
mental variables (NPIV) approach uses conditional moment restrictions (CMRs) to identify the
structural equation (see Newey and Powell (2003), Ai and Chen (2003), Hall and Horowitz (2005),
Blundell, Chen, and Kristensen (2007) and Horowitz (2011a)). The structural equation is the
solution to the integral equation implied by the CMRs. The main challenge arises from the non-
continuity of these estimators with respect to the joint distribution of the data - the ill-posed inverse
problem - and there is a large literature on restrictions that make the problem well-posed (see e.g.
Newey and Powell (2003), Florens (2003), and Darolles, Florens, and Renault (2006)).1
        A second approach is the non-parametric control function estimator of Newey, Powell, and Vella
(1999) (NPV-CF). The approach starts by using conditional mean decomposition to create control
variables that are orthogonal to all of the instruments. For identification they consider the function
that is the expected value of the structural error conditional on the instruments and the control
variables (and thus conditional also on the endogenous variables because of the way the control
variables are constructed). They then assume that this conditional expectation is independent of
the instruments conditional on the control variables. This suffices for identification because the
control function - the expected value of the structural error conditional on the instruments and
controls - is then only a function of the control variables and because the control variables are
measurably separated from the regressors as long as there are valid instruments. This implies that
including (possibly functions of) the control variables conditions out the variations in the endogenous
variables that are correlated with the structural error.
        A weakness of this identification assumption is that it does not hold in some economic settings
where endogeneity is a first-order concern. These include estimation of returns to education, pro-
duction functions, and demand or supply with non-separable reduced forms for equilibrium prices.
We provide some examples in Section 3 to illustrate why this assumption does not hold.
        Our main contribution is to develop an estimator for non-linear and non-parametric regressions
    1
     For example, assuming the structural function belongs to a class of compact functions and using a sieve approach
is one way to make the problem well-posed. This compactness assumption, however, can be relaxed and instead various
regularization methods have been used to stabilize the inversion of the integral equation (see Tikhonov, Goncharsky,
Stepanov, and Yagola (1995), Engl, Hanke, and Neubauer (1996), Kress (1999), Chernozhukov and Hansen (2005),
Hall and Horowitz (2005), Blundell, Chen, and Kristensen (2007), Chen and Pouzo (2009), Chen and Pouzo (2011),
and Darolles, Fan, Florens, and Renault (2011)). Hall and Horowitz (2005) and Chen and Reiss (2011) studied
convergence properties of the nonparametric IV estimators (also see Johannes, Bellegem, and Vanhems (2011) and
Horowitz (2011b)). Horowitz (2007) developed conditions under which the estimator in Hall and Horowitz (2005) is
asymptotically normal.




                                                         2
that is consistent in the case when the conditional expectation of the structural error depends on
both the control variables and the instruments. We also show our estimator is consistent in settings
where both NPIV and NPV-CF estimators are not, although this is not the main focus of the
paper. For identification we combine the CMRs from NPIV and the conditional mean controls from
NPV. Together they imply that the expectation of the structural error conditional on instruments
and control variables can both depend upon instruments and be distinguished from any function of
the endogenous and exogenous regressors, implying identification of the structural function in the
outcome equation. Section 4 discusses identification and provides a set of assumptions that falls
outside the realm of both NPIV and NPV-CF but not our new estimator.
   Our estimator based on this identification result is a multi-step sieve estimator and we develop
                                                                            √
asymptotic properties of this estimator in Sections 5-7. The results include n-asymptotic normality
of linear functionals of the structural function and consistent estimators for their standard errors.
       Our approach shares the strength of the NPV-CF estimator in that it is easy to implement. In
the case of sieve estimation it typically reduces to a series of Least Squares regressions, which makes
point estimation simple and approximation of standard errors possible by bootstrap methods.
       Our estimator uses the same CMR conditions as the NPIV estimators and we therefore face
potentially the same ill-posed inverse problem. Our estimator provides another set of restrictions
that our assumptions together with the use of sieve approximations for both the structural function
and the control function make the problem well-posed. Our proofs for identification, consistency,
and asymptotic normality are all based on extending the proofs from Newey, Powell, and Vella
(1999) - which are quite different from the methods of proof used in the NPIV literature - and we
therefore focus our discussion in the paper almost exclusively on the control function aspects of our
estimator.
       Our monte carlos in Section 8 are motivated by the economic examples we provide in Section 3,
where the NPV-CF identification assumption is violated. They illustrate the ease of implementing
our estimator. They also show that our new estimator performs well while the NPV-CF can be
biased in non-linear settings.
       In Section 9 we return to the empirical application from NPV and show that we reject the
NPV-CF independence assumption when the CMRs are maintained instead. However, in terms of
the economic significance of the violation in this application the point estimates of the coefficients
in the structural function do not change appreciably.
       In Section 10 we conclude. The Appendix explores the implications of our estimator for con-
trol functions in linear models with additively separable errors.2 Other technical details are also
presented in the Appendix.
   2
    For the classic control function approach linear in parameters see, for example, Telser (1964), Hausman (1978),
or Heckman (1978).




                                                        3
2         The Model

         We consider a triangular non-parametric simultaneous equations model with additivity:

                                               xi = Π0 (zi ) + vi ,              E[vi |zi ] = 0                                              (1)
                                               yi = f0 (xi , z1i ) + εi .                                                                    (2)

With zi = (z1i , z2i ) denoting the instrumental variables, (1) is a conditional mean decomposition of
xi with Π0 (zi ) denoting E[xi |zi ] (so E[vi |zi ] = 0 is not restrictive). The unknown function f0 (xi , z1i )
in the second equation is the parameter of interest with z1i a subset of zi . The endogeneity arises
because E[εi |xi ] 6= 0, i.e. the regressors xi are endogenous.
         Control function estimators express yi as a function of (zi , vi )

                                                  yi = f0 (xi , z1i ) + h0 (zi , vi ) + ηi                                                   (3)

where h0 (zi , vi ) = E[εi |zi , vi ] is the control function and ηi = εi − h0 (zi , vi ) (so E[ηi |zi , vi ] = 0).
The control function conditions out the part of the error correlated with the endogenous regressors.
However, without further restrictions on h0 (zi , vi ) the function f0 (xi , z1i ) is not identified because
one cannot separate the effect of (xi , z1i ) on f0 from their impact on h0 .
         Newey, Powell, and Vella (1999) (NPV-CF) achieve identification by assuming that the expected
value of εi conditional on vi is independent of zi ,

                                                         E[εi |zi , vi ] = E[εi |vi ],                                                       (4)

so that h0 (zi , vi ) does not depend on zi given the control vi . From (3) it is clear that this shape
restriction on h0 (z, v) can potentially rule out any additive functional relationship with (x, z1 ). NPV
achieve identification of f0 (xi , z1i ) by combining this restriction with (i) differentiability of f0 (x, z1 ),
h0 (v) = E[ε|v], and Π0 (z), (ii) zero mass on the boundary of the support of (z, v), and (iii) full rank
     ∂Π0 (z)
of    ∂z2 ′    with probability one. Indeed identification holds as long as (x, z1 ) and a control variate
v are measurably separated and imposing NPV’s sufficient conditions is one way to achieve this
separability (see Florens, Heckman, Meghir, and Vytlacil (2008) and their use of Matzkin (2003)’s
control).
         In Section 3 we show it can be hard to motivate the independence assumption in (4) from
primitives in models of demand and supply.3 In contrast conditional moment restrictions are usually
motivated by economic primitives that lead to exclusion restrictions. Our main contribution is to
show that in a control function approach framework, we can identify and consistently estimate
     3
    The NPV-CF estimator is also not robust to conditional heteroskedasticity. For example, suppose εi = σ(zi )ε̃i
where E[ε̃i |zi ] = 0. Then E[εi |zi , vi ] 6= E[εi |vi ] because E[εi |zi , vi ] = σ(zi )E[ε̃i |zi , vi ] = σ(zi )E[ε̃i |vi ], so E[εi |zi , vi ]
cannot be written as a function of vi only.




                                                                        4
f0 (x, z1 ) by restricting h0 (z, v) to satisfy the conditional moment restrictions (CMR)

                                               (CMR)            E[εi |zi ] = 0.

CMR implies that the function h0 (zi , vi ) must satisfy E[h0 (zi , vi )|zi ] = 0 because by the law of
iterated expectations

                            0 = E[εi |zi ] = E[E[εi |zi , vi ]|zi ] = E[h0 (zi , vi )|zi ] = 0.                 (5)

Our main contribution is to show that the shape restriction on h0 (zi , vi ) implied by (5) implies that
h0 (zi , vi ) can both depend upon zi and be distinguished from any function of zi only, which leads to
identification of f0 (xi , z1i ) when combined with a suitable rank condition. Specifically, in Sections
4-7 we show that CMR and a completeness condition in the control function setting is sufficient
for identification and estimation of f0 (xi , z1i ) or any linear functional of it. Before turning to the
formal development of the estimator we illustrate with a simple example.
2.1       Heuristic Example

     While our approach is consistent for much more general functions h0 (zi , vi ), for this example we
suppose h0 (zi , vi ) is given by

                                     h0 (zi , vi ) = ϕ(zi ) + a1 vi + a2 vi2 + a′3 zi vi

where ϕ(zi ) denotes any arbitrary function of zi . We show how we can identify f0 (xi , z1i ) from an
additive regression of yi on (xi , z1i ) and h0 (zi , vi ) when h0 (zi , vi ) satisfies the CMR condition. The
CMR condition implies that h0 (zi , vi ) satisfies

              0 = E[h0 (zi , vi )|zi ] = E[ϕ(zi )|zi ] + a1 E[vi |zi ] + a2 E[vi2 |zi ] + a′3 E[zi vi |zi ]
                  = ϕ(zi ) + a2 E[vi2 |zi ]

because by construction of vi we have E[vi |zi ] = 0. It follows that

               h0 (zi , vi ) = h0 (zi , vi ) − E[h0 (zi , vi )|zi ]
               = (ϕ(zi ) − ϕ(zi )) + a1 vi + a2 (vi2 − E[vi2 |zi ]) + a′3 zi vi = a1 vi + a2 ṽ2i + a′3 zi vi

where ṽ2i = vi2 − E[vi2 |zi ]. Identification in this example is then equivalent to the non-existence of
a linear functional relationship between any functions of (xi , z1i ) and linear functions of vi , ṽ2i , and
zi vi .
2.2       Computation and Testing

     A strength of our non-parametric sieve estimator is that it will typically be a series of least
squares regressions that proceeds in three steps. In the first step we obtain the control v̂i =



                                                               5
xi − Ê[xi |zi ] from the first stage (possibly non-parametric) regression of xi on zi (equation (1)). In
the second step we construct an approximation of h(zi , v̂i ) using (e.g.) polynomials. We directly
impose the restriction E[h(zi , vi )|zi ] = 0 at this point by demeaning (conditional on zi ) each term
in the approximation to h(zi , v̂i ). For example, one non-parametric sieve approximation is given by
                     XL1                                          XL                 X
    h(zi , v̂i ) ≈             al1 ,0 (v̂il1 − E[v̂il1 |zi ]) +                                         al1 ,l2 ϕl2 (zi )(v̂il1 − E[v̂il1 |zi ])
                       l1 =1                                       l=2
                                                                           l1 ≥1,l2 ≥1 s.t. l1 +l2 =l


where ϕl2 (zi ) denotes functions of zi and E[v̂il1 |zi ] are estimated using (possibly non-parametric)
regression.4 We then estimate the parameters of f (xi , z1i ) and h(zi , v̂i ) simultaneously in the final
regression step. In the example above estimation would involve a first regression where an estimate
of vi is recovered followed by a second regression of vi2 on zi to estimate ṽ2i followed by the final
nonparametric regression of yi on (xi , z1i ) with three additive regressors vi , ṽ2i , and zi vi .
        As in other control function approaches (e.g. Smith and Blundell (1986), Rivers and Vuong
(1988), Wooldridge (2005), and Wooldridge and Papke (2008)), we can test for endogeneity of the
regressors xi . In our setting this is equivalent to testing whether the control function h0 (zi , vi )
is a zero function or not. If xi is exogenous it must be that h0 (zi , vi ) = 0 because E[εi |zi , vi ] =
E[εi |zi , xi ] = 0. In terms of the simple example above, testing whether a1 = 0, a2 = 0, and a3 = 0
is equivalent to testing the null hypothesis of exogenous xi .

3        Does E[εi|zi , vi] = E[εi|vi ] Hold in Models of Demand or Supply?

    In this section we examine the control function assumption in demand and supply settings. On
the demand side we consider a buy/not-buy binary choice setting with logit demands and a single-
product monopolist.5 We let the latent utilities of consumers be ui0 = ǫi0 and ui1 = β0 + β1 z1 −
αp + ξ + ǫi1 with (ǫi0, ǫi1 ) i.i.d. extreme value and (z1 , p, ξ) denoting observed characteristics, price,
and the unobserved characteristic (to the econometrician). The market share for good 1 is given by

                                                       exp(β0 + β1 z1 − αp + ξ)
                                               s=                                 ,                                                                (6)
                                                     1 + exp(β0 + β1 z1 − αp + ξ)

which can be linearized as

                                            ln s − ln(1 − s) = β0 + β1 z1 − αp + ξ.

If we let mc(z2 , ω) denote marginal costs given as a function of a cost shifter z2 and a cost shock ω,
    4
        For consistency L1 , L → ∞ and L1 /n, L/n → 0 as n → ∞.
    5
         The idea extends immediately to more general multi-firm and multi-product settings.




                                                                       6
then the monopolist chooses price p to maximize the expected profit such that

                                                         exp(β0 + β1 z1 − αp + ξ)
                       p = arg max (p − mc(z2 , ω))                                 ,
                                   p                   1 + exp(β0 + β1 z1 − αp + ξ)

so E[ξ|p] 6= 0 in the linearized demand equation. Prices are evidently not separable in ξ and
z = (z1 , z2 ). The control is given as v = p − E[p|z]. Since price is not separable in z and ξ
conditioning on v will not generally make ξ independent of z implying E[ξ|z, v] 6= E[ξ|v].
   For the returns to education/production function setting - both of which are about input choices
conditional on productivity - we use the setup from Imbens and Newey (2009) and Florens, Heckman,
Meghir, and Vytlacil (2008). y denotes the outcome (wages/output), x is the agent’s choice variable
(schooling/input), and
                                                y = f (x) + ε.

c(x, z, η) is the cost function where z denotes a cost shifter. The agent sees a noisy signal η of ε,
with η possibly a vector. The agent optimally chooses x by maximizing the expected profit given
the information (z, η) so the observed x is the solution to

                                 x = arg max{E[f (x̃) + ε|z, η] − c(x̃, z, η)},                     (7)
                                           x̃

which leads to the endogeneity problem.
   Assuming differentiability the optimal x solves

                                       ∂f (x)/∂x − ∂c(x, z, η)/∂x = 0.                              (8)

By the implicit function theorem we have x = k(z, η) for some function k(·) and we also know that

                                   ∂x         ∂ 2 c(x, z, η)/∂x∂η
                                      = 2                               .
                                   ∂η  ∂ f (x)/∂x2 − ∂ 2 c(x, z, η)/∂x2

Since the derivative of x with respect to η depends on z, z and η are not additively separable in
x = k(z, η). With v = x − E[x|z] this implies that ε conditional on v is not generally independent
of z (so E[ε|z, v] 6= E[ε|v]).
   We illustrate further by considering the special case when

                                                        1
                                         y = ϕ0 + ϕ1 x + ϕ2 x2 + ε
                                                        2

for parameters (ϕ0 , ϕ1 , ϕ2 ) and

                                                                     1
                            c(x, z, η) = c0 (z, η0 ) + c1 (z, η1 )x + c2 (z, η2 )x2 ,
                                                                     2

for cost shocks η = (η0 , η1 , η2 ) known to the agent and possibly correlated with ε (and z independent




                                                       7
of ε and η). The optimal x is
                                                          ϕ1 − c1 (z, η1 )
                                                   x=                                                                    (9)
                                                          c2 (z, η2 ) − ϕ2
which is not additively separable in η1 or η2 , which means E[ε|z, v] is not generally equal to E[ε|v].6

4        Identification

        We ask whether f0 (xi , z1i ) and h0 (zi , vi ) are identified by equation (3) with restrictions (5). Our
approach to identification closely follows Newey, Powell, and Vella (1999) and Newey and Powell
(2003). We consider pairs of functions f (xi , z1i ) and h(zi , vi ) that satisfy

                                          E[yi |zi , vi ] = f (xi , z1i ) + h(zi , vi )                                (10)

and the CMR condition. Because conditional expectations are unique with probability one, if there
exists a pair f¯(xi , z1i ) and h̄(zi , vi ) that satisfies (10), it must be that

                             Pr(f0 (xi , z1i ) + h0 (zi , vi ) = f¯(xi , z1i ) + h̄(zi , vi )) = 1.                    (11)

Identification of f0 and h0 means we must have f0 = f¯ and h0 = h̄ with probability one whenever
(11) holds. Working with differences, we let δ(xi , z1i ) = f0 (xi , z1i ) − f¯(xi , z1i ) and κ(zi , vi ) =
h0 (zi , vi ) − h̄(zi , vi ). Identification of f0 and h0 is then equivalent to

               Pr(δ(xi , z1i ) + κ(zi , vi ) = 0) = 1 implying Pr(δ(xi , z1i ) = 0, κ(zi , vi ) = 0) = 1.

Theorem 1. Assume (1-2) and (5). If for all δ(xi , z1i ) with finite expectation E[δ(xi , z1i )|zi ] = 0
implies δ(xi , z1i ) = 0 a.s. then f0 (xi , z1i ) and h0 (zi , vi ) are identified.

Proof. Suppose it is not identified. Then there must exist functions f¯(xi , z1i ) and h̄(zi , vi ) such that
δ(xi , z1i ) 6= 0 and κ(zi , vi ) 6= 0 but Pr(δ(xi , z1i ) + κ(zi , vi ) = 0) = 1. It follows that E[κ(zi , vi )|zi ] =
0 by construction so 0 = E[δ(xi , z1i ) + κ(zi , vi )|zi ] = E[δ(xi , z1i )|zi ]. E[δ(xi , z1i )|zi ] = 0 then
implies δ(xi , z1i ) = 0 a.s., so δ(xi , z1i ) = 0 and κ(zi , vi ) = 0 with probability one. This is a
contradiction. The result then implies that h0 (zi , vi ) is also identified because the conditional
expectation E[yi |zi , vi ] is nonparametrically identified and h0 (zi , vi ) = E[yi |zi , vi ] − f0 (xi , z1i ).

        A sufficient condition for identification is that the conditional distribution of xi given zi satisfies
the completeness condition (see Newey and Powell (2003) or Hall and Horowitz (2005)), which
    6
       Sufficient conditions for E[ε|z, v] = E[ε|v] are that c1 (z, η1 ) = c1z (z) + η1 and c2 (z, η2 ) is constant, in which
case v = − c2η−ϕ 1
                   2
                     . The economic implication is that the linear cost coefficient must be separable in z and η1 and the
quadratic cost coefficient cannot depend on z nor cost shocks.
   Also even when we use the Matzkin (2003)’s control v ∗ = Fx|z , the conditional CDF of x given z as in Florens,
Heckman, Meghir, and Vytlacil (2008), the condition E[ε|z, v ∗ ] = E[ε|v ∗ ] does not hold in general unless we restrict
c1 (z, η1 ) and c2 (z, η2 ).




                                                               8
assumes that E[δ(xi , z1i )|zi ] = 0 implies δ(xi , z1i ) = 0 for any δ(xi , z1i ) with finite expectation.7
To illustrate the implication for a parametric setting we let f0 (xi , z1i ) = β0′ xi + β10        ′ z . Then an
                                                                                                       1i
                        ¯
alternative function is f (xi , z1i ) = β̄ xi + β̄ z1i 6= β xi +β z1i , so E[δ(xi , z1i )|zi ] = (β0 − β̄)′ E[xi |zi ]+
                                          ′       ′        ′     ′
                                                     1           0          10
(β10 − β̄1 )′ z1i . If zi satisfies the standard rank condition - e.g. it includes excluded instruments
from z1i that are correlated with xi - then E[δ(xi , z1i )|zi ] = 0 implies δ(xi , z1i ) = 0, so β0 = β̄ and
β10 = β̄1 .



4.1     Generalization of NPIV and NPV-CF

    While it is not the focus of this paper, there are sets of assumptions under which neither NPV-
CF nor NPIV yields identification but our approach does yield identification. Let zi = (z1i , z2i ) and
consider a setting where

                                  E[εi |z1i ] = 0 and E[εi |zi , vi ] = E[εi |z1i , vi ].                                  (12)

In this case E[εi |z2i ] 6= 0 so NPIV is not consistent. Also, since E[εi |zi , vi ] = E[εi |z1i , vi ], even
though εi is independent of z2i conditional on vi it is not independent of z1i , meaning E[εi |zi , vi ] 6=
E[εi |vi ]. Thus NPV-CF is also not consistent.8
    In order to see that our approach to identification works let h(z1i , vi ) = E[εi |z1i , vi ]. Then under
(12) the new expression for (10) combined with the CMRs yields

                                           E[yi |zi , vi ] = f (xi , z1i ) + h(z1i , vi )                                  (13)
                                     E[h(z1i , vi )|z1i ] = 0.

For identification it must be that if both (f0 , h0 ) and (f¯, h̄) satisfy (13),

                             Pr(f0 (xi , z1i ) = f¯(xi , z1i ), h0 (z1i , vi ) = h̄(z1i , vi )) = 1.

We show (f0 , h0 ) are identified under (1-2) and (12) next, and Section 5.1 develops the estimator
for this case.
                                                                     ∂Π0 (zi )
Theorem 2. Assume (1-2) and (12). Suppose                                ′
                                                                      ∂z2i       has the full rank (i.e. rank( ∂Π∂z0 (z
                                                                                                                     ′
                                                                                                                        i)
                                                                                                                           ) =
                                                                                                                      2i
dim(xi )) with probability one. Then f0 (xi , z1i ) and h(z1i , vi ) are identified.

Proof. We prove by contradiction. Suppose it is not identified. Then there must exist functions
f¯(xi , z1i ) and h̄(z1i , vi ) such that δ(xi , z1i ) = f0 (xi , z1i ) − f¯(xi , z1i ) 6= 0 and κ(z1i , vi ) = h0 (z1i , vi ) −
h̄(z1i , vi ) 6= 0 but δ(xi , z1i ) + κ(z1i , vi ) = 0 with probability one (wp1). By taking derivatives w.r.t.
                                                          ∂δ(xi ,z1i )
z2i to δ(xi , z1i ) + κ(z1i , vi ) = 0, we obtain           ∂z2i         = 0 wp1 because κ(z1i , vi ) is not a function
   7
     The completeness condition is the nonparametric analog of the rank condition for identification in the linear
setting.
   8
     This last assumption allows, for example, the error εi to be conditionally heteroskedastic in z1i .




                                                                9
                                     ∂δ(xi ,z1i )       ∂δ(xi ,z1i ) ∂xi        ∂δ(xi ,z1i ) ∂Π0 (zi )         ∂Π0 (zi )
of z2i . Then because 0 =                 ′
                                       ∂z2i         =      ∂x′i        ′
                                                                     ∂z2i   =      ∂x′i          ′
                                                                                              ∂z2i       and       ′
                                                                                                                ∂z2i       has the full rank,
                 ∂δ(xi ,z1i )
we also find        ∂xi         = 0 wp1. Therefore δ(xi , z1i ) must be a function of z1i only wp1. We then
find 0 = E[δ(xi , z1i )|z1i ] + E[κ(z1i , vi )|z1i ] = δ(xi , z1i ) because E[κ(z1i , vi )|z1i ] = 0 by construction
of h0 (z1i , vi ) and h̄(z1i , vi ). We then conclude δ(xi , z1i ) = 0 and κ(z1i , vi ) = 0 wp1, which is a
contradiction.




5        Estimation

        Our estimator is obtained in three steps. We focus on sieve estimation because it is convenient
to impose the restriction (5). We use capital letters to denote random variables and lower case
letters to denote their realizations. We assume the tuple {(Yi , Xi , Zi )} for i = 1, . . . , n are i.i.d. We
let Xi be dx × 1, Z1i be d1 ×1, Z2i be d2 ×1, dz = d1 + d2 and d = dz + dx , with dx = 1 for ease
of exposition. Let {pj (Z), j = 1, 2, . . .} denote a sequence of approximating basis functions (e.g.
orthonormal polynomials or splines). Let pkn = (p1 (Z), . . . , pkn (Z))′ , P = (pkn (Z1 ), . . . , pkn (Zn ))′ ,
and (P ′ P )− denote the Moore-Penrose generalized inverse, where kn tends to infinity but kn /n →
0. Similarly we let {φj (X, Z1 ), j = 1, 2, . . .} denote a sequence of approximating basis functions,
φKn = (φ1 (X, Z1 ), . . . , φKn (X, Z1 ))′ , where Kn tends to infinity but Kn /n → 0.9
        In the first step to estimate the controls we estimate Π0 (z) using
                                                                                Xn
                                          Π̂(z) = pkn (z)′ (P ′ P )−                     pkn (zi )xi
                                                                                   i=1


and obtain the control variable as v̂i = xi − Π̂(zi ).
        In the second step we construct approximating basis functions using v̂ and z, where we impose
the CMR condition (5) by subtracting out the conditional means (conditional on Z). We start by
assuming v is known and then show how the setup changes when v̂ replaces v. We write basis
functions when v is known as
                                                    ϕ̃l (z, v) = ϕl (z, v) − ϕ̄l (z)

where ϕ̄l (z) = E[ϕl (Z, V )|Z = z] and {ϕl (z, v), l = 1, 2, . . .} denotes a sequence of approximating
basis functions generated using (z, v) ∈ supp(Z, V ) ≡ W, the support of (Z, V ). We let H denote a
space of functions that includes h0 , and we let k·kH be a pseudo-metric on H. We define the sieve
space Hn as the collection of functions
                                                    X
                                Hn = {h : h =                   al ϕ̃l (z, v), khkH < C̄h , (z, v) ∈ W}
                                                         l≤Ln


for some bounded positive constant C̄h , with Ln → ∞ so that Hn ⊆ Hn+1 ⊆ . . . ⊆ H (and
Ln /n → 0).
    9                                                                                                                          √
    We state specific rate conditions in the next section for our convergence rate results and also for                            n-consistency
and asymptotic normality of linear functionals.


                                                                      10
   Because v is not known we use instead estimates of the approximating basis functions, which
we denote as ϕ̃ˆl (z, v̂) = ϕl (z, v̂) − ϕ̄ˆl (z), where ϕ̄ˆl (z) = Ê[ϕl (Z, V̂ )|Z = z]. We then construct the
approximation of h(z, v) as           10


                                      XLn
                ĥLn (z, v̂) =                     al {ϕl (z, v̂) − Ê[ϕl (Z, V̂ )|Z = z]}                                (14)
                                             l=1
                                      XLn                                              Xn
                                =                  al {ϕl (z, v̂) − pkn (z)′ (P ′ P )−     pkn (zi )ϕl (zi , v̂i )},
                                             l=1                                             i=1

with coefficients, (a1 , . . . , aLn ) to be estimated in the last step. We approximate the sieve space Hn
with Ĥn using (14), so Ĥn is given by
                                                    X
                            Ĥn = {h : h =                        al ϕ̃ˆl (z, v̂), khkH < C̄h , (z, v̂) ∈ W}.             (15)
                                                           l≤Ln

       In the last step we define F as the space of functions that includes f0 , and we let k·kF be a
pseudo-metric on F. We define the sieve space Fn as the collection of functions
                                           X
                    Fn = {f : f =                         βl φl (x, z1 ), kf kF < C̄f , (x, z1 ) ∈ supp(X, Z1 )}
                                               l≤Kn


for some bounded positive constant C̄f , with Kn → ∞ so that Fn ⊆ Fn+1 ⊆ . . . ⊆ F (and
Kn /n → 0). Then our multi-step sieve estimator is obtained by solving
                                                                   n
                                                                   X
                          ˆ ĥ) = arginf
                         (f,                                         {yi − (f (xi , z1i ) + h(zi , v̂i ))}2 /n            (16)
                                         (f,h)∈Fn ×Ĥn
                                                                   i=1


where v̂i = xi − Π̂(zi ).
       Equivalently we can write the last step estimation as
                                             Xn                 X Kn                     XLn
            min(β1 ,...,βKn ,a1 ,...,aLn )               {yi − (     βk φk (xi , z1i ) +     al ϕ̃ˆl (zi , v̂i ))}2 /n.   (17)
                                                   i=1                k=1                          l=1

With fixed kn , Ln , and Kn our estimator is just a three-stage least squares estimator. Once we
                       ˆ ĥ) we can also estimate linear functionals of (f0 , h0 ) using plug-in methods
obtain the estimates (f,
(see Section 7).
5.1      Estimation of Model (12)-(13)

       To estimate the model (12)-(13) we have only to replace the control function with h(z1 , v) and
  10
    We can use different sieves (e.g., power series, splines of different lengths) to approximate E[ϕl (Z, V )|Z = z]
and Π(z) depending on their smoothness, but we assume one uses the same sieves for notational simplicity.




                                                                         11
its approximation with
                                  XLn
             ĥLn (z1 , v̂) =                al ϕ̃ˆl (z1i , v̂i )
                                   l=1
                                  XLn
                             =               al {ϕl (z1 , v̂) − Ê[ϕl (Z1 , V̂ )|Z1 = z1 ]}
                                   l=1
                                  XLn                                                  Xn
                             =               al {ϕl (z1 , v̂) − pk1n (z1 )′ (P1′ P1 )−      pk1n (z1i )ϕl (z1i , v̂i )}
                                       l=1                                                i=1


where pk1n (z1 ) = (p1 (z1 ), . . . , pkn (z1 ))′ and P1 = (pk1n (z11 ), . . . , pk1n (z1n ))′ , so we demean the approx-
imating functions ϕl (z1 , v̂) w.r.t. z1 only. We then estimate the function f together with the control
function as
                                                   Xn                 X Kn                     XLn
    (β̂, â) = argmin(β1 ,...,βKn ,a1 ,...,aLn )               {yi − (     βk φk (xi , z1i ) +     al ϕ̃ˆl (z1i , v̂i ))}2 /n
                                                         i=1              k=1                        l=1

              P n                                       PLn
such that fˆ = K
               k=1 β̂k φk (xi , z1i ) and ĥ(z1 , v̂) =
                                                                  ˆ
                                                         l=1 âl ϕ̃l (z1i , v̂i ).
    In the following Sections 6-7 we develop asymptotic properties of the estimator for the model
(1-2) and (5). Similar results can be obtained for the estimator of the model (12)-(13) with minor
modifications.

6     Convergence Rates

    We obtain the convergence rates building on Newey, Powell, and Vella (1999). We differ from
their approach as we have another nonparametric estimation stage in the middle step of estimation
that arises due to our identification approach different from NPV. This creates additional terms in
the convergence rate results.
    We introduce additional notation. We write g0 (zi , vi ) ≡ g0 (xi , z1i ∪zi , vi ) = f0 (xi , z1i )+h0 (zi , vi )
for ease of notation. For a random matrix D, let kDk = (tr(D ′ D))1/2 , and let kDk∞ be the infimum
of constants C such that Pr(||D|| < C) = 1. We derive the convergence rates of the nonparametric
                                                                               √
estimator ĝ = fˆ + ĥ to g0 and fˆ to f0 only for the purpose of obtaining the n-consistency and the
asymptotic normality of the linear functional estimators of g0 or f0 . Below Assumptions C1 and
C2 along with identification of (f0 , h0 ) as shown in Section 4 ensure the rate results we derive.
Assumption 1 (C1). (i) {(Yi , Xi , Zi )}ni=1 are i.i.d., V = X − E[X|Z], and var(X|Z), var(Y |Z, V ),
and var(ϕl (Z, V )|Z) for all l are bounded; (ii) (Z, X) are continuously distributed with densities that
are bounded away from zero on their supports, which are compact and the distribution of X condi-
tional on Z satisfies the completeness condition; (iii) Π0 (z) is continuously differentiable of order
s1 and all the derivatives of order s1 are bounded on the support of Z; (iv) ϕ̄l (Z) is continuously
differentiable of order s2 and all the derivatives of order s2 are bounded for all l on the support of
Z; (v) h0 (Z, V ) is Lipschitz and is continuously differentiable of order s and all the derivatives of
order s are bounded on the support of (Z, V ); (vi) ϕl (Z, V ) is Lipschitz and is twice continuously
differentiable in v and its first and second derivatives are bounded for all l; (vii) f0 (X, Z1 ) is con-
tinuously differentiable of order s and all the derivatives of order s are bounded on the support of
(X, Z1 ).


                                                                     12
         Assumptions C1 (iii), (iv), (v), and (vii) ensure that the unknown functions Π0 (Z), ϕ̄l (Z),
h0 (Z, V ), and f0 (X, Z1 ) belong to a Hölder class of functions, so they can be approximated up to
                         −s1 /dz          −s2 /dz         −s/d                  −s/(dx +d1 )
the orders of O(kn                 ), O(kn          ), O(Ln      ), and O(Kn                   ) respectively when polynomials
or splines approximation is used (see Timan (1963), Schumaker (1981), Newey (1997), and Chen
(2007)). Assumption C1 (vi) is satisfied for polynomial and spline basis functions with appropriate
orders. Assumptions C1 (i)-(ii) are about the structure of the data. Assumption C1 (ii) includes
the completeness condition for identification and other conditions can be relaxed with additional
complexity (e.g., a trimming device as in Newey, Powell, and Vella (1999)). Assumption C1 (v)
and (vii) maintain that f0 and h0 have the same order of smoothness for ease of notation, but it is
possible to allow them to differ.
         Next we impose the rate conditions that restrict the growth of kn , Kn , and Ln as n tends to
infinity. We write Ln = Kn + Ln .
                                      1/2 √       −s /d            1/2 √     −s /d
Assumption 2 (C2). Let △n,1 = kn / n + kn 1 z , △n,2 = kn / n + kn 2 z , and △n =
                                                   1/2      1/2 3/2 √    1/2
max{△n,1 , △n,2 }. For polynomial approximations Ln (L3n +Ln kn / n+Ln )△n → 0, L3n /n → 0
                                           1/2   3/2    1/2   √      1/2
and kn3 /n → 0. For spline approximations Ln (Ln + Ln kn / n + Ln )△n → 0 , L2n /n → 0 and
kn2 /n → 0.
                                                                        Pdim(w)                                     ∂ |µ| c(w)
         For any differentiable function c(w), let |µ| =                  j=1      µj and define ∂ µ c(w) =      ∂w1 ···∂wdim(w) .
Also for integer δ define |c(w)|δ = max|µ|≤δ supw∈supp(w) ||∂ µ c(w)|| with |c(w)|0 = supw∈supp(w) ||c(w)||.
Also let φK (x, z1 ) ≡ (φ1 (x, z1 ), . . . , φK (x, z1 ))′ .

Theorem 3. Suppose Assumptions C1-C2 are satisfied. Then
      Z                                              p
 (a) ( [ĝ(z, v) − g0 (z, v)]2 dµ0 (z, v))1/2 = Op ( Ln /n + Ln △n + Ln−s/d ), and
                                                              p
               (b) |fˆ(x, z1 ) − f0 (x, z1 )|δ = Op (ζδ (Kn )[ Ln /n + Ln △n + L−s/d
                                                                                n    ] + Kn−s/(dx +d1 ) )

where µ0 (z, v) denotes the distribution function of (z, v) and |φK (x, z1 )|δ ≤ ζδ (K).

         In Theorem 3 the term Ln △n arises because of the estimation error from the first and sec-
ond steps of estimation. With no estimation error from these stages we would obtain standard
convergence rates of series estimators.



7         Asymptotic Normality

         Following Newey (1997) and Newey, Powell, and Vella (1999) we consider inference for the
linear functionals of g, θ = α(g).11 The estimator θ̂ = α(ĝ) of θ0 = α(g0 ) is a well-defined “plug-in”
estimator, and because of the linearity of α(g) we have

                                   θ̂ = Aβ̂, A = (α(φ1 ), . . . , α(φKn ), α(ϕ̃1 ), . . . , α(ϕ̃Ln ))
    11
         This includes θ’s being functionals of f only.


                                                                   13
where we let β̂ = (β̂1 , . . . , β̂Kn , â1 , . . . , âLn )′ that solves (17). This setup includes (e.g.) partially
linear models, where f contains some parametric components, and the weighted average derivative,
where one estimates the average response of y with respect to the marginal change of x or z1 .
For example if f is partially linear, then each row vector of A only consists of ones and zeros
such that ones select particular parameters from the parametric components. More generally, if A
                                                                                                                   ˆ
depends on unknown population objects, we can estimate it using Â = ∂α(ψ̂iL′ β)/∂β ′ |β=β̂ where
 ˆ
ψ̂iL = (φ1 (xi , z1i ), . . . , φK (xi , z1i ), ϕ̃ˆ1 (zi , v̂i ), . . . , ϕ̃ˆL (zi , v̂i ))′ , so that θ̂ = Âβ̂ (see Newey (1997)).
                                                                     √
     We focus on conditions that provide for n-asymptotics and allow for a straightforward con-
sistent estimator for the standard errors of θ̂.12 If there exists a Riesz representer ν ∗ (Z, V ) such
that
                                                  α(g) = E[ν ∗ (Z, V )g(Z, V )]                                               (18)

for any g = (f, h) ∈ F × H that can be approximated by power series or splines in the mean-squared
                           √
norm, then we can obtain n-consistency and asymptotic normality for θ̂, expressed as
                                                   √
                                                       n(θ̂ − θ0 ) →d N (0, Ω),

for some asymptotic variance matrix Ω. In Assumption C1 we take both F and H as Hölder spaces
of functions, which ensures the approximation of g in the mean-squared norm (see e.g., Newey
(1997), Newey, Powell, and Vella (1999), and Chen (2007)). Defining ρv (Z) = E[ν ∗ (Z, V )( ∂h0∂V
                                                                                               (Z,V )
                                                                                                      −
      (Z,V )
E[ ∂h0∂V     |Z])|Z] and ρϕ̄l (Z) = E[al ν ∗ (Z, V )|Z], the asymptotic variance of the estimator θ̂ is given
by

                     Ω = E[ν ∗ (Z, V )var(Y |Z, V )ν ∗ (Z, V )′ ] + E[ρv (Z)var(X|Z)ρv (Z)′ ]                                 (19)
                                         Ln
                                         X
                               + lim            E[ρϕ̄l (Z)var(ϕl (Z, V )|Z)ρϕ̄l (Z)′ ].
                                  n→∞
                                          l=1

The first term in the variance accounts for the final stage of estimation, the second term accounts for
the estimation of the control (v), and the last term accounts for the middle step of the estimation.
       Assumptions C1, R1, N1, and N2 below are sufficient to characterize the asymptotic normal-
ity of θ̂ and also a consistent estimator for the asymptotic variance of θ̂. Below let ϕ̃L (z, v) ≡
(ϕ̃1 (z, v), . . . , ϕ̃L (z, v))′ and ψ L (zi , vi ) ≡ (φK (x, z1 )′ , ϕ̃L (z, v)′ )′ .


Assumption 3 (R1). There exist ν ∗ (Z, V ) and βL such that E[||ν ∗ (Z, V )||2 ] < ∞, α(g0 ) =
E[ν ∗ (Z, V )g0 (Z, V )], α(φk ) = E[ν ∗ (Z, V )φk ] for k = 1, . . . , K, α(ϕ̃l ) = E[ν ∗ (Z, V )ϕ̃l ] for l =
1, . . . , L, and E[||ν ∗ (Z, V ) − ψ L (Z, V )′ βL ||2 ] → 0 as L → ∞.
                                                                                                                √
       Assumption R1 restricts the class of linear functionals we consider that yield the                           n-consisteny
and also requires ν ∗ (Z, V ) be well approximated by the approximating basis functions we use to
  12                                                                                √
    Developing the asymptotic distributions of the functionals that do not yield the n-consistency is also possible
based on the convergence rates result we obtained and alternative assumptions on the functionals of interest (see
Newey, Powell, and Vella (1999)).


                                                                  14
approximate g0 . To present the theorem, we need additional notation and assumptions. Let aL =
(a1 , . . . , aL )′ with an abuse of notation.

Assumption 4 (N1). (i) there exist δ, γ, and βL such that |g0 (z, v) − βL′ ψ L (z, v)|δ ≤ CL−γ
(which also implies |h0 (z, v) − a′L ϕ̃L (z, v)|δ ≤ CL−γ ); (ii) var(Yi |Zi , Vi ) is bounded away from
zero, E[ηi4 |Zi , Vi ] and E[Vi4 |Zi ] are bounded and E[ϕ̃l (Zi , Vi )4 |Zi ] is bounded for all l.

      Assumption N1 (i) is satisfied for f0 and h0 that belong to the Hölder class and then we can
take (e.g.) γ = s/d. The bounded conditional fourth moments are imposed to apply for appropriate
central limit theorems. Next we impose the rate conditions that restrict the growth of kn and
Ln = Kn + Ln as n tends to infinity.
                                 1/2 √    −s /d         1/2 √    −s /d
Assumption 5 (N2). Let △n,1 = kn / n + kn 1 z , △n,2 = kn / n + kn 2 z , and △n =
                  √ −s /d      √ −s /d      √ 1/2 −s/d     √ −s/d
max{△n,1 , △n,2 }. nkn 1 z → 0, nkn 2 z → 0, nkn Ln    → 0, nLn   → 0 and they are
                                                                                                        1/2      3/2       5/2
                                                                L2n +Ln L3n kn +Ln (L4n kn +kn )
sufficiently small. For the polynomial approximations                            √
                                                                                   n
                                                                                                                                 → 0 and for the
                                   3/2    3/2 1/2 1/2   5/2 3/2     3/2 3/2
                                  Ln +Ln Ln kn +Ln (Ln kn +kn )+Ln kn
spline approximations                               √
                                                      n
                                                                             → 0.

Theorem 4. Suppose Assumptions C1, R1, and N1-N2 are satisfied. Then
                                                           √
                                                               n(θ̂ − θ0 ) →d N (0, Ω).

     Based on this asymptotic distribution, one can construct the confidence intervals of θ0 and cal-
culate standard errors in a straightforward manner. Let ĝ(zi , v̂i ) = fˆ(xi , z1i ) + ĥ(zi , v̂i ) and ĝi =
                       ˆ
ĝ(zi , v̂i ). Define ψ̂iL = (φ1 (xi , z1i ), . . . , φK (xi , z1i ), ϕ̃ˆL (zi , v̂i )′ )′ where ϕ̃ˆL (zi , vi ) = (ϕ̃ˆ1 (zi , vi ), . . . , ϕ̃ˆL (zi , vi ))′ .
Let


                     Xn          ˆ ˆ                Xn                        ˆ ˆ
         T̂    =                ψ̂iL ψ̂iL′ /n, Σ̂ =    (yi − ĝ(zi , v̂i ))2 ψ̂iL ψ̂iL′ /n                                                         (20)
                          i=1                                i=1
                                           n
                                           X                                           n
                                                                                       X
        T̂1 = P ′ P/n, Σ̂1 =                      v̂i2 pk (zi )pk (zi )′ /n, Σ̂2,l =         {ϕl (zi , v̂i ) − ϕ̄ˆl (zi )}2 pk (zi )pk (zi )′ /n
                                            i=1                                        i=1
                     n X
                     X L
                             ∂ϕl (zi , v̂i ) ˆL k
      Ĥ11 =                    âl         ψ̂i p (zi )′ /n,
                                 ∂vi
                 i=1 l=1
                  n                          n            P
                 X
                      k      ′    ′    −
                                           X
                                                 k      ∂ L  l=1 âl ϕl (zj , v̂j ) ˆL k
      Ĥ12     =     p (zi ) ((P P )            p (zj )                            )ψ̂i p (zi )′ /n,
                                                                 ∂vj
                 i=1                       j=1
                 Xn          ˆL k
      Ĥ2,l    =        âl ψ̂i p (zi )′ /n, Ĥ1 = Ĥ11 − Ĥ12 .
                          i=1

Then, we can estimate Ω consistently by
                                                                               XLn                                    
                   Ω̂ = AT̂     −1
                                          Σ̂ +   Ĥ1 T̂1−1 Σ̂1 T̂1−1 Ĥ1′   +        Ĥ2,l T̂1−1 Σ̂2,l T̂1−1 Ĥ2,l
                                                                                                               ′
                                                                                                                           T̂ −1 A′ .              (21)
                                                                                 l=1


Theorem 5. Suppose Assumptions C1, R1, and N1-N2 are satisfied. Then Ω̂ →p Ω.

                                                                            15
       This Ω̂ is the heteroskedasticity robust variance estimator that accounts for the first and second
steps of estimation. The first variance term AT̂ −1 Σ̂T̂ −1 A′ corresponds to the variance estimator
without error from the first and second steps of estimation. The second variance term accounts
for the estimation of v (and corresponds to the second term in (19)). The third variance term
accounts for the estimation of ϕ̄l (·)’s (and corresponds to the third term in (19)). If we view our
model as a parametric one with fixed kn , Kn , and Ln , the same variance estimator Ω̂ can be used as
the estimator of the variance for the parametric model (e.g, Newey (1984) and Murphy and Topel
(1985)).



7.1       Discussion

       We discuss Assumption R1 for the partially linear model and the weighted average derivative.
Consider a partially linear model of the form

                                            f0 (x, z1 ) = x′1 β10 + f20 (x−1 , z1 )

where x can be multi-dimensional and x1 is a subvector of x such that x = (x1 , x−1 ). Then we have

                                          β10 = α(g0 ) = E[ν ∗ (Z, V )g0 (Z, V )]

where ν ∗ (z, v) = (E[q(Z, V )q(Z, V )′ ])−1 q(z, v) and q(z, v) is the residual from the mean-square
projection of x1 on the space of functions that are additive in functions of (x−1 , z1 ) and any h(z, v)
such that E[h(Z, V )|Z] = 0.13 Thus we can approximate q(z, v) by the mean-square projection
                   L (z , v ) ≡ (φ (x
residual of x1 on ψ−1                                                        L          ′ ′
                       i i        1 −1i , z1i ), . . . , φK (x−1i , z1i ), ϕ̃ (zi , vi ) ) , and then use these
estimates to approximate ν ∗ (z, v).
       Next consider a weighted average derivative of the form
                       Z                                              Z
                                            ∂g0 (z, v)                                         ∂f0 (x, z1 )
             α(g0 ) =    ̟(x, z1 , κ(z, v))            d(z, v) =          ̟(x, z1 , κ(z, v))                d(z, v)
                      W̄                       ∂x                                                  ∂x

where the weight function ̟(x, z1 , κ(z, v)) puts zero weights outside W̄ ⊂ W and κ(z, v) is some
function such that E[κ(Z, V )|Z] = 0. This is a linear functional of g0 . Integration by parts shows
that
                      Z
                                               ∂̟(x, z1 , κ(z, v))
         α(g0 ) = −         proj(µ0 (z, v)−1                       |S)g0 (z, v)dµ0 (z, v) = E[ν ∗ (Z, V )g(Z, V )]
                       W̄                             ∂x

where proj(·|S) denotes the mean-square projection on the space of functions that are additive
in functions of (x, z1 ) and any h(z, v) such that E[h(Z, V )|Z] = 0 (so the Riesz representer
ν ∗ (z, v) is well-defined), and ν ∗ (z, v) = −proj(µ0 (z, v)−1 ∂̟(x,z∂x
                                                                      1 ,κ(z,v))
                                                                                 |S) with µ0 (z, v) denoting
the distribution of (z, v). We can then approximate ν ∗ (z, v) using a mean-square projection of
  13
       Note that existence of the Riesz representer in this setting requires E[q(Z, V )q(Z, V )′ ] to be nonsingular.


                                                              16
µ0 (z, v)−1 ∂̟(x,z∂x
                  1 ,κ(z,v))
                             on ψ L (z, v).

8         Simulation Study

         We conduct two monte carlos simulations to evaluate the performance of the NPV-CF estima-
tor and our CMR-CF estimator. The first set of monte carlos is based on the economic examples
provided in Section 3 where the structural function f (x) is parametric and the second set uses a
nonlinear setup from Newey and Powell (2003) where the structural function is estimated nonpara-
metrically.
8.1         Monte Carlos Based on Parametric Models

   In the first set we consider six models. The outcome equations are parametric so f (x) is known
up to a finite set of parameters. The selection equations are treated as unknown to the practitioner
and we use nonparametric regressions for them in the simulation.
         The six designs are given as:

                    [1] yi = α + βxi + γx2i + εi ; xi = zi + (3εi + ςi ) · log(zi )
                    [2] yi = α + βxi + γx2i + εi ; xi = zi + (3εi + ςi )/ exp(zi )
                    [3] yi = α + βxi + γ log xi + εi ; xi = zi + (3εi + ςi )/ exp(zi )
                    [4] yi = α + βxi + γ log xi + εi ; xi = zi + (3εi + ςi + εi · ςi )/ exp(zi )
                    [5] yi = α + βxi + εi ; xi = zi + (3εi + ςi )/ exp(zi )
                    [6] yi = α + βxi + γx2i + εi ; xi = zi + (3εi + ςi ).

These designs can be obtained from the underlying decision problem of (7) by varying the structural
function f (x) and the cost function c(x, z, η).14
         We generate simulation data based on the following distributions: εi ∼ Uε , ςi ∼ Uς , zi =
2 + 2Uz , where each Uε , Uς , and Uz independently follows the uniform distribution supported on
[−1/2, 1/2] so all three random variables εi , ςi , and zi are independent of one another. In all designs
xi is correlated with εi and the CMR condition, E[εi |zi ] = 0 holds. The NPV-CF restriction
(4) is violated in designs [1]-[5] and holds in design [6].15 We set the true parameter values at
(α0 , β0 , γ0 ) = (1, 1, −1) and the data is generated with the sample size of n = 1, 000.
         All of the estimators are based on a first stage estimation residual v̂i = xi − (π̂0 + π̂1 zi + π̂2 zi2 )
although estimates are robust to adding higher order terms.16 The structural function f (xi ) is given
    14

  For example we obtain design [1] by letting c2 (z, η2 ) be constant and c1 (z, η1 ) include the leading term z and the
interaction term η1 log(z), where η1 = 3ε+ς is a noisy signal of ε. The selection equation (9) is then x = cϕ21(z,η
                                                                                                                −c1 (z,η1 )
                                                                                                                   2 )−ϕ2
                                                                                                                            =
z + (3ε + ς) · log(z). The other designs can be derived in a similar way.

    15
     For example, in design [2] we have vi = xi − E[xi |zi ] = (3εi + ςi )/ exp(zi ). Then we have εi = (exp(zi )vi − ςi )/3
and therefore E[εi |zi , vi ] = (exp(zi )vi − E[ςi |zi , vi ])/3, and this cannot be written as a function of vi only.
  16
     Root mean-squared errors were similar across all estimators whether we used two or more higher order terms.
Thus if we followed Newey, Powell, and Vella (1999) and used cross validation (CV) to discriminate between alternative



                                                             17
by the designs [1]-[6]. The classic control function (CCF) approach just includes the control in an
additive manner. Our analysis begins with this CCF estimator which then posits

                                                        yi = f (xi ) + ρv̂i + ηi

and estimates the model using least squares. The NPV-CF estimator is obtained by estimating

                                                      yi = f (xi ) + h(v̂i ) + ηi ,
                                                         P5         l
where we approximate h(v̂i ) as h(v̂i ) =                  l=1 al v̂i .   Since the NPV-CF does not separately identify
the constant term we normalize h(0) = 0 so that the constant term α is also identified. Our results
are robust to adding higher orders of polynomials to fit h(v̂i ).
     We obtain the CMR-CF estimator by using the first stage estimation residual v̂i to construct
approximating functions ṽ1i = v̂i , ṽ2i = v̂i2 − Ê[v̂i2 |zi ], ṽ3i = v̂i3 − Ê[v̂i3 |zi ] where Ê[·|zi ] is estimated
using least squares with regressors (1, zi , zi2 ). Interactions with polynomials of zi like zi v̂i and zi2 v̂i
are defined similarly. In the last step we estimate the parameters as
                                                        Xn
                            (α̂, β̂, γ̂, â) = argmin            {yi − (f (xi ; α, β, γ) + h(zi , v̂i ))}2 /n
                                                           i=1

                             PL
where h(zi , v̂i ) =             l=1 al ṽli   depends on the simulation designs. The choice of the basis in the
finite sample is not a consistency issue but it is an efficiency issue and we vary this choice across
specifications. In design [1] we use ṽ1i and zi ṽi as the controls. In designs [2], [5], and [6] we use
the controls ṽ1i , ṽ2i , and zi ṽi . In design [3] we use the controls ṽ1i , ṽ2i , zi ṽi , and zi2 ṽi , and in design
[4] we use ṽ1i , ṽ2i , ṽ3i , ṽ4i , zi ṽi .
     We report the biases and the RMSE’s based on 200 repetitions of the estimations. The simulation
results in Tables I-VI show that CCF and NPV-CF are biased in all designs except [5] and [6] for
which the theory says they should be consistent. The CMR-CF is robust regardless of the designs.
In design [5] all three approaches produce correct estimates because the outcome equation is linear,
which is consistent with our discussion in Appendix A. In design [6] all three approaches are
consistent because the restriction (4) holds. We conclude that our CMR-CF approach is consistent
in these designs regardless of whether the model is linear or nonlinear or whether the restriction
(4) holds while the CCF and NPV-CF approaches are not robust when the restriction (4) does not
hold.
specifications we would be indifferent between this simplest specification and the ones with the higher order terms.




                                                                    18
Table I: Design [1], α0 = 1, β0 = 1, γ0 = −1      Table II: Design [2], α0 = 1, β0 = 1, γ0 = −1
Nonlinear & The condition (4) does not hold       Nonlinear & The condition (4) does not hold
                 mean       bias     RMSE                              mean        bias      RMSE
  CCF       α    0.7076   -0.2924     0.2952        CCF         α     1.5331      0.5331      0.5452
            β    1.3078    0.3078     0.3094                    β     0.4056      -0.5944     0.6055
            γ   -1.0679   -0.0679     0.0682                    γ     -0.8496     0.1504      0.1529
NPV-CF      α    0.6655   -0.3345     0.3395      NPV-CF        α     1.3535      0.3535      0.3767
            β    1.3677    0.3677     0.3738                    β     0.6283      -0.3717     0.3948
            γ   -1.0917   -0.0917     0.0938                    γ     -0.9090     0.0910      0.0966
CMR-CF      α    0.9978   -0.0022     0.0548      CMR-CF        α     0.9933      -0.0067     0.1478
            β    1.0021    0.0021     0.0503                    β     1.0079      0.0079      0.1611
            γ   -1.0005   -0.0005     0.0109                    γ     -1.0021     -0.0021     0.0405


Table III: Design [3],α0 = 1, β0 = 1, γ0 = −1     Table IV: Design [4], α0 = 1, β0 = 1, γ0 = −1
Nonlinear & The condition (4) does not hold       Nonlinear & The condition (4) does not hold
                 mean       bias     RMSE                              mean        bias       RMSE
  CCF       α    0.5818   -0.4182     0.4235        CCF         α     0.6109      -0.3891     0.3950
            β    1.5048    0.5048     0.5108                    β     1.4702      0.4702      0.4769
            γ   -1.9246   -0.9246     0.9367                    γ     -1.8617     -0.8617     0.8751
NPV-CF      α    0.7750   -0.2250     0.2405      NPV-CF        α     0.7794      -0.2206     0.2371
            β    1.3042    0.3042     0.3200                    β     1.3333      0.3333      0.3497
            γ   -1.5861   -0.5861     0.6156                    γ     -1.6687     -0.6687     0.6988
CMR-CF      α    0.9943   -0.0057     0.1103      CMR-CF        α     1.0003      0.0003      0.1117
            β    1.0076    0.0076     0.1255                    β     1.0005      0.0005      0.1267
            γ   -1.0144   -0.0144     0.2249                    γ     -1.0016     -0.0016     0.2262


   Table V: Design [5], α0 = 1, β0 = 1          Table VI: Design [6], α0 = 1, β0 = 1, γ0 = −1
Linear & The condition (4) does not hold            Nonlinear & The condition (4) holds
                 mean      bias     RMSE                            mean         bias       RMSE
  CCF       α   0.9993    -0.0007   0.0343       CCF        α       0.9991      -0.0009     0.0354
            β   1.0004    0.0004    0.0172                  β       1.0010      0.0010      0.0200
                                                            γ       -1.0002     -0.0002     0.0024
NPV-CF      α   1.0010    0.0010    0.0417      NPV-CF      α       0.9997      -0.0003     0.0350
            β   0.9997    -0.0003   0.0192                  β       1.0004      0.0004      0.0210
                                                            γ       -1.0001     -0.0001     0.0032
CMR-CF      α   0.9991    -0.0009   0.0343      CMR-CF      α       0.9975      -0.0025     0.0891
            β   1.0005    0.0005    0.0171                  β       1.0068      0.0068      0.1204
                                                            γ       -1.0021     -0.0021     0.0304




                                                    19
8.2    Monte Carlos Based on Non-Parametric Models

     Next we conduct two small-scale simulation studies where we estimate the structural function
f (x) nonparametrically. Design [A] has a first stage selection equation that satisfies the NPV-CF
restriction (4) and Design [B] does not.
     For the first specification we follow the setup from Newey and Powell (2003) given as

                             yi = f (xi ) + εi = ln(|xi − 1| + 1)sgn(xi − 1) + εi
                        [A] xi = zi + ηi

where the errors εi and ηi and instruments zi are generated by
                                                                           
                                  εi                        0         1 ρ 0
                                                          
                               ηi  ∼ i.i.d N  0  ,  ρ 1 0 
                                zi                0       0 0 1

with ρ = 0.5. This design satisfies the restriction (4) with vi = xi − E[xi |zi ] because vi = ηi .
     In the second specification we use the same outcome equation but change the first stage equation
to
                                           [B] xi = zi + ηi / exp(|zi |)

and we use ρ = 0.5 and ρ = 0.9 varying the degree of endogeneity. The restriction (4) is violated
because vi = xi − E[xi |zi ] = ηi / exp(|zi |).
     Following Newey and Powell (2003) we use the Hermite series approximation of f (x) as

                                                      J
                                                      X
                                       f (x) ≈ xβ +         γj exp(−x2 )xj−1 .                         (22)
                                                      j=1


We estimate f (x) using the nonparametric least squares (NPLS), the NPV-CF estimator and our
CMR-CF estimator. We fix J = 5 for design [A] and J = 7 for design [B] and we use four different
sample sizes (n=100, 400, 1000, and 2,000). In all of the designs we obtain the control using the first
stage regression residual v̂i = xi − (π̂0 + π̂1 zi + π̂2 zi2 ). We experimented with adding several higher
order terms in the first stage and found very similar simulation results across all three estimators.
We also experimented with different choices of approximating functions of h(v) and h(z, v) for design
                        P                           P
[B]. We consider h(v) ≈ 4l=1 al v l (NPV-CF1) or 5l=1 al v l (NPV-CF2) for NPV-CF estimators
and h(z, v) = a1 v + a2 zv (CMR-CF1), a1 v + a2 zv + a3 z 2 v (CMR-CF2), or a1 v + a2 zv + a3 z 2 v + a4 ṽ2
(CMR-CF3) for CMR-CF estimators.
     The results are summarized in Tables A and B. We report the root mean-squared-error (RMSE)
averaged across the 500 replications and the realized values of x. In both designs RMSE decreases
as the sample size increases for all estimators. The RMSEs for nonparametric least squares (NPLS)
are larger than RMSEs for the estimators that correct for endogeneity. In the design [A] as expected
both NPV-CF and CMR-CF estimators perform similarly although the CMR-CF estimator shows


                                                        20
slightly larger RMSEs because it adds an irrelevant correction term (zv) in the control function.
In the design [B] the CMR-CF estimators dominate the NPV-CF estimators in terms of RMSE.
CMR-CF2 is our preferred specification that shows the smallest RMSE among CMR-CF estimators.
Comparison with CMR-CF1 suggests that including the term z 2 v significantly reduces RMSE’s. The
RMSE’s of NPLS and NPV-CF estimators tend to increase as the degree of endogeneity increases
while RMSEs decrease in CMR-CF2 and CMR-CF3 as the degree of endogeneity increases. We
conclude that our proposed CMR-CF estimator is robust to violations of the restriction (4) while
the NPV-CF estimator is not.



                       Table A: Design [A], RMSE
                          NPLS            NPV-CF                 CMR-CF
     Control Functions     None          h(v) ≈ a1 v̂      h(z, v) ≈ a1 v̂ + a2 zv̂
          n=100           0.4121           0.2685                  0.2732
          n=400           0.3844           0.1667                  0.1692
          n=1000          0.3788           0.1308                  0.1317
          n=2000          0.3695           0.1149                  0.1165
                                                    Table B: Design [B], RMSE
               NPLS      NPV-CF1           NPV-CF2           CMR-CF1               CMR-CF2                         CMR-CF3
                         P4          l     P5          l                                           2
      CF’s     None        l=1 al v̂         l=1 al v̂       a1 v̂ + a2 zv̂   a1 v̂ + a2 zv̂ + a3 z v̂   a1 v̂ + a2 zv̂ + a3 z 2 v̂ + a4 ṽ2
                                                                         ρ = 0.5
     n=100    0.3896       0.3233            0.3241            0.3049                 0.3104                          0.3231
     n=400    0.2775       0.1679            0.1671            0.1540                 0.1422                          0.1456
     n=1000   0.2511       0.1364            0.1358            0.1190                 0.0999                          0.1014
     n=2000   0.2440       0.1150            0.1156            0.0968                 0.0737                          0.0745
                                                                         ρ = 0.9
     n=100    0.5277       0.3059            0.3018            0.2833                 0.2734                          0.2889
     n=400    0.4462       0.2042            0.2003            0.1680                 0.1296                          0.1322
     n=1000   0.4375       0.1885            0.1865            0.1483                 0.0941                          0.0950
     n=2000   0.4311       0.1762            0.1773            0.1356                 0.0690                          0.0696




9    Empirical Example

    We apply our estimator to the empirical example in Newey, Powell, and Vella (1999, hereafter
NPV). They investigate the relationship between hourly wage rate and annual hours worked using
the 1989 wave of the Michigan Panel Survey of Income Dynamics (PSID). The model is as follows:
                                  ′
                            yi = z1i β + g20 (xi ) + εi xi = zi′ γ + vi

where yi is the log of the hourly wage rate of individual i, z1i is a vector of individual characteristics,


                                                                   21
xi is annual hours worked, zi is a vector of exogenous variables that includes z1i and g20 is a
non-parametric, unknown function. εi and vi are mean zero error terms such that E[ε|z, v] 6= 0,
E[v|z] = 0, and E[ε|v] 6= 0. We mimic NPV’s specification by defining z1i as education dummies,
union status, tenure status, full time work experience, regional variables and a racial dummy. The
variables included in zi but excluded from z1i are marital status, health status, presence of young
children, nonlabor income and a rural dummy. After following the data cleaning from NPV we have
2545 observations on males aged between 22 and 55 who had worked between 1000 and 3500 hours
in the previous year.
     To construct our estimator we rewrite the model as follows:
                               ′
                         yi = z1i β + g20 (xi ) + h(zi, vi ) + ηi xi = zi′ γ + vi

with E[εi |zi , vi ] = h(zi , vi ) and E[ηi |zi , vi ] = 0. In the first step we mimic NPV’s control v̂i =
xi − Ê[xi |zi ] exactly by regressing hours worked on the zi they use including the higher order
terms for the regressors related to tenure and full-time experience. In the second step, for NPV we
approximate h(zi , v̂i ) = h(v̂i ) as a third-order polynomial in v̂i and for our estimator we approximate
h(zi , v̂i ) as a demeaned (w.r.t. Ê[·|zi ]) third-order polynomial in v̂i and with interaction terms
between z1i and the demeaned third-order polynomial in v̂i (45 interaction terms in total). We
approximate g20 (xi ) as a fourth-order polynomial in xi .17 We then regress yi on our approximations
                     ′ β + g (x ) and h(z , v̂ ).
to f0 (xi , z1i ) = z1i     20 i         i i
     Table VII reports the results. Columns 1-3 mimic the specifications from NPV (estimates for
all regressors except the control and hours worked are suppressed). Column 1 is OLS and shows
that hours worked is not significant without the control, suggesting endogeneity problem. Column
2 is 2SLS and suggests that the linear model is misspecified when the higher order terms of hours
worked are omitted. Column 3 is NPV’s estimator using their preferred specification. Column 4
is our CMR-CF estimator. For our estimator there are 45 additional terms that we add to the
control function relative to NPV and an F-test of their significance rejects the null with a p-value
of less than 0.01 as the adjusted R-squared increases to 0.456 from 0.439. Thus it appears that the
expected value of εi does depend on z1i conditional on vi . However, correcting for the additional
terms does not appear to change the coefficients much as all of the NPV estimates fall within the
confidence interval of their counterpart for the CMR-CF estimator.

10      Conclusion

     We show that the CF estimator of Newey, Powell, and Vella (1999) can be modified to allow
the conditional mean of the error to depend on both the instruments and controls. We do so by
adding conditional moment restrictions which, when combined with a suitable rank condition, imply
the control function is distinguishable from functions of the endogenous and exogenous regressors,
yielding identification of the structural function. We also show our approach yields identification
in settings where neither NPIV nor NPV-CF does. When sieves are used to approximate both the
  17
     The choice of a fourth-order polynomial in xi and a third-order polynomial in v̂i is due to NPV, who state that
this is the preferred specification according to the cross validation (CV) criterion.



                                                        22
structural function and the control function our estimator is simple to implement as it reduces to
a series of Least Squares regressions. Our monte carlos are designed to mimic common economic
settings where the NPV-CF independence assumption will not generally hold and we show our new
estimator is consistent in these settings when the NPV-CF estimator is biased. Our empirical ex-
ample revisits the example from NPV and we show under the CMRs we reject that the conditional
mean of the error is fully independent of the instruments.




                   Table VII: Wage as a Function of Hours Worked (x)
                                (1) OLS        (2) 2SLS       (3) NPV      (4) CMR-CF
                                lnwage          lnwage         lnwage        lnwage


    x                          -1.51e-05      0.00102***     -0.00621**     -0.00603
                               (1.97e-05)     (0.000112)      (0.00306)     (0.00389)
    x2                                                       4.78e-06**    5.07e-06**
                                                             (2.08e-06)    (2.44e-06)
    x3                                                       -1.34e-09**   -1.54e-09**
                                                             (6.33e-10)     (7.15e-10)
    x4                                                        1.33e-13*    1.61e-13**
                                                             (7.17e-14)     (8.00e-14)
    v                                         -0.00106***    -0.00107***     0.00267
                                              (0.000114)     (0.000170)    (0.002554)
    v2                                                        -1.32e-07     -1.43e-07
                                                             (1.12e-07)     (1.83e-06)
    v3                                                        1.81e-10*     -2.42e-09
                                                             (1.10e-10)     (3.24e-09)


    R-squared                    0.416           0.435          0.444         0.471
    Adj R-squared                0.412           0.432          0.439         0.456
    F-test on Interactions        n/a            n/a            n/a           2.820
    Prob > F                      n/a            n/a            n/a           0.000
    Standard errors in parentheses account for pre-stage estimations.
    *** p<0.01, ** p<0.05, * p<0.1
    x is the number of hours worked.
    v is the residual from the first stage.




                                                  23
                                                    Appendix



A     Linear setting with additive errors

    We revisit the linear-in-parameters setting in light of our new estimator. In mean-deviated form
the equation of interest as

                                                  yi = xi β0 + εi ,                                              (23)

with yi the dependent variable and xi a scalar explanatory variable that is potentially correlated
with εi , so E[xi εi ] 6= 0 (and E[yi ] = 0 and E[xi ] = 0). We let zi denote an instrument vector
satisfying
                                            E[zi εi ] = 0, E[zi xi ] 6= 0,                                       (24)

(and E[zi ] = 0). The classic control function (CCF) estimator for the linear case posits the same
control vi = xi − E[xi |zi ] and follows directly from assuming


                       (Classic CF Restriction) E[εi |zi , xi ] = E[εi |zi , vi ] = ρvi .                        (25)

(25) implies the following estimating equation

                                                yi = xi β + ρvi + ηi ,                                           (26)

with ηi = εi − E[εi |zi , xi ]. The CCF estimator differs from our CMR-CF estimator which considers
an (unrestricted) general specification for the conditional expectation of the error

                                    E[εi |zi , vi ] ≡ h(zi , vi ) = ρ̃vi + h̃(zi , vi ),

with the function characterizing E[εi |zi , vi ] having a leading term in vi and a remaining term denoted
by the function h̃(zi , vi ).
    It is well-known by projection theory that the CCF estimator and the two-stage least squares
(2SLS) estimator produce numerically identical estimates. Our CMR-CF estimator will differ in
finite samples from CCF/2SLS estimator because of the additional regressors used to approximate
h̃(zi , vi ). In this section we show that under the CMR condition the asymptotic correlation between
xi and h̃(zi , vi ) conditional on vi vanishes so all three estimators for β0 are asymptotically equivalent.
    In the linear case “CMR-CF” is a misnomer because - like CCF and 2SLS - we only require
E[zi εi ] = 0. We relegate the proof that CCF and 2SLS are numerically equivalent to a footnote
below.18
  18
      We provide a simple proof using projection that shows why they are equivalent. Let Y = (y1 , . . . , yn )′ , X =
(x1 , . . . , xn )′ , Z = (z1 , . . . , zn )′ , X̂ = (x̂1 , . . . , x̂n )′ and V̂ = (v̂1 , . . . , v̂n )′ below.



                                                            24
    We then consider the generalization of equation (26), with

                                                 Y = Xβ0 + ρ̃V̂ + H̃(Z, V̂ ) + η̂                                                                  (27)

where Y = (y1 , . . . , yn )′ , X = (x1 , . . . , xn )′ , Z = (z1 , . . . , zn )′ , H̃(Z, V̂ ) = (h̃(z1 , v̂1 ), . . . , h̃(zn , v̂n ))′
and V̂ = X − Z π̂, the vector of controls, which are the ordinary least squares residuals from the
regression of X on Z, and η̂ is the residual after the estimated vector of controls are included. We
can rewrite (27) as
                                          Y = (Z π̂ + V̂ )β0 + ρ̃V̂ + H̃(Z, V̂ ) + η̂.                                                             (28)

Define MV̂ = I − V̂ (V̂ ′ V̂ )−1 V̂ ′ , the matrix that projects off of V̂ , and note that MV̂ V̂ = 0 and
MV̂ Z = Z (because V̂ ′ Z = 0). Then by partitioned regression theory, estimation of β0 in (28) is
numerically equivalent to the estimation of

                                    Y     = MV̂ (Z π̂ + V̂ )β0 + MV̂ H̃(Z, V̂ ) + MV̂ η̂
                                          = Z π̂β + MV̂ H̃(Z, V̂ ) + MV̂ η̂.

If MV̂ H̃(Z, V̂ ) is asymptotically uncorrelated with Z π̂, i.e., if Z ′ MV̂ H̃(Z, V̂ )/n converges to zero as
the sample size goes to infinity, then the least squares estimator of β0 in (27) is consistent whether
or not we include H̃(Z, V̂ ) in the regression as long as V̂ is included as a regressor in (27). Note
that because Z ′ V̂ = 0 we have
                                                                                                                          Xn
    Z ′ MV̂ H̃(Z, V̂ )/n = Z ′ (I − V̂ (V̂ ′ V̂ )−1 V̂ ′ )H̃(Z, V̂ )/n = Z ′ H̃(Z, V̂ )/n =                                      zi h̃(zi , v̂i )/n
                                                                                                                           i=1

and therefore consistency holds if
                                                     Xn
                                                                 zi h̃(zi , v̂i )/n →p 0.                                                          (29)
                                                           i=1

Theorem 6 couples (24) with weak regularity conditions which are sufficient for (29) to hold.
Theorem 6. Assume (i) E[kzi k · ||h̃(zi , vi )||] < ∞, (ii) h̃(z, v) is differentiable with respect to v,
                                                                               ∂ h̃(zi ,vi (π ∗ ))
(iii) for vi (π) ≡ xi − zi′ π, assume supπ∗ ∈Π0 E[kzi k2                              ∂vi            ] < ∞ for Π0 some neighborhood
                                        ∂ h̃(zi ,vi (π))
of π0 , (iv) assume E[kzi k2                  ∂vi          ] is continuous at π = π0 , and (v) π̂ →p π0 . If (24) holds
then (29) holds.
                   Pn                              Pn                               Pn
Proof. Write          i=1 zi h̃(zi , v̂i )/n   =       i=1 zi h̃(zi , vi )/n    +        i=1 zi (h̃(zi , v̂i )    − h̃(zi , vi ))/n. We have
Remark 1 (2SLS-CF Numerical Equivalence). If β̂2SLS                         =                        (X̂ ′ X̂)−1 X̂ ′ Y   and    (β̂CF , ρ̂CF )′      =
         ′         −1       ′
((X, V̂ ) (X, V̂ )) (X, V̂ ) Y are well-defined and exist, then β̂2SLS = β̂CF .

Proof. From projection theory the same numerical estimate obtains for the coefficient on xi from either regressing Y
on (X, V̂ ) or regressing Y on the projection of X off of V̂ . Numerical equivalence follows because the projection of X
off of V̂ is equal to X̂ because (I − V̂ (V̂ ′ V̂ )−1 V̂ ′ )X = (I − V̂ (V̂ ′ V̂ )−1 V̂ ′ )(X̂ + V̂ ) = X̂, as V̂ ′ X̂ = 0 by projection.




                                                                       25
Pn                                                                                                         P
                        →p E[zi h̃(zi , vi )] by the law of large numbers under (i). Obtain || ni=1 zi (h̃(zi , v̂i )−
    i=1 zi h̃(zi , vi )/n
                                P                      ,vi (π̂ ∗ ))
h̃(zi , vi ))/n|| ≤ kπ̂ ∗ − π0 k ni=1 kzi k2 || ∂ h̃(zi∂v           ||/n by applying the mean-value expansion, where
                                                          i
                                                                                  P
π̂ lies between π̂ and π0 and vi (π) = xi − zi π. Then the term ni=1 zi (h̃(zi , v̂i ) − h̃(zi , vi ))/n →p 0
  ∗                                                      ′
                                  P                         ,vi (π̂ ∗ ))
by the consistency of π̂ and ni=1 kzi k2 ∂ h̃(zi∂v             i
                                                                                               ,vi (π0 ))
                                                                         /n →p E[kzi k2 ∂ h̃(zi∂vi
                                                                                                          ] < ∞ under (iii)
                          Pn
and (iv). Therefore i=1 zi h̃(zi , v̂i )/n →p E[zi h̃(zi , vi )]. Finally note E[zi h̃(zi , vi )] = 0 if (24) holds
and vi is such that E[vi |zi ] = 0 because by the law of iterated expectations

                  0 = E[zi εi ] = E[zi E[E[εi |zi , vi ]|zi ] ] = E[zi (ρ̃E[vi |zi ] + E[h̃(zi , vi )|zi ])]
                      = E[zi E[h̃(zi , vi )|zi ]] = E[zi h̃(zi , vi )]

and therefore the conclusion follows.

       The theorem also makes it clear that the classic control function approach does not generally
yield consistency for the expected value of the error conditional on the control and exogenous
variables unless H̃(Z, V̂ ) is also included in the regression equation. Although this is not typically
the object of interest of either the classic CF estimator or the 2SLS estimator, an exception is when
one tests for endogeneity based on the estimate of ρ in (26).19
       A simple example is illustrative of these points. Consider the case that zi is a scalar and

                                                E[εi |zi , vi ] = ρ1 vi + ρ2 vi zi ,                                (30)

but the researcher only includes xi and v̂i as regressors. Even though the researcher omits the
relevant variable v̂i zi , the ordinary least squares estimator β̂ is consistent for β0 because for the
term corresponding to (29)
                                          n
                                          X
                                                zi (v̂i zi )/n →p E[vi zi2 ] = 0,
                                          i=1

which follows from v̂i →p vi (because π̂ →p π0 ) and E[vi |zi ] = 0 and by LLN under standard
regularity conditions (E[kvi k kzi k2 ] < ∞ and E[kzi k3 ] < ∞). However, ρ̂1 v̂i is not a consistent
estimator of E[εi |zi , vi ]. If one desired a consistent estimator of this conditional expectation, then
v̂i zi would have to be included in the regression, and ρ̂1 v̂i + ρ̂2 v̂i zi would be consistent for E[εi |zi , vi ].



B        Proof of convergence rates (Theorem 3)

       We first introduce notation and prove Lemma L1 below that is useful to prove the convergence
rate results and also the asymptotic normality of linear functional estimators.
  19
     See e.g. Smith and Blundell (1986). In this case the misspecification of this conditional expectation may reduce
the power of the test or call into question the test’s consistency.




                                                                26
       Define hL (z, v) = a′L ϕ̃L (z, v) and ĥL (z, v) = a′L ϕ̃ˆL (z, v) where aL 20 satisfies Assumption L1 (iv).
Define ψiL (zi , vi ) = (φ1 (xi , z1i ), . . . , φK (xi , z1i ), ϕ̃L (zi , vi )′ )′ where ϕ̃L (zi , vi ) = (ϕ̃1 (zi , vi ), . . . , ϕ̃L (zi , vi ))′
and ψ̂iL (zi , vi ) = (φ1 (xi , z1i ), . . . , φK (xi , z1i ), ϕ̃ˆL (zi , vi )′ )′ with ϕ̃ˆL (zi , vi ) = (ϕ̃ˆ1 (zi , vi ), . . . , ϕ̃ˆL (zi , vi ))′ .
                         ˆ
We further let ψ̂iL = ψ̂ L (zi , v̂i ), ψiL = ψ L (zi , vi ), and ψ̂iL = ψ̂ L (zi , vi ). We further let ψ L,n =
                                                                ˆ         ˆ              ˆ
(ψ1L , . . . , ψnL )′ , ψ̂ L,n = (ψ̂1L , . . . , ψ̂nL )′ , and ψ̂ L,n = (ψ̂1L , . . . , ψ̂nL )′ .
       Let C (also C1 ,C2 , and others) denote a generic positive constant and let C(Z, V ) or C(X, Z1 )
(also C1 (·), C2 (·), and others) denote a generic bounded positive function of (Z, V ) or (X, Z1 ). We
often write Ci = C(xi , z1i ). Recall W = supp(Z, V ).
Assumption 6 (L1). (i) (X, Z, V ) is continuously distributed with bounded density; (ii) for each
k, K, L, and L = K + L there are nonsingular matrices B1 , B2 , B3 , and B such that for
pkB1 (z) = B1 pk (z), φK                 K            L                L             L            L
                       B2 (x, z1 ) = B2 φ (x, z1 ), ϕ̃B3 (z, v) = B3 ϕ̃ (z, v), and ψB (z, v) = Bψ (z, v),
E[pkB1 (Zi )pkB1 (Zi )′ ], E[φK              K             ′       L              L            ′          L            L           ′
                              B2 (Xi , Z1i )φB2 (Xi , Z1i ) ], E[ϕ̃B3 (Zi , Vi )ϕ̃B3 (Zi , Vi ) ], and E[ψB (Zi , Vi )ψB (Zi , Vi ) ]
have smallest eigenvalues that are bounded away from zero, uniformly in k, K, L, and L; (iii) for
each integer δ > 0, there exist ζδ (K), ζδ (L), and ξδ (k) such that |φK (x, z1 )|δ ≤ ζδ (K), |ψ L (z, v)|δ ≤
ζδ (L) (this also implies that |ϕ̃L (z, v)|δ ≤ ζδ (L)), and |pk (z)|δ ≤ ξδ (k) ; (iv) There exist γ1 , γ2 , γf ,
γ > 0, and β K , aL , βL , λ1k , and λ2l,k such that |Π0 (z) − λ1′ k
                                                                k p (z)|δ ≤ Ck
                                                                               −γ1 , |ϕ̄ (z) − λ2′ pk (z)| ≤
                                                                                        0l      l,k       δ
Ck−γ2 for all l, |f0 (x, z1 ) − β K′ φK (x, z1 )|δ ≤ CK −γf , |h0 (z, v) − a′L ϕ̃L (z, v)|δ ≤ CL−γ , and
|g0 (z, v) − βL′ ψ L (z, v)|δ ≤ CL−γ ; (v) both Z and X are compact.
                   1/2 √                     1/2 √
       Let △n,1 = kn / n + kn−γ1 and △n,2 = kn / n + kn−γ2 and △n = max{△n,1 , △n,2 }.

Lemma 1 (L1). Suppose Assumptions L1 and Assumptions C1 hold. Further suppose L1/2 (ζ1 (L) +
           p
L1/2 ξ0 (k) k/n + L1/2 )△n → 0 , ξ0 (k)2 k/n → 0, and ζ0 (L)2 L/n → 0. Then,
  Xn                                                p                   p
 (     (ĝ(zi , vi ) − g0 (zi , vi ))2 /n)1/2 = Op ( L/n + Lξ0 (k)△n,1 k/n + L△n,2 + L−γ )
   i=1
                                                           p                 p
           max |ĝ(zi , vi ) − g0 (zi , vi )| = Op (ζ0 (L)[ L/n + Lξ0 (k)△n,1 k/n + L△n,2 + L−γ ]).
                   i≤n




B.1        Proof of Lemma L1

      Without loss of generality, we will let pk (z) = pkB1 (z), φK (x, z1 ) = φK                                          L
                                                                                                            B2 (x, z1 ), ϕ̃ (z, v) =
ϕ̃L                    L                   L
  B3 (z, v), and ψ (z, v) = ψB (z, v). Let Π̂i = Π̂(zi ) and Πi = Π0 (zi ). Let ϕ̄li = ϕ̄l (zi ) and ϕ̄li =
                                                                                                           ˆ      ˆ
ϕ̄l (zi ). Let ϕ̃ˆˆli = ϕ̃ˆl (zi , v̂i ) and ϕ̃li = ϕ̃l (zi , vi ). Also let ϕ̃ˆˆL    ˆL                  L      L
                                                                                 i = ϕ̃ (zi , v̂i ) and ϕ̃i = ϕ̃ (zi , vi ). Further
                                           P                                                                          P
define ϕ̄˙ l (z) = pk (z)′ (P ′ P )− ni=1 pk (zi )ϕl (zi , vi ) where we have ϕ̄ˆl (z) = pk (z)′ (P ′ P )− ni=1 pk (zi )ϕl (zi , v̂i ).
Let ϕ̄˙ L (z) = (ϕ̄˙ 1 (z), . . . , ϕ̄˙ L (z))′ and ϕ̄L (z) = (ϕ̄1 (z), . . . , ϕ̄L (z))′ . We also let
ϕL (zi , v̂i ) = (ϕ1 (zi , v̂i ), . . . , ϕL (zi , v̂i ))′ and ϕL (zi , vi ) = (ϕ1 (zi , vi ), . . . , ϕL (zi , vi ))′ .
       First note (P ′ P )/n becomes nonsingular w.p.a.1 as ξ0 (k)2 k/n → 0 by Assumption L1 (ii) and
by the same proof in Theorem 1 of Newey (1997). Then by the same proof (A.3) of Lemma A1 in
  20
       With abuse of notation we write aL = (a1 , . . . , aL )′ .




                                                                      27
Newey, Powell, and Vella (1999), we obtain
              Xn                                                       Xn
                          ||Π̂i − Πi ||2 /n = Op (△2n,1 ) and                        ||ϕ̄˙ li − ϕ̄li ||2 /n = Op (△2n,2 ) for all l.   (31)
                    i=1                                                      i=1

Also by Theorem 1 of Newey (1997), it follows that

                                            max ||Π̂i − Πi || = Op (ξ0 (k)△n,1 )                                                       (32)
                                             i≤n
                                            max ||ϕ̄˙ li − ϕ̄li || = Op (ξ0 (k)△n,2 ) for all l.                                       (33)
                                            i≤n


              ˆ         ˆ
Define T̂ = (ψ̂ L,n )′ ψ̂ L,n /n and Ṫ = (ψ L,n )′ ψ L,n /n. Our goal is to show that T̂ is nonsingular
w.p.a.1. We first note that Ṫ is nonsingular w.p.a.1 by Assumption L1 (ii) as ζ0 (L)2 L/n → 0 by
the same proof in Lemma A1 of Newey, Powell, and Vella (1999).
    For ease of notation along the proof, we will assume some rate conditions are satisfied. Then
we collect those rate conditions in Section B.2 and derive conditions under which all of them are
satisfied. Next note that

             ϕ̃ˆˆL     L
                 i − ϕ̃i       ≤       ϕL (zi , v̂i ) − ϕL (zi , vi ) + ϕ̄ˆL (zi ) − ϕ̄L (zi )                                         (34)
                               ≤       ϕL (zi , v̂i ) − ϕL (zi , vi ) + ϕ̄ˆL (zi ) − ϕ̄˙ L (zi ) + ϕ̄˙ L (zi ) − ϕ̄L (zi ) .

We find        ϕL (zi , v̂i ) − ϕL (zi , vi )        ≤ Cζ1 (L)||Π̂i − Πi || applying a mean value expansion because
ϕl (zi , vi ) is Lipschitz in vi (so in Πi ) for all l (Assumption C1 (vi)). Combined with (31), it implies
that
                                   Xn                                            2
                                                ϕL (zi , v̂i ) − ϕL (zi , vi )       /n = Op (ζ1 (L)2 △2n,1 ).                         (35)
                                       i=1

Next let ω̂l = (ϕl (z1 , v̂1 ) − ϕl (z1 , v1 ), . . . , ϕl (zn , v̂n ) − ϕl (zn , vn ))′ . Then we can write for any
l = 1, . . . , L,
       Xn                                                    nXn                                                      o
                                            2
                 ϕ̄ˆl (zi ) − ϕ̄˙ l (zi )       /n = tr          pk (zi )′ (P ′ P )− P ′ ω̂l ω̂l′ P (P ′ P )− pk (zi ) /n              (36)
          i=1
                                                        ( i=1                                n
                                                                                                                   )
                                                                                            X
                                                    = tr (P ′ P )− P ′ ω̂l ω̂l′ P (P ′ P )−       pk (zi )pk (zi )′ /n
                                                                                                    i=1
                                                        
                                                    = tr (P ′ P )− P ′ ω̂l ω̂l′ P /n
                                                                                  
                                                    ≤ C max ||Π̂i − Πi ||2 tr (P ′ P )− P ′ P /n ≤ Cξ0 (k)2 △2n,1 k/n
                                                             i≤n


where the first inequality is obtained by (32) and applying a mean value expansion to ϕl (zi , vi )
which is Lipschitz in vi (so in Πi ) for all l (Assumption C1 (vi)). From (31), (34), (35), and (36),
we conclude
  Xn
             ||ϕ̄ˆL (zi ) − ϕ̄L (zi )||2 /n = Op (Lξ0 (k)2 △2n,1 k/n) + Op (L△2n,2 ) = op (1),                   (37)
       i=1
              Xn           ˆ       L 2
                          ϕ̃ˆL
                             i − ϕ̃i     /n = Op (ζ1 (L)2 △2n,1 ) + Op (Lξ0 (k)2 △2n,1 k/n) + Op (L△2n,2 ) = op (1).
                    i=1




                                                                        28
The latter also implies that by the triangle inequality and the Markov inequality,

              Xn                              n
                                              X                        n
                                                                       X
                          ||ϕ̃ˆˆL   2
                                i || /n ≤ 2     ||ϕ̃ˆ
                                                   ˆLi − ϕ̃L 2
                                                           i || /n + 2   ||ϕ̃L   2
                                                                             i || /n = op (1) + Op (L).                                    (38)
                    i=1
                                              i=1                               i=1

                              p
Let △ϕ
     n = (ζ1 (L) + L
                     1/2 ξ (k) k/n + L1/2 )△ . It also follows that
                          0                 n

                  Xn           ˆ              2          Xn         ˆ               2
                              ψ̂iL − ψiL          /n ≤              ϕ̃ˆL     L
                                                                       i − ϕ̃i          /n = Op ((△ϕ  2
                                                                                                   n ) ) = op (1).                         (39)
                      i=1                                     i=1

                  Pn                                Pn                                           2            Pn                       2
                          ˆL 2                            ˆL                                                               ˆ
This also implies  i=1 ||ψ̂i || /n = Op (L) because  i=1 ψ̂i                                         /n ≤ 2     i=1       ψ̂iL − ψiL       /n +
 P          2
2 ni=1 ψiL /n = Op (L).
   Then applying (39) and applying the triangle inequality and Cauchy-Schwarz inequality and by
Assumption L1 (iii) , we obtain

                                Xn          ˆ             2            Xn                        ˆ
         ||Tˆ − T˙ || ≤                    ψ̂iL − ψiL         /n + 2                 ψiL        ψ̂iL − ψiL /n                              (40)
                                    i=1                                       i=1
                                                         Xn                        1/2   X                         2
                                                                                                                               1/2
                                                                          2                       n      ˆ
                           ≤ Op ((△ϕ 2
                                   n) ) + 2                         ψiL       /n                        ψ̂iL − ψiL        /n
                                                              i=1                                 i=1

                           = Op ((△ϕ  2
                                   n ) ) + Op (L
                                                 1/2 ϕ
                                                    △n ) = op (1).

It follows that

                  ||T̂ − T || ≤ ||T̂ − Ṫ || + ||T˙ − T ||
                                                                  p
                                 = Op ((△ϕ
                                         n )2
                                              + L 1/2 ϕ
                                                     △ n + ζ 0 (L)  L/n) ≡ Op (△T ) = op (1)                                               (41)
                                        p
where we obtain ||T˙ − T || = Op (ζ0 (L) L/n) by the same proof in Lemma A1 of Newey, Powell,
and Vella (1999). Therefore we conclude T̂ is also nonsingular w.p.a.1. The same conclusion holds
                                P               ˆ ˆ                  P
even when instead we take T̂ = ni=1 C(zi , vi )ψ̂iL ψ̂iL′ /n and Ṫ = ni=1 C(zi , vi )ψiL ψiL′ /n for some
positive bounded function C(zi , vi ) and this helps to derive the consistency of the heteroskedasticity
robust variance estimator later.
   Let ηi = yi − g0 (zi , vi ) and let η = (η1 , . . . , ηn )′ . Let (Z, V) = ((Z1 , V1 ), . . . , (Zn , Vn )). Then we
have E[ηi |Z, V] = 0 and by the independence assumption of the observations, we have E[ηi ηj |Z, V] =
0 for i 6= j. We also have E[ηi2 |Z, V] < ∞. Then by (39) and the triangle inequality, we bound

                ˆ                                      Xn                                                  ˆ              2
              E ||(ψ̂ L,n − ψ L,n )′ η/n||2 |Z, V ≤ Cn−2                                      E[η 2i |Z, V] ψ̂iL − ψiL
                                                                                        i=1
                                                                ≤ n−1 Op (L(△ϕ  2         −1
                                                                             n ) ) = op (n ).


Then from the standard result (see Newey (1997) or Newey, Powell, and Vella (1999)) that the
                                                                                            ˆ
bound of a term in the conditional mean implies the bound of the term itself, we obtain ||(ψ̂ L,n −
                                                             2
ψ L,n )′ η/n||2 = op (n−1 ). Also note that E[ (ψ L,n )′ η/n ] = CL/n (see proof of Lemma A1 in



                                                                    29
Newey, Powell, and Vella (1999)). Therefore, by the triangle inequality

                             ˆ                      ˆ
                         ||(ψ̂ L,n )′ η/n||2 ≤ 2||(ψ̂ L,n − ψ L,n )′ η/n||2 + 2||(ψ L,n )′ η/n||2 .                                (42)
                                                = op (1) + Op (L/n) = Op (L/n).

     Define ĝi = fˆ(xi , z1i ) + ĥ(zi , v̂i ), ĝˆLi = fK (xi , z1i ) + ĥL (zi , v̂i ), g̃Li = fK (xi , z1i ) + hL (zi , v̂i ),
                                                                                                                P
g̃0i = f0 (xi , z1i ) + h0 (zi , v̂i ), and g0i = f0 (xi , z1i ) + h0 (zi , vi ) where fK (xi , z1i ) = K          l=1 βl φl (xi , z1i ),
ĥ(zi , v̂i ) = â ϕ̃(zi , v̂i ), ĥL (zi , v̂i ) = a ϕ̃(zi , v̂i ), and hL (zi , v̂i ) = a (ϕ(zi , v̂i ) − ϕ̄ (zi )) and let ĝ, ĝˆL ,
                L
                  ′ ˆ
                                                  L
                                                     ′ ˆ
                                                                                         L
                                                                                           ′                  L

g̃L , and g̃0 stack the n observations of ĝi , ĝˆLi , g̃Li , and g̃0i , respectively. Recall βL = (β K′ , a′L )′ where
β K = (β1 , . . . , βK )′ and let this βL satisfies Assumption L1 (iv). From the first order condition of
the last step least squares we obtain

                         ˆ
                    0 = ψ̂ L,n′ (y − ĝ)/n                                                                                         (43)
                         ˆ
                      = ψ̂ L,n′ (η − (ĝ − ĝˆL ) − (ĝˆL − g̃L ) − (g̃L − g̃0 ))/n
                         ˆ            ˆ
                      = ψ̂ L,n′ (η − ψ̂ L,n (β̂ − βL ) − (ĝˆL − e gL ) − (g̃L − g̃0 ) − (g̃0 − g0 ))/n.

              ˆ       ˆ       ˆ          ˆ
Note that by ψ̂ L,n (ψ̂ L,n′ ψ̂ L,n )−1 ψ̂ L,n′ idempotent and by Assumption L1 (iv),

                  ˆ                                               ˆ       ˆ       ˆ          ˆ
         ||T̂ −1 ψ̂ L,n′ (g̃L − g̃0 )/n|| ≤ Op (1){(g̃L − g̃0 )′ ψ̂ L,n (ψ̂ L,n′ ψ̂ L,n )−1 ψ̂ L,n′ (g̃L − g̃0 )/n}1/2             (44)
                                              ≤ Op (1){(g̃L − g̃0 )′ (g̃L − g̃0 )/n}1/2 = Op (L−γ ).

                        ˆ       ˆ       ˆ          ˆ
Similarly we obtain by ψ̂ L,n (ψ̂ L,n′ ψ̂ L,n )−1 ψ̂ L,n′ idempotent, Assumption L1 (iv), and (37),

                       ˆ
              ||Tˆ −1 ψ̂ L,n′ (ĝˆL − g̃L )/n|| = Op (1){(ĝˆL − g̃L )′ (ĝˆL − g̃L )/n}1/2                                        (45)
                        Xn
            ≤ Op (1)(              ||ĥL (zi , v̂i ) − h̃L (zi , v̂i )||2 /n)1/2
                            i=1
                        Xn                                                                      p
            ≤ Op (1)(              ||aL ||2 ||ϕ̄ˆL (zi ) − ϕ̄L (zi )||2 /n)1/2 = Op (Lξ0 (k)△n,1 k/n + L△n,2 ).
                               i=1

                   ˆ       ˆ       ˆ          ˆ
Similarly also by ψ̂ L,n (ψ̂ L,n′ ψ̂ L,n )−1 ψ̂ L,n′ idempotent and (31) and applying the mean value expan-
sion to h0 (zi , vi ), we have

                                                                Xn
                            ˆ −1 ˆL,n′
                          ||T ψ̂       (g̃0 − g0 )/n|| = Op (1)(   ||h0 (zi , v̂i ) − h0 (zi , vi )||2 /n)1/2                      (46)
                                                                        i=1
                              Xn
                     ≤ Op (1)(   ||Π̂i − Πi ||2 /n)1/2 = Op (△n,1 ) = op (1).
                                        i=1


Combining (42), (43), (44), (45), (46) and by T̂ is nonsingular w.p.a.1, we obtain

                            ˆ                       ˆ                                   ˆ ′
    ||β̂ − βL || ≤ ||Tˆ −1 ψ̂ L,n′ η/n|| + ||T̂ −1 ψ̂ L,n′ (ĝˆL − g̃L )/n|| + ||Tˆ −1 ψ̂ L,n (g̃L − g̃0 )/n|| + op (1)
                             p                          p
                 = Op (1){ L/n + Lξ0 (k)△n,1 k/n + L△n,2 + L−γ } ≡ Op (△n,β ).                                        (47)

        ∗ = f (x , z ) + h∗ (z , v ) where h∗ (z , v ) = a′ (ϕL (z , v ) − ϕ̄
Define gLi                                                                 ˆL (zi )). Then applying the
             K i 1i       L i i             L i i         L       i i



                                                                  30
triangle inequality, by (37), (47), the Markov inequality, Assumption L1 (iv), and T̂ is nonsingular
w.p.a.1 (by Assumption L1 (ii) and (41)), we conclude
           Xn
                   (ĝ(zi , vi ) − g0 (zi , vi ))2 /n                                                                        (48)
             i=1
            X  n                                        Xn                             Xn
                                       ∗ 2
      ≤ 3             (ĝ(zi , vi ) − gLi ) /n + 3                ∗
                                                                (gLi − gLi )2 /n + 3             (gLi − g0 (zi , vi ))2 /n
                i=1                                       i=1                              i=1
      ≤ Op (1)||β̂ − βL ||2
             Xn
        +C1          ||aL ||2 ||ϕ̄ˆL (zi ) − ϕ̄L (zi )||2 /n + C2 sup ||βL′ ψ L (z, v) − g0 (z, v)||2
                      i=1                                                  W
      ≤    Op (△2n,β )   + LOp (Lξ0 (k)   2
                                              △2n,1 k/n   +   L△2n,2 ) +   Op (L−2γ ) = Op (△2n,β ).

This also implies that by a similar proof to Theorem 1 of Newey (1997)

                                         max |ĝi − g0i | = Op (ζ0 (L)△n,β ).                                                (49)
                                          i≤n


B.2    Proof of Theorem 3

   Under Assumption C1, all the assumptions in Assumption L1 are satisfied. We can take γ1 =
s1 /dz and γ2 = s2 /dz as discussed in Assumption C1. For the consistency, we require the following
rate conditions: (i) L1/2 △ϕ                           2
                           n → 0 from (40), (ii) ζ0 (L) L/n → 0 (such that Ṫ is nonsingular w.p.a.1),
and (iii) ξ0 (k)2 k/n → 0 (such that P ′ P/n is nonsingular w.p.a.1). The other rate conditions are
                                                                                p
dominated by these three. From the definition of △ϕ    n = (ζ1 (L) + L
                                                                       1/2 ξ (k) k/n + L1/2 )△ , we
                                                                            0                 n
             1/2         1/2
                                 p          1/2
have (i) : L (ζ1 (L) + L ξ0 (k) k/n + L )△n .
   For the polynomial approximations, we have ζδ (L) ≤ CL1+2δ and ξ0 (k) ≤ Ck and for the
spline approximations, we have ζδ (L) ≤ CL0.5+δ and ξ0 (k) ≤ Ck0.5 . Therefore for the polynomial
                                                                     √
approximations, the rate conditions become (i) L1/2 (L3 + L1/2 k3/2 / n + L1/2 )△n → 0, (ii) L3 /n →
                                                                                               √
0, and (iii) k3 /n → 0 and for the spline approximations, they become (i) L1/2 (L3/2 + L1/2 k/ n +
                                                                                            p
L1/2 )△n → 0, (ii) L2 /n → 0, and (iii) k2 /n → 0. Also note that △n,β ≡ ||β̂ − βL || = L/n +
             p                         p                                p
Lξ0 (k)△n,1 k/n + L△n,2 + L−γ = L/n + L△n + L−γ since ξ0 (k) k/n = o(1). Here we can
take γf = s/(dx + d1 ) and γ = s/d because f0 and h0 belong to the Hölder class and we can apply
the approximation theorems (e.g., see Timan (1963), Schumaker (1981), Newey (1997), and Chen
(2007)). Therefore, the conclusion of Theorem 3 (a) follows from Lemma L1 applying the dominated
convergence theorem by ĝi and g0i are bounded.
   For Theorem 3 (b) note that for β̂ = (β̂ K′ , â′L )′ and βL = (β K′ , a′L )′ ,

      |fˆ − f0 |δ ≤ |φK (x, z1 )′ (β̂ K − β K )|δ + |φK (x, z1 )′ β K − f0 (x, z1 )|δ
                   ≤ ζδ (K)||β̂ K − β K || + O(K −s/(dx +d1 ) ) ≤ ζδ (K)||β̂ − βL || + O(K −s/(dx +d1 ) )
                                 p
                   = Op (ζδ (K)[ L/n + L△n + L−s/d ] + K −s/(dx +d1 ) )

where the second inequality holds by Assumption C1 (vii) and the last equality holds by (47). This
completes the proof.


                                                               31
C      Proof of asymptotic normality (Theorem 4 and 5)

C.1     Rate conditions

    Along the proof, we obtain rate conditions to bound terms. We collect them here. Define
                                        p                       p
             △ϕ
              n = (ζ1 (L) + L
                              1/2
                                  ξ0 (k) k/n + L1/2 )△n , △n,β = L/n + L△n + L−γ
                                             p                  p
             △T = (△ϕ   2
                      n) + L
                              1/2 ϕ
                                 △n + ζ0 (L) L/n, △T1 = ξ0 (k) k/n
                              √                          √
             △H = ζ0 (L)k1/2 / n + k1/2 △ϕ      −γ
                                           n + L ζ0 (L) k

            △dϕ = ζ0 (L)L△n,2 , △g = ζ0 (L)△n,β
             △Σ = ∆T + ζ0 (L)2 L/n, △Ĥ = (ζ1 (L)△n,β + ξ0 (k)△n,1 )L1/2 ξ0 (k)

                                                         √
and we need the following rate conditions for the             n-consistency and the consistency of the variance
matrix estimator Ω̂:
                       √               √                       √             √
                           nL−γ → 0,       nk1/2 L−γ → 0,          nk−γ1 → 0, nk−γ2 → 0
             k1/2 (△T1 + △H ) + L1/2 △T → 0, n−1 (ζ0 (L)2 L + ξ0 (k)2 k + ξ0 (k)2 kL4 ) → 0,
                 k1/2 (△T1 + △H ) + L1/2 △T + △dϕ → 0, △g → 0, △Σ → 0, △Ĥ → 0.
                                                    √         √           √
Dropping the dominated ones and assuming                nL−γ , nk−γ1 , and nk−γ2 are small enough, under
the following all the rate conditions are satisfied:

      ζ0 (L)k + ζ1 (L)k3/2 + ζ0 (L)L + Lζ1 (L)ξ0 (k) + L1/2 ζ1 (L)Lξ0 (k)k1/2 + L1/2 ξ0 (k)2 k1/2
                                                √                                                 → 0.
                                                   n

                                                          L2 +LL3 k+L1/2       4 3/2
                                                                        √ (L k +k )
                                                                                     5/2
For the polynomial approximations it becomes                              n
                                                                                               → 0 and for the spline
                               L3/2 +LL3/2 k 1/2 +L1/2√(L5/2 k+k 3/2 )+L3/2 k 3/2
approximations it becomes                               n
                                                                                  → 0.
C.2     Asymptotic variance terms

    Let pki = pk (Zi ) and pki = (p1i , . . . , pki )′ . We start with introducing additional notation:

             Σ = E[ψiL ψiL′ var(Yi |Zi , Vi )], T = E[ψiL ψiL′ ], T1 = E[pki pk′
                                                                              i ],                               (50)
            Σ1 = E[Vi2 pki pk′                                       2 k k′
                            i ], Σ2,l = E[(ϕl (Zi , Vi ) − ϕ̄l (Zi )) pi pi ],
                                          Xn
                    ∂h0i L k′                  ∂h0i L k′
           H11 = E[       ψi pi ], H̄11 =            ψ p /n
                    ∂Vi                        ∂Vi i i
                                                i=1
                                                          n
                                                         X ∂h0i
                            ∂h0i
           H12 = E[E[            |Zi ]ψiL pk′
                                           i  ], H̄ 12 =  E[     |Zi ]ψiL pk′
                                                                           i /n
                            ∂Vi                              ∂Vi
                                                         i=1
                                             n
                                             X
           H2,l = E[al ψiL pik′ ], H̄2,l =         al ψiL pk′
                                                           i /n, H1 = H11 − H12 , H̄1 = H̄11 − H̄12
                                             i=1
                                                              XL
             Ω̄ = AT −1 [Σ + H1 T1−1 Σ1 T1−1 H1′ +                       H2,l T1−1 Σ2,l T1−1 H2,l
                                                                                              ′
                                                                                                  ]T −1 A′ .
                                                                   l=1



                                                         32
                                                      h               P                i
Let T1 = I without loss of generality. Then Ω̄ = AT −1 Σ + H1 Σ1 H1′ + L             ′
                                                                        l=1 2,l 2,l 2,l T
                                                                           H   Σ   H      −1 A′ .

Let Γ be a symmetric square root of Ω̄−1 . Because T is nonsingular and var(Yi |Zi , Vi ) is bounded
away from zero, CΣ − I is positive semidefinite for some positive constant C. It follows that

                   ||ΓAT −1 || = {tr(ΓAT −1 T −1 A′ Γ′ )}1/2 ≤ {tr(ΓAT −1 CΣT −1 A′ Γ′ )}1/2
                                 ≤ {tr(CΓΩ̄Γ′ )}1/2 ≤ C

and therefore ||ΓAT −1 || is bounded. Next we show Ω̄ → Ω as k, K, L → ∞. Under Assumption R1,
we have A = E[ν ∗ (Z, V )ψiL′ ]. Take νL∗ (Z, V ) = AT −1 ψiL . Then note E[||ν ∗ (Z, V )−νL∗ (Z, V )||2 ] → 0
because (i) νL∗ (Z, V ) = E[ν ∗ (Z, V )ψiL′ ]T −1 ψiL is a mean-squared projection of ν ∗ (zi , vi ) on ψiL ; (ii)
ν ∗ (zi , vi ) is smooth and the second moment of ν ∗ (zi , vi ) is bounded, so it is well-approximated in
the mean-squared error as assumed in Assumption R1. Let νi∗ = ν ∗ (Zi , Vi ) and νLi
                                                                                  ∗ = ν ∗ (Z , V ). It
                                                                                       L    i i
follows that

            ∗                    ∗′
         E[νLi var(Yi |Zi , Vi )νLi ] = AT −1 E[ψiL var(Yi |Zi , Vi )ψiL′ ]T −1 A′ → E[νi∗ var(Yi |Zi , Vi )νi∗′ ].

It concludes that AT −1 ΣT −1 A′ converges to E[νi∗ var(Yi |Zi , Vi )νi∗ ′ ] (the first term in Ω) as k, K, L →
∞. Next let                                                                    
                                                   ∗        ∂h0i      ∂h0i
                                        bLi =   E[νLi            − E[      |Zi ] pk′  k
                                                                                  i ]pi
                                                            ∂Vi       ∂Vi
           h                          i
and bi = E νi∗ ∂h  0i
                 ∂Vi  − E[ ∂h0i
                           ∂Vi  |Z i ]  pk′  k
                                         i pi . Note that because (T1 )
                                                                             −1 = I, b
                                                                                         Li and bi are least
                                                                                          
                             ∗     ∂h0i        ∂h0i             k and ν ∗ ∂h0i − E[ ∂h0i |Z ] on pk , respec-
squares mean projections of νLi     ∂Vi  −  E[ ∂Vi  |Zi ]  on p i      i   ∂Vi      ∂Vi    i      i
tively. Then E[||bLi − bi ||2 ] ≤ CE[||νLi
                                        ∗ − ν ∗ ||2 ] → 0 where the first inequality holds because the
                                             i
mean square error of a least squares projection cannot be larger than the MSE of the variable being
projected. Also hnote that                    2
                       iE[||ρv (Zi ) − bi || ] → 0 as k → ∞ because bi is a least squares projection
           ∂h0i        ∂h0i
of νi∗     ∂Vi    −E   ∂Vi |Zi        on pki and it converges to the conditional mean as k → ∞. Finally note
that
                                                              
                                                                          
                                                     ∂h0i         ∂h0i
           E[bLi var(Vi |Zi )b′Li ]    = AT E   −1
                                                         ψiL
                                                          −E           |Zi pk′
                                                                             i E[var(Vi |Zi )pki pk′
                                                                                                  i ]
                                                     ∂Vi          ∂Vi
                                                                      
                                                ∂h0i        ∂h0i
                                         ×E pki      −E          |Zi ψiL′ T −1 A′
                                                ∂Vi         ∂Vi
                                       = AT −1 H1 Σ1 H1′ T −1 A′

and therefore we conclude AT −1 H1 Σ1 H1′ T −1 A′ converges to E[ρv (Z)var(X|Z)ρv (Z)′ ] (the second
term in Ω). Similarly we can show that for all l

                         AT −1 H2,l Σ2,l H2,l
                                          ′
                                              T −1 A′ → E[ρϕ̄l (Z)var(ϕl (Z, V )|Z)ρϕ̄l (Z)′ ].

We then conclude Ω̄ → Ω as k, K, L → ∞. This also implies Γ → Ω−1/2 and Γ is bounded.




                                                                   33
C.3      Influence functions and asymptotic normality
                                                                      √
    Next we derive the asymptotic normality of                         n(θ̂ − θ0 ). After we establish the asymptotic
normality, we will show the convergence of the each term in (20) to the corresponding terms
in (50). We show some of them here to be used for deriving the asymptotic normality. Note
||Tˆ − T || = Op (△T ) = op (1) and ||Tˆ1 − T1 || = Op (△T1 ) = op (1) . We also have ||ΓA(Tˆ −1 −
T −1 )|| = op (1) and ||ΓAT̂ −1/2 ||2 = Op (1) (see proof in Lemma A1 of Newey, Powell, and Vella
                                                                    P         ∂ϕl (Zi ,Vi ) L k′
(1999)). We next show ||H̄11 − H11 || = op (1). Let H11L = E[ L        l=1 al    ∂Vi       ψi pi ] and H̄11L =
Pn PL           ∂ϕl (Zi ,Vi ) L k′                                 PL      ∂ϕl (Zi ,Vi )
         l=1 al              ψi pi /n. Similarly define H12L = E[E[ l=1 al ∂Vi |Zi ]ψiL pk′      i ] and H̄12L =
Pi=1
  n      P  L
                   ∂Vi
                  ∂ϕl (Zi ,Vi )       L  k′
  i=1 E[ l=1 al        ∂Vi      |Zi ]ψi pi /n and let H1L = H11L − H12L . By Assumption N1 (i), L1 (iii)
and the Cauchy-Schwarz inequality,

              ||H1 − H1L ||2
                      ∂h0i       ∂h0i           X ∂ϕl (Zi , Vi )        ∂ϕl (Zi , Vi )
            ≤ CE[||{(       − E[      |Zi ]) −     al (            − E[                |Zi ])}ψiL pk′  2
                                                                                                   i || ]
                      ∂Vi        ∂Vi                       ∂Vi              ∂Vi
                                                 l
                                Xk
            ≤ CL−2γ E[||ψiL ||2        p2ji ] = O(L−2γ ζ0 (L)2 k).
                                          j=1

Next consider that by Assumption L1 (iii) and the Cauchy-Schwarz inequality,

                √                           XL        ∂ϕl (Zi , Vi ) 2 L 2 Xk
              E[ n||H̄11L − H11L ||] ≤ C(E[(       al               ) ||ψi ||        p2 ])1/2
                                               l=1        ∂Vi                   j=1 ji
                                             ∂hLi 2 L 2 Xk
                                     = C(E[(     ) ||ψi ||           p2 ])1/2 ≤ Cζ0 (L)k1/2
                                             ∂Vi                 j=1 ji

                                                        ∂hLi         PL            ∂ ϕ̃l (Zi ,Vi )       PL        ∂ϕl (Zi ,Vi )
where the first equality holds because                   ∂Vi    =         l=1 al        ∂Vi          =    l=1 al      ∂Vi          and the last
result holds because hLi ∈ Hn (i.e. |hLi |1 is bounded). Similarly by (39), the Cauchy-Schwarz
inequality, and the Markov inequality, we obtain
                                        Xn              XL    ∂ϕl (Zi , Vi )        ˆ
        H̄11 − H̄11L       ≤ Cn−1                   |           al           | · ||ψ̂iL − ψiL || · ||pki ||
                                              i=1         l=1      ∂Vi
                               Xn                                         Xn                       1/2
                                                    ˆL
                           ≤ C(               Ci ||ψ̂i − ψiL ||2 /n)1/2 ·              ||pki ||2 /n        ≤ Op (k1/2 △ϕ
                                                                                                                       n ).
                                        i=1                                                  i=1

                                                     √                      √
Therefore, we have ||H̄11 − H11 || = Op (ζ0 (L)k1/2 / n + k1/2 △ϕ    −γ
                                                                n + L ζ0 (L) k) ≡ Op (△H ) = op (1).
Similarly we can show that ||H̄12 − H12 || = op (1) and ||H̄2,l − H2,l || = op (1) for all l.
     Now we derive the asymptotic expansion to obtain the influence functions. Further define
                                                              P
ĝLi = fK (xi , z1i ) + h̃L (zi , v̂i ) where fK (xi , z1i ) = K                                             ′   L
                                                                j=1 βj φj (xi , z1i ) and h̃L (zi , v̂i ) = aL (ϕ (zi , v̂i ) −
E[ϕL (Zi , V̂i )|zi ]) and gLi = fK (xi , z1i ) + hL (zi , vi ). Recall βL = (β1 , . . . , βK , a′L )′ and let this βL
satisfy Assumption N1 (i). Then from the first order condition, we obtain the expansion similar to




                                                                     34
(43) as   21


                       ˆ                √
                  0 = ψ̂ L,n′ (y − ĝ)/ n                                                                          (51)
                       ˆ                                                                    √
                    = ψ̂ L,n′ (η − (ĝ − ĝˆL ) − (ĝˆL − ĝL ) − (ĝL − gL ) − (gL − g0 ))/ n
                       ˆ            ˆ                                                            √
                    = ψ̂ L,n′ (η − ψ̂ L,n (β̂ − βL ) − (ĝˆL − ĝL ) − (ĝL − gL ) − (gL − g0 ))/ n.

Similar to (44), we obtain

                                            ˆ                 √          √
                                   ||T̂ −1 ψ̂ L,n′ (gL − g0 )/ n|| = Op ( nL−γ ).                                  (52)

Also note that because α(·) is a linear functional and by Assumption N1 (i),
          √                                √                            √
               n||Γ(α(gL ) − α(g0 ))|| =    n||Γ|| · ||α(gL − g0 )|| ≤ C n kΓk · |ψ L′ (·)βL − g0 (·)|δ            (53)
                                               √
                                         = Op ( nL−γ ) = op (1).

Then from the linearity of α(·), (51), (52), and (53) we have
          √               √                           √                       √
           nΓ(θ̂ − θ0 ) =  nΓ(α(ĝ) − α(g0 )) = nΓ(α(ĝ) − α(gL )) + nΓ(α(gL ) − α(g0 ))                           (54)
                          √                     √
                        =  nΓA(β̂ − βL ) + nΓ{a(gL ) − a(g0 )}
                                   ˆ                                        √
                        = ΓAT̂ −1 ψ̂ L,n′ (η − (ĝˆL − ĝL ) − (ĝL − gL ))/ n + op (1).

C.3.1      Influence function for the first stage
                                                       ˆ                  √
    Now we derive the stochastic expansion of ΓAT̂ −1 ψ̂ L,n′ (ĝL − gL )/ n. Note that by a second
order mean-value expansion of each h̃Li = h̃L (zi , v̂i ) around vi (also write hLi = hL (zi , vi )),
                              Xn        ˆ                 √            Xn ˆ                  √
                    ΓAT̂ −1            ψ̂iL (ĝLi − gLi )/ n = ΓAT̂ −1    ψ̂iL (h̃Li − hLi )/ n
                                 i=1                                        i=1
                              Xn  ˆ dhLi             dhLi                    √
                = ΓAT̂ −1        ψ̂ L (        − E[       |Zi ])(Π̂i − Πi )/ n + ςˆ
                            i=1 i        dv           dVi
                                  Xn i             √                        Xn
                       −1     −1                                                                1 √
                = ΓAT̂ H̄1 T̂1              pi vi / n + ΓAT̂ −1 H̄1 T̂1−1
                                             k
                                                                                   pk (Πi − pk′
                                                                                             i λk )/ n             (55)
                                       i=1                                      i=1 i
                           Xn ˆ dhLi                   dhLi                       √
                  +ΓAT̂ −1          ψ̂iL (       − E[       |Zi ])(pk′
                                                                     i λ1k − Πi )/ n + ςˆ.
                               i=1         dvi          dVi
                             √                     P                                √
                     ς || ≤ C n||ΓAT̂ −1/2 ||ζ0 (L) ni=1 Ci ||Π̂i − Πi ||2 /n = Op ( nζ0 (L)△2n,1 ) =
The remainder term ||ˆ
op (1). Then by the essentially same proofs ((A.18) to (A.23)) in Lemma A2 of Newey, Powell, and
                                                                                     √
Vella (1999), we can show the second term and the third term in (55) are op (1) under nk−s1 /dz → 0
         √
(so that n Π0 (z) − λ1′   k
                       k p (z) 0 → 0 by Assumption L1 (iv)) and under k
                                                                         1/2 (△ +△ )+L1/2 △ →
                                                                                T1   H          T

  21
     If there exists an estimation error due to tolerance in minimization, take the error arbitrary small to justify this
asymptotic expansion.




                                                           35
0 (so that we can replace T̂1 with T1 , H̄1 with H1 , and T̂ with T respectively). We therefore obtain

                                  ˆ                  √              Xn                           √
                         ΓAT̂ −1 ψ̂ L,n′ (ĝL − gL )/ n = ΓAT −1 H1                      pki vi / n + op (1).              (56)
                                                                                   i=1

This derives the influence function that comes from estimating vi in the first step.
C.3.2     Influence function for the second stage
                                                        ˆ                    √
    Next we derive the stochastic expansion of ΓAT̂ −1 ψ̂ L,n′ (ĝˆL − ĝL )/ n:
                             n
                             X                                 n
                                                               X
                        −1      ˆL ˆ              √         −1    ˆ                                            √
               ΓAT̂            ψ̂i (ĝLi − ĝLi )/ n = ΓAT̂      ψ̂iL a′L (ϕ̄ˆL (zi ) − E[ϕL (Zi , V̂i )|zi ])/ n
                             i=1                                       i=1
                   X                Xn               X                 Xn                                   √
                        −1
          = ΓAT̂ {      H̄2,l T̂1−1       pki ϕ̃li +       H̄2,l T̂1−1      pki (ϕ̄l (zi ) − pk′i  λ2l,k )}/ n             (57)
                      l               i=1                l              i=1
                     Xn ˆ X                                    √               Xn ˆ                √
                  −1
            +ΓAT̂           ψ̂i L
                                     al (pi λl,k − ϕ̄l (zi ))/ n + ΓAT̂ −1
                                          k′ 2
                                                                                          ψ̂iL ρi / n
                                   i=1         l                                                   i=1

               P                 −1   Pn
where ρi =         l   al {pk′
                            i T̂1
                                            k
                                    j=1 pj (ϕl (zj , v̂j ) − ϕl (zj , vj ))/n − (E[ϕl (Zi , V̂i )|zi ] − ϕ̄l (zi ))}. We first
                                                            −1 Pn
focus on the last term in (57). Note that pk′          i T̂1
                                                                          k
                                                                  i=1 pi (ϕl (zi , v̂i ) − ϕl (zi , vi ))/n is a least squares
projection of ϕl (zi , v̂i ) − ϕl (zi , vi ) on pki and it converges to the conditional mean E[ϕl (Zi , V̂i )|zi ] −
                                                  P
ϕ̄l (zi ). Therefore we can write ρi = L             l=1 al ρil and ρil is the projection residual from the least
squares projection of ϕl (zi , v̂i ) − ϕl (zi , vi ) on pki for each l. It follows that E[ρi |Z1 , . . . , Zn ] = 0 and
therefore
                                                             XL
                       E[||ρi ||2 |Z1 , . . . , Zn ] ≤ E[L          ||ρil ||2 |Z1 , . . . , Zn ] ≤ L2 Op (△2n,2 )
                                                              l=1

where the first inequality holds by the Cauchy-Schwarz inequality and the second inequality holds
by a similar proof to (31) and by the Markov inequality. It follows that by Assumption L1 (iii) and
the Cauchy-Schwarz inequality,
              Xn          ˆ       √                             ˆ
         E[              ψ̂iL ρi / n |Z1 , . . . , Zn ] ≤ (E[||ψ̂iL ||2 ||ρi ||2 |Z1 , . . . , Zn ])1/2 ≤ Cζ0 (L)L△n,2 .
                   i=1

                         Pn       ˆL √
This implies that            i=1 ψ̂i ρi / n   = Op (ζ0 (L)L△n,2 ) ≡ Op (△dϕ ) = op (1).
    Then again by the essentially same proofs ((A.18) to (A.23)) in Lemma A2 of Newey, Powell, and
                                                                                      √
Vella (1999), we can show the second term and the third term in (57) are op (1) under nk−s2 /dz → 0
         √                                                                      √ 1/2 −s/d
(so that n|ϕ̄l (z) − λ2′     k
                       l,k p (z)|0 → 0 for all l by Assumption L1 (iv)), under   nk L      → 0 (so
     √ 1/2                  ′  L                                                   1/2
that nk |h0 (z, v) − aL ϕ̃ (z, v)|0 → 0 by Assumption L1 (iv)), and under k (△T1 + △H ) +
L1/2 △T + △dϕ → 0 (so that we can replace T̂1 with T1 , H̄2,l with H2,l , and T̂ with T respectively).
We therefore obtain

                          ˆ                    √           X      Xn                                  √
                 ΓAT̂ −1 ψ̂ L,n′ (ĝˆL − ĝL )/ n = ΓAT −1   H2,l                           pki ϕ̃li / n + op (1).         (58)
                                                                          l           i=1

This derives the influence function that comes from estimating E[ϕli |Zi ]’s in the middle step.
                                       ˆ
   We can also show that replacing ψ̂iL with ψiL does not influence the stochastic expansion by



                                                                  36
(39). Therefore by (54), (56), and (58), we obtain the stochastic expansion,
      √                                               Xn                √    X      Xn                       √
          nΓ(θ̂ − θ0 ) = ΓAT −1 (ψ L,n′ η − H1                  pki vi / n −   H2,l                pki ϕ̃li / n) + op (1).
                                                          i=1                     l          i=1

To apply the Lindeberg-Feller theorem, we check the Lindeberg condition. For any vector q with
                                                     P                  √
||q|| = 1, let Win = q ′ ΓAT −1 (ψiL ηi − H1 pki vi − l H2,l pki ϕ̃li )/ n. Note that Win is i.i.d, given
n and by construction, E[Win ] = 0 and var(Win ) = O(1/n). Also note that ||ΓAT −1 || ≤ C,
||ΓAT −1 Hj || ≤ C||ΓAT −1 || ≤ C by CI−Hj Hj′ being positive semidefinite for j = 1, (2, 1), . . . , (2, L).
                 P                 PL                PL
Also note that ( L           4   2        2 2
                   l=1 ϕ̃li ) ≤ L ( l=1 ϕ̃li ) ≤ L
                                                   3        4
                                                      l=1 ϕ̃li . It follows that for any ε > 0,

                                2
            nE[1(|Win | > ε)Win   ] = nε2 E[1(|Win | > ε)(Win /ε)2 ] ≤ nε−2 E[|Win |4 ]
                                                                                 X
          ≤ Cnε−2 {E[||ψiL ||4 E[ηi4 |Zi , Vi ]] + E[||pki ||4 E[Vi4 |Zi ]] + L3   E[||pki ||4 E[ϕ̃4li |Zi ]]}/n2
                                                                                         l
          ≤ Cn−1 (ζ0 (L)2 L + ξ0 (k)2 k + ξ0 (k)2 kL4 ) = o(1).

              √
Therefore,  nΓ(θ̂ − θ0 ) →d N (0, I) by the Lindeberg-Feller central limit theorem. We have shown
                                                            √
that Ω̄ → Ω and Γ is bounded. We therefore also conclude n(θ̂ − θ0 ) →d N (0, Ω−1 ).
C.4       Consistency of the estimate of the asymptotic variance

    Now we show the convergence of the each term in (20) to the corresponding terms in (50). Let
η̂i = yi − ĝ(zi , v̂i ). Note that η̂i∗ ≡ η̂i2 − ηi2 = −2ηi (ĝi − g0i ) + (ĝi − g0i )2 and that maxi≤n |ĝi − g0i | =
                                                               ˆ                                    ˆ
Op (ζ0 (L)△n,β ) = op (1) by (49). Let D̂ = ΓAT̂ −1 ψ̂ L,n′ diag{1 + |ηi |, . . . , 1 + |ηn |}ψ̂ L,n T̂ −1 A′ Γ′ and
             ˆ
note that ψ̂ L,n and T̂ only depend on (Z1 , V1 ), . . . , (Zn , Vn ) and thus E[D̂|(Z1 , V1 ), . . . , (Zn , Vn )] ≤
                                                                                          P     ˆ ˆ
CΓAT̂ −1 A′ Γ′ = Op (1). Therefore, ||D̂|| = Op (1) as well. Next let Σ̃ = ni=1 ψ̂iL ψ̂iL′ ηi2 /n. Then,

                                                             ˆ                                 ˆ
              ||ΓAT̂ −1 (Σ̂ − Σ̃)T̂ −1 A′ Γ′ || = ||ΓAT̂ −1 ψ̂ L,n′ diag{η̂1∗ , . . . , η̂n∗ }ψ̂ L,n T̂ −1 A′ Γ′ ||          (59)
                                                   ≤ Ctr(D̂) max |ĝi − g0i | = Op (1)op (1).
                                                                     i≤n


Also by the essentially same proofs in Lemma A2 of Newey, Powell, and Vella (1999),

                                 ||Σ̃ − Σ|| = Op (∆T + ζ0 (L)2 L/n) ≡ Op (△Σ ) = op (1),                                     (60)
          ||ΓAT̂ −1 (Σ̂ − Σ)T̂ −1 A′ Γ′ || = op (1), ||ΓA(T̂ −1 ΣT̂ −1 − T −1 ΣT −1 )A′ Γ′ || = op (1).

It also follows that ||Σ̂ − Σ|| = op (1) because ||ΓAT̂ −1 || = Op (1). Then, by (59-60) and the triangle
inequality, we find ||ΓAT̂ −1 Σ̂Tˆ −1 A′ Γ′ − ΓAT −1 ΣT −1 A′ Γ′ || = op (1). It remains to show that for
j = 1, (2, 1), . . . , (2, L),

                       ΓA(T̂ −1 Ĥj T̂1−1 Σ̂j T̂1−1 Ĥj′ T̂ −1 − T −1 Hj Σj Hj′ T −1 )A′ Γ′ = op (1).                        (61)

As we have shown ||Σ̂−Σ|| = op (1), similarly we can show ||Σ̂j −Σj || = op (1), j = 1, (2, 1), . . . , (2, L).
We focus on showing ||Ĥj − H̄j || = op (1) for j = 1, (2, 1), . . . , (2, L). First note that ||Ĥ11 − H̄11 || =


                                                                37
     Pn     PL                 (zi ,vi ) ˆL k
                          ∂ϕl (zi ,v̂i )
||     i=1 (    l=1 âl− al ∂ϕl∂v
                             ∂vi i
                                        )ψ̂i p (zi )′ /n||. By the Cauchy-Schwarz inequality, (38), and
                             P           ˆ                 Pn      ˆL 2 k 2
Assumption L1 (iii), we have ni=1 ||ψ̂iL pk′      2
                                              i || /n ≤
                                                                                                 2
                                                            i=1 ||ψ̂i || ||pi || /n = Op (Lξ0 (k) ). Also note
that by the triangle inequality, the Cauchy-Schwarz inequality, and by Assumption C1 (vi) and
                                                            ∂ϕl (zi ,vi )
(32), applying a mean value expansion to                       ∂vi          w.r.t vi ,

          Xn            XL   ∂ϕl (zi , v̂i )        ∂ϕl (zi , vi ) 2
                   ||             (âl        − al                )|| /n
          i=1        l=1          ∂vi                   ∂vi
         Xn         XL                  ∂ϕl (zi , vi ) 2           Xn          XL            ∂ϕl (zi , v̂i ) ∂ϕl (zi , vi ) 2
     ≤ 2         ||        (âl − al )                || /n + 2             ||         âl (                 −                   )|| /n
             i=1       l=1                   ∂vi                       i=1       l=1             ∂vi                  ∂vi
                      Xn          ∂ ϕ̃L (zi , vi ) 2             Xn         XL         ∂ 2 ϕl (zi , ṽi )
     ≤ C||â − aL ||2          ||                 || /n + C1             ||        âl                    (Π̂i − Πi )||2 /n
                          i=1          ∂vi                           i=1       l=1            ∂vi2
                      Xn          ∂ ϕ̃L (zi , vi ) 2                                   Xn            XL            ∂ 2 ϕl (zi , ṽi ) 2
     ≤ C||â − aL ||2          ||                 || /n + C1 max ||Π̂i − Πi ||2                   ||           âl                   || /n
                          i=1          ∂vi                       1≤i≤n                       i=1           l=1          ∂vi2
     = Op (ζ12 (L)△2n,β + ξ02 (k)△2n,1 )

where ṽi lies between v̂i and vi , which may depend on l. We therefore conclude by the triangle in-
equality and the Cauchy-Schwarz inequality, ||Ĥ11 −H̄11 || ≤ Op ((ζ1 (L)△n,β +ξ0 (k)△n,1 )L1/2 ξ0 (k)) =
Op (△Ĥ ) = op (1). Similarly we can show that ||Ĥ12 − H̄12 || = op (1) and ||Ĥ2,l − H̄2,l || = op (1)
l = 1, . . . , L. We have shown that ||H̄j − Hj || = op (1) for j = 1, (2, 1), . . . , (2, L) previously.
Therefore, ||Ĥj − Hj || = op (1) for j = 1, (2, 1), . . . , (2, L). Then by the similar proof like (59) and
(60), the conclusion (61) follows. From (59-61) finally note that by Γ is bounded, ||Ω̂ − Ω̄|| ≤
C||ΓΩ̂Γ′ − ΓΩ̄Γ′ || = op (1).




                                                                     38
References
Ai, C., and X. Chen (2003): “Efficient Estimation of Models With Conditional Moment Restric-
  tions Containing Unknown Functions,” Econometrica, 71(6), 1795–1843.

Blundell, R., X. Chen, and D. Kristensen (2007): “Semi-Nonparametric IV Estimation of
 Shape-Invariant Engel Curves,” Econometrica, 75, 1613–1669.

Chen, X. (2007): “Large Sample Sieve Estimation of Semi-Nonparametric Models,” Handbook of
 Econometrics, Volume 6.

Chen, X., and D. Pouzo (2009): “Efficient Estimation of Semiparametric Conditional Moment
 Models with Possibly Nonsmooth Residuals,” Journal of Econometrics, 152, 46–60.

        (2011): “Estimation of Nonparametric Conditional Moment Models with Possibly Nons-
  mooth Generalized Residuals,” Econometrica forthcoming.

Chen, X., and M. Reiss (2011): “On Rate Optimality for Ill-posed Inverse Problems in Econo-
 metrics,” Econometric Theory, 75, 1–25.

Chernozhukov, V., and C. Hansen (2005): “An IV Model of Quantile Treatment Effects,”
 Econometrica, 73, 245–262.

Darolles, S., Y. Fan, J. Florens, and E. Renault (2011): “Nonparametric Instrumental
 Regression,” Econometrica forthcoming.

Darolles, S., J. Florens, and E. Renault (2006): “Nonparametric Instrumental Regression,”
 working paper, GREMAQ, University of Toulouse.

Engl, H., M. Hanke, and A. Neubauer (1996): Regularization of Inverse ProblemsDordrecht:
 Kluwer Academic.

Florens, J. (2003): “Inverse Problems and Structural Econometrics: The Example of Instrumental
  Variables,” Advances in Economics and Econometrics: Theory and Applications, Eight World
  Congress,Econometric Society Monographs, pp. 1–25.

Florens, J., J. Heckman, C. Meghir, and E. Vytlacil (2008): “Identification of Treatment
  Effects Using Control Functions in Models with Continuous Endogenous Treatment and Hetero-
  geneous Effects,” Econometrica, 76, 1191–1206.

Hall, P., and J. Horowitz (2005): “Nonparametric Methods for Inference in the Presence of
 Instrumental Variables,” Annals of Statistics, 33, 2904–2929.

Hausman, J. (1978): “Specification Tests in Econometrics,” Econometrica, 46, 1251–1272.

Heckman, J. (1978): “Specification Tests in Econometrics,” Econometrica, 46, 1261–1272.

Horowitz, J. (2007): “Asymptotic Normality of a Nonparametric Instrumental Variables Estima-
 tor,” International Economic Review, 48, 1329–1349.

        (2011a): “Applied Nonparametric Instrumental Variables Estimation,” Econometrica, 79,
  347–394.

         (2011b): “Specification Testing in Nonparametric Instrumental Variables Estimation,”
  Journal of Econometrics forthcoming.

                                              39
Imbens, G., and W. Newey (2009): “Identification and Estimation of Triangular Simultaneous
  Equations Models Without Additivity,” Econometrica, 77, 1481–1512.

Johannes, J., S. V. Bellegem, and A. Vanhems (2011): “Convergence Rates for Ill-posed
  Inverse Problems with an Unknown Operator,” Econometric Theory, 27, 522–545.

Kress, R. (1999): Linear Integral EquationsNew York: Springer-Verlag.

Matzkin, R. (2003): “Nonparametric Estimation of Nonadditive Random Functions,” Economet-
 rica, 71, 1339–1375.

Murphy, K., and R. Topel (1985): “Estimation and Inference in Two-Step Econometric Models,”
 Journal of Business and Economic Statistics, 3, 370–379.

Newey, W. (1984): “A Method of Moments Interpretation of Sequential Estimators,” Economics
 Letters, 14, 201–206.

       (1997): “Convergence Rates and Asymptotic Normality for Series Estimators,” Journal of
  Econometrics, 79, 147–168.

Newey, W., and J. Powell (2003): “Instrumental Variable Estimation of Nonparametric Mod-
 els,” Econometrica, 71, 1565–1578.

Newey, W., J. Powell, and F. Vella (1999): “Nonparametric Estimation of Triangular Simul-
 taneous Equaitons Models,” Econometrica, 67, 565–603.

Rivers, D., and Q. Vuong (1988): “Limited Information Estimators and Exogeneity Tests for
  Simultaneous Probit Models,” Journal of Econometrics, 39, 347–366.

Schumaker, L. (1981): Spline Fuctions:Basic Theory.

Smith, R., and R. Blundell (1986): “An Exogeneity Test for a Simultaneous Equation Tobit
  Model with an Application to Labor Supply,” Econometrica, 54, 679–685.

Telser, L. (1964): “Iterative Estimation of a Set of Linear Regression Equations,” Journal of the
 American Statistical Association, 59, 845–862.

Tikhonov, A., A. Goncharsky, V. Stepanov, and A. Yagola (1995): Numerical Methods
  for the Solution of Ill-Posed ProblemsMathematics and Its Applications, New York: Springer.

Timan, A. (1963): Theory of Approximation of Functions of a Real Variable.

Wooldridge, J. (2005): “Unobserved heterogeneity and estimation of average partial effects,” in
 Identification and Inference for Econometric Models: Essays in Honor of Thomas Rothenberg, ed.
 by S. J. E. Andrews, D.W.K., pp. 27–55. Cambridge.

Wooldridge, J., and L. Papke (2008): “Panel Data Methods for Fractional Response Variables
 with an Application to Test Pass Rates,” Journal of Econometrics, 145, 121–133.




                                               40
