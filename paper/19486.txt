                                     NBER WORKING PAPER SERIES




ACCOUNTING FOR EXPECTATIONAL AND STRUCTURAL ERROR IN BINARY CHOICE PROBLEMS:
                       A MOMENT INEQUALITY APPROACH

                                              Michael J. Dickstein
                                               Eduardo Morales

                                             Working Paper 19486
                                     http://www.nber.org/papers/w19486


                           NATIONAL BUREAU OF ECONOMIC RESEARCH
                                    1050 Massachusetts Avenue
                                      Cambridge, MA 02138
                                         September 2013




     We thank Steve Berry, Jesús Carro, Phil Haile, Bo Honoré, Guido Imbens, Ariel Pakes, Christoph
     Rothe, Bernard Salanie and seminar participants at Princeton University and the SITE Summer Workshop
     for helpful comments. The views expressed herein are those of the authors and do not necessarily reflect
     the views of the National Bureau of Economic Research.

     NBER working papers are circulated for discussion and comment purposes. They have not been peer-
     reviewed or been subject to the review by the NBER Board of Directors that accompanies official
     NBER publications.

     © 2013 by Michael J. Dickstein and Eduardo Morales. All rights reserved. Short sections of text, not
     to exceed two paragraphs, may be quoted without explicit permission provided that full credit, including
     © notice, is given to the source.
Accounting for Expectational and Structural Error in Binary Choice Problems: A Moment
Inequality Approach
Michael J. Dickstein and Eduardo Morales
NBER Working Paper No. 19486
September 2013
JEL No. C14,C25,F14,L10

                                               ABSTRACT

Many economic decisions involve a binary choice - for example, when consumers decide to purchase
a good or when firms decide to enter a new market. In such settings, agents’ choices often depend
on imperfect expectations of the future payoffs from their decision (expectational error) as well as
factors that the econometrician does not observe (structural error). In this paper, we show that expectational
error, under an assumption of rational expectations, is a source of classical measurement error, and
we propose a novel moment inequality estimator that accounts for both expectational error and structural
error in a binary choice model. With simulated data and Chilean firm-level customs data, we illustrate
the identifying power of our inequalities and show the biases that arise when one ignores either source
of error. We use the customs data to estimate the fixed costs exporters face when entering a new market.


Michael J. Dickstein
Department of Economics
Stanford University
579 Serra Mall
Stanford, CA 94305
and NBER
mjd@stanford.edu

Eduardo Morales
Department of Economics
Princeton University
310 Fisher Hall
Princeton, NJ 08544
and NBER
ecmorales@princeton.edu




An online appendix is available at:
http://www.nber.org/data-appendix/w19486
1    Introduction
When firms or consumers face a choice –for example, when choosing to introduce a new
product, consume a good, switch jobs, or enter new geographic markets– they seldom
possess complete information about the benefits that will result from their decision.
These decision-makers must therefore form expectations of the likely benefits, which
may differ from the returns realized ex-post. For the econometrician, this distinction
poses a problem: either find direct measures of an agent’s ex-ante expected returns, a
rarity, or devise an approximation to the unobserved expectation. In the latter case,
researchers typically specify the covariates in the agent’s information set or choose a
proxy for the expectations, such as observed ex-post returns. When expectations are
imperfect, these approximations introduce error.
    We propose a new method to estimate the parameters of a binary discrete choice
problem in the presence of expectational error. Our strategy employs moment inequal-
ities that permit two types of errors: (1) an individual structural error, which captures
choice variables known to the agent when she makes a decision but not observed by
the econometrician, and (2) expectational error, which reflects a mismatch between the
agents expectations of payoff-relevant variables and the ex-post realizations of these
variables. For example, consider a firm’s decision to enter a market, a prototypical
setting we use later as an application of our approach. At the time of its entry decision,
the firm knows the costs it must pay to enter the market. However, it has only an
expectation of the sales revenue it will earn. In this setting, expectational error is the
difference between observed ex-post sales revenue and the agent’s unobserved expec-
tations; the structural error includes variables like advertising expenditure, which are
known to the agent when she makes a decision but are not in the econometrician’s data.
    The expectational error has a parallel in regression analysis: if we think of the
agents’ expectations as a true unobserved covariate and use future observed realizations
as a proxy for that unobserved covariate, then the rational expectations assumption
implies that what we label expectational error can also be thought of as a source
of classical measurement error. That is, the measurement or expectational error is
mean independent of the true covariate and correlated with the observed covariate.
Chesher (2010), Chesher (2011) and Blundell and Powell (2004) introduce identification
frameworks that allow the researcher to identify the parameters of binary choice models
with endogenous regressors. None of these frameworks, however, accounts for the two
types of errors that our model of an agent’s choice generates.


                                            1
    The goal of this paper is to estimate the parameters of the deterministic portion of
our binary choice model, which takes the form of an index function that is linear in
observable covariates. We allow two distinct errors by relying on a restriction that arises
naturally from the rational expectations assumption. Under rational expectations, the
distribution of the expectational error is mean independent of any variable the agent
uses to form expectations. Thus, any variable observed by the econometrician and
used by the agent to form her expectations is orthogonal to the expectational error.
Our benchmark statistical model combines this mean independence assumption on the
expectational error with a standard parametric assumption on the structural error.1
We show that the parameters of the index function are only partially identified in
this setting and demonstrate the identifying power of moment inequalities generated
from the model’s restrictions. Our proposed identification and estimation approach can
be thought of as an extension of binary probit and logit models, adjusted to permit
distribution-free measurement error in the covariates.
    Our partial identification approach relies on two novel types of moment inequalities,
score function inequalities and revealed preference inequalities. Using these inequalities,
we are able to obtain a set with the following properties: (a) it contains the true values
of the index coefficients we aim to estimate; and (b) its size increases in the variance
of the expectational error. Our moment inequalities are also very simple to compute.2
They may be combined with standard inference methods for moment inequalities to
provide set estimates of the index-function coefficients.3
    We implement our estimator in two settings. First, we design a simulation exercise
to illustrate the properties of our moment inequalities. Here, we test the performance
of our estimator by comparing our output to the true parameters that generated the
dataset for the exercise. Second, we use actual data to estimate a version of the single-
agent static entry model in Morales et al. (2011).
    Our simulation results illustrate the failure of standard estimation techniques in the
presence of both structural and expectational errors. First, we demonstrate that max-
   1
      We also show how to estimate the parameters of our model when we do not impose a parametric
assumption on the structural error. Dropping this parametric assumption greatly reduces the identi-
fication power of our model. Therefore, we focus our analysis on the model that combines parametric
assumptions on the structural error with non-parametric restrictions on the expectational error.
    2
      Accompanying Matlab code to implement our methodology is posted on https://www.stanford.
edu/~mjd and https://sites.google.com/site/edumoralescasado/.
    3
      For references on how to compute confidence intervals in partially identified settings, see Cher-
nozhukov et al. (2007), Andrews and Soares (2010), Pakes et al. (2011), Andrews and Shi (2011a), and
Andrews and Shi (2011b).


                                                  2
imum likelihood estimation, which assumes no expectational error, results in estimates
that are biased. We confirm that the size of the bias is increasing in the variance of
the omitted error. Second, we apply moment inequality estimators defined in Pakes
(2010) and Pakes et al. (2011), which rule out a choice-specific and individual-specific
structural error; our results reveal that these inequalities are biased towards zero, gen-
erating identified sets that are too small and may not contain the true value of the
parameter. In contrast, our score function and revealed preference inequalities guaran-
tee that the true value of the parameter vector is contained in the identified set defined
by our moment inequalities, even in the presence of both types of errors. Moreover,
our simulation results show that our score function inequalities define bounds for the
unknown parameter vector that are far more informative than bounds generated using
only revealed preference inequality restrictions.
    In our analysis of the single-agent static entry model, we use firm-level export data
to estimate the parameters that determine the costs firms incur upon exporting to a
new foreign market. Here, firms determine export destinations in each period based
on (a) expected potential net revenue obtained by exporting to each location, and (b)
the fixed costs of exporting. The fixed costs may depend on factors known to the firm
when it decides whether to export but that are not in the data, generating structural
error. As in our benchmark statistical model, we assume that the distribution of this
structural error is known up to a scale parameter.
    We estimate the model using firm-level export data for the Chilean chemical sec-
tor during 1996-2004. The dataset contains information on firms’ export flows disag-
gregated by destination country and year, production inputs, and other firm-specific
characteristics.4 Although the data does not provide information on firms’ expecta-
tions of potential export revenues, it does report realized export revenues that firms
obtain in country-years with positive exports. Using this data, we estimate a reduced-
form equation that allows us to predict firm-specific export revenues for each potential
country-year pair.
    Our moment inequality estimator indicates that firms’ profit margins in export
markets are between 7.8% and 14.2% and that their per-year fixed costs of exporting
from Chile to the US, a distance of 5, 000 − 6, 000 miles, are about three times as large
as the costs of exporting to neighboring Argentina. A comparison of the maximum
likelihood estimates with our moment inequality estimates indicates that ignoring the
  4
      The data were provided by the Chilean Customs Agency and the Chilean Annual Industrial Survey.



                                                  3
expectational error firms’ make when deciding whether to export will bias the parameter
estimates in two directions: the maximum likelihood estimator (a) underpredicts the
number of exporters in countries that are close to the country of origin of the firm
and overpredicts entry in countries that are more distant; and (b) underpredicts the
probability of entry by large firms and overpredicts this probability for small firms.
    We structure the remainder of the paper as follows. In Section 2, we show that
expectational error is a source of classical measurement error, and introduce a static
binary choice model with both structural and expectational errors. In Section 3, we
introduce both score function and revealed preference inequalities and examine the
properties of these inequalities. Section 4 studies how the presence of both structural
and measurement error affects the performance of estimators that neglect one of the two
types of error. Section 5 generalizes the results on the benchmark model introduced in
Section 2: we eliminate the parametric restrictions on the distribution of the structural
error and introduce moment inequalities that identify the true value of the parameter
vector without these assumptions. Section 6 describes the export decision problem
and applies our inequality framework to estimate the structural parameters of a firm’s
export decision. Section 7 concludes.5


2       Static Binary Choice Model
In this section, we present our statistical model. We first demonstrate that expecta-
tional errors affecting agents with rational expectations are a special case of classical
measurement error (Section 2.1). We then introduce a binary choice model that includes
both structural and classical measurement error (Section 2.2). Finally, we compare our
approach with alternative models in the literature (Section 2.3), and present a setting
in which we carry out our simulation exercise (Section 2.4).


2.1     Expectational and Measurement Error
In this section, we demonstrate the similarities between expectational error in a ratio-
nal expectations model and measurement error in a classical errors-in-variables model.
Although this holds for decision problems in which the decision variable is continuous or
    5
    We include all proofs in an appendix to the main text. Details of the empirical application appear
in a separate online appendix.




                                                  4
discrete, we show these similarities in the context of a binary choice problem to remain
consistent with our statistical model of interest.
    Suppose individual i faces a choice between two alternatives, j ∈ {0, 1}. Firm i’s
difference in payoff between alternative 1 and alternative 0 is:6

                                         U = βE[X|J ] + ν,                                           (1)

where J is the information set the agent uses for her decision, E[·] denotes the expec-
tation operator based on the subjective believes of the agent, and β is the parameter
vector representing the agent’s preferences. The variable X may capture, for example:
characteristics of alternatives that the agent imperfectly observes at the time of her
decision (e.g. demand level in a market), decisions by other agents (e.g. in simultane-
ous move games), or a continuation value function (e.g. dynamic discrete problems).
The variable ν captures characteristics that the econometrician does not observe but
that are included in J . While the parameter vector β is assumed to be constant
across individuals, all the variables may vary with i. Denote the expectational error as
 = βX − βE[X|J ] and express the difference in payoffs as:

                                          U = βX + ν + ε,                                            (2)

where ε = −β. Define E[·] as the expectation operator of the data generating process
(DGP) for X. The DGP captures the distribution of X across individuals in the popu-
lation. Under the rational expectations assumption, agents’ subjective beliefs coincide
with the true DGP (i.e. E[·] = E[·]), hence: E[ε|J ] = 0 and E[ε|X] 6= 0. Therefore,
for any variable Z ∈ J ,
                                     E[ε|Z] = 0.                                    (3)

    An alternative way to derive the statistical model defined by equations (2) and (3)
is to assume that the decision maker observes the true value of some characteristic, X ∗ ,
and selects her preferred alternative on the basis of the utility function

                                            U = βX ∗ + ν,                                            (4)

Suppose that, instead of X ∗ , the econometrician observes some random variable X such
   6
    For notational simplicity: (1) we eliminate the subindex i except in cases for which it is strictly
necessary; (2) we express all variables as the difference between their magnitude for alternative 1 minus
their value for alternative 0.


                                                   5
that  = X − X ∗ , so that we can rewrite the payoff function as:

                                         U = βX + ν + ε,                                            (5)

where ε = −β. If we impose the classical errors-in-variables assumptions, it holds that
E[ε|X ∗ ] = 0 and E[ε|X] 6= 0, and we define a variable Z as an instrumental variable
(IV) if and only if it verifies
                                     E[ε|Z] = 0.                                     (6)

    Equations (2) and (3) are identical to equations (5) and (6). The interpretation of
the expectational error of an agent with rational expectations as a special case of the
measurement error in a classical errors-in-variables model is obtained when we think of
the expectation as the unobserved true covariate (i.e. E[X|J ] = X ∗ ) and of the realized
future value as the observed mismeasured covariate (i.e. X).7
    The difference between the rational expectations assumption and the classical errors-
in-variables assumption is that the former implies a stronger orthogonality condition.
While the classical errors-in-variables assumption exclusively imposes E[ε|X ∗ ] = 0, the
rational expectations assumption implies E[ε|J ] = 0, with the set J including but not
necessarily restricted to the variable X ∗ = E[X|J ]. For example, the interpretation of
ε as expectational error implies that E[ε|ν] = 0. On the contrary, this orthogonality
restriction is not imposed by the interpretation of ε as measurement error. Analogously,
as long as Z ∈ J , equation (3) is an implication of the rational expectations assumption.
However, equation (6) is an additional restriction that is not automatically implied by
the classical errors-in-variables assumption. Given that the notion of measurement error
implies fewer assumptions than that of expectational error, we present our model in
Section 2.2 in terms of the former and we make explicit which additional assumptions
we need for our estimator to identify the true value of the parameter vector β.


2.2     Model
In this section, we introduce a binary choice model in which agents’ choices depend on
three components: an index function that is linear in observable variables, structural
error, and measurement error.
   7
    As the comparison above shows, ε potentially captures both reporting error (pure measurement
error) and differences between agents’ expectations of each alternative’s characteristics and the actual
values of these characteristics (expectational error). In this case ε = −β[(X obs − X) + (X − E[X|J ])],
where X obs is the observed measure of the realization of the variable X.


                                                   6
Agents’ decisions. For each individual i, we normalize the utility of one alternative to
zero and express the utility of the other alternative as

                                         U = βX ∗ + ν,                                         (7)

where β ∈ Γβ ∈ RK , X ∗ ∈ X ∗ ∈ RK and ν ∈ R. We define a dummy variable d as
d = 1{U ≥ 0}, where 1{·} is the indicator function that takes a value of 1 when {·}
is true and a value of 0 otherwise. Therefore, we can write the individual revealed
preference inequality as:

                                (d − (1 − d)) · (βX ∗ + ν) ≥ 0.                                (8)

The vector β groups the index coefficients and is defined up to a scalar normalization
(possibly involving the distribution of ν). The term βX ∗ is the single index determining
the choice dummy, d. Both X ∗ and ν belong to the information set of the agent at the
time she makes a decision: (X ∗ , ν) ∈ J .

Measurement model. The econometrician does not observe ν and might observe X ∗
with error. We denote Z1 as the P × 1 subvector of X ∗ that is measured without error,

                                            Z1 = X1∗ ,                                         (9)

and X2∗ as the (K − P ) × 1 subvector of X ∗ that may be measured with error

                                          X = X2∗ + ,                                        (10)

where  ∈ RK−P , and 0 ≤ P ≤ K.8 Once we incorporate equations (9) and (10) into
equation (8), the individual-level revealed preference inequality becomes

                          (d − (1 − d)) · (β1 Z1 + β2 X + ν + ε) ≥ 0,                         (11)

with ε = −β2 . In equation (11), only the vector (d, Z1 , X) is observed by the econo-
metrician.9 The econometrician also observes a vector of instrumental variables, Z2 ∈
Z2 ∈ RT , T ≥ K − P . We stack both the true covariates that are observed without
   8
    This model is consistent with either all or none of the K covariates being observed with error.
   9
    This measurement model assumes that X and Z1 are observed independently of the alternative
chosen by the agent. Section 6 considers an alternative measurement model in which, for each agent,
we only observe X if she chooses one particular alternative (i.e. exports).

                                                7
error and the instruments into a single vector Z = (Z1 , Z2 ), Z ∈ Z ∈ RP +T .
    For our moment inequality estimator, a key restriction of equation (7) is that the
index function, βX ∗ , is linear in the unobserved true covariate, X2∗ . While equation
(7) assumes that this index function is also linear in the observed true covariates, X1∗ ,
our moment inequality estimator also applies in cases in which this index function is
nonlinear in X1∗ .

Assumptions on error terms. We impose the following assumptions on the distribution
of (ν, ε) conditional on the vector (d, X ∗ , Z2 , X):

Assumption 1 The random variable ν is independent of the random vector (X ∗ , Z2 );
i.e. Fν (ν|X ∗ , Z2 ) = Fν (ν).

Assumption 2 The marginal distribution function of ν is known up to a scale parame-
ter, log concave, has mean zero, and, for any y in the support of ν, verifies the following
property:

                                         ∂ 2 E[ν|ν ≥ y]
                                                        ≥ 0.
                                               ∂y 2


Assumption 3 The distribution of ε conditional on (d, X ∗ , Z2 ) has support (−∞, ∞)
and has mean zero; i.e. E[ε|d, X ∗ , Z2 ] = 0.

As in the standard binary probit or logit model, Assumption 1 imposes independence
between the structural error, ν, and the true vector of covariates, X ∗ . Therefore,
without expectational error, the error term in equation (11) is exogenous to the vector of
observed covariates. Assumption 1 also imposes that the observed vector of instruments,
Z2 , is independent of the structural error.
    Assumption 2 implies that the distribution of the structural error is known to
the econometrician and restricts this distribution to be log concave and have a right-
truncated expectation that is convex in the truncation point.10 Both the normal and
  10
    We can relax Assumption 2 and assume instead that the econometrician does not know the true
distribution Fν but just a finite set of distributions Fν to which Fν belongs. In this case, the econo-
metrician may generate different distribution-specific identified sets for β for each Fν ∈ Fν . The union
of these distribution-specific identified sets incorporates the econometrician’s uncertainty in Fν .




                                                   8
logistic densities are log concave.11 As Heckman and Honoré (1990) show, not every log
concave distribution has a right-truncated expectation that is convex in the truncation
point. Nevertheless, the following remark clarifies that both the normal and the logistic
distribution are consistent with Assumption 2.

Remark 1 If ν ∼ N(0, σ 2 ), for any σ 2 < ∞; or ν ∼ Logistic, then                         E
                                                                                         ∂ 2 [ν|ν≥y]
                                                                                                       ≥ 0, for
                                                                                             ∂y 2
any y ∈ (−∞, ∞).

The proof of Remark 1 is contained in Section A.1. An implication of both Assumptions
1 and 2 is that E[ν|X ∗ , Z2 ] = 0.
    Assumption 3 does not impose a parametric restriction on the distribution of the
expectational error, . In contrast, as discussed in Section 4.3, estimating this binary
discrete choice model using maximum-likelihood techniques would require assuming
both the distribution of the measurement error conditional on the true covariates as
well as the marginal distribution of these unobserved true covariates.
    In addition, Assumption 3 does not require full independence between the mea-
surement error and the vector of decisions, true covariates, and instruments. This is
particularly important for the case in which the error ε captures expectational error
and agents have rational expectations. In that case, given that (X ∗ , d) ∈ J and, by
definition, E[ε|J ] = 0, Assumption 3 can be simplified to E[ε|Z2 ] = 0. This condition
will be satisfied as long as the econometrician defines the vector of instruments, Z2 ,
using variables included in J .
    The data are informative about the joint density of (d, Z, X), P(d, Z, X). Any
structure S a ≡ {β a , f a (X|X ∗ , Z2 ) f a (Z2 |X ∗ ) f a (X ∗ )} is admissible as long as it verifies
the restriction in Assumptions 1 to 3 and
                             Z
            P(d, Z, X) =         f (d|X ∗ , X; β a )f a (X|X ∗ , Z2 )f a (Z2 |X ∗ )f a (X ∗ )dX ∗ .

   In the model described in this section, the parameter vector β is set—not point—
identified, because there exists at least two admissible structures S a1 and S a2 such that
  11
     A random variable y has a log concave distribution if its density function fy satisfies that fy (λy1 +
(1 − λ)y2 ) ≥ [fy (y1 )]λ [fy (y2 )]1−λ , 0 ≤ λ ≤ 1, for any given values y1 and y2 in the support of
y. Some general references on log concave density functions are Pratt (1981), Heckman and Honoré
(1990), and Bagnoli and Bergstrom (2005). Heckman and Honoré (1990) clarify that the class of log
concave densities also includes uniform, exponential, extreme value and laplace (or double exponential)
densities. Under some parameter restrictions, it also includes the power function, Weibull, gamma,
chi-squared and beta.


                                                      9
β a1 6= β a2 . For a restricted version of this model, Appendix A.2 characterizes the set
of structures S a that are consistent with the joint density P(d, Z, X), and shows that
these structures imply different values of the parameter vector β.
    Although Assumptions 1 to 3 do not point-identify β, they have nontrivial identifi-
cation power. In Section 3, we derive moment inequalities based on these assumptions;
these inequalities define a set that contains the true value of β and is strictly contained
in RK .


2.3    Related Literature
Section 2.2 introduces a binary choice model in which the observable covariates are
endogenous due to the existence of classical errors-in-variables. Our paper is not the
first to study identification of index-function parameters in a binary choice model with
endogenous observable covariates. But we are, to our knowledge, the first to examine
the identifying properties of Assumptions 1 to 3 in an economic model that contains
two distinct errors with different economic interpretation.
    There are three alternative models that build on conditional independence assump-
tions to identify the parameters of binary choice models with endogenous regressors: (1)
the IV model of Chesher (2010) and Chesher (2011); (2) the triangular system model
that motivates the use of control function methods; and, (3) the special regressor ap-
proach.
    As Blundell and Powell (2003) show, even when the econometrician observes an
excluded variable that is independent of the error term in the random utility function,
semi-parametric and non-parametric binary response models are generally not point
identified. Chesher (2010) shows that this result holds even if we impose parametric
restrictions both on the random utility function and on the marginal distribution of the
error term. Chesher (2010) provides the inequalities that sharply define the identified
set under the assumption that the econometrician observes a excluded variable that
is independent of the error term (i.e. fully independent instrument). While Chesher
(2010) focuses on the case in which the endogenous variable is continuous, Chesher
(2011) performs an analogous exercise for the case in which it is discrete.12 Following
our notation, Chesher (2010) and Chesher (2011) assume that (ν + ε)|Z ∼ (ν + ε).
    Our model is stricter than the one proposed in Chesher (2010) in that we formally
  12
    Other papers that explore this IV approach are Chesher and Smolinski (2010), Chesher et al.
(2011), Chesher and Rosen (2012).


                                              10
define the error term as the sum of two different unobserved components: structural
error, ν, and expectational error, ; we only allow for endogeneity that is due to ex-
pectational error. However, our model can be viewed as more flexible than that in
Chesher (2010) because our identification strategy does not assume that the aggregate
error term, (ν + ), is fully independent of the instrument vector, Z2 . We only need to
impose mean independence between this instrument and . This weaker independence
assumption of our statistical model matches the assumptions common to economic
models of agents with rational expectations.13
    The triangular system control function model is attractive because, under certain
conditions, point identifies the parameters of interest. In particular, Blundell and Pow-
ell (2004) obtain point identification by applying a control-function approach. This
approach assumes that the endogenous variables are determined by an equation X
= h(Z, e) such that there is a one-to-one mapping from the latent variables, e, to
the endogenous variables, X, at each value of the instrument vector, Z.14 Using
the notation introduced in Section 2.2, the latent variable e is assumed to verify:
(ν + ε)|X, Z ∼ (ν + ε)|X, e ∼ (ν + ε)|e. In contrast, our model is a single-equation
model: there is no specification of any structural equation that would imply that the
error term (ν + ε) is independent of the endogenous regressor X, conditional on some
latent variable, e.15
    The special regressor approach assumes that the aggregate unobservable component,
(ν + ), is distributed independently of a continuously distributed explanatory variable
(i.e. special regressor) and impose a particular index restriction (Lewbel (2000)). This
model is point identified only if the special regressor has large support. In an application
in which the only source of endogeneity is expectational error, this approach implies
that one covariate, the special regressor with large support, is measured without error.16
Our statistical model allows all the regressors to contain expectational error.
  13
      The only type of independence that the rational expectations assumption imposes on the definition
of the expectational error is mean independence between this error and any variable contained in the
information set of the agent.
   14
      This restriction rules out cases in which there are discrete endogenous variables. In our case, we
allow for discrete endogenous variables as long as the measurement error can verify the restriction in
Assumption 3.
   15
      Other papers that explore the use of control function methods for the identification of binary
choice models in semi- and non-parametric settings are Blundell and Powell (2003), Chesher (2003),
Chesher (2005), Chesher (2007), Vytlacil and Yildiz (2007), Florens et al. (2008), Imbens and Newey
(2009), and Shaikh and Vytlacil (2011).
   16
      Other papers that explore the special regressor approach are Magnac and Maurin (2007) and
Magnac and Maurin (2008).


                                                  11
2.4     Simulation Exercise: Setting
We now introduce a simple setting that we will use in Sections 3 and 4 to illustrate the
properties of our moment inequality estimator. The simulated data is generated by a
structure in which
                          U = β1 X1∗ + β2 X2∗ + β3 X3∗ + ν,                         (12)

with β = (0.5, 0.5, 0.25). The error ν is distributed normally and independently of X ∗ :
ν|X ∗ v N(0, 2).17 We assume that X1∗ and X3∗ are measured without error: {Z1 , Z3 } =
{X1∗ , X3∗ }. On the other hand, the variable X2∗ is measured with error: X2 = X2∗ + x ,
with x distributed normally and independently of (X ∗ , ν): x |(X ∗ , ν) v N(0, 0.4). We
also repeat the estimation for values of σ2x between 0.2 and 1.8, in order to study how the
properties of the different estimators change as the variance of the expectational error
increases. Finally, we generate our instrumental variable, Z2 , as a second measurement
of X2∗ : Z2 = X2∗ + z , with z distributed normally and independently from (X ∗ , ν, x ):
z |(X ∗ , ν, x ) v N(0, 0.04).18


3      Moment Inequalities
In this section, we introduce two different types of moment inequalities that (partially)
identify the vector β of index coefficients under the assumptions of the static model
described in Section 2.2. Section 3.1 derives both types of inequalities conditional on
the vector of instruments, Z. Section 3.2 converts these conditional moments into
unconditional moments. In Section 3.3, we illustrate the identification power of these
unconditional moment inequalities in our simulation exercise.


3.1     Conditional Moment Inequalities
For any given value of the instrument vector Z, we derive two different types of inequal-
ities: score function moment inequalities and revealed preference moment inequalities.
  17
     We assume that ν is the difference in the structural error between alternative 1 and alternative 0.
With ν1 |X ∗ v N(0, 1), ν0 |X ∗ v N(0, 1), and both independent from each other, ν|X ∗ = (ν1 −ν0 )|X ∗ v
N(0, 2).
  18
     For our identification approach to define a set that contains the true value of the parameter vector
under Assumptions 1 to 3, we need not assume that Z2 is a second measurement of X2∗ generated
independently from X2 . The second measurement assumption here is used simply to generate the
simulated data.




                                                   12
3.1.1    Conditional Score Function Moment Inequalities

For any z ∈ Z, we define the conditional score function moment inequality as
                                         "                          #
                                             m− (d, Z  , X; β)
                        Ms (z; β) = E         s      1
                                                               Z = z ≥ 0,
                                             m+
                                              s (d, Z1 , X; β)


where the two moment functions are defined as
                                                              
                                         Fν − (β1 Z1 + β2 X)
                  m−
                   s (d, Z1 , X; β) =d                           − (1 − d),                     (13a)
                                       1 − Fν − (β1 Z1 + β2 X)
                                                                      
                   +                          1 − Fν − (β1 Z1 + β2 X)
                  ms (d, Z1 , X; β) = (1 − d)                       − d.                        (13b)
                                                Fν − (β1 Z1 + β2 X)

                                                                      −
 While m+ s is increasing in β1 Z1 + β2 X, the opposite is true for ms . We derive these
inequalities by differentiating the log probability of d = 1 -hence, the label score func-
tion inequality.19 We use Ms (Z M ; β) to group the conditional moment inequalities
corresponding to the set of values of z contained in the vector Z M : Ms (Z M ; β) =
{Ms (z; β); for all z ∈ Z M }. We denote the identified set defined by these inequalities
as Ω(Ms , Z M ) = {β ∈ Γβ : Ms (z; β) ≥ 0 for all z ∈ Z M }. The following theorem
contains the properties of the set Ω(Ms , Z M ). We use β ∗ to denote the true value of
β.

Theorem 1 Properties of Ω(Ms , Z M ).

   1. If Assumptions 1, 2 and 3 hold, then β ∗ ∈ Ω(Ms , Z M ), for any Z M ⊆ Z and any
      β ∗ ∈ Γβ .

   2. If Assumptions 1 and 2 hold and X2∗ = X = Z2 , then β ∗ = Ω(Ms , Z M ), for any
      Z M ⊆ Z such that the rank of the K · M matrix (z 1 , . . . , z M ) ∈ Z M is larger
      than K, and any β ∗ ∈ Γβ .

                                         ¯ε2 ≥ σ̄ε2 , then Ω(Ms , Z M |σ̄ε2 ) ⊆ Ω(Ms , Z M |σ̄
   3. If Assumptions 1, 2 and 3 hold and σ̄                                                 ¯ε2 ),
      for any Z M ⊆ Z.

The proofs of the three properties of Ω(Ms , Z M ) in Theorem 1 are contained in Sections
A.3, A.4, and A.5, respectively.20 The first property indicates that, if Assumptions 1, 2
  19
    We describe its derivation in detail in Section A.3.
  20
    As Sections A.3, A.4, and A.5 show, Theorem 1 does not require that the right-truncated expec-
tation of ν is convex. However, it still requires the distribution of ν to be log concave. The reason is

                                                  13
and 3 hold, then the score function inequalities contain the true value of the parameter
vector, for any β ∗ and any valid set of instruments. In general, the set Ω(Ms , Z M ) will
also contain values of β other than its true value. However, as the second property in
Theorem 1 states, in the particular case in which there is no expectational error (i.e. X2∗
= X) and we use the observed vector of choice characteristics as instruments (i.e. Z2 =
X), the identified set defined by the score function inequalities is a singleton, containing
only the true value of the parameter vector. Finally, in those cases in which there is
measurement error (i.e. X2∗ 6= X) and, therefore, we use a vector of instruments different
from the observed covariates (i.e. Z2 6= X), the third property in Theorem 1 states that
the set Ω(Ms , Z M ) increases monotonically in the variance of the expectational error.
Specifically, every value of the vector β contained in the identified set that corresponds
to a given value of the variance of the measurement error will also be contained in the
identified set generated by a larger variance.
    Computing the moments in equation (13) is no harder than evaluating the likelihood
function for a binary choice model that assumes there is no expectational error. We
describe the formation of the moments below.

3.1.2     Conditional Revealed Preference Inequalities

For any z ∈ Z, we define the conditional revealed preference moment inequality as
                                          "                          #
                                              m−
                                               r (d, Z1 , X; β)
                        Mr (z; β) = E                           Z = z ≥ 0,
                                              m+
                                               r (d, Z1 , X; β)


where the two moment functions are defined as

   r (d, Z1 , X; β) = −(1 − d)(β1 Z1 + β2 X) + dE ν|ν ≥ −(β1 Z1 + β2 X), Z1 , X , (14a)
  m−
                                                                                     

  m+                                             E
                                                                                     
   r (d, Z1 , X; β) = d(β1 Z1 + β 2 X) + (1 − d)     − ν| − ν ≥ β 1 Z1 + β2 X, Z1 , X   . (14b)

                                                                                    −
 While m+ r is increasing in the index β1 Z1 + β2 X, the opposite is true for mr . We
derive these inequalities from the individual revealed preference inequality of the version
of the model described in Section 2.2 that does not contain expectational error (see
equation (8)) –hence, the label revealed preference inequality.21 Analogously to the case
of the score function inequality, we define Mr (Z M ; β) = {Mr (z; β); for all z ∈ Z M }
that the log concavity of the distribution of ν is sufficient to guarantee that both Fν (·)/(1 − Fν (·)) and
(1 − Fν (·))/Fν (·) are convex.
  21
     We describe its derivation in detail in Section A.6.

                                                    14
and Ω(Mr , Z M ) = {β ∈ Γβ : Mr (z; β) ≥ 0 for all z ∈ Z M }. The following theorem
contains the properties of the set Ω(Mr , Z M ).

Theorem 2 Properties of Ω(Mr , Z M ).

   1. If Assumptions 1, 2 and 3 hold, then β ∗ ∈ Ω(Mr , Z M ), for any Z M ⊆ Z and
      any β ∗ ∈ Γβ .

   2. If Assumptions 1 and 2 hold and X2∗ = X = Z2 , then ∃ β ∈ Γβ such that β 6=
      β ∗ and β ∈ Ω(Mr , Z M ), for any Z M ⊆ Z and any β ∗ ∈ Γβ .

                                         ¯ε2 ≥ σ̄ε2 , then Ω(Mr , Z M |σ̄ε2 ) ⊆ Ω(Mr , Z M |σ̄
   3. If Assumptions 1, 2 and 3 hold and σ̄                                                 ¯ε2 ),
      for any Z M ⊆ Z.


The proof of the three properties of Ω(Mr , Z M ) in Theorem 2 are contained in Sections
A.6, A.7, and A.8, respectively. The first property indicates that, if Assumptions 1,
2 and 3 hold, then Ω(Mr , Z M ) contains the true value of the parameter vector, for
any β ∗ and instrument vector. As Theorem 2.2 indicates, the set Ω(Mr , Z M ) will
generally also contain values of β other than its true value. In fact, contrary to the
case of the score function inequalities, the set identified by the revealed preference
inequalities is never a singleton. Even in cases in which the econometrician assumes
that the observed covariate, X, is identical to the true covariate, X2∗ , and, therefore,
uses X as an instrument, it is still true that the set Ω(Mr , Z M ) will contain values
of the parameter vector that are different from β ∗ . A comparison of Theorem 1.2
and Theorem 2.2 leads to the conclusion that, for any Z M , if X2∗ = X = Z2 , then
Ω(Ms , z|σε2 = 0) ⊂ Ω(Mr , z|σε2 = 0). This does not imply that, for any Z M , the
inequalities Mr (Z M ; β) are irrelevant once we include the inequalities Ms (Z M ; β). On
the contrary, Theorems 1 and 2 provide little guidance on how Ω(Ms , Z M ) compares
to Ω(Mr , Z M ) when the variance of the expectational error is different from zero. The
third property in Theorem 2 is analogous to the third property in Theorem 1.
    Computing the moment inequalities in equation (14) requires evaluating the right-
truncated expectation of ν at different values in its support. For both probit and logit
models, the right-truncated expectation of the structural error may be computed using
standard statistical packages.22
  22
    If ν ∼ N(0, σ 2 ), then E[ν|ν ≥ y] = (σφ(y/σ))/(1 − Φ(y/σ)). If ν ∼ Logistic, then   E[ν|ν ≥ y] =
−y exp(y) + (1 + exp(y)) ln(1 + exp(y)).


                                                15
3.2      Unconditional Moment Inequalities
The moment inequalities described in equations (13) and (14) condition on particular
values of the instrument vector, Z. In empirical applications in which at least one
of the variables in the vector Z is continuous, the sample analogue of these moment
inequalities will likely involve an average over very few observations (if any). Therefore,
for estimation, it will be more useful to work with unconditional moment inequalities.
    Each of the unconditional moment inequalities is defined by an instrument function.
Here, we propose a particular set of instrument functions.23 We identify each of these
functions by a K × 1 vector q of 0s and 1s. We group all these vectors q into a matrix
Q that forms the standard basis in RK . For each q, we define an instrument function:

                    K
                        (1{Zk ≥ 0})qk (1{Zk < 0})1−qk ; qk ∈ {0, 1}, k = 1, . . . , K ,
                   Y
        Ψq (Z) =                                                                              (15)
                   k=1


where Zk is the kth element of Z and qk is the kth element of q.24
   Using the instrument functions Ψq , q ∈ Q, we define Q unconditional score function
moment inequalities and Q unconditional revealed preference inequalities. We denote
the unconditional score function inequality defined by the instrument Ψq as Mqs (β).
Analogously, we denote the corresponding revealed preference inequality as Mqr (β).

3.2.1     Unconditional Score Function Moment Inequalities

For each q ∈ Q, the resulting unconditional score function moment inequality is
                     h                                                         i
          Mqs (β) = E Ψq (Z) · m+
                                s (d, Z1 , X; β) + Ψ q (−Z) · m−
                                                               s (d, Z1 , X; β)  ≥ 0.         (16)

We group the Q unconditional score function moment inequalities into the vector Ms (β)
and define an identified set for this group of inequalities: Ω(Ms ) = {β ∈ Γβ : Ms (β) ≥
  23
     The unconditional moment inequalities proposed here generate a larger identified set than that
defined by the conditional moments described in Section 3.1. The main advantage of the moments
proposed here is its computational simplicity, while they still generate a bounded set. Papers that
define unconditional moments that imply no loss of information with respect to their conditional
counterpart are Amstrong (2012) and Andrews and Shi (2013). The instrument functions used in
these papers are computationally very intensive in our setting.
  24
     Here we are implicitly assuming that the vector Z has dimensions K × 1 (i.e. T = K − P ). As
an example, in the particular case in which K = 2, the matrix Q defines 4 instrument functions:
Ψ1 (Z) = 1{Z1 ≥ 0}1{Z2 ≥ 0}, Ψ2 (Z) = 1{Z1 ≥ 0}1{Z2 < 0}, Ψ3 (Z) = 1{Z1 < 0}1{Z2 ≥ 0},
Ψ4 (Z) = 1{Z1 < 0}1{Z2 < 0}.



                                                16
0}. The following theorem contains the properties of the set Ω(Ms ).

Theorem 3 Properties of Ω(Ms ).

  1. If Assumptions 1, 2 and 3 hold, then β ∗ ∈ Ω(Ms ), for any β ∗ ∈ Γβ .

  2. If Assumptions 1 and 2 hold, X2∗ = X = Z2 , and, for every q ∈ Q,
                                    "                   #
                                  E Ψq (Z) + Ψq (−Z) 6= 0,                               (17)


     then β ∗ = Ω(Ms ).

                                        ¯ε2 ≥ σ̄ε2 , then Ω(Ms |σ̄ε2 ) ⊆ Ω(Ms |σ̄
  3. If Assumptions 1, 2 and 3 hold and σ̄                                     ¯ε2 ).

  4. If Assumptions 1, 2 and 3 hold and, for every β ∈ Γβ and q ∈ Q,

                                   ∂Mqs (β)
                                            > 0,     if qk = 1,                         (18a)
                                    ∂βk
                                   ∂Mqs (β)
                                            < 0,     if qk = 0,                         (18b)
                                    ∂βk

      then Ω(Ms ) is bounded and closed.

  5. If Assumptions 1, 2 and 3 hold, then Ω(Ms ) is convex.

The first property indicates that the true value of the parameter vector is contained in
the identified set defined by the Q unconditional score function moment inequalities in
equation (16). This comes immediately from Theorem 1.1 and the fact that, for every
q ∈ Q, we can rewrite Mqs (β) as
             h                                                                     i
  Mqs (β) = E Ψq (Z) · E m+                                 E
                                                            −
                          s (d, Z1 , X; β) Z   + Ψ q (−Z) ·   m s (d, Z1 , X; β) Z    . (19)

For every Z ∈ Z, E m+   s (d, Z1 , X; β ) Z ≥ 0 and E ms (d, Z1 , X; β ) Z ≥ 0. There-
                                       ∗
                                                    −               ∗
                                                                           

fore, Mqs (β ∗ ) ≥ 0.
    The proof of Theorem 3.2 is contained in Section A.9. The condition in equation
(17) establishes that, for every q ∈ Q, there is some value in the support of Z such
that either Ψq (Z) = 1 or Ψq (−Z) = 1. Equation (17) guarantees that none of the Q
moments described in equation (15) will be equal to 0 for lack of observations that verify
the instrument function. If equation (17) holds, then Theorem 3.2 establishes that, in

                                            17
the particular case in which there is no expectational error and we use the observed
covariate X as an instrument, Z2 , then the only value of the parameter vector β that
is consistent with these Q unconditional inequalities is the true value of the parameter
vector, β ∗ .
     In cases in which X 6= X2∗ , the identified set Ω(Ms ) will generally contain values of
the parameter vector β other than its true value. Theorem 3.3 indicates that the set of
values that are included in Ω(Ms ) increases in the variance of the expectational error,
σε2 . This is a direct consequence of Theorem 1.3 and the unconditional inequalities
being weighted averages of the conditional moment inequalities across different values
of Z (equation (19)).
     Equation (18) in Theorem 3.4 establishes some conditions under which the Q un-
conditional score function moment inequalities imply that the identified set is bounded.
That is both, for every βk , k = 1, . . . , K, both +∞ and −∞ are excluded from Ω(Ms ).
Equation (18) requires that the partial derivative of each moment function, Mqs (β),
with respect to each element of the parameter vector, βk , is determined by the instru-
ment function in equation (15). In order for equation (18) to hold, the correlation
between the endogenous variable X, and its instrument, Z2 , must be sufficiently large.
Therefore, this property of the identified set Ω(Ms ) depends on the instruments being
sufficiently strong. The proof of Theorem 3.4 is contained in Section A.10.
     Finally, Theorem 3.5 establishes convexity of the set Ω(Ms ). Convexity is an im-
portant property for for inference in partially-identified models (see Beresteanu and
Molinari (2008), Kaido and Santos (2011), and references therein). The proof of Theo-
rem 3.5 is contained in Section A.11.

3.2.2   Unconditional Revealed Preference Moment Inequalities

For each q ∈ Q, the resulting unconditional revealed preference moment inequality is
                     h                                                             i
        Mqr (β)   = E Ψq (Z) ·   m+
                                  r (d, Z1 , X; β)   + Ψq (−Z) ·   m−
                                                                    r (d, Z1 , X; β)   ≥ 0.   (20)

We group the Q unconditional revealed preference moment inequalities into the vector
Mr (β) and define an identified set for this group of inequalities: Ω(Mr ) = {β ∈ Γβ :
Mr (β) ≥ 0}. The following theorem contains the properties of the set Ω(Mr ).

Theorem 4 Properties of Ω(Mr ).

  1. If Assumptions 1, 2 and 3 hold, then β ∗ ∈ Ω(Mr ), for any β ∗ ∈ Γβ .

                                               18
  2. If Assumptions 1 and 2 hold and X2∗ = X = Z2 , then ∃ β ∈ Γβ such that β 6=
     β ∗ and β ∈ Ω(Mr ), for any β ∗ ∈ Γβ and β ∗ 6= 0.

                                        ¯ε2 ≥ σ̄ε2 , then Ω(Mr |σ̄ε2 ) ⊆ Ω(Mr |σ̄
  3. If Assumptions 1, 2 and 3 hold and σ̄                                     ¯ε2 ).

  4. If Assumptions 1, 2 and 3 hold and, for every β ∈ Γβ and q ∈ Q,

                                   ∂Mqr (β)
                                            > 0,     if qk = 1,                         (21a)
                                    ∂βk
                                   ∂Mqr (β)
                                            < 0,     if qk = 0,                         (21b)
                                    ∂βk

        then Ω(Mr ) is bounded and closed.

  5. If Assumptions 1, 2 and 3 hold, then Ω(Mr ) is convex.

Properties 1, 3, 4, and 5 are identical to those described in Theorem 3 for the case of
the set Ω(Ms ). We refer to the comments in Section 3.2.1 and proofs in Sections A.10
and A.11.
    Among the list of characteristics listed in Theorems 3 and 4, the only difference
between the sets Ω(Ms ) and Ω(Mr ) is the identification properties of these two sets
in the case in which the econometrician assumes that there is no expectational error
in covariates, X2∗ = X, and, accordingly, uses such observed covariates as arguments
of the instrument function. While the set defined by the unconditional score function
inequalities collapses to a singleton containing only β ∗ (see Theorem 3.4), the set defined
by the unconditional revealed preference inequalities will include values of the parameter
vector other than its true value (see Theorem 4.4). This is an immediate consequence
of Theorem 2.2 and that each moment Mqr (β) is a weighted average of a particular set
of conditional moments, Mr (z; β).

3.2.3    Combining Score Function and Revealed Preference Inequalities

We define a vector M(β) that groups the Q unconditional score function moment
inequalities and the Q unconditional revealed preference inequalities. We denote these
2Q inequalities as M(β); i.e. M(β) = (Ms (β), Mr (β)). We define the identified
set characterized by these moment inequalities: Ω(M) = {β ∈ Γβ : M(β) ≥ 0}.
By definition, the set Ω(M) will be weakly smaller than both Ω(Ms ) and Ω(Mr );
i.e. Ω(M) ⊆ Ω(Ms ) and Ω(M) ⊆ Ω(Mr ). The following theorem describes some
properties of the set Ω(M).

                                             19
Theorem 5 Properties of Ω(M).

  1. If Assumptions 1, 2 and 3 hold, then β ∗ ∈ Ω(M), for any β ∗ ∈ Γβ .

  2. If Assumptions 1 and 2 hold, X2∗ = X = Z2 , and, for every q ∈ Q,
                                   "                  #
                                 E Ψq (Z) + Ψq (−Z) 6= 0,                            (22)


     then β ∗ = Ω(M).

                                        ¯ε2 ≥ σ̄ε2 , then Ω(M|σ̄ε2 ) ⊆ Ω(M|σ̄
  3. If Assumptions 1, 2 and 3 hold and σ̄                                 ¯ε2 ).

  4. If Assumptions 1, 2 and 3 hold and, for every β ∈ Γβ and q ∈ Q,

                              ∂Mqs (β) ∂Mqr (β)
                                               
                        max           ,           > 0,      if qk = 1,              (23a)
                               ∂βk      ∂βk
                              ∂Mqs (β) ∂Mqr (β)
                                               
                        min           ,           < 0,      if qk = 0,              (23b)
                               ∂βk      ∂βk

      then Ω(M) is bounded and closed.

  5. If Assumptions 1, 2 and 3 hold, then Ω(M) is convex.


Theorem 5 is a direct implication of Theorems 3 and 4 and the vector M(β) including
both the unconditional score and revealed preference inequalities.
    Given that both the unconditional score and revealed preference inequalities are
satisfied at the true value of the parameter vector, it holds that this true value is
contained in the set Ω(M) (Theorem 5.1). Given that Ω(M) ⊆ Ω(Ms ) and β ∗ =
Ω(Ms ) in those cases in which X2∗ = X = Z2 , it is immediate that Ω(M) will also
exclusively contain the true value of the parameter vector, β ∗ , in these same cases
(Theorem 5.2). As Theorems 1.3 and 2.3 indicate, both the conditional score function
and revealed preference moment inequalities become “weaker” as the variance of the
expectational error increases. More specifically, each moment in the vector M(β) is
a weighted average of a subset of conditional score function and revealed preference
inequalities. Therefore, as the variance of the expectational error gets bigger, the set
Ω(Ms ) will also include more values of the parameter vector β (Theorem 5.3). Theorem
5.4 is a direct consequence of Theorems 3.4 and 4.4; given that Ω(M) ⊆ Ω(Ms ) and
Ω(M) ⊆ Ω(Mr ), it is enough that either Ω(Ms ) or Ω(Mr ) are closed and bounded for

                                          20
Ω(M) to be closed and bounded. Finally, Theorem 5.5 is just an immediate implication
of all the unconditional score function and revealed preference inequalities being convex.


3.3     Simulation Exercise
Table 1 contains the identified sets Ω(Ms ), Ω(Mr ) and Ω(M) for our simulation exer-
cise. For all three sets, we assume that the econometrician knows the exact distribution
of the structural error. We compute the maximum and minimum value of each of the
three parameters that belongs to the three identified sets.

                                   Table 1: Identified Set

           Set      min(β1 )    max(β1 )     min(β2 )    max(β2 )    min(β3 )    max(β3 )
         Ω(Mr )       -0.47        1.17        -0.21       0.90        -0.61        0.95
         Ω(Ms )        0.44        0.56         0.48       0.56        0.20         0.31
         Ω(M)          0.44        0.56        0.48        0.56        0.20         0.31


    In Table 1, the true value of the parameter vector, (0.5, 0.5, 0.25), lies within the
smallest square circumscribing each of the three identified sets. The set Ω(Mr ) is much
larger than the set Ω(Ms ); Ω(M) and Ω(Ms ) are virtually identical. For example,
the set Ω(Mr ) indicates that the true value of β1 lies on the interval (−0.47, 1.17),
Ω(Ms ) and Ω(M) indicate that it lies on the interval (0.44, 0.56). In sum, the revealed
preference moment inequalities are far less informative about the true value of the
parameter vector than the score function inequalities. We illustrate the relative size of
these two identified sets in a 2-dimensional space in Figure 1.25
    We illustrate the predicted probabilities in Figure 2. In this figure, we report both
the true probability as well as the minimum and maximum predicted probability from
the model, using the “true” covariates for a particular agent and the set of parameter
values contained in the identified sets Ω(Ms ), Ω(Mr ) and Ω(M). In Figure 2, we fix
(X1∗ , X3∗ ) to arbitrary values and plot the minimum, maximum, and the true probabili-
ties in our simulation for a range of values of the X2∗ covariate. Combining the revealed
preference and score function moments yields a range of predicted probabilities that
closely follows the true probabilities.
  25
    These figures provide a slice of the 3-dimensional identified set, whose end points are reported
in Table 1. We project this 3-dimensional object into the 2-dimensional (β2 , β3 ) space, and draw the
corresponding region.



                                                 21
                                                                                                            Figure 1: Identified Set
                                                        Identified Set for Sample Dataset in Monte Carlo Exercise                                                                        Identified Set for Sample Dataset in Monte Carlo Exercise
                                                               =0.5, Using only Revealed Preference Moments                                                                                        =0.5, Using only Score Function Moments
                                                              1                                                                                                                                 1

                              0.8                                                                                                                                  0.8




                              0.6                                                                                                                                  0.6




                              0.4                                                                                                                                  0.4




                              0.2                                                                                                                                  0.2
                  3




                                                                                                                                                        3
                                  0                                                                                                                                    0




                             −0.2                                                                                                                                 −0.2




                             −0.4                                                                                                                                 −0.4


                                       0         0.1              0.2          0.3        0.4    0.5       0.6      0.7         0.8                                        0      0.1          0.2          0.3        0.4      0.5        0.6       0.7         0.8
                                                                                          2                                                                                                                            2




                                           (a) Revealed Preference                                                                                                                 (b) Score Function


                                                                                                 Figure 2: Predicted Probability
                                                         Predicted Probabilities for Difference Ranges of X2
                                      Moment Inequalities from Dickstein Morales (2013), Revealed Preference Moments Only
                                                                                                                                                                                          Predicted Probabilities for Difference Ranges of X2
                                                                                                                                                                                  Moment Inequalities from Dickstein Morales (2013), All Moments
                                                            Uses Observed Z as an Instrument for X
                                                                                     2                 2
                                                                                                                                                                                           Uses Observed Z2 as an Instrument for X2
                             1                                                                                                                                    1
                                                                                                                          Min Prob                                                                                                                         Min Prob
                                                                                                                          Max Prob                                                                                                                         Max Prob
                            0.9                                                                                           True Prob                              0.9                                                                                       True Prob

                            0.8                                                                                                                                  0.8


                            0.7                                                                                                                                  0.7
  Predicted Probabilities




                                                                                                                                       Predicted Probabilities




                            0.6                                                                                                                                  0.6


                            0.5                                                                                                                                  0.5


                            0.4                                                                                                                                  0.4


                            0.3                                                                                                                                  0.3


                            0.2                                                                                                                                  0.2


                            0.1                                                                                                                                  0.1


                             0                                                                                                                                    0
                                        −0.6           −0.4             −0.2             0      0.2        0.4        0.6                                                  −0.6         −0.4         −0.2             0        0.2          0.4        0.6
                                                                                     X2 range                                                                                                                     X2 range




                                           (a) Revealed Preference                                                                                                                 (b) Score Function


    Property 3 in Theorem 5 states that the size of the identified set Ω(M) increases
as the variance of the expectational error increases. We show this in Table 2. The
bounds in this table use both the score function and the revealed preference inequalities.
In order to compute each row of this table, we have generated samples of 100, 000
observations using a statistical model identical to that described in Section 2.4, but
with the variance of the expectational error, σ2x , set to equal the value indicated in the
first column. Table 2 shows that, for each element of the parameter vector β, the lower
bound diminishes and the upper bound increases as we set the variance to larger values
in our simulation exercise.



                                                                                                                                      22
                 Table 2: Identified Set for Different Variances

          σε2x   min(β1 )   max(β1 )   min(β2 )   max(β2 )   min(β3 )   max(β3 )
          0.2      0.47       0.53      0.49        0.53      0.22        0.29
          0.4      0.44       0.56      0.47        0.56      0.20        0.31
          0.6      0.41       0.60      0.46        0.61      0.17        0.34
          0.8      0.39       0.64      0.45        0.70      0.13        0.37
          1.0      0.36       0.67      0.44        0.96      0.10        0.40
          1.2      0.33       0.71      0.43        0.98      0.07        0.43
          1.4      0.30       0.75      0.42        1.00      0.04        0.46
          1.6      0.20       0.79      0.41        1.02      -0.07       0.49


4    Potential Misspecification of the Model
In spite of the empirical relevance of the statistical model described in Section 2.2, to
our knowledge, the previous literature does not provide a valid estimator for such a
model. In this section we consider three possible deviations from the model described
in Section 2.2 that appear in the literature. These alternative specifications substitute
one of the three assumptions of our binary choice model for a more restrictive alter-
native assumption. These additional restrictions lead to a different identification and
estimation approach from the moment inequality estimator introduced in Section 3. In
this section, we explore the properties of these alternative procedures as ways to iden-
tify the parameter vector β for the binary choice model described in Section 2.2. We
show that, if the model described in Section 2.2 is the true model, assuming a simplified
version of it may lead to large biases in the identification of the index coefficients and
in the prediction of the choice probabilities.
    Section 4.1 discusses the case in which the variable Z selected as an instrument by
the researcher does not satisfy Assumption 3; i.e. E[ε|Z] 6= 0. Section 4.2 considers the
properties of those estimators that rely on the assumption that there is no difference
in the structural error across choices that drives the individual’s decision; i.e. ν = 0.
Section 4.3 studies the properties of the typical estimation procedure used in those
cases in which Assumption 3 is substituted by the stronger assumption that there is no
expectational error; i.e. ε = 0. We study each of these cases in turn.




                                            23
4.1    Invalid Instrument
Even if the researcher believes that the statistical model described in Section 2.2 is
adequate to capture the main features of her empirical setting, three issues may com-
plicate the application of the inequality estimator described in Section 3. First, there
may be no observed vector of excluded variables, Z2 , in the dataset available. Second,
even if such a vector of excluded variables exist, the correlation between these vari-
ables and the mismeasured vector of covariates, X, might be so small such that the
resulting identified set Ω(M) is unbounded (i.e. the restrictions in equation (23) fail to
hold). Finally, there may be cases in which the econometrician wrongly assumes that
X2 has no expectational error (i.e. assumes that X2∗ = X). In these three cases, the
econometrician may decide to include the endogenous variable X as an argument of the
instrument function, {Ψq ; q = 1, . . . , Q}, in place of Z2 . Implicitly, the econometrician
substitutes Assumption 3 by the following alternative assumption:

Assumption 3(b) The distribution of ε conditional on (X, ν) has support equal to
(−∞, ∞) and expectation equal to 0: E[ε|X, ν] = 0.

Assumption 3(b) imposes that the expectational error is mean independent of the ob-
served covariate instead of being mean independent of true (unobserved) covariate.
Given this assumption, one can define an identified set that contains β ∗ without the
instrument vector Z. We can write a vector instrument functions {Ψq ; q = 1, . . . , Q}
identical to that in equation (15) that takes the vector X as their argument. The
resulting moment inequalities are:
              h                                                                     i
 Mqs (β) = E Ψq ((Z1 , X)) · m+s (d, Z1 , X; β) + Ψq ((−Z1 , −X)) · m−s (d, Z1 , X; β) ≥ 0,
              h                                                                     i
 Mqr (β) = E Ψq ((Z1 , X)) · m+r (d, Z1 , X; β) + Ψq ((−Z1 , −X)) · m−r (d, Z1 , X; β) ≥ 0.

for q = {1, . . . , Q}. We use M(β) to denote the 2 · Q moment inequalities whose
instrument function depends on the observed vector of covariates (Z1 , X) (contrary to
M(β), which identifies the inequalities described in Section 3 and whose instrument
function depends on Z = (Z1 , Z2 )). We denote Ω(M) as the set of values of β ∈ Γβ
that are consistent with the moments identified by M(β). Analogously, we can define
Ω(Ms ) and Ω(Mr ).
    If Assumptions 1, 2 and 3(b) hold, then Ω(M) contains the true value of the pa-
rameter vector and is convex, bounded, and closed. If Assumption 3 holds instead of

                                             24
Assumption 3(b), then the non-zero correlation between the expectational error affect-
ing X and the instrument vector, {Ψq ((X, Z)); q = 1, . . . , Q}, will affect the moments
in the vector M(β). We use our simulation to explore the properties of the identified
set, Ω(M), when Assumption 3 holds.

                                    Table 3: Identified Set

            Set     min(β1 )     max(β1 )    min(β2 )     max(β2 )    min(β3 )     max(β3 )
          Ω(Mr )      -0.46        1.16        -0.21        0.81        -0.59        0.94


   The results from the simulation appear in Table 3. In this example, Ω(Ms ) = Ω(M)
= ∅. At least two of the score function moments cross each other and, therefore, there is
no value of the parameter vector that satisfies all score function moments. This shows
that, given the statistical model described in Section 2.2, the true value of the parameter
vector is not consistent with the vector of moments M(β).26 In sum, if Assumption 3
holds instead of Assumption 3(b), we cannot guarantee that Ω(Ms ) or Ω(Mr ) contain
the true value of the parameter β.


4.2     No Structural Error
The previous empirical literature that applies moment inequalities to identify the pa-
rameters of a discrete choice model often explicitly or implicitly rules out an idiosyn-
cratic structural error. That is, the econometrician assumes that the unobserved com-
ponents of the agent’s utility function are identical across choices: ν = 0. Examples
of empirical applications using moment inequalities that impose this assumption in-
clude Ho (2009), Pakes (2010), Pakes et al. (2011), Holmes (2011), and Morales et al.
(2011).27
  26
     We also find that β ∗ ∈ Ω(Mr ). However, Ω(Mr ) is smaller than Ω(Mr ). The fact that Ω(Mr )
contains the true value of the parameter vector is due to the weak identification power of the revealed
preference moment inequalities. That is, the revealed preference inequalities admit a wide range of β
values in the identified set.
  27
     Most of these papers consider empirical applications in which the choice set includes more than two
options. Therefore, the results presented in Section 3 are not immediately applicable to the settings
considered in these papers; we are working on extending our results to discrete choice models with
more than two choices. The assumption imposing that ν = 0 has been weakened in applications
of moment inequalities to ordered choice models (see Katz (2007), Ishii (2008), Pakes et al. (2011))
and to the estimation of dynamic games (see Ciliberto and Tamer (2009)). Modelling a particular
discrete choice decision as an ordered choice model implies that the error term ν can modeled as j · ν.
In these models, the index j should have a quantitative interpretation; its applicability to general


                                                  25
    In order to derive score function and revealed preference moment inequalities under
the assumption of no structural error, we work with a statistical model that is equiv-
alent to that in Section 2.2, except that we replace Assumption 2 with the following
alternative assumption:

Assumption 2(b) The marginal distribution function of ν has a degenerate distribu-
tion at ν = 0.

If Assumptions 1, 2(b) and 3 hold, then the score function moment inequality (equation
(16)) has no identification power.28 As a consequence, for each q ∈ Q, the only inequality
with identification power is the following revealed preference inequality:
                h                                                      i
     Mqr (β) = E − Ψq (Z)(1 − d)(β1 Z1 + β2 X) + Ψq (−Z)d(β1 Z1 + β2 X) ≥ 0.                         (24)

and the identified set defined by these Q moment inequalities is Ω(Mr ). If Assumptions
1, 2(b) and 3 hold, then the identified set Ω(Mr ) contains the true value of the parameter
vector and is convex, bounded, and closed.29
    We use our simulation to explore the properties of the identified set Ω(Mr ) in
settings in which Assumption 2 holds and Assumption 2(b) does not. That is, we
examine the coverage properties of the set Ω(Mr ) in settings in which the structural
error varies across choices but the econometrician wrongly assumes that it is identical
across choices. Note that, for every q ∈ Q, we can write Mqr (β) = Mqr (β) + γ q (β) and,
for every value of β ∈ Γβ and every q ∈ Q, γ q (β) ≥ 0.30 Therefore, Ω(Mr ) ⊂ Ω(Mr )
and we cannot prove that, if Assumptions 1, 2 and 3 hold (and Assumption 2(b) does
not) β ∗ is contained in Ω(Mr ). In words, when we wrongly assume that there is no
structural error, the identified set defined by the resulting moment inequalities is biased
inwards. This can lead to a false sense of precision. If Assumption 2(b) does not hold,
discrete choice problems is thus limited. In the application in Ciliberto and Tamer (2009), the moment
inequalities estimator allows a structural error but contains no expectational error. We consider the
effect of incorrectly assuming that there is no expectational error in Section 4.3.
   28
      Proof in Section A.12.
   29
      More precisely, the set Ω(Mr ) is a K-dimensional cube. The “flat faces” in the boundary of the
identified set is a direct implication of the linearity of the moment inequalities in the parameter vector
β (see Kaido and Santos (2011)).
   30
      The function γ q (β) is a weighted average of truncated expectations and works as a structural error
correction term. Its exact expression is:
       h                                                                                             i
     E Ψq (Z)dE[ν|ν ≥ −(β1 Z1 + β2 X), Z1 , X] + Ψq (−Z)(1 − d)E[−ν| − ν ≥ β1 Z1 + β2 X, Z1 , X] .



                                                   26
the identified set Ω(Mr ) may be so small that it may fail to contain the true value of the
parameter vector. The results from the simulation exercise demonstrate this possibility:
Table 4 shows that Ω(Mr ) is much smaller than Ω(Mr ). While the resulting identified
set still contains the true value of β3 , this is not true for the parameter β2 . The true
value of β2 is 0.5, while the identified set found for this parameter is (−0.11, 0.24).31

                                    Table 4: Identified Set

            Set      min(β1 )    max(β1 )       min(β2 )   max(β2 )    min(β3 )    max(β3 )
          Ω(Mr )       0.50         0.50         -0.11       0.24        -0.21        0.28

    The absence of structural error makes the resulting model fully deterministic. There-
fore, for any given vector X ∗ , the predicted choice probability for any alternative is
either 1 or 0.


4.3     No Expectational Error
Finally, we consider the potential misspecification in which the econometrician ignores
expectational error. To our knowledge, no paper allows for classical errors-in-variables
in a binary choice model.32 The standard binary choice model allows for a single error
term and assumes that its distribution is known up to a finite vector of parameters.
Formally, the standard model estimated in the binary choice literature assumes d ·
(βX + η) ≥ 0, η|X ∼ fη (η|X; ρ), where the distribution of the error term, fη (η|X; ρ),
is unknown up to the parameter ρ. The probability that an individual with observable
characteristics X selects choice j is equal to:
                                            Z
                          P (d = 1|X) =          1{η ≥ −βX}dFη (η|X; ρ).                            (25)
                                             η


Only under very strong assumptions would this choice probability arise from a model
that allows both for structural and measurement error, as in the model introduced
in Section 2.2. In order to facilitate the comparison, assume the statistical model
  31
      When we assume that the structural error equals 0, we first must define an alternative normaliza-
tion by scale for the parameter β. For comparison purposes, we set the correct scale by fixing β1 to
be equal to its true value, 0.5.
   32
      The moment inequality papers based on Pakes (2010) and Pakes et al. (2011) allow for measurement
error but not for an individual and choice specific structural error (see Section (4.2)). In Berry et al.
(2004), the authors allow only for measurement error in covariates that are constant across subsets of
individuals.

                                                   27
described in equations (7) to (11) and impose the following distributional assumptions:
ν ∼ Fν (ν|X ∗ ; ρ1 ), and ε ∼ Fε (ε|X ∗ ; ρ2 ). The resulting conditional choice probabilities
are
                            Z hZ                                  i
          P (d = 1|X) =             1{ν ≥ −βX ∗ }dFν (ν|X ∗ ; ρ1 ) dFx∗ (X ∗ |X; ρ2 )    (26)
                                x∗    ν


where computing Fx∗ (X ∗ |X; ρ2 ) requires knowledge of Fε (ε|X ∗ ; ρ2 ) and the marginal
distribution of the vector of unobserved covariates, Fx∗ (X ∗ ). Therefore, in the model
in Section 2.2, in order to account for the measurement error in a maximum likelihood
setting, one must make parametric assumptions on two additional distributions: (a)
the distribution of the measurement error conditional on the true covariates, and, (b)
the marginal distribution of the true covariates.33
    The results of the simulation exercise show the asymptotic bias that would arise
from ignoring expectational error. Table 5 presents the ML estimates for data gener-
ated following the statistical model described in Section 2.4, with the variance of the
measurement error, σ2x , set equal to the value indicated in the first column. In order
to make the results comparable, we rescale the ML estimates so that the estimates
presented in every row are subject to the same normalization. The results show that as
the variance of the measurement error in X2 increases, the ML estimate of β2 is biased
downward and the ML estimate of β3 has an upward bias.
    Consider the particular case in which the variance of the expectational error is set
to 0.2 (10% of the variance of the structural error). As shown in Table 5, at 0.4, the
95% confidence interval for β2 and β3 do not contain the true value of the parameter
vector. Thus, even a relatively small variance of the measurement error might generate
significant bias in the ML estimator.
  33
    By redefining η as ε + ν, one might be tempted to consider equation (25) as the reduced form
analogue of equation (26). To do so, however, requires the researcher to impose strong independence
assumptions on Fν (ν|X ∗ ; ρ1 ) and Fε (ε|X; ρ2 ). As an example, if one assumes that Fν (ν|X ∗ ; ρ1 ) =
Fν (ν; ρ1 ) and Fε (ε|X; ρ2 ) = Fε (ε; ρ2 ), then equation (26) becomes
                                           Z
                        P (d = 1|Xi ) =         1{ν + ε ≥ −βX ∗ }dFν (ν + ε; (ρ1 , ρ2 )),           (27)
                                          ν+ε

which is equivalent to equation (25) with Fη (η|Xi ; ρ) = Fν (ν + ε; (ρ1 , ρ2 )). This approach requires the
researcher to depart from the classical errors-in-variables assumption and impose that the measurement
error is independent of the observed covariates. Only under these assumptions could we interpret the
single unobserved component in standard discrete choice models as the sum of the structural and
measurement error.




                                                    28
          Table 5: Maximum Likelihood with Measurement Error

                    σε2x   β1 , 95% CI     β2 , 95% CI     β3 , 95% CI
                    0.2    [0.485,0.502]   [0.466,0.479]   [0.260,0.275]
                    0.4    [0.483,0.500]   [0.444,0.455]   [0.271,0.287]
                    0.6    [0.481,0.498]   [0.423,0.434]   [0.282,0.297]
                    0.8    [0.479,0.495]   [0.404,0.415]   [0.291,0.307]
                    1.0    [0.477,0.494]   [0.387,0.398]   [0.300,0.315]


5      Nonparametric Assumptions on Structural Errors
All the moment inequalities derived in Section 3 rely on the assumption that that
the econometrician knows the marginal distribution of the structural error up to a
scale parameter. In this section, we derive moment inequalities that do not require
the econometrician to specify a distribution for ν. These moment inequalities hold at
the true value of the parameter vector as long as Assumptions 1 and 3 hold and the
structural error has zero mean. The lack of parametric assumptions on the distribution
of ν impedes the derivation of the score function inequalities. However, we are still able
to derive the revealed preference inequalities.


5.1    Revealed Preference Inequalities
If the econometrician does not assume a particular distribution function for ν, then she
cannot compute the terms E[ν|ν ≥ −(β1 Z1 + β2 X), Z1 , X] and E[−ν| − ν ≥ β1 Z1 +
β2 X, Z1 , X] in equation (14). Nevertheless, we can still find an inequality that will be
satisfied at the true (normalized) value of the parameter vector. For every instrument
function Ψq , q = 1, . . . , Q (see equation (15)), we define the following inequality:
                    h                                                          i
         Mqr (β) = E Ψq (Z) · m+
                               r (d, Z1 , X; β) + Ψ q (−Z) · m −
                                                               r (d, Z1 , X; β)  ≥ 0,    (28)

with

             m−                                               E   1
                                                                          
              r (d, Z1 , X; β) = (1 − d)(−(β  1 Z1 + β2 X)) +   ν   {ν ≥ 0}  ,          (29a)
              r (d, Z1 , X; β) = d(β1 Z1 + β2 X) + E − ν 1{−ν ≥ 0} .
             m+
                                                                      
                                                                                        (29b)

 We denote as Ω(Mr ) the identified set defined by these Q inequalities. In order to state
the properties of this identified set, we first need to introduce the assumption that we

                                             29
substitute for Assumption 2:

Assumption 2(c) The structural error ν has mean equal to 0; i.e. E(ν) = 0.

Assumption 2(c) is more general than Assumption 2. Assumption 2(c) is compatible
with any distribution for ν as long as it has zero mean, independently of whether it is
log-concave or whether the truncated expectation is convex or concave in the truncation
point.

Theorem 6 If Assumptions 1, 2(c) and 3 hold, then, for any β ∗ ∈ Γβ , β ∗ ∈ Ω(Mr ).

The proof of Theorem 6 is contained in Appendix A.13. This theorem shows that
one can derive a non-trivial identified set (i.e. a strict subset of the parameter space)
based on linear moment inequalities that do not rely on parametric assumptions on the
distribution of either the structural or the expectational error.
    Since we do not know the marginal distribution of ν, one might question whether
the inequality Mr (β) is applicable. After all, the value of E[ν 1{ν ≥ 0}] is a constant
that depends on this marginal distribution. However, if Assumption 2(c) holds, then
E[ν 1{ν ≥ 0}] > 0.34 For any distribution Fν , the parameter vector β is identified only
up to scale and, therefore, we can divide by the term E[ν 1{ν ≥ 0}] on both sides of
the inequality in equation (28). This is equivalent to setting the normalizing constant
equal to this truncated expectation. The resulting normalized inequality is
                      h                                                              i
          M̃qr (β) = E Ψq (Z) · m̃+
                                  r (d, Z1 , X; β̃) + Ψ q (−Z) · m̃ −
                                                                    r (d, Z1 , X; β̃)  ≥ 0,         (30)

with

                       m̃−
                         r (d, Z1 , X; β) = (1 − d)(−(β̃1 Z1 + β̃2 X)) + 1,                       (31a)
                       m̃+
                         r (d, Z1 , X; β) = d(β̃1 Z1 + β̃2 X) + 1.                                (31b)

  and the resulting identified set is Ω(M̃r ). For any β ∈ Γβ such that β ∈ Ω(Mr ),
β̃ = β/E[ν 1{ν ≥ 0}] ∈ Ω(M̃). Therefore, if the assumptions in Theorem 6 hold, the
true parameter vector scaled by E[ν 1{ν ≥ 0}] will be in Ω(M̃).
     In Table 6, we show that the identified set defined by the Q inequalities M̃qr (β), q =
1, . . . , Q, contains the true value of the parameter vector, β ∗ .35 A comparison of Ω(M̃r )
  34
    Assumption 2(c) also implies that E[ν 1{ν ≥ 0}] = E[−ν 1{−ν ≥ 0}].
  35
    In order to facilitate the comparison of the identified sets across the different identification pro-
cedures, Table 6 presents the results from the numerical simulation that correspond to a value of

                                                   30
                                   Table 6: Identified Set

            Set     min(β1 )    max(β1 )     min(β2 )    max(β2 )    min(β3 )     max(β3 )
          Ω(M̃r )     -1.65        5.37        -0.83       3.87        -2.28        4.14


and Ω(Mr ) shows that dropping the distributional assumption on the structural error
results in an identified set that includes a much larger area of the parameter space. The
identification power of the set Ω(M̃r ) seems likely to be limited.


6     Application: Entry into Export Markets
In this empirical application, we apply the methods developed in Section 3 to revisit a
question that is at the core of the international trade literature on the gravity equation:
we study how a firm’s entry and exit decisions vary across destination countries de-
pending on the distance between the destination and the country of origin. We assume
that firms maximize the sum of profits across all foreign markets simply by maximizing
profits market by market. This allows us to treat each firms’ decision to export to a
given destination in each time period as a binary decision problem.


6.1     Structural Model of Entry into Export Markets
Let i index firms, c countries and t time periods. Each firm must choose whether to
export to each country in each period. Let the dummy dict denote firm i’s export choice
with respect to country c and period t. We model the static profits of exporting as:

                                                               π
                                πict = β1 Rict − β2 − β3 Dc + νict ,                              (32)

where Rict denotes the revenue for firm i of exporting to country c at period t, Dc
denotes the physical distance between the consumers located in country c and the
firm’s country of origin, and β2 + β3 Dc is the fixed cost. All of the other determinants
                                                                  π
of the static profits are captured by the unobserved term, νict     . When firms do not
export, we normalize profits to zero. Because our data is limited to Chilean firms, we
assume that firms share the same country of origin. Equation (32) implicitly assumes
E[ν 1{ν ≥ 0}] such that the scale of the parameter vector β is the same across the different estimation
methods.



                                                  31
that β1 Rict yields the revenue net of variable costs. In other words, variable costs are
assumed to be a constant fraction of revenue across firms, countries and time periods.36
A firm pays the fixed costs in every country-period pair in which it sells a positive
quantity.
     Our estimation approach does not require us to specify precisely the content of the
information set of firm i at the time it decides whether to export to a country c at
                                                                        π
period t, Jit . The only restriction we impose is that (Rict−1 , Dc , νict ) ∈ Jit . In words,
at the time the firm decides whether to export or not, it knows the distance to the
                                                                      π
destination market, some determinants captured in the variable νict      , and the revenue it
                                                            37
would have obtained last period if it had exported, Rict−1 .
     Firms may have imperfect information about the actual revenue they would obtain
                                ∗
if they were to export. Let Rict   be the expected value of Rict conditional on the infor-
mation set of the firm at the time it decides whether to export or not, Rict ∗
                                                                               = E[Rict |Jit ].
                                                                                ∗
Using the notation introduced in Section 2, we define the state vector (Xict       , νict ), with
   ∗       ∗
Xict = (Rict , Dc ), and write the expected payoffs from exporting as

                           ∗
                Uict = U (Xict , νict ) = E[πict |Jit ] = β1 Rict
                                                              ∗                   π
                                                                  − β2 − β3 Dc + νict .              (33)

                                                                   ∗
    We denote the optimal action that each firm takes as dict = d(Xict , νict ). We assume
that firms export to some country if and only if the expected value of exporting is
higher than the expected value of not exporting. Accordingly, the policy function is
defined by the following inequality:

                                     (dict − (1 − dict ))Uict ≥ 0.                                   (34)

Measurement model. The econometrician does not observe the agents’ expectations,
 ∗
Rict , and observes the realized revenue only for those observations with dict = 1.38
  36
      As Morales et al. (2011) shows, one can provide micro foundations for this relationship between
revenues and variable costs in a model that assumes firms that are monopolistically competitive and
face CES demands in each potential destination market. In this setting, β1 is equal to the reciprocal
of the elasticity of substitution.
   37
      We will assume below that we can write Rict as a function of firm and foreign market characteristics.
Therefore, assuming that Rict−1 is known to firm i at period t is equivalent to assuming that those
firm and country characteristics are revealed ex post.
   38
      This example departs from the measurement model described in Section 2 in that the econometri-
cian only observes the realized value of the variable whose expectation enters the objective function for
a selected sample. We chose this modification of the measurement model for our application because
applied researchers often face data structures of this form. The key properties of our estimator remain
unchanged under this modification.


                                                    32
Therefore, the econometrician needs to predict the value of Rict for those observations
such that dict = 0. The econometrician bases this prediction on a reduced form approx-
imation to the realized revenue. We assume the following expression for the realized
revenue from exporting:

                                             R           R
                                   Rict = r(Xict ; θ) + νict + εR
                                                                ict ,                                (35)

                                                            R
where Xict is a vector of observed covariates, both νict      and εR
                                                                   ict are unobserved to the
econometrician, and E[νict |Jit ] = νict , and E[εict |Jit ] = 0. We impose no assumption
                            R          R           R

                                                         R
on the statistical relationship between the vector Xict      and the agent’s information set,
Jit . Defining θ̂ as an estimate of θ, we will use

                                                      R
                                           R̂ict = r(Xict ; θ̂)                                      (36)

                ∗
as a proxy for Rict in our moment inequalities.
    Using the same notation as in Section 2, Xict = R̂ict , and Z1ict = Dc . Finally,
the econometrician also observes an instrument: a variable that is correlated with the
measurement R̂ict and is independent of the structural error and mean independent of
                                        R
the expectational error. Given that Xict   is observed for every (i, c, t), we will use R̂ict−1
as an instrument for R̂ict . Therefore, using the notation in Section 2, Z2ict = R̂ict−1 .


6.2     Estimation
The parameter vector to estimate is (θ, β). The vector θ projects the actual revenue
                                                                                  R
from exporting, Rict , onto the vector of firm and country characteristics Xict     . The
vector β transforms revenue from exporting into variable profits (β1 ) and parameterizes
the fixed costs from exporting (β2 and β3 ).
    We follow a two-step estimation procedure. In the first stage, we apply panel data
estimation techniques to obtain point estimates of θ that are independent of the value
estimated for β. In the second stage, we use the moment inequality framework described
in Section 3 in order to obtain set estimates for β that are conditional on the first stage
estimates of θ.39
  39
     We could have followed an alternative estimation procedure and combine equations (33), (34) and
(35) with Assumptions 1 to 3 to derive moment inequalities that set identify the parameter vector
(θ, β). However, this estimation procedure has two potential shortcomings: (a) it does not use the
available data on revenue (i.e. for the subset of observations with positive exports); (b) it is likely to
generate very large identified sets. The reason for expecting large identified sets from this procedure


                                                   33
    Using data on observed export revenues for firms, countries and years with positive
exports, we obtain an estimate for θ. Section 2 of the Online Appendix describes the
                                                                                      R
estimation of θ in detail. The outcome of this first stage is the variable R̂ict = r(Xict ; θ̂)
                             ∗
and we use it to proxy for Rict :

                              Uict = β1 R̂ict − β2 − β3 Dc + νict + εict ,                           (37)

                   + β1 E Rict − R̂ict |Jict ], and εict = β1 (Rict
                                                                ∗
                 π
                                                                                
where νict = νict                                                   − Rict ) + β1 (Rict − R̂ict ) −
E Rict − R̂ict |Jict ] . The structural error, νict , incorporates both the determinants of
                     
                                                                  π
the static profits the econometrician does not observe, νict        , as well as the part of the
approximation error from the first stage, Rict - R̂ict , that is in the information set of the
                                                                                               ∗
exporter. The measurement error, εict , incorporates both the expectational error, Rict            -
Rict , and approximation error from the first stage that is orthogonal to the information
set of the exporter.
      We impose Assumptions 1 and 3 to the distribution of (νict , εict ) conditional on
     ∗
(Xict , Zict ) and assume that νict is iid across firms, countries and time periods and
normally distributed with mean 0 and variance equal to 1.40 Assumption 1 imposes
exogeneity restrictions that are common in the international trade literature: unobserv-
able (to the econometrician) determininants of entry in a given country and time period
are independently distributed from observable determinants of this decision. Assump-
tion 2 is automatically satisfied once we impose normality on the distribution of the
structural error term. Concerning the error term εict , Assumption 3 is an immediate
consequence of this error term being mean independent of the agent’s information set,
Jit , as long as we assume that lagged determinants of export revenues are contained in
                                       R
firms’ information sets (i.e. ∀ c, Xict−1     ∈ Jit ).

6.2.1    Moment Inequalities

In this section, we apply the general identification framework in Section 3 to the model
described in Section 6.1 in order to derive moment inequalities that contain the true
value of the parameter vector β.
                                                               R
is that the variable Dc is likely to be very correlated with Xict .
   40
      In order to interpret the assumption that the variance of νict is equal to 1, it is important to note
that the variable R̂ict is expressed in hundreds of thousands of real 1995 dollars. Therefore, assuming
that the structural error has a variance of 1 is equivalent to assuming that there are unobservable
factors affecting firms’ export decisions and that the standard deviation of these unobservable factors
is equal to USD 100,000.


                                                    34
Instrument functions. If we apply here the instrument functions defined in equation
(15), then Ψq = 0 for many q ∈ Q. The joint support of the vector Z is such that, for
some q ∈ Q, there exists no observation (i, c, t) such that Ψq (Zict ) = 1. For example,
if the variable that multiplies the parameter β1 (i.e. R̂ict ) enters with positive sign for
some observation, the variables that multiply β2 (i.e. −1) and β3 (i.e. −Dc ) will always
enter with negative sign. Therefore, at least one of the instrument functions will be
zero for all observations. Since we cannot directly apply the instrument functions in
equation (15), we define alternative instrument functions. We describe them in detail
in Section 3 of the Online Appendix.

Moment Inequalities. We base our estimation on an application of the moments Ms
and Mr defined in equations (16) and (20). We present results for the three type of
instrument vectors described in Section 3 of the Online Appendix.41


6.3     Results
Table 7 presents the results of a probit in which we project the variable dict on predicted
revenues, R̂ict , a constant, and distance to country c, Dc . The estimates from this probit
model would be consistent estimates of the parameter vector β if: (a) our approximation
R̂ict perfectly captures firms’ expectations at the time of deciding whether to export or
not; and, (b) the variable capturing factors known to the agent and unobserved to the
econometrician follow a standard normal distribution. Different assumptions on the
variance of the structural error affect the value of the estimates but not their sign or
their relative value.

                              Table 7: ML Estimation: Probit

                               Parameter      Estimate      Std. Error
                                    β1          0.045          0.002
                                    β2          0.883          0.015
                                    β3          1.166          0.018


   The value of β1 indicates that exporters keep 4.5% of export revenue as variable
profits. The value of β2 implies a constant component of fixed export costs equal to
USD 88, 300. The value of β3 predicts that, if we compare two countries whose distance
  41
     In order to interpret the estimates presented in Section 6.3, note that the variable Dc is expressed
in terms of thousands of kilometers.

                                                   35
to Chile differs in 10, 000 km, then fixed export costs for the further away country will
be USD 116, 600 larger.

                                   Table 8: Identified Set

          Set          min(β1 )      max(β1 )   min(β2 )   max(β2 )    min(β3 )    max(β3 )
    No Meas. Error         0.045       0.045       0.883       0.883       1.166       1.166
        Set 1              0.031       0.176       0.615       1.465       0.749       1.697
        Set 2              0.078       0.142       0.626       0.753       1.435       1.685



                                   Table 9: Confidence Set

          Set    min(β1 )     max(β1 )    min(β2 )    max(β2 )    min(β3 )    max(β3 )
         Set 2     0.078       0.142       0.626       0.804       1.435       1.685

    Our moment inequality estimator allows us to relax the assumption that our approx-
imation R̂ict is a perfect proxy for firms’ expectations. For different sets of instruments,
Table 8 shows the maximum and minimum numbers for each parameter contained in
the estimated set. We focus here on the results generated by the set of instruments
labeled Set 2 (see Section 3 of the Online Appendix for a detailed description of the
three types of instruments considered in Table 8). Table 9 shows the confidence inter-
val for the second set of instruments. We show the results with moment selection (see
Andrews and Soares (2010)).
    Given the scale implied by the assumption that the variance of the structural error
term is equal to 1, our estimate of the identified set in Table 8 predicts a profit margin
over export revenue between 7.8% and 14.2%, a constant component of fixed costs
between USD 62, 600 and USD 75, 300, and an increase in these entry costs between
USD 143, 500 and USD 168, 500 per additional 10, 000 km. While Table 8 indicates
the limits of the smallest possible cube containing the actually estimated set, Figure 3
characterizes the actual set. From Figure 3a, it is immediate that the estimated set is
much smaller than the smallest cube containing it. Therefore, our moment inequality
estimates are much more informative than the numbers in Table 8 imply. Figure 3b
makes this even more transparent, by projecting the estimated set on the space defined
by the coordinates (β2 , β3 ).
    A comparison of Tables 7 and 8 shows that our estimation of the identified set does
not include the Maximum Likelihood estimate. In particular, the MLE underpredicts

                                                36
                                    Confidence Set for Static Export Decision Model
                                                                                        Figure 3: Confidence Set
                              Uses both revealed preference and score function inequalities
                       Uses instrumental variables: rev,distance,sqrt(rev),sqrt(distance),interactions
                                                                                                                                                  Slice of Confidence Set for Static Export Decision Model
                                                                                                                                               Uses both revealed preference and score function inequalities
                                                                                                                                        Uses instrumental variables: rev,distance,sqrt(rev),sqrt(distance),interactions
                                          Moment selection in inference routine                                                                  Uses all grid points, moment selection in inference routine
                                                                                                                             1.7




        1.7                                                                                                                 1.65


       1.65

                                                                                                                             1.6
        1.6
   3




       1.55




                                                                                                                        3
                                                                                                                            1.55
        1.5


       1.45
                                                                                                                             1.5

          0.8

                0.75                                                                                            0.14
                                                                                                         0.13               1.45
                              0.7                                                              0.12
                                                                                        0.11
                                                                              0.1
                                         0.65                         0.09
                                                              0.08                                                             0.62   0.64     0.66       0.68       0.7       0.72       0.74       0.76      0.78       0.8
                               2                                                                                                                                               2
                                                                                    1




                                   (a) 3 Dimensions                                                                                               (b) 2 Dimensions


the share of export revenue that accrues to the exporter as well as the slope at which
entry costs increase with distance. It overpredicts the constant term of entry costs. The
downward bias in the estimate of β1 implies that the MLE will tend to underpredict
entry of those firms and countries that are expected to generate large export revenues:
e.g. it underpredicts entry by large firms and in large countries. The upward bias in
the estimates of the constant term on fixed costs, β2 , and the downward bias in the
estimates of their slope with respect to distance, β3 , implies that the MLE will tend to
underpredict entry in countries that are close to the country of origin of the firm and
overpredict entry in countries that are more distant.

                                    Table 10: Predictions Across Distance Quantiles

                                                [0,1)                [1,2)                 [2,3)                [3,4)          [4,5)            [5,6)               [6,7) [7,8)                             [8,9)               [9,10)
 Moment Inequality Predictions
 Min. number 1,857                                                    759   704                                    244   147   140                                         41                 35                   15              15
 Max. number 2,211                                                    925   790                                    311   197   181                                         75                 65                   30              26
 Min. sales  5,317                                                    105 4,782                                    592 1,721 3,311                                          5                 14                   16             749
 Max. sales  6,267                                                    138 5,880                                    641 1,923 3,660                                          9                 37                   64             994
 Probit Predictions
 Number                                     1,634                     891                  731                     373   247   200                                     127                 115                     60              31
 Sales                                      4,215                      86                4,169                     491 1,615 2,826                                       8                  17                     16             553


   Table 10 shows that the biases in the MLE may have important implications for

                                                                                                                       37
the total number of exporters and volume of exports.42 As indicated in the previous
paragraph, the MLE implies a lower number of entrants than the moment inequalities
estimator for the 10% of countries that are closer to Chile, and a larger number of
entrants than the moment inequalities estimator for the furthest 70% of countries (with
the exception of the decile 0.9 to 1). However, because the MLE tends to underpredict
entry by those firms that would export the largest quantities conditional on entry, the
MLE significantly underpredicts the total volume of exports for each of the country
groups in Table 10.


7      Conclusion
This paper shows how to identify and estimate the parameters of a binary choice model
with covariates that are endogenous due to expectational error. We introduce a sta-
tistical model in which the error component determining agents’ choices is the sum of
a structural error and a expectational error term. We motivate this statistical model
as a natural extension of the standard binary choice model for cases in which agents
with rational expectations face uncertainty about the exact payoffs associated with
their actions. Accordingly, the only restriction we impose on the distribution of the
expectational error is that it is mean independent of the information set of the agent
at the time she makes a decision.
    We apply our moment inequality estimation approach to the analysis of a single-
agent static entry model. Using firm-level export data, we estimate parameters that
determine costs incurred by exporting firms upon entering new foreign markets. We
allow firms’ decisions to be driven both by expectational errors and components of their
information set that are unobserved to the econometrician. Our results suggest that
the typical assumption of perfect foresight can severely bias the parameter estimates
and provide a distinct economic interpretation of the observed decisions.




  42
    In order to compute Table 10, we fit the empirical distribution of Dc and assume a distribution
     ∗
of Rict that coincides with the empirical distribution of R̂ict . This is not meant as a substantive
assumption. It is only done for the sake of illustrating the implications of the bias in the MLE
estimates.


                                                38
References
Amstrong, Tim B., “Asymptotically Exact Inference in Conditional Moment In-
 equality Models,” mimeo, December 2012. [16]

An, Mark Yuying, “Log Concave Probability Distributions: Theory and Testing,”
 mimeo, November 1995. [47]

Andrews, Donald W.K. and Gustavo Soares, “Inference for Parameters Defined
 by Moment Inequalities Using Generalized Moment Selection,” Econometrica, 2010,
 78, 119–157. [2, 36]

   and Xiaoxia Shi, “Inference Based on Conditional Moment Inequalities,” mimeo,
  July 2011. [2]

   and    , “Nonparametric Inference Based on Conditional Moment Inequalities,”
  mimeo, December 2011. [2]

   and , “Inference Based on Conditional Moment Inequalities,” Econometrica, 2013,
  81 (2), 609–666. [16]

Bagnoli, Mark and Theodore C. Bergstrom, “Log-Concave Probability and Its
 Applications,” Economic Theory, 2005, 26 (2), 445–469. [9, 46]

Beresteanu, Arie and Francesca Molinari, “Asymptotic Properties for a Class of
 Partially Identified Models,” Econometrica, 2008, 76 (4), 763–814. [18]

Berry, Steve, James Levinsohn, and Ariel Pakes, “Estimating Differentiated
 Product Demand Systems from a Combination of Micro and Macro Data: The Market
 for New Vehicles,” Journal of Political Economy, 2004, 112 (1), 68–105. [27]

Blundell, Richard W. and James L. Powell, “Endogeneity in Nonparametric and
  Semiparametric Regression Models,” in Mathias F. Dewatripont, Lars P. Hansen,
  and Stephen J. Turnovsky, eds., Advances in Economics and Econometrics: Theory
  and Applications, Eight World Congress, Vol. II, Cambridge: Cambridge University
  Press, 2003. [10, 11]

   and , “Endogeneity in Semiparametric Binary Response Models,” Review of Eco-
  nomic Studies, 2004, 71, 655–679. [1, 11]

Chernozhukov, Victor, Han Hong, and Elie Tamer, “Estimation and Confidence
 Regions for Parameter Sets in Econometric Models,” Econometrica, 2007, 75, 1243–
 1284. [2]

Chesher, Andrew, “Identification in Nonseparable Models,” Econometrica, 2003, 71
 (2), 1401–1444. [11]


                                       39
  , “Nonparametric Identification under Discrete Variation,” Econometrica, 2005, 73
  (2), 1525–1550. [11]

  , “Instrumental Values,” Journal of Econometrics, 2007, 139 (1), 15–34. [11]

  , “Instrumental Variable Models for Discrete Outcomes,” Econometrica, 2010, 75 (2),
  575–601. [1, 10, 11]

  , “Semiparametric Structural Models of Binary Response: Shape Restrictions and
  Partial Identification,” mimeo, 2011. [1, 10]

  , Adam Rosen, and Konrad Smolinski, “An Instrumental Variable Model of
  Multiple Discrete Choice,” mimeo, 2011. [10]

   and , “An Instrumental Variable Random Coefficients Model for Binary Out-
  comes,” mimeo, 2012. [10]

   and Konrad Smolinski, “Sharp identied sets for discrete variable IV models,”
  mimeo, 2010. [10]

Ciliberto, Federico and Elie Tamer, “Market Structure and Multiple Equilibria in
  the Airline Markets,” Econometrica, 2009, 77, 1791–1828. [25, 26]

Florens, Jean-Pierre, James J. Heckman, Costas Meghir, and Edward
  Vytlacil, “Identification of Treatment Effects Using Control Functions in Models
  With Continuous, Endogenous Treatment and Heterogeneous Effects,” Economet-
  rica, 2008, 76 (5), 1191–1206. [11]

Heckman, James J. and Bo E. Honoré, “The Empirical Content of the Roy
 Model,” Econometrica, 1990, 58 (5), 1121–1149. [9, 46]

Ho, Katherine, “Insurer-Provider Networks in the Medical Care Market,” American
 Economic Review, 2009, 99 (1), 393–430. [25]

Holmes, Thomas J., “The Diffusion of Wal-Mart and Economies of Density,” Econo-
 metrica, 2011, 79 (1), 253–302. [25]

Imbens, Guido W. and Whitney K. Newey, “Identification and Estimation of Tri-
  angular Simultaneous Equations Models Without Additivity,” Econometrica, 2009,
  77 (5), 1481–1512. [11]

Ishii, Joy, “Compatibility, Competition, and Investment in Network Industries: ATM
  Networks in the Banking Industry,” mimeo, 2008. [25]

Kaido, Hiroaki and Andrés Santos, “Asymptotically Efficient Estimation of Modes
 Defined by Convex Moment Inequalities,” mimeo, April 2011. [18, 26, 56]



                                         40
Katz, Michael, “Supermarkets and Zoning Laws,” Unpublished PhD Dissertation,
 2007, Harvard University. [25]

Lewbel, Arthur, “Semiparametric Qualitative Response Model Estimation With Un-
  known Heteroskedasticity or Instrumental Variables,” Journal of Econometrics, 2000,
  97, 145–177. [11]

Magnac, Thierry and Eric Maurin, “Identification and Information in Monotone
 Binary Models,” Journal of Econometrics, 2007, 139, 76–104. [11]

   and , “Partial Identification in Monotone Binary Models: Discrete Regressors
  and Interval Data,” The Review of Economic Studies, 2008, 75 (3), 835–864. [11]

Morales, Eduardo, Gloria Sheu, and Andrés Zahler, “Gravity and Extended
 Gravity: Estimating a Structural Model of Export Entry,” mimeo, 2011. [2, 25, 32]

Pakes, Ariel, “Alternative Models for Moment Inequalities,” Econometrica, 2010, 78
 (6), 1783–1822. [3, 25, 27]

  , Jack R. Porter, Katherine Ho, and Joy Ishii, “Moment Inequalities and their
  Application,” mimeo, December 2011. [2, 3, 25, 27]

Pratt, John W., “Concavity of the the Log Likelihood,” Journal of the American
 Statistical Association, 1981, 76, 103–106. [9]

Shaikh, Azeem M. and Edward J. Vytlacil, “Partial Identification in Triangular
  Systems of Equations With Binary Dependent Variables,” Econometrica, 2011, 79
  (3), 949–955. [11]

Vytlacil, Edward J. and Nese Yildiz, “Dummy Endogenous Variables in Weakly
 Separable Models,” Econometrica, 2007, 75 (3), 757–779. [11]




                                         41
Appendix
A.1     Proof of Remark 1
Let fν be the density function of ν and fν0 its derivative. We can write

             ∂ 2 E[ν|ν ≥ y]
                                                        !                 !
                                  fν0 (y)      fν (y)
                            =−            −2              · y − E[ν|ν ≥ y] − 1.
                   ∂y 2           fν (y)     1 − Fν (y)

For the case in which ν ∼      N(0, σ2 ), this second derivative becomes
                                                                                      !
                     Φ(y/σ)    y 2     φ(y/σ)                     φ(y/σ)    y
                             −      +                                      −              − 1.            (38)
                   1 − Φ(y/σ) σ       1 − Φ(y/σ)                 1 − Φ(y/σ) σ

For the case in which ν ∼ Logistic, the same second derivative becomes:

                               (1 + exp(y)) · (y + ln(1 + exp(y))) − 1.                                   (39)

By simulating equations (38) and (39) one can show that both expressions are positive for
every y ∈ (−∞, ∞). 


A.2     Partial Identification: Example
Here, we show that β is partially identified in a model that imposes restrictions that are
stronger than those in Assumptions 1 to 3. Even under the assumptions of this stricter
model, we can still find at least two structures

                       S a1 ≡ {β a1 , f a1 (X|X ∗ , Z2 )f a1 (Z2 |X ∗ )f a1 (X ∗ )},
                       S a2 ≡ {β a2 , f a2 (X|X ∗ , Z2 )f a2 (Z2 |X ∗ )f a2 (X ∗ )},

such that
                           Z
            P(d, Z, X) =        f (d|X ∗ , X; β ai )f ai (X|X ∗ , Z2 )f ai (Z2 |X ∗ )f ai (X ∗ )dX ∗ ,    (40)

and β a1 6= β a2 . If β is partially identified in this stricter model, it will also be partially
identified in the more general model described in Section 2.2.
    The distributional assumptions that characterize this stricter model are:

                      f (d|X ∗ , X; β) = 1 − Φ(−βX ∗ ),                                                  (41a)
                                             1    h 1  X − X ∗ 2 i
                                                            2
                      f (X2 |X2∗ , Z2 ) = √ exp −                  2
                                                                      ,                                  (41b)
                                          σx 2π         2     σx
                                             1    h 1  Z − X ∗ 2 i
                                                             2
                           f (Z2 |X2∗ ) = √ exp −                   2
                                                                        ,                                (41c)
                                          σz 2π         2 (1 − ρz )σz
                                             1      h 1  X ∗ 2 i
                               f (X ∗ ) =    √ exp −               .                                     (41d)
                                          σx∗ 2π         2 σ x∗



                                                      42
 In words, these distributional assumptions are consistent with a discrete choice model in
which:

   1. The distribution of ν conditional on (X, X ∗ ) is normal with mean 0 and variance 1.

   2. We can write X2 = X2∗ + 2 and Z2 = X2∗ + z2 and

        (a) The distribution of 2 conditional on (X2∗ , Z2 ) is normal with mean 0 and variance
            σx2 .
        (b) The distribution of z2 conditional on X2∗ is normal with mean 0, variance σz2 , and
            the correlation coefficient between z2 and X ∗ is equal to ρz .

   3. The marginal distribution of X ∗ is normal with mean 0 and variance σx2∗ .

The distributional assumptions in equation (41) are stronger than Assumptions 1 to 3 in
Section 2.2. In particular, they impose parametric functional forms on the marginal distribu-
tion of the true unobserved covariates, X2∗ , and on the distribution of both the mismeasured
covariates, X2 , and instrument, Z2 , conditional on the true unobserved covariates. Equation
(41) also imposes full independence between the the measurement error, 2 , and the vector
(X2∗ , ν).
     Importantly, both Assumptions 1 to 3 and the assumptions implicitly contained in equa-
tion (41) are similar in that they do not specify the joint distribution of the instrument vector,
Z2 , and the true unobserved covariates, X2∗ . In equation (41c), this joint distribution depends
on the value of the parameter ρz , which captures the correlation between the true covariate
and the measurement error affecting the instrument.
     As the following result indicates, even in the statistical model described in equation (41),
it is true that the parameter vector β is not point-identified.

Result A.2.1 There exists empirical distributions of the vector of observable variables (d, Z, X),
P(d, Z, X), such that there are at least two structures S a1 and S a2 for which

   1. both S a1 and S a2 verify equation (40).

   2. both S a1 and S a2 verify the distributional assumptions in equation (41).

   3. β a1 6= β a2 .

This result can be proved by combining the following two lemmas.

Lemma A.2.1 The parameter vector β is point-identified only if the parameter σx∗ is point-
identified.

Proof: Define X ∗ = σx∗ X̃ ∗ , such that var(X̃ ∗ ) = 1. We can then rewrite equation (41a) as

                             f (d|X ∗ , X; β) = 1 − Φ(−(βσx∗ )X̃ ∗ ).

The vector β only enters the likelihood function in equation (40) multiplied by σx∗ . Therefore,
we can only identify the value of β if we know the value of σx∗ . 



                                                 43
Lemma A.2.2 The parameter vector σx is point-identified if and only if the parameter ρz is
assumed to be equal to zero.


Proof: Restrictions (41b), (41c), and (41d) imply that the variables (X2 , Z2 ) are jointly
normal and, therefore, their joint distribution is fully characterized by their variances and
covariances. Consequently, the only restrictions that can identify the parameter vector (σx ,
σz , ρz , σx∗ ) are

                                   var(X2 ) = σx2∗ + σx2 ,
                                   var(Z2 ) = σx2∗ + σz2 + ρz σx σz ,
                               cov(X2 , Z2 ) = σx2∗ + ρz σx σz .

From these restrictions, we obtain that

                                  σz2 = var(Z2 ) − cov(X2 , Z2 ).

Given that σx2 ≥ 0 and −1 ≤ ρz ≤ 1, we obtain that

          max{cov(X2 , Z2 ) − σx σz , 0} ≤ σx2∗ ≤ min{var(X2 ), cov(X2 , Z2 ) + σx σz }.

Only in the particular case in which ρz is assumed to be equal to 0, we obtain that

                                     σx2∗ = cov(X2 , Z2 ). 

    In summary, we can conclude that: (a) the model in equation (41) is a special case of the
model in Section 2.2; (b) the parameter vector β is generally set-identified in the model in
equation (41), unless we assume that ρz = 0; (c) the model in Section 2.2 does not specify
the distribution of the instrument conditional on the true covariate (i.e. does not specify the
value of ρz ); (d) therefore, the parameter vector β is set-identified in the model in Section 2.2.


A.3      Proof of Theorem 1.1
Lemma A.3.1 For any distribution function Fν , and any true value of the parameter vector,
β ∗ ∈ Γβ , the score function of the model defined in Section 2.2, conditional on the vector
(X ∗ , Z2 ), implies
                          "                                     #
                               Fν − β ∗ X ∗
                                            
                        E d                                ∗
                                               − (1 − d) X , Z2 = 0,                  (42a)
                             1 − Fν − β ∗ X ∗
                          "                                     #
                                   1 − Fν − β ∗ X ∗
                                                    
                        E (1 − d)                          ∗
                                                   − d X , Z2 = 0.                    (42b)
                                     Fν − β ∗ X ∗



Proof: Let L(d|X ∗ , Z2 ) denote the log-likelihood conditional on (X ∗ , Z2 ). Given Assumption
1:                          h                                                          i
         L(d|X ∗ , Z2 ) = E d log 1 − Fν (−βX ∗ ) + (1 − d) log Fν (−βX ∗ ) |X ∗ , Z2 .
                                                                              


                                                 44
The corresponding score function is:

                                    ∂L(d|X ∗ , Z2 )
                                                    =
                                           ∂β
     "                                                                              #
                        ∂ 1 − Fν (−βX ∗ )
                                          
               1                                           1      ∂Fν (−βX ∗ ) ∗
    E d                                     + (1 − d)                         X , Z2 .          (43)
        1 − Fν (−βX ∗ )        ∂β                     Fν (−βX ∗ )      ∂β

Reordering terms:
                              "                       ∂(1−Fν (−βX ∗ ))                      #
            ∂L(d|X ∗ , Z2 )        Fν (−βX ∗ )
                            =E d
                                                             ∂β                       ∗
                                                        ∂Fν (−βX ∗ )
                                                                         + (1 − d) X , Z2
                ∂β               1 − Fν (−βX ∗ )
                                                             ∂β

For any Fν and any value of β,
                                       ∂Fν (−βX ∗ )
                                            ∂β
                                                       = −1.                                   (44)
                                     ∂ 1−Fν (−βX ∗ )
                                          ∂β

Therefore, we can rewrite the score function as:
                            "                                    #
                                 Fν − βX ∗
                                            
                          E d                              ∗
                                               − (1 − d) X , Z2
                               1 − Fν − βX ∗

and, for the true value of the parameter vector, β ∗ , it holds that
                           "                                      #
                               Fν − β ∗ X ∗
                                            
                         E d                   − (1 − d) X ∗ , Z2 = 0,
                             1 − Fν − β ∗ X ∗

which is identical to equation (42a). In order to derive equation (42b), we reorder the terms
in equation (43) into
                               "                                   ∂Fν (−βX ∗ )           #
            ∂L(d|X ∗ , Z2 )                 1 − Fν (−βX ∗ )
                            = E d + (1 − d)
                                                                        ∂β           ∗
                                                                 ∂(1−Fν (−βX ∗ ))
                                                                                    X , Z2 .
                ∂β                            Fν (−βX ∗ )
                                                                        ∂β

Using again equation (44), we obtain
                          "                                      #
                                       1 − Fν − βX ∗
                                                     
                         E   − (1 − d)              + d X ∗ , Z2 ,
                                         Fν − βX ∗

and, for the particular case in which β = β ∗ , it holds
                         "                                      #
                                    1 − Fν − β ∗ X ∗
                                                      
                       E − (1 − d)                         ∗
                                                     + d X , Z2 = 0.
                                      Fν − β ∗ X ∗

Multiplying by −1 on both sides, we obtain equation (42b).               



                                               45
Lemma A.3.2 For any distribution function Fν , and any true value of the parameter vector,
β ∗ ∈ Γβ , the score function of the model defined in Section 2.2, conditional on the vector Z,
implies
                              "                                #
                                   Fν − β ∗ X ∗
                                                
                            E d                    − (1 − d) Z = 0,                      (45a)
                                 1 − Fν − β ∗ X ∗
                              "                                #
                                      1 − Fν − β ∗ X ∗
                                                        
                            E (1 − d)                 − d Z = 0.                         (45b)
                                        Fν − β ∗ X ∗


Proof: From equation (9) and Z = (Z1 , Z2 ) note that Z is a subset of the vector (X ∗ , Z2 ).
Therefore, Lemma A.3.2 follows from equation (42) and the Law of Iterated Expectations
(LIE). 

Lemma A.3.3 For any log concave distribution function Fν ,

                                     ∂ 2 (Fν (y)/(1 − Fν (y)))
                                                               ≥0,                         (46a)
                                                ∂y 2
                                      ∂ 2 (1 − Fν (y))/Fν (y))
                                                               ≥0,                        (46b)
                                                 ∂y 2
for any y in the support of Fν .


Proof: The first derivative of Fν (y)/(1 − Fν (y)) is:

                         ∂(Fν (y)/(1 − Fν (y)))       1        Fν0 (y)
                                                =                       ,
                                  ∂y              1 − Fν (y) 1 − Fν (y)

and the second derivative of Fν (y)/(1 − Fν (y)) is:

  ∂ 2 (Fν (y)/(1 − Fν (y)))   ∂(1/(1 − Fν (y))) Fν0 (y)       1      ∂(Fν0 (y)/(1 − Fν (y)))
                            =                            +                                   .
             ∂y 2                    ∂y        1 − Fν (y) 1 − Fν (y)           ∂y

For any distribution function Fν , it holds

                       1                 Fν0 (y)           ∂(1/(1 − Fν (y)))
                              ≥ 0,                ≥ 0,                       ≥ 0.
                   1 − Fν (y)          1 − Fν (y)                 ∂y

Moreover, as Heckman and Honoré (1990) and Bagnoli and Bergstrom (2005) show, for any
log concave distribution function Fν , it holds

                                     ∂(Fν0 (y)/(1 − Fν (y)))
                                                             ≥ 0.
                                               ∂y
Therefore, we can conclude that

                                     ∂ 2 (Fν (y)/(1 − Fν (y)))
                                                               ≥ 0.
                                                ∂y 2

                                                  46
Analogously, the first derivative of (1 − Fν (y))/Fν (y) is:

                            ∂((1 − Fν (y))/Fν (y))     1 −Fν0 (y)
                                                   =               ,
                                     ∂y              Fν (y) Fν (y)

and the second derivative of (1 − Fν (y))/Fν (y) is:

           ∂ 2 ((1 − Fν (y))/Fν (y))   ∂(1/Fν (y)) −Fν0 (y)     1 ∂(−Fν0 (y)/Fν (y))
                                     =                      +                        .
                      ∂y 2                 ∂y       Fν (y)    Fν (y)      ∂y

For any distribution function Fν , it holds

                          1             −Fν0 (y)         ∂(1/Fν (y))
                               ≥ 0,               ≤ 0,               ≤ 0.
                        Fν (y)         1 − Fν (y)            ∂y

Moreover, as Yuying An (1995) shows, for any log concave distribution function Fν , it holds

                                      ∂(−Fν0 (y)/Fν (y))
                                                         ≥ 0.
                                              ∂y
Therefore, we can conclude that

                                 ∂ 2 ((1 − Fν (y))/Fν (y))
                                                           ≥ 0. 
                                            ∂y 2
Lemma A.3.4 For any log concave distribution function Fν and any random variable η such
that
                                 E[η|X ∗ , Z2 ] = 0,
it holds that
                              h F (y + η)              i    Fν (y)
                            E      ν
                                               X ∗ , Z2 ≥            ,
                                1 − Fν (y + η)            1 − Fν (y)
                              h 1 − F (y + η)          i 1 − F (y)
                            E         ν
                                               X ∗ , Z2 ≥
                                                                ν
                                                                     .
                                  Fν (y + η)                Fν (y)


Proof: From Lemma A.3.3 and Jensen’s Inequality. 

Lemma A.3.5 If Assumptions 2 and 3 hold, then, for every value of (d, X ∗ , Z2 ) and for
every value of β ∈ Γβ :

                  Fν − (β1 Z1 + β2 X2∗ + ε)         Fν − βX ∗
                                                                        
            E                                 −                      ∗
                                                                 d, X , Z2 ≥ 0,
                1 − Fν − (β1 Z1 + β2 X2∗ + ε)     1 − Fν − βX ∗
                1 − Fν − (β1 Z1 + β2 X2∗ + ε)     1 − Fν − βX ∗
                                                                        
            E                                   −                     ∗
                                                                  d, X , Z2 ≥ 0.
                  Fν − (β1 Z1 + β2 X2∗ + ε)
                                                             
                                                    Fν − βX ∗


Proof: From Lemmas A.3.3 and A.3.4. 


                                                47
Proof of Theorem 1.1 By the LIE, we can rewrite equation (42a) as
            " "                                                 # #
                      Fν − β ∗ X ∗
                                                         
           E E E d                                   ∗      ∗
                                      − (1 − d) d, X , Z2 X , Z2 Z = 0,
                    1 − Fν − β ∗ X ∗

or, equivalently,
                " "                                                          # #
                              Fν − β ∗ X ∗
                                                        
              E E dE                          d, X ∗ , Z2 − (1 − d) X ∗ , Z2 Z = 0.
                            1 − Fν − β ∗ X ∗

From this expression and Lemma A.3.5,
        " "                                                                # #
                   Fν − (β1∗ Z1 + β2∗ X2∗ + ε)
                                                          
      E E dE                                          ∗                ∗
                                                 d, X , Z2 − (1 − d) X , Z2 Z ≥ 0.
                 1 − Fν − (β1∗ Z1 + β2∗ X2∗ + ε)

Simplifying via LIE, we obtain
                     "                                               #
                          Fν − (β1∗ Z1 + β2∗ X2∗ + ε)
                                                      
                   E d                                   − (1 − d) Z ≥ 0,
                        1 − Fν − (β1∗ Z1 + β2∗ X2∗ + ε)

or, equivalentely,
                          "                                           #
                                  Fν − (β1∗ Z1 + β2∗ X)
                                                       
                        E     d                           − (1 − d) Z ≥ 0,
                                1 − Fν − (β1∗ Z1 + β2∗ X)

for every Z ∈ Z. In conclusion, the true value of the parameter vector, β ∗ , satisfies the
score function moment inequality in equation (13b). Following the exact same steps, we can
immediately prove that β ∗ also satisfies the score function moment inequality in equation
(13a). Therefore, the true value of the parameter vector is included in the set Ω(Ms , Z M ).



A.4      Proof of Theorem 1.2
Lemma A.4.1 If Assumptions 1 and 2 hold and X2∗ = X = Z2 , for any given z ∈ Z M and
any β ∗ ∈ Γβ , it holds

                      Ω(Ms , Z M ) = {β ∈ Γβ : βz = β ∗ z, for all z ∈ Z M }.



Proof: Given that X2∗ = X = Z2 , we rewrite the index function as:

                                     βX ∗ = β1 Z1 + β2 Z2 = βZ.




                                                 48
In this case, the vectorial conditional moment Ms (z; β) becomes
                              "                               #
                                   Fν (−βZ)
                            E d                − (1 − d) Z = z ≥0,
                                1 − Fν (−βZ)
                              "                               #
                                       1 − Fν (−βZ)
                            E (1 − d)               − d Z = z ≥0.
                                         Fν (−βZ)

Doing simple algebra, we can rewrite these expressions as
                                  Fν (−βz)
                       E d|Z = z                 − E (1 − d)|Z = z ≥ 0,
                                                                
                                  1 − Fν (−βz)
                                        1 − Fν (−βz)
                         E 1 − d|Z = z                − E d|Z = z ≥ 0,
                                                                
                                           Fν (−βz)

and, reorganizing terms,

                                     E (1 − d)|Z = z
                                                                
                        Fν (−βz)                           Fν (−β ∗ z)
                                   ≥                   =                 ,                      (49a)
                                        E d|Z = z        1 − Fν (−β ∗ z)
                                                 
                      1 − Fν (−βz)
                                     E (1 − d)|Z = z
                                                    
                        Fν (−βz)                           Fν (−β ∗ z)
                                   ≤                   =                 .                      (49b)
                                        E d|Z = z        1 − Fν (−β ∗ z)
                                                 
                      1 − Fν (−βz)

 For any value of the vector z, equations (49a) and (49b) are satisfied only if βz = β ∗ z. We
prove this statement by contradiction: (a) if β is such that βz > β ∗ z, then equation (49a) is
not satisfied; (b) if β is such that βz < β ∗ z, then equation (49b) is not satisfied. Therefore,
from equations (49a) and (49b) we obtain the equality:

                                              βz = β ∗ z. 

Proof of Theorem 1.2. If we derive equations (49a) and (49b) for a set of vectors {z 1 , z 2 , . . . ,
z M }, we can analogously derive the following linear system of equations:

                                              βz 1 = β ∗ z 1 ,
                                                   ..
                                                    .
                                           βz M = β ∗ z M .

As long as the matrix (z 1 , . . . , z M ) has rank larger than K (i.e. the number of elements in
β), there is a single value of β that solves this linear system of equations and it is β ∗ . 


A.5      Proof of Theorem 1.3
Lemma A.5.1 Let g(Y ) be a convex function of Y and let fY denote the density function of
a random variable Y with support(Y ) = (−∞, ∞), E(Y ) = 0 and V ar(Y ) = σY2 , then

                                           ∂ E[g(Y )]
                                                      ≥ 0.                                       (50)
                                              ∂σY


                                                    49
Proof: Let fỸ be the density function of the random variable Ỹ = (1/σY )Y . Then
                                Z ∞                 Z ∞
                     E[g(Y )] =      g(y)fY (y)dy =     g(σY ỹ)fỸ (ỹ)dỹ
                                             −∞                             −∞

and
      ∂ E[g(Y )]         ∞                                  0                                       ∞
                     Z                                  Z                                      Z
                               0                                     0
                 =           ỹg (σY ỹ)fỸ (ỹ)dỹ =            ỹg (σY ỹ)fỸ (ỹ)dỹ +               ỹg 0 (σY ỹ)fỸ (ỹ)dỹ.
         ∂σY         −∞                                  −∞                                     0

Given that   E(Y ) = 0,
                                     Z   0                      Z    ∞
                                             ỹfỸ (ỹ)dỹ +             ỹfỸ (ỹ)dỹ = 0,
                                      −∞                         0

and, given that g(Y ) is convex,

                                     g 0 (ȳ) ≤ g 0 (ȳ¯), if and only if ȳ¯ ≥ ȳ.

Consequently,

                                                  ∂ E[g(Y )]
                                                             ≥ 0. 
                                                     ∂σY

Proof of Theorem 1.3 From Appendix A.3, we write equation (13a) as:
       " "                                                                  # #
                Fν − (β1∗ Z1 + β2∗ X2∗ + ε)
                                                         
     E E dE                                   d, X ∗ , Z2 − (1 − d) X ∗ , Z2 Z ≥ 0.
              1 − Fν − (β1∗ Z1 + β2∗ X2∗ + ε)

Given Assumptions 2 and 3 and Lemmas A.3.3 and A.5.1, for any (d, X ∗ , Z2 ) and β ∈ Γβ
                      (                                          )
                            Fν −(β1 Z1 +β2 X2∗ +ε)
                     ∂ E                       ∗
                                                      d, X ∗ , Z2
                                         1−Fν −(β1 Z1 +β2 X2 +ε)
                                                                                              ≥ 0,
                                                        ∂σε
and, therefore, for any z ∈ Z and β ∈ Γβ
        ( " "                                                           #      #)
                      Fν −(β1 Z1 +β2 X2∗ +ε)
       ∂ E E dE                          ∗
                                                     ∗                ∗
                                                d, X , Z2 − (1 − d) X , Z2 Z = z
                         1−Fν −(β1 Z1 +β2 X2 +ε)
                                                                                                                       ≥ 0.
                                                        ∂σε
Following the same steps for equation (13b), we can conclude that:
        ( " "                                                      #      #)
                          1−Fν −(β1 Z1 +β2 X2∗ +ε)
      ∂ E E (1 − d)E                       ∗
                                                      ∗          ∗
                                                 d, X , Z2 − d X , Z2 Z = z
                                   Fν −(β1 Z1 +β2 X2 +ε)
                                                                                                                       ≥ 0.
                                                        ∂σε

                                                                50
                  ¯ε ≥ σ̄ε , then, for any given value of β ∈ Γβ such that Ms (z; β|σ̄ε ) ≥ 0, it will
    Therefore, if σ̄
                       ¯ε ) ≥ 0. This implies that, for any z ∈ Z, Ω(Ms , z|σ̄ε2 ) ⊆ Ω(Ms , z|σ̄
be true that Ms (z; β|σ̄                                                                          ¯ε2 ).
Therefore, for any Z M , Ω(Ms , Z M |σ̄ε2 ) ⊆ Ω(Ms , Z M |σ̄¯ε2 ). 


A.6      Proof of Theorem 2.1
Lemma A.6.1 For any distribution function Fν such that E[ν|, X ∗ , Z2 ] = 0, it holds

       E dE[ν|d = 1, X ∗ , Z2 ] X ∗ , Z2 = E (1 − d)E[−ν|d = 0, X ∗ , Z2 ] X ∗ , Z2 .
                                                                                




Proof: Using the LIE,

                                E ν X ∗ , Z2 = 0,
                                            

             E E[ν|d, X ∗ , Z2 ] X ∗ , Z2 = 0,
                                            

         E[d|X ∗ , Z2 ]E[ν|d = 1, X ∗ , Z2 ] = −E[1 − d|X ∗ , Z2 ]E[ν|d = 0, X ∗ , Z2 ],
         E[d|X ∗ , Z2 ]E[ν|d = 1, X ∗ , Z2 ] = E[1 − d|X ∗ , Z2 ]E[−ν|d = 0, X ∗ , Z2 ],
         E dE[ν|d = 1, X ∗ , Z2 ] X ∗ , Z2 = E (1 − d)E[−ν|d = 0, X ∗ , Z2 ] X ∗ , Z2 . 
                                                                                      

Lemma A.6.2 For any distribution function Fν such that E[ν|, X ∗ , Z2 ] = 0 and any β ∗ ∈
Γβ , it holds:

               E (1 − d)(−(β ∗ X ∗ )) + dE[ν|ν ≥ −β ∗ X ∗ , X ∗ ] X ∗ , Z2 ≥0,
                                                                         
                                                                                    (51a)
                 E dβ X + (1 − d)E[−ν| − ν ≥ β X , X ] X , Z2 ≥0.
                    ∗ ∗                            ∗ ∗       ∗     ∗
                                                                          
                                                                                   (51b)



Proof: For simplicity of notation, we show the proof for equation (51b). The proof for equation
(51a) is completely equivalent. Applying the conditional expectation operator E[·|X ∗ , Z2 ] to
equation (8) for the case in which d = 1,

                                       E[dβ ∗ X ∗ + dν|X ∗ , Z2 ] ≥ 0,
and, by the LIE,
                                     h                                  i
                                E E dβ ∗ X ∗ + dν d, X ∗ , Z2 X ∗ , Z2 ≥ 0,
                                                                
                            h                                            i
                           E dβ ∗ X ∗ + dE ν d = 1, X ∗ , Z2 X ∗ , Z2 ≥ 0.
                                                               


From Lemma A.6.1,
                       h                                                     i
                     E dβ ∗ X ∗ + (1 − d)E − ν d = 0, X ∗ , Z2 X ∗ , Z2 ≥ 0,
                                                                    


and, from Assumption 1,
                     h                                          i
                   E dβ ∗ X ∗ + (1 − d)E − ν d = 0, X ∗ X ∗ , Z2 ≥ 0.
                                                      



                                                     51
Using again equation (8), we obtain equation (51b),
                  h                                                   i
                E dβ ∗ X ∗ + (1 − d)E − ν − ν ≥ β ∗ X ∗ , X ∗ X ∗ , Z2 ≥ 0.
                                                            


If we follow the same steps starting from equation (8) for the case in which d = 0, then we
obtain equation (51a). 

Lemma A.6.3 If Assumptions 2 and 3 hold, then, for every value of (d, X ∗ , Z2 ) and for
every value of β ∈ Γβ :
                                                                                    
       E E ν|ν ≥ −(β1 Z1 + β2 X2 − ε), Z1 , X2 − (1/β2 )ε − E ν|ν ≥ −βX , X d, X , Z2 ≥0,
                               ∗             ∗                         ∗   ∗     ∗
                                                                          

                                                                                    
E E − ν| − ν ≥ β1 Z1 + β2 X2 − ε, Z1 , X2 − (1/β2 )ε − E − ν| − ν ≥ βX , X d, X , Z2 ≥0.
                            ∗           ∗                              ∗   ∗     ∗
                                                                          




Proof: From Jensen’s Inequality. 

Proof of Theorem 2.1 For simplicity of notation, we show the proof for equation (14b).
The proof for equation (14a) is completely equivalent.
   By the LIE, we can rewrite equation (51b) as
        "                                                          #
      E E E dβ X + (1 − d)E − ν| − ν ≥ β X , X d, X , Z2 X , Z2 Z ≥ 0.
                  ∗ ∗                           ∗ ∗    ∗   ∗    ∗
                                                        


From this expression and Lemma A.6.3, we obtain a weaker inequality
     "                                                                           #
   E E d(β1∗ Z1 + β2∗ X) + (1 − d)E − ν| − ν ≥ β1∗ Z1 + β2∗ X2 , Z1 , X2 X ∗ , Z2 Z ≥ 0,
                                                                       


and, simplifying this expression, we conclude that
            "                                                             #
         E d(β1∗ Z1 +β2∗ X) + (1 − d)E − ν| − ν ≥ β1∗ Z1 + β2∗ X, Z1 , X Z ≥ 0. 
                                                                       



A.7     Proof of Theorem 2.2
Lemma A.7.1 If Assumptions 1 and 2 hold and X2∗ = X = Z2 , for any given z ∈ Z M and
any β ∗ ∈ Γβ , ∃ β ∈ Γβ such that β 6= β ∗ and β ∈ Ω(Mr , Z M ).

Proof: Given that X2∗ = X = Z2 , we rewrite the index function as βX ∗ = β1 Z1 + β2 X = βZ.
The two revealed preference inequalities in equations (14a) and (14b) conditional on Z = z
become
                      h                                          i
                     E (1 − d)(−βZ) + dE ν|ν ≥ −βZ, Z Z = z ≥ 0,
                                                         
                                                                                      (52a)
                      h                                          i
                    E dβZ + (1 − d)E − ν| − ν ≥ βZ, Z Z = z ≥ 0.
                                                         
                                                                                      (52b)


                                            52
Given Assumption 2, both E ν|ν ≥ −βz, z and E − ν| − ν ≥ βz, z are larger or equal to
                                                                   

zero for any value of β ∈ Γβ and z ∈ RK . Therefore, both inequalities (52a) and (52b) hold
for any value of β ∈ Γβ such that βz = 0.
    Also, as long as var(ν) > δ1 , for any δ1 > 0, ∃ δ2 > 0 such that kβk < δ2 and β ∈
Ω(Mr, Z M ). In words, by continuity of the functions E ν|ν ≥ −βZ, Z and E − ν| − ν ≥
βZ, Z in the variance of ν, as long as this variance is different from zero, we can define a
small ball around β = 0 such that all the values of the parameter vector in that ball are
included in Ω(Mr , Z M ). 

Proof of Theorem 2.2. Lemma A.7.1 holds for any arbitrary vector Z M . Therefore, for
any arbitrary vector Z M , the set Ω(Mr , Z M ) always includes the point β = 0. Also, or any
arbitrary vector Z M , as long as var(ν) > δ1 , for any δ1 > 0, ∃ δ2 > 0 such that kβk < δ2 and
β ∈ Ω(Mr , Z M ). 


A.8      Proof of Theorem 2.3
We show the proof for equation (14b). The proof for equation (14a) is completely equivalent.
From Section A.6, we can write the revealed preference moment inequality in eq. (14b) as:
    " 
  E E d(β1 Z1 + β2 X ∗ − ε)+
                                                                                            #
                 (1 − d)E       − ν| − ν ≥ β1 Z1 + β2 X2∗ − ε, Z1 , X2∗ − (1/β2 )ε X ∗ , Z2 Z ≥ 0.
                                                                                 


Assumption 3 implies that
                                                              
                                ∂ E d(β1 Z1 + β2 X2 + ε) X , Z2
                                                  ∗       ∗

                                                                      = 0.
                                                ∂σε
Assumptions 2 and 3 and Lemma A.5.1 imply that, for any (X ∗ , Z2 )
                                                                         
     ∂ E (1 − d)E − ν| − ν ≥ β1 Z1 + β2 X2 − ε), Z1 , X2 − (1/β2 )ε X , Z2
                                         ∗             ∗
                                                                    ∗

                                                                                           ≥ 0.
                                                ∂σε
Therefore, for any z ∈ Z and β ∈ Γβ ,

                                             ∂Mr (z; β)
                                                        ≥ 0.
                                               ∂σε
In conclusion, if σ̄¯ε ≥ σ̄ε , then, for for any z ∈ Z, and any given value of β ∈ Γβ such that
                                                  ¯ε ) ≥ 0. Therefore, for any Z M , Ω(Mr , Z M |σ̄ε2 )
Mr (z; β|σ̄ε ) ≥ 0, it will be true that Mr (z; β|σ̄
⊆ Ω(Mr , Z M |σ̄¯ε2 ). 




                                                   53
A.9       Proof of Theorem 3.2
Given that X2∗ = X = Z2 , we write the index function as βX ∗ = β1 Z1 + β2 X = βZ. For any
vector q ∈ Q (see equation (15)), we can find some other vector q 0 ∈ Q such that q 0 = 1 − q.
In this way, we can partition the Q vectors into Q/2 pairs. Let us index these pairs of vectors
by p = 1, . . . , Q/2. We define each pair p = {q0p , q1p }, with q1p = 1 − q0p .
    Both vectors in a single pair p have the property that, for every value of z ∈ Z such
that Ψq0p (z) = 1, it will be true that Ψq1p (−z) = 1. Therefore, for each given p, using the
assumption X = Z2 , we can rewrite the moment inequalities
               qp
                         h                                                              i
           Ms0 (β) = E Ψq0p (Z) · m+ s (d, Z1 , Z2 ; β) + Ψ q0
                                                                        −
                                                             p (−Z) · m (d, Z1 , Z2 ; β) ≥0,
                                                                        s
               q1p
                         h                                                              i
           Ms (β) = E Ψq1p (Z) · m+  s (d, Z1 , Z2 ; β) + Ψ q1
                                                                        −
                                                             p (−Z) · m (d, Z1 , Z2 ; β) ≥0,
                                                                        s

as
            qp
                      h                                                                  i
           Ms0 (β) = E Ψq0p (Z) · m+
                                   s (d, Z1 , Z2 ; β) + Ψ  p
                                                          q0 (−Z)  · m −
                                                                       s (d, Z1 , Z2 ; β)  ≥0,
            qp
                      h                                                                  i
           Ms1 (β) = E Ψq0p (−Z) · m+                                  −
                                     s (d, Z1 , Z2 ; β) + Ψq0p (Z) · ms (d, Z1 , Z2 ; β) ≥0.


or, using the LIE,
       qp
                h                                                                             i
    Ms0 (β) = E Ψq0p (Z) · E m+                                      E
                                                                      −
                                s (d, Z1 , Z2 ; β) Z   + Ψ  p
                                                           q0 (−Z) ·    ms (d, Z1 , Z2 ; β) Z    ≥0,
       qp
                h                                                                              i
    Ms1 (β) = E Ψq0p (−Z) · E m+  s (d, Z1 , Z2 ; β) Z + Ψq0p (Z) · E ms (d, Z1 , Z2 ; β) Z
                                                                      −                    
                                                                                                 ≥0.

Using equation (13), we can write
                                                                
                                                     Fν − βZ
           E ms (d, Z1 , Z2 ; β) Z = (1 − Fν − β Z )
             −                                  ∗                 − Fν − β ∗ Z ,
                                                                                
                                                      1 − Fν − βZ
                                                             
                                               1 − Fν − βZ
           E ms (d, Z1 , Z2 ; β) Z = Fν − β Z
                                            ∗               − (1 − Fν − β ∗ Z ),
             +                                                                
                                                   Fν − βZ

                                              qp           qp
and, plugging these expressions into Ms0 (β) and Ms1 (β), we obtain
                                                   qp
                                               Ms0 (β) =
      h                                                                     h       1        ii
     E Ψq0p (Z)(1 − Fν − β ∗ Z )(Gν (β, Z) − 1) + Ψq0p (−Z)Fν − β ∗ Z
                                   
                                                                                           − 1 ≥ 0,
                                                                                 Gν (β, Z)
                                                   qp
                                               Ms1 (β) =
      h                     h       1        i                                           i
     E Ψq0p (Z)Fν − β ∗ Z                  − 1 + Ψq0p (−Z)(1 − Fν − β ∗ Z )(Gν (β, Z) − 1) ≥ 0,
                                                                         
                                 Gν (β, Z)

where
                                             Fν − β ∗ Z 1 − Fν − βZ
                                                                   
                                 Gν (β, Z) =                       .
                                             Fν − βZ 1 − Fν − β ∗ Z


                                                    54
                                         qp             qp
Given that G(β ∗ , Z) = 0 for all Z, Ms0 (β ∗ ) = Ms1 (β ∗ ) = 0. Also, for any k ∈ K,
                                         p                    p
                                    ∂Mq0 (β)          ∂Mq1 (β) 
                                       s                   s
                               sign      k
                                                = −sign             ,
                                      ∂β                  ∂β k
                          qp            qp
and, therefore, either Ms0 (β) or Ms1 (β) will become negative as we move away from β =
0 by moving each of the scalar components of the vector β one at a time. We can define
                              qp         qp
inequalities analogous to Ms0 (β) or Ms1 (β) for every p = 1, . . . , Q/2. Given that the matrix
Q is the standard basis in RK , for every β 6= β ∗ there exists at least one p such that one of
the inequalities in the pair p does not hold at that value of β. Therefore, the only value of β
∈ Γβ such that Mqs (β) ≥ 0 for all q = {1, . . . , Q} is β = β ∗ . 


A.10      Proof of Theorem 3.4 and Theorem 4.4
We prove boundedness and closedness for Ω(Ms ). The proof for Ω(Mr ) is analogous.

Boundedness.

Lemma A.10.1 The set Ω(Ms ) is bounded if, for very element of PK = {p ∈ RK : kpk =
1}, the unit sphere in RK , there exists at least one q ∈ {1, . . . , Q} such that
                                         K
                                   (                     )
                                         X      ∂Mqs (β)
                               lim c ·       pk            = −∞.
                              c→∞                 ∂βk
                                             k=1



This condition means that the identified set Ω(Ms ) is bounded if, as we move far away
from the origin in any possible direction in RK , there is at least one inequality, Mqs (β), that
becomes negative. This moment q may be different for each p ∈ PK . A sufficient condition
for Lemma A.10.1 to hold is that, for every p in PK , ∃ q ∈ Q such that, for all β ∈ Γβ and
all k ∈ K,
                                                              !
                                                    ∂Mqs (β)
                               sign(pk ) = −sign                .                            (53)
                                                      ∂βk

The matrix Q is the standard basis in RK (see equation (15)). Therefore, as long as equation
(18) holds, for any p in P, ∃ q ∈ Q that verifies equation (53). 

Closedness. Given that Ω(Ms ) is bounded, it will also be closed because all the inequalities
in the vector Ms (β) are weak inequalities. 


A.11      Proof of Theorem 3.5 and Theorem 4.5
Lemma A.11.1 Let β and Y be two K · 1 vectors, g be a strictly convex function, and EY (·)
denote the expectation with respect to the random vector Y , then EY (g(Y 0 β)) is a convex




                                                   55
function of the random vector β; i.e.

                                           ∂ 2 EY (g(Y 0 β))
                                                 ∂ββ 0
is positive definite.
Proof: Using basic algebra,

                             ∂ 2 EY (g(Y 0 β))
                                                = EY [Y Y 0 g 00 (Y 0 β)].
                                   ∂ββ 0

Given that g is strictly convex, g 00 (Y 0 β) > 0, g 00 (Y 0 β) = g 00 (Y 0 β) g 00 (Y 0 β), and we can
                                                                  p           p

rewrite
                                    ∂ 2 EY (g(Y 0 β))
                                                      = EY [Ỹ Ỹ 0 ]
                                          ∂ββ 0
              p
where Ỹ = Y g 00 (Y 0 β). Using this equation and properties of quadratic forms, we can
conclude that the Hessian of EY (g(Y 0 β)) is positive definite.

Proof of Theorem 3.5 and Theorem 4.5 The set Ω(Ms ) = {β ∈ Γβ : Ms (β) ≥ 0} is
convex if all the moment inequalities in the vector Ms (β) are convex in β (see Kaido and
Santos (2011)). Lemmas A.3.3 and A.11.1 show that all the elements of Ms (β) are convex in
β. Therefore, we can conclude that Ω(Ms ) is a convex set. Analogously, since Assumption 2
and Lemma A.11.1 yield that all the elements of Mr (β) are convex in β, then Ω(Mr ) is also
a convex set. 

A.12       Score Function Inequalities when ν = 0.
Lemma A.12.1 The infinite support condition on Assumption 3 implies that, for any vector
(d, X ∗ , Z2 ), any β ∈ Γβ ,

                              P r(−β1 Z1 − β2 X > 0|d, X ∗ , Z2 ) > 0.



Proof:

          P r(−β1 Z1 − β2 X > 0|d, X ∗ , Z2 ) = P r(−β1 Z1 − β2 X ∗ + ε > 0|d, X ∗ , Z2 )
                                               = 1 − Fε (β1 Z1 + β2 X ∗ |d, X ∗ , Z2 ) > 0. 

Proof of Lack of Identification Power of Score Function Inequalities when ν = 0.
Assumption 2(b) implies that, for any given real value y, Fν (y) = 0 if y < 0, and Fν (y) =
1 if y ≥ 0. In Appendix A.3, we showed that we can rewrite the conditional score function
inequality in equation equation (42a) as
            " "                                                           # #
                       Fν − (β1 Z1 + β2 X)
                                                         
          E E dE                             d, X ∗ , Z2 − (1 − d) X ∗ , Z2 Z ≥ 0.
                     1 − Fν − (β1 Z1 + β2 X)


                                                  56
Therefore, if ν has a degenerate distribution at 0, then
                                                      
                                 Fν − (β1 Z1 + β2 X)
                                                          = ∞,
                               1 − Fν − (β1 Z1 + β2 X)

for any vector (Z1 , X) such that −(β1 Z1 + β2 X) ≥ 0. Given that, from Lemma A.12.1, for
any β ∈ Γβ

                               P r(−β1 Z1 − β2 X > 0|d, X ∗ , Z2 ) ≥ 0,

this implies that, for any β ∈ Γβ ,
                                                   
                                Fν − (β1 Z1 + β2 X)
                                                                 
                        E                             d, X ∗ , Z2 = ∞.
                              1 − Fν − (β1 Z1 + β2 X)

Using the LIE, we can conclude that, for any value of β ∈ Γβ and any value of the vector Z,

                              Mr (z; β) = E[m−
                                             r (d, Z1 , X; β)|Z] = ∞.

The proof is completely analogous for the moment function m+  r (d, Z1 , X; β). Therefore, if ν
has a degenerate distribution at 0, then the score function moment inequalities are equal to
∞ for any value of the parameter vector β in the parameter space. Therefore, these moment
inequalities have no identification power. 


A.13      Proof of Theorem 6
Lemma A.13.1 For any random variable Y and any constant y ∈               R, it holds
                                 E[Y 1{Y ≥ 0}] ≥ E[Y 1{Y ≥ y}].


Proof:
                                                      Z
                                 E[Y 1{Y ≥ y}] =          Y fY (Y )dY,
                                                      y

where fY (Y ) is the density function of Y . Then

                                   ∂ E[Y 1{Y ≥ y}]
                                                   = −yfY (y),
                                          ∂y

which is positive if y < 0 and negative if y > 0. Therefore, E[Y 1{Y ≥ y}] reaches it maximum
at y = 0. 




                                                 57
Lemma A.13.2 For any value of β ∈ Γβ , any d ∈ {0, 1}, and any value of (Z1 , X) ∈ RK ,

                      E[m−
                         r (d, Z1 , X; β)|Z1 , X] ≥ E[mr (d, Z1 , X; β)|Z1 , X],
                                                       −

                      E[m+
                         r (d, Z1 , X; β)|Z1 , X] ≥ E[mr (d, Z1 , X; β)|Z1 , X],
                                                       +


where (m−                    +                                                    −
          r (d, Z1 , X; β), mr (d, Z1 , X; β)) are defined in equation (29) and (mr (d, Z1 , X; β),
m+
 r (d, Z1 , X; β)) are defined in equation (14).

Proof: We show the proof for the inequality

                      E[m−
                         r (d, Z1 , X; β)|Z1 , X] ≥ E[mr (d, Z1 , X; β)|Z1 , X].
                                                       −


The proof for E[m+r (d, Z1 , X; β)|Z1 , X] ≥ E[mr (d, Z1 , X; β)|Z1 , X] is completely equivalent.
                                                +

From equation (14a), we can rewrite E[mr (d, Z1 , X; β)|Z1 , X] as
                                            −


    E (1 − d)(−(β1 Z1 + β2 X)) + dE ν|ν ≥ −(β1 Z1 + β2 X), Z1 , X Z1 , X =
                                                                             

    E[1 − d|Z1 , X](−(β1 Z1 + β2 X)) + E[d|Z1 , X]E ν|ν ≥ −(β1 Z1 + β2 X), Z1 , X =
                                                                                     

                                                    E ν 1{ν ≥ −(β1 Z1 + β2 X)}|Z1 , X
                                                                                        
    E[1 − d|Z1 , X](−(β1 Z1 + β2 X)) + E[d|Z1 , X]                                         =
                                                                  E[d|Z1 , X]
    E[1 − d|Z1 , X](−(β1 Z1 + β2 X)) + E ν 1{ν ≥ −(β1 Z1 + β2 X)}|Z1 , X .
                                                                             


From Lemma A.13.1,

             E ν 1{ν ≥ −(β1 Z1 + β2 X)}|Z1 , X ≤ E ν 1{ν ≥ 0}|Z1 , X ,
                                                                 


and, therefore,

                             E[m−r (d, Z1 , X; β)|Z1 , X] ≤
   E[1 − d|Z1 , X](−(β1 Z1 + β2 X)) + E ν 1{ν ≥ 0}|Z1 , X = E[m−
                                                           
                                                               r (d, Z1 , X; β)|Z1 , X]. 

Lemma A.13.3 For any value of β ∈ Γβ and any q = 1, . . . , Q, Mqr (β) ≥ Mqr (β).

Proof: Using the LIE, we can write Mqr (β) as
   h                                                                                                  i
 E E[Ψq (Z)|Z1 , X]E[m+r (d, Z1 , X; β)|Z 1 , X] + E [Ψq (−Z)|Z 1 , X] E [m −
                                                                            r (d, Z1 , X; β)|Z 1 , X]  ,

and Mqr (β) as
   h                                                                                         i
 E E[Ψq (Z)|Z1 , X]E[m+
                      r (d, Z1 , X; β)|Z1 , X] + E[Ψq (−Z)|Z1 , X]E[mr (d, Z1 , X; β)|Z1 , X] .
                                                                     −



Given Lemma A.13.2, it is immediate that Mqr (β) ≥ Mqr (β). 

Proof of Theorem 6 From Theorem 4.1, we know that, for every q = 1, . . . , Q, Mqr (β ∗ ) ≥ 0.
Therefore, Lemma A.13.3 implies, for every q = 1, . . . , Q, Mqr (β ∗ ) ≥ 0. By definition, if for
every q = 1, . . . , Q, Mqr (β ∗ ) ≥ 0, then β ∗ ∈ Ω(Mr ). 




                                                   58
