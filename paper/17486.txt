                                 NBER WORKING PAPER SERIES




 IF YOU BUILD IT WILL THEY COME? TEACHER USE OF STUDENT PERFORMANCE
                        DATA ON A WEB-BASED TOOL

                                              John H. Tyler

                                          Working Paper 17486
                                  http://www.nber.org/papers/w17486


                       NATIONAL BUREAU OF ECONOMIC RESEARCH
                                1050 Massachusetts Avenue
                                  Cambridge, MA 02138
                                      October 2011




I would like to thank Ben Zhang, Miriam Joelson, David Stern, Max Ashby, and David Storch for
for their research assistance, and Amy Wooten, Eric Taylor, Dan Goldhaber, Jeffrey Wayman, and
Richard Murnane for helpful conversations and intellectual contributions to this work. I also want
to thank the Council of Great City Schools for their generous support for this project. Finally, a particular
debt of gratitude is owed to the Cincinnati Public School system and especially to Elizabeth Holtzapple,
Director of Research, Evaluation, and Test Administration, and Sarah Trimble-Oliver, Academic and
Assessment System Administrator. The views expressed herein are those of the author and do not
necessarily reflect the views of the National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2011 by John H. Tyler. All rights reserved. Short sections of text, not to exceed two paragraphs,
may be quoted without explicit permission provided that full credit, including © notice, is given to
the source.
If You Build It Will They Come? Teacher Use of Student Performance Data on a Web-Based
Tool
John H. Tyler
NBER Working Paper No. 17486
October 2011
JEL No. I21

                                              ABSTRACT

The past decade has seen increased testing of students and the concomitant proliferation of computer-based
systems to store, manage, analyze, and report the data that comes from these tests. The research to
date on teacher use of these data has mostly been qualitative and has mostly focused on the conditions
that are necessary (but not necessarily sufficient) for effective use of data by teachers. Absent from
the research base in this area is objective information on how much and in what ways teachers actually
use student test data, even when supposed precursors of teacher data use are in place. This paper addresses
this knowledge gap by analyzing usage data generated when teachers in one mid-size urban district
log onto the web-based, district-provided data deliver and analytic tool. Based on information contained
in the universe of web logs from the 2008-2009 and 2009-2010 school years, I find relatively low
levels of teacher interaction with pages on the web tool that contain student test information that could
potentially inform practice. I also find no evidence that teacher usage of web-based student data is
related student achievement, but there is reason to believe these estimates are downwardly biased.


John H. Tyler
Box 1938
21 Manning Walk
Brown University
Providence, RI 02912
and NBER
john_tyler@brown.edu
                                                    Abstract
The past decade has seen increased testing of students and the concomitant proliferation of computer-
based systems to store, manage, analyze, and report the data that comes from these tests. The research to
date on teacher use of these data has mostly been qualitative and has mostly focused on the conditions
that are necessary (but not necessarily sufficient) for effective use of data by teachers. Absent from the
research base in this area is objective information on how much and in what ways teachers actually use
student test data, even when supposed precursors of teacher data use are in place. This paper addresses
this knowledge gap by analyzing usage data generated when teachers in one mid-size urban district log
onto the web-based, district-provided data deliver and analytic tool. Based on information contained in
the universe of web logs from the 2008-2009 and 2009-2010 school years, I find relatively low levels of
teacher interaction with pages on the web tool that contain student test information that could potentially
inform practice. I also find no evidence that teacher usage of web-based student data is related student
achievement, but there is reason to believe these estimates are downwardly biased.




1. Introduction

        Schools and teachers are increasingly being called upon to utilize student performance

data in making decisions about policy and practice. Indeed, actors in the K-12 arena are likely to

be seen as out of touch and behind the times if they are not engaging in “data driven decision

making” or do not claim to be “data driven schools” or “data driven teachers.” As recently as

2005, however, Wayman reported that “…the use of student data for educational improvement

has not been widespread. Until only recently, examining student data was a difficult chore for

most educators” (Wayman 2005). The recent proliferation of web-based tools to present and

assist in the analysis of student performance data has eased this concern and so the questions

now turn to how are schools and teachers using the new tools to improve student outcomes.1


        This paper provides some answers to this question by examining how teachers in one

mid-size urban district in the Midwest use a web based tool designed to provide them with



1
  The availability of web-based student data tools range from several commercial products now available to districts
to systems developed within-house by districts to customized products built to specification by outside vendors. An
example of the latter is the New York City school system’s $180M, five-year agreement with IBM in 2007 to build a
system for tracking and analyzing student and school performance (New York Times, March 6, 2007).

                                                                                                                  1
student achievement information that can potentially improve their practice. In particular I seek

to answer three questions: how much do teachers in the district use the web tool, what types of

information do teachers access when they do use the tool, and is usage of web-based student data

related to student achievement gains? In addressing these questions this paper is primarily a

descriptive study. Nevertheless, solid answers to these questions are critical as the field moves

forward in trying to better utilize the vast amounts of student performance that are now collected

every year.


         This descriptive study is the first that captures and analyzes at a detailed level objective

information on teacher usage of student performance data presented through a web based tool.

Data for the study come from web logs that are generated each time a teacher logs into the

district’s web-based tool that is designed to present student data to teachers in user-friendly

formats. This system, a data “Dashboard” system, was developed in-house and brought online at

the beginning of the 2005-2006 school year. The analysis in this paper is based on web log data

from the 2008-2009 and 2009-2010 school years.2 For these years I analyze teacher logins to the

Dashboard system, the types of pages in Dashboard that teachers view when logged in, the

amount of time teachers spend on the different kinds of pages, and whether this activity is related

to student test score growth.


         A simple theory of action for the way in which teacher usage of student performance data

could affect student achievement would have the following sequential components:


              1. Test students to gather performance information.




2
 For narrative simplicity in the remainder of the paper, I will refer to the 2008-2009 school year as the 2009 school
year and the 2009-2010 school year as the 2010 school year.

                                                                                                                    2
           2. Provide the test results to the teacher in a manner and in formats that foster

               meaningful analysis.

           3. The teacher accesses the test data.

           4. The teacher spends time analyzing the test data.

           5. The teacher draws knowledge from that analysis that can inform her practice.

           6. The teacher knows how and has the ability to alter practice based upon the new

               knowledge.

           7. The teacher acts on the new knowledge and classroom practice is altered.

           8. The altered practice has a positive impact on student achievement.


       A break down in any one of these steps would prevent the effective use of student test

data as in input to instructional improvement and eventual student achievement gains. This

project examines the third and fourth steps in the model: do teachers access student performance

data and how much time do they spend with the data when it is provided to them? In particular I

analyze the extent to which core subject (math, English, social studies, and science) teachers in

grades 3 through 8 accessed the performance data of their students via the Dashboard web tool

during the 2009 school year. I then use 2010 data to examine changes in usage over a two-year

period and to explore the relationship between usage and student performance as measured by

their scores on various tests.


       In a preview of the findings, the average teacher targeted in the study logged into the

Dashboard system just less than once per week during the 2009 school year, and 43 percent of

these teachers spent a total of one hour or less during the year viewing Dashboard pages

containing test data information on their students (17 percent spent 20 minutes or less during the

year on these pages). I also find very little change in usage between the 2009 and 2010 school

                                                                                                    3
years and no evidence that usage is related to student achievement growth. These relatively low

usage levels leave one concerned about the extent to which the average teacher is using

Dashboard-presented student test data to inform practice, and the low usage levels also hamper

our ability to effectively study the usage-student performance linkage.


       The remainder of this paper is organized as follows. In the next section I discuss the

literature on teacher use of student performance data. This is followed by a discussion of the data

used for this project in section 3 and a presentation and discussion of the results of the analyses

of the data in section 4. Section 5 summarizes and concludes.


       The district in this study is a typical mid-sized urban school district, and it has much in

common with larger urban districts. There are approximately 35,000 K-12 students in the district

and like most urban districts the students tend to come from low income and minority families

and student achievement in the district lags behind that of the state as a whole. In the most recent

year for which data is available, about 70 percent of the students are eligible for free or reduced

price lunch, about 70 percent of the students are African-American, and 25 percent of the

students are white.




2. Prior Research

       The recent push for schools and teachers to use student test data as inputs to decision

making rests on a relatively recent and thin research base. Studies of how districts, schools, and

teachers utilized data began only about a decade ago and the first research in this area tended to

be case studies describing the many ways in which data was being used to support education




                                                                                                      4
decisions (Pardini 2000; Feldman and Tung 2001; Protheroe 2001; Lachat 2002).3 Following this

early optimistic assessment regarding the role data could play in assisting school improvement

efforts, Ingram, Louis, and Schroeder (2004) used interviews and focus group data from nine

schools to caution against assuming that the mere presence of data from standardized tests would

translate into the use of that data by schools and teachers. Nevertheless, the increased testing of

students combined with the falling price of computing and data storage and the proliferation of

data management and analysis tools meant that schools would both be awash in student

performance data and subject to pressures to use those data to increase student achievement.


          One can get a sense of the rapid growth of the use of education data that was occurring

during last decade by looking at looking at the growth in revenue from data management and

analysis software and services in the K-12 sector. A 2003 report estimated that between 2000 and

2003 vendor revenues in this area grew from $98.8 million to $145 million (Stein 2003). This

same report ventured that the (then) recent passage of the No Child Left Behind (NCLB) meant

that school districts were facing new data reporting challenges that few were prepared to meet,

thus suggesting a market ripe for additional investments in data management and analysis tools.


          Spurred by both the testing and the reporting requirements of NCLB, and the desire to

use student test data for school improvement and student achievement gains, the push was on to

develop systems that could store, manage, present, and help practitioners analyze student data.

The resulting development and proliferation of software and web-based tools designed to make

data analysis both cheaper for districts and more user-friendly for teachers and administrators,

helped foster a series of studies of how the field was using data and a focus on the factors that



3
    This work is summarized in Wayman, Stringfield, and Yakimowski (2004).

                                                                                                      5
seemed to promote or hinder effective data use. District and school-level surveys, interviews,

case studies, focus groups, and ethnographic studies were all employed to better understand what

made schools and teachers “data driven” (e.g., Brunner, Fasca et al. 2005; Chen, Heritage et al.

2005; Kerr, Marsh et al. 2006; Marsh, Pane et al. 2006; Datnow, Park et al. 2007; Crawford,

Schlager et al. 2008).


        A summary of this research falls into three areas. First, the probability of data use by

teachers taking hold in a school is increased when a “culture of data use” is developed in the

school, when the school has strong leadership that is supportive of teacher use of data, when

there is sufficient professional development around data use, when there is allotted time for data

use, and when teachers are provided with data systems that are easy to navigate. Second, factors

that affect self-reported levels of teacher data use include the timeliness of data that is turned

back to teachers, the perceived validity of the test data, and flexibility in the ability to alter

instructional practice and pace, particularly vis a vis curriculum pacing guides. Third, at the top

of the list regarding how teachers use data are using data to learn about their new students at the

beginning of the year, discerning student needs in order to group students for instruction, and

determining class-wide strengths and weaknesses for instructional planning.


        The most comprehensive information on teacher usage of data comes from the U.S.

Department of Education’s National Educational Technology Trends Study (NETTS), surveys

administered in 2005 and 2007 to nationally representative samples of teachers each year. In

these data the percentage of teachers reporting having access to district student data systems

went from 48 percent in 2005 to 74 percent in 2007 (U.S. Department of Education 2008).

NETTS respondents also reported a greater likelihood of access to grades and attendance data



                                                                                                      6
than to student achievement data in 2007, and they expressed a desire for more professional

development around data use.


       Of the 74 percent of teachers who report having access to student data in 2007, 3 percent

reported using data at least once a week for the purpose of identifying skills gaps of individual

students so that instructional could be individually tailored according to student needs. Another

15 percent reported engaging in this type of activity at least once or twice a month (U.S.

Department of Education 2009). This type of interaction between a teacher and the data of

individual students is what many have in mind when they think of using data to improve

instruction and increase student achievement.


       At this point there is one study in the literature of which I am aware that is based on

objective measures of teacher usage. Wayman, Cho, and Shaw (2009) use usage report data from

a commercially available tool used by the district in their study. The usage data provided by the

system tell us about how many times teachers accessed the system and which sections of the

system were accessed. There are, however, limitations to what we learn from this study. For

example, we learn from the study that 93 percent of the teachers accessed the Reports section of

the tool, the section that contains student performance data. However, given the availability of

information from different levels contained in the Reports section (e.g., district, school, ,class,

and student), we do not know what percentage of the teachers viewed report data at the

individual student level, and importantly, we do not know how much time teachers spent viewing

student level data. Nevertheless, the very fact that objective usage measures are being collected

and reported is a notable step forward in the field.




                                                                                                      7
         In summary, the availability of student performance data and the tools for using that data

have both grown substantially in the last decade. A research base regarding what we think needs

to be in place and what needs to occur if teachers are to intelligently use data has also developed

apace. Noticeably absent from the research, however, is objective information on how much

teachers actually use student achievement data when the hypothesized precursors for teacher data

use are in place. This descriptive study is the first to provide detailed information on how much

teachers in a given setting actually use computer-resident student test data, how they spend their

time when they do access the data, and whether these efforts are related to student achievement

gains.




3. Benchmark Testing, the Dashboard System, and the Resulting Data

         The district in this study has made substantial investments in a system that regularly tests

their students in grades 3-8 on Benchmark formative assessments and feeds this test information

back to teachers and administrators via the Dashboard tool. For a subset of schools that I will call

Targeted Assistance (TA) schools, the district also provides ongoing professional development to

teachers on Dashboard use. The 15 TA schools were low-performing schools targeted to receive

extra resources beginning in the 2009 school year. All teachers in the district were provided with

the opportunity for voluntary, initial training when the Dashboard system was first brought

online. In addition to the end-of-year state level assessments, district students in grades 3-8 take

four Benchmark assessments through the course of the school year, and students in the 15 TA

schools take a pre-test in September and a post-test in January. The Benchmark tests are

designed to provide feedback regarding the extent to which district students are making

satisfactory progress toward mastering material that will be on the end-of-year state exams.

                                                                                                       8
Using Dashboard, a teacher can access his students’ data on a just-completed Benchmark exam

within 24 hours from the time the teacher turns in test results to the district assessment office.

Each teacher has access to the complete testing record, current and historical, of every student he

is teaching in given year. Teachers cannot view information on students they are not teaching

that year.4


        The Dashboard tool was developed in-house during 2004 and brought online in

September of 2005. While the district seems to realize the importance of providing training and

support to teachers around Dashboard use, district teachers tend to report uneven amounts of

training and support, with some teachers reporting sufficient levels of support and others

reporting little support in how to navigate and use Dashboard.5 The primary source of training

and professional development around Dashboard usage comes from the district’s professional

development support teams, seven teams of (usually) six individuals—a former principal, a math

coach, a language arts coach, a science coach, a social studies coach, and an individual who

specializes in special education. The role of the support teams is to “audit schools and assist with

academic improvement.”6 Prior to 2009 the support teams worked with all schools and teachers

in the district. Beginning in 2009 all five of the teams working at the elementary level were

assigned to the fifteen TA schools where one of their primary responsibilities was to help the

teachers in these schools utilize Dashboard in ways that would inform and improve their

classroom instruction.




4
  Dashboard also provides teachers with student level information in areas such as number of absences, number of
detentions, etc. I do not analyze teacher usage of this information in this paper.
5
  This information is based on four different focus group discussions with teachers from four elementary schools
conducted in December of 2008 by the author and Amy Wooten.
6
  Taken from the district’s 2006-2001 Strategic Plan.

                                                                                                                   9
       The district has worked to put in place a connected system of regular student testing, the

ability to turn that test data back to teachers in a timely manner via a tool that provides relatively

easy access and usability, and district support and encouragement around teacher data use for

instructional improvement. Information used in this study to address questions of how much, in

what ways, and to what effect teachers use this system derive from the web logs that are

generated every time a teacher logs into the Dashboard system. These web logs capture, among

other things, the employee id number of the teacher who has logged in, the day and time of the

login, the pages that are viewed during each Dashboard session, the sequencing of the teacher’s

journey through the pages, and information that allows for the construction of the amount of time

the teacher spent on each page during the session. Since certain student-related pages also have

an associated student id number, I am also able to capture when teachers view the Dashboard

data of specific students.


       After stripping the data of all personally identifiable information, district administrators

supplied me with the universe of raw web log files that were generated from every teacher login

that occurred between August 3, 2008 and May 31, 2010. In converting these web logs into

analytic data files a key task was coding the Dashboard pages into common groups. Individual

pages were grouped into the following page-type categories:


              Class level pages that have information on a given class of a given teacher

              Students-in-class level pages that have information on multiple students in a

               teacher’s class

              Individual-student level pages that have information on an individual student in a

               teacher’s class

              Item pages that have information on particular test items

                                                                                                    10
               Resource pages that have resource information for teachers such as model lesson

                plans.


       Figure 1 gives an example of a “class” level page for class taught by a hypothetical 5th

grade teacher. This page tells the teacher that on the English language arts Benchmark test given

on 11/30/2009, her students answered, on average, 44 percent of the questions correctly

compared to 39 percent for all the students in her school and 45 percent in the district. Similar

statistics for the Benchmark math test are displayed below the language arts results.


                                         <Figures 1-5 about here>


       Figure 2 gives an example of a “students-in-class” level page from this same class on the

math Benchmark from 11/30/2009. Here the score of each student in the teacher’s class is

displayed in ascending order down the column. A click on a student, for example Suzie

(fictitious name) who got 55 percent correct, would take the teacher to a page with information

on Suzie.


       Figure 3 gives an example of an “individual student” level page, in this case the page

with information on Suzie’s responses to all of the questions on the math Benchmark on which

she scored 55 percent correct. A click on “1” takes the teacher to a page that displays the first

question in the Benchmark which, in this case, Suzie answered incorrectly.


       Figure 4 gives an example of an “item” level page, in this case the first test item in the

aforementioned math Benchmark exam. The item level pages give teachers the exact test

question along with the grade level “indicator” and the state “standard” being tested by that

question.



                                                                                                    11
           Figure 5 gives an example of a “resource” page in Dashboard. In this case the resource

page is a list of the grade level indicators for 5th grade math in Ohio. The bottom part of Figure 5

shows a second resource page which is the page the teacher would be taken to if she were to

click on one of the indicators in the graphic above. This second resource page then has links to a

model lesson plan to teach that indicator, along with other links to related resources for the

teachers.


           The pages in Figures 1-5 are meant to be examples of the page type groupings that were

created for this analysis. Under each of the groupings (class, students-in-class, individual student,

item, and resource) there are many different pages that can be accessed on Dashboard.


           In addition to the coding of pages as to page type, other variables that were necessary or

convenient for later analysis were created from the raw web logs in the process of converting the

web logs into an analytic data file. Following the processing of the web log files, information

from district administrative personnel files, course files, and student test files were merged in.7


           The resulting data files have complete Dashboard usage information on 429 core subject

grade 3-8 teachers in 2009 and 359 teachers in 2010. The 2009 data set is a teacher by

Dashboard-page panel with 214,779 lines of data that were generated from 14,228 separate

logins between August 2008 and May 2009. Similarly the 2009-2010 web logs produce a data set

that has 230,323 lines of data generated from 15,655 logins between August 2009 and May 2010.

Throughout the study only these core subject grade 3-8 teachers are used. It is in these grades

and subjects where students are tested regularly on the Benchmark exams and thus where




7
    I thank Eric Taylor for his assistance in the student-teacher matching process.

                                                                                                      12
teachers are expected to use Dashboard on a relatively regular basis to access the performance

data of their students.




4. Results and Discussion

4.1 Teacher Use of Dashboard


        A first look at teacher Dashboard usage as captured in the web logs indicates that the

average teacher in the targeted group logged into the Dashboard system 33 times during the 2009

school year and spent a total of about 7 hours on Dashboard over the course of the school year.8

The average teacher apportioned her 7 hours during the year on Dashboard in the following

ways:


                3.2 percent on class level pages

                26.8 percent on students-in-class pages

                9 percent on individual student pages

                6.6 percent on item pages

                31.6 percent on resource pages

                5.2 percent entering student test data information,9 and

                17.4 percent of the time on login, password, or navigational pages containing

                 decision nodes (links) for users, but no information beyond the potential

                 destination pages.



8
  The median number of logins was 28 and the median time spent logged into the Dashboard system was about 3 ½
hours.
9
  Some of the grade 3-8 teachers also teach in grades K-2 and teachers of these grades enter student test scores
directly into Dashboard.

                                                                                                              13
        Table 1 gives a more detailed view of how teachers spent time on Dashboard on a per

week basis. Panel A give Dashboard login information, with the first row indicating that on

average the 429 district teachers logged into Dashboard slightly less than one time per week

during the 2009 school year. The second column in the first row indicates that conditional on

ever logging in during a week, the average number of logins is about two times per week. The

mean time logged in per week across all teachers is about 10 minutes per week, and conditional

on having logged in at least once during a week the mean time logged in per week is almost 30

minutes.10


                                                <Table 1 about here>


        Panel B provides statistics on the extent to which teachers are viewing student test data

during the time they are logged into the Dashboard site. Of particular interest is teacher usage of

students-in-class and individual student pages, since these are the pages that provide teachers

with student test data and test item information. The average teacher spends about 2.3 minutes

per week on students-in-class pages and slightly over half a minute per week on individual

student pages. Among teachers who spend any time on these pages during the week, the mean

times are 7.6 minutes on students-in-class pages and 6.33 minutes on individual student pages.

The average teacher accesses (hits) a students-in-class page about 2.5 times per week and an

individual student page only about once every two weeks (0.58 times per week). Panel C

provides similar statistics for item and resource pages.




10
 Note that the 30 minutes online could be accumulated in one or more than one Dashboard sessions during the
week.

                                                                                                              14
        As reported earlier, Wayman, Cho, and Shaw (2009) found that 93 percent of the teachers

in their study district accessed the Reports section of the student data tool, the section containing

information on student performance. A comparable measure on Dashboard is the percent of

teachers who accessed a class, students-in-class, or individual student page at least once during

the year. That figure is 98 percent.


        Another comparison that can be made is to the previously cited figures from NETTS of 3

percent of the surveyed teachers who reported using individual student data at least once weekly

and 15 percent who reported doing so at least once a month (U.S. Department of Education

2009). I too find that about 15 percent of the teachers are observed accessing individual student

pages on Dashboard at least once a month. There are, however, no teachers in the Dashboard

data who are observed accessing individual student pages at least once a week throughout the

year.


        There are two ways that teachers can use Dashboard to access student test data. They can

view the information online, the focus of the analysis thus far, or they can use Dashboard to print

out student test data information. Panels D and E of Table 1 provide information on this latter

method of interacting with Dashboard. On average teachers go to pages that print students-in-

class information only about once every three weeks (0.35 times per week), and they go to pages

that print individual student information only once every 6 weeks (0.16 times per week). These

print statistics suggest that teachers use Dashboard more as an interactive tool than as a tool for

printing off student test data.


        While the usage statistics in Table 1 are suggestive regarding the extent to which and the

ways in which teachers use Dashboard, we can get a better sense of teacher usage by looking at


                                                                                                   15
patterns of teacher usage by week throughout the year. Figure 6 provides information on the

pattern of Dashboard logins by district teachers during the 2009 school year. In Figure 6 and

figures that follow, key test dates are marked with vertical lines: blue for the Fall pretest given to

the 15 TA schools, green for each of the four Benchmark tests given during the year, maroon for

the January posttest given the TA schools, and red for the end-of-year state tests. Following each

test a two week period is shaded in with the corresponding color. This two week period

represents the period during which test results from that test will be appearing on Dashboard,

with the results for most classes available within two weeks of the test administration.11


         Figure 6 shows variation through the year in the percentage of teachers who login to

Dashboard during the week. In particular, Figure 6 suggests higher percentages of teachers

logging into Dashboard in the weeks following a Benchmark assessment than at other times

during the year, ranging from about 45 percent on the Fall pretest and the 1st Benchmark to

slightly over 70 percent of the logging in immediately after the last Benchmark in March.


         Figure 7 gives weekly information on the median time spent logged in per week, among

teachers who ever logged in during that week. Except for just before and just after the final

Benchmark in March, all of the median login times in Figure 7 are around or less than 10

minutes per week.12




11
   Following a Benchmark test the teacher turns the test sheets into district central office where they are scanned for
scoring and posted to Dashboard within 24 hours. Teachers are responsible for scoring the relatively few open-
response test items of their students and this can sometimes cause a delay in getting the tests in to central office.
Also, a teacher may delay turning in test sheets to allow a student who was absent a chance to take the Benchmark
test upon return to school. Results from the end-of-year state test require longer since they have to be sent to the
state for scoring before being returned to the district. They will not be available within two weeks of test
administration.
12
   Mean login times range from close to zero during the middle of October to about 40 minutes in the week
following the last Benchmark.

                                                                                                                     16
       Figures 6 and 7 provide information on how much teachers use Dashboard, and a glance

at these figures suggests that on any given week somewhere around 10 to 40 percent of the

district teachers we are studying logged into the system that week and that the “average” teacher

who logged in spent somewhere around 6 to 8 minutes online with Dashboard during the week.

One way to think about whether this represents substantial usage of student data is to consider

two elementary school teachers who each have self contained classrooms of, say, 21 students.

Assume that one-third of each teacher’s students are struggling and that Benchmark tests have

just been administered. With 50 percent or fewer teachers logging in each week according to

Figure 6, we can assume that only one of the two teachers would go to Dashboard to get

information that might help her with her seven struggling students, and Figure 7 suggests that the

teacher who did turn to Dashboard for information spent only about one minute per struggling

student logged into the system (7 struggling students and a median login time of around 7

minutes per week for those who ever logged in that week). While only a rough barometer, this

back-of-the-envelope estimation suggests that the average teacher may not be making extensive

use of Dashboard as a tool for helping their struggling students.


       A second question pursued in this project is how do teachers use Dashboard, and in

particular, to what extent do teachers view student test data information? Since the bulk of

student test data is presented on either students-in-class pages or individual student pages, the

focus on teacher usage will now turn to those pages. Figure 8 displays information on the mean

amount of time teachers spent each week viewing Dashboard students-in-class pages. Averaged

across all teachers, including those who never logged on during the week, Figure 8 indicates that

on average teachers spent from one to four minutes per week viewing students-in-class pages,

with the exceptions of 6 and 9 minutes per week spikes after the 2nd and 4th Benchmark tests.


                                                                                                    17
       Under a model where more intensive Dashboard usage is represented by teachers who

“burrow” deeper down in Dashboard to the level where test data on individual students is

presented, the information in Figure 9 is somewhat discouraging. In Figure 9 the average teacher

in the sample spends less than 2 minutes per week viewing individual student pages, even during

peak weeks. As we learned earlier in Table 1, the mean time per week spent on individual

student pages is only 0.6 minutes per week for all teachers, and 6.33 minutes per week for all

teachers who viewed an individual student page in a given week. There is a long right-hand tail

to the conditional distribution, however, as the median time on individual student pages among

those with non-zero values in a given week is only 1.4 minutes, and the 25th percentile is half a

minute. These statistics and Figure 9 suggest that the bulk of teachers rarely spend substantive

amounts of time on Dashboard viewing performance data at the individual student level.


       Figure 10 provides information on how teachers apportion their time on Dashboard

between viewing student level information and using the web-based tool for other purposes such

as looking at lesson plans or reviewing state standards. According to Figure 10, teachers who

login to Dashboard spend from 20 to 50 percent of their time looking at student performance

data, figures that seem reasonably high given all of the other types of information a teacher can

access on Dashboard. Thus, concerns about how much teachers are using student performance

data on Dashboard to inform and improve their practice should focus more in whether they login

at all and how much time they spend while logged in, rather than in what they are doing while

they are on Dashboard.


       As mentioned earlier, one issue that might cloud our understanding of how teachers use

student performance data to inform their practice and how they utilize the Dashboard tool in this

endeavor is the extent to which teachers use Dashboard as a tool for accessing and printing out

                                                                                                    18
performance data. Simply put, if teachers login, print, and log out of Dashboard then focusing on

time-on-page will substantially underestimate Dashboard usage of student performance data.

However, supporting what we learned in Table 1, Figure 11 suggests that teachers’ primary use

of Dashboard is as an interactive tool rather than a printing tool. In every week of the school year

the number of “viewing” hits by teachers on students-in-class and individual student pages easily

dominates the number of “printing” hits on these pages.


       Figure 12 summarizes the information thus far about the extent to which teachers use

Dashboard to view and analyze student performance data. Figure 12 displays the distribution of

total time during the year spent by teachers on students-in-class and individual student pages

combined. According to this figure 17 percent of the core subject grade 3-8 teachers 2009 (73 of

429 teachers) spent a total of 20 minutes or less during the entire school year viewing these types

of pages and 43 percent of the teachers (187 out of 429) spent an hour or less during the year on

these student level pages. There is a long right hand tail to this distribution, however, and a third

of the teachers spent more than two hours during the year on these pages and 20 percent spent

more than three hours




4.2 Correlates of Dashboard Use


       The ability to link other district data sources to the Dashboard web log information

allows for the examination of the correlates of Dashboard use. District personnel files are used to

obtain information on teacher characteristics such as gender, race/ethnicity, years of experience




                                                                                                   19
teaching, education level, and salary.13 District student and course files are used to match

students and their test scores to teachers and construct value-added measures for teachers and

measures of average class achievement at the beginning of the year.


         Table 2 shows the results from OLS regressions of the natural log of the sum of total time

spent during the year on students-in-class and individual student pages on a set of class and

teacher characteristics including:


                 the baseline mean achievement level of the teacher’s students at the beginning of

                  the year along with an indicator for whether the baseline level was imputed,14

                 teacher value-added,

                 an indicator for whether a teacher taught in grades 3 through 6 relative to grades 6

                  through 8,15

                 indicators for gender and race/ethnicity, years of teaching experience, and

                  indicators for education level.


         The estimates in Table 2 are based on the 325 teachers who have non-missing values on

all of the variables in the model.16 In the first column of the table the only statistically significant

predictors of total time spent during the year viewing student level pages are the prior




13
   Even though annual salary is in the data, because it is essentially determined by years of experience and education
level, we use these other variables in regression models instead of salary.
14
   This baseline measure was computed by averaging, for each student, the prior year’s state exam math score and
state exam reading score (both mean zero, standard deviation one variables), and then averaging these scores across
the students in a teacher’s class.
15
   The dummy variable indicator equaled one for the 311 teachers who taught in any configuration of grades 3
through 6, and zero for the 9 teachers who taught in a grade 6 through 8 configuration and zero for the 89 teachers
who taught in grades 7 or 8 or a grade 7 and 8 combination.
16
   409 teachers have non-missing values on all variables except for value-added. When the models in Table 2 are fit
over these teachers and value-added is excluded as a predictor, the results, available from the author, are essentially
unchanged.

                                                                                                                    20
achievement level of the class and the grade level indicator. Teachers in higher performing

classes tend to view student level Dashboard pages less frequently, and teachers in grades 3-6,

traditional elementary grades, view student data on Dashboard at lower rates than teachers in

middle school grades.


                                             <Table 2 about here>


           Estimates from a school fixed effects model are in column 2. Within schools it is no

longer the case that the data of students in higher performing classrooms is viewed at lower rates,

but the lower viewing rates of elementary teachers remains. With a mean total time spent on the

two types of pages of about two hours during the course of the year (mean = 122.0 minutes with

standard deviation = 137.2), teachers who taught in grades 3 through 6 spent about 36 percent

less time viewing these student level pages than did observationally similar teachers who taught

in middle schools or taught middle school grades in combined-grade schools though this estimate

is only marginally significant in the school fixed effects model.17


           Following up on the time dependence of Dashboard usage depicted in Figure 8, Table 3

presents estimates from models fit to a teacher-week panel that included indicator variables for

whether a given week in the school year was a week before a Benchmark or state test, within two

weeks after a Benchmark or state test, or the week when the Benchmark or state test was

administered. The excluded time category is any “off test” week that is not in one of the before,

during, or within two weeks after test intervals.


                                             <Table 3 about here>




17
     Calculated as exp(-0.304) - 1.

                                                                                                   21
         Estimates in column 1 indicate that teachers are not spending more time per week

viewing student level pages in Dashboard in the week just prior to a Benchmark test than during

the “off test” weeks. In fact they spend about 13 percent less time in any week before a

Benchmark than they do during any “off test” week. Teachers do, however, spend more time

viewing student level pages in the two weeks after a Benchmark test. They spend about 50

percent more time per week in the two weeks just after a Benchmark test than during the “off

test” weeks.18 The estimates in the first column also indicate that teachers spend less time during

and after the state exams than during “off test” weeks.


         The second and third columns of Table 3 explore the extent to which the timing of

teachers’ use of Dashboard to view student level pages can be explained by average class ability

and teacher characteristics (column 2) or by school fixed effects (column 3). The estimates from

these specifications are very similar to those of the basic model in column 1.19




4.3 Change Over Time in Usage


         The evidence thus far is that teachers spent relatively little time in 2009 viewing student-

level performance data on Dashboard. To examine whether these patterns of usage changed

between 2009 and 2010 the weblog data from 2010 are utilized. Summary statistics for the 359

core subject grade 3-8 teachers in 2010 are similar to those in Table 1 that are based on the 2009

teachers. The only difference is evidence that teachers in 2010 spent some more time viewing




18
  Calculated as exp(0.436) - 1
19
  I note that the low viewing rates during and following the state exams is not surprising since this is the end of the
school year and teachers will not have yet gotten back the state exam results.

                                                                                                                     22
students-in-class pages than did teachers in 2009. On average, 2010 teachers spent 117.3 minutes

looking at students-in-class pages versus the 97.4 minutes teachers spent on these pages in 2009

(p-value of the difference is 0.022). There is no statistical difference in the mean total time for

the year viewing individual-student pages (27.1 minutes for 2010 teachers versus 25.2 minutes

for 2009 teachers, p=0.611). Similarly, the 2010 teachers printed out more students-in-class

pages than did the earlier teachers (23.1 pages for the year versus 14.7, p=0.000), but there was

no difference in the printing of individual-student pages (8.5 versus 6.7, p=0.173) .


       In addition to these aggregate statistics, there are 243 teachers who are observed in both

years in the data. Using these teachers we can examine within-teacher changes over time in

Dashboard usage of student-level data. The results in Table 4 are from teacher fixed effects

models that control for any changes across the years in class size and average class ability as

measured by the average class scores on the previous year’s state exams. The dependent

variables in models 1 and 2 measure log teacher time spent on students-in-class (column 1) and

individual-student pages (column 2), while the dependent variables in models 3 and 4 measure

log number of times students-in-class or individual-student data was printed. Results in the first

two columns reinforce the aggregate statistics and suggest that the observed changes in mean

time viewing students-in-class pages is primarily a within-teacher change rather than a

compositional change in teachers across the years. On the other hand, the marginally significant

estimates on the 2010 indicator in column 4 suggest that the aggregate statistics may mask

within-teacher increases in the number of times individual-student level pages were printed out

between 2009 and 2010. Also, column 3 indicates that teachers in TA schools tended to print out

students-in-class information at higher rates in 2010 than in the previous year and at higher rates

than teachers in non-EI schools.


                                                                                                      23
                                                 <Table 4 >


       The fixed effects estimates in Table 4 show some areas where teachers increased

Dashboard usage between 2009 and 2010. Overall, however, there is no robust evidence of

systematic increases in Dashboard usage across the years.




4.4 Dashboard Usage and Student Test Score Gains


       Ultimately we are interested in the extent to which teacher usage of student performance

data is related to student achievement gains. I use the 2010 data that has both state exam and

Benchmark score data to explore this question, and since the interaction that matters occurs when

a teacher views the data of a particular student, I first use these data to examine the predictors of

that happening.


       In the 2010 data, 309 of the 359 teachers viewed the Dashboard data of one or more of

their students at least once during the year. Of these 309 teachers, 271 of these teachers can be

matched to the value-added distribution in the district, have non-missing information in the

personnel files, and have at least one student who can be matched to the test score file. These

teachers are matched to 4,106 unique students in fitting the following models that explore the

correlates of teacher usage of individual student data:


                                Yij = β*pre-testij +f(Xij) + g(Wj) + εij   (1)


                                Yij = β’*pre-testij + f(Xij) + αj + ηij    (2)




                                                                                                    24
where i indexes students and j indexes teachers and the dependent variable is the log of the total

time teacher j viewed the data of student i on Dashboard during 2010. As before, the student pre-

test score is the average of the previous year’s state exam scores in math and English language

arts (ELA). For the few students who did not have both scores to average, the available math or

the ELA score was used.20 The vector X contains student characteristics that might be predictive

of Y and in this case include indicators for eligibility in the district’s gifted and talented

programs, whether a special education student, and whether designated as an English language

learner.21 The vector W is composed of teacher characteristics including value-added score,

gender, race, years of experience in the district, grade taught in 2010, education level, and

whether or not a national board certified teacher. Equation 2 replaces the teacher characteristics

with a teacher fixed effect.


         Results from estimating equations 1 and 2 are in Table 5. The only variables that are

consistently predictive of teacher use of a student’s Dashboard data are the prior year’s state test

score and special education status, with teachers viewing the data of students who started the

year at lower achievement levels at higher rates, and also viewing the data of special education

students at higher rates. Estimates from the teacher fixed effects model in column 3 indicate that

given two students in the same teacher’s class who were one standard deviation apart in terms of

their pre-test scores, the teacher viewed the Dashboard data of the lower achieving student about

5 percent more than the data of the higher achieving student. That same teacher also viewed the




20
  There were 22 students with only prior test scores in math and 30 students with only prior ELA scores.
21
  The data also contain information on gender and race/ethnicity, but these are not included in the model since
conditional on pre-test score they should not be theoretically linked to a teacher’s use of student data. Also, these
variables are never statistically important when they are included.

                                                                                                                        25
data of her special education students at a 21 percent higher rate.22 In this model it is also the

case that the data of English language learners was also viewed at a higher rate. While

statistically significant, these estimates must be put in context. The mean total time spent

viewing a random individual student in 2010 was about two minutes and twenty seconds. Thus,

even the 16 percent higher rate of viewing the data of special education students translates into

only about an extra 20 seconds per year on average that teachers spent on special education

versus non-special education students. Nevertheless, these results—more time spent on lower

achieving and special education students—are consistent with where we might predict a teacher

would spend their Dashboard time.23


                                                         <Table 5>


         The study now turns from exploring what predicts teacher usage of individual student

data to whether that usage is related to increased student achievement. Absent exogenous

variation in the amount of time teachers spend viewing student Dashboard data, developing a

satisfactory model relating teacher Dashboard usage to student achievement growth is not a

straightforward exercise. To see this consider the following model:


         Aijt = β*Aij,t-1 + δ*(Tijt) + f(Xijt) + g(Wjt) + εijt             (3)


where i, j, and t index students, teachers, and time respectively. A is a measure of achievement

and, as before, X and W (in equation 3) are vectors of student and teacher characteristics, though

the X vector in these achievement equations also contains information on the gender and




22
  Calculated as exp(0.158) – 1.
23
  Similar results available from the author are obtained in models where the dependent variable is an indicator for
ever printing out Dashboard data on student i during the year.

                                                                                                                  26
race/ethnicity of the student. T is a measure of the amount of time that teacher j spent viewing

the Dashboard data of student i between period t-1 when a prior measure of achievement was

gathered and time t. As before, we could also consider a model such as equation 4 where the

vector of teacher characteristics is replaced with a teacher fixed effect, α.


       Aijt = β*Aij,t-1 + δ’*(Tijt) + f(Xijt) + αj + εijt        (4)


The parameters of interest in equations 3 and 4 are δ and δ’, measures of the relationship between

time spent viewing a student’s data and student achievement growth.


       A priori, one would expect δ to be non-negative since viewing a student’s Dashboard data

should not lead to a decrease in achievement. However, a concern in estimating δ in this model is

that teachers likely use information unavailable to the researcher in making decisions about the

use of student performance data on Dashboard. In particular, consider two observationally

similar students who have equal levels of prior achievement. If one of these students is having

more academic problems during the year than the other and these unobserved (in the data)

problems are positively correlated with a teachers time viewing that student’s data, then

estimates of δ would be downwardly biased. Nevertheless, lacking suitable instruments for T, I

estimate equations 3 and 4 using the available data.


       Equations 3 and 4 are first estimated using the end-of-year state exams as measures of

prior and final student achievement. In these models t-1 is the end of the 2009 school year and

time t is the end of the 2010 school year. Thus, T represents the total time that teacher j spent on

student i’s Dashboard data during the 2010 school year. In these models the estimate of δ based

on equation 4 is 0.0008 (s.e. = 0.001) and the estimate from the fixed effects model of equation 5




                                                                                                   27
is even closer to zero.24 From these estimates we would conclude that either teacher usage of

Dashboard is unrelated to student achievement growth or that the estimates are biased toward

zero.


         A more proximate measure of student achievement available in the data comes from

student test scores on the quarterly Benchmark assessments. In models using Benchmark test

scores the immediately prior Benchmark test is used as the measure of prior achievement for

each subsequent Benchmark except in the case of the first Benchmark of the year where prior

achievement is measured by the previous year’s state exam score. In the Benchmark test model

the measure of T is teacher time on the Dashboard data of student i in the interval between the

current Benchmark exam and the prior measure of achievement.25 When fitting the Benchmark

test models only students in grades 3, 4, and 5 who are observed as having only one teacher are

used. With this subsample of students and teachers one can be more certain that all of the activity

that is occurring between a teacher and a given student’s Dashboard data is being captured. In

this sample there are 1,535 students across 149 grade 3-5 teachers with at least one math

Benchmark score and 1,530 students across 150 teachers with at least one ELA Benchmark

score.


         Before turning to estimates of equations 3 and 4, Figures 13 (for math) and 14 (for ELA)

display scatter plots of the Benchmark scores versus teacher time on Dashboard. Each point on

either graph represents the Benchmark score of an individual student graphed against the time



24
   Estimates on the other variables in the models are generally as expected (e.g., eligibility for gifted and talented
programs is positively related to achievement growth, special education designation is negatively related, and prior
achievement is the strongest predictor of current achievement). None of the observed teacher characteristics in W are
statistically significant. Full regression results available from the author.
25
   In the case of the first Benchmark T is measured by teacher Dashboard use between the beginning of school and
the first Benchmark.

                                                                                                                   28
spent by the teacher viewing the data of that student in the interval between the prior and current

Benchmark test. Any given student will be represented by from one to four data points

depending upon the number of Benchmark tests for that student in the data.26


         Figures 13and 14 certainly do not suggest a relationship between the Dashboard usage

and Benchmark scores. Moreover, these figures also highlight yet again the limited usage of

Dashboard by teachers for viewing student data. The great bulk of the data is massed at very low

levels of time spent on Dashboard. While the mean of time spent on a student in any given

interval between Benchmark tests is just less than half a minute (0.45 with s.d. = 2.07), 74

percent of the 6,279 student-Benchmark-intervals have a value of zero for the time spent by the

teacher viewing student-level data.


         Estimates of equations 3 and 4 using Benchmark test scores as the achievement level are

presented in Table 6, and they bear out the graphical information in Figures 13 and 14. As with

the state test score estimates, in none of the models is the time spent on Dashboard in a

Benchmark-interval related to the subsequent Benchmark test score.27 The poorly estimated

negative point estimates of δ suggest the possible presence of downward bias due to unobserved

heterogeneity. In any case, there is no evidence in any of the models used to estimate equations 3

and 4 that Dashboard usage of student performance data leads to student achievement gains.


                                                         <Table 6>




26
   For example, among the observations that will be used in the estimates, there are 1,540 students with math scores
in the first Benchmark interval, 1,539 in the second, 1,542 in the third, and 1,367 in the fourth. The numbers are
very similar for ELA scores.
27
   In addition to fitting versions of equations 3 and 4, I also use the fact that there are up to four Benchmark exams
for each student to fit a student fixed effects model. The results in these models, available from the author upon
request, are very similar to the estimates in Table 6.

                                                                                                                    29
           There is a notable pattern of Dashboard usage in the district that may help explain some

of the null results in Table 6. If teachers were using Dashboard throughout the year as a tool for

promoting student achievement, we would expect to observe teachers viewing the data of their

students at various times during the year. Instead, it is the case that one of every three teachers

used in the Benchmark test analysis spent all of their time viewing individual student data in one

of the four possible Benchmark test intervals, and they spent no time during any of the other

intervals.28 Also, across teachers who concentrate all of their time in one interval, no interval

tends to have more teachers than other intervals. These patterns suggest that something other

than data use for instructional purposes is driving teacher usage of Dashboard, at least when it

comes to viewing data on individual students. For example, it may be that teachers only look at

the data of their individual students when prompted by campus principals or district professional

development staff, or as a function of the district’s teacher evaluation system. Unfortunately,

these data have no information that would allow us to better understand this pattern.




 5. Summary and Conclusions

           This paper has drawn upon unique data to present some of the first detailed objective

estimates of how much and in what ways teachers actually use web-based student performance,

and the extent to which such use might be related to student achievement. Though primarily

descriptive in nature, the information in this paper should help to fill a void in our understanding

of how student performance data might inform and improve classroom practice. It is obvious that

teachers must first access student performance data if these data are to be used in ways that can



28
     Also, 51 percent of the teachers spent 75 percent or more of their time in just one of the intervals.

                                                                                                             30
inform practice and improve student achievement. To date there has been limited information on

this critical step, a knowledge gap this paper addresses.


        While there is no other district against which to compare the teacher usage statistics from

the district used in this study, it is fair to say that the results from this district are less than

encouraging. Three years after the launching of the Dashboard system, and well into a substantial

district efforts encouraging teacher use of Dashboard, measures of teacher usage of student data

are relatively low. On average, teachers targeted by the district as the primary group to use

student performance data—core subject teachers in grades 3-8—view pages with student level

information about 3 minutes per week. Perhaps more telling, the average teacher in this group

views information at the individual student level an average of only 36 seconds per week during

the course of the school year. Furthermore, a close examination of the data indicates that one in

three teachers spent all of their limited time viewing data on their individual students at only one

point during the year, never visiting their student data on Dashboard at any other time. The levels

and patterns of observed usage give little indication of systematic use of student performance

data on Dashboard by district teachers.


        Focus group research conducted in the district during the two years of this study suggests

some reasons that teacher usage of Dashboard may be sub-optimal. Teachers in these meetings

were quite candid in expressing their opinions about and experiences with Dashboard. One factor

that arose with relative frequency was an expressed concern that the Benchmark tests lacked

some validity because they often tested material that the teachers had yet to cover in class. A

second factor that was supported across several focus group discussions was a perceived lack of

instructional time to act on information that a teacher might gain from Dashboard data. In

particular, teachers expressed frustration with the lack of time to “reteach” topics and concepts to

                                                                                                       31
students that had been identified on Dashboard as in need of “reteaching” based on their

performance on a given indicator. A third concern was a lack of training in how to use

Dashboard effectively and efficiently. A fourth common barrier to Dashboard use cited by

teachers was a lack of time for Dashboard-related data analysis.


       Regarding this last point, in spite of the investment in the student testing and data

provision system of which Dashboard is central, it is not clear what model the district has in

mind when it comes to time use and teacher interaction with Dashboard. If teachers are now

expected to spend time analyzing student performance data relative to how they were spending

time in a pre-Dashboard era then either:


           1. the district expects time-saving efficiency gains from Dashboard such that time

               spent on Dashboard during the work day makes other essential tasks less time

               consuming,

           2. the district expects teachers to reallocate time from other tasks that are now

               deemed as non-essential to spending time on Dashboard,

           3. the district feels that there is slack time during a teacher’s work day that can be

               used for Dashboard data analysis, or

           4. the district expects teachers to analyze Dashboard data during out-of-school time.


       An articulation by district leaders regarding which of the above scenarios is the one they

envision could help define future priorities and provide greater teacher buy in. In the meantime,

if teachers anticipate that the district has an unarticulated scenario #4 in mind, then it is unlikely

that data analysis on Dashboard will ever be a significant factor in informing a teacher’s practice




                                                                                                    32
even if all of the supposed precursors of effective data use such as good building leadership,

district support and encouragement, a good data analysis tool, etc., were in place.


        This study should provide a cautionary note to districts that are investing in systems

designed to bring student performance data to teachers via regular testing and web-based data

presentation and analytic tools. The evidence from this study is that one should be careful in

assuming how much teachers may actually base their teaching on the evidence that comes from a

even a carefully designed system that tests students and then provides that data to teachers as

inputs to their instructional practice.




                                                                                                  33
                                          References


Brunner, C., C. Fasca, et al. (2005). "Linking Data and Learning: The Grow Network Study."
      Journal of Education for Students Placed At Risk 10(3): 241-267.

Chen, E., M. Heritage, et al. (2005). "Identifying and Monitoring Students' Learning Needs with
       Technology." Journal of Education for Students Placed At Risk 10(3): 309-332.

Crawford, V. M., M. S. Schlager, et al. (2008). Supporting the Art of Teaching in a Data-Rich,
      High-Performance Learning Environment. Data-Driven School Improvement: Linking
      Data and Learning. E. B. Mandinach and M. Honey. New York, Teachers College Press:
      109-129.

Datnow, A., V. Park, et al. (2007). Achieving with Data: How High-Performing School Systems
      Use Data to Improve Instruction for Elementary Students. Los Angeles, CA, Center on
      Educational Governance, University of Southern California.

Feldman, J. and R. Tung (2001). "Using Data Based Inquiry and Decision-Making to Improve
      Instruction." ERS Spectrum 19(3): 10-19.

Ingram, Debra, Karen R. S. Louis, and Roger Schroeder. (2004). "Accountability Policies and
      Teacher Decision Making: Barriers to the Use of Data to Improve Practice." Teachers
      College Record 106(6): 1258-1287.

Kerr, K. A., J. A. Marsh, et al. (2006). "Strategies to Promote Data Use for Instructional
       Improvement: Actioins, Outcomes, and Lessons from Three Urban Districts." American
       Journa of Education 112(August): 496-520.

Lachat, M. A. (2002). Data-Driven High School Reform: The Breaking Ranks Model.
       Providence, RI, Northeast and Islands Regional Educational Laboratory at Brown
       University.

Marsh, J. A., J. F. Pane, et al. (2006). Making Sense of Data-Driven Decision Making in
       Education: Evidence from Recent RAND Research. Washington, D.C., RAND
       Corporation.

Pardini, P. (2000). "Data, Well Done." Journal of Staff Development 21(1): 12-18.

Protheroe, N. (2001). "Improving Teaching and Learning with Data Based Decisions: Asking the
       Right Questions and Acting on the Answers." ERS Spectrum 19(3): 4-9.

Stein, M. (2003). Making Sense of the Data: Overview of the K-12 Data Management and
       Analysis Market. Boston, MA, Eduventures, Inc.




                                                                                              1
U.S. Department of Education, Office of Planning, Evaluation, and Policy Development (2008).
       Teachers' Use of Student Data Systems to Improve Instruction: 2005-2007. Washington,
       D.C.
U.S. Department of Education, Office of Planning, Evaluation, and Policy Development (2009).
       Implementing Data-Informed Decision Making in Schools--Teacher Access, Supports,
       and Use. Washington, D.C.
Wayman, Jeffrey C., Cho, and Shaw (2009)

Wayman, Jeffrey. C. (2005). "Guest Editor's Introduction." Journal of Education for Students
     Placed At Risk 10(3): 235-239.

Wayman, Jeffrey. C., Sam Stringfield, and Mary Yakimowski. (2004). Software Enabling School
     Improvement Through Analysis of Student Data. Balitomore, MD, Center for Research
     on the Education of Students Placed At Risk, The Johns Hopkins University.




                                                                                               2
Figure 1. Example of a “class” level page from Dashboard.




                                                            1
Figure 2. Example of a “students” level page from Dashboard.




   Suzie




                                                               2
Figure 3. Example of “individual student” page from Dashboard.




         Suzie




                                                                 3
Figure 4. Example of “item” page from Dashboard.




                                                   4
Figure 5. Example of “resource” page from Dashboard.




                                                       5
Figure 6. Percentage of district teachers who logged into Dashboard by week.




Figure 7. Median time logged in among teachers who ever logged into Dashboard during a week,
by week.




                                                                                           6
Figure 8. Mean time spent on “students” pages by week.




Figure 9. Mean time spent on “individual student” pages by week.




                                                                   7
Figure 10. Proportion of total login time spent on “students” and “individual students” pages.




Figure 11. Comparisons of mean viewing page hits versus mean printing page hits for “students”
and “individual student” pages by week.




                                                                                                 8
Figure 12. Distribution of total time spent on “students” and “individual student” pages during
the 2008-2009 school year.




                                                                                                  9
Figure 13. Benchmark math test score by teacher time on that student on Dashboard.



                                             Standardized Benchmark Test Score vs.
                                           Time Spent Before That Test on Dashboard
                                                                 By Student
                    4
  Math Standardized Score
   -2       0       -42




                                       0         10             20           30           40          50
                                           Minutes on An Individual Student Between Benchmark Tests




Figure 14. Benchmark ELA test score by teacher time on that student on Dashboard.



                                             Standardized Benchmark Test Score vs.
                                           Time Spent Before That Test on Dashboard
                                                                 By Student
                                   4
  Language Arts Standardized Score
       -2        0  -4     2




                                       0         10             20           30           40          50
                                           Minutes on An Individual Student Between Benchmark Tests



                                                                                                           10
Table 1. Descriptive statistics on various per week Dashboard usage measures for all teachers
and conditional on teachers who have non-zero values of the measure under consideration (all
time is in minutes).
                                            All Teachers         Conditional on
                                                                having a non-zero
                                                                      value
 Panel A: Dashboard logins
                                              0.79                    2.22
      Average # logins
                                             (1.60)                  (2.01)

                                              9.99                   28.29
      Mean time logged in
                                            (115.38)                (192.78)
Panel B: Performance
information pages
                                              0.25                    2.84
      Mean time on class pages
                                             (2.03)                  (6.20)

      Mean time on student                    2.31                    7.64
      pages                                  (8.37)                  (13.83)

      Mean time on individual                 0.60                    6.33
      student page                           (5.14)                  (15.58)

                                              0.28                    2.97
      Mean hits on class pages
                                             (1.31)                  (3.19)
                                              2.52                    8.22
      Mean hits on student pages
                                             (6.70)                  (9.98)
      Mean hits on individual                 0.58                    5.69
      student pages                          (4.32)                  (12.41)

 Panel C: Resource-type pages

                                              0.57                    12.90
      Mean time on item pages
                                             (6.05)                  (25.85)

      Mean time on resource                   3.17                   51.86
      pages                                 (96.96)                 (388.82)

                                              0.49                    10.48
      Mean hits on item pages
                                             (3.76)                  (14.08)

      Mean hits on resource                   0.33                    4.83

                                                                                                11
     pages                         (2.11)   (6.51)
Panel D: Printing performance
information pages
     Mean hits on print class       0.04     2.66
     pages                         (0.46)   (2.42)
     Mean hits on print             0.35     4.04
     students-in-class pages       (1.56)   (3.63)
     Mean hits on print             0.16     3.77
     individual student pages      (1.71)   (7.56)
Panel E: Printing resource-type
pages
     Mean hits on print item        0.13     10.83
     pages                         (3.41)   (29.36)
     Mean hits on print resource    0.00     4.00
     pages                         (0.14)   (4.82)




                                                      12
Table 2. OLS estimates from regressions with log total time during the year spent on “students”
and/or “individual student” level pages as the dependent.

                                            (1)                            (2)
Baseline average student                  -0.464~                         0.081
achievement level                         (0.255)                        (0.260)
Value-added estimate                       0.040                         -0.133
                                          (0.619)                        (0.633)
Elementary grade teacher                  -0.413*                        -0.304~
                                          (0.199)                        (0.181)
Female                                     0.072                          0.023
                                          (0.237)                        (0.222)
Black                                      0.116                         -0.028
                                          (0.197)                        (0.384)
Hispanic or Asian                         -0.951                         -0.981
                                          (0.903)                        (0.682)
Years of experience                        -0.006                         0.001
                                          (0.010)                        (0.010)
Bachelors degree plus                      -0.271                        -0.272
                                          (0.340)                        (0.384)
Masters degree                             -0.181                        -0.133
                                          (0.334)                        (0.363)
Masters degree plus                        -0.296                        -0.290
                                          (0.363)                        (0.398)


School fixed effect                         No                            Yes
N                                           325                           325
R-squared                                  0.079                         0.151
Robust standard errors (in parentheses) clustered at the school level.
* = p<0.05, ~ = p< 0.10




                                                                                              13
Table 3. OLS estimates from regressions with log total time during the week spent on “students”
and/or “individual student” level pages as the dependent variable (robust standard errors in
parentheses).


                                              (1)              (2)             (3)

1 week before a Benchmark exam             -0.131*          -0.150*          -0.155*
                                           (0.063)          (0.063)          (0.063)
The week during a Benchmark exam            0.014           -0.043           -0.042
                                           (0.072)          (0.071)          (0.071)
2 weeks after a Benchmark exam             0.436***         0.412***         0.417***
                                           (0.058)          (0.057)          (0.058)
1 week before the state exams               0.022            0.016            0.014
                                           (0.103)          (0.103)          (0.102)
2 weeks during the state exams             -0.281~          -0.262~          -0.263*
                                           (0.145)          (0.145)          (0.145)
2 weeks after the state exams              -0.489***        -0.478***        -0.464**
                                            (0.127)          (0.127)          (0.124)


Teacher and ClassVariablesa                  No               Yes              Yes
School fixed effect                          No               No               Yes
Number of teachers                           325              325              325
Number of observations                      4,385            4,385            4,385
R squared                                   0.027            0.062            0.080
Standard errors (in parentheses) clustered at the teacher level.
a. Including beginning of year average class achievement level, teacher value-added estimate,
years of teaching experience, and indicators for whether an elementary teacher, African-
American, Hispanic or Asian, whether average student ability was imputed, and education level
of teacher.
*** = p<0.001, ** = p<0.01, * = p<0.05, ~ = p<0.10




                                                                                             14
Table 4. Within teacher estimates of the change in Dashboard usage between 2008-2009 and
2009-2010.


                       (1)               (2)                   (3)                   (4)
                 Log minutes on    Log minutes on       Log number of         Log number of
                students-in-class individual-student         times                 times
                   level data         level data       students-in-class    individual-student
                                                       level data printed    level data printed

2010                 0.234*            0.0124                0.197               0.322~
indicator           (0.0966)           (0.201)              (0.102)              (0.194)

Targeted
                     0.0535            -0.378                0.171               -0.565
Assistance
                     (0.319)           (0.616)              (0.321)              (0.671)
(TA) School

TA School in         -0.239            -0.338               0.383*               0.0726
2009-2010            (0.160)           (0.324)              (0.153)              (0.284)

Number of
                    -0.00425*         -0.00655            -0.000810             -0.00165
students in
                    (0.00182)         (0.00377)           (0.00216)             (0.00366)
class

Average
                    -0.00643           -0.132               -0.334               -0.447
class
                     (0.172)           (0.344)              (0.175)              (0.298)
achievement

Teacher
fixed effects          Yes               Yes                 Yes                   Yes

Number
                       243               243                  243                  243
teachers

 Adjusted
 R-sq                 0.615             0.286                0.709                0.381
Standard errors in parentheses.
~ = p<0.10, * = p<0.05




                                                                                             15
Table 5. Student and teacher predictors of how much teachers view the data of individual
students in Dashboard.

                                        (1)               (2)                (3)
                                 Log minutes on     Log minutes on     Log minutes on
                                individual student individual student individual student
                                       data               data               data

                                    -0.0914*             -0.0659~          -0.0479~
 Prior year test score
                                    (0.0460)             (0.0380)          (0.0245)

 Eligible for gifted &                0.174                0.180            0.0599
 talented                            (0.124)              (0.113)          (0.0824)

                                      0.264                0.204            0.186~
 English language learner
                                     (0.172)              (0.155)           (0.109)

                                     0.171*              0.192**           0.158***
 Special education
                                    (0.0708)             (0.0681)          (0.0471)

 Teacher characteristics               No                   Yes               --

 Teacher fixed-effects                 No                   No               Yes

 Number of student
                                      4,106                4,106            4,106
 observations

 Number of teachers                    271                  271              271

 Adjusted R-sq                        0.007                 0.017           0.302
Standard errors (in parentheses) clustered at the teacher level.
~ = p<0.10, * = p<0.05, ** = p<0.01




                                                                                           16
Table 6. Estimates of the relationship between Benchmark test scores in math and ELA and the
amount of time spent by the teacher viewing the student’s data in the just prior interval between
the last and the current Benchmark test.
                                  (1)            (2)              (4)                (5)
                                 Math           Math             ELA                ELA
                             Benchmark       Benchmark       Benchmark          Benchmark

 Minutes of teacher
 Dashboard time on            -0.00640         -0.00862            -0.00339     -0.00403
 student in just prior       (0.005667)        (0.00472)           (0.00351)    (0.00406)
 Benchmark test interval

                              0.627***         0.583***            0.618***     0.588***
 Prior test score
                              (0.0151)         (0.0141)            (0.0172)     (0.0140)

 Eligible for gifted &        0.225***         0.276***            0.240***     0.294***
 talented                      (0.053)         (0.0378)            (0.0343)     (0.0343)

 English language               0.0405           0.0671              0.0450       0.0720
 learner                       (0.0552)         (0.0596)            (0.0749)     (0.0617)

                              -0.144***        -0.132***           -0.156***    -0.1868***
 Special education
                               (0.0346)         (0.0308)            (0.0271)      (0.0304)

                              0.001080                              0.000734
 Students in class                                 --                               --
                             (0.000652)                            (0.000738)

 Average class                 0.067*                              0.162***
                                                   --                               --
 achievement                   (0.034)                             (0.0263)

 Student gender and
                                 Yes              Yes                 Yes          Yes
 race/ethnicity

 Teacher characteristics         Yes               --                 Yes           --

 Teacher fixed effects           No               Yes                 No           Yes

 Number of student-
                                5,652            5,652               5,644        5,644
 interval observations

 Number of teachers              149              149                 150          150

 Adjusted R-sq                 0.4923            0.505               0.494        0.497
Standard errors (in parentheses) clustered at the teacher level.
** = p<0.01, *** = p<0.001

                                                                                                17
