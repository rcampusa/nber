                               NBER WORKING PAPER SERIES




          THE ROLE OF THE PROPENSITY SCORE IN FIXED EFFECT MODELS

                                      Dmitry Arkhangelsky
                                        Guido Imbens

                                       Working Paper 24814
                               http://www.nber.org/papers/w24814


                     NATIONAL BUREAU OF ECONOMIC RESEARCH
                              1050 Massachusetts Avenue
                                Cambridge, MA 02138
                                     July 2018




We are grateful for comments by participants in the Harvard-MIT econometrics seminar, the
SIEPR lunch at Stanford, the International Association of Applied Econometrics meeting in
Montreal, Pat Kline, and Matias Cattaneo. The views expressed herein are those of the authors
and do not necessarily reflect the views of the National Bureau of Economic Research.

At least one co-author has disclosed a financial relationship of potential relevance for this
research. Further information is available online at http://www.nber.org/papers/w24814.ack

NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.

© 2018 by Dmitry Arkhangelsky and Guido Imbens. All rights reserved. Short sections of text,
not to exceed two paragraphs, may be quoted without explicit permission provided that full
credit, including © notice, is given to the source.
The Role of the Propensity Score in Fixed Effect Models
Dmitry Arkhangelsky and Guido Imbens
NBER Working Paper No. 24814
July 2018
JEL No. C1,C21,C23,C31

                                         ABSTRACT

We develop a new approach for estimating average treatment effects in the observational studies
with unobserved cluster-level heterogeneity. The previous approach relied heavily on linear fixed
effect specifications that severely limit the heterogeneity between clusters. These methods imply
that linearly adjusting for differences between clusters in average covariate values addresses all
concerns with cross-cluster comparisons. Instead, we consider an exponential family structure on
the within-cluster distribution of covariates and treatments that implies that a low-dimensional
sufficient statistic can summarize the empirical distribution, where this sufficient statistic may
include functions of the data beyond average covariate values. Then we use modern causal
inference methods to construct flexible and robust estimators.


Dmitry Arkhangelsky
CEMFI
5 Calle Casado del Alisal
Madrid 28014
Spain
darkhangel@cemfi.es

Guido Imbens
Graduate School of Business
Stanford University
655 Knight Way
Stanford, CA 94305
and NBER
Imbens@stanford.edu
1     Introduction
Suppose a researcher is interested in the average causal effect of a binary treatment in a setting
where the population of interest is partitioned into a number of subpopulations, clusters or
strata. The potential outcome distributions, both marginal and conditional on observed pre-
treatment variables, as well as the marginal and conditional treatment assignment probabilities,
may differ between the strata. The researcher has available a random sample of units from a
randomly selected subset of clusters. A popular estimation strategy in such settings is fixed
effect regression where the differences between the strata are assumed to be fully accounted
for by additive stratum-specific components in the regression function. Under this approach,
all clusters are assumed to be comparable once these additive stratum-specific components are
removed. However, especially in settings with additional covariates (pretreatment) variables,
the additivity and linearity assumptions imposed by such fixed effect methods impose strong,
possibly undesirable, conditions on the relationship between the potential outcome distributions
in the different strata.
    In this paper, we develop methods for this setting that allow for heterogeneity between clus-
ters beyond the additive component. This will imply that we cannot simply compare treated
and control units in any pair of clusters once we remove the additive component. Instead only
treated and control units in “similar” clusters are comparable. The key will be in constructing
measures of similarity between clusters. If we wish to be very flexible in the amount of hetero-
geneity between clusters we allow for, we end up in the extreme case where only units within the
same cluster are comparable. However, if there are few units per cluster in the sample we may
in that case not be able to adjust flexibly for differences between treated and control units in
terms of fixed pretreatment variables. We, therefore, may need to balance the desire to compare
treated and control units in similar clusters and the desire to compare only treated and control
units with similar covariates.
    To relax the functional assumptions, we start by first noting that, by omitted variable bias
arguments, biases arise from differences in the conditional distributions of the potential outcomes
in different strata only if the conditional assignment probabilities differ by stratum. In fact, it is
obvious that, if the treatment is completely randomly assigned, one need not be concerned with
systematic differences in potential outcome distributions between strata, additive or not. One
can, therefore, remove biases from comparisons between treated and control units in the same or


                                                  1
different clusters by ensuring that treatment/control comparisons are between units with iden-
tical assignment probabilities (identical values for the propensity score). We propose to exploit
this idea, well known in the evaluation literature since Rosenbaum and Rubin [1983] by mod-
eling and estimating the conditional assignment probabilities in the different strata. Using this
strategy, we can relax the functional form assumptions on the potential outcome distributions
substantially. The complication in following this approach is that in settings with few sampled
units per cluster we cannot consistently estimate the population assignment probabilities for
each unit. We address this problem by imposing an exponential family structure on the joint
distribution of treatments and covariates within a cluster to achieve consistency of the estimator
for the average treatment effect under asymptotic sequences with a fixed number of sampled
units per cluster.
    In the fixed effect approach units in different clusters are directly comparable once we remove
the additive fixed component. In our approach, the differences are more complex, and we rely
on comparing units in similar clusters, requiring us to define smooth measures of the distance
between clusters. Another alternative that also allows for heterogeneity between clusters beyond
additive components is to assume that sets of clusters are similar and use the data to identify
such sets, e.g., Bonhomme and Manresa [2015].
    Although we focus in the current paper on a cross-section setting with clusters, as in Altonji
and Mansfield [2014], the issues raised here are also relevant to proper panel or longitudinal data
settings (Hsiao et al. [2012], Chamberlain [1984], Pesaran [2006], Arellano and Honoré [2001],
Abadie [2005], Bertrand et al. [2004], as we discuss in Section 6. In that literature the paper
fits into a recent set of studies Abadie et al. [2010], de Chaisemartin and D’Haultfœuille [2018],
Bonhomme and Manresa [2015], Imai and Kim [2016], Athey and Imbens [2018] that connects
more directly with the causal (treatment effect) literature than the earlier panel data literature
by allowing for general heterogeneity beyond additive effects.



2     Fixed Effect versus Propensity Score Methods
In this section we set up the problem and introduce the notation. We then discuss fixed effect
regression, and state assumptions on the potential outcome distributions that justify this esti-
mation strategy. Next, we contrast these with estimation methods from the program evaluation


                                                 2
literature under unconfoundedness.


2.1     The Set Up
Using the potential outcome set up (e.g., Imbens and Rubin [2015]), we consider a set up with
a large, possibly infinite, population of units, characterized by a pair of potential outcomes
(Yi (0), Yi (1)), and a K-component vector of pretreatment variables Xi . The population is par-
titioned into strata or clusters, with Ci indicating the stratum or cluster unit i is a member of.
The number of strata in the population is large, and so is the number of units per cluster. We
are interested in the average treatment effects. Ideally we might wish to estimate the population
average effect,


      τ = E[Yi (1) − Yi (0)],


but this may be challenging, and we may need to settle for some other average of Yi (1) −
Yi (0), e.g., the average over some subpopulation defined in terms of clusters, covariates and
assignments. Unit i receives treatment Wi ∈ {0, 1}. We first randomly sample C clusters, and
then draw a random sample of size N from the subpopulation defined by the sampled clusters.
For the sampled units we observe the quadruple (Yi , Wi , Xi , Ci ), i = 1, . . . , N , where Yi = Yi (Wi )
is the realized outcome, that is, the potential outcome corresponding to the treatment received,
and Ci ∈ {1, . . . , C} is the cluster label for unit i. Also define Cic = 1Ci =c as the binary cluster
indicators, and let Nc = N
                            P
                               i=1 Cic be the number of sampled units in stratum c. For any variable
              P
Zi , let Z c = i:Ci =c Zi /Nc be the corresponding cluster average in cluster c.
   In the settings we are interested in the number of strata or clusters in the sample, C, may
be substantial, on the order of hundreds or even thousands. The dimension of Xi may be
modest. The number of units in the population in each cluster is large, but we observe only
few units in each stratum, possibly as few as two or three. As a result methods that rely on
accurate estimation of features of the population distribution of potential outcomes or treatments
conditional on covariates within clusters may have poor properties.




                                                    3
2.2       Fixed Effect Regression
A common approach to estimating the causal effect of Wi in this setting is to use a fixed effect
regression (e.g., Hsiao [2014], Arellano [2003], Chamberlain [1984], Angrist and Pischke [2008],
Wooldridge [2010]). Here the regression function is specified as

                                              C
                                              X
      Yi = αCi + Wi τ +     Xi> β   + εi =          Cic αc + Wi τ + Xi> β + εi ,               (2.1)
                                              c=1


with τ the object of interest, and β and the αc nuisance parameters. The parameters (β, τ ) and
the fixed effects αc , for c = 1, . . . , C, are then estimated by least squares:

                                        N 
                                          X        C
                                                   X                      2
          α̂cfe , β̂ fe , τ̂ fe = arg min     Yi −   Cic αc − Wi τ − Xi> β .                   (2.2)
                              αc ,τ,β
                                        i=1         c=1


This set up is the starting point of the discussion in this paper. The fixed effect specification and
corresponding estimator are widely used in the empirical literature. In the absence of the fixed
effects the concern is that comparisons of treated and control units, say based on least squares
regression using the same specification of the regression function other than the omission of the
fixed effects,


      Yi = α + Wi τ + Xi> β + εi ,


would not have a credible causal interpretation.
   In typical applications the number of strata is substantial, the dimension of the covariates is
modest, and the number of sampled units per stratum is modest. Asymptotic approximations
are often based on the number of strata increasing proportional to the number of sampled units,
so that the average number of sampled units per stratum converges to a finite limit. This leads
to the incidental parameters problem (Neyman and Scott [1948], Bonhomme [2012], Bonhomme
and Manresa [2015], Arellano and Hahn [2006], Hsiao [2014], Hahn and Newey [2004]). However,
the incidental parameter problem does not create complications for estimators of all parameters,
and in this case it does not compromize our ability to get a consistent estimator for τ .
   To motivate the paper, we first present a set of assumptions that justify the fixed effects
estimator τ̂ fe as an estimator for the causal effect τ , and then discuss some concerns with these

                                                           4
assumptions. Note that we are not concerned with the properties of α̂cfe or β̂ fe , solely with
the estimator τ̂ fe for the treatment effect. In fact there are no consistent estimators for αc
under asymptotic sequences of the type we consider where the number of units per stratum
remains finite. To formally justify the fixed effect estimator for τ we can make the following
assumptions. First, unconfoundedness, which implies that the comparison of treated and control
units within the same stratum, and with the same value for the pretreatment variables, has a
causal interpretation:

                               
     Wi ⊥⊥        Yi (0), Yi (1)  Xi , Ci .                                                (2.3)


The second assumption adds functional form restrictions. Define the within-stratum conditional
expectation of the potential outcomes given Xi :


     µw (x, c) = E[Yi (w)|Xi = x, Ci = c].                                                 (2.4)


Then assume


     µw (x, c) = αc + x> β + wτ.                                                           (2.5)


Both the unconfoundedness assumption and the functional form assumption are strong and
often controversial. In the current discussion however, we focus on the functional form assump-
tion and maintain the unconfoundedness assumption. For discussions of the unconfoundedness
assumption the reader is referred to the general treatment effect literature, e.g., Imbens and
Rubin [2015], Morgan and Winship [2014]. There are multiple concerns with the functional
form assumption. First, the strata may differ not only in the level of the outcome, but also in
the response to the treatment, so that the constant treatment effect assumption is violated. The
strata may also differ in the association between other covariates and the outcome, so that the
additivity and linearity assumptions are violated. Under the fixed effect assumptions treated
units in a cluster with 90% of the units treated can be compared to control units in a cluster
with 10% of the units treated, as long as we remove the corresponding αc . Such strong im-
plications may be unrealistic, and it may be more reasonable to compare treated and control
units in clusters that are similar. The question is how to define and operationalize the notion


                                               5
of similarity of clusters in a setting with few sampled units per cluster.
   Note that in the case without pretreatment variables these functional form assumption matter
substantially less. In the absence of covariates, the fixed effects estimator is no longer unbiased
for the average effect of the treatment if the effects of the treatment differ between strata.
Nevertheless, the fixed effects estimator does estimate a weighted average of the within-stratum
average effects, with the weights equal to the inverse of the within-stratum variances, and so that
it has a meaningful causal interpretation (e.g., Angrist and Pischke [2008]). However, this result
does not extend to the case with covariates that enter into a nonlinear or interactive way if the
specification of the regression function is linear. In that case the fixed effect estimators may have
substantial bias if the functional form assumption is violated, and the sign of the probability
limit of the fixed effect estimator can be of the opposite sign, even if all the within-cluster
average treatment effects are the same sign.
   Relaxing the functional form is not straightforward. Even parametric extensions, by, for
example, allowing for separate τ and β by stratum may be difficult to implement with com-
monly available data. For example, Chernozhukov et al. [2013] develop methods that allow the
unobserved component αc to enter the conditional expectation of Yi in a nonlinear manner.


2.3    Propensity Score Methods
In contrast to the fixed effect literature, functional form assumptions are often avoided in the
treatment effect literature (see Abadie and Cattaneo [2018] and Imbens and Wooldridge [2009]
for recent surveys), by using more flexible estimators. Many of the recommended estimators in
that literature go beyond estimating the conditional expectation of the outcomes given treat-
ment and covariates. The concern with estimators that rely only on estimating the conditional
expectation of the outcomes is that they are often sensitive to the specific estimation method
employed, in particular in settings where the covariate distributions differ substantially between
treatment groups. In the current setting, this would correspond to a concern that the probability
of receiving the active treatment may differ substantially by both strata and covariates. The es-
timators that are recommended in that literature involve in some fashion or another estimating
the propensity score,


      e(x, c) = pr(Wi = 1|Xi = x, Ci = c).


                                                 6
Estimators in this literature include weighting on the inverse of the propensity score (Hirano
et al. [2003]), matching on the propensity score (Abadie and Imbens [2016]), or blocking on
the propensity score, often of them in combination with direct regression adjustment through
doubly robust methods (Robins and Rotnitzky [1995]). One specific approach is to use the
influence function (Bickel et al. [1998], Chernozhukov et al. [2016]). In the current case that
would correspond to estimating e(·) and µw (·), and then estimate τ as

                      N                                                                                           
           eif     1 X                                         Yi − µ̂1 (Xi , Ci )             Yi − µ̂0 (Xi , Ci )
      τ̂         =        µ̂1 (Xi , Ci ) − µ̂0 (Xi , Ci ) + Wi                     − (1 − Wi )                      .
                   N i=1                                          ê(Xi , Ci )                  1 − ê(Xi , Ci )

In settings with a substantial number of units per cluster, we can directly implement these ideas
(e.g., Yang [2016]).
   The main issue with this approach in the current setting is that it relies on the number
of sampled units per cluster being sufficiently large so that we can estimate the conditional
potential outcome means and the propensity score with sampling error going to zero. This is
not in the spirit of the fixed effects literature where, by differencing out the fixed effects, one
can obtain consistent estimates of the parameters of interest in settings with a small number
of units per cluster, sometimes as few as two. One approach would be to use some of the
recent methods that allow for high-dimensional covariates (e.g., Farrell [2015], Athey et al.
[2016], Chernozhukov et al. [2016]), and use the cluster indicators simply as additional control
variables. The structure of the control variables, with the cluster indicator partitioning the
population in many subpopulations, may prevent such methods from being effective.
   In this paper we propose a new approach that allows us to exploit insights from the propensity
score literature in settings with a finite number of sampled units per cluster. In fact, this number
may be as small as two. We do so by imposing structure on the joint distribution of the treatment
assignment and covariates in the clusters, so that we can characterize the propensity score as
function of only a small number of individual characteristics.


2.4        An Alternative Representation of the Fixed Effect Estimator
To motivate our approach it is useful to observe is that we can characterize the fixed effect
estimator in an alternative way. Using the notation for cluster averages of Yi , Wi , and Xi



                                                             7
respectively,

                1 X                                1 X                          1 X
      Yc =                Yi ,              Wc =             Wi ,        Xc =             Xi ,
                Nc i:C =c                          Nc i:C =c                    Nc i:C =c
                         i                                  i                          i



we can write the fixed effect estimator in a different way.

Lemma 1. (An Alternative Representation of the Fixed Effect Estimator -
Mundlak [1978]) Consider the regression

                                                        >
      Yi = α + Wi τ + Xi> β + W Ci δ + X Ci γ + εi ,                                                                (2.6)


with the least squares estimates defined as

                                                          N                                                 2
                                                            X                                              >
           ca   ca      ca   ca
          α̂ , δ̂ , γ̂ , τ̂ , β̂   ca
                                            = arg min             Yi − α − Wi τ −   Xi> β   − W Ci δ −   X Ci γ ,
                                                α,δ,γ,τ,β
                                                            i=1


(where the superscript “ca” stands for cluster averages). Then:


      τ̂ ca = τ̂ fe .


The proof can be found in Appendix B.1.
Comment 1: This alternative representation of the fixed effects estimator, mentioned in passing
in Mundlak [1978], can be derived easily using omitted variable bias expressions. We state it as a
formal result merely to facilitate the interpretation of the novel results below. The intuition for
the equivalence of the two regressions is that bias from omitting cluster indicators comes from a
non-zero effect of the stratum indicator, in combination with a correlation between the stratum
indicator and the treatment indicator. If we have two clusters with the same distribution of
conditional treatment probabilities, there is no bias from combining them into a single cluster.

Comment 2: Altonji and Mansfield [2014] also use cluster averages as a method for controlling
for unobserved cluster differences. Their focus is on linear methods and the identifying power
of such regressions in the context of structural models. 
Comment 3: These two representations show that the adjusting for differences between the


                                                                   8
clusters takes a very simple form, exploiting the additivity of the regression function. Instead of
including indicators Cic for the cluster, we can include the cluster average covariates, or subtract
cluster averages from outcomes and covariates. Suppose we view (2.6) as a regression version
of an attempt to compare treated and control units after adjusting for all differences between
the cluster in (Xi , W Ci , X Ci ). Then the program evaluation literature under unconfoundedness
would suggest that if the distribution of these variables were substantially different for treated
and control units, simple least squares regression might not be a reliable way of adjusting
for these differences. Instead, more sophisticated methods of adjusting for such differences,
involving the propensity score, and relaxing linearity and additivity of the regression function
in (Xi , W Ci , X Ci ), might be more effective. 
Comment 4: In addition (2.6) suggests that the differences between clusters are fully captured
by differences in the values of the pair of averages (W c , X c ). In many cases, there may be
concerns that the differences between clusters are more complex, and would require accounting
also for differences in XW c , the cluster average of the product of Wi and Xi , or in the average
value of higher order moments of Xi . Given the representation in (2.6) it would be natural to
include averages of such functions in the regression function. 
    Our proposed approach addresses these last two comments. It provides a formal justification
for an unconfoundedness condition given sample cluster averages of functions of the covariates
and treatment indicators, which motivates adjusting for those in a flexible, nonlinear way. It
also suggests when it would be appropriate to include averages of additional functions of the
covariates and treatment indicators beyond (W c , X c ).



3     An Alternative to the Fixed Effect Estimator
In this section we present our main results, We propose a new estimator for average treatment
effects in the setting with clustered data. The estimator has features in common with the
efficient influence function estimators from the program evaluation literature, as well as with
the fixed effect estimators from the panel data literature. Unlike fixed effect estimators, it
can accommodate differences in potential outcome distributions between clusters that are not
additive. There are two issues involved in our approach. First, we have to be careful in defining
the estimand to account for the fact that there may be few units in a cluster. In general, we can


                                                     9
not consistently estimate the overall average causal effect, because there are likely to be clusters
with no treated or no control units. To take this into account, we define a subset of units for
which we estimate the average effect. This subset will depend on fixed characteristics of units as
well as on realizations of the sampling and assignment processes in a somewhat unusual manner.
Of course, this is not new to our approach: standard fixed effect estimators do not estimate the
average effect of the treatment if there is systematic variation in treatment effects by strata.
Second, we need to adjust for features of the clusters that cannot be estimated consistently
under the asymptotic sequences we consider.


3.1    Some Preliminary Assumptions
The set up we consider has a large population of clusters. In the population, each cluster has a
large number of units. We randomly sample a finite number of clusters and then sample a finite
number of units from the subpopulation of sampled clusters. Large sample approximations to
estimators are based on the number of sampled clusters increasing, with the average number of
sampled units per cluster converging to a constant.

Assumption 3.1. (Balanced clustered sampling) There is a super-population of clusters,
we randomly sample n of them and for each cluster we randomly sample Nc = |c| units, with
N = nc=1 Nc the total sample size. Nc is the same for all clusters
     P


   This assumption describes the sampling process, it is not the only possible sampling scheme
that we can allow for, but this is the simplest one. In particular, it is possible to generalize our
results to the settings with variation in the number of sampled units for each cluster.
   For each unit in the population the (unobserved) data tuple is given by {(Yi (0), Yi (1), Wi , Xi , Ui , Ci )}N
                                                                                                                 i=1 .

The variable Ui is a cluster-level variable that varies only between clusters, so that it is equal to
its cluster average for all units, U Ci = Ui for all i.
   Our second assumption imposes restrictions on the treatment assignment process:

Assumption 3.2. (Unconfoundedness within Clusters)


                         
      Wi ⊥⊥ Yi (0), Yi (1)       Xi , Ci .                                                           (3.1)



                                                   10
       This assumption implies that we can always compare individuals with the same characteris-
tics within the cluster.
       The second assumption imposes restrictions on the fixed effects.

Assumption 3.3. (Random effects)
For the unobserved cluster-level variable U Ci we have the following:

                                   
             Yi (1), Yi (0), Xi , Wi ⊥⊥ Ci          U Ci                                       (3.2)


       This assumption essentially turns the problem into a random effects set up: the labels of the
clusters Ci are not important, only the cluster-level characteristics U Ci are. Conceptually, this
assumption allows us to conceptualize similarity of clusters.
       Since U Ci is measurable with respect to cluster indicator variable, an implication of the
previous pair of assumptions is:

                                     
         Wi ⊥⊥          Yi (0), Yi (1)  X i , U Ci .                                           (3.3)


Now we can also compare treated and control units in different clusters, as long as the clusters
have the same value for U Ci .


3.2          Identification results
For the first identification result we need some additional notation. For each cluster c define
define Pc to be the empirical distribution of (Xi , Wi ) in cluster c. In the case with discrete
Xi this amounts to the set of frequencies of observations in a cluster for each pair of values
(Wi , Xi ).1

Proposition 1. (Unconfoundedness with empirical measure) Suppose Assumptions
3.1-3.3 hold. Then:

                                    
         Wi ⊥⊥         Yi (0), Yi (1)    Xi , PCi                                              (3.4)


For the proofs of the results in this section see Appendix A.
   1
       For the formal definition of this object including continuous Xi see Appendix A.


                                                           11
Comment 1: This result states that as long as units have the same characteristics, and they
come from clusters identical in terms of PCi , they are comparable. This is a propensity score
type result in the sense that subpopulation with the same value for (Xi , PCi ) are balanced: the
distribution of treatments is the same for all units within such subpopulations. 
Comment 2: One can view this result as a statement that PCi captures all the information
about U Ci from the data and thus it is enough to condition on it. While intuitive, this statement
is not entirely correct. Clusters can be different in terms of empirical distribution of Yi , and
this can potentially help in predicting U Ci , but we are not utilizing this for the identification.
Instead, we are using the fact that the conditional distribution of U Ci given (Xi , PCi ) and Wi
does not depend on Wi , and thus we are averaging over the same distribution for control and
treated units. 
Comment 3: Proposition 1 can be directly used for identification only in cases where the dis-
tribution of Xi is discrete and supported on the small number of points. Otherwise, we would
never observe clusters with the same value of PCi .Thus this result is more important from the
conceptual point of view, rather than a basis for an actual algorithm for estimation. Neverthe-
less, it shows how adjusting for features of the cluster that are not estimated consistently (the
empirical distribution of (Wi , Xi ), not the population distribution) can still lead to consistent
estimates of causal effects. 
   In order to get an operational identification result, that is, one that works even with multi-
valued Xi , we impose additional structure:

Assumption 3.4. (Exponential family) Conditional on Ui distribution of (Xi , Wi ) belongs
to an exponential family with a known sufficient statistic:

                                        n            o
                                          T
      fXi ,Wi |Ui (x, w|u) ∝ h(x, w) exp η (u)S(x, w) ,                                        (3.5)


with potentially unknown carrier h.

Define Si := S(Xi , Wi ), and let S c be the cluster average of Si for cluster c.
Comment 4: This assumption restricts the statistical model for (Xi , Wi )|U Ci but not for
Yi |Xi , Wi , U Ci . We also do not put any restrictions on the carrier h(·), which can be a gen-
eral function of its arguments. This makes this model quite flexible. 



                                                 12
Theorem 1. (Unconfoundedness with sufficient statistic) Suppose Assumptions 3.1–
3.4 hold. Then:

                               
      Wi ⊥⊥       Yi (0), Yi (1)    X i , S Ci .                                                 (3.6)


   Theorem 1 can be viewed as essentially a direct consequence of Proposition 1, but it is
substantially more operational. It reduces the potentially high-dimensional object PCi to a
lower dimensional average S Ci . It is also unusual in that one of the conditioning variables,
S Ci , is not a fixed unit-level characteristic. Instead, it is a characteristic of the cluster and the
sampling process. If we change the sampling process, say to sampling twice as many units per
cluster, the distribution of S Ci changes. Nevertheless, this conceptual difference in the nature
of S Ci relative to the unit-level characteristic Xi does not affect how it is used in the estimation
procedures.
   There is another key difference between the unconfoundedness condition in Theorem 1 and
in Proposition 1. With continuous covariates, the latter essentially makes it impossible to have
overlap. Indeed, unless we have individuals with the same value of covariates within the cluster,
the distribution of Wi given Xi and PCi is degenerate. It is well known that overlap is crucial in
the semiparametric estimation of treatment effects and without it, the identification is possible
only under functional form assumptions.
   The result in Theorem 1 is more useful because it allows us to control the degree of overlap
as well. With |c| being fixed the higher is the dimension of S(·) the closer we are to controlling
for PCi , and thus the smaller is the region for which we have overlap. It would be interesting to
balance this effect with other statistical effects that arise from having higher-dimensional S(·),
but we leave this for future work. In this paper, we will assume that S(·) is known, fixed and
there is a known region of the covariate space where we have overlap.
   In particular, define propensity score:


      e(x, s) := E[Wi |Xi = x, S Ci = s]                                                         (3.7)


We are making the following assumption:

Assumption 3.5. (Known overlap) We assume that there exists η > 0 and a nonempty
known set A, such that for any (x, s) ∈ A we have η < e(x, s) < 1 − η.

                                                   13
Comment 5: This assumption has two parts: the first part restrict e(x, s) to be non-degenerate
on a certain set. This is necessary if we want to identify treatment effects without relying on
functional form assumptions. The second part is different: we assume that the set is known to a
researcher. This is a generalization of the standard overlap assumption, where we assume that
the set A is equal to the support of the covariate space. See Crump et al. [2009]. 


3.3     An Example
As a natural example of the density that satisfies Assumption 3.4 consider the following family:


      
      E[Wi |Ui ] = π(Ui )
      
                                                                                                       (3.8)
      Xi |Ui , Wi ∼ N (µ(Wi , Ui ), σ 2 (Wi , Ui ))
      


It is easy to see that the conditional distribution of (Xi , Wi ) given Ui has exponential family
representation:


  f (Xi , Wi |Ui ) ∝ exp{η1 (Ui )Xi2 + η2 (Ui )Wi Xi2 + η3 (Ui )Xi + η4 (Ui )Wi Xi + η5 (Ui )Wi }h(Xi , Wi )







                  1




 η1 (Ui ) := σ2 (0,U i)



η2 (Ui ) := 2 1 − 2 1

              σ (1,Ui )  σ (0,Ui )

  η3 (Ui ) := −2 σµ(0,U   i)


                   2 (0,U )

                          i

                                          
                  µ(0,Ui )       µ(1,Ui )
                              −



 η4 (Ui ) := 2   σ 2 (0,Ui )   σ 2 (1,Ui )


                             
η5 (Ui ) := log π(Ui ) + µ2 (1, Ui ) − µ2 (0, Ui )


                      1−π(Ui )

                                                                                                       (3.9)

                                                                                                     
Thus in this case the sufficient statistics is 5-dimensional: S c = X 2 c , X c , X 2 W c , XW c , W c .




                                                       14
4     Estimating Average Treatment Effects
In this section, we discuss two ways to exploit the results in Theorem 1. In the first, we
use a linear model approach, where we depart from the fixed effect specification in (2.1) by
using additional sufficient statistics, and by adjusting for those in a more general way, though
maintaining much of the linear model structure. This is a straightforward approach that may
be reasonable when the covariate and sufficient statistic distributions do not differ much by
treatment status. In the second we use a general doubly robust approach that is more likely to
be appropriate when the distributions differ substantially by treatment status.


4.1     A Linear Model Approach
First we select a set of sufficient statistics. Whereas implicitly the fixed effect approach uses
X c and W c , we may wish to include in addition the average of the product of Xi and Wi ,
        P
XW c = i:Ci =c Xi Wi /Nc . Given S c = (X c , W c , XW c ), we can estimate a linear model with

                                                  >
      Yi = α + τ Wi + Xi> β + S Ci δ + Wi (Xi − X)> θ + Wi (S Ci − S)> γ + εi ,


where X and S are the sample averages of Xi and S Ci respectively. This allows for more general
associations between the potential outcomes and the covariates and sufficient statistics, as well
as for interactions with the treatment.
    We estimate the parameters by least squares:


      (α̂, τ̂ , β̂, δ̂, θ̂, γ̂) =

                                  N                                                                                2
                                  X                                      >
              arg       min                    Yi − α − τ Wi − Xi> β − S Ci δ − Wi (Xi − X)> θ − Wi (S Ci − S)> γ        .
                    α,τ,β,δ,θ,γ
                                    i=1

Additionally define the following parameters:


      (α̂p , β̂p , δ̂p , θ̂p , γ̂p ) =

                                        N
                                        X                                                                           2
             arg         min                    Wi − αp − Xi> βp − δp> S Ci − Wi (Xi − X)> θp − Wi (S Ci − S)> γp        .
                   αp ,βp ,δp ,θp ,γp
                                         i=1



                                                                  15
and let (αp , βp , δp , θp , γp ) be the corresponding population parameters.
                                                                  
   Define Di := 1, Wi , Xi , S Ci , Wi (Xi − X), Wi (S Ci − S) . We make the following standard
assumptions about Di and εi :

Assumption 4.1. (Projection assumptions) The following restrictions are satisfied for Di
an εi :
          
            E[Di DiT ] is invertible
          
          
          
          
          
          
            E[ε4i ] < ∞                                                                          (4.1)
          
          
          
          
          E[kDi k42 ] < ∞
          


Assumption 4.2. (Linearity) The conditional expectation satisfies


          E[Yi (w)|Xi , S Ci ] = α + τ w + Xi> β + δ > S Ci + w(Xi − X)> θ + w(S Ci − S)> γ.


    The proofs of the next lemmas can be found in Appendix B.1.

Lemma 2. Suppose Assumptions 3.1–3.4, and 4.1-4.2 hold with sufficient statistic S Ci . Then
the least squares estimator τ̂ls is consistent for the average treatment effect τ = E[Yi (1) − Yi (0)].

    Define the asymptotic variance and its empirical analog:

                                             2
                   h P                          i
                   E ( i:C             ui εi )       /Nc
          V :=
                               i =c
                         E[u2i ]
                   Pn                                      2                                     (4.2)
                        (                  ûi ε̂i /Nc ) /n
                        P
          V̂ :=
                   c=1         i:Ci =c
                    Pn
                            (                û2i /Nc )/n
                            P
                     c=1          i:Ci =c



where
          
                                            >
            ui := Wi − αp − Xi> βp − S Ci δp − Wi (Xi − X)> θp − Wi (S Ci − S)> γp
          
          
          
          
          
          
                                            >
            ûi := Wi − α̂p − Xi> β̂p − S Ci δ̂p − Wi (Xi − X)> θ̂p − Wi (S Ci − S)> γ̂p         (4.3)
          
          
          
          ε̂i := Yi − α̂ − Xi> β̂ − S >                   >                   >
          
                                       Ci δ̂ − Wi (Xi − X) θ̂ − Wi (S Ci − S) γ̂
          


We have the following result:


                                                               16
Lemma 3. Suppose Assumptions 3.1–3.4,4.1 hold with sufficient statistic S Ci . Suppose in
addition that


       E[Yi |Wi , Xi , S Ci ] = α + τ Wi + Xi> β + δ > S Ci + Wi (Xi − X)> θ + Wi (S Ci − S)> γ.


Then

       √                 d
           n(τ̂ls − τ ) −→ N (0, V),


and


       V̂ = V + op (1).


4.2     The General Case
In this subsection we collect several inference results for the general semiparametric estimator.
All proofs can be found in Appendix B.2.
   For the further use we use following notation for the conditional mean, propensity score and
residuals:
       
         µ(Wi , Xi , S Ci ) := E[Yi |Wi , Xi , S Ci ]
       
       
       
       
       
       
         e(Xi , S Ci ) := E[Wi |Xi , S Ci ]                                                        (4.4)
       
       
       
       
       εi (w) := Yi (w) − µ(w, Xi , S Ci )
       


Note that these expectations are defined conditional on Assumption 3.1, which determines the
distribution of S Ci .
   We will use µ̂i (·) and êi (·) for generic estimators of µ(·) and e(·). Subscript i is used to allow
for cross-fitting (Chernozhukov et al. [2016]). Define {Ai } := {(Xi , S Ci ) ∈ A}, where A is the
(known) set with overlap in the distribution of (Xi , S Ci ). Define true and estimated share of




                                                        17
observations with overlap:
           
           π(A) := E[{Ai }]
           
                                                                                                                     (4.5)
                        1      1
                            P       P
                                      i:Ci =c {Ai }
           π̂(A) :=
           
                        n    c Nc


        We assume the generic estimators êi and µ̂i satisfy several high-level consistency properties.
These restrictions are standard in the program evaluation literature.

Assumption 4.3. (High-level conditions) The following conditions are satisfied for êi and
µ̂i :
        
          η < êi (Xi , S Ci ) < 1 − η a.s.
        
        
        
        
        
        
        
          1
            Pn h 1 P                                                         2
                                                                               i
         n c=1 Nc i:Ci =c {Ai }(e(Xi , S Ci ) − ê(Xi , S Ci )) = op (1)
        
        
        
        
        
            Pn h 1 P                                                                          i
          1
              c=1 Nc         i:Ci =c {Ai }(µ(W i , X i , S Ci ) − µ̂ i (Wi , X   i , S Ci ))2
                                                                                                = op (1)             (4.6)
         n
        
                  h                                                            i
         1
            Pn        1
                         P                                                    2
                             i:Ci =c {Ai }(e(Xi , S Ci ) − êi (Xi , S Ci ))
        
        
        
         n   c=1 Nc
        
                             P h P                                                                      i
                        × n1 nc=1 N1c i:Ci =c {Ai }(µ(Wi , Xi , S Ci ) − µ̂i (Wi , Xi , S Ci ))2 = op
        
                                                                                                            1
                                                                                                                 
        
                                                                                                             n


        We also restrict moments of the residuals:

Assumption 4.4. (Moment conditions)
           
           E[ε2i (k)|Xi , S Ci ] < K a.s.
           
                                                                                                                     (4.7)
           E[ε4 (k)] < ∞
           
                 i



        For arbitrary (subject to appropriate integrability conditions) functions (µ(·), e(·)) define the
following functional:
                                                                                           
                                                                 w       1−w
        ψ(y, w, x, s, µ(·), e(·)) := µ(1, x, s) − µ(0, x, s) +        −                         (y − µ(w, x, s)). (4.8)
                                                               e(x, s) 1 − e(x, s)

        We focus on the following causal estimand:

                            n
                                                                                        !
                    1 1X            1 X                                                
           τ̃A =                              {Ai } µ(1, Xi , S Ci ) − µ(0, Xi , S Ci )                              (4.9)
                 π̂ (A) n c=1       Nc i:C =c
                                          i



                                                             18
This is a quantity that is random because of its dependence on {Ai }.2

Theorem 2. (Consistency) Suppose Assumptions 3.1–3.4 and Assumption 4.3 hold. Then:

                              n
                                                                                                                !
                     1 1X            1 X
          τ̂dr :=                              {Ai }ψ(Yi , Wi , Xi , S Ci , µ̂(Wi , Xi , S Ci ), ê(Xi , S Ci )) ,   (4.10)
                  π̂(A) n c=1        Nc i:C =c
                                             i



satisfies τ̂dr − τ̃A = op (1).

       For inference results we need to use µ̂i with cross-fitting. We also need to take account of
the clustering. Define

                                  1 X
          ρ(c, µ(·), e(·)) :=              {Ai }ψ(Yi , Wi , Xi , S Ci , µ(Wi , Xi , S Ci ), e(Xi , S Ci )),
                                  Nc i:C =c
                                         i



so that
                       n
                 1X
          τ̂dr =       ρ(c, µ̂(·), ê(·))/π̂(A).
                 n c=1

Theorem 3. (Inference for semiparametric case) Suppose Assumptions 3.1–3.4 and
Assumption 4.3 hold. Assume that µ̂i is estimated using cross-fitting with L folders. Then:

          √                   d                                      E [ξc2 ]
              n(τ̂dr − τ̃A ) −→ N (0, V),            where V =                ,
                                                                     π 2 (A)

where ξc is defined in the following way:
                                                              
                X 1               Wi            1 − Wi
          ξc :=        {Ai }                −                    (Yi − µ(Wi , Xi , S Ci ))
                i∈c
                    Nc         e(Xi , S Ci ) 1 − e(Xi , S Ci )

       Finally, we address the estimation of variance. For this define the following empirical version
of ξc :
                                                                                              
                   X 1                 Wi            1 − Wi
          ξˆc :=          {Ai }                  −                     (Yi − µ̂(Wi , Xi , S Ci ))                    (4.11)
                   i∈c
                       Nc          ê(Xi , S Ci ) 1 − ê(Xi , S Ci )
   2
    It is straightforward to extend our inference results to a more standard target τA , in which case we will have
a different (larger) variance.


                                                                19
The proposed variance estimator is just the variance of ξˆc :

                     n           n
                                           !2
              1 1X ˆ         1Xˆ
      V̂ := 2           ξc −         ξc0        .                                           (4.12)
           π̂ (A) n c=1      n c0 =1

    The following proposition says that asymptotically variance of the estimated influence func-
tion is equal to the variance of the true influence function:

Proposition 2. (Variance consistency) Suppose the assumptions of Theorem 3 hold. Then
the variance estimator is consistent:


      V̂ = V + op (1).                                                                      (4.13)



5     Applications

5.1    Empirical Illustration
We consider data from Das et al. [2016], in this paper authors want to estimate the differences
in quality between public and private healthcare providers in rural India. To achieve this,
they sent 15 standardized (fake) patients, each with three different cases to both private and
public providers in 100 villages in 5 districts. For each provider the authors also observe several
covariates, in our analysis, we will use gender and age.
    In this setup, W is the indicator for a public/private healthcare provider, X is (gender, age).
We define clusters by interacting patient, the case, and the district. This leads to potentially
15 × 3 × 5 = 225 clusters, but in the data, only a fraction of this is observed. The distribution
of the cluster sizes is summarized in Table 1.
                                                                                      
    We use a 5-dimensional sufficient statistic in this setup:    W Ci , X Ci , W X Ci , where Xi =
(agei , genderi ). We use A = {i : 0 < W Ci < 1}. We are left with 520 observations out of 635.
In order to construct the doubly-robust estimator we estimate a simple logit model. The results
for the logit model are presented in Table 2. While none of the sufficient statistic is significant
on its own, the LR test strongly rejects the model without them (LR = 44.09 at 5 degrees of
freedom).
    Estimation results are presented in Table 3. We report the standard fixed effect estimator


                                                    20
along with a simple OLS estimator based on (Xi , Si ). We report these estimators for the full
and restricted sample. For the restricted sample we also compute the doubly-robust estimator.
We use cluster-bootstrap to compute the standard errors.
    Results for all five estimators are qualitatively similar, which is expected because the data
come from an experiment. We view this exercise as a proof of concept.


5.2     Simulations
For the simulation we consider the following DGP:
      
      
      
      
       Ui ∼ Bern(0.5)
      
      
      
      Wi |Ui ∼ Bern(π(Ui ))
      
                                                                                               (5.1)
        X |W , U ∼ N (µ(Wi , Ui ), 1)
      
      
       i i i
      
      
      
      
      Y |X , W , U ∼ N (α(U ) + β(U )X + τ (U )W , 1)
      
         i  i   i  i          i       i i     i  i



Parameters (π(Ui ), µ(Wi , Ui ), α(Ui ), β(Ui ), τ (Ui )) are provided in Table 4. In this case, the
sufficient statistic is given by (X Ci , W Ci , W X Ci , X 2 Ci ).
    For the simulation, we consider a situation with small clusters (4 units per cluster) and a
large number of them (200 clusters). For each simulation, we consider two sets of estimators.
The first set includes the standard fixed effect estimator, OLS estimator with (Xi , Si , (XS)i )
as covariates and the residual-adjusted OLS estimator, with weights based on logit propensity
score. For the second set of estimators, we restrict the sample and consider only clusters with
0 < W Ci < 1. For this sample, we again run the standard fixed effects model, OLS model
with (Xi , Si , (XS)i ) as covariates and three residual-adjusted estimators, with propensity score
model estimated by a simple logit model, flexible tree model, and random forest.
    Results averaged over 500 simulations are presented in Table 5 and Table 6. In both full and
restricted samples, the fixed effect estimator performs very poorly: the average result is equal
to 7.51 which is outside of the convex hull of possible treatment effects (given by the interval
[4, 6]). In both cases, the OLS estimator performs significantly better, and the doubly-robust
estimator performs better in terms of RMSE. The best RMSE is achieved using a combination
of OLS and a random forest estimator for the propensity score. The resulting estimator is nearly


                                                       21
unbiased and has only a slightly higher standard deviation. Note that the restricted sample is
80% of the original one.



6    The Setting with Panel Data
In this section we briefly consider the case with panel data. The ideas developed in the current
paper for the cluster setting extend readily to the panel case, although the assumptions that
would justify them might be more controversial in this setting. In the panel case the current
paper fits into a recent literature that connects more explicitly the panel data literature with the
causal treatment effect literature by allowing for general heterogeneity beyond additive effects.
For example, in an influential paper Abadie et al. [2010] develops a synthetic control approach
that focuses directly on estimating counterfactual outcomes for units exposed to the treatment
of interest as a weighted average of outcomes for units exposed to the control treatment. Xu
[2017] and Athey et al. [2017] build on Bai and Ng [2002, 2017] to develop matrix completion
methods for this setting. Bonhomme and Manresa [2015] and Bonhomme et al. [2017] consider
a set up where the units form clusters such that within clusters the units are homogenous, but
between clusters there may be heterogeneity beyond additive components. Which cluster a unit
belongs to is is estimated using methods similar to k-means. de Chaisemartin and D’Haultfœuille
[2018] focus on the staggered adoption case where, once a unit is exposed to the treatment, it
remains exposed in all periods after the initial one. They investigate the interpretation of the
two-way fixed effects estimator under general heterogeneity, but maintaining the assumption
that the adoption date has no effects beyond the effect of being exposed in the current period.
Athey and Imbens [2018] also focus on the staggered adoption case and consider randomization
inference, allowing for general treatment effects of the adoption date. Imai and Kim [2016]
discusses models that take the dynamic aspects of the panel setting more seriously than the
simple two-way fixed effects set up, using graphical models of the type advocated by Pearl
[2000]. De Chaisemartin and DHaultfŒuille [2017] study a two-group and two-period difference-
in-differences setting where only some units in the second period treatment group receive the
treatment, again allowing for general treatment effect heterogeneity. Borusyak and Jaravel
[2016] study the staggered adoption case within a model with additive time and unit effects, but
allowing for general dynamic treatment effects.


                                                22
   Now let us consider the generalization of the set up in the current paper to the panel case.
Suppose we have N observations on C individuals, and T time periods, so that N = C × T . We
observe Yi for all units and a binary treatment Wi . Let Ti ∈ {1, . . . , T } denote the time period
observation i is from, and let Ci ∈ {1, . . . , C} denote the individual it goes with.
   For any variable Zi , define the time and individual averages:

               1 X                      1 X
      Z ·t =            Yi ,   Z c· =            Yi ,
               C i:T =t                 T i:C =c
                    i                        i



and the overall average

            N
         1 X
      Z=       Zi ,
         N i=1

and the residual


      Żi = Zi − Z ·t − Z c· + Z


   Let τ̂fe be the least squares estimator for the regression


      Yi = αTi + βCi + τ Wi + Xi> γ + εi                                                       (6.1)


Compare this to the least squares regression


      Yi = τ Wi + Xi> γ + δW ·Ti + µW Ci · + ψX ·Ti + ϕX Ci · + εi


The two least squares estimators for τ are numerically identical. This suggests that we can view
the standard fixed-time effects approach in (6.1) as controlling for time and individual level
sufficient statistics. This view opens a road to generalizing the standard estimators.
   At the same time, this type of generalization is not completely satisfactory. For one, con-
trolling for future values of Xit and Wit seems controversial. Also, it seems that the outcome
information should be used to control for individual-level heterogeneity. Finally, in the panel
case, the definition of treatment effects is inherently more complex, because of the dynamic
structure of the problem. For these reasons, we think that the approach of this paper while

                                                        23
insightful should be refined to make it appropriate for the panel data settings. We leave this for
future research.



7     Conclusion
In this work, we proposed a new approach to identification and estimation in the observational
studies with unobserved cluster-level heterogeneity. The identification argument is based on the
combination of random effects and exponential family assumptions. We show that given this
structure we can identify a specific average treatment effect even in cases where the observed
number of units per cluster is small. From the operational point of view, our approach allows
researchers to utilize all the recently developed machinery from the standard observational
studies. In particular, we generalize the doubly-robust estimator and prove its consistency and
asymptotic normality under common high-level assumptions. We also show that the standard
fixed effects estimation is a particular case of our procedure.
    As a direction for future research, it will be interesting to see whether it is possible to
utilize machine learning methods to learn sufficient statistics from the data. Additionally, it is
essential to understand the statistical trade-off between the dimension of the sufficient statistic,
cluster size and estimation rate for the propensity score. Finally, we view this work as a first
step towards understanding a more challenging and arguably more practically important data
design, where we observe panel data.




                                                24
References
A Abadie and MD Cattaneo. Econometric methods for program evaluation. Annual Review of
  Economics, 18, 2018.

Alberto Abadie. Semiparametric difference-in-differences estimators. The Review of Economic
  Studies, 72(1):1–19, 2005.

Alberto Abadie and Guido W Imbens.            Matching on the estimated propensity score.
  Econometrica, 84(2):781–807, 2016.

Alberto Abadie, Alexis Diamond, and Jens Hainmueller. Synthetic control methods for com-
  parative case studies: Estimating the effect of California’s tobacco control program. Journal
  of the American Statistical Association, 105(490):493–505, 2010.

Joseph G Altonji and Richard K Mansfield. Group-average observables as controls for sorting on
  unobservables when estimating group treatment effects: The case of school and neighborhood
  effects. Technical report, National Bureau of Economic Research, 2014.

Joshua Angrist and Steve Pischke. Mostly Harmless Econometrics: An Empiricists’ Companion.
  Princeton University Press, 2008.

Manuel Arellano. Panel data econometrics. Oxford university press, 2003.

Manuel Arellano and Jinyong Hahn. A likelihood-based approximate solution to the incidental
  parameter problem in dynamic nonlinear models with multiple effects. Technical report,
  CEMFI, 2006.

Manuel Arellano and Bo Honoré. Panel data models: some recent developments. Handbook of
  econometrics, 5:3229–3296, 2001.

Susan Athey and Guido Imbens. Design-based analysis in difference-in-differences settings with
  staggered adoption. 2018.

Susan Athey, Guido Imbens, and Stefan Wager. Efficient inference of average treatment effects in
  high dimensions via approximate residual balancing. arXiv preprint arXiv:1604.07125, 2016.



                                              25
Susan Athey, Mohsen Bayati, Nikolay Doudchenko, Guido Imbens, and Khashayar Khosravi.
  Matrix completion methods for causal panel data models. arXiv preprint arXiv:1710.10251,
  2017.

Jushan Bai and Serena Ng. Determining the number of factors in approximate factor models.
  Econometrica, 70(1):191–221, 2002.

Jushan Bai and Serena Ng. Principal components and regularized estimation of factor models.
  arXiv preprint arXiv:1708.08137, 2017.

Marianne Bertrand, Esther Duflo, and Sendhil Mullainathan. How much should we trust
  differences-in-differences estimates? The Quarterly Journal of Economics, 119(1):249–275,
  2004.

Peter Bickel, Chris Klaassen, Yakov Ritov, and Jon Wellner. Efficient and adaptive estimation
  for semiparametric models. 1998.

Stéphane Bonhomme. Functional differencing. Econometrica, 80(4):1337–1385, 2012.

Stéphane Bonhomme and Elena Manresa. Grouped patterns of heterogeneity in panel data.
  Econometrica, 83(3):1147–1184, 2015.

Stéphane Bonhomme, Thibaut Lamadon, and Elena Manresa. Discretizing unobserved hetero-
  geneity. Technical report, IFS Working Papers, 2017.

Kirill Borusyak and Xavier Jaravel. Revisiting event study designs. 2016.

Gary Chamberlain. Panel data. Handbook of econometrics, 2:1247–1318, 1984.

Victor Chernozhukov, Iván Fernández-Val, Jinyong Hahn, and Whitney Newey. Average and
  quantile effects in nonseparable panel models. Econometrica, 81(2):535–580, 2013.

Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whit-
  ney Newey, and Robins James. Double machine learning for treatment and causal parameters.
  arXiv preprint arXiv:1608.00060, 2016.

Richard K Crump, V Joseph Hotz, Guido W Imbens, and Oscar A Mitnik. Dealing with limited
  overlap in estimation of average treatment effects. Biometrika, pages 187–199, 2009.

                                             26
Jishnu Das, Alaka Holla, and Aakash Mohpal. Quality and accountability in health care delivery:
  Audit-study evidence from primary care in india. THE AMERICAN ECONOMIC REVIEW,
  106(12):3765–3799, 2016.

Clément De Chaisemartin and Xavier DHaultfŒuille. Fuzzy differences-in-differences. The
  Review of Economic Studies, 85(2):999–1028, 2017.

Clement de Chaisemartin and Xavier D’Haultfœuille. Two-way fixed effects estimators with
  heterogeneous treatment effects. 2018.

Max H Farrell. Robust inference on average treatment effects with possibly more covariates
  than observations. Journal of Econometrics, 189(1):1–23, 2015.

Jinyong Hahn and Whitney Newey. Jackknife and analytical bias reduction for nonlinear panel
  models. Econometrica, 72(4):1295–1319, 2004.

Keisuke Hirano, Guido W Imbens, and Geert Ridder. Efficient estimation of average treatment
  effects using the estimated propensity score. Econometrica, 71(4):1161–1189, 2003.

Cheng Hsiao. Analysis of panel data. Number 54. Cambridge university press, 2014.

Cheng Hsiao, H Steve Ching, and Shui Ki Wan. A panel data approach for program evaluation:
  measuring the benefits of political and economic integration of hong kong with mainland
  china. Journal of Applied Econometrics, 27(5):705–740, 2012.

Kosuke Imai and In Song Kim. When Should We Use Linear Fixed Effects Regression Models for
  Causal Inference with Longitudinal Data? PhD thesis, Working paper, Princeton University,
  Princeton, NJ, 2016.

Guido Imbens and Jeffrey Wooldridge. Recent developments in the econometrics of program
  evaluation. Journal of Economic Literature, 47(1):5–86, 2009.

Guido W Imbens and Donald B Rubin. Causal Inference in Statistics, Social, and Biomedical
  Sciences. Cambridge University Press, 2015.

Stephen L Morgan and Christopher Winship. Counterfactuals and causal inference. Cambridge
  University Press, 2014.


                                              27
Yair Mundlak. On the pooling of time series and cross section data. Econometrica: journal of
  the Econometric Society, pages 69–85, 1978.

Jerzy Neyman and Elizabeth L Scott. Consistent estimates based on partially consistent obser-
  vations. Econometrica: Journal of the Econometric Society, pages 1–32, 1948.

Judea Pearl. Causality: Models, Reasoning, and Inference. Cambridge University Press, New
  York, NY, USA, 2000. ISBN 0-521-77362-8.

M Hashem Pesaran. Estimation and inference in large heterogeneous panels with a multifactor
  error structure. Econometrica, 74(4):967–1012, 2006.

James Robins and Andrea Rotnitzky. Semiparametric efficiency in multivariate regression mod-
  els with missing data. Journal of the American Statistical Association, 90(1):122–129, 1995.

Paul R Rosenbaum and Donald B Rubin. The central role of the propensity score in observational
  studies for causal effects. Biometrika, 70(1):41–55, 1983.

Jeffrey M Wooldridge. Econometric analysis of cross section and panel data. MIT press, 2010.

Yiqing Xu. Generalized synthetic control method: Causal inference with interactive fixed effects
  models. Political Analysis, 25(1):57–76, 2017.

Shu Yang. Propensity score weighting for causal inference with multi-stage clustered data. arXiv
  preprint arXiv:1607.07521, 2016.




                                               28
A      Identification results
First, we need to formally define Pc . For this fix an arbitrary linear order % on X × {0, 1} (e.g.,
a lexicographic order). For any cluster c consider a tuple Ac = {(Xi , Wi )}i∈c , order elements
of Ac with respect to % and define Pc = (X(1) , W(1) ), . . . , (X(c) , W(c) ) ∈ (X × {0, 1})c . Under
                                                                              

Assumption 3.1 this construction ensures that Pc is a well-defined random vector. It is clear that
there is a one-to-one relationship between this vector and the empirical distribution of (Xi , Wi )
within the cluster which makes the notation appropriate.
    Below we will use the following definition of conditional independence. Let X, Y, Z be three
random elements and A, B be the elements of the σ(X)- and σ(Y )-algebras, respectively. The
X ⊥⊥ Y |Z if the following holds:


      E[{X ∈ A}{Y ∈ B}|Z] = E[{X ∈ A}|Z]E[{Y ∈ B}|Z]                                                     (1.1)


In the proofs below we are using A and B as generic elements of the appropriate σ-algebras,
without explicitly specifying them.
    We start stating several lemmas that are important for the first identification result (Propo-
sition 1). The first lemma says that given the (Xi , Wi , Ui ) other covariates cannot help in
predicting (Yi (0), Yi (1)).

Lemma A1. (Statistical exclusion) Under Assumptions 3.1, 3.3 the following is true:


      (Yi (1), Yi (0)) ⊥⊥ {(PCj , Xj , Wj )}N
                                            i=1 |Xi , Wi , Ui                                            (1.2)


Proof. From the repeated application of the iterated expectations and Assumptions 3.1, 3.3 we
have the following:


      E[{(Yi (1), Yi (0)) ∈ A}{{(PCj , Xj , Wj )}N
                                                 j=1 ∈ B}|Xi , Wi , Ui ] =

      E[E[{(Yi (1), Yi (0)) ∈ A}{{(PCj , Xj , Wj )}N                            n
                                                   j=1 ∈ B}|{Xi , Wi , Ui , Ci }i=1 ]|Xi , Wi , Ui ] =

      E[{{(PCj , Xj , Wj )}N                                                    n
                           j=1 ∈ B}E[{(Yi (1), Yi (0)) ∈ A}|{Xi , Wi , Ui , Ci }i=1 ]|Xi , Wi , Ui ] =

      E[{{(PCj , Xj , Wj )}N
                           j=1 ∈ B}E[{(Yi (1), Yi (0)) ∈ A}|Xi , Wi , Ui ]|Xi , Wi , Ui ] =

                   E[{{(PCj , Xj , Wj )}N
                                        j=1 ∈ B}|Xi , Wi , Ui ]E[{(Yi (1), Yi (0)) ∈ A}|Xi , Wi , Ui ] (1.3)



                                                       29
Equality between the first and the last expression implies the independence result.

   The second lemma states that only PCi are useful in predicting Ui .

Lemma A2. (Statistical sufficiency) Under Assumption 3.1 the following holds:


     Ui ⊥⊥ {Wj , Xj }N
                     j=1 |PCi                                                                            (1.4)


Proof. The proof follows from the following equalities:


     E[{Ui ∈ A}{{Wj , Xj }N
                          j=1 ∈ B}|PCi ] =

     E E[{Ui ∈ A}{{Wj , Xj }N                             N                  N
                                                                                        
                             j=1 ∈ B}|PCi , {Wj , Xj }j=1 , {Cj = Ci }j=1 ]|PCi =

     E {{Wj , Xj }N                                       N                  N
                                                                                        
                  j=1 ∈ B}E[{U i ∈ A}|P  Ci
                                            , {W j , Xj } j=1 , {C j = C i } j=1 ]|P C i
                                                                                           =

     E {{Wj , Xj }N
                                                                      
                  j=1 ∈ B}E[{Ui ∈ A}|PCi , {Wj , Xj }j:Cj =Ci ]|PCi =

     E {{Wj , Xj }N
                                                 
                  j=1 ∈ B}E[{Ui ∈ A}|PCi ]|PCi =

                                              E {{Wj , Xj }N
                                                                                
                                                                 j=1 ∈ B}|PCi E [{Ui ∈ A}|PCi ] (1.5)



The third equality holds by random sampling (observations in different clusters are independent),
the fourth equality holds by exchangeability of data within the cluster.

Proof of Proposition 1: We start with the following equalities:


     E[{(Yi (1), Yi (0)) ∈ A}|Wi , Xi , PCi ] =

     E[E[{(Yi (1), Yi (0)) ∈ A}|Wi , Xi , Pn,Ci , Ui ]|Wi , Xi , PCi ] =

                                                E[E[{(Yi (1), Yi (0)) ∈ A}|Wi , Xi , Ui ]|Wi , Xi , PCi ] (1.6)




                                                      30
The last equality follows from Lemma A1. As a next step we have the following result:


     E[E[{(Yi (1), Yi (0)) ∈ A}|Wi , Xi , Ui ]|Wi , Xi , PCi ] =

     E[E[{(Yi (1), Yi (0)) ∈ A}|Xi , Ui ]|Wi , Xi , PCi ] =

     E[E[{(Yi (1), Yi (0)) ∈ A}|Xi , Ui ]|Xi , PCi ] =

          E[E[{(Yi (1), Yi (0)) ∈ A}|Xi , PCi , Ui ]|Xi , PCi ] = E[{(Yi (1), Yi (0)) ∈ A}|Xi , Pn,Ci ] (1.7)


The first equality follows directly from Assumption 3.2, the second equality follows from Lemma
A2. Combining the two chains of equalities we get the following:


     E[{(Yi (1), Yi (0)) ∈ A}|Wi , Xi , PCi ] = E[{(Yi (1), Yi (0)) ∈ A}|Xi , PCi ]                    (1.8)


which proves the conditional independence.                                                                

Corollary A1. (Exclusion in exponential families) Under the assumptions of Lemma
A1 the following is true:


     (Yi (1), Yi (0)) ⊥⊥ {(S Cj , Xj , Wj )}N
                                            j=1 |Xi , Wi , Ui                                          (1.9)


Proof. Because SCi is a function of PCi the result follows from Lemma A1.

Lemma A3. (Sufficiency in exponential families) Under Assumptions 3.1 and 3.4 the
following holds:


     Ui ⊥⊥ {Wj , Xj }N
                     j=1 |S Ci                                                                        (1.10)


Proof. The proof is exactly the same as in Lemma A2 with SCi used instead of Pn,Ci . The fourth
equality now holds directly by the exponential family assumption.

Proof of Theorem 1: The same as for Proposition 1, use Corollary A1 and Lemma A3 instead
of Lemmas A1 and A2.                                                                                      




                                                      31
Corollary A2. For any function f such that E[|f (Y (k))|] < ∞ the following is true:


     E[f (Yi )|{Wj , Xj , S Cj }N
                                j=1 ] =

                                 {Wi = 0}E[f (Yi (0))|Xi , S Ci ] + {Wi = 1}E[f (Yi (1))|Xi , S Ci ] (1.11)


Proof. The proof follows from the following equalities:


     E[f (Yi )|{Wj , Xj , S Cj }N                                    N                           N
                                j=1 ] = E[E[f (Yi )|{Wj , Xj , S Cj }j=1 , Ui ]|{Wj , Xj , S Cj }j=1 ] =

     E[E[f (Yi )|Wi , Xi , Ui ]|{Wj , Xj , S Cj }N
                                                 j=1 ] = E[f (Yi )|Wi , Xi , S Ci ] =

                                 {Wi = 0}E[f (Yi (0))|Xi , S Ci ] + {Wi = 1}E[f (Yi (1))|Xi , S Ci ] (1.12)


where the third equality follows from Corollary A1, the fourth from Lemma A3 and the final
one from Proposition 1.




                                                        32
B      Inference results
Notation: We are using standard notation from the empirical processes literature adapted to
our setting. For any cluster-level random vector Xc : Pn (Xc ) := n1 nc=1 Xc and Gn (Xc ) :=
                                                                        P
√
  n (Pn (Xc ) − E[Xc ]). Define Bi = (Xi , S Ci ) and Di := (Wi , Bi ).


B.1     Linear case
We have the standard linear projection expansion:

                        n
                                                          !−1            n
                   1 X X                                            1 X X
      β̂ls =                      Di DiT                                           Yi Di =
                 n × c c=1 i:C =c                                 n × c c=1 i:C =c
                                         i                                        i

                                n
                                                          !−1               n
                  1             X     X                             1       X    X
      β+                                        Di DiT                                    Di εi =
                n × |c| c=1 i:Ci =c
                                                                  n × |c|   c=1 i:Ci =c
                                  n                                          
                     T −1 1                        1 X                     1
                               X
      β+       E[Di Di ]                                     D i εi + o p √     =
                             n c=1                |c| i:C =c                n
                                                              i
                                                                                                              n                     
                                                                                                     −1   1X                   1
                                                                                 β+       E[Di DiT ]             φc + op       √         (2.1)
                                                                                                           n c=1                 n

                 1
                      P
where φc :=     |c|        i:Ci =c   Di εi . This implies consistency and asymptotic normality.
    For the variance of τ̂ls we have the following expression:
                                                     2 
                           1
                                P
                E         |c|       i:Ci =c   u i εi
      Vls =                                                                                                                              (2.2)
                                E[u2i ]

We are using the plug-in estimator:
                                                 2
                           1
                                P
                Pn        |c|    i:Ci =c   ûi ε̂
      V̂ls =                1
                              P               2
                                                                                                                                        (2.3)
                    Pn      c       i:Ci =c ûi




                                                                            33
By construction we have:
     
     ûi = ui + (γ − γ̂)T Bi
     
                       T                                                                                                     (2.4)
     ε̂i = εi + β − β̂ Di
     


For the numerator we have the following

                                !2                                   !2
            1 X                                  1 X
     Pn               ûi ε̂i         = Pn                 εi ui           +
           |c| i:C =c                           |c| i:C =c
                   i                                     i
                                  !                                                            !
              1 X                       1 X                 X               X
     2Pn                εi u i                    ui R1i +        εi R2i +        R1i R2i          +
             |c| i:C =c                |c| i:C =c          i:C =c          i:C =c
                       i                        i                      i               i
                                                                                                                          !2
                                                          1 X                 1 X                 1 X
                                                Pn                  ui R1i +            εi R2i +            R1i R2i            (2.5)
                                                         |c| i:C =c          |c| i:C =c          |c| i:C =c
                                                                 i                         i            i



which implies the following bound:

                                  !2                                   !2
              1 X                                    1 X
      Pn                ûi ε̂i        − Pn                    εi ui           ≤
             |c| i:C =c                             |c| i:C =c
                       i                                     i
      v                               !2                                                                         !2
      u
      u          1 X                                 1 X                 X 1                 1 X
     2tPn                  εi ui           Pn                  ui R1i +            εi R2i +            R1i R2i        +
                |c| i:C =c                          |c| i:C =c          i:C =c
                                                                               |c|          |c| i:C =c
                           i                                 i                     i               i
                                                                                                                          !2
                                                          1 X                 1 X                 1 X
                                                Pn                  ui R1i +            εi R2i +            R1i R2i            (2.6)
                                                         |c| i:C =c          |c| i:C =c          |c| i:C =c
                                                                 i                         i            i



   Jensen’s inequality implies the following bounds:
                                   2
             1
                P                               P                  2
                                         ≤
     
     
     
      P n |c|    i:C i =c u i R  1i        P n   i:Ci =c (ui R1i )
     
                                   2
             1
                                         ≤ Pn i:Ci =c (εi R2i )2                                                               (2.7)
                P                               P
      P n  |c|   i:Ci =c  ε i R  2i
     
           P                          2
     P n 1
                                                 P                    2
                           R      R        ≤        i:Ci =c (R1i R2i )
     
            |c|   i:C i =c     1i    2i      P  n




                                                                       34
CS inequality implies:
     
       2
     R1i ≤ kβ̂ − βk22 kDi k22
     
                                                                                                                                           (2.8)
     R2 ≤ kγ̂ − γk2 kBi k2
     
       2i          2      2



This implies the following bound for the averages:
     
           1            2 2                    1
                           ui ≤ kβ̂ − βk22 Pn |c|                  2 2
              P                                   P
                                                     i:Ci =c kDi k2 ui = op (1)
     
     
     
      Pn |c|  i:Ci =c R1i
     
     
           1            2 2             2     1                  2 2                                                                       (2.9)
              P                                   P
      Pn |c|  i:Ci =c R2i εi ≤ kγ̂ − γk2 Pn |c|    i:Ci =c kBi k2 εi = op (1)
     
     
     P n 1                                                  1
             P         2    2             2           2
                                                               P               2      2
               i:Ci =c R1i R2i ≤ kβ̂ − βk2 kγ̂ − γk2 Pn |c|       i:Ci =c kBi k2 kDi k2 = op (1)
     
          |c|


Combining all these together we have the following:

                                       !2                                      !2
                1 X                                         1 X
      Pn                  ûi ε̂i               − Pn                  εi u i            ≤ op (1)                                         (2.10)
               |c| i:C =c                                  |c| i:C =c
                            i                                        i




The same argument implies the following bound:
                                  !                                                                !                         !
            1 X 2                                   1 X 2                     2
                                                                                                              1 X 2
     Pn              û               = Pn                    ui + 2ui R2i + R2i                       = Pn             u        + op (1) (2.11)
           |c| i:C =c i                            |c| i:C =c                                                 |c| i:C =c i
                        i                                    i                                                     i



Finally, combining all bounds together we have the following:
                                          2                                     2
               1                                            1
                    P                                            P
     Pn       |c|       i:Ci =c   ûi ε̂          Pn       |c|    i:Ci =c   ui ε
                                           =                                     + op (1) = V + op (1)                               (2.12)
                1                     2                      1                2
                     P                                            P
      Pn       |c|          i:Ci =c ûi            Pn       |c|      i:Ci =c ui




                                                                               35
Proof of Lemma 1: There are two well known preliminary results we use. First, consider the
two regression functions


       Yi = Xi βX + Zi βZ + εi ,


       Yi = Xi γX + ηi ,

       Zi = Xi ∆X + νi ,

with the corresponding least squares estimators β̂X , β̂Z , γ̂X , and ∆X . Then, the omitted variable
bias formula states that

                   ˆ X β̂Z .
       γ̂X = β̂X + ∆


Second, consider the two regression functions


       Yi = Xi βX + Zi βZ + εi ,


and,


       Yi = (Xi − Zi Γ)βX + Zi βZ + εi .


The least squares estimators for βX based on the two regression functions are identical.
   Next, consider the regression function

              S
              X
       Yi =         αs Sis + Wi τ + Xi β + ε,                                                 (2.13)
              s=1


with the least squares estimators α̂s , β̂ and τ̂ . Because W Si and X Si are linear combinations of
the indicators Sis , it follows by the second preliminary result that the least squares coefficients
(τ̂ , β̂) are the same as those based on the regression function

              S
              X
       Yi =         αs Sis + (Wi − W Si )τ + (Xi − X Si )β + ε,                               (2.14)
              s=1




                                                     36
Because Sis is uncorrelated with Wi − W Si and Xi − X Si it follows by the first preliminary
result that the least squares coefficients (τ̂ , β̂) are also identical to those based on the regression
function


      Yi = (Wi − W Si )τ + (Xi − X Si )β + ε,                                                    (2.15)


Because the means of Wi − W Si and Xi − X Si are zero, it follows by the first preliminary result
we get the same estimates for τ and β if we estimate by least squares the regression function


      Yi = α + (Wi − W Si )τ + (Xi − X Si )β + ε.                                                (2.16)


Because W Si and X Si are both uncorrelated with both Wi − W Si and Xi − X Si it follows by the
first preliminary result that the least squares coefficients (τ̂ , β̂) are also identical to those based
on the regression function


      Yi = α + W Si δ + X Si γ + (Wi − W Si )τ + (Xi − X Si )β + ε.                              (2.17)


Finally, using the second preliminary result again it follows that the least squares estimators for
τ and β are the same if based on the regression function


      Yi = α + W Si δ + X Si γ + Wi τ + Xi β + εi .                                              (2.18)




                                                  37
B.2     Semiparametric case
We start with a reminder on notation:
  
    µ(Di ) := E[Yi |Di ]
  
  
  
  
  
  
  
  
  
  
  
   e(Bi ) := E[Wi |Bi ]
  
  
  
  ε(k) := Yi (k) − µ(k, Bi )
  
                                                                                                           (2.19)
                                                                       w         1−w
    ψ(y, w, x,  s, µ(·), e(·))  :=   µ(1, x, s) − µ(0,   x, s) +            −             (y − µ(w, x, s))
  
  
                                                                     e(x,s)   1−e(x,s)
  
  
  
  
  
                          1
                             P
                                i:Ci =c {Ai }ψ(Yi , Wi , Xi , S Ci , µ(Wi , Xi , S Ci ), e(Xi , S Ci ))
  
  
  
   ρ(c, µ(·), e(·)) := |c|
  
  
                                                      
  ξc := i∈c 1 {Ai }             Wi            1−Wi
         P
  
                 Nc           e(X ,S )
                                         −  1−e(X ,S )
                                                          (Yi − µ(Wi , Xi , S Ci ))
                                 i   Ci          i   Ci



In order to prove Theorem 2 we consider a more general case that allows for misspecification.
First we prove Lemma B4 which states that we get identification if either the propensity score
or the conditional mean is potentially misspecified. Then we prove Proposition B1 which is a
general consistency result under possible misspecification. Theorem 2 follows as a special case.
After that we prove Theorem 3 and Proposition 2.




                                                          38
Lemma B4. Assume that at least one of the following statements is true:
      
      µ̃(Wi , Xi , S Ci ) = µ(Wi , Xi , S Ci )
      
                                                                                                          (2.20)
      ẽ(Xi , S C ) = e(Xi , S C )
      
                  i              i




If the assumptions of Theorem 1 hold then we have the following result:
                          "                   #
                          X 1
      E[ρ(c, m̃, ẽ)] = E         {Ai }τ (Bi )                                                            (2.21)
                          i∈c
                              |c|

where τ (Bi ) := µ(1, Bi ) − µ(0, Bi ).

Proof. By construction we have the following:

                          "                                                                                #
                            X 1                                         Wi         1 − Wi
      E[ρ(c, µ̃, ẽ)] = E           {Ai } µ̃(1, Bi ) − µ̃(0, Bi ) +            −                (Yi − µ̃(Di ))   =
                            i∈c
                                |c|                                   ẽ(B i )   1 −  ẽ(Bi )
        "                                        #
         X 1                                       X 1             
                                                                         Wi        1 − Wi
                                                                                                              
      E            {Ai }(µ̃(1, Bi ) − µ̃(0, Bi )) +         E {Ai }            −                 (Yi − µ̃(Di ))
          i∈c
               |c|                                  i∈c
                                                        |c|            ẽ(Bi ) 1 − ẽ(Bi )
                                                                                                           (2.22)

For the second part we have the following (using unconfoundedness):

                                                     
                  Wi        1 − Wi
      E {Ai }          −                (Yi − µ̃(Di )) =
                ẽ(Bi ) 1 − ẽ(Bi )
                                                          
                      Wi        1 − Wi
      E E {Ai }             −                (Yi − µ̃(Di ))|Bi =
                    ẽ(Bi ) 1 − e(B̃i )
                                                                                              
                           e(Bi ) (µ(1, Bi ) − µ̃(1, Bi )) (1 − e(Bi ) (µ(0, Bi ) − µ̃(0, Bi ))
              E {Ai }                                     −                                        (2.23)
                                      ẽ(Bi )                          1 − ẽ(Bi )
                                                                                            hP                          i
                                                                                                      1
This implies that if either µ̃(Di ) = µ(Di ) or ẽ(Bi ) = e(Bi ) then E[ρ(c, m̃, ẽ)] = E        i∈c |c| {Ai }τ (Bi )    .




                                                     39
Proposition B1. (Consistency with wrong specifications) Assume that the following
                     ˆ µ̃):
conditions hold for (ẽ, ˆ

                                                  2 
                     1         ˆ Bi ) − µ̃(1, Bi )
           P
                        {Ai }  µ̃(1,                       = op (1)
     
     
     
      Pn   i∈c     |c|
     
                                                 
     
                                              2
                     1         ˆ
           P
                        {Ai } ẽ(Bi ) − ẽ(Bi )
     
     
     
      Pn   i∈c     |c|
                                                     = op (1)
     
     
       η < ẽ(Bi ) < 1 − η a.s.                                                                            (2.24)
     
     
     
     
            ˆ i ) < 1 − η a.s.
     
       η < ẽ(B
     
     
     
     
     
     
     
     E[ε̃2 (k)] < ∞
     
          i



where ε̃i (k) : Yi (k) − µ̃(k, Bi ). Additionally assume that the conditions of Lemma B4 hold. Then
we have the following:


     Pn ρ(c, µ̃, ẽ) = Pn ρ(c, µ̃, ẽ) + op (1) = E[ρ(c, µ̃, ẽ)] + op (1)                                 (2.25)


Proof. To prove the consistency result we need to separate the functional into two parts:

                     X 1                                                  
                                                   Wi
     ρ(c, µ̃, ẽ) =          {Ai } µ̃(1, Bi ) +           (Yi − µ̃(1, Bi )) −
                     i∈c
                         |c|                     ẽ(B i )
                                                                          
               X 1                           1 − Wi
                        {Ai } µ̃(0, Bi ) +               (Yi − µ̃(0, Bi )) = ρ1 (c, µ̃, ẽ) − ρ0 (c, µ̃, ẽ) (2.26)
                i∈c
                    |c|                    1  − ẽ(B i )

In what follows we are working only with the first part of the functional, the second can be
analyzed in the exactly the same way. Define the empirical version:

                     X 1                                      
            ˆ ẽ)
                ˆ :=                ˆ Bi ) +   W i      ˆ Bi ))
     ρ1 (c, µ̃,              {Ai } µ̃(1,           (Y − µ̃(1,                                              (2.27)
                         |c|                 ˆ i) i
                                             ẽ(B
                     i∈c




                                                        40
We can decompose this expression into three parts:

                      X 1                                                    
            ˆ ẽ)
                ˆ =                                   W i
     ρ1 (c, µ̃,                  {Ai } µ̃(1, Bi ) +          (Yi − µ̃(1, Bi )) +
                        i∈c
                             |c|                    ẽ(B i )
         X 1                                                     
                               ˆ Bi ) − µ̃(1, Bi ) 1 −           W i
     +             {Ai } µ̃(1,                                           +
               |c|                                             ˆ i)
                                                               ẽ(B
         i∈c
                    X 1                                                    
                                                              1        1
                             {Ai }(Yi − µ̃(1, Bi ))Wi              −          = ρ1 (c, µ̃, ẽ) + R1c + R2c (2.28)
                         |c|                               ˆ i ) ẽ(Bi )
                                                          ẽ(B
                    i∈c


The result will follow once we prove two approximations:
     
     Pn R1c = op (1)
     
                                                                                                         (2.29)
     Pn R2c = op (1)
     


We start with the second one. Observe that we have the following:

                                                                          
                                 X 1                         {Ai }Wi                         ˆ i) ≤
      |Pn R2c | ≤ Pn |R2c | ≤ Pn          {Ai }|ε̃i (1)|                     {Ai } ẽ(Bi ) − ẽ(B
                                      |c|                  ẽ(Bi ) ˆ
                                                                  ẽ(B i )
                                 i∈c
                          s X                          s
                {Ai }Wi                 1          2
                                                               X 1            
                                                                                          ˆ
                                                                                                2
      max                      Pn          {Ai }ε̃i (1) Pn               {Ai } ẽ(Bi ) − ẽ(Bi ) =
        i            ˆ i)
              ẽ(Bi )ẽ(B              |c|                           |c|
                                  i∈c                          i∈c
                                                                              q        q
                                                                       Op (1) Op (1) op (1) = op (1) (2.30)


For the first term we have the following:

             X 1                                             
                              ˆ Bi ) − µ̃(1, Bi )            W i
     R1c =            {Ai } µ̃(1,                     1−            =
                  |c|                                      ˆ i)
                                                           ẽ(B
              i∈c
     X 1                                               
                        ˆ Bi ) − µ̃(1, Bi )          W i
              {Ai } µ̃(1,                      1−              +
      i∈c
          |c|                                      ẽ(Bi )
                                                                                   !!
                X 1                                            ˆ i ) − ẽ(Bi )
                                                                 ẽ(B
                        {Ai }   ˆ
                                µ̃(1, Bi ) − µ̃(1, Bi ) Wi                              = R11c + R12c (2.31)
                    |c|                                                   ˆ i)
                                                                   ẽ(Bi )ẽ(B
                i∈c




                                                      41
The first part can be bounded in the following way:


      |Pn R11c | ≤ Pn |R11c | ≤
                                v                                         !
                                u
          {Ai }(Wi − ẽ(Bi ))   u    X 1          
                                                    ˆ Bi ) − µ̃(1, Bi )
                                                                       2
      max                     × tP n         {Ai } m̃(1,                    =
       i        ẽ(Bi )              i∈c
                                         |c|

                                                                    Op (1) × op (1) = op (1) (2.32)


   The second part can be bounded in the following way:

                                                   
                                        {Ai }Wi
     |Pn R12c | ≤ Pn |R12c | ≤ max                  ×
                                  i          ˆ i)
                                      ẽ(Bi )ẽ(B
     v                                             !v                                      !
     u                                           2 u
     u      X 1                                    u    X 1                           2
     tPn                     ˆ Bi ) − µ̃(1, Bi )
                      {Ai } µ̃(1,                   tP n                ˆ i ) − ẽ(Bi )
                                                                 {Ai } ẽ(B                  =
              i∈c
                  |c|                                    i∈c
                                                             |c|

                                                             Op (1) × op (1) op (1) = op (1) (2.33)


Combining all the results together we have the proof.




                                                    42
Proof of Theorem 2: Observe that ê and µ̂ satisfy the assumptions of Proposition B1 with
µ̃ and ẽ equal to m and e. As a result, combining Proposition B1 and Lemma B4 we get the
following:

         1                       1
            Pn ρ(c, µ̂, ê) =       (E[ρ(c, µ, e)] + op (1)) =
      π̂(A)                   π̂(A)
                         
           1                                             1
               + op (1) (E[ρ(c, µ, e)] + op (1)) =           E[ρ(c, µ, e)] + op (1) =
         π(A)                                         π(A)
                                                                    1
                                                                        E[{Ai }τ (Xi , S Ci )] + op (1) (2.34)
                                                                 π(A)




                                                     43
Proof of Theorem 3: The start of the argument is the same as in proof for the consistency
result. We decompose the empirical version of ρ1 (c, m̂, ê):

                       X 1                      X 1                                     
                                                                   Wi
      ρ1 (c, m̂, ẽ) −         {Ai }µ(1, Bi ) =         {Ai }            (Yi − µ(1, Bi )) +
                       i∈c
                           |c|                  i∈c
                                                    |c|           e(Bi )
          X 1                                                  
                                                           Wi
      +            {Ai } (µ̂(1, Bi ) − µ(1, Bi )) 1 −                  +
          i∈c
               |c|                                       ê(Bi )
                           X 1                                                    
                                                                   1         1
                                    {Ai }(Yi − µ(1, Bi ))Wi              −           = ξ1c + R1c + R2c (2.35)
                           i∈c
                                |c|                             ê(B i )   e(B i )

The result will follow once we prove the following:
                  
      Pn R1c = op √1
      
                     n
                                                                                                    (2.36)
      Pn R2c = op √1
      
                     n


In exactly the same way as before we can decompose R1c into R11c and R12c . For R12c we have
the following:

                                                       
                                            {Ai }Wi
      |Pn R12c | ≤ Pn |R12c | ≤ max                       ×
                                    i     e(Bi )ê(Bi )
      v                                                !v                                      !
      u                                                  u
      u      X 1                                     2
                                                         u    X 1
      tPn              {Ai } (µ̂(1, Bi ) − µ(1, Bi )) tPn             {Ai } (ê(Bi ) − e(Bi ))2 =
               i∈c
                   |c|                                        i∈c
                                                                  |c|
                                                                                               
                                                                                1              1
                                                               Op (1) × op √          = op √        (2.37)
                                                                                 n              n




                                                    44
For R11c we use the following argument:

                                                                                                                   2
                                                                                                                  
                                                                                                     
                                   X       X      1  X      1                                               Wi  
     E (Pn R11c )2 = E 
                                                                                                
                                                                µ̂−l(c) (1, Bi ) − µ(1, Bi ) 1 −                       ≤
                                    l∈L c:l(c)=l
                                                  n i∈c |c|                                                e(Bi )

                                                                                                  2
                                                                                                   
                        X 1X 1                                                 
         X                                                                               Wi  
     |L|     E                              µ̂−l(c) (1, Bi ) − µ(1, Bi ) 1 −                           =
         l∈L
                               n  i∈c
                                        |c|                                             e(B  i )
                      c:l(c)=l
                                                                                                        
         X X 1                                                                                      !2
                               1    X      1                                               Wi
     |L|                  E                    µ̂−l(c) (1, Bi ) − µ(1, Bi ) 1 −                         ≤
         l∈L c:l(c)=l
                       n       n     i∈c
                                          |c|                                              e(B   i )
                            "                                                                     2 #
         X X 1                 1X 1                                                        Wi
     |L|                  E                     µ̂−l(c) (1, Bi ) − µ(1, Bi ) 1 −                          =
         l∈L c:l(c)=l
                       n       n i∈c |c|                                                  e(Bi )
                            "                                                                             #
         X X 1                 1X 1                                            2 e(Bi )(1 − e(Bi )
     |L|                  E                     µ̂−l(c) (1, Bi ) − µ(1, Bi )                 2 (B )
                                                                                                                  ≤
         l∈L c:l(c)=l
                       n       n i∈c
                                       |c|                                                  e        i
                                                              "                                                     !#
                                                         1            X 1                                        2
                                                      K E Pn                      µ̂−l(c) (1, Bi ) − µ(1, Bi )         (2.38)
                                                         n            i∈c
                                                                            |c|

Using this we get the following:

                     q 
     E [|Pn R11c |] ≤ E (Pn R11c )2 ≤
                                   
                                  "                                          !#       
                            K         X 1                                 2        1
                           √ E Pn             µ̂−l(c) (1, Bi ) − µ(1, Bi )      =o √     (2.39)
                             n        i∈c
                                          |c|                                        n

                                                                                 
This implies (by Markov’s inequality) that Pn R11c = op                     √1
                                                                              n


                                                              
                           X 1                   {Ai }Wi
        2
     E[R2c |{Di }N
                 i=1 ]   ≤            2
                                   E[εi |Di ] 2         2
                                                                 {Ai } (e(Bi ) − ê(Bi ))2 ≤
                           i∈c
                               |c|             e (Bi )ê (Bi )
                                                {Ai }E[ε2i |Di ]Wi X 1
                                                                  
                                         max       2
                                                                               {Ai } (e(Bi ) − ê(Bi ))2 (2.40)
                                          i       e (Bi )ê(Bi )       i∈c
                                                                           |c|

We also have the following:


     E[R2c |{Di }N
                 i=1 ] = 0                                                                                           (2.41)


                                                           45
Using these two things we get the following:

                                            {Ai }E[ε2i |Di ]Wi
                                                              
                2
      E[(Pn R2c )   |{Di }N
                          i=1 ]   ≤ max                           ×
                                      i       e2 (Bi )ê(Bi )
                                                                                           
                                   1 X 1                                 2          1        1
                                     Pn          {Ai } (e(Bi ) − ê(Bi )) ≤ K × op     = op     (2.42)
                                   n    i∈c
                                             |c|                                    n        n

                                                                                                                
                                  2       1                                                                √1
                                                               2
This implies that E[(Pn R2c ) ] = o       n
                                                  (because (ê−e) is bounded by 1) and thus R2c = op         n
                                                                                                                     .




                                                          46
Proof of Proposition 2: Similarly to all other proofs we can divide ξc into two parts ξ1c and
ξ0c . We will analyze ξ1c , analysis for ξ0c is the same. We have the following decomposition:

                  X 1                                            
                                                           W i
     ξˆ1c − ξ1c =         {Ai } (µ(1, Bi ) − µ̂(1, Bi ))             +
                  i∈c
                      |c|                                ê(B i )
                            X 1                                                 
                                                                     1      1
                                     {Ai }(Yi − µ(1, Bi ))Wi             −         = R11c + R12c (2.43)
                             i∈c
                                 |c|                              ê(Bi ) e(Bi )

For the first term we have the following bound:

                                                                
         2        1X                                  2    Wi
     Pn R11c≤ Pn        {Ai } (µ(1, Bi ) − µ̂(1, Bi )) 2           ≤
                  c i∈c                                 ê (Bi )
                                                                     !
             {Ai }Wi            1X                                   2
         max 2           × Pn         {Ai } (µ(1, Bi ) − µ̂(1, Bi )) = Op (1)op (1) = op (1) (2.44)
          i   ê (Bi )          c i∈c

   For the second term we have the following bound:


         2      1X                     2    (ê(Bi ) − e(Bi ))2
          ≤ Pn
     Pn R12c           {Ai }{Wi }εi (1) 2                         ≤
                c i∈c                          ê (Bi )e2 (Bi )
     v                                               !                             !
     u
          1 X         {A   }(ê(B    ) − e(B     ))4        1
                         i         i           i
     u                                                         X
     t Pn       {Ai }                                    Pn       {Ai }{Wi }ε4i (1) ≤
          c i∈c             ê4 (Bi )e4 (Bi )               c i∈c
       v                                        !                                !
       u
       u    1 X                                          1 X
     K t Pn       {Ai }(ê(Bi ) − e(Bi ))2            Pn        {Ai }{Wi }ε4i (1) =
            c i∈c                                        c i∈c

                                                                              op (1)Op (1) = op (1) (2.45)


   Putting these results together we have the following:


     Pn (ξˆ1c + ξˆ2c )2 − Pn (ξ1c + ξ2c )2 = Pn (ξ1c + ξ2c + R11c + R12c + R01c + R02c )2 − Pn (ξ1c + ξ2c )2 =

     Pn (ξ1c + ξ2c )(R11c + R12c + R01c + R02c ) + Pn (R11c + R12c + R01c + R02c )2 ≤
     q
                              2      2      2       2           2      2       2       2
       Pn (ξ1c + ξ2c )2 4Pn (R11c + R12c + R01c + R02c ) + Pn (R11c + R12c + R01c  + R02c )=
                                                               q
                                                                Op (1)op (1) + op (1) = op (1) (2.46)



                                                    47
This argument also implies that Pn (ξˆ1c ) = Pn (ξ1c ) = op (1) and thus we have the final result:

                                                   2 
           1         ˆ     ˆ   2
                                  
                                        ˆ     ˆ                 1
         2
                Pn (ξ1c + ξ2c ) − Pn (ξ1c + ξ2c )        − 2       Pn (ξ1c + ξ2c )2 =
      π̂ (A)                                                 π (A)
           1                                        1               1
                                                                           
                    ˆ     ˆ    2                 2
                Pn (ξ1c + ξ2c ) − Pn (ξ1c + ξ2c ) +               −          Pn (ξ1c + ξ2c )2 + Op (1)op (1) =
      π̂ 2 (A)                                           π̂ 2 (A) π 2 (A)
                                               Op (1)op (1) + op (1)Op (1) + Op (1)op (1) = op (1) (2.47)




                                                     48
       Table 1: Distribution of cluster sizes

Min.    1st Qu.    Median    Mean      3rd Qu.   Max.
1          6         9        9.6         13      24



                Table 2: Logit model

                            Estimate    s.e.

          Intercept          -2.60      2.64

          age                 0.03      0.01

          gender             -1.12      0.46

          W                   5.17      5.01

          age                -0.02      0.03

          (age × W )          0.00      0.09

          gender              0.50      2.28

          W × gender          0.71      3.93




                        49
            Table 3: Estimates of causal effect

               τ̂f e        τ̂ols        τ̂f e (A)         τ̂ols (A)     τ̂dr (A)
     Est.     -1.62         -1.60         -1.60               -1.55          -1.44
     s.e.      0.23         0.25          0.24                0.25           0.30




       Table 4: Parameters for the simulation

     π(Ui )       µ(0, Ui ) µ(1, Ui ) α(Ui )                          β(Ui ) τ (Ui )
0     0.4              -1                2               2.59          1             4
1     0.6              1                -2               2.71          -1            6




       Table 5: Simulation results: full sample

                                 τ            τ̂ f e     τ̂ ols       dr
                                                                   τ̂logit
              Mean 5.00 7.53 5.23                                  5.16
              SD   0.08 0.15 0.18                                  0.18
              RMSE      2.53 0.28                                  0.24




    Table 6: Simulation results: restricted sample

                                                      ols−ad       ols−ad        ols−ad
              τ         τ̂ f e       τ̂ ols        τ̂logit        τtree         τrf
Mean 5.00 7.51 5.21                                    5.19        4.87          5.03
SD   0.08 0.15 0.18                                    0.19        0.31          0.19
RMSE      2.51 0.26                                    0.26        0.33          0.17




                                          50
