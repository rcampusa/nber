                              NBER WORKING PAPER SERIES




                                       I DON'T KNOW

                                        Matthew Backus
                                         Andrew Little

                                      Working Paper 24994
                              http://www.nber.org/papers/w24994


                     NATIONAL BUREAU OF ECONOMIC RESEARCH
                              1050 Massachusetts Avenue
                                Cambridge, MA 02138
                                   September 2018




Thanks to Charles Angelucci, Jonathan Bendor, Sylvain Chassang, Wouter Dessein, James
Hollyer, Navin Kartik, Greg Martin, Mallesh Pai, Matthew Mitchell Andrea Prat, Michael Raith,
Daniel Rappaport, Maher Said, Jim Snyder, Joel Sobel, Philipp Strack, and and audiences at
MPSA 2015, EARIE 2017, The 28th International Game Theory Conference, QPEC 2017,
Petralia Workshop 2017, SAET 2017, ESSET 2018, Columbia, Harvard, the Higher School of
Economics, Peking University, and Stanford for thoughtful comments and suggestions. We thank
Alphonse Simon and Brenden Eum for excellent research assistance. All remaining errors are our
own. The views expressed herein are those of the authors and do not necessarily reflect the views
of the National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.

© 2018 by Matthew Backus and Andrew Little. All rights reserved. Short sections of text, not to
exceed two paragraphs, may be quoted without explicit permission provided that full credit,
including © notice, is given to the source.
I Don't Know
Matthew Backus and Andrew Little
NBER Working Paper No. 24994
September 2018
JEL No. D8,D83,L22

                                         ABSTRACT

Experts with reputational concerns, even good ones, are averse to admitting what they don’t
know. This diminishes our trust in experts and, in turn, the role of science in society. We model
the strategic communication of uncertainty, allowing for the salient reality that some questions
are ill-posed or unanswerable. Combined with a new use of Markov sequential equilibrium, our
model sheds new light on old results about the challenge of getting experts to admit uncertainty –
even when it is possible to check predictive success. Moreover, we identify a novel solution:
checking features of the problem itself that only good experts will infer – in particular, whether
the problem is answerable – allows for equilibria where uninformed experts do say “I Don’t
Know.”


Matthew Backus
Graduate School of Business
Columbia University
3022 Broadway, Uris Hall 619
New York, NY 10027
and NBER
matthew.backus@columbia.edu

Andrew Little
University of California at Berkeley
736 Barrows Hall
Berkeley, CA 94720
andrew.little@berkeley.edu
          [...] it is in the admission of ignorance and the admission of uncertainty that
          there is a hope for the continuous motion of human beings in some direction
          that doesn’t get confined, permanently blocked, as it has so many times before
          in various periods in the history of man.

          — Richard Feynman, John Danz Lecture, 1963



          It seemed to him that a big part of a consultant’s job was to feign total certainty
          about uncertain things. In a job interview with McKinsey, they told him that
          he was not certain enough in his opinions. “And I said it was because I wasn’t
          certain. And they said, ‘We’re billing clients five hundred grand a year, so you
          have to be sure of what you are saying.”’

          — Michael Lewis quoting Daryl Morey in The Undoing Project, 2016




1        Introduction

Executives are experts in the domain of decision-making, but rarely experts in the domains
in which they make decisions. As a result, organizations are built around them to aggregate
expertise – they hire consultants, employ specialists, and take the testimony of experts in
order to make better choices.1 However, Crawford and Sobel (1982) and the large literature
they inspired make clear that experts often have incentives to distort advice, pander to the
decision-makers’ beliefs, and overstate the precision of their information. This limits how
much decision-makers can learn from experts and leads them to make poor decisions.

Here we focus on particularly prickly part of this problem: honest revelation of uncertainty.
Experts frequently don’t know the answer to questions they are asked. This may reflect on
the expert: their ignorance can be driven by a lack of knowledge or incompetence. Al-
ternatively, it may reflect on the question: some questions truly are unanswerable, e.g.
predicting close elections or making price recommendations when demand is unidentified
in the available data. Since incompetent experts are more likely to lack the answers, ex-
pressing uncertainty comes with a reputational cost. In order to prevent uninformed experts
    1
        See Lazear (2005) for the economic formulation of this argument with respect to CEOs.


                                                       1
from feigning knowledge and pushing executives to risky decisions, they must be willing
to say “I don’t know”.

Can experts who care about perceptions of their competence be induced to admit uncer-
tainty? The prior literature is bleak. We contribute by introducing a cheap talk model with
an explicit focus on problem difficulty and showing that it creates room for positive results.
Consistent with prior work, fact-checking experts – with the attendant threat of reputa-
tional consequences when wrong – is never enough to induce honesty. In the language of
our model, checking whether experts are correct is not enough to get them to say “I don’t
know." However, new to our setting, we show that if the decision-maker can learn ex-post
whether the question was well-formulated, then the threat of catching the expert answering
an unanswerable question can make experts willing to admit uncertainty.

A direct and timely implication of our finding concerns fits in a small literature on the man-
agement of experts in organizations (Garicano, 2000). Should they be assigned to project
teams, evaluated by the objective outcomes of A/B tests, or should they be managed by
other experts in semi-independent “research labs"? Our results suggests that, particularly
in the domain of difficult questions, the most effective way to discipline reputational con-
cerns among experts may be to have them managed by other experts. This is consistent
with the recent trend of hiring academic economists to lead research labs in the technol-
ogy sector, an environment with an abundance of new and challenging questions; some
well-posed, and some less so.

The main innovation of the model is an explicit focus on heterogeneity in the difficulty of
problems. We view this as a salient and neglected source of uncertainty for real decision-
makers: often, because of the same inexpertise that requires them to hire experts whose
quality they cannot evaluate, they ask questions that cannot be answered.2 To borrow ter-
minology from the engineering literature on risk management (Kiureghian and Ditlevsen,
2009), our model makes a distinction missing from previous principal-expert models be-
tween epistemic and aleatory uncertainty. What we call “easy” problems are driven by
epistemic uncertainty, which corresponds to problems which are answerable with the right
information; here, by hiring the right expert. “Hard” problems are driven by aleatory un-
certainty, which fundamentally can’t be resolved. Put another way, the DM’s uncertainty
   2
     That a question cannot be answered does not imply that the answer does not exist. An economic consul-
tant may be asked to estimate demand using data in which it is not econometrically identified, or a political
pundit might be asked to predict the outcome of an election. In both cases, an answer exists and may ulti-
mately be revealed, whether by price experiments or simply waiting.


                                                     2
about the difficulty of the problem is uncertainty about what kind of uncertainty he faces.3

To make our contribution clear, we introduce the concept of problem difficulty into a
principal-expert model in the simplest possible fashion. The state of the world, expert
quality, and problem difficulty are all binary.4 Bad experts learn nothing about the state
of the world. Good experts learn the state if and only if the problem is solvable. All ex-
perts then communicate a message to the decision-maker, who takes an action. Finally, the
decision-maker – potentially endowed, ex post, with information about the state or prob-
lem difficulty – forms posterior beliefs about the expert quality. In conjectured honest
equilibria, in which experts reveal their decision-relevant information, off-path beliefs em-
bed threats: if I catch you red-handed in a lie, I will believe that you are a bad expert. But
are such beliefs credible? They are off-path, and so we need more discipline than perfect
Bayesian equilibrium offers – in particular, to account for the simplicity of our binary setup,
we want an equilibrium notion that imposes a notion of robustness on off-path beliefs.

Therefore we study Markov sequential equilibria, a solution concept that combines the
beliefs of sequential equilibrium (Kreps and Wilson, 1982) with the restriction to Markov
strategies.5 Reputational games are a particularly natural setting to study Markov sequential
equilibrium because in such games, the desired behavior is induced by the threat of punitive
beliefs. But threatened beliefs, just like ordinary threats (Selten, 1978), may not be credible.
   3
      It is important to note that this distinction does not stretch the usual theoretical framework for modeling
uncertainty in economics; both aleatory and epistemic uncertainty may be represented with standard infor-
mation partitions. Moreover, not unlike the economic notion of of sunk costs, what is aleatory in one model
might be epistemic in another. It is a typology that is specific to the model under consideration. Here, how-
ever, we show that it is a useful typology with import for the predicting degree of information that can be
communicated in equilibrium.
   It is tempting to suspect that this distinction is related to the precision of expert reports in models such as
Ottaviani and Sørensen (2006a), or accuracy in related and recent work on screening forecasters by Deb et al.
(2018) – it creates variation in the quality of signals. However, there is an important feature that differentiates
difficulty as we have formulated it: it is a property of the problem itself, not the expert or the expert’s signal.
This drives our results. Validating the difficulty of problems generates the key informational wedge between
good uninformed experts (who know the problem difficulty) and bad experts (who do not). The closest to
what we are calling difficulty in the prior literature that we are aware of is the information endowment of
managers in Dye (1985) and Jung and Kwon (1988), in an altogether different setting where shareholders
are uncertain as to the informational endowment of managers. Alternatively, one could interpret difficulty as
concerning whether the question is in the the expert’s “knowledge set," as in Garicano (2000), independent
of the expert’s quality.
    4
      In appendix E we show the main results hold in a version of the model with a continuous type space.
    5
      We will be precise in Section 3, but the novel restriction of Markov sequential equilibrium is that off-path
beliefs must be rationalizable as the limit of a sequence of Markov strategies. Looking for perfect Bayesian
equilibrium or sequential equilibrium in this setting yields what we consider to be unreasonable equilibria
that violate this restriction, although many of the qualitative features of the results still hold. The interested
reader can anticipate a discussion of this at the end of Section 5.



                                                        3
How ought we formalize the notion of credible beliefs? Markov sequential equilibrium
requires that the threatened beliefs be structurally sound, in the following sense: first, that
they be derived from a feasible strategy given the information structure of the game and
second, that they do not update on payoff-irrelevant information. This restriction sheds
light on the simple logic of the negative results concerning honesty that precede us in the
literature: facing an unanswerable question, good experts and bad experts are strategically
equivalent, and so the severity of our beliefs is bounded by the conflation. We show that
this bound implies that honesty is infeasible: in such an equilibrium uninformed experts
can always improve their lot by guessing, and so honesty is never an equilibrium strategy.6

Negative results concerning honesty in reputational games are not new.7 Rather, our main
contributions are twofold. First, to our knowledge ours is the first model to formulate and
exploit the restriction imposed by Markov sequential equilibrium on off-path beliefs. In
Section 4 we show how this casts new light on the structure of the reputational problem.
Second, guided by that insight, we offer new positive results concerning the feasibility of
admitting uncertainty. The restriction imposed my MSE makes clear the symmetry that
we need to break in order to induce honesty: the principal needs to learn something that
allows them to differentially affect the incentives of good experts facing an impossible
question and bad experts. Validating problem difficulty does precisely this – because it is a
property of the question itself, and because it is observable to good experts. It gives them
an opportunity to signal their type, even when uninformed, by correctly predicting whether
questions are answerable. We believe this mechanism may be more general, suggesting an
avenue for future work. Though we have focused on problem difficulty here, in general,
any feature of the problem that the good experts are more likely to observe creates an
informational asymmetry that a properly-informed principal might use to induce desired
behavior.
   6
      Markov sequential equilibrium is an extension of Markov perfect equilibrium (Maskin and Tirole, 2001)
to incomplete information games. It has appeared in Bergemann and Hege (2005) and Bergemann and Hörner
(2010), however to the best of our knowledge, our formulation and application of Markov consistency is
novel, if immediate.
    7
      Prior work has shown that experts may have an incentive to bias and overstate their reports in order
to convince a decision-maker that they are the “good" type (e.g., Prendergast, 1993; Prendergast and Stole,
1996; Brandenburger and Polak, 1996; Morris, 2001; Ottaviani and Sørensen, 2006a). Perhaps the most
common application of this argument in the literature is in models of financial forecasting (Scharfstein and
Stein, 1990; Avery and Chevalier, 1999; Chevalier and Ellison, 1999; Ottaviani and Sørensen, 2006b; Hong
and Kubik, 2003; Bolton et al., 2012). There is also a related literature on the value of transparency in agency
relationships, where more transparency can lead to worse outcomes as reputational incentives are distorted
(Raith and Fingleton, 2005; Prat, 2005). Moreover, in some such models, equilibrium behavior of the bad
types often distorts the incentives of the good, leading to particularly perverse outcomes (Ely and Välimäki,
2003).



                                                       4
Despite the pervasiveness of the phenomenon – from the media’s ineffective efforts to fact-
check electoral candidates, to the decisive confidence of a consultant – reflections on the
difficulty of saying “I don’t know" in economics at large are scant (beyond, of course, a
small reputational literature which we summarize soon).8 When experts “fake it," decision-
makers may be misled into poor business decisions or bad policy choices. Still worse,
trusting in the false precision of their expert reports, they may fail to see the value of
investing in resources for methodical attempts to tackle hard questions, e.g. measuring the
returns to advertising or evaluation of educational interventions. Or perhaps, anticipating
these problems, decision-makers and the public at large learn to discount expert advice
altogether. However, in academia strong reputation concerns seem to induce many experts
to be deliberately circumspect in their claims. We believe that our positive results offer a
perspective on this distinction and a fruitful direction for future work.



Structure. In Sections 2 and 3 we present our model and equilibrium notions, respec-
tively. Section 4 presents the non-technical intuition for our main result that difficulty
validation is necessary for honesty. Sections 5 and 6 offer the technical characterization of
equilibria and the relevant restrictions on the parameter space for the cases without and with
policy concerns, respectively. Next, Section 7 offers comparative statics for our preferred
scenario, with difficulty validation and small policy concerns. Finally, Section 8 offers a
brief discussion of our results. Except where discussed explicitly in the text, all proofs are
presented in Appendix B.
   8
     Levitt and Dubner (2014) argue that we struggle with admitting to ourselves what we don’t know, let
alone to others. Motivating the problem, Steven Levitt observes “I could count on one hand the number of
occasions in which someone in a company, in front of their boss, on a question that they might possibly have
ever been expected to know the answer, has said ‘I don’t know.’ Within the business world there’s a general
view that your job is to be an expert. And no matter how much you have to fake or how much you are making
it up that you just should give an answer and hope for the best afterwards." Freakonomics Podcast – May 15,
2014. Manski (2013) offers an alternative perspective: that experts anticipate that decision-makers are “either
psychologically unwilling or cognitively unable to cope with uncertainty.” To be precise, Manski (2013) is
concerned with the expression of uncertainty as bounds in place of point estimates, rather than the coarser but
more tractable environment we study. Where these two agree is that eliciting expert uncertainty is first-order
important.




                                                      5
2         The Model

Our model is motivated by the following scenario: a decision-maker (abbreviated DM,
pronoun “she”) is making a policy choice. There is an unknown state of the world which
captures decision-relevant information. However, the DM does not observe this state; to
this end she employs an expert (pronoun “he”). Experts may be competent or incompetent.
Competent experts sometimes know the state of the world, but other times the state is
unknowable. Incompetent experts know nothing.



State of the World. Let the state of the world be ω ∈ Ω ≡ {0, 1}. Define pω to be the
common knowledge probability of the ex ante more likely state, so pω ≥ 1/2, and without
loss of generality assume this is state 1.9 That is, pω ≡ P(ω = 1).

The state of the world encodes the decision-relevant information for the DM. It is unknown,
and learning it is the “problem" for which she consults an expert. However, the problem
is complicated for the DM by two hazards that follow directly from her lack of expertise:
first, she may inadvertently hire an incompetent expert, and second, she may unwittingly
ask him to solve a problem that is unsolvable.



Expert Types. The expert has a type θ ∈ Θ ≡ {g, b}, which indicates whether he is
good (alternatively, “competent”) or bad (“incompetent”). Let pθ ≡ P(θ = g) represent the
probability that the expert is good, with pθ ∈ (0, 1) and pθ common knowledge. Experts
know their type.



Problem Difficulty. The difficulty of the question is captured by another random variable
δ ∈ ∆ ≡ {e, h}. That is, the problem may be easy (alternatively, “solvable"), or hard,
(“unsolvable"), where P{δ = e} = pδ ∈ (0, 1) is the common knowledge probability of an
easy problem.

The difficulty of the problem is not directly revealed to either actor at the outset. However,
the expert will receive a signal, which depends on (ω, θ, δ), and good experts will be able
to infer the difficulty of the problem.
    9
        By the symmetry of the payoffs introduced below, identical results hold if state 0 is more likely.

                                                          6
Experts Signal. The expert’s type and the problem difficulty determine what he learns
about the state of the world. This takes the form of a signal s ∈ S ≡ {s0 , s1 , s∅ }. The
expert receives a completely informative signal (i.e., sω ∈ {s0 , s1 }) if and only if he is
good and the problem is solvable. If not, he learns nothing about the state. Formally, let
the signal be:
                               
                                s      ω = 1 , θ = g and δ = e
                                1
                               
                               
                            s = s0      ω = 0 , θ = g and δ = e                          (1)
                               
                               
                               s       otherwise.
                               
                                    ∅




Note that our signal structure implies that the good expert infers δ from their signal. In
what follows, we will often refer to informed an uninformed experts, which is not the same
as good and bad. An informed expert receives signal s0 or s1 , and is therefore always good.
However an uninformed expert receives signal s∅ , and may be good or bad.



Sequence of Play and Validation. The game proceeds in the following sequence: first,
nature plays and the state of the game (ω, θ, δ) is realized according to independent binary
draws with probabilities (pω , pθ , pδ ). Second, the expert observes his competence and sig-
nal (i.e., his information set in the second stage is IE = (θ, s)), and chooses a message
from a infinite message space M. The information sets of the expert are summarized in
Figure 1. There are four: first, the expert may be bad; second, the expert may be good and
the problem hard; third, the expert may be good, the problem easy, and the state 0, and
finally, the expert may be good, the problem easy, and the state 1.

Next the DM observes m and takes an action a ∈ [0, 1], the policy choice. Her information
set in this stage consists only of the expert report, i.e. IDM 1 = (m).

Let v(a, ω) be the value of choosing policy a in state ω. We assume the policy value is
given by v(a, ω) ≡ 1 − (a − ω)2 . The value of the policy is equal to 1 for making the right
choice, 0 for making the worst choice, and an intermediate payoff when taking an interior
action with an increasing marginal cost the further the action is from the true state. Let πω
denote the DM’s posterior belief that ω = 1. Then, the expected value of taking action a is

                              1 − [πω (1 − a)2 + (1 − πω )a2 ],                          (2)


                                             7
                      Figure 1: Nature’s Play and Experts’ Information Sets


                                          0                          1


                       g              b                                  g             b


                                     e        h                                       e        h
                        h                                                h

              e                                                e




Notes: This figure depicts Nature’s moves – i.e. the choice of (ω, θ, δ) – as well as the information sets of the
expert in our model. Nature moves at hollow nodes, and at solid nodes the expert choses a message. We omit
the DM’s policy choice as well as payoffs for simplicity.


which is maximized at a = πω .

The quadratic loss formulation conveniently captures the realistic notion that when the ex-
pert does not know the state, the decision-maker makes a better policy choice (on average)
when learning this rather than being misled into thinking the state is zero or one. For-
mally, If the problem is unsolvable, the optimal action is a = pω , giving average payoff
1 − pω (1 − pω ), which is strictly higher than the average value of the policy for any other
action.10

In the final stage of the game, the DM makes an inference about the expert’s competence;
let IDM 2 represent her information set at this stage, and πθ ≡ P(θ = g|IDM 2 ) represent
the assessment of the expert. We consider several variations on IDM 2 . In all cases, the
structure of IDM 2 is assumed to be common knowledge.

In the no validation case, IDM 2 = (m). This is meant to reflect scenarios where it is
difficult or impossible to know the counterfactual outcome had the decision maker acted
  10
    For example, picking a = 0 yields an expected value of (1 − pω ) and a = 1, yielding an expected value
of pω , both strictly less than 1 − pω (1 − pω ).



                                                       8
differently.11

The state validation case, IDM 2 = (m, ω), reflects a scenario in which the DM can check
the expert’s advice against the true state of the world.

This may come about ex post because the information is only valuable if received ex ante,
e.g. in forecasting exercises from stock picks to elections. Alternatively, in business and
policy settings, the decision-maker may be able to validate the expert’s message directly,
whether through experimental A/B testing of a new product feature or observational pro-
gram evaluation.

In the difficulty validation case, IDM 2 = (m, δ), meaning that the DM learns whether the
problem was hard, i.e. whether the answer could have been learned by a good expert.

There are several interpretations for this information structure. On the one hand, it could
stand in for “peer review," whereby other experts evaluate the feasibility of expert’s design
without attempting the question themselves. Alternatively, subsequent events (such as the
implementation of the policy) may reveal auxiliary information about whether the state
should have been knowable, such as an extremely close election swayed by factors which
should not have been ex ante predictable.

In the full validation case, IDM 2 = (m, ω, δ), the DM learns both the state and the difficulty
of the problem.12



Payoffs.     The decision-maker only cares about the quality of the decision made, so

                                              uDM = v(a, ω).                                                (3)



The expert cares about appearing competent, and potentially also about a good decision
being made.13 We parameterize his degree of policy concerns by γ ≥ 0 and write his
  11
      For instance, it is generally very difficult to estimate counterfactual outcomes to determine the value of
an online advertising campaign (Gordon and Zettelmeyer, 2016).
   12
      Another potentially realistic validation regime is one where the DM only learns the state if the problem
is easy, i.e., has the same information as a good expert. However, the DM inference about competence when
the problem is hard does not depend on the revelation of ω, so the analysis of this case is the same as full
validation.
   13
      As in the career concerns literature following Holmström (1999), this payoff structure is a static repre-

                                                       9
payoff
                                         uE = πθ + γv(a, ω).                                             (4)

We first consider the case where γ = 0, i.e., experts who only care about reputational
concerns. We then analyze the case where γ > 0, focusing attention on the case where
policy concerns are small (γ → 0) in the main text.

To summarize, nature plays first and realizes (ω, θ, δ). Next, the expert chooses a message
m given information set IE = (θ, s). Third, the DM chooses an action a given information
set IDM 1 = (m). Finally, the DM evaluates the competence of the expert and forms
beliefs πθ given IDM 2 , the manipulation of which constitutes the design problem of interest.
Payoffs are v(a, ω) for the DM and πθ + γv(a, ω) for the expert.



Comments on Alternative Specifications. As mentioned in the introduction, we use the
simple information structure where the primitive random variables (ω, δ, θ) are binary to
make the argument as transparent as possible. We have explored several adjacent per-
mutations (e.g., allowing trembles, faulty signals, and other means of putting outcomes
on-path) and found the arguments less transparent and the conclusions qualitatively un-
changed; therefore we focus here on the simplest model for the sake of clarity. However,
in Appendix E we present a brief analysis of one alternative specification, where both the
expert competence and difficulty of the problem are continuous variables.




3     Equilibrium Definition and Properties

We search for Markov sequential equilibrium (MSE) of the model. Compared to perfect
Bayesian equilibrium (PBE), this solution concept does two things. First, restricting atten-
tion to Markov strategies implies that agents making strategically equivalent decisions play
the same action; in other words, their behavior cannot depend on payoff-irrelevant infor-
mation. Second, it restricts off-path beliefs in a manner distinct from standard refinements.
Intuitively, if agents cannot condition on payoff-irrelevant information in their actions, then
consistency of beliefs implies that we cannot learn about that payoff-irrelevant information
from their actions, even off-path. We introduce this solution concept to rule out unrea-
sentation of dynamic incentives to appear competent in order to secure future employment as an expert.



                                                   10
sonable off-path beliefs necessary to sustain certain PBE; the interested reader can find a
discussion of those equilibria in Appendix A.

Let each node (history) be associated with information set I and an action set A. Beliefs µ
map information sets into a probability distribution over their constituent nodes. Strategies
σ map information sets into a probability distribution over A. Write the probability (or
density) of action a at information set I as σa (I). Let the function uI (a, σ) denote the
von Neumann-Morgenstern expected utility from taking action a ∈ A at an information
set I when all subsequent play, by all players, is according to σ. In our setting the payoff-
relevant state depends on the information set of the DM, IDM 2 , and so in order to define it,
we look to affine payoff equivalence following Harsanyi and Selten (1988) and Fudenberg
and Tirole (1991).

Definition 1. A strategy σ is a Markov strategy if whenever, for any pair of information
sets I and I 0 with associated action sets A and A0 , and for some constants α > 0 and β,
there exists a bijection f : A → A0 such that uI (a, σ) = αuI 0 (f (a), σ) + β, ∀a ∈ A, then
σa (I) = σf (a) (I 0 ).


The extension of equilibrium in Markov strategies to a setting with incomplete information
requires some additional language. Our notation and terminology parallels the treatment
of sequential equilibrium in Tadelis (2013). As consistency is to sequential equilibrium, so
Markov consistency is to Markov sequential equilibrium.

Definition 2. A profile of strategies σ and a system of beliefs µ is Markov consistent if there
exists a sequence of non-degenerate, Markov mixed strategies {σk }∞    k=1 and a sequence of
             ∞
beliefs {µk }k=1 that are derived from Bayes’ Rule, such that limk→∞ (σk , µk ) → (σ, µ).


Markov consistency has two implications. The first is a restriction to Markov strategies.
Because the Markov property is preserved under limits, players cannot condition their be-
havior on payoff-irrelevant private information. Anticipating the analysis that follows, it
will be critical to know whether bad experts (θ = b) and uninformed good experts (θ = g,
δ = h) face a strategically equivalent choice; that is, whether knowledge of δ is payoff-
relevant.

However, Markov consistency also constrains players’ beliefs. In particular, it rules out off-
path inferences about payoff-irrelevant information, because off-path beliefs which condi-
tion on payoff-irrelevant information can not be reached by a sequence of Markov strate-

                                              11
gies.14 Our restriction is related to that implied by D1 and the intuitive criterion. However,
where these refinements require players to make inferences about off-path play in the pres-
ence of strict differences of incentives between types, our restriction rules out inference
about types in the absence of strict differences of incentives.15 With this in hand, a notion
of Markov sequential equilibrium follows directly.

Definition 3. A profile of strategies σ, together with a set of beliefs µ, is a Markov sequen-
tial equilibrium if (σ ∗ , µ∗ ) is a Markov consistent perfect Bayesian equilibrium.


In Appendix A we offer a discussion of prior usage of this solution concept, as well as
an illustration of its implications for off-path beliefs designed to parallel the discussion of
consistency and sequential equilibrium from Kreps and Wilson (1982).16



Behavioral Motivation Markov strategies have axiomatic foundations (Harsanyi and
Selten, 1988), and can be motivated by purification arguments as well as finite memory
in forecasting (Maskin and Tirole, 2001; Bhaskar et al., 2013). In the complete information
settings to which it is commonly applied, the Markovian restriction prevents the players
from conditioning their behavior on payoff-irrelevant aspects of the (common knowledge)
history.17 The natural extension of this idea to asymmetric information games is to prevent
players from conditioning on elements in their information set that are payoff-irrelevant.
Or, in our setting, types facing strategically equivalent scenarios must play the same strat-
egy.

Our restriction on beliefs is also related to the notion of structural consistency proposed
by Kreps and Wilson (1982).18 In that spirit, Markov consistency formalizes a notion of
“credible" beliefs, analogous to the notion of credible threats in subgame perfect equilib-
rium. Instead of using arbitrarily punitive off-path beliefs to discipline on-path behavior,
  14
      This is analogous to the way that sequential equilibrium restricts off-path beliefs – in that case, one
cannot update, following an off-path action, on information that is not in the actor’s information set; see
Appendix A for further discussion.
   15
      For this same reason it will become apparent in Section 4 that D1 and the intuitive criterion do not have
the same power to restrict off-path beliefs, see Footnote 22.
   16
      We are not aware of any work demonstrating general conditions that guarantee existence of MSE, but
existence will be trivial in our setting (a babbling equilibrium is always a MSE).
   17
      Applications of Markov equilibrium have been similarly focused on the infinitely-repeated, complete
information setting. See, e.g. Maskin and Tirole (1988a,b); Ericson and Pakes (1995).
   18
      Kreps and Ramey (1987) demonstrated that consistency may not imply structural consistency, as conjec-
tured by Kreps and Wilson (1982). We observe that as the Markov property is preserved by limits, Markov
consistency does not introduce any further interpretive difficulty.

                                                     12
we require that off-path beliefs are credible in the sense that, ex post, on arriving at such an
off-path node, the relevant agent could construct a convergent sequence of Markov strate-
gies to rationalize them.



Robustness of MSE. Though the restriction to Markov strategies itself enforces a no-
tion of robustness, there is a trivial sense in which the restriction of Markov equilibrium –
whether in a complete information setting or an incomplete information setting – is non-
robust. That is, because it imposes symmetric strategies only when incentives are exactly
symmetric, small perturbations of a model may permit much larger sets of equilibria. In
the standard applications of the Markov restriction, this could be driven by future payoffs
being slightly different depending on the history of play. In our setting, good and bad unin-
formed experts could have marginally different expected payoffs. Either way, we maintain
that this is a red herring. The point of the refinement, like the symmetry condition of Nash
(1950), is to hold the economist to a simple standard: that we be precise about exactly what
kind of asymmetry in the model construction explains asymmetries in the predicted behav-
ior. From this perspective, another interpretation of our work is that we are reflecting on
exactly what informational structures introduce the asymmetry we need to obtain honesty
in equilibrium.19



Properties of Equilibria. Since we allow for a generic message space, there will always
be many equilibria to the model even with the Markov restriction. To organize the discus-
sion, we will focus on how much information about the state and competence of the expert
can be conveyed. On one extreme, we have babbling equilibria, in which all types employ
the same strategy.

On the other extreme, there is never an equilibrium with full separation of types. To see
why, suppose there is a message that is only sent by the good but uninformed types mg,∅
(“I don’t know because the problem is hard”) and a different message only sent by the
bad uninformed types mb,∅ (“I don’t know because I am incompetent”). If so, the policy
choice upon observing these messages would be the same. However, for any validation
regime there is some chance that a bad type can send mg,∅ and receive a strictly positive
competence evaluation, and so these types have an incentive to deviate.
  19
     We thank an anonymous referee for pointing out that one could also develop a notion of ε−Markov
equilibrium to make this point. This is beyond the theoretical ambition of the current work, but an interesting
direction for future work.


                                                     13
It will sometimes be possible to have an equilibrium where experts fully reveal their in-
formation about the state (but not their competence). That is, the uninformed types say “I
don’t know" (if not why), and the informed types report the state of the world. We call this
an honest equilibrium.20

Definition 4. Let πs (m) be the DM posterior belief that the expert observed signal s upon
sending message m. An equilibrium is honest if πs (m) ∈ {0, 1} ∀ s ∈ S and all on-path
m


It is most intuitive to formulate this equilibrium as if there were a natural language, i.e. a
message mx sent by each type observing sx with probability 1, x ∈ {0, 1, ∅}. However,
more generally an equilibrium is honest if the DM always infers what signal the expert
observed with certainty.

This is a particularly important class of equilibria in our model as it conveys the most
information about the state possible:

Proposition 1. The expected value of the decision in an honest equilibrium is pθ pδ + (1 −
pθ pδ )(1 − pω (1 − pω )) ≡ v, which is strictly greater than the expected value of the decision
in any equilibrium which is not honest.


Proof. Unless otherwise noted, all proofs are in Appendix B.


This result formalizes our intuition that it is valuable for the DM to learn when the expert
is uninformed, and follows directly from the convexity of the loss function for incorrect
decisions. As in all cheap-talk games, the messages sent only convey meaning by which
types send them in equilibrium. We define admitting uncertainty as sending a message
which is never sent by either informed type:

Definition 5. Let M0 be the set of messages sent by the s0 types and M1 be the set of
message sent by the s1 types. Then an expert admits uncertainty if he sends a message
m 6∈ M0 ∪ M1
  20
     Our definition of an honest equilibrium is more stringent than Sobel (1985), who only requires that good
types report a message corresponding to the state. In our definition, all types must report a message which
indicates their signal.




                                                    14
Finally, an important class of equilibria will be one where the informed types send distinct
message from each other, but the uninformed types sometimes if not always mimic these
messages:

Definition 6. A guessing equilibrium is one where M0 ∩ M1 = ∅, and P r(m ∈ M0 ∪
M1 |θ, s∅ ) > 0 for at least one θ ∈ {g, b}. In an always guessing equilibrium, P r(m ∈
M0 ∪ M1 |θ, s∅ ) = 1 for both θ ∈ {g, b}.


That is, an always guessing equilibrium is one where the informed types report their signal
honestly, but the uninformed types never admit uncertainty.

Combined with proposition 1, these definitions highlight the importance of admission of
uncertainty for good decision-making. In any equilibrium with guessing, the fact that the
uninformed types send messages associated with informed types leads to worse policies
than an honest equilibrium. This is for two reasons. First, when the expert actually is
informed, their advice will be partially discounted by the fact that the DM knows some un-
informed types send the informed message as well. Second, when the expert is uninformed,
they will be induce the DM to take more decisive action than the expert’s knowledge war-
rants.




4    Preliminary Observations

In the sections that follow we will provide a case-by case analysis of the MSE in our game
for the four validations regimes, both without policy concerns (Section 5) and with (Section
6). While this is a lot to keep track of, our argument for difficulty validation has a simple
structure that runs throughout the results. For the sake of exposition, here we offer the
broad strokes of that argument.

Markov sequential equilibrium has two main implications: Markov strategies, implying
that payoff-irrelevant information cannot affect equilibrium play, and Markov consistency,
which implies in addition that off-path beliefs cannot update on payoff-irrelevant informa-
tion. To see the immediate implications of these restrictions, it is helpful to construct the
classes of payoff-equivalent information sets. We put information sets in the same pay-
off equivalence class if experts at those decision nodes are payoff-equivalent for any DM


                                             15
               Figure 2: Payoff Equivalence Classes With No Policy Concerns



                      NV        b, ·, ·      g, h, ·       g, e, 0        g, e, 1

                      SV        b, ·, ·      g, h, ·       g, e, 0        g, e, 1

                      DV        b, ·, ·      g, h, ·       g, e, 0        g, e, 1

                      FV        b, ·, ·      g, h, ·       g, e, 0        g, e, 1




Notes: This figure depicts equivalence classes under each validation regime for the case with no policy
concerns. Each row represents a validation regime: respectively, no validation, state validation, difficulty
validation, and full validation. Each column represents an expert information set, as derived in Figure 1.


strategy.21 Figure 2 illustrates for the case with no policy concerns. Each row represents
an assumption on the DM’s information set at the end of the game, IDM 2 . Each column
represents one of the four information sets depicted in Figure 1.

Note that we have formulated our game as a one-shot sequential game with reputational
concerns, so the payoff equivalence holds exactly. In the repeated game embodied by the
reputational concerns, this will not generally be the case: depending on the formulation of
payoffs, good experts will most likely have a higher continuation value than bad ones in
all but a babbling equilibrium. This is where the affine transformation in our definition of
Markov strategies is useful – if the prior beliefs of the DM at the beginning of a period are
sufficient for the history of the game, then setting β equal to the difference in continuation
values means that the Markov restriction still binds.

Setting aside the parameterization (pω , pθ , pδ ), many of our results follow directly from the
structure of the payoff equivalence classes. First, in the no validation (NV) case, IDM 2 =
(m) and so the signal of the expert is payoff-irrelevant. There is a single payoff equivalence
class comprised of all four information sets, and therefore the Markov strategies restriction
implies that any MSE is a babbling equilibrium.

In order to sustain an honest equilibrium in Markov strategies, we need to break the payoff
  21
     Any two information sets can be payoff equivalent for some DM strategy: e.g., if they always pick the
same policy and competence assessment for all messages.


                                                    16
equivalence in a way that permits honest messages. This is exactly what state validation
(SV) does, as depicted in the second row. Bad experts and uninformed good experts can
pool on a message interpreted as “I don’t know”, and informed experts can send messages
interpreted as “the state is zero” and “the state is one”.

In this case, the problem is not Markov strategies but Markov consistency. Uninformed
experts who deviate from saying “I don’t know” risk incurring punitive beliefs if they guess
incorrectly, but what can the DM credibly threaten to believe in this off-path scenario?
Markov consistency bounds the severity of these beliefs because uninformed good types
are payoff equivalent to bad types. In fact, the worst that the DM can threaten to believe
upon observing an incorrect guess is not that the expert is bad, just that he is uninformed
(i.e., in the left-most equivalence class).22 Importantly, this is no worse than the reputational
payoff associated with admitting uncertainty directly, which is what the honest equilibrium
requires. Since there is a chance that the deviation is successful (if they guess the state
of the world correctly), guessing is always profitable. Therefore there is never an honest
equilibrium under state validation and no policy concerns.

For the DM to effectively threaten punitive off-path beliefs, we need to break the pay-
off equivalence of bad types and good but uninformed types, and this is precisely what
difficulty validation (DV) does, depicted in the third row. Further, this effect on beliefs is
complemented how DV affects the relative incentives to guess for good and bad uninformed
types. With DV, admitting uncertainty is relatively palatable for good but uninformed types,
who know the validation will reveal the problem is hard and give them an excuse for be-
ing uninformed. Bad types, on the other hand, do not know what the difficulty validation
may reveal, and so are more tempted to (and sometimes will) guess at the state, meaning
incorrect guessing is on-path and gives the expert away as incompetent.

However, difficulty validation is not enough to sustain honesty, because we also need to
break the equivalence between informed experts. This we view as a more minor problem,
which can be accomplished by either combining state and difficulty validation (FV), as in
the fourth row, or by adding small policy concerns, which yields payoff equivalence classes
represented in Figure 3.
  22
     Note here that because bad experts and uninformed good experts are strategically equivalent, D1 and the
intuitive criterion do not help to restrict off-path beliefs.




                                                    17
                  Figure 3: Payoff Equivalence Classes With Policy Concerns



                       NV         b, ·, ·       g, h, ·        g, e, 0        g, e, 1

                       SV         b, ·, ·       g, h, ·        g, e, 0        g, e, 1

                       DV         b, ·, ·       g, h, ·        g, e, 0        g, e, 1

                       FV         b, ·, ·       g, h, ·        g, e, 0        g, e, 1




Notes: This figure depicts equivalence classes under each validation regime for the case with policy concerns.
Each row represents a validation regime: respectively, no validation, state validation, difficulty validation, and
full validation. Each column represents an expert information set, as derived in Figure 1.


Remark: It would be possible to construct additional informative equilibria if we al-
lowed different types to play different actions, even when they are payoff equivalent. We
view this as a modeling contrivance, and this is precisely what the Markov consistency re-
striction, above and beyond standard consistency, restricts. This point was previously made
by Harsanyi and Selten (1988), who contend that the property of “invariance with respect to
isomorphisms," on which our definition of Markov strategies is based, is “an indispensable
requirement for any rational theory of equilibrium point selection that is based on strategic
considerations exclusively." Or, in the appeal of Maskin and Tirole (2001) to payoff per-
turbations, “minor causes should have minor effects." We discuss such equilibria further in
Appendix A.2.




5     No Policy Concerns

No Validation. In the case of no policy concerns and in the absence of any form of
validation, the expert payoff for sending message m given their type θ and signal s is simply
πθ (m). This does not depend upon his private information. The restriction to Markov
strategies immediately implies a strong negative result:

Proposition 2. With no validation and no policy concerns (i.e., γ = 0), any MSE is bab-
bling, and there is no admission of uncertainty.


                                                       18
It may seem odd that even the types who know the state is zero or one can not send different
messages to partially communicate this. However, if the expert does not care about the
decision made, faces no checks on what he says via validation, and has no intrinsic desire
to tell the truth, then his knowledge about the state is payoff-irrelevant.23

Small changes to the model will break the payoff equivalence of types with different knowl-
edge about the state. In particular, any policy concerns (γ > 0) or chance that the decision-
maker will learn the state will allow for separation on this dimension. However, neither
of these realistic additions will change the payoff equivalence between the good and bad
uninformed types who have the same knowledge about the state, which, we will show, has
important implications for how much information can be transmitted in equilibrium.



State Validation. Now consider the case where the decision-maker learns the state after
making their decision but before making the inference about expert competence. Write
the competence assessment upon observing message m and validation ω as πθ (m, ω). The
expected payoff for the expert with type and signal (θ, s) sending message m is:
                                         X
                                                 P(ω|s, θ)πθ (m, ω)
                                       ω∈{0,1}




–where the signal structure implies
                                                   
                                                   
                                                   
                                                    0           s = s0
                                                   
                                    P(ω = 1|θ, s) = pω           s = s∅
                                                   
                                                   
                                                   1
                                                   
                                                                 s = s1

–and P(ω = 0|θ, s) = 1 − P(ω = 1|θ, s). Now the types with different information
about the state are not generally payoff-equivalent since P(ω|θ, s) depends on the signal.
However, good and bad uninformed experts, i.e. (s∅ , g) and (s∅ , b), are always payoff
equivalent since P(ω|g, s∅ ) = P(ω|b, s∅ ).

Given the arbitrary message space, the expert strategies can be quite complex. To search for
an equilibrium as informative as possible, it is natural to start by supposing the informed
  23
     As discussed in appendix A.2, there is a PBE to the model with admission of uncertainty, though this
is not sensitive to small perturbations to the model. Further, there is no honest equilibrium even without the
Markov restrictions.

                                                     19
types send distinct (and, for simplicity, unique) messages m0 and m1 , respectively. We
also restrict our search to messaging strategies where there is only one other message,
which we label m∅ (“I don’t know”). These restrictions are innocuous for any non-babbling
equilibrium, an argument we make precise in Appendix C.

So, the problem of characterizing non-babbling MSE boils down to proposing a mixed
strategy over (m0 , m1 , m∅ ) for the uninformed types, computing the relevant posterior be-
liefs, and checking that no type has an incentive to deviate. A mixed strategy σ is a map-
ping from the set of expert information sets {(b, s∅ ), (g, s∅ ), (g, s0 ), (g, s1 )} into probability
weights on {m∅ , m0 , m1 }, with arguments σIE (m). In what follows we will abuse notation
somewhat when describing IE – for instance, where Markov strategies impose symmetric
strategies between uninformed experts, we write σ∅ (m∅ ) for the likelihood that an unin-
formed expert admits uncertainty, regardless of their competence. Moreover, in general,
when an expert sends a signal mx upon observing sx , we will say that he sends an “honest"
message.

First consider the possibility of an honest equilibrium, i.e., σ0 (m0 ) = σ1 (m1 ) = σ∅ (m∅ ) =
1. In such an equilibrium, the decision-maker takes action 0 or 1 corresponding to messages
m0 and m1 , respectively, and a = pω when observing m∅ . When forming a belief about
the competence of the expert, state validation implies that the DM’s information set is
IDM 2 = (m, ω). The on-path information sets include cases where a good expert makes
an accurate recommendation, (m0 , 0) and (m1 , 1), and cases where an uninformed expert
(good or bad) says “I don’t know" along with either validation result: (m∅ , 0), and (m∅ , 1).
When observing (m0 , 0) or (m1 , 1), the DM knows the expert is good, i.e., πθ (mi , i) = 1
for i ∈ {0, 1}. When observing m∅ and either validation result, the belief about the expert
competence is:

                               P(θ = g, δ = h)               pθ (1 − pδ )
          πθ (m∅ , ω) =                              =                       ≡ π∅ .
                          P(θ = g, δ = h) + P(θ = b)   pθ (1 − pδ ) + 1 − pθ

This expression, which recurs frequently throughout the analysis, represents the share of
uninformed types who are competent (but facing a hard problem).

The informed types know that reporting their signal will yield a reputational payoff of one,
and so they never have an incentive to deviate.

For the uninformed types, the expected payoff for sending m∅ in the honest equilibrium


                                                 20
is π∅ . Since 0 < π∅ < pθ , the expert revealing himself as uninformed leads to a lower
belief about competence than the prior, but is not zero, since there are always competent
but uninformed types.

Consider a deviation to m1 . When the state is in fact 1, the DM observes (m1 , 1), and
believes the expert to be competent with probability 1. When the state is 0, the DM observes
(m1 , 0), which is off-path.

However, MSE places some restriction on this belief, as it must be the limit of a sequence of
beliefs consistent with a sequence of Markovian strategies. In general, the posterior belief
upon observing this information set (when well-defined) is:

                    P(m1 , 0, θ = g)   pθ pδ (1 − pω )σ0 (m1 ) + (1 − pω )pθ (1 − pδ )σ∅ (m1 )
   πθ (m1 , 0) =                     =                                                         .
                      P(m1 , 0)        pθ pδ (1 − pω )σ0 (m1 ) + (1 − pω )(1 − pθ pδ )σ∅ (m1 )

This belief is increasing in σ0 (m1 ) and decreasing in σ∅ (m1 ), and can range from π∅ (when
σ0 (m1 ) = 0 and σ∅ (m1 ) > 0) to 1 (when σ0 (m1 ) > 0 and σ∅ (m1 ) = 0). Importantly,
this lower bound results from the fact that the bad uninformed types can not send a mes-
sage more often than the good uninformed types. So, upon observing an incorrect guess,
MSE requires that the worst inference the DM can make is that the expert is one of the
uninformed types, but can not update on which of the uninformed types is relatively more
likely to guess incorrectly.24

Given this restriction on the off-path belief, in any honest MSE the payoff to sending m1
must be at least:

                                         pω + (1 − pω )π∅ > π∅ .



The expert can look no worse from guessing the state is one and being incorrect than they
would when just admitting they are uncertain. Since there is a chance to look competent
when guessing and being correct, the expert will always do so. This means there is always
an incentive to deviate to m1 (or, by an analogous argument, m0 ), and hence no honest
equilibrium.
  24
     Without the restriction to Markov beliefs, this off-path belief could be set to zero, and an honest equi-
librium is sometimes possible. See appendix A.2 for further discussion of this point and why this off-path
inference is fragile.




                                                     21
A related argument implies that there is no MSE where the uninformed types sometimes
admit uncertainty (i.e., σ∅ (m∅ ) ∈ (0, 1)) and sometimes guess m0 or m1 : guessing and
being correct always gives a higher competence evaluation than incorrectly guessing, which
gives the same competence evaluation as sending m∅ .

However, there is an always guessing equilibrium where the uninformed types either send
only m1 or mix between m0 and m1 . In this equilibrium, the DM believes the expert is more
likely to be competent when the state matches the message, as they either face a competent
expert or an uninformed one who guessed right. Upon observing an incorrect guess, they
at worst infer that the expert is uninformed if the message is off-path, and infer exactly this
if the message is on-path. (Note this is exactly the somewhat-but-not-completely-punitive
off-path inference used when checking for an honest MSE)

The blend of sending m1 and m0 depends on the probability parameters: in general, if pω
is high the uninformed expert guesses m1 more often if not always, as this is more likely to
be what the validation reveals.

Summarizing:

Proposition 3. With state validation and no policy concerns:
i. In any MSE, there is no admission of uncertainty, and
ii. any non-babbling MSE is equivalent, subject to relabeling, to an MSE where both
uninformed types send m1 with probability
                                 
                                  pω (1+pθ pδ )−pθ pδ   if   pω < 1/(1 + pθ pδ )
                   σ∅∗ (m1 ) =           1−pθ pδ
                                 1                      otherwise

–and m0 with probability σ∅∗ (m0 ) = 1 − σ∅∗ (m1 ).


The always guessing MSE is more informative than babbling since the messages provide
some information about the state. Further, the decision-maker learns something about the
expert’s competence because upon observing a correct message the expert is more likely to
be a good type, and when observing an incorrect guess the decision-maker learns that the
expert was uninformed (and hence less likely to be competent.)

However, since the experts never send m∅ , there is never any admission of uncertainty. Put
another way, the DM may learn that the expert was uninformed ex post, but he never says

                                                   22
“I Don’t Know.” Further, the fact that the uninformed types guess dilutes the information
conveyed in equilibrium.



Difficulty Validation. With difficulty validation, the informed types know that validation
will reveal the problem is easy, and the good but uninformed types know that the validation
will reveal the problem is hard. The bad types are unsure of what the validation will reveal.

Write the competence evaluation when the expert observes m and δ as πθ (m, δ). We can
now write the expected payoff for the expert with type and signal (θ, s) sending message
m as:
                                 X
                                       P(δ|s, θ)πθ (m, δ)
                                   δ∈{e,h}



–where the signal structure implies
                                         
                                         
                                         
                                          0        s = s∅ and θ = g
                                         
                          P(δ = e|s, θ) = pδ        s = s∅ and θ = b
                                         
                                         
                                         1         s ∈ {s0 , s1 }
                                         


– and P(δ = h|θ, s) = 1 − P(δ = e|θ, s).

In this case, the restriction to Markov strategies requires that the two informed types send
the same message. Since both informed types send the same message, the DM learns
nothing about the state. And since any other message is only sent by uninformed types, the
DM learns nothing about the state from any message sent in an MSE.

However, with a different natural interpretation for the messages, there can be an MSE
where the informed types always send a message me (“the problem is easy”), the good but
uninformed types always send a different message mh (“the problem is hard”), and the bad
types mix between these messages.

Proposition 4. With no policy concerns and difficulty validation,
i. in any MSE a∗ (m) = pω for all on-path m, and
ii. there is an MSE where the good uninformed types always admit uncertainty.


If we care about the admission of uncertainty in and of itself, this is a positive result: diffi-

                                               23
culty validation ensures at least good uninformed types can be honest about their ignorance.
However, admission of uncertainty is not particularly useful if those who are informed don’t
reveal their information, as happens in this MSE. Put another way, one reason to desire ad-
mission of uncertainty is to avoid uninformed types diluting the value of the messages m0
and m1 , but if there are no informative messages in the first place there is no information
to dilute.

However, as we will see below, the negative aspects of the results will be fragile; combined
with either state validation or any policy concerns, difficulty validation will lead to more
admission of uncertainty and superior information transmission about the state.25



State and Difficulty (Full) Validation. While both negative in isolation, the results with
just state and just difficulty validation hint at how combining the two can lead to a more
positive outcome. Recalling Figure 2, with no validation, all types form one equivalence
class. State validation breaks the payoff equivalence between types with different knowl-
edge about the state, so only the good and bad uninformed types are payoff equivalent.
Difficulty validation breaks the payoff equivalence with different knowledge about the dif-
ficulty of the problem, placing the informed types in the same equivalence class. Com-
bining the two, SV and DV, no two types are payoff equivalent, which permits an honest
equilibrium.

Formally, there are now four possible validation results for each message. The expected
payoff to sending message m given one’s type and message is:
                             X       X
                                              P(δ|s, θ)P(ω|s, θ)πθ (m, ω, δ).
                            δ∈{e,h} ω∈{0,1}


No pair of types share the same P(ω|s, θ) and P(δ|s, θ), so none must be payoff equivalent.
As a result, all types can use distinct strategies, and off-path beliefs are unrestricted.

In an honest equilibrium, upon observing (m0 , 0, e) or (m1 , 1, e), the DM knows that the
expert is competent. Upon observing (m∅ , ω, e) the DM knows that the expert is not com-
petent, as a competent expert would have received and sent an informative message since
the problem is easy. The last on-path message/validation combination is (m∅ , ω, h). Upon
  25
    As discussed in Appendix A, there can also be an honest equilibrium with just difficulty if we study PBE
without the Markov restriction.



                                                     24
observing this the DM belief about the expert competence is the same as the prior, since
if the problem is hard no one gets an informative message (and all send m∅ ).26 So, the
competence evaluations for the on-path messages are:

                        πθ (m0 , 0, e) = 1                           πθ (m1 , 1, e) = 1
                        πθ (m∅ , ω, e) = 0                       πθ (m∅ , ω, h) = pθ

To make honesty as easy as possible to sustain, suppose that for any off-path message
(“guessing wrong”), the competence evaluation is zero. (Since no types are payoff equiv-
alent, this belief can be rationalized as the limit of a sequence of strategies where the bad
experts send m0 and m1 with probabilities that converge to zero more slowly than the good
experts send these messages.)

The informed types get a competence evaluation of 1 for sending their honest message, so
face no incentive to deviate.

A good but uninformed type knows the difficulty validation will reveal δ = h, but does
not know ω. Sending the honest message m∅ gives a competence payoff of pθ . However,
sending either m0 or m1 will lead to an off-path message/validation combination, and hence
a payoff of zero. So, these types face no incentive to deviate.

Finally, consider the bad uninformed types, who do not know what either the state or dif-
ficulty validation will reveal. If they send m∅ , they will be caught as uninformed if the
problem was in fact easy (probability pδ ). However, if the problem is hard, the DM does
not update about their competence for either state validation result. So, the expected payoff
to sending m∅ is (1 − pδ )pθ .

If guessing m1 , the expert will be “caught” if either the problem is hard or the state is
0. However, if guessing correctly, the competence evaluation will be 1. So, the expected
payoff to this deviation is pδ pω . Similarly, the expected payoff to guessing m0 is pδ (1 −
pω ) < pδ pω .

Honesty is possible if admitting uncertainty leads to a higher competence evaluation than
  26                                                                         pθ (1−pδ )
       Formally, applying Bayes’ rule gives P r(θ = g|m∅ , ω, h) =   pθ (1−pδ )+(1−pθ )(1−pδ )   = pθ .




                                                      25
guessing m1 , or:

                                                            pθ
                           (1 − pδ )pθ ≥ pδ pω =⇒ pδ ≤           .
                                                         pθ + pω


If this inequality does not hold, a fully honest MSE is not possible. However, there is always
an MSE where the good but uninformed types always send m∅ . In such an equilibrium, the
bad types pick a mixed strategy over m0 , m1 , and m∅ . Whenever the DM observes an
“incorrect guess” they assign a competence evaluation of zero. So, the good uninformed
types have no reason to guess since they know the problem is hard. Returning to the
derivation of the honest equilibrium, the off-path beliefs in this MSE are justified, in the
sense that the good types all have strict incentives to report their honest message, and the
bad types are the only ones who potentially face an incentive to send m0 or m1 when the
problem is hard or m∅ when the problem is easy.

Summarizing:

Proposition 5. With no policy concerns and full validation, there is an MSE where the
informed types send distinct messages and the good but uninformed types always admit
uncertainty. If pδ ≤ pθp+p
                         θ
                           ω
                             , there is an honest MSE.


This threshold has natural comparative statics. First, it is easy to maintain when pδ is
small, meaning the problem is likely to be hard. When the problem is likely to be hard, an
uninformed expert is more likely to be caught guessing m0 or m1 , and also less likely to
be revealed as incompetent when sending m∅ . Second, the threshold is easier to maintain
when pθ is high, meaning the prior is that the expert is competent. Finally, honesty is easier
to sustain when pω is low, as this makes it more likely to be caught when guessing m1 .




6    Policy Concerns

We now consider the case where the decision-maker also cares about the policy chosen.
A complete characterization of the set of equilibria for all regions of the parameter space
and all validation regimes is unwieldy. In the main text we focus on what happens in the
(often realistic) case where the expert primarily cares about his reputation, but also has
small policy concerns. The main implication of this perturbation is that expert types with

                                             26
different information about the state are no longer always payoff equivalent, which allows
for more communication with no validation and difficulty validation. Since the payoff
equivalence of types with different information about the state is already broken by state
validation and full validation, adding small policy concerns has no effect in these cases.

The appendix contains more analysis of the case where policy concerns can be larger, with
an emphasis on the minimal level of policy concerns required to induce full honesty under
different validation regimes.



No Validation. First consider the case with no validation and γ > 0. For a fixed DM
strategy and inference about competence, the expected payoff for expert with information
(θ, s) from sending message m is:
                                       X
                         πθ (m) + γ             P(ω|θ, s)v(a∗ (m), ω).
                                      ω∈{0,1}




Here, expert type enters the payoff through the P(ω|θ, s) terms. So the types observing s∅
are always payoff equivalent (whether good or bad), but types observing s0 and s1 are not.
So, the restriction to Markov strategies no longer precludes an informative equilibrium, or
even an honest equilibrium.

As shown in the Appendix C, it is again without loss of generality to search for equilibria
with the informed types send messages m0 and m1 , and the uninformed types send at most
one other message m∅ .

In the honest equilibrium with these messages, the payoff for an uninformed type to send
m∅ is
                                 π∅ + γ(1 − pω (1 − pω )).                            (5)

If the expert deviates to m ∈ {m0 , m1 }, his payoff changes in two ways: he looks com-
petent with probability 1 (as only competent analysts send these messages in an honest
equilibrium), and the policy payoff gets worse on average. So, the payoff to choosing m1
is:
                                         1 + γpω .                                    (6)




                                                27
Since π∅ < 1, if γ is sufficiently small, then (6) is always greater than (5), and so the unin-
formed types always prefer to deviate to m1 . So, as γ → 0, there is no honest equilibrium.
A similar argument (see Appendix B) shows that there can be no equilibrium where the
uninformed types ever admit uncertainty as γ → 0.

Unlike the case with no policy concerns where any MSE is babbling, it is possible to have
an always guessing MSE with small policy concerns. The equilibrium condition for such an
equilibrium is that the posterior belief about expert competence is the same when sending
m0 and m1 . This is true if and only if the uninformed type send m1 with probability pω .

Summarizing:

Proposition 6. With small policy concerns (γ → 0) and no validation:, there is a unique
(subject to relabeling) always guessing equilibrium where σ∅∗ (m1 ) → pω and σ∅∗ (m0 ) →
1 − pω .



State Validation. Now consider the case with state validation and small policy concerns.
The expected payoff for sending message m is now:
                         X
                                  P(ω|θ, s)(πθ (m, ω) + γv(a∗ (m), ω)).
                        ω∈{0,1}


So, as with state validation and γ = 0, the good and bad uninformed types are always
payoff equivalent, but no other two pairs of types are. Since the payoffs for sending each
message approach that of the no policy concerns case as γ → 0, the potential equilibrium
strategies are the same. There can be no admission of uncertainty because guessing m0 or
m1 gives some chance of achieving a strictly higher competence payoff, which is worth it
when γ is sufficiently small:

Proposition 7. With small policy concerns (γ → 0) and state validation, there is a unique
always guessing equilibrium with the same strategies identified by the γ = 0 case.



Difficulty Validation. As with the no validation case, an important difference generated
by introducing any policy concerns along with difficulty validation is to break the payoff
equivalence among types with different information about the state. Further, difficulty
validation breaks the payoff equivalence precisely among the two types that are payoff


                                               28
equivalent from policy concerns alone: the good and bad uninformed types. So, in this
case no two types are always payoff-equivalent.

In an honest equilibrium, the competence assessment upon observing (m∅ , h) is pθ and
upon observing (m∅ , e) = 0. So, the payoff for the good but uninformed type for sending
m∅ (who knows the validation will reveal δ = h) is

                                  pθ + γ(1 − pω (1 − pω )).



The bad uninformed type does not know if the validation will reveal the problem is hard,
and so receives a lower expected competence evaluation and hence payoff for sending m∅ :

                             (1 − pδ )pθ + γ(1 − pω (1 − pω )).



Since no types are payoff-equivalent, any off-path competence evaluations can be set to
zero. In the case with only difficulty validation, these are the information sets (m0 , h) and
(m1 , h), i.e., getting caught guessing about an unsolvable problem. If these are equal to
zero, then a good but uninformed type knows they will look incompetent and get a worse
policy upon sending either m0 or m1 , so they have no incentive to deviate. A bad type
guessing m1 gets expected payoff:

                                         pδ + γpω .

which is strictly higher than the payoff for sending m0 . So, the constraint for an honest
equilibrium is:

                       (1 − pδ )pθ + γ(1 − pω (1 − pω )) ≥ pδ + γpω
                                                      pδ (1 + pθ ) − pθ
                                                γ≥                      .
                                                          (1 − pω )2

As γ → 0, this holds when:

                                                 pθ
                                        pδ ≥          .                                   (7)
                                               1 + pθ


When (7) holds, then there can be a fully honest equilibrium even as γ → 0. Importantly,


                                               29
any policy concerns, when combined with difficulty validation, can induce all uninformed
experts to admit uncertainty.

If (7) does not hold, there can also be an MSE where all of the good types report honestly
and the bad types play a mixed strategy over (m0 , m1 , m∅ ). There is a more subtle incentive
compatibility constraint that must be met for this equilibrium to hold: if the bad types are
indifferent between sending m0 and m1 , it can be the case that the informed types prefer to
deviate to sending the other informed messages (i.e., the s1 type prefers to send m0 ). See
the proof in Appendix B for details; in short, if the probability of a solvable problem is not
too low or the probability of the state being one is not too high, then this constraint is not
violated and there is an MSE where all of the good types send their honest message.27

Proposition 8. As γ → 0 with difficulty validation, there is an honest MSE if and only if
       pθ
pδ ≤ 1+p  θ
            . If not, and pδ ≥ 2pω − 1, then there is an MSE where the good types send their
honest message and the bad types use the following strategy:
                                                                                
                                          1−pδ (1+pθ )    pδ ∈        pθ
                                                                          , 1
                                               1−pθ                   1+pθ 1+pθ
                           σb∗ (m∅ ) =                                                ,                     (8)
                                         0                         1
                                                           pδ >   1+pθ
                                                                       ,
                           σb∗ (m0 ) = (1 − pω )(1 − σb∗ (m∅ )),
                           σb∗ (m1 ) = pω (1 − σb∗ (m∅ )).



Full validation. Since no pair of types is always payoff equivalent with full validation
and no policy concerns, adding small policy concerns in this case has no impact on the set
of MSE strategies for similar reasons as the state validation case.
  27
     Here is an example where this constraint is violated. Suppose pω is close to 1, and the bad types usually
send m1 , and rarely m0 . Then the tradeoff they face is that sending m1 leads to a better policy, but a lower
competence payoff when the problem is easy (when the problem is hard, the competence payoff for either
guess is zero). Now consider the good expert who observes signal s1 . Compared to the bad expert, this type
has a marginally stronger incentive to send m1 (since pω is close to 1). However, this type knows that he
will face a reputational loss for sending m1 rather than m0 , while the bad type only experiences this loss
with probability pδ . So, the bad type being indifferent means the type who knows the state is 1 has a strict
incentive to deviate to m0 . In general, this deviation is tough to prevent when pδ is low and pω is close to 1,
hence the condition in the proposition.




                                                      30
7    Comparative Statics

We now ask how changing the probability parameters of the model affects the communi-
cation of uncertainty by uninformed experts, and the expected value of the DM’s action in
equilibrium. In principle, there are many cases to consider – four validation regimes, with
no, small, or large policy concerns. Here we will focus on the three message equilibrium
identified in Proposition 8 (i.e., our preferred case of difficulty validation and small policy
concerns).



Better Experts Yields Better Outcomes. First consider the effect of increasing pθ . Hold-
ing fixed expert strategies, adding more competent experts has the obvious effect of more
informative messages and better decisions.

Equilibrium comparative statics on the frequency of experts admitting uncertainty are more
nuanced.

To see how having more competent experts affects the unconditional probability of the
expert admitting uncertainty, an important intermediate factor is whether competent types
are more or less likely to send m∅ . Given they are always honest in this equilibrium, good
types will send m∅ with probability 1 − pδ . The bad types may admit uncertainty more or
less frequently: in an honest equilibrium they send m∅ with probability 1, and for parts of
the parameter space they always guess. Plugging in their mixed strategy derived in equation
8, the bad types send m∅ with a probability higher than 1 − pδ if and only if pδ < 1/2.

Next, consider how increasing the proportion of competent experts affects the bad type
strategy (recall the good types always report honestly in this equilibrium). Algebraically,
the effect is obtained by differentiating (8). From this we find that the change in the prob-
ability that the bad expert sends m∅ is (1−p1−2pδ
                                               θ)
                                                 2 . This is positive if pδ < 1/2, negative if

pδ > 1/2, and zero if pδ = 1/2. Intuitively, when pδ is low, most of the competent types
will be uninformed. Since they are honest in this equilibrium, adding more competent types
tends to make admitting uncertainty more attractive for the bad types since there is a larger
group of good but uninformed experts to pool with. On the other hand, when pδ is high,
competent types are usually informed, and so adding more of them makes guessing more
attractive for the bad types.



                                              31
Figure 4 shows how changing pθ affects the equilibrium strategies and probability of ad-
mitting uncertainty in the top panels; and the expected value of the decision in the bottom
panels. In the left panels, pδ = 0.3, and in the right panels pδ = 0.7.

Starting with the top panels, the grey line represents the bad type’s probability of admitting
uncertainty. As noted above, this is increasing in pθ when pδ is small (left panel), and
eventually the equilibrium is honest.

The black line represents the unconditional probability of sending m∅ . When pδ < 1/2
(left panels), for a fixed bad type strategy adding more competent experts decreases the
unconditional probability of the expert admitting uncertainty. So, to the right of the dashed
line (where the equilibrium is honest and hence the bad type strategy goes not change),
increasing pθ leads to less admission of uncertainty. However, to the left of the dashed line,
adding more competent experts makes the bad types admit uncertainty more often. And
since this part of the parameter space is when most experts are incompetent, this effect
dominates and the unconditional probability of admitting uncertainty goes up.

When pδ > 1/2, (right panels), the bad types admit uncertainty less often than the good
types. So, increasing pθ leads to more admission of uncertainty for a fixed bad type strat-
egy, as in the right part of the figure where the bad types always guess. However, the effect
on the bad type strategy again works in the opposite direction for low pθ : adding more com-
petent experts makes the bad types guess more, and since most experts aren’t competent
this leads to less admission of uncertainty.

The bottom panels show the expected value of the decision with (hypothetical) honesty
using a dotted line, and the equilibrium value with a black line. These panels illustrate the
more intuitive fact that as there are more competent experts, the value of the decision made
is always increasing in the proportion of competent experts. The only part of the parameter
space where there is a countervailing effect where adding more competent experts makes
the bad types use a less informative strategy is when pδ is high (and pθ low), which is
precisely when it is valuable to have more competent exerts since they will learn the state.

Keeping the exogenous parameters fixed, more admission of uncertainty when experts are
in fact uninformed clearly leads to better decisions. However, one might expect that in
environments where experts admit uncertainty more often, there is generally less informa-
tion to convey, and hence worse decisions are made. Comparing the top and bottom panels


                                             32
                                                 Figure 4: Comparative Statics in pθ


                                                     Honest
                         1




                                                                                               1
Probability of m∅




                                                                      Probability of m∅
                       1 − pδ




                                                                                             1 − pδ
                         0




                                                                                               0
                                0                                1                                    0                                1
                                    Probability Competent (pθ)                                            Probability Competent (pθ)
                                                     Honest
                         1




                                                                                               1
   Value of Decision




                                                                         Value of Decision
                         v




                                                                                               v




                                0                                1                                    0                                1
                                    Probability Competent (pθ)                                            Probability Competent (pθ)




Notes: Admission of uncertainty (top panels) and expected value of decision (bottom panels) as a function of
pθ . For the left two panels, pδ = 0.3, and for the right two panels pδ = 0.7. (pω does not affect these figures.)
In the top panels, the black line is the unconditional probability of admitting uncertainty, and the grey line
is the probability of admitting uncertainty when θ = b. In the bottom panels, the dotted line is the expected
value of the decision if the expert is honest, and the black line is the equilibrium expected value.


highlights two scenarios where this intuition is wrong, as the frequency of admission of
uncertainty and the quality of decisions move in the same direction. First, when most prob-
lems are hard and most experts are incompetent, adding more competent experts makes
the modal (i.e., bad) expert more willing to admit uncertainty. Second, when most prob-
lems are easy and most experts are competent, only competent experts are willing to admit
uncertainty, so adding more of them leads to hearing “I Don’t Know” more often.

Formalizing these observations:

Proposition 9. With DV and γ → 0, in the equilibrium where the good types send the
honest message:
(i) P(m∅ |θ = b) is increasing in pθ if pδ > 1/2 and decreasing pθ if pδ < 1/2, and

                                                                 33
(ii) The expected value of the decision is strictly increasing in pθ .



Easy Problems Can be Harder. Now consider how changing the probability that the
problem is solvable affects admission of uncertainty and the value of decisions. On the
admission of uncertainty, the result is straightforward: making the problem more likely to
be solvable decreases the probability of sending m∅ . This is for two reasons. First, the
good experts get an informative signal more often, and hence (correctly) admit uncertainty
less often. Second, the bad experts are less apt to admit uncertainty when problems are
more likely to be easy. Again, this results from the fact that increasing pδ means there is a
larger pool of good experts sending informed messages and a smaller proportion sending
m∅ . The top panels of Figure 5 illustrate these claims: when pδ is small the equilibrium
is honest, and when it is large the bad types always guess. For an intermediate range, the
probability of admitting uncertainty is interior and decreasing in pδ .

While the direct and equilibrium effects move in the same direction for the admission
of uncertainty, they move in opposite directions for the value of decisions. The bottom
panels of Figure 5 illustrate. The expected value of the decision if the expert were to
be honest (dashed line) is unsurprisingly increasing in pδ , as the good experts are more
likely to send an informative message. However, in the intermediate range where easier
problems lead to more guessing, the equilibrium expected value of the decision (solid)
can decrease as problems get easier. In this part of the parameter space, experts send
informative messages more often, but this is partly because of the fact that bad experts are
guessing more. So, the decision-maker can no longer ever be confident that the state is zero
or one, and this confidence decreases as problems get easier and more bad types guess.
While this equilibrium effect need not always outweigh the benefits of the good experts
being informed more often, for any value of pω and pθ there is always some range of pδ
where marginal increases in pδ to worse decisions:

Proposition 10. With DV and γ → 0, in the equilibrium where the good types send the hon-
est message, there exists a p̃δ ∈ (pθ /(1 + pθ ), 1/(1 + pθ )] such that v ∗ is strictly decreasing
in pδ for pδ ∈ (pθ /(1 + pθ ), p̃δ ).




                                                34
                                                    Figure 5: Comparative Statics in pδ


                               Honest                                                                   Honest
                       1




                                                                                                1
Probability of m∅




                                                                         Probability of m∅
                       0




                                                                                                0
                           0                                         1                              0                                  1
                                        Probability Solvable (pδ)                                          Probability Solvable (pδ)
                               Honest                                                                   Honest
                       1




                                                                                                1
   Value of Decision




                                                                            Value of Decision
                       v




                                                                                                v




                           0                                         1                              0                                  1
                                        Probability Solvable (pδ)                                          Probability Solvable (pδ)




Notes: Admission of uncertainty (top panels) and expected value of decision (bottom panels) as a function
of pδ . In the left panels, pθ = 1/3, and in the right panels pθ = 2/3. (pω does not affect these figures.) In
the top panels, the black line is the unconditional probability of admitting uncertainty, and the grey line is the
probability of admitting uncertainty when θ = b. In the bottom panels, the dotted line is the expected value
of the decision if the expert is honest, and the solid line is the equilibrium expected value.




                                                                    35
8    Discussion

This paper has studied the strategic communication of uncertainty by experts with reputa-
tional concerns. Our analysis is built on two theoretical innovations: first, in our setup, the
decision-maker is uncertain not only about the state of the world, but also about whether
the state is knowable, that is, whether a qualified expert could know it. This formalizes the
idea that part of being a domain expert is not merely knowing the answers to questions, but
knowing how to formulate the questions themselves. The second innovation concerns the
notion of “credible beliefs," which is closely tied to structural consistency of beliefs (Kreps
and Wilson, 1982). Honest communication in our model is disciplined by experts’ reputa-
tional concerns– off-path, they are punished by the low opinion of the decision-maker. But
what can the she credibly threaten to believe? Our use of Markov sequential equilibrium
restricts the decision maker to structurally consistent beliefs – that is, we do not allow the
decision-maker to update on payoff-irrelevant information. We say that such beliefs are not
credible because she cannot construct a candidate Markov strategy to rationalize them.

In this setting we have asked the following question: what would the decision maker want
to learn, ex post, in order to induce the experts, ex ante, to communicate their information
honestly? We found that the intuitive answer – checking experts’ reports against the true
state of the world – is insufficient. Even if the decision-maker catches an expert red-handed
in a lie, they are constrained by the fact that good experts facing unanswerable questions
are in the same conundrum as bad experts. Therefore, we show, state validation alone
never induces honesty. In order to elicit honest reports from experts, it is necessary that
the decision-maker also learns whether the problem is difficult. Indeed, in environments
where the expert has even very small policy concerns, difficulty validation alone may be
sufficient.

What does it mean for the decision-maker to “learn the difficulty" of the problem ex post?
On the one hand, we note that this is functionally how empirical work is evaluated in aca-
demic journals in economics. Referee reports in empirical economics typically center on
questions of identification – whether a parameter is knowable in the research design – rather
than the parameter value itself. However an alternative interpretation of difficulty valida-
tion concerns organizational structure and the management of experts. Should experts be
allocated to product teams, managed by decision-makers who cannot evaluate their work?
Or alternatively, should organizations subscribe to the “labs" model, in which experts are


                                              36
managed by other experts? We view our results as evidence for the latter. Perhaps it is
unsurprising then, that this is precisely what firms in the tech sector – a sector opening new
markets and raising new economic questions, pricing salient among them – are doing.

The literature that precedes us has shown the following to be robust: that when good experts
receive imperfect signals and all experts have reputational concerns, it is difficult to induce
honest strategic communication. We offer a simple intuition for this finding: predictive ac-
curacy distinguishes the informed from the uninformed, not necessarily the good from the
bad. If the decision-maker can learn about the problem itself they can generate informa-
tional asymmetries between good uninformed experts and bad ones, and more effectively
incentivize honesty. Here we have focused on problem difficulty because we believe that
decision-makers often ask unanswerable questions, but we also believe that this simple in-
tuition may take other forms, and that further development of this idea is a fruitful area for
future work.




                                              37
References
Avery, C. N. and Chevalier, J. A. (1999). Herding over the career. Economics Letters,
  63:327–333.

Bergemann, D. and Hege, U. (2005). The financing of innovation: Learning and stopping.
  RAND Journal of Economics, 36(4):719–752.

Bergemann, D. and Hörner, J. (2010). Should auctions be transparent? Cowles Foundation
  Discussion Paper No. 1764.

Bhaskar, V., Mailath, G. J., and Morris, S. (2013). A foundation for markov equilibria in
  sequential games with finite social memory. Review of Economic Studies, 80(3):925–
  948.

Bolton, P., Freixas, X., and Shapiro, J. (2012). The credit ratings game. The Journal of
  Finance, 67(1):85–111.

Brandenburger, A. and Polak, B. (1996). When managers cover their posteriors: Making
  the decisions the market wants to see. The RAND Journal of Economics, 27(3):523–541.

Chevalier, J. and Ellison, G. (1999). Career concerns of mutual fund managers. Quarterly
  Journal of Economics, 114(2):389–432.

Crawford, V. P. and Sobel, J. (1982). Strategic information transmission. Econometrica:
  Journal of the Econometric Society, pages 1431–1451.

Deb, R., Pai, M., and Said, M. (2018). Evaluating strategic forecasters. forthcoming,
  American Economic Review.

Dye, R. A. (1985). Disclosure of nonproprietary information. Journal of Accounting Re-
  search, 23(1):123–145.

Ely, J. C. and Välimäki, J. (2003). Bad reputation. Quarterly Journal of Economics,
  118(3):785–814.

Ericson, R. and Pakes, A. (1995). Markov-perfect industry dynamics: A framework for
  empirical work. Review of Economic Studies, 62(1):53–82.

Fudenberg, D. and Tirole, J. (1991). Game Theory. MIT Press.

Garicano, L. (2000). Hierarchies and the organization of knowledge in production. Journal
  of Political Economy, 108(5):874–904.

Gordon, B. and Zettelmeyer, F. (2016). A comparison of approaches to advertising mea-
  surement: Evidence from big field experiments at facebook. Working Paper.

Harsanyi, J. C. and Selten, R. (1988). A General Theory of Equilibrium Selection in Games.
  MIT Press, Cambridge, MA.


                                           38
Holmström, B. (1999). Managerial incentive problems: A dynamic perspective. The Re-
  view of Economic Studies, 66(1):169–182.

Hong, H. and Kubik, J. D. (2003). Analyzing the analysts: Career concerns and biased
  earnings forecasts. Journal of Finance, pages 313–351.

Jung, W.-O. and Kwon, Y. K. (1988). Disclosure when the market is unsure of the infor-
  mation endowment of managers. Journal of Accounting Research, 26(1):146–153.

Kartik, N. (2009). Strategic communication with lying costs. Review of Economic Studies,
  76:1359–1395.

Kiureghian, A. D. and Ditlevsen, O. (2009). Aleatory or epistemic? does it matter? Struc-
  tural Safety, 31:105–112.

Kreps, D. M. and Ramey, G. (1987). Structural consistency, consistency, and sequential
  rationality. Econometrica, 55(6):1331–1348.

Kreps, D. M. and Wilson, R. (1982). Sequential equilibria. Econometrica, 50(4):863–894.

Lazear, E. P. (2005). Entrepreneurship. Journal of Labor Economics, 23(4):649–680.

Levitt, S. and Dubner, S. J. (2014). Think Like a Freak. William Morrow and Company.

Manski, C. F. (2013). Public Policy in an Uncertain World: Analysis and Decisions. Har-
 vard University Press.

Maskin, E. and Tirole, J. (1988a). A theory of dynamic oligopoly, i: Overview and quantity
 competition with large fixed costs. Econometrica, 56(3):549–569.

Maskin, E. and Tirole, J. (1988b). A theory of dynamic oligopoly, ii: Price competition,
 kinked demand curves, and edgeworth cycles. Econometrica, 56(3):571–599.

Maskin, E. and Tirole, J. (2001). Markov perfect equilibriu: 1. observable actions. Journal
 of Economic Theory, 100:191–219.

Morris, S. (2001). Political correctness. Journal of Political Economy, 109(2):231–265.

Nash, J. F. (1950). The bargaining problem. Econometrica, 18(2):155–162.

Ottaviani, M. and Sørensen, P. N. (2006a). Reputational cheap talk. RAND Journal of
  Economics, 37(1).

Ottaviani, M. and Sørensen, P. N. (2006b). The strategy of professional forecasting. Journal
  of Financial Economics, 81:441–466.

Prat, A. (2005). The wrong kind of transparency. American Economic Review, 95(3):862–
  877.

Prendergast, C. (1993). A theory of “yes men". American Economic Review, 83(4):757–
  770.

                                            39
Prendergast, C. and Stole, L. (1996). Impetuous youngsters and jaded old-timers: Acquir-
  ing a reputation for learning. Journal of Political Economy, 104(6):1105–1134.

Raith, M. and Fingleton, J. (2005). Career concerns of bargainers. Journal of Law, Eco-
  nomics, and Organization, 21(1):179–204.

Scharfstein, D. S. and Stein, J. C. (1990). Herd behavior and investment. The American
  Economic Review, pages 465–479.

Selten, R. (1978). The chain store paradox. Theory and Decision, 9:127–159.

Sobel, J. (1985). A theory of credibility. The Review of Economic Studies, pages 557–573.

Tadelis, S. (2013). Game Theory: An Introduction. Princeton University Press, Princeton,
  NJ.




                                           40
A         Markov Sequential Equilibrium

A.1        MSE and SE

Here we offer a brief discussion of the prior use of the Markov sequential equilibrium
(MSE) solution concept as well as an illustration of its implications as a refinement on
off-path beliefs.

MSE is the natural extension of Markov Perfect Equilibrium to incomplete information
games. However, its usage is infrequent and sometimes informal. To our knowledge,
there is no general treatment nor general guidance to the construction of the maximally
coarse (Markov) partition of the action space, unlike the case of MPE (Maskin and Tirole,
2001) Bergemann and Hege (2005) and Bergemann and Hörner (2010) employ the solution
concept, defining it as a perfect Bayesian equilibrium in Markovian strategies. In other
words, they impose the Markov restriction only on the sequential rationality condition.
This is different and rather weaker than our construction; our definition of MSE imposes
the Markov assumption on both sequential rationality as well as consistency. While they
do not use the Markov restriction to refine off-path beliefs, this is of no consequence for
their applications.

To see the relevance of MSE to off-path beliefs, consider the game illustrated in Figure A.1,
which is constructed to mirror an example from Kreps and Wilson (1982).28 First, nature
choses Player 1’s type, a or b. Next, Player 1 choses l or r. Finally, Player 2 choses u or
d. Player 2 is never informed of Player 1’s type. Whether Player 1 knows their own type is
the key difference between the two games.

In the first game, the player does not know their type. Posit an equilibrium in which Player
1 always choses l. What must Player 2 believe at a node following r? If the economist
is studying perfect Bayesian equilibrium (PBE), they may specify any beliefs they wish.
Alternatively, if they are studying sequential equilibrium (SE), Player 2 must believe that
Player 1 is of type a with probability p.

In the second game depicted, SE imposes no restriction on Player 2’s off-path beliefs.
However, MSE may. If π1 (a, l, ·) = π1 (b, l, ·) and π1 (a, r, ·) = π1 (b, r, ·) then we say
  28
       See, in particular, their Figure 5 (p.873).


                                                     41
             Figure A.1: Consistency, Markov Consistency, and Off-Path Beliefs


  Nature
                              a             b                               a             b

  Player 1

                                  r                     r                       r                     r
                      l                     l                       l                     l
 Player 2



                 u        d   u       d u       d   u       d   u       d   u       d u       d   u       d




Notes: This figure depicts two games, which differ in whether Player 1 knows their own type. Their type, a
or b, is chosen by Nature with P{a} = p and P{b} = 1 − p. Player 1 chooses l or r, and Player 2 sees this
and reacts with u or d. Payoffs are omitted, but can be written πi (·, ·, ·).


that Player 1’s type is payoff irrelevant. The restriction to Markov strategies implies that
Player 1’s strategy does not depend upon their type. Markov consistency implies that,
further, Player 2 cannot update about payoff irrelevant information. Therefore Player 2
must believe that Player 1 is of type a with probability p.



A.2     Non-Markovian PBE

Here we briefly discuss PBE that fail the Markov consistency requirement of MSE, and
argue why we believe these equilibria are less sensible

In particular, we demonstrate that the most informative equilibrium under no policy con-
cerns and all but full validation involves more transmission of uncertainty and also infor-
mation about the state. However, these equilibria are not robust to minor perturbations,
such as introducing a vanishingly small cost of lying.




                                                    42
Example 1: Admission of Uncertainty with No Validation. Even without the Markov
restriction, it is immediate that there can be no fully honest equilibrium with no validation.
In such an equilibrium, the competence assessment for sending either m0 or m1 is 1, and
the competence assessment for sending m∅ is π∅ < 1. So the uninformed types have a strict
incentive to deviate to m0 or m1 .

However, unlike the case with the Markov restriction which leads to babbling, there is an
always guessing equilibrium: If all uninformed types send m1 with probability pω and m0
otherwise, the competence assessment upon observing either message is pθ . So no type has
an incentive to deviate from the honest mesage.

Further, it is possible to get admission of uncertainty if the good and bad uninformed types
play different strategies. In the extreme, suppose the good types always send their honest
message, including the uninformed sending m∅ . If the bad types were to always send m0 or
m1 , then the competence assessment upon sending m∅ would be 1. In this case, saying “I
don’t know” would lead to the highest possible competence evaluation, giving an incentive
for all to admit uncertainty even if they know the state.

It is straightforward to check that if the bad types mix over message (m0 , m1 , m∅ ) with
probabilities (pδ (1 − pω ), pδ pω , 1 − pδ ), then the competence assessment upon observing
all messages is pθ , and so no expert has an incentive to deviate.

A common element of these equilibria is that the competence assessment for any on-path
message is equal to the prior. In fact, a messaging strategy can be part of a PBE if and only
if this property holds: the competence assessments must be the same to prevent deviation,
and if they are the same then by the law of iterated expectations they must equal the prior.
So, there is a range of informative equilibria, but they depend on types at payoff-equivalent
information sets taking different actions, a violation of Markov strategies that renders them
sensitive to small perturbations of the payoffs.



Example 2: Honesty with State Validation or Difficulty Validation. Now return to the
state validation case, and the conditions for an honest equilibrium. Without the Markov
restriction on beliefs, it is possible to set the off-path belief upon observing an incorrect
guess to 0. With this off-path belief, the incentive compatibility constraint to prevent send-
ing m1 becomes π∅ ≥ pω . Since π∅ is a function of pθ and pδ (but not pω ), this inequality


                                             43
holds for a range of the parameter space. However, this requires beliefs that are not Markov
consistent – the DM who reaches that off-path node cannot construct a Markov strategy to
rationalize their beliefs. So we argue that the threat of these beliefs not credible.

Similarly, without the Markov restriction it is possible to get honesty with just difficulty
validation. The binding constraint is that if any off-path message leads to a zero com-
petence evaluation, the bad type gets a higher payoff from sending m∅ (as will the full
validation case, (1 − pδ )pθ ) than from sending m1 (now pδ ). So, honesty is possible if
(1 − pδ )pθ > pδ . This is a violation of Markov strategies and therefore sensitive to payoff
perturbations, however in the following section we show that the same equilibrium is a
MSE in the presence of small policy concerns.



The Fragility of These Examples. A standard defense of Markov strategies in repeated
games is that they represent the simplest possible rational strategies (Maskin and Tirole,
2001). The similar principle applies here: rather than allowing for types with the same
(effective) information to use different mixed strategies sustained by indifference, MSE
focuses on the simpler case where those with the same incentives play the same strategy.

Further, as shown by Bhaskar et al. (2013) for the case of finite social memory, taking limits
of vanishing, independent perturbations to the payoffs – in the spirit of Harsanyi and Selten
(1988) “purification” – results in Markov strategies as well. Intuitively, suppose the expert
receives a small perturbation to his payoff for sending each message which is independent
of type and drawn from a continuous distribution, so he has a strict preference for sending
one message over the others with probability one. Payoff-indifferent types must use the
same mapping between the perturbations and messages, analogous to Markovian strategies.
Further, if these perturbations put all messages on path, then all beliefs are generated by
Markovian strategies.29
  29
     A related refinement more specific to our setting is to allow for a small “lying cost” for sending a message
not corresponding to the signal, which is independent of the type (Kartik, 2009).




                                                      44
B     Proofs

Proof of Proposition 1: For convenience, we extend the definition of v so v(a, πω ) rep-
resents the expected quality of policy a under the belief that the state is 1 with probability
πω .

The DM’s expected payoff from the game can be written as the sum over the (expected)
payoff as a function of the expert signal:
                           X                    X
                                         P(s)       P(m|s)v(a∗ (m), P(ω|s)).              (9)
                       s∈{s0 ,s1 ,s∅ }          m


In the honest equilibrium, when the expert observes s0 or s1 , the DM takes an action equal
to the state with probability 1, giving payoff 1. When the expert observes s∅ , the equi-
librium action is pω giving payoff v(pω , pω ) = 1 − pω (1 − pω ). So, the average payoff
is:

                              pθ pδ 1 + (1 − pθ pδ )pω (1 − pω ) = v.



This payoff as expressed in (9) is additively separable in the signals, and v is and globally
concave in a for each s. So, for each s ∈ {s0 , s1 , s∅ }, this component of the sum is
maximized if and only if a∗ (m) is equal to the action taken upon observing the honest
message is with probability 1. That is, it must be the case that:
                                     
                                     
                                     
                                      1             m : P r(m|s1 ) > 0
                                     
                             a∗ (m) = pω             m : P r(m|s∅ ) > 0                  (10)
                                     
                                     
                                     0
                                     
                                                     m : P r(m|s0 ) > 0



If the equilibrium is not honest, then there must exist a message m0 such that P(s|m0 ) < 1
for all s. At least one of the informed types must send m0 with positive probability; if
not, P(s∅ |m0 ) = 1. Suppose the type observing s0 sends m0 with positive probability.
(An identical argument works if it is s1 .) To prevent P(s0 |m0 ) = 1 another type must
send this message as well, and so in response the DM chooses an action strictly greater
than 0, contradicting condition (10) and hence the expected quality of the decision in any
equilibrium which is not honest is strictly less than v.

                                                     45
Proof of Proposition 2: For any messaging strategy, the DM must form a belief about
the expert competence for any message (on- or off-path), write these πθ (m). So, for any
type θ, the expected utility for sending message m is just πθ (m). All types are payoff-
equivalent in any equilibrium, and therefore in any MSE they must use the same strategy.
Since all messages are sent by both informed and uninformed types, there is no admission
of uncertainty.



Proof of Proposition 3: Part i is immediate in a babbling equilibrium: there is no ad-
mission of uncertainty since there are no messages only sent by the uninformed types. So,
given propositions 12 and 13 in Appendix C, what remains to be shown is that there is no
admission of uncertainty in a MSE where the s0 types send m0 and the s1 types send m1 .
Equivalently, in this equilibrium the uninformed types must always send m0 or m1 .

Recall the Markov strategy restriction implies the good and bad uninformed types use the
same strategy. Suppose the uninformed types send m∅ with positive probability. The com-
petence assessment for sending m∅ is π∅ . Writing the probability the uninformed types
send m1 with σ∅ (m1 ), the competence assessment for sending m1 is:

                                            pθ pδ pω + pθ (1 − pδ )σ∅ (m1 )
           P(θ = g|m1 ; σ∅ (m1 )) =
                                    pθ pδ pω + (pθ (1 − pδ ) + (1 − pθ ))σ∅ (m1 )
                                            pθ pδ pω + pθ (1 − pδ )
                                  ≥
                                    pθ pδ pω + (pθ (1 − pδ ) + (1 − pθ ))
                                            pθ (1 − pδ )
                                  >                            = π∅ .
                                    pθ (1 − pδ ) + (1 − pθ ))

Since the competence assessment for sending m1 is strictly higher than for sending m∅ ,
there can be no MSE where the uninformed types admit uncertainty, completing part i.

For part ii, first consider the condition for an equilibrium where both m0 and m1 are sent
by the uninformed types. The uninformed types must be indifferent between guessing m0
and m1 . This requires:

            pω πθ (m1 , ω = 1) + (1 − pω )π∅ = (1 − pω )πθ (m0 , ω = 0) + pω π∅      (11)




                                            46
where the posterior beliefs upon “guessing right” are given by Bayes’ rule:

                            P(θ = g, ω = 1, m1 )     pω pθ (pδ + (1 − pδ )σ∅ (m1 ))
            πθ (m1 , ω = 1) =                    =
                                P(m1 , ω = 1)       pω (pθ pδ + (1 − pθ pδ )σ∅ (m1 ))
                       P(θ = g, ω = 0, m0 )    (1 − pω )pθ (pδ + (1 − pδ )σ∅ (m0 ))
     πθ (m0 , ω = 0) =                      =
                          P(m0 , ω = 0)       (1 − pω )(pθ pδ + (1 − pθ pδ )σ∅ (m0 ))


Plugging these into (11) and solving for the strategies with the additional constraint that
σ∅ (m0 ) + σ∅ (m1 ) = 1 gives:

                                           1 − pω (1 + pθ pδ )
                                σ∅ (m0 ) =
                                                1 − pθ pδ
                                           pω (1 + pθ pδ ) − pθ pδ
                                σ∅ (m1 ) =                         .
                                                  1 − pθ pδ

For this to be a valid mixed strategy, it must be the case that both of these expressions are
between zero and one, which is true if and only if pω < 1/(1 + pθ pδ ) ∈ (1/2, 1). So, if
this inequality holds and the off-path beliefs upon observing m∅ are sufficiently low, there
is an MSE where both messages are sent by the uninformed types. And the competence
assessment for any off-path message/validation can be set to π∅ , which is less than the
expected competence payoff for sending either m0 or m1 .

Now consider an equilibrium where uninformed types always send m1 . The on-path mes-
sage/validation combinations are then (m1 , ω = 0), (m1 , ω = 1), and (m0 , ω = 0), with
the following beliefs about the expert competence:

                                        pθ (1 − pδ )
                πθ (m1 , ω = 0) =                        ;
                                  pθ (1 − pδ ) + 1 − pθ
                                         pθ pδ + pθ (1 − pδ )
                πθ (m1 , ω = 1) =                                  = pθ , and
                                  pθ pδ + pθ (1 − pδ ) + (1 − pθ )
                πθ (m0 , ω = 0) = 1.



Preventing the uninformed types from sending m0 requires:

                                    pθ (1 − pδ )
          pω pθ + (1 − pω )                         ≥ pω πθ (m0 , ω = 1) + (1 − pω ).
                              pθ (1 − pδ ) + 1 − pθ

This inequality is easiest to maintain when πθ (m0 , ω = 1) is small, and by the argument in
the main text in an MSE it must be at least π∅ . Setting πθ (m0 , ω = 1) = π∅ and simplifying

                                                47
gives pω ≥ 1/(1 + pθ pδ ), i.e., the reverse of the inequality required for an MSE where both
m0 and m1 are sent. Again, setting the competence assessment for an off-path message to
π∅ prevents this deviation.

So, if pω ≤ 1/(1 + pθ pδ ) there is an MSE where both messages are sent, and if not there is
an MSE where only m1 is sent.

Finally, it is easy to verify there is never an MSE where only m0 is sent, as the uninformed
types have an incentive to switch to m1 .



Proof of Proposition 4: Given the payoff equivalence classes, the good and informed
type must used the same mixed strategy. In any MSE, the posterior belief about the state
upon observing an on-path message m can be written as a weighted average of the belief
about the state conditional on being in each equivalence class, weighted by the probability
of being in the class:

P(ω = 1|m) = P(ω = 1|m, θ = g, s ∈ {s0 , s1 })P(θ = g, s ∈ {s0 , s1 }|m)
               + P(ω = 1|m, θ = g, s = s∅ )P(θ = g, s = s∅ |m)
               + P(ω = 1|m, θ = b)P(θ = b|m)
             = pω P(θ = g, s ∈ {s0 , s1 }|m) + pω P(θ = g, s = s∅ |m) + pω P(θ = b|m) = pω .

For each equivalence class there is no information conveyed about the state, so these con-
ditional probabilities are all pω , and hence sum to this as well.

For part ii, we construct an equilibrium where the informed types always send me (“the
problem is easy”), the good but uninformed types send mh (“the problem is hard”), and
the bad types mix over these two messages with probability (σb (me ), σb (mh )). Since mh
is never sent by the informed types, sending this message admits uncertainty.

There can be an equilibrium where both of these messages are sent by the bad types if
and only if they give the same expected payoff. Writing the probability of sending me as
σb (me ), this is possible if:

           pδ πθ (me , e) + (1 − pδ )πθ (me , h) = pδ πθ (mh , e)(1 − pδ )πθ (mh , h),




                                               48
– or, rearranged:

                        pθ pδ                                     pθ (1 − pδ )
         pδ                             = (1 − pδ )                                        .                        (12)
              pθ pδ + (1 − pθ )σb (me )             pθ (1 − pδ ) + (1 − pθ )(1 − σb (me ))

The left-hand side of this equation (i.e., the payoff to guessing the problem is easy) is
decreasing in σb (me ), ranging from pδ to pδ pθ pδp+(1−p
                                                      θ pδ
                                                           θ)
                                                              . The right hand side is increasing in
                                      pθ (1−pδ )
σb (me ), ranging from (1 − pδ ) pθ (1−pδ )+(1−pθ ) to 1 − pδ . So, if

                                                  pθ pδ
                                       pδ                     − (1 − pδ ) ≥ 0,                                      (13)
                                            pθ pδ + (1 − pθ )

then payoff to sending me is always higher. After multiplying through by pθ pδ + (1 − pθ ),
the left-hand
       √      side of (13) is quadratic in pδ (with a positive pδ term), and has a root at
2pθ −1+     1+4pθ −4p2θ
           4pθ
                   which is always on (1/2, 1), and a negative root.30 So, when pδ is above
this root, the payoff to sending me is always higher, and hence there is a MSE where the
uninformed types always send this message.

On the other hand, if

                                                     pθ (1 − pδ )
                                   (1 − pδ )                            − pδ ≥ 0,
                                               pθ (1 − pδ ) + (1 − pθ )

then the
       √payoff for sending mh is always higher, which by a similar argument holds if pδ ≤
2pθ +1−     1+4pθ −4p2θ
           4pθ
                        .
                    However, if neither of this inequalities hold, then there is a σb (me ) ∈
(0, 1) which solves (12), and hence there is an MSE where me is sent with this probability
and mh with complementary probability. Summarizing, there is an MSE where the bad
type sends message me with probability:
                                                                       √
                                                               2pθ +1−    1+4pθ −4p2θ
                       
                        0                              pδ ≤             4pθ
                       
                       
                                                                       √                      √               
                        pδ (pδ −pθ +2pδ pθ −2p2δ pθ )              2pθ +1−    1+4pθ −4p2θ 2pθ −1+ 1+4pθ −4p2θ
       σb∗ (me )   =                                    pδ ∈                             ,
                        (1−pθ )(1−2pδ (1−pδ ))
                                                                       √
                                                                             4pθ                  4pθ

                                                               2pθ −1+    1+4pθ −4p2θ
                       
                       
                       1                               pδ ≥             4pθ


and message mh with probability σb∗ (mh ) = 1 − σb∗ (mh ).
  30
       All of these observations follow from the fact that 1 + 4pθ − 4p2θ ∈ (1, (2pθ + 1)2 ).




                                                           49
Proof of Proposition 5: The condition for the honest equilibrium is proven in the main
text. So what remains is to show there is always an MSE where the good but uninformed
type always sends m∅ .

In such an equilibrium, message/validation combinations (m0 , 0, e), (m1 , 1, e) and (m∅ , 0, h)
and (m∅ , 1, h) are the only ones observed when the expert is competent. So, any other mes-
sage/validation combination is either on-path and only sent by the bad types, in which case
the competence assessment must be 0, or is off-path and can be set to 0.

The informed type observing s0 knows the validation will be 0, e, and (m, 0, e) leads to
competence assessment zero for m 6= m0 . So, this type has no incentive to deviate, nor
does the s1 type by an analogous argument. The good but uninformed type knows the
validation will reveal h, and the DM observing (mi , ω, h) for i ∈ {0, 1} and ω ∈ {0, 1}
will lead to a competence assessment of zero. So this type faces no incentive to deviate.

What remains is showing the bad type strategy. Write the whole strategy with σb =
(σb (m0 ), σb (m1 ), σb (m∅ )). Explicitly deriving the conditions for all forms the (mixed)
strategy can take is tedious; e.g., if pω is close to 1 and pδ is close to 1, the expert always
sends m1 , when pω is close to 1/2 and pδ is just below the threshold for an honest equi-
librium, all three message are sent. Write the bad type’s expected competence assessment
for sending each message when the DM expects strategy σ (averaging over the validation
result) as:

                                                        pθ
            Πθ (m∅ , b, σ) ≡ pδ 0 + (1 − pδ )                        ,
                                              pθ + (1 − pθ )σb (m∅ )
                                                   pθ
            Πθ (m0 , b, σ) ≡ pδ (1 − pω )                        + (1 − pδ )0, and
                                          pθ + (1 − pθ )σb (m0 )
                                             pθ
            Πθ (m1 , b, σ) ≡ pδ pω                         + (1 − pδ )0.
                                   pθ + (1 − pθ )σb (m1 )

Write the expected payoff to the bad expert choosing mixed strategy σ when the decision-
maker expects mixed strategy σ̂b as Π(σ, σ̂) = i∈{0,1,∅} σb (mi )Πθ (mi ; σ̂), which is con-
                                                    P

tinuous in all σb (mi ), so optimizing this objective function over the (compact) unit simplex
must have a solution. So, BR(σ̂b ) = arg maxσ Π(σ; σ̂) is a continuous mapping from the
unit simplex to itself, which by the Kakutani fixed point theorem must have a fixed point.
So, the strategy (or strategies) given by such a fixed point are a best response for the bad
type when the decision-maker forms correct beliefs given this strategy.




                                                50
Proof of Proposition 6:     See the proof of proposition 14 in Appendix D



Proof of Proposition 7:     See the proof of proposition 15 in Appendix D



Proof of Proposition 8     See the proof of proposition 16 in Appendix D



Proof of Proposition 9:     Part i is demonstrated in the main text.

For part ii, the result is immediate in the range of pδ where pθ does not change the bad
type strategy. For the range where the bad type strategy is a function of pθ , plugging in the
strategies identified in (8) and simplifying gives the expected quality of the decision is:

                                                (pδ pθ )2 pω (1 − pω )
                           1 − pω (1 − pω ) +                          .                 (14)
                                                 pδ − pθ (1 − 2pδ )

The derivative of (14) with respect to pθ is:

                            pω (1 − pω )p2δ pθ (2pδ (1 + pθ ) − pθ )
                                                                     .
                                   (pδ − pθ (1 − 2pδ ))2

which is strictly positive if pδ > 2(1+p
                                      pθ
                                         θ)
                                            . Since the range of pδ where the bad type plays a
mixed strategy is pδ ∈ (pθ /(1 + pθ ), 1/(1 + pθ )), this always holds.



Proof of Proposition 10: For the range pδ ∈ (pθ /(1 + pθ ), 1/(1 + pθ )), the expected
quality of the decision is (14). Differentiating with respect to pδ gives:

                             pω (1 − pω )pδ p2θ (pδ − 2pθ + 2pδ pθ )
                                     (pδ − pθ + 2pδ pθ )2

which, evaluated at pδ = pθ /(1 + pθ ) simplifies to −pω (1 − pω ). So, the value of the
decision must be locally decreasing at pδ = pθ /(1 + pθ ), and by continuity, for an open
interval pδ ∈ (pθ /(1 + pθ ), p̃δ ).




                                                51
C        Relabeling

We prove two kinds of results in the main text. Some are existence results: that for a
particular validation regime and part of the parameter space, an MSE with certain properties
exists. For these results the fact that we often restrict attention to the (m0 , m1 , m∅ ) message
set poses no issues: it is sufficient to show that there is an equilibrium of this form with the
claimed properties. However, propositions 2, 3, 6ii-iii, and 7, make claims that all (non-
babbling) MSE have certain properties.31 The proofs show that all equilibrium where the
s0 and s1 types send distinct and unique messages (labelled m0 and m1 ) and there is at
most one other message (labelled m∅ ) have these properties. Here we show this is WLOG
in the sense that with no validation or state validation, any non-babbling equilibrium can
be relabeled to an equilibrium of this form.

Consider a general messaging strategy where M is the set of messages sent with positive
probability. Write the probability that the informed types observing s0 and s1 and σ0 (m)
and σ1 (m). When the good and bad uninformed types are not necessarily payoff equivalent
we write their strategies σθ,∅ (m). When these types are payoff equivalent and hence play
the same strategy, we drop the θ: σ∅ (m). Similarly, let M0 and M1 be the set of messages
sent by the respective informed types with strictly positive probability, and Mg,∅ , Mb,∅ , and
M∅ the respective sets for the uninformed types, divided when appropriate.

As is standard in cheap talk games, there is always a babbling equilibrium:

Proposition 11. There is a class of babbling equilibria where σ0 (m) = σ1 (m) = σg,∅ (m) =
σb,∅ (m) for all m ∈ M .


Proof. If all play the same mixed strategy, then πθ (m, IDM 2 ) = pθ and a∗ (m, IDM ) = pω
for any m ∈ M and IDM . Setting the beliefs for any off-path message to be the same as
the on-path messages, all types are indifferent between any m ∈ M.


The next result states that for all cases with either state validation or policy concerns, in
any non-babbling equilibrium the informed types send no common message (note this re-
sult does not hold with difficulty validation; in fact the proof of proposition 4 contains a
counterexample):
  31
       Proposition 4 also makes a claim about all equilibria, but this is already proven in Appendix B.


                                                       52
Proposition 12. With either no validation or state validation (and any level of policy con-
cerns), any MSE where M0 ∩ M1 6= ∅ is babbling, i.e., σ0 (m) = σ1 (m) = σg,∅ (m) =
σb,∅ (m) for all m ∈ M .




Proof. We first prove the result with state validation, and then briefly highlight the aspects
of the argument that differ with no validation.

Recall that for this case the good and bad uninformed types are payoff equivalent, so we
write their common message set and strategy M∅ and σ∅ (m) The proof proceeds in three
steps.

Step 1: If M0 ∩ M1 6= ∅, then M0 = M1 . Let mc ∈ M0 ∩ M1 be a message sent by both
informed types. Suppose there is another message sent only by the s0 types: m0 ∈ M0 \M1 .
For the s0 type to be indifferent between m0 and mc :

                 πθ (mc , 0) + γv(a∗ (mc ), 0) = πθ (m0 , 0) + γv(a∗ (m0 ), 0).

For this equation to hold, it must be the case that the uninformed types send m0 with
positive probability: if not, then πθ (mc , 0) ≤ πθ (m0 , 0) = 1, but v(a∗ (mc ), 0) < 1 =
v(a∗ (m0 ), 0), contradicting the indifference condition.

For the uninformed types to send m0 , it must also be the case that his expected payoff for
sending this message, which can be written

          pω (πθ (m0 , 1) + γv(a∗ (m0 ), 1)) + (1 − pω )(πθ (m0 , 0) + v(a∗ (m0 ), 0))

– is at least his payoff for sending mc :

          pω (πθ (mc , 1) + γv(a∗ (mc ), 1)) + (1 − pω )(πθ (mc , 0) + v(a∗ (mc ), 0)).

The second terms, which both start with (1 − pω ), are equal by the indifference condition
for s0 types, so this requires:

                 πθ (m0 , 1) + γv(a∗ (m0 ), 1) ≥ πθ (mc , 1) + γv(a∗ (mc ), 1).

Since m0 is never sent by the s1 types, πθ (m0 , 1) = π∅ , while πθ (mc , 1) > π∅ . So, this


                                               53
inequality requires v(a∗ (m0 ), 1) > v(a∗ (mc ), 1), which implies a∗ (m0 ) > a∗ (mc ). A
necessary condition for this inequality is σσ∅0 (m
                                                (m0 )
                                                  0)
                                                      > σσ∅0 (m
                                                             (mc )
                                                                c)
                                                                   , which also implies πθ (mc , 0) >
πθ (m0 , 0). But if a∗ (m0 ) > a∗ (mc ) and πθ (mc , 0) > πθ (m0 , 0), the s0 types strictly prefer
to send mc rather than m0 , a contradiction. By an identical argument, there can be no
message in M1 \ M0 , completing step 1.

Step 2: If M0 = M1 , then σ0 (m) = σ1 (m) for all m. If M0 = M1 is a singleton, the result
is immediate. If there are multiple common messages and the informed types do not use
the same mixed strategy, there must be a message m0 such that σ0 (m0 ) > σ1 (m0 ) > 0 and
another message m1 such that σ1 (m1 ) > σ0 (m1 ) > 0. (We write the message “generally
sent by type observing sx ” with a superscript to differentiate between the subscript notation
referring to messages always sent by type sx .) The action taken by the DM upon observing
m0 must be strictly less than pω and upon observing m1 must be strictly greater than pω ,32
so a∗ (m0 ) < a∗ (m1 ).

Both the s1 and s0 types must be indifferent between both messages, so:

                      πθ (m0 , 0) + γv(a∗ (m0 ), 0) = πθ (m1 , 0) + γv(a∗ (m1 ), 0)
                      πθ (m0 , 1) + γv(a∗ (m0 ), 1) = πθ (m1 , 1) + γv(a∗ (m1 ), 1)

Since v(a∗ (m0 ), 0) > v(a∗ (m1 ), 0), for the s0 to be indifferent it must be the case that
πθ (m0 , 0) < πθ (m1 , 0). Writing out this posterior belief:

                                         (1 − pω )(pθ (pδ σ0 (m) + (1 − pδ )σ∅ (m))
                     P(θ = g|m, 0) =                                                 .
                                         (1 − pω )(pθ pδ σ0 (m) + (1 − pθ pδ )σ∅ (m)


                                                             σ0 (m0 )       σ∅ (m0 )
Rearranging, πθ (m0 , 0) < πθ (m1 , 0) if and only if        σ0 (m1 )
                                                                        <   σ∅ (m1 )
                                                                                      . Similarly, it must      be
                                                             σ1 (m0 )       σ∅ (m0 )                 σ0 (m0 )
the case that πθ (m , 1) < πθ (m , 1), which implies
                        1            0
                                                             σ1 (m1 )
                                                                        >        1
                                                                            σ∅ (m )
                                                                                     . Combining,    σ0 (m1 )
                                                                                                                <
σ1 (m0 )
σ1 (m1 )
         ,   which contradicts the definition of these messages. So, σ0 (m) = σ1 (m) for all m.

Step 3: If M0 = M1 and σ0 (m) = σ1 (m), then M∅ = M0 = M1 and σ∅ (m) = σ0 (m) =
σ1 (m). By step 2, it must be the case that a∗ (m) = pω for all messages sent by the
informed types. So, the uninformed types can’t send a message not sent by the informed
types: if so, the payoff would be at most π∅ + γv(pω , pω ), which is strictly less than the
  32
    The action taken upon observing m can be written P(s1 |m) + pω P(s∅ |m). Rearranging, this is greater
                             P(s1 ,m)
than pω if and only if P(s1 ,m)+P(s   0 ,m)
                                            > pω which holds if and only if σ1 (m) > σ0 (m).


                                                    54
payoff for sending a message sent by the informed types. If there is only one message
in M then the proof is done. If there are multiple types, all must be indifferent between
each message, and by step 2 they lead to the same policy choice. So, they must also lead
to the same competence assessment for each revelation of ω, which is true if and only if
σ∅ (m) = σ0 (m) = σ1 (m).

Next, consider the no validation case. For step 1, define m0 and m1 analogously. The
uninformed types must send m0 by the same logic, and these types at least weakly prefer
sending this to mc (while the s0 types are indifferent) requires:

                    πθ (m0 ) + γv(a∗ (m0 ), 1) ≥ πθ (mc ) + γv(a∗ (mc ), 1).

This can hold only weakly to prevent the s1 types from sending m0 (as required by the
definition). Combined with the s0 indifference condition:

 πθ (m0 ) − πθ (mc ) = γv(a∗ (mc ), 1) − γv(a∗ (m0 ), 1) = γv(a∗ (mc ), 0) − γv(a∗ (m0 ), 0),

which requires a∗ (m0 ) = a∗ (mc ). Since the s1 types send mc but not m0 this requires
σ∅ (m0 )
σ0 (m0 )
         > σσ∅0 (m
                (mc )
                   c)
                      , which implies πθ (m0 ) < πθ (mc ), contradicting the s0 types being indif-
ferent between both messages.

Steps 2 and 3 follow the same logic.




Finally, we prove that any MSE where the messages sent by the s0 and s1 types do not
overlap is equivalent to an MSE where there is only one message sent by each of these
types and only one “other” message. This provides a formal statement of our claims about
equilibria which are “equivalent subject to relabeling”:

Proposition 13. Let MU = M∅ \ (M0 ∪ M1 ) (i.e., the messages only sent by the uninformed
types). With no validation or state validation:
i. In any MSE where M0 ∩ M1 = ∅, for j ∈ {0, 1, U }, and any m0 , m00 ∈ Mj , a∗ (m0 ) =
a∗ (m00 ) and πθ (m0 , IDM 2 ) = πθ (m00 , IDM 2 )
ii. Take an MSE where |Mj | > 1 for any j ∈ {0, 1, U }, and the equilibrium actions and pos-
terior competence assessments for the messages in this set are a∗ (mi ) and πθ (mi , IDM 2 )
(which by part i are the same for all mi ∈ Mj ). Then there is another MSE where Mj =

                                               55
{m}, and equilibrium strategy and beliefs a∗new and πθ,new such that a∗ (mi ) = a∗new (m), and
πθ (mi , IDM 2 ) = πθ,new (m, IDM 2 )


Proof. For part i, first consider the message in MU . By construction the action taken upon
observing any message in this set is pω . And since the good and bad uninformed types are
payoff equivalent and use the same strategy, the competence assessment upon observing
any message in this set must be π∅ .

For M0 , first note that for any m0 , m00 ∈ M0 , it can’t be the case that the uninformed types
only send one message but not the other with positive probability; if so, the message not
sent by the uniformed types would give a strictly higher payoff for the s0 types, and hence
they can’t send both message. So, either the uninformed types send neither m0 nor m00 , in
which case the result is immediate, or they send both, in which case they must be indifferent
between both. As shown in the proof of proposition 12, this requires that the action and
competence assessment are the same for both m0 and m00 . An identical argument holds for
M1 , completing part i.

For part ii and M∅ , the result immediately follows from the same logic as part i.

For M0 , if the uninformed types do not send any messages in M0 , then the on-path re-
sponse to any mj0 ∈ M0 are a∗ (mj0 ) = 0 and πθ (mj0 , 0) = 1. Keeping the rest of the
equilibrium fixed, the responses in a proposed MSE where the s0 types always send m0 are
also a∗new (m0 ) = 0 and πθ,new (mj0 , 0) = 1. So there is an MSE where the s0 types all send
m0 which is equivalent to the MSE where the s0 types send multiple messages.

If the uninformed types do send the messages in M0 , then part i implies all messages must
                                                                                  σ (m0 )
lead to the same competence evaluation, which implies for any m00 , m000 ∈ M0 , σ∅0 (m00 ) =
                                                                                                     0
σ∅ (m00
     0)
σ0 (m00
          ≡ r0 . In the new proposed equilibriuum where M0 = {m0 }, set σ0,new (m0 ) = 1 and
     0)
                            σ    (m )     σ (m0 )
σ∅,new (m0 ) = r0 . Since σ∅,new   0
                           0,new (m0 )
                                       = σ∅0 (m00 ) , a∗new (m0 ) = a∗ (m00 ) and πθ,new (m00 , 0) = 1, and
                                               0
all other aspects of the MSE are unchanged.




                                                    56
D     Large Policy Concerns

In the main text, we demonstrate how adding small policy concerns affects the MSE of the
model under each validation regime. Not surprisingly, when policy concerns are “large”,
there is always an honest MSE since the expert primarily wants the DM to take the best
possible action. Here we analyze how high policy concerns have to be in order to attain this
honest equilibrium, and provide some results about what happens when policy concerns are
not small but not large enough to induce honesty.

Since they have policy concerns, in any MSE which is not babbling, the types observing
s0 and s1 can not send any common messages, i.e., they fully separate. Combined with
a relabeling argument, for all of the analysis with policy concerns we can again restrict
attention to MSE where the informed types always send m0 and m1 , respectively, and
uninformed types send at most one other message m∅ . This is shown formally in Appendix
C.



No Validation Informed types never face an incentive to deviate from the honest equilib-
rium: upon observing sx for x ∈ {0, 1}, the DM chooses policy a∗ (sx ) = x, and knows the
expert is competent, giving the highest possible expert payoff.

Uninformed types, however, may wish to deviate. Upon observing m∅ , the DM takes action
a = πω = pω , which gives expected policy value 1 − pω (1 − pω ), and the belief about the
competence is π∅ . So, for the uninformed experts of either competence type, the payoff for
reporting honestly and sending signal m∅ is:

                                 π∅ + γ(1 − pω (1 − pω )).                             (15)

If the expert deviates to m ∈ {m0 , m1 }, his payoff changes in two ways: he looks com-
petent with probability 1 (as only competent analysts send these messages in an honest
equilibrium), and the policy payoff gets worse on average. So, the payoff to choosing m1
is:
                                         1 + γpω .                                   (16)


As above, the payoff to deviating to m0 is lowest, and so m1 is the binding deviation to


                                            57
check. Preventing the uninformed type from guessing m1 requires

                            π∅ + γ(1 − pω (1 − pω )) ≥ 1 + γpω .

Rearranging, define the threshold degree of policy concerns γN
                                                             H
                                                               V required to sustain hon-
esty by

                                       1 − π∅
                                  γ≥
                                      (1 − pω )2
                                            (1 − pθ )
                                    =
                                      (1 − pθ pδ )(1 − pω )2
                                       H
                                    ≡ γN V.                                                 (17)



If γ < γN H
           V , the uninformed types strictly prefer sending m1 to m∅ if the DM expects
honesty. Given our concern with admission of uncertainty, it is possible that there is a
mixed strategy equilibrium where the uninformed types sometimes send m∅ and sometimes
send m0 or m1 . However, as the following result shows, when policy concerns are too
small to induce full honesty, the payoff for sending m1 is always higher than the payoff
for admitting uncertainty. Moreover, since γN H
                                               V is strictly greater than zero, when policy
concerns are sufficiently small some form of validation is required to elicit any admission
of uncertainty.

Proposition 14. When γ > 0 and no validation:
            H
i. If γ ≥ γN V , then there is an honest MSE,
                H                                                            ∗
ii. If γ ∈ (0, γN V ), then all non-babbling MSE are always guessing (i.e., σ∅ (m∅ ) = 0)




Proof. Part i is shown above

For part ii, it is sufficient to show that if γ < γN
                                                   H
                                                     V , then in any proposed equilibrium where
σ∅ (m∅ ) > 0, the payoff for an expert to send m1 is always strictly higher than the payoff to
sending m∅ . We have already shown that for this range of γ there is no honest equilibrium,
i.e., if all uninformed types send m∅ , the payoff to sending m1 is higher than the payoff to
m∅ .

The competence evaluation upon observing m1 as a function of the uninformed expert




                                              58
mixed strategy is:

                                P(θ = g, m1 )           pθ pω pδ + pθ (1 − pδ )σ∅ (m1 )
         πθ (m1 ; σ∅ (m1 )) =                 =
                                   P(m1 )       pθ pω pδ + (pθ (1 − pδ ) + (1 − pθ ))σ∅ (m1 )

– and the belief about the state is:

                                P(ω = 1, m1 )          pω (pθ pδ + (1 − pθ pδ )σ∅ (m1 ))
     πω (m1 ; σ∅ (m1 )) =                     =                                               .
                                   P(m1 )       pω pθ pδ + (pθ (1 − pδ ) + (1 − pθ ))σ∅ (m1 )


When observing m∅ , the DM knows with certainty that the expert is uninformed, so πθ (m∅ ) =
π∅ and πω (m∅ ) = pω .

Combining, the expected payoff for an uninformed type to send each message is:

EU(m1 ; s∅ , σ∅ (m1 )) = πθ (m1 ; σ∅ (m1 ))
                           + γ(1 − [pω (1 − πω (m1 ; σ∅ (m1 )))2 + (1 − pω )πω (m1 ; σ∅ (m1 ))2 ])

– and,

                                 EU(m∅ ) = π∅ + γ(1 − pω (1 − pω )).



Conveniently, EU(m∅ ) is not a function of the mixed strategy.

If γ = 0, then EU(mi ; σi ) > EU(m∅ ) for both i ∈ {0, 1}, because πθ (mi ; σi ) > π∅ .
Further, by the continuity of the utility functions in γ and σ∅ (m1 ), there exists a γ ∗ > 0
such that message m1 will give a strictly higher payoff than m∅ for an open interval (0, γ ∗ ).
The final step of the proof is to show that this γ ∗ is exactly γN
                                                                 H
                                                                   V.



To show this, let σ cand (γ) be the candidate value of σ∅ (m1 ) that solves EU(m1 ; s∅ , σ∅ (m1 )) =
EU(m∅ ). Rearranging, and simplifying this equality gives:

                                             pω pθ pδ    pω pθ pδ (1 − pω )2
                           σ cand (γ) = −             +γ
                                            1 − pθ pδ          1 − pθ

which is linear in γ. When γ = 0, σ cand (γ) is negative, which re-demonstrates that with
no policy concerns the payoff to sending m1 is always higher than m∅ . More generally,
whenever σ cand (γ) < 0, the payoff to sending m1 is always higher than m∅ so there can be


                                                   59
no admission of uncertainty. Rearranging this inequality gives:

                              pω p θ p δ     pω pθ pδ (1 − pω )2
                            −            +γ                      <0
                             1 − pθ pδ             1 − pθ
                                            1 − pθ               H
                            ⇔γ<                             = γN  V,
                                     (1 − pθ pδ )(1 − pω )2

completing part ii.

Now that we have demonstrated any equilibrium is always guessing, we can prove propo-
sition 6. As γ → 0, the condition for an equilibrium where the uninformed types send both
m0 and m1 is that the competence assessments are the same. Writing these out gives:

                                       πθ (m0 ; σ∅ ) = πθ (m1 ; σ∅ )
       pθ (1 − pω )pδ + pθ (1 − pδ )σ∅ (m0 )                  pθ pω pδ + pθ (1 − pδ )σ∅ (m1 )
                                                    =
pθ (1 − pω )pδ + (pθ (1 − pδ ) + (1 − pθ ))σ∅ (m0 )   pθ pω pδ + (pθ (1 − pδ ) + (1 − pθ ))σ∅ (m1 )


which, combined with the fact that σ∅ (m1 ) = 1 − σ∅ (m0 ) (by part ii) is true if and only
if σ∅ (m0 ) = 1 − pω and σ∅ (m1 ) = pω . There is no equilibrium where σ∅ (m0 ) = 0; if
so, πθ (m0 ; σ∅ ) = 1 > πθ (m1 ; σ∅ ). Similarly, there is no equilibrium where σ∅ (m1 ) = 0.




With no validation, admission of uncertainty is now possible, though through a mechanical
channel. In the extreme, when γ → ∞, the incentives of the expert and decision-maker are
fully aligned, and there is no downside to admitting uncertainty.



State Validation.     Suppose there is an honest equilibrium with state validation.

As in the case with no policy concerns, upon observing message (m0 , 0) or (m1 , 1) the
DM knows the expert is competent and takes an action equal to the message, and upon
(m∅ , 0) or (m∅ , 1) takes action pω and knows the expert is uninformed, giving competence
evaluation π∅ . So, the payoff for an uninformed type to send the equilibrium message is:

                                  pi∅ + γ(1 − pω (1 − pω )).                                (18)




                                               60
By an identical argument to that made with no policy concerns, upon observing an off-
path message, the payoff equivalence of the good and bad uninformed types implies the
belief about competence in an MSE must be greater than or equal to π∅ . So, the payoff to
deviating to m1 must be at least

                                   pω + (1 − pω )π∅ + γpω

–and the corresponding policy concerns threshold to prevent this deviation is:

                     π∅ + γ(1 − pω (1 − pω )) ≥ pω + (1 − pω )π∅ + γpω

– which reduces to

                                                   H
                                          γ ≥ p ω γN V
                                               H
                                            ≡ γSV                                            (19)



Adding state validation weakens the condition required for an honest equilibrium, partic-
ularly when pω is close to 1/2. However, this threshold is always strictly positive, so for
small policy concerns there can be no honesty even with state validation.

As shown in the proof of the following, if this condition is not met, then as with the no
validation case there can be no admission of uncertainty. Further, since adding policy
concerns does not change the classes of payoff equivalence, the case as γ → 0 is the same
as γ = 0.
Proposition 15. With policy concerns and state validation:
            H          H
i. If γ ≥ γSV   = p ω γN V , then there is an honest MSE,
                H
ii. If γ ∈ (0, γSV ), then all non-babbling MSE are always guessing (i.e., σ∅∗ (m∅ ) = 0).


Proof. Part i is demonstrated above

For part ii, our strategy mirrors the proof with no validation – that is, by way of contradic-
tion, if the constraint for honesty is not met, then the payoff to sending m1 is always strictly
higher than m∅ . As above, in any MSE where σ∅ (m1 ) > 0, the payoff for sending m∅ is
π∅ + γ(1 − pω (1 − pω )). The payoff to sending m1 is:

pω πθ (m1 , 1) + (1 − pω )π∅ + γ(1 − pω (1 − πω (m1 , σ∅ (m1 )))2 + (1 − pω )πω (m1 , σ∅ (m1 ))2 ).

                                               61
Next, the posterior beliefs of the decision-maker are the same as in the no validation case
except:



                P(θ = g, m1 , ω = 1)   pω pθ pδ + pω pθ (1 − pδ )σ∅ (m1 )   pθ pδ + pθ (1 − pδ )σ∅ (m1 )
πθ (m1 , 1) =                        =                                    =                              .
                   P(m1 , ω = 1)       pω pθ pδ + pω (1 − pθ pδ )σ∅ (m1 )   pθ pδ + (1 − pθ pδ )σ∅ (m1 )

The difference between the payoffs for sending m1 and m∅ can be written:

                                                 z(σ∅ (m1 ); γ)
    pδ pθ pω
               (1 − pδ pθ )(pδ pθ (1 − σ∅ (m1 )) − σ∅ (m1 ))(pδ pθ (pω − σ∅ (m1 )) + σ∅ (m1 ))2

– where

    z(σ∅ (m1 ); γ) = γpδ pθ (−1 + pδ pθ )(−1 + pω )2 pω (pδ pθ (−1 + σ∅ (m1 )) − σ∅ (m1 ))
                         + (−1 + pθ )(pδ pθ (pω − σ∅ (m1 )) + σ∅ (m1 ))2 ).

So any equilibrium where both m1 and m∅ are sent is characterized by z(σ∅ (m1 ); γ) =
0. It is then sufficient to show that for γ < γSV
                                                H
                                                   , there is no σ∅ (m1 ) ∈ [0, 1] such that
z(σ∅ (m1 ); γ) = 0. The intuition is the same as for part ii proposition 6: the substitution
effect that makes sending m1 less appealing when other uninformed types do so is only
strong when policy concerns are weak, which is precisely when sending m1 is generally
preferable to m∅ regardless of the uninformed type strategy.

Formally, it is easy to check that z is strictly decreasing in γ and that z(0, γSV H
                                                                                     ) = 0. So,
z(0, γ) > 0 for γ < γSVH
                         . To show z is strictly positive for σ∅ (m1 ) > 0, first observe that:

      ∂z
                         = (1 − pθ )(1 − pδ pθ )(pδ pθ (2 − pω )pω + (2 − 2pδ pθ )σ∅ (m1 )) > 0
    ∂σ∅ (m1 )       H
                 γ=γSV


– and

                             ∂ 2z
                                     = −pδ pθ (1 − pδ pθ )2 (1 − pω )2 pω < 0.
                         ∂σ∅ (m1 )∂γ

Combined, these inequalities imply ∂σ∅∂z
                                       (m1 )
                                             > 0 when γ < γSV
                                                           H
                                                              . So, z(σ∅ (m1 ), γ) > 0 for
any σ∅ (m1 ) when γ < γSV , completing part ii.
                       H



Now that we have proved that any equilibrium is always guessing, proposition 7 is imme-
diate: the utilities for sending each message approach the no policy concern case as γ → 0.

                                                  62
Difficulty Validation. As shown in the main text, the condition for an honest equilibrium
with difficulty validation and policy concerns is.

                            (1 − pδ )pθ + γ(1 − pω (1 − pω )) ≥ pδ + γpω
                                                     pδ (1 + pθ ) − pθ    H
                                               γ≥                      ≡ γDV .
                                                         (1 − pω )2


As discussed in the main text, γDV  H
                                        can be negative, meaning that there is an honest equi-
librium even with no policy concerns. Even if not but difficulty validation also maintains a
secondary advantage over state validation. Like with full validation and no policy concerns,
even if it is impossible to get the bad uninformed types to report honestly, there is always
an equilibrium where good uninformed types admit uncertainty. However, unlike any other
case considered thus far, it is not guaranteed that the informed types report their honest
message with positive but not too large policy concerns. See the proof in Appendix B for
details; in short, if the probability of a solvable problem is not too low or the probability of
the state being one is not too high, then there is an MSE where all of the good types send
their honest message.33

Proposition 16. With policy concerns and difficulty validation:
           H
i. If γ ≥ γDV , then there is an honest MSE.
            H
ii. If γ ≤ γDV  , then there is an MSE where the uninformed good types admit uncertainty,
              pω
and if pδ ≥ 2−pω there is an MSE where all of the good types send their honest message.


Proof. Part i is shown above. For part ii, first note the equilibrium constructed in proposi-
tion 4 also holds with policy concerns: the policy choice upon observing both equilibrium
messages is pω , so each type’s relative payoff in this equilibrium is unaffected by the value
  33
     Here is an example where this constraint is violated. Suppose pω is close to 1, and the bad types usually
send m1 , and rarely m0 . Then the tradeoff they face is that sending m1 leads to a better policy, but a lower
competence payoff when the problem is easy (when the problem is hard, the competence payoff for either
guess is zero). Now consider the good expert who observes signal s1 . Compared to the bad expert, this type
has a marginally stronger incentive to send m1 (since pω is close to 1). However, this type knows that he
will face a reputational loss for sending m1 rather than m0 , while the bad type only experiences this loss
with probability pδ . So, the bad type being indifferent means the type who knows the state is 1 has a strict
incentive to deviate to m0 . In general, this deviation is tough to prevent when pδ is low and pω is close to 1,
hence the condition in the proposition.



                                                      63
of γ. Since the good uninformed types always admit uncertainty in this equilibrium, this
demonstrates the first claim.

Now suppose the good types all send their honest message. By the same fixed point argu-
ment as proposition 3, the bad types must have at least one mixed strategy (σb (m0 ), σb (m1 ), σb (m∅ ))
which is a best response given the good types strategy and DM strategy. What remains is
to show the good types have no incentive to deviate from the honest message.

The message/validation combinations (m0 , e), (m1 , e), and (m∅ , h) are on-path and yield
competence evaluations which are all strictly greater than zero.

Message/validation combinations (m0 , h), (m1 , h), and (m∅ , e) are never reached with a
good type. So, if the bad types send those respective messages, they are on-path and the
competence assessment must be zero. If these information sets are off-path the competence
assessment can be set to zero.

Since only uninformed types send m∅ , the policy choice upon observing m∅ must be
a∗ (m∅ ) = pω . The m0 message is sent by the informed type who knows ω = 0, and poten-
tially also by uninformed bad types, so a∗ (m0 ) ∈ [0, pω ). Similarly, a∗ (m1 ) ∈ (pω , 1]. So
a∗ (m0 ) < a∗ (m∅ ) < a∗ (m1 ).

The good and uninformed type has no incentive to deviate from sending message m∅ be-
cause for m ∈ {m0 , m1 }, πθ (m∅ , h) > πθ (m, h) and v(a∗ (m∅ ), pω ) > v(a∗ (m), pω ).

The s0 type has no incentive to deviate to m∅ since πθ (m0 , e) > πθ (m∅ , e) = 0 and
v(a∗ (m0 ), 0) > v(a∗ (m∅ , 0)). Similarly, the s1 type has no incentive to deviate to m∅ .

So, the final deviations to check are for the informed types switching to the message as-
sociated with the other state; i.e., the s0 types sending m1 and the s1 types sending m0 .
Preventing a deviation to m1 requires:

                 πθ (m0 , e) + γv(a∗ (m0 ), 0) ≥ πθ (m1 , e) + γv(a∗ (m1 ), 0)
                          ∆π + γ∆v (0) ≤ 0,                                                (20)

where ∆π ≡ πθ (m1 , e) − πθ (m0 , e) is the difference in competence assessments from send-
ing m1 versus m0 (when the problem is easy), and ∆v (p) ≡ v(a∗ (m1 ), p) − v(a∗ (m0 ), p)
is the difference in the expected quality of the policy when sending m1 vs m0 for an expert

                                              64
who believes ω = 1 with probability p. This simplifies to:

                  ∆v (p) = (a∗ (m1 ) − a∗ (m0 ))(2p − a∗ (m1 ) − a∗ (m0 )).

Since a∗ (m1 ) > a∗ (m0 ), ∆v (p) is strictly increasing in p, and ∆v (0) < 0 < ∆v (1).

The analogous incentive compatibility constraint for the s1 types is:

                                      ∆π + γ∆v (1) ≥ 0                                    (21)

If the bad types never send m0 or m1 , then ∆π = 0, and (20)-(21) both hold. So, while
not explicitly shown in the main text, in the honest equilibrium such a deviation is never
profitable.

Now consider an equilibrium where the bad types send both m0 and m1 , in which case they
must be indifferent between both messages:

             pδ πθ (m0 , e) + γv(a∗ (m0 ), pω ) = pδ πω (m1 , e) + γv(a∗ (m1 ), pω )
                             pδ ∆π + γ∆v (p) = 0                                          (22)

Substituting this constraint into (20) and (21) and simplifying gives:

                                   pδ ∆v (0) − ∆v (pω ) ≤ 0                               (23)
                                   pδ ∆v (1) − ∆v (pω ) ≥ 0.                              (24)

If ∆v (pω ) = 0 the constraints are both met. If ∆v (pω ) < 0 then the second constraint is
always met, and the first constraint can be written:

                             ∆v (pω )   a∗ (m0 ) + a∗ (m1 ) − 2pω
                      pδ ≥            =                           ≡ p̌δ                   (25)
                             ∆v (0)         a∗ (m0 ) + a∗ (m1 )

This constraint is hardest to meet when p̌δ is large, which is true when a∗ (m0 ) + a∗ (m1 ) is
high. The highest value this sum can take on is pω + 1, so p̌δ ≤ 1+p
                                                                   1−pω
                                                                      ω
                                                                        .

If ∆v (pω ) > 0, then the first constraint is always met, and the second constraint becomes:

                            ∆v (pω )   2pω − (a∗ (m0 ) + a∗ (m1 ))
                     pδ ≥            =                             ≡ p̂δ                  (26)
                            ∆v (1)      2 − (a∗ (m0 ) + a∗ (m1 ))



                                               65
This is hardest to meet when a∗ (m0 ) + a∗ (m1 ) is small, and the smallest value it can take
on is pω . Plugging this in, p̂δ ≥ 2−p
                                    pω
                                       ω
                                         ≥ p̌δ .

For pω ≥ 1/2, p̂δ ≥ p̌δ . Without placing any further restrictions on the value of a∗ (m0 ) +
a∗ (m1 ) – which will be straightforward on the value, this constraint ranges from p̂δ ∈
(1/3, 1). Still, if pδ is sufficiently high, the informed types never have an incentive to
deviate when the bad types send both m0 and m1 .

If the bad types only send m1 but not m0 , then the s0 types get the highest possible payoff,
so the relevant deviation to check is the s1 types switching to m0 . The bad types sending
weakly preferring m1 implies pδ ∆π + γ∆v (p) ≥ 0, and substituting into equation 24 gives
the same pδ ≥ p̂δ . Similarly, if the bad types only send m0 but not m1 , then the relevant
constraint is the s0 types sending m1 , for which pδ ≥ p̌δ is sufficient.

Summarizing, a sufficient condition for the existence of a MSE where the good types report
honestly (for any value of γ) is pδ ≤ pθ /(1 + pθ ) (in which case γ ≤ γDV
                                                                        H
                                                                           ), or pδ ≥ 2−p
                                                                                       pω
                                                                                          ω
                                                                                            .
This completes part ii.

Now to prove proposition 8 we first characterize the optimal strategy for the bad types as
γ → 0, assuming the good types send their honest message. If sending m∅ , the expert will
reveal his type if δ = e, but appear partially competent if δ = h, giving expected payoff

                                                    pθ
                              (1 − pδ )                           .
                                           pθ + (1 − pθ )σb (m∅ )

When sending m0 , the expert will reveal his type if δ = h (as only bad types guess when
the problem is hard), but look partially competent if δ = e:

                                            pθ (1 − pω )
                             pδ                                    .
                                  pθ (1 − pω ) + (1 − pθ )σb (m0 )

and when sending m1 the expect payoff is:

                                                 pθ pω
                                  pδ                             .
                                       pθ pω + (1 − pθ )σb (m1 )




                                                 66
setting these three equal subject to σb (m0 ) + σb (m1 ) + σb (m∅ ) = 1 gives:

                                                  1 − pδ (1 + pθ )
                                         σb (m∅ ) =                ;
                                                      1 − pθ
                                       (1 − pω )(pδ − pθ (1 − pδ ))
                            σb (m0 ) =
                                                 1 − pθ
                                            pω (pδ − pθ (1 − pδ ))
                                 σb (m1 ) =                        .
                                                    1 − pθ

These are all interior if and only if:

                         1 − pδ (1 + pθ )          pθ            1
                    0<                    < 1 =⇒        < pδ <        .
                             1 − pθ              1 + pθ        1 + pθ

If pδ ≤ 1+p
          pθ
             θ
               , then there is no fully mixed strategy for the bad expert in equilibrium because
they would always prefer to send m∅ ; and recall this is exactly the condition for an honest
equilibrium with no validation. If pδ ≥ 1+p   1
                                                θ
                                                  , then the bad type always guesses. Setting the
payoff for a bad type sending m0 and m1 equal along with σb (m0 ) + σb (m1 ) = 1 gives the
strategies in the statement of the proposition.

The final step is to ensure the informed types do not send the message associated with
the other state. Recall the IC constraints depend on a∗ (m0 ) + a∗ (m1 ), which we can now
restrict to a narrower range given the bad type strategy:

                                          (1 − pθ )pω (1 − pω )(1 − σb (m∅ ))
            a∗ (m0 ) + a∗ (m1 ) =
                                 pδ pθ (1 − pω + (1 − pθ )(1 − pω )(1 − σb (m∅ )))
                                    pδ pθ pω + (1 − pθ )pω pω (1 − σb )(m∅ )
                                 +
                                     pδ pθ pω + (1 − pθ )pω (1 − σb )(m∅ )
                                 pδ pθ + (1 − σb (m∅ ))(1 − pθ )2pω
                               =                                      .
                                   pδ pθ + (1 − σb (m∅ ))(1 − pθ )

This can be interpreted as weighted average of 1 (with weight pδ pθ ) and 2pω > 1 (with
weight (1 − σb (m∅ )(1 − pθ )), and so must lie on [1, 2pω ]. So, (26) is always the binding
constraint, and is hardest to satisfy when a∗ (m0 )+a∗ (m1 ) → 1, in which case the constraint
becomes p̂δ = 2pω − 1. So, pδ ≥ 2pω − 1 is a sufficient condition for the informed types to
never deviate. For any pδ > 0, this holds for pω sufficiently close to 1/2, completing part
proposition 8.




                                               67
                                                Figure D.2: Comparative Statics of Honesty Threshold




                                                                                                                                                                  12
                                                                                                                                                                               DV




                                                                                               8
                                                NV




                                                                                                                                                                  10
                                                                                                                    NV
Policy Concern Threshold




                                                                    Policy Concern Threshold




                                                                                                                                       Policy Concern Threshold
                            5




                                                                                               6




                                                                                                                                                                  8
                                                                                                                                                                               NV
                                                SV




                                                                                               4




                                                                                                                                                                  6
                                                                                                                     SV
                                                                                                                                                                               SV
                            0




                                                                                               2




                                                                                                                                                                  4
                                                                                                                    DV
                                                DV
                                                                                               0




                                                                                                                                                                  2
                            −5




                                                                                               −2




                                                                                                                                                                  0
                                 0.0     0.2 0.4 0.6 0.8 1.0                                        0.0     0.2 0.4 0.6 0.8 1.0                                        0.0     0.2 0.4 0.6 0.8 1.0
                                       Probability Competent (pθ)                                         Probability Competent (pθ)                                         Probability Competent (pθ)




Notes: Comparison of threshold in policy concerns for full honesty under different validation regimes as a
function of pθ . The panels vary in the likelihood the problem is solvable, which is 0.25 in the left panel, 0.5
in the middle panel, and 0.75 in the right panel.


Comparative Statics: Difficulty Validation Can be the Wrong Kind of Transparency.
As long as policy concerns are strictly positive but small, difficulty validation is more
effective at eliciting honesty than state validation.

For larger policy concerns the comparison becomes less straightforward. Figure D.2 shows
the policy concern threshold for honesty under no validation (solid line), state validation
(dashed line), and difficulty validation (dotted line) as a function of the prior on the expert
competence, when the problem is usually hard (pδ = 0.25, left panel), equally likely to
be easy or hard (pδ = 0.5, middle panel) and usually easy (pδ = 0.75, right panel). In all
panels pω = 0.67; changing this parameter does not affect the conclusions that follow.34 For
intuition, difficulty validation makes it hard to compensate bad experts for saying “I don’t
know," as there are fewer good experts who don’t know. For very easy problems difficulty
validation can be worse than no validation. This mirrors the result in Prat (2005), where
transparency can eliminate incentives for bad types to pool with good types by exerting
more effort.
                           34
    In general, honesty is easier to sustain under all validation regimes when pω is lower, with state validation
being particularly sensitive to this change.


                                                                                                            68
This figure illustrates several key conclusions from the model. First, in all cases, the policy
concern threshold required is decreasing in pθ , which means it is easier to sustain honesty
when the prior is that the expert is competent. This is because when most experts are
competent in general, most uninformed experts are competent as well, and so there is less
of a penalty for admitting uncertainty. Second, the threshold with state validation is always
lower than the threshold with no validation, though these are always strictly positive as
long as pθ < 1. Further, for most of the parameter space these thresholds are above two,
indicating the expert must care twice as much about policy than about perceptions of his
competence to elicit honesty. On the other hand, in the right and middle panels there are
regions where the threshold with difficulty validation is below zero, indicating no policy
concerns are necessary to induce admission of uncertainty (in fact, the expert could want
the decision-maker to make a bad decision and still admit uncertainty).

Finally, consider how the relationship between the thresholds changes as the problem be-
comes easier. When problems are likely to be hard (left panel), difficulty validation is the
best for eliciting honesty at all values of pθ . In the middle panel, difficulty validation is
always better than no validation, but state validation is best for low values of pθ . When the
problem is very likely to be easy, difficulty validation is always worse than state validation
and is even worse than even no validation other than for a narrow range of pθ .

However, even in this case difficulty validation still can elicit honesty from good but unin-
formed experts when policy concerns are not high enough, while there is no admission of
uncertainty at all when policy concerns are not high enough with no validation and state
validation.




E     Continuous Type Space

Let the competence of the expert θ and the difficulty of the problem δ both be uniform on
[0, 1]. Let the private signal to the expert be:
                                    
                                     s       ω = 0, θ > βδ
                                     0
                                    
                                    
                                 s = s1       ω = 1, θ > βδ
                                    
                                    
                                    s        o/w
                                    
                                          ∅




                                              69
where β > 0.

If β < 1, then even the hardest problems (δ = 1) are solvable by a strictly positive pro-
portion of experts. If β > 1, then there are some problems which are so difficult that no
expert can solve them. For reasons which will become apparent, we focus on the case
where β > 1, and so δ = 1/β < 1 is the “least difficult unsolvable problem”.

The expert learns s and θ, which is always partially informative about δ. In particular, an
expert who gets an informative signal knows that δ ∈ [0, θ/β], and an expert who does not
get an informative signal knows that δ ∈ [θ/β, 1]. An interesting contrast with the binary
model is that better experts don’t always know more about the problem difficulty: when
they learn the state, the range of possible values of δ is increasing in θ. However, when
the expert learns the state (particularly with state validation) knowing the difficulty is not
particularly relevant. On the other hand, when the expert is uninformed those who are more
competent can restrict the difficulty of the problem to a smaller interval.

As with the binary model, we search for honest equilibria in the sense that the expert fully
separates with respect to their signal (if not with respect to their competence). Here we
only consider the case with no policy concerns.



No validation, State validation, Difficulty validation Even though the state space is
much larger in this version of the model, with no validation (and no policy concerns) all
types are still payoff equivalent. So any MSE must be babbling.

Similarly, with state validation there are now multiple types (differentiated by θ) who learn
the state and multiple types who do not learn the state. However, since the knowledge of
the state is the only payoff-relevant component of the type space, all types observing a
particular s must play the same strategy in an MSE. So, by the same logic as the binary
model, any MSE must be always guessing.

Difficulty validation alone (and again with no policy concerns; with small policy concerns
honesty might possible for some parameters) also hits the same problem as in the binary
model. Among the informed types with competence θ, those observing s0 and those ob-
serving s1 are payoff equivalent, and so no information can be communicated about the
state. It is possible that information about the difficulty of the problem can be conveyed.


                                             70
Full Validation Now consider the full validation case. No pairs of types are payoff equiv-
alent, since even those observing the same signal have different beliefs about what the
difficulty validation will reveal for each value of θ. So, it is possible to use punitive off-
path beliefs where those who guess incorrectly or when no expert could solve the problem,
which is possible when β > 1.

We now show an honest equilibrium can be possible in this case. First, consider the on-path
inferences by the DM. When seeing a correct message and difficulty δ, the DM knows the
expert competence must be on [βδ, 1], and so the average competence assessment is:

                                                                  βδ + 1
                      πθ (m1 ; ω = 1, δ) = πθ (m0 , ω = 0, δ) =
                                                                    2

which is at least 1/2, and increasing in δ.

Upon observing m∅ and δ, there are two possible cases. If δ > 1/β, then no expert could
have solved the problem, and so there is no information conveyed about the expert compe-
tence. If δ < 1/β, then the DM learns that the expert competence is uniform on [0, βδ].
Combining:
                                               
                                               1/2 δ > 1/β
                              πθ (m∅ , ω, δ) =
                                                βδ δ ≤ 1/β
                                                 2




All other message/validation combinations are off-path, and can be set to zero.

Now consider the expert payoffs.

An informed expert (of any competence) gets a payoff of βδ+1  2
                                                                 > 1/2 for playing sending
the equilibrium message, 0 for sending the other informed message (i.e., m1 rather than m0
when s = s0 ), and πθ (m∅ , ω, δ) ≤ 1/2 for sending m∅ . So these types never deviate.

Uninformed experts know the difficulty – which again will be revealed to the DM – is
uniform on [θ/β, 1]. Note that for all but the (measure zero) θ = 1 types, 1/β lies on
this interval. So, all but the most competent experts don’t know for sure if the problem is
solvable by some experts, though very competent experts can become nearly certain the
problem is unsolvable.



                                              71
We can write the expected competence for admitting uncertainty to be the probability that
δ ≥ 1/β times 1/2, plus the probability that δ < 1/β times the average competence
assigned on this interval. Since the competence assessment is linear in δ on this interval,
ranging from β(θ/β)
                2
                     = 2θ to 1/2, this average is θ+1
                                                   4
                                                      . Combining, the expected competence
for sending m∅ is:

                                                    θ+1                 1
                 Eδ [πθ (m∅ , ω, δ)] = P r(δ ≤ 1/β)      + P r(δ > 1/β)
                                                      4                 2
                                        1/β − θβ θ + 1 1 − 1/β 1
                                      =                +
                                         1 − θ/β 4       1 − θ/β 2
                                        1−θ θ+1 β −11
                                      =            +
                                        β−θ 4         β−θ2

which is increasing in θ.

Next consider the payoff for sending m1 ; as before, this is the “better guess” since it is more
likely to be matched by the state validation. This will lead to a competence evaluation of
βδ+1
  2
      if ω = 1 (probability pω ) and if δ < 1/β (probability β−θ 1−θ
                                                                     ), and 0 otherwise. Since
the guessing correct payoff is linear in δ and the belief about δ conditional on a solvable
problem is uniform on [θ/β, 1/β], so the average competence assessment when getting
away with a guess is:

                                        θ+1
                                         2
                                            +1   3+θ
                                               =
                                           2      4


So the payoff to this deviation is:

                                              1−θ3+θ
                                         pω
                                              β−θ 4

which is decreasing in θ.

So, the binding constraint is that the θ = 0 prefers sending m∅ , which again reinforces the
assumption made about off-path beliefs. Honesty is possible when:

                                      11 β −11     1
                                         +     ≥ pω (3/4)
                                      β4   β 2     β
                              β ≥ (3/2)pω + 1/2

Since pω ∈ [1/2, 1], this threshold ranges from 5/4 to 2. That the threshold in β is strictly

                                               72
greater than 1 means that there must be some possibility of getting caught answering an
unanswerable question. The threshold is lower when pω is lower since this makes guessing
less attractive as one is more likely to be caught guessing wrong.




                                          73
