                                 NBER WORKING PAPER SERIES




                               THE ROLE OF INFORMATION
                                 IN MEDICAL MARKETS:
                           AN ANALYSIS OF PUBLICLY REPORTED
                             OUTCOMES IN CARDIAC SURGERY

                                           David M. Cutler
                                          Robert S. Huckman
                                          Mary Beth Landrum

                                         Working Paper 10489
                                 http://www.nber.org/papers/w10489


                       NATIONAL BUREAU OF ECONOMIC RESEARCH
                                1050 Massachusetts Avenue
                                  Cambridge, MA 02138
                                       May 2004




The views expressed herein are those of the author(s) and not necessarily those of the National Bureau of
Economic Research.

©2004 by David M. Cutler, Robert S. Huckman, and Mary Beth Landrum. All rights reserved. Short sections
of text, not to exceed two paragraphs, may be quoted without explicit permission provided that full credit,
including © notice, is given to the source.
The Role of Information in Medical Markets: An Analysis of Publicly Reported Outcomes in
Cardiac Surgery
David M. Cutler, Robert S. Huckman, and Mary Beth Landrum
NBER Working Paper No. 10489
May 2004
JEL No. I1
                                       ABSTRACT

       During the past two decades, several public and private organizations have initiated

programs to report publicly on the quality of medical care provided by specific hospitals and

physicians. These programs have sparked broad debate among economists and policy makers

concerning whether, and to what extent, they have improved or harmed medical productivity. We

take advantage of a cross-sectional time series of different hospitals to address two fundamental

questions about quality reporting. First, we examine whether report cards affect the distribution of

patients across hospitals. Second, we determine whether report cards lead to improved medical

quality among hospitals identified as particularly bad or good performers. Our data are from the

longest-standing effort to measure and report health care quality – the Cardiac Surgery Reporting

System (CSRS) in New York State. Using data for 1991 through 1999, we find that CSRS affected

both the volume of cases and future quality at hospitals identified as poor performers. Poor

performing hospitals lost relatively healthy patients to competing facilities and experienced

subsequent improvements in their performance as measured by risk-adjusted mortality.

David M. Cutler                  Robert S. Huckman                 Mary Beth Landrum
Department of Economics          T17 Morgan Hall                   Harvard University
Harvard University               Harvard Business School           landrum@hcp.med.harvard.edu
Cambridge, MA 02138              Boston, MA 02163
and NBER                         and NBER
ddcutler@fas.harvard.edu         rhuckman@hbs.edu
       During the past two decades, several public and private organizations have

initiated programs to report publicly on the quality of medical care provided by specific

hospitals and physicians. These programs have sparked broad debate among economists

and policy makers. At issue is the question of whether, and to what extent, these “report

card” programs have improved or harmed medical productivity.

       Economists typically see more information as obviously beneficial. How can

patients be harmed by knowing more about the quality of their medical care providers?

The key problem is the potential inaccuracy of quality data. Consider an attempt to rate

the quality of surgeons performing cardiac bypass surgery, which we examine in this

paper. The outcome of any bypass operation depends on physician and hospital

characteristics and on the underlying health of the patient. Patients who are sicker are

more likely to have poor outcomes than those who are healthier.

       If the quality reporting system does not take these patient differences into

account, several problems might result. First, patients and referring physicians—many of

whom have prior informal data about the quality of various providers—may make

decisions based on the new data without realizing its potential inaccuracies. Second,

even if the patients do not rely on the new data, its existence may confuse them to the

point that they ignore both sources of information. Either way, it is possible that referral

patterns might change for the worse.

       The situation could be even more problematic. If hospitals know that their

reputations may be penalized if they treat high-risk patients, they might reduce their

willingness to operate on such patients, denying them potentially needed care or forcing

them to travel to other areas. As a result, health outcomes may suffer.




                                              1
       These possibilities are not simply theoretical. David Dranove et al. (2003) argued

that quality report cards for coronary artery bypass graft (CABG) surgery in New York

State and Pennsylvania hurt quality, by limiting access to surgery for high-risk patients.

Similarly, Nowamagbe Omoigui et al. (1996) found that a larger percentage of high-risk

bypass surgery patients were transferred out of New York State following the

introduction of report cards, and Eric Schneider and Arnold Epstein (1996) found that

many cardiac surgeons reported a decreased willingness to operate on severe patients

after introduction of CABG report cards in Pennsylvania. But the evidence on this issue

is not uniform. Edward Hannan et al. (1994) and Eric Peterson et al. (1998) found that

the average in-hospital mortality rate for CABG surgery in New York declined

significantly after introduction of hospital report cards.1

       With few exceptions,2 the above studies all examine the impact of report cards by

considering trends at the state level. Identification of the impact of report cards thus

comes from average effects within states with report cards relative to average effects in

those without such programs. An alternate approach, and that which we employ, is to

examine the impact of report cards on the outcomes for and allocation of patients across

individual providers within a given state that adopted a report card system. This allows

more observations on quality as well as very complete data for risk adjustment.

       We take advantage of a cross-sectional time series of different hospitals to address

two fundamental questions about quality reporting. First, we examine whether report

cards affect the distribution of where patients go for bypass surgery. Second, we

determine whether report cards lead to improved medical quality among hospitals

identified as particularly bad or good performers. Our data are from the longest-standing




                                               2
effort to measure and report health care quality – the Cardiac Surgery Reporting System

(CSRS) in New York State.



New York’s Cardiac Surgery Reporting System

        Since the late 1980s, CSRS has collected detailed information on every patient

receiving bypass surgery at a hospital in New York State. In addition to identifying the

surgeon and hospital involved with the procedure, CSRS collects data on clinical

outcomes—specifically whether or not the patient died in the hospital following the

procedure—and incoming severity of the patient. The severity data contains information

on whether a given patient has a history of one or more of roughly 40 clinical conditions

including diabetes, kidney failure, liver failure, or prior heart attack (for a detailed

description of this clinical information, see Hannan et al., 1994). Once every 12 to 18

months, the state releases these data, along with information on which hospitals were

statistically significantly better or worse than the statewide average. These data are

reported with a lag of several years. For example, the data on hospitals for 1999 was

released in September 2002.

        To adjust for underlying health risks of the patients, New York uses a logistic

regression to predict the expected probability of in-hospital mortality. The covariates for

the regression are drawn from the clinical variables contained in CSRS. We follow this

procedure in our analysis.3

        Figure 1 illustrates the 25th percentile, median, and 75th percentile values of risk-

adjusted mortality across all hospitals performing bypass surgery in New York in a given

year. Risk-adjusted mortality fell significantly across the state during the first decade of




                                               3
the report card program. Further, the range between the hospitals at the 25th and 75th

percentiles narrows during the early years of the reporting program, suggesting improved

performance among initial poor performers. Although these results do not compare New

York to other states without report-card programs, they are consistent with prior findings

that suggest an improvement in outcomes in New York during the initial years of the

reporting.

       To examine the impact of report cards on the allocation of patients across

hospitals, we use the total number of CABG cases for each hospital-month as the

dependent variable in regressions that include hospital and year fixed effects. Our basic

equation is of the form:



(1)    Casesimt = β1(Highimt) + β2(Lowimt) + δi + λt + εit



where i denotes hospital, m denotes month of hospital admission (for years 1991 through

1996) or discharge (for years 1997 through 1999), and t denotes year of admission or

discharge. The vectors δ and λ include fixed effects for hospital and year, respectively.

The key independent variables are indicators for whether the hospital previously has

received a high-mortality flag (Highimt) or low-mortality flag (Lowimt). Given lags in the

release of the report cards, the flags assigned to hospitals in period t are typically based

on performance data for year t-2 or t-3. Each of these indicators is interacted with

indicators for the number of months since a hospital received its first flag4 (e.g., 1-12

months, 13-24 months, 25-36 months, and more than 36 months) to estimate the

persistence of any effects on volume.




                                              4
          The first column of Table 1 shows the results of this regression. Being identified

as a high-mortality hospital is associated with a decline of approximately 4.9 bypass

surgery patients per month during the twelve months following that designation. This

decline is significant at the 5 percent level. The decline is reasonably large. The average

hospital performs about 50 bypass surgery operations per month, so the change in volume

is about 10 percent. While this negative impact on volume remains in the estimates

beyond one year after the initial report, these effects are not significant at conventional

levels.

          In contrast to their impact on low-quality hospitals, the reports do not seem to

increase future volume for those facilities receiving low-mortality flags. The estimated

coefficient is positive in the first year following a report, and becomes negative in

subsequent years. It is always imprecisely estimated. With only data from New York

State, we cannot tell whether the patients moving out of low quality hospitals are not

receiving surgery at all, or are simply being spread over all non-low quality hospitals.

          We can understand more about what is happening by considering the type of

patients who are not receiving bypass surgery at high-mortality hospitals. A reduction in

relatively healthy patients is more indicative that patients (or their referring doctors) are

choosing not to go to low quality providers. A reduction in sicker patients, who often

receive more immediate surgery would suggest that low quality hospitals are choosing to

operate less frequently.

          The next two columns of Table 1 repeat our regressions separately for low- and

high-severity subsamples of the patient population.5 The decline in volume for hospitals

receiving high-mortality flags is driven by reductions in the number of low-severity




                                                5
cases. The number of such cases declines by roughly 15 percent and 10 percent relative

to the sample average in the first and second years, respectively. There is no change in

the number of high-severity patients. Thus, it appears that patients and referring

cardiologists are choosing to reduce operations at low quality institutions.

       These volume effects notwithstanding, we recall that the stated goal of cardiac

surgery reporting is to improve the quality of medical care. To examine the impact of

reporting on outcomes, we repeat these regressions using the risk-adjusted mortality rate

as the dependent variable. We form values of risk-adjusted mortality for each hospital-

month by dividing average observed mortality by average predicted mortality for a given

hospital-month and multiplying that ratio by the statewide average mortality rate for the

entire nine-year period. To account for heterogeneity based on sample size and to

mitigate the effect of small numbers of observations, we weight the observations by the

total number of bypass surgery cases for a given hospital-year.

       The last column of Table 1 shows the results for mortality. Identification as a

high-mortality hospital is associated with improved future performance. Hospitals that

are publicly identified as being low quality experience a decline of 1.2 percentage points

(significant at the 1 percent level) during the first 12 months after an initial report. This

decrease remains for the three years following the report, though the significance is only

at the 10 percent level for months 13 through 24. The average risk-adjusted mortality

rate is 2.55 percent for the entire sample. For those hospitals receiving a high-mortality

flag, the average risk-adjusted mortality rate prior to receiving that flag is 3.82 percent.

Relative to either of these baselines, the change we identify represents a significant

improvement in performance.




                                               6
        Those hospitals receiving a low-mortality flag show little evidence of mortality

changes. The coefficients are slightly positive, but only statistically significant in the

third year after the report.



Discussion

        The evidence from the first decade of New York’s bypass surgery reporting

program suggests that the public release of quality information has had an impact on both

the volume of cases and future quality at hospitals identified as poor performers. With

respect to procedural volume, low performing hospitals have lost relatively healthy

patients to competing facilities. This change may be the result of demand-side factors,

such as the possibility that healthier patients are more able to search for higher quality

providers, although several studies have suggested that similar reports have not been used

by cardiologists (Schneider and Epstein, 1996) or managed care insurers (Mukamel et al.,

2000) in affecting referral or contracting patterns.

        Another possibility is supply-side factors within the affected hospitals are

generating this result. For example, surgeons at poorly performing hospitals may simply

be choosing to do fewer procedures or may be encouraged to do fewer procedures by

hospital administrators. In the extreme, some surgeons may be exiting the market

entirely. To the extent that those surgeons leaving the market were previously doing

relatively routine cases, this reduction might be concentrated among relatively healthy

patients. Regardless of the relative importance of demand- and supply-side factors in

affecting procedural volumes, our results do suggest that the largest reductions for poor

performing hospitals are among low- rather than high-severity patients. Whatever efforts




                                              7
physicians may be making to reduce surgery to high-risk patient are not reducing their

volumes of those patients in equilibrium.

       Our findings with respect to quality improvement might also be explained by

multiple factors. First, to the extent that hospitals flagged as poor performers lose

surgical volume, they may work harder to replace that volume, particularly given the high

marginal profitability associated with cardiac procedures. Alternatively, hospitals and

surgeons may make efforts to improve their future quality for reasons that are motivated

not by financial concerns, but by concern for patient health and their reputations as

providers of high quality medical care. Stanley Dziuban et al. (1994) argue that this is

why one hospital improved its performance after quality reporting.

       While New York’s experience with bypass surgery report cards has provided

clear evidence of market changes associated with the public release of quality

information, the exact mechanisms underlying these changes represents an area for future

research. Identifying such mechanisms will provide a better understanding of the impact

of public quality reports on medical productivity.




                                              8
References

Dranove, David; Kessler, Daniel; McClellan, Mark and Satterthwaite, Mark. “Is

   More Information Better? The Effects of ‘Report Cards’ on Health Care Providers.”

   Journal of Political Economy, June 2003, 111(3), pp. 555-88.

Dziuban, Stanley W.; McIlduff, Joseph B.; Miller, Stuart J. and Dal Col, Richard H.

   “How a New York Cardiac Surgery Program Uses Outcomes Data.” Annals of

   Thoracic Surgery, December 1994, 58, pp. 1871-6.

Ghali, William A; Ash, Arlene S.; Hall, Ruth E. and Moskowitz, Mark A. 1997.

   “Statewide Quality Improvement Initiatives and Mortality After Cardiac Surgery.”

   Journal of the American Medical Association, 1997 277(5), pp. 379-82.

Hannan, Edward L.; Kilburn, Harold; Racz, Michael; Shields, Eileen and Mark R.

   Chassin. “Imporoving the Outcomes of Coronary Artery Bypass Surgery in New

   York State.” Journal of the American Medical Association, March 9, 1994, 271(10),

   pp. 761-6.

Mukamel, Dana B. and Mushlin, Alvin I. “An Analysis of Market Share and Price

   Changes After Publication of the New York State Cardiac Surgery Mortality

   Reports.” Medical Care, July 1998, 36(7), pp. 945-54.

Mukamel, Dana B.; Mushlin, Alvin I.; Weimer, David; Zwanziger, Jack; Parker,

   Todd and Indridason, Indridi. “Do Quality Report Cards Play a Role in HMOs’

   Contracting Practices? Evidence from New York State.” Health Services Research,

   April 2000, 35(1), pp. 319-32.

Omoigui, Nowamagbe A.; Miller, Dave P.; Brown, Kimberly J.; Annan, Kingsley;

   Cosgrove, Delos; Lytle Bruce; Loop, Floyd and Topol, Eric J. “Outmigration for




                                          9
   Coronary Bypass Surgery in an Era of Public Dissemination of Clinical Outcomes.”

   Circulation, January 1, 1996, 93(1), pp. 27-33.

Peterson, Eric D.; DeLong, Elizabeth R.; Jollis, James G.; Muhlbaier, Lawrence H.;

   Mark, Daniel B. “The Effects of New York’s Bypass Surgery Provider Profiling on

   Access to Care and Patient Outcomes in the Elderly.” Journal of the American

   College of Cardiology, October 1998, 32(4), pp. 993-9.

Schneider, Eric C. and Epstein, Arnold M. “Influence of Cardiac Surgery Performance

   Reports on Referral Practices and Access to Care.” New England Journal of

   Medicine, July 25, 1996, 335(4), pp. 251-6.




                                           10
            Figure 1: Distribution of Hospital-Level Risk-Adjusted Mortality Rates in New
                                              York, 1991-99

                                 6




                                 5
  Risk-Adjusted Mortality Rate




                                 4




                                 3




                                 2




                                 1




                                 0
                                 1990   1991   1992   1993         1994         1995      1996        1997   1998   1999   2000
                                                                                Year

                                                             25th Percentile     Median   75th Percentile

Note: The risk-adjusted mortality rates in this figure are based on a single logistic regression using
isolated CABG procedures in New York State for the period from 1991 through 1999. See footnote 3 for a
comparison of our procedure to that used in New York’s CABG report cards.




                                                                               11
 Table 1: Regressions of CABG Cases and Risk-Adjusted Mortality on Publicly
                           Reported Performance



                                                                           Risk-Adjusted
                                               CABG Cases                  Mortality Rate
                                                     Low         High
                                All Patients        Severity    Severity    All Patients
Months Since Initial High
Mortality Flag
             1-12                    -4.9 **         -5.4 ***     0.6               -1.2 ***
                                    (2.4)           (2.0)       (0.9)              (0.4)
            13-24                    -3.1            -3.7 *       0.7               -1.3 *
                                    (2.6)           (2.1)       (1.0)              (0.7)
            25-36                    -3.7            -4.0         0.4               -1.3 ***
                                    (3.7)           (2.8)       (1.4)              (0.5)
            More than 36             -7.1            -5.9        -1.2               -0.6
                                    (4.9)           (3.8)       (1.5)              (0.6)

Months Since Initial Low
Mortality Flag
             1-12                     3.0             1.5         1.5                0.2
                                    (6.6)           (4.5)       (2.4)              (0.3)
            13-24                    -0.8            -0.3        -0.6                0.3
                                    (6.8)           (4.2)       (3.1)              (0.2)
            25-36                    -1.8            -1.9         0.0                0.3 **
                                    (5.9)           (4.1)       (2.5)              (0.2)
            More than 36             -7.1            -3.2        -4.0                0.2
                                    (8.8)           (6.3)       (2.8)              (0.2)

Hospital Fixed Effects?             Yes               Yes         Yes            Yes
Year Fixed Effects?                 Yes               Yes         Yes            Yes

Average Value of
Dependent Variable                 47.49             35.63       12.13          2.55

Number of Observations             3,406             3,404       3,337          3,406
           2
Adjusted R                         0.862             0.838       0.706          0.073




*,**, and *** denote statistical signficance at the 10%, 5%, and 1% levels, respectively.

Note: The level of observation is the hospital-month. Standard errors (in parentheses)
are heteroskedasticity robust and clustered by hospital. In regression with risk-adjusted
mortality as the dependent variable, observations are weighted by the total number of
cases for the relevant hospital-month.


                                               12
*
    Cutler and Huckman: Harvard University and National Bureau of Economic Research.
Landrum: Harvard University. This research was funded by the National Institutes of Aging. We
are grateful to Jay Bhattacharya for helpful comments and to Michael Conti for research
assistance. The views presented in this paper do not necessarily reflect the findings of the New
York State Department of Health or the New York State Cardiac Advisory Committee.
Corresponding author: Robert S. Huckman, Harvard Business School, Soldiers Field, Boston,
MA 02163; Email: rhuckman@hbs.edu; Phone: 617-495-6649; Fax: 617-496-4059.
1
    This issue is also debated. William Ghali et al. (1997) find that the decline in mortality in New
York from 1990 to 1992 was not significantly different from that in Massachusetts—which had
not adopted a report card system as of 1992—over the same period.
2
    Dana Mukamel and Alvin Mushlin (1998) find that hospitals and surgeons with better reported
outcomes experienced higher rates of growth in the market shares following the reports, though
they do not consider how the outcomes for or severity of future patients seen by these providers is
affected. Stanley Dziuban et al. (1994) consider the impact of the New York report cards on one
hospital that was cited as having significantly poorer than average outcomes. This case study
provides some insight into steps that one hospital took in response to the reports.
3
    One difference in our analysis is that we keep the set of covariates constant over time, while
New York State varies them each year based on a stepwise regression procedure. In our risk-
adjustment model, we include any covariate that: 1) was used in the New York models for all data
from 1991 through 1999 and 2) was reliably collected for all nine years in our sample. A detailed
list of the covariates included in this regression is available from the authors upon request.
4
    While there is a small set of hospitals that received multiple high- or low-mortality flags during
the nine-year period, we use only the first occurrence of either type of flag in determining when
the high- or low-mortality indicator shifts from zero to one.
5
    We divide bypass surgery patients into low and high severity based on whether the predicted
mortality determined by our patient-level, logistic regression is below or above the mean
predicted mortality for the state of New York in a given year.



                                                   13
