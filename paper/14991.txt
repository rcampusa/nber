                               NBER WORKING PAPER SERIES




   IMPROVING THE NUMERICAL PERFORMANCE OF BLP STATIC AND DYNAMIC
       DISCRETE CHOICE RANDOM COEFFICIENTS DEMAND ESTIMATION

                                        Jean-Pierre H. Dubé
                                           Jeremy T. Fox
                                            Che-Lin Su

                                       Working Paper 14991
                               http://www.nber.org/papers/w14991


                      NATIONAL BUREAU OF ECONOMIC RESEARCH
                               1050 Massachusetts Avenue
                                 Cambridge, MA 02138
                                      May 2009




We thank Daniel Ackerberg, Steven Berry, John Birge, Amit Gandhi, Philip Haile, Lars Hansen, Panle
Jia, Kyoo il Kim, Samuel Kortum, Kenneth Judd, Sven Leyffer, Denis Nekipelov, Aviv Nevo, Jorge
Nocedal, Ariel Pakes, John Rust, Hugo Salgado, Azeem Shaikh and Richard Waltz for helpful discussions
and comments. We also thank workshop participants at CREST-INSEE / ENSAE, EARIE, the ESRC
Econometrics Study Group Conference, the Econometric Society, the Federal Trade Commission,
INFORMS, the International Industrial Organization Conference, the 2009 NBER winter IO meetings,
Northwestern University, the Portuguese Competition Commission, the Stanford Institute for Theoretical
Economics, the UK Competition Commission, the University of Chicago, and the University of Rochester.
Dubé is grateful to the Kilts Center for Marketing and the Neubauer Faculty Fund for research support.
Fox thanks the NSF, grant 0721036, the Olin Foundation, and the Stigler Center for financial support.
Su is grateful for the financial support from the NSF (award no. SES-0631622) and the University
of Chicago Booth School of Business. The views expressed herein are those of the author(s) and do
not necessarily reflect the views of the National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2009 by Jean-Pierre H. Dubé, Jeremy T. Fox, and Che-Lin Su. All rights reserved. Short sections
of text, not to exceed two paragraphs, may be quoted without explicit permission provided that full
credit, including © notice, is given to the source.
Improving the Numerical Performance of BLP Static and Dynamic Discrete Choice Random
Coefficients Demand Estimation
Jean-Pierre H. Dubé, Jeremy T. Fox, and Che-Lin Su
NBER Working Paper No. 14991
May 2009
JEL No. C01,C61,L0

                                            ABSTRACT

The widely-used estimator of Berry, Levinsohn and Pakes (1995) produces estimates of consumer
preferences from a discrete-choice demand model with random coefficients, market-level demand
shocks and endogenous prices. We derive numerical theory results characterizing the properties of
the nested fixed point algorithm used to evaluate the objective function of BLP's estimator. We discuss
problems with typical implementations, including cases that can lead to incorrect parameter estimates.
As a solution, we recast estimation as a mathematical program with equilibrium constraints, which
can be faster and which avoids the numerical issues associated with nested inner loops. The advantages
are even more pronounced for forward-looking demand models where Bellman's equation must also
be solved repeatedly. Several Monte Carlo and real-data experiments support our numerical concerns
about the nested fixed point approach and the advantages of constrained optimization.


Jean-Pierre H. Dubé                                Che-Lin Su
University of Chicago                              University of Chicago
Booth School of Business                           che-lin.su@chicagogsb.edu
5807 South Woodlawn Avenue
Chicago, IL 60637
and NBER
jdube@gsb.uchicago.edu

Jeremy T. Fox
Department of Economics
University of Chicago
1126 East 59th Street
Chicago, IL 60637
and NBER
fox@uchicago.edu
1        Introduction
Discrete-choice demand models have become popular in the demand estimation literature due
to their ability to accommodate rich substitution patterns between a large array of products.
Berry, Levinsohn and Pakes (1995), hereafter BLP, made an important contribution to this
literature by accommodating controls for the endogeneity of product characteristics (namely
prices) without sacrificing the flexibility of these substitution patterns. Their methodological
contribution comprises a statistical, generalized method-of-moments (GMM) estimator and
a numerical algorithm. However, many implementations of the BLP algorithm may produce
incorrect parameter estimates because they are vulnerable to numerical inaccuracy. We study
the underlying numerical theory leading to some of these computational problems and propose
an alternative algorithm that is robust to these sources of numerical inaccuracy.
        As in Berry (1994), the evaluation of the GMM objective function requires inverting the
nonlinear system of market share equations. BLP and Berry suggest nesting this inversion
step directly into the parameter search. BLP propose a contraction mapping algorithm to
solve this system of equations numerically. Following the publication of Nevo’s (2000b) “A
Practitioner’s Guide” to implementing BLP, numerous studies have emerged using BLP’s
algorithm for estimating discrete choice demand systems with random coefficients.
        Our first objective consists of exploring the numerical properties of BLP’s nested con-
traction mapping algorithm. We refer to this approach as the nested fixed point, or NFP,
approach.1 The GMM objective function and the corresponding nested inner loop may be
called hundreds or thousands of times during a numerical optimization over structural pa-
rameters. Therefore, it may be tempting to use a less stringent stopping criterion for the
inner loop to speed up estimation. We derive theoretical results to show how a less stringent
stopping criterion for the inner loop may cause two types of errors in the parameter estimates.
First, the inner loop error propagates into the outer loop GMM objective function and its
derivatives, which may cause an optimization routine to fail to converge. Second, even when
an optimization run converges, it may falsely stop at a point that is not a local minimum. To
illustrate these problems empirically, we construct examples based on pseudo-real data and
data generated from a known model. In these examples, the errors in the parameter estimates
from using loose tolerances for the NFP inner loop are large.
        Our second objective is to propose a new computational algorithm for implementing the
BLP estimator that eliminates the inner loop entirely and, thus, removes the potential for nu-
merical inaccuracy discussed above. Following Su and Judd (2008), we recast the optimization
of BLP’s GMM objective function as a mathematical program with equilibrium constraints
(MPEC). The MPEC method minimizes the GMM objective function subject to a system of
    1
   We borrow the term “nested fixed point” from the work of Rust (1987) on estimation of dynamic pro-
gramming models, without demand shocks.



                                                 2
nonlinear constraints requiring that the predicted shares from the model equal the observed
shares in the data. Because both the GMM objective function and the market share equations
are smooth, we solve a standard constrained optimization problem.
       We prefer the MPEC approach over the existing NFP approach for four reasons. First,
there is no nested inner loop and, hence, no numerical error from nested calls. This aspect
eliminates the potential for an optimization routine to converge to a point that is not a local
minimum of the true GMM objective function.2 Second, by eliminating the nested calls, the
procedure could be faster than the contraction mapping method proposed by BLP. Third,
the MPEC algorithm allows the user to relegate all the numerical operations to a call to
a state-of-the-art optimization package. The speed advantage of MPEC may be enhanced
further if a Newton-based solver with a quadratic rate of convergence is used.3 In contrast,
the convergence rate of the entire NFP approach has not been established in the literature.
Fourth, the MPEC approach applies to more general demand models: those where there is
no contraction mapping (Gandhi 2008).
       We emphasize the distinction between the estimation problems associated with poorly-
implemented numerical optimization (our focus) and problems of poor identification. From
a statistical perspective, the MPEC algorithm generates the same estimator as the correctly-
implemented NFP approach. Conceptually, MPEC is just an alternative formulation for
implementing BLP’s original estimator. Therefore, the theoretical results on consistency and
statistical inference in Berry, Linton and Pakes (2004) apply equally to NFP and MPEC.
       To illustrate the relative performance of MPEC and NFP, we conduct a series of sampling
experiments. Our first set of experiments documents the estimation problems that arise when
NFP is implemented poorly. In three examples, we document cases where a loose tolerance
for the contraction mapping in the NFP approach leads to incorrect parameter estimates
and/or the failure of an optimization routine to report convergence. We observe this problem
with optimization routines that use closed-form and numerical derivatives. The errors in the
estimated own-price elasticities are also found to be large in both pseudo-real field data and
in simulated data. Further, in one example we show that the parameter estimates always
produce the same incorrect point (a point that is not even a local minimum), so that using
multiple starting points may not be able to diagnose the presence of errors in the parameter
   2
     Petrin and Train’s (2008) control-function approach also avoids the inner loop by utilizing additional non-
primitive assumptions relating equilibrium prices to the demand shocks. Although MPEC is computationally
more intensive than the control-function approach, it nevertheless avoids the need for numerical inversion
while retaining the statistical properties of BLP’s original GMM estimator.
   3
     Alternative methods to a contraction mapping for solving systems of nonlinear equations with faster
rates of convergence typically have other limitations. For instance, the traditional Newton’s method is only
guaranteed to converge if the starting values are close to a solution, unless one includes a line-search or
a trust-region procedure subject to some technical assumptions. In general, most practitioners would be
daunted by the task of nesting a hybrid Newton method customized to a specific demand problem inside the
outer optimization over structural parameters.



                                                       3
estimates. We also use this example to show that an alternative Nelder-Meade or simplex
algorithm, which does not use gradient information, also usually converges to the wrong
solution.
   In a second set of sampling experiments, we explore the relative speed of the NFP ap-
proach (correctly implemented) and MPEC. We use numerical theory to show that the rate
of convergence of the NFP’s inner loop contraction mapping is bounded above by a function
that is linear in the Lipschitz constant of the contraction mapping. We derive an analytic
expression for the Lipschitz constant that depends on the data and the parameter values
from the demand model. We construct several Monte Carlo experiments in which MPEC
becomes several times faster than NFP when we manipulate the data in ways that increase
the Lipschitz constant. As expected, MPEC’s speed is relatively invariant to the value of the
Lipschitz constant because the contraction mapping is not used. Some may be concerned that
MPEC’s speed scales poorly with the number of parameters in the estimation problem. For
instance, adding more markets increases the number of demand shocks to be estimated as well
as the number of constraints to be satisfied. We conduct additional sampling experiments in
which we find that the relative speed advantage of MPEC over NFP is robust to substantial
increases in the number of markets. Finally, we show that the relative performance of MPEC
versus NFP is robust to the “data quality” in a series of sampling experiments that manipulate
the power of the instruments.
   The theoretical results we derive can be generalized well beyond the standard static BLP
model. In particular, the numerical problems associated with NFP will be magnified in
dynamic discrete-choice demand models with forward-looking consumers. Consider the recent
empirical literature on durable and semi-durable goods markets, where consumers can alter
the timing of their purchase decision based on expectations about future products and prices
(Carranza 2008, Gowrisankaran and Rysman 2007, Hendel and Nevo 2007, Melnikov 2002,
and Nair 2007). Estimating demand using the NFP algorithm now involves three numerical
loops: the outer optimization routine, the inner inversion of the market share equations, and
the inner evaluation of the consumers’ value functions (the Bellman equations) for each of
the several heterogeneous consumer types. The consumer’s dynamic programming problem
is typically solved with a contraction mapping with a slow rate of convergence. Furthermore,
Gowrisankaran and Rysman point out that the recursion proposed by BLP may no longer be
a contraction mapping for some specifications of dynamic discrete choice models. Hence, the
market share inversion is not guaranteed to converge to a solution, which, in turn, implies
that one may not evaluate the GMM objective function correctly.
   MPEC extends naturally to the case with forward-looking consumers. We optimize the
statistical objective function and impose consumers’ Bellman equations and market share
equations as constraints. Our approach eliminates both inner loops, thereby eliminating
these two sources of numerical error when evaluating the outer loop objective function. We

                                              4
produce benchmark results that show that MPEC can be faster than NFP under realistic
data generating processes. More importantly, we sometimes find that NFP fails to report
convergence to a local optimum, whereas MPEC routinely converges. In practice, when
the researcher does not know the true parameters, it may be difficult to assess the validity
of estimates from a non-converged run. We expect the relative performance of MPEC to
improve for more complex dynamic demand models that nest more calls to inner loops (Lee
2008 and Schiraldi 2008).
    Knittel and Metaxoglou (2008) explore the potential multiplicity of local minima of BLP’s
GMM objective function. To make sure our estimates are reliable, we employ multiple starting
points in each run of our Monte Carlo experiments and routinely find that both NFP and
MPEC recover the true structural parameters using data generated by the model. We also
examine the same pseudo-real dataset used by Knittel and Metaxoglou. Using a state-of-the-
art solver with 50 starting points in our implementation, we find the same local minimum
each time. We briefly comment that some non-gradient-based solvers may report estimates
that are not local minima.
    The remainder of the paper is organized as follows. We discuss BLP’s model in Section
2 and their statistical estimator in Section 3. Section 4 provides a theoretical analysis of
the NFP algorithm. Section 5 presents our alternative MPEC algorithm. Section 6 presents
examples of practices leading to errors in the estimates of parameters. Section 7 provides
Monte Carlo evidence for the relative performances of the NFP and MPEC algorithms. Section
8 discusses the extension to the dynamic analog of BLP, where MPEC’s advantages over NFP
are magnified. We conclude in Section 9.


2    The Demand Model
In this section, we present the standard random coefficients, discrete choice model of aggregate
demand. Consider a set of markets, t = 1, ..., T , each populated by a mass Mt of consumers
who each choose one of the j = 1, ..., J products available, or opt not to purchase. Each
product j is described by its characteristics (xj,t , ξj,t , pj,t ) . The vector xj,t consists of K
product attributes. Let xt be the collection of the vectors xj,t for all J products. The scalar
ξj,t is a vertical characteristic that is observed by the consumers and firms, but is unobserved
by the researcher. ξj,t can be seen as a market- and product-specific demand shock that
is common across all consumers in the market. For each market, we define the J-vector
ξt = (ξ1,t , ..., ξJ,t ). Finally, we denote the price of product j by pj,t and the vector of the J
prices by pt .
    Consumer i in market t obtains the utility from purchasing product j

                            ui,j,t = βi0 + x0j,t βix − βip pj,t + ξj,t + εi,j,t .               (1)


                                                     5
The utility of the outside good, the “no-purchase” option, is ui,0,t = εi,0,t . The parameter
vector βix contains the consumer’s tastes for the K characteristics and the parameter βip
reflects the marginal utility of income, i0 s “price sensitivity”. The intercept βi0 captures the
value of purchasing an inside good instead of the outside good. Finally, εi,j,t is an additional
idiosyncratic product-specific shock. Let εi,t be the vector of all J + 1 product-specific shocks
for consumer i.
    Each consumer is assumed to pick the product j that gives her the highest utility. If
tastes, βi = βi0 , βix , βip and εi,t , are independent draws from the distributions Fβ (β; θ),
                            

characterized by the parameters θ, and Fε (ε), respectively, the market share of product j is
                                             Z
                   sj (xt , pt , ξt ; θ) =                                              dFβ (β; θ) dFε (ε) .
                                             {βi ,εi,t |ui,j,t ≥ui,j 0 ,t ∀ j 0 6=j }
To simplify aggregate demand estimation, we follow the convention in the literature and
assume ε is distributed type I extreme value so that we can integrate it out analytically,
                                                                                
                                   Z       exp β 0 + x0j,t β x − β p pj,t + ξj,t
         sj (xt , pt , ξt ; θ) =                                                      dFβ (β; θ) .           (2)
                                       1 + Jk=1 exp β 0 + x0k,t β x − β p pk,t + ξk,t
                                          P
                                   β


This assumption gives rise to the random coefficients logit model.
   The empirical goal is to estimate the parameters θ characterizing the distribution of con-
sumer random coefficients, Fβ (β; θ). For practicality, BLP assume that Fβ (β; θ) is the prod-
uct of K independent normals, with θ = (µ, σ), the vectors of means and standard devi-
ations for each component of the K normals. However, several papers have studied the
non-parametric identification of the model (Bajari, Fox, Kim and Ryan 2009, Berry and Haile
2008, Berry and Haile 2009, Chiappori and Komunjer 2009, and Fox and Gandhi 2009). If a
parametric assumption is made about Fβ (β; θ), the integrals in (2) are typically evaluated by
Monte Carlo simulation. The idea is to generate ns draws of β from the distribution Fβ (β; θ)
and to simulate the integrals as
                                                                                           
                                       ns      exp   β 0,r + x0 β x,r − β p,r p + ξ
                                   1  X                         j,t            j,t      j,t
          ŝj (xt , pt , ξt ; θ) =                                                                 .         (3)
                                   ns     1 +
                                              PJ
                                                    exp     β 0,r + x0 β x,r − β p,r p      + ξ
                                      r=1       k=1                  k,t               k,t      k,t


In principle, many other numerical methods could be used to evaluate the market-share inte-
grals (Judd 1998, Chapters 7–9).
   While a discrete-choice model with heterogeneous preferences dates back at least to Haus-
man and Wise (1978), the inclusion of the aggregate demand shock, ξj,t , was introduced by
Berry (1994) and BLP. The demand shock ξj,t is the natural generalization of demand shocks
in the textbook linear supply and demand model. We can see in (2) that without the shock,


                                                               6
ξj,t = 0 ∀ j, market shares are deterministic functions of the x’s and p’s. In consumer-level
data applications, the econometric uncertainty is typically assumed to arise from randomness
in consumer tastes, ε. This randomness washes out in a model that aggregates over a suffi-
ciently large number of consumer choices (here a continuum). A model without market-level
demand shocks will not be able to fit data on market shares across markets, as the model
does not give full support to the data.


3    The BLP GMM Estimator
We now briefly discuss the GMM estimator typically used to estimate the vector of structural
parameters, θ. Like the textbook supply and demand model, the demand shocks, ξj,t , force
the researcher to deal with the potential simultaneous determination of price and quantity. To
the extent that firms observe ξj,t and condition on it when they set their prices, the resulting
correlation between pj,t and ξj,t introduces endogeneity bias into the estimates of θ.
    BLP address the endogeneity of prices with a vector of D instrumental variables that
do not appear in demand, zj,t . They propose a GMM estimator based on the conditional
moment condition E [ξj,t | zj,t , xj,t ] = 0. The instruments zj,t can be product-specific cost
shifters, although frequently other instruments are used because of data availability. Here
the K non-price characteristics in xj,t are also assumed to be mean independent of ξj,t and
hence to be valid instruments, although this is not a requirement of the statistical theory. The
estimator does not impose a parametric distributional assumption on the demand shocks ξj,t ,
as the identifying assumption is only E [ξj,t | zj,t , xj,t ] = 0. Computationally, the researcher
often implements the moments as E [ξj,t · h (zj,t , xj,t )] = 0 for some known, vector-valued
function h that gives C moment conditions. To summarize, the researcher’s data consist of
n                                  oT
  (xj,t , pj,t , sj,t , zj,t )Jj=1    for J products in each of T markets.
                           t=1
    To form the empirical analog of E [ξj,t · h (zj,t , xj,t )], the researcher needs to find the im-
plied values of the demand shocks, ξj,t , corresponding to a guess for θ. The system of market
shares, (2), defines a mapping between the vector of demand shocks and the market shares:
St = s (xt , pt , ξt ; θ) , or St = s (ξt ; θ) for short. Berry (1994), Berry and Pakes (2007), and
Gandhi (2008) prove that s has an inverse, s−1 , such that any observed vector of shares can
be explained by a unique vector ξt (θ) = s−1 (St ; θ). An individual demand shock ξj,t given
by this vector is s−1
                   j (St ; θ). For the random coefficients logit specification, we can compute
ξt using the contraction mapping proposed in BLP.
    A GMM estimator can now be constructed by using the empirical analog of the C moment
conditions,

                         T J                                 T J
                       1 XX                                1 X X −1
         g (ξ (θ)) =         ξj,t (θ) · h (zj,t , xj,t ) =       sj (St ; θ) · h (zj,t , xj,t ) .
                       T                                   T
                         t=1 j=1                              t=1 j=1


                                                     7
Let S be the vector of observed market shares in all markets and let s−1 (S; θ) be the implied
demand shocks in all markets. For some weighting matrix, W, we define the GMM estimator
as the vector, θ̂, that solves the problem
                                                        0
                            min Q (θ) = min g s−1 (S; θ) W g s−1 (S; θ) .
                                                                       
                                                                                                               (4)
                              θ              θ

The statistical efficiency of the GMM estimator can be improved by using more functions of
zj,t in the vector of moments, finding more instruments, using an optimal weighting matrix in
a second step, or using an efficient one-step method such as continuously-updated GMM or
empirical likelihood. However, as we show in the following sections, the numerical accuracy
of the algorithms used to compute Q (θ) is equally important, from a practical perspective,
as matters of statistical efficiency.


4     A Theoretical Analysis of the NFP Algorithm
In this section, we analyze the numerical properties of BLP’s NFP algorithm. The GMM
estimator described in Section 3 consists of an outer loop to minimize the objective function,
Q (θ) , and an inner loop to evaluate this function by inverting the system of market shares
using a contraction mapping. Therefore, each evaluation of the GMM objective function,
Q (θ) , nests a call to a fixed-point calculation.
    From a practical perspective, the speed of the NFP algorithm is determined by the number
of calls to evaluate the objective function and the computation time associated with the inner
loop for each function evaluation. In the subsections below, we first provide formal results on
the speed of convergence of the inner loop. We then show how numerical error from the inner
loop can propagate into the outer loop, potentially leading to incorrect parameter estimates.

4.1     The Rate of Convergence for the NFP Contraction Mapping
The evaluation of the GMM objective function, Q (θ) , requires us to compute the inverse:
ξt (θ) = s−1 (St ; θ) . For a given θ, the inner loop of the NFP algorithm solves the share
equations for the demand shocks ξ by iterating the contraction mapping
                                                              
                           ξth+1 = ξth + log St − log s ξth ; θ ,       t = 1, . . . , T,                      (5)

until the successive iterates ξth+1 and ξth are sufficiently close.4 Formally, we choose a small
number, for example 10−8 or 10−14 , for in as the inner loop tolerance level and require ξth+1
   4
     In our implementation of NFP, we iterate over exp(ξ) to speed up the computation because taking log-
arithms is slow. However, depending on the magnitude of ξ, the use of the exponentiated form exp(ξ) in a
                                                    ` ´in ξ, and` as a ´˛
contraction mapping can lose 3˛ to˛5 digits of accuracy                result, introduce ˛an additional
                                                                                                    ˛ source of nu-
merical error. For example, if ˛ξth ˛ = −8 and ˛exp ξth − exp ξth+1 ˛ = 10−10 , then ˛ξth − ξth+1 ˛ = 2.98 × 10−7 .
                                                ˛



                                                        8
and ξth to satisfy the stopping rule

                                                         ξth − ξth+1 ≤ in                       (6)

for the iteration h + 1 when we terminate the contracting mapping (5).5 Let ξt (θ, in ) denote
the first ξth+1 such that the stopping rule (6) is satisfied. We then use ξt (θ, in ) to approximate
ξt (θ) .
       We state the contraction mapping theorem, which provides the rate of convergence of the
inner loop of the NFP algorithm.
Theorem 1. Let Tθ : Rn → Rn be an iteration function and let Ar = ξ | ξ − ξ 0 < r be
                                                                 

a ball of radius r around a given starting point ξ 0 ∈ Rn . Assume that Tθ is a contraction
mapping in Ar , meaning
                                                           
                                  ξ, ξ˜ ∈ Ar ⇒ Tθ (ξ) − Tθ ξ˜ ≤ L (θ) ξ − ξ˜ ,

where L (θ) < 1 is called a Lipschitz constant. Then if

                                                ξ 0 − Tθ ξ 0
                                                                
                                                                    ≤ (1 − L (θ)) r,

the multidimensional equation ξ = Tθ (ξ) has a unique solution ξ ∗ in the closure of Ar , Ār =
{ξ | kξ − ξ0 k ≤ r}. This solution can be obtained by the convergent iteration process ξ h+1 =
Tθ ξ h , for h = 0, 1, . . . The error at the hth iteration is bounded:
       


                                                                 L (θ)               L (θ)h
                            ξ h − ξ ∗ ≤ ξ h − ξ h−1                      ≤ ξ1 − ξ0           .
                                                               1 − L (θ)           1 − L (θ)

       Theorem 1 states that at every iteration of the contraction mapping, the upper bound
for the norm of the error is multiplied by L (θ). Consequently, the rate of convergence of the
contraction mapping is linear and is measured by the Lipschitz constant L(θ). For a proof of
the theorem, see Dahlquist and Björck (2008).
       The following theorem shows how to express the Lipschitz constant for a mapping Tθ (ξ)
in terms of ∇Tθ (ξ), the Jacobian of Tθ (ξ). We then use the Lipschitz constant result to assess
an upper bound for the performance of the NFP algorithm.
Theorem 2. Let the function Tθ (ξ) : Rn → Rn be differentiable in a convex set D ⊂ Rn .
Then L (θ) = max k∇Tθ (ξ)k is a Lipschitz constant for T .
                      ξ∈D

The contraction mapping proposed by BLP to invert the market shares is

                                              Tθ (ξ) = ξ + log S − log s (ξ; θ) .
   5
       k(a1 , . . . , ab )k is a distance measure, such as max (a1 , . . . , ab ).


                                                                    9
We define a Lipschitz constant for the BLP contraction mapping T given structural parameters
θ as

                                     L(θ) = max k∇Tθ (ξ)k = max kI − ∇ (log s (ξ; θ))k ,
                                                  ξ∈D                        ξ∈D


      where
                         8      20          “                                       ”                 “                                       ”      12 3
                                        exp β 0,r + x0j,t β x,r − β p,r pj,t + ξj,t               exp β 0,r + x0j,t β x,r − β p,r pj,t + ξj,t
                                                                                           1 0
                         >
                         >   ns
                             X
                         >
                         >
                         >      6@
                                4               “                                        ” A−@            “                                        ” A 75
                                     1+ J    exp β 0,r + x0k,t β x,r − β p,r pk,t + ξk,t       1+ J    exp β 0,r + x0k,t β x,r − β p,r pk,t + ξk,t
                         >
                         >             P                                                         P
                             r=1
                         >
                         >
                         >               k=1                                                       k=1
                         >
                         >                                                  “                                  ”                                            ,   if   j =l
                                                                         exp β 0,r +x0 β x,r −β p,r pj,t +ξj,t
                         >
                         >
                         >
                         >                                      Pns                  j,t
                         >                                                                                         ” .
                                                                 r=1 1+ J
                                                                                 “
                                                                             exp β 0,r +x0 β x,r −β p,r pk,t +ξk,t
                         >
                         >                                             P
                         >
     `           ´
                         >
                         >
                         >                                               k=1             k,t
∂ log sj (ξt ; θ)
                         >
                         <
                     =                                                                                                                                                         .
       ∂ξlt              >
                         >
                         >                  “                                       ”               “                                        ”
                                        exp β 0,r + x0j,t β x,r − β p,r pj,t + ξj,t              exp β 0,r + x0l,t β x,r − β p,r pl,t + ξl,t
                         >          20                                                     10                                                      13
                         >
                         >
                         >     ns
                               X
                         >
                             −
                         >
                         >        4@                                                       A@                                                      A5
                         >                      “                                        ”               “                                       ”
                                     1+ J    exp β 0,r + x0k,t β x,r − β p,r pk,t + ξk,t      1+ J   exp β 0,r + x0k,t β x,r − β p,r pk,t + ξk,t
                         >
                         >             P                                                        P
                         >
                         >    r=1        k=1                                                     k=1
                                                                                                                                                        ,       if   j 6= l.
                         >
                         >                                                   “                                  ”
                                                                          exp β 0,r +x0 β x,r −β p,r pj,t +ξj,t
                         >
                         >
                         >                                      Pns
                         >
                         >
                         >                                                            j,t
                                                                  r=1 1+ J
                                                                                  “                                 ”
                                                                              exp β 0,r +x0 β x,r −β p,r pk,t +ξk,t
                         >
                         :                                              P
                                                                          k=1             k,t



It is difficult to get precise intuition for the Lipschitz constant as it is the norm of a matrix.
But, roughly speaking, the Lipschitz constant is related to the matrix of own and cross demand
elasticities with respect to the demand shocks, ξ, as the jth element along the main diagonal
     ∂sj,t 1
is   ∂ξj,t sj,t .    In Section 7.2, we use the Lipschitz constant to distinguish between simulated
datasets where we expect the contraction mapping to perform relatively slowly or rapidly.

4.2       Ensuring Convergence for the Outer Loop in NFP
In this subsection we show how numerical error from the inner loop propagates into the
outer loop. We then characterize the corresponding numerical inaccuracy in the criterion
function, Q (θ) , and its gradient. This analysis gives a rate result for the tolerance level for
the optimization in the outer loop to ensure that the optimization routine is able to report
convergence.
      We denote by ξ (θ, in ) the numerical calculation of the demand shocks corresponding to
a given value for θ and an inner loop tolerance in . We also denote the true demand shocks
as ξ (θ, 0). We let Q (ξ (θ, in )) be the programmed GMM objective function with the inner
loop tolerance in . In a duplication of notation, let Q (ξ) be the GMM objective function for
an arbitrary guess of ξ. We use big-O notation.
      The following theorem characterizes the bias in evaluating the GMM objective function
and its gradient at any structural parameters, θ, when there exist inner loop numerical errors.

Theorem 3. Let L (θ) be the Lipschitz constant for the inner loop contraction mapping. For
any structural parameters θ and given an inner loop tolerance in ,
                                                   
                                          L(θ)
  1. |Q (ξ(θ, in )) − Q (ξ (θ, 0))| = O 1−L(θ) in



                                                                        10
                                                                                            
                                                                                 L(θ)
   2.     ∇θ Q (ξ (θ))   ξ=ξ(θ,in )   − ∇θ Q (ξ (θ))      ξ=ξ(θ,0)   =O        1−L(θ) in       ,

                    ∂Q(ξ)                            ∂∇θ Q(ξ(θ))
assuming both        ∂ξ   ξ=ξ(θ,0)           and         ∂ξ      ξ=ξ(θ,0)   are bounded.

       The proof is in the appendix. Theorem 3 states that the biases in evaluating the GMM
objective function and its gradient at any structural parameters are of the same order as
the inner loop tolerance adjusted by the Lipschitz constant for the inner loop contraction
mapping.
       Convergence of an unconstrained optimization problem is declared when the norm of the
gradient, k∇Q (θ)k , of the GMM objective function is smaller than a pre-determined outer
loop tolerance level, out : k∇Q (θ)k ≤ out . The next theorem shows that the choice of the
outer loop tolerance, out , should depend on the inner loop tolerance, in .

Theorem 4. Let θ̂(in ) = arg max {Q (ξ (θ, in ))} . In order for the outer loop GMM mini-
                                             θ
mization to converge, the outer loop tolerance out should be chosen to satisfy out = O (in ) ,
assuming ∇2θ Q (ξ)        ξ=ξ (θ̂(in ),0)       is bounded.

The proof of this theorem shows that if in is large (the inner loop is loose), then the gradient
will be numerically inaccurate. Therefore, out needs to be large (the outer loop must be
loose) for an optimization routine to converge. The proof is in the appendix.
       These theorems summarize the numerical concerns associated with loosening the inner
loop tolerance, in , to speed up the convergence of the contraction mapping. By Theorem 4,
the resulting imprecision in the gradient could prevent the optimization routine from detecting
a local minimum and converging. In turn, the researcher may need to loosen the outer loop
tolerance to ensure the convergence of the outer loop optimization. Raising out reduces
precision in the estimates and, worse, could generate an estimate that is not a valid local
minimum.
       The default value for out in most packages is a small number, such as 10−6 . In practice,
we have found cases where BLP’s approach was implemented using out = 10−2 . Alternatively,
others have customized an adaptive version of the inner loop tolerance.6 In our Monte Carlo
simulations below, we will illustrate how a loose stopping criterion for the outer loop can cause
the routine to terminate early and produce incorrect point estimates. In some instances, these
estimates do not satisfy the first-order conditions for a local minimizer.
   6
     The procedure consists of using a loose inner loop tolerance when the parameter estimates appear “far”
from the solution and switching to a tighter inner loop tolerance when the parameter estimates are “close” to
the solution. The switch between the loose and tight     inner loop  tolerances is usually based on the difference
                                                                          then in = 10−8 ; otherwise, in ‚= 10−6 .
                                                    ‚           ‚
between the successive parameter iterates, e.g, if ‚θk+1‚ − θk ‚ ≤ 0.01,
                                                                                       −6
                                                            k+1    k
                                                                     ‚                      ‚ k+2     k+1
Suppose that the following   sequence of iterates occur: ‚θ     − θ ‚ ≥ 0.01 (in = 10 ), ‚θ      −θ       ‚ ≤ 0.01
(in = 10−8 ), and ‚θk+2 − θk+1 ‚ ≥ 0.01 (in = 10−6 ). The NFP objective value can oscillate because of the
                   ‚             ‚

use of two different inner loop tolerances. This oscillation can prevent the NFP approach from converging.




                                                               11
4.3     Finite-Sample Bias in Parameter Estimates from the Inner Loop Nu-
        merical Error
In this section, we discuss the small-sample biases associated with inner loop numerical error.
Assume, given in , that we have chosen out to ensure that NFP is able to report convergence.
Let θ∗ = arg min {Q (ξ(θ, 0)} be the minimizer of the finite-sample objective function without
                θ
numerical error. We study the upper bound on the bias in the final estimates, θ̂(in ) − θ∗ ,
from using an inner loop-tolerance in .

Theorem 5. Assuming ∇ξ Q (ξ)                    ξ=ξ (θ̂(in ),0)   is bounded, the difference between the finite-
sample minimizers with and without inner loop error satisfies
                                                                                                     
        
                             2
                                                                               L θ̂ (in )
    O       θ̂ (in ) − θ∗           ≤ Q ξ θ̂ (in ) , in − Q (ξ (θ∗ , 0)) + O                  in  .
                                                                                  1 − L θ̂ (in )

The proof is in the appendix. The right side of the inequality provides an upper bound for
the order of the square of the numerical
                                    error in 
                                                the parameters. There are two terms in this
upper bound. The first term, Q ξ θ̂ (in ) , in − Q (ξ (θ∗ , 0)) , is the bias in the GMM
function evaluatedat the finite-sample
                                          true   and estimated parameter values.7 The second
term arises from ξ θ̂ (in ) , in − ξ θ̂ (in ) , 0 , the bias in demand shocks with and without
                                                                    L(θ̂(in ))
the inner loop error, and is of the same order as                                .8
                                                                   1−L(θ̂(in )) in
    The bound in Theorem 5 is not always sharp or large. In our Monte Carlo experiments
below, we will show that the parameter errors associated with improper minimization and
failure to detect convergence are more severe issues in practice.

4.4     Large Sample Bias from the Inner Loop Numerical Error
The previous section focused only on numerical errors for a finite data set. Now consider θ0 ,
the true parameters in the data generating process. Here we explore how numerical errors in
the inner loop affect the consistency of the BLP estimator.
    Recall that θ̂ (in ) corresponds to the minimizer of Q (ξ (θ, in )) , the biased GMM objec-
tive function with the inner loop tolerance, in. Let Q̄ (ξ (θ, in )) = E [Q (ξ (θ, in ))] be the
probability limit of Q (ξ (θ, in )), as either T → ∞ or J → ∞, as in Berry, Linton and Pakes
(2004). Clearly, θ0 = arg min Q̄ (ξ (θ, 0)) if the BLP model is identified.
   7
     This term is related to a formalization of folk knowledge in the numerical-optimization literature . If a
                                                                                                       √
function to be optimized has error , then the minimizer of the function with error could have error of ; see
Chapter 8 in Gill, Murray and Wright (1981).
   8
     Ackerberg, Geweke and Hahn (Theorem 2, 2009) studied the case where the objective function is dif-
ferentiable in the equivalent of inner loop error and found a linear rate. However, in the BLP setting, the
GMM objective function is not differentiable with respect to inner loop error in . In our proof, we take this
non-differentiability into account and obtain a square-root upper bound.


                                                              12
   Let asymptotics be in the number of markets, T , and let each market be an iid observation.
By standard consistency arguments (Newey and McFadden 1994), θ∗ will converge to θ0
if Q (ξ (θ, 0)) converges to Q̄ (ξ (θ, 0)) uniformly, which is the case with a standard GMM
estimator. Further, the rate of convergence of the estimator without numerical error from the
                                            √
inner loop is the standard parametric rate, T . By the triangle inequality,

      θ̂ (in ) − θ0 ≤ θ̂ (in ) − θ∗ + θ∗ − θ0 ≤
                 vu                                                                   
                   u                                             L θ̂ (in )               √ 
              O t Q ξ θ̂ (in ) , in − Q (ξ (θ∗ , 0)) +                           in 
                                                                                          + O 1/ T . (7)
                 u                                                      
                                                                   1 − L θ̂ (in )

The asymptotic bias from numerical error in the inner loop persists and does not shrink
asymptotically. This is intuitive: inner loop error would introduce numerical errors in the
parameter estimates even if the population data were used.

4.5     Loose Inner Loop Tolerances and Numerical Derivatives
Most researchers use gradient-based optimization routines, as perhaps they should given that
the GMM objective function is smooth. Gradient-based optimization requires derivative in-
formation, by definition. One approach is to derive algebraic expressions for the derivatives
and then to code them manually. Our results above assume that the researcher’s optimizer
has information on the exact derivatives. In many applications, such as the dynamic de-
mand model we study below, calculating and coding derivatives can be very time consuming.
Instead, one may use numerical derivatives that approximate the gradient as follows
                                                                                            |θ|
                                           Q (ξ (θ + dek , in )) − Q (ξ (θ − dek , in ))
                ∇d Q (ξ (θ, in )) =                                                                 ,   (8)
                                                                 2d                            k=1

where d is a scalar perturbation and ek is a vector of 0’s, except for a 1 in the kth position of
ek . As d → 0, ∇d Q (ξ (θ, in )) converges to ∇Q (ξ (θ, in )), the numerically accurate gradient
of Q (ξ (θ, in )). However, the optimization of the GMM objective function requires the true
derivatives without inner loop error, ∇Q (ξ (θ, 0)).
   Lemma 9.1 in Nocedal and Wright (2006) shows that the numerical error in the gradient
is bounded,
                                                                                                  
                                                                    2
                                                                            1          L (θ)
               k∇d Q (ξ (θ, in )) − ∇Q (ξ (θ, 0))k∞ ≤ O d                  + O                 in .
                                                                             d        1 − L (θ)

There are two terms in this bound. The O d2 term represents the standard error that arises
                                            

from numerical differentiation, (8). As d → 0, the O d2 term converges to 0. The second
                                                       



                                                        13
                             
        1         L(θ)
term    dO       1−L(θ) in
                    arises from the numerical error in the objective function, for a given
                                                                                    
                  L(θ)                                                         L(θ)
in   > 0. The O 1−L(θ) in term comes from part 1 of Theorem 3. If d1 O 1−L(θ)     in is
relatively large, as it is when the inner loop tolerance is loose, then the bound on the error in
the gradient is large. In this case, a gradient-based routine can search in the wrong direction,
and end up stopping at a parameter far from a local minimum. Therefore, combining loose
inner loop tolerances and
                        numerical
                                  derivatives may produce an unreliable solver. Note that
                    1     L(θ)
as d → 0, the term d O 1−L(θ) in → ∞. So setting d to be small exacerbates the numerical
error arising from using a loose inner loop tolerance.


5      MPEC: A Constrained Optimization Approach
In this section, we propose an alternative algorithm to compute the BLP objective function
based on Su and Judd’s (2008) constrained optimization approach for estimating structural
models, MPEC. Originally, Su and Judd use this approach to compute the equilibrium for
an economic model. We use this same insight to solve for the fixed point associated with the
inversion of market shares. MPEC relegates all numerical computation to a single call to a
state-of-the-art optimization package, rather than the user’s own customized algorithm.
      Let W be the GMM weighting matrix. Our constrained optimization formulation is

                                        min      g (ξ)0 W g (ξ)
                                         θ,ξ
                                                                  .                                      (9)
                                  subject to     s (ξ; θ) = S

                                                      1   PT     PJ
The moment condition term g (ξ) is simply g (ξ) =     T    t=1        j=1 ξj,t   · h (zj,t , xj,t ). In MPEC,
the market share equations are introduced as nonlinear constraints to the optimization prob-
lem. The objective function is specified primitively as a function of the demand shocks ξ. We
optimize over both the demand shocks ξ and the structural parameters θ.
      The constrained optimization problem defined by (9) can be solved using a modern non-
linear optimization package developed by researchers in numerical optimization. The defaults
on feasibility and optimality tolerances in nonlinear solvers for constrained optimization are
usually sufficient. In contrast, NFP requires a customized nested fixed point calculation,
including the choice of tolerance, which could result in naive errors.
      The following theorem shows the equivalence of the first-order conditions between NFP
(4) and the constrained optimization formulation (9).

Theorem 6. Let the BLP demand model admit a contraction mapping. The set of first-order
conditions to the MPEC minimization problem in (9) is equivalent to the set of first-order
conditions to the true (no numerical error) NFP method that minimizes (4).

      The proof appears in the appendix. Theorem 6 states that any first-order stationary

                                               14
point of (4) is also a stationary point of (9), and vice versa. The MPEC and NFP algorithms
produce the same statistical estimator. Any statistical property of the original BLP estimator
applies to the estimator when computed via MPEC. Hypothesis tests, standard errors and
confidence intervals are the same for both methods. The GMM standard errors are discussed
in Berry, Linton and Pakes (2004) for J → ∞ and standard references for GMM for T → ∞
(Newey and McFadden 1994).
       The MPEC approach is theoretically superior to NFP in terms of the rate of conver-
gence because modern constrained optimization solvers use Newton-type methods to solve
the Karush-Kuhn-Tucker (KKT) system of the first-order optimality conditions. To formalize
this discussion , we first define rate of convergence as follows:

Definition 1. Let {θk } be a sequence in Rn and θ∗ be a point in Rn . Then we say that
                                                                                        kθk+1 −θ∗ k
   1. θk converges to θ∗ Q-linearly if there is a constant r ∈ (0, 1) such that          kθk −θ∗ k    ≤ r for
         all k sufficiently large.
                                                           kθk+1 −θ∗ k
   2. θk converges to θ∗ Q-superlinearly if limk→∞          kθk −θ∗ k    = 0.

                                                                                           kθk+1 −θ∗ k
   3. θk converges to θ∗ Q-quadratically if there is a constant K > 0 such that             kθk −θ∗ k2
                                                                                                         ≤K
         for all k sufficiently large.

       The theoretical convergence properties of the NFP outer loop have not been studied.
However, due to the numerical inversion in the inner loop, we conjecture that NFP is at best
superlinearly convergent. We will verify this property in our numerical experiments. A theo-
retical advantage of Newton-type methods, and hence MPEC, is that they are quadratically
convergent when the iterates are close to a local solution (e.g., Kelley 1995, 1999, 2003 and
Nocedal and Wright 2006). We will verify this property in our Monte Carlo experiments
below.
       MPEC eliminates the call to the inner loop, which can create an additional theoretical
speed advantage. In contrast, NFP makes a call to a linearly convergent contraction mapping
in the inner loop. As we show in Section 7.2, the contraction mapping in the NFP algorithm
might slow down as the Lipschitz constant approaches one. A partial solution might consist
of using a Newton-type method to solve the inner loop of the NFP algorithm (Rust 1987,
Bresnahan, Stern and Trajtenberg 1997, Davis 2006). However, this too would require cus-
tomized coding.9 Also, this approach does not resolve the issue of inner loop error if the inner
loop tolerance is too loose.
       There are several reasons why MPEC may be faster than NFP. One potential source of
speed improvement comes from the fact that MPEC allows constraints to be violated during
   9
    Most commercial optimization software does not permit being nested inside another call to the same
software. Therefore, it would be more convenient to use MPEC and have a single call to a Newton-type solver.


                                                    15
optimization. In contrast, the NFP algorithm requires solving the share equation (3) exactly
for every parameter θ examined in the optimization outer loop. Modern optimization solvers
do not enforce that the constraints are satisfied at every iteration. The constraints only need
to hold at the solution. This flexibility avoids wasting computational time on iterates far away
from the true parameters. Another speed advantage arises from the sparsity of the market
share equations, because demand shocks for a market t do not enter the constraints for other
markets t0 6= t. The solver can exploit the sparsity of the corresponding constraint Jacobian.
    We can exploit sparsity even further in the implementation of MPEC for the BLP model.
By treating the moments as additional parameters, we can re-state the problem in (9) as

                                               min          η0W η
                                               θ,ξ,η
                                       subject to           g (ξ) = η     .                              (10)
                                                            s(ξ; θ) = S

It is easy to see that the two formulations, (9) and (10), are equivalent. The objective
function in (10) is now a simple quadratic, η 0 W η, rather than a more complex function of
ξ. The additional constraint g(ξ) − η = 0 is linear in both ξ and η and, hence, does not
increase computational difficulty. The advantage with this alternative formation is that we
increase the sparsity of the constraint Jacobian and the Hessian of the Lagrangian function
by adding additional variables and linear constraints. In numerical optimization, it is often
easier to solve a large but sparse problem than a small but dense problem. In our Monte Carlo
experiments below, we will show that increasing the sample size and, hence, the dimension of
the optimization problem do not appear to disadvantage MPEC relative to NFP.
    The relative advantages of MPEC over NFP are not unique to GMM estimation. In
Appendix C, we show that likelihood-based approaches require inverting the market share
system and, hence, have typically been estimated using NFP.10


6    Parameter Errors from Loose Inner Loop Tolerances in the
     NFP Algorithm
In this section, we provide empirical support for the numerical problems derived in Section
4. We provide examples in which parameter errors can arise both in the context of sampling
experiments and with pseudo-real field data. We use the generated data to show that a com-
  10
     Nemirovsky and Yudin (1979) derive a lower bound on the number of function evaluations needed to
approximate a global solution for a non-convex, unconstrained optimization problem. The bound involves a
constant raised to a power in the number of parameters. However, this type of theoretical bound does not offer
a full comparison between MPEC and NFP, as NFP involves solving a nested inner loop, which is not part
of the analysis in Nemirovsky and Yudin. Furthermore, the evaluation error in the GMM objective function
could result in inaccurate approximation of a global solution.


                                                       16
bination of numerical derivatives and loose inner loop tolerances can lead to grossly incorrect
parameter estimates. We use the field data to show that incorrect parameter estimates can
arise even with closed-form derivatives.

6.1      NFP Algorithm Implementation
For estimation, we use a one-step GMM estimator with the weighting matrix W = (Z 0 Z)−1 ,
where Z is the T J × C matrix of instruments h (zj,t , xj,t ). We also exploit the normality
assumption for Fβ (β; θ) to concentrate the parameters characterizing the means of the random
coefficients out of the parameter search. Both decisions follow Nevo (2000b).
       We compare three implementations of the NFP algorithm, each initialized with the same
starting values. In the first scenario, we use a tight outer loop tolerance, out = 10−6 , and a
loose inner loop tolerance, in = 10−4 . While this outer loop tolerance is tight in the sense
that it is the default setting for most state-of-the-art optimization algorithms, we conjecture
that the inner loop tolerance is too loose.
       In the second scenario, we use a loose outer loop tolerance, out = 10−2 , and a loose
inner loop tolerance, in = 10−4 . Following Theorem 4, one can think of this scenario as
representing the attempt of the researcher to loosen the outer loop to promote convergence.
In practice, the converged point may not actually satisfy the first-order conditions. In the third
scenario, we implement the “best practice” settings for the NFP algorithm with in = 10−14
and out = 10−6 .
       Our estimation code is written in the MATLAB programming environment. We use the
TOMLAB interface to call the KNITRO optimization package (Byrd, Hribar, and Nocedal
1999, Byrd, Nocedal, and Waltz, 2006) in MATLAB.11

6.2      The Synthetic Data-Generating Process
Our first empirical example consists of a synthetic dataset based on the demand model in
Section 2. We construct T = 50 independent markets, each with the same set of J = 25
products. Each product j has K = 3 observed, market-invariant characteristics that are
generated as follows:
                                                                    
                            x1,j             0          1     −0.8 0.3
                         x2,j  ∼ N  0  ,  −0.8            1    0.3  .
                                                                   

                          x3,j          0       0.3            0.3    1
  11
   We found that MATLAB’s included solvers, fminunc and fmincon, often fail to converge to a local mini-
mum.




                                                  17
In addition, each product j has a market-specific vertical characteristic: ξj,t ∼ i.i.d. N (0, 1).
Finally, each product j has a market-specific price generated as follows:

                                                                      3
                                                                      X
                                   pj,t = 0.5 · ξj,t + ej,t + 1.1 ·         xk,j ,
                                                                      k=1

where ej,t ∼ N (0, 1) is an innovation that enters price.
    For each product j in market t, there is a separate     vector, zj,t , of D = 6 underlying
instruments generated as follows: zj,t,d ∼ U (0, 1) + 4 ej,t + 1.1 · 3k=1 xk,j,t , where U (0, 1)
                                                      1             P

is the realization of a uniform random variable and ej,t is the price innovation. In addition,
we also use higher-order polynomial expansions of the excluded instruments, zjt , and the
                            2 , z 3 , x2 , x3 ,
                                                QD            QK
exogenous regressors, xj : zj,t,d j,t,d j,k j,k  d=1 zj,t,d ,  k=1 xj,k , zj,t,d · xj1 , and zj,t,d ·
xj,t,2 . There are 42 total moments.
       There are five dimensions of consumer preference, βi = βi0 , βi1 , βi2 , βi3 , βip (an intercept,
                                                             

K = 3 attributes and price), each distributed independently normal with means and variances:
E [βi ] = {−1.0, 1.5, 1.5, 0.5, −3.0} and Var [βi ] = {0.5, 0.5, 0.5, 0.5, 0.2}.
       We simulate the integral in the market share equation, (3), with ns = 100 independent
standard normal draws. Because our focus is not on numerical integration error, we use the
same set of 100 draws to compute market shares in the data generation and estimation phases.

6.3       Synthetic Data, Numerical Derivatives and False Parameter Estimates
We create one simulated synthetic dataset, using the data-generating process from Section 6.2.
For this example, we use numerical derivatives. We construct 100 independent starting values
for the model parameters SD [βi ] by drawing them from a uniform distribution U (0, 7).12 We
run each of the three NFP implementations described in Section 6.1 for each of the 100 vectors
of starting values.
       For each implementation, we report the results across the 100 different starting values in
Table 1. The first row reports the fraction of runs for which the routine reports convergence.
Supporting Theorem 4, we find in column one that the optimization routine will never report
convergence if the inner loop tolerance is loose, in = 10−4 , even when the outer loop tolerance
has the default tight tolerance of out = 10−6 . In contrast, column two indicates that the
algorithm is more likely to converge (54% of the runs) when we also loosen the tolerance on
the outer loop. Below, we will show that convergence in this case is misleading; the estimates
are far from the truth. Finally, NFP with tight tolerances converges in 95% of the runs.
       To diagnose the quality of the estimates, the second row of Table 1 shows the fraction of
runs where the reported GMM objective function value was within 1% of the lowest objective
function that we found across all three NFP implementations and all 100 starting values (300
  12
       Recall that the means E [βi ] are concentrated out of the optimization problem (Nevo 2000b).


                                                       18
cases). We call this value the “global” minimum, although of course we cannot prove we have
found a true global minimum. In the first two columns, corresponding to the scenario with a
loose inner loop and the scenario with a loose inner and a loose outer loop, respectively, none
of the 100 starting values produced the so-called global minimum. In contrast, NFP tight
found the global minimum in each of the 100 runs. In general, there can be multiple local
minima and NFP could have converged to any one of them.
   The third and fourth rows of Table 1 provide measures to assess the economic implica-
tions of our different implementations. We use estimated price elasticities to show how naive
implementations could produce misleading economic predictions. In the third row, we report
the mean own-price elasticity, across all H = 100 starting values, all J products and all T
markets:

                                   H   T   J
                                 1 X 1 X 1 X p  h
                                             ηj,t θ̂ ,
                                 H   T   J
                                    h=1   t=1        j=1
                                                                                    
                                                                               p
where θ̂h is the vector of parameter estimates for the hth starting value and ηj,t  θ̂h is the
own price-elasticity of firm j in market t, at those parameters. The fourth row reports the
standard deviation of the mean own-price elasticities across all 100 starting values.
   Referring to the third row, the final column reports the own-price elasticity of demand
evaluated at the true parameter values: -5.68. As expected, NFP with a tight tolerance
produces an estimate near the truth, -5.77. However, our two loose implementations of NFP
produce mean elasticities that are not nearly as close to the truth. The mean of the NFP
loose-inner implementation is -7.24, higher in absolute value than the true value of -5.68.
The loose-both mean is -7.49. The standard deviations of own-price elasticities for the loose
inner loop tolerances are huge: 5.48 and 5.55. These findings are not surprising in light of
the numerical theory results about loose inner loop tolerances and numerical derivatives in
Section 4.
   It is true that the reported elasticities evaluated at each implementation’s best estimate, in
terms of the objective function values, appear to be relatively close to the truth. One should
not conclude from this evidence that loose implementations can be resolved simply by using
many starting values. None of the 100 runs of NFP with a loose inner loop converged, and
only 54% of the runs of NFP with both a loose inner and outer loop converged. In practice,
a researcher cannot prove whether non-converged runs are in fact close to a solution.

6.4   Parameter Errors with Nevo’s Data and Closed-Form Derivatives
Our second empirical example consists of the cereal dataset from Nevo (2000b), which we use
to study the numerical properties of NFP in the context of a pseudo-real dataset. We refer
the reader to Nevo (2000b) for a description of these data. For this example, we use analytic

                                                19
derivatives, which should improve the performance of all three NFP implementations.
    The results in Table 2 are of the same format as Table 1. For each of the three implemen-
tations (loose inner, loose both and tight both) we use the same set of 50 starting values for
the same cereal dataset. We pick our starting values by taking 50 draws from the standard
normal distribution.13 We set the inner loop tolerance to be 10−14 . We also report results for
the Nelder-Meade or simplex algorithm, which we will discuss below. As Theorem 4 predicts,
in row 1 we find that 0% of the NFP-loose inner loop starting values converge. Loosening
the outer loop is one approach to finding convergence; the second column finds that 76% of
starting values report convergence when this is done. 100% of the starting values converge
for NFP tight. The second row shows that 100% of the NFP-tight starting values find the
apparent global minimum, 0.00202, in Nevo’s cereal data. None of the NFP loose tolerance
implementations find the global minimum.
    The loose inner loop and loose-both methods find a mean own-price elasticity of -3.82
and -3.69, respectively. This is about half the value of -7.43 found with NFP tight. Further,
the estimates are all tightly clustered around the same points. With standard deviations of
0.40 and 0.07 for the loose inner loop methods, the answers are for the most part consistently
wrong across runs. The fifth row shows that the smallest objective function values found by
the loose-inner loop and loose-both routines are 0.00213 and 0.00683, respectively. The second
result is far from the truth of 0.00202. We manually inspected all 50 starting values and found
that only 1 of the 50 runs using the loose inner loop was anywhere near the apparent global
minimum. More striking is the fact that the remaining 49 runs all converge to the same
incorrect point as in the loose-both case. In short, the numerical imprecision of the gradient
of the objective function under the loose inner loop cases causes the optimization routine to
terminate at a point that is not a local minimum. This problem is not mitigated by using
a professional optimization package like KNITRO. The only solution is to use a tight inner
loop tolerance, such as 10−14 .
    Knittel and Metaxoglou (2008) recently used these same data to study a related potential
concern with BLP. They report that the parameter estimates are extremely sensitive to the
starting values used for the NFP algorithm because many local minima could exist in the GMM
objective function. Since the BLP problem is not convex, it may indeed have multiple optima.
As we saw above, our NFP code finds the same objective function value, 0.00202, which is
also the lowest objective value found by Knittel and Metaxoglou (2008).14 We conclude that
with multiple starting values, careful implementation of the numerical procedures ( a tight
inner loop), and a state-of-the-art optimization solver, the BLP estimator produces reliable
  13
     We also experimented with multiplying the starting values by the solution reported in Nevo (2000b). The
results were similar.
  14
     We also use the MATLAB code by Nevo (2000b), except that we use the KNITRO solver as the search
algorithm. For 50 starting points, KNITRO only converges for 25 of the 50 runs. However, all 25 successful
runs converge to the same solution with objective value 0.00202.


                                                    20
estimates.
    Interestingly, when we resort to non-derivative-based solvers, convergence occurs at points
that are not local minima. We start with the Nelder-Meade, or simplex algorithm, using the
same 50 starting values as above and a tight inner loop and tight outer loop. Results are
summarized in column four of Table 2. None of the 50 runs of the simplex algorithm find the
global minimum. Moreover, none of these runs satisfy the first-order optimality conditions.
Further, the elasticity estimate of -3.76 is around half of the numerically correct -7.43, and
the elasticity’s standard deviation across the fifty starting values is a relatively tight 0.35.
See McKinnon (1998) and Wright (1996) for further discussion of the problems with the
Nelder-Meade algorithm.
    Although not reported, we also used MATLAB’s genetic algorithm routine for one run.
The genetic algorithm found a point with the objective function value 0.043629, which is an
order of magnitude higher than 0.00202, the minimum we found using the gradient-based
method. We then started KNITRO from this point found by the generic algorithm and
KNITRO found the solution with objective value 0.00202. Once again, a non-derivative-based
solver fails to locate a local solution.


7     Speed Comparisons of MPEC and NFP
In this section, we use various manipulations of synthetic data and the Nevo cereal data to
compare the speed of MPEC and NFP. Hereafter, we focus only on NFP tight since in Sec-
tion 6 it was found routinely to recover the global optimum with multiple starting values.
Our approach involves manipulating aspects of the data-generating process that influence the
Lipschitz constant. A higher Lipschitz constant should slow the speed of the contraction map-
ping. We conjecture that the speed of MPEC should be relatively invariant to the Lipschitz
constant since it does not nest a call to the contraction mapping.

7.1    NFP and MPEC Implementations
We code NFP and MPEC using closed-form, first-order derivatives. We use the quadratic
objective-function form of MPEC in (10). We supply the sparsity pattern of the constraints
to the optimization routine, for MPEC. For all implementations of NFP and MPEC, we
use the interior point algorithm in KNITRO. Because we only supply the exact first-order
derivatives, we choose options for calculating second-order derivatives based on the number of
parameters in the NFP outer loop and the entire MPEC problem. To make our comparisons
fair, we select the best settings for each algorithm. In particular, the KNITRO options are
HESSOPT = 2 for NFP and HESSOPT = 4 for MPEC. We also use the algorithm options
within the interior point method that work the best for NFP and MPEC, respectively. For


                                              21
NFP, we choose the KNITRO option ALG=1, which is a direct decomposition of the first-
order Karush Kuhn Tucker (KKT) matrix, because the number of parameters in the NFP
outer loop is small. For MPEC, we choose the option ALG=2, which uses a conjugate gradient
iteration to solve the first-order KKT matrix, because the number of variables in MPEC is
usually on the order of several thousand. For detailed descriptions of algorithm options in
KNITRO, see Waltz and Plantenga (2009).
    An important point for our speed comparison is the choice of starting values. We always
use five starting values, which are uniform random numbers. For each NFP starting value,
we run the inner loop once and use this vector of demand shocks and mean taste parameters
as starting values for MPEC. Effectively, we use the same starting values for both NFP and
MPEC in that the two algorithms are initialized to have the same objective function value.

7.2    Lipschitz Constants
We define a base synthetic data case that will then be perturbed to vary the Lipschitz constants
in the examples that follow. The model is nearly the same as Section 6.2. As before, we use
T = 50 markets. The mean of the random coefficients is E [βi ] = (0.1, 1.5, 1.5, 0.5, −3.0). The
prices are pj,t = 3 + ξj,t · 1.5 + uj,t + 3k=1 xk,j,t , where uj,t is a uniform(0, 5) random variable.
                                          P

Likewise, zj,t,d = ũj,t,d + 41 uj,t + 1.1 · 3k=1 xk,j,t , where ũj,t,d is another uniform(0, 1) random
                                            P

variable and uj,t is the same variable as before. For each table below, we calculate 20 different
synthetic datasets, and reported means are across these 20 replications.
    Recall that the Lipschitz constant derived in Section 4.1 is related to the demand sensitivity
to the unobserved quality, ξj,t . Therefore, we experiment with different features of the data-
generating process that affect the relative importance of ξj,t for the market shares. Table 3
reports the Lipschitz constants for the base-case data-generating process. Each cell reports
the mean of the Lipschitz constants evaluated at the true parameter values across 30 data
sets / replications.
    In our first experiment, reported in the first column of Table 3, we manipulate the scale
of the parameters, βi . We multiply the βi of each of our ns simulated consumers in the
data-generating process by one of the constants listed in the table. We find that the Lipschitz
constant is non-monotone in the scale, with the constant first falling and then rising again.
This non-monotonicity comes from the fact that our manipulation also changes the levels of
the market shares. Nevertheless, holding the sample size fixed, we see fairly large changes in
the upper bound on the rate of convergence of the contraction mapping.
    The second column of Table 3 increases the standard deviation of the product-and-market-
specific demand shocks, ξj,t . When these shocks are more variable, products become more
vertically differentiated. Over the range of values we investigate, increases in the standard
deviation of the demand shocks increase the Lipschitz constant. The third column of Table


                                                  22
3 changes the number of markets. The number of markets has little impact on the Lipschitz
constant. Finally, the fourth column of Table 3 increases the mean of the intercept, E βi0 ,
                                                                                       

which changes the value of the inside goods relative to the outside good. As the inside-good
share increases, the Lipschitz constant increases.

7.3      Speed Comparisons of MPEC and NFP Using Synthetic Data
We now explore whether there is an implication of the Lipschitz constant for execution time.
We compare performances as we vary the mean of the intercept, E βi0 , from -1.9 to 3.1.
                                                                      

For each scenario, we run 20 replications of the data. For each data replication, we estimate
the GMM parameters using our two numerically-accurate algorithms, NFP with a tight inner
loop and MPEC. We use five different starting values in each dataset, taking the final point
estimates for each algorithm as the run with the lowest objective function value. In all cases,
the lowest objective function corresponded to a case where the algorithm reported that a
locally optimal solution had been found. We assess the estimates by looking at the own-price
elasticities, computed as a mean across products within each market and then across markets.
For each algorithm, we report the total CPU time required across all five starting values. The
results are reported in Table 4. All numbers in Table 4 are means across the 20 replications.
       Turning to Table 4, we can see that our numerical theory prediction holds in practice.
As expected, NFP with a tight inner loop tolerance and MPEC converged in all scenarios.
We also find that MPEC and NFP almost always generated identical point estimates, as one
would expect since they are statistically the same estimator (Theorem 6). Across the 20 runs,
MPEC and NFP produced identical estimates for the first four values of E βi0 . For the last
                                                                            

two values of E βi0 , MPEC and NFP are nearly identical. With only five starting values,
                 

by happenstance in one or two of the 20 replications, MPEC and NFP found different local
minima. Increasing the number of starting values to ten or fifteen would probably resolve this
discrepancy. We compute the root mean-squared
                                            h error
                                               i    (RMSE) and the bias of the own-price
elasticities. For a parameter θ1 , the bias is E θ̂1 − θ1 , where θ1 is the true value and the
expectation
  s        is taken over many estimates with independent samples. Likewise, the RMSE
         h i       2 
is E E θ̂1 − θ1         . In all cases, the RMSE is low and the bias is moderate at around
0.2, in comparison with a base elasticity of around -12, suggesting that the BLP estimator is
capable of recovering true demand elasticities.
   Run times for NFP tight vary dramatically with the level of the Lipschitz constant.15 For
the low Lipschitz case with E βi0 = −1.9, the average run time across the 20 replications
                              

is roughly 17 minutes for NFP and for MPEC. As we increase the intercept, the run times
  15
     We use ns = 100 draws to simulate the integrals under NFP and MPEC. In a real empirical application,
one would probably use around 10,000 draws, which would increase run times considerably; although most
likely not the relative run times of the two algorithms.


                                                   23
for NFP increase, while the run times for MPEC change very little. When E βi0 = 3.1, the
                                                                          

highest Lipschitz case, a single run with five starting values of NFP takes, on average, 60
minutes, whereas MPEC takes only 13 minutes. Remarkably, in this example the speed of
MPEC actually increases slightly as E βi0 decreases. The more striking effect is the decrease
                                      

in the speed of NFP, as expected. The shares of the outside good range from 90% to 47%,
consistent with most empirical applications of BLP. Hence, these speed results are not an
artifact of unusual data.
   We run an additional set of Monte Carlo experiments to ensure that the relative perfor-
mance of MPEC is robust to a much larger dataset with many more markets and, hence,
many more parameters. Of course, increasing the number of markets increases the number
of contraction mappings at each stage of the outer loop in the NFP optimization problem.
In principle, NFP might be affected more adversely as the experiment exacerbates the linear
convergence of the contraction mapping, versus the quadratic convergence of MPEC.
   Table 5 returns to the base specification, and varies only the number of markets, T . The
mean intercept is E βi0 = 1.1. As the number of markets increases, not surprisingly both
                     
                                              1373
methods take longer. MPEC requires only       555    = 40% of the time required by NFP for
T = 25, 41% of the time for T = 50, and only 27% of the time for T = 100. We conclude
that, in this example, the performance advantage of MPEC over NFP actually increases as
the number of demand shocks increase.
   As Appendix F reports, we also ran a set of Monte Carlo experiments that varied the
quality of our data by changing the power of the instruments. In principle, the quality of the
data could influence the convexity of the objective function and, hence, the trajectory of the
outer loop search. In Appendix F, the speed advantage of MPEC relative to NFP is found to
be robust to the quality of the data.
   As predicted by the numerical theory, it is easy to find cases where the slow rate of con-
vergence of the inner loop slows the overall execution time of NFP. In contrast, the execution
time of MPEC is fairly robust across scenarios. This relationship to run time highlights our
earlier concern about the choice of the inner loop tolerance. For real applications with many
more products and/or markets (e.g. 25 products and 450 market/quarters in Nevo (2000a,
2001) and 250 products and 10 years in BLP (1995)), run times could be considerably slower
than in our Monte Carlo experiments with only 25 products, 50 markets, and 100 simulation
draws. Because we have previously demonstrated the potential pitfalls from loosening the
inner loop, we therefore recommend MPEC as a safer and more reliable algorithm for the
computation of the BLP GMM estimator.




                                             24
7.4   Speed Comparisons of MPEC and NFP Using Nevo’s Data
As a final robustness check, we re-run the comparison of NFP and MPEC using Nevo’s pseudo-
real cereal data. Like NFP, MPEC converges to the same local minimum with an objective
function value of 0.00202 for 48 out of 50 starting values. For only two of the runs, MPEC
converges to a different local minimum with a higher objective-function value. In terms of
run time for one starting value, we find that MPEC required an average CPU time of 544
seconds whereas NFP required an average CPU time of 763 seconds. In short, the relative
performance of MPEC and NFP documented in our Monte Carlo experiments appears to hold
in the context of field data.
    Recall that part of the underlying theory for why MPEC might perform faster is the
quadratic rate of convergence of the Newton-based solver. We confirm this rate of convergence
by inspecting the output of the two estimation procedures. Table 6 reports the last iterations
of NFP and MPEC for one starting value on the Nevo dataset. The optimality error for NFP
does not decrease monotonically and takes 9 iterations to move from an optimality error of
10−4 to one of 10−7 . In contrast, MPEC takes only five iterations to move from 10−1 to 10−8 .
The feasibility error (the error in the market-share constraints) also takes five iterations to
move from 10−2 to 10−8 . Thus, the relative speed of MPEC versus NFP may indeed reflect,
to some extent, the theoretical advantage of a quadratically convergent algorithm.


8     Extension: Dynamic Demand Models
An even more promising frontier for MPEC is in the application of dynamic demand esti-
mation. Starting with Melnikov (2000), a new stream of literature has considered dynamic
analogs of BLP with forward-looking consumers making discrete choice purchases of durable
goods (Nair 2007, Gordon 2007, Carranza 2008, Gowrisankaran and Rysman 2008, Dubé,
Hitsch and Chintagunta 2008, Lee 2008, Schiraldi 2008). The typical implementation in-
volves a nested fixed point approach with two nested inner loops. The first inner loop is
the usual numerical inversion of the demand system to obtain the demand shocks, ξ. The
second inner loop is the iteration of the Bellman equation to obtain the consumers’ value
functions. In this section, we describe how MPEC can once again serve as a computationally
more attractive solution than NFP.

8.1   Dynamic BLP Model and Algorithms
We specify a simple model of discrete choice demand for a durable good with falling prices over
time and two competing products. There is a mass M of potential consumers at date t = 1.
Consumers are assumed to drop out of the market once they make a purchase. Abstracting



                                              25
from supply-side specifics, we assume that prices evolve over time as a function of the lagged
prices of both firms according to the rule


                 pj,t = ρj,0 + ρj,1 pj,t−1 + ρj,2 p−j,t−1 + ψj,t = p0t−1 ρj + ψj,t , j = 1, ..., 2               (11)

where ψj,t is a random supply shock. For the remainder of our discussion, we assume that
this supply shock is jointly distributed with the demand shock, (ξj,t , ψj,t ) ∼ N (0, Ω) , and is
independent across time periods, firms and markets. We assume that consumers have rational
expectations in the sense that they use the true price process, (11), to forecast future prices.
       On the demand side, forward-looking consumers now have a real option associated with
not purchasing because they can delay adoption to the future, when prices are expected to
be lower. A consumer r’s expected value of waiting is16


         v0r (pt ; θr )
                                r (p0 ρ + ψ; θ r ) + 
                     (                                       )
           R                  v 0   t  j              0
       = δ max             n                                o dF ()dF (ψ, ξ)
                                                                     ψ,ξ
                      maxj βjr − αr (p0t ρj + ψ) + ξj + j
                                                                              !!
                                                                           
                           r   0          r               r   r 0
             R                                    P
       = δ       log exp (v0 (pt ρj + ψ; θ )) + exp βj − α (pt ρj + ψ) + ξj      dFψ,ξ (ψ, ξ) .
                                                     j
                                                                                                                 (12)
To simplify the calculation of the expected value of waiting             v0r   (pt   ; θr ),   we approximate it with
Chebyshev polynomials (Judd 1998). We outline the Chebyshev approximation in Appendix
D. Our focus on the expected value of waiting, rather than the consumer’s value function, is
merely exploiting the special structure of this model.
       We use a discrete distribution with R mass points to characterize the consumer popula-
tion’s tastes at date t = 1,
                                       
                                       
                                       
                                        θ1 ,    Pr(1) = λ1
                                        .
                                       
                                                      ..
                                         ..            .
                                    θ=                                           ,
                                       
                                                         R−1
                                        θR , Pr(R) = 1 −
                                                          P
                                                             λr
                                       
                                       
                                                                   r=1

where θr = (αr , β r ). This heterogeneity implies that certain types of consumers will systemat-
ically purchase earlier than others. The mass of consumers of a given type r at the beginning
of period t, Mtr , is                    
                                                   M λr            , t=1
                                 Mtr =                                                 .
                                          M r S r (X ; θr ) , t > 1
                                            t−1 0    t−1

  16
    Here we make the normalization that the location parameter of the Type I Extreme Value distribution
equals -0.577.


                                                         26
In a given period t, the market share of product j is

                                 R
                                 P                           exp(βjr −αr pj,t +ξj,t )
                 sj (pt ; θ) =       λt,r                                                      ,    j = 1, ..., 2   (13)
                                            exp(              )+ Jk=1 exp(βkr −αr pk,t +ξk,t )
                                                   v0r (pt ;θr )
                                                                 P
                                 r=1

where                                                   
                                                                  λr          ,t = 1
                                        λt,r =                 r
                                                            PMt               ,t > 1
                                                                   r   Mtr

is the probability mass associated with type r consumers still in the market at date t. The
finite-types assumption eases dynamic programming because there is only one unknown value-
of-waiting function for each type.
     The empirical model consists of the system (11) and (13), which we write more compactly
as                                           "          #          "                        #
                                                 ψt                          pt − p0t−1 ρ
                                     ut ≡                    =                                  .
                                                   ξt                   s−1 (pt , St ; Θ)
The multivariate normal distribution of ut induces the density on the observable outcomes,
Yt = (p, St ),                                                       
                                                         1    1 0 −1
                        fY (Yt ; θ, ρ, Ω) =           1 exp  − ut Ω ut |Jt,u→Y |
                                            (2π)J |Ω| 2       2

where Jt,u→Y is the (2J × 2J) Jacobian matrix corresponding to the transformation-of-variables
from ut to Yt . We provide the derivation of the Jacobian in Appendix E.
     An NFP approach to maximum likelihood estimation of the model parameters amounts
to solving the optimization problem

                                                         T
                                                         Y
                                             max               fY (Yt ; θ, ρ, Ω) .                                  (14)
                                             {θ,ρ,Ω}
                                                         t=1

This problem nests two inner loops. Each stage of the outer loop maximization of the like-
lihood function in (14) nests a call to compute the fixed point of the contraction mapping,
(12), in order to obtain the expected value of waiting. There is also a nested call to compute
the fixed point of the BLP contraction mapping, (5), to compute the demand shocks ξt (the
inversion). Numerical error from both these inner loops propagates into the outer loop. Thus,
the numerical concerns regarding inner loop convergence tolerance discussed for static BLP
are exacerbated with dynamic analogs of BLP.
    Let D be the support of the state variables. An MPEC approach to maximum likelihood




                                                                       27
estimation of the model parameters amounts to solving the optimization problem
                               T                “      0
                                                              ”
                                    1
                                             exp − 21 ut Ω−1
                               Q
               max                       1                u ut |Jt,u→Y |
             {θ,ρ,Ω,ξ,v}   t=1 (2π)J |Ω| 2
            subject to     s(ξt ; θ) = St ∀ t = 1, . . . , T

                                                       exp (v0r (p0d ρj + ψ)) + ...
                                             0                                        1

                           v0r (pd ) = δ log @      exp βjr − αr (p0d ρj + ψ) + ξj A dFψ,ξ (ψ, ξ)
                                                P          `                        ´
                   and
                                             B                                        C
                                                  j


                           ∀ d ∈ D, r = 1, . . . , R.

In this formulation, we now optimize over the demand shocks, ξ, and the expected value of
waiting evaluated at each point, v r (pd ). In this case, D ⊂ R2+ is the support of the two
products’ prices. While this approach increases the number of parameters in the outer loop
optimization problem substantially compared to NFP, MPEC completely eliminates the two
inner loops. Chebyshev approximation reduces the dimension of this problem substantially
by searching over the Chebyshev weights, rather than over the value function at each point
in a discretized state space.

8.2   Dynamic BLP Monte Carlo Experiments
To assess the relative performance of MPEC versus NFP in the context of our dynamic durable
goods example, we construct the following Monte Carlo experiments. In the first experiment,
we assume there is only a single consumer type, R = 1. It is easy to show that in this case, ξt
can be computed analytically by log-linearizing the market shares, (13). We begin with this
case because it only involves a nested call to the calculation of the expected value of waiting.
We assume that the consumers’ preferences are: (β1 , β2 , α) = (4, 3, −1) . It is straightforward
to show that the speed of the contraction mapping associated with the consumer’s expected
value of waiting is related to the discount factor. Therefore, we compare performance with
the two different discount factors δ = 0.96 and δ = 0.99 corresponding roughly to an annual
rate and a quarterly rate, respectively, if the interest rate is 4%. We assume that prices are
generated as follows:
                           "                                                   #
                               p1,t = 5 + 0.8p1,t−1 + 0.0p2,t−1 + ψ1,t
                                                                                   .
                               p2,t = 5 + 0.0p1,t−1 + 0.8p2,t−1 + ψ2,t
                                                                                                "             #!
                                                                                                    1   0.5
Finally, we assume the supply and demand shocks satisfy (ψj,t , ξj,t ) ∼ N                 0,
                                                                         0.5 1
and are independent across markets and time periods. For our Chebyshev approximation,
we use six nodes and a fourth-order polynomial. For the NFP algorithm, we use an inner
loop tolerance of 10−14 for the calculation of the expected value of waiting. We use data on
M = 20 markets and T = 50 time periods per market.


                                                        28
          It is very difficult to derive analytic expressions for the Jacobian of the outer loop op-
timization associated with dynamic BLP, both under NFP and MPEC. As we discussed in
Section 4.5, the use of numerical derivatives introduces yet another source of numerical error
into the outer loop optimization. However, due to its formulation as a standard constrained-
optimization problem, the MPEC algorithm can potentially exploit automatic differentiation
to obtain exact derivatives for the outer loop (Griewank and Corliss 1992). To the best of our
knowledge, it would be non-standard to apply automatic differentiation to an NFP problem
because of the nested calls to fixed-point computations. Therefore, in our Monte Carlo ex-
periments, we compare the performance of NFP using numerical differentiation and MPEC
using automatic differentiation.17
          Results from twenty replications of this first experiment are reported in Table 7, where we
use the discount factor δ=0.96. We report the average point estimate and RMSE associated
with each of the structural parameters, for MPEC and NFP respectively. With tight inner
loop settings and allowing for five different starting values, we find that MPEC and NFP
produce identical point estimates. Inspection of the replications found that MPEC and NFP
found the same solution for all replications. However, in terms of speed, MPEC is roughly
60% faster in terms of CPU time than NFP.
          In Table 8, we run another twenty replications of the same one-type model using a discount
factor of δ = 0.99. Even with five starting values per replication, MPEC appears to perform
slightly better overall in terms of RMSE, especially for the utility intercepts. Furthermore,
MPEC is just under twice as fast at NFP in terms of CPU time.
          We also find that NFP fails to converge to the solution in several runs. Focusing on the last
row of Table 8, for two replications of NFP, the optimization routine was not able to diagnose
convergence for the greatest likelihood function value out of the five starting values. In these
two replications, NFP found a worse objective function value than MPEC. All replications
of MPEC converged to a local maximum. These results highlight the benefit of automatic
differentiation in allowing the routine to check whether it has found a valid local maximum.
18

          We now run a second Monte Carlo experiment that allows for consumer heterogeneity,
one of the main elements of the BLP model. We only consider the MPEC algorithm so as
to illustrate its applicability to this more computationally challenging case. We allow for a
     17
    We use the MAD (MATLAB Automatic Differentiation) package, which is part of TOMLAB.
     18
    For three of the twenty replications, MPEC converged to a worse solution than NFP. For δ = 0.99 and
unlike the δ = 0.96 case, it appears five starting values are not enough to always find the same solution with
MPEC or NFP.




                                                     29
second type of consumer with heterogeneity in price sensitivity:
                               
                                (4, 3, −1) , with probability λ=0.7
                            0
               (β1 , β2 , α) =                                                 .
                                (4, 3, −2) , with probability (1 − λ)=0.3

This case now requires constraints corresponding both to the expected value of waiting for
each of the R = 2 consumer types and to the share equations. To conserve on computer time,
we use only M = 5 markets per replication and set δ = 0.90. Results are reported in Table
9. Not only do we find that the parameters are recovered quite well, the average run time
requires only 2.6 hours of CPU time. In short, these results are encouraging for MPEC as a
practical approach to estimating dynamic BLP with unobserved heterogeneity in a broader
context.


9    Conclusions
In this paper, we analyzed the numerical properties of the NFP approach proposed by BLP
to estimate the random coefficients logit demand model. NFP is vulnerable to error due to
the inner loop. In practice, the NFP approach may also be slow. We used the Lipschitz
constant to establish particular data-generating processes for which the inner loop is likely
to be slow. A concern is that a loose inner loop tolerance is used precisely when the speed
of NFP is slow. Using numerical theory and computational examples with both pseudo-real
and synthetic data, we showed that setting loose inner loop tolerances can lead to incorrect
parameter estimates and a failure of an optimization routine to report convergence. Using
the field data, we showed a case where the estimates with multiple starting values always
reported (more or less) the same incorrect estimate.
    We then proposed a new constrained optimization formulation, MPEC, which avoids the
inner loop for repeatedly inverting the market share equations and, hence, eliminates the
numerical error in evaluating the objective function and its gradient. It also delegates all the
numerical computation to a single call to a state-of-the-art constrained optimization solver.
MPEC produces good estimates relatively fast for all the data-generating processes we con-
sidered. Its speed is invariant to the Lipschitz constant, as expected.
    As an extension, we adapt the MPEC approach to a new class of applications with forward-
looking consumers. The relative advantage of MPEC is even stronger with dynamics because
two inner loops must be solved: the dynamic programming problem and the market share
inversion. This burdensome collection of three loops (optimization, market shares, dynamic
programming) makes the traditional BLP approach nearly untenable in terms of computa-
tional time. Indeed, the lack of easily-derivable closed-form derivatives prevents the NFP
routine from detecting convergence, in many instances. Current work (Lee 2008, Schiraldi


                                              30
2008) further extends the number of inner loops being solved in estimation. As demand mod-
els become richer, the computational benefits of MPEC over NFP become greater. MPEC
can also be used for demand models where there is a unique vector of demand shocks that
rationalize the market shares, but no contraction mapping.
   While we have conducted our analysis in the context of random coefficients demand es-
timation, we emphasize that our numerical theory results along with several of our insights
generalize to broader empirical contexts using a nested fixed-point approach. One area where
we expect our findings to generalize quite nicely is the empirical study of dynamic games.
Researchers frequently solve dynamic games using iterative algorithms and, hence, we ex-
pect issues of loose tolerances to be problematic for applications nesting the solution of the
game inside the estimation procedure. We also expect the speed advantages of MPEC to be
important in this class of research.


References

 [1] Ackerberg, D., J.. Geweke and J.. Hahn (2009): “Comments on ’Convergence Properties of
     the Likelihood of Computed Dynamic Models’ by Fernandez-Villaverde, Rubio-Ramirez
     and Santos”, Econometrica, Forthcoming.

 [2] Andrews, D. W. K. (2002): “Generalized Method of Moments Estimation When a Pa-
     rameter is on a Boundary,” Journal of Business and Economic Statistics, 20 (4), 530–544.

 [3] Bajari, P., J. T. Fox, K.-I. Kim and S. P. Ryan (2009): “The Random Coefficients Logit
     Model Is Identified,” Working Paper, University of Chicago.

 [4] Berry, S. (1994): “Estimating Discrete-Choice Models of Product Differentiation,” RAND
     Journal of Economics, 25(2), 242–262.

 [5] Berry, S. and P. A. Haile (2008): “Nonparametric Identification of Multinomial Choice
     Models with Heterogeneous Consumers and Endogeneity,” Working Paper, Yale Univer-
     sity.

 [6] Berry, S. and P. A. Haile (2009): “Identification of Discrete Choice Demand From Market
     Level Data,” Working Paper, Yale University.

 [7] Berry, S., J. Levinsohn, and A. Pakes (1995): “Automobile Prices in Market Equilibrium,”
     Econometrica, 63(4), 841–890.

 [8] Berry, S., O. B. Linton, and A. Pakes (2004): “Limit Theorems for Estimating the
     Parameters of Differentiated Product Demand Systems,” Review of Economic Studies,
     71(3), 613–654.

 [9] Berry, S. and A. Pakes (2007): “The Pure Characteristics Demand Model,”International
     Economic Review, 48(4), 1193–1225.


                                             31
[10] Byrd, R. H., M. E. Hribar, and J. Nocedal (1999): “An Interior Point Method for Large
     Scale Nonlinear Programming.” SIAM Journal on Optimization, 9(4), 877–990.

[11] Byrd, R. H., J. Nocedal and R. A. Waltz (1999): “KNITRO: An Integrated Package
     for Nonlinear Optimization,” in Large-Scale Nonlinear Optimization, G. di Pillo and
     M. Roma, eds„ 35–59. Springer.

[12] Carranza, J. E. (2008): “Product Innovation and Adoption in Market Equilibrium: The
     Case of Digital Cameras,” Working Paper, University of Wisconsin.

[13] Dahlquist, G. and Å. Björck (2008): Numerical Methods in Scientific Computing. SIAM,
     Philadelphia, PA.

[14] Davis, P. J. (2006): “The Discrete Choice Analytically Flexible (DCAF) Model of De-
     mand for Differentiated Products,” CEPR Discussion Papers 5880.

[15] Dubé, J.-P., G. Hitsch and P. Chintagunta (2008): “Tipping and Concentration in Mar-
     kets With Indirect Network Effects,” Working Paper, University of Chicago.

[16] Fox, J. T. and A. Gandhi (2009): “Identifying Heterogeneity in Economic Choice Models,”
     Working Paper, University of Chicago.

[17] Gandhi, A. (2008): “On the Nonparametric Foundations of Product Differentiated De-
     mand Systems,”Working Paper, University of Wisconsin-Madison.

[18] Gill, P. E., W. Murray and M. H. Wright (1981): Practical Optimization, Academic
     Press, London.

[19] Gowrisankaran, G. and M. Rysman (2007): “Dynamics of Consumer Demand for New
     Durable Goods,” Working Paper, The University of Arizona.

[20] Griewank, A. and G. F. Corliss, editors (1992): Automatic Differentiation of Algorithms:
     Theory, Implementation, and Application. SIAM, Philadelphia, PA.

[21] Hausman, J. A. and D. A. Wise (1976): “A Conditional Profit Model for Qualita-
     tive Choice: Discrete Decisions Recognizing Interdependence and Heterogeneous Pref-
     erences,” Econometrica, 46(2), 403–426.

[22] Hendel, I. and A. Nevo (2007): “Measuring the Implications of Sales and Consumer
     Inventory Behavior,” Econometrica, 74(16), 1637–1673.

[23] Jiang, R., P. Manchanda and P. E. Rossi (2009): “Bayesian Analysis of Random Coeffi-
     cient Logit Models Using Aggregate Data,” Journal of Econometrics, 149, 126–148.

[24] Judd, K. L. (1992): “Projection Methods for Solving Aggregate Growth Models,” Journal
     of Economic Theory, 58(2), 410–452.

[25] Judd, K. L. (1998): Numerical Methods in Economics. MIT Press, Cambridge, MA.

[26] Kelley, C. T. (1995): Iterative Methods for Linear and Nonlinear Equations. SIAM,
     Philadelphia, PA.


                                             32
[27] Kelley, C. T. (1999): Iterative Methods for Optimization, SIAM, Philadelphia, PA.

[28] Kelley, C. T. (2003): Solving Nonlinear Equations with Newton’s Method. SIAM,
     Philadelphia, PA.

[29] Knittel, C. R. and K. Metaxoglou (2008): “Estimation of Random Coefficient Demand
     Models: Challenges, Difficulties and Warnings,” Working Paper, U.C. Davis.

[30] Lee, R. S. (2008): “Vertical Integration and Exclusivity in Platform and Two-Sided Mar-
     kets,” Working Paper, New York University.

[31] McFadden, D. and K. Train (2000): “Mixed MNL Models for Discrete Response,” Journal
     of Applied Econometrics, 15(5): 447–470.

[32] McKinnon, K. I. M. (1998): “Convergence of the Nelder-Mead Simplex Method to a
     Nonstationary Point,” SIAM Journal on Optimization, 9(1), 148–158.

[33] Melnikov, O. (2001): “Demand for Differentiated Durable Products: The Case of the
     U.S. Computer Printer Market,” Working Paper, Yale University.

[34] Nair, H. (2007): “Intertemporal Price Discrimination with Forward-looking Consumers:
     Application to the US Market for Console Video-Games,” Quantitative Marketing and
     Economics, 5(3), 239–292.

[35] Nemirovsky, A. S. and D. B. Yudin (1979 Russian, 1983): Problem Complexity and
     Method Efficiency in Optimization. John Wiley & Sons.

[36] Nevo, A. (2000a): “Mergers with Differentiated Products: The Case of the Ready-to-Eat
     Cereal Industry,” RAND Journal of Economics, 31(3), 395–421.

[37] Nevo, A. (2000b): “A Practitioner’s Guide to Estimation of Random Coefficients Logit
     Models of Demand,” Journal of Economics and Management Strategy, 9(4), 513–548.

[38] Nevo, A. (2001): “Measuring Market Power in the Ready-to-Eat Cereal Industry,” Econo-
     metrica, 69(2), 307–342.

[39] Nocedal, J. and S. J. Wright (2006): Numerical Optimization. Springer, New York, NY.

[40] Petrin, A. (2002): “Quantifying the Benefits of New Products: The Case of the Mini-
     van”,Journal of Political Economy, 110, 705–729.

[41] Petrin, A. and K. Train (2009): “Control Function Corrections for Omitted Attributes
     in Differentiated Products Markets,”Journal of Marketing Research, Forthcoming.

[42] Rust, J. (1987): “Optimal Replacement of GMC Bus Engines: An Empirical Model of
     Harold Zurcher,” Econometrica, 55(5), 999–1033.

[43] Schiraldi, P. (2008): “Automobile Replacement: a Dynamic Structural Approach,” Work-
     ing Paper, London School of Economics.

[44] Su, C.-L. and K. L. Judd (2008): “Constrained Optimization Approaches to Estimation
     of Structural Models,” Working Paper, University of Chicago..

                                            33
[45] Waltz, R. A. and T. D. Plantenga (2009): KNITRO 6.0 Users’s Manual. Ziena Opti-
     mization, Inc.

[46] Wright, M. H. (1996): “Direct Search Method: Once Scorned, Now Respectable,” in
     Numerical Analysis 1995: Proceedings of the 1995 Dundee Biennial Conference in Nu-
     merical Analysis, D. F. Griffiths and G. A. Watson, eds., Addison Wesley Longman,
     Harlow, UK, 191–208.

[47] Yang, Sha, Yuxin Chen and Greg M. Allenby (2003). "Bayesian Analysis of Simultaneous
    Demand and Supply," Quantitative Marketing and Economics, 1, 251-304.




                                           34
A       Proofs
In all the proofs below, we assume the sum of the second order and other higher order terms
in a Taylor series expansion is bounded. This is a conventional assumption in the numerical
optimization
            literature and allowsus to use
                                            the big-O notation
                                                               with a second order term, e.g.,
                                     2                    2
O ξ(θ̂(in ), in ) − ξ(θ̂(in ), 0)   or O θ̂(in ) − θ∗    .


A.1      Proof of Theorem 3
By a Taylor series expansion of Q (ξ) around ξ (θ, 0), we have

             Q (ξ (θ, in )) − Q (ξ (θ, 0))
             h                 i0                                                            
           = ∂Q(ξ)
                ∂ξ   ξ=ξ(θ,0)     (ξ (θ,  in ) − ξ (θ, 0)) + O   kξ (θ,  in ) − ξ (θ, 0)k 2


                and
                ∇θ Q (ξ)     ξ=ξ(θ,in ) − ∇θ Q (ξ) ξ=ξ(θ,0)
                h                         i0                                                      
                 ∂∇θ Q(ξ(θ))                                                                     2
           =         ∂ξ           ξ=ξ(θ,0) (ξ (θ, in ) − ξ (θ, 0)) + O kξ (θ, in ) − ξ (θ, 0)k     .

                                              L(θ)                                                ∂Q(ξ)
Because kξ (θ, in ) − ξ (θ, 0)k ≤           1−L(θ) in   by Theorem 1, and assuming both          ∂ξ   ξ=ξ(θ,0)
        ∂∇θ Q(ξ(θ))
and         ∂ξ      ξ=ξ(θ,0)      are bounded, we obtain

                                                                                     
                                                                           L(θ)
                                  |Q (ξ (θ, in )) − Q (ξ (θ, 0))| = O            in
                                                                         1 − L(θ)
                                                                                     
                                                                           L(θ)
               ∇θ Q (ξ (θ)) ξ=ξ(θ,in ) − ∇θ Q (ξ (θ)) ξ=ξ(θ,0)    = O            in .
                                                                         1 − L(θ)

A.2      Proof of Theorem 4
We define θ̂(in ) to be the numerically incorrect estimates with the inner loop tolerance in ,

                                       θ̂(in ) = arg max {Q (ξ (θ, in ))} .
                                                               θ

Because ∇θ Q (ξ)      ξ=ξ (θ̂(in ),in )   = 0, the application of the second result in Theorem 3 at θ̂(in )
gives
                                                                        
                                                                                      
                                                                      L θ̂(in )
                             ∇θ Q (ξ)        ξ=ξ(θ̂(in ),0)   = O               in  .                  (15)
                                                                    1 − L θ̂(in )

Note that we have evaluated the GMM objective function with no numerical error at the
point θ̂(in ) that minimizes the GMM objective function with inner loop numerical error.

                                                                   35
     Let θ̃ be any value of the structural parameters near θ̂(in ). By first the inverse triangle
inequality, then the regular triangle inequality, and then finally a Taylor series expansion, we
have

       ∇θ Q (ξ (θ))   ξ=ξ (θ̃,in )    − ∇θ Q (ξ (θ))       ξ=ξ (θ̂(in ),0)

 ≤     ∇θ Q (ξ (θ))   ξ=ξ (θ̃,in )   − ∇θ Q (ξ (θ))     ξ=ξ (θ̂(in ),0)

 =     ∇θ Q (ξ (θ))   ξ=ξ (θ̃,in )   − ∇θ Q (ξ (θ))     ξ=ξ(θ̃,0)    + ∇θ Q (ξ (θ))   ξ=ξ(θ̃,0)   − ∇θ Q (ξ (θ))   ξ=ξ (θ̂(in ),0)



 ≤     ∇θ Q (ξ (θ))   ξ=ξ (θ̃,in )   − ∇θ Q (ξ (θ))     ξ=ξ (θ̃,0)

   + ∇θ Q (ξ (θ)) ξ=ξ(θ̃,0) − ∇θ Q (ξ (θ)) ξ=ξ(θ̂(in ),0)
                                                                                 
                                                                              2
      L(θ̃)           2
 ≤ O 1−L(θ̃) in + ∇θ Q (ξ (θ)) ξ=ξ(θ̂(in ),0) θ̃ − θ̂(in ) + O θ̃ − θ̂(in )     .

                                                                                                                                   
                                                                                                                                2
As we have assumed        ∇2θ Q (ξ (θ)) ξ=ξ(θ̂(in ),0)       is bounded, the second order term O               θ̃ − θ̂(in )
term can be ignored. By rearranging the above inequality, we obtain

                      ∇θ Q (ξ (θ))       ξ=ξ (θ̃,in )                        
                                                                       L(θ̃)
                                                                               
               ≤  ∇θ Q (ξ (θ)) ξ=ξ(θ̂(in ),0) + O 1−L θ̃ in + O θ̃ − θ̂(in )
                                                     ()
                                               (   )
                                            L θ̃
                                                                     
               = O L(θ̂(in )) in + O 1−L θ̃ in + O θ̃ − θ̂(in )
                     1−L(θ̂(in ))
                                              ()
               = O (in ) + O θ̃ − θ̂(in ) ,

where the first equality uses (15).

A.3     Proof of Theorem 5
We define θ∗ to be the true estimate when there are no inner loop numerical errors (in = 0),
i.e., θ∗ = arg max {Q (ξ (θ, 0))} . First, we can quantify the bias between the numerically
               θ                                                   
correct and incorrect objective function values, Q ξ(θ̂ (in ) , in and Q (ξ(θ∗ , 0)). By two
Taylor series expansions, we have




                                                            36
                         
        Q ξ θ̂ (in ) , in − Q (ξ(θ∗ , 0))
                                                                       
      = Q ξ θ̂ (in ) , in − Q ξ θ̂ (in ) , 0 + Q ξ θ̂ (in ) , 0 − Q (ξ(θ∗ , 0))
        h                            i0                                    
      = ∇ξ Q (ξ (θ)) ξ=ξ(θ̂(in ),0)      ξ(θ̂(in ), in ) − ξ(θ̂(in ), 0)
                                                    
                                                   2
        +O ξ(θ̂(in ), in ) − ξ(θ̂(in ), 0)
                                                                                        
                                              0                                     2
                    0                                         ∗                     ∗
        + (∇θ ξ (θ)) ∇ξ Q (ξ) ξ=ξ(θ∗ ,0)           θ̂(in ) − θ + O θ̂(in ) − θ
        h                        i0                                    
      = ∇ξ Q (ξ) ξ=ξ(θ̂(in ),0)      ξ(θ̂(in ), in ) − ξ(θ̂(in ), 0)
                                                                              
                                                   2                           2
        +O ξ(θ̂(in ), in ) − ξ(θ̂(in ), 0)           + O θ̂(in ) − θ     ∗     ,


because ∇ξ(θ∗ )0 ∇ξ Q (ξ(θ∗ )) = 0 at the true estimates θ∗ .                                                                               
                                                                                                                                       2
                                                                 ∗
   Rearranging the equality involving Q ξ(θ̂(in ), in ) −Q (ξ(θ , 0)) to focus on the O θ̂(in ) − θ∗
term, we have
                                                                
                                                             2
                                                         ∗
                              O           θ̂(in ) − θ
                                          
                      = Q ξ(θ̂(in ), in ) − Q (ξ(θ∗ , 0))
                         h                         i0                                    
                        − ∇ξ Q (ξ) ξ=ξ(θ̂(in ),0)      ξ(θ̂(in ), in ) − ξ(θ̂(in ), 0)
                                                                   
                                                                  2
                        −O ξ(θ̂(in ), in ) − ξ(θ̂(in ), 0)
                                           
                      ≤ Q ξ(θ̂(in ), in ) − Q (ξ(θ∗ , 0))
                                                                                         
                        + ∇ξ Q (ξ) ξ=ξ(θ̂(in ),0)      ξ θ̂ (in ) , in − ξ θ̂(in ), 0
                                                                   
                                                                  2
                        −O ξ(θ̂(in ), in ) − ξ(θ̂(in ), 0)         .

                                                                                                                                                        
                                                                                                                                                     2
Because we assume ∇ξ Q (ξ)                    ξ=ξ(θ̂(in ),0)        is bounded, the second-order term O        ξ(θ̂(in ), in ) − ξ(θ̂(in ), 0)
can be ignored. This allows us to focus on the numerical error from the NFP algorithm’s inner
loop and the bias in objective values. From Theorem 1, we also know ξ(θ̂(in ), in ) − ξ(θ̂(in ), 0) ≤
 L(θ̂(in ))
              .   Hence, we obtain
1−L(θ̂(in )) in


                                                                                                                    !
                              ∗
                                  2                                   
                                                                                  ∗             L(θ̂(in ))
       O       θ̂(in ) − θ               ≤ Q ξ(θ̂(in ), in ) − Q (ξ(θ , 0)) + O                              in       .
                                                                                              1 − L(θ̂(in ))



                                                                        37
A.4        Proof of Theorem 6
The NFP method (4) solves the following unconstrained problem

                                                 minθ Q (ξ (θ)) .                              (16)

The first-order condition of (16) is

                                         ∂Q (ξ (θ))   dξ 0 ∂Q
                                                    =         = 0.                             (17)
                                            ∂θ        dθ ∂ξ

The constrained optimization formulation of (16) is

                                             min Q (ξ)
                                             (θ,ξ)                                             (18)
                                                 s.t. s(ξ; θ) = S.

The Lagrangian for (18) is L (θ, ξ, λ) = Q(ξ) + λT (S − s(ξ; θ)), where λ is the vector of
Lagrange multipliers. The first-order conditions of (18) are

                                  ∂L (θ, ξ, λ)              ds(ξ; θ) 0
                                                   = −                 λ=0
                                      ∂θ                      dθ
                                  ∂L (θ, ξ, λ)           ∂Q ds(ξ; θ) 0                         (19)
                                                   =        −          λ=0
                                      ∂ξ                 ∂ξ   dξ
                                  ∂L (θ, ξ, λ)
                                                   = S − s(ξ; θ) = 0.
                                      ∂λ
                                                                                        0
Because the NFP inner loop is a contraction mapping, the matrix ds(ξ;θ)      dξ     is invertible.19
                                                                        0  −1
Solving the second set of first order conditions for λ gives λ = ds(ξ;θ)
                                                                   dξ
                                                                                ∂Q
                                                                                ∂ξ . Then

                                                                        −1
                                      ds(ξ; θ) 0           ds(ξ; θ) 0
                                                       
                                ∂L                                            ∂Q
                                   =−                                            = 0,          (20)
                                ∂θ      dθ                   dξ               ∂ξ

which is identical to (17), the first-order condition from the NFP formulation. To see the
equivalence, note that the implicit function theorem (Theorem M.E.1 in Mas-Collel, Whinston
and Green 1995) states
                                                                 −1
                                    ∂ξ (θ)             ds(ξ; θ)          ds(ξ; θ)
                                           =−                                     ,
                                     ∂θ                  dξ                dθ
so by substitution, the two FOC’s are identical.
  19
       We thank Ken Judd and John Birge for pointing out this property.




                                                           38
B    Gradients for the MPEC Objective Function and Constraints
Here we derive the gradients of the MPEC objective function and constraints with respect
to the optimization parameters in MPEC. These gradients are an important input, for both
numerical accuracy and speed. Nevo (2000b) lists the gradients for NFP. This section uses
the independent normal distribution for each of the random coefficients, as in BLP (1995)
and many other empirical papers.
    Market Share

                                  exp(x0j,t β−ᾱpj,t +ξj,t + k x0k,j,t νk σβk −pj,t νK+1 σα )
                                                            P
                           R
          sj (ξt ; θ) =          PJ                                                               dF              (ν)
                               1+ i=1 exp(x0i,t β−ᾱpi,t +ξi,t + k x0k,i,t νk σβk −pi,t νK+1 σα )
                                                                P
                                                          R
                     =                                        Tj (ξt , ν; θ)dF (ν)

                          where θ = (β, α, σβ , σα )0 , and ν ∼ N (0, IK+1 ).

    MPEC Criterion Function

                                                  min g (ξ)0 W g (ξ)
                                                   θ,ξ
                                            subject to s(ξ; θ) = S,

                                                                     T
                                                                   1X 0
                                             where g(ξ) =              ξt zt .
                                                                   T
                                                                        t=1

    Gradients for MPEC
                                   Z
                ∂sj (ξt ; θ)                                            X
                             =            Tj (ξt , ν; θ)(xj,k,t −                Ti (ξt , ν; θ)xk,i,t )dF (ν)
                  ∂βk                                                       i
                                   Z
                ∂sj (ξt ; θ)                                            X
                             =            Tj (ξt , ν; θ)(pj,k,t −                Ti (ξt , ν; θ)pk,i,t )dF (ν)
                   ∂α
                                                                            i
                                   Z
               ∂sj (ξt ; θ)                                         X
                            =           Tj (ξt , ν; θ)(xj,k,t −                 Ti (ξt , ν; θ)xk,i,t )νk dF (ν)
                 ∂σβk
                                                                        i
                               Z
             ∂sj (ξt ; θ)                                          X
                          =            Tj (ξt , ν; θ)(pj,k,t −              Ti (ξt , ν; θ)pk,i,t )νK+1 dF (ν)
               ∂σα
                                                                    i
                                              Z
                       ∂sj (ξt ; θ)
                                    =             Tj (ξt , ν; θ)(1 − Tj (ξt , ν; θ))dF (ν)
                         ∂ξj,t
                                                  Z
                          ∂sj (ξt ; θ)
                                       =−             Tj (ξt , ν; θ))Ti (ξt , ν; θ))dF (ν)
                            ∂ξi,t




                                                              39
                                     ∂g(ξ)0 W g(ξ)            ∂g(ξ)
                                                   = 2g(ξ)0 W
                                          ∂ξ                   ∂ξ

C     Extension: Maximum Likelihood Estimation
In this section, we outline how a researcher would adapt MPEC to a likelihood-based esti-
mation of random-coefficients-logit demand. Some researchers prefer to work with likelihood-
based estimators and, more specifically, with Bayesian MCMC estimators (Yang et al 2003
and Jiang et al. 2008) based on the joint density of observed prices and market shares.20
Besides efficiency advantages, the ability to evaluate the likelihood of the data could be use-
ful for testing purposes. The trade-off relative to GMM is the need for additional modeling
structure which, if incorrect, could lead to biased parameter estimates. Like GMM, the cal-
culation of the density of market shares still requires inverting the system of market share
equations. Once again, MPEC can be used to circumvent the need for inverting the shares,
thereby offsetting a layer of computational complexity and a potential source of numerical
error. Below we outline the estimation of a limited information approach that models the
data-generating process for prices in a “reduced form” (this motivation is informal as we do
not specify a supply-side model and solve for a reduced form). However, one can easily adapt
the estimator to accommodate a structural (full-information) approach that models the data-
generating process for supply-side variables, namely prices, as the outcome of an equilibrium
to a game of imperfect competition (assuming the equilibrium exists and is unique).
    Recall that the system of market shares is defined in (2). We assume, as in a triangular
system, that the data-generating process for prices is

                                                      0
                                              pj,t = zj,t γ + ηj,t ,                                   (21)

where zj,t is a vector of price-shifting variables and ηj,t is a mean-zero, i.i.d. shock. To
capture the potential endogeneity in prices, we assume the supply and demand  "      shocks
                                                                                          # have
                                                                                  2
                                                                                 σξ σξ,η
the following joint distribution: (ξj,t , ηj,t )0 ≡ uj,t ∼ N (0, Ω) where Ω =               . Let
                                                                                σξ,η ση2
     σ
ρ = σξξ,η
       ση .
    The system defined by equations (2) and (21) has the joint density function

                   fs,p (st , pt ; Θ) = fξ|η (st | xt , pt ; θ, Ω) |Jξ→s | fη (pt | zt ; γ, Ω) ,
  20
     One can also think of Jiang et al. (2008) as an alternative algorithm for finding the parameters. The
MCMC approach is a stochastic search algorithm that might perform well if the BLP model produces many
local optima because MCMC will not be as likely to get stuck on a local flat region. Because our goal is not
to study the role of multiple local minima, we do not explore the properties of a Bayesian MCMC algorithm.




                                                       40
                                           
where Θ =          θ, γ, σξ2 , σξ,η , ση2       is the vector of model parameters, fξ|η (·|·) is the marginal
density of ξ conditional on η, fη (·|·) is a Gaussian density with variance ση2 , and Jξ→s is the
Jacobian matrix corresponding to the transformation of variables of ξj,t to shares. The density
of ξj,t conditional on ηj,t is
                                                                                                               2
                                                                                                         σ
                                                  J
                                                  Y             1            1 ξj,t −                 ρ σηξ ηj,t 
             fξ|η (st | xt , pt ; θ, Ω) =               √               exp −                                     .
                                                                               2 σξ2 (1 − ρ2 )
                                                                p
                                                  j=1       2πσξ 1 − ρ2


Note that the evaluation of ξj,t requires inverting the market share equations, (2).
    The element Jj,k in row l and column k of the Jacobian matrix, Jξ→s , is
                                                                        
           R               exp(β 0 +x0j,t β x −β p pj,t +ξj,t )                  exp(β 0 +x0j,t β x −β p pj,t +ξj,t )
            β 1−                                                                                                          dFβ   (β; θ) , j = l
         
                         PJ                                                    PJ
                        1+ k=1 exp(β 0 +x0k,t β x −β p pk,t +ξk,t )           1+ k=1 exp(β 0 +x0k,t β x −β p pk,t +ξk,t )
         
         
         
         
         
Jj,l =                                                                                                                                           .
         
         
                                                                 exp(β 0 +x0l,t β x −β p pl,t +ξl,t )
         
                  R     exp(β 0 +x0j,t β x −β p pj,t +ξj,t )
              −                                                                                                                       , j 6= l
         
                    β 1+ J exp(β 0 +x0 β x −β p pk,t +ξk,t ) 1+ J exp(β 0 +x0 β x −β p pk,t +ξk,t ) dFβ                   (β; θ)
         
                       P                                      P
                          k=1              k,t                  k=1               k,t



    Standard maximum likelihood estimation would involve searching for parameters, ΘLISML ,
that maximize the following log-likelihood function

                                                          T
                                                          X
                                                l (Θ) =           log (fs,p (st , pt ; Θ)) .
                                                            t=1

This would consist of a nested inner loop to compute the demand shocks, ξj,t , via numerical
inversion (the NFP contraction mapping).
    The equivalent MPEC approach entails searching for the vector of parameters (Θ, ξ) that
maximizes the constrained optimization problem
                                        PT                                                                        
             max l (Θ, ξ) =                     t=1 log     fξ|η (st | xt , pt ; θ, Ω) |Jξ→s | fη (pt | zt ; γ, Ω)
               subject to               s(ξ; θ) = S.


D        Chebyshev Approximation of the Expected Value of Waiting
                                                                                       0
First, we bound the range of prices as follows, p = (p1 , p2 ) ∈ [0, b] × [0, b], where b is large (b
is 1.5 times the largest observed price in the data). We then approximate the expected value
                                                                                                   0
of delaying adoption with Chebyshev polynomials, v0r (p; θr ) ≈ γ r Λ(p), where γ r is a K × 1
vector of parameters and Λ (p) is a K × 1 vector of K Chebyshev polynomials. Therefore, we




                                                                     41
can rewrite the Bellman equation as
                                                                             
                 Z                         
   0                             0
                                               X
 γ r Λ (p) = δ       log exp γ r Λ (pρ + ψ) +   exp βjr − αr p0 ρj + ψ + ξj  dFψ,ξ (ψ, ξ) .
                                                                           

                                                     j


To solve for the Chebyshev weights, we use the Galerkin method described in Judd (1992).
We define the residual function
                     0
 R (p; γ) = γ r Λ (p) − . . .                                            !
                               0      P                                             .
              δ log exp γ r Λ (pρ + ψ) + exp βjr − αr (p0 ρj + ψ) + ξj
                R
                                                                           dFψ,ξ (ψ, ξ)
                                                          j


                                                                                           (22)

Next, we let X be the matrix of K Chebyshev polynomials at each of the G points on our
grid (i.e. G nodes). Our goal is to search for parameters, γ, that set the following expression
to zero:
                                              X 0 R (p; γ) = 0.

We use an iterated least squares approach for NFP.
                                                              0
   1. Pick a starting value γ r,0 , v0r,0 (p; Θr ) = γ r,0 ρ (p)

   2. Compute
                                                                                       
                         Z                           
                                           0
                                                         X
       Y p; γ r,0 = δ        log exp γ r,0 Λ (pρ + ψ) +   exp βjr − αr p0 ρj + ψ + ξj  dFψ,ξ (ψ, ξ)
                                                                                    

                                                                   j


       using quadrature

   3. solve the least squares problem: minR(p; γ)0 R(p; γ) or
                                               γ

                                                           0
                                   min Xγ r − Y p; γ r,0          Xγ r − Y p; γ r,0
                                                                                      
                                    γ


           • for which the solution is: γ r,1 = (X 0 X)−1 X 0 Y p; γ r,0 .
                                                                        

                                        0
   4. Compute v0r,1 (p; Θr ) = γ r,1 Λ (p)

   5. Repeat steps 2 and 3 until convergence.




                                                     42
E       Jacobian of the Density of (pt , St ) in the Dynamic BLP model
The Jacobian is defined as follows:
                                                          "               #
                                                              ∂ψt   ∂ψt
                                                              ∂pt   ∂St
                                               Jt,u→Y =       ∂ξt   ∂ξt       .
                                                              ∂pt   ∂St

           ∂ψt                     ∂ψt
Since   ∂ log(pt )   = IJ and   ∂ log(p   = 0J      (a square matrix of zeros), we only need to compute
                                   h t )i
                                     ∂ξt
the matrix of        derivatives, ∂St . We         can simplify this calculation by applying the implicit
function theorem to the following system

                                          G (St , ξt ) = s (p, ξt ; θ) − St = 0

and computing the lower block of the Jacobian as
                                                        h i−1 h i
                                                           ∂G  ∂G
                                              Jt,ξ→S = − ∂ξ  t ∂St
                                                       h i−1       ,
                                                        ∂s
                                                     = ∂ξ t


                                      ∂sj,t
where the (j, k) element of           ∂ξk,t   is

                                P
                                  λr,t sj (pt , ξt ; θr ) (1 − sj (pt , ξt ; θr )) , if j = k
                     ∂Sj,t     
                                 r
                             =
                     ∂ξk,t     
                                  − λr,t sj (p, ξt ; θr ) sk (p, ξt ; θr )
                                     P
                                                                                    , otherwise.
                                          r


F       Monte Carlo: Varying the Quality of the Data
In principle, the quality of the data could influence the convexity of the objective function
and, hence, the trajectory of the outer loop search. To assess the role of “data quality,” we
construct a set of sampling experiments that manipulate the power of the instruments (i.e.
the correlation between the prices, p, and the instruments, z). Let zj,t,d = ũj,t,d + νuj,t ,
where uj,t is a random shock that also affects price and ν is a measure of the power of the
instruments. Higher ν’s result in more powerful instruments. By generating the prices before
the instruments, we ensure that prices, shares, and product characteristics are unaffected
by the power of the instruments. Thus, the NFP inner loop and the MPEC market share
constraints are identical when ν varies. Only the instruments in the moment conditions vary.
We use the identity matrix for the GMM weighting matrix to avoid the instrument power
affecting the choice of weighting matrix.
    Table 10 lists the results from five runs with differing levels of instrument power. The table
lists the value of ν and the resulting R2 from a regression of price on all excluded-from-demand


                                                           43
and non-excluded instruments. We see that R2 decreases as the power of the instruments
decreases. In all specifications, MPEC is faster than NFP. MPEC’s speed decreases with
instrument power, although the decrease from 118 to 159 seconds is not large compared to
the total run time of NFP, which ranges from 342 to 619 seconds. We have no theoretical
explanation for the pattern relating NFP’s speed and instrument power. To some extent, the
pattern is driven by the fact that NFP encounters convergence problems as we increase the
power of the instruments. When ν = 0.5, only 50% (10 out of 20) of the replications fail to
converge for all five starting values, for NFP. Naturally, this convergence problem could be a
practical concern if the researcher mistakenly interprets the point at which the solver stops
as a local minimum even though the gradient-based solver is unable to detect convergence.




                                             44
Table 1: Three NFP Implementations: Varying Starting Values for One synthetic Dataset,
with Numerical Derivatives
                                                           NFP          NFP        NFP      Truth
                                                       Loose            Loose      Tight
                                                       Inner            Both
                     Fraction Convergence                  0.0           0.54      0.95
                     Frac.< 1% > “Global” Min.             0.0           0.0       1.00
                     Mean Own Price Elasticity             -7.24        -7.49      -5.77      -5.68
                     Std. Dev. Own Price Elasticity        5.48          5.55       ~0
                     Lowest Objective                  0.0176           0.0198    0.0169
                     Elasticity for Lowest Obj.          -5.76   -5.73     -5.77    -5.68
We use 100 starting values for one synthetic dataset. The NFP loose inner loop implementation has in = 10−4
and out = 10−6 . The NFP loose-both implementation has in = 10−4 and out = 10−2 . The NFP-tight
implementation has in = 10−14 and out = 10−6 . We use numerical derivatives using KNITRO’s built-in
procedures.


Table 2: Three NFP Implementations: Varying Starting Values for Nevo’s Cereal Dataset,
with Closed-Form Derivatives
                                                              NFP                  NFP            NFP      NFP Tight
                                                           Loose Inner           Loose Both       Tight     Simplex
  Fraction Reported Convergence                                   0.0              0.76           1.00       1.00
  Frac. Obj. Fun. < 1% Greater than “Global” Min.                 0.0               0.0           1.00        0.0
  Mean Own Price Elasticity Across All Runs                   -3.82                -3.69          -7.43      -3.84
  Std. Dev. Own Price Elasticity Across All Runs                  0.4              0.07               ~0     0.35
  Lowest Objective Function Value                            0.00213              0.00683       0.00202     0.00683
 Elasticity for Run with Lowest Obj. Value               -6.71        -3.78        -7.43      -3.76
We use the same 50 starting values for each implementation. The NFP loose inner loop implementation has
in = 10−4 and out = 10−6 . The NFP loose both implementation has in = 10−4 and out = 10−2 . The
NFP tight implementation has in = 10−14 and out = 10−6 . The Nelder-Meade or simplex method uses a
tighter inner loop tolerance of in = 10−14 and MATLAB’s default values for the simplex convergence criteria.
We manually code closed-form derivatives for all methods other than for Nelder-Meade, which does not use
derivative information.




                                                      45
                             Table 3: Lipschitz Constants for the NFP Algorithm
                    Parameter                Std. Dev. of                        # of            Mean of Intercept
                                                                                                     E βi0
                                                                                                        ˆ ˜
                      Scale                       Shocks ξ                 Markets T
              Altered        Mean         Altered       Mean           Altered          Mean     Altered     Mean
              Value         Lipschitz     Value       Lipschitz        Value       Lipschitz     Value      Lipschitz
               0.01           0.985         0.1         0.808             25            0.860      -2         0.771
                0.1           0.971        0.25         0.813             50            0.871      -1         0.871
               0.50           0.887         0.5         0.832           100             0.888      0          0.936
               0.75           0.865         1           0.871           200             0.888      1          0.971
                1             0.871         2           0.934                                      2          0.988
                1.5           0.911         5           0.972                                      3          0.996
                2             0.938         20          0.984                                      4          0.998
                3             0.970
                5             0.993




                      Table 4: Monte Carlo Results Varying the Lipschitz Constant
  Intercept     Lipschitz       Implementation         Runs Converged            CPU Time (s)               Elasticities            Outside
   E βi0
      ˆ ˜
                Constant                                     (fraction)                             Bias     RMSE          Value    Share
    -1.9            0.789             NFP tight                 1                       1012.9     -0.200      0.265       -12.00    0.900
                                       MPEC                     1                       981.0      -0.200      0.265       -12.00    0.900
    -0.9            0.858             NFP tight                 1                       1365.9     -0.203      0.266       -11.98    0.845
                                       MPEC                     1                       1015.2     -0.203      0.266       -11.98    0.845
     0.1            0.913             NFP tight                 1                       1608.4     -0.205      0.266       -11.97    0.775
                                       MPEC                     1                       1001.4     -0.205      0.266       -11.97    0.775
     1.1            0.952             NFP tight                 1                       2057.7     -0.201      0.256       -11.96    0.687
                                       MPEC                     1                       832.4      -0.201      0.256       -11.96    0.687
     2.1            0.976             NFP tight                 1                       2544.8     -0.202      0.256       -11.95    0.583
                                       MPEC                     1                       810.2      -0.199      0.254       -11.96    0.583
     3.1            0.989             NFP tight                 1                       3730.3     -0.195      0.252       -11.97    0.472
                                       MPEC                     1                       767.5      -0.202      0.254       -11.96    0.472

There are 20 replications for each experiment. Each replication uses five starting values to do a better job
at finding a global minimum. The NFP-tight implementation has in = 10−14 and out = 10−6 . There is no
inner loop in MPEC; out = 10−6 and feasible = 10−6 . The same 100 simulation draws are used to generate
the data and to estimate the model.




                                                                  46
                  Table 5: Monte Carlo Results Varying the Number of Markets
  # Markets      Lipschitz     Implementation            Runs Converged          CPU Time (s)                Elasticities            Outside
      T          Constant                                   (fraction)                                Bias    RMSE          Value    Share
      25          0.903           NFP tight                    1                    1372.9          -0.265      0.385       -12.16    0.640
                                     MPEC                      1                    555.2           -0.269      0.389       -12.16    0.640
      50          0.952           NFP tight                    1                    2060.6          -0.201      0.256       -11.96    0.687
                                     MPEC                      1                    839.0           -0.201      0.256       -11.96    0.687
     100          0.956           NFP tight                    1                    8068.2          -0.092      0.174       -12.30    0.893
                                     MPEC                      1                    2143.6          -0.106      0.225       -12.29    0.893

There are 20 replications for each experiment. Each replication uses five starting values to do a better job
at finding a global minimum . The NFP-tight implementation has in = 10−14 and out = 10−6 . There is no
inner loop in MPEC; out = 10−6 and feasible = 10−6 . The same 100 simulation draws are used to generate
the data and to estimate the model.




           Table 6: Last iterations of runs using the Nevo dataset, for one starting value
                                         NFP                               MPEC
                             Iteration    Optimality       Iteration     Optimality     Feasibility
                                               Error                       Error            Error
                               115        3.5 ×   10−4        19         3.1 ×   10−1   8.6 × 10−2
                               116        3.9 ×   10−5        20         3.0 ×   10−2   5.6 × 10−3
                               117        3.8 × 10−4          21         5.4 × 10−3     3.0 × 10−4
                               118        2.9 ×   10−5        22         3.3 ×   10−5   7.4 × 10−6
                               119        3.9 × 10−5          23         1.6 × 10−8     9.1 × 10−8
                               120        5.9 ×   10−6
                               121        1.8 × 10−5
                               122        1.7 × 10−6
                               123        9.7 × 10−7




                                                              47
Table 7: Monte Carlo Results for Dynamic BLP with One Consumer Type for δ = 0.96: NFP
versus MPEC


                                                                       MPEC                 NFP
 Speeds                                                             335.55 secs.        553.50 secs.
 Parameters                                                        Mean     RMSE       Mean     RMSE       Truth
 Utility intercept product 1                                      3.9557    0.1780    3.9556    0.1780    4.0000
 Utility intercept product 2                                      2.9572    0.2015    2.9572    0.2015    3.0000
 Utility price coefficient, type 1                                -1.0030   0.0101    -1.0030   0.0101    -1.0000
 Price, product 1, constant                                       0.2111    0.0345    0.2111    0.0345    0.2000
 Price, product 1, lagged price of product 1                      0.7962    0.0136    0.7962    0.0136    0.8000
 Price, product 1, lagged price of product 2                      0.0026    0.0098    0.0026    0.0098    0.0000
 Price, product 2, constant                                       0.2071    0.0378    0.2071    0.0378    0.2000
 Price, product 2, lagged price of product 1                      0.0037    0.0168    0.0037    0.0168    0.0000
 Price, product 2, lagged price of product 2                      0.7935    0.0156    0.7935    0.0156    0.8000
 Demand shocks, Cholesky variance term                            0.9958    0.0173    0.9958    0.0173    1.0000
 Covariance btw supply and demand, Cholesky variance term         0.5015    0.0215    0.5015    0.0215    0.5000
 Supply shocks, Cholesky variance term                            0.8647    0.0152    0.8647    0.0152    0.8660
 % of replications routine reports convergence                          100%                100%

There are 20 replications for each of MPEC and NFP. The same synthetic data are used for both MPEC
and NFP. Each replication uses five starting values to do a better job at finding a global minimum. The
NFP implementation has ξin = 10−14 , Vin = 10−14 and out = 10−6 . There is no inner loop in MPEC;
out = 10−6 and feasible = 10−6 . The data have T = 50 periods and M = 20 distinct markets. Each
market has two competing products. The Chebyshev regression approximation to the value function uses a
fourth-order polynomial and five interpolation nodes. The numerical integration of future states uses Gauss-
Hermite quadrature with three nodes. NFP uses numerical derivatives, as coding the derivatives of dynamic
BLP is infeasible for many problems and it is not clear automatic differentiation works with nested inner
loops. MPEC uses automatic differentiation in the form of the package MAD. The percentage of replications
where the routine reports convergence is the fraction of the 20 replication where the lowest objective function
coincided with an exit flag of 0 from KNITRO.




                                                      48
Table 8: Monte Carlo Results for Dynamic BLP with One Consumer Type for δ = 0.99: NFP
versus MPEC


                                                                       MPEC                 NFP
 Speeds                                                             671.49 secs.        1295.50 secs.
 Parameters                                                        Mean     RMSE       Mean     RMSE       Truth
 Utility intercept product 1                                      4.0684    0.7235    3.3907    1.7473    4.0000
 Utility intercept product 2                                      3.0692    0.7316    2.3913    1.7844    3.0000
 Utility price coefficient, type 1                                -0.9885   0.0380    -0.9987   0.0152    -1.0000
 Price, product 1, constant                                       0.1929    0.0682    0.2171    0.0655    0.2000
 Price, product 1, lagged price of product 1                      0.8170    0.0532    0.8022    0.0546    0.8000
 Price, product 1, lagged price of product 2                      -0.0044   0.0295    0.0000    0.0520    0.0000
 Price, product 2, constant                                       0.1770    0.1102    0.2058    0.0813    0.2000
 Price, product 2, lagged price of product 1                      -0.0195   0.0557    -0.0065   0.0436    0.0000
 Price, product 2, lagged price of product 2                      0.8330    0.0860    0.8089    0.0585    0.8000
 Demand shocks, Cholesky variance term                            1.0139    0.0468    1.0053    0.0334    1.0000
 Covariance btw supply and demand, Cholesky variance term         0.4985    0.0255    0.5050    0.0219    0.5000
 Supply shocks, Cholesky variance term                            0.8652    0.0152    0.8640    0.0159    0.8660
 % of replications routine reports convergence                          100%                90%

There are 20 replications for each of MPEC and NFP. The same synthetic data are used for both MPEC
and NFP. Each replication uses five starting values to do a better job at finding a global minimum. The
NFP implementation has ξin = 10−14 , Vin = 10−14 and out = 10−6 . There is no inner loop in MPEC;
out = 10−6 and feasible = 10−6 . The data have T = 50 periods and M = 20 distinct markets. Each
market has two competing products. The Chebyshev regression approximation to the value function uses a
fourth-order polynomial and four interpolation nodes. The numerical integration of future states uses Gauss-
Hermite quadrature with three nodes. NFP uses numerical derivatives, as coding the derivatives of dynamic
BLP is infeasible for many problems and it is not clear automatic differentiation works with nested inner
loops. MPEC uses automatic differentiation in the form of the package MAD. The percentage of replications
where the routine reports convergence is the fraction of the 20 replication where the lowest objective function
coincided with an exit flag of 0 from KNITRO.




                                                      49
Table 9: Monte Carlo Results for Dynamic BLP with Two Consumer Types and δ = 0.90
MPEC Only

                                                                             MPEC
         Speeds                                                            9397 secs.
         Parameters                                                      Mean     RMSE      Truth
         Utility intercept product 1                                    3.6604    0.5719    4.0000
         Utility intercept product 2                                    2.6980    0.5287    3.0000
         Utility price coefficient, type 1                              -1.0159   0.0299   -1.0000
         Utility price coefficient, type 2                              -2.0369   0.2623   -2.0000
         Frequency, type 1                                              0.7907    0.1016    0.7000
         Price, product 1, constant                                     0.1894    0.0818    0.2000
         Price, product 1, lagged price of product 1                    0.7919    0.0333    0.8000
         Price, product 1, lagged price of product 2                    -0.0013   0.0274    0.0000
         Price, product 2, constant                                     0.2283    0.0929    0.2000
         Price, product 2, lagged price of product 1                    -0.0013   0.0262    0.0000
         Price, product 2, lagged price of product 2                    0.7919    0.0341    0.8000
         Demand shocks, Cholesky variance term                          0.9001    0.4386    1.0000
         Covariance btw supply and demand, Cholesky variance term       0.4537    0.4609    0.5000
         Supply shocks, Cholesky variance term                          0.6891    0.5616    0.8660
         % of replications routine reports convergence                        100%

There are 20 replications. Each replication uses two starting values to do a better job at finding a global
minimum. There is no inner loop in MPEC; out = 10−6 and feasible = 10−6 . The data have T = 50 periods and
M = 5 distinct markets. Each market has two competing products. The Chebyshev regression approximation
to the value function uses a fourth-order polynomial and four interpolation nodes. The numerical integration
of future states uses Gauss-Hermite quadrature with three nodes. The code uses automatic differentiation in
the form of the package MAD.




                                                         50
    Table 10: Monte Carlo Results Varying the Data Quality: The Power of Instruments
    Instrument    R2     Implementation     Runs Converged     CPU Time (s)              Elasticities
    Power (ν)                                  (fraction)                        Bias      RMSE         Value
        1/2       0.75      NFP tight             0.50              619          0.028      0.173       -8.16
                             MPEC                  1                118          0.023      0.173       -8.16
        1/4       0.69      NFP tight              1                342          0.058      0.140       -8.16
                             MPEC                  1                129          0.058      0.140       -8.16
        1/6       0.62      NFP tight              1                447         -0.020      0.158       -8.15
                             MPEC                  1                135         -0.020      0.158       -8.15
        1/8       0.57      NFP tight              1                376         -0.022      0.186       -8.13
                             MPEC                  1                135         -0.022      0.186       -8.13
       1/16       0.46      NFP tight              1                512         -0.111      0.312       -8.07
                              MPEC               1                 159         -0.090    0.323    -8.05
There are 20 replications for each experiment. Each replication uses ten starting values to ensure a global
minimum is found. The NFP-tight implementation has in = 10−14 and out = 10−6 . There is no inner loop
in MPEC; out = 10−6 and feasible = 10−6 . The same 100 simulation draws are used to generate the data
and to estimate the model. The column R2 reports the (mean across replications) R2 from the regression of
price on all instruments, treating each product in each market as a separate observation. The meaning of ν is
described in the text. These simulations were run on a different computer than the earlier simulations.




                                                       51
