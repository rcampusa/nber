                                NBER WORKING PAPER SERIES




    BEHAVIORAL IMPLICATIONS OF RATIONAL INATTENTION WITH SHANNON
                               ENTROPY

                                            Andrew Caplin
                                             Mark Dean

                                        Working Paper 19318
                                http://www.nber.org/papers/w19318


                      NATIONAL BUREAU OF ECONOMIC RESEARCH
                               1050 Massachusetts Avenue
                                 Cambridge, MA 02138
                                     August 2013




We thank Dirk Bergemann, Xavier Gabaix, Sen Geng, Andrei Gomberg, John Leahy, Daniel Martin,
Filip Matejka, Alisdair McKay, Stephen Morris, and Michael Woodford for their constructive contributions.
The views expressed herein are those of the authors and do not necessarily reflect the views of the
National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2013 by Andrew Caplin and Mark Dean. All rights reserved. Short sections of text, not to exceed
two paragraphs, may be quoted without explicit permission provided that full credit, including © notice,
is given to the source.
Behavioral Implications of Rational Inattention with Shannon Entropy
Andrew Caplin and Mark Dean
NBER Working Paper No. 19318
August 2013
JEL No. D83

                                             ABSTRACT

The model of rational inattention with Shannon mutual information costs is increasingly ubiquitous.
We introduce a new solution method that lays bare the general behavioral properties of this model
and liberates development of alternative models. We experimentally test a key behavioral property
characterizing the elasticity of choice mistakes with respect to attentional incentives. We find that
subjects are less responsive to such changes than the model implies. We introduce generalized entropy
cost functions that better match this feature of the data and that retain key simplifying features of the
Shannon model.


Andrew Caplin
Department of Economics
New York University
19 W. 4th Street, 6th Floor
New York, NY 10012
and NBER
andrew.caplin@nyu.edu

Mark Dean
Department of Economics
New York University
19 W. 4th Street, 6th Floor
New York, NY 10012
mark.dean@nyu.edu
1       Introduction

The model of rational inattention with Shannon mutual information costs
(henceforth the Shannon model) is increasingly ubiquitous. While the pi-
oneering contributions of Sims [1998, 2003] considered its implications for
macroeconomic dynamics, the ensuing period has seen applications to such
diverse subjects as stochastic choice (Matejka and McKay [2011, 2013]), in-
vestment decisions (e.g van Nieuwerburgh and Veldkamp [2008]), global games
(Yang [2011]), and pricing decisions (Mackowiak and Wiederholt [2009], Mate-
jka [2010] Martin [2013]). Yet despite its growing importance, little of a general
nature is known about the behaviors the Shannon model produces. Even solv-
ing the model can be challenging absent additional restrictions on the signal
space and/or the utility function (Sims [2006]).

    We introduce a new approach to solving the Shannon model that lays bare
its behavioral properties and liberates development of alternative models. Our
“posterior-based” approach explicitly models the decision maker’s choice of
signals, as characterized by the resulting posterior distribution over states of
the world. The resulting necessary and su¢ cient conditions for rationality
identify behavioral patterns that characterize the Shannon model.1

    We establish two de…ning behavioral properties of the Shannon model. The
…rst property relates to changes in incentives. We show that the ratio of the
di¤erence in utilities across chosen acts to the log di¤erence in posterior beliefs
is constant across states and decision problems. This “invariant likelihood
ratio”(ILR) property pins down the rate at which choice mistakes respond to
changes in the cost of these mistakes. The second property relates to changes
in prior beliefs. We show that posterior beliefs are invariant to local changes
in prior beliefs - the “locally invariant posteriors”(LIP) property.
    1
     In general, rational inattention problems have been approached by considering the prob-
lem of directly choosing the optimal joint distribution of state and chosen act. One exception
is Matejka and McKay [2011], who consider the choice of posteriors but do not use the net
utility approach. The net utility approach has been used in a related setting by Kamenica
and Gentzkow [2011].



                                              2
   We experimentally test the ILR property characterizing the elasticity of
choice mistakes with respect to attentional incentives. We …nd that subjects
are less responsive to such changes than the model implies. We introduce
generalized entropy cost functions that better match this feature of the data
while retaining key simplifying features of the Shannon model.

    In addition to the two key invariants, the posterior-based approach enables
us to analyze many other properties of the Shannon model. We show that an
optimal strategy exists involving no more acts being taken than there are
states of the world, that collections of chosen acts relate uniquely to posterior
beliefs, and that an envelope condition characterizes dependence of the value of
the optimal strategy on model parameters. We identify also conditions under
which the model has a unique solution.

   We adapt the design of Caplin and Dean [2013] (henceforth CD13) to
conduct our experimental test of the ILR property. Subjects are shown a
display with a number of red and blue balls, with the true state determined by
the number of red balls. They are then asked to choose between acts with state
dependent payo¤s. We observe how subjects’patterns of choice vary with the
value of choosing the correct act. We compare this experimental variation to
that predicted by the Shannon model, as embodied in the ILR condition. We
…nd that subjects are generally less responsive to changes in incentives than
the Shannon model implies. To accommodate our …ndings, we generalize the
model to a broader class of “posterior-separable”attention cost functions. We
show that this class allows a better …t to the experimental data, while retaining
key simplifying features of the Shannon model.

    Section 2 introduces the posterior-based approach and uses it to solve the
Shannon model. Section 3 derives the two behavioral invariants, with other
theoretical results in section 4. Section 5 introduces our experimental design
and results. Section 6 introduces posterior-separable models. Section 7 con-
cludes. Our approach is of particular value in dynamic settings in which beliefs
evolve due to the interaction between attentional e¤ort and exogenous shocks.
It is equally of value in strategic settings (e.g. Martin [2013]).

                                       3
2         The Posterior-Based Approach

2.1        The Decision Problem

A decision making environment comprises possible states of the world         =
                                                                  f
f1; ; m; ; M g and a set F of acts with state dependent payo¤s Um    for f 2 F
and m 2 . We de…ne = ( ) as the set of probability distributions over
states, with m indicating the probability of state m given 2 . The state of
the world is assumed to be knowable in principle, but obtaining (or processing)
information is taken to be costly.

    A decision problem consists of a prior distribution over these states of
the world and the subset of acts from which the decision maker (DM) must
choose.2 We reserve the special notation 2 for prior beliefs, with         the
corresponding set of possible states of the world. To ensure that maximization
problems have a solution, we assume that the set of feasible utility vectors is
closed and bounded above.


De…nition 1 A decision problem comprises a pair ( ; A) 2       F, where
F     2F =; comprises all non-empty sets A F such that fU f 2 RM jf 2 Ag
is closed and bounded above.


2.2        Attention Strategies and Shannon Costs

Prior to choosing an act, the DM must choose an attention strategy detailing
their method of learning about the state of the world. As is standard in the
rational inattention literature, we model the DM as choosing a …xed attention
strategy for each decision problem that (stochastically) maps states of the
world to a set of signals. Since we will be characterizing an expected utility
maximizing agent, we identify these signals with the posteriors 2 which
they produce. For simplicity, we consider only attention strategies with …nitely
    2
        Both of which are assumed known to the DM.

                                            4
many possible posteriors. Since we are assuming a …nite state space, this
restriction is immaterial in the current context, as we discuss in section 4.2.

    An attention strategy is identi…ed with a function : ! ( ) that speci-
…es the probability of receiving each signal in each state of the world.3 Feasible
attention strategies are constrained to satisfy Bayes’law. Given 2 ; any
corresponding attention strategy :        ! ( ) has (…nite) image ( )
and must be such that, for all m 2       and 2 ( ),

                                                m m(         )
                                      m   =                          ,
                                              X
                                              M

                                                    j   j(       )
                                              j=1


where m ( )      (m)(f g) is the probability of receiving the signal that gives
posterior beliefs in objective state m.

   We assume that information is costly, with costs measured in the same
expected utility units in which the prizes are measured. Furthermore, we
assume that the cost of an attentional strategy is linearly related to Shannon’s
measure of the mutual information between 2 and ,
                                                    0                                       1
                          X X
                            M
                                                                           m(      )
               I( ; ) =               m m(     ) ln @            hP m                      iA :
                                                                         M
                          2 ( ) m=1                      m               m=1   m       m( )


The Shannon mutual information cost function has been heavily used in the
rational inattention literature since it was popularized by Sims [1998]. Such
use has been justi…ed both axiomatically and through links to optimal coding
in information theory (see Sims [2010] and Matejka and McKay [2013] for
discussions).
  3
      We use    ( ) to denote the set of all simple probability distributions on :




                                                5
2.3    The Posterior-Based Approach and Net Utility

The Shannon model assumes that a rationally inattentive DM selects an at-
tention strategy which maximizes expected utility from the subsequent choice
of acts net of mutual information attention costs. The most commonly used
approach to solving such models is to focus directly on the DM’s probability
of choosing each act f 2 A in each state m 2 (see for example Sims [2006],
Matejka and McKay [2013]). The attention strategy can then be mechanically
computed from the joint distribution of states and acts. In our approach this
procedure is reversed. We solve the model by focusing directly on choice of
posterior beliefs, deriving the state dependent choice probabilities from this
solution.

    To develop the posterior-based method, we …rst reformulate the optimiza-
tion problem as in Matejka and McKay [2011]. A feature of the Shannon cost
function is that strictly more informative strategies are strictly more atten-
tionally expensive than less informative such strategies. Hence optimization is
inconsistent with choice of the same act at two distinct posteriors. This im-
plies that, for purposes of optimization, an attention strategy can be speci…ed
as a subset of available acts B A chosen with strictly positive unconditional
probabilities P f > 0, and corresponding act-speci…c posteriors, f 2 . The
posteriors and probabilities must satisfy Bayes’law with respect to the prior
beliefs.

De…nition 2 Given ( ; A) 2      F, a (posterior-based) attention strategy
           ( ;A)
(B; P; ) 2       comprises a …nite set B   A, probability weights P : B !
          X
              f
R++ with    P = 1, and posteriors : B ! such that,
          f 2B

                                      X
                                  =          Pf   f
                                                      :                    (1)
                                      f 2B



   A standard result is that the mutual information of two random variables
X and Y can be written as the di¤erence between the Shannon entropy of X,

                                        6
and the expected Shannon entropy of X conditional on Y .4 Hence the Shannon
attention cost function with parameter > 0 is de…ned on (B; P; ) 2 ( ;A)
by,                              "                      #
                                          X
                  S (B; P; ) =     H( )       P f H( f ) ;               (2)
                                                                   f 2B
                 PM
where H( ) =       j=1 j ln j is the Shannon entropy function extended to
boundary points using the limit condition lim &0 ln = 0.

    Specifying information costs this way allows us to write the DM’s optimiza-
tion problem in a particularly convenient way. As the gross bene…t associated
                                                 P         P
with an attention strategy (B; P; ) is given by f 2B P f M        f  f
                                                              j=1 m Um , the ob-
jective function of the DM facing decision problem ( ; A) can be de…ned in
terms of the “net utility”associated with each posterior belief/act pair,

                       X              X
                                      M                                       X
    ( ;A)                         f                 f  f
N           (B; P; )          P                     m Um    S k (B; P; ) =           P fNf(   f
                                                                                                  )   H( );
                       f 2B               j=1                                 f 2B
                                                                                                         (3)
where N f :       ! R is the net utility of act f ,

                                                      X
                                                      M
                                      f     f               f  f          f
                              N (               )           m Um   + H(       ):
                                                      j=1


Each posterior belief/act pair has a net utility associated with it, which is equal
to the expected utility of using that act at that posterior minus the information
cost associated with that posterior. Since the term H( ) is independent of the
attention strategy, we can ignore it, and characterize the DM as maximizing
the weighted average of act-speci…c net utilities.

   It is convenient to treat the posterior probability of state M as the residual,
                                            P 1
and consider X = f( 1 ; :: M 1 ) 2 RM +
                                         1
                                           j Mm=1 m      1g as the domain of the
  4
      See for example Cover and Thomas [2006] p. 20.




                                                            7
net utility functions.5 Having done so, a simple geometric construction illus-
trates computation of net utilities. Figure 1 illustrates for a decision problem
with M = 2, A = ff; gg, U1f = U2g = ln(1 + e), U2f = U1g = 0, and = 1. The
horizontal axis represents the probability 1 2 [0; 1] of state 1.




          Figure 1: Net Utility Functions                                        Figure 2: Optimal Posteriors



    Figure 1 enables the value of all attention strategies to be visualized. For
any given prior 1 2 [0; 1], a feasible attention strategy in which both acts are
taken with positive probability corresponds to posteriors f1 ; g1 that contain
the prior interior to their convex hull. Assuming that acts are chosen optimally
given posterior beliefs this requires f1 > 1 and g1 < 1 . Given two such
posteriors, the utility of the corresponding attention strategy is the weighted
average of the net utility associated with each posterior, with the expectation
taken using the act-speci…c probabilities. Given the choice of posteriors, the
act speci…c probabilities are pinned down by condition 1. Geometrically, this
means that the value of an attention strategy consisting of posteriors f1 ; g1
  5
      Hence,
                               M
                                                                  !                                          !
           f      f
                               X1                   M
                                                    X1                 f         f      f
                                                                                                   M
                                                                                                   X1
      f                              f  f
  N (      1 ; :: M   1)   =         m Um   +   1             m       UM   + H   1 ; :: M   1; 1         m       :
                               j=1                  m=1                                            m=1




                                                          8
is equal to the height of the chord connecting N f ( f1 );                f
                                                                          1    and N f ( g1 );   g
                                                                                                 1
as it goes over the prior, as demonstrated in Figure 1.


2.4      Solving the Shannon Model

In this section we characterize rationally inattentive behavior for the Shannon
model.

                                                                              ( ;A)
De…nition 3 Given ( ; A) 2      F, strategy (B; P; ) 2                                is rationally
inattentive if,
                  N ( ;A) (B; P; ) N ( ;A) (B 0 ; P 0 ; 0 );

all (B 0 ; P 0 ; 0 ) 2   ( ;A)
                                 , with ^ (   ;A)
                                                    the corresponding set of rationally inatten-
tive strategies.

    The geometric approach to computing the payo¤ to attention strategies
suggests that rationally inattentive strategies are de…ned by the posteriors
and acts whose associated net utility functions support the highest chord above
the prior. In order to identify such posteriors, one can concavify the upper
envelope of the net utility functions by …nding the minimal concave function
that majorizes them all, a construction familiar from the work of Kamenica and
Gentzkow [2011] and others (see section 2.4.1 for further details). The optimal
attention strategy for any prior is given by the posteriors that support this
concavi…ed net utility function above that prior.

    Figure 2 illustrates this concavi…cation operation for our running example
in the case where 1 = 0:5. The shaded region is the lower epigraph of the
concavi…ed version of the maximum net utility function. This epigraph is a
closed, convex set that is bounded above in the net value coordinate. This im-
plies that a rationally inattentive strategy has associated with it a hyperplane
in RM which supports it (also illustrated in …gure 2). The net utility functions
associated with all acts lie weakly below this hyperplane, while those of cho-
sen acts touch the hyperplane at their associated posterior beliefs. Lemmas

                                                       9
1 and 2 in the appendix demonstrate that both of these results generalize to
arbitrary decision problems.

    The Shannon mutual information cost function has many features which
make the hyperplane characterization particularly powerful. First, it is dif-
ferentiable. Second, it e¤ectively rules out corner solutions (as the marginal
cost of information goes to in…nity as a posterior belief approaches 0). These
two conditions enable us to provide a derivative-based characterization of the
supporting hyperplane (lemma 3 in the appendix). Finally, its functional form
provides easily tractable tangency conditions. Theorem 1 uses these features
to characterize rationally inattentive behavior in terms of suitably transformed
utility parameters,
                                           Uf
                                  f
                                  m    exp[ m ]:

                                                                   ( ;A)
Theorem 1 Given ( ; A) 2          F and          > 0, (B; P; ) 2           is rationally
inattentive if and only if:

  1. Invariant Likelihood Ratio (ILR) Equations for Chosen Acts:
     given f; g 2 B, and 1 m M ,
                                        f         g
                                        m         m
                                        f
                                             =    g :
                                        m         m


  2. Likelihood Ratio Inequalities for Unchosen Acts: given f 2 B
     and g 2 AnB,
                            X
                            M     f
                                  m    g
                                  f    m 1:
                                  m=1    m


Proof. Appendix.

    The …rst of these properties derives from the fact that the net utility func-
tions of the chosen acts must support the same hyperplane at their associated
posteriors. The second derives from the fact that the net utility functions of
all unchosen acts must lie weakly below this hyperplane.

                                        10
    These equations can, in many cases, be used directly to solve for the op-
timal attention strategy. For example, the main behavioral equation in Yang
[2011] (equation 10) corresponds to the analogous condition for continuous
state spaces. The equations also highlight a number of important general
properties of such solutions, as we discuss in sections 3 and 4.


2.4.1   Comparison to Other Approaches

The most commonly used approach to solving models of rational attention
is to directly focus on the decision maker’s state dependent stochastic choice
function (see for example Sims [2003], [2006]). This method takes as the control
variable the DM’s probability of choosing each act f 2 A in state m 2 , with
information costs based on the mutual information between the distribution
of chosen acts and prior distribution over states of the world. While this
formulation is equivalent to ours, there are insights into behavior that are
easier to identify based on choice of posterior. We discuss these insights in the
next two sections.

   Another approach, taken by Matejka and McKay [2011] is to focus on
choice of posterior beliefs, but to attack the problem directly using the Karush-
Kuhn-Tucker (KKT) conditions, rather than using concavi…ed net utility and
the separating hyperplane theorem. While this approach results in somewhat
similar conditions, it turns out that, because the problem is not convex, the
KKT conditions are necessary, but not su¢ cient for optimality (we provide a
demonstration of the lack of su¢ ciency in the online appendix).

    The approach we take shares many technical similarities with that taken
by Kamenica and Gentkow [2011] (henceforth KG) to solve their model of
optimal persuasion. KG consider a situation in which a sender chooses a signal
structure with which to convey information about the true state of the world to
a receiver, who then chooses an action to take. In essence, the sender chooses
a set of posterior beliefs in order to maximize their expected “net utility”, just
as they do in our formulation of the rational inattention model. The di¤erence

                                       11
is that, while in the rational inattention model the DM is constrained by the
cost of information, in the KG model they are constrained by the fact that
the receiver will choose an action at each posterior in order to maximize their
own utility rather than that of the sender. The key formal connection is that
the same concavi…cation operation is used in both cases to identify optimal
strategies. This suggests that certain of the insights and techniques we develop
may be of value in models of optimal persuasion.



3     Two Behavioral Invariants

Theorem 1 highlights two important behavioral regularities associated with
the Shannon model. These go far beyond the conditions that characterize the
general class of rationally inattentive models (CD13).6 They involve invariance
properties of the model solution to changes in the underlying decision problem,
in particular the incentives for attention and prior beliefs.


3.1     Invariant Likelihood Ratio and the Cost Elasticity
        of Mistakes

The ILR condition of theorem 1 embodies an invariance condition with respect
to changes in the state dependent utilities associated with chosen acts. In any
decision problem, for any two acts that are chosen with positive probability
in some state, the ratio of the relative utility of those two acts in that state
to the log ratio of the posterior probability of that state when each act is
chosen is …xed, and equal to the cost of information. Thus, as the di¤erence in
utilities between two acts in some state changes, so the relative probabilities
of choosing those acts must also change in a constrained way.
   6
     A “No Improving Action Switch (NIAS)” condition which ensures that chosen acts are
optimal at each posterior and a “No Improving Attention Cycle (NIAC)” condition which
ensures that choice of attention strategy can be rationalized by some cost function.




                                          12
    We illustrate the power of this property by demonstrating how it pins down
the response of choice “mistakes” to changes in incentives. From the point
of view of a fully informed observer, a rationally inattentive DM may make
mistakes: they will sometimes choose an action which is suboptimal given
the true state of the world (though is optimal given their posterior beliefs).
The ILR condition speci…es how the probability of such mistakes responds
to changes in incentives. To illustrate, consider symmetric decision problems
with two states, two acts ff; gg and = (0:5; 0:5) such that,

                                U1f = U2g = U2f + c = U1g + c;

with c > 0. The superior choice is therefore f in state 1 and g in state 2,
while c parameterizes the cost of making the wrong choice. Direct application
of the ILR equation for chosen acts shows that the probability of choosing the
correct action in each state is related to the cost of a mistake in that state in
a simple manner,

                                            U1f
                       f             exp                          1                 g
          1 (f )   =   1   =                             =              c       =   2   =   2 (g):
                                     U1f           U1g       1 + exp(       )
                               exp         + exp

This enables us to compute the elasticity of errors with respect to the cost of
mistakes as,
                                          c
                       @ 1 (f ) c           exp c
                                       =              :
                         @c     1 (f )   1 + exp( c )
We test this property explicitly in section 5


3.2    Locally Invariant Posteriors

We now consider how optimal attention strategies respond to changes in prior
beliefs - an issue of interest in dynamic and in strategic applications of the
Shannon model [see for example Martin [2013]]. Figure 2 suggests a powerful
invariance condition - that optimal posterior distributions are locally invariant


                                                    13
to changes in prior beliefs. As per lemma 2, we identify the optimal behavior
for prior 1 = 0:5 by identifying the posterior beliefs that support that tangent
hyperplane to the lower epigraph of the concavi…ed net utility function above
that prior. However …gure 2 makes clear these posteriors also support the
tangent hyperplane above all priors in the range [ 11 ; 21 ]. It follows that the
optimal strategy for priors in this range uses the same posterior beliefs as for
  1 = 0:5.

    This highlights a general result: if a set of acts B and act speci…c posteriors
  f
     f 2B
          form the basis for an optimal strategy for some decision problem
( ; A), then they also form the basis for an optimal strategy for any decision
problem ( A) from which it is feasible to use these posteriors - that is for
                                                                            f
every prior belief that is in the convex hull of the posterior beliefs        f 2B
                                                                                   .
This and all other corollaries are proved in the online appendix.


Corollary 1 (Locally Invariant Posteriors - LIP): If (B; P; ) 2 ^ ( ;A)
    and (C; Q; ) 2 ( ;A) with C   B satisfying f = f all f 2 C, then
    (C; Q; ) 2 ^ ( ;A) .


    Corollary 1 has important implications for solving rational inattention
models: the solution to one decision problem identi…es a solution to many
related problems. It also has comparative static implications for how the un-
conditional probability of choosing a given act must change with local changes
in the prior belief. With posterior beliefs unchanged, the unconditional proba-
bilities of choosing each act must change mechanically in order to ensure that
Bayes’ rule is obeyed. Consider for example the two act, two state, case in
…gure 2 with prior 1 2 ( f1 ; g1 ). In this case we can use Bayes’ rule to ex-
plicitly solve for P f ( 1 ) as a function of 1; f1 ; and g1 , and establish that the
local response of P f to changes in prior belief depends only on the di¤erence
   f      g
   1      1 ,
                                       g
                                 1     1        @P f         1
                    P f ( 1) =   f     g
                                           =)        =   f       g
                                                                     :
                                 1     1
                                                @ 1      1       1



                                           14
    Thus, once posterior beliefs have been observed, the response of act choice
probabilities to local changes in the prior can be calculated without recourse
to any further details of the model.



4     Further Implications

We highlight additional features of the Shannon model that are revealed by
the posterior-based approach which are of use for understanding the Shannon
model.


4.1    The Envelope Condition

Our approach allows us to use the envelope condition to characterize the e¤ect
of changing prior beliefs on expected utility. Given A 2 F, de…ne V A :   !R
to be the maximal value obtainable for decision problem ( ; A),

                     V A( ) =       max            N(     ;A)
                                                                (B; P; ):
                                (B;P; )2   ( ;A)



It is a direct corollary of theorem 1 that the value function is di¤erentiable and
that a version of the standard envelope theorem characterizes local changes in
value with respect to changes in prior beliefs.


Corollary 2 (Envelope Condition): Given ( ; A) 2       F such that m >
    0, the value function V A : ! R is di¤erentiable at and has contin-
    uous partial derivatives,

                                 @V A ( )   @N f f
                                          =     (^ );
                                  @ m       @ m

                ^ some (B;
      where f 2 B       ^ P^ ; ^ ) 2 ^ (        ;A)
                                                      .




                                           15
4.2    States Bound Acts

Theorem 1 implies that an optimal attention strategy exists that uses no more
acts than there are states of the world. Intuitively, the optimal strategy for a
given decision problem can be found by identifying acts whose net utility func-
tions touch the hyperplane that supports the concavi…ed net utility function
above the prior. With M states of the world, Charateodory’s theorem implies
that any such hyperplane is de…ned by any M points it contains. For example,
in the two dimensional case of …gure 2, the supporting hyperplane is a line, as
de…ned by any pair of its points. This in turn implies that the hyperplane can
be supported by and M act/posterior pairs, which in turn form an optimal
strategy.


Corollary 3 (States Bound Acts - SBA): Given ( ; A) 2                  F, there
    exists a rationally inattentive strategy with jBj M .


4.3    Unique Posteriors

Consider decision problems which share the same prior, have di¤erent available
acts, and yet for which the same acts are optimally chosen. In this case we
show in corollary 4 that the posteriors associated with all chosen acts will be
identical. In two decision problems with the same prior in which the subjects
make use of the same acts, they will have the same posteriors.


Corollary 4 (Unique Posteriors): If (B; P; ) 2 ^ (         ;A)
                                                                 and (B; Q; ) 2
    ^ ( ;C) , then (f ) = (f ) all f 2 B.


    The Unique Posteriors property tells us that there can be at most one
possible set of posteriors that satis…es the ILR conditions for a given prior and
a given set of chosen acts.



                                       16
4.4    Uniqueness

One natural question is whether the Shannon model always makes unique
behavioral predictions. The answer is no, yet we can provide conditions under
which uniqueness is guaranteed. Consider our running example with A =
ff; gg, = 1, U1f = U2g = ln(1 + e), and U2f = U1g = 0. Note that,

                            f       g                  f       g
                            1   =   2   = 1 + e;       2   =   1   = 1:

By the necessity aspect of theorem 1, both acts can be chosen only if the ILR
equation is satis…ed, which in this case uniquely pins down the posteriors:

                        f               f    f    1+e 1
                                = (     1;   2)   =( ;    );
                                                  2+e 2+e
                                                   1 1+e
                        g
                                = ( g1 ; g2 ) = (    ;    );
                                                  2+e 2+e

as already indicated in …gure 2.

    Now consider adding a third act h 2 F with U1h = U2h = ln(1+e)    2
                                                                         , so that
  h    h    2+e
  1 = 2 = 2 . Note that the ILR equations are satis…ed for acts f; g; h; at the
corresponding posteriors f ; g , and h = (0:5; 0:5). By the su¢ ciency aspect
of theorem 1, choosing all three acts with equal probability at these posteriors
identi…es an optimal policy for prior ( 1 ; 2 ) = (0:5; 0:5), as does choosing act
h for sure.

   An independence condition rules examples of this kind.

                                                  f
Axiom 1 (A¢ ne Independence) f                         2 Rm jf 2 Ag is a¢ nely indepen-
dent.


    By de…nition f f 2 Rm jf 2 Ag is a¢ nely independent if one cannot …nd
                                    P                 P
scalars f , not all zero, such that f 2A f = 0 and f 2A f fm =0. A¢ ne
independence rules out having M + 1 transformed utility vectors in any hyper-
plane and ensures that there is one and only one optimal attention strategy.


                                              17
Theorem 2: With a¢ ne independence, ^ (       ;A)
                                                    = 1 all   2 .


    Theorem 2 has strong implications for data derived from more than one
decision problem. By theorem 1, observation of behavior in a single decision
problem enables the cost parameter to be identi…ed, provided two or more
distinct acts are chosen. Theorem 2 implies that this uniquely pins down
observed behavior in all other decision problems provided the independence
condition is satis…ed.


4.5    Global Comparative Statics

One of the major di¢ culties in solving rational inattention models is identify-
ing which acts will be chosen with positive probability as part of the optimal
strategy. As theorem 1 demonstrates, once this is known, it is relatively easy
to solve for the associated optimal posteriors and unconditional probabilities
associated with each act.

    The posterior-based approach o¤ers help in this regard by identifying, for
any set A 2 F, all subsets of acts that can possibly be chosen together as part
of an optimal strategy for some prior beliefs. Because optimal strategies can
be characterized by a supporting hyperplane that touches the lower epigraph
of the concavi…ed net utility function at the net utility functions of the used
acts, for any collection of acts to be used together there must be a tangent
hyperplane that is supported by the net utility functions of those acts.




                                      18
     Figure 3: An example with 3 acts                           Figure 4: The Dual Cone



    Figure 3 illustrates this concept in the case of two states of the world and
three acts h1 (which pays 2 in state 1 and 0 otherwise), h2 (which pays 2 in
state 2 and 0 otherwise) and h3 (which pays 1.5 in both states). This makes
it clear that there are …ve classes of solution to this problem, depending on
prior beliefs.

           Table 1
           Prior       1   Acts Chosen Slope of hyperplane Region
                                                           2   2)
             [0; 21 ]          fh2 g           (1; @N@ (            ]    A
                                          @N 2 ( 2 )       @N 3 ( 3a )
            [ 21 ; 3a
                    1 ]      fh2 ; h3 g      @
                                                       =   @
                                                                         B
                                           @N 3 ( 3a ) @N 3 ( 3b )
              3a
           [ 1 ; 3b  1 ]       fh3 g      [ @         ; @          ]     C
                                          @N 3 ( 3b )   @N 1 ( 1 )
            [ 1 ; 11 ]
               3b
                             fh1 ; h3 g      @
                                                      = @                D
                                             @N 1 ( 1 )
             [ 11 ; 1]         fh1 g        [ @ ; 1)                     E


   Theorem 1 provides a general method for identifying these regions. and
associated acts. Consider an arbitrary strictly positive vector 2 RM
                                                                   + , and




                                          19
de…ne B( ) as all acts that maximize the corresponding dot product,

                                       f         g
                   B( ) = ff 2 Aj :          :       all g 2 Ag:

Set B( ) identi…es acts whose net utility functions would support a hyperplane
with slope . With 2 RM    + and B( ) identi…ed, the Unique Posterior property
implies that there are unique corresponding posteriors f f 2B( ) that satisfy
the ILR property. By the su¢ ciency condition of theorem 1, these posteriors
form the basis for an optimal attention strategy for any decision problem
( ; A) for which these posteriors are feasible. The related unconditional choice
probabilities P f are then determined by Bayes’law.

    By identifying optimal acts for all vectors , one can in this manner char-
acterize rationally inattentive policies for all priors 2 . Identi…cation of the
mapping from normal vectors to maximizers of the corresponding dot product
at extreme points of a convex set is a well-studied problem. It is equivalent to
…nding the dual cone associated with extreme points of this convex set de…ned
by f g jg 2 Ag(Rockafellar [1972]). Figure 4 illustrates this for the three act
case. Table 1 indicates the link between the regions of the dual cone and the
set of priors for which the supporting acts form an optimal solution.



5    An Experimental Test of ILR

In this section we present an experiment that allows us to observe subjects’
attentional responses to changing incentives, which we compare to the pre-
dictions of the Shannon model. To perform these tests we generate “state
dependent stochastic choice data” (see CD13). For a given decision problem
( ; A) 2      F, we estimate the probability of choosing each act in each state
of the world - i.e. a mapping q :       ! (A). This identi…es not only all
chosen acts, but also how unconditionally likely each such act is to be cho-
sen, and the corresponding revealed posteriors. Together, these constitute the
revealed posterior-based attention strategy.

                                      20
5.1    Experimental Design

We use the method of CD13 to generate state dependent stochastic choice
data. In a typical round of the experiment, a subject is shown a screen on
which there are displayed 100 balls. Some of the balls are red with the re-
mainder blue. The state of the world is identi…ed by the precise number of
balls that are red as opposed to blue. Prior to observing the screen, subjects
are informed of the probability distribution over such states. Having seen the
screen they choose from a number of di¤erent acts whose payo¤s are state
dependent. A decision problem is de…ned by this prior information and the
set of available acts, as in section 2.1. Each subject faces each speci…c decision
problem 50 times, allowing us to approximate their state dependent stochas-
tic choice function. In a given session, each subject faced 4 distinct decision
problems. All occurrences of the same problem were grouped, with the order
of the problem block-randomized. At the end of the experiment, one question
was selected at random for payment, which reward was added to the show up
fee of $10.


5.2    Description of Experiments and Theoretical Predic-
       tions

                         Table 2: Experimental Design
                         Decision Problem      Payo¤s
                                 1                 2
                                 2                10
                                 3                20
                                 4                30

    Our experiment involves 2 treatments. In both treatments there are two
equiprobable states and two acts f (x) and g(x), with f (x) paying o¤ x in state
1 (and zero otherwise) and g(x) paying x in state 2 (and zero otherwise). The
value x varies between decision problems, as in table 2. The di¤erence between

                                       21
the two treatments rests in the di¢ culty of the underlying perceptual task. In
one case (treatment 1), it is relatively easy to discriminate, with states 1 and
2 involving 47 and 53 red balls respectively, while in the other (treatment 2) it
is harder, with 49 and 51 red balls respectively. Overall, the experiment allows
us to estimate how mistakes (i.e. the probability of choosing the lower payo¤
act) vary with rewards. The Shannon model makes strong predictions in this
regard, summarized by the ILR condition. Rearranging the …rst condition of
theorem 1 gives,

                           U (x)                           U (x)
                          f (x)       g(x)
                                             =            g(x)       f (x)
                                                                               =
                    ln(   1           1 )        ln(      2          2     )

where U (x) is the expected utility of monetary prize x. Assuming that the cost
of attention does not vary within a treatment, then neither should the ratio
of the di¤erence in utilities between prizes to the log di¤erences in posterior
beliefs. Further assuming that costs are higher in treatment 2 than treatment
1, this ratio should be higher in decision problems in the former than the
latter. This leads to the following hypothesis.


Hypothesis: (ILR) Given any x; y 2 f2; 10; 20; 30g in the same treatment,

                                    U (x)                          U (y)
                                  f (x)      g(x)
                                                      =          g(y)          f (y)
                                                                                           :
                           ln(    1          1 )          ln(    2             2     )

                                    f (x)              g(x)                    f (x)           g(x)
      Given x 2 f2; 10; 20; 30g,          ;                      and                   ;              observed in
      treatments 1 and 2 respectively,

                                   U (x)                           U (x)
                                  f (x)      g(x)
                                                      <          f (x)         f gx)
                                                                                           :
                          ln(     1          1 )          ln(    1             1     )

    In order to test this hypothesis it is necessary to observe the utilities asso-
ciated with each prize x. We assume initially that utility is linear in money,
before controlling for subject-speci…c utility curvature.



                                                 22
5.3        Results

41 subjects took part in treatment 1, while 46 took part in treatment 2. Figure
5 shows the aggregate probability of correct choice for each decision problem in
both treatments. As expected, the probability of choosing correctly is higher in
the easier experiment 1, and is increasing in the rewards for making the correct
choice in both experiments. In order to compare these expansion paths to those
                                                                     U (x)
predicted by the Shannon model, we can calculate the ratio        f (x)    g(x) and
                                                                         ln   1   ln   1
       U (x)
   g(x)       for each value of x in treatments 1 and 2, using the aggregate
               f (x)
ln(2           2     )
data and assuming U is the identity function. If the aggregate data can
be explained by the Shannon model, then this ratio should equal to the cost
parameter and be invariant within each treatment. Assuming that treatment
2 is harder than 1, we would expect estimated costs to be higher in the former
than the latter. Figure 6 plots this ratio for aggregate data, and associated
standard errors.7




                         Figure 5: Probability of choosing the correct act as
                                          a function of x

   7
     For all analysis in this paper, standard errors are calculated taking into account clus-
tering at the subject level.




                                                 23
           Figure 6a: Treatment 1                               Figure 6b: Treatment 2



    The key observation is that estimated costs appear signi…cantly higher
for higher reward levels, meaning that subjects do not increase attention in
response to increasing rewards as much as the Shannon model predicts. For
treatment 1, the estimated cost in state 1 is signi…cantly di¤erent between
all reward levels.8 For treatment 2, estimated cost is signi…cantly di¤erent
between all reward levels apart from between $20 and $30.9

    One possible explanation for deviations between our measured behavior
and the predictions of the Shannon model is curvature of the utility function.
In order to control for this, subjects also made choices between lotteries in
the manner of the multiple price list task of Holt and Laury [2002].10 This
allows us to estimate subject-speci…c utility functions which can be used to
   8
     At the 0.1% level between the $2 and $10 rewards and $10 and $20 rewards, and at the
5% level between the $20 and $30 rewards.
   9
     In many cases there are also signi…cant di¤erences (at the 5% level) in estimated costs
between states at the same reward amount. This is true for the $10 and $20 rewards in
treatment 1 and the $2, $10 and $20 rewards in treatment 2. This …nding violates the
predictions of the Shannon model, but is driven by the fact that subjects who are completely
inattentive tend to choose option f rather than option g, meaning they are less accurate in
state 2 than in state 1.
  10
     Subjects had to make 10 choices between p$6:00+(1 p)$4:80 and p$11:55+(1 p)$0:30
for p = 0:1 to p = 1. Their responses are then used to estimate a CRRA utility function



                                            24
convert prizes from monetary amounts to utility amounts. We use these data
to estimate subject-speci…c utility functions which can be used to convert
prizes from monetary amounts to utility amounts.11

    In order to control for the e¤ect of risk aversion, we use subject-speci…c
data. We focus on subjects who (a) exhibited consistency in the risk aversion
questions (i.e. obeyed stochastic dominance) (b) were never inattentive and
(c) choose the correct act given their posterior beliefs (i.e. never violate the No
Improving Action Switches condition from CD13). The …rst condition means
that we have a well de…ned estimate of the subject’s utility function, while the
latter two mean that we can obtain a precise estimate of their costs from each
decision problem. These conditions leave us with 28 subjects in treatment 1
and 22 in treatment 2. For each of these subjects, we estimate their costs us-
ing data from state 1 in each decision problem, and then test for statistically
di¤erent costs across decision problems.12 We do this both assuming linear
utility, and using our subject-speci…c estimated utility function. In the former
case, 34%, (or 17 of 50) of subjects across the two experiments exhibit sig-
ni…cantly (at the 10% level) increased cost estimates between the $2 and $30
reward case. Controlling for risk aversion this number drops to 26% (13 of 50),
with 2 subjects (4%) exhibiting a signi…cant decrease in costs. We conclude
that a signi…cant fraction of subjects are less reactive to changes in rewards
than the Shannon model would predict, even controlling for risk aversion.
  11
     Note that, because only 1 in 200 questions is rewarded, the true value of choosing the
correct act for any x is,
                                       1          1
                                         U (x) +     U (0):
                                     200         199
Normalizing u(0) to 0, note that hypothesis ILR still holds.
  12
     In some cases, our subjects were perfectly accurate at some reward level. The Shannon
model predicts that subjects will never choose to be perfectly discriminatory for positive
costs. In such cases, we test the probability of observing perfect accuracy in our sample in
decision problem x given the costs estimated in decision problem y, and reject the Shannon
model if this probability is less that 10%




                                            25
6    Separable Models of Rational Inattention

Our experimental results suggests that there may be value to considering ra-
tional inattention models that allow for di¤erent responses to incentives than
does the Shannon model. To that end, we introduce a family of attention
cost functions that maintain much of the structure of the Shannon model, but
allows for di¤erent response elasticities. These cost functions maintain the
form of equation 2, which enables us to apply many of our same posterior-
based methods. However, we allow the cost of a posterior distribution to di¤er
from the negative of its entropy. We call this the posterior-separable class of
attention cost functions.


De…nition 4 A Strictly convex function G : RM + ! R 2 G generates a
posterior-separable attention cost function KG : ( ;A) ! R if, for all
( ; A) 2    F,
                                              X
                    KG (B; P; ) =    G( ) +        P ( )G( ):
                                              2B



   Note that the Shannon mutual information function …ts into this class with
G( ) =      H( ). Note also that requiring strict convexity ensures that strictly
more Blackwell informative signals always involve strictly higher costs. We use
the notation NGf ( f ) to denote the net utility function associated with act f
                                      ( ;A)
when the cost function is G. We let ^ G denote the corresponding rationally
inattentive strategies.

    Because posterior-separable cost functions are structurally similar to Shan-
non mutual information costs, many of the results of the paper apply equally
to this class. Importantly, lemmas 1 and 2 hold, so that rationally inattentive
behavior can be characterized by a supporting hyperplane to the lower epi-
graph of the concavi…ed net utility function. For di¤erentiable cases, lemma
3 also holds, which carries with it the ability to solve the model using deriv-
ative conditions. This in turn implies that corollaries 1, 3, and 4 hold for all

                                       26
posterior-separable cost functions, while corollary 2 holds for all such functions
which are di¤erentiable.

    We illustrate how the posterior-separable family allow for di¤erent elastici-
ties of attention with respect to incentives by introducing a parametrized class
of cost functions Gf ; g 2 G:
                           8          PM            h           1
                                                                                i
                           >
                           >                                    m
                                                                                         if        6= 1 and          6= 2;
                           >
                           <              m=1   m       (       1)(        2)
                                                PM
      Gf   ; g(   )=                                m=1               m    ln        m        if         = 1:                ;
                           >
                           >                        PM
                           >
                           :                                               ln
                                                        m=1           m
                                                                                    m
                                                                                              if         = 2:
                                                                                m



In the two state case, derivatives with respect to                                        1    obey,
                                          (                 1
                                                                      (1             1
                                                                                1)
                  @Gf ; g ( )                               1
                                                                  (        1)
                                                                                                   if        6= 1;
                              =
                    @ 1                         (ln                   ln(1                              if
                                                            1                             1 ))                = 1:
                  2
              @ Gf         ; g(   )
                              2       =         1   + (1                    1)                if         6= 1;
                      @(    1)


Note that the second derivative of these costs functions is continuous in ,
with the Shannon entropy cost function …tting smoothly into the parametric
class at = 1.




       Figure 7a: Cost functions                                           Figure 7b: Marginal Costs




                                                        27
    Figures 7a and 7b illustrate the shape of these cost functions (normalized
to equal 0 at 1 = 0:5) and their …rst derivatives in the symmetric two state
case and with = 1. Functions with less than 1 have a …rst derivative that
does not tend to in…nity as 1 tends to zero. This means that the marginal
cost of information does not go to in…nity, so that subjects may choose to be
fully informed.

    We solve the model computationally using the derivative characterization
of lemma 3. Figure 8 plots the probability of correct choice against the cost
of mistake for di¤erent values of . For = 0:5, the DM would choose to
become fully informed when the utility di¤erence hits about 1.1. For the other
cost functions, subjects never become fully informed. Note that larger values
of imply that, for any given cost of mistakes, subjects will choose to be less
informed, and will be less responsive to a change in the cost of mistakes.




                Figure 8: Attentional Response to Incentives



   Figures 9a and 9b show how the extra degree of freedom introduced by
the parameter can help to match our experimental data. The two bars in
…gure 9a show the posterior probability of state 1 given the choice of f and
the posterior probability of state 2 given choice of g for each decision problem

                                      28
in treatment 1 according to our aggregate data. Figure 9b shows the same
data for treatment 2.

   On both graphs, the dashed line shows the predictions of the Shannon
model, using a treatment speci…c cost parameter chosen to minimize the mean
squared di¤erence between predicted and actual posterior beliefs (note that,
                                                     f (x)  g(x)
due to symmetry, the Shannon model predicts that 1 = 2 for every x).
The best …tting cost parameters are 8:7 in treatment 1 and 22:2 in treatment
2, which leads to a mean squared di¤erence between observed and predicted
data of 0.020.

    The solid line shows the predictions of the best …tting model in the class
Gf ; g , with …xed across treatments but costs allowed to vary.13 The para-
meters that best …t the model are = 7:01, with costs in the …rst treatment
of 0.002 and in the second treatment of 0.046. Clearly this model provides a
much better …t of the data, with a mean squared di¤erence of 0.002.




         Figure 9a: Treatment 1                            Figure 9b: Treatment 2



    Since the Shannon model is nested within the class of Gf ; g functions,
this broader class must weakly provided a better …t of the data. However,
criteria that punish models for having additional parameters suggest rejecting
  13
    Thus the shape of the cost function is constrained to be the same in the two treatments,
but the level of costs varies. This is equivalent to the way we treated the Shannon model.


                                            29
Shannon in favor of the broader parametrized class. For example the Akaike
information criterion is lower for the model that allows for variable than for
the Shannon model.14



7        Concluding Remarks

Rational inattention theory is of rapidly growing importance. Yet general
behavioral implications can be hard to identify even with Shannon mutual
information costs. We develop posterior-based methods that identify key be-
havioral properties of this model. We experimentally test a key implication of
the Shannon model regarding changes in incentives. We …nd our subjects to
be less responsive along this dimension than is implied by the Shannon model.
We introduce a class of generalized entropy cost functions that allow for a
more ‡exible such response, and identify the improved …t that results.

    The posterior-based method is currently being applied in a variety of set-
tings. It is of particular value in dynamic settings in which the beliefs evolve
as a result of the interaction between attentional e¤ort and exogenous shocks.
It is equally of value in strategic settings (e.g. Martin [2013]).
  14
       The AIC is de…ned by the equation,

                                    AIC = 2k     2 ln(L);

where k is the number of parameters of the model and L its likelihood. The AIC for the
Shannon model (allowing for di¤erent costs in the two treatments) is 10028, while for the
extended model (constant but di¤erent costs in the two treatments) the AIC is 9521. The
favored model is the one with the lowest AIC. The ‡exibility that our generalized entropy
model adds is therefore of signi…cant value in …tting our experimental data.




                                            30
8    Bibliography

References
Andrew Caplin and Mark Dean. Rational inattention and state dependent
 stochastic choice. Mimeo, New York University, 2013.

T. Cover and J. Thomas. Elements of Information Theory 2nd Edition. John
  Wiley and Sons, Inc., New York, 2006.

C.A. Holt and S.K. Laury. Risk aversion and incentive e¤ects. American
  Economic Review, 92(5):1644–1655, 2002.

Emir Kamenica and Matthew Gentzkow. Bayesian persuasion. American
 Economic Review, 101(6):2590–2615, September 2011.

Bartosz Mackowiak and Mirko Wiederholt. Optimal sticky prices under ratio-
  nal inattention. American Economic Review, 99(3):769–803, June 2009.

Daniel Martin. Strategic pricing and rational inattention to quality. Mimeo,
 New York University, 2013.

Filip Matejka and Alisdair McKay. Rational inattention to discrete choices:
  A new foundation for the multinomial logit model. CERGE-EI Working
  Papers wp442, The Center for Economic Research and Graduate Education
  - Economic Institute, Prague, June 2011.

Filip Matejka and Alisdair McKay. Rational inattention to discrete choices:
  A new foundation for the multinomial logit model. Mimeo, CERGE, 2013.

Filip Matejka. Rationally inattentive seller: Sales and discrete pricing.
  CERGE-EI Working Papers wp408, The Center for Economic Research and
  Graduate Education - Economic Institute, Prague, March 2010.

R. Tyrrell Rockafellar. Convex Analysis. Princeton University Press, Prince-
  ton, 1972.

                                    31
Christopher A. Sims. Stickiness. Carnegie-Rochester Conference Series on
  Public Policy, 49(1):317–356, December 1998.

Christopher Sims. Implications of Rational Inattention. Journal of Monetary
  Economics, 50(3):665–690, 2003.

Christopher A. Sims. Rational inattention: Beyond the linear-quadratic case.
  American Economic Review, 96(2):158–163, May 2006.

Christopher A. Sims. Rational inattention and monetary economics. In Ben-
  jamin M. Friedman and Michael Woodford, editors, Handbook of Monetary
  Economics, volume 3 of Handbook of Monetary Economics, chapter 4, pages
  155–181. Elsevier, 2010.

Stijn van Nieuwerburgh and Laura Veldkamp. Information Immobility and
  the Home Bias Puzzle. Journal of Finance (forthcoming), 2008.

Ming Yang. Coordination with rational inattention. Technical report, 2011.




                                    32
9     Appendix

9.1     Lemmas

Our proof of theorem 1 relies on three lemmas that hold more broadly than
for the Shannon cost function. They apply to the general posterior-separable
cost functions KG : ( ;A) ! R based on arbitrary strictly convex function
G : RM+ ! R 2 G as introduced in section 6 and the corresponding net utility
functions,

    ( ;A)
                        X           X
                                    M                                     X
 NG         (B; P; )           Pf          f  f
                                           m Um       KG (B; P; ) =              P f NGf (   f
                                                                                                 ) + G( ):
                        f 2B        j=1                                   f 2B


As for the Shannon model, the de…nition of rational inattention for G 2 G
involves maximization of this net utility function. Proofs of these Lemmas are
in the online appendix.


Lemma 1 Given A 2 F and G 2 G, the set EG (A)                              RM de…ned by,
                   (                                                                                             )
                                        (y; 1 ; ::; M 1 ) 2 R X
      EG (A)                                                    P        f                                           ;
                       s.t. 9A 2 F and (B; P; ) 2 ( ;A) s.t. y         f
                                                                 f 2B P NG (
                                                                                                         f
                                                                                                             )

      is closed, convex, and bounded above in its …rst coordinate.
                                                ( ;A)
Lemma 2 Strategy (B; P; ) 2                             is rationally inattentive for G 2 G if
   and only if it there exists                  m   for 1 m M 1 such that property
   SH holds:

                                      X1
                                      M                                   X1
                                                                          M
                         NGg ( )                m m       NGf (   f
                                                                      )            f
                                                                                 m m;
                                          m=1                             m=1


      all f 2 B, g 2 A and                2 .

Lemma 3 Given G 2 G that is di¤erentiable on I , the interior of , strategy
                                                           ( ;A)
   (B; P; ) 2 ( ;A) with fm 2 (0; 1) satis…es (B; P; ) 2 ^ G     if and only

                                                     33
      if it satis…es CT, ED, and UB:

       A. Common Tangent for Chosen Acts (CT): Given f; g 2 B,
                                      "                #
                                X1
                                M
                                          @NGf ( f )                                    X1
                                                                                        M
                                                                                                  @NGg ( g )
            NGf (   f
                        )                                    f
                                                             m     = NGg ( g )                                     g
                                                                                                                   m:
                                m=1
                                            @ m                                         m=1
                                                                                                   @ m

       B. Equal Derivative for Chosen Acts (ED) : Given f; g 2 B,

                                               @NGf ( f )   @NGg ( g )
                                                          =            :
                                                 @ m         @ m

       C. Unchosen Act Bound (UB) : Given f 2 B and g 2 BnA,
                                      "                #                                      "                #
                                X1
                                M
                                          @NGf ( f )                                    X1
                                                                                        M
                                                                                                  @NGf ( f )
            NGg ( g )                                       g
                                                            m        NGf (    f
                                                                                  )                                f
                                                                                                                   m;
                                m=1
                                            @ m                                         m=1
                                                                                                    @ m

                                                                         X1 h
                                                                         M
                                                                                      @NGf
                                                                                           ( f)
                                                                                                  i
          where     g
                            2    maximizes on              NGg (    )                   @ m           m   on   2 .
                                                                         m=1



9.2    Theorem 1
                                                                                                  ( ;A)
Theorem 1 Given ( ; A) 2                       F and             > 0, (B; P; ) 2                          is rationally
    inattentive if and only if:

       A. ILR Equations for Chosen Acts: given f; g 2 B, and 1                                                     m
          M,
                                                            f           g
                                                            m           m
                                                            f
                                                                 =      g :
                                                            m           m

       B. ILR Inequalities for Unchosen Acts: given f 2 B and g 2
          AnB,
                               X
                               M    f
                                    m  g
                                    f  m   1:
                                                 m=1          m




                                                       34
Proof. The Shannon model satis…es the di¤erentiability condition of lemma
3, and also has unbounded derivatives at the boundary points of the domain,

                       @G( )                                        @G( )
                   lim       =                 1 and lim                  = 1.
                   m &0 @ m                                     m %1 @ m



so that all rationally inattentive strategies necessarily involve fm > 0. Hence
condition UB reduces to the “bounded tangent”(BT) condition (as g occurs
at the tangent point),

        @NGg ( g )                                       X1
                                                         M                                X1
                                                                                          M
                   =     m   =)      NGg ( g )                    g
                                                                m m         NGf ( f )             f
                                                                                                m m:
         @ m                                             m=1                              m=1


Given the unbounded derivatives at corners of G =         H, there can only be
                                                f
interior rationally inattentive posteriors with m > 0 all f 2 B as in lemma 3.
Our proof of theorem 1 is therefore based on conditions CT, ED, and BT.

    Necessity: Note that Shannon net utility functions have particularly sim-
ple form,
                                                                    !                                      !!
          X1
          M                                               X1
                                                          M
                                                                           f
                                                                                                X1
                                                                                                M
  f                 f
N ( )           m (Um           ln    m )+       1              m         UM            ln 1           m        ;
          m=1                                             m=1                                   m=1


De…ne
                                                           @N f ( f )
                                           m     max                  ;
                                                 f 2B       @ m
for 1   m     M        1. for        > 0. Hence ED implies that,
                                                     h                      i
                         f                 f              f             f
                        Um            ln   m             UM      ln     M       =   m:




                                                         35
Substitution in CT yields,
                                                                                                                          !
                    X1
                    M                   X1
                                        M
                                                    f                     f
                                                                                                             X1
                                                                                                             M
                                                                                                                               f          f
Nf(     f
            )                f
                           m m      =         f
                                              m    UM            ln       M   +     m       +    1                    f
                                                                                                                      m       UM     ln   M
                    m=1                 m=1                                                                  m=1

                                         f              f        g                      g
                                                                                                                      X1
                                                                                                                      M
                                                                                                 g           g                  f
                                    =   UM         ln   M   =   UM                 ln   M   =N ( )                            m m:
                                                                                                                      m=1


Overall, given f; g 2 B, we conclude therefore that,
                                         f                   g
                                        Um         f        Um                     f
                                              ln   m    =                 ln       m;


all 1   m    M . This establishes necessity of the ILR equations for chosen
acts upon exponentiation.
                                                                                                                          g
    To prove necessity of the ILR inequalities consider g 2 AnB and                                                           2
satisfying,

                          @N g ( g )    g               g         g                     g
                                     = Um          ln   m       [UM                ln   M]   =       m;
                           @ m

all 1           m    M       1. By property BT,

                    X1
                    M
                                    g               g        f                      f
                                                                                                                 X1
                                                                                                                 M
N g( g)                      g
                           m m   = UM         ln    M       UM                ln    M   = Nf(        f
                                                                                                         )                  f
                                                                                                                          m m;
                    m=1                                                                                          m=1


for f 2 B. Hence
                     g                   f                                      g            f
                    Um          g       Um         f                  f        Um           Um                   g
                           ln   m             ln   m    =) ln         m   +                           ln         m;


for 1           m        M . Exponentiating and taking the summation we arrive at,

                                        X
                                        M     f              X
                                                             M
                                              m         g                 g
                                              f         m                 m   = 1;
                                        m=1   m              m=1


establishing necessity.


                                                        36
   Su¢ ciency: Given ( ; A) 2          F, consider (B; P; ) 2 ( ;A) such
that the ILR conditions are met. We …rst con…rm that the equal derivative
condition is therefore met. The ILR equations imply that, given f; g 2 B, and
1 m M,
                                                  f             g
                                                  m             m
                                                  f
                                                       =        g :
                                                  m             m

Hence, upon substituting the logarithmic versions of the ILR equations for
arguments m and M , we conclude that indeed derivatives are equal,

@N f ( f )                         h                        i                                                    @N g ( g )
              f               f         f              f           g                g      g            g
           = Um          ln   m        UM         ln   M        = Um           ln   m    [UM      ln    M]   =                m:
 @ m                                                                                                              @ m

      To establish the CT, note as before that for any f 2 B,

                                            X1
                                            M
                                                               f                    f
                              f    f                f
                          N (          )          m m       = UM               ln   M:
                                            m=1


Applying again the ILR equation for chosen acts with m = M we con…rm that
indeed, for all f:g 2 B,

                X1
                M
                                   f              f          g                 g
                                                                                                  X1
                                                                                                  M
  f     f               f                                                                g    g            f
N (         )         m m     =   UM         ln   M    =    UM            ln   M    =N ( )               m m:
                m=1                                                                               m=1


                                                                      g
      To establish BT, consider g 2 AnB and                               2    with,

                      @N g ( g )    g                  g          g                 g
                                 = Um             ln   m         UM            ln   M   =    m;
                       @ m

for all 1 m M 1. As before, this enables us to compute the value of the
relevant tangent as,

                                            X1
                                            M
                                                               g                    g
                              g    g                g
                          N ( )                   m m       = UM               ln   M:
                                            m=1




                                                       37
Hence the bounded tangent property applies provided that,
                           g                        f
                          UM             g         UM           f
                                    ln   M                ln    M:



   To the contrary, suppose that,
                           g                        f
                          UM              g        UM           f
                                    ln    M   >           ln    M:


Given their de…ning derivative conditions, this implies that the same inequality
holds for all 1 m M 1,
   g                                                                                 f
  Um          g       g         g                f                  f               Um        f
         ln   m   = [UM   ln    M]   +    m   > UM             ln   M   +   m   =        ln   m:


Rearrangement yields,

                                          g         f
                                f        Um        Um           g
                          ln    m   +                   > ln    m:

Exponentiating and taking the summation we arrive at,

                          X
                          M         f              X
                                                   M
                                    m     g               g
                                    f     m   >           m    = 1;
                          m=1       m              m=1


contradicting the ILR inequalities for unchosen acts, and with it establishing
validity of the bounded tangent property. This rounds out the proof that the
su¢ cient conditions of corollary 3 hold, completing the proof of theorem 1.


9.3    Theorem 2

Theorem 2 (Unique Optimal Strategy): With a¢ ne independence, ^ (                                  ;A)
                                                                                                         =
    1 all 2 .

Proof. Given the di¤erentiability of the value function V : X ! R at
all points with m > 0, we know that for such points there is a single tan-

                                              38
gent plane at the corresponding boundary of set E(A). Let (1; 1 ; ::; M 1 )
be a normal vector de…ning this supporting hyperplane at boundary point
(y; 1 ; ::; M 1 ), and de…ne corresponding posteriors f on f 2 A to solve the
…rst order conditions,

                       @N f ( f )
                                  +       m   = 0; for 1              m     M      1.
                        @ m

Now consider the set C     A comprising all acts f 2 A that maximize the
corresponding dot product,
     (                                                                                                            )
                                                               X1
                                                               M                              X1
                                                                                              M
                 f
C=    f 2 Aj9        2       such that N f (          f
                                                          )+            f
                                                                      m m         N g( g) +           g
                                                                                                    m m   all g 2 A :
                                                               m=1                            m=1


By lemma 2 all optimal strategies (B; P; ) 2 ^ (                      ;A)
                                                                            must satisfy B      C, since,
given f; g 2 C and g 2 AnC we know that,

                                  X1
                                  M                                         X1
                                                                            M
                Nf(      f
                             )+             f
                                          m m         = N g( g) +                    g
                                                                                   m m;
                                  m=1                                       m=1
                                  X1
                                  M                                         X1
                                                                            M
                Nf(      f
                             )+             f
                                          m m         > N g( g) +                    g
                                                                                   m m:
                                  m=1                                       m=1


By the logic of lemma 2, satisfaction of the upper equation implies satisfaction
of the ILR equations, while satisfaction of the lower strict inequality produces
a strict form of the ILR inequality, establishing indeed that B C is necessary
for optimality.

   Now suppose that there are two distinct sets of probability weights P f ; Qf
     X      X
with   Qf =      P f = 1 (these need not all be strictly positive) such that,
     f 2C       f 2C

                                  X                   X
                                         Pf   f
                                                  =          Qf   f
                                                                      = :
                                  f 2C                f 2C




                                                      39
                                                                                        X
                                                                    f                          f
Subtraction produces a non-zero set of weights                           Pf   Qf with              =0
                                                                                        f 2C
such that,
                                    X
                                               f    f
                                                    m   = 0;
                                    f 2C

for all 1    m      M . Substitution of the ILR condition yields,
                                  X
                                           f         f
                                                   m m      = 0;
                                  f 2C

                                                            g
whereupon division by any non-zero term                         m   yields,

                                  X            f
                                                        f
                                               g        m   = 0:
                                  f 2C

             Xh fi
Given that           g   = 0, this directly contradicts a¢ ne independence. This
             f 2C
completes the proof for all cases with m > 0 for some 1 m M 1: This
leaves only the case M = 1, for which case identical logic establishes that
there can only be one optimizing act that must be chosen for sure.




                                                   40
10      Online Appendix

10.1      Proofs of Lemmas

Lemma 1 Given A 2 F and G 2 G, the set EG (A)                                 RM de…ned by,
                  (                                                                                                               )
                                                                                                           X
       EG (A)       (y;   1 ; ::;    M 1)       2R     Xj9A 2 F and (B; P; ) 2            ( ;A)
                                                                                                  s.t. y          P f NGf (   f
                                                                                                                                  ) ;
                                                                                                           f 2B


       is closed, convex, and bounded above in its …rst coordinate.


Proof. Given A 2 F and G 2 G, consider (y; 1 ; ::; M 1 ), (~      y ; ~ 1 ; ::; ~ M 1 ) 2
                                    ~
EG (A) together with …nite sets B; B A, probabilities on actions and associ-
ated posteriors, P f ; fm for f 2 B and P~ f ; ~ fm for f 2 B,
                                                            ~ all 1 m M 1
such that,
                                    X                         X
                      m    =               Pf    f
                                                 m   and y           P f NGf (   f
                                                                                     );
                                    f 2B                      f 2B
                                     f
                                    X                         X
                      ~m =                 P~ f ~ fm and y~          P~ f NGf (~ f ):
                                       ~
                                    f 2B                         ~
                                                              f 2B


De…ne C = B [ B ~ and extend P f ; P~ f to this domain by setting them to zero
on the unchosen acts.

   Given        2 (0; 1), de…ne Rf = P f + (1                  )P~ f and

                                f           Pf    f
                                                  m   + (1     )P~ f ~ fm
                                m    =                                    :
                                                 Pf   + (1     )P~ f




                                                      41
                                                                                X
                                    f
It is immediate that                     2      all f 2 C and that                        Rf   f
                                                                                               m   =        m   + (1   )~ m
                                                                                f 2C
                                    +~
so that (C; ; R) 2                  2    Note that, for each f 2 C

           NGf        f

           X
           M
                   f  f
       =           m Um         G( f )
           j=1

                       Pf                    X
                                             M
                                                                       (1     )P~ f       XM
                                                     f  f                                               f  f
       =                                             m Um      +                                        m Um       G( f )
                P f + (1            )P~ f    j=1
                                                                     P f + (1       )P~ f j=1
                       P   f
                                                                 (1    )P~ f
                                             NGf (   f
                                                         )+                       NGf (~ f );
                P f + (1            )P~ f                        f
                                                                P + (1        ~
                                                                             )P f



       By the convexity of G

       Thus we have that
X                               X
       Rf NGf     f
                          =                    P f + (1            )P~ f NGf          f

f 2C                            f 2C
                                    X                                          X
                                             P f NGf (    f
                                                              ) + (1       )          P~ f NGf (~ f ) = y + (1              )y;
                                    f 2B                                          ~
                                                                               f 2B


con…rming that                 y;    1 ; ::;    M 1       + (1         )(~
                                                                         y ; ~ 1 ; ::; ~ M      1)   2 EG (A)

    To establish closedness, consider a sequence (y(n); (n)) 2 EG (A) converg-
ing to (y L ; L ) (to simplify notation we use the full prior as the second argu-
ment since M is anyway implied) and corresponding triples (B(n); P (n); (n)) 2
  ( (n);A)
                          P                                 P                  f
           , so that (n) = f 2B(n) P f (n) f (n) and y(n)               f
                                                               f 2B(n) P (n)NG (
                                                                                   f
                                                                                     (n)).
We show now that there is no loss of generality in assuming jB(n)j             M+
1. Suppose initially that jB(n)j > M . By Charateodory’s theorem, since
f f (n) 2 jf 2 B(n)g contain (n) in its convex hull, there exists B1 (n)
B(n) with jB1 (n)j       M + 1 for which there exists a strictly positive proba-
                    f
                                                             X
bility weights P1 (n) > 0 on f 2 B1 (n) such that =                P1f (n) f (n). If
                                                                                               f 2B 1 (n)




                                                                42
expected net utility is no lower,
                         X                                           X
               y(n)               P f (n)NGf (       f
                                                         (n))                P1f (n)NGf (        f
                                                                                                     (n));
                        f 2B(n)                                  f 2B1 (n)


we are done. If not, identify the smallest scalar                         1   2 (0; 1) such that,

                                                f
                                             1 P1 (n)      = P f (n);

some f 2 B1 (n). That such a scalar exists follows from the fact that
                                  X                         X
                                         P1f (n) =                   P f (n) = 1;
                             f 2B1 (n)                     f 2B(n)


with all components in both sums strictly positive and with jB(n)j > jB1 (n)j.

    We now de…ne a second set of probability weights P2f (n),

                                                                        f
                                               P f (n)               1 P1 (n)
                                   P2f (n)   =                        1
                                                                              :
                                                     1

for f 2 B1 (n). Correspondingly, we de…ne,

                              B2 (n) = ff 2 B(n)jP2f (n) > 0g;
                                                                                                X
noting that jB2 (n)j              jB(n)j        1. By construction                     =               P2f (n)   f
                                                                                                                     (n).
                                                                                            f 2B(n)
Moreover,
                                                "                               #
  X                                    X            P f (n)             f
                                                                     1 P1 (n)
                                                                                                             X
            P2f (n)NGf ( f (n))   =                                   1
                                                                                    NGf (   f
                                                                                                (n)) >             P f (n)NGf (   f
                                                                                                                                      (n)):
                                                          1
f 2B2 (n)                             f 2B(n)                                                            f 2B(n)


                                                                        ~
Iteration from this point establishes that indeed we can identify a set B(n)
             ~
B(n) with B(n)         M + 1 and P~ (n) > 0 on f 2 B(n)   ~    such that     =




                                                          43
 X
          P1f (n)   f
                        (n) and,
   ~
f 2B(n)

                                    X
                                             P1f (n)NGf (     f
                                                                  (n))    y(n):
                                      ~
                                   f 2B(n)


Given this, there is no loss of generality in assuming that jB(n)j                M + 1 in
our original sequence.

    With this, we can focus on a subsequence (we continue to index by n for
notational simplicity) with all sets B(n) of the same cardinality K       M . In
each set B(n) we index the acts in (arbitrary) order by f (k; n) 2 for 1 k K,
and correspondingly label that associated posteriors and act probabilities as
 k
   (n); P k (n). Given the compactness of , we can further select subsequences
to ensure that there is a full set of limit posteriors and limit probabilities k
and P k , for 1 k K,

                                        k            k
                               lim          (n) =        ; lim P k (n) = P k
                               n!1                        n!1


For all acts f 2 A, we can compute the net utility at all limit posteriors,

                                                    X
                                                    M
                                   NGf ( k )    =          f
                                                          Um       k
                                                                   m     G( k )
                                                    m=1

        f
Since fUm 2 RM jf 2 Ag is bounded above then so is NGf ( k ) (with respect to
                   f
f 2 A). . Since fUm  2 RM jf 2 Ag is closed, the upper bound is achieved.
Hence we can …nd acts f (k) 2 A that maximize the above net utilities,

                                        N (f (k); k)           N (f; k);

all f 2 A.




                                                         44
   We now de…ne B = [K
                     k=1 f (k). Note that, by construction


                                          X
                                          K
                                                Pk      k
                                                            =    L
                                                                     ;
                                          k=1


                         (   L ;A)
so that (B; ; P ) 2                  . Note also that, for each for all n,

                       X
                       K                                X
                                                        K
                                                                         f (k;n)
                              P k N f (k) ( k )                 P k NG             ( k ):
                        k=1                             k=1


In light of continuity of all functions NGf , taking the limit on the RHS as
n ! 1 yields,

             X
             K                                      X
                                                    K
                    P k N f (k) ( k )         lim           P k (n)NGf (           f
                                                                                       (n))        yL;                  (4)
                                           n!1
              k=1                                    k=1


This completes the proof that (y L ; L ) 2 EG (A), hence that EG (A) is closed.
                                                                           f
Boundedness above of the …rst coordinate follows from the fact that fUm      2
 M
R jf 2 Ag is bounded above for all A 2 F.

                                                ( ;A)
Lemma 2 Strategy (B; P; ) 2                           is rationally inattentive for G 2 G if
   and only if it there exists                m   for 1 m M 1 such that property
   SH holds:

                                        X1
                                        M                                          X1
                                                                                   M
                       NGg (    )               m m          NGf ( f )                          f
                                                                                              m m;
                                        m=1                                        m=1


     all f 2 B, g 2 A and               2 .

                                                                                                                        !
                                     ( ;A)
                                                                         X
Proof. Necessity: Given (B; P; ) 2 ^ G ,                                        P f NGf (      f
                                                                                                   );   1 ; ::;   M 1       is
                                                                         f 2B
an upper boundary point boundary of EG (A). Lemma 1 establishes that such
sets are always closed, convex, and bounded above in the …rst coordinate.
This implies existence of a supporting hyperplane de…ned by normal vector

                                                     45
(1;   1 ; :::;     M 1)    such that, for all (y0 ; y1 ; ::; yM                1)       2 EG (A),

      X1
      M                   X                           X1
                                                      M                    X
 y0              m ym            P   f
                                         NGf ( f )          m m       =             P f [NGf (   f
                                                                                                     )     f
                                                                                                         m m ]:   (5)
      m=1                 f 2B                        m=1                  f 2B


We show now property SH is satis…ed for such a normal vector. Substitution
of (NGf ( f ); f1 ; ::; fM 1 ) 2 EG (A) on the LHS for f 2 B yields,

                                     X1
                                     M                 X
                    NGf ( f )                     f
                                                m m           P f [NGf (       f
                                                                                    )        f
                                                                                           m m ]:
                                     m=1               f 2B


This implies that these inequalities are in fact equations for all f 2 B, since
this is the only way to prevent one of the sums on the RHS from being strictly
higher than their weighted average on the LHS. This implies that NGf ( f )
X1
M
          f
       m m can be plugged in to the right hand side of equation 5, which in
m=1
turn establishes that, given f; g 2 B, and                       2

                                          X1
                                          M                                   X1
                                                                              M
                        NGg ( g )                  g
                                                 m m   = NGf (    f
                                                                      )                     f
                                                                                          m m;
                                          m=1                                 m=1

                                                                                                                   f
as necessary for property SH. Again, equation 5 tells us that all f 2 B,
solves,
                               f
                                       X1
                                       M
                          max NG ( )        m m;
                                           2
                                                            m=1

as again required for SH. The …nal aspect of condition SH to con…rm is that,
given f 2 B, g 2 AnB and 2

                                         X1
                                         M                                   X1
                                                                             M
                        NGg (    )              m m      NGf ( f )                          f
                                                                                          m m;
                                          m=1                                m=1


This is again immediate from 5 since (N g ( );                            1 ; ::;       M 1)   2 EG (A).




                                                       46
   Su¢ ciency: If property SH holds, it directly implies existence of a normal
vector (1; 1 ; :::; M 1 ) such that, given (y0 ; y1 ; ::; yM 1 ) 2 EG (A),


                               X1
                               M
                         y0               m ym          NGf (   f
                                                                    )         f
                                                                            m m;
                                   m=1


any f 2 B. Applying lemma 1, this implies that all points (NGf ( f ); f1 ; ::; fM 1 ) 2
EG (A) are in the upper boundary of EG (A). Hence this applies also to any con-
                                                 X
vex combination of them such as that de…ned by (    P f NGf ( f ); 1 ; ::; M 1 ) 2
                                                                        f 2B
EG (A), completing the proof.

Lemma 3 Given G 2 G that is di¤erentiable on I , the interior of , strategy
                                                           ( ;A)
   (B; P; ) 2 ( ;A) with fm 2 (0; 1) satis…es (B; P; ) 2 ^ G     if and only
   if it satis…es CT, ED, and UB:

       A. Common Tangent for Chosen Acts (CT): Given f; g 2 B,
                                   "                #
                             X1
                             M
                                       @NGf ( f )                                     X1
                                                                                      M
                                                                                                @NGg ( g )
             NGf ( f )                                    f
                                                          m     =   NGg ( g )                                    g
                                                                                                                 m:
                             m=1
                                         @ m                                          m=1
                                                                                                 @ m

       B. Equal Derivative for Chosen Acts (ED) : Given f; g 2 B,

                                            @NGf ( f )   @NGg ( g )
                                                       =            :
                                              @ m         @ m

       C. Unchosen Act Bound (UB) : Given f 2 B and g 2 BnA,
                                   "                #                                       "                #
                             X1
                             M
                                       @NGf ( f )                                     X1
                                                                                      M
                                                                                                @NGf ( f )
             NGg ( g )                                   g
                                                         m          NGf (   f
                                                                                )                                f
                                                                                                                 m;
                             m=1
                                         @ m                                          m=1
                                                                                                  @ m

                                                                        X1 h
                                                                        M
                                                                                    @NGf
                                                                                         ( f)
                                                                                                i
           where    g
                         2    maximizes on              NGg (   )                     @ m           m   on   2 .
                                                                        m=1


Proof. In light of lemma 2, the …rst part requires us to show that, when G 2 G
is di¤erentiable, property SH is satis…ed for (B; P; ) 2 ( ;A) with fm 2 (0; 1)

                                                    47
if and only if (B; P; ) satis…es conditions ED, CT, and UB. That these three
                                                                   @N f ( f )
conditions are su¢ cient for property SP is immediate using m = @G            for
                                                                        m
any f 2 B and applying UB. That they are necessary for SP to be satis…ed in
cases with fm 2 (0; 1) and with G di¤erentiable derives from the fact that SP
certainly requires that, for each f 2 B, f solves,

                                              X1
                                              M
                           max NGf ( )              m m:
                             2
                                              m=1


Given that fm 2 (0; 1) and that G 2 G is di¤erentiable, solving this problem
                 @N f ( f )
requires m = @G . Given this, SP directly implies CT, ED, and UB as
                      m
illustrated in the proof of lemma 1.


10.2     Proofs of Corollaries

All corollaries apply more generally than to the Shannon model. However for
consistency with the text they are stated only for this case. The appropriate
generalization to separable cost functions is in each case clear.


Corollary 1 (Locally Invariant Posteriors - LIP): If (B; P; ) 2 ^ ( ;A)
    and (C; Q; ) 2 ( ;A) with C    B satis…es f = f all f 2 C, then
    (C; Q; ) 2 ^ ( ;A) .


Proof. Note by the necessity aspect of lemma 2 that if (B; P; ) 2 ^ ( ;A)
then condition SH is satis…ed. Neither the prior 2 nor the probability map
P : B ! R feature in condition SP, while deletion of acts can only weaken
the check. Hence if (C; Q; ) 2 ( ;A) with C B satis…es f = f all f 2 C,
condition SH remains valid and the su¢ ciency aspect of lemma 2 implies it is
optimal.


Corollary 2 (Envelope Condition): Given ( ; A) 2   F such that m >
                            A
    0 , the value function V :    ! R is di¤erentiable at   and has

                                         48
     continuous partial derivatives,

                                @V A ( )   @N f f
                                         =     (^ );
                                 @ m       @ m

               ^ some (B;
     where f 2 B       ^ P^ ; ^ ) 2 ^ (          ;A)
                                                       .

Proof. Given ( ; A) 2          F , note that  1 ; ::; M 1 is in the interior
                                   PM 1
of X = f( 1 ; ::; M 1 ) 2 RM +
                                1
                                  j m=1 m    1g. By lemma 1 an optimal
policy exists. Consider a corresponding optimal strategy (B; P; ) 2 ^ ( ;A)
that therefore achieves the value,
                                        X
                           V A( ) =              P fNf(         f
                                                                    ):
                                        f 2B


                      ^ 2 F with state dependent payo¤s,
De…ne a composite act h

                                   ^
                                        X
                                h
                               Um =              P f Um
                                                      f
                                                        :

                                        ^
De…ne the net payo¤ function to N h : ! R in standard fashion, and ap-
ply the envelope theorem of Benveniste and Scheinkman [1979] to functions
        ^                                                            ^
V A ; N h : ! R, noting that both are concave, that V ( )          N h ( ) on the
                                  ^                 ^
interior of X, and that V ( ) = N h ( ), and that N h ( ) is di¤erentiable on the
interior of X. With this we conclude that V A is di¤erentiable at and that,

                    @V A
                                 ^
                              @N h      X @N f
                         ( )=      ( )=      Pf     (                    f
                                                                             ):
                    @ m       @ m       f 2B
                                                @ m


By lemma 3, the ED conditions are satis…ed,

                                       @N f        f            @N g g
                      f; g 2 B =)           (              )=       ( );
                                       @ m                      @ m
                                   X
completing the proof in light of              P f = 1.
                                       f 2B


Corollary 3 (States Bound Acts - SBA): Given ( ; A) 2                             F, there

                                            49
      exists a rationally inattentive strategy with jBj                         M.


Proof. Consider (B; P; ) 2 ^ ( ;A) such that jBj > M . By the necessity
aspect of lemma 2, condition SH is satis…ed. This condition remains valid
                       ~
for any subset of acts B      B with ~ f = f on f 2 B.   ~ By the su¢ ciency
aspect of lemma 2, B;~ P~ ; ~ 2 ^ ( ;A) provided only that in the convex hull
                           f
of the family of vectors        ~ . Charateodory’s theorem implies that we
                           m f 2B
can reduce the cardinality of B to M while retaining in this convex hull,
completing the proof.


Corollary 4 (Unique Posteriors): If (B; P; ) 2 ^ (                                  ;A)
                                                                                          and (B; Q; ) 2
    ^ ( ;C) , then (f ) = (f ) all f 2 B.


Proof. Note …rst that if (B; P; ) 2 ^ ( ;A) and B C A, then (B; P; ) 2
^ ( ;C) . To see this, note from the necessity aspect of lemma 2 that if (B; P; ) 2
^ ( ;A) , then condition SH is satis…ed. Since B C A, (B; P; ) 2 ( ;C) and
condition SH is still satis…ed. Hence (B; P; ) 2 ^ ( ;C) follows in light of the
su¢ ciency aspect of lemma 2. We conclude that since (B; P; ) 2 ^ ( ;A) and
(B; Q; ) 2 ^ ( ;C) , then (B; P; ); (B; Q; ) 2 ^ ( ;B) .
                                   P f +Qf
   Given f 2 B de…ne Rf =              2
                                               and       f
                                                             2       by,

                                  f       Pf    f
                                                m   + Qf         f
                                                                 m
                                  m   =                              :
                                               Pf   + Qf

By construction, (B; ; R) 2 ( ;B) . If (f ) 6= (f ) some f 2 B, we can
apply the strict version of Jensen’s inequality as in lemma 1 to establish the
contradiction that net utility must be strictly higher at (B; ; R) than at either
(B; P; ) and (B; Q; ),
          X                       X                                          X
                 Rf N f ( f ) >          P fNf(     f
                                                        ) + (1           )          Qf N f ( f ):
          f 2B                    f 2B                                       f 2B




                                               50
11      Comparison with KKT Conditions

Following Matejka and McKay [2011], consider the constrained optimization
problem of maximizing expected prize utility less Shannon attention costs,
subject to constraints associated with rational expectations, with act-speci…c
posteriors adding to unity, and with probabilities being non-negative. Let
  2 RM be the multipliers on the rational expectations constraints and :
A ! R the multipliers on posteriors. With act set A countable, the associated
Lagrangean is,



     X              X
                    M                              X
                                                   M          X                                 X
                                                                                                M
                f         f   f             f
L=          P             m (Um        ln   m)              m(  Pf   f
                                                                     m        m)
                                                                                        f
                                                                                            (        f
                                                                                                     m   1):
     f 2A           m=1                           m=1        f 2A                           m=1



    Treating this using standard KKTcondition, a necessary condition for (B; P; )
to be rationally inattentive is that there exists ^ 2 RM , ^ : A ! R, and pos-
teriors fm 2 for f 2 A=B such that conditions KKT 1, KKT 2, and KKT 3
are satis…ed:


KKT1: For f 2 B,

                      P f Um
                           f
                                      ln    f
                                            m           ^m     ^f = 0 for 1        m            M.


KKT2: For f 2 B, if

                                                  X
                                                  M
                              f                         f     f                    f
                          P       2 (0; 1) =)           m    Um     ^m        ln   m    = 0;
                                                  m=1
                                            X
                                            M
                              f                    f     f                    f
                          P       = 1 =)           m    Um    ^m         ln   m        0:
                                            m=1




                                                       51
KKT3: For f 2 A=B,

                                 X
                                 M
                                       f     f                f
                                       m    Um    ^m     ln   m     0:
                                 m=1


    The reason that these KKT conditions do not characterize rationally inat-
tentive policies is that the objective function is not concave in the choice
variables, involving as it does product terms as between beliefs and posteriors.
As a result, one can …nd non-optimal solutions. To illustrate, consider the case
in the text with two acts, f and g, with = 1, and with U1f = U2g = ln(1 + e)
and U2f = U1g = 0. Note that an attention strategy can be fully speci…ed by
P f and f;g
          1 2 [0; 1]    0. Now consider the equal prior = 0:5 and note that
the following strategy is feasible and, together with the speci…ed multipliers,
satis…es all KKT necessary conditions:

             f    g
    (P f ;   1;   1)   = (1; 0:5; 0:5); ^ 1 = ln(1 + e) + ln 2; ^ 2 = ln 2; ^1 = ^2 = 0:

                                                                                    1+e    1
Yet (P f ; 11 ; 21 ) is not optimal, since net utility to the feasible triple (0:5, 2+e ; 2+e )
is strictly higher,

          1+e 1                  1+e                          ln(2 + e)
N (0:5;      ;    )=                       ln(1+e) ln 0:5 >             ln 0:5 = N (1; 0:5; 0:5):
          2+e 2+e                2+e                              2




                                                 52
