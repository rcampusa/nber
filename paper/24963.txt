                               NBER WORKING PAPER SERIES




       DESIGN-BASED ANALYSIS IN DIFFERENCE-IN-DIFFERENCES SETTINGS
                       WITH STAGGERED ADOPTION

                                          Susan Athey
                                        Guido W. Imbens

                                       Working Paper 24963
                               http://www.nber.org/papers/w24963


                     NATIONAL BUREAU OF ECONOMIC RESEARCH
                              1050 Massachusetts Avenue
                                Cambridge, MA 02138
                                    August 2018




We are grateful for comments by participations in the conference in honor of Gary Chamberlain
at Harvard in May 2018, and in particular by Gary Chamberlain. Gary's insights over the years
have greatly affected our thinking on these problems. We also wish to thank Sylvia Kloskin and
Michael Pollmann for superb research assistance. This research was generously supported by
ONR grant N00014-17-1-2131. The views expressed herein are those of the authors and do not
necessarily reflect the views of the National Bureau of Economic Research.

At least one co-author has disclosed a financial relationship of potential relevance for this
research. Further information is available online at http://www.nber.org/papers/w24963.ack

NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.

© 2018 by Susan Athey and Guido W. Imbens. All rights reserved. Short sections of text, not to
exceed two paragraphs, may be quoted without explicit permission provided that full credit,
including © notice, is given to the source.
Design-based Analysis in Difference-In-Differences Settings with Staggered Adoption
Susan Athey and Guido W. Imbens
NBER Working Paper No. 24963
August 2018
JEL No. C01,C23,C31

                                           ABSTRACT

In this paper we study estimation of and inference for average treatment effects in a setting with
panel data. We focus on the setting where units, e.g., individuals, firms, or states, adopt the policy
or treatment of interest at a particular point in time, and then remain exposed to this treatment at
all times afterwards. We take a design perspective where we investigate the properties of
estimators and procedures given assumptions on the assignment process. We show that under
random assignment of the adoption date the standard Difference-In-Differences estimator is an
unbiased estimator of a particular weighted average causal effect. We characterize the properties
of this estimand, and show that the standard variance estimator is conservative.


Susan Athey
Graduate School of Business
Stanford University
655 Knight Way
Stanford, CA 94305
and NBER
athey@stanford.edu

Guido W. Imbens
Graduate School of Business
Stanford University
655 Knight Way
Stanford, CA 94305
and NBER
Imbens@stanford.edu
1     Introduction
In this paper we study estimation of and inference for average treatment effects in a setting
with panel data. We focus on the setting where units, e.g., individuals, firms, or states, adopt
the policy or treatment of interest at a particular point in time, and then remain exposed to
this treatment at all times afterwards. The adoption date at which units are first exposed to
the policy may, but need not, vary by unit. We refer to this as a staggered adoption design
(SAD), such designs are sometimes also referred to as event study designs. An early example is
Athey and Stern [1998] where adoption of an enhanced 911 technology by counties occurs over
time, with the adoption date varying by county. This setting is a special case of the general
Difference-In-Differences (DID) set up (e.g., Card [1990], Meyer et al. [1995], Angrist and Pischke
[2008], Angrist and Krueger [2000], Abadie et al. [2010], Borusyak and Jaravel [2016], Athey
and Imbens [2006], Card and Krueger [1994], Freyaldenhoven et al. [2018], de Chaisemartin
and D’Haultfœuille [2018], Abadie [2005]) where, at least in principle, units can switch back and
forth between being exposed or not to the treatment. In this SAD setting we are concerned with
identification issues as well as estimation and inference. In contrast to most of the DID literature,
e.g., Bertrand et al. [2004], Shah et al. [1977], Conley and Taber [2011], Donald and Lang [2007],
Stock and Watson [2008], Arellano [1987, 2003], Abraham and Sun [2018], Wooldridge [2010],
de Chaisemartin and D’Haultfœuille [2017, 2018], we take a design-based perspective where
the stochastic nature and properties of the estimators arises from the stochastic nature of the
assignment of the treatments, rather than a sampling-based perspective where the uncertainty
arises from the random sampling of units from a large population. Such a design perspective
is common in the analysis of randomized experiments, e.g., Neyman [1923/1990], Rosenbaum
[2002, 2017]. See also Aronow and Samii [2016], Abadie et al. [2016, 2017] for this approach in
cross-section regression settings. This perspective is particularly attractive in the current setting
when the sample comprises the entire population, e.g., all states of the US, or all countries of
the world. Our critical assumptions involve restrictions on the assignment process as well as
exclusion restrictions, but in general do not involve functional form assumptions. Commonly
made common trend assumptions (de Chaisemartin and D’Haultfœuille [2018], Abraham and
Sun [2018]) follow from some of our assumptions, but are not the starting point.
    As in Abraham and Sun [2018] we set up the problem with the adoption date, rather than the
actual exposure to the intervention, as the basic treatment defining the potential outcomes. We


                                                 1
consider assumptions under which this discrete multivalued treatment (the adoption date) can
be reduced to a binary one, defined as the indicator whether or not the treatment has already
been adopted. We then investigate the interpretation of the standard DID estimator under
assumptions about the assignment of the adoption date and under various exclusion restrictions.
We show that under a random adoption date assumption, the standard DID estimator can be
interpreted as the weighted average of several types of causal effects; within our framework,
these concern the impact of different types of changes in the adoption date of the units. We
also consider design-based inference for this estimand. We derive the exact variance of the
DID estimator in this setting. We show that under a random adoption date assumption the
standard Liang-Zeger (LZ) variance estimator (Liang and Zeger [1986], Bertrand et al. [2004]),
or the clustered bootstrap, are conservative. For this case we propose an improved (but still
conservative) variance estimator.
    Our paper is most closely relateds to a very interesting set of recent papers on DID methods
that explicitly focus on issues with heterogenous treatment effects (Abraham and Sun [2018],
de Chaisemartin and D’Haultfœuille [2018], Han [2018], Goodman-Bacon [2017], Callaway and
Sant’Anna [2018], Hull [2018], and Borusyak and Jaravel [2016]). Among other things these
papers derive interpretations of the DID estimator as weighted averages of causal effects and
bias terms under various assumptions. In many cases they find that these interpretations involve
weighted averages of basic average causal effects with potentially negative weights and propose
alternative estimators that do not involve negative weights.



2     Set Up
Using the potential outcome framework for causal inference, we consider a setting with a
population of N units. Each of these N units are characterized by a set of potential out-
comes in T periods for T + 1 treatment levels, Yit (a). Here i ∈ {1, . . . , N } indexes the units,
t ∈ T = {1, . . . , T } indexes the time periods, and the argument of the potential outcome func-
tion Yit (·), a ∈ A = T ∪ {∞} = {1, . . . , T, ∞} indexes the discrete treatment, the date that
the binary policy was first adopted by a unit. Units can adopt the policy at any of the time
periods 1, . . . , T , or not adopt the policy at all during the period of observation, in which case
we code the adoption date as ∞. Once a unit adopts the treatment, it remains exposed to the


                                                 2
treatment for all periods afterwards. This set up is like that in Abraham and Sun [2018], and in
contrast to most of the DID literature where the binary indicator whether a unit is exposed to
the treatment in the current period indexes the potential outcomes. We observe for each unit
in the population the adoption date Ai ∈ A and the sequence of T realized outcomes, Yit , for
t ∈ T, where


     Yit ≡ Yit (Ai ),


is the realized outcome for unit i at time t. We may also observe pre-treatment characteristics,
denoted by the K-component vector Xi , although for most of the discussion we abstract from
their presence. Let Y , A, and X denote the N × T , N × 1, and N × K matrices with typical
elements Yit , Ai , and Xik respectively. Implicitly we have already made a sutva-type assumption
(Rubin [1978], Imbens and Rubin [2015]) that units are not affected by the treatments (adop-
tion dates) for other units. Our design-based analysis views the potential outcomes Yit (a) as
deterministic, and only the adoption dates Ai , as well as functions thereof such as the realized
outcomes as stochastic. Distributions of estimators will be fully determined by the adoption
date distribution, with the number of units N and the number of time periods T fixed, un-
less explicitly stated otherwise. Following the literature we refer to this as a randomization, or
designed-based, distribution (Rosenbaum [2017], Imbens and Rubin [2015], Abadie et al. [2017]),
as opposed to a sampling-based distribution.
   In many cases the units themselves are clusters of units of a lower level of aggregation.
For example, the units may be states, and the outcomes could be averages of outcomes for
individuals in that state, possibly of samples drawn from subpopulations from these states. In
such cases N and T may be as small as 2, although in many of the cases we consider N will
be at least moderately large. This distinction between cases where Yit is itself an average over
basic units or not, affects some, but not all, of the formal statistical analyses. It may make some
of the assumptions more plausible, and it may affect the inference, especially if individual level
outcomes and covariates are available.
   Define W (a, t) = 1a≤t to be the binary indicator for the adoption date a preceeding t, and
define Wit to be the indicator for the the policy having been adopted by unit i prior to, or at,




                                                3
time t:


     Wit ≡ W (Ai , t) = 1Ai ≤t ,


so that the N × T matrix W with typical element Wit has the form:
                                                                           
                           0 0 0 0 ...           0        (never adopter)
                                                                        
                           0 0 0 0 ...           1        (late adopter) 
                                                                        
                 
                                                                        
                           0 0 0 0 ...           1
                                                                        
                                                                        
                                                                        
     WN ×T      =
                                                                        
                           0 0 1 1 ...           1                       
                                                                        
                                                                        
                          0    0    1    1 ... 1      (medium adopter) 
                           ..   ..   ..   .. . . ..
                                                                        
                                                . .
                                                                        
                           .    .    .    .                             
                                                                        
                           0 1 1 1 ...           1       (early adopter)

              PN
Let Na ≡         1Ai =a be the number of units in the sample with adoption date a, and define
                i=1

πa ≡ Na /N , for a ∈ A, as the fraction of units with adoption date equal to a, and Πt ≡ ts=1 πs ,
                                                                                        P

for t ∈ T, as the fraction of units with an adoption date on or prior to t.
   Also define Y t (a) to be the population average of the potential outcome in period t for
adoption date a:

                  N
               1 X
     Y t (a) ≡       Yit (a),                  for t ∈ T, a ∈ A.
               N i=1

Define the average causal effect of adoption date a0 relative to a, on the outcome in period t, as

                                        N
                       0             1 Xn                     o
     τt,aa0   ≡ Y t (a ) − Y t (a) =       Yit (a0 ) − Yit (a) .
                                     N i=1

Abraham and Sun [2018] focus on slighlty different building blocks, what they call CAT Ta,t ,
                                                                         P
which, for 0 ≤ t ≤ T − a, are the super-population equivalent of (1/Na ) i|Ai =a {Yia+t (a) −
Yia+t (∞)}. The average causal effects τt,aa0 are the building blocks of many of the estimands we




                                                             4
consider later. A particularly interesting average effect is

                   N
                1 X                   
      τt,∞1   =       Yit (1) − Yit (∞) ,
                N i=1

the average effect of switching the entire population from never adopting the policy (a = ∞),
to adopting the policy in the first period (a = 1). Formally there is nothing special about the
particular average effect τt,∞1 relative to any other τt,aa0 , but τt,∞1 will be useful as a benchmark.
Part of the reason is that for all t and i the comparison Yit (1) − Yit (∞) is between potential
outcomes for adoption prior to or at time t (namely adoption date a = 1) and potential outcomes
for adoption later than t (namely, never adopting, a = ∞). In contrast, any other average effect
τt,aa0 will for some t involve comparing potential outcomes neither of which correspond to having
adopted the treatment yet, or comparing potential outcomes both of which correspond to having
adopted the treatment already. Therefore, τt,∞1 reflects more on the effect of having adopted
the policy than any other τt,aa0 .



3     Assumptions
We consider three sets of assumptions. The first set, containing only a single assumption, is
about the design, that is, the assignment of the treatment, here the adoption date, conditional
on the potential outcomes and possibly pretreatment variables. We refer to this as a design
assumption because it can be guaranteed by design. The second set of assumptions is about
the potential outcomes, and rules out the presence of certain treatment effects. These exclusion
restrictions are substantive assumptions, and they cannot be guaranteed by design. The third
set of assumptions consists of four auxiliary assumptions, two about homogeneneity of certain
causal effects, one about sampling from a large population, and one about an outcome model
in a large population. The nature of these three sets of assumptions, and their plausibility, is
very different, and it is in our view useful to carefully distinguish between them. The current
literature often combines various parts of these assumptions implicitly in the notation used and
in assumptions about the statistical models for the realized outcomes.




                                                  5
3.1    The Design Assumption
The first assumption is about the assignment process for the adoption date Ai . Our starting
point is to assume that the adoption date is completely random:

Assumption 1. (Random Adoption Date) For some set of positive integers Na , for a ∈ A,

                                    −1
                           N!
      pr(A = a) =       Q                  ,
                          a∈A Na !

                                                PN
for all N -vectors a such that for all a ∈ A,    i=1   1ai =a = Na .

   This assumption is obviously very strong. However, without additional assumptions that
restrict either the potential outcomes, or expand what we observe, for example by including
pre-treatment variables or covariates, this assumption has no testable implications in a setting
with exchangeable units.

Lemma 1. (No Testable Restrictions) Suppose all units are exchangeable. Then Assump-
tion 1 has no testable implications for the joint distribution of (Y , A).

All proofs are given in the Appendix.
   Hence, if we wish to relax the assumptions, we need to bring in additional information. Such
additional information can come in the form of pretreatment variables, that is, variables that
are known not to be affected by the treatment. In that case we can relax the assumption by
requiring only that the adoption date is completely random within subpopulations with the
same values for the pre-treatment variables. Additional information can also come in the form
of limits on the treatment effects. The implications of such restrictions on the ability to relax
the random adoption assumption is more complex, as discussed in more detail in Section 3.2.
   Under Assumption 1 the marginal distribution of the adoption dates is fixed, and so also
the fraction πa is fixed in the repated sampling thought experiment. This part of the set up is
similar in spirit to fixing the number of treated units in the sample in a completely randomized
experiment. It is convenient for obtaining finite sample results. Note that it implies that the
adoption dates for units i and j are not independent. Note also that in the standard framework
where the uncertainty arises solely from random sampling, this fraction does not remain constant
in the repeated sampling thought experiment.


                                                 6
   An important role is played by what we label the adjusted treatment, adjusted for unit and
time period averages:


      Ẇit ≡ Wit − W ·t − W i· + W ,


where W ·t , W i· , and W are averages over units, time periods, and both, respectively:

                N           N              N                      N
             1 X         1 X            1 XX               X 1 X              X
      W ·t ≡       Wit =       1Ai ≤t =           1Ai =s =           1Ai =s =     πs ,
             N i=1       N i=1          N i=1 s≤t          s≤t
                                                               N i=1          s≤t


                T
             1X                 T + 1 − Ai
      W i· ≡       Wit = 1Ai ≤T            ,
             T t=1                  T

and

             T            T              T
          1X           1 XX           1X
      W ≡       W ·t =           πs =       (T + 1 − t)πt ,
          T t=1        T t=1 s≤t      T t=1

where, with some minor abuse of notation, we adopt the convention that a1a≤T is zero if a = ∞.
Note that under Assumption 1, ow·t and W are non-stochastic. Using these representations we
can write the adjusted treatment indicator as


      Ẇit = g(t, Ai ),


where
                                       !                   T
                                                                       !
                            X                1             X                   T +1
      g(t, a) ≡    1a≤t −         πs       +     a1a≤T −         sπs       +        (1a=∞ − π∞ ) .                (3.1)
                            s≤t
                                             T             s=1
                                                                                 T


                                                                                                           Ẇit2 is non-
                                                                                                 P
Because the marginal distribution of Ai is fixed under Assumption 1, the sum                         i,t

stochastic under this assumption, even though Ẇit and thus Ẇit2 are stochastic. This fact enables
us to derive exact finite sample results for the standard DID estimator as discussed in Section 4.
This is similar in spirit to the derivation of the exact variance for the estimator for the average
treatment effect in completely randomized experiments when we fix the number of treated and
controls.

                                                       7
3.2    Exclusion Restrictions
The next two assumptions concern the potential outcomes. Their formulation does not involve
the assignment mechanism, that is, the distribution of the adoption date. In essence these
are exclusion restrictions, assuming that particular causal effects are absent. Collectively these
two assumptions imply that we can think of the treatment as a binary one, the only relevant
component of the adoption date being whether a unit is exposed to the treatment at the time
we measure the outcome. Versions of such assumptions are also considered in Borusyak and
Jaravel [2016], de Chaisemartin and D’Haultfœuille [2018], Abraham and Sun [2018] and Imai
and Kim [2016], where in the latter a graphical approach is taken in the spirit of the work by
Pearl [2000].
   The first of the two assumptions, and likely the more plausible of the two in practice, rules
out effects of future adoption dates on current outcomes. More precisely, it assumes that if the
policy has not been adopted yet, the exact future date of the adoption has no causal effect on
potential outcomes for the current period.

Assumption 2. (No Anticipation) For all units i, all time periods t, and for all adoption
dates a, such that a > t,


      Yit (a) = Yit (∞).


   We can also write this assumption as requiring that for all (i, t, a),

                                                                             
      Yit (a) = 1a≤t Yit (a) + 1a>t Yit (∞),   or 1a>t       Yit (a) − Yit (∞) = 0,


with the last representation showing most clearly how the assumption rules out certain causal
effects. Note that this assumption does not involve the adoption date, and so does not restrict
the distribution of the adoption dates. Violations of this assumption may arise if the policy is
anticipated prior to its implementation.
   The next assumption is arguably much stronger. It asserts that for potential outcomes in
period t it does not matter how long the unit has been exposed to the treatment, only whether
the unit is exposed at time t.

Assumption 3. (Invariance to History) For all units i, all time periods t, and for all

                                                 8
adoption dates a, such that a ≤ t,


      Yit (a) = Yit (1).


   This assumption can also be written as

                                                                                    
      Yit (a) = 1a≤t Yit (1) + 1a>t Yit (a),          or 1a≤t       Yit (a) − Yit (1) = 0,


with again the last version of the assumption illustrating the exclusion restriction in this assump-
tion. Again, the assumption does not rule out any correlation between the potential outcomes
and the adoption date, only that there is no causal effect of an early adoption versus a later
adoption on the outcome in period t, as long as adoption occurred before or on period t.
   In general, this assumption is very strong. However, there are important cases where it may
be more plausible. Suppose the units are clusters of individuals, where in each period we observe
different sets of individuals. To be specific, suppose the the units are states, the time periods
are years, and outcome is the employment rate for twenty-five year olds, and the treatment is
the presence or absence of some regulation, say a subsidy for college tuition. In that case it may
well be reasonable to assume that the educational choices for students graduating high school
in a particular state depends on what the prevailing subsidy is, but much less on the presence
of subsidies in previous years.
   If both the exclusion restrictions, that is, both Assumptions 2 and 3, hold, then the potential
outcome Yit (a) can be indexed by the binary indicator W (a, t) = 1a≤t :

Lemma 2. (Binary Treatment) Suppose Assumptions 2 and 3 hold. Then for all units i,
all time periods t and adoption dates a > a0 , (i)

                                                          
           0
      Yit (a ) − Yit (a) = 1a0 ≤t<a       Yit (1) − Yit (∞) ,


so that,
                                                         
                                                       Yit (∞)                  if a ≤ t
      Yit (a) = Yit (∞) + 1a≤t       Yit (1) − Yit (∞) =
                                                          Y (1)                   otherwise,
                                                            it




                                                           9
and, for all time periods t, and adoption dates a > a0 , (ii)
                                  
                                   τ
                                      t,∞1   if a0 ≤ t < a,
      τt,aa0 = τt,∞1 1a0 ≤t<a   =
                                   0        otherwise.

   If these two assumptions hold, we can therefore simplify the notation for the potential out-
comes and focus on Yit (1) and Yit (∞).
   Note that these two assumptions are substantive, and cannot be guaranteed by design. This
in contrast to the Assumption 1, which can be guaranteed by randomization of the adoption
date. It is also important to note that in many empirical studies Assumptions 2 and 3 are
made, often implicitly by writing a model for realized outcome Yit that depends solely on the
contemporaneous treatment exposure Wit , and not on the actual adoption date Ai or treatment
exposure Wit0 in other periods t0 . In the current discussion we want to be explicit about the fact
that this restriction is an assumption, and that it does not automatically hold. Note that the
assumption does not restrict the time series dependence between the potential outcomes.
   It is trivial to see that without additional information, the exclusion restrictions in Assump-
tions 2 and 3 have no testable implications because they impose restrictions on pairs of potential
outcomes that cannot be observed together. However, in combination with random assignment,
2 and 3, there are testable implications as long as T ≥ 2 and there is some variation in the
adoption date.

Lemma 3. (Testable Restrictions from the Exclusion Restrictions) (i) Assump-
tions 2 and 3 jointly have no testable implications for the joint distribution of (Y , W ).
(ii) Suppose T ≥ 2, and π2 , π∞ > 0. Then the combination of Assumptions 1–3 impose testable
restrictions on the joint distribution of (Y , W ).


3.3    Auxiliary Assumptions
In this section we consider four auxiliary assumptions that are convenient for some analyses,
and in particular can have implications for the variance of specific estimators, but that are
not essential in many cases. These assumptions are often made in empirical analyses without
researchers explicitly discussing them.
   The first of these assumptions assumes that the effect of adoption date a0 , relative to adoption

                                                 10
date a, on the outcome in period t, is the same for all units.

Assumption 4. (Constant Treatment Effect Over Units) For all units i, j and for
all time periods t and all adoption dates a and a0


       Yit (a) − Yit (a0 ) = Yjt (a) − Yjt (a0 ).


    The second assumption restricts the heterogeneity of the treatment effects over time.

Assumption 5. (Constant Treatment Effect over Time) For all units i and all time
periods t and t0


       Yit (1) − Yit (∞) = Yit0 (1) − Yit0 (∞).


    We only restrict the time variation for comparisons of the adoption dates 1 and ∞ because
we typically use this assumption in combination with Assumptions 2 and 3. In that case we
obtain a constant binary treatment effect set up, as summarized in the following Lemma.

Lemma 4. (Binary Treatment and Constant Treatment Effects) Suppose Assump-
tions 2-5 hold. Then for all t and a0 < a


       Yit (a0 ) − Yit (a) = 1a0 ≤t<a τ1∞ .


    The final assumption allows us to view the potential outcomes as random by postulating a
large population from which the sample is drawn.

Assumption 6. (Random Sampling) The sample can be viewed as a random sampling from
an infinitely large population, with joint distribution for (Ai , Yit (a), a ∈ A, t ∈ T) denoted by
f (a, y1 (1), . . . , yT (∞)).

    Under this assumption we can put additional structure on average potential outcomes.

Assumption 7. (Additivity)


       E [Yit (∞)] = αi + βt .


                                                    11
4     Difference-In-Differences Estimators: Interpretation and
      Inference
In this section we consider the standard DID set up (e.g., Meyer et al. [1995], Bertrand et al.
[2004], Angrist and Pischke [2008], Donald and Lang [2007], de Chaisemartin and D’Haultfœuille
[2018]). In the simplest setting with N units and T time periods, without additional covariates,
the realized outcome in period t for unit i is modeled as


      Yit = αi + βt + τ Wit + εit .                                                                                  (4.1)


In this model there are unit effects αi and time effects βt , but both are additive with interactions
between them ruled out. The effect of the treatment is implicitly assumed to be additive and
constant across units and time periods.
    We interpret the DID estimand under the randomized adoption date assumption, leading to
a different setting from that considered in de Chaisemartin and D’Haultfœuille [2018], Abraham
and Sun [2018], Goodman-Bacon [2017]. We also derive its variance and show that in general
it is lower than the standard random-sampling based variance. Finally we propose a variance
estimator that is is smaller than the regular variance estimators such as the Liang-Zeger and
clustered bootstrap variance estimators.


4.1       Difference-In-Differences Estimators
Consider the least squares estimator for τ based on the specification in (4.1):

                                                                           N X
                                                                             X T
          τ̂did , {α̂i }N           T
                        i=2 , {β̂t }t=1       = arg          min                       (Yit − αi − βt − τ Wit )2 .
                                                      τ,{αi }N         T
                                                             i=2 ,{βt }t=1   i=1 t=1


It is convenient to write τ̂did in terms of the adjusted treatment indicator Ẇit as
                 P
                     i,t   Ẇit Yit
      τ̂did = P               2
                                      .
                       i,t Ẇit




                                                                       12
The primary question of interest in this section concerns the properties of the estimator τ̂did .
This includes the interpretation of its expectation under various sets of assumptions, and its
variance. Mostly we focus on exact properties in finite samples.
   In order to interpret the expected value of τ̂did we consider some intermediate objects. Define,
for all adoption dates a ∈ A, and all time periods t ∈ T the average of the outcome in period t
for units with adoption date a:
               
                    1
                        P
               
                   Na       i:Ai =a   Yit    if Na > 0,
     Y t,a =
                0                           otherwise.

Under Assumption 1 the stochastic properties of these averages are well-defined because the Na
are fixed over the randomization distribution. The averages are stochastic because the realized
outcomes depend on the adoption date. Define also the following two difference between outcome
averages:


     τ̂t,aa0 = Y t,a0 − Y t,a .


In general these differences do not have a causal interpretation. Such an interpretation requires
some assumptions, for example, on random assignment of the adoption date.
Example: To facilitate the interpretation of some of the results it is useful to consider a
special case where the results from completely randomized experiments directly apply. Suppose
T = {1, 2}, and A = {2, ∞}, with a fraction π = π2 = 1 − π∞ adopting the policy in the second
period. Suppose also that Yi1 (a) = 0 for all i and a. Then the DID estimator is

                                            1 X              1 X
     τ̂did = τ̂2,2∞ = Y 2,2 − Y 2,∞ =                 Yi2 −           Yi2 ,
                                            N2 i:A =2       N∞ i:A =∞
                                                  i               i



the simple difference in means for the second period outcomes for adopters and non-adopters.
Under Assumption 1, the standard results for the variance of the difference in means for a
randomized experiments apply (e.g., Neyman [1923/1990], Imbens and Rubin [2015]), and the




                                                        13
exact variance of τ̂did is,

                                     N                                            N
                         1     X                             2         1      X                                        2
        V(τ̂did ) =                Yi2 (2) − Y 2 (2)              +                Yi2 (∞) − Y 2 (∞)
                    N2 (N − 1) i=1                                  N∞ (N − 1) i=1

                                    N
                             1     X                                                       2
                       −                 Yi2 (2) − Y 2 (2) − Yi2 (∞) − Y 2 (∞)                    .
                         N (N − 1) i=1

The standard Neyman estimator for this variance ignores the third term, and uses unbiased
estimators for the first two terms, leading to:

                            1        X                   2            1        X                               2
        V̂(τ̂did ) =                      Yi2 − Y 2,2         +                      Yi2 − Y 2,∞                     .
                       N2 (N2 − 1) i:A =2                         N∞ (N∞ − 1) i:A =∞
                                         i                                         i






4.2      The Interpretation of Difference-In-Differences Estimators
The following weights play an important role in the interpretation of the DID estimand:

                          π g(t, a)
                         Pa
                                                                  X                               X
        γt,a ≡ P                        0 0 2
                                               ,    γt,+ ≡              γt,a ,   and γt,− ≡             γt,a ,               (4.2)
                   t0 ∈T   a0 ∈A πa0 g(t , a )                    a≤t                             a>t


with g(a, t) as defined in (3.1). Note that these weights are non-stochastic, that is, fixed over
the randomization distribution.
Example (ctd): Continuing the example with two periods and adoption in the second period
or never, we have in that case
          
          
          
           0              if (t, a) = (1, 1),
          
          
            0              if (t, a) = (2, 1),
          
          
          
          
          
                                                                                                  
           −1
          
                           if (t, a) = (1, 2),               0 if t = 1,                            0  if t = 1,
 γt,a   =                                          γt,+   =                            and γt,−   =
          
           1              if (t, a) = (2, 2),               1 if t = 2,                            −1 if t = 2.
          
          
          
                           if (t, a) = (1, ∞),
          
          
          
           1
          
          
           −1
          
                           if (t, a) = (2, ∞),



                                                          14

    The weights γt,a have some important properties,

      X                       X                                      T X
                                                                     X                   X              X
            γt,+ = 1                γt,− = −1,            and                  γt,a =          γt,+ +         γt,− = 0.
      t∈T                     t∈T                                    t=1 a∈A             t∈T            t∈T


Now we can state the first main result of the paper.

Lemma 5. We can write τ̂did as

                XX                       X                     XX                        XX
      τ̂did =             γt,a Y t,a =         γt,+ τ̂t,∞1 +             γt,a τ̂t,∞a −             γt,a τ̂t,a1 .          (4.3)
                t∈T a∈A                  t∈T                   t∈T a>t                   t∈T a≤t


Comment 1. Alternative characterizations of the DID estimator or estimand as a weighted
average of potentially causal comparisons are presented in Abraham and Sun [2018], de Chaise-
martin and D’Haultfœuille [2018], Han [2018], Goodman-Bacon [2017], and Borusyak and Jaravel
[2016]). The characterizations differ in terms of the building blocks that are used in the repre-
sentation and the assumptions made. Like our representation, the representation in Abraham
and Sun [2018] is in terms of average causal effects of different adoption dates, but it imposes
no-anticipation. Goodman-Bacon [2017] presents the DID estimator in terms of basic two-group
DID estimators. Like our representation, the Goodman-Bacon [2017] is mechanical and does
not rely on any assumptions. To endow the building blocks and the representation itself with a
causal interpretation requires some assumption on, for example, the assignment mechanism. 

Comment 2. The lemma implies that the DID estimator has an interpretation as a weighted
average of simple estimators for the causal effect of changes in adoption dates, the τ̂t,aa0 . More-
over, the estimator can be written as the sum of three averages of these τ̂t,aa0 . The first is a
weighted average of the τ̂t,∞1 , which are all averages of switching from never adopting to adopt-
ing in the first period, meaning that these are averages of changes in adoption dates that involve
switching from not being treated at time t to being treated at time t. The sum of the weights
for these averages is one, although not all the weights are necessarily non-negative. The second
sum is a weighted sum of τ̂t,∞a , for a > t, so that the causal effect always involves changing the
adoption date from never adopting to adopting some time after t, meaning that the comparison
is between potential outcomes neither of which involves being treated at the time. The sum of
the weights for these averages is one again. The third sum is a weighted sum of τ̂t,a1 , for a ≤ t,

                                                                15
so that the causal effect always involves changing the adoption date from adopting prior to,
or at time, t relative to adopting at the initial time, meaning that the comparison is between
potential outcomes both of which involves being treated at the time. These weights sum to
minus one. 

   If we are willing to make the random adoption date assumption we can give this representa-
tion a causal interpretation:

Theorem 1. Suppose Assumption 1 holds. Then (i):


      E [τ̂t,aa0 ] = τt,aa0 ,


and (ii)

                     X                    XX                       XX
      E [τ̂did ] =         γt,+ τt,∞1 +             γt,a τt,∞a −             γt,a τt,a1 .
                     t∈T                  t∈T a>t                  t∈T a≤t


Suppose also Assumption 2 holds. Then (iii):

                     XX
      E [τ̂did ] =              γt,a τt,∞a .
                     t∈T a≤t


Suppose also Assumption 3 holds. Then (iv):

                     T
                     X
      E [τ̂did ] =         γt,+ τt,∞1 .
                     t=1


Suppose also Assumption 5 holds. Then (v):


      E [τ̂did ] = τ∞1 .


   Part (iii) of the theorem where we make the no-anticipation assumption is closely related to
one of the results in Abraham and Sun [2018], who make a super-population common trend as-
sumption that, in the super-population context, weakens our random adoption date assumption.
Part (iv) of the theorem, where we assume both the exclusion restrictions so that the treatment
is effectively a binary one, is related to the results in de Chaisemartin and D’Haultfœuille [2018],

                                                              16
although unlike those authors we do not restrict the trends in the potential outcomes.
   Without either Assumptions 2 or 3, the estimand τdid has a causal interpretation, but it
is not clear it is a very interesting one concerning the receipt of the treatment. With the no-
anticipation assumption (Assumption 2), the interpretation, as given in part (iii) of the theorem,
is substantially more interesting. Now the estimand is a weighted average of τt,∞a for a ≤ t,
with weights summing to one. These τt,∞a are the average causal effect of changing the adoption
date from never adopting to some adoption date prior to, or equal to, time t, so that the average
always involves switching from not being exposed to the treatment to being exposed to the
treatment.


4.3     The Randomization Variance of the Difference-In-Differences Es-
        timators
In this section we derive the randomization variance for τ̂did under the randomized adoption
date assumption. We do not rely on other assumptions here, although they may be required for
making the estimand a substantively interesting one. The starting point is the representation
       P
τ̂did = t,a γt,a Y t,a . Because under Assumption 1 the weights γt,a are fixed, the variance is

                    X                            X
                           2
      V(τ̂did ) =         γt,a V(Y t,a ) +                      γt,a γt0 ,a0 C(Y t,a , Y t0 ,a0 ).
                    t,a                      (t,a)6=(t0 ,a0 )



Note that the γt,a are known. Working out the variance V(Y t,a ), and finding an unbiased estima-
tor for it, is straightforward. It is more challenging to infer the covariance terms C(Y t,a , Y t0 ,a0 ),
and even more difficult to estimate them. In general that is not possible. Note that for a
sampling-based variance the γt,a are not fixed, because in different samples the fractions with
a particular adoption date will be stochastic. This in general leads to a larger variance, as we
verify in the simulations.
   Define

                 T
                 X                                         T
                                                           X                                          T
                                                                                                      X
      Yi (a) =         γt,a Yit (a),         Y (a) =                γt,a Y t (a)          and Y a =         γt,a Y t,a .
                 t=1                                        t=1                                       t=1




                                                                      17
Now we can write τ̂ did as

                  XX                        X
       τ̂ did =              γt,a Y t,a =         Y a.
                  a∈A t∈T                   a∈A


Define also

                            N
                 1 X                   2
       Sa2   =           Yi (a) − Y (a) ,
               N − 1 i=1

and

                         N
                    1 X                                                               2
         2
                            Yi (a) − Y (a) + Yi (a0 ) − Y (a0 )
                                                              
       Va,a 0 =                                                                            .
                  N − 1 i=1

Theorem 2. Suppose Assumptions 1 holds. Then the exact variance of τ̂did over the random-
ization distribution is

                                                                              2
                                                                            Va,a
                                                     
                       X               1    T −1              X      X           0
       V (τ̂did ) =          Sa2          +               −                        ,
                       a∈A
                                       Na     N                    0    0
                                                              a∈A a ∈A,a >a
                                                                             N


with

                       X
       V (τ̂did ) ≤          Sa2 /Na .
                       a∈A


Comment (ctd): In our two period example with some units adopting in the second period
and the others not at all, and Yi1 (a) = 0, we have
                                                                                          
                   0                            −1                                         1
       γ1 =           ,              γ2 =          ,            and γ∞ =                  .
                   0                              1                                    −1

       S12 = 0,
                     N               N
                                               !2
                1   X             1 X
       S,22 =           Yi2 (2) −       Yj2 (2) ,
              N − 1 i=1           N j=1



                                                                    18
                    N               N
                                              !2
       2       1 X               1 X
      S∞   =           Yi2 (∞) −       Yj2 (∞) ,
             N − 1 i=1           N j=1

        2             2
      V1,2 = 0,      S1,∞ = 0,
                      N                   N
                                                    !                   N
                                                                                  !!2
        2        1 X                   1 X                           1 X
      V2,∞   =               Yi2 (2) −       Yi2 (2) −     Yi2 (∞) −       Yi2 (∞)    ,
               N − 1 i=1               N j=1                         N j=1

so that in this special

                              N               N
                                                        !2
                       1     X             1 X
      V(τ̂did ) =                Yi2 (2) −       Yi2 (2)
                  N2 (N − 1) i=1           N j=1


                                N               N
                                                          !2
                        1      X             1 X
                  +                Yi2 (∞) −       Yi2 (∞)
                    N∞ (N − 1) i=1           N j=1

                                 N                  N
                                                              !                   N
                                                                                            !!2
                        1     X                  1 X                           1 X
                  −                    Yi2 (2) −       Yi2 (2) −     Yi2 (∞) −       Yi2 (∞)    ,
                    N (N − 1) i=1                N j=1                         N j=1

which agrees with the Neyman variance for a completely randomized experiment. 


4.4    Estimating the Randomization Variance of the Difference-In-
       Differences Estimators
In this section we discuss estimating the variance of the DID estimator. In general there is
no unbiased estimator for V (τ̂did ). This is not surprising, because there is no such estimator
for the simple difference in means estimator in a completely randomized experiment, and this
corresponds to the special case with T = 1. However, it turns out that just like in the simpled
randomized experiment case, there is a conservative variance estimator. In the current case
it is based on using unbiased estimators for the terms involving Sγ2a ,a , and ignoring the terms
involving Vγ2a ,a,γa0 ,a0 . Because the latter are non-negative, and enter with a minus sign, ignoring
them leads to an upwardly biased variance estimator. One difference with the simple randomized
experiment case is that there is no simple case with constant treatment effects such that the
variance estimator is unbiased.



                                                 19
   Next, define the estimated variance of this by adoption date:

                1      X            2
      s2a ≡                 Yi − Y a .
              Na − 1 i:A =a
                           i



Now we can characterize the proposed variance estimator as

                X s2
      b did ≡        a
      V                .
                a∈A
                    Na



Theorem 3. Suppose Assumption 1 holds. Then

       h      i
      E Vb did ≥ V(τ̂did ),


so that V
        b did is a conservative variance estimator for τ̂did .

   There are two important issues regarding this variance estimator. The first is its relation to
the standard variance estimator for DID estimators. The second is whether one can improve on
this variance estimator given that in general it is conservative.
   The relevant variance estimators are the Liang-Zeger clustered variance estimator and the
clustered bootstrap (Bertrand et al. [2004], Liang and Zeger [1986]). Both have large sample
justifications under random sampling from a large population, so they are in general not equal
to the variance estimator here. In large samples both the Liang-Zeger and bootstrap variance
will be more conservative than V̂did because they also take into account variation in the weights
γt,a . These weights are kept fixed under the randomization scheme, because that keeps fixed
the marginal distribution of the adoption dates. In contrast, under the Liang-Zeger calculations
and the clustered bootstrap, the fraction of units with a particular adoption date varies, and
that introduces additional uncertainty.
   The second issue is whether we can improve on the conservative variance estimator V̂did .
In general there is only a limited ability to do so. Note, for example, that in the two period
example this variance reduces to the Neyman variance in randomized experiments. In that case
we know we can improve on this variance a little bit exploiting heteroskedasticity, e.g., Aronow
et al. [2014], but in general those gains are modest.




                                                  20
5      Some Simulations
The goal is to compare the exact variance, and the corresponding estimator in the paper to
the two leading alternatives, the Liang-Zeger (stata) clustered standard errors and the clustered
bootstrap. We want to confirm settings where the proposed variance estimator differs from
the Liang-Zeger clustered variance, and settings where it is the same. We have N units, ob-
served for T time periods. We focus primarily on the case with T = 3. The adoption date is
randomly assigned, with πI = (π1 , π2 , π3 , π∞ ) = (0, 0.67, 0, 0.33), and πII = (π1 , π2 , π3 , π∞ ) =
(0, 0.5, 0.4, 0.1).
    We consider two designs for the potential outcome distributions in the population, the Yi (a)
for a ∈ {1, 2, 3, ∞}. In design A the potential outcomes, are generated as
                                                                    
           Yi1 (2)                   0           1 0 0 0 0 0 0 0 0
                                                                    
           Yi1 (3)                   0           0 1 0 0 0 0 0 0 0
                                                                    
                                                                    
                                                                    
           Yi1 (∞)                   0           0 0 1 0 0 0 0 0 0
                                                                    
                                                                    
                                                                    
           Yi2 (2)                   4           0 0 0 1 0 0 0 0 0
                                                                    
                                                                    
                                                                    
                                          2
                      ∼N                ,σ                              .
                                                                      
          Yi2 (3)                 3           0 0 0 0 1 0 0 0 0
                                                                    
                                                                    
          Yi2 (∞)                3         0 0 0 0 0 1 0 0 0        
                                                                    
                                                                    
          Yi3 (2)                2         0 0 0 0 0 0 1 0 0        
                                                                    
                                                                    
          Yi3 (3)                2         0 0 0 0 0 0 0 1 0        
                                                                    
           Yi3 (∞)                   1           0 0 0 0 0 0 0 0 1

    In this design the treatment effect is constant, and depends only on whether the adoption
date preceeds the potential outcome date, or


       Yit (a) = 1a≤t + εit ,


where the εit are correlated over time.
    In design B the potential outcomes are generated as




                                                    21
                                                                        
         Yi1 (2)                0            1     0       0 0 0 0 0 0 0
                                                                        
         Yi1 (3)                0            0 10 0 0 0 0 0 0 0
                                                                        
                                                                        
                                                                        
         Yi1 (∞)                0            0     0       1 0 0 0 0 0 0
                                                                        
                                                                        
                                                                        
         Yi2 (2)                2            0     0       0 1 0 0 0 0 0
                                                                        
                                                                        
                                                                        
                                      2
                   ∼N               ,σ                                      .
                                                                          
        Yi2 (3)              1            0     0       0 0 1 0 0 0 0
                                                                        
                                                                        
        Yi2 (∞)             1          0     0       0 0 0 1 0 0 0      
                                                                        
                                                                        
        Yi3 (2)             2          0     0       0 0 0 0 1 0 0      
                                                                        
                                                                        
        Yi3 (3)             11         0     0       0 0 0 0 0 1 0      
                                                                        
         Yi3 (∞)                1            0     0       0 0 0 0 0 0 1

   Here the treatment effects depend on the treatment having been adopted, but the effect
differs by the adoption date.
   In design C the potential outcomes are generated with positive correlations between the
potential outcomes as


                                                                                       
         Yi1 (2)                0            1         0.9 0.9    0   0   0    0     0   0
                                                                                 
         Yi1 (3)                0            0.9       1    0.9   0   0   0    0     0
                                                                                    0 
                                                                                 
                                    
                                                                                 
         Yi1 (∞)                0            0.9 0.9         1     0   0  0  0   0  0 
                                                                                 
                                    
                                                                                 
         Yi2 (2)                2            0         0     0     1 0.9 0.9 0   0  0 
                                                                                 
                                    
                                                                                 
                                      2
                   ∼N               ,σ                                           0  .
                                                                                   
        Yi2 (3)              1            0         0     0    0.9 1 0.9 0    0
                                                                                 
                                                                                 
        Yi2 (∞)             1          0         0     0    0.9 0.9 1  0   0  0 
                                                                                 
                                                                                 
        Yi3 (2)             2          0         0     0     0   0  0  1 0.9 0.9 
                                                                                 
                                                                                 
        Yi3 (3)             11         0         0     0     0   0  0 0.9 1 0.9 
                                                                                 
         Yi3 (∞)                1            0         0     0     0   0  0 0.9 0.9 1

   In design D the potential outcomes are generated with negative correlations between the
potential outcomes as




                                                   22
                                                                                                                            
    Yi1 (2)                      0          1   −0.4 −0.4  0    0    0    0    0    0
                                                                                  
    Yi1 (3)                      0        −0.4  1   −0.4  0    0    0    0    0    0 
                                                                                  
                                 
                                                                                  
    Yi1 (∞)                      0        −0.4 −0.4  1    0    0    0    0    0    0 
                                                                                  
                                 
                                                                                  
    Yi2 (2)                      2        0     0    0    1   −0.4 −0.4  0    0    0 
                                                                                  
                                 
                                                                                  
                                      2
              ∼N                    ,σ  0              −0.4      −0.4            0  .
                                                                                    
   Yi2 (3)                    1               0    0         1         0    0
                                                                                  
                                                          −0.4 −0.4
                                                                                  
   Yi2 (∞)                   1       0     0    0              1    0    0    0 
                                                                                  
                                                                              −0.4 −0.4 
                                                                                  
   Yi3 (2)                   2       0     0    0    0    0    0    1
                                                                                  
                                                                         −0.4      −0.4 
                                                                                  
   Yi3 (3)                  11       0     0    0    0    0    0         1
                                                                                  
    Yi3 (∞)                      1          0    0    0    0    0    0   −0.4 −0.4  1

    For a particular design, eg (A, 2) draw the four sets of three-component vectors of potential
outcomes for each unit (the three components corresponding to the three time periods), one set
for each of the values of a ∈ {1, 2, 3, ∞}. We keep these sets of potential outcomes fixed across
all simulations for a given design. Then for each simulation draw the adoption date according
to the distribution for that design, keeping the fraction of units with a particular adoption date
fixed.
    We want to look at variances and the corresponding confidence intervals based on four
methods for estimating the variance for the DID estimator. The confidence intervals are Normal-
distribution based, simply equal to the point estimates plus and minus 1.96 times the square
root of the variances. We can write τ̂did as a regression estimator with N T observations, and
N + T regressors. Let with j = 1, . . . , N T . For observation j, Tj ∈ {1, . . . , T } denotes the time
period the observation is from, and Nj ∈ {1, . . . , N } denotes the unit is corresponds to. Now
let Yj = YNj ,Tj and Wj = WNj ,Tj , so that the regression function can be written as

                    N
                    X −1                 T −1
                                         X
         Yj = µ +          αn 1Nj =n +          βt 1Tj =t + τ Wj + εj = Yj = Xj> θ + εj ,
                     n=1                 t=1


where Xj = (1, 1Nj =1 , . . . , 1Nj =N −1 , , 1Tj =1 , . . . , 1Tj =T −1 , Wj ), and θ = (µ, α1 , . . . , αN −1 , β1 , . . . , βT −1 , τ ).




                                                               23
    We compare five variances. The first is exact randomization-based variance,

                         X Sγ2
                                 a ,a
                                            X        X          Vγ2a ,a,γa0 ,a0
      Vdid = V (τ̂ ) =                  −                                         .
                         a∈A
                                Na          a∈A   a0 ∈A,a0 >a
                                                                     N


The other four are estimators of the variance.
    First, the feasible conservative variance estimator V
                                                        b did .
    Second, the standard Liang-Zeger clustered variance. Start with the representation Yj =
Xj> θ + εj . Let ε̂j = Yj − Xj> θ̂ be the residual from this regression. Calculate the variance as

                                                               > 
                                                                    
               J
                               !−1     N                              J
                                                                               !−1
               X                       X   X            X             X
      b LZ =          Xj Xj>                 Xj ε̂j     Xj ε̂j      Xj Xj>     ,
                                                                   
      V                                 
                j=1                      n=1       j:Nj =n                 j:Nj =n    j=1



and get the corresponding variance estimator for τ̂did .
   Third, the clustered bootstrap, V
                                   b B1 . Draw bootstrap samples based on drawing units, with
all time periods for each unit drawn. Note that this explicitly changes from bootstrap sample
to bootstrap sample the fraction of units with a particular adoption date.
   Fourth, a modification of the clustered bootstrap, V b B2 , where we fix the fraction of units
with each value for the adoption date.
    In Table 1 we report the results. For each of the five variances we report the average of
variance, and the coverage rate for the 95% confidence interval.
    We see that the standard Liang-Zeger and the clustered bootstrap (V
                                                                      b B1 ) substantially over-
estimate the variance in Design B. The fixed adoption date bootstrap (Vb B2 ) and the proposed
variance estimator (V
                    b did ) have the appropriate coverage.



6     Conclusion
We develop a design-based approach to Difference-In-Differences estimation in a setting with
staggered adoption. We characterize what the standard DID estimator is estimating under a
random adoption date assumption, and what the variance of the standard estimator is. We show
that the standard DID estimatand is a weighted average of different types of causal effects,
for example, the effect of changing from never adopting to adopting in the first period, or


                                                                  24
                                     Table 1: : Simulations

 Design   π      N    Vdid    Cov     V̂did    Cov     V̂LZ    Cov     V̂B1    Cov     V̂B2    Cov
 A        I     30   0.144   0.951   0.239    0.979   0.214   0.974   0.232   0.975   0.219   0.973
 B        I     30   0.111   0.947   0.187    0.986   0.163   0.978   0.182   0.982   0.172   0.978
 C        I     30   0.201   0.953   0.217    0.947   0.181   0.925   0.211   0.942   0.200   0.932
 D        I     30   0.064   0.949   0.265    1.000   0.230   0.999   0.257   1.000   0.244   0.999
 A        II    30   0.112   0.946   0.165    0.972   0.146   0.966   0.158   0.969   0.142   0.956
 B        II    30   0.085   0.947   0.139    0.973   0.268   0.999   0.269   0.999   0.119   0.962
 C        II    30   0.184   0.949   0.191    0.939   0.279   0.983   0.285   0.981   0.162   0.920
 D        II    30   0.081   0.950   0.164    0.992   0.285   1.000   0.280   0.999   0.142   0.987
 A        I    150   0.027   0.953   0.047    0.991   0.045   0.989   0.047   0.989   0.046   0.989
 B        I    150   0.022   0.955   0.041    0.994   0.039   0.992   0.041   0.992   0.041   0.992
 C        I    150   0.035   0.956   0.038    0.960   0.036   0.956   0.037   0.955   0.037   0.954
 D        I    150   0.019   0.950   0.044    0.997   0.044   0.997   0.044   0.996   0.043   0.995
 A        II   150   0.020   0.952   0.033    0.989   0.033   0.989   0.033   0.987   0.032   0.987
 B        II   150   0.021   0.945   0.036    0.985   0.053   0.997   0.052   0.997   0.035   0.984
 C        II   150   0.034   0.952   0.035    0.953   0.051   0.985   0.052   0.983   0.034   0.947
 D        II   150   0.016   0.950   0.028    0.990   0.044   0.998   0.044   0.998   0.028   0.987



changing from never adopting to adopting later. In this approach the standard Liang-Zeger
and clustered bootstrap variance estimators are unnecessarily conservative, and we propose an
improved variance estimator.




                                                 25
                                                           Appendix
Proof of Lemma 1: Let Y p denote the N × (T · (T + 1)) dimensional matrix with all the potential
outcomes. Because the units are exchangeable we can write the joint distribution of the potential
outcomes and A as

                        N
                              f (Yip , Ai ).
                        Y
             p
        f (Y , A) =
                        i=1


Now we shall construct a distribution f (Yip , Ai ) that satisfies two conditions. First, Ai is independent of
all the potential outcomes and second, the implied distribution for the adoption date and the realized
outcome is consistent with the actual distribution. To do so we assume independence of the sets
potential outcomes Yi1 (a), . . . , fiT (a) for different a, and assume that


        f (Yi1 (a), . . . , fiT (a)) = f (Yi1 (a), . . . , fiT (a)|Ai = a) = f (Yi1 , . . . , fiT |Ai = a).



Proof of Lemma 2: By Assumption 2 we have


        Yit (a) = 1a≤t Yit (a) + 1a>t Yit (∞),


and by Assumption 3 we have


        Yit (a) = 1a≤t Yit (1) + 1a>t Yit (a).


Combining the two assumptions implies


        Yit (a) = 1a≤t Yit (1) + 1a>t Yit (∞).


Hence


        Yit (a0 ) − Yit (a) = 1a0 ≤t Yit (1) + 1a0 >t Yit (∞) − (1a≤t Yit (1) + 1a>t Yit (∞))


                 = 1a0 ≤t<t (Yit (1) − Yit (∞)) ,

which proves part (i).

                                                                26
For part (ii)

                     N
                   1 X                     
        τt,aa0 =        Yit (a0 ) − Yit (a)
                   N
                      i=1


                     N
                   1 X
              =        1a0 ≤t<t (Yit (1) − Yit (∞))
                   N
                      i=1

                                N
                              1 X
              =1    a0 ≤t<t       (Yit (1) − Yit (∞)) = 1a≤t<a0 τt,∞1 .
                              N
                                 i=1


Proof of Lemma 3: Part (i) follows directly from the fact that the exclusion restrictions place
restrictions only on potential outcomes that cannot be observed together.
Let us turn to part (ii). By assumption


        Yit (a) ⊥
                ⊥ Ai ,


which as a special case includes


        Yi1 (∞) ⊥
                ⊥ Ai .


Hence


        Yi1 (∞) ⊥
                ⊥ Ai           Ai ≥ 2


which implies


        Yi1 ⊥⊥ Ai       Ai ≥ 2


and thus


        Yi1 ⊥⊥ Ai       Ai ∈ {2, ∞},


which is a testable restriction. 



                                                           27
Proof of Lemma 4: By Assumptions 2 and 3 we have

                                               
      Yit (a) − Yit (∞) = 1a≤t Yit (1) − Yit (∞) .


By Assumptions 4 and 5, Yit (1) − Yit (∞) = τ1∞ , so that


      Yit (a) − Yit (∞) = 1a≤t τ1∞ .



Proof of Lemma 5: Using the definition for g(t, a), we can write τ̂did as

                P                 PP     P                   P P      P
                i,t Ẇit Yit       t∈T
                                     a∈A  i:Ai =a Ẇit Yit    t∈T            g(a, t)Yit
      τ̂did =                  =                       2
                                                           =     P P i:Ai =a
                                                                  a∈A
                                                              N t∈T a∈A πa g(a, t)2
                                  P P
                         2       N t∈T a∈A πa g(a, t)
                P
                  i,t Ẇit

                P   P     P
                    t∈T             g(a, t)Na Y t,a
             =        P Pi:Ai =a
                      a∈A
                   N t∈T a∈A πa g(a, t)2
               P P
                      a∈A g(a, t)πa Y t,a
                                            X
                t∈T
             = P      P               2
                                          =     γt,a Y t,a ,
                  t∈T   a∈A πa g(a, t)      t,a

where γt,a is as given in (4.2). 
Proof of Theorem 1: First consider part (i). We will show that


      E[Y ta ] = Y t (a),


which in turn implies the result in (i). We can write

                      N                     N
                    "             #    "                    #
                   1 X                   1 X
      E[Y ta ] = E      1Ai =a Yit = E        1Ai =a Yit (a) .
                   Na                    Na
                            i=1                    i=1


By Assumption 1 this is equal to

         N                          N               N
      1 X                        1 X Na           1 X
           E [1Ai =a ] Yit (a) =        Yit (a) =     Yit (a) = Y t (a),
      Na                         Na   N           N
          i=1                            i=1                   i=1


which is the desired result.




                                                         28
Next consider part (ii). By Lemma 5,

                X                      XX                           XX
      τ̂did =         γt,+ τ̂t,∞1 +              γt,a τ̂t,∞a −                γt,a τ̂t,a1 ,
                t∈T                    t∈T a>t                      t∈T a≤t


so that
                                                                     
                      X               XX               XX
      E [τ̂did ] = E   γt,+ τ̂t,∞1 +    γt,a τ̂t,∞a −    γt,a τ̂t,a1  ,
                            t∈T                 t∈T a>t                       t∈T a≤t


which by Assumption 1 is equal to

      X                           XX                                XX
            γt,+ E [τ̂t,∞1 ] +               γt,a E [τ̂t,∞a ] −               γt,a E [τ̂t,a1 ] .
      t∈T                         t∈T a>t                           t∈T a≤t


This in turn, by part (i), is equal to

      X                      XX                         XX
            γt,+ τt,∞1 +                γt,a τt,∞a −                γt,a τt,a1 ,
      t∈T                     t∈T a>t                    t∈T a≤t


which finishes the proof of part (ii).
Next consider part (iii). If Assumption 2 holds, then for all a > t, τt,∞a = 0, so that

                      X                      XX
      E [τ̂did ] =          γt,+ τt,∞1 −               γt,a τt,a1
                      t∈T                    t∈T a≤t

                  XX
              =               γt,a τt,∞a .
                  t∈T a≤t

Next consider part (iv). If also Assumption 3 holds, then also for all a ≤ t, τt,a1 = 0, so that

                      X                      XX                        XX
      E [τ̂did ] =          γt,+ τt,∞1 +               γt,a τt,∞a −                γt,a τt,a1
                      t∈T                    t∈T a>t                   t∈T a≤t

                  X
              =         γt,+ τt,∞1 .
                  t∈T

Finally, consider part (v). This follows directly from part (iv) in combination with the constant
treatment effect assumption (Assumption 5). 
Next we give a preliminary result.


                                                                       29
Lemma A.1. Suppose that Assumption 1 holds. Then (i) the variance of Y a is

                    Sa2
                                 
                               Na
      V(Y a ) =             1−      ,
                    Na         N

(ii), the covariance of Y a and Y a0 is

                                1                        1
                                  Sa2 + Sa20 − Saa
                                                2
                                                           Sa2 + Sa20 − Vaa
                                                                          2
                                                                            
      C(Y a , Y a0 ) = −                          0   =                     0 ,
                               2N                       2N

(iii), the variance of the sum of the Y a is
                       !                                  
            X                  X               1    T −1            1      X
      V           Ya       =         Sa2          +            −                        2
                                                                                      Vaa 0,
                                               Na     N            2N
            a∈A                a∈A                                      a,a0 :a6=a0


and (iv),
                       !
            X                  X S2
                                  a
      V           Ya       ≤        .
                                 Na
            a∈A                a∈A


Proof of Lemma A.1: Part (i) follows directly from the variance of a sample average with random
sampling from a finite population.
Next consider part (ii). Define

                       N
                    1 X
       2
                         Yi (a0 ) − Y (a0 ) − Yi (a) − Y (a) .
                                                           
      Saa0 =
                  N −1
                           i=1


Recall that the variance of the difference between Y a0 and Y a is

                               Sa2  S 20 S2 0
      V(Y a0 − Y a ) =             + a − aa ,
                               Na Na0     N

from the results in Neyman [1923/1990], Imbens and Rubin [2015] for completely randomized experi-
ments with a binary treatment. In general it is also true that


      V(Y a0 − Y a ) = V(Y a ) + V(Y a0 ) − 2C(Y a , Y a0 ).


Combining these two characterizations of the variance of the standard estimator for the average treat-



                                                                    30
ment effect, it follows that the covariance is equal to

                             1
      C(Y a , Y a0 ) =          V(Y a ) + V(Y a0 ) − V(Y a0 − Y a )
                             2

                          Sa2              Sa20                  Sa20    2 
                                                       2
                 1                  Na               Na0    Sa          Saa0
               =                 1−      +        1−      −    +      −
                 2        Na        N      Na0       N      Na Na0       N
                    1  2
               =−      Sa + Sa20 − Saa2
                                         0
                   2N
                    1  2
               =−      Sa + Sa20 + Vaa 2       2     2
                                         0 − 2Sa − 2Sa0
                   2N
                  1  2
               =      Sa + Sa20 − Vaa
                                    2
                                      0    .
                 2N

Next, consider part (iii). Using the result in part (ii),
                      !
           X                    X                        X
      V          Ya       =           V(Y a ) +                      C(Y a , Y a0 )
           a∈A                  a∈A                  a,a0 :a6=a0


                 X S2    Na
                             
                                  1                                  X
                    a                                                                             2
                                                                                   Sa2 + Sa20 − Vaa
                                                                               
               =       1−      +                                                                    0
                   Na     N      2N
                   a∈A                                           a,a0 :a6=a0
                                                            
                   X               1   1   T                          1        X
               =         Sa2         −   +                       −                         2
                                                                                         Vaa 0
                                   Na N    N                         2N
                   a∈A                                                    a,a0 :a6=a0
                                                        
                   X               1    T −1                      1       X
               =         Sa2          +                      −                          2
                                                                                      Vaa 0.
                                   Na     N                      2N
                   a∈A                                                 a,a0 :a6=a0

                                                          2 terms is not directly estimable. Because it
Finally, consider part (iv). The third term, the sum of Vaa0


has a negative sign, we need to find a lower bound on this sum. A trivial lower bound is zero, but we
can do better. We will show that

       1       X
                           2
                                      X           T −1
                         Vaa 0 ≥            Sa2        .                                                (A.1)
      2N                                            N
           a,a0 :a6=a0                a∈A


This in turn implies

           1       X
                               2
                                            X            T −1
      −                      Vaa 0 ≤ −             Sa2        ,
          2N                                               N
               a,a0 :a6=a0                  a∈A




                                                                                    31
and thus
                            !                                          
                X                   X                 1    T −1                    1 X 2
        V              Ya       =           Sa2          +                   −          Vaa0
                                                      Na     N                    2N  0
                a∈A                 a∈A                                               a6=a

                                                             
                       X                1    T −1                     X           T −1
                  ≤          Sa2           +                      −         Sa2
                                        Na     N                                    N
                       a∈A                                            a∈A

                       X S2
                          a
                  =         .
                         Na
                       a∈A

The last inequality to prove is (A.1). First,

                                    N
                    1 X                                                        2
          2
                                            Yi (a0 ) − Y (a0 ) + Yi (a) − Y (a)
                                                              
        Vaa 0   =
                  N −1
                                i=1


                         N
                      1 Xn                   2               2                                    2 o
                           Yi (a0 ) − Y (a0 ) + Yi (a) − Y (a) + 2 Yi (a0 ) − Y (a0 ) Yi (a) − Y (a)
                                                                                     
                  =
                    N −1
                                    i=1

                        1
                          Sa2 + Sa20 + 2C(Yi (a), Yi (a0 )) .
                                                           
                  =
                        N

Hence

         1 X 2        1                         X
                                                              Sa2 + Sa20 + 2C(Yi (a), Yi (a0 ))
                                                          
              Vaa0 =
        2N  0
                     2N
                a6=a                        a,a0 :a6=a0


                       X            T   1 X
                  =          Sa2      +      C(Yi (a), Yi (a0 )).                                            (A.2)
                                    N   N  0
                       a∈A                         a6=a

Next,
                                        !
                       X                          X                          X
        0≤V                  Yi (a)         =           V(Yi (a)) +                   C(Yi (a), Yi (a0 )).
                       a∈A                        a∈A                   a,a0 :a6=a0


Therefore

            X                                             X                            X
                       C(Yi (a), Yi (a0 )) ≥ −                    V(Yi (a)) = −              Sa2 .           (A.3)
        a,a0 :a6=a0                                       a∈A                          a∈A




                                                                                    32
Combining (A.2) and (A.3) we get the bound

       1      X                      X           T   1      X
                           2
                         Vaa 0 =           Sa2     +                   C(Yi (a), Yi (a0 ))
      2N                                         N   N
           a,a0 :a6=a0               a∈A                 a,a0 :a6=a0


                 X             T   X       X T −1
             ≥           Sa2     −   Sa2 =  Sa2   ,
                               N                N
                 a∈A                  a∈A          a∈A

which proves (A.1). 
Proof of Theorem 2: This follows directly from the results in Lemma A.1. 
Proof of Theorem 3: By Assumption 1 it follows that


      E s2γa ,a = Sγ2a ,a .
              



This implies that
                  "              #
       h     i      X               X
      E V̂did = E     s2γa ,a /Na =   Sγ2a ,a /Na ≥ V(τ̂did ),
                               a∈A                   a∈A


where the inequality is by Theorem 2. 




                                                                          33
References
Alberto Abadie. Semiparametric difference-in-differences estimators. The Review of Economic Studies,
  72(1):1–19, 2005.

Alberto Abadie, Alexis Diamond, and Jens Hainmueller. Synthetic control methods for comparative
  case studies: Estimating the effect of California’s tobacco control program. Journal of the American
  Statistical Association, 105(490):493–505, 2010.

Alberto Abadie, Susan Athey, Guido Imbens, and Jeffrey Wooldrige. Clustering as a design problem.
  2016.

Alberto Abadie, Susan Athey, Guido W Imbens, and Jeffrey M Wooldridge. Sampling-based vs. design-
  based uncertainty in regression analysis. arXiv preprint arXiv:1706.01778, 2017.

Sarah Abraham and Liyang Sun. Estimating dynamic treatment effects in event studies with hetero-
  geneous treatment effects. 2018.

Joshua Angrist and Alan Krueger. Empirical strategies in labor economics. Handbook of Labor
  Economics, 3, 2000.

Joshua Angrist and Steve Pischke. Mostly Harmless Econometrics: An Empiricists’ Companion. Prince-
  ton University Press, 2008.

Manuel Arellano. Computing robust standard errors for within group estimators. Oxford bulletin of
  Economics and Statistics, 49(4):431–434, 1987.

Manuel Arellano. Panel data econometrics. Oxford university press, 2003.

P. Aronow, D. Green, and D. Lee. Sharp bounds on the variance in randomized experiments. Annals
  of Statistics, 42(3):850–871, 2014.

Peter M. Aronow and Cyrus Samii. Does regression produce representative estimates of causal effects?
  American Journal of Political Science, 60(1):250–267, 2016.

Susan Athey and Guido Imbens. Identification and inference in nonlinear difference-in-differences
  models. Econometrica, 74(2):431–497, 2006.


                                                   34
Susan Athey and Scott Stern. An empirical framework for testing theories about complimentarity in
  organizational design. Technical report, National Bureau of Economic Research, 1998.

Marianne Bertrand, Esther Duflo, and Sendhil Mullainathan. How much should we trust differences-
  in-differences estimates? The Quarterly Journal of Economics, 119(1):249–275, 2004.

Kirill Borusyak and Xavier Jaravel. Revisiting event study designs. 2016.

Brantly Callaway and Pedro HC Sant’Anna. Difference-in-differences with multiple time periods and
  an application on the minimum wage and employment. arXiv preprint arXiv:1803.09015, 2018.

David Card. The impact of the mariel boatlift on the miami labor market. Industrial and Labor
  Relation, 43(2):245–257, 1990.

David Card and Alan Krueger. Minimum wages and employment: Case study of the fast-food industry
  in new jersey and pennsylvania. American Economic Review, 84(4):772–793, 1994.

Timothy G Conley and Christopher R Taber. Inference with difference in differences with a small
  number of policy changes. The Review of Economics and Statistics, 93(1):113–125, 2011.

Clément de Chaisemartin and Xavier D’Haultfœuille. Fuzzy differences-in-differences. The Review of
  Economic Studies, 85(2):999–1028, 2017.

Clément de Chaisemartin and Xavier D’Haultfœuille. Two-way fixed effects estimators with heteroge-
  neous treatment effects. 2018.

Stephen G Donald and Kevin Lang. Inference with difference-in-differences and other panel data. The
  review of Economics and Statistics, 89(2):221–233, 2007.

Simon Freyaldenhoven, Christian Hansen, and Jesse Shapiro. Pre-event trends in the panel event-study
  design. Technical report, Brown University Working Paper, 2018.

Andrew Goodman-Bacon. Difference-in-differences with variation in treatment timing. Technical re-
  port, Working Paper, 2017.

Sukjin Han. Identification in nonparametric models for dynamic treatment effects. 2018.

Peter Hull. Estimating treatment effects in mover designs. arXiv preprint arXiv:1804.06721, 2018.

                                                 35
Kosuke Imai and In Song Kim. When Should We Use Linear Fixed Effects Regression Models for Causal
  Inference with Longitudinal Data? PhD thesis, Working paper, Princeton University, Princeton,
  NJ, 2016.

Guido W Imbens and Donald B Rubin. Causal Inference in Statistics, Social, and Biomedical Sciences.
  Cambridge University Press, 2015.

Kung-Yee Liang and Scott L Zeger.       Longitudinal data analysis using generalized linear models.
  Biometrika, 73(1):13–22, 1986.

Bruce D Meyer, W Kip Viscusi, and David L Durbin. Workers’ compensation and injury duration:
  evidence from a natural experiment. The American Economic Review, pages 322–340, 1995.

Jerzey Neyman. On the application of probability theory to agricultural experiments. essay on princi-
  ples. section 9. Statistical Science, 5(4):465–472, 1923/1990.

Judea Pearl. Causality: Models, Reasoning, and Inference. Cambridge University Press, New York,
  NY, USA, 2000. ISBN 0-521-77362-8.

Paul R Rosenbaum. Observational studies. In Observational Studies. Springer, 2002.

Paul R Rosenbaum. Observation and Experiment: An Introduction to Causal Inference. Harvard
  University Press, 2017.

Donald B Rubin. Bayesian inference for causal effects: The role of randomization. The Annals of
  statistics, pages 34–58, 1978.

Bbabubhai V Shah, Mary Margaret Holt, and Ralph E Folsom. Inference about regression models from
  sample survey data. Bulletin of the International Statistical Institute, 47(3):43–57, 1977.

James H Stock and Mark W Watson. Heteroskedasticity-robust standard errors for fixed effects panel
  data regression. Econometrica, 76(1):155–174, 2008.

Jeffrey M Wooldridge. Econometric analysis of cross section and panel data. MIT press, 2010.




                                                  36
