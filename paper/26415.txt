                              NBER WORKING PAPER SERIES




                RATIONAL INATTENTION WHEN DECISIONS TAKE TIME

                                      Benjamin M. Hébert
                                      Michael Woodford

                                      Working Paper 26415
                              http://www.nber.org/papers/w26415


                     NATIONAL BUREAU OF ECONOMIC RESEARCH
                              1050 Massachusetts Avenue
                                Cambridge, MA 02138
                                    October 2019




The authors would like to thank Mark Dean, Sebastian Di Tella, Mira Frick, Xavier Gabaix,
Matthew Gentzkow, Emir Kamenica, Divya Kirti, Jacob Leshno, Stephen Morris, Pietro
Ortoleva, José Scheinkman, Ilya Segal, Ran Shorrer, Joel Sobel, Miguel Villas-Boas, Ming Yang,
and participants at the Cowles Theory conference, 16th SAET Conference, Barcelona GSE
Summer Conference on Stochastic Choice, Stanford GSB research lunch, 2018 ASSA meetings,
UC Berkeley Theory Seminar, and UC San Diego Theory for helpful discussions on this topic,
and the NSF for research support. We would particularly like to thank Philipp Strack and Doron
Ravid for discussing an earlier version of the paper. Portions of this paper circulated previously
as the working papers "Rational Inattention with Sequential Information Sampling," "Rational
Inattention in Continuous Time," and "Information Costs and Sequential Information Sampling,"
and appeared in Benjamin Hébert's Ph.D. dissertation at Harvard University. All remaining errors
are our own. The views expressed herein are those of the authors and do not necessarily reflect
the views of the National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.

© 2019 by Benjamin M. Hébert and Michael Woodford. All rights reserved. Short sections of
text, not to exceed two paragraphs, may be quoted without explicit permission provided that full
credit, including © notice, is given to the source.
Rational Inattention when Decisions Take Time
Benjamin M. Hébert and Michael Woodford
NBER Working Paper No. 26415
October 2019
JEL No. D8,D83

                                         ABSTRACT

Decisions take time, and the time taken to reach a decision is likely to be informative about the
cost of more precise judgments. We formalize this insight in the context of a dynamic rational
inattention (RI) model. Under standard conditions on the flow cost of information in our discrete-
time model, we obtain a tractable model in the continuous-time limit. We next provide conditions
under which the resulting belief dynamics resemble either diffusion processes or processes with
large jumps. We then demonstrate that the state-contingent choice probabilities predicted by our
model are identical to those predicted by a static RI model, providing a micro-foundation for such
models. In the diffusion case, our model provides a normative foundation for a variant of the
DDM models studied in mathematical psychology.


Benjamin M. Hébert
Graduate School of Business
Stanford University
655 Knight Way
Stanford, CA 94305
and NBER
bhebert@stanford.edu

Michael Woodford
Department of Economics
Columbia University
420 W. 118th Street
New York, NY 10027
and NBER
mw2230@columbia.edu




A technical appendix is available at: http://www.nber.org/data-appendix/w26415
1     Introduction
It is common in economic modeling to assume that, when presented with a choice set, a decision
maker (DM) will choose the option that is ranked highest according to a coherent preference ordering.
However, observed choices in experimental settings often appear to be random, and while this could
reflect random variation in preferences, it is often more sensible to view choice as imprecise. Models
of rational inattention (such as Matêjka et al. [2015]) formalize this idea by assuming that the DM
chooses her action based on a signal (a subjective assessment of the situation) that provides only
an imperfect indication of the true state. The information structure that generates this signal is
optimal, in the sense of allowing the best possible joint distribution of states and actions, net of a
cost of information. In the terminology of Caplin and Dean [2015], models of rational inattention
make predictions about patterns of state-dependent stochastic choice. These predictions will depend
in part on the nature of the information cost, and several recent papers have attempted to recover
information costs from observed behavior in laboratory experiments (Caplin and Dean [2015], Dean
and Neligh [2019]).
    However, in both laboratory experiments and real-world economic settings, decisions take time,
and the time required to make a decision is likely to be informative about the nature of information
costs. In this paper, we develop a framework to study rational inattention problems in which
decisions take time, providing a means of connecting decision times to information costs and state-
dependent stochastic choice.
    There is an extensive literature in mathematical psychology that focuses on these issues. Variants
of the drift-diffusion model (DDM, Ratcliff [1985], Ratcliff and Rouder [1998]) also make predictions
about stopping times and state-dependent stochastic choice.1 In particular, these models are de-
signed to match the empirical observation that hasty decisions are likely to be of lower quality.2
However, these models are not based on optimizing behavior, and this raises a question as to the
extent to which they can be regarded as structural; it is unclear how the parameters of the DDM
model should be expected to change when incentives or the costs of delay change, and this limits
the use of the model for making counter-factual predictions. The framework we develop includes as
a special case variants of the DDM model, while at the same time making predictions about state-
dependent stochastic choice that match those of a static rational inattention model. Consequently,
our framework is able to both speak to the relationship between stopping times and state-dependent
stochastic choice (unlike standard rational inattention models) and make counter-factual predictions
(unlike standard DDM models).
    Specifically, we develop a class of rational inattention models in which the DM's imprecise per-
ception of the decision problem is not merely random, but evolves with the passage of time, and
a constrained optimization problem determines a joint probability distribution over both stopping
   1 DDM models were originally developed to explain imprecise perceptual classifications. See Woodford [2019] for

a more general discussion of the usefulness of the analogy between perceptual classification errors and imprecision in
economic decisions.
   2 The existence of a speed-accuracy trade-off is well-documented in perceptual classification experiments (e.g.,

Schouten and Bekker [1967]). Variants of the DDM that have been fit to stochastic choice data include Busemeyer
and Townsend [1993] and more recently Krajbich et al. [2014] and Clithero [2018]; see Fehr and Rangel [2011] for a
review of other early work. Shadlen and Shohamy [2016] provide a neural-process interpretation of sequential-sampling
models of choice.


                                                          1
times and choices. We then demonstrate that the resulting state-dependent stochastic choice prob-
abilities of our continuous-time model are equivalent to those of a static rational inattention model.
Any cost function for a static rational inattention model in the uniformly posterior-separable family
(in the terminology of Caplin et al. [2019]) can be justified in our framework. This result offers both
a justification for using such cost functions in static rational inattention problem and provides a
means of connecting those cost functions to dynamic processes for beliefs. Our results also provide
a relationship between information costs in the static problem and decision times in the dynamic
problem, and hence offer a new perspective on how the costs of information for rational inattention
models might be uncovered from observable behavior.
    Moreover, we provide conditions under which the process for beliefs in our model resembles a
diffusion process, allowing our model to be interpreted as a variant of the DDM model. Our results
therefore contribute to the literature on DDM-style models by offering an optimizing model that
makes predictions about how decision boundaries and choice probabilities should change in response
to changes in incentives, in particular by showing that these changes can be calculated using the
comparative statics of a static rational inattention problem.
    We begin by considering a discrete-time dynamic model of optimal information sampling, in which
at each of the discrete time steps a random signal is obtained. We then develop our continuous-time
framework as the limit of our discrete-time model when the length of the time steps between suc-
cessive signals becomes small. In our discrete-time model, we assume that the information sampling
process is optimally chosen from within a very flexible class of possibilities, in the spirit of models
of rational inattention (Sims [2010]), with a flow information cost function that satisfies conditions
commonly assumed in static models of rational inattention. The key step of our convergence proof
is our demonstration that all such flow information cost functions can be approximated, for signal
structures sufficiently close to uninformative, by a posterior-separable (in the terminology of Caplin
et al. [2019]) flow information cost function.
    Standard rational inattention models consider a static problem, in which a decision is made after
a single noisy signal is obtained by the DM. This allows the set of possible signals to be identified
with the set of possible decisions, which is no longer true in our dynamic setting. Steiner et al. [2017]
also discuss a dynamic model of rational inattention, but their model is one in which, because of
the kind of information cost assumed, it is never optimal to acquire information other than what is
required for the current action. As a result, in each period of their discrete-time model, the set of
possible signals can again be identified with the possible actions at that time. We instead consider
situations in which evidence is accumulated over time before any action is taken, as in the DDM;
this requires us to model the stochastic evolution of a belief state that is not simply an element
of the set of possible actions.3 Our central concerns are to study the conditions under which the
resulting continuous-time model of optimal information sampling gives rise to belief dynamics and
stochastic choices similar to those implied by a DDM-like model, and to study how variations in the
   3 The model differs from the one analyzed by Steiner et al. [2017] in several key respects. First, as just noted,

we study a setting in which the DM can take an action only once, and chooses when to stop and take an action
endogenously. Second, we consider a much more general class of flow cost functions than the mutual information cost
assumed in their paper. And third, we assume that the DM has a motive to smooth her information gathering over
time, rather than learn all of the relevant information at a single point in time.



                                                         2
opportunity cost of time or the payoffs of actions should affect stochastic choice.
    A number of prior papers have also sought to endogenize aspects of a DDM-like process. Moscarini
and Smith [2001] consider both the optimal intensity of information sampling per unit of time and
the optimal stopping problem, when the only possible kind of information is given by the sample path
of a Brownian motion with a drift that depends on the unknown state, as assumed in the DDM.4
Fudenberg et al. [2018] consider a variant of this problem with a continuum of possible states, and an
exogenously fixed sampling intensity.5 Woodford [2014] instead takes as given the kind of stopping
rule posited by the DDM, but allows a very flexible choice of the information sampling process, as in
theories of rational inattention. Our approach differs from these earlier efforts in seeking to endog-
enize both the nature of the information that is sampled at each stage of the evidence accumulation
process and the stopping rule that determines how much evidence is collected before a decision is
made.6
    We place minimal restrictions on the nature of the flow information costs, and yet obtain relatively
sharp conclusions about the nature of the continuous-time limit. While we allow in general for
decisions about both stopping and information sampling to be arbitrary functions of the compete
history of information sampled to that point, we show that it is possible to characterize the dynamics
in terms of the evolution of the belief state in a finite-dimensional space, with stopping if and only
if one of the stopping regions associated with the different possible actions is reached. We further
show that under relatively weak conditions the dimensionality of the space in which beliefs move
will be one less than the number of actions (thus, for example, a line in the case of a binary decision
problem, as assumed in the DDM). Moreover, we show in general that the dynamics of the belief
state prior to stopping can be described by a jump-diffusion process, and we give conditions under
which it will be either a pure diffusion (as assumed in the DDM) or a pure jump process (as in the
models of Che and Mierendorff [2019] and Zhong [2019]).
    We also offer, under relative general conditions, a characterization of the boundaries of the
stopping regions and the predicted ex ante probabilities of different actions, as functions of model
parameters including the opportunity cost of time. The key to this characterization is a demonstra-
tion that in a broad class of cases, both the stopping regions and the ex ante choice probabilities for
any given initial prior are the same as in a static RI problem with an appropriately chosen static in-
formation cost function. Thus in addition to providing foundations for interest in DDM-like models
of the decision process, our paper provides novel foundations for interest in static RI problems of par-
ticular types. For example, we provide conditions under which the predictions of our model will be
equivalent to those of a static RI model with the mutual-information cost function proposed by Sims
[2010]) -- and thus equivalent to the model of stochastic choice analyzed by Matêjka et al. [2015] --
but the foundations that we provide for this model do not rely on an analogy with rate-distortion
   4 Moscarini and Smith [2001] allow the instantaneous variance of the observation process to be freely chosen (subject

to a cost), but this is equivalent to changing how much of the sample path of a given Brownian motion can be observed
by the DM within a given amount of clock time.
   5 See also Tajima et al. [2016] for analysis of a related class of models, and Tajima et al. [2019] for an extension to

the case of more than two alternatives.
   6 Both Morris and Strack [2019] and Zhong [2019] adopt our approach, and obtain special cases of the relationship

between static and dynamic models of optimal information choice that we present below. Other recent dynamic
models of optimal evidence accumulation include Che and Mierendorff [2019] and Zhong [2019], which differ from our
treatment in not considering conditions under which beliefs will evolve as a diffusion process.


                                                            3
theory in communications engineering (the original motivation for the proposal of Sims).
    More generally, we show that any cost function for a static RI model in the uniformly posterior-
separable family studied by Caplin et al. [2019] can be justified by the process of sequential evidence
accumulation that we describe. This includes the neighborhood-based cost functions discussed in
Hébert and Woodford [2018], that lead to predictions that differ from those of the mutual-information
cost function in ways that arguably better resemble the behavior observed in experiments such as
those of Dean and Neligh [2019]. Our result provides both a justification for using such cost functions
in static RI problems, and an answer (not given by static RI theory alone) to the question of how
the cost function in an equivalent static RI model should change in the case of a change in the
opportunity cost of time.
    The connection that we establish between the choice probabilities implied by a dynamic model
of optimal evidence accumulation and those implied by an equivalent static RI model holds both in
the case that the belief dynamics in the dynamic model are described by a pure diffusion process
and in the case that they are described by a jump process; thus we also show that with regard to
these particular predictions, these two types of dynamic models are equivalent. However, this does
not mean that no observations would make it possible to distinguish between them. We show that
the predictions of the two types of model with regard to the distribution of decision times can be
different, so that it should be possible in principle to use empirical evidence to determine which
better describes actual decision making. And to the extent that we are interested in the predictions
of our model for decision times and not solely for choices (as a number of authors have argued we
should be), it matters which specification is the more realistic one.
    We begin in section 2 by defining static RI problems, and introducing the discrete-time dynamic
problem that we study and the continuous-time problem that is its limit. In section 3, we state
the conditions that we impose on the flow information cost function, and present our result that
all cost functions satisfying these conditions resemble posterior-separable cost functions in a local
approximation. Section 4 that demonstrates that the continuous-time model described in section
2 represents a limiting case of our discrete-time problem. In section 4 we also describe conditions
under which beliefs follow a diffusion or a diffusion-like process, and conditions under which the
belief dynamics instead involve large jumps. In section 5 we demonstrate that the state-dependent
choice probabilities predicted by the continuous time models (in both the diffusion case and the
jump case) are equivalent to those predicted by a static rational inattention model with a uniformly
posterior-separable cost function. In section 6 we discuss how models involving jumps in beliefs and
models involving diffusions can in principle be distinguished using stopping time data. In section 7
we conclude.


2    Static and Dynamic Models of Rational Inattention
We begin by describing the class of static rational inattention models surveyed by Sims [2010], and
then describe the discrete and continuous time dynamic models we study.
   Notation: given a set X , we define P (X ) as the probability simplex associated with that set.
                                                                   |X |
We describe an element of the simplex r  P (X ) by a vector in R+ whose elements sum to one,


                                                  4
each of which corresponds to the likelihood of a particular element of x  X . Except when necessary,
we will call this vector r as well, and will not distinguish between the element of the simplex and its
coordinate representation. We use the notation rx to refer to the probability under r that x  X
occurs. We discuss additional notation that appears in our proofs in the appendix, Section §B.


2.1     Static Models of Rational Inattention
Let x  X be the underlying state of nature, and let s  S be a signal the decision maker (DM) can
receive, which might convey information about the state. We assume that X and S are finite sets.
Let q denote the DM's prior belief (before receiving a signal) about the probability of state x; that
is, q is element of the probability simplex P (X ). Define ps,x as the probability of receiving signal s
in state x, let px  P (S ) be the associated conditional probability distribution of the signals given
state x, and let p be the |S | × |X | matrix whose elements are ps,x . The matrix p, which is a set
of conditional probability distributions for each state of nature, {px }xX , defines an "information
structure."
    By Bayes' rule, the DM believes under her prior that the unconditional probability of receive
a signal s  S is s (p, q ) = xX ps,x qx . After receiving signal s, the DM will hold a posterior,
qs (p, q )  P (X ), which is defined by

                                                                ps,x qx
                                               qs,x (p, q ) =
                                                                s (p, q )

for all s  S such that s (p, q ) > 0. We adopt the convention that qs (p, q ) = q for all s  S such
that s (p, q ) = 0.
    Let a  A be the action taken by the decision maker (DM). For simplicity, A is also a finite set,
and we assume that the number of states is weakly larger than the number of actions, |X |  |A|.
The DM's utility from taking action a in state x is ua,x . We assume that ua,x is strictly positive
and bounded above by a positive constant, u¯.
    The maximum achievable expected payoff, given an information structure p and prior q , can be
written as
                                u(p, q )  max         qx ps,x ua(s),x .                          (1)
                                                 {a(s)}
                                                          xX sS

The standard static rational inattention problem, given the signal alphabet S ,7 is then

                                            max        u(p, q ) - C (p, q ; S ),                                  (2)
                                       {px P (S )}xX


where
                                       C (·, ·; S ) : P (S )|X | × P (X )  R                                      (3)

is a cost function for information structures.
    In the classic formulation of Sims, a problem of the form equation (2) is considered, in which the
   7 The full problem includes a choice over the signal alphabet S . A standard result, which will hold for all of the

cost functions we study, is that |S | = |A| is sufficient.




                                                           5
cost function C (p, q ; S ) is proportional to the Shannon mutual information between the signal and
the state. Mutual information can be defined using Shannon's entropy,

                                   H Shannon (q )  -              qx ln(qx ).                      (4)
                                                             xX


Shannon's entropy can in turn be used to define a measure of the degree to which the posterior
q  P (X ) associated with any signal differs from the prior q , the Kullback-Leibler (KL) divergence,

              DKL (q ||q )  H Shannon (q ) - H Shannon (q ) + (q - q )T · Hq
                                                                           Shannon
                                                                                   (q ),           (5)

        Shannon
where Hq        (q ) denotes the gradient of Shannon's entropy evaluated at beliefs q . Mutual in-
formation is then the expected value of the KL divergence over possible signals, and the cost of an
information structure is assumed to be proportional to the mutual information,

                            C M I (p, q ; S ) =         s (p, q )DKL (qs (p, q )||q ),             (6)
                                                   sS


where  > 0 is a positive constant. Mutual information is a measure of the informativeness of the
signal, in that it provides a measure of the degree to which the signal changes what one should
believe about the state, on average. However, Shannon's mutual information is not the only possible
measure of the informativeness of an information structure, or the only plausible cost function for
a static rational inattention problem. We discuss two large classes of cost functions, both of which
will feature prominently in our analysis, below. For specific alternative proposals, see Caplin et al.
[2019], Hébert and Woodford [2018], Pomatto et al. [2018].
    Caplin et al. [2019] define a large class of cost functions, the "posterior-separable" class, which
can written as
                               C P S (p, q ; S ) = s (p, q )D(qs (p, q )||q ),                      (7)
                                                   sS

where D(·||·) is a divergence. Recall that a divergence is defined as a function D : P (X ) × P (X ) 
R+ , with D(q ||q ) = 0 if and only if q = q . Divergences can be thought of as distances between
probability distributions, although they are not necessarily symmetric and do not necessarily satisfy
the triangle inequality. Mutual information is a posterior-separable cost function, whose associated
divergence is (proportional to) the KL divergence.
   Within this class, a cost function is said to be "uniformly posterior-separable" if the divergence
D is a Bregman divergence, meaning that

                             D(q ||q ) = H (q ) - H (q ) - (q - q )T · Hq (q )                     (8)

for some convex function H : P (X )  R. The KL divergence is a Bregman divergence, whose
associated convex function is the negative of Shannon's entropy. These two classes, the posterior-
separable cost functions and the uniformly posterior-separable cost functions, will play a key role in
our analysis.



                                                         6
    In our derivation of the continuous time limit of our discrete time model, we adopt a more general
approach (following De Oliveira et al. [2017] and Caplin and Dean [2015]) and study the entire class
of costs functions satisfying certain conditions. We introduce these conditions after describing our
discrete and continuous time dynamic models.


2.2    Dynamic Discrete Time Models of Rational Inattention
We next extend the static rational inattention model just described to a discrete time dynamic
setting, in which the DM has numerous opportunities to gather information before taking an action.
Our extension is similar in spirit to the one described by Steiner et al. [2017], with several significant
differences. First, unlike those authors, we study a setting in which the DM can take an action
only once, and chooses when to stop and take an action endogenously. Second, we are interested in
general cost functions of the form described by equation (3), not just mutual information. Third,
we assume the DM has a motive to smooth her information gathering over time, rather than learn
all of the relevant information in a single period.
    As in the static model, there is a state of the world, x  X , that remains constant over time.
At each time t, the DM can either stop and take an action a  A, or continue and receive a signal
drawn from the information structure {pt,x  P (S )}xX , for some signal alphabet S . We assume
that the signal alphabet S is finite and fixed over time, with |S |  2|X | +2. However, the information
structure {pt,x }xX is a choice variable that can depend on the past history of signal realizations.
Fixing the signal alphabet S has no economic meaning, because the information content of receiving
a particular signal s  S can change over time; our assumption that S is finite allows us to avoid
technicalities associated with infinite-dimensional signal structures. We also assume, as a technical
device, that S contains one signal, s ¯, that is required to be uninformative. This assumption ensures
that the DM can choose to mix any arbitrary signal structure with an uninformative one, even if
she has already used up her "useful" signals.
    The DM's prior beliefs at time t, before receiving the signal, are denoted qt . Each time period
has a length . Let  denote the time at which the DM stops and makes a decision, with  = 0
corresponding to making a decision without acquiring any information. At this time, the DM receives
utility ua,x -  if she takes action a at time  and the true state of the world is x. Let u  ^(q ) be the
utility (not including the penalty for delay) associated with taking an optimal action under beliefs
q :
                                        u^(q ) = max       q,x ua,x .
                                                aA
                                                     xX

We assume, as in the static model, that ua,x is strictly positive and bounded above by the constant
¯, and that |A|  |X |.
u
    Each time period has length , and the DM discounts the future exponentially, with discount
factor   , for some   (0, 1]. The parameter  and discount factor  together govern the size of
the penalty the DM faces from delaying his decision. The DM does not always make a decision
immediately because she is able to gather information over time before making a more-informed
decision.



                                                    7
    The DM can choose an information structure that depends on the current time and past history
of the signals received. As we will see, the problem has a Markov structure, and the current time's
"prior," qt , summarizes all of the relevant information that the DM needs to design the information
structure. The DM is constrained to satisfy

                                 -1 -1                                       -1 -1
                                                            1                            1
                      E0 [                j  C (pj  , qj  ; S ) ]   cE0 [             j]  ,                   (9)
                                 j =0                                        j =0


if the DM choose to acquire any information at all ( > 0 always in this case).8 In words, the
L -norm of the flow information cost function C (·) over time and possible histories must be less
that the constant c per unit time. In the particular case in which information gathering is constant
                      ¯ ), this constraint simplifies to
(C (pj  , qj  ; S ) = C

                                                    ¯
                                                    C   1
                                                         c,
                                                    

emphasizing that the constant c can be thought of as a limit on the flow rate of information acqui-
sition.
    The parameter  governs the substitutability of information acquisition across time. In the limit
as   , the L norm becomes the essential supremum, and the constraint approaches a per-
period constraint on the amount of information the DM can obtain. For finite values of , the DM
can allocate more information gathering to states and times in which it is more advantageous to
gather more information. We assume, however, that  > 1, to ensure that it is optimal for the
DM to gather information gradually, rather than all at once. Our assumption of  > 1 is similar
to the convex cost of the rate of experimentation assumed by Moscarini and Smith [2001]. It is a
critical assumption that separates our model from the model of Steiner et al. [2017], in which the
DM will (under some circumstances) learn a large amount of information in a single period. (Further
assumptions about the flow cost function C (·) are introduced in the next section.)
    Let V (q0 ; ) denote the value obtained in the sequence problem for a DM with prior beliefs q0 ,
and let q denote the DM's beliefs when stopping to act. The DM's problem is

                                                                        1 - 
                                 V (q0 ; ) = max E0 [  u
                                                       ^(q ) -               ],                              (10)
                                              {pj  },                   1 - 

subject to the information-cost constraint (9).
   Note that the constraint (9) can equivalently be written in a form that takes each side of the
inequality to the power ; we then obtain a constraint that is additively separable over time and
across possible histories. This allows us to write a Lagrangian for the DM's problem in the additively
   8 The factor  inside the operator E [·] on both sides of this inequality might seem redundant. We include it so
                                        0
that the expression inside the operator on the left-hand side approaches a time integral of discounted information
costs in the limit as  is made arbitrarily small.




                                                        8
separable form

                                                                1 - 
                    W (q0 , ; ) = max E0 [  u
                                            ^(q ) -                  ]-
                                    {pj  },                     1 - 
                                               -1 -1
                                        1-                  1
                                 E0 [                   j  { C (pj  , qj  ; S ) -  c }],               (11)
                                               j =0
                                                            

where   0 is a Lagrange multiplier associated with the information-cost constraint. With this
definition, the value function defined in (10) corresponds to V (q0 ; ) = min0 W (q0 , ; ). An
optimal information sampling policy then necessarily maximizes W (q0 , ; ) for some value of   0.
This alternative formulation of the problem can be thought of as the value function of a different
                                                                            1
problem, in which there is a cost of gathering information proportional to    C (·) .
    We will next analyze the continuous time limits of these functions,

                                      W (q0 , ) = lim W (q0 , ; )
                                                      0+

and
                                         V (q0 ) = lim V (q0 ; ).
                                                      0+

We will first present the limiting continuous time problem, and then state assumptions that ensure
this continuous time problem is the limit of the discrete time problem. Our approach is to assume
standard conditions on the flow cost function C (·), which we describe in the next section, and then
characterize the cost of a small amount of information under these conditions. We demonstrate that,
to an approximation of first order in , every cost function satisfying our conditions is equivalent
to a posterior-separable cost function. Because of our assumption that  > 1, it will be optimal, as
  0+ , for the DM to acquire a small amount of information each period, and consequently our
results about the cost of a small amount of information will determine the form of the continuous
time limit.


2.3    Dynamic Continuous Time Models of Rational Inattention
Here we introduce the continuous time models that are the limits of our discrete time model. We will
discuss two cases: a model with exponential discounting ( < 1) and a model with no discounting
( = 1). In both of these models, beliefs follow a jump-diffusion process. In our analysis of the
models (section §4), we will discuss conditions under which optimal information sampling implies
that beliefs follow pure jump or pure diffusion processes.
    Our continuous time models are defined in terms of a divergence, D (r||q ). This divergence is
continuously twice-differentiable in r for r sufficiently close to q , and we define an |X | × |X | positive
semi-definite matrix-valued function k (q ) by

                                 2 D (r||q )
                                             |r=q = Diag (q )k (q )Diag (q ),                          (12)
                                  ri rj

where Diag (q ) is an |X | × |X | diagonal matrix whose diagonal is the vector q . We call this k (q )


                                                       9
function the "information cost matrix" and we prove it satisfies certain properties as part of our
derivation, which we describe in section §4.
   The key step in our derivation of the continuous time model is Theorem 1, which demonstrates
that, up to first-order in , every cost function satisfying our conditions is posterior-separable:

                                C (p, q ; S ) =         s (p, q )D (qs (p, q )||q ) + o()                        (13)
                                                   sS


for all sufficiently uninformative p (for a more formal statement, see Theorem 1 below). Loosely,
there are two ways for a signal structure to be close to uninformative. First, it might have signals
that occur frequently but whose associated posteriors are close to the value of the prior. Second, it
might have signals that occur rarely but whose posteriors are far from the prior. These two kinds of
signals correspond in the continuous time limit, respectively, to a diffusion component and a jump
component of the stochastic process for beliefs.9
    We prove in the limit as   0+ that the process qt converges to a jump-diffusion:

                                    dqt = -¯t zt dt + zt dJt + Diag (qt )t dBt ,


where dJt is a poisson process with intensity  ¯t , zt is change in beliefs conditional on a jump, dBt
is an |X |-dimensional Brownian motion, and t is an |X | × |X | matrix that controls the diffusion
component of beliefs. Because beliefs must remain in the simplex, qt- + zt is absolutely continuous
                           T
with respect to qt- , and qt t = 0.
    The flow information cost in this limit is

                         C (t , ¯t , zt ; qt )dt = 1 tr[t  T k (qt )]dt + ¯t D (qt + zt ||qt )dt,
                                                          t
                                                   2
which the continuous time analog of equation (13).
   We now introduce the continuous time analog of the (dual) dynamic discrete time problem
(equation (11)), which we will refer to as W + (qt , ).

Definition 1. The dual continuous time problem with discounting ( < 1) is

                                                                                  1
    W (qt , ) =                     sup                   Et [ ( -t) u
                                                                     ^(q ) -           (1 -  ( -t) )( - c )]-
                   {s R|X |×|X | ,¯s R+ ,zs R|X | }, R+                        - ln( )
                        ^ 
                                      1                        ¯s D (qs- + xs ||qs- )} ds],
               -    Et [     (s-t) { tr[s s   T
                                                  k (qs )] +   
                         t            2

subject to the evolution of beliefs,

                                    dqt = -¯t xt dt + xt dJt + Diag (qt )t dBt ,
    9 Signals whose frequency converges to zero and whose posterior converges to the prior can in fact be informative,

if they converge at slow enough rates. When the frequency does not converge to zero, the signal is a diffusion in the
continuous time limit. When the posterior does not converge to a prior, the signal is a jump in the continuous time
limit. When both converge but sufficiently slowly, the signal becomes a non-jump-diffusion semi-martingale in the
continuous time limit. However, part of our convergence proof demonstrates that it is without loss of generality to
assume beliefs follow a jump-diffusion process.



                                                               10
where dJt is a Poisson process with intensity  ¯t and dBt is an |X |-dimensional Brownian motion,
                                             T
and the constraints that qt- + xt   qt- and qt t = 0, and the constraint that, for all stopping times
                                                    
T measurable with respect filtration generated by qs ,
             ^    T
                              1                  ¯s D (qs- + x||qs- )}ds]  (  ) -
                                                                                1      1 -  (T -t)
         Et [          (s-t) { tr[s s
                                    T
                                      k (qt )] +                                  1E [
                                                                                    t              ],
              t               2                                                          - ln( )

where  is a positive constant.

    For the problem with discounting, there exists some  such that the limit of the original sequence
problem, limn V (q0 ; n ) = V (q0 ), is equal to W + (qt ,  ).
    In the particular case where the DM has no exponential discounting ( = 1), we demonstrate
that the amount of information acquired at each moment (C (t , t , xt ; qt ) above) is constant. This
property comes from the fact that the cost of delay is constant. In the case with discounting ( < 1),
the cost of delay depends in part on the current level of the value function, which generates variation
and leads to a non-constant flow cost function. In the  = 1 case, using the fact that the quantity
of information acquired is constant, the problem can be written in a simpler form. We introduce
this simpler form below, and explicitly characterize both the dual continuous time problem and the
original sequence problem.

Definition 2. The dual continuous time problem without discounting ( = 1) is, for all  
(0, c- ),

                                                                                         
           W (qt , ) =                        sup                      u(q  ) - ( - t)
                                                                   Et [^                    ( - c )]
                            {s R|X |×|X | ,¯s R+ ,zs R|X | }, R+                         -1

                                                             T
subject to the constraints that qt- + zt              qt- , qt t = 0, and

                                 1      T            ¯s D (qs- + zs ||qs- )  (),
                                   tr[s s k (qt )] + 
                                 2

for all times s  [t,  ) and some constant () > 0.
    The limit of the original sequence problem, limn V (q0 ; n ) = W (qt ,  ) = V (q0 ), is

                         V (qt ) =                   sup                        u(q  ) - ( - t)]
                                                                            Et [^
                                     {s R|X |×|X | ,¯s R+ ,zs R|X | }, R+


                                                             T
subject to the constraints that qt- + zt              qt- , qt t = 0, and

                                     1      T            ¯s D (qs- + zs ||qs- )   1
                                       tr[s s k (qt )] +                            c.
                                     2

     These continuous time problems are described using the "unconditional" dynamics for the beliefs
qt , meaning that beliefs are martingales. Using Bayes' rule, we can also define the conditional
dynamics for beliefs. Suppose that xtrue  X is the true state of nature. The beliefs qt follow the




                                                             11
dynamics process

                                     T
                   dqt = Diag (qt )t t extrue - ¯t zt dt + xt dJ xtrue + Diag (qt )t dBt ,                  (14)
                                                                t


where extrue is a vector equal to one in the state corresponding to xtrue and zero otherwise, and
                                                        T
  xtrue
dJt     is a Poisson process with arrival rate ¯t (1 + ex     z
                                                          true t
                                                                 ). Intuitively, beliefs will tend to move
                                                       eT     qt
                                                             xtrue

towards xtrue . Consider in particular the value eT       xtrue dqt . There is a positive (by the positive
                     T
definiteness of t t ) drift term associated with the diffusion of beliefs, and jumps are more likely if
eT
 xtrue zt is positive and less likely if it is negative. Certain other models in the literature (e.g. DDM
models, see Fudenberg et al. [2018]) are usually expressed in terms of the conditional dynamics of
beliefs, and equation (14) allows us to relate our continuous time models to these other models.
    Before proceeding to our derivation, we briefly discuss the differences between the  < 1 and
 = 1 cases, and of the role of jumps vs. diffusion. Beginning with the issue of jumps vs. diffusions,
observe that there is a kind of continuity between the two. Because the cost of both jumps and
diffusion in beliefs are determined by the same divergence, D , there exists a limit in which the
jump becomes very likely (      ¯t  ) and very small (zt  0) and for which the stochastic process
of beliefs and the cost of the information converge to the stochastic process and cost of a diffusion
process. That is, it is without loss of generality in both our continuous time problems to assume
that t = 0, because the supremum of a sequence of jump controls (             ¯t , zt ) can perfectly replicate
a diffusion process. Relatedly, although both of these problems appear to allow the DM to have
jumps in only a single direction zt , nothing requires that the DM choose controls (          ¯t , zt ) that vary
smoothly in the state qt . Consequently, the DM is perfectly able to replicate a process with jumps
in multiple directions. These statements are proven as part of the proof of our convergence result,
Theorem 2 below.
    Turning now to the issue of  < 1 vs.  = 1, observe first that many decisions are made over
short periods of time (seconds or minutes). With conventional rates of time preference,  should be
extremely close to one. As we will demonstrate, in the  = 1 case, the model is tractable and we
are able (under certain additional assumptions) to characterize the value function. Consequently,
provided that behavior is continuous in the limit as  approaches one (and we will show that it is),
we believe it is reasonable to focus on the  = 1 model.
    Moreover, as noted by Fudenberg et al. [2018], when  < 1, the dynamics of beliefs are not
invariant to non-action-contingent transformations of the utility function. That is, increasing the
utility function will change the DM's behavior, because with  < 1 a higher value function creates
a higher cost of delay. We view this as undesirable, and describe in Section §6 an experimentally
feasible test of this implication of  < 1. This association between the cost of delay and the level
of the value function is the key property that leads to the result of Zhong [2019] that, in a special
case of our setting, jumps in beliefs and not diffusions are optimal. This result might lead one to
conclude that there is a discontinuity between the  < 1 and  = 1 cases. However, we show below
that continuity holds between the  < 1 and  = 1 cases. That is, in situations in which the  = 1
model leads to a diffusion process for beliefs, the  < 1 model has beliefs that are a sequence of tiny
jumps, whose magnitude is on the order of - ln( ), and the belief process converges in the limit to


                                                       12
a diffusion.
   Having introduced static, discrete time dynamic, and continuous time dynamic models of rational
inattention, we next outline the key conditions we impose on flow cost functions in the discrete time
model that lead the continuous time model as the limit.


3       Flow Information Costs
At each date in the discrete-time sequential rational inattention problem introduced above, the DM
chooses an information structure. Each information structure p has a flow cost C (p, q ; S ), given by
a function of the form of (3), where q is the DM's prior in this date (that is, the posterior beliefs
following from observations prior to the current date of the dynamic problem), and S is the signal
alphabet.10 Our results depend only on assuming that this flow information-cost function satisfies
a set of five general conditions, stated below.
    All of these conditions are satisfied by the mutual-information cost function (6) proposed by
Sims, but they are also satisfied by many other cost functions (for example, the neighborhood-based
cost functions we describe in Hébert and Woodford [2018]). They are closely related to conditions
that other authors ( De Oliveira et al. [2017] and Caplin and Dean [2015]) have also proposed as
attractive general properties to assume about information-cost function, in the context of static
rational inattention models.

Condition 1. Information structures that convey no information (px = px for all x, x in the
support of q ) have zero cost. All other information structures have a strictly positive cost.

    This condition ensures that the least costly strategy for the DM in the standard static rational
inattention problem is to acquire no information, and make her decision based on the prior. The
requirement that gathering no information has zero cost is a normalization.
    The next condition is called mixture feasibility by Caplin and Dean [2015]. Consider two infor-
mation structures, {p1,x }xX , with signal alphabet S1 , and {p2,x }xX , with alphabet S2 . Given a
parameter   (0, 1), we define a mixed information structure, {pM,x }xX over the signal alphabet
SM = (S1  S2 ) × {1, 2}. For each s = (s1 , 1) in the alphabet SM , pM,x (s) is equal to p1,x (s) if
s1  S1 , and equal to 0 otherwise. Likewise, for each s = (s2 , 2), pM,x (s) is equal to (1 - )p2,x (s)
if s2  S2 , and equal to 0 otherwise.
    That is, this information structure results, with probability , in a posterior associated with
information structure p1 , and with probability 1 -  in a posterior associated with information
structure p2 . The distribution of posteriors under the mixed information structure is a convex
combination of the distributions of posteriors under the two original information structures, as if the
DM flipped a coin and used the result to chose one of the two information structures, and (crucially)
remembered the result of the coin flip. The mixture feasibility condition requires that choosing a
   10 The information-cost functions that we study, like mutual information, are defined for all finite signal alphabets

S . Note, however, that mutual information is also defined over alternative sets of states of nature X . We do not
impose this requirement on our more general cost functions -- all of our analysis takes the set of states of nature as
given.




                                                          13
mixed information structure costs no more than the cost of randomizing over information structures
(using a mixed strategy in the rational inattention problem).

Condition 2. Given two information structures, {p1,x }xX , with signal alphabet S1 , and {p2,x }xX ,
with alphabet S2 , the cost of the mixed information structure is weakly less than the weighted average
of the cost of the separate information structures:

                         C (pM , q ; SM )  C (p1 , q ; S1 ) + (1 - )C (p2 , q ; S2 ).

    The next condition uses Blackwell's ordering. Consider two signal structures, {px }xX , with sig-
nal alphabet S , and {px }xX , with alphabet S . The first information structure Blackwell dominates
the second information structure if, for all utility functions ua,x and all priors q  P (X ),

                                            u(p, q )  u(p , q ),

where u(p, q ) is defined as in equation (1). In words, if one information structure Blackwell dominates
another, it is weakly more useful for every decision maker, regardless of that decision maker's utility
function and prior. In this sense, it conveys weakly more information. This ordering is incomplete;
most information structures neither dominate nor are dominated by a given alternative information
structure. However, when an information structure does Blackwell dominate another one, we assume
that the dominant information structure is weakly more costly.

Condition 3. If the information structure {px }xX with signal alphabet S is more informative, in
the Blackwell sense, than {px }xX , with signal alphabet S , then, for all q  P (X ),

                                 C (px }xX , q ; S )  C (px }xX , q ; S ).

    The first three conditions are, from a certain perspective, almost innocuous. For any joint
distribution of actions and states that could have been generated by a DM solving a static rational
inattention problem, with an arbitrary information cost function, there is a cost function consistent
with these three conditions that also could have generated that data (Theorem 2 of Caplin and Dean
[2015]). The result arises from the possibility of the DM pursuing mixed strategies over information
structures and mixed strategies in the mapping between signals and actions. These conditions also
characterize "canonical" rational inattention cost functions, in the terminology of De Oliveira et al.
[2017].
    We demonstrate that the mixture feasibility condition (Condition 2) and Blackwell monotonicity
condition (Condition 3) are equivalent to requiring that the cost function be convex over information
structures and Blackwell monotone.

Lemma 1. Let p and p be information structures with signal alphabet S . A cost function is convex
in information structures if, for all   (0, 1), all signal alphabets S , and all q  P (X ),

                      C (p + (1 - )p , q ; S )  C (p, q ; S ) + (1 - )C (p , q ; S ).



                                                     14
A cost function satisfies mixture feasibility and Blackwell monotonicity (Conditions 2 and 3) if and
only if it is convex in information structures and satisfies Blackwell monotonicity.

Proof. See the appendix, section B.1.

    This result is useful because it allows us to restrict attention to cost functions that are convex
in the signal structure, which is the choice variable at each date.
    The fourth condition that we assume, which is not imposed by Caplin and Dean [2015], Caplin
et al. [2019], or De Oliveira et al. [2017], is a differentiability condition that will allow us to charac-
terize the local properties of our cost functions. Let p0 (p, q ) be an uninformative signal structure,
with p0 x,s (p, q ) = s (p, q ) for all x  X and s  S . That is, for each state x  X , with the signal
structure p0 , the likelihood of receiving each signal s  S is equal to the unconditional likelihood,
and hence the signal is completely uninformative. Treating p and p0 (p, q ) as elements of R|X |×|S | ,
let ||p - p0 (p, q )|| be an arbitrary norm on the difference between two signal structures.

Condition 4. There exists an > 0 such that, for all signal alphabets S and priors q and all
information structures p sufficiently close to uninformative (||p - p0 (p, q )|| < ), the information cost
function is continuously twice-differentiable with respect to p in all directions that do not change
the support of the signal distribution, and directionally differentiable, with a continuous directional
derivative, with respect to perturbations that increase the support of the signal distribution.

    While this may seem a relatively innocuous regularity condition, it is not completely general; for
example, it rules out the case in which the DM is constrained to use only signals in a parametric
family of probability distributions, and the cost of other information structures is infinite. Thus
it rules out information structures of the kind assumed in Fudenberg et al. [2018] or Morris and
Strack [2019]. Condition 4 also rules out other proposed alternatives, such as the channel-capacity
constraint suggested by Woodford [2012]. The " > 0" part of the condition indicates that that
this differentiability need only hold at nearly uninformative information structures; obviously, if
differentiability holds everywhere, the condition will be satisfied.
    We should also note that this condition imposes the assumption that the cost function is finite
for posteriors on the boundary of the simplex. This feature of the condition is convenient but
unnecessary for our results; we could weaken the condition to require directional differentiability with
respect to perturbations that increase the support of the signal distribution only for perturbations
for which the resulting posteriors are interior to the simplex defined by the support of q . We should
also observe that, because the convex function is convex in p, the existence of a directional derivative
is guaranteed. What the condition is imposing is that this directional derivative be continuous.
    The next condition that we assume, which is also not imposed by Caplin and Dean [2015], Caplin
et al. [2019], or De Oliveira et al. [2017], is a sort of local strong convexity. We will assume that
the cost function exhibits strong convexity, in the neighborhood of an uninformative information
structure, with respect to information structures that hold fixed the unconditional distribution of
signals, uniformly over the set of possible priors.




                                                    15
Condition 5. There exists constants m > 0 and B > 0 such that, for all priors q  P (X ), and all
information structures that are sufficiently close to uninformative (C (p, q ; S ) < B ),

                                              m
                             C (p, q ; S )              s (p, q )||qs (p, q ) - q ||2
                                                                                    X,
                                              2
                                                   sS


where qs is the posterior given by Bayes' rule and || · ||X is an arbitrary norm on the tangent space
of P (X ).

    This condition is slightly stronger than Condition 1; it it essentially an assumption of "local
strong convexity" instead of merely local strict convexity. It implies that all informative information
structures have a non-trivial positive cost, and that (regardless of the DMs' current beliefs) there
are no informative information structures that are "almost free." This condition allows us to assert
that if the flow cost C (p, q ; S ) is converging to zero, then either the posteriors must become close
to the prior (qs (p, q ) close to q ) or the signals must become rare (s (p, q ) close to zero), or some
combination thereof.
    The mutual-information cost function (6) satisfies each of these five conditions. However, it is
not the only cost function to do so. For example, we can construct a family of such cost functions,
using the family of "f-divergences" (see, e.g., Ali and Silvey [1966] or Amari and Nagaoka [2007]),
defined as
                                                                 q
                                          Df (q ||q ) =    qx f ( x ),
                                                                 qx
                                                          xX

where f is any strictly convex, twice-differentiable function with f (1) = f (1) = 0 and f (1) = 1.
The KL divergence is a member of this family, corresponding to f (u) = u ln u - u + 1.
   The posterior-separable cost function associated with this divergence,

                               C f (p, q ; S ) =        s (p, q )Df (qs (p, q )||q ),              (15)
                                                   sS


satisfy all five of the conditions described above. Another example satisfying our conditions, as
mentioned previously, are the neighborhood cost functions of described by Hébert and Woodford
[2018]. These cost functions fall into the uniformly posterior-separable family of cost functions
described by Caplin et al. [2019], and (under mild regularity assumptions) all such functions satisfy
our conditions. We discuss these issues in more detail in the appendix, section §A.
    We are now in a position to use these conditions to characterize the cost of a small amount of
information.

Theorem 1. Let m , m  N, denote a sequence such that limm m = 0. Given an prior
q  P (X ) and signal alphabet S , let pm  P (S )|X | be a sequence of signal structures, and suppose
that there exists a B > 0 such that, for all s  S and all m  N,

                                  s (pm , q )||qs (pm , q ) - q ||2
                                                                  X  B m .




                                                         16
Then any cost function satisfying Conditions 1-4 is posterior-separable up to first order in :

                         C (pm , q ; S ) =        s (pm , q )D (qs (pm , q )||q ) + o(m ),
                                             sS


where D is a divergence.
   The divergence D is finite and convex in its first argument, and continuous in both arguments.
 
D (r||q ) is twice-differentiable in r for r sufficiently close to q , with

                                  2 D (r||q )
                                              |r=q = Diag (q )k (q )Diag (q )
                                   ri rj

for some continuous matrix-valued function k (q ). For all q , k (q ) is positive semi-definite and sym-
metric, and satisfies z T k (q )z = 0 for all z  R|X | that are constant in the support of q . If in addition
the cost function satisfies Condition 5, then there exists a constant mg > 0 such that

                                      k (q ) - mg (Diag (q ) - qq T )      0.

Proof. See the appendix, section B.2.

    Going forward, we will use the notation

                                        ¯(q ) = Diag (q )k (q )Diag (q )
                                        k

to describe the Hessian of the divergence D (r||q ) evaluated at r = q , and observe that it is positive
semi-definite.
    Now that we have derived our approximation result, we turn to considering the continuous time
limit of the dynamic rational inattention problem introduced in section §2.


4    Rational Inattention in Continuous Time
We next study the limit of the dynamic rational inattention problem described in section §2. We will
derive these limits for several specific cases, depending whether the DM's has exponential discounting
( < 1 vs.  = 1) and on the nature of the flow information cost function C (·). In particular, we
consider two cases for C (·): when there is a "preference for gradual learning" and when there is a
"preference for discrete learning," terms we define below. These two classes of flow cost functions will
lead, respectively, to beliefs that move in small increments and beliefs that move in large increments.
    Our most general convergence result proves that the discrete time dual problem converges
W (q0 , ; ) (defined in equation (11)) converges to our the continuous time dual value function,
W + (qt , ), described in Definition 1. We also prove, for some value of  , the convergence of the
original problem V (q0 ; n ) to W + (qt ,  ).

Theorem 2. Let m , m  N, denote a sequence such that limm m = 0, let V (q0 ; m ) and
W (q0 , ; m ) denote the discrete-time value functions defined in equations (10) and (11), and sup-
pose that the flow cost function satisfies Conditions 1-5.

                                                          17
   The value function of the continuous time problem of Definition 1, W (q0 , ), is bounded, convex,
and differentiable, and there exists a sub-sequence n  N such that, for all  if  < 1 and all
  (0, c- ) if  = 1,

                                     lim W (q0 , ; n ) = W (q0 , ).
                                    n

There exists a  such that
                                      lim V (q0 ; n ) = W (q0 ,  ).
                                     n

Moreover, it is without loss of generality to suppose that the diffusion terms (s ) of the optimal
policy associated with W (q0 , ) are zero.

Proof. See the appendix, section B.6.

     Theorem 2 demonstrates the convergence of the original and dual problems, and shows (as part
of the proof) that it is without loss of generality to assume there is no diffusion component. The
intuition for the latter result, which we mentioned previously, is that it is possible to synthesize a
"diffusion-like" process using the jump controls.
     We next present the convergence result that is specific to the  = 1 case.

Corollary 1. Let m , m  N, denote a sequence such that limm m = 0, let V (q0 ; m ) and
W (q0 , ; m ) denote the discrete-time value functions defined in equations (10) and (11), and sup-
pose that the flow cost function satisfies Conditions 1-5. If  = 1, then for all   (0, c- ),
limn W (q0 , ; n ) is equal to the dual value function defined in Definition 2, and the limit
limn V (q0 ; n ) is equal to the value function defined in Definition 2.

Proof. See the appendix, section B.10.

   These results demonstrate that the value functions in our discrete time problems converge to
the value functions of our continuous time problem. However, the results leave unanswered two key
questions: first, what are the dynamics of beliefs in the continuous time problem, and second, are
these dynamics the limit of the optimal policies in the discrete time problem? To address these
questions, we present conditions under which the beliefs will evolve gradually and conditions under
which they will evolve in large increments.


4.1    Gradual Learning
We begin by defining what we call a "preference for gradual learning." This condition describes the
relative costs of learning via jumps in beliefs vs. continuously diffusing beliefs, which are governed
by the properties of the divergence D that defines the cost function up to first order (Theorem 1).

Definition 3. The cost function C (p, q ; S ) exhibits a " preference for gradual learning " if the asso-
                                    ¯(q ) satisfy, for all q, q  P (X ) with q
ciated divergence D and its Hessian k                                                q,
                                          ^ 1
                                                  ¯(sq + (1 - s)q )ds)(q - q )  0.
                  D (q ||q ) - (q - q )T ( (1 - s)k                                                 (16)
                                          0


                                                   18
This preference is "strict" if the inequality is strict for all q = q , and is "strong" if, for some  > 0
and some m > 0,
                                     ^ 1
                                             ¯(sq + (1 - s)q )ds)(q - q ) > m||q - q ||2+ .
             D (q ||q ) - (q - q )T ( (1 - s)k                                                                  (17)
                                                                                       X
                                        0

   Recall that a divergence is a Bregman divergence if it can be written, using some convex function
H : P (X )  R, as
                           DH (q ||q ) = H (q ) - H (q ) - (q - q )T · Hq (q ).

To clarify the meaning of a preference for gradual learning, consider the "chain rule" (Cover and
Thomas [2012]) that characterizes the Bregman divergences (including in particular the Kullback-
Leibler divergence),
                         DH (q ||q ) +   s DH (qs ||q ) =   s DH (qs ||q )
                                              sS                    sS

for all probability distributions s such that sS s qs = q . The following lemma demonstrates
that, for any divergence D for which this chain rule is a "less-than-or-equal" inequality, there is a
preference for gradual learning.

Lemma 2. If the divergence D satisfies, for all s  P (S ) and q, q , {qs }sS  P (X ) such that
  sS s qs = q and q    q,

                               D (q ||q ) +        s D (qs ||q )         s D (qs ||q ),
                                              sS                    sS


and is twice-differentiable in its first argument, then the cost function C (p, q ; S ) exhibits a preference
for gradual learning.

Proof. See the appendix, section B.11.

    This chain-rule inequality has a simple interpretation­ it is more costly to jump directly to the
beliefs {qs } than to have beliefs travel first to q and then on to {qs }. It is straightforward to see why
such an assumption leads directly to gradual learning, although it is worth noting that the preference
for gradual learning is a weaker condition than this chain rule inequality. This chain rule inequality
could also be called "super-additivity," as the opposite of the "sub-additivity" assumption discussed
by Zhong [2017].11 In the next subsection, building on the results of Zhong [2019] and Zhong [2017],
we will show in our model that that opposite inequality leads to immediate decision-making.
    We begin by stating our gradual learning result for the  = 1 case. With a preference for
gradual learning, we show that the value function converges to a continuous time problem with only
a diffusion control (that is, without jumps). If the preference is strict, then the limiting process for
beliefs must be a diffusion. If not (i.e. if D is a Bregman divergence, as in Zhong [2019]), there
may be sequences of optimal policies in the discrete time models that converge to jump-diffusions,
or even pure jump processes.
  11 This is not the exact mirror of the sub-additivity assumption, because the inequality only applies to D  , which

describes the cost of a small amount of information, as opposed being an assumption on the cost function C (·) as in
Zhong [2017].


                                                         19
Theorem 3. Under the assumptions of Theorem 2, if  = 1 and the cost function satisfies a
preference for gradual learning, there exists a sub-sequence, indexed by n, and a  such that

                     lim W (q0 ,  ; n ) = lim V (q0 ; n ) = W + (q0 ,  ) = V (q0 ),
                    n                        n


where
                             V (q0 ) =          sup                u(q ) -  )]
                                                               E0 [^
                                         {t R|X |×|X | }, R+

                                 T
subject to the constraints that qt t = 0,

                                           dqt = Diag (qt )t dBt

and
                                          1      T            1
                                            tr[t t k (qt )]    c.
                                          2
If the cost function exhibits a strict preference for gradual learning, every convergent sub-sequence
                     
of belief processes qt,n associated with optimal policies in the discrete-time model converges in law to
a diffusion.

Proof. See the appendix, section B.12.

    The sequence problem V (q0 ) has a straightforward interpretation­ the DM controls the diffu-
sion coefficient of her beliefs subject to a maximum rate of information acquisition defined by the
information cost matrix function k (qt ).
    Under an additional assumption (described in the next section), a strong preference for gradual
learning is not only sufficient but necessary for beliefs to follow a diffusion process in the  = 1
case. In particular, we will demonstrate that if, for all utility functions, the belief process in the
continuous time limit is a diffusion, then the divergence D must exhibit a preference for gradual
learning. However, to make this statement, we must be able to characterize the belief dynamics,
which we are able to do given an additional assumption. We therefore postpone our proof of necessity
to the next section.
    We next turn to the case with discounting ( < 1). A remarkable result by Zhong [2019] (theorem
5 of that paper) demonstrates that, in a model very much like our continuous time limit, specialized
to the case of only two states and with no linear time costs, the DM will generically choose to have no
diffusion in her process, only jumps. This result can be understood in two parts. First, as discussed
previously, it is without loss of generality to write the DM's belief process as a pure jump process,
even when the law of the supremum over such processes is equivalent to the law of a diffusion.
    The second part of the result of Zhong [2019] can be thought of as a (generic) lower bound on the
magnitude of the jumps. That is, not only is it without loss of generality to consider a pure jump
process, but the optimal policy of the DM is in fact a jump process with non-infinitesimal jumps.
What is remarkable about this result, from the perspective of our results for the  = 1 case, is that
it applies even when the cost function exhibits a preference for gradual learning. Zhong [2019] also
shows, in the particular case of indifference to gradual learning (equality in equation (16)), which is


                                                      20
the setting for most of his results, that the beliefs jump all the way to stopping points, a result we
will replicate below.
    To understand how our  = 1 results are connected to the results of Zhong [2019], for the  < 1
case, we suppose that the cost function exhibits a "strong" preference for gradual learning, as defined
in Definition 3 and equation (17). Under this assumption, we prove an upper bound on the size of
the jumps, as a function of  , and show that in the limit as   1, the bound converges to zero. In
other words, there is no discontinuity­ with discounting and a strong preference for gradual learning,
the "pure jump process" for beliefs will become increasingly like a diffusion as the discount factor
converges to unity. Recall that m and  are part of the definition of a strong preference for gradual
learning, and u¯ is the upper bound on the flow utility.

Theorem 4. In problem defined in Definition 1, if the cost function satisfies a strong preference for
                                                                    
gradual learning (equation (17)), then the optimal jump directions zt satisfy

                                                    -u
                                                     ¯ ln( ) -1
                                      ||zt ||X  (           ) ,
                                                       m
              
and lim 1- ||zt ||X = 0. Moreover, jumps always strictly increase the value function,

                                              
                                    W (qt- + zt , ) > W (qt- , ).

Proof. See the appendix, section B.13.

    The optimal policy for the discounting case features upward (in the sense of the value function)
jumps and downward drift. The jumps become shorter and more frequent as the discount factor
approaches one, eventually converging to a diffusion. The fact that jumps only increase and never
decrease the value function is a consequence of the exponential discounting. Exponential discounting
can be thought of as a penalty for delay that is increasing in the current level of the value func-
tion. For this reason, drifting upward and jumping downward are sub-optimal, because the former
causes information to be acquired at a time when the cost of delay is high, and the latter acquires
information at a time when the cost of delay is high rather than waiting for the cost of delay to
decrease. We should note that the result that jumps always strictly increase the value function
requires only a strict preference for gradual learning, rather than a strong preference, and such a
result is without loss of generality (but not necessarily required) under a weak preference for gradual
learning. Our result that jumps always increase the value function is reminiscent of a result in Che
and Mierendorff [2019] and Zhong [2019], which is sometimes characterized as the DM seeking out
confirmatory evidence. For reasons that we discuss below in our analysis of stopping times (and are
illustrated by the one-sided learning example of Zhong [2019]), we prefer not to describe the result
in this fashion.
    Having analyzed the case of a preference for gradual learning, we next turn to the case of a
preference for discrete learning,




                                                    21
4.2    Discrete Learning
Building on Zhong [2019] and Zhong [2017], to provide contrast to our results on discrete learning,
we provide conditions under which, in the  = 1 case, the DM jumps immediately to stopping
beliefs. We define what we call a "preference for discrete learning" if the divergence D satisfies the
opposite of the "chain rule" inequality described previously (Lemma 2).

Definition 4. The cost function C (p, q ; S ) exhibits a " preference for discrete learning " if divergence
D associated with it satisfies, for all s  P (S ) and q, q , {qs }sS  P (X ) such that sS s qs = q
and q    q,
                          D (q ||q ) +      s D (qs ||q )        s D (qs ||q ).                        (18)
                                           sS                   sS

The cost function C (p, q ; S ) exhibits a " strict preference for discrete learning " if this inequality is
strict for all s , q, q , {qs }sS with q = q and qs = q for some s  S such that s > 0.

    If the cost function satisfies a preference for discrete learning, it its cheaper for the DM to jump
to beliefs {qs } rather than visit the beliefs q . Unsurprisingly, because this holds everywhere, it
leads to optimal policies that stop immediately after jumping. Before discussing this result, we note
that our definitions of a preference for discrete learning and a preference for gradual learning are
not symmetric, although they are linked by Lemma 2. As mentioned previously, our definition of a
preference for gradual learning includes cost functions that do not satisfy the chain rule inequality
everywhere. The following lemma clarifies the relationship between a preference for gradual and
discrete learning, showing that the strict versions of those preferences are incompatible.

Lemma 3. Assume that the divergence D associated with the cost function C (p, q ; S ) is twice-
differentiable with respect to its first argument. If a cost function C (p, q ; S ) exhibits a strict preference
for gradual learning, it does not exhibit a preference for discrete learning. If a cost function C (p, q ; S )
exhibits a strict preference for discrete learning, it does not exhibit a preference for gradual learning.

Proof. See the appendix, section B.14.

    Note in particular that any twice-differentiable Bregman divergence (recall equation (8)) exhibits
both a preference for gradual and discrete learning. Note also that there is no reason, in general,
to expect either of the chain rule inequalities to hold over the entire simplex. That is, the cases we
have an analyzed are not exhaustive, and there many be cost functions that lead to large jumps in
certain regions of the parameter space and small jumps or diffusions in other regions.
    We now turn to characterizing the behavior of beliefs given a preference for discrete learning.

Theorem 5. In the problem described by Corollary 1 (that is,  = 1), if the cost function satisfies
a preference for discrete learning (Definition 4), then it is without loss of generality to suppose that,
                                                                                                    
for all qt- in the continuation region and any zt  that characterizes an optimal policy , V (qt- + zt )=
           
u
^(qt- + zt ). That is, without loss of generality, all jumps enter the stopping region.

Proof. See the appendix, section B.15.



                                                      22
    The statement of Theorem 5 shows that is without loss of generality to assume that the DM
stops immediately after a jump in beliefs. This result would be necessary, as opposed to without
loss of generality, if the cost function exhibited a strict preference for discrete learning (although
we will see below that such cost functions may not exist). In contrast, in the case of indifference
(equation (18) holds with equality everywhere), both Theorem 3 and Theorem 5 hold. This happens
if D is the KL divergence, or a Bregman divergence more generally. This observation implies that
the two continuous time value functions must be identical, despite one being written as controlling
a diffusion process and the other (without loss of generality) a pure jump process. We revisit this
observation in the next section.
    For completeness, we also provide an analog to our result about the magnitude and direction
of jumps in the gradual learning case (Theorem 4) for the discounting case ( < 1). This result
generalizes slightly a result in appendix A.3 of Zhong [2019]. With a preference for discrete learning,
as with a preference for gradual learning, jumps will increase the value function. The intuition
is essentially the same as the gradual learning case, and comes from the observation that with
discounting, delay is particularly costly when the value function is high. However, unlike the gradual
learning case, in which jumps are of bounded size, with a preference for discrete learning jumps are
always immediately following by stopping.

Theorem 6. In the problem defined in Definition 1, if  < 1 and the cost function satisfies a
                                                                                                         
preference for discrete learning (Definition 4), then for all qt- in the continuation region and any zt
                                                                 
that characterizes an optimal policy , V (qt- + zt )=u ^(qt- + zt  ). That is, without loss of generality,
all jumps enter the stopping region. Moreover, jumps always strictly increase the value function,

                                               
                                     W (qt- + zt , ) > W (qt- , ).

Proof. See the appendix, section B.16.

   Moving beyond the results of Zhong [2019], we provide the "only-if" result: if a divergence always
results in large jumps and immediate stopping, then it must satisfy a preference for discrete learning.
The intuition is that if it is always optimal to jump outside the continuation region, it cannot possibly
be less costly, in the sense of the divergence D , to jump to some intermediate point. Otherwise,
there would be some utility function for which such behavior is optimal.

Theorem 7. In the problem defined in Definition 1, if there exists a cost function C (p, q ; S ) and
associated divergence D such that, for all strictly positive utility functions ua,x , there exists an
                                                     
optimal policy such that V (qt- + zt  )=u  ^(qt- + zt ) for all qt- in the continuation region, then the
cost function satisfies a preference for discrete learning.

Proof. See the appendix, section B.17.

    We have proven the convergence of our discrete time problems to our continuous time rational
inattention problems and shown that there are at least two different kinds of belief dynamics that
might arise from the model, depending on whether there is a preference for gradual or discrete
learning. We next provide an example of a family of cost functions that exhibit a preference for

                                                   23
gradual learning, and demonstrate that, subject to a regularity condition, Bregman divergences are
the only cost functions that exhibit a preference for gradual learning.


4.3    Examples with a Preference for Discrete and Gradual Learning
Here, we provide an intuitive example of families of cost functions that generate preferences for
gradual and discrete learning. We provide the examples in the context of posterior-separable cost
functions,
                              C (p, q ; S ) = s (p, q )D (qs (p, q )||q ).
                                             sS

Note that our derivation of the model (Theorem 1) imposes a number of conditions on D (p||q ),
including continuity and convexity in its first argument.
    Cost functions exhibiting a (strict or strong) preference for gradual learning can be easily con-
structed from Bregman divergences. Suppose that

                          D (q ||q ) = f (H (q ) - H (q ) - (q - q )T · Hq (q )),

where f (·) is a twice-differentiable, strictly increasing, convex function with f (0) = 0, and H (·) is
a strictly convex function on the simplex. It is straight-forward to observe that the Hessian of D
evaluated at q = q is f (0)Hqq (q ), and by convexity

                        D (q ||q )  f (0)(H (q ) - H (q ) - (q - q )T · Hq (q )),

implying that the divergence D satisfies a preference for gradual learning. This preference will be
strict if f (·) is strictly convex, and will be strong if both f (·) and H (·) are strongly convex.
    Constructing examples of cost functions with a strict preference for discrete learning has proven
challenging. This class of cost functions may seem like a large class; however, as the following lemma
demonstrates, subject to mild regularity conditions it only contains uniformly posterior separable
cost functions (the Bregman divergence case).

Lemma 4. Suppose the cost function C (p, q ; S ) exhibits a preference for discrete learning and that
the associated divergence D is continuously differentiable in both its arguments. Then D is a
Bregman divergence and the cost function C (p, q ; S ) is a uniformly posterior-separable cost function.

Proof. See the appendix, section B.18. The proof builds on Banerjee et al. [2005].

    As noted above, if D is a Bregman divergence, it exhibits a (non-strict) preference for discrete
learning, and also a (non-strict) preference for gradual learning. Consequently, under mild regularity
conditions, no cost functions exhibit a strict preference for discrete learning. In contrast, many cost
functions exhibit a strict or strong preference for gradual learning, and many others fall into neither
category (i.e. they have a preference for gradual learning in some parts of the parameter space and
discrete learning in others). Combining this result with Theorem 7, we have demonstrated that,
subject to mild regularity conditions, the jump-and-immediately-stop result of Zhong [2019] holds
for all utility functions if and only if D is a Bregman divergence.

                                                    24
   The results of this section naturally lead to the question of whether the different belief dynamics
that occur under a preference for gradual or discrete learning lead to different predictions about the
DM's behavior. We explore this question in the next two sections.


5     The Equivalence of Static and Dynamic Models
In this section, we analyze the  = 1 continuous time model under a preference for gradual learn-
ing and a preference for discrete learning. The main result of this section is that, both with a
preference for gradual learning and a preference for discrete learning (under an integrability as-
sumption in the case of a preference for gradual learning, and assuming continuous differentiability
in the case of a preference for discrete learning), the value function with  = 1 is equivalent to a
static rational inattention problem with a uniformly posterior-separable cost function. Moreover,
any twice-differentiable uniformly posterior-separable cost function can be justified through either
of these routes.
    This result has several implications. First, it demonstrates that both jump and diffusion-based
models are tractable and that the value functions can be characterized without directly solving the
associated partial differential equation. Second, it provides a micro-foundation for the uniformly
posterior-separable cost functions that have been emphasized in the literature (see, e.g., Caplin
et al. [2019]). Third, it proves that the two approaches are completely equivalent in terms of the
predicted joint distribution of states x  X and actions a  A. That is, any joint distribution of
(x, a) that could be observed under discrete learning could be observed under gradual learning and
vice versa.
    One this last point, however, we do not mean to imply that the diffusion and jump processes
are equivalent. Both of them endogenously will result in the same joint distribution of actions and
states, but will have different predictions about the joint distribution of actions, states, and stopping
times. As a consequence, considering stopping times can help differentiate the two models, and we
take some steps toward this in the next section.
    Finally, before presenting our results, we should emphasize that in the case of gradual learning,
our results depend on additional assumptions beyond those implied by our derivation of the con-
tinuous time model. Our assumption for the gradual learning case is an integrability assumption,
and will not hold generically. In contrast, in the discrete learning case, we make the mild additional
assumption of continuous differentiability. Consequently, equivalence with static models holds for
essentially all cost functions with a preference for discrete learning but only some cost functions for
a preference for gradual learning, and essentially all cost functions with a preference for discrete
learning generate the same joint distribution of actions and states as some cost function with a
preference for gradual learning, but the reverse is not true.


5.1    Gradual Learning
To prove our equivalence result, we restrict our attention to information-cost matrix functions that
are "integrable," in the sense described by the following assumption.


                                                   25
                                                                               |X |
Assumption 1. There exists a twice-differentiable function H : R+  R such that, for all q in
the interior of the simplex,
                              Diag (q )k (q )Diag (q ) = Hqq (q ),                      (19)

where Hqq (q ) denotes the Hessian of H evaluated at q .

     This class includes a number of information-cost matrix functions of interest: for example, it
includes the case in which k (qt ) is the inverse Fisher information matrix, which we will show corre-
sponds to the standard rational inattention model, and the case in which k (qt ) is the "neighborhood-
based" function that we introduce in Hébert and Woodford [2018]. It is, however, a restrictive
assumption. It rules out, for example, constant k (q ) (a hypothetical H would have asymmetric
third-derivative cross-partials). We shall refer to the function H as the "entropy function," for rea-
sons that will become clear below. Note that H (q ) is convex, by the positive semi-definiteness of
k (q ), and homogenous of degree one (q T · Hqq (q ) = T k (q )Diag (q )-1 = 0).
     For every convex function H , there is a Bregman divergence,

                               DH (qs ||q ) = H (qs ) - H (q ) - (qs - q )T Hq (q ),

and a corresponding uniformly posterior-separable cost function.
   The problem we are analyzing is the result of Theorem 3, the problem with  = 1 and a diffusion
process for beliefs. To analyze our continuous time problem, we begin by proving that the information
constraint,
                                         1       T            1
                                           tr[t t  k (qt )]    c,
                                         2
binds. Because the constraint binds, we can substitute the constraint into the HJB equation for the
problem and obtain the following result:

Lemma 5. At every point at which the value function V (qt ) is twice-differentiable and the DM
chooses not to stop, for all  such that q T  = 0,

                              tr[ T {Diag (qt )Vqq (qt )Diag (qt ) - k (qt )} ]  0,                   (20)

            
where  =    1   , with equality under the optimal policy.
            c

Proof. See the appendix, Section B.19.

    The parameter , introduced in the lemma, describes the race between information acquisition
and time in this model. The larger the penalty for delay, and the tighter the information constraint,
the larger the parameter . We now describe our equivalence result, and then outline the key step
of its proof, which relies on this lemma.

Theorem 8. Under Assumption 1, the value function that solves the continuous time problem with
 = 1 (Definition 2) and with a preference for gradual learning is

                V (q0 ) =           max                  (a)(uT
                                                              a · qa ) -          (a)DH (qa ||q0 ),
                             P (A),{qa P (X )}aA
                                                   aA                       aA


                                                        26
subject to the constraint that aA  (a)qa = q0 , where DH is the Bregman divergence associated
with the entropy function H that is defined by Assumption 1.
    There exist maximizers   and qa 
                                     such that   is the unconditional probability, in the continuous
                                                    
time problem, of choosing a particular action, and qa , for all a such that   (a) > 0, is the unique
belief the DM will hold when stopping and choosing that action.

Proof. See the appendix, Section section B.20.

   The continuous time sequential evidence accumulation problem is equivalent to a static rational
inattention problem, with a particular uniformly posterior-separable cost function,

                                C (p, q0 ; S )   =        s (p, q0 )DH (qs (p, q0 )||q0 ),                           (21)
                                                     sS


and with the signal space S identified with the set of possible actions A.
    The mutual information cost function (6) proposed by Sims is one such cost function. In this
case, the entropy function H is the negative of Shannon's entropy (4), the corresponding information-
cost matrix function is the inverse Fisher information matrix k (q ) = Diag (q ) - qq T , the Bregman
divergence is the Kullback-Leibler divergence (5), and the information measure defined by (21) is
mutual information (6). Thus Theorem 8 provides a foundation for the standard static rational
inattention model, and hence for the same predictions regarding stochastic choice as are obtained by
Matêjka et al. [2015]. On the other hand, Theorem 8 also implies that other cost functions can also
be justified. Indeed, any (twice-differentiable) uniformly posterior-separable cost function (21) can
be given such a justification, by choosing the information cost matrix function defined by equation
(19).
    However, not all information cost matrix functions are reasonable. We interpret the information
cost matrix function k (q ) as a description of how hard it is to distinguish any pair of states. In many
economic applications, there is a natural ordering or structure of the states, and we would like the
information cost matrix function and the associated entropy function and Bregman divergence to
reflect this structure. In Hébert and Woodford [2018], we propose such a cost function.
    We next outline the key step of our proof. Our proof strategy is best described as "guess and
verify," in that we start with the static value function described in Theorem 8 and then show that it
is the value function of the continuous time model described in Theorem 3. The key step of the proof
is to show that the static value function satisfies (20) in Lemma 5.12 For expositional purposes, we
will assume that the optimal policies of the static model,   (a; q0 ) and qa 
                                                                              (q0 ), are differentiable with
respect to q0 and strictly interior (we do not require these assumptions in the proof).
    We begin by examining the first-order conditions with respect to qa , and applying the envelope
theorem. Let (q0 ) denote the vector of multipliers on the constraint that aA  (a)qa = q0 . We
  12 Technical footnote: the "anywhere the value function is twice differentiable" caveat of Lemma 5 is relevant for our

problem. The PDE described by equation (20) is "degenerate elliptic" and hence will not in general have a classical
solution. Indeed, we do not prove that our static value function is twice differentiable everywhere, and suspect it is not
at points where the "consideration set" (the set of actions with  (a) > 0, Caplin et al. [2018]) changes. In our proof,
we establish that the static problem value function is convex and continuously differentiable, which is sufficient to
invoke a generalized version of Ito's lemma for convex functions to verify that the static value function is the solution
to the continuous time problem.


                                                           27
have the first-order condition (FOC) and envelope theorem condition (ET),

                                                    
                             FOC: ua - (q0 ) - Hq (qa (q0 )) + Hq (q0 ) = 0, a  A,                                             (22)

                     ET: Vq (q0 ) = (q0 ) +                (a; q0 )(qa
                                                                     
                                                                       (q0 ) - q0 )T · Hqq (q0 ) = (q0 ).
                                                   aA

Now consider a perturbation q0  q0 + z , for some tangent vector z . Combining the FOC and ET,
and then differentiating with respect to and evaluating at = 0,
                                                                                 
                                                                               dqa (q0 + z )
                     Vqq (q0 ) · z = Hqq (q0 ) · z - Hqq (qa (q0 )) ·                        |    =0 ,   a  A.                 (23)
                                                                                     d
                                                           d(  (a;q0 + z )qa
                                                                           
                                                                             (q0 + z ))
Observe that, due to the constraint,                 aA               d                 | =0   = z . Multiplying both sides of
                       d(  (a;q0 + z )qa
                                       
                                         (q0 + z )T )
equation (23) by                   d                  | =0 ,   and then taking sums,

            z T · Vqq (q0 ) · z = z T · Hqq (q0 ) · z
                                                                                                          
                                                       dqa (q0 + z )                                    dqa (q0 + z )
                              -            (a; q0 )(                 |     =0 )
                                                                               T           
                                                                                   · Hqq (qa (q0 )) ·                 |   =0
                                                             d                                                d
                                    aA
                                          d  (a; q0 + z )                               
                                                                                                       
                                                                                                     dqa (q0 + z )
                              -                           |      =0   qa (q0 )T · Hqq (qa (q0 )) ·                 |   =0 .
                                                d                                                          d
                                    aA


By Assumption 1, q T · Hqq (q ) = 0, and hence the last line in this expression is zero. By the convexity
of H , the summation on the second line is positive. Therefore, by Assumption 1,

                              z T · Vqq (q0 ) · z  z T · Diag (q0 )-1 k (q0 )Diag (q0 )-1 · z,

establishing that (20) holds. To show that there is a direction z  in which (20) holds with equality,
                                    dq  (q0 + z  )
it is sufficient to show that a d                  | =0 = 0 for all a  A. In any direction z spanned by the
                                                                         
initial qa (q0 ) - q0 , it is not optimal for the DM to change the qa     (q0 ), only the probabilities   (a, q0 )
(this property, "Locally Invariant Posteriors," was shown by Caplin et al. [2019]). Thus, any of these
directions can serve as z  .13
    We conclude that all continuous time models with gradual learning that also satisfy our integra-
bility condition are equivalent to a static model with a uniformly posterior-separable cost function,
and that any such static model can be justified from some model with gradual learning. We next
show that the same set of static models can be justified from a model with discrete learning. Before
proceeding, however, we observe that this result allows us to demonstrate that a preference for grad-
ual learning is necessary for beliefs to always result in a diffusion process, provided that Assumption
1 holds.

Corollary 2. In the problem defined in Definition 2 (the  = 1 case), if a cost function C (p, q ; S ),
associated divergence D , and information cost matrix function k (q ) satisfying Assumption 1 are
  13 That any such direction can serve as z  indicates that there are (usually) many optimal policies in the continuous
time problem that achieve the same value function. Intuitively, at each point, if the DM does not learn in some
particular direction, she could always learn in that direction in the next instant.


                                                                  28
such that, for all strictly positive utility functions ua,x , there exists an optimal policy such that
 ¯s = 0 everywhere in the continuation region (meaning beliefs follow a diffusion process), then the
cost function satisfies a preference for gradual learning.

Proof. See the appendix, section B.21.


5.2       Discrete Learning
The result with a preference for discrete is an immediate corollary of Lemma 4 and the preceding
Theorem 8 (the result with gradual learning).

Corollary 3. Assume the cost function C (p, q ; S ) exhibits a preference for discrete learning, and
the associated divergence D is continuously differentiable. Then D is a Bregman divergence and
value function that solves the continuous time problem with  = 1 (Definition 2) is identical to the
static rational inattention problem described in Theorem 8 with D in the place of DH .

Proof. Immediate from Lemma 4 and Theorem 8.

    The requirement that D be continuously differentiable is likely unnecessary for the result. Our
derivation of D requires that it be convex in its first argument and continuous in both arguments.
Applying "mollification" techniques for convex functions (see for example Banerjee et al. [2005])
would likely allow us to extend the result to all D functions satisfying the conditions of Theorem 1.
    Given any uniformly posterior-separable cost function in a static rational inattention model, by
setting D equal to the Bregman divergence associated with that cost function, we can justify that
static model as the result of a dynamic model with a preference for discrete learning. We therefore
conclude that models with a preference for gradual learning satisfying our integrability condition
and models with a preference for discrete learning are indistinguishable from the perspective of their
predictions about the joint distribution of states and actions.14 In the next section, we begin to
explore how information about stopping times can be used to distinguish the models.


6         Implications for Response Times
Because our model is dynamic, the observable behavior of the DM includes not only the joint
distribution of actions and states, but also information about the length of time taken to decide,
including how this may vary depending on the action and the state. The psychophysics literature
gives considerable emphasis to facts about response times as a window on the nature of decision
processes (e.g., Ratcliff and Rouder [1998]). In economic contexts as well, response times provide
important information that can be used to discriminate between models, even when response times
themselves are not what the economic analyst cares about. For example, Clithero [2018] and Alos-
Ferrer et al. [2018] argue that preferences can more accurately be recovered from stochastic choice
data when data on response times are used alongside observed choice frequencies.
  14 Insituations in which the static rational inattention problem does not itself have a unique solution, we have not
ruled out the possibility that the models with discrete and gradual learning will make different predictions. However,
we have no reason to believe this is the case.


                                                         29
    Here we propose that data on response times can be used to discriminate between alternative
information-cost specifications. We will show that cost functions that are equivalent in the sense
of implying the same value function nevertheless make different predictions about the stopping
time conditional on taking a particular action. Consequently, data on stopping times can inform
us about whether there is a preference for gradual learning or for learning through discrete jumps.
Interestingly, it is possible to distinguish between these two hypotheses even when (as in the problems
considered here) actions are taken only infrequently.15
    We illustrate how response time data can discriminate between the two hypotheses using a simple
example. In this example, we consider the no-discounting limit,   1. We focus on this limit for
several reasons. First, with regards to decision-making experiments, the cumulative discount factor
over the length of time required to make a decision (often seconds or minutes) should be small.
Second, it is easier to characterize stopping times when beliefs are a diffusion process than when
they are a sequence of small jumps. Third, as noted by Fudenberg et al. [2018], behavior in the
 < 1 case is not invariant to transformations of the utility function. Fourth, and most significantly,
our results from the previous section demonstrate that in this case the gradual-learning and jump-
learning approaches are equivalent in terms of their predictions for the joint distribution of actions
and states. We consider the limit   1, rather than simply assuming no discounting, as this allows
us to choose, in the case of a preference for discrete learning, between the multiple optimal policies
for the  = 1 case.
    We also restrict attention to the case of only two states. This allows us to ignore the integrability
condition in the gradual learning case, as it is always satisfied. We can then compare cost functions
with a preference for gradual and discrete learning that generate the exact same predictions for
the joint distribution of actions and signals. Second, with  = 1, two states, and a preference for
gradual learning, the optimal policies that generate the stochastic process for beliefs are uniquely
determined by the results of Theorem 3. Similarly, in the limit as  approaches 1 from below, with
two states and a preference for discrete learning, the optimal policies that generate the stochastic
process for beliefs are uniquely determined by the results of Theorem 6.16
    The difference that we illustrate concerns the way in which changes in the payoffs associated with
particular actions is predicted to affect the stopping time dynamics. We will show that a preference
for gradual learning implies an invariance result, in which a shift in the level of the utility for all
actions in a given state does not change the joint distribution of actions, states, and stopping times.
In contrast, we will show that such a shift can radically alter the distribution of stopping times
under a preference for discrete learning.
    Label the states X = {G, B }, let the two actions available be A = {L, R}, and suppose that
the optimal action in state G is L, while the optimal action in state B is R. We assume that these
actions are strictly optimal, meaning that u(L, G) > u(R, G) and u(R, B ) > u(L, G) (without these
assumptions, the DM should immediately stop and not gather any information). We also assume,
  15 It would obviously be easier to tell whether beliefs evolve continuously or in discrete jumps in a case where the

DM is required to continuously adjust some response variable that can provide an indicator of her current state of
belief.
  16 In the  = 1 case, with a preference for discrete learning, there are multiple optimal policies. However, only one

of these is the limit of the optimal policies from the  < 1 case.



                                                         30
without loss of generality, that taking the optimal action in state G generates a weakly higher payoff
than taking the optimal action in state B , u(L, G)  u(R, B ).
    With two states, we can interpret the beliefs q as a scalar, and we adopt the convention that q
is the probability of state G. We compare two different situations, one in which the value function
W + (q, ) is "u-shaped" and one in which the value function W + (q, ) is upward-sloping. The "u-
shaped" value function occurs when u(R, B ) > u(R, G); that is, when taking the correct action in
state B generates a higher payoff than taking the wrong action in state G. The downward-sloping
value function occurs when u(R, B )  u(R, G). In the case, the state G is simply better for the DM
than the state B , regardless of the action the DM takes. For both of these models, there will be
unique beliefs 0 < qR < qL < 1 associated with taking actions R and L, respectively.
    The objects we will study are the cumulative probabilities of stopping by time t and taking
action L or R, FL (t) and FR (t). We also study the conditional (on states G and B ) versions of
these processes, FL,G (t), FL,B (t), FR,G (t), FR,B (t). We construct two different numerical examples,
described in terms of the utility function.

Case 1.    u(L, G) = u(R, B ) = 3, u(L, B ) = u(R, G) = 2

Case 2.    u(L, G) = 3, u(R, B ) = u(R, G) = 2, u(L, B ) = 1

Note that in both cases, utility increases by one when making the correct decision relative to the
wrong decision (u(L, G) - u(R, G) = u(R, B ) - u(L, B ) = 1). What differentiates the two cases is
that in case 2 utilities are lower by one in state B . This is convenient because it implies that the
stopping beliefs qR and qL are identical in the two cases. This result in fact applies regardless of the
number of states and actions, as the following lemma describes.

Lemma 6. In the problem described by Definition 2 (that is,  = 1), if ua,x ( ) = ua,x + vx for
some v  R|X | , then the set of joint distributions of stopping times, actions, and states under optimal
policies is identical for all  R.

Proof. This follows immediately from the invariance of the value function to level shifts in the utility
function.

    This conclusion depends only on the assumption that  = 1. It also provides a way of testing
if  = 1 or not­ does shifting all rewards up or down change the observed distribution of stopping
times, actions, and states? Note also that this result holds regardless of whether the cost function
exhibits a preference for gradual or discrete learning. However, the import of the result is different
in the two cases.
    With a strict preference for gradual learning, in the two-state case, there is a unique optimal
policy involving a diffusion, which generates a unique joint distribution of stopping times, actions,
and states, and this unique distribution will occur with both the case 1 and case 2 utility functions.
Moreover, this unique optimal policy is the limit of the optimal policies in the  < 1 case. This
result, for the case of a strict preference for gradual learning, does depend on the assumption that
 = 1. That is, with  < 1, different policies will be optimal under case 1 and case 2 utility functions,



                                                  31
although they will remain qualitatively similar and converge to an identical limit as  approaches
one.
    In contrast, with a preference for discrete learning, there are many optimal policies, all of which
lead to the same joint distribution of actions and states, but which vary in terms of their joint
distribution of actions, states, and stopping times. Moreover, in the two state case, only one policy
from this set is the limit of the optimal policies from the  < 1 case (which are themselves unique),
and which policy corresponds to this limit depends on whether the utility function is from case 1 or
case 2.
    Putting together Theorem 8, Corollary 3, and Lemma 6, we proceed by choosing a cost function
that satisfies a preference for discrete learning, and construct the cost function with a strict preference
for gradual learning that makes equivalent predictions about the joint distributions of actions and
states. We then analyze the stopping time distributions FL (t) and FR (t) with gradual learning and
for both cases with a preference for discrete learning.
    Because it is a standard in the literature, we choose mutual information as our cost function
that satisfies a preference for discrete learning. The associated divergence D is the Kullback-
Leibler divergence, which is a Bregman divergence. To construct another cost function with a strict
preference for gradual learning that makes identical predictions, we use a posterior-separable cost
function with
                                         D (p||q ) = f (DKL (p||q )),                                  (24)

where f is any strictly-increasing, strictly convex function with f (0) = 0 and f (0) = 1 (all such
functions will generate the same predicted behavior).
                                                        1
   To complete the model specification, we set  =   c = 1, and assume a prior q0 = 3      5 . Using
Theorem 8 and Corollary 3, it follows that the stopping boundaries qL and qR solve

                                max               L qL u(L, G) + L (1 - qL )u(L, B )+
                      L [0,1],qL [0,1],qR [0,1]

                                 (1 - L )qR u(R, G) + (1 - L )(1 - qR )u(R, B )-
                                        L DKL (qL ||q0 ) - (1 - L )DKL (qR ||q0 ),

subject to L qL + (1 - L )qR = 3     5 . The unique solution to this problem, for both case 1 and case
             e                         1
2, is qL = 1+   e   0 . 73 and q R = 1+e  0.27. As mentioned previously, these stopping boundaries
apply regardless of whether the cost function exhibits a preference for discrete learning or a strict
preference for gradual learning.
    We begin by discussing the case of a strict preference for gradual learning. This case is in one sense
simple. Beliefs follow a diffusion process, eventually hitting one of two boundaries. Consequently,
the cumulative distributions FL (t) and FR (t) are the first-passage times of a diffusion process. The
diffusion coefficient of this process is determined by the information cost matrix function k (qt ) (see
Theorem 3). The resulting CDFs FL (t) and FR (t) will generally have "sigmoid" shapes. What
makes this case complicated is that the diffusion coefficient is usually not constant, and as a result
we cannot rely on analytical results about normal distributions.
    Fortunately, in our example, the resulting diffusion process is well-studied. When D is defined


                                                       32
as in equation (24), the associated Hessian matrix k ¯ is the Fisher information matrix. As a result,
the dynamics of beliefs follow a diffusion that is studied in the mathematical genetics literature as
the "Fisher-Wright" model.17 The lemma below summarizes these results.

Lemma 7. In the problem described by Corollary 1 (that is,  = 1), if the divergence D is defined
by equation (24), the process for beliefs in the two-state model is a diffusion,

                                                    1                    1
                                         dqt = (2  cqt (1 - qt )) 2 dBt .

Conditional on the true state being G, the process is

                                         1                           1           1
                               dqt = 2  c(1 - qt )dt + (2  cqt (1 - qt )) 2 dBt ,

and conditional on the true state being B the process is

                                              1                  1           1
                                 dqt = -2  cqt dt + (2  cqt (1 - qt )) 2 dBt .

Proof. See the appendix, section B.22.

   Armed with this diffusion process, we can solve for the functions FL (t) and FR (t). Let L (q, t,  )
be the probability of hitting qL before time  given qt = q , and observe that FL (t) = L (q0 , 0, t).
By the Markov property, L (q, t,  ) = L (q, 0,  - t), which we abbreviate as L (q, s).
   By the usual dynamic programming arguments, we have

                                                        1
                                       L                      L
                                       s (q, s) =  cq (1 - q )qq (q, s),
                                                  




with the boundary conditions L (qL , s) = 1 for all s  0, (qR , s) = 0 for all s  0, and (q, 0) = 0
for all q  (qL , qR ). The same partial differential equation, but with different boundary conditions,
characterizes FR (t). The conditional (on G or B ) partial differential equations involve a first-order
term for the drift, but are otherwise identical.
    We numerically solve this PDE, and show in figure 1 that (intuitively) both FL (t) and FR (t)
have the expected sigmoid shapes. Note, as discussed above, that these stopping time distributions
apply in both case 1 and case 2. We show conditional (on G or B ) CDFs in figure 2.
  17 The connection between these models, as indicated by R. A. Fisher's name appearing in both the Fisher-Wright

model and the Fisher information matrix, is not a coincidence. One minor difference between our application and the
Fisher-Wright model is the location of the boundaries. Morris and Strack [2019] study this process as a special case
of our framework.




                                                            33
      Figure 1: Stopping Times, Strict Preference for Gradual Learning




Figure 2: Conditional Stopping Times, Strict Preference for Gradual Learning




                                    34
   We now turn to the case of discrete learning. Consider first the symmetric case, case 1. In this
case, because the value function is symmetric, it is minimized at q = 1
                                                                      2 . It follows by our result that
the value function always jumps up and drifts down (or the equivalent result in Zhong [2019]) that
while q > 12 , the intensity of jumping to qL is

                                                           1

                                         ¯L (q ) =      c
                                                                   ,
                                                     DKL (qL ||q )

and the intensity of jumping to qR is zero. After this point, this hazard rates will both be equal
                                                                  1
to half this value. Define  12
                               as the time at which qt reaches 2    . Because beliefs are martingales,
                                                                                         qL -q0
the likelihood of this occurring (that is, of failing to jump before reaching q = 1
                                                                                  2 ) is qL - 2
                                                                                              1 . For all

t > 1 2
        ,

                               qL - q0      1 qL - q0             ¯ 1
                   FL (t) = (1 -      1 ) + 2(      1 )(1 - exp(-L ( 2 )(t -  2 ),
                                                                              1
                               qL - 2          qL - 2
                           1 qL - q0              ¯ 1
                   FR (t) = (      1 )(1 - exp(-L ( 2 )(t -  2 ).
                                                              1
                           2 qL - 2

                                                                                       1
   Prior to time  2
                  1 , conditional on not stopping, the beliefs q (t) drift towards q =
                                                                                       2,


                                      q (t) = ¯L (q (t))(q (t) - qL ),


and the probability of stopping accumulates at a hazard rate based on ¯L (q (t)),


                                     FL (t) = (1 - FL (t))¯L (q (t)).


Solving this system of differential equations with initial conditions FL (0) = 0 and q (0) = q0 , and
                                                                  q0 - 1
finding the unique time value where q ( 1  2
                                             )= 1
                                                2 and FL ( 2 ) = qL - 2
                                                            1          1 , which occur simultaneously,
                                                                       2


characterizes the system.
     Our numerical solution is shown below in figure 3, and the conditional stopping times are shown
in figure 4. Note that, aside from a few kinks and the like, the stopping time distribution is almost
identical to the case of a preference for gradual learning.




                                                     35
      Figure 3: Stopping Times, Preference for Discrete Learning, Case 1




Figure 4: Conditional Stopping Times, Preference for Discrete Learning, Case 1




                                     36
    Now consider the asymmetric case, case 2. In this case, the value function is minimized at qR ,
                                   d
due to smooth pasting. That is, dq   ^(q )|q=qR = 0 because u(R, B ) = u(R, G), and by the convexity
                                     u
                     18
of the value function , it follows that the value function is increasing on its domain q  [qR , qL ]. It
immediately follows by Theorem 6 that, in the  < 1 case, the DM will always attempt to jump to
qL and drift towards qR , a result Zhong [2019] calls one-sided learning.
    The system that characterizes beliefs is identical to the t <  1  2
                                                                        domain of the previous case,
                                      q0 -qR
stopping at q = qR and FL (R ) = qL -qR instead. After R , the DM immediately decides to stop
and choose action R. As a result, the cumulative distributions of stopping times look quite a bit
different than the corresponding functions from case 1. We show these results below, in figure 5,
and the conditional results in figure 6.
    We are not aware of any direct experimental evidence that manipulates the level of payoffs while
preserving the incentive for correct choices, though we find the predicted abrupt effect of a small
change in payoffs implausible.19 Note that the abrupt change in behavior under a preference for
discrete learning depends on use of the   1 limit as an optimal policy selection device. In a model
with  < 1, the DM's behavior is not invariant to transformations of the utility function. The use
of   1 as a selection device yields predicted behavior even in the limit of no discounting that
continues not to satisfy the invariance property.20
  18 This follows from appendix Lemma 13, which shows convexity for the discrete value functions, and Theorem 2.
  19 An  experiment to test this prediction ought to be feasible.
  20 In our view, this issue should cast doubt on the usefulness of theories of behavior in the undiscounted case that

are premised on the importance of continuity with behavior in the discounted case.




                                                         37
      Figure 5: Stopping Times, Preference for Discrete Learning, Case 2




Figure 6: Conditional Stopping Times, Preference for Discrete Learning, Case 2




                                     38
7    Discussion and Conclusion
We have derived a continuous-time rational-inattention model as the limit of a discrete-time sequen-
tial evidence accumulation problem. In the limit of a very large number of successive signals, each
of which is only minimally informative, only the local properties of the flow cost function matter.
Using these properties, we have demonstrated (without discounting) cases in which beliefs converge
to either a pure diffusion process or a pure jump process. With discounting, we have described a
more general limiting problem, and shown that it may result in "diffusion-like" pure jump processes.
Furthermore, in the case without discounting, we have demonstrated that the resulting behavior
(under an additional integrability assumption in the gradual learning case) is equivalent to the be-
havior predicted by a static rational inattention model. This equivalence provides a relatively simple
way of generating predictions about the effects of variation in the opportunity cost of time for the
probability of choosing different actions, without having to numerically solve a complex dynamic
model.
    We have left unresolved the question of whether it is appropriate to use the discounting or no-
discounting models, and whether a preference for gradual learning is a reasonable way to characterize
the evolution of beliefs. It is worth noting that in many decision-making contexts, the time it takes to
make a decision (seconds or minutes) is likely to call for a cumulative discount factor (  ) very close
to one. As a result, the limiting case of  = 1 may provide a useful and tractable approximation. We
also note that when our key divergence D is a Bregman divergence, both our diffusion and jump
results apply. However, this does not imply that everything is the same about the two problems­
in particular, the joint distribution of decision times and actions taken may differ between the two
models. This joint distribution is the subject of a great deal of existing research (e.g. Fudenberg
et al. [2018]) and results in this literature might help determine whether a preference for gradual or
discrete learning provides a better description of decision making.
    The continuous time limit we have derived in this paper can serve as a foundation for analytically
tractable models of rational inattention. One strength of our approach is its generality­ we have
imposed relatively minimal assumptions on the nature of information costs, and yet derived models
that are specific and tractable enough to be tested in data. Exploring the properties of these
models, and in particular whether jump processes or diffusions better characterize beliefs, and what
cost functions should be employed, is the next step in this research agenda.


References
Syed Mumtaz Ali and Samuel D Silvey. A general class of coefficients of divergence of one distribution
  from another. Journal of the Royal Statistical Society. Series B (Methodological), pages 131­142,
  1966.

Carlos Alos-Ferrer, Ernst Fehr, and Nick Netzer. Time will tell: Recovering preferences when choices
  are noisy. Unpublished manuscript, October 2018.




                                                  39
Shun-ichi Amari and Hiroshi Nagaoka. Methods of information geometry, volume 191. American
  Mathematical Soc., 2007.

Nihat Ay, Jürgen Jost, Hông Vân Lê, and Lorenz Schwachhöfer. Information geometry and sufficient
  statistics. Probability Theory and Related Fields, pages 1­38, 2014.

Arindam Banerjee, Xin Guo, and Hui Wang. On the optimality of conditional expectation as a
  Bregman predictor. IEEE Transactions on Information Theory, 51(7):2664­2669, 2005.

David Blackwell. Equivalent comparisons of experiments. The annals of mathematical statistics, 24
  (2):265­272, 1953.

Jerome R Busemeyer and James T Townsend. Decision field theory: A dynamic-cognitive approach
  to decision making in an uncertain environment. Psychological Review, 100:432­459, 1993.

Andrew Caplin and Mark Dean. Revealed preference, rational inattention, and costly information
 acquisition. American Economic Review, 105(7):2183­2203, 2015.

Andrew Caplin, Mark Dean, and John Leahy. Rational inattention, optimal consideration sets, and
 stochastic choice. The Review of Economic Studies, 86(3):1061­1094, 2018.

Andrew Caplin, Mark Dean, and John Leahy. Rationally inattentive behavior: Characterizing and
 generalizing Shannon entropy. Unpublished manuscript, February 2019.

Yeon-Koo Che and Konrad Mierendorff. Optimal dynamic allocation of attention. American Eco-
  nomic Review, 109(8):2993­3029, 2019.

Nikolai Nikolaevich Chentsov. Statistical decision rules and optimal inference. Number 53. American
  Mathematical Soc., 1982.

John A Clithero. Improving out-of-sample predictions using response times and a model of the
  decision process. Journal of Economic Behavior & Organization, 148:344­375, 2018.

Thomas M Cover and Joy A Thomas. Elements of information theory. John Wiley & Sons, 2012.

Henrique De Oliveira, Tommaso Denti, Maximilian Mihm, and Kemal Ozbek. Rationally inattentive
  preferences and hidden information costs. Theoretical Economics, 12:621­624, 2017.

Mark Dean and Nathaniel Neligh.        Experimental tests of rational inattention.     Unpublished
 manuscript, June 2019.

Ernst Fehr and Antonio Rangel. Neuroeconomic foundations of economic choice -- recent advances.
  Journal of Economic Perspectives, 25(4):3­30, 2011.

Drew Fudenberg, Philipp Strack, and Tomasz Strzalecki. Speed, accuracy, and the optimal timing
  of choices. American Economic Review, 108(12):3651­84, 2018.

Benjamin Hébert and Michael Woodford. Information costs and sequential information sampling.
  NBER Working Paper no. 25316, November 2018.

                                                40
Jean Jacod and Albert Shiryaev. Limit theorems for stochastic processes, volume 288. Springer
  Science & Business Media, 2013.

Emir Kamenica and Matthew Gentzkow. Bayesian persuasion. American Economic Review, 101(6):
 2590­2615, 2011.

Ian Krajbich, Bastiaan Oud, and Ernst Fehr. Benefits of neuroeconomics modeling: New policy
  interventions and predictors of preference. American Economic Review, 104(5):501­506, 2014.

Harold Kushner and Paul G Dupuis. Numerical methods for stochastic control problems in continuous
 time, volume 24. Springer Science & Business Media, 2013.

Filip Matêjka, Alisdair McKay, et al. Rational inattention to discrete choices: A new foundation for
  the multinomial logit model. American Economic Review, 105(1):272­98, 2015.

Stephen Morris and Philipp Strack. The Wald problem and the relation of sequential sampling and
  ex-ante information costs. Unpublished manuscript, February 2019.

Giuseppe Moscarini and Lones Smith. The optimal level of experimentation. Econometrica, 69(6):
  1629­1644, 2001.

Huyên Pham. Continuous-time stochastic control and optimization with financial applications, vol-
 ume 61. Springer Science & Business Media, 2009.

Luciano Pomatto, Philipp Strack, and Omer Tamuz. The cost of information. arXiv preprint
  arXiv:1812.04211, 2018.

Roger Ratcliff. Theoretical interpretations of speed and accuracy of positive and negative responses.
  Psychological Review, 92:212­225, 1985.

Roger Ratcliff and Jeffrey N Rouder. Modeling response times for two-choice decisions. Psychological
  Science, 9:347­356, 1998.

R Tyrrell Rockafellar. Convex analysis. 1970.

JF Schouten and JAM Bekker. Reaction time and accuracy. Acta Psychologica, 27:143­153, 1967.

Michael Shadlen and Daphna Shohamy. Decision making and sequential sampling from memory.
 Neuron, 90(5):927­939, 2016.

Christopher A Sims. Rational inattention and monetary economics. Handbook of Monetary Eco-
 nomics, 3:155­181, 2010.

Jakub Steiner, Colin Stewart, and Filip Matjka. Rational inattention dynamics: Inertia and delay
  in decision-making. Econometrica, 85(2):521­553, 2017.

Satohiro Tajima, Jan Drugowitsch, and Alexandre Pouget. Optimal policy for value-based decision-
  making. Nature communications, 7, 2016.



                                                 41
Satohiro Tajima, Jan Drugowitsch, Nishit Patel, and Alexandre Pouget. Optimal policy for multi-
  alternative decisions. Nature Neuroscience, 22:1503­1511, 2019.

Cédric Villani. Topics in optimal transportation. Number 58. American Mathematical Soc., 2003.

Michael Woodford. Inattentive valuation and reference-dependent choice. Unpublished manuscript,
 May 2012.

Michael Woodford. Stochastic choice: An optimizing neuroeconomic model. Technical Report 5,
 2014.

Michael Woodford. Modeling imprecision in perception, valuation and choice. NBER Working Paper
 no. 26258, September 2019.

Weijie Zhong. Indirect information measure and dynamic learning. arXiv preprint arXiv:1809.00697,
 June 2017.

Weijie Zhong. Optimal dynamic information acquisition. Unpublished manuscript, January 2019.




                                               42
A     Cost Functions Satisfying Conditions 1-5
In this appendix section, we discuss various families of cost functions that satisfy our conditions.
    We first show that any posterior-separable cost function (in the terminology of Caplin et al.
[2019]) that is sufficiently convex satisfies conditions 1-5. This includes all uniformly posterior-
separable cost functions, such as mutual information and the neighborhood-based cost functions we
describe in Hébert and Woodford [2018].
    Posterior-separable cost functions are defined as

                                 C (p, q ; S ) =        s (p, q )D(qs (p, q )||q ),
                                                   sS


where D(·||·) is a divergence.

Lemma 8. In the posterior-separable family if cost functions, if the divergence D(·||·) is twice
differentiable and strongly convex in its first argument, and differentiable in its second argument, the
cost function C (p, q ; S ) satisfies Conditions 1-5.

Proof. See the appendix, section B.23.

   These posterior-separable cost functions a popular choice, but by no means the only possibility.
As an alternative, consider "state-separable" cost functions,

                                  C (p, q ; S ) =        qx D(px || (p, q ); S ),
                                                    xX


where D(px || (p, q ); S ) is a family of divergences defined on the signal alphabets S . Mutual infor-
mation is both "posterior-separable" and "state-separable," but in there are many cost functions in
one family but not the other.
   We will assume that the family of divergences is constant with respect to the addition of signals
that never occur. That is, if S  S , and eT            T
                                                s p = es p for s  S and zero otherwise, where es is a
vector with one corresponding to signal s and zero otherwise, then

                                 D(px || (p, q ); S ) = D(px || (p , q ); S ).

We also assume a Blackwell-type inequality for these divergences,

                                       D(r||r ; S )  D(r||r ; S )

for all r, r and all garbling matrices  : S  S . Note that this implies that D is invariant in the
sense of Chentsov [1982]. Under these assumptions, and some regularity assumptions, we prove that
this family also satisfies our conditions.

Lemma 9. In the state-separable family of cost functions, if the divergences D(·||·; S ) are convex in
their first argument and twice differentiable, and satisfies, for some m > 0 and all r, r  P (S ) with



                                                         43
r    r,
                               D(r ||r; S )  m(r - r)T Diag (r)+ (r - r),

then the cost function C (p, q ; S ) satisfies Conditions 1-5.

Proof. See the appendix, section B.24.

   Lastly, we note that if some of cost function C (p, q ; S ) satisfies our conditions, so does a convex
transformation of that cost function, or a linear combination of two cost functions. Consequently,
the two "separable" families described above can be used to generate a huge variety of non-separable
cost functions.

Lemma 10. If C (p, q ; S ) is a family of cost functions satisfying Conditions 1-5, then so is Ch (p, q ; S ) =
h(C (p, q ; S )), where h : R+  R+ is a strongly convex, strictly-increasing function with h(0) = 0.
Likewise, if C1 (p, q ; S ) and C2 (p, q ; S ) are families of cost functions satisfying Conditions 1-5, then
so is C (p, q ; S ) = C1 (p, q ; S ) + (1 - )C2 (p, q ; S ), for any   (0, 1).

Proof. The proofs are almost immediate, given the the assumptions.




                                                     44
