                                 NBER WORKING PAPER SERIES




    CLOSED-FORM LIKELIHOOD EXPANSIONS FOR MULTIVARIATE DIFFUSIONS


                                           Yacine Ait-Sahalia


                                          Working Paper 8956
                                  http://www.nber.org/papers/w8956


                       NATIONAL BUREAU OF ECONOMIC RESEARCH
                                1050 Massachusetts Avenue
                                  Cambridge, MA 02138
                                       May 2002




This research was partly funded by the NSF under grants SBR-9996023 and SES-0111140. The views
expressed herein are those of the author and not necessarily those of the National Bureau of Economic
Research.


© 2002 by Yacine Ait-Sahalia. All rights reserved. Short sections of text, not to exceed two paragraphs, may
be quoted without explicit permission provided that full credit, including © notice, is given to the source.
Closed-Form Likelihood Expansions for Multivariate Diffusions
Yacine Ait-Sahalia
NBER Working Paper No. 8956
May 2002
JEL No. C32, G12



                                              ABSTRACT

        This paper provides closed-form expansions for the transition density and likelihood function of
arbitrary multivariate diffusions. The expansions are based on a Hermite series, whose coefficients are
calculated explicitly by exploiting the special structure afforded by the diffusion hypothesis. Because the
transition function for most diffusion models is not known explicitly, the expansions of this paper can
help make maximum-likelihood a practical estimation method for discretely sampled multivariate
diffusions. Examples of interest in financial econometrics are included.




Yacine Ait-Sahalia
Department of Economics
Princeton University
Princeton, NJ 08544-1021
and NBER
(609) 258-4015
yacine@princeton.edu
1     Introduction
Diﬀusions and more generally continuous-time Markov processes are generally speciÞed in economics and
Þnance by their evolution over inÞnitesimal instants, that is, by writing down the stochastic diﬀerential
equation followed by the state vector. However, for most estimation techniques relying on discrete data, we
need to be able to infer the implications of the inÞnitesimal time evolution of the process for longer time
intervals, for instance the time interval at which the process is actually sampled, say daily or weekly. The
transition function plays a key role in that context. The transition function of a Markov process is the
conditional density for the values of the state variable at a Þxed future date, given the current level of the
state vector. It eﬀectively gives a precise answer to the time aggregation problem inherent in the dichotomy
between the time scale of the model (continuous) and that of the observed data (discrete): if the process
evolves at each instant according to a given inÞnitesimal continuous-time equation, what is the distribution
of the values of the process after a Þnite amount of time has elapsed?
    Continuous-time models in Þnance have long been predominantly univariate, whether the variable in an
asset price as in the Black-Scholes and Merton models, or an interest rate as in the Cox, Ingersoll and Ross
or Vasicek models. In recent years, however, the literature has naturally evolved towards the inclusion of
multiple variables in continuous-time diﬀusion models. Typical examples include asset pricing models with
multiple explanatory factors, term structure models with multiple yields or factors, and stochastic volatility
or stochastic mean reversion models (see Sundaresan (2000) for a recent survey).
    In response to this trend towards multivariate models, this paper describes the construction of closed-
form approximations to the transition density of arbitrary multivariate diﬀusions, thereby extending to the
multivariate setting the results of Aït-Sahalia (2002). The form of the likelihood expansions derived here is
based on Hermite polynomials. While writing down a Hermite series can be done for any model, the key
idea is to exploit the speciÞcity aﬀorded by the diﬀusion hypothesis in order to obtain the expressions for
the coeﬃcients of the series fully explicitly, as functions of the state vectors at the present and future dates,
the time interval that separates them and the parameters of the assumed stochastic diﬀerential equation.
Other methods can be used to approximate the transition function, which involve solving numerically the
Fokker-Planck-Kolmogorov equation, simulating the process to Monte Carlo integrate the transition density
or approximating the process with binomial trees (see Aït-Sahalia (2002) for a review of the literature, and
Jensen and Poulsen (2002) for a comparison of the diﬀerent methods). None however produces a closed form
approximation.
    The extension from the univariate to the multivariate setting presents many challenges. Through judicious
use of Itô’s Lemma, every univariate diﬀusion can be transformed into one with unit diﬀusion, whose density
can then be approximated around a standard Normal. This is no longer the case for multivariate diﬀusions.
I therefore introduce the concept of reducibility for multivariate diﬀusions, which essentially characterizes
diﬀusions for which such a transformation exists. For reducible multivariate diﬀusions, the ideas introduced in
the univariate setting can be extended, leading to an expansion for the log-likelihood function in the form of a
Taylor series in the time variable, which is a particularly convenient way of gathering the Hermite terms. For
irreducible diﬀusions, however, one must proceed diﬀerently. The situation is more involved, yet still amenable


                                                       1
to a closed-form result, but this time in the form of a double Taylor expansion in the time variable and the
state vector. Extensions of the results of Aït-Sahalia (2002) in two diﬀerent univariate directions have also
recently been developed, for time-inhomogenous diﬀusions (Egorov, Li, and Xu (2001)) and for models driven
by Lévy processes other than Brownian motion (Schaumburg (2001)).
    Once the expansion is computed for the diﬀusion model at hand, it can be immediately applied to the
estimation of parameters of the discretely sampled diﬀusion by maximum-likelihood, or to a variety of other
estimation methods which require an expression for the transition density of the state variables, such as
Bayesian methods where one wishes to obtain a posterior distribution for the parameters of a stochastic
diﬀerential equation. The method can also be applied to generate simulated data at the desired frequency
from the continuous-time model, or to serve as the instrumental or auxiliary model in indirect inference and
simulated or eﬃcient moments methods. The point is that the explicit nature of the expansion as a function
of all the relevant variables makes these computations, whether maximization of the classical likelihood or
computation of posterior distributions, straightforward and computationally very eﬃcient.
    The paper is organized as follows. Section 2 sets up the model, notation and assumptions. In Section
3, I introduce the concept of reducibility of a diﬀusion and provide a necessary and suﬃcient condition for
the reducibility of a multivariate diﬀusion. When diﬀusions are reducible, the coeﬃcients of the expansion
are obtained by a change of variable, which I show in Section 4. When the diﬀusion is not reducible, the
expressions for the coeﬃcients are given in Section 5. Section 6 contains examples of multivariate diﬀusions
relevant for Þnancial econometrics and gives their corresponding likelihood expansions. Finally, Section 7
concludes. All proofs are in the Appendix.


2     Setup and Assumptions
Consider the multivariate diﬀusion

                                        dXt = µ (Xt ; θ) dt + σ (Xt ; θ) dWt                                  (2.1)

where Xt and µ (Xt ; θ) are m × 1 vectors, σ (Xt ; θ) is an m × m matrix, θ is a p-dimensional parameter and
Wt is an m × 1 vector of independent Brownian motions. Independence of the components is without loss of
generality as arbitrary correlation structures between the shocks to the diﬀerent equations can be modelled
through the inclusion of oﬀ-diagonal terms in the σ matrix. Note that σ need not be symmetric, and if
convenient attention can be restricted to triangular matrices by appropriate rotation of the m−dimensional
Brownian motion.
    The objective of this paper is to derive closed-form approximations to the transition function pX (∆, x|x0 ; θ)
of the process X, that is the conditional density of Xt+∆ = x given Xt = x0 induced by the model (2.1).
Assume that we observe the process at dates { t = i∆ | i = 0, . . . , n}, where ∆ > 0 is Þxed. Bayes’ rule
combined with the Markovian nature of (2.1), which the discrete data inherit, imply that the log-likelihood




                                                         2
function has the simple form
                                                     Xn          ¡                    ¢
                                      0n (θ) ≡ n−1             lX ∆, Xi∆ |X(i−1)∆ ; θ                                    (2.2)
                                                         i=1


where lX ≡ ln pX . In practice, the issue is that for most models of interest, the function pX , hence lX , is not
available in closed-form.
   If the sampling interval ∆ is time-varying deterministically, say ∆i is the actual time interval between
the (i − 1)th and ith observations X̃i−1 and X̃i , then it suﬃces to replace ∆ in (2.2) by its actual value ∆i
when evaluating the transition density for the ith pair of observations. If the sampling interval is random
and either drawn independently of the X process or conditionally on X̃i−1 , then one can write down the joint
likelihood function of the pair of observations and ∆i and utilize Bayes’ rule to express it as the product
of the conditional density of X̃i given X̃i−1 and ∆i , times the marginal density d of ∆i given X̃i−1 , that
      ³                  ´   ³             ´
is pX ∆i , X̃i |X̃i−1 ; θ × d ∆i |X̃i−1 ; κ where κ is a parameter vector parametrizing the sampling density
d. Aït-Sahalia and Mykland (2000) study the diﬀerent eﬀects resulting in the likelihood framework from
randomly and discretely spaced observations. In all cases, an expression is needed for lX , which is what this
paper delivers.
   I will use the following notation. Let SX , a subset of Rm , denote the domain of the diﬀusion X and Θ ⊂ Rp
the open parameter space. SX can often be taken to be of the form of a product of m intervals with limits xi
                                                                                                                ¯
and x̄i , where possibly xi = −∞ and/or x̄i = +∞. The intervals are closed at Þnite limits and open at inÞnite
                         ¯
limits. For simplicity, I will assume that Θ is such that SX is identical for each value of the parameter vector θ
                                                                                                     T
in Θ. I will use T to denote transposition and, for a function η(x; θ) = (η1 (x; θ), ..., η d (x; θ)) , diﬀerentiable in
x, I will write ∇η(x; θ) for the Jacobian matrix of η, i.e., the matrix ∇η(x; θ) = [∂η i (x; θ)/∂xj ]i=1,...,d;j=1,...,m .
For x ∈ Rm , kxk denotes the usual Euclidean norm. The binomial coeﬃcients will be denoted
                                            µ ¶
                                              k        k!
                                                 ≡            .                                                          (2.3)
                                              j    j!(k − j)!

   If a = [aij ]i,j=1,...,m is a m × m invertible matrix then I write a−1 = [a−1
                                                                              ij ]i,j=1,...,m for the matrix inverse,
rather than using the tensor notation (note that a−1
                                                  ij denotes the element (i, j) of the inverse matrix, not the
inverse of the element (i, j) of the original matrix). Det [a] and tr[a] denote the determinant of a and its trace,
respectively. If a = [ai ]i=1,...,m is a vector, tr[a] denotes the sum of the elements of a. a = diag[ai ]i=1,...,m
denotes the m × m diagonal matrix with diagonal elements ai . When a function η(x; θ) is invertible in x, I
write ηinv (y; θ) for its inverse, i.e., the solution in x of the equation y = η(x; θ) is x = ηinv (y; θ). By the Inverse
Function Theorem (see e.g., Theorem 8.7.8 in Haaser and Sullivan (1991)), η(x; θ) is invertible in x at x = x0
if ∇η(x; θ) has a bounded matrix inverse at x = x0 ; the inverse function ηinv then inherits the smoothness
properties of η.
   Let AX denote the inÞnitesimal generator of the process X, which is characterized by its action on functions
f (∆, x, x0 ; θ) in its domain:
                                              m                                  m m
                          ∂f (∆, x, x0 ; θ) X              ∂f (∆, x, x0 ; θ) 1 X X                 ∂ 2 f (∆, x, x0 ; θ)
 AX ·f (∆, x, x0 ; θ) =                    +     µi (x; θ)                  +           vij (x; θ)                      . (2.4)
                               ∂∆            i=1
                                                                ∂xi           2 i=1 j=1                  ∂xi ∂xj



                                                                3
The domain of AX includes at least functions that, for each (x0 ; θ) ∈ SX × Θ, are once continuously diﬀeren-
tiable in ∆ in R+ , twice continuously diﬀerentiable in x in SX and have compact support.
   In some instances, it may be more natural to parametrize directly the inÞnitesimal variance-covariance
matrix of the process

                                            v (x; θ) ≡ σ (x; θ) σ T (x; θ) ,                                    (2.5)

that σ(x; θ) itself. In that case, σ (x; θ) is deÞned indirectly as the positive deÞnite square root of v (x; θ) ,

                                                                     1/2
                                               σ (x; θ) = v (x; θ)         .

σ can be obtained by the Cholesky decomposition of the matrix v (x; θ). While it is traditional to parametrize
the process by (µ, σ), every characterization of the process, such as its transition probability, depends in fact
on (µ, v). In particular, it can be shown that, should there exist a continuum of solutions in σ to the equation
(2.5), the transition probability of the process is identical for each one of these σ (see Remark 5.17 and Section
5.3 in Stroock and Varadhan (1979)). This is also quite clear from the deÞnition (2.4) of the inÞnitesimal
generator of the process, which is an equivalent characterization of the process, and depends on v rather than
σ. As this will pay a role in the likelihood expansions, deÞne

                                                        1
                                          Dv (x; θ) ≡     ln (Det[v(x; θ)]) .                                   (2.6)
                                                        2

   To avoid the issues associated with the multiple σ scenario, I assume from now on that σ is uniquely
determined, either directly as part of the assumed speciÞcation of the model (2.1) or indirectly as the unique
solution of (2.5), in which case the form of v is such that it yields a unique square root matrix. I will assume
that this matrix σ satisÞes the following regularity condition:

Assumption 1. The matrix σ (x; θ) is positive deÞnite for all x in the interior of SX and θ ∈ Θ.

   Further assumptions are required to insure the existence and unicity of a solution to (2.1), and to make
the computation of likelihood expansions possible. I will assume the following:

Assumption 2. For each θ ∈ Θ, µ (x; θ) and σ (x; θ) are inÞnitely diﬀerentiable in x on SX .

   Assumption 2 insures the unicity of solutions to (2.1). Indeed, Assumption 2 implies in particular that the
coeﬃcients of the stochastic diﬀerential equation are locally Lipschitz under their assumed (once) diﬀerentia-
bility, by applying the mean value theorem. That is, for each C > 0, there exists a constant K > 0 such that
for every x and x0 in SX , kxk ≤ C and kx0 k ≤ C, we have

                                       |µi (x; θ) − µi (x0 ; θ)| ≤ K kx − x0 k                                  (2.7)
                                     |σij (x; θ) − σ ij (x0 ; θ)| ≤ K kx − x0 k                                 (2.8)

for i, j = 1, ..., m. This insures that a solution, if it exists, will be unique (see e.g., Theorem 5.2.5 in Karatzas
and Shreve (1991)). The inÞnite diﬀerentiability assumption in x is unnecessary for that purpose, but it allows


                                                           4
the computation of expansions of the transition density, which as we will see involve repeated diﬀerentiation
of the coeﬃcient functions µ and σ.
   There exist models of interest in Þnance, such as Feller’s square-root diﬀusion used in the Cox, Ingersoll and
Ross model of the term structure, that fail to satisfy (2.8) since they violate the diﬀerentiabilty requirement of
Assumption 2 at a boundary of SX : for instance, σ(x; θ) = σ0 x1/2 is not diﬀerentiable at the left boundary
0 of SX . Fortunately, it is possible to weaken Assumption 2 to cover such cases:

Assumption 3. (Yamada-Watanabe Conditions) Assumption 2 can be replaced by:
   1. For each θ ∈ Θ, µ (x; θ) and σ (x; θ) are inÞnitely diﬀerentiable in x on the interior of SX .
   2. There exist real-valued, continuous, positive and increasing functions ρ(u) and κ(u) deÞned on [0, C)
for some C > 0 such that ρ(0) = κ(0) = 0, ρ2 (u)u−1 and κ(u) are concave and satisfy
                                                   Z       C
                                                                 u
                                            lim+                      du = +∞                                 (2.9)
                                           ε→0     ε           ρ2 (u)
                                                   Z       C
                                                                1
                                            lim+                    du = +∞.                                (2.10)
                                            ε→0        ε       κ(u)

Then

                                      |µi (x; θ) − µi (x0 ; θ)| ≤ κ (kx − x0 k)                             (2.11)
                                    |σij (x; θ) − σ ij (x0 ; θ)| ≤ ρ (kx − x0 k)                            (2.12)

                  2
for all (x, y) ∈ SX such that kx − x0 k < C and all i, j = 1, ..., m.
   3. If σ(x; θ) is of the form σ(x; θ) = diag [σ i (xi ; θ)]i=1,..,m (this is always the case if m = 1), condition
(2.9) can be weakened to
                                                       Z       C
                                                                     1
                                              lim+                          du = +∞                         (2.13)
                                             ε→0           ε       ρ2 (u)

with no concavity requirement.
   4. If m = 2 and σ(x; θ) is of the isotropic form σ(x; θ) = diag [s (x; θ)]i=1,2 then condition (2.9) can be
weakened to
                                                   Z       C
                                                               u ln(1/u)
                                            lim+                         du = +∞                            (2.14)
                                           ε→0         ε         ρ2 (u)

provided that G(u) = u3 exp(2/u)ρ2 (exp(−1/u)) is concave.

   As in the case of Assumption 2, Assumption 3.1 is there for the purpose of computing likelihood expansions.
The fact that Assumption 3.2 insures unicity of the solution follows from Theorem 4 in Watanabe and Yamada
(1971); Assumption 3.3 from Theorem 1 in Yamada and Watanabe (1971); Assumption 3.4 from Theorem
3 in Watanabe and Yamada (1971). Examples of functions ρ that satisfy (2.9) are: ρ(u) = uα with α ≥ 1,
ρ(u) = u(ln(1/u))1/2 . The functions ρ(u) = uα with α ≥ 1/2 satisfy (2.13). The functions ρ(u) = uα with
α ≥ 1/2 and ρ(u) = u ln(1/u) satisfy (2.14). A function σ ij satisfying condition (2.12) with ρ(u) = uα is said
to be Hölder-continuous of order α.


                                                                     5
  Assumption 3.3 with ρ(u) = u1/2 allows us in particular to consider mutivariate Cox, Ingersoll and Ross
                           h        i
                                1/2
models where σ(x; θ) = diag ηi xi        (see the term structure examples in Section 6.3). The issue with
                                       i=1,..,m
these aﬃne models (linear µ and v = σσT ) lies in the non-Lipschitz behavior of the σ function rather than
that of the µ function. In that case, Assumption 3 for µ with κ(u) = k.u reduces to the Lipschitz condition
(2.7) for the drift µ.
   These conditions are essentially the best possible, in that examples where multiple solutions to the sto-
chastic diﬀerential equation (2.1) arise when they are violated. If m ≥ 3, take any subadditive ρ(u) (i.e.,
ρ(u + v) ≤ ρ(u) + ρ(v)) such that
                                                    Z   C
                                                              u
                                             lim+                    du < +∞,
                                            ε→0     ε       ρ2 (u)

for instance ρ(u) = u1/2 , then the stochastic diﬀerential equation dXt = σ(Xt )dWt , X0 = 0, with isotropic
σ matrix σ(x) = diag [ρ (kxk)]i=1,...,m , has, apart from the solution Xt = 0, other non-zero solutions. Thus
condition (2.9) in Assumption 3.2 is sharp. In dimension m = 1, the famous example of Girsanov, dXt =
|Xt |α dWt , has a unique solution if α ≥ 1/2, namely Xt = 0, but that solution is no longer unique if 0 < α < 1/2;
hence condition (2.13) in Assumption 3.3 is also sharp. In Assumption 3.4 concerning the dimension m = 2,
the restriction that the matrix σ(x; θ) be of the isotropic form cannot be relaxed: a counterexample was
provided recently in Swart (2001). The condition (2.14) is also seen to be sharp, by forming a counterexample
with a subadditive ρ as in dimension m ≥ 3.
   The next assumption restricts the growth behavior of the coeﬃcients near the boundaries of the domain:

Assumption 4. The drift and diﬀusion functions satisfy linear growth conditions, that is, for each θ ∈ Θ
there exists a constant K such that for all x ∈ SX , and i, j = 1, ..., m :

                                           |µi (x; θ)| ≤ K (1 + kxk)                                        (2.15)
                                          |σij (x; θ)| ≤ K (1 + kxk) .                                      (2.16)

   The role of Assumption 4 is to insure existence of a solution to the stochastic diﬀerential equation (2.1)
by preventing explosions of the process in Þnite expected time. While it can be relaxed in speciÞc examples,
it is not possible to do so in full generality as shown by the following counterexamples, illustrating the need
for restricting the growth of both µ and σ. The one-dimensional equation dXt = (1 + Xt2 )dt, X0 = 0,
                                                                                                  2
has the exploding solution Xt = tan(t). The three-dimensional equation dXt = (1 + kXt k )dWt explodes
in Þnite time. In dimension one, however, Þner results are available (see the Engelbert-Schmidt criterion in
Theorem 5.5.15 in Karatzas and Shreve (1991)) allowing linear growth to be imposed only when the drift
coeﬃcient pulls the process towards an inÞnity boundary (see Proposition 1 of Aït-Sahalia (2002)). Even in
higher dimensions, the condition can sometimes be reÞned in speciÞc examples (see Section 6.2 below). In all
dimensions, the linear growth condition in Assumption 4 is only an issue near the boundaries of SX . On any
compact set, the growth condition (boundedness, in fact) follows from diﬀerentiability of the functions and
the mean value theorem.
   While nothing in this paper hinges upon the stationarity of the process X, it is useful to have a suﬃcient


                                                              6
condition that would guarantee it, if need be. From Hasminskii (1980), for given θ ∈ Θ, there exists a unique
stationary distribution for the process X if there exists C > 0 and some positive deÞnite matrix V such that

                                                      1
                                          µ(x; θ)V x + tr [v (x; θ) V ] < −1                                    (2.17)
                                                      2

for all x in SX such that kxk > C. Then the stationary density of X is the solution π (x0 ; θ) of the equation

                   Xm                                     m m
                        ∂                              1 XX         ∂2
                            [µi (x0 ; θ) π (x0 ; θ)] −                     [vij (x0 ; θ) π (x0 ; θ)] = 0        (2.18)
                   i=1
                       ∂x0i                            2 i=1 j=1 ∂x0i ∂x0j

that integrates to one. The process X will be stationary provided that the initial random variable X0 is
distributed with density π (x0 ; θ) . Of course, the process may be stationary for some values of θ in Θ and not
others. For example, in an Ornstein-Uhlenbeck process stationarity depends upon the positivity of the real
parts of the eigenvalues of the mean reversion matrix.
     If the approximation to the function lX is to be used for maximum-likelihood estimation of the parameters
θ, then care must be taken to insure that all the parameters are identiÞed. The MLE is well-deÞned and
identiÞcation is achieved if we assume:

Assumption 5. For each x ∈ SX , µ (x; θ) and σ (x; θ) are three times continuously diﬀerentiable in θ on Θ,
                                                                  ¡             ¢
and, if there exist (θ, θ 0 ) ∈ Θ2 such that pX (∆, x|x0 ; θ) = pX ∆, x|x0 ; θ 0 on a set of values of (x, x0 ) ∈ SX
                                                                                                                   2
                                                                                                                     of
non-zero measure, then θ = θ0 .

     More primitive conditions, not involving the function pX , can be given in speciÞc examples, see Section 6
below. This paper deals only with the construction of an approximation to lX , which can then be used for
purposes other than maximum likelihood estimation. In that case, there is no reason to assume Assumption
5.
     One last remark. The diﬀusion process X is fully deÞned by the speciÞcation of the functions µ and σ
and its behavior at the boundaries of SX . In many examples, the speciÞcation of µ and σ predetermines the
boundary behavior of the process, but this will not be the case for models that represent limiting situations.
For instance, in Cox, Ingersoll and Ross processes with aﬃne µ and v, the behavior at the 0 boundary depends
upon the values of the parameters θ in (µ, σ). When this situation occurs for a particular model, the behavior
of the likelihood expansion near such a boundary will be speciÞed exogneously to match that of the assumed
model.


3      Reducible Diﬀusions
Whenever possible, I will Þrst transform the diﬀusion X into one that is more amenable to the derivation of
an expansion for its transition density. For that purpose, I introduce the following deÞnition:

DeÞnition 1. (Reducibility) The diﬀusion X is said to be reducible to unit diﬀusion (or reducible, in short)
if and if only if there exists a one-to-one transformation of the diﬀusion X into a diﬀusion Y whose diﬀusion
matrix σY is the identity matrix. That is, there exists an invertible function γ (x; θ) , inÞnitely diﬀerentiable in


                                                            7
X on SX and three times continuously diﬀerentiable in θ on Θ such that Yt ≡ γ (Xt ; θ) satisÞes the stochastic
diﬀerential equation

                                               dYt = µY (Yt ; θ) dt + dWt                                    (3.1)

on the domain SY .

   To avoid needless complications, I will assume that the domain of the transformed process, SY , is inde-
pendent of the parameter value θ. As discussed for SX already, in typical examples, SX and SY are both
products of intervals with lower limits xi and yi that are either −∞ or 0, and upper limits x̄i and ȳi that are
                                        ¯      ¯
either 0 or +∞.
   By Itô’s Lemma, when the diﬀusion is reducible, the change of variable γ satisÞes

                                                ∇γ(Xt ; θ) = σ−1 (x; θ) .                                    (3.2)

Every scalar (i.e., one-dimensional) diﬀusion is reducible, by means of the transformation
                                                                   Z   Xt
                                                                               du
                                             Yt ≡ γ (Xt ; θ) =                                               (3.3)
                                                                            σ (u; θ)

and we have by Itô’s Lemma:
                                               ¡                ¢
                                             µ γ inv (y; θ) ; θ     1 ∂σ ¡ inv        ¢
                                 µY (y; θ) =     inv
                                                                  −       γ (y; θ) ; θ .
                                             σ (γ (y; θ) ; θ)       2 ∂x

   This transformation played a critical role in the derivation of closed-form Hermite approximations to the
transition density of univariate diﬀusions in Aït-Sahalia (2002). However, not every multivariate diﬀusion is
reducible. Whether or not a given multivariate diﬀusion is reducible depends on the speciÞcation of its σ
matrix, namely:

Proposition 1. (Necessary and Suﬃcient Condition for Reducibility) The diﬀusion X is reducible if and only
                                    £    ¤
if the inverse diﬀusion matrix σ−1 = σ−1
                                      i,j i,j=1,...,m satisÞes on SX × Θ the condition that


                                               ∂σ −1
                                                  ij (x; θ)   ∂σ−1
                                                                ik (x; θ)
                                                            =                                                (3.4)
                                                   ∂xk           ∂xj

for each triplet (i, j, k) = 1, ..., m such that k > j.

   In the bivariate case m = 2, the state vector is Xt = (X1t , X2t )T and the components of the µ vector and
σ matrix are
                                                                                                  
                      dX1t            µ1 (Xt ; θ)              σ 11 (Xt ; θ) σ 12 (Xt ; θ)        dW1t
                            =                     dt +                                               (3.5)
                      dX2t            µ2 (Xt ; θ)              σ 21 (Xt ; θ) σ 22 (Xt ; θ)        dW2t

and condition (3.4) reduces to

                             ∂σ−1
                               11 (x; θ)   ∂σ −1
                                              12 (x; θ)   ∂σ −1
                                                             21 (x; θ)   ∂σ−1
                                                                           22 (x; θ)
                                         −              =              −             = 0.                    (3.6)
                                ∂x2            ∂x1            ∂x2           ∂x1


                                                               8
Example 1. Diagonal Systems: If σ12 = σ21 = 0, then the reducibility condition becomes ∂σ −1
                                                                                          11 /∂x2 =
∂σ−1                   −1
  22 /∂x1 = 0. Since σ ii = 1/σ ii in the diagonal case, reducibility is equivalent to the fact that σ ii depends
only on xi (and θ) for each i = 1, 2. This is true more generally in dimension m. Note that this is not the
case if oﬀ-diagonal elements are present.

    Another example is provided by the class of stochastic volatility models:

Example 2. Stochastic Volatility: If
                                                                                   
                                                     σ11 (x2 ; θ)        0
                                     σ (x; θ) =                                    
                                                           0        σ 22 (x2 ; θ)

then the process is not reducible in light of the previous example, as this is a diagonal system where σ11 depends
on x2 . However, if
                                                                                   
                                                  a(x1 ; θ) a(x1 ; θ)b(x2 ; θ)
                                   σ (x; θ) =                                      
                                                       0            c(x2 ; θ)

then the process is reducible as can be seen by applying (3.6).

    The situation now is as follows. Whenever a diﬀusion is reducible, an expansion can be computed for
the transition density pX of X by Þrst computing it for the density pY of the reduced process Y and then
transforming Y back into X, proceeding essentially by extending the univariate method: see Section 4. When
a diﬀusion is not reducible, I explain below how to nevertheless derive a closed-form expansion directly for the
transition density pX : this is done in Section 5.


4     Closed-Form Expansion for the Transition Density of a Reducible
      Diﬀusion

4.1    Form of the Hermite Series in the Univariate Case
As discussed above, every univariate diﬀusion is reducible. To motivate the approach in the multivariate case,
let me Þrst recall how one proceeds in the univariate case, summarizing brießy the results of Aït-Sahalia (2002).
To understand the construction of the sequence of approximations to the transition function pX , the following
analogy may be helpful. Consider a standardized sum of random variables to which the Central Limit Theorem
(CLT) apply. Often, one is willing to approximate the actual sample size n by inÞnity and use the N (0, 1)
limiting distribution for the properly standardized transformation of the data. If not, higher order terms of
the limiting distribution (for example the classical Edgeworth expansion based on Hermite polynomials) can
be calculated to improve the small sample performance of the approximation. The basic idea is to create an
analogy between this situation and that of approximating the transition density of a diﬀusion. Think of the
sampling interval ∆ as playing the role of the sample size n in the CLT. If we properly standardize the data,
then we can Þnd out the limiting distribution of the standardized data as ∆ tends to 0 (by analogy with what


                                                           9
happens in the CLT when n tends to ∞). Properly standardizing the data in the CLT means summing them
and dividing by n1/2 ; here it will involve transforming the original diﬀusion X into another one, called Z
below. In both cases, the appropriate standardization makes N (0, 1) the leading term. I will then reÞne this
N (0, 1) approximation by “correcting” for the fact that ∆ is not 0 (just like in practical applications of the
CLT n is not inÞnity), i.e., by computing the higher order terms. As in the CLT case, it is natural to consider
higher order terms based on Hermite polynomials, which are orthogonal with respect to the leading N (0, 1)
term.
   So let pY denote the transition function of the process Y, whose dynamics are given by (3.1). As shown
in Aït-Sahalia (2002), the tails of pY have a Gaussian-like upper bound; but while Y is “closer” to a Normal
variable than X is, it is not practical to expand pY . This is due to the fact that pY gets peaked around
the conditional value y0 when ∆ gets small. And a Dirac mass is not a particularly appealing leading term
for an expansion. For that reason, a further transformation is performed, deÞning the “pseudo-normalized”
increment of Y as

                                                  Z∆ ≡ ∆−1/2 (Y∆ − y0 ) .

I then expand the density of Z around a N(0, 1), leading to an expansion for pY of the form:
                                             µ             ¶
             (J)                  1              (y − y0 )2 XJ
           pùY (∆, y|y0 ; θ) =       1/2
                                         exp   −                 η(j) (∆, y0 ; θ) Hj (∆−1/2 (y − y0 ))               (4.1)
                               (2π∆)                2∆       j=0


where the Hermite coeﬃcients η(j) (∆, y0 ; θ) are given by
                                                     Z   +∞
                        (j)
                    η         (∆, y0 ; θ) = (1/j!)             Hj (z) pZ (z|y0 , ∆; θ) dz
                                                      −∞
                                                     Z +∞                       ³               ¯          ´
                                                                                                ¯
                                        = (1/j!)               Hj (z) ∆1/2 pY       ∆1/2 z + y0 ¯ y0 , ∆; θ dz
                                                         −∞
                                                     Z  ³ +∞           ´
                                        = (1/j!)     Hj ∆−1/2 (y − y0 ) pY (y|y0 , ∆; θ) dy
                                                  −∞
                                                   h  ³                ´¯           i
                                                                        ¯
                                        = (1/j!) E Hj ∆−1/2 (Y∆ − y0 ) ¯ Y0 = y0 ; θ .                               (4.2)

   To evaluate the conditional expectation (4.2), I use the Taylor expansion
                                                         K
                                                         X ∆k                                             ¡     ¢
                EY1 [f(∆, Y∆ , Y0 ; θ)|Y0 = y0 ] =                   AkY · f (δ, y, y0 ; θ)| y=y0 ,δ=0 + O ∆K+1      (4.3)
                                                                k!
                                                         k=0

where AY is the inÞnitesimal generator of the process, i.e., the operator whose action is deÞned by

                                        ∂f (∆, y, y0 ; θ)               ∂f (∆, y, y0 ; θ) 1 ∂ 2 f (∆, y, y0 ; θ)
             AY · f (∆, y, y0 ; θ) =                      + µY (y, θ0 )                  +                       .   (4.4)
                                             ∂∆                              ∂y            2       ∂y 2

In all cases, this expression is a proper Taylor series; whether the series is analytic at ∆ = 0 is not guaranteed,
although suﬃcient conditions can be given (see Proposition 4 in Aït-Sahalia (2002), who also discusses the
class of functions f, such as polynomials, for which this representation is admissible).
                                            ¡                 ¢
    Applying (4.3) to f(∆, Y∆ , Y0 ; θ) = Hj ∆−1/2 (Y∆ − Y0 ) up to order K for the purpose of evaluating ηj


                                                                     10
   (J)                             (J,K)
in pùY yields the expansion pùY            . Diﬀerent ways of gathering the terms are available (as in the Central Limit
Theorem, where for example both the Edgeworth and Gram-Charlier expansions are based on a Hermite
expansion). One particularly convenient way of gathering the terms of the expansion consists in grouping
them in powers of ∆. This is then in the same spirit as the “small-time” expansions of Azencott (1984), except
that the expansions obtained here are fully explicit instead of relying of moments of functionals of Brownian
Bridges. Indeed, if we gather all the terms according to increasing powers of ∆ instead of increasing order of
                                            (K)         (∞,K)                                                                 (K)
the Hermite polynomials, and let pY               ≡ pùY          , we obtain an explicit representation of pY , given by:
                                                   µ             ¶         µZ     y                    ¶X
              (K)                       −1/2           y − y0                                                 K      (k)            ∆k
             pY (∆, y|y0 ; θ)      =∆          φ                     exp              µY (w; θ) dw                  cY (y|y0 ; θ)         (4.5)
                                                        ∆1/2                     y0                           k=0                   k!

                                                                                             (0)
where φ (w) = exp(−w2 /2)/(2π) is the N (0, 1) density function, cY = 1 and for all k > 1:
                                                                  Z   y                n
                   (k)                                                                               (k−1)
                  cY (y|y0 ; θ) = k (y − y0 )−k                           (w − y0 )
                                                                                   k−1
                                                                                         λY (w; θ) cY      (w|y0 ; θ)
                                                                     y0
                                                                             ³                          ´ o
                                                                                   (k−1)
                                                                           + ∂ 2 cY      (w|y0 ; θ) /∂w2 /2 dw                            (4.6)

with
                                                                   µ                        ¶
                                                                 1               ∂µY (y; θ)
                                           λY (y; θ) = −            µ2Y (y; θ) +              .                                           (4.7)
                                                                 2                  ∂y
                                                                                                                      (0)
Equation (4.6) allows the recursive computation of the coeﬃcients, starting from cY = 1.
   When we are interested in computing the logarithm of the transition function, an alternative form of the
Taylor series can be more amenable to the computation of the log-likelihood, and guarantee positivity of
the density. Indeed, the function lY (∆, y|y0 ; θ) can also be expressed directly as a series in ∆, namely by
                    P      (j)            j
Taylor-expanding ln( Jj=0 cY (y|y0 ; θ) ∆j! ) in ∆. This yields the form

                                                                                 (y|y0 ; θ) XK
                                                                          (−1)
                  (K)                     1          C                                            (k)          ∆k
                 lY      (∆, y|y0 ; θ) = − ln (2π∆) + Y                                    +     CY (y|y0 ; θ)                            (4.8)
                                          2                                      ∆           k=0               k!

and, by application of the Jacobian change of variable formula,

                          (K)                1 ¡          ¢   (K)
                         lX (∆, x|x0 ; θ) = − ln σ2 (x; θ) + lY (∆, γ (x; θ) |γ (x0 ; θ) ; θ) .                                           (4.9)
                                             2

   The coeﬃcients are given by

                                (−1)                                       2
                           CY    (y|y0 ; θ) = − (y − y0 ) /2                                                                             (4.10)
                                              Z y
                             (0)
                            CY (y|y0 ; θ) =       µY (w; θ) dw                                                                           (4.11)
                                                            y0
                                                                                                   Z   y
                                 (1)                       (1)
                            CY (y|y0 ; θ) = cY (y|y0 ; θ) = (y − y0 )−1                                    λY (w; θ) dw                  (4.12)
                                                                                                   y0

   Note that

                         (−1)                             (0)                              (1)
                      CY        (y0 |y0 ; θ) = 0, CY (y0 |y0 ; θ) = 0, CY (y0 |y0 ; θ) = λY (y0 ; θ) ,                                   (4.13)


                                                                           11
the last equation being a consequence of L’Hôpital’s Rule.
                                                                                (−1)      (0)           (k−1)                    (k)
   The other coeﬃcients are obtained recursively. Given CY                             , CY , ..., CY           , the coeﬃcient CY , k ≥ 2,
is given by:
                                                 Z                            (          (k−1)
                                                     y
            (k)                             −k                        k−1         1 ∂ 2 CY   (w|y0 ; θ)
           CY (y|y0 ; θ)     = k (y − y0 )               (w − y0 )
                                                 y0                               2        ∂w2
                                                    µ    ¶ (h)                                )
                                             1 Xk−2 k − 1 ∂CY (w|y0 ; θ) ∂CY
                                                                           (k−1−h)
                                                                                   (w|y0 ; θ)
                                           +                                                    dw.                                    (4.14)
                                             2  h=1   h        ∂w               ∂w

   For consistency with the multivariate case to appear below, note for now that these expressions can also
be written in the form
                                                                      Z   y
                          (k)                                    −k                              (k)
                         CY (y|y0 ; θ)    = k (y − y0 )                       (w − y0 )k−1 GY (w|y0 ; θ) dw
                                                                      y0
                                                 Z       1
                                                              (k)
                                          = k                GY (y0 + u (y − y0 ) |y0 ; θ) uk−1 du                                     (4.15)
                                                     0

where
                                                   (0)                 (0)
                                                                                       " (0)         #2
       (1)                ∂µY (y; θ)             ∂CY (y|y0 ; θ) 1 ∂ 2 CY (y|y0 ; θ) 1 ∂CY (y|y0 ; θ)
      GY (y|y0 ; θ)   = −            − µY (y; θ)               +                   +
                             ∂y                        ∂y        2       ∂y 2        2       ∂y
                      = λY (y; θ)                                                                                                      (4.16)

and for k ≥ 2
                                              (k−1)                                  (k−1)
           (k)                            ∂CY       (y|y0 ; θ) 1 ∂ 2 CY      (y|y0 ; θ)
         GY (y|y0 ; θ) = −µY (y; θ)                           +
                                                  ∂y            2         ∂y 2
                                          µ        ¶
                               1 Xk−1 k − 1 ∂CY (y|y0 ; θ) ∂CY
                                                        (h)             (k−1−h)
                                                                                 (y|y0 ; θ)
                             +                                                                                                         (4.17)
                               2      h=0    h              ∂y                ∂y
                                                                  µ      ¶ (h)
                                          (y|y0 ; θ) 1 Xk−2 k − 1 ∂CY (y|y0 ; θ) ∂CY
                                    (k−1)                                                   (k−1−h)
                             1 ∂ 2 CY                                                               (y|y0 ; θ)
                           =                         +
                             2         ∂y 2             2     h=1    h            ∂y             ∂y

4.2     Determination of the Coeﬃcients in the Multivariate Reducible Case
In the case of a multivariate reducible diﬀusion, I proceed along the same lines. Hermite polynomials are
available in the multivariate case (see e.g., Chapter 5 of McCullagh (1987) or Withers (2000)). Let φ(x)
denote the density of the m−dimensional multivariate Normal distribution with mean zero and covariance
matrix κ = [κij ]i,j=1,..,m . The inverse of κ is κ−1 = [κ−1
                                                          ij ]i,j=1,..,m , so that

                                                                                  Xm Xm
                            φ(x; κ) = (2π)−m/2 Det[κ]−1/2 exp(−                                        κ−1
                                                                                                        ij xi xj ).
                                                                                       i=1       j=1


For each vector h = (h1 , ..., hm )T ∈ Nm , recall that tr[h] = h1 + ... + hm , I will denote by Hh (x) the associated
Hermite polynomials, which are deÞned by

                                                                (−1)tr[h] ∂ tr[h] φ(x; κ)
                                           Hh (x; κ) =
                                                                 φ(x; κ) dxh1 1 ...dxhmm



                                                                    12
and can be computed explicitly to an arbitrary order tr[h]. The dual Hermite polynomials are

                                                                (−1)tr[h] ∂ tr[h] φ(z; κ)
                                             H̃h (x; κ) =
                                                                 φ(x; κ) dz1h1 ...dzm hm


at z = κ−1 x. We have that H̃h (x; κ) = Hh (κ−1 x; κ−1 ). The polynomials are orthogonal with respect to their
duals in the sense that
                                       Z
                                             Hh (x; κ)H̃k (x; κ)φ(x; κ)dx = h1 !...hm !
                                        Rm

if h = k and 0 otherwise.
   The Hermite series approximation of pY is in the form
                              ³                    ´X
     (J)
   pùY (∆, y|y0 ; θ) = ∆−m/2 φ ∆−1/2 (y − y0 ) ; I                                    ηh (∆, y0 ; θ) Hh (∆−1/2 (y − y0 ); I)    (4.18)
                                                                      h∈Nm :tr[h]≤J


i.e., with κ = I, and the Hermite coeﬃcients η h (∆, y0 ; θ) can be computed as in the univariate case, by relying
on their orthogonality. Also as in the univariate case, the Hermite expansions can be written directly for the
log-density. The key question addressed in this paper is the computation of the coeﬃcients, and this is where
I rely on the structure aﬀorded by the diﬀusion hypothesis (note of course that I do not assume that the
characteristic function of the process is known).
   The inÞnitesimal generator AY corresponding to the reduced diﬀusion Y in (3.1) is
                                                   m                                              m   m
                              ∂f (∆, y, y0 ; θ) X                ∂f (∆, y, y0 ; θ) 1 X X ∂ 2 f (∆, y, y0 ; θ)
    AY · f (∆, y, y0 ; θ) =                    +     µY i (y; θ)                  +                           .                 (4.19)
                                   ∂∆            i=1
                                                                      ∂yi           2 i=1 j=1  ∂yi ∂yj

   Gathering again the coeﬃcients in an expansion in increasing powers of ∆, the form of the expansion
analogous to (4.8) is then

                                                                             (y|y0 ; θ) XK
                                                                      (−1)
                   (K)                       m           C                                    (k)          ∆k
                  lY     (∆, y|y0 ; θ) = −     ln (2π∆) + Y                            +     CY (y|y0 ; θ)                      (4.20)
                                             2                               ∆           k=0               k!
                                                                      (k)
leaving us with the computation of the coeﬃcients CY , k = −1, 0, 1, 2, ..., K. The following result gives an
explicit expression for each one of these coeﬃcients:


                                                                                      (K)
Theorem 1. The coeﬃcients of the log-density Taylor expansion lY                            (∆, y|y0 ; θ) are given explicitly by:

                           (−1)            1 Xm
                         CY       (y|y0 ; θ) = −  (yi − y0i )2                                                                  (4.21)
                                           2  i=1
                                          Xm              Z 1
                           (0)
                          CY (y|y0 ; θ) =     (yi − y0i )     µY i (y0 + u (y − y0 ) ; θ) du                                    (4.22)
                                                       i=1                    0

and, for k ≥ 1,
                                                    Z      1
                                (k)                             (k)
                               CY (y|y0 ; θ)   =k              GY (y0 + u (y − y0 ) |y0 ; θ) uk−1 du                            (4.23)
                                                       0




                                                                      13
where

                         (1)
                                               Xm ∂µ (y; θ) Xm                          (0)
                                                                                      ∂CY (y|y0 ; θ)
                                                        Yi
                        GY (y|y0 ; θ) = −                        −        µY i (y; θ)
                                                 i=1     ∂y           i=1                  ∂yi
                                                        i                 "                 #2 
                                               1 X m      2 (0)
                                                         ∂ CY (y|y0 ; θ)     ∂CY (y|y0 ; θ) 
                                                                                 (0)
                                             +                   2       +                                         (4.24)
                                               2   i=1        ∂yi                  ∂yi         

and for k ≥ 2

                  (k)
                                         Xm                (y|y0 ; θ) 1 Xm ∂ 2 CY
                                                               ∂CY
                                                                  (k−1)           (k−1)
                                                                                        (y|y0 ; θ)
                GY (y|y0 ; θ) = −                µY i (y; θ)         +
                                           i=1           ∂yi           2  i=1        ∂yi2
                                                     µ   ¶
                                         1 Xm Xk−1 k − 1 ∂CY (y|y0 ; θ) ∂CY
                                                              (h)          (k−1−h)
                                                                                   (y|y0 ; θ)
                                       +                                                      .                    (4.25)
                                         2   i=1 h=0   h         ∂yi           ∂yi


     To obtain an expansion for the density pY instead of the log-density lY , one can either take the exponential
      (K)
of   lY ,   yielding
                                                             Ã                                                 !
                                                                         (y|y0 ; θ) XK
                                                                  (−1)
                    (K)                         −m/2             CY                       (k)          ∆k
                   pY (∆, y|y0 ; θ)    = (2π∆)         exp                         +     CY (y|y0 ; θ)             (4.26)
                                                                         ∆           k=0               k!

or alternatively, given the coeﬃcients Ck for the log-density, the coeﬃcients ck for the density expansion can
                                                                                       (K)       (K)
be obtained by matching the coeﬃcients in the two Taylor expansions lY                       and pY .


4.3     Change of Variable
Given an expansion for the density pY of Y, an expansion for the density pX of X can be obtained by a direct
application of the Jacobian formula. DeÞne the Jacobian matrix ∇γ(x; θ). Then the transition density of X
is related to that of Y by

                               pX (∆, x|x0 ; θ) = Det [∇γ(x; θ)] pY (∆, γ (x; θ) | γ (x0 ; θ) ; θ) .               (4.27)

Then from (3.2) and (2.6), we have
                                                       £           ¤               −1/2
                                   Det [∇γ(x; θ)] = Det σ −1 (x; θ) = Det [v(x; θ)]     .                          (4.28)

                                                                       (K)                        (K)
Then, replacing pY on the right-hand-side of (4.27) by pY                    yields an expansion pX     for pX .
     In terms of log-densities, we have

                                             1
                         lX (∆, x|x0 ; θ) = − ln (Det [v(x; θ)]) + lY (∆, γ (x; θ) |γ (x0 ; θ) ; θ)
                                             2
                                          = −Dv (x; θ) + lY (∆, γ (x; θ) |γ (x0 ; θ) ; θ)                          (4.29)




                                                                  14
                                                                                                       (K)
which I mimic at the level of the approximations of order K in ∆, thereby deÞning lX

                (K)                                  (K)
            lX (∆, x|x0 ; θ) = −Dv (x; θ) + lY (∆, γ (x; θ) |γ (x0 ; θ) ; θ)
                                 m
                             = − ln (2π∆) − Dv (x; θ)                                                                     (4.30)
                                 2
                                 CY (γ (x; θ) |γ (x0 ; θ) ; θ) XK
                                   (−1)
                                                                            (k)                         ∆k
                               +                              +           CY (γ (x; θ) |γ (x0 ; θ) ; θ)
                                             ∆                      k=0                                 k!
      (K)                                                  (k)
from lY     given in (4.20), using the coeﬃcients CY , k = −1, 0, ..., K given in Theorem 1. This fully describes
the construction of the expansion of lX for a reducible diﬀusion.


4.4    Independent Variables
An important special case occurs when the m variables in (2.1) are independent. In that case, the multivariate
transition density pX is simply the product of the m univariate transition densities, and the log-likelihood lX
is the sum of the univariate ones. The following proposition shows that the expansion shares this feature:


Proposition 2. Suppose that for each i = 1, ..., m, µi (x; θ) and σii (x; θ) depend on xi only, and that σ ij (x; θ) =
0 for j 6= i. Then the diﬀusion is reducible and we have

                                         (K)
                                                                 Xm       (K)
                                         lX (∆, x|x0 ; θ) =             l     (∆, xi |x0i ; θ)                            (4.31)
                                                                     i=1 X

          (K)
where lX (∆, xi |x0i ; θ) is the univariate expansion corresponding to the ith variable, deÞned in (4.9).




5     Closed-Form Expansion for the Transition Density of an Irre-
      ducible Diﬀusion
I now turn to the irreducible case. Mimicking the form of the Taylor expansion in ∆ obtained in the reducible
case, namely (4.30), leads to postulating the following form for an expansion of the log likelihood

                                                                                   (x|x0 ; θ) XK
                                                                            (−1)
                (K)                    m                       C                                    (k)          ∆k
                lX (∆, x|x0 ; θ) = −     ln (2π∆) − Dv (x; θ) + X                            +     CX (x|x0 ; θ)    .      (5.1)
                                       2                                           ∆           k=0               k!
                                                                                                                   (k)
    The idea now is to derive an explicit Taylor approximation in (x − x0 ) of the coeﬃcients CX (x|x0 ; θ) ,
                                                                                                             (k)
k = −1, 0, ..., K. SpeciÞcally, I calculate a Taylor series in (x − x0 ) of each coeﬃcient CX , at order jk in
                                                                 (j ,k)
(x − x0 ). Such an expansion will be denoted by CX k                      . A Taylor series in (x − x0 ) is the form that arises
                                                                          (J)
directly from the representation of the Hermite series pùX as in the univariate case (4.1), with the order J
of the truncation of the series now representing the order of the polynomial term in (x − x0 ) as opposed to
the order of the Hermite polynomials (which are polynomials in (x − x0 )). In the reducible case, we are able
to expand that series in powers of ∆, gather the terms as the coeﬃcient of the term ∆k in the series, take
the limit of the series as the number of Hermite polynomials increase and obtain an explicit expression for


                                                                   15
 (k)      (∞,k)                                                  (k)
CX = CX           , so that we obtained the coeﬃcients CX with no need to Taylor-expand them in (x − x0 ). This
last step is what’s no longer possible when the diﬀusion is irreducible.
                                                                                         (j ,k)
   However, it is still possible to compute the Taylor expansions CX k                            explicitly. Before describing how
to compute such a coeﬃcient, one remaining question to solve is the choice of the order jk (in (x − x0 ))
                                                                                  ¡    ¢
corresponding to a given order k (in ∆). For that purpose, recall that x − x0 = Op ∆1/2 so that
                    ¯                                      ¯     ¡              ¢
                    ¯ (k)                (j ,k)            ¯
                    ¯CX (x|x0 ; θ) ∆k − CX k (x|x0 ; θ) ∆k ¯ = Op (x − x0 )jk ∆k = Op (∆jk /2+k )                                     (5.2)

and setting jk /2 + k = K, i.e.,

                                                        jk = 2(K − k)                                                                 (5.3)

for k = −1, 0, ..., K, will therefore provide an approximation error due to the Taylor expansion in (x − x0 ) of
the same order ∆K for each one of the terms in the series (5.1).
   The resulting expansion will then be

                                                                            (x|x0 ; θ) XK
                                                                 (j     ,−1)
       ˜(K)                m                      C −1                                       (j ,k)         ∆k
       lX (∆, x|x0 ; θ) = − ln (2π∆) − Dv (x; θ) + X                                  +     CX k (x|x0 ; θ)    .                      (5.4)
                           2                                               ∆            k=0                 k!

This double Taylor expansion (in ∆ and in (x − x0 )) can be viewed as a Taylor expansion in ∆ only, in light of
(5.2). In general, the function need not be analytic at ∆ = 0, hence the expansion is to be interpreted strictly
as a Taylor expansion.
                                                                                                      (j ,k)                           (k)
   What remains to be done is to compute explicitly the Taylor expansion CX k                                  of each coeﬃcient CX .
                                                                                                                     (j   ,−1)
As I will now show, this involves solving a cascade of diﬀerential equations, starting with CX −1                                , then use
                                (j ,0)
that solution to determine     CX 0 ,    etc. Fortunately, each one of these diﬀerential problems has a closed-form
solution as we will now see in Section 5.2.


5.1     The Leading Term: Geometric Interpretation
                              (−1)
While the leading term CX            in the case of a reducible diﬀusion is simply

                                        (−1)                   (−1)
                                      CX       (x|x0 ; θ) = CY         (γ (x; θ) |γ (x0 ; θ) ; θ) ,

        (−1)
with CY        (y|y0 ; θ) = − 12 ky − y0 k2 (see (4.21) and (4.30)), the situation is more involved when the diﬀusion
X is not reducible.
   Consider the set Ω (x|x0 ) of m−dimensional diﬀerentiable paths ω(τ ), starting at x0 at time 0 and ending
at x at time 1. An example of such a path is the straight line ω(τ ) = x0 + τ (x − x0 ). Consider now the
Riemannian metric derived from the coeﬃcients of the matrix v(x; θ)−1 , that is the distance between points
x and x + dx deÞned by
                                                  ³Xm                                 ´1/2
                                                                −1
                                           ds =                vij (x; θ) dxi dxj            .                                        (5.5)
                                                       i,j=1




                                                                 16
With this metric, the length of any diﬀerentiable path ω is
                                         Z      1 µXm                                               ¶1/2
                                                                             dω i (τ) ωj (τ )
                          d(ω; θ) =                            vij (ω(τ); θ)                               dτ .
                                            0            i,j=1                 dτ      dτ

   Varadhan (1967) has shown that

                                lim −2∆ lX (∆, x|x0 ; θ)                 =           inf       d(ω; θ)2 .
                                ∆→0                                              ω∈Ω(x|x0 )


Since from (5.1)

                                                  (K)                               (−1)
                                   lim ∆ lX (∆, x|x0 ; θ)                   = CX           (x|x0 ; θ)
                                   ∆→0

the appropriate leading term of the expansion (5.1) ought to be

                                             (−1)                      1
                                         CX         (x|x0 ; θ) = −         inf      d(ω; θ)2                      (5.6)
                                                                       2 ω∈Ω(x|x0 )

that is, minus one half the square of the shortest distance from x to x0 in the metric induced in Rm by the
matrix v(x; θ)−1 .
   An important special case occurs when σ, hence ν, is the identity matrix. In this case, the distance (5.5)
reduces to the usual Euclidean distance, the inÞmum in (5.6) is achieved by the straight line, and we have

                                (−1)                 1               1 Xm
                           CY          (y|y0 ; θ) = − ky − y0 k2 = −        (yi − y0i )2
                                                     2               2  i=1


which is the result obtained in the reducible case for the reduced diﬀusion Y : see equation (4.21).
   But, for any v(x; θ), the distance (5.6) is invariant under coordinate transformations. This applies in
particular to the transformation from X to Y ≡ γ (X; θ) when the diﬀusion is reducible. In this situation, we
have

                                        (−1)                  1
                                   CX           (x|x0 ; θ) = − kγ (x; θ) − γ (x0 ; θ)k2 .
                                                              2

   In dimension m = 1, where every diﬀusion is reducible, this can be recovered directly. We already know
from the univariate case that
                                                                       µZ    x                 ¶2
                                         (−1)                      1                1
                                       CX        (x|x0 ; θ) = −                           dw        .             (5.7)
                                                                   2      x0     σ (w; θ)

Now, the only way to move on the real line (including the shortest distance path) is to stay on that straight
line. Suppose, without loss of generality, that x ≥ x0 . With ω(τ) = x0 + τ(x − x0 ), we have
                                                     Z   1          µ        ¶
                                                            1         dω(τ)
                                 d(ω; θ) =                                     dτ
                                                   0 σ (ω(τ ); θ)      dτ
                                                            Z 1
                                                                            1
                                                = (x − x0 )                              dτ
                                                              0  σ (x0 + τ (x − x0 ); θ)
                                                  Z x
                                                         1
                                                =               dw
                                                   x0 σ (w;  θ)



                                                                   17
with the last equality resulting from the change of variable τ 7→ w = x0 + τ(x − x0 ). Since γ is given by (3.3)
when m = 1, we indeed recover (5.7) from the general formula (5.6).


5.2    Determination of the Coeﬃcients in the Multivariate Irreducible Case
                                                                                                 (j ,k)
I now turn to the determination of a closed-form expression for the Taylor expansions CX k                of the coeﬃcients
 (k)                                                                                               (j ,−1)
CX . Essentially, the coeﬃcients are determined one by one, starting with the leading term CX −1           . Given
 (j−1 ,−1)                  (j0 ,0)
CX         , the next term CX       is calculated explicitly, and so on. The orders of the Taylor expansions j−1 ,
j0 , etc., are chosen to control the order of the remainder terms, setting each jk according to (5.3). This
means in particular that the highest order term (k = −1) is Taylor-expanded to a higher degree of precision
                                                                   (j   ,−1)
than the successive terms. This is to be expected, given that CX −1            in a input to the diﬀerential equation
               (j ,0)
determining   CX 0 ,    and so on.
                                                                                        (j ,k)
   In order to state the main result pertaining to the closed-form solutions CX k                , I deÞne the following
functions of the coeﬃcients and their derivatives:

                           m Xm                   ∂CX (x|x0 ; θ) Xm Xm ∂vij (x; θ) ∂CX (x|x0 ; θ)
                                                     (−1)                              (−1)
       (0)
      GX (x|x0 ; θ) =         −         µi (x; θ)                   +
                           2        i=1                  ∂xi           i=1    j=1  ∂xi    ∂xj
                             1 X  m   X   m               2  (−1)
                                                         ∂ CX (x|x0 ; θ)
                           +                  vij (x; θ)                                          (5.8)
                             2    i=1     j=1                 ∂xi ∂xj
                             Xm Xm                        (−1)
                                                       ∂CX (x|x0 ; θ) ∂Dv (x; θ)
                           −                vij (x; θ)                           ,
                                i=1     j=1                  ∂xi         ∂xj


              (1)
                                   Xm ∂µ (x; θ) 1 Xm Xm ∂ 2 vij (x; θ)
                                               i
             GX (x|x0 ; θ) = −                         +
                                      i=1      ∂xi        2     i=1     j=1   ∂xi ∂xj
                                                     Ã                                !
                                   Xm                      (0)
                                                       ∂CX (x|x0 ; θ) ∂Dv (x; θ)
                                 −         µi (x; θ)                     −
                                      i=1                     ∂xi              ∂xi
                                                                Ã (0)                           !
                                   Xm Xm ∂vij (x; θ) ∂C (x|x0 ; θ) ∂Dv (x; θ)
                                                                      X
                                 +                                                 −                                  (5.9)
                                      i=1      j=1     ∂xi               ∂xj            ∂xj
                                                                (
                                   1 Xm Xm
                                                                       (0)
                                                                  ∂ 2 CX (x|x0 ; θ) ∂ 2 Dv (x; θ)
                                 +                   vij (x; θ)                     −
                                   2    i=1      j=1                   ∂xi ∂xj          ∂xi ∂xj
                                   Ã                                   !Ã                            !)
                                         (0)                                   (0)
                                     ∂CX (x|x0 ; θ) ∂Dv (x; θ)             ∂CX (x|x0 ; θ) ∂Dv (x; θ)
                                 +                     −                                  −
                                            ∂xi                ∂xi                ∂xj           ∂xj




                                                        18
and for k ≥ 2 :

        (k)
                                       Xm                      ∂CX
                                                                   (k−1)
                                                                            (x|x0 ; θ) Xm Xm ∂vij (x; θ) ∂CX  (x|x0 ; θ)
                                                                                                                                   (k−1)
     GX (x|x0 ; θ) = −                             µi (x; θ)                          +
                                             i=1                           ∂xi          i=1 j=1 ∂xi          ∂xj
                                    1 Xm Xm
                                                                                   (k−1)
                                                            ∂ 2 CX     (x|x0 ; θ)
                                  +              vij (x; θ)                                                    (5.10)
                                    2   i=1  j=1                  ∂xi ∂xj
                                                            ( Ã                            !
                                    1 Xm Xm
                                                                     (0)                      (k−1)
                                                                  ∂CX (x|x0 ; θ) ∂Dv (x; θ) ∂CX     (x|x0 ; θ)
                                  +              vij (x; θ) 2                     −
                                    2   i=1  j=1                        ∂xi            ∂xi       ∂xj
                                           µ   ¶                                       )
                                    Xk−2 k − 2 ∂C (h) (x|x0 ; θ) ∂C (k−1−h) (x|x0 ; θ)
                                                      X                X
                                  +                                                      .
                                       h=1   h           ∂xi                ∂xj

                                                                            (k)
   Note that the computation of each function GX requires only the ability to diﬀerentiate the previously
                                      (−1)             (k−1)
determined coeﬃcients CX                     , ..., CX         . The same applies to their Taylor expansions. Let i ≡ (i1 , i2 , ..., im )
denote a vector of integers and

                                               Ik = {i ≡ (i1 , i2 , ..., im ) ∈ Nm : 0 ≤ tr[i] ≤ jk }                                                    (5.11)

                            (j ,k)
so that the form of CX k               is

                       (j ,k)
                                                   X            (k)
                  CX k          (x|x0 ; θ) =                   γ i (x0 ; θ) (x1 − x01 )i1 (x2 − x02 )i2 ... (xm − x0m )im .                              (5.12)
                                                        i∈Ik

                                                                                                     (j ,k)                                (k)
   The following theorem can now describe how the coeﬃcients CX k                                             , i.e., the coeﬃcients γ i , i ∈ Ik , are
determined:

                                                                                       (k)
Theorem 2. For each k = −1, 0, ..., K, the coeﬃcient CX (x|x0 ; θ) in (5.1) solves the equation

                                                                       (k−1)
                                                                      fX       (x|x0 ; θ) = 0                                                            (5.13)

where

         (−2)                                   (−1)
                                                                        Xm Xm                              ∂CX
                                                                                                               (−1)                (−1)
                                                                                                                      (x|x0 ; θ) ∂CX (x|x0 ; θ)
        fX      (x|x0 ; θ) = −2CX                      (x|x0 ; θ) −                           vij (x; θ)                                        (5.14)
                                                                            i=1        j=1                           ∂xi             ∂xj
         (−1)
                                             Xm Xm                             ∂CX
                                                                                   (−1)                    (0)
                                                                                         (x|x0 ; θ) ∂CX (x|x0 ; θ)    (0)
        fX      (x|x0 ; θ) = −                                   vij (x; θ)                                        − GX (x|x0 ; θ) .                     (5.15)
                                                i=1        j=1                          ∂xi             ∂xj

and for k ≥ 1

              (k−1)                             (k)
                                                                       Xm Xm                              ∂CX
                                                                                                              (−1)                (k)
                                                                                                                  (x|x0 ; θ) ∂CX (x|x0 ; θ)
          fX          (x|x0 ; θ) = CX (x|x0 ; θ) −                                           vij (x; θ)
                                                                            i=1        j=1                       ∂xi             ∂xj
                                                (k)
                                              −GX (x|x0 ; θ) .                                                                                           (5.16)

                                (k)                                                            (k)                                               (h)
where the functions GX , k = 0, 1, ..., K are given above. GX involves only the coeﬃcients CX for h =
−1, ..., k−1, so this system of equation can be utilized to solve recursively for each coeﬃcient at a time, meaning
                         (−2)                                   (−1)               (−1)        (0)                                                     (−1)
that the equation fX             = 0 determines CX                     ; given CX         , GX becomes known and the equation fX                              =0
                (0)               (−1)               (0)    (1)                                                         (0)                                   (1)
determines     CX ;    given     CX          and    CX ,   GX         becomes known and the equation                   fX     = 0 then determines CX ,

                                                                                  19
etc.
                                                                                                                              (j ,k)
    Each one of these equations can be solved explicitly in the form of the Taylor expansion CX k                                      of the
               (k)                                                        (k)                         (j ,k)
coeﬃcient     CX ,    at order jk in (x − x0 ). The coeﬃcients          γ i (x0 ; θ) ,   i ∈ Ik of   CX k      are determined by setting
                               (j ,k−1)          (k−1)
the Taylor expansion          fX k        of    fX        to zero. The key feature that makes this problem solvable in closed
                                                                                                                        (k)
form is that the coeﬃcients solve a succession of systems of linear equations: Þrst determine γ i                             for tr[i] = 0,
        (k)
then   γi     for tr[i] = 1, and all the way to tr[i] = jk .


                                                    (−1)
    Note in particular, for k = −1 : γ i                   = 0 for tr[i] = 0, 1 (i.e., the polynomial has no constant or linear
terms) and the terms corresponding to tr[i] = 2 (with of course j−1 ≥ 2) are:


    X                     (−1)                                                              1
                         γi       (x0 ; θ) (x1 − x01 )i1 (x2 − x02 )i2 ... (xm − x0m )im = − (x − x0 )T v−1 (x0 ; θ)(x − x0 ).
        i∈I−1 :tr[i]=2                                                                      2

which is the anticipated term given the Gaussian limiting behavior of the transition density when ∆ is small.
                                                                            (−1)
Thus with j−1 ≥ 3, we only need to determine the terms γ i                         corresponding to tr[i] = 3, ..., j−1 .
                    (0)
    For k = 0 :    γi     = 0 for tr[i] = 0, so the polynomial has no constant term. For k ≥ 1, the polynomials have
                                        (k)
a constant term (for k ≥ 1, γ i               6= 0 for tr[i] = 0 in general).


5.3      Applying the Irreducible Method to a Reducible Diﬀusion
Theorem 2 is more general than Theorem 1 in that it does not require that the diﬀusion be reducible. In
exchange for that generality, the coeﬃcients are available in closed form only in the form of a Taylor series
expansion in (x − x0 ). The following proposition describes the relationship between these two methods when
Theorem 2 is applied to a diﬀusion that is in fact reducible:
                                                                                           (K)
Proposition 3. Suppose that the diﬀusion X is reducible, and let lX                              denote its log-likelihood expansion
                                                                                                                                         (K)
calculated by applying Theorem 1. Suppose now that we also calculate its log-likelihood expansion, l̃X ,
without Þrst transforming X into the unit diﬀusion Y, that is by applying Theorem 2 to X directly. Then
                         (j ,k)                      (K)
each coeﬃcient CX k               (x|x0 ; θ) from l̃X       is a Taylor expansion in (x − x0 ) at order jk of the coeﬃcient
 (k)                   (k)                                       (K)
CX (x|x0 ; θ)     =   CY (γ        (x; θ) |γ (x0 ; θ) ; θ) from lX .

    In other words, applying the irreducible method to a diﬀusion that is in fact reducible involves replacing
                                                    (k)
(needlessly) the exact expression for CX (x|x0 ; θ) by its Taylor series in (x − x0 ). Of course, there is no reason
to do so when the diﬀusion is reducible.


6      Examples
In this section, I apply the results above to three examples of multivariate diﬀusion processes of interest in
Þnancial econometrics.




                                                                       20
6.1     The Bivariate Ornstein-Uhlenbeck Model
Consider the model
                                                                                                            
              dX1t     β 11 (α1 − X1t ) + β 12 (α2 − X2t )          σ                            σ12        dW1t
                  =                                      dt +  11                                              (6.1)
              dX2t     β 21 (α1 − X1t ) + β 22 (α2 − X2t )          σ21                          σ22        dW2t

                                                                                                  T
where the parameter vector is θ = (α1 , α2 , β 11 , β 12 , β 21 , β 22 , σ 11 , σ12 , σ21 , σ 22 ) . Let
                                                                                                 
                                α1                     β 11 β 12                        σ11 σ12
                        α=         , β =                            , σ =                        
                                α2                     β 22 β 22                        σ21 σ22

so that dXt = β (α − Xt ) dt + σdWt , and assume that β has full rank (as well as σ, recall Assumption ??.3).
This is the most basic model capturing mean reversion in the state variables.
    Consider the matrix equation

                                                        βλ + λβ T = σσT                                                (6.2)

whose solution in the bivariate case is the 2 × 2 symmetric matrix λ given by

                                       1        ³                                              ´
                           λ=                    Det [β] σσ T + (β − tr [β]) σσ T (β − tr [β])T .                      (6.3)
                                2tr [β] Det [β]

When the process is stationary, i.e., when the eigenvalues of the matrix β have positive real parts, λ is the
stationary variance-covariance matrix of the process. That is, the stationary density of X is the bivariate
Normal density with mean α and variance-covariance λ.
    The transition density of X is the bivariate Normal density

    pX (∆, x|x0 ; θ) = (2π)−1 Det[Ω (∆; θ)]−1/2 exp(− (x − m (∆, x0 ; θ))T Ω−1 (∆; θ) (x − m (∆, x0 ; θ)))             (6.4)

where

                                      m (∆, x0 ; θ) = α + exp (−β∆) (x0 − α)                                           (6.5)
                                                                                           T
                                           Ω (∆; θ) = λ − exp (−β∆) λ exp(−β ∆)                                        (6.6)

and exp applied to a matrix denotes the matrix exponential (which does not in general reduce to the exponential
of each term of the matrix).
    I now discuss the identiÞcation of the continuous time parameters from the discrete data. This presence of
the matrix exponential exp (−β∆) provides a clear insight into the aliasing phenomenon as it applies to this
model. From the form of the transition function (6.4) with conditional mean and variance (6.5)-(6.6), discrete
data sampled at time interval ∆ may not distinguish between two sets of parameters β and β 0 such that
                   ¡     ¢
exp (−β∆) = exp −β 0 ∆ . The eigenvalues of β are either both real, or both complex conjugates. If they
are complex, then for any given B, there are countably many solutions in β to the equation exp (−β∆) = B.
This phenomenon was noted by Philips (1973). If the eigenvalues of β are a pair of distinct complex conjugate



                                                                  21
numbers that do not diﬀer by an integer multiple of 2πi/∆, let β = T ΛT −1 where T and Λ are respectively
the matrices of eigenvectors and eigenvalues of β. Then for any integer g, the matrix β 0 deÞned by

                                                    2πi
                                        β0 = β +        T · diag(g, −g) · T −1
                                                     ∆
            ¡      ¢
satisÞes exp −β 0 ∆ = exp (−β∆) = B. The phenomenon does not occur if the eigenvalues of β are all real
because β 0 would then have complex elements since the eigenvalues of β 0 are Λ + (2πi/∆) diag(g, −g) with
T and T −1 real in that case.
   This does not necessarily mean that β is not identiÞed, because the conditional variance (6.6) conveys
identifying information about β. Indeed, while the matrix exp (−β∆) and exp(−β T ∆) = exp(−β∆)T are
identical for β and β 0 , the λ matrix may be diﬀerent and as a result the conditional variances Ω (∆; θ)
corresponding to β and β 0 may be diﬀerent. To lose identiÞcation, we would need to Þnd a pair (β 0 , σ 0 ) which
produce the same (m, Ω) as (β, σ). Let v = σσ T and v 0 = σ0 σ 0T . Identical conditional variances under both
sets of parameters would require that

                                 2πi ¡                                                       ¢
                      v0 = v +        T · diag(g, −g) · T −1 · λ + λ · T · diag(g, −g) · T −1 .
                                  ∆

Such a matrix v0 always exist but, as pointed out by Hansen and Sargent (1983), except in degenerate
cases, there is at most a Þnite number of integers g for which v0 is positive deÞnite (which is necessary since
v0 = σ 0 σ0T ). Hence the identiÞcation problem is not as severe as it Þrst seems from looking at the inÞnite
number of solutions to the equation exp (−β∆) = B when β has complex eigenvalues.
   But in any event, if we wish to identify the parameters in θ from discrete data sampled at the given time
interval ∆, then we must restrict the set of admissible parameter values Θ. For instance, we may restrict Θ
in such a way that that the mapping β 7→ exp (−β∆) is invertible, for instance by restricting the admissible
parameter matrices β to have real eigenvalues. This will be the case for example if we restrict attention to
matrices β which are triangular (and of course have real elements). For the rest of this discussion, I will
assume that Θ has been restricted in such a way.
   By applying Proposition 1, we see that the process X is reducible, and that γ (x; θ) = σ−1 x so
                                             ¡ −1               ¢
                                     dYt   =  σ βα − σ−1 βσYt dt + dWt
                                                   ¡           ¢
                                           = σ−1 βσ σ −1 α − Yt dt + dWt
                                           ≡ κ (γ − Yt ) dt + dWt                                           (6.7)

where
                                                                                    
                                               γ1                          κ11   κ12
                            γ = σ−1 α =             , κ = σ−1 βσ =                  .
                                               γ2                          κ21   κ22




                                                          22
       One can therefore apply Theorem 1 which gives:

                                 (−1)                                      2                 2
                               CY        (y|y0 ; θ) = − 12 (y1 − y01 ) − 12 (y2 − y02 )
                                 (0)
                               CY (y|y0 ; θ) = − 12 (y1 − y01 ) ((y1 + y01 − 2γ 1 ) κ11 + (y2 + y02 − 2γ 2 ) κ12 )
                                                             1
                                                         −   2   (y2 − y02 ) ((y1 + y01 − 2γ 1 ) κ21 + (y2 + y02 − 2γ 2 ) κ22 )

                                        ³                                                                                              ´
            (1)
           CY (y|y0 ; θ) =          1
                                    2    κ11 − ((y01 − γ 1 ) κ11 + (y02 − γ 2 ) κ12 )2 + κ22 − ((y01 − γ 1 ) κ21 + (y02 − γ 2 ) κ22 )2
                                                        ¡             ¡          ¢                                     ¢
                                        − 12 (y1 − y01 ) (y01 − γ 1 ) κ211 + κ221 + (y02 − γ 2 ) (κ11 κ12 + κ21 κ22 )
                                                          ¡                                     ¢
                                        + 241
                                              (y1 − y01 )2 −4κ11 2 + κ12 2 − 2κ12 κ21 − 3κ221
                                                        ¡                                                ¡            ¢¢
                                        − 12 (y2 − y02 ) (y01 − γ 1 ) (κ11 κ12 + κ21 κ22 ) + (y02 − γ 2 ) κ212 + κ222
                                                          ¡                                   ¢
                                        + 241
                                              (y2 − y02 )2 −4κ222 + κ221 − 2κ12 κ21 − 3κ212
                                            1
                                        −   3   (y1 − y01 ) (y2 − y02 ) (κ11 κ12 + κ21 κ22 )

                                 ³                              ´
             (2)               1
            CY (y|y0 ; θ) = − 12  2κ211 + 2κ222 + (κ12 + κ21 )2
                                                          ¡                                                ¡            ¢¢
                             + 16 (y1 − y01 ) (κ12 − κ21 ) (y01 − γ 1 ) (κ11 κ12 + κ21 κ22 ) + (y02 − γ 2 ) κ212 + κ222
                                                 1            2
                                         +      12 (y1 − y01 ) (κ12 − κ21 ) (κ11 κ12 + κ21 κ22 )
                                                1
                                                                           ¡           ¡ 2       2
                                                                                                   ¢                                         ¢
                                         +      6 (y2 − y02 ) (κ21 − κ12 ) (y01 − γ 1 ) κ11 + κ21 + (y02 − γ 2 ) (κ11 κ12         + κ21 κ22 )
                                                 1            2
                                         +      12 (y2 − y02 ) (κ21 − κ12 ) (κ11 κ12 + κ21 κ22 )
                                                 1
                                                                                       ¡ 2       2   2     2
                                                                                                              ¢
                                         +      12 (y1 − y01 ) (y2 − y02 ) (κ12 − κ21 ) κ22 + κ12 − κ11 + κ21

       Because this is one of the few multivariate models with a known closed-form density, the Ornstein-
Uhlenbeck process can serve as a useful benchmark to examine the accuracy of the expansions. Table 1
reports the results of 1,000 Monte Carlo simulations comparing the distribution of the maximum-likelihood
                   (EXACT )
estimator θ̂                        based on the exact transition density for this model, around the true value of the para-
                                                                                                        (EXACT )
meters θ0 , to the distribution of the diﬀerence between the exact MLE θ̂                                          and the approximate MLE
     (2)
θ̂         based on the expansion with K = 2 terms shown above. The results in the table show that the diﬀerence
     (EXACT )            (2)                                                                               (EXACT )
θ̂                − θ̂         is several orders of magnitude smaller than the diﬀerence θ̂                           − θ0 due to the sampling
noise.


6.2          A Stochastic Volatility Model
Consider as a second example the prototypical stochastic volatility model
                                                                              
                    dX1t               µ                 γ 11 exp(X2t ) 0       dW1t
                        =                    dt +                                                                                        (6.8)
                    dX2t          κ (α − X2t )                  0       γ 22    dW2t

where X1t plays the role of the log of an asset price and exp(X2t ) is the stochastic volatility variable. While
the term exp(X2t ) violates the linear growth condition, it does not cause explosions due to the mean reverting
nature of the stochastic volatility. This model has no closed-form solution.
       The diﬀusion (6.8) is in general not reducible, so I will apply the method of Theorem 2 to derive the


                                                                                  23
                                                                                                                             (j ,k)
expansion. The expansion at order K = 3 is given by (5.1), with the coeﬃcients CX k                                                   , k = −1, 0, ..., 3 given
by:
                                                      2                    2                2                           2             2              4 2
          (8,−1)
        CX                               1 −x01 )
                    (x|x0 ; θ) = − 12 (xe2x02 γ 2
                                                  −         1 (x2 −x02 )
                                                            2      γ 222
                                                                             + (x1 −x   01 ) (x2 −x02 )
                                                                                       2e2x02 γ 211
                                                                                                                                             1 −x01 ) γ 22
                                                                                                         − (x1 −x6e012x) 02(xγ22−x02 ) + (x24e  4x02 γ 4
                                                    11                                                                         11                       11
                                       (x1 −x01 )4 (x2 −x02 )γ 222       (x1 −x01 )2 (x2 −x02 )4       (x1 −x01 )4 (x2 −x02 )2 γ 222    (x1 −x01 )6 γ 422
                                   −             4x
                                             12e 02 γ 11 4          +              2x    2
                                                                               90e 02 γ 11
                                                                                                     +            4x
                                                                                                             15e 02 γ 11 4            −       6x    6
                                                                                                                                         180e 02 γ 11
                                       (x1 −x01 )4 (x2 −x02 )3 γ 222       (x1 −x01 )6 (x2 −x02 )γ 422
                                   −          45e4x02 γ 411
                                                                       +         60e6x02 γ 611
                                       (x1 −x01 )2 (x2 −x02 )6         (x1 −x01 )4 (x2 −x02 )4 γ 222                6             2 4                8 6
                                   −        945e2x02 γ 211
                                                                 −           630e4x02 γ 411
                                                                                                     − 3(x1 −x140e
                                                                                                                01 ) (x2 −x02 ) γ 22
                                                                                                                     6x02 γ 6           + (x 1 −x01 ) γ 22
                                                                                                                                          1120e8x02 γ 811
                                                                                                                             11



                                                              ³                     ´                                                2 2       (x2 −x02 )2 (6κ+γ 222 )
  (6,0)
 CX       (x|x0 ; θ) =      µ(x1 −x01 )
                             e2x02 γ 211
                                           + (x2 − x02 ) 12 + κ(α−x         γ 222
                                                                                  02 )
                                                                                        − µ(x1 −x     01 )(x2 −x02 )
                                                                                                   e2x02 γ 211
                                                                                                                      − (x12e1 −x01 ) γ 22
                                                                                                                                2x02 γ 2     −        12γ 222
                                                                                                                                       11
                   µ(x1 −x01 )(x2 −x02 )2       µ(x1 −x01 )3 γ 222       (x1 −x01 )2 (x2 −x02 )γ 222
               +         3e2x02 γ 211
                                            − 6e4x02 γ 4             +            12e2x02 γ 211
                                                            11
                   (x2 −x02 )4       µ(x1 −x01 )3 (x2 −x02 )γ 222       (x1 −x01 )2 (x2 −x02 )2 γ 222       7(x1 −x01 )4 γ 422
               +       360      +           3e4x02 γ 411
                                                                   −             45e2x02 γ 211
                                                                                                        + 720e     4x02 γ 4
                                                                                                                          11
                   µ(x1 −x01 )(x2 −x02 )4       4µ(x1 −x01 )3 (x2 −x02 )2 γ 222         (x1 −x01 )2 (x2 −x02 )3 γ 222       µ(x1 −x01 )5 γ 422
               −        45e 2x 02   2
                                  γ 11
                                            −            15e 4x  02   4
                                                                    γ 11
                                                                                      −          180e2x 02   2
                                                                                                           γ 11
                                                                                                                       +       30e6x02 γ 611
                   7(x1 −x01 )4 (x2 −x02 )γ 422     (x2 −x02 )6        4µ(x1 −x01 )3 (x2 −x02 )3 γ 222         (x1 −x01 )2 (x2 −x02 )4 γ 222
               −         360e4x02 γ 411
                                                − 5670 +                           45e4x02 γ 411
                                                                                                           +         315e2x02 γ 211
                   µ(x1 −x01 )5 (x2 −x02 )γ 422     223(x1 −x01 )4 (x2 −x02 )2 γ 422           71(x1 −x01 )6 γ 622
               −          10e6x02 γ 611
                                                +           15120e4x02 γ 411
                                                                                         − 45360e6x02 γ 6
                                                                                                               11




          (4,1)                       (12e2x02 α2 κ2 γ 211 −24e2x02 ακ2 x02 γ 211 +12e2x02 κ2 x02 γ 211 +12µ2 γ 222 −12e2x02 κγ 211 γ 222 +e2x02 γ 211 γ 422 )
       CX          (x|x0 ; θ) = −                                                        24e2x02 γ 211 γ 222
                         µ(x1 −x01 )γ 222      (x2 −x02 )(−e2x02 ακ2 γ 211 +e2x02 κ2 x02 γ 211 −µ2 γ 222 )
                     +     6e2x02 γ 211
                                            −                        2e2x02 γ 211 γ 222
                         µ(x1 −x01 )(x2 −x02 )γ 222        (x1 −x01 )2 (−30e2x02 ακ2 γ 211 +30e2x02 κ2 x02 γ 211 −90µ2 γ 222 −e2x02 γ 211 γ 422 )
                     −         6e2x02 γ 211
                                                      −                                      360e4x02 γ 411
                         (x2 −x02 )2 (−60e2x02 κ2 γ 211 −60µ2 γ 222 +e2x02 γ 211 γ 422 )    2µ(x1 −x01 )(x2 −x02 )2 γ 222                     3 4
                     +                         360e2x02 γ 211 γ 222
                                                                                         +           45e2x02 γ 211
                                                                                                                             − 7µ(x   1 −x01 ) γ 22
                                                                                                                                 180e4x02 γ 411
                         (x1 −x01 )2 (x2 −x02 )(15e2x02 κ2 γ 211 +30e2x02 ακ2 γ 211 −30e2x02 κ2 x02 γ 211 +180µ2 γ 222 +e2x02 γ 211 γ 422 )
                     −                                                     360e4x02 γ 411
                         µ(x1 −x01 )(x2 −x02 )3 γ 222        7µ(x1 −x01 )3 (x2 −x02 )γ 422    (x2 −x02 )4 (−42µ2 +e2x02 γ 211 γ 222 )
                     +         90e2x02 γ 211
                                                       +            90e4x02 γ 411
                                                                                           −                 3780e2x02 γ 211
                         (x1 −x01 )2 (x2 −x02 )2 (98e2x02 κ2 γ 211 +56e2x02 ακ2 γ 211 −56e2x02 κ2 x02 γ 211 +1008µ2 γ 222 +e2x02 γ 211 γ 422 )
                     +                                                      2520e4x02 γ 411
                         (x1 −x01 )4 γ 222 (42e2x02 κ2 γ 211 +112e2x02 ακ2 γ 211 −112e2x02 κ2 x02 γ 211 +840µ2 γ 222 +5e2x02 γ 211 γ 422 )
                     −                                                  10080e6x02 γ 611



      (2,2)                  −30e2x02 κ2 γ 211 −30e2x02 ακ2 γ 211 +30e2x02 κ2 x02 γ 211 −30µ2 γ 222 +e2x02 γ 211 γ 422
 CX           (x|x0 ; θ) =                                       180e2x02 γ 211
                               (x2 −x02 )(e2x02 κ2 γ 211 +2µ2 γ 222 )   µ(x1 −x01 )(30e2x02 ακ2 γ 211 −30e2x02 κ2 x02 γ 211 +30µ2 γ 222 +e2x02 γ 211 γ 422 )
                             +            12e   2x02   2
                                                     γ 11
                                                                      −                                     90e4x02 γ 411
                               µ(x1 −x01 )(x2 −x02 )(15e2x02 κ2 γ 211 +30e2x02 ακ2 γ 211 −30e2x02 κ2 x02 γ 211 +60µ2 γ 222 +e2x02 γ 211 γ 422 )
                             +                                                  90e4x02 γ 411
                               (x1 −x01 )2 γ 222 (−105e2x02 κ2 γ 211 −21e2x02 ακ2 γ 211 +21e2x02 κ2 x02 γ 211 −441µ2 γ 222 +4e2x02 γ 211 γ 422 )
                             −                                                 3780e4x02 γ 411
                               (x2 −x02 )2 (−21e2x02 κ2 γ 211 −42e2x02 ακ2 γ 211 +42e2x02 κ2 x02 γ 211 +168µ2 γ 222 +4e2x02 γ 211 γ 422 )
                             −                                             3780e2x02 γ 211



  (0,3)                      1890µ4 γ 422 +126e2x02 µ2 γ 211 γ 222 (30κ2 (α−x02 )+γ 422 )+e4x02 γ 411 (1890κ4 (x02 −α)2 −63κ2 (1−2α+2x02 )γ 422 −16γ 822 )
 CX       (x|x0 ; θ) =                                                             7560e4x02 γ 411 γ 222


      Finally, while in many instance Þnancial econometricians are willing to let X2t denote an observable
volatility variable (option-implied from the underlying asset’s option price, direct observation of volatility
derivatives contracts such as the VIX, or other sources), if the variable X2t is not observable (latent) then



                                                                                 24
the transition density pX cannot be used directly in (2.2). Instead, the latent variable must be integrated out
from the joint likelihood of prices and volatility in order to obtain the likelihood function to be maximized.
Alternatively, Bayesian methods can make use of pX .


6.3      Multivariate Term Structure Models
Aït-Sahalia and Kimmel (2002) apply the method of this paper to the class of aﬃne yield models for the term
structure of interest rates. They derive the likelihood expansions for the nine canonical models of Dai and
Singleton (2000). For instance, in dimension m = 3, the four canonical models are respectively
                                                                                            
                                    dX1t        κ11     0    0      −X1t          dW1t
                                                                                            
                                dX2t       = κ21           0                                
                                                    κ22        −X2t  dt +  dW2t             
                                 dX3t           κ31    κ32   κ33    −X3t          dW3t

                                                    1/2                                                             
        dX1t        κ11        0      0     θ1 − X1t      X                    0                       0              dW1t
                                                    1t                                                         
     dX2t      = κ21       κ22                     
                                     κ23   −X2t  dt +  0          (1 + β 21 X1t ) 2
                                                                                         1
                                                                                                       0         dW2t 
                                                                                                                   
                                                                                                             1
      dX3t          κ31       κ32    κ33     −X3t          0                   0             (1 + β 21 X1t ) 2     dW3t

                                                     1/2                                                            
        dX1t        κ11       κ12    0     θ1 − X1t        X            0                      0                      dW1t
                                                     1t                                                     
     dX2t      = κ21       κ22                      
                                     0  θ2 − X2t  dt +  0        X2t
                                                                        1/2
                                                                                               0              dW2t 
                                                                                                                
                                                                                                          1
      dX3t          κ31       κ32   κ33      −X3t           0           0      (1 + β 31 X1t + β 32 X2t ) 2     dW3t

                                                                  1/2                                    
                   dX1t        κ11        κ12   κ13    θ1 − X1t         X           0         0            dW1t
                                                                  1t                              
                dX2t      = κ21                                  
                                                κ23  θ2 − X2t  dt +  0
                                                                                    1/2
                                                                                              0          
                                       κ22                                      X2t              dW2t  .
                                                                                              1/2
                 dX3t          κ31        κ32   κ33    θ3 − X3t          0          0        X3t      dW3t

Likelihood expansions for all these models are given in Aït-Sahalia and Kimmel (2002), as well as a Monte
Carlo investigation of the properties of maximum-likelihood estimators of the parameters derived from these
expansions. They show that error due to replacing the exact transition density (for the models where it is
known) with this paper’s approximation is again several orders of magnitude smaller than the uncertainty in the
parameter estimates due to the sampling noise, and that maximum-likelihood estimates are substantially more
eﬃcient (as expected from standard asymptotic theory and the Cramer-Rao lower bound) than alternative
estimates for these models.


7       Conclusions
This paper provides a method to derive closed-form expansions to the likelihood function of arbitrary mul-
tivariate diﬀusions. The multivariate diﬀusion setting presents many challenges, including the fact that not
all diﬀusions are reducible. Nevertheless, the paper provides a method that delivers closed form likelihood


                                                             25
expansions whether the diﬀusion is reducible or not. I hope that this will contribute to making maximum-
likelihood the method of choice for estimating diﬀusion models with discretely sampled data, as is the case for
other time series models.




                                                      26
References
Aït-Sahalia, Y. (2002): “Maximum-Likelihood Estimation of Discretely-Sampled Diﬀusions: A Closed-Form
 Approximation Approach,” Econometrica, 70, 223—262.
Aït-Sahalia, Y., and R. Kimmel (2002): “Estimating Aﬃne Multifactor Term Structure Models Using
 Closed-Form Likelihood Expansions,” Discussion paper, Princeton University.
Aït-Sahalia, Y., and P. A. Mykland (2000): “The Eﬀects of Random and Discrete Sampling When
 Estimating Continuous-Time Diﬀusions,” Discussion paper, Princeton University.
Azencott, R. (1984): “Densité des Diﬀusions en Temps Petit: Développements Asymptotiques, Partie I,”
 in Seminar on Probability XVIII, Lecture Notes in Mathematics 1059, pp. 402—498. Springer, Berlin.
Dai, Q., and K. J. Singleton (2000): “SpeciÞcation Analysis of Aﬃne Term Structure Models,” Journal
 of Finance, 55, 1943—1978.
Edwards, C. H. (1973): Advanced Calculus of Several Variables. Dover, New York.
Egorov, A., H. Li, and Y. Xu (2001): “Maximum Likelihood Estimation of Time Inhomogeneous Diﬀu-
 sions,” Discussion paper, Cornell University.
Haaser, N. B., and J. A. Sullivan (1991): Real Analysis. Dover, New York.
Hansen, L. P., and T. J. Sargent (1983): “The Dimensionality of the Aliasing Problem in Models with
 Rational Spectral Densities,” Econometrica, 51, 377—387.
Hasminskii, R. Z. (1980): Stochastic Stability of Diﬀerential Equations. Sijthoﬀ and Noordhoﬀ, The Nether-
 lands.
Jensen, B., and R. Poulsen (2002): “Transition Densities of Diﬀusion Processes: Numerical Comparison
 of Approximation Techniques,” Journal of Derivatives, 9, 1—15.
Karatzas, I., and S. E. Shreve (1991): Brownian Motion and Stochastic Calculus. Springer-Verlag, New
 York.
McCullagh, P. (1987): Tensor Methods in Statistics. Chapman and Hall, London, U.K.
Philips, P. C. B. (1973): “The Problem of IdentiÞcation in Finite Parameter Continuous Time Models,”
 Journal of Econometrics, 1, 351—362.
Schaumburg, E. (2001): “Maximum Likelihood Estimation of Jump Processes with Applications to Finance,”
 Ph.D. thesis, Princeton University.
Stroock, D. W., and S. R. S. Varadhan (1979): Multidimensional Diﬀusion Processes. Springer-Verlag,
 New York.
Sundaresan, S. M. (2000): “Continuous-Time Finance: A Review and an Assessment,” Journal of Finance,
 55, 1569—1622.
Swart, J. M. (2001): “A 2-Dimensional SDE Whose Solutions Are Not Unique,” Electronic Communications
 in Probability, 6, 67—71.
Varadhan, S. R. S. (1967): “Diﬀusion Processes in a Small Time Interval,” Communications in Pure and
 Applied Mathematics, 20, 659—685.
Watanabe, S., and T. Yamada (1971): “On the Uniqueness of Solutions of Stochastic Diﬀerential Equations
 II,” Journal of Mathematics of Kyoto University, 11, 553—563.
Withers, C. S. (2000): “A Simple Expression for the Multivariate Hermite Polynomials,” Statistics and
Probability Letters, 47, 165—169.

                                                   27
Yamada, T., and S. Watanabe (1971): “On the Uniqueness of Solutions of Stochastic Diﬀerential Equa-
 tions,” Journal of Mathematics of Kyoto University, 11, 155—167.




                                                28
                                           Appendix: Proofs


A       Proof of Proposition 1
                                                                                  T
Suppose that a transformation γ (x; θ) = (γ 1 (x; θ) , ..., γ m (x; θ)) exists and deÞne Yt ≡ γ (Xt ; θ). By Itô’s
Lemma, the diﬀusion matrix of Y is

                                         σY (Yt ; θ) = ∇γ(Xt ; θ) σ (Xt ; θ) .

For σY to be Id, it must therefore be that

                                               ∇γ(Xt ; θ) = σ−1 (x; θ) .

    Thus
                                                                      ∂γ i (x; θ)
                                                σ −1
                                                  ij (x; θ) =                     ,                                 (A.1)
                                                                         ∂xj
hence                                      µ                 ¶              µ                 ¶
                      ∂σ −1
                         ij (x; θ)    ∂        ∂γ i (x; θ)          ∂           ∂γ i (x; θ)           ∂σ−1
                                                                                                        ik (x; θ)
                                   =                             =                                =
                          ∂xk        ∂xk          ∂xj              ∂xj             ∂xk                   ∂xj
for all (i, j, k) = 1, ..., m. Continuity of the second order partial derivatives is required for the order of diﬀer-
entiation to be interchangeable. Here, we have inÞnite diﬀerentiability.
    Conversely, suppose that σ−1 satisÞes (3.4). Then, for each i = 1, ..., m, use row i of the matrix σ −1 ,
        £      ¤
σi· = σ−1
  −1
           i,j ,j=1,...,m , to deÞne the diﬀerential 1−form

                                                             m
                                                             X
                                                    ωi =           σ−1
                                                                    ij dxj
                                                             j=1

and calculate its diﬀerential, the diﬀerential 2−form dω i . We have that
                                       m                    m
                                                               (m            )
                                      X                   X      X ∂σ−1
                                                                      ij
                                              −1
                            dω i =        d(σij ) ∧ dxj =                dxk ∧ dxj
                                      j=1                 j=1 k=1
                                                                    ∂xk
                                       m X  m
                                                 ( −1            )
                                      X           ∂σij      ∂σ−1
                                                              ik
                                  =                     −          dxk ∧ dxj
                                      j=1
                                                   ∂x k      ∂xj
                                           k=j+1

since dxj ∧ dxk = −dxk ∧ dxj and dxj ∧ dxj = 0 (for notation and deÞnitions of diﬀerential forms, see e.g.,
Chapter V in Edwards (1973)).
    Thus condition (3.4) implies that dω i = 0, that is the diﬀerential 1−form ω i is closed on SX . Note also that
because of its form, the domain SX is open and star-shaped (meaning that there exists a point w in its interior
such that for every x ∈ SX the line segment from x to w is contained in SX ). Therefore by Poincaré’s Lemma
(see e.g., Theorem V.8.1 in Edwards (1973)) the form ω i is exact, i.e., there exists a diﬀerential 0−form γ i
such that dγ i = ω i . In other words, for each row i of the matrix σ−1 there exists a function γ i deÞned by
                                                         Z xj
                                            γ i (x; θ) =      σ −1
                                                                ij (x; θ) dxj


(the choice of the index j is irrelevant) which satisÞes (A.1), the required diﬀerentiability properties and is
invertible. The function γ is then deÞned by each of its d components γ i , i = 1, ..., m. By construction,
Yt ≡ γ (Xt ; θ) has unit diﬀusion and therefore X is reducible.



                                                                 29
B     Proof of Theorem 1
The expression for the coeﬃcients is obtained by computing a Taylor expansion using the multivariate generator
(4.19), in eﬀect extending the approach used in the univariate case by Aït-Sahalia (2002). This process establish
the form of the solution. But as is often the case when a diﬀerential operator is involved, it is easier to verify
that a given functional form (in this case established using the generator) is the right solution. Indeed, to show
that (4.20) with the coeﬃcients given in the statement of Theorem 1 represent indeed the Taylor expansion in
∆ of the log-density function lY , at order K − 1, it suﬃces to verify that the diﬀerence between the left and
right hand sides in the Fokker-Planck-Kolmogorov (FPK) forward and backward partial diﬀerential equations
is of order ∆K .
    The forward and backward FPK equations for pY are respectively:
                                                Xm
               ∂pY (∆, y|y0 ; θ)                     ∂
                                        = −              {µY i (y; θ) pY (∆, y|y0 ; θ)}
                    ∂∆                          i=1
                                                    ∂y i
                                                   m       m
                                                1 X X ∂2
                                            +                     {vij (y; θ) pY (∆, y|y0 ; θ)}                                          (B.1)
                                                2 i=1 j=1 ∂yi ∂yj
                                            m
                                            X                                                    m   m
               ∂pY (∆, y|y0 ; θ)                                    ∂pY (∆, y|y0 ; θ) 1 X X ∂ 2 pY (∆, y|y0 ; θ)
                                        =         µY i (y0 ; θ)                      +                                                   (B.2)
                    ∂∆                      i=1
                                                                         ∂y0i          2 i=1 j=1 ∂y0i ∂y0j

   DeÞne FY (∆, y|y0 ; θ) (resp. BY (∆, y|y0 ; θ)) as the the diﬀerence left and right hand sides of (B.1) (resp.
                                          (K)       (K)
(B.2)), divided by pY (∆, y|y0 ; θ); let FY and BY denote the analogous quantities when pY is replaced by
the expansion
                                                    Ã                                          !
                                                      CY (y|y0 ; θ) XK
                                                        (−1)
                 (K)                       −m/2                                 (k)         ∆k
               pY (∆, y|y0 ; θ) = (2π∆)         exp                  +         C (y|y0 ; θ)                (B.3)
                                                             ∆             k=0 Y            k!

obtained by exponentiation of (4.20).
   Starting with the Gaussian leading term (4.21), tedious but otherwise straightforward computations show
that:
                            (K)
                                              XK−1 (k)              ∆k      ¡   ¢
                           FY (∆, y|y0 ; θ) =         fY (y|y0 ; θ)    + O ∆K
                                                k=−1                k!
(with the convention that (−1)! = 0! = 1). The Þrst term is

                   (−1)
                                            Xm                                     Xm                          (0)
                                                                                                            ∂CY (y|y0 ; θ)
                  fY      (y|y0 ; θ) = −           (yi − y0i ) µY i (y; θ) +                  (yi − y0i )                  .
                                             i=1                                        i=1                     ∂yi
    Solving the equation
                                                                (−1)
                                                               fY      (y|y0 ; θ) = 0
     (0)                                                                (0)
for CY (y|y0 ; θ) with the boundary condition that CY be Þnite when going through the axes yj = yj0 for all
j = 1, ..., m yields the solution (4.22). The boundary condition serves to set the generic integration constants
  (0)
αij in the full solution

                               Xm                     Z    1                                         Xm
            (0)                                                                                                         (0)   yi − y0i
           CY (y|y0 ; θ) =              (yi − y0i )            µY i (y0 + u (y − y0 ) ; θ) du +                        αij
                                  i=1                  0                                                 i,j=1, j6=i          yj − y0j
to zero.




                                                                         30
    The next term is

                    (0)                          (1)
                                                                 Xm                          (1)
                                                                                          ∂CY (y|y0 ; θ)
                   fY (y|y0 ; θ) = CY (y|y0 ; θ) +                          (yi − y0i )
                                                                      i=1                     ∂yi
                                               Xm ∂µ (y; θ) Xm                          (0)
                                                                                      ∂CY (y|y0 ; θ)
                                                        Yi
                                             +                   +        µY i (y; θ)
                                                 i=1     ∂y           i=1                  ∂yi
                                                        i                 "                 #2 
                                               1 X m      2 (0)
                                                         ∂ CY (y|y0 ; θ)     ∂CY (y|y0 ; θ) 
                                                                                 (0)
                                             −                   2       +
                                               2   i=1        ∂yi                  ∂yi         

                                                 (1)
                                                                 Xm                          (1)
                                                                                          ∂CY (y|y0 ; θ)    (1)
                                      = CY (y|y0 ; θ) +                     (yi − y0i )                  − GY (y|y0 ; θ)
                                                                      i=1                     ∂yi
           (1)                                                                                     (−1)             (0)
where GY is given in (4.24) and depends on the previously determined CY                                     and CY . Solving the equation
                                                                (0)
                                                               fY (y|y0 ; θ) = 0
     (1)                                                          (1)
for CY , including generic integration constants αij , the explicit solution is
                                        Z    1                                             Xm
                       (1)                         (1)                                                       (1)    yi − y0i
                   CY (y|y0 ; θ) =               GY (y0 + u (y − y0 ) |y0 ; θ) du +                         αij
                                         0                                                    i,j=1, j6=i          (yj − y0j )2
                                                                                                                  (0)
which reduces to (4.23) after accounting for the same boundary condition as for CY .
                               (k−1)
   More generally, the term fY       , k ≥ 1, is given by

                                                               1 Xm
                                                                                    (k)
                   (k−1)                     (k)                                  ∂CY (y|y0 ; θ)    (k)
                  fY         (y|y0 ; θ) = CY (y|y0 ; θ) +             (yi − y0i )                − GY (y|y0 ; θ)
                                                               k  i=1                  ∂yi
           (k)                                                                                        (−1)         (0)      (k−1)
where GY         is given in (4.25) and depends on the previously determined CY                              , CY , ..., CY         . Solving the
equation
                                                                (k)
                                                               fY (y|y0 ; θ) = 0
     (k)                                                                (0)          (1)
for CY (with the same boundary condition as for CY and CY ) yields the explicit solution (4.23). In this
case, the full solution including generic integration constants αij is
                              Z 1                                         Xm
            (k)                    (k)                                                   (k)    yi − y0i
           CY (y|y0 ; θ) = k      GY (y0 + u (y − y0 ) |y0 ; θ) uk−1 du +               αij                 .
                               0                                           i,j=1, j6 =i      (yj − y0j )k+1
                                                         (k)
    Thus by construction, the solution CY , k = −1, 0, ..., K given in the statement of the theorem is such that
                                          (K)                    ¡    ¢
                                         FY (∆, y|y0 ; θ) = O ∆K

which along with the linearity of (B.1) in pY insures that (4.20) is a Taylor expansion of order K − 1 of lY .
Similar calculations show that
                                           (K)                ¡    ¢
                                         BY (∆, y|y0 ; θ) = O ∆K .


C      Proof of Proposition 2
                          (K)
To establish that lX is the sum of the univariate components, it suﬃces to establish that each multivariate
              (k)
coeﬃcient CX of the expansion is the sum of the corresponding univariate coeﬃcients. Further, it suﬃces
to establish this for the coeﬃcients CYk , since the reducibility transformation γ (x; θ) involves each component
separately:
                                      γ (x; θ) = (γ 1 (x1 ; θ) , ..., γ m (xi ; θ))T

                                                                        31
where γ i (xi ; θ) is given from σii (x; θ) by equation (3.3). Therefore, we need to establish that
                                            (k)
                                                          Xm     (k)
                                          CY (y|y0 ; θ) =       CY (yi |y0i ; θ)                                                      (C.1)
                                                                      i=1

for k = −1, 0, ..., K.
    From (4.21), it can be seen that (C.1) is always satisÞed for k = −1 (whether the variables are independent
or not). For k = 0, we have from (4.22) that
                                               Xm                         Z     1
                       (0)
                      CY (y|y0 ; θ)      =                  (yi − y0i )             µY i (y0 + u (y − y0 ) ; θ) du
                                                     i=1                    0
                                               Xm                         Z     1
                                         =      (yi − y0i )    µY i (y0i + u (yi − y0i ) ; θ) du
                                            i=1             0
                                           Xm   Z yi
                                         =           µY i (w; θ) dw
                                                     i=1     y0i
                                               Xm             (0)
                                         =                  CY (yi |y0i ; θ) .
                                                     i=1

   For k = 1, we have

                    (1)
                                         Xm ∂µ (yi ; θ) Xm                           (0)
                                                                                 ∂CY (yi |y0i ; θ)
                                                   Yi
                   GY (y|y0 ; θ) = −                         −       µY i (y; θ)
                                           i=1      ∂y           i=1                     ∂yi
                                                   i                  "                     #2 
                                         1 Xm  ∂ 2 CY (yi |y0i ; θ)     ∂CY (yi |y0i ; θ) 
                                                       (0)                   (0)
                                       +                             +
                                         2   i=1          ∂yi2                  ∂yi            
                                       Xm      (1)
                                     =      GY (yi |y0i ; θ)
                                              i=1

and for k ≥ 2

             (k)
                                     Xm                       ∂CY
                                                                    (k−1)
                                                                           (yi |y0i ; θ) 1 Xm ∂ 2 CY
                                                                                                              (k−1)
                                                                                                                      (yi |y0i ; θ)
           GY (y|y0 ; θ) = −                  µY i (yi ; θ)                             +
                                        i=1                               ∂yi             2 i=1                      ∂yi2
                                                   µ      ¶ (h)
                                    1 Xm Xk−1 k − 1 ∂CY (yi |y0i ; θ) ∂CY
                                                                        (k−1−h)
                                                                                (yi |y0i ; θ)
                                  +
                                    2  i=1   h=0        h       ∂yi          ∂yi
                                  Xm     (k)
                                =      GY (yi |y0i ; θ)
                                      i=1

   Therefore, for k ≥ 1, we have
                                                 Z      1
                          (k)                               (k)
                     CY (y|y0 ; θ) = k        GY (y0 + u (y − y0 ) |y0 ; θ) uk−1 du
                                                    0
                                            Xm Z 1 (k)
                                        = k         GY (y0i + u (yi − y0i ) |y0i ; θ) uk−1 du
                                             i=1 0
                                          Xm    (k)
                                        =     CY (yi |y0i ; θ) .
                                                    i=1




                                                                    32
D      Proof of Theorem 2
This proof proceed along the same lines as that of Theorem 1. The forward and backward FPK equations for
pX are respectively:
                                         Xm
          ∂pX (∆, x|x0 ; θ)                   ∂
                               = −                {µi (x; θ) pX (∆, x|x0 ; θ)}
               ∂∆                        i=1
                                             ∂x i
                                            m   m
                                         1 X X ∂2
                                     +                     {vij (x; θ) pX (∆, x|x0 ; θ)}                                      (D.1)
                                         2 i=1 j=1 ∂xi ∂xj
                                     m
                                     X                                                m    m
          ∂pX (∆, x|x0 ; θ)                              ∂pX (∆, x|x0 ; θ) 1 X X                   ∂ 2 pX (∆, x|x0 ; θ)
                               =           µi (x0 ; θ)                    +           vij (x0 ; θ)                            (D.2)
               ∂∆                    i=1
                                                              ∂x0i          2 i=1 j=1                   ∂x0i ∂x0j

   DeÞne FX (∆, x|x0 ; θ) (resp. BX (∆, x|x0 ; θ)) as the the diﬀerence left and right hand sides of (D.1) (resp.
                                          (K)          (K)          (K)         (K)
(D.2)), divided by pX (∆, x|x0 ; θ); let FX and F̃X (resp. BX and B̃X ) and denote the analogous
quantities when pX is replaced by the expansions
                                        Ã                                                            !
                                                           CX (x|x0 ; θ) XK
                                                            (−1)
     (K)                      −m/2                                                    (k)        ∆k
    pX (∆, x|x0 ; θ) = (2π∆)       exp − ln Dv (x; θ) +                   +         C (x|x0 ; θ)           (D.3)
                                                                 ∆              k=0 X             k!

and
                                           Ã                                                                              !
                                                                               (x|x0 ; θ) XK
                                                                       (j   ,−1)
   (K)                        −m/2                                  CX −1                       (j ,k)         ∆k
 p̃X (∆, x|x0 ; θ)   = (2π∆)         exp − ln Dv (x; θ) +                                +     CX k (x|x0 ; θ)                (D.4)
                                                                              ∆            k=0                 k!

respectively, obtained by exponentiation of (5.1) and (5.4) respectively.
   We have
                             (K)
                                               XK−1 (k)               ∆k     ¡   ¢
                           FX (∆, x|x0 ; θ) =           fX (x|x0 ; θ)     + O ∆K
                                                   k=−2               k!
                                                                                                      (−2)
(with the convention that (−2)! = 2 and (−1)! = 0! = 1). The highest order term is fX given by (5.14) and
                             (−1)                          (−2)                                             (0)
the coeﬃcient function CX is such that it sets fX                 to zero. Then we have successively CX determined
               (−1)                                                      (−1)   (0)       (k−1)
by setting fX in (5.15) to zero, and more generally, given CX , CX , ..., CX                    , the expression (5.16) for
  (k−1)                                                                               (k)
fX      is deÞned and can be set to zero to determine the next coeﬃcient CX .
                                                                                  (k)                     (k)      (j ,k)
    To determine the Taylor expansions in x − x0 for each coeﬃcient CX , k ≥ −1, replace CX by CX k in
                                                                                           (−2)
each equation in turn, starting with (5.14). calculate a Taylor expansion of f˜X                 in (x − x0 ) to order j−1 .
                                                                               (−1)                                     (−1)
This determines a system of equations in the unknown coeﬃcients γ i , i ∈ I−1 (which appear when CX
is Taylor expanded as in (5.12)). By construction, there are as many equations as unknowns (both are given
by the number of elements in I−1 ). This system of equation can always be solved explicitly because it has the
following form.
             (−1)
    First, γ i      = 0 for tr[i] = 0, 1 (i.e., the polynomial has no constant or linear terms) and the terms
corresponding to tr[i] = 2 (with of course j−1 ≥ 2) are:
     X                 (−1)
                      γi    (x0 ; θ) (x1 − x01 )i1 (x2 − x02 )i2 ... (xm − x0m )im = −(x − x0 )T v−1 (x0 ; θ)(x − x0 ).
        i∈I−1 :tr[i]=2

which is the anticipated term given the Gaussian limiting behavior of the transition density when ∆ is small.
                                                              (−1)
Thus with j−1 ≥ 3, we only need to determine the terms γ i         corresponding to tr[i] = 3, ..., j−1 .
    Then, the next order coeﬃcients in (x − x0 ) , i.e., the coeﬃcients corresponding to tr[i] = 3, each appear
linearly in a separate equations. That is, we have a system
                                            (−1)               (−1)                (−1)
                                         M3        (x0 ; θ) · γ 3     (x0 ; θ) = b3       (x0 ; θ)


                                                                    33
                                        (−1)                    (−1)            (−1)
whose explicit solution is given by γ 3 (x0 ; θ) = Inv[M3       (x0 ; θ)] · b3 (x0 ; θ) , and so on. Given the
previously determined coeﬃcients corresponding to tr[i] = 0, ..., r, the equations determining the coeﬃcients
for tr[i] = r + 1 are given by a linear system:
                                      (−1)            (−1)             (−1)
                                    Mr+1 (x0 ; θ) · γ r+1 (x0 ; θ) = br+1 (x0 ; θ)
                       (−1)                   (−1)                                                         (−1)
where the matrix Mr+1 and the vector br+1 are functions of the previously determined coeﬃcients γ i             for
tr[i] = 0, ..., r, and of course x0 and the parameters θ of the process.
                                                                    (0)
    The same principle applies to all values of k. For k = 0 : γ i = 0 for tr[i] = 0, so the polynomial has no
                                                                                   (k)
constant term. For k ≥ 1, the polynomials have a constant term (for k ≥ 1, γ i 6= 0 for tr[i] = 0 in general).
                                                              (j−1 ,−1)
The same principle applies to each equation in turn: once CX            is determined, a Taylor expansion of (5.15)
                                (0)
determines the coeﬃcients γ i , i ∈ I0 , etc.
    Finally, note that the term Dv (x; θ) which arose in the reducible case from the Jacobian transformation is
                                                     (0)
independent of ∆ and so could be built into the CX coeﬃcient. Doing so however would subject it to being
Taylor-expanded in x − x0 , which is unnecessary anyway since Dv (x; θ) is known. If Dv (x; θ) were being
                                    (j ,0)                                             (K)
Taylor-expanded along with CX 0 in (D.4), we would lose the property that p̃X also solves the backward
FPK equation (D.2) to order K − 1 in ∆. Hence the form of the log-likelihood I adopted in (5.1) with Dv
                         (0)
kept separate from CX is essential to obtain
                                                (K)                ¡   ¢
                                              B̃X (∆, x|x0 ; θ) = O ∆K
                 (K)                    ¡  ¢
in addition to F̃X     (∆, x|x0 ; θ) = O ∆K .


E     Proof of Proposition 3
                                        (k)               (k)
If the diﬀusion X is reducible, then CX (x|x0 ; θ) = CY (γ (x; θ) |γ (x0 ; θ) ; θ) . By construction (see the proof
                                  (j ,k)                                              (k)
of Theorem 2), the coeﬃcients CX k are Taylor expansions of the coeﬃcients CX (which are the expressions
                            (k−1)
solutions of the equations fX      = 0).




                                                         34
                                                          (MLE)                                       (MLE)          (2)
                 Parameter          θ(T RU E)        θ̂           − θ (T RUE)                    θ̂           − θ̂

                                                     Mean            Stnd. Dev.            Mean               Stnd. Dev.


                       γ1               0          −0.0013              0.069       −0.0000015                  0.000035

                       γ2               0           0.00070             0.033        0.00000012                 0.000016

                       κ11              5             0.52              1.17               0.012                 0.0085

                       κ12              1           −0.066              1.74               0.0087                    0.017

                       κ22              5             0.35              1.50               0.069                     0.029




           Table 1: Monte-Carlo Simulations for the Bivariate Ornstein-Uhlenbeck Model


This table reports the results of 1,000 Monte Carlo simulations comparing the distribution of the maximum-likelihood
             (MLE)
estimator θ̂        based on the exact transition density for this model, around the true value of the parameters θ0 , to
                                                                   (MLE)                                 (2)
the distribution of the diﬀerence between the exact MLE θ̂                 and the approximate MLE θ̂ based on the expansion
with K = 2 terms, for the process (6.7). To insure full identiÞcation, the oﬀ-diagonal term κ21 is constrained to be zero.
As discussed in the text, this guarantees that the eigenvalues of the mean reversion matrix are both real and avoids the
aliasing problem altogether. The constraints κ11 > 0 and κ22 > 0 are imposed to insure stationarity of the process. The
true values of the parameter vector θ = (γ 1 , γ 2 , κ11 , κ12 , κ22 ) used to generate the data are θ(T RU E) = (0, 0, 5, 1, 5). Each
of the 1,000 samples is a series of n = 500 weekly observations (∆ = 1/52), generated using the exact discretization
                                                                            (MLE)          (2)
of the process. The results in the table show that the diﬀerence θ̂                 − θ̂         is several orders of magnitude smaller
                      (MLE)
than the diﬀerence θ̂       − θ(T RU E) due to the sampling noise.




                                                                   35
