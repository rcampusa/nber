                                NBER WORKING PAPER SERIES




HIGHER ORDER PROPERTIES OF THE WILD BOOTSTRAP UNDER MISSPECIFICATION

                                           Patrick M. Kline
                                            Andres Santos

                                        Working Paper 16793
                                http://www.nber.org/papers/w16793


                      NATIONAL BUREAU OF ECONOMIC RESEARCH
                               1050 Massachusetts Avenue
                                 Cambridge, MA 02138
                                     February 2011




The views expressed herein are those of the authors and do not necessarily reflect the views of the
National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2011 by Patrick M. Kline and Andres Santos. All rights reserved. Short sections of text, not to exceed
two paragraphs, may be quoted without explicit permission provided that full credit, including © notice,
is given to the source.
Higher Order Properties of the Wild Bootstrap Under Misspecification
Patrick M. Kline and Andres Santos
NBER Working Paper No. 16793
February 2011
JEL No. C12

                                               ABSTRACT

We examine the higher order properties of the wild bootstrap (Wu, 1986) in a linear regression model
with stochastic regressors. We find that the ability of the wild bootstrap to provide a higher order refinement
is contingent upon whether the errors are mean independent of the regressors or merely uncorrelated.
In the latter case, the wild bootstrap may fail to match some of the terms in an Edgeworth expansion
of the full sample test statistic, potentially leading to only a partial refinement (Liu and Singh, 1987).
To assess the practical implications of this result, we conduct a Monte Carlo study contrasting the
performance of the wild bootstrap with the traditional nonparametric bootstrap.


Patrick M. Kline
Department of Economics
UC, Berkeley
508-1 Evans Hall #3880
Berkeley, CA 94720
and NBER
pkline@econ.berkeley.edu

Andres Santos
Department of Economics
8&6DQ'LHJR
9500 Gilman Drive
La Jolla, CA 92093-0508
a2santos@ucsd.edu
1     Introduction

The wild bootstrap of Wu (1986) and Liu (1988) provides a procedure for conducting inference in
the model:
                                           Y = X 0 β0 +  ,                                       (1)

where Y ∈ R, X ∈ Rdx and  may have a heteroskedastic structure of unknown form. This
robustness to arbitrary heteroscedasticity provides a distinct advantage over the residual bootstrap
of Freedman (1981) while retaining some of its computational and statistical advantages. This has
led to increasing attention among economists who are often concerned with robust inference in small
sample environments (Horowitz (1997, 2001), Cameron et al. (2008), Davidson and Flachaire (2008))
and to a variety of recent extensions beyond the basic linear regression model (Cavaliere and Taylor
(2008), Gonçalves and Meddahi (2009), Davidson and MacKinnon (2010)). To date, however, the
higher order properties of the wild bootstrap have only been studied under the assumption that
the errors are mean independent of the regressors. Liu (1988) established that when this condition
holds the wild bootstrap provides a refinement over a normal approximation.

    In this paper we contribute to the literature by analyzing the higher order properties of the wild
bootstrap in instances where the conditional mean function may be misspecified. Concretely, we
examine the ability of the wild bootstrap to provide a refinement over the normal approximation
when  is uncorrelated with X but not necessarily mean independent of it – a setting pervasive in
economics where regressors are stochastic rather than fixed or chosen by the econometrician. It is
precisely in such environments that heteroskedasticity is likely to arise (White (1982)) making the
higher order properties of the wild bootstrap of particular interest.

    We conduct our analysis in two steps. First, we compute the approximate cumulants (Bhat-
tacharya and Ghosh (1978)) of t-statistics under both the full sample and bootstrap distributions
with general assumptions on the wild bootstrap weights. We show that both the first and third
                                                         1
approximate cumulants may disagree up to order Op (n− 2 ) if higher powers of X are correlated with
; a situation that is ruled out under proper specification. This higher order discordance between
the approximate cumulants under the full sample and bootstrap distribution implies that if valid
                                                                               1
Edgeworth expansions exist they would only be equivalent up to order Op (n− 2 ) (Hall (1992)). As a
result, despite remaining consistent under misspecification, the wild bootstrap may fail to provide
a higher order refinement over a normal approximation.

    We complement this result by formally establishing the existence of valid one term Edgeworth ex-


                                                  2
pansions when the distribution of the wild bootstrap weights is additionally assumed to be strongly
nonlattice (Bhattacharya and Rao (1976)). In accord with Liu (1988) we note that one-sided wild
bootstrap tests obtain a refinement to order Op (n−1 ) under proper specification. However, this
result is undermined by certain forms of misspecification under which only some, but not all, of
the second order terms in the full sample Edgeworth expansion are matched by their bootstrap
counterparts. Consequently, the wild bootstrap may provide only a partial refinement over the
normal approximation (Liu and Singh (1987)). To assess the practical implications of this result,
we conclude by conducting a Monte Carlo study contrasting the performance of the wild bootstrap
with the traditional nonparametric bootstrap in the presence of misspecification.

    The rest of the paper is organized as follows. Section 2 contains our theoretical results while
Section 3 examines the implications of our analysis in a simulation study. We briefly conclude in
Section 4 and relegate all proofs to the Appendix.



2     Theoretical Results

While numerous variants of the wild bootstrap exist, we study the original version proposed by Wu
(1986) and Liu (1988). Succinctly, given a sample {Yi , Xi }ni=1 and β̂ the OLS estimator from such
sample, the wild bootstrap generates new errors and dependant variables:

                           Yi∗ ≡ Xi0 β̂ + ∗i             ∗i ≡ (Yi − Xi0 β̂)Wi ,                      (2)

where {Wi }ni=1 is an i.i.d. sample independent of the original data {Yi , Xi }ni=1 . A bootstrap estimator
                                                                                     √
β̂ ∗ can then be computed from the sample {Yi∗ , Xi }ni=1 and the distribution of n(β̂ ∗ − β̂) conditional
                                                                    √
on {Yi , Xi }ni=1 (but not {Wi }ni=1 ) used to approximate that of n(β̂ − β0 ). While it may not be
possible to compute the bootstrap distribution analytically, it is straightforward to simulate it.

    We focus our analysis on inference on linear contrasts of β0 , which includes both individual
coefficients and predicted values as special cases. In particular, for an arbitrary c ∈ Rdx we examine:
                            √
                               n 0
                      Tn ≡      c (β̂ − β0 )             σ̂ 2 ≡ c0 Hn−1 Σn (β̂)Hn−1 c ,             (3)
                             σ̂
where the dx × dx matrices Hn and Σn (β) are defined by:
                            n                                     n
                       1X                                    1X
                  Hn ≡       Xi Xi0                 Σn (β) ≡       Xi Xi0 (Yi − Xi0 β)2 .              (4)
                       n i=1                                 n i=1
The bootstrap statistic Tn∗ is then the analogue to Tn but computed on {Yi∗ , Xi }ni=1 instead. Namely,
                              √
                                  n
                       Tn ≡ ∗ c0 (β̂ ∗ − β̂)
                         ∗
                                                   (σ̂ ∗ )2 ≡ c0 Hn−1 Σ∗n (β̂ ∗ )Hn−1 c ,          (5)
                               σ̂
                                                    3
where Hn is as in (4), and Σ∗n (β) ≡       1
                                                    Xi Xi0 (Yi∗ − Xi0 β)2 .
                                               P
                                           n    i


       As argued in Mammen (1993), under mild assumptions on the wild bootstrap weights {Wi }ni=1 ,
the distribution of Tn∗ conditional on {Yi , Xi }ni=1 , (but not {Wi }ni=1 ) provides a consistent estimator
for the distribution of Tn . Consequently, tests based upon a comparison of the statistic Tn to the
quantiles of the bootstrap distribution of Tn∗ are asymptotically justified. In what follows, we explore
whether such a procedure provides a refinement over employing the quantiles of a standard normal
distribution instead.



2.1        Assumptions

We explore the higher order properties of the wild bootstrap under the following assumptions:

Assumption 2.1. (i) {Yi , Xi }ni=1 is an i.i.d. sample, satisfying (1) with E[X] = 0; (ii) (Y, X)
are bounded almost surely; (iii) E[XX 0 ] = I and Σ0 ≡ E[XX 0 2 ] is full rank; (iv) For Z ≡
(X 0 , vech(XX 0 )0 , vech(XX 0 2 )0 )0 , and ξZ its characteristic function, lim supktk→∞ |ξZ (t)| < 1.1

Assumption 2.2. (i) {Wi }ni=1 is i.i.d., independent of {Yi , Xi }ni=1 with E[W ] = 0, E[W 2 ] = 1 and
E[W ω ] < ∞, ω ≥ 9; (ii) For U ≡ (W, W 2 )0 , ξU its characteristic function, lim sup|t|→∞ |ξU (t)| < 1.


       Assumption 2.1(i) allows for misspecification of the conditional mean function by requiring
E[X] = 0 rather than E[|X] = 0. In Assumption 2.1(ii) we impose that (Y, X) be bounded.
This specialized (yet widely applicable) setting simplifies the arguments employed in obtaining an
Edgeworth expansion for Tn∗ . Our finding that the wild bootstrap may fail to provide a higher order
refinement under misspecification would not be overturned if Assumption 2.1(ii) were weakened to
less stringent moment conditions. Assumption 2.1(ii) additionally imposes that E[XX 0 ] = I, which
is just a normalization in the present context; see Remark 2.1. The requirements on {Wi }ni=1 in
Assumption 2.2(i) are standard in the wild bootstrap literature and satisfied by all commonly used
choices of wild bootstrap weights.

       Assumptions 2.1(i)-(iii) and 2.2(i) suffice for showing that the approximate cumulants of Tn and
                                                                                    1
of Tn∗ under the bootstrap distribution may disagree up to order Op (n− 2 ) under misspecification.
In order to additionally establish the existence of Edgeworth expansions, however, we also impose
Assumptions 2.1(iv) and 2.2(ii). These requirements, also known as Cramer’s condition, are stan-
dard in the Edgeworth expansion literature (Bhattacharya and Rao (1976)). They are satisfied,
   1
       For a symmetric matrix A, vech(A) denotes a column vector composed of its unique elements.


                                                             4
for example, if the distributions of Z and U have a component that is absolutely continuous with
respect to Lebesgue measure. Unfortunately, this requirement rules out two frequently used wild
bootstrap weights: Rademacher random variables and the weighting scheme advocated in Mammen
(1993). Thus, while our results on approximate cumulants are applicable to these choices of weights,
our results on Edgeworth expansions are not.

Remark 2.1. Since we study Tn for generic vectors c ∈ Rdx , Assumption 2.1(iii) is just a convenient
normalization. Specifically, suppose E[XX 0 ] = ΣX for ΣX full rank. We may then rewrite (1) as:

                                                                          −1                             1
                               Y = X̃ 0 βI,0 +                 X̃ = ΣX 2 X                 βI,0 = ΣX2 β0 .                           (6)

It is then immediate that E[X̃ X̃ 0 ] = I. Moreover, since                                X̃i X̃i0 is invertible if and only if Hn is,
                                                                                  P
                                                                                      i

we obtain that for any c̃ ∈ Rdx and β̂I the OLS estimator on {Yi , X̃i }ni=1 :
                                                  n            n
                     √ 0                √ 0 12 X             1 X
                                                        0 −1 2    −1         √
                      nc̃ (β̂I − β̃0 ) = nc̃ ΣX (   Xi Xi ) ΣX   ΣX 2 Xi i = nc0 (β̂ − β0 ) ,                                        (7)
                                                       i=1                      i=1

                 1                                                                                                                    1
where c = ΣX2 c̃. Similarly, c̃0 ( n1                    −1 1               0         0      2 1                0 −1       2
                                         P                      P                                   P
                                             i X̃i X̃i )            i X̃i X̃i (Yi − X̃i β̂I ) ( n       i X̃i X̃i ) c̃ = σ̂ for c = ΣX c̃.
                                                                                                                                     2
                                                            n

Hence, since the choice of c ∈ Rdx is arbitrary, studying Tn for some c under Assumption 2.1(iii) is
                                                                                                                        −1
equivalent to studying it under the assumption that E[XX 0 ] be full rank and c̃ = ΣX 2 c.

Remark 2.2. Assumption 2.1(iv) precludes X from containing a constant term. To accommodate
this common case, if the constant is the first element of the vector X, then Assumption 2.1(iv)
should hold for Z ≡ (X 0 , vech−1 (XX 0 )0 , vech(XX 0 2 )0 )0 where for a vector v = (v (1) , . . . , v (d) ) we
define v−1 ≡ (v (2) , . . . , v (d) ).



2.2       Approximate Cumulants

In what follows, for notational simplicity, we denote expectations, probability and law statements
conditional on {Yi , Xi }ni=1 (but not {Wi }ni=1 ) by E ∗ , P ∗ and L∗ respectively. Additionally, we define
the following parameters which play a fundamental role in our higher order analysis:

      σ 2 ≡ c0 Σ 0 c         κ ≡ E[(c0 X)3 3 ]                 γ0 ≡ E[(c0 X)2 X]                  γ1 ≡ E[(c0 X)(X 0 X)] .          (8)

Finally, we let Φ denote the distribution of a standard normal random variable and φ its density.

    We begin our analysis by obtaining an asymptotic expansion for Tn and Tn∗ .




                                                                      5
Theorem 2.1. Suppose Assumption 2.1(i)-(iii) and 2.2(i) hold, and for c ∈ Rdx define:
                                        n                 n                              n
                   0            1 X                  1 X 0                  2    2    2X 0
           Ln ≡ c {I + ∆n } √              Xi i − 3 √        (c Xi )i {(σ̂R − σ ) −      γ Xi i }           (9)
                                 nσ i=1            2σ n i=1                           n i=1 0
                          1 X             1      1
           L∗n ≡ c0 Hn−1 √        Xi ∗i { − 3 ((σ̂s∗ )2 − σ̂ 2 )}                                            (10)
                           n i=1          σ̂ 2σ̂

where ∆n ≡ I − Hn , σ̂R2 ≡ c0 Σn (β0 )c + 2c0 ∆n Σ0 c and (σ̂s∗ )2 ≡ c0 Hn−1 Σ∗n (β̂)Hn−1 c. It then follows that:
                                               1                                1
                           Tn = Ln + op (n− 2 )          Tn∗ = L∗n + op∗ (n− 2 ) a.s.


   Recall that in Assumption 2.1(ii) the covariance E[XX 0 ] was normalized to equal the identity
matrix. Therefore ∆n ≡ I − Hn is the estimation error in the Hessian and the first term in (9)
captures the contribution to Tn of not knowing the true value of E[XX 0 ]. Similarly, the contribution
of having to estimate the variance is divided into two parts: (i) n2 i γ00 Xi i which reflects use of β̂
                                                                    P

rather than β0 in the sample variance calculations and (ii) σ̂R2 − σ 2 which captures the randomness
that would be present in estimating σ 2 if β0 were known. Interestingly, these terms are smaller
order under the bootstrap distribution due to the mean independence of ∗ and X.

   Due to their polynomial form, the moments of Ln and L∗n are considerably easier to compute
than those of Tn and Tn∗ . However, the cumulants of Ln and L∗n provide only an approximation
to those of Tn and Tn∗ and were for this reason termed “approximate cumulants” by Bhattacharya
and Ghosh (1978). Despite their approximate nature, the cumulants of Ln and L∗n play a crucial
role as they may be employed in place of the cumulants of Tn and Tn∗ when computing their second
order Edgeworth expansions if such expansions are indeed valid. Thus, a discordance between the
approximate cumulants is indicative of an analogous difference in the corresponding Edgeworth
expansions if such expansions do exist.

   Theorem 2.2 shows the approximate cumulants may disagree under misspecification.

Theorem 2.2. Let Xk (Ln ) and Xk∗ (L∗n ) denote the k th cumulants of Ln and L∗n respectively and
define κ̂ ≡ n1 i (c0 Hn−1 Xi )3 (Yi − Xi0 β̂)3 . If Assumptions 2.1(i)-(iii) and 2.2(i) hold, then:
              P

                           κ    γ1   2c0 Σ0 γ0                                  E[W 3 ]κ̂
         X1 (Ln ) = −       √ − √  +     √                     X1∗ (L∗n ) = −        √
                        2σ 3 n σ n    σ3 n                                      2σ̂ 3 n
         X2 (Ln ) = 1 + O(n−1 )                                X2∗ (L∗n ) = 1 + Oa.s. (n−1 )
                         2κ    6c0 Σ0 γ0                                        2E[W 3 ]κ̂
         X3 (Ln ) = −     √  +     √ + O(n−1 )                 X3∗ (L∗n ) = −        √     + Oa.s. (n−1 ) .
                        σ3 n    σ3 n                                             σ̂ 3 n

   Observe first that unless κ = 0, the wild bootstrap fails to correct the first term in the first and
third cumulants if E[W 3 ] 6= 1. This property has already been noted in Liu (1988) who advocates

                                                        6
imposing E[W 3 ] = 1 for precisely this reason. However, even with this restriction, two additional
terms in the first and third cumulants of Ln remain. These terms capture (i) the correlation between
Hn and n1 i Xi i , and (ii) the additional randomness of employing β̂ rather than β0 in estimating
          P

σ 2 . Both these expressions are of smaller order under mean independence but may be present
otherwise. Because the wild bootstrap imposes mean independence in the bootstrap distribution
it fails to mimic these terms. As a result, a discordance between the full sample and bootstrap
approximate cumulants will arise under misspecification if the error term  is correlated with higher
powers of X so that γ0 or γ1 are nonzero.



2.3    Edgeworth Expansions

Under the additional requirement that the Cramer conditions hold (Assumptions 2.1(iv) and 2.2(ii))
we now establish that the discordance in approximate cumulants indeed translates into an analogous
disagreement between Edgeworth expansions.

Theorem 2.3. Under Assumptions 2.1(i)-(iv) and 2.2(i)-(ii) it follows that uniformly in z:
                                 φ(z)κ              φ(z)                                      1
          P (Tn ≤ z) = Φ(z) +      3
                                     √ (2z 2 + 1) − 3 √ (c0 Σ0 γ0 (z 2 + 1) − γ1 σ 2 ) + o(n− 2 )   (11)
                                6σ n               σ n
                                           3
                                φ(z)κ̂E[W ] 2                1
         P ∗ (Tn∗ ≤ z) = Φ(z) +       3
                                        √    (2z + 1) + o(n− 2 )      a.s.                          (12)
                                   6σ̂ n

    As Theorem 2.3 shows, the wild bootstrap provides the usual skewness correction whenever
E[W 3 ] = 1. However, when the conditional mean function is misspecified, imposing mean inde-
pendence in the wild bootstrap sample implies the bootstrap distribution may fail to match all the
second order terms in the expansion for Tn . In particular, if  is correlated with higher moments of
X, so that γ0 and γ1 are not equal to zero, the wild bootstrap will only provide a partial refinement
over a normal approximation. The importance of such a refinement is dependent on the degree of
misspecification as measured by the magnitude of γ0 and γ1 . In particular, if the misspecification is
                         1
local with γ0 , γ1 = O(n− 2 ), then the wild bootstrap does attain the usual higher order refinement.



3     Monte Carlo

We turn now to a study of the effect of misspecification on the finite sample performance of the wild
bootstrap through a series of Monte Carlo sampling experiments. To ensure that our theoretical
results are relevant, we restrict our attention to cases where: (i) X is continuously distributed

                                                    7
           Table 1: Rejection rates for 0.05 nominal size - One sided tests


                               Sample Size n = 10. Alternative Hypothesis H1 : β < 0
                 Noise Level λ = 0.25           Noise Level λ = 0.5             Noise Level λ = 1
               Analytical Wild Pairs         Analytical Wild Pairs         Analytical Wild Pairs
    ψ = −0.2     0.100    0.054 0.073          0.102     0.061 0.077          0.096     0.071 0.073
    ψ = 0.0      0.094    0.076 0.078          0.094     0.076 0.078          0.094     0.076 0.078
    ψ = 0.2      0.221    0.186 0.163          0.153     0.130 0.120          0.114     0.092 0.095

                               Sample Size n = 10. Alternative Hypothesis H1 : β > 0
                 Noise Level λ = 0.25           Noise Level λ = 0.5             Noise Level λ = 1
               Analytical Wild Pairs         Analytical Wild Pairs         Analytical Wild Pairs
    ψ = −0.2     0.207    0.136 0.115          0.149     0.104 0.082          0.112     0.079 0.061
    ψ = 0.0      0.078    0.052 0.039          0.078     0.052 0.039          0.078     0.052 0.039
    ψ = 0.2      0.094    0.047 0.049          0.083     0.055 0.047          0.078     0.050 0.046

                               Sample Size n = 20. Alternative Hypothesis H1 : β < 0
                 Noise Level λ = 0.25           Noise Level λ = 0.5             Noise Level λ = 1
               Analytical Wild Pairs         Analytical Wild Pairs         Analytical Wild Pairs
    ψ = −0.2     0.077    0.053 0.060          0.076     0.059 0.060          0.075     0.069 0.087
    ψ = 0.0      0.068    0.072 0.095          0.068     0.072 0.095          0.068     0.072 0.095
    ψ = 0.2      0.175    0.155 0.145          0.127     0.109 0.127          0.090     0.078 0.110

                               Sample Size n = 20. Alternative Hypothesis H1 : β > 0
                 Noise Level λ = 0.25           Noise Level λ = 0.5             Noise Level λ = 1
               Analytical Wild Pairs         Analytical Wild Pairs         Analytical Wild Pairs
    ψ = −0.2     0.148    0.107 0.093          0.101     0.081 0.081          0.070     0.057 0.054
    ψ = 0.0      0.048    0.035 0.049          0.048     0.035 0.049          0.048     0.035 0.049
    ψ = 0.2      0.070    0.044 0.042          0.052     0.043 0.047          0.049     0.039 0.043




and bounded, (ii)  is continuously distributed and bounded and (iii) the bootstrap weights W are
continuously distributed with E[W ] = 0, E[W 2 ] = 1, and E[W 3 ] = 1.

   Let Z ∼ T N (µ, σ 2 , τ ) denote a normal random variable with mean µ and variance σ 2 , truncated
to lie in the interval [−τ, τ ]. The regressor X was drawn from a mixture of Z1 ∼ T N (0, 1, 2) with
probability 0.1 and from Z2 ∼ T N (1, 4, 4) with probability 0.9, recentered and scaled to have mean
zero and variance one. We generate the variable Y according to the relationship:

                                 Yi = ψ{Xi2 − E[X 3 ]Xi − 1} + λη ,                                   (13)

where η is the exponential of a T N (0, 1, 2) random variable, recentered to have mean zero, and ψ, λ
are scalar parameters that will be changed across different Monte Carlo specifications.

   We examine the ability of the wild bootstrap to control size when conducting inference on the

                                                   8
              Table 2: Rejection rates for 0.05 nominal size - Two sided tests


                                  Sample Size n = 10. Alternative Hypothesis H1 : β 6= 0
                    Noise Level λ = 0.25           Noise Level λ = 0.5             Noise Level λ = 1
                  Analytical Wild Pairs         Analytical Wild Pairs         Analytical Wild Pairs
       ψ = −0.2     0.242    0.145 0.070          0.181     0.121 0.052          0.136     0.097 0.039
       ψ = 0.0      0.109    0.087 0.033          0.109     0.087 0.033          0.109     0.087 0.033
       ψ = 0.2      0.244    0.167 0.062          0.174     0.130 0.051          0.131     0.106 0.042

                                  Sample Size n = 20. Alternative Hypothesis H1 : β 6= 0
                    Noise Level λ = 0.25           Noise Level λ = 0.5             Noise Level λ = 1
                  Analytical Wild Pairs         Analytical Wild Pairs         Analytical Wild Pairs
       ψ = −0.2     0.156    0.110 0.068          0.120     0.094 0.051          0.095     0.071 0.032
       ψ = 0.0      0.066    0.060 0.028          0.066     0.060 0.028          0.066     0.060 0.028
       ψ = 0.2      0.176    0.139 0.074          0.109     0.099 0.048          0.079     0.072 0.035




slope coefficient in the following linear regression model:

                                              Yi = α + Xi β +  .                                                     (14)

By construction, the unique parameters (α, β) ensuring that E[X] = 0 in (14) are (α, β) = 0. The
parameter ψ in (13) therefore governs the extent of misspecification in the regression model, with
ψ = 0 corresponding to proper specification (E[Y |X] = 0). Similarly, the scale parameter λ in (13)
controls the level of noise in the linear regression.

      Table 1 shows the empirical rejection rates of one-sided tests under different values of the pa-
rameters governing misspecification and residual noise. Code for our Monte Carlo experiments
is available online. All rejection rates were computed using 200 bootstrap repetitions and 1,000
Monte Carlo replications. We implement the wild bootstrap drawing the weights W from a recen-
tered Gamma distribution with shape parameter 4 and scale parameter 1/2 as suggested by Liu
(1988). For comparison with the wild bootstrap, we also examine the ability of the nonparametric
                                                                       2
(“pairs”) bootstrap and analytical t-tests to control size.

      The results suggest both the wild and nonparametric bootstraps yield improvements over an
analytical t-test for one sided alternaitves. The relative performance of the two bootstraps under
misspecification (ψ 6= 0) is dependent on the level of misspecification, the direction of the test
and the level of noise. Table 2 provides false rejection rates for two-sided tests. Here the ranking
of the various techniques is more clear cut with the nonparametric bootstrap performing best
                                                                 √
  2
      The nonparametric bootstrap computes the distribution of       nc0 (β̂ − β0 )/σ̂ under the empirical measure.


                                                        9
under misspecification and the normal approximation worst. Notably, the improvement of the wild
bootstrap over the first order analytical approximation is still substantial, illustrating the practical
importance of our theoretical finding of a partial refinement.



4    Conclusion

We find that the wild bootstrap may provide only a partial refinement over a normal approximation
when the conditional mean function is misspecified. This suggests that while the wild bootstrap
may not work as well as the nonparametric bootstrap in many settings where regression is used,
it will likely still generate an improvement over analytical techniques. Our Monte Carlo study, for
example, found that the wild bootstrap performed nearly as well as the nonparametric bootstrap
in one-sided tests and still provided substantial improvements over normal approximations in two-
sided tests. We conclude that in small sample environments where misspecification is of concern,
the nonparametric bootstrap possesses a modest advantage over the wild bootstrap.




                                                  10
                                            APPENDIX A - Proofs of Theorems 2.1 and 2.2

    The following is a table of the notation and definitions that will be used throughout the appendix.

         k · kF          On a matrix A, kAkF denotes the Frobenius norm.
         k · ko          On a matrix A, kAko denotes the usual operator norm.
                         For a vector λ of positive integers and λ(i) its ith coordinate |λ| = i λ(i) .
                                                                                              P
           |λ|
                                                                                 ∂ |λ| f
            Dλ f         For f : Rd → R and λ ∈ R, Dλ f =                       (1)      (d)   .
                                                                           ∂λ      ...∂ λ
             ei          The OLS residual ei = (Yi − Xi β̂).
             Φ           The distribution of a standard normal random variable in Rd (d may be context specific).


Lemma A.1. Let {Zi }ni=1 be an i.i.d. sample of Z a k × p random matrix with kZkF bounded a.s.. Then:
                                                               n
                                                         1 X                                  1
                                                   P (k √       {Zi − E[Zi ]}kF > Mn ) = o(n− 2 ) ,
                                                          n i=1

for any sequence Mn ↑ ∞ such that log(n) = o(Mn ).


Proof: Let Z (l,j) denote the (l, j) entry of Z. To establish the claim of the Lemma, then note that:
              n                                                                       n
        1 X                                              kp X (l,j)      (l,j)
  P (k √       {Zi − E[Zi ]}kF > Mn ) ≤ P (    max      |√      {Z  − E[Zi ]}| > Mn )
         n i=1                              1≤l≤k,1≤j≤p    n i=1 i
                                                                                             p
                                                                                           k X                n
                                                                                           X                kp X (l,j)       (l,j)
                                                                                       ≤               P (| √       {Zi − E[Zi ]}| > Mn ) . (15)
                                                                                                   j=1
                                                                                                              n i=1
                                                                                           l=1

        (l,j)              (l,j)
Since |Zi         − E[Zi           ]| ≤ K a.s. for some K > 0 and 1 ≤ l ≤ k, 1 ≤ j ≤ p, Bernstein’s inequality implies:
                                                       n
                                                 kp X (l,j)       (l,j)                   Mn
                                            P (| √       {Zi − E[Zi ]}| > Mn ) ≤ 2 exp{−      },                                              (16)
                                                   n i=1                                 2kpK

since Mn ↑ ∞. Results (15), (16) and log(n) = o(Mn ) then establish the Lemma.

                               2
Lemma A.2. Let ∆n ≡ I − Hn , σ̂R ≡ c0 Σn (β0 )c + 2c0 ∆n Σ0 c and Assumptions 2.1(i)-(iii) hold. Then:

                                                       1
  (i) P (k √1n           Xi i k > Mn ) = o(n− 2 ) for any sequence Mn ↑ ∞ with log(n) = o(Mn ).
                   P
                     i
                          Pk                                       1
  (ii) P (kHn−1 −            j=0    ∆jn ko > n−α ) = o(n− 2 ) for any α ∈ [0, k+1
                                                                               2 ).
                                                   1
 (iii) P (kβ̂ − β0 k > n−α ) = o(n− 2 ) for any α ∈ [0, 21 ).
                                                                       1
 (iv) P (|σ̂ 2 − σ̂R
                   2          2
                                         γ00 Xi i | > n−α ) = o(n− 2 ) for any α ∈ [0, 21 ).
                                   P
                     +        n      i



Proof: Since kXk is bounded a.s. by Assumption 2.1(ii), the first claim follows by Lemma A.1. For the second
claim, notice Lemma A.1 implies that for any Mn ↑ ∞ such that log(n) = o(Mn ) we must have:
                                                       Mn          1
                                           P (k∆n kF ≥ √ ) = o(n− 2 ) .                                                                       (17)
                                                        n
                                                 P∞
Moreover, notice that if k∆n kF < 1, then Hn−1 = j=0 ∆jn . Hence, we obtain:

                   k
                   X                                    X
  P (kHn−1 −             ∆jn ko > n−α ) ≤ P (k                 ∆jn ko > n−α and k∆n kF < 1) + P (k∆n kF ≥ 1)
                   j=0                                 j≥k+1
                                           X                                                           1      ξ k+1 (∆n )                1
                                   ≤ P(           ξ(∆jn ) > n−α and k∆n kF < 1) + o(n− 2 ) ≤ P (                          > n−α ) + o(n− 2 ) , (18)
                                                                                                              1 − ξ(∆n )
                                          j≥k+1



                                                                                 11
where ξ(∆jn ) is the largest eigenvalue of ∆jn and we have exploited k∆jn ko = ξ(∆jn ) and ξ(∆jn ) = ξ j (∆n ). for the
second and third inequalities. Moreover, since ξ(∆n ) = k∆n ko ≤ k∆n kF , result (17) implies that P (|ξ(∆n )| ≥ 1/2) =
     1
o(n− 2 ). Therefore, from (18) we are able to conclude that:
                      k
                                                                                      1                                              1
                      X
         P (kHn−1 −         ∆jn ko > n−α ) ≤ P (2ξ k+1 (∆n ) > n−α ) + o(n− 2 ) ≤ P (2k∆n kk+1
                                                                                           F   > n−α ) + o(n− 2 ) .                              (19)
                      j=0

                                                      1   α                                                   α              1
To conclude, exploit (19) and set Mn = n 2 − k+1 in (17) to obtain P (2k∆n kF > n− k+1 ) = o(n− 2 ).

    Next, note that Corollary III.2.6 in Bhatia (1997) implies |ξ(Hn−1 ) − 1| = |ξ(Hn−1 ) − ξ(I)| ≤ kHn−1 − IkF . By
                                                                             1
part (ii) of the Lemma, it follows that P (kHn−1 ko > 2) = o(n− 2 ). Hence, we obtain:

  P (kβ̂ − β0 k > n−α )
                                          n                                                            n                 1
                                     2X                                               1 X              n 2 −α          1
                            ≤ P (k         Xi i k > n−α ) + P (kHn−1 ko > 2) = P (k √       Xi i k >        ) + o(n− 2 ) . (20)
                                     n i=1                                             n i=1              2

The third claim of the Lemma is then established by (20), part (i) and α < 1/2.

    In order to establish the final claim of the Lemma, first observe that by direct calculation we obtain:
                                                          n
                                       α              1X                                                           α          1
    P (kΣn (β̂) − Σn (β0 )kF > n− 2 ) = P (k                Xi Xi0 {(Xi0 (β̂ − β0 ))2 − 2i Xi0 (β̂ − β0 )}kF > n− 2 ) = o(n− 2 )                (21)
                                                      n i=1

where the final result is implied by part (iii), (X, ) bounded a.s. by Assumption 2.1(ii) and α < 1. Similarly, by
Lemma A.1, for any sequence Mn ↑ ∞ such that log(n) = o(Mn ) we also have:
                                                                         Mn         1
                                                  P (kΣn (β0 ) − Σ0 kF > √ ) = o(n− 2 ) .                                                        (22)
                                                                          n
                                                                                                                                 α           1
Let K > 0 be such that kΣ0 ko < K and note that since (21)-(22) imply P (kΣn (β̂) − Σ0 ko > n− 2 ) = o(n− 2 ), it
                                                  1
follows that P (kΣn (β̂)ko > K) = o(n− 2 ). Hence, we conclude from part (ii) of the Lemma that:

  P (|c0 (Hn−1 − I)Σn (β̂)(Hn−1 − I)c| > n−α )
                                                                                                                                         1
                                                          ≤ P (Kkck2 kHn−1 − Ik2o > n−α ) + P (kΣn (β̂)ko > K) = o(n− 2 ) . (23)
                                                                         1
Similarly, exploiting again that P (kΣn (β̂)ko > K) = o(n− 2 ) and part (ii) of the Lemma we also obtain:
                                                                                                   1
                                          P (|c0 (Hn−1 − I − ∆n )Σn (β̂)c| > n−α ) = o(n− 2 ) .                                                  (24)

Moreover, since α < 1, exploiting (17), (21) and (22) we also conclude:

  P (|c0 ∆n (Σn (β̂) − Σ0 )c| > n−α ) ≤ P (kck2 k∆n ko kΣn (β̂) − Σ0 ko > n−α )
                                                                              α                                              α           1
                                                      ≤ P (kckk∆n kF > n− 2 ) + P (kckkΣn (β̂) − Σ0 kF > n− 2 ) = o(n− 2 ) . (25)

                                                                                                       M             1
                                                                            0
Since (X, ) is bounded, Lemma A.1 implies that P (k n1                          2
                                                                                                              = o(n− 2 ) for any Mn ↑ ∞ with
                                                                     P
                                                                        i (c Xi ) i Xi   − γ0 k >     √n )
                                                                                                        n

log(n) = o(Mn ). Hence, using manipulations as in (25) we can conclude that:
                                              n
                                          1X                                                        1
                                   P (k         {i (c0 Xi )2 Xi0 − γ00 }(β̂ − β0 )k > n−α ) = o(n− 2 ) .                                        (26)
                                          n i=1

Next, exploit parts (i) and (ii) of the Lemma and argue as in (25) to additionally conclude that:
                                      n                                                        n
                                   1X 0                                             1X                           1
            P (|γ00 (β̂ − β0 ) −         γ0 Xi i | > n−α ) ≤ P (kγ0 kkHn−1 − Iko k       Xi i k > n−α ) = o(n− 2 ) .                           (27)
                                   n i=1                                            n i=1

                                                                      12
Hence, by results (26), (27), X bounded a.s. and part (iii) of the Lemma we establish that:
                                                n
         0                0      2X 0
  P (|c Σn (β̂)c − c Σn (β0 )c +      γ Xi i | > n−α )
                                 n i=1 0
                                  n                     n                                          n
                              2X 0             2X 0                               1X 0                                              1
                     = P (|         γ0 Xi i −       (c Xi )2 i Xi0 (β̂ − β0 ) +       (c Xi )2 (Xi0 (β̂ − β0 ))2 | > n−α ) = o(n− 2 ) . (28)
                              n i=1            n i=1                              n i=1

To conclude, note that by direct manipulations we obtain that:

                                  σ̂ 2 = c0 (Hn−1 − I)Σn (β̂)(Hn−1 − I)c + c0 Σn (β̂)c + 2c0 (Hn−1 − I)Σn (β̂)c ,                                (29)

and hence the final claim of the Lemma follows from (23), (24), (25) and (28).

Lemma A.3. Let Assumptions 2.1(i)-(iii) hold and Ln be as in (9). Then for any α ∈ [0, 1):
                                                                                                              1
                                                        lim sup P (|Tn − Ln | > n−α ) = o(n− 2 ) .
                                                            n→∞


Proof: By a Taylor expansion we obtain for some σ̄ 2 a convex combination of σ̂ 2 and σ 2 that:
                                                        n                                                     n
                                1 X            (σ − σ̂) 0 −1       1 X
  Tn − Ln = c0 {Hn−1 − I − ∆n } √      Xi i +         c {Hn − I} √       Xi i
                               σ n i=1           σ̂σ                n i=1
                                                                          n                                         n
                                                               1 X 0             1               2X 0               3
                                                             +√       c Xi i {− 3 (σ̂ 2 − σ̂R
                                                                                             2
                                                                                               +       γ0 Xi i ) + 5 (σ̂ 2 − σ 2 )2 } . (30)
                                                                n i=1           2σ               n i=1             4σ̄

To study the right hand side of (30), first observe that Lemma A.2(i) and A.2(ii) imply that:
                                     n
         0                       1 X
  P (|c      {Hn−1   − I − ∆n }  √      Xi i | > n−α )
                                σ n i=1
                                                                                                                   n
                                                                                          1               1 X                               1
                                       ≤ P (kckkHn−1 − I − ∆n ko >                          2   ) + P (k √       Xi i k > log2 (n)) = o(n− 2 ) . (31)
                                                                                       α
                                                                                      n log (n)            n i=1

Moreover, by identical manipulations but exploiting Lemma A.2(i) and A.2(iv) we can similarly conclude:
                                                n                         n
                                            1  X
                                                    0                  2X 0                           1
                                      P (| 3 √                2    2
                                                   c Xi i {σ̂ − σ̂R +      γ Xi i }| > n−α ) = o(n− 2 ) .                                      (32)
                                          2σ n i=1                     n i=1 0

Next, notice that (X, ) bounded a.s. and Lemma A.1 further imply that:
                                                                                                          n
                                                              α                   1                    1X 0                α          1
                       P (|c0 (Σn (β0 ) − Σ0 )c| > n− 2 ) = o(n− 2 )                            P (|        γ Xi i | > n− 2 ) = o(n− 2 ) .      (33)
                                                                                                       n i=1 0

Therefore, we obtain from (29) together with (23) and (28) that since α < 1 we must have:
                                                                                            α             1
                                                             P (|σ̂ 2 − σ 2 | > n− 2 ) = o(n− 2 ) .                                              (34)
                                                    α             1
This implies that P (|σ̂ − σ| > n− 2 ) = o(n− 2 ) and since σ̄ is a convex combination of σ 2 and σ̂ 2 that P (σ̄ > ) =
     1
o(n− 2 ) for any  < σ. Hence, exploiting (34) and manipulations as in (31) we can conclude:
                                             n
                              (σ̂ 2 − σ 2 )2 X 0             −α                           5              1          1
                      P (|          5
                                      √        c Xi  i | > n   ) ≤ P ((σ̂ 2
                                                                             − σ 2 2
                                                                                  )  >       2   ) + o(n− 2 ) = o(n− 2 ) .                       (35)
                                  σ̄ n i=1                                              α
                                                                                       n log (n)
                                                                              1
Similarly, for  < σ we can exploit P (σ̂ > ) = o(n− 2 ) and Lemma A.2(i) to obtain:
                                            n
         (σ − σ̂) 0 −1       1 X                          |σ − σ̂|kck                  1              1
  P (|           c {Hn − I} √       Xi i | > n−α ) ≤ P (       2
                                                                      kHn−1 − Iko > α    2   ) + o(n− 2 )
           σ̂σ                n i=1                                               n log (n)
                                                                  2                               1              1          1
                                        ≤ P (|σ − σ̂| >               α   ) + P (kHn−1 − Iko > α         ) + o(n− 2 ) = o(n− 2 ) . (36)
                                                              kckn log(n)
                                                                      2                       n 2 log(n)


                                                                                       13
where the final result follows from Lemma A.2(ii), equation (34) and α < 1. The Lemma is then established due to
the decomposition in (30) and results (31), (32), (35) and (36).

Lemma A.4. Let {Ain }ni=1 be a triangular array of k × p matrices, {cn }ni=1 be a sequence of scalars with {Ain }ni=1
and {cn }ni=1 measurable functions of {Yi , Xi }ni=1 . Suppose Assumptions 2.1(i) and 2.2(i) hold and
                                                              n
                                                        1X
                                            lim sup           kAin kω
                                                                    F <∞                      c−1     α
                                                                                               n = o(n )          a.s.                          (37)
                                              n→∞       n i=1

for some α ∈ [0, ω−1                                         ω
                  2ω ). Then, for any g : R → R such that E[g (W )] < ∞, it follows that:
                                                 n
                                              1X                                             1
                                     P ∗ (k         Ain {g(Wi ) − E[g(Wi )]}kF > cn ) = o(n− 2 )                    a.s. .
                                              n i=1

                    (l,j)
Proof: Let Ain              denote the (l, j) entry of Ain and proceed as in equation (15) to conclude that:
                        n                                                        k    p            n
                    1X                                        XX          kp X (l,j)
           P ∗ (k         Ain {g(Wi ) − E[g(Wi )]}kF > cn ) ≤      P ∗ (|      A     {g(Wi ) − E[g(Wi )]}| > cn ) .                             (38)
                    n i=1                                      j=1
                                                                          n i=1 in
                                                                              l=1

Next, apply Markov’s inequality and the Marcinkiewicz and Rosenthal inequalities (Lemmas 1.4.13 and 1.5.9 in de la
Pena and Gine (1999)) to obtain for some constants C1 and C2 that:
                   n                                       √           n
  √             1 X (l,j)                                    n     1 X (l,j)
      nP ∗ (|         Ain {g(Wi ) − E[g(Wi )]}| > cn ) ≤ ω E ∗ [|         A       {g(Wi ) − E[g(Wi )]}|ω ]
                n i=1                                       cn     n i=1 in
                           √               n                                         √         n
                             nC1 ∗ 1 X (l,j)                               2 ω         nC2 1 X (l,j) 2                ω
                         ≤       E [(         (A in {g(W i ) − E[g(W i )]}) ) 2 ] ≤       ω (     (Ain ) Var(g(Wi ))) 2 , (39)
                            cωn       n 2
                                          i=1
                                                                                     cωn 2 n
                                                                                      n       i=1

where in the final result we have used (37) and ω ≥ 2. The claim of the Lemma then follows by (37), (38), (39) and
α ∈ [0, ω−1
         2ω ) by hypothesis.


Lemma A.5. Let (σ̂s∗ )2 ≡ c0 Hn−1 Σ∗n (β̂)Hn−1 c and {cn }ni=1 be measurable scalar-valued functions of {Yi , Xi }ni=1 . Let
Assumptions 2.1(i)-(ii), 2.2(i) hold and c−1     α                       ω−1
                                          n = o(n ) a.s. for some α ∈ [0, 2ω ). Then:


                                                1
   (i) P ∗ (kβ̂ ∗ − β̂k > cn ) = o(n− 2 ) almost surely.
                                                          1
  (ii) P ∗ (|(σ̂ ∗ )2 − (σ̂s∗ )2 | > c2n ) = o(n− 2 ) almost surely.
                                                    1
 (iii) P ∗ (|(σ̂s∗ )2 − σ 2 | > ) = o(n− 2 ) almost surely for any  > 0.

                      a.s.                                                                                 a.s.
Proof: Since β̂ → β, (Y, X) are bounded by Assumption 2.1(ii) and kHn−1 ko → 1, Lemma A.4 implies:
                                                                             n
                                                                         1X                                      1
                       P ∗ (kβ̂ ∗ − β̂k > cn ) ≤ P ∗ (kHn−1 ko k               Xi (Yi − Xi β̂)Wi k > cn ) = o(n− 2 )              a.s.          (40)
                                                                         n i=1


    For the second claim of the Lemma, proceed by standard manipulations to obtain the inequalities:

                 P ∗ (|(σ̂ ∗ )2 − (σ̂s∗ )2 | > c2n )
                                                 n                                        n
                                            1X                                2X
                        = P ∗ (|c0 Hn−1 {         Xi Xi0 (Xi0 (β̂ ∗ − β̂))2 −       Xi Xi0 ∗i Xi0 (β̂ ∗ − β̂)}Hn−1 c| > c2n )
                                            n i=1                             n i=1
                                                              n                                     n
                                                        1X                                     2X
                        ≤ P ∗ (kck2 kHn−1 k2o {k              Xi Xi0 (Xi0 (β̂ ∗ − β̂))2 ko + k       Xi Xi0 ∗i Xi0 (β̂ ∗ − β̂)ko } > c2n ) .   (41)
                                                        n i=1                                  n i=1


                                                                                 14
Since X is bounded a.s., we then obtain from part (i) of the Lemma that for some K > 0 we must have:

                                        n
                                1X
  P ∗ (kck2 kHn−1 k2o k               Xi Xi0 (Xi0 (β̂ ∗ − β̂))2 ko > c2n )
                                n i=1
                                                                                                                                             1
                                                                                          ≤ P ∗ (kck2 kHn−1 k2o Kkβ̂ ∗ − β̂k2 > c2n ) = o(n− 2 )    a.s. (42)

Let X (k) denote the k th coordinate of the vector X. Using k · ko ≤ k · kF , we can then conclude that:
                                                           n
                                                     2X
                    P ∗ (kck2 kHn−1 k2o k                  Xi Xi0 ∗i Xi0 (β̂ ∗ − β̂)ko > c2n )
                                                     n i=1
                                                                                                             n
                                                                                                        2d2x X (j) (k) ∗ 0 ∗
                                                    ≤ P ∗ (kck2 kHn−1 k2o {               max       |          X Xi i Xi (β̂ − β̂)|} > c2n )
                                                                                 1≤j≤dx ,1≤k≤dx          n i=1 i
                                                          dx X
                                                             dx                                    n
                                                          X                                   2d2x X (j) (k)
                                                    ≤                 P ∗ (kck2 kHn−1 k2o k          X Xi Xi ∗i kkβ̂ ∗ − β̂k > c2n ) .                     (43)
                                                          j=1 k=1
                                                                                               n i=1 i

Moreover, for any (j, k) we can then conclude from Lemma A.4 and part (i) of this Lemma that:

                                        n
    ∗                     2d2           X           (j)    (k)
  P (kck   2
               kHn−1 k2o k x                      Xi Xi Xi ∗i kkβ̂ ∗ − β̂k > c2n )
                                    n       i=1
                                                                    n
                                                               2d2x X (j) (k)                                                               1
                                                  ≤ P ∗ (k            Xi Xi Xi ∗i k > cn ) + P ∗ (kck2 kHn−1 k2o kβ̂ ∗ − β̂k > cn ) = o(n− 2 ) , (44)
                                                                n i=1

almost surely. The second claim of the Lemma then follows from (41)-(44).

                                                                  a.s.                 a.s.
    To conclude, exploit that kHn−1 ko → 1 and σ̂ 2 → σ together with Lemma A.4 to obtain:


  P ∗ (|(σ̂s∗ )2 − σ 2 | > ) ≤ P ∗ (|(σ̂s∗ )2 − σ̂ 2 | >  − |σ̂ 2 − σ 2 |)
                                                                       n
                                                                 1X                                        − |σ̂ 2 − σ 2 |          1
                                                    ≤ P ∗ (k           Xi Xi0 (Yi − Xi β̂)2 (Wi2 − 1)kF >                   ) = o(n− 2 )           a.s. ,   (45)
                                                                 n i=1                                    kck2 kHn−1 k2o

which establishes the third and final claim of the Lemma.

Lemma A.6. Let Assumptions 2.1(i)-(ii), 2.2(i), and for c ∈ Rdx define the following random variables:
                               √ 0
                         ∗        nc ∗
                        Ts,n ≡        (β̂ − β)         (σ̂s∗ )2 ≡ c0 Hn−1 Σ∗n (β̂)Hn−1 c .                                                                  (46)
                                 σ̂s∗
                                                                                   1
It then follows that P ∗ (|Tn∗ − Ts,n
                                  ∗
                                      | > n−α ) = o(n− 2 ) almost surely for any α ∈ [0, 2ω−3
                                                                                          2ω ).


                                                                                                                                        1
Proof: Let  < σ 2 and note that parts (ii) and (iii) of Lemma A.5 imply P ∗ (σ̂ ∗ σ̂s∗ < ) = o(n− 2 ) almost surely. For
any γ ∈ [0, ω−1
             2ω ), part (i) of Lemma A.5 then establishes that:
                                                                       √
                    ∗                                                n|σ̂ ∗ − σ̂s∗ |
                P       (|Tn∗   −    ∗
                                    Ts,n |    >n    −α
                                                          )≤P (   ∗
                                                                                     × kckkβ̂ ∗ − β̂k > n−α )
                                                                       σ̂ ∗ σ̂s∗
                                                                   √                                                1
                                                            ≤ P ∗ ( n|σ̂ ∗ − σ̂s∗ | > α−γ ) + P ∗ (kβ̂ ∗ − β̂k > γ      ) + P ∗ (σ̂ ∗ σ̂s∗ < )
                                                                                       n                          n kck
                                                                   √                                  1
                                                            = P ∗ ( n|σ̂ ∗ − σ̂s∗ | > α−γ ) + o(n− 2 )        a.s. .                                        (47)
                                                                                       n

Since for any α ∈ [0, 2ω−3                     ω−1
                       2ω ) we may pick γ ∈ [0, 2ω ) so that α − γ +
                                                                                                               1
                                                                                                               2   ∈ [0, ω−1
                                                                                                                          ω ), the claim of the Lemma then

follows from result (47) and part (ii) of Lemma A.5.



                                                                                              15
Lemma A.7. Let Assumptions 2.1(i)-(iii), 2.2(i) hold, ei ≡ (Yi − Xi0 β̂) and κ̂ ≡                       1           0 −1    3 3
                                                                                                            P
                                                                                                        n       i (c Hn Xi ) ei .   Then:

                                           κ    γ   2c0 Σ γ                                                     E[W 3 ]κ̂
                        E[Ln ] = −          √ − √1 + 3 √0 0                                    E ∗ [L∗n ] = −        √ .
                                        2σ 3 n σ n   σ n                                                        2σ̂ 3 n

Proof: We first derive an expression for E[Ln ]. Note that E[XX 0 ] = I and E[X] = 0 imply:
                                    n                          n                        n
                         1 X                    1X                  1 X                1
                 E[c0 ∆n √      Xi i ] = c0 E[       (I − Xi Xi0 ) √      Xi i ] = − √ E[(c0 X)X 0 X]                                    (48)
                        σ n i=1                 n i=1              σ n i=1            σ n

due to the i.i.d. assumption. Similarly, exploiting the i.i.d. assumption and E[(c0 X)] = E[∆n ] = 0 yields:
                            n                                          n
                       1   X
                                 0                                1   X
               E[       √      (c  Xi ) i (σ̂ 2
                                               R − σ 2
                                                       )] = E[     √      (c0 Xi )i {c0 (Σn (β0 ) − Σ0 )c + 2c0 ∆n Σ0 c}]
                    2σ 3 n i=1                                 2σ 3 n i=1
                                                                  1
                                                           =       √ {E[(c0 X)3 3 ] − 2E[(c0 X)2 X 0 ]Σ0 c} .                             (49)
                                                               2σ 3 n

The expression for E[Ln ] can then be obtained from (48), (49) and by analogous arguments concluding:
                                                       n                   n
                                                  1   X
                                                            0           2X 0               c0 Σ0 γ0
                                          E[       √      (c  Xi ) i ×      γ  Xi  i ] =     √ .                                          (50)
                                               2σ 3 n i=1               n i=1 0            σ3 n


    In order to compute E ∗ [L∗n ], observe that W ⊥ (Y, X) and E[W 2 ] = 1 implies that:
                                                 n          n
                                   1 ∗ c0 Hn−1 X        ∗1
                                                           X                                          E[W 3 ]κ̂
                 E ∗ [L∗n ] = −       3
                                        E [ √       X 
                                                     i i       c0 Hn−1 Xi Xi0 Hn−1 ce2i (Wi2 − 1)] = − 3 √ ,                                (51)
                                  2σ̂         n i=1      n i=1                                        2σ̂ n

which establishes the second claim of the Lemma.

Lemma A.8. Under Assumptions 2.1(i)-(iii) and 2.2(i), the second moments of Ln and L∗n satisfy:

                                   E[L2n ] = 1 + O(n−1 )                 E ∗ [(L∗n )2 ] = 1 + Oa.s. (n−1 ) .


Proof: To calculate E[L2n ], first note that E[XX 0 ] = I, E[X] = 0 and direct calculations yield:

                 n                      n                      n                                        n
            1 X                      c0 X                 1 X                  1                        X
  E[(c0 ∆n √        Xi i )2 ] = E[(       (I − Xi Xi0 ) √        Xi i )2 ] = 2 2 E[(c0 (I − Xi Xi0 )(   Xk k ))2 ]
             nσ i=1                  n i=1                 nσ i=1             σ n
                                                                                                                       k=1
                                                                                n                                n
                                                   (n − 1)                      X                                X
                                               +           E[{c0 (I − Xi Xi )         Xk k }{c0 (I − Xj Xj0 )         Xk k }] = O(n−1 ) . (52)
                                                    σ 2 n2
                                                                                k=1                              k=1

Similarly, exploiting the i.i.d. assumption together with E[X] = 0 and E[I − XX 0 ] = 0 we obtain:

            n                       n                      n                 n                  n
       1 X 0              0    1 X                 1     X
                                                               0          0
                                                                            X
                                                                                         0
                                                                                              X
  E[( √        c Xi i )(c ∆n √        Xi i )] = 2 2 E[(     c Xi i )(c       (I − Xi Xi ))(     Xi i )]
        nσ i=1                  nσ i=1           n σ      i=1               i=1                i=1
                                                                                     1
                                                                                =        E[(c0 X)(c0 X − c0 XX 0 X)] = O(n−1 ) . (53)
                                                                                    nσ 2

Exploiting identical arguments to (52) on the squares of the remaining terms of Ln and the Cauchy-Schwarz inequal-
ity and arguments identical to those in (53) to address cross terms arising from expanding the square, it is then
straightforward to establish that:
                                               n
                                  1 X 0                           c0 E[XX 0 2 ]c
                    E[L2n ] = E[( √      c Xi i )2 ] + O(n−1 ) =                 + O(n−1 ) = 1 + O(n−1 ) .                                 (54)
                                 σ n i=1                               σ2



                                                                       16
    For notational simplicity, let ain ≡ c0 Hn−1 Xi and set ei ≡ (Yi − Xi0 β̂). To compute E ∗ [(L∗n )2 ], first note that the
i.i.d. assumption together with E ∗ [(∗i )4 ] = e4i E[Wi4 ], E ∗ [(∗i )2 ] = e2i and E ∗ [∗i ] = 0 imply that:
                                    n               n                                   n
                       1      ∗
                                  X
                                             ∗ 2
                                                   X
                                                               ∗ 2                1 X 4 4
                            E   [(     ain  i ) (     a2
                                                        in {( i ) − e 2
                                                                       i })] =             ain ei (E[W 4 ] − 1) = Oa.s (n−1 ) .                            (55)
                    σ̂ 4 n2        i=1             i=1
                                                                               σ̂ 4 n2
                                                                                       i=1

Next, also note that by direct calculations, {Wi }ni=1 being i.i.d. and E ∗ [(∗i )3 ] = e3i E[W 3 ] we may establish:
                      n              n
        1       ∗
                    X
                              ∗ 2
                                   X
              E   [(     a  
                          in i )  (     a2in {(∗i )2 − e2i })2 ]
     4σ̂ 6 n3        i=1            i=1
                            n                   n                          n X                             n
                     1      X
                                 ∗ 2      ∗ 2
                                                X
                                                          ∗ 2
                                                                          X
                                                                                 ∗          ∗         ∗
                                                                                                           X
              =           {    E  [ain ( i ) (   a2
                                                   kn {( k ) − e2 2
                                                                 k }) ] +      E   [(a in  i )(ajn  j )(   a2kn {(∗k )2 − e2k })2 ]}
                  4σ̂ 6 n3 i=1                                            i=1
                                                        k=1                                    j6=i                              k=1
                            n X
                              n                                                            n X
                     1      X                                                              X
              =           {               a2in a4kn E ∗ [(∗i )2 {(∗k )2 − e2k }2 ] + 2              a3in e3i a3jn e3j (E[W 3 ])2 } .                     (56)
                  4σ̂ 6 n3 i=1                                                             i=1 j6=i
                                 k=1

                                                                  1
                                                                           a2in e2i = σ̂ 2 and exploiting (55) and (56):
                                                                      P
Therefore, expanding the square, noting that                      n    i

                                                                    n
                                     ∗                   1 ∗ X
                                 E       [(L∗n )2 ]   =       E [(     ain ∗i )2 ] + Oa.s. (n−1 ) = 1 + Oa.s. (n−1 ) ,                                    (57)
                                                        nσ̂ 2      i=1

which establishes the second and final claim of the Lemma.

Lemma A.9. Let Assumptions 2.1(i)-(iii), 2.2(i) hold ei ≡ (Yi − Xi0 β̂) and κ̂ ≡                                       1           0 −1    3 3
                                                                                                                           P
                                                                                                                       n       i (c Hn Xi ) ei .   Then:

                             7κ    3γ   12c0 Σ0 γ0                                                                     7E[W 3 ]κ̂
            E[L3n ] = −      3
                               √ − √1 +      √     + O(n−1 )                                    E ∗ [(L∗n )3 ] = −           √ + Oa.s. (n−1 ) .            (58)
                           2σ n σ n      σ3 n                                                                           2σ̂ 3 n

Proof: The calculations are cumbersome and for brevity we provide only the essential steps. Define:
                                                        n                         n                                        n
                                      1 X             1  X                             2X 0
                          Γ n ≡ c0 ∆n √      Xi i − 3 √     (c0 Xi )i {(σ̂R
                                                                            2
                                                                              − σ2 ) −      γ Xi i } .                                                    (59)
                                     σ n i=1        2σ n i=1                           n i=1 0

                                                                                                        3
                    1
                         c0 i Xi i +Γn . Under Assumption 2.1(ii), it can be shown that E[Γ3n ] = O(n− 2 ) and similarly
                            P
Notice that Ln = σ√   n
                                    1
that E[( √1n i c0 Xi i )3 ] = O(n− 2 ). Therefore, by direct calculation and Holder’s inequality:
            P

                              n                           n                              n
                          1 X 0                       1 X 0                          1 X 0
          E[L3n ]   = E[( √                 3
                                 (c Xi )i ) ] + 3E[( √                 2
                                                             (c Xi )i ) Γn ] + 3E[( √      (c Xi )i )Γ2n ] + E[Γ3n ]
                         σ n i=1                     σ n i=1                        σ n i=1
                                     n                                        n
                          1 X 0                        1 X 0
                    = E[( √      (c Xi )i )3 ] + 3E[( √      (c Xi )i )2 Γn ] + O(n−1 ) .                                                                (60)
                         σ n i=1                      σ n i=1

Hence, we can establish the first claim of the Lemma by analyzing the remaining terms in (60). Note that
                                                                 n
                                                          1 X 0                    1
                                                      E[( √      (c Xi )i )3 ] = 3 √ E[(c0 X)3 3 ] ,                                                     (61)
                                                         σ n i=1                 σ n

by the i.i.d. assumption and E[X] = 0. Similarly, by direct calculation we can also obtain the expression:
                       n                      n
                   1 X 0               c0 ∆n X
               E[( √      (c Xi )i )2 √         Xi i ]
                  σ n i=1                 nσ i=1
                                                       n                n                           n                  n
                                           1           X                X            X              X                  X
                                 =              5   E[{ (c0 Xi )2 2i +   (c0 Xi )i   (c0 Xj )j }   c0 (I − Xk Xk0 )   Xl l ]
                                         σ3 n   2
                                                        i=1                 i=1              j6=i               k=1                      l=1
                                    c0 Σ0 c                   2                             3
                                 = − 3 √ E[(c0 X)(X 0 X)] − 3 √ E[(c0 X)(γ00 X)2 ] + O(n− 2 ) .                                                          (62)
                                    σ n                     σ n


                                                                                  17
                                                                  1
                                                                                                         c0 Xi i )2 Γn ] and obtain:
                                                                                                     P
By analogous arguments we can compute the remaining terms in E[( σ√ n                                i

                                             n
                            1        1 X 0                                       3c0 Σ0 c                    3

                              5
                                E[( √       (c Xi )i )3 c0 {Σn (β0 ) − Σ0 }c] =     √ E[(c0 X)3 3 ] + O(n− 2 )                                  (63)
                           2σ         n i=1                                      2σ 5 n
                                                        n
                                       1       1 X 0              3 0                 3c0 Σ0 c 0          3
                                          E[( √       (c Xi ) i ) {c ∆ n Σ 0 c}] = −     √ γ Σ0 c + O(n− 2 )                                     (64)
                                       σ5       n i=1                                 σ5 n 0
                                                    n                       n
                                  1        1 X 0              3 1
                                                                   X
                                                                         0             3c0 Σ0 c 0           3

                                    5
                                      E[( √       (c Xi ) i ) {       γ 0 Xi  i }] =   5
                                                                                           √ c Σ0 γ0 + O(n− 2 ) .                                 (65)
                                  σ         n i=1                n i=1                 σ n

The first claim of the Lemma then follows by combining the results from (60)-(65).

    Letting ain = c0 Hn−1 Xi and employing Assumption 2.1(ii), it can then be shown that:
                                                        n
                                            ∗   1 X                 1                                    3
                                          E [( √       ain ∗i )3 ( 3 {(σ̂s∗ )2 − σ̂ 2 })2 ] = Oa.s. (n− 2 )                                      (66)
                                                 n i=1             2σ̂
                                                        n
                                                  1 X                 1                                    3
                                          E ∗ [( √       ain ∗i )3 ( 3 {(σ̂s∗ )2 − σ̂ 2 })3 ] = Oa.s. (n− 2 ) .                                  (67)
                                                   n i=1             2σ̂

Therefore, expanding the cube and exploiting that W ⊥ (Y, X) and E ∗ [(∗i )k ] = E[W k ]eki , it follows that:
                                                    n
                                        1 X                 1   3((σ̂s∗ )2 − σ̂ 2 ) 3((σ̂s∗ )2 − σ̂ 2 )2   ((σ̂s∗ )2 − σ̂ 2 )3
               E ∗ [(L∗n )3 ] = E ∗ [( √       ain ∗i )3 { 3 −            5
                                                                                   +            7
                                                                                                         −                     }]
                                         n i=1             σ̂          2σ̂                  4σ̂                    8σ̂ 9
                                                        n                                 n
                                     E[Wi3 ]   1X 3 3           3 ∗ 1 X                                                     3
                                 =       √   ×       ain e i −       E [( √       ain ∗i )3 {(σ̂s∗ )2 − σ̂ 2 }] + Oa.s (n− 2 ) .                 (68)
                                     σ̂ 3 n    n i=1           2σ̂ 5        n i=1

Moreover, also note that by analogous arguments and direct calculations we further obtain:
                   n                            n
              1 X                  3 X 2
      E ∗ [( √       ain ∗i )3 { 5       a {(∗ )2 − e2i }}]
               n i=1             2σ̂ n i=1 in i
                                  n                                                   n                               n
                 3             1X 5 ∗ ∗ 3 ∗ 2                           9       ∗
                                                                                    X
                                                                                                ∗
                                                                                                    X
                                                                                                               ∗ 2
                                                                                                                     X
          =            3   ×         ain E [(i ) {(i ) − e2i }] +         5 E   [{     ain ( i )      a2
                                                                                                            (
                                                                                                          jn j  )  }     a2kn {(∗k )2 − e2i }]
              2σ̂ 5 n 2        n i=1                                2σ̂ 5 n 2        i=1            j6=i             k=1
                                   n                        n
                  9    1 X 2 2 E[W 3 ] X 3 3                  3
          =        √ ×     a e ×           ain ei + Oa.s. (n− 2 ) .                                                                               (69)
              2σ̂ 5 n n i=1 in i n     i=1

The second claim of the Lemma is then established by (68) and (69).

Proof of Theorem 2.1: The first claim of the Theorem is an immediate consequence of Lemma A.3. For the second
                                                                                                               1
                                                                 ∗
claim, note that in lieu of Lemma A.6, it suffices to show that Tn,s = L∗n + op∗ (n− 2 ) a.s.. For notational simplicity,
let ain = c0 Hn−1 Xi (Yi − Xi0 β̂) and apply Markov’s inequality to conclude that:

                                                    n
                            C            1X 2                   C
  P ∗ (|(σ̂s∗ )2 − σ̂ 2 | > √ ) = P ∗ (|       ain (Wi2 − 1)| > √ )
                             n           n i=1                   n
                                                                                      n                               n
                                                                            n ∗ 1X 2                           1 X 4
                                                                        ≤       E [(       ain (Wi2 − 1))2 ] = 2     a E[(Wi2 − 1)2 ] . (70)
                                                                            C 2      n i=1                    C n i=1 in

                                                                                              a.s.
                                                            1
                                                                        a4in E[(Wi2 − 1)2 ] → E[(c0 X)4 4i ]E[(W 2 − 1)2 ] < ∞, and therefore
                                                                P
However, under our moment assumptions,                      n       i
                                                                1
from (70) it follows that (σ̂s∗ )2 = σ̂ 2 + Op∗ (n− 2 ) almost surely. The second claim of the Lemma then follows from a
second order Taylor expansion.

Proof of Theorem 2.2: Follows immediately from Lemmas A.7, A.8, A.9 and direct calculation.


                                                                                 18
                                                      APPENDIX B - Proofs of Theorem 2.3

Lemma B.1. Let Assumption 2.1(i)-(iv) hold and Ln be as in (9) with c 6= 0. Then, uniformly in z ∈ R:
                                                              φ(z)κ              φ(z)                                      1
                      P (Ln ≤ z) = Φ(z) +                       3
                                                                  √ (2z 2 + 1) − 3 √ (c0 Σ0 γ0 (z 2 + 1) − γ1 σ 2 ) + o(n− 2 ) .
                                                             6σ n               σ n

Proof: Letting Z ≡ (X 0 , vech(XX 0 )0 , vech(XX 0 2 )0 )0 , it is clear that Ln is a smooth functional of                                        1
                                                                                                                                                        P
                                                                                                                                                    n    i   Zi and that
Z satisfies Cramer’s condition by Assumption 2.1(iv). The claim of the Lemma then follows from Theorem 2.2 in
Hall (1992) and Theorem 2.2.

Lemma B.2. Let {ain }ni=1 be a triangular array of measurable scalar valued functions of {Yi , Xi }ni=1 and define
                                                                        P −1
Vin ≡ (ain Wi , a2in (Wi2 − 1))0 , Ωn ≡ n1 i E ∗ [Vin Vin
                                                        0
                                                          ] and Sn ≡ √1n i Ωn 2 Vin . Suppose Assumptions 2.2(i)-(ii)
                                          P
                          a.s.
hold and (i) Ωn → Ω with Ω full rank, (ii) lim supn→∞ max1≤i≤n |ain | < ∞ a.s. and (iii) For Kn () ≡ #{i :
min{|ain |, a2in } ≥ }, there a.s. exists an 0 such that Kn (0 )/ log(n) ↑ ∞. Then:
                                                                   1 Z
                                                                                                                          1
                                                                   X
                                             P ∗ (Sn ∈ B) =                     dPj (−Φ : {Xk∗ (Sn )}) + o(n− 2 )                  a.s.
                                                                   j=0     B

uniformly over all Borel sets B with Φ((∂B) ) ≤ C for some constant C, (∂B) the  enlargement of ∂B, Xk∗ (Sn )
the k th cumulant of Sn under P ∗ and Pj the Cramer-Edgeworth measures.


Proof: We proceed by verifying the conditions of Theorem 3.4 in Skovgaard (1986). For t ∈ R2 , define:
                                                                      1                      1
                                                     ρn (t) ≡            |X ∗ (t0 Sn )| =        |E ∗ [(t0 Sn )3 ]| ,                                               (71)
                                                                   3!ktk3 3               3!ktk3
since E[W ] = 0, E[W 2 ] = 1 and W ⊥ (Y, X). Hence, by Cauchy-Schwartz and convexity we obtain:
                                                                       1
                           n                                       −       n
                  1        X
                                  ∗      0    −1                 kΩn 2 k3o X
  ρn (t) ≤    3                  E [|t       Ωn 2 Vin |3 ]   ≤        3               E ∗ [kVin k3 ]
             n 2 ktk3      i=1
                                                                    n2          i=1
                                                                                                        1
                                                                                                  −       n
                                                                                               4kΩn 2 k3o X
                                                                                           ≤        3             {E ∗ [|ain |3 |Wi |3 ] + E ∗ [a6in |Wi2 − 1|3 ]} . (72)
                                                                                                  n2        i=1

                   a.s.                                                                              − 12    a.s.         1
Note that Ωn → Ω with Ω full rank by hypothesis, implies kΩn ko → kΩ− 2 ko < ∞. Moreover, since {ain }ni=1 is
not random with respect to P ∗ , we obtain from condition (ii) and result (72) that almost surely:
                                 √                                         −1
          lim sup{ sup               nρn (t)} ≤ lim sup{4kΩn 2 k3o (E[|W |3 ] + E[|W 2 − 1|3 ]) × max {|ain |3 + a6in }} < ∞ .                                      (73)
             n→∞          t∈R2                         n→∞                                                                     1≤i≤n

Therefore, we conclude that almost surely there exists a sequence {rn } satisfying the following:
                                                                                 1                                    √
                                                         sup ρn (t) ≤                                        rn          n,                                        (74)
                                                         t∈R2                   rn
which verifies conditions (I) and (II) of Theorem 3.4 in Skovgaard (1986).

    Next, let ξn∗ (t) ≡ E ∗ [exp(it0 Sn )]. We aim to show that almost surely there exists a δ > 0 such that:
                                                                                           d4            th
                                                  lim sup{            sup             |      4
                                                                                               log(ξn∗ (     ))| × rn2 } < ∞ .                                      (75)
                                                    n→∞          0<h<δrn ,t∈R2            dh             ktk

                          ∗                      −1       √
Towards this end, define ξin (t) ≡ E ∗ [exp(it0 Ωn 2 Vin / n)]. By Corollary 8.2 in Bhattacharya and Rao (1976),
{ain }ni=1 being nonrandom with respect to P ∗ and direct calculation it then follows that:
                                                                                             −1
                    ∗                    ktk2 ∗   −1             ktk2 kΩn 2 k2o
                  |ξin (t) − 1| ≤            E [kΩn 2 Vin k2 ] ≤                E[W 2 + (W 2 − 1)2 ] × max (a2in + a4in ) .                                         (76)
                                          2n                           2n                             1≤i≤n


                                                                                             19
                          −1         a.s.         1                            √
Condition (ii), kΩn 2 ko → kΩ− 2 ko < ∞ and rn                                    n then imply that almost surely there is a δ > 0 with:
                                                                                                          −1
                                  ∗                   δE[W 2 + (W 2 − 1)2 ]           r2 kΩn 2 k2o                         1
       lim sup{ sup             |ξin (t)      − 1|} ≤                       × lim sup{ n           { max (a2in + a4in )}} < .                              (77)
        n→∞        ktk≤δrn                                     2                n→∞        n        1≤i≤n                  2

Since ξn∗ (t) =            ∗
                  Q
                        i ξin (t)   by the i.i.d. assumption and W ⊥ (Y, X) we obtain by direct calculation:
                                                                             n
                         d4     ∗ th           2                          2
                                                                            X     d4     ∗    th
   lim sup{  sup       | 4 log(ξn (     ))| × rn } ≤ lim sup{  sup       rn     | 4 log(ξin (     ))|}
     n→∞ 0<h<δrn ,t∈R2  dh          ktk                n→∞ 0<h<δrn ,t∈R2
                                                                            i=1
                                                                                 dh           ktk
                                                                         n X                                               n               −1
                                                                         X
                                                                                              ∗
                                                                                                                           X              Ωn 2 Vin 4
                                         ≤ lim sup{ sup rn2                          |Dλ log(ξin (t))|} ≤ lim sup{16rn2          E ∗ [k     √     k ]} , (78)
                                               n→∞        ktk≤δrn        i=1 |λ|=4
                                                                                                           n→∞
                                                                                                                           i=1
                                                                                                                                              n

                                                                                                        ∗                                                     1
where the final inequality holds by Lemma 9.4 in Bhattacharya and Rao (1976) and result (77) implying |ξin (t)−1| <                                           2

for all ktk ≤ δrn and all 1 ≤ i ≤ n for n large enough. Moreover,
                                              1                                            1
                         n         −                            −        n
                         X        Ωn 2 Vin 4
                                    ∗                    rn2 2kΩn 2 k4o X 4
       lim sup{rn2           E [k   √     k ]} ≤ lim sup{ ×                 {ain E[W 4 ] + a8in E[(W 2 − 1)4 ]}} < ∞                                       (79)
         n→∞
                         i=1
                                      n            n→∞   n      n       i=1

almost surely, by condition (i), (ii) and (74). It follows from (78) and (79) that (75) holds almost surely, which
verifies condition (IV) of Theorem 3.4 in Skovgaard (1986).

    To conclude, we aim to show that almost surely for any δ > 0 it follows that:

                                                                  lim sup{rn6 × sup |ξn∗ (t)|} < ∞ .                                                       (80)
                                                                   n→∞              δrn ≤ktk

Let ξU denote the characteristic function of U ≡ (W, W 2 − 1)0 , η() ≡ supktk≥ |ξU (t)| and define:
                                                                   !
                                                       ain 0
                                               Ain ≡                  .                                                                                    (81)
                                                         0 a2in

        −1
Since Ωn 2 , Ain are not random with respect to P ∗ and W ⊥ (Y, X) we then obtain by direct calculation:
                                               n                               n                −1                  −1    √
                                               Y                               Y     Ain Ωn 2                        2
         sup      |ξn∗ (t)|     =       sup             ∗
                                                      |ξin (t)|   = sup         |ξU ( √       t)| ≤ {η()}#{i:kAin Ωn tk≥ n              ∀ktk≥δrn }
                                                                                                                                                           (82)
       δrn ≤ktk                     δrn ≤ktk i=1                   δrn ≤ktk i=1          n

                                                                                         −1                −1
for any  > 0. Moreover, since the smallest eigenvalue of Ωn 2 equals kΩn ko 2 , we also have:
                                                                                                          √       1
                                              −1             √                                    2       nkΩn ko2
                            #{i :       kAin Ωn 2 tk      ≥  n ∀ktk ≥ δrn } ≥ #{i : min{|ain |, ain } ≥            }.                                     (83)
                                                                                                            δrn
                    1
                         a.s.           1                            √                                              √       1
Thus, as kΩn ko2 → kΩko2 < ∞ and rn                                     n we may almost surely pick ∗ such that ∗ nkΩn ko2 /δrn < 0 for
n sufficiently large. In addition, by Assumption 2.2(ii), η(∗ ) < 1; see page 207 in Bhattacharya and Rao (1976).
Hence, by result (82) and condition (iii) we conclude that almost surely:

                                               lim sup{rn6 × sup |ξn∗ (t)|} ≤ lim sup rn6 η(∗ )Kn (0 ) = 0 ,                                             (84)
                                                  n→∞               δrn ≤ktk                   n→∞

verifying Condition (III”) of Theorem 3.4 in Skovgaard (1986). The claim of the Lemma therefore follows by direct
application of Theorem 3.4 in Skovgaard (1986).

                                                                                 ∗
                                                                                                                                  √
Lemma B.3. Suppose Assumptions 2.1(i)-(iv) and 2.2(i)-(ii) hold and let c 6= 0, Ts,n ≡                                                nc0 (β̂ ∗ − β̂)/σ̂s∗ where
(σ̂s∗ )2 ≡ c0 Hn−1 Σ∗n (β̂)Hn−1 c. It then follows that almost surely, uniformly in z ∈ R:
                                                                                   φ(z)κ̂E[W 3 ]                   1
                                               P ∗ (Ts,n
                                                     ∗
                                                         ≤ z) = Φ(z) +                  3
                                                                                          √      (2z 2 + 1) + o(n− 2 ) .                                   (85)
                                                                                     6σ̂ n


                                                                                      20
Proof: We proceed by verifying the conditions of Theorem 3.2 in Skovgaard (1981). First, define:

                                     ain ≡ c0 Hn−1 Xi (Yi − Xi β̂)                           ai ≡ c0 Xi (Yi − Xi β0 ) .                    (86)

           a.s.                         a.s.
Since β̂ → β0 , kHn−1 − Iko → 0 and (X, ) is bounded a.s. by Assumption 2.1(ii), we obtain:


  lim sup{ max |ain − ai |}
    n→∞          1≤i≤n

                             ≤ lim sup{kckkHn−1 − Iko max kXi i k} + lim sup{kckkHn−1 ko kβ̂ − β0 k max kXi k2 } = 0 . (87)
                                  n→∞                             1≤i≤n                 n→∞                               1≤i≤n


Let Vin ≡ (ain Wi , a2in (Wi2 − 1))0 and Vi ≡ (ai Wi , a2i (Wi2 − 1))0 . By result (87), it then follows that:
                                                                     n
                                                                  1X ∗           0 a.s.
                                                         Ωn ≡           E [Vin Vin ] → E[V V 0 ] .                                         (88)
                                                                  n i=1

Assumption 2.2(ii) rules out Rademacher weights, which are the only ones satisfying E[W ] = 0 and P (W 2 = 1) = 1.
By Assumption 2.1(iii), W ⊥ (Y, X), c 6= 0 and W not being Rademacher, it is then possible to show E[V V 0 ] is full
rank. Next, pick a δ0 such that:
                                                        P (min{|(c0 X)|, (c0 X)2 2 } ≥ δ0 ) > 0 ,                                        (89)

which is possible since E[(c0 X)2 2 ] > 0 by Assumption 2.1(iii) and c 6= 0. By result (87), then:
                                    n                                                    n
                                 1X                          δ0           1X
                    lim inf            1{min{|ain |, a2in } ≥ } ≥ lim inf      1{min{|ai |, a2i } ≥ δ0 } > 0                      a.s. .   (90)
                       n→∞       n i=1                       2     n→∞ n
                                                                           i=1

                                   −1
                       √1
                             P
Defining Sn ≡            n    i   Ωn 2 Vin , (88), (87) with Assumption 2.1(ii) and (90) verify conditions(i)-(iii) of Lemma B.2
respectively. Therefore, we can conclude that almost surely:
                                                                  1 Z
                                                                                                             1
                                                                  X
                                               P ∗ (Sn ∈ B) =                 dPj (−Φ : {Xk∗ (Sn )}) + o(n− 2 )                            (91)
                                                                  j=0     B


uniformly over all Borel sets B with Φ((∂B) ) ≤ C for some constant C. This verifies condition (3.1) of Theorem
3.2 in Skovgaard (1981).

       Next, let t(i) denote the ith coordinate of t ∈ R2 and define the functions gn , fn : R2 → R by:
                                                              1                                  t(2)        1
                                               fn (t) ≡ gn (Ωn2 t)             gn (t) ≡ t(1) × ( √ + σ̂n2 )− 2 .                           (92)
                                                                                                   n
                                       ∗
Note that by construction, fn (Sn ) = Ts,n , fn (0) = 0 and kDfn (0)k = 1. Further, define the set:

                                                            Γn ≡ {t ∈ R2 : ktk ≤ log(n)} .                                                 (93)

                                                                                 √                       a.s.
The functions gn are differentiable everywhere except at t ∈ R2 with t(2) = −σ̂n2 n. However, since σ̂n2 → σ 2 and
   1
          a.s.     1
kΩn2 ko → kΩ 2 ko we obtain that almost surely for n sufficiently large, fn is differentiable on Γn . Moreover, since a.s.
                      −1           √
for n large enough kΩn 2 ko log(n)/ n ≤ σ̂n2 /2 we obtain by direct calculation:
                                                                                                      5           1                 7
                 √                                  √    1             3  2 2 15kΩn2 ko log(n) 2 2
         lim sup{ n sup sup |Dλ fn (t)|} ≤ lim sup{4 nkΩn2 k3F × max{    × 5,          3      × 7}=0                                       (94)
           n→∞      t∈Γn |λ|=3               n→∞                      4n σ̂n       8n 2        σ̂n

almost surely; which verifies condition (3.11) of Theorem 3.2 in Skovgaard (1981). Similarly,

                          √                        √    1            1               √    1                                  1
                   lim sup nk∇2 fn (0)k2F = lim sup nkΩn2 ∇2 gn (0)Ωn2 k2F ≤ lim sup{ nkΩn2 k2F ×                                 }=0      (95)
                    n→∞                                  n→∞                                     n→∞                       2nσ̂n6

                                                                                 21
almost surely, verifying condition (3.12) of Theorem 3.2 in Skovgaard (1981). Therefore, we conclude from (91), (94),
(95), Theorem 3.2 and Remark 3.4 in Skovgaard (1981) that an Edgeworth expansion for P ∗ (Ts,n
                                                                                           ∗
                                                                                               ∈ B) holds almost
surely for all sets B such that Φ((∂B) ) = O() (which includes all sets of the form (−∞, z])). In particular, (85)
holds by Theorem 3.2 in Skovgaard (1981) and Theorem 2.2.

Proof of Theorem 2.3: The first claim of the Theorem follows from Lemma B.1, Lemma A.3 and Lemma 5(a) in
Andrews (2002) while the second claim follows by Lemma B.3, Lemma A.6 and Lemma 5(a) in Andrews (2002).




References
Andrews, D. W. K. (2002). Higher-order improvements of a computationally attractive k-step bootstrap for
  extremum estimators. Econometrica, 70 119–162.

Bhatia, R. (1997). Matrix Analysis. Springer, New York.

Bhattacharya, R. N. and Ghosh, J. K. (1978). On the validity of the formal edgeworth expansion. The Annals
  of Statistics, 6 434–451.

Bhattacharya, R. N. and Rao, R. R. (1976). Normal Approximation and Asymptotic Expansions. John Wiley
  & Sons, New York.

Cameron, A. C., Gelbach, J. B. and Miller, D. L. (2008). Bootstrap-based improvements for inference with
  clustered errors. Review of Economics and Statistics, 90 414–427.

Cavaliere, G. and Taylor, A. M. R. (2008). Bootstrap unit root tests for time series with nonstationary volatility.
  Econometric Theory, 24 43–71.

Davidson, R. and Flachaire, E. (2008). The wild bootstrap, tamed at last. Journal of Econometrics, 146
  162–169.

Davidson, R. and MacKinnon, J. G. (2010). Wild bootstrap tests for iv regression. Journal of Business and
  Economic Statistics, 28 128–144.

de la Pena, V. H. and Gine, E. (1999). Decoupling: From Dependence to Independence. Springer-Verlag, New
  York.

Freedman, D. A. (1981). Bootstrapping regression models. The Annals of Statistics, 9 1218–1228.

Gonçalves, S. and Meddahi, N. (2009). Bootstrapping realized volatility. Econometrica, 77 283–306.

Hall, P. (1992). The Bootstrap and Edgeworth Expansion. Springer-Verlag, New York.

Horowitz, J. L. (1997). Bootstrap methods in econometrics: Theory and numerical performance. In Advances in
  Economics and Econometrics: Theory and Applications, Seventh World Congress (D. M. Kreps and K. F. Wallis,
  eds.), vol. 3. Cambridge University Press.



                                                         22
Horowitz, J. L. (2001). The bootstrap. In Handbook of Econometrics (J. J. Heckman and E. Leamer, eds.), vol. 5,
  chap. 52. Elsevier.

Liu, R. Y. (1988). Bootstrap procedures under some non-i.i.d. models. The Annals of Statistics, 16 1696–1708.

Liu, R. Y. and Singh, K. (1987). On a partial correction by the bootstrap. The Annals of Statistics, 15 1713–1718.

Mammen, E. (1993). Bootstrap and wild bootstrap for high dimensional linear models. The Annals of Statistics,
  21 255–285.

Skovgaard, I. M. (1981). Transformation of an edgeworth expansion by a sequence of smooth functions. Sandi-
  navian Journal of Statistics, 8 207–217.

Skovgaard, I. M. (1986). On multivariate edgeworth expansions. International Statistical Review, 54 169–186.

White, H. (1982). Maximum likelihood estimation of misspecified models. Econometrica, 50 1–25.

Wu, C. F. J. (1986). Jacknife, bootstrap, and other resampling methods in regression analysis. Annals of Statistics,
  14 1261–1295.




                                                        23
