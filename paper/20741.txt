                                NBER WORKING PAPER SERIES




       SIGN RESTRICTIONS, STRUCTURAL VECTOR AUTOREGRESSIONS, AND
                        USEFUL PRIOR INFORMATION

                                        Christiane Baumeister
                                         James D. Hamilton

                                        Working Paper 20741
                                http://www.nber.org/papers/w20741


                      NATIONAL BUREAU OF ECONOMIC RESEARCH
                               1050 Massachusetts Avenue
                                 Cambridge, MA 02138
                                    December 2014




The views expressed in this paper are those of the authors and do not necessarily reflect those of the
Bank of Canada. We thank Luc Bauwens, Fabio Canova, Gary Chamberlain, Drew Creal, Lutz Kilian,
Adrian Pagan, Elie Tamer, Harald Uhlig, and anonymous referees for helpful comments. The views
expressed herein are those of the authors and do not necessarily reflect the views of the National Bureau
of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2014 by Christiane Baumeister and James D. Hamilton. All rights reserved. Short sections of text,
not to exceed two paragraphs, may be quoted without explicit permission provided that full credit,
including © notice, is given to the source.
Sign Restrictions, Structural Vector Autoregressions, and Useful Prior Information
Christiane Baumeister and James D. Hamilton
NBER Working Paper No. 20741
December 2014
JEL No. C11,C32,E24

                                               ABSTRACT

This paper makes the following original contributions to the literature. (1) We develop a simpler analytical
characterization and numerical algorithm for Bayesian inference in structural vector autoregressions
that can be used for models that are overidentified, just-identified, or underidentified. (2) We analyze
the asymptotic properties of Bayesian inference and show that in the underidentified case, the asymptotic
posterior distribution of contemporaneous coefficients in an n-variable VAR is confined to the set
of values that orthogonalize the population variance-covariance matrix of OLS residuals, with the
height of the posterior proportional to the height of the prior at any point within that set. For example,
in a bivariate VAR for supply and demand identified solely by sign restrictions, if the population correlation
between the VAR residuals is positive, then even if one has available an infinite sample of data, any
inference about the demand elasticity is coming exclusively from the prior distribution. (3) We provide
analytical characterizations of the informative prior distributions for impulse-response functions that
are implicit in the traditional sign-restriction approach to VARs, and note, as a special case of result
(2), that the influence of these priors does not vanish asymptotically. (4) We illustrate how Bayesian
inference with informative priors can be both a strict generalization and an unambiguous improvement
over frequentist inference in just-identified models. (5) We propose that researchers need to explicitly
acknowledge and defend the role of prior beliefs in influencing structural conclusions and illustrate
how this could be done using a simple model of the U.S. labor market.


Christiane Baumeister
Bank of Canada
234 Laurier Ave West
Ottawa ON K1A 0G9
Canada
cbaumeister@bankofcanada.ca

James D. Hamilton
Department of Economics, 0508
University of California, San Diego
9500 Gilman Drive
La Jolla, CA 92093-0508
and NBER
jhamilton@ucsd.edu


A data appendix is available at:
http://www.nber.org/data-appendix/w20741
1     Introduction.

In pioneering papers, Blanchard and Diamond (1990), Faust (1998), Davis and Haltiwanger

(1999), Canova and De Nicoló (2002), and Uhlig (2005) proposed that structural inference

using vector autoregressions might be based solely on prior beliefs about the signs of the

impacts of certain shocks. This approach has since been adopted in hundreds of follow-up

studies, and today is one of the most popular tools used by researchers who seek to draw

structural conclusions using VARs.

    But an assumption about signs is not enough by itself to identify structural parameters.

What the procedure actually delivers is a set of possible inferences, each of which is equally

consistent with both the observed data and the underlying restrictions.

    There is a huge literature that considers econometric inference under set identiﬁcation

using a frequentist approach; see for example the reviews in Manski (2003) and Tamer

(2010).   However, to our knowledge Moon, Schorfheide and Granziera (2013) is the only

eﬀort to apply these methods to sign-restricted VARs, where the number of parameters can

be very large and the topology of the identiﬁed set quite complex. Instead, the hundreds

of researchers who have estimated sign-restricted VARs have virtually all used numerical

methods that are essentially Bayesian in character, though often without acknowledging

that the methods represent an application of Bayesian principles.

    If the data are uninformative for distinguishing between elements within a set, for some

questions of interest the Bayesian posterior inference will continue to be inﬂuenced by prior

beliefs even if the sample size is inﬁnite. This point has been well understood in the literature


                                               3
on Bayesian inference in set-identiﬁed models; see for example Poirier (1998), Gustafson

(2009), and Moon and Schorfheide (2012). However, the implications of this fact for the

procedures popularly used for set-identiﬁed VARs have not been previously documented.

   The popular numerical algorithms currently in use for sign-identiﬁed VARs only work

for a very particular prior distribution, namely the uniform Haar prior.        By contrast, in

this paper we provide analytical results and numerical algorithms for Bayesian inference for

a quite general class of prior distributions for structural VARs. Our expressions simplify

the methods proposed by Sims and Zha (1998) as well as generalize them to the case in

which the structural assumptions may not be suﬃcient to achieve full identiﬁcation. Our

formulation also allows us to characterize the asymptotic properties of Bayesian inference in

structural VARs that may not be fully identiﬁed.

   We demonstrate that as the sample size goes to inﬁnity, the analyst could know with

certainty that contemporaneous structural coeﬃcients fall within a set S(Ω) that orthogo-

nalizes the true variance-covariance matrix, but within this set, the height of the posterior

distribution is simply a constant times the height of the prior distribution at that point. In

the case of a bivariate model of supply and demand in which sign restrictions are the sole

identifying assumption, if the reduced-form residuals have positive correlation, then S(Ω) al-

lows any value for the elasticity of demand but restricts the elasticity of supply to fall within

a particular interval. With negatively correlated errors, the elasticity of supply could be any

positive number while the elasticity of demand is restricted to fall in a particular interval.

   We also explore the implications of the Haar prior. We demonstrate that although this



                                               4
is commonly regarded as uninformative, in fact it implies nonuniform distributions for key

objects of interest. It implies that the impact of a one-standard-deviation structural shock

is regarded before seeing the data as coming from a distribution with more mass at the

center when the number of variables n in the VAR is greater than 3 but more mass at the

extremes when n = 2. We also show that the Haar distribution implies Cauchy priors for

structural parameters such as elasticities. We demonstrate that users of these methods can

in some cases end up performing hundreds of thousands of calculations, ostensibly analyzing

the data, but in fact are doing nothing more than generating draws from a prior distribution

that they never even acknowledged assuming.

   We recommend instead that any prior beliefs should be acknowledged and defended

openly and their role in inﬂuencing posterior conclusions clearly identiﬁed.     We claim a

number of advantages of this approach over existing methods for both underidentiﬁed and

just-identiﬁed VARs. First, although sign restrictions only achieve set-identiﬁcation, most

users of these methods are tempted to summarize their results using point estimates, an

approach that is deeply problematic from a frequentist perspective (see for example Fry and

Pagan, 2011).   By contrast, if priors accurately reﬂect our uncertainty about parameters

and the underlying structure, then given a loss function there is an unambiguously optimal

posterior estimate to report for any object of interest, and the Bayesian posterior distribu-

tion accurately represents our combined uncertainty resulting from having a limited set of

observed data as well as possible doubts about the true structure itself. Second, researchers

like Kilian and Murphy (2012) and Caldara and Kamps (2012) have argued persuasively



                                             5
for the beneﬁts of using additional prior information about parameters as a supplement to

sign restrictions in VARs. Our approach provides a ﬂexible and formal apparatus for doing

exactly this in quite general settings. Third, our proposed Bayesian methods can be viewed

as a strict generalization of the conventional approach to identiﬁcation, with multiple ad-

vantages, including better statistical treatment of joint uncertainty about contemporaneous

and lagged coeﬃcients as well as the opportunity to visualize, as in Leamer (1981), the

consequences for the posterior inference of relaxing the role of the prior.1

       The plan of the paper is as follows. Section 2 describes a possibly set-identiﬁed n-variable

VAR and derives the Bayesian posterior distribution for an arbitrary prior distribution on

contemporaneous coeﬃcients assuming that priors for other parameters are chosen from the

natural conjugate classes. We also analyze the asymptotic properties of Bayesian inference

in this general setting. Section 3 provides an analytical characterization of the priors that

are implicit in the popular approach to sign-identiﬁed VARs. Section 4 discusses the use of

additional information about impacts at longer horizons, noting the need to formulate these

in terms of joint prior beliefs about contemporaneous and lagged structural coeﬃcients.

Section 5 illustrates our recommended methods using a simple model of the U.S. labor

market. Section 6 brieﬂy concludes.

   1
      Giacomini and Kitagawa (2013) proposed forming priors directly on the set of orthogonal matrices that
could transform residuals orthogonalized by the Cholesky factorization into an alternative orthogonalized
structure, and investigate the sensitivity of the resulting inference to the priors. By contrast, our approach
is to formulate priors directly in terms of beliefs about the economic structure.




                                                      6
2     Bayesian inference for partially identiﬁed structural
      vector autoregressions.

We investigate dynamic structural models of the form


                                            Ayt = Bxt−1 + ut                                      (1)


for yt an (n × 1) vector of observed variables, A an (n × n) matrix summarizing their con-

temporaneous structural relations, xt−1 a (k × 1) vector (with k = mn + 1) containing a
                                        ′
constant and m lags of y (x′t−1 = (yt−1 , yt−2
                                           ′           ′
                                               , ..., yt−m , 1)′ ), and ut an (n × 1) vector of struc-

tural disturbances assumed to be i.i.d. N (0, D) and mutually uncorrelated (D diagonal).

The reduced-form VAR associated with the structural model (1) is


                                            yt = Φxt−1 + εt                                       (2)


                                               Φ = A−1 B                                          (3)

                                               εt = A−1 ut                                        (4)

                                    E(εt ε′t ) = Ω = A−1 D(A−1 )′ .                               (5)

Note that maximum likelihood estimates of the reduced-form parameters are given by

                                                                                    −1
                                        T                      T
                            Φ̂T =       t=1   yt x′t−1         t=1     xt−1 x′t−1                 (6)

                                                             T
                                        Ω̂T = T −1           t=1   ε̂t ε̂′t                       (7)

for ε̂t = yt − Φ̂T xt−1 .




                                                         7
       In this section we suppose that the investigator begins with prior beliefs about the values

of the structural parameters represented by a density p(A, D, B), and show how observation

of the data YT = (x′0 , y1′ , y2′ , ..., yT′ )′ would lead the investigator to revise those beliefs.2

       We represent prior information about the contemporaneous structural coeﬃcients in the

form of an arbitrary prior distribution p(A). This prior could incorporate any combination

of exclusion restrictions, sign restrictions, and informative prior beliefs about elements of A.

For example, our procedure could be used to calculate the posterior distribution even if no

sign or exclusion restrictions were imposed. We also allow for interaction between the prior

beliefs about diﬀerent parameters by specifying conditional prior distributions p(D|A) and

p(B|A, D) that potentially depend on A. We assume that there are no restrictions on the

lag coeﬃcients in B other than the prior beliefs represented by the distribution p(B|A, D).

       To represent prior information about D and B we employ natural conjugate distributions

which facilitate analytical characterization of results as well as allow for simple empirical

implementation. We use Γ(κi , τ i ) priors for the reciprocals of diagonal elements of D, taken

to be independent across equations,3

                                                        n
                                       p(D|A) =         i=1   p(dii |A)

   2
     Our derivations draw on insights from Sims and Zha (1998). The main diﬀerence is that they para-
meterize the contemporaneous relations in terms of a single matrix, whereas we use two matrices A and D,
and take advantage of the fact that the posterior distribution of D is known analytically. As a consequence
the core expression in our result (equation (21)) is simpler than their equation (10). Among other beneﬁts,
we show that the asymptotic properties of (21) can be obtained analytically.
   3
      We will follow the notational convention of using p(.) to denote any density, with the density being
referred to implicit by the argument. Thus p(A) is shorthand notation for pA (A) and represents a diﬀerent
function from p(D), which in more careful notation would be denoted pD (D).




                                                    8
                                    
                                    
                                           κ
                                           τi i
                                               (d−1 )κi −1
                                          Γ(κi ) ii
                                                              exp(−τ i d−1        −1
                                                                        ii ) for dii ≥ 0
                    p(d−1
                       ii |A)   =                                                             ,        (8)
                                    
                                    
                                                         0                       otherwise

where dii denotes the (i, i) element of D. Note that κi /τ i denotes the prior mean for d−1
                                                                                         ii and


κi /τ 2i its variance. Our general results below allow both κi and τ i to be arbitrary functions

of A.

    Normal priors are used for the lagged structural coeﬃcients B, with results particularly

simple if coeﬃcients are taken to be independent N(mi , dii Mi ) across equations:

                                                              n
                                      p(B|D, A) =             i=1   p(bi |D, A)                        (9)

                                      1
           p(bi |D, A) =                           exp[−(1/2)(bi − mi )′ (dii Mi )−1 (bi − mi )].     (10)
                           (2π)k/2 |dii Mi |1/2

Here b′i denotes the ith row of B (the lagged coeﬃcients for the ith structural equation).

Thus mi denotes the prior mean for the lagged coeﬃcients in the ith equation and dii Mi

denotes the variance associated with this prior. We allow mi and Mi to be functions of A

but not of D.

    The overall prior is then

                                                         n
                           p(A, D, B) = p(A)             i=1 [p(dii |A)p(bi |D, A)].                  (11)


With Gaussian residuals, the likelihood function (conditioning on the pre-sample values of

y0 , y−1 , ..., y−m+1 ) is given by


          p(YT |A, D, B) = (2π)−T n/2 | det(A)|T |D|−T /2 ×

                                                        T
                                    exp −(1/2)          t=1 (Ayt      − Bxt−1 )′ D−1 (Ayt − Bxt−1 )   (12)

                                                          9
where | det(A)| denotes the absolute value of the determinant of A.

   Components of the Bayesian posterior distributions can be conveniently characterized by

regressions on augmented data sets deﬁned by
                                                      
                                                  ′
                                               y1 ai 
                                                      
                                               . 
                                               .. 
                                                      
                                       Ỹi = 
                                                       
                                                                                       (13)
                                   [(T +k)×1]  y′ a 
                                               T i 
                                                      
                                                      
                                                P′i mi
                                                      
                                               x′0    
                                                      
                                               .      
                                               ..     
                                                      
                                       X̃i = 
                                                       
                                                                                       (14)
                                   [(T +k)×k]  x′     
                                               T −1   
                                                      
                                                      
                                                 P′i

for Pi the Cholesky factor of M−1      ′
                               i = Pi Pi . In Appendix A we derive the following charac-


terization of the posterior distribution and detail in Appendix B an algorithm that can be

used to generate draws from this distribution.

   Proposition 1. Let a′i denote the ith row of A, φ(x; µ, Σ) denote the multivariate

Normal density with mean µ and variance Σ evaluated at x and γ(x; κ, τ ) denote a gamma

density with parameters κ and τ evaluated at x.      If the likelihood is (12) and priors are

given by (8)-(11), then for Ỹi and X̃i deﬁned by (13) and (14), the posterior distribution

can be written as


                    p(A, D, B|YT ) = p(A|YT )p(D|A, Y T )p(B|A, D, YT )



                                            10
with
                                                            n
                                 p(B|A, D, Y T ) =          i=1   φ(bi ; m∗i , dii M∗i )

                                                                 −1
                                           m∗i = X̃′i X̃i               X̃′i Ỹi                              (15)

                                                                        −1
                                                M∗i = X̃′i X̃i                                                (16)

                                                            n
                                      p(D|A, YT ) =         i=1   γ(d−1    ∗     ∗
                                                                     ii ; κi , τ i )


                                                 κ∗i = κi + (T /2)                                            (17)

                                                τ ∗i = τ i + (ζ ∗i /2)                                        (18)

                                                                                   −1
                               ζ ∗i = Ỹi′ Ỹi − Ỹi′ X̃i             X̃′i X̃i          X̃′i Ỹi              (19)
                                                                         n
                                    kT p(A)[det(AΩ̂T A′ )]T /2                     |M∗i |1/2 τ κi i
                    p(A|YT ) =            n                κ∗i
                                                                                                    Γ(κ∗i )   (20)
                                                   ∗
                                          i=1 [(2τ i /T )]              i=1
                                                                                   |Mi |1/2 Γ(κi )

for Ω̂T given by (7) and kT the constant for which (20) integrates to unity.

      Note that if the prior parameters Mi , κi and τ i do not depend on A, the last term in

(20) can be subsumed into the constant term kT , in which case (20) simpliﬁes to4

                                               kT p(A)[det(AΩ̂T A′ )]T /2
                                    p(A|YT ) =       n        ∗      κ∗i
                                                                          .                                   (21)
                                                     i=1 [(2τ i /T )]


      Consider ﬁrst the posterior distribution for bi , the lagged coeﬃcients in the ith structural

equation, conditional on A and D. In the special case of a noninformative prior for these

coeﬃcients (M−1
             i  = 0), this takes the form of a Normal distribution centered at m∗i =
                         −1
      T                       T
      t=1   xt−1 x′t−1        t=1   xt−1 yt′ ai , or the coeﬃcient from an OLS regression of a′i yt on
                                                                  −1
                                              T
xt−1 , with variance given by dii             t=1   xt−1 x′t−1         , again the OLS formula. Although the

  4
      Note also for the special case of noninformative priors, as κi , τ i → 0, τ κi i → 1.

                                                          11
Bayesian would describe m∗i and dii M∗i as moments of the posterior distribution, they are

simple functions of the data, and it is also straightforward to use a frequentist perspective to

summarize the properties of the Bayesian posterior inference. In particular, as long as M−1
                                                                                         i


is ﬁnite and the true process for yt is covariance-stationary and ergodic for second moments,

we have that as the sample size T gets large,

                                                          −1
                            T                                         T
         m∗i =       T −1   t=1   xt−1 x′t−1 + T −1 M−1
                                                     i         T −1   t=1   xt−1 yt′ ai + T −1 M−1
                                                                                                i mi

                    p
                   → E(xt−1 x′t−1 )−1 E(xt−1 yt′ )ai

            p
and M∗i → 0. In other words, as long as M−1
                                         i is ﬁnite, the values of the prior parameters mi


and Mi are asymptotically irrelevant, and the Bayesian posterior distribution for bi collapses

to a Dirac delta function around the same plim that characterizes the OLS regression of a′i yt

on xt−1 .       Conditional on ai , the data are perfectly informative asymptotically about bi ,

reproducing the familiar result that, for these features of the parameter space, the Bayesian

inference is the same asymptotically as frequentist inference and correctly uncovers the true

value.

   Similarly for dii , the variance of the ith structural equation, in the special case of a

noninformative prior for B (that is, when M−1
                                           i  = 0) we have that ζ ∗i = T a′i Ω̂T ai . If the

priors for dii are also noninformative (κi = τ i = 0) , then the posterior expectation of d−1
                                                                                           ii is


given by κ∗i /τ ∗i = 1/(a′i Ω̂T ai ), the reciprocal of the average squared residual from the OLS

regression of a′i yt on xt−1 , with variance κ∗i /(τ ∗i )2 = 2/[T (a′i Ω̂T ai )2 ] again shrinking to zero

as T gets large. In the case of general but nondogmatic priors (κi , τ i and M−1
                                                                              i  all ﬁnite),

as T → ∞, the value of ζ ∗i /T still converges to a′i Ω0 ai for Ω0 = E(εt ε′t ) the true variance

                                                     12
matrix, and the Bayesian posterior distribution for d−1
                                                     ii conditional on A collapses to a point


mass at 1/(a′i Ω0 ai ). Hence again the priors are asymptotically irrelevant for inference about

D conditional on A.

   By contrast, prior beliefs about A will not vanish asymptotically unless the elements of

A are point identiﬁed.    To see this, note that in the special case of noninformative prior

beliefs about B and D, the posterior (20) simpliﬁes to

                                         kT p(A)| det(AΩ̂T A′ )|T /2
                            p(A|YT ) =                           T /2
                                                                                           (22)
                                          det diag(AΩ̂T   A′ )

where diag(AΩ̂T A′ ) denotes a matrix whose diagonal elements are the same as those of AΩ̂T A′

and whose oﬀ-diagonal elements are zero. Thus when evaluated at any value of A that di-

agonalizes Ω̂T , the posterior distribution is proportional to the prior. Recall further from

Hadamard’s Inequality that if A has full rank and Ω̂T is positive deﬁnite, then


                             det diag(AΩ̂T A′ ) ≥ det AΩ̂T A′


with equality only if AΩ̂T A′ is diagonal. Thus if we deﬁne


                            S(Ω) = {A : AΩA′ = diag(AΩA′ )},                               (23)


then


                          p(A|Y T ) = kT p(A)          if A ∈ S(Ω̂T )

                                           p
                                          →0          if A ∈
                                                           / S(Ω̂T )


   More formally, for any A and Ω we can measure the distance q(A, Ω) between A and

                                               13
S(Ω) by the sum of squares of the oﬀ-diagonal elements of the Cholesky factor of AΩA′ ,

                               n        i−1
               q(A, Ω) =       i=2      j=1   p2ij (A, Ω)      P(A, Ω)[P(A, Ω)]′ = AΩA′ ,                    (24)


so that q(A, Ω) = 0 if and only if A ∈ S(Ω). Let Hδ (Ω) be the set of all A that are within

a distance δ of the set S(Ω):


                                      Hδ (Ω) = {A : q(A, Ω) ≤ δ}.                                            (25)


As long as the prior puts nonzero mass on some values of A that are consistent with the

true Ω0 (Prob[A ∈ Hδ (Ω0 )] > 0, ∀δ > 0), then asymptotically the posterior will have no

mass outside of this set (Prob{[A ∈ Hδ (Ω0 )]|YT } → 1, ∀δ > 0). Proposition 2 summarizes

the above asymptotic claims; see Appendix C for the proofs.

   Proposition 2. Let yt be any process that is covariance stationary and ergodic for
                              (n×1)

second moments. Let xt = (yt′ , yt−1
                                 ′           ′
                                     , ..., yt−m+1 , 1)′ and YT = (x′0 , y1′ , y2′ , ..., yT′ )′ . Deﬁne
                          (k×1)

                                                                           −1
                                      Φ0 = E(yt x′t−1 ) {E(xt x′t )}
                                      (n×k)

                                                                           −1
                       Ω0 = E(yt yt′ ) − E(yt x′t−1 ) {E(xt x′t )}              E(xt−1 yt′ )
                      (n×n)


with E(xt x′t ) and Ω0 both assumed to be positive deﬁnite. Deﬁne                        A , d−1         −1
                                                                                              11 , ..., dnn , B
                                                                                       (n×n)               (n×k)

to be random variables whose joint density conditional on YT is given by

                                                               ′ T /2
               p(A,d−1         −1
                    11 , ..., dnn , B|YT ) = kT p(A)[det(AΩ̂T A )]    ×

                                                        n     γ(d−1    ∗     ∗           ∗       ∗
                                                                 ii ; κi , τ i )φ(bi ; mi , dii Mi )
                                                        i=1                            ∗                     (26)
                                                                         [(2τ ∗i /T )]κi

where a′i and b′i denote the ith rows of A and B, respectively, and κ∗i , τ ∗i , m∗i , M∗i , ζ ∗i and

Ω̂T are the functions of YT deﬁned in Proposition 1 where Mi is any invertible (k × k)

                                                        14
matrix, mi any ﬁnite (k × 1) vector, τ i and κi any ﬁnite nonnegative constants, and kT an

integrating constant that depends only on Ω̂T , τ i , κi (i = 1, ..., n) and the functional form of

p(A). Let p(A) be any bounded density for which            A∈Hδ (Ω0 )
                                                                        p(A)dA > 0 for all δ > 0 with

Hδ (Ω) deﬁned in (25). Then as the sample size T goes to inﬁnity, the random variables

characterized by (26) have the following properties:
                                 p
   (i) B|A,d11 , ..., dnn , YT → AΦ0 ;
                           p
   (ii) (ζ ∗i /T )|A, Y T → a′i Ω0 ai ;
                      p
   (iii) dii |A, Y T → a′i Ω0 ai ;

   (iv) Prob[A ∈ Hδ (Ω0 )|YT ] → 1 for all δ > 0.

   Moreover, if κi = τ i = 0 and Mi = 0 for i = 1, ..., n, then when evaluated at any

A ∈ S(Ω̂T ),

                               (v)        p(A|YT ) = kT p(A)   for all T.

   Note that while we originally motivated the expression in (26) as the Bayesian posterior

distribution for a Gaussian structural VAR, the results in Proposition 2 do not assume that

the actual data are Gaussian or even that they follow a VAR. Nor does the proposition

make any use of the fact that there exists a Bayesian interpretation of these formulas. The

proposition provides a frequentist interpretation of what Bayesian inference amounts to when

the sample gets large. The proposition establishes that as long as the prior assigns nonzero

probability to a subset of A that diagonalizes the value Ω0 deﬁned in the proposition, then

asymptotically the posterior density will be conﬁned to that subset and at any point within

the set will converge to some constant times the value of the prior density at that point.


                                                    15
   In the special case where the model is point-identiﬁed, there exists only one allowable

value of A for which AΩ0 A′ is diagonal. Provided that p(A) is nonzero in all neighborhoods

including that point, the posterior distribution collapses to the Dirac delta function at this

value of A. This reproduces the familiar result that under point identiﬁcation, the priors

on all parameters (A, D, and B) are asymptotically irrelevant and Bayesian inference is

asymptotically equivalent to maximum likelihood estimation, producing consistent estimates

of parameters.

   We close this section with comments on some alternative ways we could parameterize

the model. One possibility would be to work directly with the reduced form in (2),


                                     yt = Φxt−1 + H̃ut ,                                 (27)


and specify priors over H̃, the matrix of date-zero impacts of structural shocks,

                                                 ∂yt
                                          H̃ =        .
                                                 ∂u′t

                            −1
Recall from (4) that H̃ = A . Since Proposition 1 established results for an arbitrary set of

prior beliefs about A, those expressions also immediately characterize the posterior inference

that would emerge from any set of prior beliefs about H̃. Alternatively we could consider

forming priors directly over the impacts of one-standard-deviation shocks: H = A−1 D1/2 .

   One potential advantage that might be claimed for the latter approaches is that if we

are interested only in the eﬀects of the jth structural shock, we only need information about

the jth column of H̃ or H rather than the entire matrix A as in our proposed treatment.

However, in our view it would be unusual to know nothing about many of the speciﬁc

                                             16
structural relations in A and yet have conﬁdence about how those diﬀerent equations interact

to produce equilibrium outcomes summarized by certain elements of A−1 . The vast majority

of useful prior economic information (for example, all of the hundreds of studies on which we

draw in our empirical application in Section 5) have been formulated in terms of things we

know about A rather than A−1 . Simulations by Canova and Paustian (2011) also document

practical limitations to trying to draw inference using restrictions on only one column of

H even if those restrictions are known to be correct. For this reason we believe that the

parameterization used in Propositions 1 and 2 is preferred.

   Yet another approach would be to calculate the prior distribution for the reduced-form

parameters {Φ, Ω} that is implied by our prior p(A, B, D) and then rewrite the latter

as p(A|Φ, Ω)p(Φ, Ω). For such a parameterization we know as in Drèze (1974) that the

posterior p(Φ, Ω|YT ) will become perfectly informative about the reduced-form parameters

while p(A|Φ, Ω, YT ) = p(A|Φ, Ω). Again the practical challenge is how to characterize the

implications of previous research and prior knowledge in terms of a distribution p(A|Φ, Ω).

For example, we are aware of no empirical studies that have actually implemented Drèze’s

suggestion in the half-century since it was ﬁrst proposed for purposes of analyzing systems

of simultaneous equations.    While one could use numerical methods to characterize the

distributions p(Φ, Ω) and p(A|Φ, Ω) that are implicit in our suggested priors, our impression

is that calculating these would be more computationally taxing than the approach spelled

out in Appendix B, most of whose key steps are known analytically.




                                             17
3     Priors implicit in the traditional approach to sign-
      identiﬁed VARs.

In this section we characterize the prior distributions that are implicit in the approach

to sign-restricted VARs developed by Rubio-Ramírez, Waggoner, and Zha (2010).            Their

algorithm begins by generating an (n × n) matrix X = [xij ] of independent N (0, 1) variates

and then calculating the decomposition X = QR where Q is an orthogonal matrix and R

is upper triangular. From this a candidate draw for the contemporaneous response to the

vector of structural shocks is calculated as


                                            H = PQ                                        (28)


for P the Cholesky factor of the variance matrix of the reduced-form innovations:


                                           Ω = PP′ .                                      (29)


    This algorithm can be viewed as generating draws from a Bayesian prior distribution for

H conditional on Ω, the properties of which we now document. Note that the ﬁrst column

of Q is simply the ﬁrst column of X normalized to have unit length:
                                                          
                              q11   x11 / x211 + · · · + x2n1     
                                                                  
                               ..                ..               
                                .  
                                    =               .               .                   (30)
                                                                   
                                                                  
                                                                  
                               qn1       xn1 / x211 + · · · + x2n1

In the special case when n = 2, q11 has the interpretation as the cosine of θ, the angle deﬁned

by the point (x11 , x21 ). In this case, the fact that Q is an orthogonal matrix will result in

half the draws being the rotation matrix associated with θ and the other half the reﬂection

                                               18
matrix:                                                    
                                
                                
                                
                                    cos θ − sin θ          
                                
                                                            with probability 1/2
                                
                                                           
                                
                                
                                     sin θ cos θ
                          Q=                                           .                                 (31)
                                
                                
                                
                                  cos θ sin θ 
                                
                                                 with probability 1/2
                                
                                                
                                
                                
                                   sin θ − cos θ

Independence of x11 and x21 ensures that θ will have a uniform distribution over [−π, π]. It

is in this sense that the prior implicit in the procedure (28) is thought to be uninformative.

       However, a prior that is uninformative about some features of the parameter space can

turn out to be informative about others. It is not hard to show that each element of (30)

has a marginal density given by5
                          
                          
                                Γ(n/2)            2 (n−3)/2
                                            (1 − qi1 )      if qi1 ∈ [−1, 1]
                            Γ(1/2)Γ((n−1)/2)
                p(qi1 ) =                                                     .                            (32)
                          
                          
                                            0                  otherwise

Note next that the (1, 1) element of (28) therefore implies a prior distribution for the eﬀect

of a one-standard-deviation increase in structural shock number 1 on variable number 1 that

is characterized by the random variable

                                                                    √
                                           h11 = p11 q11 =           ω 11 q11

   5                                                                            2
       Note from Theorem 3.1 of Devroye (1986, p. 403) that the variable yi1 = qi1 ,

                                                               x2i1
                                              yi1 =                         ,
                                                      x211   + · · · + x2n1
has a Beta(1/2,(n − 1)/2) distribution:
                                     Γ(n/2)       (1/2)−1
                                Γ(1/2)Γ((n−1)/2) yi1      (1      − yi1 )((n−1)/2)−1   if yi1 ∈ [0, 1]
                    p(yi1 ) =                                                                          .
                                                        0                                otherwise
                     √
The density of qi1 = yi1 in (32) is then obtained using the change-of-variables formula. Alternatively, one
can verify directly that when θ ∼ U (−π, π), the random variables ± cos θ and ± sin θ each have a density
that is given by the special case of (32) corresponding to n = 2.

                                                             19
for ω 11 the (1, 1) element of Ω. In other words, the implicit prior belief about the eﬀect of
                                                    √
this structural shock has the distribution of        ω 11 times the random variable in (32). Note

that if we invoke no further normalization or sign restrictions, “shock number 1” would be

associated with no diﬀerent identifying information than “shock number j”. Invariance of

the algorithm6 and the change-of-variables formula for probabilities thus implies a prior

density for hij = ∂yit /∂ujt , the eﬀect on impact of structural shock j on observed variable i

given by
                    
                    
                            Γ(n/2)                                              √      √
                                       √1 (1   − h2ij /ω ii )(n−3)/2 if hij ∈ [− ω ii , ω ii ]
                        Γ(1/2)Γ((n−1)/2) ωii
      p(hij |Ω) =                                                                                 .   (33)
                    
                    
                                         0                                 otherwise

   Panel A of Figure 1 plots this density for two diﬀerent values of n.                   When there are
                                                           √
n = 2 variables in the VAR, this prior regards impacts of ± ω ii as more likely than 0. By

contrast, when there are n ≥ 4 variables in the VAR, impacts near 0 are regarded as more

likely. The distribution is only uniform in the case n = 3.

   Alternatively, researchers are often interested in the eﬀect of a structural shock nor-

malized by some deﬁnition, for example, the eﬀect on observed variable 2 of a shock that

increases observed variable number 1 by one unit. Such an object is characterized by the

  6
    This conclusion can also be veriﬁed by direct manipulation of the relevant equations, as shown in
Appendix E.




                                                     20
ratio of the (2,1) to the (1,1) element of (28):7

                                     ∂y2t   p21 q11 + p22 q21   p21 p22 x21
                          h∗21 =       ∗
                                          =                   =    +
                                     ∂u1t        p11 q11        p11 p11 x11
                                                     ω 22 − ω 221 /ω 11 x21
                                = ω 21 /ω 11 +                              .
                                                           ω 11         x11

Recall that the ratio of independent N(0, 1) has a standard Cauchy distribution. Invariance

properties again establish that the choice of subscripts 1 and 2 is irrelevant. Thus for any

i = j,

                                        h∗ij |Ω ∼ Cauchy(c∗ij , σ ∗ij )                                  (34)

                                                                                     ωii −ω 2ij /ωjj
with location and scale parameters given by c∗ij = ω ij /ω jj and σ ∗ij =                 ωjj
                                                                                                     .   This

density is plotted in Panel B of Figure 1.

      Figure 1 establishes that although the prior is uninformative about the angle of rotation

θ, it can be highly informative for the objects about which the researcher intends to form

an inference, namely the impulse-response functions.

      In practice, applied researchers often ignore posterior uncertainty about Ω and simply

condition on the average residual variance Ω̂T . Identifying normalization or sign restrictions

then restrict the distributions hij |Ω̂T or h∗ij |Ω̂T to certain allowable regions.           But within

these regions, the shapes of the posterior distributions are exactly those governed by the

implicit prior distributions plotted in Figure 1.               One can see how this happens using a

simple bivariate example in which the ﬁrst variable is a measure of price and the second

  7
      The ﬁnal equation here is derived from the upper (2 × 2) block of (29):

                                    ω11   ω21          p211       p11 p21
                                                 =                              .
                                    ω21   ω22         p11 p21   p221 + p222



                                                      21
variable is a measure of quantity.            Taking the case of the reﬂection matrix in (31) for

illustration8 , we have
                                                                                       
                   h11 h12          p11 cos θ               p11 sin θ        
                           =                                                 .                      (35)
                                                                             
                    h21 h22     (p21 cos θ + p22 sin θ) (p21 sin θ − p22 cos θ)

Suppose our normalizing and partially identifying restrictions consist of the requirements

that a demand shock raises price (h11 ≥ 0) and raises quantity (h21 ≥ 0) while a supply

shock raises price (h12 ≥ 0) and lowers quantity (h22 ≤ 0). The ﬁrst and third inequalities

then restrict θ ∈ [0, π/2]. Suppose further that the correlation between the price and quantity

residuals is positive (p21 > 0). Then h21 ≥ 0 for all θ ∈ [0, π/2], as called for.                 But the

condition h22 ≤ 0 requires θ ∈ [0, θ̃] where θ̃ is the angle in [0, π/2] for which cot θ̃ = p21 /p22 .

Recall that the response of quantity to a supply shock that raises the price by 1% is given

by the ratio of the (2,2) to the (2,1) element in (35):

                                                    p21 p22
                                           h∗22 =      −    cot θ.
                                                    p11 p11

As θ goes from 0 to θ̃, h∗22 varies from −∞ to 0. One can verify directly9 that when

θ ∼ U(−π, π), cot θ ∼ Cauchy(0,1). Thus when the correlation between the OLS residuals

is positive, the posterior distribution of h∗22 is a Cauchy(c∗22 , σ ∗22 ) truncated to be negative.

       The value of h∗21 (which measures the response of quantity to a demand shock that raises

the price by 1%) is given by
                                                    p21 p22
                                           h∗21 =      +    tan θ.
                                                    p11 p11

   8
        Draws using the rotation matrix would always be ruled out by the normalization described below.
   9
        See for example Gubner (2006, p. 194). This of course is just a special case of the general result in
(34).

                                                      22
As θ varies from 0 to θ̃, this ranges from

                                             p21 p22      ω 21
                                      hL =      +    ×0 =      .                                (36)
                                             p11 p11      ω 11

to
                                      p21 p22 p22  p2 + p222   ω 22
                               hH =      +        = 21       =                                  (37)
                                      p11 p11 p21   p11 p21    ω 21

     Note that the magnitude h∗21 usually goes by another name— it is the short-run price

elasticity of supply, while h∗22 is just the short-run price elasticity of demand.         We have

thus seen that, when the correlation between price and quantity is positive, the posterior

distribution for the demand elasticity will be the original Cauchy(c∗22 , σ ∗22 ) truncated to

be negative, while the posterior distribution for the supply elasticity will be the original

Cauchy(c∗21 , σ ∗21 ) truncated to the interval [hL , hH ].

     Alternatively, if price and quantity are negatively correlated, the magnitudes in (36) and

(37) would be negative numbers.           In this case, the posterior distribution for the supply

elasticity will be the original Cauchy(c∗21 , σ ∗21 ) truncated to be positive, while that for the

demand elasticity will be the original Cauchy(c∗22 , σ ∗22 ) truncated to the interval [hH , hL ].

     For illustration, we applied the Rubio-Ramírez, Waggoner, and Zha (2010) algorithm

as detailed in Appendix E to an 8-lag VAR ﬁt to U.S. data on growth rates of real labor

compensation and total employment over t = 1970:Q1 - 2014:Q2. The reduced-form VAR

residual variance matrix is estimated by OLS to be
                                                             
                                            0.5920 0.0250 
                                      Ω̂ = 
                                           
                                                           .
                                                                                               (38)
                                             0.0250 0.1014


                                                   23
Since the correlation between wages and employment is positive, the set S(Ω̂) does not

restrict the demand elasticity h∗22 . The blue histogram in the top panel of Figure 2 plots

the magnitude that a researcher using the sign-identiﬁcation methodology would interpret

as the response of employment to a shock to supply that increases the real wage by 1%.

The red curve plots a Cauchy(c∗22 , σ ∗22 ) density truncated to be negative.

       The blue histogram in the bottom panel of Figure 2 plots the magnitude that the re-

searcher would describe as the response of employment to a shock to demand that increases

the real wage by 1%. From (38) we calculate that S(Ω̂) restricts this supply elasticity to fall

between hL = 0.0421 and hH = 4.0626. The green curve plots a Cauchy(c∗21 , σ ∗21 ) truncated

to be positive and the red curve further truncates it to the interval [hL , hH ]. Because the

correlation between the reduced-form residuals is quite small, there is very little diﬀerence

between the red and green distributions.10

       The ﬁgure illustrates that researchers using the traditional methodology can end up

performing hundreds of thousands of calculations, ostensibly analyzing the data, but in the

end are doing nothing more than generating draws from a prior distribution that they never

even acknowledged that they had assumed!

       Another implication of these analytical results is that it would generally be necessary to

report the posterior medians rather than posterior means for inference about magnitudes

such as the demand elasticity when the traditional method is used because the mean of a

  10
     The distributions in Figure 2 will be recognized as similar to some of the shapes found numerically
for posterior distributions in a 5-variable VAR analyzed by Arias, Rubio-Ramírez, and Waggoner (2013),
though these authors did not note the connection to the implicit prior nor provide the theory for why this
is what would be found from the method.



                                                   24
Cauchy variable does not exist.       We can also see analytically what would happen if we

were to apply the traditional methodology to a data set in which the reduced-form variance

matrix Ω̂T is diagonal. In this case the posterior distribution for the time-zero impact of any

structural shock will be nothing more than the distribution given in (33) truncated either

to the positive or negative region.


4     Sign restrictions for higher-horizon impacts.

In an eﬀort to try to gain additional identiﬁcation, many applied researchers impose sign re-

strictions not just on the time-zero structural impacts ∂yt /∂u′t but also on impacts ∂yt+s /∂u′t

for some horizons s = 0, 1, ..., S. These are given by

                                        ∂yt+s
                                              = Ψs A−1                                      (39)
                                         ∂u′t

for Ψs the ﬁrst n rows and columns of Fs for
                                                                
                                     Φ1 Φ2 · · · Φm−1 Φm 
                                                          
                                                          
                                     In 0 · · ·   0    0 
                                                          
                             F=
                               
                                                                                           (40)
                                      .. ..        ..  .. 
                                       .  . ···     .   . 
                                                           
                                                          
                                                          
                                      0 0 ···      In   0

                       yt = c + Φ1 yt−1 + Φ2 yt−2 + · · · + Φm yt−m + εt .

Beliefs about higher-order impacts are of necessity joint beliefs about A and B. For example,

                                          ∂yt
                                               = A−1                                        (41)
                                          ∂u′t
                                        ∂yt+1
                                              = Φ1 A−1 .                                    (42)
                                         ∂u′t

                                               25
If Φ1 is diagonal with negative elements, the signs of ∂yt+1 /∂u′t are opposite those of A−1

itself. In this case, as the sample size grows to inﬁnity, there will be no posterior distribution

satisfying a restriction such as ∂yit /∂ujt and ∂yi,t+1 /∂ujt are both positive. In a ﬁnite sam-

ple, a simulated draw from the posterior distribution purporting to impose such a restriction

would at best be purely an artifact of sampling error. Canova and Paustian (2011) demon-

strated using a popular macro model that implications for the signs of structural multipliers

beyond the zero horizon (∂yt+s /∂u′t for s > 0) are generally not robust.

      In our parameterization prior beliefs about structural impacts for s > 0 would be repre-

sented in the form of the prior distribution p(B|A, D). Our recommendation is that nondog-

matic priors should be used for this purpose, since we have seen the data are asymptotically

fully informative about the posterior p(B|A, D, Y T ).

      A common source of the expectation that signs of ∂yt+s /∂u′t should be the same as

those of ∂yt /∂u′t is a prior expectation that Φ1 is not far from the identity matrix and that

elements of Φ2 , ..., Φm are likely small.      Nudging the unrestricted OLS estimates in the

direction of such a prior has long been known to help improve the forecasting accuracy of a

VAR.11 This suggests that we might want to use priors for A and B that imply a value for

η = E(Φ) given by

                                      η =        In       0        .                             (43)
                                    (n×k)      (n×n)   [n×(k−n)]


As noted by Sims and Zha (1998), since B = AΦ, this calls for setting the prior mean for

 11
      See for example Doan, Litterman and Sims (1984), Litterman (1986), and Smets and Wouters (2003).




                                                  26
B|A to be

                                                E(B|A) = Aη

suggesting a prior mean for bi given by m̃i = E(bi |A) = η ′ ai . We can also follow Doan,

Litterman and Sims (1984) as modiﬁed by Sims and Zha (1998) in putting more conﬁdence

in our prior beliefs that higher-order lags are zero, as we describe in detail in Appendix D.

   Some researchers may want to use additional prior information about structural dynam-

ics. In the example in the following section, we consider a prior belief that a labor demand

shock should have little permanent eﬀect on the level of employment.                                  We have found it

convenient to implement these as supplements to the general recommendations in Appendix

D, where we could put as little weight as we wanted on the general recommendations by

specifying λ0 in expression (64) to be suﬃciently large. For example, suppose we wanted

to supplement the beliefs about reduced-form coeﬃcients from the Doan, Litterman and

Sims prior with the additional prior belief that hi linear combinations (represented by Ri bi )

should be close to some expected value ri , where ri could be a function of A. As in Theil

(1971, pp. 347-49) it is convenient to represent this additional information in the form of hi

pseudo observations


                        ri      = Ri     bi       + vi              vi ∼ N(0, dii Vi )                            (44)
                      (hi ×1)    (hi ×k)(k×1)           (hi ×1)                      (hi ×hi )


allowing us to simply replace (13) and (14) with
                                                                                                  ′
                           Ỹi         =       a′i y1    ···      a′i yT   m′i Pi   r′i PVi                       (45)
                      [(T +k+hi )×1]

                                                                                              ′
                             X̃i           =     x0 · · · xT −1 Pi              R′i PVi                           (46)
                        [(T +k+hi )×k]


                                                           27
for M−1
     i  = Pi P′i and Vi−1 = PVi P′Vi . This is numerically identical to using bi |A, D ∼

N (m̃i , dii M̃i ) as a uniﬁed prior in (10) with M̃−1   −1  ′ −1                    −1
                                                    i = Mi +Ri Vi Ri and m̃i = M̃i (Mi mi +


R′i Vi−1 ri ).

    Although such priors can help improve inference about the structural parameters in a

given observed sample, they do not change any of the asymptotics in Proposition 2.


5      Application: Bayesian inference in a model of labor
       supply and demand.

In this section we illustrate these methods using the example of labor supply and labor

demand introduced in Section 3.        We noted in Figure 2 how prior beliefs inﬂuence the

results when the traditional sign-restriction methodology is applied to these data. The goal

of this section is to elevate such prior information from something that the researcher imposes

mechanically without consideration to something that is explicitly acknowledged and can be

motivated from economic theory and empirical evidence from other data sets. In order to

do this, the ﬁrst step is to write down the structural model with which we are proposing to

interpret the observed correlations. Consider dynamic labor demand and supply curves of

the form:


      demand:     ∆nt = k d + β d ∆wt + bd11 ∆wt−1 + bd12 ∆nt−1 + bd21 ∆wt−2 + bd22 ∆nt−2 +

                  · · · + bdm1 ∆wt−m + bdm2 ∆nt−m + udt


       supply:    ∆nt = k s + αs ∆wt + bs11 ∆wt−1 + bs12 ∆nt−1 + bs21 ∆wt−2 + bs22 ∆nt−2 +

                  · · · + bsm1 ∆wt−m + bsm2 ∆nt−m + ust .                                     (47)

                                                28
Here ∆nt is the growth rate of total U.S. employment,12 ∆wt is the growth rate of real

compensation per hour,13              β d is the short-run wage elasticity of demand, and αs is the

short-run wage elasticity of supply. Note that the system (47) is a special case of (1) with

yt = (∆wt , ∆nt )′ and                                                  
                                                                  d
                                                   −β 1 
                                                A=
                                                  
                                                          .
                                                                                                    (48)
                                                    −αs 1

OLS estimation of the reduced-form (2) for this system with m = 8 lags and t = 1970:Q1

through 2014:Q2 led to the estimate of the reduced-form residual variance matrix Ω̂ =
          T
T −1      t=1   ε̂t ε̂′t reported in (38).

5.1       Maximum likelihood estimates.

As in Shapiro and Watson (1988), for any given α we can ﬁnd the maximum likelihood

estimate of β by an IV regression of ε̂2t on ε̂1t using ε̂2t − αε̂1t as instruments, where ε̂it are

the residuals from OLS estimation of the reduced-form VAR,

                                             T
                                             t=1 (ε̂2t   − αε̂1t )ε̂2t       (ω̂ 22 − αω̂ 12 )
                                β̂(α) =      T
                                                                         =                     ,     (49)
                                             t=1 (ε̂2t   − αε̂1t )ε̂1t       (ω̂ 12 − αω̂ 11 )

for ω̂ ij the (i, j) element of Ω̂. One can verify directly that any pair (α, β) satisfying (49)
                                                 ′
produces a diagonal matrix for AΩ̂A .

       The top panel of Figure 3 plots the function β̂(α) for these data. Any pair (α, β) lying

  12
     The level nt was measured as 100 times the natural log of the seasonally adjusted number of people on
nonfarm payrolls during the third month of quarter t from series PAYEMS downloaded August 2014 from
http://research.stlouisfed.org/fred2/.
  13
     The level wt was measured as 100 times the natural log of seasonally adjusted real compensa-
tion per hour for the nonfarm business sector from series COMPRNFB downloaded August 2014 from
http://research.stlouisfed.org/fred2/.


                                                             29
on these curves would maximize the likelihood function, and there is no basis in the data

for preferring one point on the curves to any other.

      If we restrict the supply elasticity α to be positive and the demand elasticity β to be

negative, we are left with the lower right quadrant in the ﬁgure.                When, as in this data

set, the OLS residuals are positively correlated, the sign restrictions are consistent with any

β ∈ (−∞, 0), but require α to fall in the interval (hL , hH ) deﬁned in (36)-(37). We earlier

derived these bounds considering allowable angles of rotation but it is also instructive to

explain their intuition in terms of a structural interpretation of the likelihood function.14

Note that hL is the estimated coeﬃcient from an OLS regression of ε̂2t on ε̂1t , which is a

weighted average of the positive supply elasticity α and negative demand elasticity β (see

for example Hamilton, 1994, equation [9.1.6]). Hence the MLE for β can be no larger than

hL and the MLE for α can be no smaller than hL . The fact that the MLE for β can be no

larger than hL is not a restriction, because we have separately required that β < 0 and in

the case under discussion, hL > 0. However, the inference that α can be no smaller than hL

puts a lower bound on α. At the other end, the OLS coeﬃcient from a regression of ε̂1t on

ε̂2t (that is, h−1
                H ) turns out to be a weighted average of α
                                                            −1
                                                               and β −1 , requiring β −1 < h−1
                                                                                            H


(again not binding when hH > 0) and α−1 > h−1
                                           H ; the latter gives us the upper bound that


α < hH . This is the intuition for why hL < α < hH .

      The bottom panel in Figure 3 plots contours of the concentrated likelihood function, that

 14
      Leamer (1981) discovered these points in a simple OLS setting years ago.




                                                    30
is, contours of the function

                                                                         ′
                        T log | det(A)| − (T /2) log det diag(AΩ̂A )          .


The data are quite informative that α and β should be close to the values that diagonalize

Ω̂, that is, that α and β are close to the function β = β̂(α) shown in black.

   The set S(Ω) in expression (23) is calculated for this example as follows.              When the

correlation between the VAR residuals ω 12 is positive, S(Ω) is the set of all A in (48) such

that β ≤ 0, (ω 21 /ω 11 ) ≤ α ≤ (ω 22 /ω 21 ), and β = (ω 22 − αω 12 )/(ω 12 − αω 11 ), in other words,

the set of points on the black curve in Figure 3 between hL and hH .

5.2     Prior information about elasticities.

While the frequentist would regard any of the points in S(Ω̂) as equally plausible, from the

standpoint of economic theory that is surely not the case; for example, a demand elasticity of

−100 would not be consistent with any coherent economic model. We propose to represent

prior information about α and β using Student t distributions with ν degrees of freedom.

Note that this includes the Cauchy distribution in (34) as a special case when ν = 1. One

beneﬁt of using ν ≥ 3 is that the posterior distributions for α and β would then be guaranteed

to have ﬁnite mean and variance. Our proposal is that the location and scale parameters for

these distributions should be chosen on the basis of prior information about these elasticities.

   Hamermesh’s (1996) survey of microeconometric studies concluded that the absolute

value of the elasticity of labor demand is between 0.15 and 0.75. Lichter, Peichl, and

Siegloch’s (2014) meta-analysis of 942 estimates from 105 diﬀerent studies favored values


                                                  31
at the lower end of this range. On the other hand, theoretical macro models can imply a

value of 2.5 or higher (Akerlof and Dickens, 2007; Galí, Smets, and Wouters, 2012). A prior

for β that reﬂects the uncertainty associated with these ﬁndings could be represented with

a Student t distribution with location parameter cβ = −0.6, scale parameter σ β = 0.6, and

degrees of freedom ν β = 3, truncated to be negative:
                         
                         
                         
                          F (0; cβ , σ β , ν β )−1 f(β; cβ , σ β , ν β ) if β ≤ 0
                  p(β) =                                                            .               (50)
                         
                         
                                                0                        otherwise

Here f (x; c, σ, ν) denotes the density for a Student t variable with location c, scale σ, and

degrees of freedom ν evaluated at x,
                                                   !             "−(ν+1)/2
                                          Γ( ν+1
                                              2
                                                 )      (x − c)2
                       f(x; c, σ, ν) = √             1+                                             (51)
                                         νπσΓ(ν/2)        σ2 ν
                                                                        x
and F (.) the cumulative distribution function F (x; c, σ, ν) =         −∞
                                                                           f (z; c, σ, ν)dz.   The prior

in (50) places a 5% probability on values of β < −2.2 and another 5% probability on values

above −0.1.

   In terms of the labor supply elasticity, a key question is whether the increase in wages is

viewed as temporary or permanent. A typical assumption is that the income and substitu-

tion eﬀects cancel, in which case there would be zero observed response of labor supply to a

permanent increase in the real wage (Kydland and Prescott, 1982). On the other hand, the

response to a temporary wage increase is often interpreted as the Frisch (or compensated

marginal utility) elasticity, about which there are again quite diﬀerent consensuses in the

micro and macro literatures. Chetty et al. (2013) reviewed 15 diﬀerent quasi-experimental

studies all of which implied Frisch elasticities below 0.5. A separate survey of microecono-

                                                   32
metric studies by Reichling and Whalen (2012) concluded that the Frisch elasticity is between

0.27 and 0.53.    By contrast, values above one or two are common in the macroeconomic

literature (see for example Kydland and Prescott, 1982, Cho and Cooley, 1994, and Smets

and Wouters, 2007). For the prior used in this study, we speciﬁed cα = 0.6, σ α = 0.6, and

ν α = 3, which associates a 90% probability with α ∈ (0.1, 2.2):

                          
                          
                          
                           [1 − F (0; cα , σ α , ν α )]−1 f (α; cα , σ α , ν α )   if α ≥ 0
                 p(α) =                                                                         (52)
                          
                          
                                                     0                             otherwise


   The prior distribution p(A) was thus taken to be the product of (50) with (52). Contours

for this prior distribution are provided in the top panel of Figure 4, while the bottom panel

displays contours for the posterior distribution that would result if we used only this prior

p(A) with no additional information about D or B. The observed data contain suﬃcient

information to cause all of the posterior distribution to fall within a close neighborhood of

S(Ω̂). But whereas the frequentist regards all points within this set as equally plausible,

a sensible person with knowledge of the literature would regard values such as α = 0.5,

β = −0.3 as much more plausible than α = 0.06, β = −9.5. The Bayesian approach gives

the analyst a single coherent framework for combining uncertainty caused by observing a

limited data set (that is, uncertainty about the true value of Ω) with uncertainty about the

correct structure of the model itself (that is, uncertainty about points within S(Ω)), which

combined uncertainty is represented by the contours in the bottom panel of Figure 4.




                                                       33
5.3    Long-run restrictions.

Unfortunately, this combined uncertainty remains quite large for this example; we have no

basis for distinguishing alternative elements of S(Ω̂) other than the priors just described.

To infer anything more from the data would require imposing additional structure, which

from a Bayesian perspective means drawing on additional sources of outside information.

We illustrate how this could be done by making use of additional beliefs about the long-run

labor supply elasticity. Let ỹt = (wt , nt )′ denote the levels of the variables so that yt = ∆ỹt .

From (39) the eﬀect of the structural shocks on the future levels of wages and employment

is given by

                         ∂ỹt+s   ∂∆ỹt+s ∂∆ỹt+s−1         ∂∆ỹt
                             ′
                                =     ′
                                         +     ′
                                                    + ··· +
                          ∂ut      ∂ut      ∂ut              ∂u′t
                                 = Ψs A−1 + Ψs−1 A−1 + · · · + Ψ0 A−1                           (53)

with a permanent or long-run eﬀect given by

                          ∂ỹt+s
                       lim       = (Ψ0 + Ψ1 + Ψ2 + · · · )A−1
                       s→∞ ∂u′t


                                    = (In − Φ1 − Φ2 − · · · − Φm )−1 A−1

                                    = [A(In − Φ1 − Φ2 − · · · − Φm )]−1 .                       (54)

The long-run eﬀect of a labor demand shock on employment is given by the (2,1) element of

this matrix. This long-run elasticity would be zero if and only if the above matrix is upper

triangular, or equivalently if and only if the following matrix is upper triangular

        A(In − Φ1 − Φ2 − · · · − Φm ) = A(In − A−1 B1 − A−1 B2 − · · · − A−1 Bm )

                                          = A − B1 − B2 − · · · − Bm                            (55)

                                                 34
requiring

                                  0 = −αs − bs11 − bs21 − · · · − bsm1

                                    bs11 + bs21 + · · · + bsm1 = −αs .                               (56)

      If we were to insist that (56) has to hold exactly, then the model would become just-

identiﬁed even in the absence of any information about αs or β d , and indeed hundreds of

empirical papers have used exactly such a procedure to perform structural inference using

VARs.15      We propose instead to represent the idea as a prior belief of the form


                          (bs11 + bs21 + · · · + bsm1 )|A, D ∼ N (−αs , d22 V2 ).                    (57)


Note that our proposal is a strict generalization of the existing approach, in that (57) becomes

(56) in the special case when V2 → 0. We would further argue that our method is a

strict improvement over the existing approach in several respects.                  First, (56) is usually

implemented by conditioning on the reduced-form estimates Φ̂. By contrast, our approach

will generate the statistically optimal joint inference about A and B taking into account

the fact that both are being estimated with error. Second, we would argue that the claim

that (56) is known with certainty is indefensible. A much better approach in our view is to

acknowledge openly that (56) is a prior belief about which any reasonable person would have

some uncertainty. Granted, an implication of Proposition 2 is that some of this uncertainty

will necessarily remain even if we had available an inﬁnite sample of observations on y.

However, our position is that this uncertainty about the speciﬁcation should be openly

 15
      See for example Shapiro and Watson (1988), Blanchard and Quah (1989), and Galí (1999).



                                                    35
acknowledged and reported as part of the results, and indeed as we have demonstrated this

is exactly what is accomplished using the algorithm suggested in Proposition 1.

   In addition to these prior beliefs about the long-run elasticity, we also used the general

priors suggested in Appendix D, using (13) and (14) for Ỹ1 and X̃1 and (45) and (46) for

Ỹ2 and X̃2 with r2 = −αs , V2 = 0.1, and


                                    R2     =   1′m ⊗ e′2 0
                                   (1×k)


for 1m an (m × 1) vector of ones and e′2 = (1, 0). The value for V2 corresponds to putting a

weight on the long-run restriction equivalent to 10 observations.

   Note that our recommended approach also helps address some of the concerns about long-

run restrictions raised by Faust and Leeper (1997). For example, rather than dogmatically

impose that coeﬃcients beyond some ﬁxed lag are all zero, our approach instead allows the

researcher to shrink coeﬃcients toward zero gradually as the lag length increases up to some

large m, with the pace of the shrinkage governed by choice of the parameter λ1 in expression

(63).

5.4     Empirical results.

The top panels in Figure 5 display prior densities (red curves) and posterior densities (blue

histograms) for the short-run demand and supply elasticities. The data cause us to revise

our prior beliefs about β d , regarding elasticities near zero as less likely having seen these

data. Our beliefs about the short-run supply elasticity are more strongly revised, favoring

estimates at the lower range of the microeconometric literature over values often assumed


                                               36
in macroeconomic studies. Although our prior expectation was for a zero long-run response

of employment to a labor demand shock, the data provide support for a signiﬁcant positive

permanent eﬀect (see the bottom panel in Figure 5).

   Median posterior values for the impulse-response functions in (53) are plotted as the

solid lines in Figure 6. The shaded 95% posterior credible regions reﬂect both uncertainty

associated with having observed only a ﬁnite set of data as well as uncertainty about the

true structure. A 1% leftward shift of the labor demand curve raises worker compensation

on impact (and permanently as well) by about 1%, and raises employment on impact by

much less than 1% in equilibrium due to the limited short-run labor-supply elasticity. And

although we approached the data with an expectation that this would not have a permanent

eﬀect on employment, after seeing the data we would be persuaded that it does. An increase

in the number of people looking for work depresses labor compensation (upper right panel

of Figure 6), and raises employment over time.

   Figure 7 shows the consequences of putting diﬀerent weights on the prior belief about the

long-run labor-supply elasticity. The ﬁrst panel in the second row reproduces the impulse-

response function from the lower-left panel of Figure 6, while the second panel in that row

reproduces the prior and posterior distributions for the short-run labor supply elasticity (the

upper-right panel of Figure 5, drawn here on a diﬀerent scale for easier visual comparisons).

The ﬁrst row of Figure 7 shows the eﬀects of a weaker weight on the long-run restrictions,

V2 = 1.0, weighting the long-run belief as equivalent to only one observation rather than 10.

With weaker conﬁdence about the long-run eﬀect, we would not conclude that the short-run



                                              37
labor supply elasticity was so low or that the equilibrium eﬀects of a labor demand shock

on employment were as muted. The third and fourth rows show the eﬀects of using more

informative priors (V2 = 0.01 and V2 = 0.001, respectively). Even when we have a fairly

tight prior represented by V2 = 0.01, the data still lead us away from believing that the

long-run eﬀect of a labor demand shock is literally zero, and to reach that conclusion we

need to impute a very small value to the short-run labor supply elasticity as well.     Only

when we specify V2 = 0.001 is the long-run eﬀect pushed all the way to zero.

    This exercise demonstrates an important advantage of representing prior information in

the form proposed in Section 2.    In a typical frequentist approach, the restriction (56) is

viewed as necessary to arrive at a just-identiﬁed model and is therefore regarded as inher-

ently untestable.   By contrast, we have seen that if we instead regard it as one of a set

of nondogmatic prior beliefs, it is possible to examine what role the assumption plays in

determining the ﬁnal results and to assess its plausibility. Our conclusion from this exercise

is that it would not be a good idea to rely on exact satisfaction of (56) as the identifying

assumption for structural analysis. A combination of a weaker belief in the long-run impact

along with information from other sources about short-run impacts is a superior approach.


6     Conclusion.

Drawing structural inference from observed correlations requires making use of prior beliefs

about economic structure. In just-identiﬁed models, researchers usually proceed as if these

prior beliefs are known with certainty.    In vector autoregressions that are only partially



                                             38
identiﬁed using sign restrictions, the way that implicit prior beliefs inﬂuence the reported

results has not been recognized in the previous literature. In this paper we have explicated

the prior beliefs that are implicit in sign-restricted VARs and proposed a general Bayesian

framework that can be used to make optimal use of prior information and elucidate the

consequences of prior beliefs in any vector autoregression. Our suggestion is that explicitly

defending the prior information used in the analysis and reporting the way in which the

observed data causes these prior beliefs to be revised is superior to pretending that prior

information was not used and has no eﬀect on the reported conclusions.




                                             39
                                               Appendix

   A. Proof of Proposition 1.

   The likelihood (12) can be written


                                 −T n/2           T       −T /2
                                                                  n             (a′i yt − b′i xt−1 )2
                                                                                T
     p(YT |A, D, B) = (2π)                | det(A)| |D|               exp −                           .
                                                                  i=1       t=1         2dii

If we deﬁne X∗i = P′i and yi∗ = P′i mi , the prior for bi in (10) can be written

                                         1                 (y∗i − X∗i bi )′ (y∗i − X∗i bi )
               p(bi |D, A) =                         exp −                                  .
                                (2π)k/2 |dii Mi |1/2                   2dii

Comparing the above two equations we see that, conditional on A, prior information about

bi can be combined with the information in the data by regressing Ỹi on X̃i as represented

by the value of m∗i in equation (15). From the property that the OLS residuals Ỹi − X̃i m∗i

are orthogonal to X̃i , we further know


(Ỹ i − X̃i bi )′ (Ỹi − X̃i bi ) = (Ỹi − X̃i m∗i + X̃i m∗i − X̃i bi )′ (Ỹi − X̃i m∗i + X̃i m∗i − X̃i bi )

                                 = (Ỹi − X̃i m∗i )′ (Ỹi − X̃i m∗i ) + (bi − m∗i )′ X̃′i X̃i (bi − m∗i )

                                 = Ỹi′ Ỹi − Ỹi′ X̃i (X̃′i X̃i )−1 X̃′i Ỹi + (bi − m∗i )′ (M∗i )−1 (bi − m∗i )

                                 = ζ ∗i + (bi − m∗i )′ (M∗i )−1 (bi − m∗i ).


The product of the likelihood (12) with the prior for B (9) can thus be written


                p(B|A, D)p(YT |A, D, B) = (2π)−T n/2 | det(A)|T |D|−T /2 ×                                   (58)

                   n              1                 ζ ∗i + (bi − m∗i )′ (M∗i )−1 (bi − m∗i )
                   i=1                        exp −                                          .
                         (2π)k/2 |dii Mi |1/2                         2dii



                                                      40
Multiplying (58) by the priors for A and D and rearranging gives


p(YT , A, D, B) = p(A)p(D|A)p(B|A, D)p(YT |A, D, B)
                                                         n
                                                         τ κi i Γ(κ∗i ) (τ ∗i )κi −1 κi −1
                                                                                                 ∗
                                  −T n/2             T           −T /2
                  = p(A)(2π)     | det(A)|                      dii ∗ κ∗i      ∗
                                                                                 (dii )    exp(−τ ∗i d−1
                                                                                                      ii )×
                                              i=1
                                                        Γ(κi ) (τ i ) Γ(κi )
                                                                                           #
                       ∗ 1/2                                    ∗ ′       ∗ −1          ∗
                    |Mi |          1                 (bi − mi ) (Mi ) (bi − mi )
                          1/2  k/2      ∗ 1/2
                                               exp −
                     |Mi | (2π) |dii Mi |                             2dii
                  = p(A)(2π)−T n/2 | det(A)|T ×
                       n
                             |M∗i |1/2 τ κi i Γ(κ∗i )
                                                     ∗       γ(d−1    ∗     ∗          ∗        ∗
                                                                ii ; κi , τ i )φ(bi ; mi , dii Mi ).                 (59)
                       i=1
                             |Mi |1/2 Γ(κi ) (τ ∗i )κi

Note that the product in (59) can be interpreted as


                p(YT , A, D, B) = p(YT )p(A|YT )p(D|A, YT )p(B|A, D, Y T ).


Thus the posterior p(B|A, D, YT ) is the product of N(m∗i , dii M∗i ) densities, the posterior

p(D|A, YT ) the product of Γ(κ∗i , τ ∗i ) densities, and

                                                                      n
                                            −T n/2               T            |M∗i |1/2 τ κi i Γ(κ∗i )
           p(YT )p(A|YT ) = p(A)(2π)                 | det(A)|                                        ∗   .   (60)
                                                                      i=1
                                                                              |Mi |1/2 Γ(κi ) (τ ∗i )κi

The posterior p(A|Y T ) is thus proportional to (60). Since Ω̂T is not a function of A, we

can write multiply (60) by |Ω̂T |T /2 to write the result in an equivalent form that facilitates

numerical calculation and interpretation:

                                                                 n
                            p(A)[det(AΩ̂T A′ )]T /2                       |M∗i |1/2 τ κi i
                 p(A|YT ) ∝     n               κ∗i
                                                                                           Γ(κ∗i )
                                         ∗
                                i=1 [(2τ i /T )]                i=1
                                                                          |Mi |1/2 Γ(κi )

as claimed in equation (20).       Note that we could replace Ω̂T in the numerator with any

matrix not depending on unknown parameters, with any such replacement simply changing

the deﬁnition of kT in (20). Our use of Ω̂T in the numerator helps the target density (which

                                                      41
omits kT ) behave better numerically for large T , as will be seen in the asymptotic analysis

below.

      B. Numerical algorithm for drawing from the posterior distribution in Propo-

sition 1.16

      We use a random-walk Metropolis Hastings algorithm to draw from the posterior distri-

bution of A and use the known closed-form expressions to generate draws from D|A, Y T

and B|A, D, YT . Deﬁne the target function to be


  q(A) = log p(A) + (T /2) log[det(AΩ̂T A′ )]                                                                (61)
                    $n                                                                 n
                −             (κi (A) + T /2) log{[2τ i (A)/T ] + [ζ ∗i (A)/T ]} +     i=1   κi (A) log τ i (A)
                        i=1

                       n
                +      i=1   {(1/2) log [|M∗i (A)|/|Mi (A)|] + log [Γ(κi (A) + T /2)/Γ(κi (A)]} .


If κi and Mi do not depend on A (as in the priors recommended in Appendix D) the last

term in (61) can be dropped. Those priors specify τ i (A) = κi a′i Sai and ζ ∗i (A) = a′i Wai

where

                 T
      W =        t=1   yt yt′ + ηM−1 η ′ −
                                                                            −1
                    T                              T                             T
                    t=1   yt x′t−1 + ηM−1          t=1   xt−1 x′t−1 + M−1        t=1   xt−1 yt′ + M−1 η ′


for M the diagonal matrix given in (65) and η the matrix in (43). For more general priors,

ζ ∗i (A) can be calculated from (19).

      As a ﬁrst step we calculate an approximation to the shape of the posterior distribution

that will help the numerical components of the algorithm be more eﬃcient. Collect elements

 16
      Code to implement this procedure is available at http://econweb.ucsd.edu/~jhamilton/BHcode.zip.

                                                          42
of A that are not known with certainty in an (nα × 1) vector α, and ﬁnd the value α̂ that

maximizes (61) numerically. This value α̂ oﬀers a reasonable guess for the posterior mean

of α, while the matrix of second derivatives (again obtained numerically) gives an idea of

the scale of the posterior distribution:
                                                       %
                                          ∂ 2 q(A(α)) %%
                                     Λ̂ =                .
                                             ∂α∂α′ %α=α̂

   We then use this guess to inform a random-walk Metropolis Hastings algorithm to gen-

erate candidate draws of α from the posterior distribution, as follows. We can begin the

algorithm at step 1 by setting α(1) = α̂. As a result of step ℓ we have generated a value of

α(ℓ) . For step ℓ + 1 we generate

                                                             ′
                                   α̃(ℓ+1) = α(ℓ) + ξ P̂−1
                                                        Λ        vt


for vt an (nα × 1) vector of Student t variables with 2 degrees of freedom, P̂Λ the Cholesky

factor of Λ̂ (namely P̂Λ P̂′Λ = Λ̂ with P̂Λ lower triangular), and ξ a tuning scalar to be

described shortly.    If q(A(α̃(ℓ+1) )) < q(A(α(ℓ) )), we set α(ℓ+1) = α(ℓ) with probability

1 − exp q(A(α̃(ℓ+1) )) − q(A(α(ℓ) )) ; otherwise, we set α(ℓ+1) = α̃(ℓ+1) . The parameter ξ is

chosen so that about 30% of the newly generated α̃(ℓ+1) get retained. The values after the

ﬁrst D burn-in draws {α(D+1) , α(D+2) , ..., α(D+N) } represent a sample of size N drawn from

the posterior distribution p(α, D, B|Y T ); in our applications we have used D = N = 106 .
                                                                       (ℓ)
   For each of these N ﬁnal values for α(ℓ) we further generate δ ii ∼ Γ(κ∗i (A(α(ℓ) )), τ ∗i (A(α(ℓ) )))

for i = 1, ..., n and take D(ℓ) to be a diagonal matrix whose row i, column i element is given by
   (ℓ)                                     (ℓ)                        (ℓ)
1/δii . From these we also generate bi ∼ N (m∗i (A(α(ℓ) )), dii M∗i (A(α(ℓ) ))) for i = 1, ..., n

                                                 43
                                                                          (ℓ)′
and take B(ℓ) the matrix whose ith row is given by bi . The triple {A(α(ℓ) ), D(ℓ) , B(ℓ) }D+N
                                                                                           ℓ=D+1


then represents a sample of size N drawn from the posterior distribution p(A, D, B|YT ).

   C. Proof of Proposition 2.



                                                                                                              −1
                                                                                 T
(i) E[(bi − m∗i )(bi − m∗i )′ |A,d11 , ..., dnn , YT ] =                         t=1   xt−1 x′t−1 + M−1
                                                                                                     i

                                                                                                                             −1
                                                                                               T
                                                                   = T −1 T −1                 t=1   xt−1 x′t−1 + T −1 M−1
                                                                                                                        i

                                                                            p
                                                                           →0


and

                                                    −1
                         T                                     T
      m∗i =              t=1   xt−1 x′t−1 + M−1
                                             i                 t=1   xt−1 yt′ ai + M−1
                                                                                    i mi

                                                                −1
                                T                                                  T
            =    T −1           t=1   xt−1 x′t−1 + T −1 M−1
                                                         i                T −1     t=1   xt−1 yt′ ai + T −1 M−1
                                                                                                             i mi

                 p
                → Φ′0 ai .


Hence                                                         
                                                        b′1
                                                         
                                                        
                                                    .  p
                                                   
                                               B =  ..  → AΦ0 .
                                                        
                                                        
                                                     b′n



                                T                                                            T
(ii) ζ ∗i /T = T −1             t=1   a′i yt yt′ ai + T −1 m′i M−1
                                                                i mi − T
                                                                         −1
                                                                                             t=1   a′i yt x′t−1 + T −1 m′i M−1
                                                                                                                            i  ×
                                                                     −1
                                 T                                                     T
                     T −1        t=1   xt−1 x′t−1 + T −1 M−1
                                                          i                 T −1       t=1   xt−1 yt′ ai + T −1 M−1
                                                                                                                 i mi

                     p
                  → a′i Ω0 ai .



                                                           44
                                        ∗ 2
                   (iii) E[(d−1    ∗                 ∗    ∗2
                             ii − κi /τ i ) |YT ] = κi /τ i

                                                           κi + (T /2)
                                                       =
                                                         [τ i + (ζ ∗i /2)]2
                                                              (κi /T ) + (1/2)
                                                       =
                                                         T [(τ i /T ) + (ζ ∗i /2T )]2
                                                            p
                                                           →0


and
                                          κ∗i p (1/2)
                                               → ′          .
                                          τ ∗i  ai Ω0 ai /2

   (iv) We ﬁrst demonstrate that


                           Prob{[A ∈
                                   / Hδ (Ω̂T )]|YT } → 0             ∀δ > 0.                 (62)


To see this, let pij (A, Ω) denote the row i, column j element of P(A, Ω) for P(A, Ω) the

lower-triangular Cholesky factor P(A, Ω)[P(A, Ω)]′ = AΩA′ . Note that


                          |AΩA′ | = p211 (A, Ω)p222 (A, Ω) · · · p2nn (A, Ω)


                       a′i Ωai = p2i1 (A, Ω) + p2i2 (A, Ω) + · · · + p2ii (A, Ω).

Furthermore, ζ ∗i , the sum of squared residuals from a regression of Ỹi on X̃i , by construction

is larger than T a′i Ω̂T ai , the SSR from a regression of a′i yt on xt−1 . Thus


                                (2τ ∗i /T ) = (2τ i /T ) + ζ ∗i /T

                                            ≥ a′i Ω̂T ai

                                                   i     2            2
                                            =      j=1 [pij (A, Ω̂T )] .


                                                  45
                                                                                         2
              / Hδ (Ω̂T ), ∃ j ∗ < i∗ such that pi∗ j ∗ (A, Ω̂T )
But for all A ∈                                                                               > δ ∗ for δ ∗ = 2δ/[n(n − 1)]
                                                                                    n
meaning a′i∗ Ω̂T ai∗ > [δ ∗ +pi∗ i∗ (A, Ω̂T )]2 for some i∗ and                             ∗
                                                                                    i=1 (2τ i /T )      > [p11 (A, Ω̂T ]2 · · · [δ ∗ +

pi∗ i∗ (A, Ω̂T )]2 · · · [pnn (A, Ω̂T ]2 . Thus
                                    &
                                                      kT p(A)[p211 (A, Ω̂T )p222 (A, Ω̂T ) · · · p2nn (A, Ω̂T )]T /2
  Prob[A ∈
         / Hδ (Ω̂T )|YT ] =                                   n
                                                                                                                               dA
                                         / δ (Ω̂T )
                                        A∈H                   i=1 [(2τ i /T )   + ζ ∗i /T ]κi [(2τ i /T ) + ζ ∗i /T ]T /2

                         &
                                                     kT p(A)
                  <                         n                    ∗      κi
                                                                           ×
                           / δ (Ω̂T )
                          A∈H               i=1 [(2τ i /T ) + (ζ i /T )]

                                  [p211 (A, Ω̂T )p222 (A, Ω̂T ) · · · p2nn (A, Ω̂T )]T /2
                                                                                                      dA
                         {[p211 (A, Ω̂T )] · · · [δ ∗ + p2i∗ i∗ (A, Ω̂T )] · · · [p2nn (A, Ω̂T ]}T /2

                     &                                                                                      T /2
                                               kT p(A)                          p2i∗ i∗ (A, Ω̂T )
                 =                      n                   ∗     κi
                                                                                                                   dA
                       / δ (Ω̂T )
                      A∈H               i=1 [(2τ i /T ) + ζ i /T ]         δ ∗ + p2i∗ i∗ (A, Ω̂T )

which goes to 0 as T → ∞.

    Note next that

                                                                         n        i−1
                Prob{[A ∈
                        / Hδ (Ω0 )]|YT } = Prob                          i=2      j=1   [pij (A, Ω0 )]2 > δ .


But

                                                                                                               2
                [pij (A, Ω0 )]2 =           pij (A, Ω̂T ) + pij (A, Ω0 ) − pij (A, Ω̂T )
                                                                 2                                                  2
                                    ≤ 2 pij (A, Ω̂T )                + 2 pij (A, Ω0 ) − pij (A, Ω̂T )                   .


Hence

                      Prob{[A ∈
                              / Hδ (Ω0 )]|YT } ≤ Prob{[(A1T + A2T ) > δ]|YT }

                                                                                          2
                                                        n        i−1
                                        A1T = 2         i=2      j=1    pij (A, Ω̂T )

                                                                                                    2
                                             n        i−1
                             A2T = 2         i=2      j=1     pij (A, Ω0 ) − pij (A, Ω̂T )              .

                                                                46
Given any ε > 0 and δ > 0, by virtue of (62) and result (ii) of Proposition 2, there exists a

T0 such that Prob{[A1T > δ/2]|YT } < ε/2 and Prob{[A2T > δ/2]|YT } < ε/2 for all T ≥ T0 ,

establishing that Prob{[(A1T + A2T ) > δ]|YT } < ε as claimed.

   (v) When κi = τ i = 0 and Mi = 0, we have ζ ∗i = T a′i Ω̂T ai and

                                           kT p(A)[det(AΩ̂T A′ )]T /2
                            p(A|Y T ) =           n     ′         T /2
                                                  i=1 [ai Ω̂T ai ]


which equals kT p(A) when evaluated at any A for which AΩ̂T A′ is diagonal.

   D. Suggested standard priors for D and B.

   Prior beliefs about structural variances should reﬂect in part the scale of the underlying

data. Let êit denote the residual of a mth-order univariate autoregression ﬁt to series i and S
                                                                          T
the sample variance matrix of these univariate residuals (sij = T −1      t=1 êit êjt ).   We propose

setting κi /τ i (the prior mean for d−1
                                     ii ) equal to the reciprocal of the ith diagonal element of


ASA′ . Given equation (17), the prior is given a weight equivalent to 2κi observations of

data; for example, setting κi = 2 (as is done in the empirical illustration in Section 5) would

give the prior as much weight as 4 observations.

   Doan, Litterman and Sims (1984) suggested that we should have greater conﬁdence in

our expectation that coeﬃcients on higher lags are zero, represented by smaller diagonal
                                                             √
elements for Mi associated with higher lags.           Let    sjj denote the estimated standard

deviation of a univariate mth-order autoregression ﬁt to variable j. Deﬁne

                                 '       (
                            v1′ = 1/(12λ1 , 1/(22λ1 ), ..., 1/(m2λ1 ))                             (63)
                          (1×m)


                                    v2′ = (s−1    −1         −1 ′
                                            11 , s22 , ..., snn )
                                   (1×n)


                                                47
                                                         
                                              v1 ⊗ v2 
                                    v3 = λ20 
                                             
                                                       .
                                                                                           (64)
                                                  λ23

Then Mi is taken to be a diagonal matrix whose row r column r element is the rth element

of v3 :

                                          Mi,rr = v3r .                                     (65)

Here λ0 summarizes the overall conﬁdence in the prior (with smaller λ0 corresponding to

greater weight given to the random walk expectation), λ1 governs how much more conﬁdent

we are that higher coeﬃcients are zero (with a value of λ1 = 0 giving all lags equal weight),

and λ3 is a separate parameter governing the tightness of the prior for the constant term,

with all λk ≥ 0.

    Doan (2013) discussed possible values for these parameters. In the application in Section

5 we set λ1 = 1 (which governs how quickly the prior for lagged coeﬃcients tightens to zero

as the lag ℓ increases), λ3 = 100 (which makes the prior on the constant term essentially

irrelevant), and set λ0 , the parameter controlling the overall tightness of the prior, to 0.2.

    E. Algorithm using the uniform Haar prior.

    Here we describe the sign-restriction algorithm developed by Rubio-Ramírez, Waggoner,

and Zha (2010) that was used to generate the histograms in Figure 2 and verify directly the

invariance claim used to produce equation (33).

    Let X denote an n × n matrix whose elements are random draws from independent

standard Normal distributions. Take the QR decomposition of X such that X = QR where

R is an upper triangular matrix whose diagonal elements have been normalized to be positive


                                                 48
and Q is an orthogonal matrix (QQ′ = In ). Let P be the Cholesky factor of the reduced-

form variance-covariance matrix Ω (so that Ω = PP′ ) and generate a candidate impact

matrix H̃ = PQ. Instead of checking the sign restrictions directly for H̃, normalize H̃ by

dividing each column by its ﬁrst element as a way to account for both positive and negative

shocks which increases the eﬃciency of the algorithm. Given that in sign-identiﬁed VARs

the ordering of the variables does not determine which shock is contained in which column,

each column needs to be checked for the sign pattern associated with one particular shock.

If the normalized H̃ satisﬁes all the sign restrictions jointly, keep the draw; otherwise discard

it.

      To verify the invariance of the priors for impulse-response coeﬃcients that are implicit

in this algorithm in the absence of sign restrictions or other identifying assumptions, note

from (28) that


                             hi1 = pi1 q11 + pi2 q21 + · · · + pii qi1
                                       pi1 x11 + pi2 x21 + · · · + pii xi1
                                  =                                          .
                                             x211 + x221 + · · · + x2n1

The numerator of hi1 is a zero-mean Normal variable with variance p2i1 + p2i2 + · · · + p2ii = ω ii
                                                       √
and therefore the numerator can be written as           ω ii vi1 where vi1 ∼ N (0, 1) for

                                       pi1 x11 + pi2 x21 + · · · + pii xi1
                              vi1 =                  √
                                                       ω ii
                                  = α′i x1


for x1 the ﬁrst column of X and α′i a vector with the property that α′i αi = 1.



                                                  49
   If we then consider the square of the impact,

                                                            2
                                                      ω ii vi1
                                   h2i1 =                              ,
                                            x211 + x221 + · · · + x2n1

the claim is that this is ω ii times a Beta(1/2,(n − 1)/2) random variable. This would be

the case provided that we can write the denominator as


                          x211 + x221 + · · · + x2n1 = v11
                                                        2     2
                                                           + v21            2
                                                                 + · · · + vn1


where (v11 , ...., vn1 ) are independent N (0, 1) and the ith term vi1 is the same term as appears

in the numerator. That this is indeed the case can be veriﬁed by noting that since x1 ∼

N (0, In ), it is also the case that v1 ∼ N (0, In ) when v1 = αx1 and α is any orthogonal

(n × n) matrix.      We can start with any arbitrary unit-length vector α′i for the ith row

of α and always ﬁll in the other rows {α′1 , α′2 , ..., α′i−1 , α′i+1 , ..., α′n } so as to ﬁnd such an

orthogonal matrix α. Then v1′ v1 = x′1 α′ αx1 = x′1 x1 as claimed.




                                                    50
                                     References

   Akerlof, George A., and William T. Dickens (2007). "Unﬁnished Business in the Macro-

economics of Low Inﬂation: A Tribute to George and Bill by Bill and George," Brookings

Papers on Economic Activity 2: 31-47.

   Arias, Jonas E., Juan F. Rubio-Ramírez, and Daniel F. Waggoner (2013). "Algorithm

for Inference with Sign and Zero Restrictions," working paper, Duke University.

   Blanchard, Olivier J. and Peter Diamond (1990). "The Cyclical Behavior of Gross Flows

of Workers in the United States," Brookings Papers on Economic Activity 1990(2): 85-155.

   Blanchard, Olivier J., and Danny Quah (1989). "The Dynamic Eﬀects of Aggregate

Demand and Supply Disturbances," American Economic Review 79: 655-673.

   Caldara, Dario and Christophe Kamps (2012). "The Analytics of SVARs: A Uniﬁed

Framework to Measure Fiscal Multipliers," working paper, Federal Reserve Board.

   Canova, Fabio, and Gianni De Nicoló (2002). "Monetary Disturbances Matter for Busi-

ness Fluctuations in the G-7," Journal of Monetary Economics 49: 1131-1159.

   Canova, Fabio, and Matthias Paustian (2011). "Business Cycle Measurement with Some

Theory," Journal of Monetary Economics 58: 345-361.

   Chetty, Raj, Adam Guren, Day Manoli, and Andrea Weber (2013). "Does Indivisible

Labor Explain the Diﬀerence between Micro and Macro Elasticities? A Meta-Analysis of

Extensive Margin Elasticities," NBER Macroeconomics Annual, Volume 27, pp. 1-55, edited

by Daron Acemoglu, Jonathan Parker, and Michael Woodford. Chicago, IL: University of

Chicago Press.


                                           51
   Cho, Jang-Ok, and Thomas F. Cooley (1994). "Employment and Hours over the Business

Cycle," Journal of Economic Dynamics and Control 18(2): 411-432.

   Davis, Steven J. and John Haltiwanger (1999). "On the Driving Forces Behind Cyclical

Movements in Employment and Job Reallocation," American Economic Review 89: 1234-

1258.

   Doan, Thomas, Robert B. Litterman, and Christopher A. Sims (1984). "Forecasting and

Conditional Projection Using Realistic Prior Distributions," Econometric Reviews 3: 1-100.

   Doan, Thomas (2013). RATS User’s Guide, Version 8.2, www.estima.com.

   Devroye, Luc (1986). Non-Uniform Random Variate Generation. Springer-Verlag.

   Drèze, Jacques H. (1974). "Bayesian Theory of Identiﬁcation in Simultaneous Equations

Models", in Studies in Bayesian Econometrics and Statistics, edited by S.E. Fienberg and

A. Zellner. Amsterdam: North-Holland.

   Faust, Jon (1998). "The Robustness of Identiﬁed VAR Conclusions about Money,"

Carnegie-Rochester Series on Public Policy 49: 207-244.

   Faust, Jon and Eric M. Leeper (1997). "When Do Long-Run Identifying Restrictions

Give Reliable Results?," Journal of Business & Economic Statistics 15: 345-353.

   Fry, Renée, and Adrian Pagan (2011). "Sign Restrictions in Structural Vector Autore-

gressions: A Critical Review," Journal of Economic Literature 49(4): 938-960.

   Galí, Jordi (1999). "Technology, Employment, and the Business Cycle: Do Technology

Shocks Explain Aggregate Fluctuations?", American Economic Review 89: 249-271.




                                           52
   Galí, Jordi, Frank Smets, and Rafael Wouters (2012). "Unemployment in an Estimated

New Keynesian Model," NBER Macroeconomics Annual, Volume 26, pp. 329-360, edited by

Daron Acemoglu, and Michael Woodford. Chicago, IL: University of Chicago Press.

   Giacomini, Raﬀaella, and Toru Kitagawa (2013). "Robust Bayes Inference in Non-

Identiﬁed SVARs," working paper, University College London.

   Gubner, John A. (2006). Probability and Random Processes for Electrical and Computer

Engineers. Cambridge: Cambridge University Press.

   Gustafson, Paul (2009). "What Are the Limits of Posterior Distributions Arising From

Nonidentiﬁed Models, and Why Should We Care?", Journal of the American Statistical

Asociation 104: 1682-1695.

   Haar, Alfred (1933). "Der Massbegriﬀ in der Theorie der kontinuierlichen Gruppen",

Annals of Mathematics second series 34(1): 147—169.

   Hamermesh, Daniel S. (1996). Labor Demand. Princeton: Princeton University Press.

   Hamilton, James D. (1994). Time Series Analysis. Princeton: Princeton University

Press.

   Kilian, Lutz, and Daniel P. Murphy (2012). "Why Agnostic Sign Restrictions Are Not

Enough: Understanding the Dynamics of Oil Market VAR Models," Journal of the European

Economic Association 10(5): 1166-1188.

   Kydland, Finn E., and Edward C. Prescott (1982). "Time to Build and Aggregate

Fluctuations," Econometrica 50(6): 1345-1370.




                                          53
   Leamer, Edward E. (1978). Speciﬁcation Searches: Ad Hoc Inference with Nonexperi-

mental Data. New York: John Wiley & Sons, Inc.

   Leamer, Edward E. (1981). "Is it a Demand Curve, Or Is It A Supply Curve? Partial

Identiﬁcation through Inequality Constraints," Review of Economics and Statistics 63: 319-

327.

   Lichter, Andreas, Andreas Peichl, and Sebastian Siegloch (2014). "The Own-Wage Elas-

ticity of Labor Demand: A Meta-Regression Analysis," working paper, Institute for the

Study of Labor.

   Litterman, Robert B. (1986). "Forecasting With Bayesian Vector Autoregressions— Five

Years of Experience," Journal of Business & Economic Statistics 4:25-38.

   Manski, Charles F. (2003). Partial Identiﬁcation of Probability Distributions, New York:

Springer.

   Moon, Hyungsik Roger, and Frank Schorfheide (2012). "Bayesian and Frequentist Infer-

ence in Partially Identiﬁed Models," Econometrica 80: 755-782.

   Moon, Hyungsik Roger, Frank Schorfheide, and Eleonora Granziera (2013). "Inference

for VARs Identiﬁed with Sign Restrictions", working paper, University of Pennsylvania.

   Poirier, Dale J. (1998). "Revising Beliefs in Nonidentiﬁed Models," Econometric Theory,

14: 483-509.

   Reichling, Felix, and Charles Whalen (2012). "Review of Estimates of the Frisch Elas-

ticity of Labor Supply," working paper, Congressional Budget Oﬃce.




                                            54
   Rubio-Ramírez, Juan, Daniel F. Waggoner, and Tao Zha (2010). "Structural Vector

Autoregressions: Theory of Identiﬁcation and Algorithms for Inference", Review of Economic

Studies, 77(2): 665-696.

   Shapiro, Matthew D., and Mark W. Watson (1988). "Sources of Business Cycle Fluctu-

ations," NBER Macroeconomics Annual, Volume 3, pp. 111-156, edited by Stanley Fischer.

Cambridge, MA: M.I.T. Press.

   Sims, Christopher A., and Tao Zha (1998). "Bayesian Methods for Dynamic Multivariate

Models," International Economic Review 39: 949-968.

   Tamer, Elie (2010). "Partial Identiﬁcation in Econometrics," Annual Review of Eco-

nomics 2: 167-195.

   Theil, Henri (1971). Principles of Econometrics. New York: John Wiley & Sons, Inc.

   Uhlig, Harald (2005). "What are the Eﬀects of Monetary Policy on Output? Results

from an Agnostic Identiﬁcation Procedure," Journal of Monetary Economics 52: 381—419.




                                           55
                                                    Panel A
                 3



                 2

                                                                                  n=2

                 1                                    n=6



                 0
                            -ω1/2                       0                        1/2
                                                                               ωii          h ij
                               ii

                                                    Panel B
               0.4


               0.3


               0.2


               0.1


                 0
                                                     ωij / ωjj                               *
                                                                                           h ij



Figure 1. Prior densities for initial effect of shocks implicit in traditional approach to sign-restricted VAR.
Panel A: response of variable i to one-standard-deviation increase of any structural shock when number
of variables in VAR is 2 (solid blue) or 6 (dashed green). Panel B: response of variable i to a structural
shock that increases variable j by one unit.




                                                      56
Figure 2. Effects of structural shocks as imputed by the traditional sign-restriction methodology (blue)
and truncated priors implicit in that approach (red). Top panel: short-run wage elasticity of labor
demand; bottom panel: short-run wage elasticity of labor supply.




                                                    57
                                               The function β(α )
       5




                                                             hL                            hH

       0
   β




       -5
        -5      -4      -3       -2       -1             0          1   2     3        4        5
                                                         α

                                         Contours for log likelihood
       5




                                                                                           hH
   β




       0




       -5
        -5      -4      -3       -2       -1             0          1   2     3        4        5

                                                       α

Figure 3. Maximum likelihood estimates and likelihood contours for α and β. Distance between contour
lines is 10, and unshaded regions are exceedingly unlikely given the data.




                                                    58
                                        Contours for log of prior
            0

           -1

           -2
      β




           -3

           -4

           -5
                            1                2               3                4               5
                                                     α
                                      Contours for log of posterior
            0

           -1

           -2
      β




           -3

           -4

           -5
                            1                2               3                4               5
                                                     α

Figure 4. Maximum likelihood estimates and contours of prior and posterior distribution. Distance
between contour lines is 2, and unshaded regions are exceedingly unlikely given prior beliefs and the
data.




                                                   59
                                       d                                         s
                                   β                                         α
            1
                                                                6


                                                                4
           0.5

                                                                2


            0                                                   0
             -3             -2              -1         0            0    1           2           3


                       s
                   α       + b s + b s + ... + b s
                              11       21         m1
            6



            4



            2



             0
            -0.5             0              0.5        1




Figure 5. Distributions for prior (red curves) and posterior (blue histograms). Panel A: short-run
elasticity of labor demand; panel B: short-run elasticity of labor supply; panel C: long-run labor-supply
parameter (negative of lower left element of the matrix in equation (55)).




                                                           60
                         Labor Demand Shock                                    Labor Supply Shock

                              Real wage                                            Real wage
                 3                                                    1

                                                                      0
                 2
       Percent




                                                          Percent
                                                                      -1

                 1
                                                                      -2

                 0                                                    -3
                     0    5       10       15   20                         0   5      10        15   20
                                Quarters                                            Quarters


                              Employment                                           Employment
                 6                                                    6
                 5                                                    5

                 4                                                    4
       Percent




                                                            Percent



                 3                                                    3

                 2                                                    2
                 1                                                    1

                 0                                                    0
                     0    5       10       15   20                         0   5      10        15   20
                                Quarters                                            Quarters



Figure 6. Posterior median (blue line) and 95% posterior credibility sets (shaded regions) for effects of
labor demand and supply shocks on levels of employment and the real wage at alternative horizons (V =
0.1).




                                                     61
                 Response of employment to labor demand shock                           αs

                                    V=1                                                V=1
            6
                                                                      10
 Percent




            4

            2                                                         5

            0                                                         0
             0             5          10         15         20         0   0.2   0.4             0.6   0.8   1

                                   V = 0.1                                             V = 0.1
            6
                                                                      10
            4
  Percent




            2                                                         5


            0                                                         0
             0             5          10         15         20         0   0.2   0.4             0.6   0.8   1

                                   V = 0.01                                        V = 0.01
            6
                                                                      10
 Percent




            4

            2                                                         5

            0                                                         0
             0             5          10         15         20         0   0.2   0.4             0.6   0.8   1

                                  V = 0.001                                       V = 0.001
            6
                                                                      10
 Percent




            4

            2                                                         5

            0                                                         0
             0             5          10         15         20         0   0.2   0.4             0.6   0.8   1
                                   Quarters



Figure 7. Effect of stronger priors about the long-run labor supply elasticity. Left column: response of
employment to a labor demand shock for prior weights given by V = 1, 0.1, 0.01, or 0.001; right column:
distribution for short-run labor supply elasticity for prior (red curve) and posterior (blue histograms)
associated with different values of V.




                                                                 62
