                              NBER WORKING PAPER SERIES




                        IS ATTENTION PRODUCED RATIONALLY?

                                       Erin T. Bronchetti
                                        Judd B. Kessler
                                      Ellen B. Magenheim
                                       Dmitry Taubinsky
                                           Eric Zwick

                                      Working Paper 27443
                              http://www.nber.org/papers/w27443


                     NATIONAL BUREAU OF ECONOMIC RESEARCH
                              1050 Massachusetts Avenue
                                Cambridge, MA 02138
                                     June 2020




We thank Stephen O'Connell, Devin Pope, Andrew Caplin, Mark Dean, three grant reviewers at
the Russell Sage Foundation, and seminar participants for helpful comments and advice. We
thank Alexander Hirsch and Caleb Wroblewski for excellent research assistance. We gratefully
acknowledge Mike Walmsley and CodeAvengers.com for their support with the education
experiment. We gratefully acknowledge research funding from the Russell Sage Foundation,
Swarthmore College, the Wharton School of the University of Pennsylvania, and the Wharton
Behavioral Lab. The first experiment was approved by the Swarthmore (covering Haverford and
Muhlenberg), Bryn Mawr, Lafayette, and Ursinus IRBs, numbers: 14-15-065, R17-042,
AY1617-12, 01-18-17. The second experiment was approved by the University of Pennsylvania
IRB, number 832335. The opinions expressed in this paper are solely the authors', and do not
necessarily reflect the views of any individual or institution listed above, nor of the National
Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.

© 2020 by Erin T. Bronchetti, Judd B. Kessler, Ellen B. Magenheim, Dmitry Taubinsky, and Eric
Zwick. All rights reserved. Short sections of text, not to exceed two paragraphs, may be quoted
without explicit permission provided that full credit, including © notice, is given to the source.
Is Attention Produced Rationally?
Erin T. Bronchetti, Judd B. Kessler, Ellen B. Magenheim, Dmitry Taubinsky, and Eric Zwick
NBER Working Paper No. 27443
June 2020
JEL No. C91,C93,D91

                                           ABSTRACT

A large and growing literature shows that attention-increasing interventions, such as reminders
and planning prompts, can promote important behaviors. This paper develops a method to
investigate whether people value attention-increasing tools rationally. We characterize how the
demand for attention improvements must vary with the pecuniary incentive to be attentive and
develop quantitative tests of rational inattention that we deploy in two experiments. The first is an
experiment with an online education platform run in the field (n=1,373), in which we randomize
incentives to complete course modules and incentives to make plans to complete the modules.
The second is an online survey-completion experiment (n=944), in which we randomize
incentives to complete a survey three weeks later and the price of reminders to complete the
survey. In both experiments, as incentives to complete a task increase, demand for attention-
improving technologies also increases. However, our tests suggest that the increase in demand for
attention improvements is too small relative to the null of full rationality, indicating that people
underuse attention-increasing tools. In our second experiment, we estimate that individuals
undervalue the benefits of reminders by 59%.

Erin T. Bronchetti                                Dmitry Taubinsky
Department of Economics                           University of California, Berkeley
Swarthmore College                                Department of Economics
500 College Avenue                                530 Evans Hall #3880
Swarthmore, PA 19081-1397                         Berkeley, CA 94720-3880
ebronch1@swarthmore.edu                           and NBER
                                                  dmitry.taubinsky@berkeley.edu
Judd B. Kessler
The Wharton School                                Eric Zwick
University of Pennsylvania                        Booth School of Business
3620 Locust Walk                                  University of Chicago
Philadelphia, PA 19104                            5807 South Woodlawn Avenue
and NBER                                          Chicago, IL 60637
judd.kessler@wharton.upenn.edu                    and NBER
                                                  ezwick@chicagobooth.edu
Ellen B. Magenheim
Swarthmore College
Department of Economics
500 College Avenue
Swarthmore, PA 19081-1397
emagenh1@swarthmore.edu
1       Introduction
A large and rapidly growing body of work shows that provision of ostensibly small "nudges"
to people's attention--such as reminders and planning prompts--can have significant effects
on behaviors in economically important domains such as medical compliance, educational
attainment, savings, loan repayment, voting, and charitable donation.1 How can such simple
interventions have such significant effects? After all, the market already provides individuals
with many opportunities to acquire reminder technologies and plan-making tools in the
form of various smartphone and computer applications, online calendars, smart-caps on pill
bottles, and many others. If these various attention-increasing tools were indeed valuable
to individuals, wouldn't individuals already utilize them, and therefore not be affected by
external provision of additional attention nudges?
    One possibility is that individuals can perfectly forecast their attention levels with and
without various attention-increasing tools--i.e., their attention production function --but
that the costs of the tools exceed the benefits. In addition to any pecuniary costs, re-
minders and plan-making tools may carry private nuisance costs (see, e.g., Damgaard and
Gravert, 2018), time costs, or detract scarce attention from other important tasks (Nafziger,
forthcoming). Under this "rational inattention" hypothesis (for reviews, see Caplin, 2016;
Ma´ ckowiak et al., 2018; Gabaix, 2019), external interventions may be suboptimal as individ-
uals are already equating marginal benefits to marginal costs in setting their optimal level
of attention.2
    Another possibility is that individuals misunderstand their attention production functions
and, in particular, do not appreciate the value of creating reminders and implementation
plans. For example, individuals who are overconfident about their future attentiveness may
systematically underestimate the value of reminders. In this case, despite a rich offering of
attention enhancements by the market, there is still a market failure because individuals'
demand for attention-increasing technologies will be below the optimum. In a world with
such market failures, promoting take-up of attention-improving technologies could increase
efficiency and welfare, as it would lead to investment in attention improvements that are
closer to individuals' optima.
    1
     See, e.g., Nickerson and Rogers (2010); Cadena and Schoar (2011); Milkman et al. (2011, 2013); Altmann
and Traxler (2014); Castleman and Page (2016); Bronchetti et al. (2015); Karlan et al. (2016); Calzolari and
Nardotto (2017); Damgaard and Gravert (2018); Marx and Turner (2019); and see Rogers et al. (2015) for
a review. But see also Carrera et al. (2018) and Oreopoulos et al. (forthcoming) for examples of null effects.
   2
     Important exceptions include situations in which it is cheaper for a policymaker to create a reminder or
plan-making prompt than it is for individuals themselves and situations in which the behavior under con-
sideration generates positive externalities so that the private optimum is different from the social optimum.
But even with these nuances, whether individuals' investment in attention-improving technologies is at or
below their private optima is a crucial question for cost-benefit analysis.


                                                      2
    This paper develops a method to investigate whether individuals rationally value attention-
improving technologies. We deploy this method in two complementary experiments.
    Our method builds on the Caplin et al. (forthcominga) characterization of costly attention
models using a competitive supply framework. The key insight behind the testable predic-
tions of full rationality that we develop is to examine not only how the attention-increasing
technologies increase the likelihood of completing the task, but also how pecuniary incen-
tives to complete the task affect individuals' propensity to obtain the attention-increasing
technologies.
    More specifically, the first prediction is a precise condition on how individuals' willingness
to pay for the attention-increasing technology changes with the pecuniary rewards for the
task. The second prediction is a form of a Slutsky Symmetry condition, which states that
the impact of task-completion incentives on take-up of the attention-improving technology
is equal to the impact of the price of the attention-increasing technology on the propensity
to complete the task.3
    Guided by this framework, we carry out two experiments. The first is an experiment run
in the field with 1,373 students and alumni from six Philadelphia-area colleges who enrolled
in an 8-week online coding course.4 The experiment randomized incentives to complete three
15-minute coding lessons each week and randomized incentives to make a plan to complete
three 15-minute course modules each week. Making a plan involved clicking a link that
automatically created three 15-minute events in the participant's online calendar of choice
for the following week and allowed the participant to rearrange the planned events to suit
their schedule.
    We document three key facts in our first experiment. First, use of our plan-making
tool increased the likelihood of completing coding lessons, especially in the initial weeks.5
Second, take-up of our plan-making tool was elastic to the direct incentives for plan-making,
but remained below 100 percent, even with the incentives. The combination of incomplete
take-up and the positive elasticity suggests that the use of our tool imposes internal or
"nuisance" costs on at least some individuals. Third, we find that take-up of our plan-
making tool increased with incentives for completing coding lessons.
    This third result is consistent with the qualitative rational inattention prediction that
individuals should value attention-improving technologies more when the rewards for com-
   3
      See Gabaix (2019) for a discussion about testing Slutsky Symmetry as an empirical strategy for testing
limited attention.
    4
      Setting our experiment in an educational domain has the appeal that it requires a repeated investment
of time, involves an intrinsic reward, and participants may fail to follow through on their intentions despite
being highly motivated.
    5
      Because online calendars typically have built-in reminder features for planned events, the plan-making
tool can be thought of as providing a combined plan and reminder.


                                                      3
pleting a task are higher. At the same time, we estimate that the impact of completion incen-
tives on plan-making is quantitatively too small relative to the Slutsky Symmetry condition,
suggesting that participants undervalue the plan-making tool. However, our confidence in-
tervals are wide and do not permit us to reject the null of full rationality at conventional
levels.
    Our second experiment is an online survey-completion experiment that elicits richer data
that allows us to fully quantify the demand for a reminder technology and to test the first
prediction from our model. The richer data also generates greater statistical power. The
study was conducted on Amazon Mechanical Turk (MTurk) with 944 participants. Partic-
ipants who enrolled in the study were offered a bonus for completing a survey that would
only be accessible for a week-long period beginning in three weeks' time. In the main part
of the study, we elicited participants' willingness to pay (WTP) for a set of three emails
reminding participants to complete the survey. We elicited their WTP for these reminder
emails for various bonuses they might receive for completing the survey. We then introduced
exogenous variation in whether participants received the reminders.
    We find that WTP for reminders increased significantly with the size of the bonus for
completing the survey. However, this relationship is weaker than the predicted relationship
under the null of full rationality. The average impact of reminders on completing the survey
is 37 percentage points, which implies that when the completion bonus is increased by
$1, rational individuals should increase their WTP for the reminders by $0.37, on average.
Instead, we find that participants' WTP only increases by $0.15, on average. This significant
difference implies that individuals do not fully appreciate the value of the reminders.
    Collectively, these results show that while individuals are willing to pay more for attention
improvements when the stakes are higher, they do not do so as much as the full rationality
benchmark predicts. In other words, individuals appear to undervalue attention-improving
technologies. This finding suggests that external attention-increasing interventions can in-
crease efficiency by aligning individuals' attention levels with their private optima.
    Our results contribute to the literature in several ways. First, we build off of the supply
theory framework developed by Caplin et al. (forthcominga) to develop a method to assess
whether individuals understand their attention production functions and to test models
of rational inattention. Despite the recent proliferation of work on rational inattention,
surprisingly little work has been done on individuals' understanding of the limitations of
their attention.6
   6
     More work is done on whether individuals' information acquisition strategies and subsequent behavior
align with the predictions of rationality. See, e.g., Gabaix et al. (2006); Hanna et al. (2014); Bartos et
al. (2016); Martin (2016); Dean and Neligh (2018); Ambuehl et al. (2018); Caplin et al. (forthcominga);
Carvalho and Silverman (2019).


                                                    4
    Second, we describe two empirical implementations of our method. We demonstrate
how to deploy our method in both field and lab settings, across different domains, and
evaluating different attention-increasing technologies. We demonstrate how to evaluate
whether individuals optimally value attention-increasing activities both by directly measur-
ing willingness-to-pay for an attention-increasing technology and by leveraging our Slutsky
Symmetry condition. Both implementations find evidence that individuals undervalue and
underuse attention-increasing technologies.
    Third, while a large body of work looks at the impact of attention-increasing technologies
on behavior, our main advance is to study individuals' demand for attention-increasing
technologies. By studying this new comparative static, we provide a link between this
reduced-form empirical literature and recent advances in the modeling of rational inattention.
Better understanding whether individuals invest in their attention optimally is a necessary
input into a comprehensive evaluation of the costs and benefits of deploying reminders,
plan-making prompts, and other behavioral nudges.
    Perhaps closest to our work, Ericson (2011) and Tasoff and Letzler (2014) conduct lab
experiments that find that individuals' willingness to pay for a rebate exceeds the expected
returns because individuals' use of the rebate is low. This result suggests overestimation
of future attention to the rebate, although naivet´ e about other psychological biases, such
as present focus, could also play a role. Our results complement Ericson (2011) and Tasoff
and Letzler (2014) by developing new methods to directly study individuals' valuations of
attention-increasing technologies, both in field and in online experiments.7
    The rest of the paper proceeds as follows. Section 2 presents our theoretical framework.
Section 3 describes the online education experiment and presents the results. Section 4
describes the online survey experiment and presents the results. Section 5 concludes with
a discussion of other potential applications of our framework in both laboratory and field
settings.


2         Theoretical framework
We consider individuals who choose a level of costly attention, which is needed to (correctly)
complete a task. The level of attention can correspond to the likelihood of being attentive
to the task in the future, as in our experiments, or to the likelihood of correctly solving a
cognitively demanding or psychometric task, as in the lab-experimental literature on rational
inattention (e.g., Dean and Neligh, 2018; Ambuehl et al., 2018; Caplin et al., forthcominga)
or the field-experimental literature studying cognitively effortful production (e.g., Dean,
    7
        See also Taubinsky (2014) and Ericson (2017) for modeling of economic implications.


                                                       5
2019; Kaur et al., 2019; Bessone et al., 2020). We narrate our model in the context of a
structure that most closely resembles our experiments, and then explain how it applies to
other settings.
    In the context of our specific experiments, we think of individuals as making three de-
cisions. First, individuals choose an "attention production function." In the context of our
experiments, this is a choice of whether individuals take up our plan-making tool (experiment
1) or our reminder tool (experiment 2). Second, individuals select other ways of increasing
attention to the task, such as setting their own reminders, engaging in internal "memory
rehearsal" (e.g., Mullainathan, 2002), or asking others to remind them. Third, individuals
choose whether or not to complete the task if they remain attentive to it. If they are not
attentive, they default to not completing the task.
    In principle, we could model the first- and second-step choices as occurring simultane-
ously. However, because the second-step choices are unobservable to the analyst, we formally
distinguish them from the observable choices of attention technologies in our experiments.
    To formalize, individuals i first make a choice j  {0, 1} between attention cost functions
Ki0 (µ) and Ki1 (µ), where the argument µ is the probability of being attentive to the task.
We let p denote the incremental cost of choosing j = 1 over j = 0. In our online education
experiment, -p corresponds to the incentives we create for choosing our plan-making tool in
the first experiment, while in our online survey experiment p is the price of our reminders.
    Individual differences in Ki0 and/or Ki1 could result from individual differences in baseline
attentiveness, differences in how well-suited the specific attention-improving technology is to
an individual's needs, or differences in the nuisance costs of reminders and the personal and
social costs of failing to execute a plan that one creates. The difference between Ki1 and Ki0
could also capture, in reduced-form, the potential indirect costs of having one's attention to
other activities reduced.
    After choosing j  {0, 1}, individuals choose µ, the probability of being attentive to
the task in the future, at cost K j (µ). This corresponds to the unobservable investments
in attention we described above (e.g., setting their own reminders or engaging in memory
rehearsal). If individuals are inattentive to the task, they default to a = 0. If individuals
are attentive, they choose whether or not to complete the task, with action a = 1 denoting
completing the task and a = 0 denoting not completing the task.
    For expositional simplicity, we assume here that the benefits of choosing a = 1 over a = 0
are a deterministic value r + bi , where bi > 0 is the intrinsic benefit and r is the observable
pecuniary incentive. This assumption is easily relaxed without altering any results, as shown
in Appendix A.1 and A.2.
    Given an attention technology K j , the net utility benefit of an attention level µ is thus


                                               6
(bi + r)µ - Kij (µ). Under the assumption that utility is locally linear in the pecuniary
incentives, rationally inattentive individuals choose j and µ to maximize (bi +r)µ-Kij (µ)-pj .
    Our main result characterizes testable restrictions of the rationality assumption on a set
of statistics that we measure in our two experiments. The first statistic is the willingness to
pay (WTP) for technology j = 1; that is, the highest p at which j = 1 is preferred to j = 0.
Note that if the nuisance cost of j = 1 is sufficiently high, this statistic can be negative, even
if j = 1 lowers the marginal cost of attention. Average WTP is given by

             ¯ (r) := E max (bi + r)µ - Ki1 (µ) - max (bi + r)µ - Ki0 (µ)
             W
                            µ                             µ


    We also consider P r(j = 1|p, r), the probability of individuals choosing technology j = 1
given financial incentives p and r, and P r(a = 1|p, r), the probability of individuals doing
the task (i.e., choosing a = 1) given incentives p and r. In the set-up here, the latter is
simply the probability of being attentive, but more generally this probability may be smaller
than µ if the net benefits of choosing a = 1 are not always positive. Finally, we consider
P r(a = 1|j, r), the probability of individuals choosing a = 1 if individuals are exogenously
assigned attention technology j .
    Our main assumption--which we state formally in Appendix A.1--is that individual
differences are sufficiently "smoothly distributed" such that W    ¯ (r), P r(j = 1|p, r), and
P r(a = 1|p, r) are differentiable functions of p and r. Under this assumption, rational
allocation of attention implies the following testable restrictions on these statistics:

Proposition 1. Average willingness to pay for the attention-increasing technology, as a
function of the task-completion incentive r, satisfies

                      d ¯
                        W (r) = P r(a = 1|j = 1, r) - P r(a = 1|j = 0, r).                 (1)
                     dr
The likelihood of choosing technology j = 1 and the likelihood of completing the task, as
functions of the task-completion incentive r and the technology price p, satisfy the equality

                            d                     d
                               P r(j = 1|p, r) = - P r(a = 1|p, r).                           (2)
                            dr                    dp
    Equation (1) of Proposition 1 states that, if individuals are more likely to choose a = 1
by, e.g., 10% under attention technology j = 1 and incentive r, then a $1 increase in r
should increase individuals' average willingness to pay for j = 1 by approximately $0.10.
The result and intuition follow from the Envelope Theorem. Rationality implies that a
small increase dr in the task incentive should be worth P r(a = 1|j = 1, r)dr to individuals
exogenously assigned technology j = 1, and should be worth P r(a = 1|j = 0, r)dr to

                                                7
individuals exogenously assigned technology j = 0. Consequently, the average impact on the
WTP for j = 1 is (P r(a = 1|j = 1, r) - P r(a = 1|j = 0, r)) dr.
    Figure 1 illustrates this intuition graphically for a representative individual, for the case
in which the marginal costs are linear. In this case, the likelihood of executing the task
equals the chosen level of attention µ. In analogy to standard theories of competitive supply,
individuals' choice of µ with attention technology j is determined by the intersection of the
                                                             
marginal benefit curve r + b and the marginal cost curve µ    K j . As in theories of competitive
supply, the total surplus of an individual with technology j = 0 at incentive r is equal to the
area of triangle OAD, which is (r + b)P r(a = 1|j = 0) - K 0 (0), where K 0 (0) can be thought
of as the "fixed cost." Similarly, the total surplus of an individual with technology j = 1 is
equal to the area of triangle OAF, which is (r + b)P r(a = 1|j = 1) - K 1 (0). Increasing the
incentives r by an amount  increases surplus by an amount ABCD under technology j = 0,
and by an amount ABEF under technology j = 1. The change in WTP for technology j = 1
is thus given by the area DCEF. In the limit of very small , the areas of ABCD and ABEF
are approximately  · P r(a = 1|j = 0) and  · P r(a = 1|j = 1), respectively, and the WTP
for j = 1 over j = 0 is thus  · [P r(a = 1|j = 1) - P r(a = 1|j = 0)].
    It is important to note that the difference in fixed costs, K 1 (0) - K 0 (0), may result
from the potential nuisance costs of attention-improving technologies--which is consistent
with negative WTP for reminders by some individuals in our second experiment. Thus, the
value of a reminder cannot be equated with its impact on the change in expected earnings
rP r(a = 1). Simply documenting that, for example, individuals' valuations for a reminder
that increases their chance of earning $20 by 10% is smaller than $2 is not a rejection of
rational valuation of the reminder, since nuisance costs could make a rational individual
value the reminder less than or more than $2. Our more robust test focuses instead on how
individuals' valuations of the reminder change as the pecuniary incentives for being attentive
change.
    The condition in equation (1) requires rich data that is difficult to collect in some field
settings and that we do not have in our first experiment. Equation (2) builds on equation (1)
by characterizing how the probability of choosing j = 1 and the probability of choosing a = 1
are related to each other. The condition in equation (2) formalizes the basic intuition that
if attention is allocated optimally, then increasing the incentives for choosing a = 1 should
increase the desire to adopt a technology that increases the likelihood of choosing a = 1. But
while the qualitative comparative static could still be consistent with individuals under- or
over-valuing the benefits of attention technology improvements, the quantitative condition
clarifies exactly how much individuals should seek attention technology improvements.
    The condition is a variation on the Slutsky Symmetry condition that cross-price elastici-


                                               8
                                                                                     d
ties of compensated demand functions must be equal to each other. Intuitively, - dp    P r (a =
1|p, r) is an indication of how adoption of technology j = 1 impacts the probability of choos-
ing a = 1. In our online education experiment, this derivative is the average impact of
our plan-making incentives on the likelihood of completing course modules. The higher this
number is, the higher the impact of our plan-making tool on the likelihood of completing the
course modules will be. And the higher is the impact of the plan-making tool, the higher is
the impact of a small change in r on its value, as formalized in the first part of Proposition
                                            d
1. This translates to a higher derivative dr  P r(j = 1|p, r).


2.1    Applications to other settings
Our framework can be implemented for a variety of other attentional tasks studied in lab
and field experiments. Suppose that b = 0 is fixed so that the expected payoff is simply
µr. This corresponds to psychometric experiments such as those of Dean and Neligh (2018)
and Caplin et al. (forthcominga), or settings where employees exert mental effort under a
piece-rate incentive scheme (e.g., Dean, 2019; Kaur et al., 2019; Bessone et al., 2020). The
observable choice of K j can capture the choice of task difficulty, decision aids, or the level of
distraction in the environment. The choice of µ conditional on j corresponds to various forms
of exerting mental effort not directly observable to the analyst. The observable outcome a
simply captures whether the individual executed the task correctly or not.
    Our modeling of attention as a production technology makes use of results in Caplin
et al. (forthcominga), which shows that standard rational inattention models (e.g., Sims,
2003; Matejka and McKay, 2015; Caplin and Dean, 2015; Caplin et al., forthcomingb) can be
represented by a production model in which individuals pay a cost to obtain a probability µ
of taking the right action. The Caplin et al. (forthcominga) results imply that our modeling
framework makes minimal assumptions about the structure and dimensionality of attention
allocation.


2.2    Deviations from the full-rationality benchmark
Deviations from the full-rationality implications derived in Proposition 1 can be used to
quantify behavioral biases. Misperception of attention production functions will lead to
violations of the conditions in Proposition 1. For example, if individuals are overconfi-
dent about the likelihood of being attentive in the future, they will undervalue improve-
ments to their attention production function. In particular, if individuals think that their
                                                                                      d ¯
likelihood of being inattentive is only  < 1 as high as it actually is, then dr        W (r ) =
                                             d                    d
 [P r(a = 1|K1 , r) - P r(a = 1|K0 , r)] and dr P r(K1 |p, r) = - dp P r(a = 1|p, r). Deviations

                                                9
from the conditions in Proposition 1 can thus provide estimates of parametric models of
overconfidence. We formalize these claims in Appendix A.3.
    Of course, such a parametric model of overconfidence need to not be the only possible
micro-foundation. For example, individuals could under-appreciate the efficacy of particular
attention-increasing technologies, but be well-calibrated about their level of attention in the
absence of attention-increasing technologies. Rejection of the conditions in Proposition 1
rejects the rational inattention assumption in a robust way that is not tied to any particular
parametric model of misperceptions.


3       Online Education Experiment
Our first experiment was designed around the Slutsky Symmetry test in Equation (2) of
Proposition 1. It investigates how individuals respond to incentives that reward task com-
pletion and incentives that reward plan making, in the context of completing the coursework
of an online computer coding course. We conducted a six-arm experiment in which partici-
pants were randomly assigned to groups that faced different levels of financial incentives for
task completion (completing at least three 15-minutes sessions of the online coding course
in a week) and plan making (creating calendar events specifying when during the week they
would complete the coding lessons). The experiment lasted for eight weeks during the fall
of 2018.
    We partnered with Code Avengers, an online interactive platform for learning to code, to
offer participants a free, 8-week course in three different programming languages (HTML/CSS,
Javascript, and Web Dev).8 We choose an educational domain because it requires a repeated
investment of time, involves an intrinsic reward, and participants may fail to appropriately
plan to complete their courswork. It also allows our experiment to contribute to the growing
literature on behavioral economics interventions in education (see, e.g., Castleman and Page,
2016; Levitt et al., 2016; Damgaard and Nielsen, 2018).
    Our results provide evidence on the responsiveness of plan making to small incentives and
on the degree to which plan making affects task completion. Importantly, our experiment
also provides estimates of the two statistics necessary to perform the Slutsky Symmetry
test: how task completion responds to incentives to plan, and how plan making responds to
incentives to complete the task.
    8
    These languages are commonly used           tools   for   building   modern   web   sites.   See
http://www.codeavengers.com for more details.




                                                 10
3.1     Design and implementation
3.1.1   Subject pool

We recruited students and recent alumni from six Philadelphia-area colleges using an email
campaign. Enrollees were eligible to be included in our study if they reported in the onboard-
ing survey that they regularly used either Google Calendar or Apple's iCal as an electronic
calendar. Perhaps due to the relative youth of the subject pool, usage rates were high, at
around 60­70 percent. Recruitment resulted in a pool of 1,373 study-eligible participants.
    Table 1 presents characteristics of the subject pool; females, first-years, and seniors were
most likely to participate. Participants were highly motivated and expressed strong in-
tentions to complete the course. In a survey of a random subset of participants, subjects
reported their expected likelihood of completing the course was 79 percent. They reported
that likelihood as 86 percent if they were to make a plan each week for when to do the coding
lessons, 90 percent if they received $2 each week for completing the coding lessons, and 92
percent if they were to receive $5 each week for completing the coding lessons.

3.1.2   Implementation

Just before the 8-week course began, participants received an introductory email with in-
formation on their treatment assignment (see Appendix Figures B.1, B.2, and B.3). This
email also contained a recommendation that participants aim to complete three, 15-minute
sessions of the coding course per week, a prompt to encourage participants to make a plan for
when they would do the coding lessons, and a link to make plans for working on the coding
lessons, which would be created in their electronic calendars. Participants who were eligible
for financial rewards were informed that they would be paid their cumulative earnings in the
form of an Amazon gift card at the end of the 8-week period.
    Over 90 percent of participants opened the initial emails that informed them of the
incentives they faced (i.e., their treatment group), giving us confidence that most were aware
of the incentives for which they were eligible. As expected from the random assignment of
treatment, email opening rates were very similar across treatment groups, ranging from 88%
to 91%.
    After the course had begun, all participants received a reminder email at the start of
each week. The reminder email contained the same recommendation, planning prompt, and
a link to create plans as the initial email (Appendix Figure B.4).




                                              11
3.1.3    Experimental design

The experiment consisted of a control group and five treatment arms, with varying levels
of incentives for plan making and/or coding task completion. Participants assigned to the
control group received the initial and reminder emails encouraging them to plan and complete
the coding lessons and offering them the plan-making tool, but they were not eligible for
financial rewards.
    Those randomly assigned to the two Pay-to-Plan treatments received either $1 or $2 for
making a plan for when to do their coding lessons that week (i.e., clicking the plan-making
link within the weekly email). In the two Pay-to-Code treatments, participants received
either $2 or $5 for completing three 15-minute sessions of the coding course during the
week. Finally, participants in the Combination treatment arm were paid $1 for making a
plan plus $2 if they completed three 15-minute sessions of the coding course during the
week. Participants could earn these amounts each week, regardless of what they had done
in previous weeks. In addition, making a plan did not restrict when a participant could do
the coding lessons (i.e., participants in the Pay-to-Code and Combination treatments could
complete the 15-minute sessions at any time during the week and still earn their coding-task
incentives, regardless of whether or not they made a plan or when they had scheduled the
three 15-minute sessions).
    To measure plan making, we tracked whether a participant clicked on the provided plan-
making link to create calendar events for when they planned to complete the 15-minute
coding sessions.9 Consistent with our theoretical model, this observable plan making is not
the only available attention-increasing technology, or even the only available plan-making
opportunity. For example, some participants might have other means of making plans or
might directly edit their calendars without using our link. However, nearly forty percent
of the control group clicked to make a plan in the first week, despite receiving no financial
rewards for doing so, and participants with higher incentives for completing the coding task
were more likely to use the plan-making tool, implying that our plan-making tool was not a
perfect substitute for the plan making individuals would do otherwise.10 In part, this may
be because the act of making a plan by using our link generates an internal cue, as theorized
in the implementation intentions literature (Gollwitzer and Sheeran, 2006).
   9
     When participants clicked on the plan-making link, they were given three default times, which they
could change. This default ensured that as long as a participant clicked on the link, a calendar event would
be created.
  10
     Our theoretical framework only requires that the plan-making tool we offer is not a perfect substitute to
other forms of planning individuals already undertake. Heterogeneity in attention cost functions accommo-
dates the possibility that some participants who use our plan-making tool simply substituted from creating
their own calendar reminders while others who use our plan-making tool would not have created a plan
themselves.


                                                     12
    To measure completion of the coding coursework, we received real-time, backend data
from Code Avengers on the number of minutes participants spent actively working on their
coding coursework each day. The session timer stopped running after approximately 30
seconds of inactivity within the course. Once they had completed 15 minutes of active work,
participants were notified with a pop-up that congratulated them but did not prevent or
discourage them from continuing.


3.2     Results
3.2.1   Empirical framework

Our primary analysis focuses on measuring the effect of plan-making and coding-task in-
centives on plan making and coding task completion. We estimate treatment effects using
regressions of the form
                           yict = Tict + c + t + Xi + ict ,                         (3)

where yict measures either plan making or completing at least   {0, 10, 20, 30, 40, 45, 50, 60}
minutes in week t for participant i at campus c. We include fixed effects c for campus inter-
acted with student status (i.e., current student or alumni), which was the level at which we
randomized. We also control for course week t and a vector of participant characteristics
Xi , but random assignment implies that these additional controls do not affect our estimated
treatment effects. Our preferred measure of treatment Tict is value in dollars of the partic-
ipant's incentive, which assumes a linear relationship between the incentive and behavior.
We also consider a specification with indicators for different incentive sizes. We estimate
regressions separately for the Pay-to-Plan sample, which includes the control group and the
two Pay-to-Plan treatments, and the Pay-to-Code sample, which includes the control group
and the two Pay-to-Code treatments.

3.2.2   Plan-making incentives

In Table 2, we estimate the impacts of plan-making incentives on plan making and on
coding task completion. The analysis sample includes 705 participants and 8 pooled weekly
observations per participant. In Panel A, we estimate the effect of plan-making incentives
on the propensity to plan in week 1, weeks 1 to 4, and weeks 1 to 8. Multiple-week outcomes
average the indicator for whether a participant made a plan (or completed the coding task)
in each week. In Panel B, we estimate the effect of plan-making incentives on the propensity
to complete at least 20 minutes or at least 45 minutes of coding during week 1, weeks 1
to 4, and weeks 1 to 8, respectively. Although our financial incentives were specifically for


                                              13
completing at least 45 minutes of the coding task (i.e., the three 15-minute sessions), we also
include the 20-minute benchmark in the main tables and text to show robustness. Appendix
Tables B.1 and B.2 consider other time thresholds: 0, 10, 30, 40, 50, and 60 minutes per
week. Our interpretation of the results is consistent with the evidence from these alternative
thresholds.
    The results indicate strong impacts of plan-making incentives on plan making and modest
impacts of plan-making incentives on coding task completion. For each $1 of plan-making
incentive, participants increase their plan making by 11.6 percentage points (s.e.=1.3) on
average over the eight weeks of the study, an increase of 140% relative to the control group
mean of 8.2 percentage points. Plan-making effects are 18.0 percentage points (s.e.=2.0) in
week 1, and 14.2 percentage points (s.e.=1.4) on average over weeks 1 to 4, which suggests
an attenuated response over the course of the study. However, the control mean falls even
more quickly, from 38.1% in week 1, to 15.0% in the first four weeks, to 8.2% over the full
study, such that the relative impact of plan-making incentives increases over time. Panel A
of Appendix Table B.3 shows the effects of the $1 and $2 plan-making incentives separately.
    The treatment effect of plan-making incentives on coding task completion is more modest
but still meaningful. Focusing on course completion of at least 45 minutes a week, we
find that $1 of plan-making incentive increases coding task completion by 3.8 percentage
points (s.e.=1.8) in week 1, an increase of 22% relative to the control group mean of 17.4
percentage points. However, the effect declines to a marginally significant 1.7 percentage
points (s.e.=1.2) over weeks 1 to 4, and to a statistically insignificant 0.6 percentage points
(s.e.=0.9) over weeks 1 to 8. In Panel C, we combine the plan making and coding task
completion estimates in an instrumental variables estimation of the effect of plan making on
coding task completion. Making a plan increases the probability of coding task completion
by 21 to 22 percentage points in week 1, an 81% to 124% increase relative to control group
means. This large effect is precisely estimated for week 1 and weeks 1 to 4 but diminishes
over the full experimental period. Overall, the results point to the value of plan making
for people who have some intrinsic motivation to complete the coding sessions. Panel B of
Appendix Table B.3 shows the effects of the $1 and $2 plan-making incentives separately.
    The decrease in treatment effects over time is not surprising, as many participants appear
to attrit out of the coding course. Figure 2 plots control group means for plan making and
coding task completion over the weeks of the experiment. Engagement in the first two
weeks of the study is relatively high in the absence of monetary incentives--control group
participation hovers between 20 and 30 percent. However, many participants disengage from
both the plan-making tool, which falls close to zero by week 3, and from continuing the
coding course, which falls to 10 percent participation by week 5, suggesting that motivation


                                              14
for the coding course diminished over time.

3.2.3    Coding-task incentives

Table 3 estimates the impacts of coding-task incentives on plan making and coding task
completion. The analysis sample includes 714 participants and 8 pooled weekly observations
per participant. Following the structure of Table 2, in Panel A, we estimate the effect of
coding-task incentives on the propensity to plan in week 1, weeks 1 to 4, and weeks 1 to 8.
In Panel B, we estimate the effect of coding-task incentives on the propensity to complete
at least 20 minutes or at least 45 minutes of coding during week 1, weeks 1 to 4, and weeks
1 to 8, respectively.
    Coding-task incentives have substantial effects on coding task completion, as shown in
Panel B. We estimate that each $1 of coding-task incentive increases completion rates for
45-minutes in week 1 by 3.5 percentage points (s.e.=0.8), an increase of 20% relative to the
control group mean of 17.4 percentage points. For the $2 incentive and $5 incentive groups,
this coefficient implies an increase in the probability of coding task completion of 7 and
17.5 percentage points, respectively, or 40% and 101% relative to the control mean of 17.4
percentage points. Again, the treatment effects diminish over time to 2.4 percentage points
(s.e.=0.6) per $1 over weeks 1 to 4, and to 2.0 percentage points (s.e.=0.5) per $1 over the
eight weeks of the study.11
    A more novel result is that coding-task incentives also increase the probability of plan
making, as shown in Panel A. Column 1 shows that for each $1 of coding-task incentive,
participants increase their plan making by 2.5 percentage points (s.e.=0.9) in week 1, by
1.0 percentage point (s.e.=0.4) in weeks 1 to 4, and by 0.7 percentage points (s.e.=0.3) over
the eight weeks of the study. Relative to the control group means of 0.38, 0.15, and 0.08,
these correspond to plan making increases of 6.6%, 6.7%, and 8.5% per $1 of plan-making
incentive.
  11
     We exclude the Combination treatment from our main analysis and separately evaluate whether this
treatment exhibits complementarity effects (i.e., whether combining a $1 plan-making incentive with a $2
coding-task incentive induces plan making or coding effects that are significantly different from the $1 Pay-
to-Plan or $2 Pay-to-Code treatments in isolation). For weeks 1 to 8, the Combination treatment effect on
plan making is 26.7 percentage points (s.e.=2.6) compared to 23.9 percentage points (s.e.=2.7) for the $1
Pay-to-Plan treatment (p-value of difference = 0.31). The Combination treatment effect on average course
completion is 3.8 percentage points (s.e.=2.4) compared to 4.6 percentage points (s.e.=2.1) for the $2 Pay-
to-Code treatment (p-value of difference = 0.72). Thus, we find no statistically significant complementarity
effect of the Combination treatment.




                                                     15
3.2.4    Symmetry test

Participants clearly recognize the potential value of plan making in helping them achieve
their coding course participation. Yet do they value plan making enough? To answer this
question, we compare the cross-price coefficients across the Pay-to-Plan and Pay-to-Code
samples, implementing the test in Equation (2) of Proposition 1. The coefficients for $1
of plan-making incentives on coding task completion are 0.039, 0.017, and 0.006 in week 1,
weeks 1 to 4, and weeks 1 to 8, respectively. The analogous coefficients for $1 of coding-task
incentives on plan making are 0.025, 0.010, and 0.006. The difference in coefficients provides
our first test of under-planning, delivering estimates of 0.014 (s.e.=0.019), 0.007 (s.e.=0.012),
and -0.0004 (s.e.=0.009), respectively.12 The positive sign of the differences, in particular
in the early weeks of the study, hints at the possibility that participants might undervalue
plan making. However, the standard errors are too wide to draw strong conclusions from
this data about whether participants plan optimally.
    Figure 3 plots week-by-week coefficients for plan-making and coding-task incentives to
illustrate how the effect of incentives evolves over the course of the experiment. The effect of
coding-task incentives on plan making is consistently close to zero (after week 1) and tightly
estimated. In contrast, the effect of plan-making incentives on coding task completion are
positive for the first half of the study and then decay toward zero, with relatively wider
confidence intervals.13 This fact provides suggestive evidence of under-planning.


4       Online Survey-Completion Experiment
Estimates from our field experiment show that incentives for plan-making increase coding
task completion, and incentives for coding task completion lead to more plan-making. While
this second result is qualitatively consistent with rational management of attention, our
estimates are not sufficiently precise to provide a conclusive test of the Slutsky Symmetry
condition in Equation (2) of Proposition 1. Similarly, our point estimates are consistent
with participants under-appreciating the benefits of plan-making early in the study, but we
cannot reject that individuals value the plan-making tool rationally.
    To generate more statistical power and to test more directly whether individuals ra-
tionally value an attention-increasing technology, we designed and ran a complementary
survey-completion experiment on Amazon's Mechanical Turk platform (MTurk). The ex-
periment is tightly tied to the test in Equation (1) of Proposition 1, described in Section 2.
  12
    Standard errors for coefficient differences are estimated via seemingly unrelated regression.
  13
    The difference in standard errors across treatments owes to having higher variance in coding-task incen-
tives ($0, $2, and $5) relative to plan-making incentives ($0, $1, and $2).


                                                    16
The test states that for individuals who optimally invest in attention increasing technology,
a $1 increase in the incentive for task completion must increase willingness to pay for such a
technology by $1 times its efficacy (i.e., by the change in the probability of task completion
due to the technology). To show robustness to a new setting, our online experiment replaces
plan-making to complete the coding task with email reminders to complete a 20-minute
survey.


4.1     Design and Implementation
In May of 2019, we initiated a two-part study on MTurk. Our recruitment material informed
potential participants that part 1 of the study would require five minutes of time immediately
(for which participants were paid a guaranteed $1 and had the possibility of earning a bonus),
and that they would be invited to participate in part 2 of the study at a later date for
additional compensation by accessing a website provided to participants at part 1 of the
study.
    When participants clicked to begin the study, they were told that part 2 of the study--
a survey that needed to be completed in one 20-minute sitting--would only be available
starting in three weeks and that they would have a one-week window to complete it.14
    The first part of the study elicited participants' willingness to pay (WTP) for a set of
three reminder emails that would come during the one-week window in which participants
would be able to complete the survey. The goal was to generate data that would allow us
to directly measure how much more participants were willing to pay for reminder emails as
the incentive to complete the survey increased.
    Participants were informed that their incentive for completing the survey would be either
$2, $3, $4, or $5, and that each was equally likely to be selected. For each of the four possible
incentive amounts, participants faced an incentivized multiple price list (MPL) that traded
off part 1 bonus payments (up to $1.50, which would be paid out a few days after participants
completed part 1 of the study) against being sent the three reminder emails to complete the
survey.15 The four MPLs, one for each possible incentive amount participants could earn for
  14
     Participants were asked a screening question (see Appendix Figure C.1) about whether they would be
available to complete the survey in the designated window. Those who said they would be unable to do so
were screened out of the study (see additional details in Appendix C.1). Participants completed the first
part of the study on either May 3 or May 7 of 2019. Those who participated on May 3 were able to complete
the second part of the study between May 25 and May 31 of 2019. Those who participated on May 7 were
able to complete the second part of the study between May 29 and June 4 of 2019.
  15
     Part 1 bonus payments were paid out a few days after participants completed part 1 of the study,
mitigating concerns that part 1 bonuses would be viewed as being paid immediately, which might have made
them particularly valuable from the perspective of a quasi-hyperbolic discounter. The results of Augenblick
et al. (2015) and Augenblick (2020) suggest that monetary rewards paid out a few days later are very unlikely
to be treated as a form of immediate gratification.


                                                     17
completing the survey, were shown to participants in a random order.
    To ensure that participants understood the specific details of the reminder emails, we
explained that the emails would come at 12pm ET on the first, middle, and final days of the
one-week window when they could complete the survey.16 Participants were told that emails
would be sent using the MTurk email system--which MTurk uses for communicating with
workers on their platform--so participants did not have to provide an email address and the
reminder emails would be unlikely to go to spam. Participants were told that the link to
the survey would be included in the reminder emails so that initiating the survey would be
as easy as clicking a link in the email. Participants were also explicitly told that they would
not receive any reminders to complete the survey unless they were selected to receive these
three reminder emails.17
    Appendix Figure C.10 shows an example MPL decision screen. Because nuisance costs
can lead participants to have negative WTP for the reminders, the MPL allowed partici-
pants to report both positive and negative willingness-to-pay for the reminder emails.18 We
anticipated that some participants might be at the extreme ends of the MPL: Top-censored
participants choose the option on the left in each row, indicating a WTP for reminders of
more than $1.50. Bottom-censored participants choose the option on the right in each row,
indicating a WTP for reminders of less than -$1.50. If a participant was censored on an
MPL, we asked them an unincentivized question about how much they would be willing
to pay for reminders (if W T P  $1.50) or would need to be paid to accept reminders (if
W T P  -$1.50). In practice, no participants indicated a W T P  -$1.50 on any of the
MPLs, and thus in our analysis we only discuss top-censored participants.19
    Participants were told that whichever incentive amount was randomly selected for them
would be the bonus they would receive for completing the survey. In addition, they were
told that for the randomly selected incentive amount, there was a 10% chance that one of
the rows of that MPL would be randomly selected (each with equal probability) and that
  16
      The emails were sent on May 25, May 28, and May 31 for those who took part 1 of the study on May
3; and May 29, June 1, and June 4 for those who took part 1 of the study on May 7.
   17
      In order to remain in the study, participants needed to correctly answer questions demonstrating their
understanding of the compensation structure and conditions for receiving reminders (e.g., answering "True"
to the statement: "You will not receive any reminders unless you are selected to get them based on random
chance and your choices in part 1.")
   18
      Consistency on an MPL requires a participant to always choose the option on the left, always choose
the option on the right, or switch from choosing the option on the left to choosing the option on the right in
one row of the MPL. Always choosing the option on the right or switching to the right between row 2 and 6
indicates a negative WTP for the reminder emails (weakly negative if the switch occurs in row 6). Switching
to the right between rows 7 and 11 or always choosing the option on the left indicates a positive WTP for
the reminder emails (weakly positive if the switch occurs in row 7). The approach of allowing both positive
and negative WTP is similar to the designs in Allcott and Kessler (2019) and Butera et al. (2019).
   19
      Appendix Figures C.11 and C.12 show examples of these questions.



                                                     18
whatever the participant chose in that row would be implemented (i.e., they would receive
whatever part 1 bonus payment was indicated in their choice, and they would receive the
reminder emails if they chose the option on the left).
    Because testing Proposition 1 also requires estimating the effect of the reminder emails
on completing the survey, we did not guarantee that one of the MPL rows would be selected.
Instead, we randomized 45% of participants to receive the reminder emails and 45% of
participants not to receive the reminder emails, regardless of their MPL choices.20 We use
this random variation to estimate the effect of reminder emails on completing the survey. We
randomly assign reminder emails in this way, and estimate the effect of reminders using this
sample, in order to avoid potential selection bias that might arise if there were a correlation
between WTP for reminders and the rate at which individuals completed the survey.


4.2     Results
4.2.1    Sample

As detailed in Appendix C.1, 1,034 individuals fully completed the first part of the study. Of
these, 90 individuals were excluded from analysis for: having an invalid MTurk ID (n = 7),
accessing the survey after recruitment had been completed (n = 4), or having inconsistent
MPL responses (n = 79). We report on data from the remaining 944 participants.21

4.2.2    How WTP changes with the incentive to complete the survey

Figure 4 shows CDFs of WTP for the reminder emails for each part 2 incentive level. For
uncensored participants, the MPL identifies a range for WTP (e.g., if a participant chooses
to get the reminders when they cost 25 cents and not to get the reminders when they cost
50 cents, we infer that the participant values reminders between 25 and 50 cents). We use
the midpoint of each WTP range as our estimate of WTP. As the figure shows, participants
are willing to pay more for reminders as the incentive to complete the survey in part 2 of
the study increases.22 In addition, the number of participants who are willing to pay more
  20
     In addition, all of these participants received a $1.00 bonus for completing part 1 of the study.
  21
     As detailed in Appendix C.1, our 1,034 number excludes participants who were not allowed to continue
with part 1 of the study because they failed to correctly input a captcha or failed to correctly answer
understanding questions. An advantage of excluding these participants from the study ex ante--not allowing
them to answer any WTP questions--is that it ties our hands to only analyze data from participants who
were attentive and clearly understood the study instructions.
  22
     While the CDFs look rather identical in the negative WTP region, as the incentives for completing the
survey increase, the postive WTP region of the CDFs fall lower and to the right, indicating higher willingness
to pay.




                                                     19
than $1.50 for the reminder emails--shown as a mass in the CDF at a willingness to pay of
150 cents--increases with the part 2 incentive level.23
    We formalize the results from Figure 4 in Table 4, combining data from all four incentive
levels to estimate how average WTP changes with the incentive to complete the survey. We
estimate treatment effects using regressions of the form,

                                W T Pij =  [Incentivej ] + i + ij

where W T Pij is participant i's willingness to pay for reminders given incentive j . The
coefficient  (labeled Incentive ($) in the table) shows the average effect on WTP of increasing
the incentive to complete the survey by $1. i are participant fixed effects to account for the
possibility that different participants have different average WTP.
    Regressions in Table 4 differ in how they handle participants whose WTP is censored
by the MPLs. Column (1) shows our baseline specification in which any top-censored WTP
estimate is replaced by the median WTP reported in the unincentivized question that we
asked each participant who was top-censored on an MPL (see Appendix Figure C.11 and the
discussion in Section 4.1). Using the median mitigates potential concerns about participants
reporting extreme values to those unincentivized questions. In subsequent columns, we show
that our results are robust to multiple approaches to handling censored WTPs.
    Results from column (1) suggest that participants are willing to pay 15 cents more on
average for every $1 increase in the incentive for completing the survey. Column (2) replicates
the analysis in column (1) but replaces the median of unincentivized WTP reports of all
participants censored at that incentive level with each censored participant's specific WTP
report. Results from column (2) are very similar and imply that participants are willing to
pay 17 cents more on average for every $1 increase in the incentive for completing the survey.
Column (3) replaces any censored WTP values with $5, the maximum incentive amount,
and also estimates that participants are willing to pay 17 cents more on average for every
$1 increase in the incentive for completing the survey. Finally, column (4) does not attempt
to replace the censored WTP values but instead keeps them at $1.50 and estimates a Tobit
specification to account for the censoring at $1.50. This specification estimates participants
are willing to pay 6 cents more for every $1 increase in the incentive for completing the
survey. The difference between the Tobit estimates and columns (1) and (2) of the table
suggests that the distribution of WTPs has a thicker right tail than the normal distribution
assumed in the Tobit model.
    Taken together, the results show that for every $1 increase in the incentive for completing
  23
    The percentage of participants willing to pay more than $1.50 is 14% when the incentive is $2 and
increases to 25% when the incentive is $5.


                                                 20
the survey, participants are on average willing to pay 6 to 17 cents more for the reminders.
   To further explore how average WTP changes with the survey incentives, we construct
Table 5. The table presents the average WTP of participants at each incentive level (using
the median reported WTP for censored participants, as in our baseline specification) and
shows how the average WTP changes with each $1 increase in the incentive for completing
the survey. The change in average WTP is significantly positive at each incentive level, and
reaches its highest value ($0.22) when moving from the $3 to the $4 incentive.

4.2.3    Robustness of incentive effects on WTP for reminders

Because participants in our study are asked to value reminders for four different incentive
levels, a potential concern is that anchoring bias might dampen the sensitivity of participants'
WTP to reminders relative to what we would estimate in a pure between-subjects design.
Table 6 examines this concern, leveraging the fact that we randomized the order in which
participants faced the MPLs. Column (1) uses WTP data only from the first incentive level
the participant was asked about, thus utilizing only between-subject variation. Column (2)
uses data from the first two incentive levels. Column (3) uses data from the first three.
Column (4) includes all data and thus replicates column (1) of Table 4. The coefficient on
Incentive ($) increases as we move across the table, suggesting that as we add within-subject
variation, participants appear to react more to the incentive level than when we consider
the between-subject variation only. This result suggests that our within-subject variation
may overstate the extent to which participants respond to the incentive when considering
their WTP. As we will show below, this reinforces our result that participants undervalue
the reminder technology.24

4.2.4    The efficacy of the reminder technology

As described in Section 4.1, we randomized 90% of participants to either get or not get
the reminder emails, regardless of their reported WTP. This randomization allows us to
generate an estimate of the effect of the reminders on survey completion. In addition, since
we independently randomized the incentive level for completing the survey, we can estimate
the effect of reminders at each incentive level.25
   Table 7 reports the probability that participants complete the survey when they received
reminder emails and when they did not, restricting to these 90% of participants. The All
  24
     Appendix Table C.2 shows the average WTP at each incentive level at each of the levels of restriction
shown in Table 6.
  25
     Appendix Table C.1 replicates Table 4 for the 90% of participants who either receive or do not receive
the reminder emails based on random assignment. As one would expect from the fact that this 90% is
randomly selected, estimates are nearly identical to those in Table 4.


                                                    21
Randomized Reminders column shows that, across all incentive levels, 80% of participants
who receive the reminder emails complete the survey, while only 43% of participants who
do not receive the reminder emails do so. Consequently, the effect of reminders on survey
completion is 37 percentage points, which is highly statistically significantly different from
zero.
    The other four columns of Table 7 show that the difference between the reminder and
no reminder groups is large at each of the four incentive levels. The largest difference is 50
percentage points for the $2 incentive group, driven by the fact that only 29% of participants
complete the survey without the reminder emails when the incentive is $2.
    Table 7 also suggests two other interesting features of participant behavior. First, inat-
tention appears to be the biggest factor in failure to complete the survey. The fraction of
participants who do not complete the survey when they receive reminders ranges from 18%­
21%, which is significantly smaller than the 32­50 percentage point effect of the reminders.
Second, in the absence of reminders, participants appear to use their own reminder strategies
more when the incentives are higher: financial incentives have a significant effect on comple-
tion of the survey in the absence of reminders, but they have a muted effect in the presence of
reminders. This differential effect of financial incentives can be explained by our reminders
and participants' own internal attention-increasing strategies being substitutes: participants
are more likely to utilize their own attention-increasing strategies when the stakes are higher
and they do not receive external reminders.

4.2.5   Do participants invest optimally in reminder technology?

Part 1 of Proposition 1 states that if participants are optimally investing in the reminder
technology, then a $1 increase in incentives for completing the survey should increase WTP
for the reminder emails by $1 times the increase in the likelihood of survey completion due
to the reminder emails. Results from the prior sections allow us to directly make this com-
parison. Column (1) of Table 4 estimates that the WTP for the reminder emails increases
by 15 cents for each $1 increase in part 2 incentive (with a standard error of 1.1 cents). The
All Participants column of Table 7 estimates that the reminder emails increased survey com-
pletion by 37 percentage points (with a standard error of 3.1 percentage points). These two
estimates are highly statistically significantly different (Wald test, standard errors calculated
using the delta method; p < 0.01), which contradicts the first condition in Proposition 1.
Comparing Table 5 to the last four columns of Table 7 further shows that the incremental
change in average WTP at each incentive level is smaller than the effect of reminders on
survey-completion at each incentive level.



                                               22
4.3    Implications for under-planning
The results of both of our studies show that individuals appear to under-value the attention-
increasing technologies they were offered. The results of our survey-completion study imply
that individuals' WTP for the reminder technology was 0.15/0.37 = 41% as sensitive to the
survey-completion incentives as the null of rational inattention would imply. In other words,
individuals significantly under-appreciate how the implicit value of reminders increases when
the stakes increase. Through the lens of the parametric model of overconfidence about
one's attention described in Appendix A.3, this statistic can be interpreted to mean that
participants think that their likelihood of being inattentive is only  = 0.41 as high as it
actually is.
    Although not statistically significant, the results of Section 3.2.4 are qualitatively consis-
tent with the results. The estimates in Section 3.2.4 imply that in week 1, weeks 1-4, and
weeks 1-8 participants' demand for the reminder technology was, respectively, 0.025/0.038 =
66%, 0.010/0.017 = 59%, and 0.0065/0.0061 = 107% percent as sensitive to the task incen-
tives as the null of full rationality would imply. Again, through the lens of the parametric
model of overconfidence in Appendix A.3, these statistic imply that individuals think their
likelihood of being inattentive to the survey task is  = 0.66,  = 0.59, or  = 1.07 as high as
it actually is. That participants appear to under-value plan making the most in the earlier
weeks of the study is consistent with learning over time about the value of plan-making,
although the standard errors are too large to draw this conclusion with reasonable certainty.


5     Conclusion
While a large and growing literature shows that attention-increasing interventions such as
reminders and plan-making tools can have significant effects on economically important
behaviors, this literature rarely asks the question of whether individuals value these tools
rationally. This paper addresses this question with two theory-driven, quantitative tests.
We find that individuals' demand for attention-increasing tools is qualitatively consistent
with the predictions of rational inattention but is quantitatively inconsistent with the null of
rationality. While this under-valuation may be context-dependent, our method can be ap-
plied more broadly to explore how individuals value attention-increasing technologies across
various domains.
    First, our methods are immediately portable to other settings where the impact of re-
minders and planning prompts has already been documented--e.g., medical compliance,
savings, loan repayment, and voting (see footnote 1 for references). In many of these do-


                                               23
mains, the market already offers various attention enhancements (e.g., smart-caps on pill
bottles), and thus our methods can help assess whether individuals value these products
rationally, or whether take-up of these products needs to be encouraged. Some of these
domains, like medical compliance, are additionally interesting because they feature repeated
behaviors in a relatively stable environment and thus ample opportunities to learn. A key
question for future research is whether individuals learn to appreciate the value of attention
enhancements with more experience.
    Second, our methodology could be used more broadly to test rational inattention models
in the types of laboratory psychometric tasks traditionally used to quantify attention costs
(e.g., Dean and Neligh, 2018; Ambuehl et al., 2018; Caplin et al., forthcominga). For example,
one could test if participants' WTP to decrease the difficulty of a task is consistent with the
full-rationality benchmark. In a controlled laboratory setting, researchers could also vary
participants' experience with a given task to explore how perceptions of attention production
functions change with experience.
    Third, our methodology could be used to test whether people understand their production
functions for attention-consuming tasks in field settings such as those of Dean (2019), Kaur
et al. (2019), or Bessone et al. (2020). In these settings, avoiding noise and other distracting
stimuli, or investing in sleep, constitute investments in reducing the cost of attention. In
such settings, researchers could quantify how the willingness to pay for reductions in the cost
of attention varies with the piece-rate for task completion.
    More generally, our tests could be applied to any setting that involves domains of behav-
ior that feature "intermediate" actions. For example, our methods could be used to quantify
whether students fully understand the relationship between studying and test performance,
whether individuals understand the link between education and earnings, or whether indi-
viduals properly invest in "good habits."




                                              24
References
Allcott, Hunt and Judd Kessler, "The Welfare Effects of Nudges: A Case Study of
 Energy Use Social Comparisons," American Economic Journal: Applied Economics, 2019,
 11 (1), 236­276.

Altmann, Steffen and Christian Traxler, "Nudges at the Dentist," European Economic
 Review, 2014, 72, 19­38.

Ambuehl, Sandro, Axel Ockenfels, and Colin Stewart, "Attention and Selection
 Effects," working paper, 2018.

Augenblick, Ned, "Short-Term Discounting of Unpleasant Tasks," working paper, 2020.

  , Muriel Niederle, and Charles Sprenger, "Working over time: Dynamic inconsis-
  tency in real effort tasks," Quarterly Journal of Economics, 2015, 130 (3), 1067­1115.

Bartos, Vojt^ech, Michal Bauer, Julie Chytilov´    a, and Filip Mat^  ejka, "Attention
 Discrimination: Theory and Field Experiments with Monitoring Information Acquisition,"
 American Economic Review, June 2016, 106 (6), 1437­75.

Bessone, Pedro, Gautam Rao, Frank Schilbach, Heather Scofield, and Mattie
 Toma, "Sleepless in Chennai: The Consequences of Increasing Sleep among the Urban
 Poor," working paper, 2020.

Bronchetti, Erin Todd, David B. Huffman, and Ellen Magenheim, "Attention,
 intentions, and follow-through in preventive health behavior: Field experimental evidence
 on flu vaccination," Journal of Economic Behavior and Organization, 2015, 116, 270­291.

Butera, Luigi, Robert Metcalfe, William Morrison, and Dmitry Taubinsky, "The
 Deadweight Loss of Social Recognition," working paper, 2019.

Cadena, Ximena and Antoinette Schoar, "Remembering to Pay? Reminders vs. Fi-
 nancial Incentives for Loan Payments," working paper, 2011.

Calzolari, Giacomo and Mattia Nardotto, "Effective Reminders," Management Sci-
 ence, 2017, 63 (9), 2915­2932.

Caplin, Andrew, "Measuring and Modeling Attention," Annual Review of Economics,
 2016, 8 (1), 379­403.

   and Mark Dean, "Revealed Preference, Rational Inattention, and Costly Information
  Acquisition," American Economic Review, July 2015, 105 (7), 2183­2203.

  , Daniel Csaba, John Leahy, and Oded Nov, "Rational Inattention, Competitive
  Supply, and Psychometrics," Quarterly Journal of Economics, forthcoming.

  , Mark Dean, and John Leahy, "Rational Inattention, Optimal Consideration Sets,
  and Stochastic Choice," Review of Economic Studies, forthcoming.


                                           25
Carrera, Mariana, Heather Royer, Mark Stehr, Justin Sydnor, and Dmitry
 Taubinsky, "The Limits of Simple Implementation Intentions: Evidence from a Field
 Experiment on Making Plans to Exercise," Journal of Health Economics, 2018, 62, 95­
 104.
Carvalho, Leandro and Dan Silverman, "Complexity and Sophistication," working
 paper, 2019.
Castleman, Benjamin L. and Lindsay C. Page, "Freshman Year Financial Aid Nudges:
 An Experiment to Increase FAFSA Renewal and College Persistence," Journal of Human
 Resources, 2016, 51 (2), 389­415.
Damgaard, Mette Trier and Christina Gravert, "The hidden costs of nudging: Ex-
 perimental evidence from reminders in fundraising," Journal of Public Economics, 2018,
 157, 15­26.
   and Helena Skyt Nielsen, "Nudging in education," Economics of Education Review,
  2018, 64 (313-342).
Dean, Joshua T., "Noise, Cognitive Function and Worker Productivity," working paper,
 2019.
Dean, Mark and Nathaniel Neligh, "Experimental Tests of Rational Inattention," work-
 ing paper, 2018.
Ericson, Keith, "Forgetting We Forget: Overconfidence and Memory," Journal of the
  European Economic Association, 2011, 9 (1), 867­875.
  , "On the Interaction of Memory and Procsastination: Implications for Reminders, Dead-
  lines, and Empirical Estimation," Journal of the European Economic Assocation, 2017, 15
  (4), 692­729.
Gabaix, Xavier, "Behavioral Inattention," in Douglas Bernheim, Stefano DellaVigna, and
 David Laibson, eds., Handbook of Behavioral Economics, Vol. 2, Elsevier, 2019.
  , David Laibson, Guillermo Moloche, and Stephen Weinberg, "Costly Information
  Acquisition: Experimental Analysis of a Boundedly Rational Model," American Economic
  Review, 2006, 96, 1043­1068.
Gollwitzer, Peter M. and Paschal Sheeran, "Implementation Intentions and Goal
 Achievement: A Meta-analysis of Effects and Processes," Advances in Experimental Social
 Psychology, 2006, 38, 69­119.
Hanna, Rema, Sendhil Mullainathan, and Joshua Schwartzstein, "Learning
 Through Noticing: Theory and Evidence from a Field Experiment," Quarterly Journal
 of Economics, 2014, 129 (3), 1311­1353.
Karlan, Dean, Margaret McConnell, Sendhil Mullainathan, and Jonathan Zin-
 man, "Getting to the Top of Mind: How Reminders Increase Saving," Management Sci-
 ence, 2016, 62 (12), 3393­3411.

                                           26
Kaur, Supreet, Sendhil Mullainathan, Suanna Oh, and Frank Schilbach, "Does
 Financial Strain Lower Productivity?," working paper, 2019.

Levitt, Steven D., John A. List, Susanne Neckermann, and Sally Sadoff, "The
  Behavioralist Goes to School: Leveraging Behavioral Economics to Improve Educational
  Performance," American Economic Journal: Economic Policy, 2016, 8 (4), 183­219.

Ma´
  ckowiak, Bartosz, Filip Mat^   ejka, and Mirko Wiederholt, "Rational Inattention:
 A Disciplined Behavioral Model," Working Paper May 2018.

Martin, Daniel, "Rational Inattention in Games: Experimental Evidence," working paper,
 2016.

Marx, Benjamin M. and Lesley J. Turner, "Student Loan Nudges: Experimental
 Evidence on Borrowing and Educational Attainment," American Economic Journal: Eco-
 nomic Policy, 2019, 11 (2), 108­141.

Matejka, Filip and Alisdair McKay, "Rational Inattention to Discrete Choices: A New
 Foundation for the Multinomial Logit Model," American Economic Review, January 2015,
 105 (1), 272­298.

Milkman, Katherine L., John Beshears, James Choi, David Laibson, and
 Brigitte C. Madrian, "Planning prompts as a means of increasing preventive screening
 rates," Preventive Medicine, 2013, 56 (1), 92­93.

  ,   , James J. Choi, David Laibson, and Brigitte C. Madrian, "Using imple-
  mentation intentions prompts to enhance influenza vaccination rates," Proceedings of the
  National Academy of Sciences, 2011, 108 (26), 10415­10420.

Mullainathan, Sendhil, "A Memory-Based Model of Bounded Rationality," Quarterly
 Journal of Economics, 2002, 117 (3), 735­774.

Nafziger, Julia, "Spillover Effects of Nudges," Economics Letters, forthcoming.

Nickerson, David W. and Todd Rogers, "Do You Have a Voting Plan? Implementation
 Intentions, Voter Turnout, and Organic Plan Making," Psychological Science, 2010, 21 (2),
 194­199.

Oreopoulos, Philip, Richard W. Patterson, Uros Petronijevic, and Nolan G.
 Pope, "When Studying and Nudging Don't Go as Planned: Unsuccessful Attempts to
 Help Traditional and Online College Students," Journal of Human Resources, forthcoming.

Rogers, Todd, Katherine L. Milkman, Leslie K. John, and Michael I Norton,
 "Beyond good intentions: Prompting people to make plans improves follow-through on
 important tasks," Behavioral Science and Policy, 2015, 1 (2).

Sims, Christopher A., "Implications of Rational Inattention," Journal of Monetary Eco-
  nomics, 2003, 50 (3), 665 ­ 690.


                                           27
Tasoff, Joshua and Robert Letzler, "Everyone believes in redemption: Nudges and
  overoptimism in costly task completion," Journal of Economic Behavior and Organization,
  2014, 107, 107­122.

Taubinsky, Dmitry, "From Intentions to Actions: A Model and Experimental Evidence
  of Inattentive Choice," working paper, 2014.




                                           28
                          Figure 1: Illustration of Proposition 1




            Incentives
                                                     !
                                                                     "
                                                                     
                                                                    
                                             C              E
     +  +  B

          +  A                      D         F




  ! +  = 0 O
                         ( = 1| = 0)        ( = 1| = 1)                     Attention 

The figure illustrates equation (1) of Proposition 1. The top line plots the marginal costs
of attention under technology j = 0, while the bottom line plots marginal costs under
technology j = 1. The area DCEF corresponds to the change in WTP for technology j = 1
over j = 0 when the financial incentive is increase from r to r + .




                                            29
      Figure 2: Online Education Experiment Control Group Means (Week-by-Week)

              .4
     Control Group Mean
           .2 .1
              0     .3




                          1      2          3      4          5       6         7           8
                                                    Study Week

                              Plan Making       Coding > 20 Minutes       Coding > 45 Minutes


This figure shows control group means for plan making and completing at least 20 minutes or at
least 45 minutes of the coding course for each week of the study.




                                                     30
      Figure 3: The Effect of Incentives on Plan Making and Coding Task Completion
                        A. The Effect of Incentives on Plan Making


      Task Incentive
                                                                                               Week 1
                                                                                               Week 2
                                                                                               Week 3
                                                                                               Week 4
                                                                                               Week 5
                                                                                               Week 6
                                                                                               Week 7
                                                                                               Week 8
      Plan Incentive




                             0            .05                     .1         .15    .2
                                                Effect of $1 in Incentives


              B. The Effect of Incentives on Coding Task Completion (>20 Minutes)



      Task Incentive
                                                                                               Week 1
                                                                                               Week 2
                                                                                               Week 3
                                                                                               Week 4
                                                                                               Week 5
                                                                                               Week 6
                                                                                               Week 7
                                                                                               Week 8
      Plan Incentive




                             0            .05                     .1         .15    .2
                                                Effect of $1 in Incentives


              C. The Effect of Incentives on Coding Task Completion (>45 Minutes)



      Task Incentive
                                                                                               Week 1
                                                                                               Week 2
                                                                                               Week 3
                                                                                               Week 4
                                                                                               Week 5
                                                                                               Week 6
                                                                                               Week 7
                                                                                               Week 8
      Plan Incentive




                             0            .05                     .1         .15    .2
                                                Effect of $1 in Incentives


This figure shows estimates for the effect of incentives on plan making and coding task completion for each
week of the study. We use "Task Incentive" as shorthand for coding-task incentive and "Plan Incentive" for
plan-making incentive. Panel A shows estimates of the effect of incentives on whether or not participants
made a plan. Panel B shows the effect of incentives on completing at least 20 minutes of coding during the
week. Panel C shows the effect of incentives on completing at least 45 minutes of coding during the week.
Whiskers report 95% confidence intervals around each estimate.
                                                        31
                     Figure 4: CDFs of WTP for the Reminder Emails




This figure shows CDFs of the willingness to pay (WTP), in cents, for the set of three reminder
emails. There is a CDF for each possible incentive for completing the survey. While the CDFs look
rather identical in the negative WTP region, the CDFs indicate higher willingness to pay when the
incentives for completing the survey increase.




                                               32
                     Table 1: Participant Characteristics (Experiment 1)
                        Students                               Alumni
           First-year                     0.28 2017             0.22
                                         (0.45)                 (0.41)
           Sophomore                      0.22 2016             0.18
                                         (0.41)                 (0.39)
           Junior                         0.23 2015             0.21
                                         (0.42)                 (0.41)
           Senior                         0.28 2014             0.19
                                         (0.45)                 (0.39)
                                                2013            0.20
                                                                (0.40)
           Female                   0.65 Female                 0.70
                                   (0.48)                       (0.46)
           Male                     0.31 Male                   0.27
                                   (0.46)                       (0.44)
           Non-binary or no answer 0.04 Non-binary or no answer 0.03
                                   (0.20)                       (0.21)
           N                        686   N                     687

This table presents summary statistics for the participants in experiment 1, split between student
and alumni groups. The Pay-to-Code sample includes 496 participants divided between $2 and
$5 incentive arms. The Pay-to-Plan sample includes 487 participants divided between $1 and $2
incentive arms. The remaining participants include 218 control participants and 172 participants
assigned to the Combination treatment of $1 plan-making incentives and $2 coding-task incentives.




                                               33
Table 2: The Effect of Plan-Making Incentives on Plan Making and Coding Task Completion
                    Panel A: The Effect on Plan Making (First Stage)
                                       (1)              (2)                   (3)
                                      Week 1          Weeks 1-4             Weeks 1-8
                    Plan Incentive   0.180***         0.142***              0.116***
                                      (0.020)          (0.014)               (0.013)
                    Obs.                705                 705                705
                    R2                 0.137               0.163              0.131
                    Control Mean       0.381               0.150              0.082
                    Controls            Yes                 Yes                Yes
                    Campus FE           Yes                 Yes                Yes


          Panel B: The Effect on Coding Task Completion (Reduced Form)
                             (1)       (2)         (3)           (4)         (5)          (6)
                           >20 (1)   >20 (1-4)   >20 (1-8)     >45 (1)     >45 (1-4)    >45 (1-8)
          Plan Incentive   0.040**    0.028**      0.013       0.038**       0.017        0.006
                           (0.020)    (0.013)     (0.011)      (0.018)      (0.012)      (0.009)
          Obs.               705        705         705             705       705          705
          R2                0.057      0.049       0.051           0.036     0.035        0.041
          Control Mean      0.280      0.212       0.158           0.174     0.156        0.116
          Controls           Yes        Yes         Yes             Yes       Yes          Yes
          Campus FE          Yes        Yes         Yes             Yes       Yes          Yes


          Panel C: The Effect of Plan Making on Coding Task Completion (IV)
                             (1)       (2)         (3)           (4)         (5)          (6)
                           >20 (1)   >20 (1-4)   >20 (1-8)     >45 (1)     >45 (1-4)    >45 (1-8)
          Plan Making      0.221**    0.194**      0.114       0.213**       0.118        0.053
                           (0.105)    (0.087)     (0.086)      (0.096)      (0.076)      (0.074)
          Obs.               705        705         705             705       705          705
          R2                0.147      0.174       0.133           0.092     0.120        0.085
          Control Mean      0.280      0.212       0.158           0.174     0.156        0.116
          Controls           Yes        Yes         Yes             Yes       Yes          Yes
          Campus FE          Yes        Yes         Yes             Yes       Yes          Yes

This table estimates the effect of plan-making incentives on plan making and coding task comple-
tion. We use "Plan Incentive" as shorthand for plan-making incentive (in dollars). Panel A shows
the effect of plan-making incentives on whether experimental participants made a plan. Column
(1) shows the effect of plan-making incentives in the first week of the experiment. Column (2)
shows the average effect for the first four weeks. Column (3) shows the average effect for the en-
tire experiment. Panel B shows the effect of plan-making incentives on coding task completion.
Columns (1-3) show the effect on an indicator variable for whether or not the participant worked
on the coding task for more than 20 minutes: Column (1) estimates the effect over the first week,
Column (2) over the first four weeks, and Column (3) over the entire experiment. Columns (4-6)
show analogous estimates, but for an indicator variable for whether or not the participant worked
on the coding task for more than 45 minutes each week. Panel C shows the 2SLS estimates instru-
menting for whether participants made a plan using the plan-making incentive as an instrument.
The dependent variables are the same as in Panel B. Standard errors are shown in parentheses.
*p < 0.1, **p < 0.05, ***p < 0.01.               34
Table 3: The Effect of Coding-Task Incentives on Plan Making and Coding Task Completion
                      Panel A: The Effect on Plan Making
                                          (1)              (2)           (3)
                                         Week 1          Weeks 1-4     Weeks 1-8
                      Task Incentive    0.025***          0.010**       0.007**
                                         (0.009)          (0.004)       (0.003)
                      Obs.                   714             714             714
                      R2                    0.050           0.058           0.049
                      Control Mean          0.381           0.150           0.082
                      Controls               Yes             Yes             Yes
                      Campus FE              Yes             Yes             Yes


   Panel B: The Effect on Coding Task Completion
                        (1)        (2)            (3)             (4)           (5)           (6)
                      >20 (1)    >20 (1-4)      >20 (1-8)       >45 (1)       >45 (1-4)     >45 (1-8)
   Task Incentive     0.038***    0.031***      0.025***        0.035***       0.024***     0.020***
                       (0.009)     (0.006)       (0.005)         (0.008)        (0.006)      (0.005)
   Obs.                 714          714             714             714             714       714
   R2                  0.043        0.059           0.069           0.041           0.057     0.075
   Control Mean        0.280        0.212           0.158           0.174           0.156     0.116
   Controls             Yes          Yes             Yes             Yes             Yes       Yes
   Campus FE            Yes          Yes             Yes             Yes             Yes       Yes

This table shows estimates for the effect of coding-task incentives on plan making and coding task
completion. We use "Task Incentive" as shorthand for coding-task incentive (in dollars). Panel
A shows estimates of the effect of coding-task incentives on whether or not participants made a
plan. Column (1) shows the effect of coding-task incentives in the first week of the experiment.
Column (2) shows the average effect over the first four weeks. Column (3) shows the effect over the
entire experiment. Panel B shows the effect of coding-task incentives on coding task completion.
Columns (1-3) show the effect on an indicator variable for whether or not the participant worked
on the coding task for more than 20 minutes: Column (1) estimates the effect over the first week,
Column (2) over the first four weeks, and Column (3) over the entire experiment. Columns (4-6)
show analogous estimates, but for an indicator variable for whether or not the participant worked
on the coding task for more than 45 minutes each week. Standard errors are shown in parentheses.
*p < 0.1, **p < 0.05, ***p < 0.01.




                                                    35
                          Table 4: Willingness to Pay for Reminders
                                        (1)             (2)         (3)               (4)
                                       OLS              OLS         OLS              Tobit
                                    WTP ($)          WTP ($)     WTP ($)            WTP ($)
     Incentive ($)                     0.15***           0.17***        0.17***      0.06***
                                       (0.011)           (0.024)        (0.020)      (0.006)
     Observations                      3776               3775           3776         3776
     Number of Participants             944                944            944          944
     Participant FE                     Yes               Yes             Yes          No
     Mean WTP, $2 Incentive            0.49               0.58           0.92         0.42
     Censoring Specification       Median Survey      Survey Resp.      $5 Top        Tobit

This table presents estimates of how individual's willingness to pay (WTP) for reminders varies
with incentives. The columns vary how they treat censored responses--i.e., responses that were
at the boundary of the multiple price list presented to participants. In Column (1), we replace a
participant's WTP if they are top-censored with the median reported WTP from an unincentivized
survey question among all top-censored participants within a particular incentive level. In Column
(2), we replace this value with the participant's own reported WTP from that unincentivized survey
question. In Column (3), we replace this value with $5.00. In Column (4), we estimate the regression
using a Tobit estimator. Values are represented in dollars. The number of observations falls by
one in Column (2) because one participant did not complete one of the survey questions. Standard
errors, clustered by participant, are shown in parentheses. *p < 0.1, **p < 0.05, ***p < 0.01.



       Table 5: Willingness to Pay for     Reminders by Survey Completion Incentive
     Mean WTP           $2 Incentive       $3 Incentive  $4 Incentive   $5 Incentive
                        0.49               0.60          0.82           0.91
                        (0.022)            (0.029)       (0.038)        (0.040)

     Difference in WTP                      $3 Incentive   $4 Incentive   $5 Incentive
                                            - $2 Incentive - $3 Incentive - $4 Incentive
                                            0.11           0.22           0.09
                                            (0.037)        (0.048)        (0.055)

Notes: This table presents mean willingness to pay (WTP) for the reminder, and how it changes
with the incentive for completing the survey. The top row presents the mean WTP in each incentive
condition. The second row presents how the WTP increases from one incentive condition to the
next. The censored responses are as in column (1) of Table 4: we replace a participant's WTP
if they are top-censored with the median reported WTP from an unincentivized survey question
among all top-censored participants within a particular incentive level. Standard errors are shown
in parentheses.




                                                 36
                  Table 6: Willingness to Pay for Reminders: Order Effects
                                   (1)                 (2)                    (3)                 (4)
                                   OLS                OLS                    OLS                 OLS
                              1st Incentive   1st and 2nd Incentive    1st­3rd Incentive    All Incentives
 Incentive ($)                  0.09***              0.12***                0.13***            0.15***
                                (0.029)              (0.017)                (0.012)            (0.011)
 Observations                    944                 1888                   2832               3776
 Participant FE                  No                   Yes                    Yes                Yes
 Mean WTP, $2 Incentive         0.61                 0.59                   0.59               0.58
 Censoring Specification    Median Survey        Median Survey          Median Survey      Median Survey


This table presents estimates of how individual's willingness to pay (WTP) for reminders varies
with incentives, by whether the WTP for reminders elicitation includes only the first elicitation
(column 1), the first and second elicitations (column 2), the first through third elicitations
(column 3), or all four elicitations (column 4). The censored responses are treated as in column
(1) of Table 4: we replace a participant's WTP if they are top-censored with the median reported
WTP from an unincentivized survey question among all top-censored participants within a
particular incentive level. Standard errors, clustered by participant, are shown in parentheses.
*p < 0.1, **p < 0.05, ***p < 0.01.



              Table 7: Completion Rates by Incentive Size and Reminder Group
 Experimental Group        All Participants   $2 Incentive   $3 Incentive   $4 Incentive   $5 Incentive
 Reminder                        0.80             0.79           0.79           0.82           0.80
 No Reminder                     0.43             0.29           0.47           0.47           0.47

 Difference                       0.37            0.50            0.32          0.35          0.33
                              (0.031)***       (0.060)***     (0.065)***     (0.060)***    (0.061)***
 Number of Participants           847              208            199            223           217
This table presents survey completion rates for different incentive sizes and reminder groups. The
"Reminder" and "No Reminder" rows present completion rates for the group that randomly received
the reminders and the group that randomly did not. Results are shown by incentive level and for all
incentive levels pooled together. The "Difference" row shows the difference between the reminder
experimental group and the no reminder group. Standard errors for the difference are shown in
parentheses. *p < 0.1, **p < 0.05, ***p < 0.01. The final row shows the number of participants in
the pooled sample and in each incentive level.




                                                37
               Appendices (not for publication)
A       Mathematical appendix
A.1     Further discussion of the framework
We extend the model such that bi is draw from some distribution Gi , and is realized only in
the future when the individual has an opportunity to choose a = 0. Our only assumption is
that Gi (·) has a continuous density function for each i, and that Gi (x) is smoothly distributed
for each x.
    In period 2, individuals are attentive with probability µ. If individuals are inattentive
they default to a = 0. If individuals are attentive, they choose whether or not to complete
the task, with action a = 1 denoting completion and a = 0 denoting not completion. The
benefits of choosing a = 1 over a = 0 are value r + b, where b > 0 is the intrinsic benefits and
r is the observable pecuniary incentive. We assume that b is drawn from some distribution
Gi that is independent of  . Thus, with some abuse of notation, we sometimes denote this
distribution by G . Individuals only know Gi in period 1. They learn the realization b in
period 2 only (if they are attentive).
    Given an attention technology j , the net utility benefit of an attention level µ is therefore

                                          µBi (r) - Kij (µ)

where
                                 Bi (r) :=            (b + r)dGi (b)                          (4)
                                              b+r>0

Under the assumption that utility is locally linear in the pecuniary incentives, rationally
inattentive individuals choose j and µ to maximize

                     Ui (j, µ|p, r) = µ           (x + r)dGi (x) - Kij (µ) - pj               (5)
                                          x+r>0


    Define the indirect utility functions



                      Mi1 (r) := max µ               (x + r)dGi (x) - Kij (µ)                 (6)
                                  µ          x+r>0

                      Mi0 (r) := max µ               (x + r)dGi (x) - Ki0 (µ))                (7)
                                  µ          x+r>0




                                                   38
   Our main assumption is:

Assumption A. Mi1 (r) and Mi1 (r) - Mi0 (r) are smoothly distributed in i for each r.

    As one example under which this assumption holds, suppose that types can be partitioned
into three-dimensional types i = (,  1 ,  2 ), and that Kij = K   j
                                                                    +  j where  j is a random
variable on R that possesses a continuous density function, and that is independent of Gi (b).
In this case,  j corresponds to the nuisance cost associated with choosing technology j . As
another example, suppose that Ki0 = K     0
                                            and Ki1 = K , where  is random variable on R+
that is independent of Gi (b). Here, the interpretation is that  captures individual differences
in the extent to which j = 1 reduces the marginal costs of attention.


A.2     Proof of Proposition 1 under more general assumptions
Proof of part 1

Proof. Individuals who maximize the function in (5) choose technology j = 1 iff p 
Mi1 (r) - Mi0 (r). Individual i's WTP for technology j = 1 is thus Wi (r) := Mi1 (r) - Mi0 (r).
Assumption A implies that Wi is differentiable almost everywhere. Let µj  i denote individual
i's optimal choice of µ given technology j . Repeatedly applying the Envelope Theorem thus
implies that


                 d                 d 1           d 0
                    E [Wi (r)] = E    Mi (r) - E   M (r)
                 dr                dr            dr i
                             = E µ1                       0
                                  i · (1 - Gi (-r )) - E µi · (1 - Gi (-r ))                       (8)
                             = P r(a = 1|j = 1, r) - P r(a = 1|j = 0, r)                           (9)

                                                          d                  d
where the expectations are taken over all i for which     dr
                                                             Mi1 (r)   and   dr
                                                                                Mi0 (r)   exist.

Proof of part 2

Proof. Define
                                  Vi (p, r) = max Ui (j, µ|p, r)
                                              j,µ

Assumption A implies that Vi is differentiable almost everywhere in p and r, and thus
repeated application of the Envelope Theorem implies that




                                               39
d d                  d      d
      E[Vi (p, r)] = E         Vi (p, r)
dp dr                dp     dr
                     d                       d 1                                d 0
                   =     P r(j = 1|p, r)E       Mi (r) + P r(j = 1|p, r)EE         M (r)
                     dp                     dr                                  dr i
                     d
                   =     P r(j = 1|p, r)E µ1                                          0
                                             i · (1 - Gi (-r )) + P r (j = 0|p, r )E µi · (1 - Gi (-r ))
                     dp
                     d
                   =    [P r(j = 1|p, r)P r(a = 1|j = 1, r) + P r(j = 0|p, r)P r(a = 1|j = 0, r)]
                     dp
                     d
                   = P r(a = 1|p, r)
                     dp
                                                            d                  d
where the expectations are taken over all i for which       dr
                                                               Mi1 (r)   and   dr
                                                                                  Mi0 (r)   exist.
  Similarly,


                              d d                  d    d
                                    E[Vi (p, r)] = E       Vi (p, r)
                              dr dp                dr   dp
                                                      d
                                                 = - P r(j = 1|p, r)
                                                     dr
                                                           d               d
where the expectation is taken over all i for which dr       Mi1 (r) and   dr
                                                                              Mi0 (r)   exist.
         d d                 d d
  Since dp dr
              E[Vi (p, r)] = dr dp
                                   E[Vi (p, r)], the result follows.


A.3      A parametric model of overconfidence about future attentive-
         ness
Suppose that consumers are overconfident about being attentive to the activity in the future.
In particular, when in reality they are inattentive with probability 1 - µ, they think they will
be inattentive with probability (1 - µ), where  < 1. Equivalently, individuals' perceived
                 ^ = (1 - ) + µ. Let µ
attentiveness is µ                         ^j
                                            i be individual i's perceived attentiveness if they
have technology j .
    Extending equation (9),

       d
                     ^1
          Wi (r) = E µ i · (1 - Gi (-r )) - E µ ^0
                                                 i · (1 - Gi (-r ))
       dr
                 = E 1 -  + µ1     i · (1 - Gi (-r )) - E     1 -  + µ0
                                                                      i · (1 - Gi (-r ))

                 =  E µ1                   0
                       i (1 - Gi (-r )) - µi · (1 - Gi (-r ))

                 =  [P ri (a = 1|j = 1) - P ri (a = 1|j = 0)] .


                                                  40
                                                    ^i and to denote perceived utilities, we
Similarly, following the proof of part 2, and using V
have that
d d ^                 d      d ^
      E[Vi (p, r)] =    E       Vi (p, r)
dp dr                dp      dr
                      d
                   =      P r(j = 1|p, r)E µ^1                                         ^0
                                              i · (1 - Gi (-r )) + P r (j = 1|p, r )EE µ i · (1 - Gi (-r ))
                     dp
                      d
                   =      P r(j = 1|p, r)E 1 -  + µ1      i · (1 - Gi (-r ))
                     dp
                     d
                   +      P r(j = 0|p, r)E 1 -  + µ0     i · (1 - Gi (-r ))
                     dp
                      d
                   =     [P r(j = 1|p, r) + P r(j = 0|p, r)] (1 - )
                     dp
                       d
                   +  [P r(j = 1|p, r)P r(a = 1|j = 1, r) + P r(j = 0|p, r)P r(a = 1|j = 0, r)]
                      dp
                       d
                   =  [P r(j = 1|p, r)P r(a = 1|j = 1, r) + P r(j = 0|p, r)P r(a = 1|j = 0, r)]
                      dp
                       d
                   =  P r(a = 1|p, r)
                      dp

On the other hand,

                                d d ^                d    d ^
                                      E[Vi (p, r)] = E       Vi (p, r)
                                dr dp                dr   dp
                                                        d
                                                   = - P r(j = 1|p, r)
                                                       dr

It thus follows that
                                   d                d
                                      P r(j = 1) = - P r(a = 1).
                                   dr               dp




                                                     41
B     Additional results and screenshots for experiment 1
B.1   Additional results




                            42
       Table B.1: The Effect of Coding-Task Incentives on Coding Task Completion
                                                (1)         (2)         (3)
                                               Week 1     Weeks 1-4   Weeks 1-8
                         >0                    0.036***   0.032***    0.026***
                                                (0.009)    (0.007)     (0.006)

                         Obs.                    714         714         714
                         R2                     0.039       0.059       0.064
                         Control Mean           0.385       0.278       0.210
                         >10                   0.037***   0.034***    0.027***
                                                (0.009)    (0.007)     (0.006)

                         Obs.                    714         714         714
                         R2                     0.047       0.067       0.072
                         Control Mean           0.339       0.243       0.179
                         >30                   0.036***   0.027***    0.023***
                                                (0.009)    (0.006)     (0.005)

                         Obs.                    714         714         714
                         R2                     0.043       0.053       0.068
                         Control Mean           0.239       0.186       0.138
                         >40                   0.038***   0.026***    0.021***
                                                (0.009)    (0.006)     (0.005)

                         Obs.                    714         714         714
                         R2                     0.044       0.058       0.074
                         Control Mean           0.183       0.161       0.119
                         >50                   0.032***   0.022***    0.017***
                                                (0.008)    (0.005)     (0.004)

                         Obs.                    714         714         714
                         R2                     0.037       0.058       0.071
                         Control Mean           0.165       0.142       0.107
                         >60                   0.027***   0.019***    0.013***
                                                (0.008)    (0.005)     (0.004)

                         Obs.                    714         714         714
                         R2                     0.044       0.066       0.066
                         Control Mean           0.138       0.118       0.093
                         Controls                Yes        Yes         Yes
                         Campus × Student FE     Yes        Yes         Yes


This table presents estimates for the effect of coding-task incentives (in dollars) on coding task
completion. Each panel of the table corresponds to an analysis of whether participants completed
at least that number of minutes of the coding task in a given week. The columns correspond to
different periods during the experiment over which the effect of the incentives is tested: Column
(1) shows the effect in the first week, Column (2) shows the effect during the first four weeks, and
Column (3) shows the effect over the entire experiment. In Column (1), the dependent variable
is an indicator for whether a participant completed at least that many minutes of the coding task
in the first week. In Columns (2) and (3), the dependent variable is the mean of the indicators,
constructed as in Column (1), for each of the weeks being considered. Each panel-by-column
corresponds to a separate specification, and thus 18 distinct specifications are shown in the table.
Standard errors are shown in parentheses. *p < 0.1, **p < 0.05, ***p < 0.01



                                                 43
       Table B.2: The Effect of Plan-Making Incentives on Coding Task Completion
                                                 (1)        (2)         (3)
                                                Week 1    Weeks 1-4   Weeks 1-8
                          >0                    0.037*     0.029*       0.014
                                                (0.022)    (0.015)     (0.012)

                          Obs.                    705        705         705
                          R2                     0.041      0.040       0.046
                          Control Mean           0.385      0.278       0.210
                          >10                   0.037*     0.027*       0.014
                                                (0.021)    (0.014)     (0.011)

                          Obs.                    705        705         705
                          R2                     0.045      0.045       0.046
                          Control Mean           0.339      0.243       0.179
                          >30                   0.045**    0.023*       0.010
                                                (0.020)    (0.013)     (0.010)

                          Obs.                    705        705         705
                          R2                     0.054      0.042       0.045
                          Control Mean           0.239      0.186       0.138
                          >40                   0.036**     0.019       0.008
                                                (0.018)    (0.012)     (0.009)

                          Obs.                    705        705         705
                          R2                     0.034      0.036       0.041
                          Control Mean           0.183      0.161       0.119
                          >50                   0.034*      0.015       0.005
                                                (0.018)    (0.011)     (0.008)

                          Obs.                    705        705         705
                          R2                     0.035      0.039       0.042
                          Control Mean           0.165      0.142       0.107
                          >60                   0.027*      0.013       0.002
                                                (0.016)    (0.010)     (0.008)

                          Obs.                    705        705         705
                          R2                     0.044      0.038       0.042
                          Control Mean           0.138      0.118       0.093
                          Controls               Yes        Yes         Yes
                          Campus × Student FE    Yes        Yes         Yes


This table presents estimates for the effect of plan-making incentives (in dollars) on coding task
completion. Each panel of the table corresponds to an analysis of whether participants completed
at least that number of minutes of the coding task in a given week. The columns correspond to
different periods during the experiment over which the effect of the incentives is tested: Column
(1) shows the effect in the first week, Column (2) shows the effect during the first four weeks, and
Column (3) shows the effect over the entire experiment. In Column (1), the dependent variable
is an indicator for whether a participant completed at least that many minutes of the coding task
in the first week. In Columns (2) and (3), the dependent variable is the mean of the indicators,
constructed as in Column (1), for each of the weeks being considered. Each panel-by-column
corresponds to a separate specification, and thus 18 distinct specifications are shown in the table.
Standard errors are shown in parentheses. *p < 0.1, **p < 0.05, ***p < 0.01



                                                  44
   Table B.3: The Effect of Plan-Making Incentives on Coding Task Completion (2SLS)
                        Panel A: The Effect on Plan Making (First Stage)
                                          (1)               (2)                 (3)
                                         Week 1           Weeks 1-4           Weeks 1-8
                        $1 Plan         0.282***          0.285***             0.240***
                                         (0.048)           (0.033)              (0.030)
                        $2 Plan         0.368***          0.297***             0.242***
                                         (0.033)           (0.028)              (0.025)
                        Obs.               705                 705               705
                        R2                0.144               0.189             0.157
                        Control Mean      0.381               0.150             0.082
                        Controls           Yes                 Yes               Yes
                        Campus FE          Yes                 Yes               Yes


              Panel B: The Effect on Coding Task Completion (Reduced Form)
                               (1)        (2)         (3)           (4)         (5)         (6)
                             >20 (1)    >20 (1-4)   >20 (1-8)     >45 (1)     >45 (1-4)   >45 (1-8)
              $1 Plan          0.034      0.017       0.013        0.034        -0.000      0.005
                              (0.048)    (0.032)     (0.025)      (0.043)      (0.027)     (0.021)
              $2 Plan         0.079*     0.054**      0.026       0.076**       0.032       0.012
                              (0.040)    (0.027)     (0.021)      (0.036)      (0.023)     (0.018)
              Obs.              705        705         705             705       705         705
              R2               0.057      0.049       0.051           0.036     0.035       0.041
              Control Mean     0.280      0.212       0.158           0.174     0.156       0.116
              Controls          Yes        Yes         Yes             Yes       Yes         Yes
              Campus FE         Yes        Yes         Yes             Yes       Yes         Yes


              Panel C: The Effect of Plan Making on Coding Task Completion (IV)
                               (1)        (2)         (3)           (4)         (5)         (6)
                             >20 (1)    >20 (1-4)   >20 (1-8)     >45 (1)     >45 (1-4)   >45 (1-8)
              Plan Making    0.203**     0.146*       0.092       0.197**       0.076       0.041
                             (0.102)     (0.080)     (0.078)      (0.093)      (0.070)     (0.066)
              Obs.              705        705         705             705       705         705
              R2               0.143      0.151       0.120           0.091     0.094       0.076
              Control Mean     0.280      0.212       0.158           0.174     0.156       0.116
              Controls          Yes        Yes         Yes             Yes       Yes         Yes
              Campus FE         Yes        Yes         Yes             Yes       Yes         Yes


This table shows estimates for the effect of plan-making incentives on plan making and coding
task completion using treatment dummies rather than a linear plan-making incentive variable.
Panel A shows estimates of the effect of plan-making incentives on whether or not participants
made a plan. Column (1) shows the effect of plan-making incentives in the first week of the
experiment. Column (2) shows the average effect over the first four weeks. Column (3) shows the
average effect over the entire experiment. Panel B shows the effect of plan-making incentives on
coding task completion. Columns (1-3) show the effect on an indicator variable for whether or not
the participant worked on the coding task for more than 20 minutes: Column (1) estimates the
effect over the first week, Column (2) over the first four weeks, and Column (3) over the entire
experiment. Columns (4-6) show analogous estimates, but for an indicator variable for whether
or not the participant worked on the coding task for more than 45 minutes each week. Panel C
shows the 2SLS estimates instrumenting for whether or not participants made a plan using the
plan-making treatment dummies as instruments. The dependent variables are the same as those in
Panel B. Standard errors are shown in parentheses. *p < 0.1, **p < 0.05, ***p < 0.01.


                                                     45
B.2   Screenshots

              Figure B.1: Pay-to-Plan Treatment Emails, Week 1
                           $1 Plan-Making Incentive




                          $2 Plan-Making Incentive




                                    46
Figure B.2: Pay-to-Code Treatment Emails, Week 1
             $2 Coding-Task Incentive




            $5 Coding-Task Incentive




                      47
    Figure B.3: Combined and Control Group Emails, Week 1
Combined Treatment ($1 Plan-Making and $2 Coding-Task Incentive)




                         Control Group




                              48
Figure B.4: Weekly Reminder Email, All Groups




                     49
C      Additional sample details, results, and screenshots
       for experiment 2
C.1      Sample details
A total of 1,330 participants were recruited for our study, and we analyze data from 944. The
other 386 individuals were excluded from the study for one of the following seven reasons:

    1. [14 individuals] Indicating that they were unavailable during the window when they
       would have needed to complete the survey in the second part of the study (see Appendix
       Figure C.1).

    2. [12 individuals] Incorrectly entering the captcha both times it was asked of them (see
       Appendix Figures C.4 and C.5).

    3. [245 individuals] Incorrectly answered one or both of the understanding questions about
       the reminders and bonus structure (see Appendix Figure C.7).

    4. [25 individuals] Did not complete the first part of the study.

    5. [79 individuals] Responding inconsistently on at least one of the MPL responses.

    6. [4 individuals] Attempting to complete the first part of the study after the recruitment
       quota had been filled.

    7. [7 individuals] Providing an invalid MTurk ID.

Note that we have no data from anyone excluded for one of the first three reasons. These
individuals were excluded from continuing from the study based on their response (see, for
example, Appendix Figure C.2, which was shown to those excluded for reason 1). In all cases,
participants were excluded immediately after they: indicated that they would be unavailable
to complete part 2 (reason 1), failed the captcha the second time (reason 2), or answered an
understanding question incorrectly (reason 3). Participants who were excluded in this way
did not provide any answers to any of the MPL questions. An advantage of excluding these
participants from the study ex ante--rather than collecting their data and excluding it ex
post--is that it ties our hands to only analyze data from participants who are attentive and
clearly understand the study instructions.
    We have incomplete data from anyone excluded for the fourth reason. These first four
groups total 296 individuals. Consequently, we have complete responses to part 1 from 1,034
participants, which is the number reported in the main text.

                                              50
   As indicated in the fifth reason for exclusion, 79 participants answered inconsistently on
one or more MPLs. These participants may have misunderstood the MPL or made choices
randomly. We drop them because we cannot define a WTP for the reminder emails when
responses are inconsistent on the MPL.


C.2     Additional results

               Table C.1: Willingness to Pay for Reminders: Randomized 90%


                                       (1)               (2)            (3)           (4)
                                       OLS               OLS            OLS          Tobit
                                      WTP ($)           WTP ($)        WTP ($)      WTP ($)
      Incentive ($)                    0.15***           0.17***        0.17***      0.06***
                                       (0.012)           (0.027)        (0.021)      (0.006)
      Observations                     3396               3395           3396         3396
      Number of Participants            849                849            849          849
      Participant FE                    Yes               Yes             Yes          No
      Mean WTP, $2 Incentive           0.50               0.60           0.94         0.43
      Censoring Specification      Median Survey      Survey Resp.      $5 Top        Tobit




This table presents estimates of how individual's willingness to pay (WTP) for reminders varies
with incentives for the 90% of participants who were randomly assigned to receive or not receive
the reminders. The columns vary how they treat censored responses--i.e., responses that were at
the boundary of the multiple price list presented to participants. In Column (1), we replace a
participant's WTP if they are top-censored with the median reported WTP from an unincentivized
survey question among all top-censored participants within a particular incentive level. In Column
(2), we replace this value with the participant's own reported WTP from that unincentivized survey
question. In Column (3), we replace this value with $5.00. In Column (4), we estimate the regression
using a Tobit estimator. Values are represented in dollars. The number of observations falls by
one in Column (2) because one participant did not complete one of the survey questions. Standard
errors, clustered by participant, are shown in parentheses. *p < 0.1, **p < 0.05, ***p < 0.01.




                                                 51
             Table C.2: Willingness-to-Pay by Experimental Incentive Ordering

                                 $2 Incentive    $3 Incentive    $4 Incentive    $5 Incentive
      1st Incentive                  0.53            0.66            0.79            0.77
                                   (0.044)         (0.058)         (0.079)         (0.079)
      1st and 2nd Incentive          0.50            0.64            0.77            0.83
                                   (0.030)         (0.042)         (0.054)         (0.055)
      1st­3rd Incentive              0.49            0.61            0.81            0.87
                                   (0.026)         (0.034)         (0.043)         (0.045)
      All Incentives                 0.49            0.60            0.82            0.91
                                   (0.022)         (0.029)         (0.038)         (0.040)

This table presents the average willingness to pay (WTP) for reminders in each incentive level, by
order in which it was answered by participants. The columns vary by incentive level. The first three
rows report average WTP for reminders at that incentive level for all participants for whom that
incentive was: (i) the first incentive that they were asked about; (ii) the first or second incentive;
(iii) the first, second, or third incentive. The fourth row shows all the data. The censored responses
are treated as in column (1) of Table 4, we replace a participant's WTP if they are top-censored
with the median reported WTP from an unincentivized survey question among all top-censored
participants within a particular incentive level. Standard errors, clustered by participant, are shown
in parentheses. *p < 0.1, **p < 0.05, ***p < 0.01.




                                                 52
C.3    Screenshots

                              Figure C.1: Eligibility Screen




This is the screen in which the participant entered his or her MTurk ID and indicated
availability to complete part 2 of the study in the designated window.



                              Figure C.2: If Ineligible Screen




If participants had participated in the study at an earlier date or indicated they were not
available in the designated window, they were shown this screen and excluded from partici-
pating.




                                            53
        Figure C.3: Consent Form




Figure C.4: Attention Check (first attempt)




                    54
                       Figure C.5: Attention Check (second attempt)




If participants answered the attention check question incorrectly the first time, they saw this
screen which warned them that failure to enter the sequence correctly would remove them
from the study.



                             Figure C.6: Instructions, Screen 1




                                              55
Figure C.7: Understanding Questions for Instructions, Screen 1




                             56
Figure C.8: Instructions, Screen 2




               57
Figure C.9: Instructions, Screen 3




               58
             Figure C.10: Example Multiple Price List with Incentive Level of $2




This figure shows the multiple price list when the incentive for completing the survey is $2. In row 1, the
option on the left is to get a part 1 bonus payment of $1.50 and the three reminder emails while the option
on the right is to get no part 1 bonus payment and no reminder emails. In the next 5 rows (i.e., rows 2­6),
the option on the right remains the same while the option on the left has a part 1 bonus (that accompanies
the reminder emails) that decreases in $0.25 increments. In row 6, participants choose between getting and
not getting the reminder emails (with no part 1 bonus payment associated with either option). In the next
5 rows (i.e., rows 7­11), the option on the left remains the same (i.e., getting the reminder emails and no
part 1 bonus) while the option on the right has a part 1 bonus that increases in $0.25 increments. In row
11, the option on the left is to get the reminder emails and receive no part 1 bonus while the option on the
right is to get no reminder emails and a part 1 bonus of $1.50. Participants saw a version of this screen four
times, once for each possible incentive for completing the survey: $2, $3, $4, and $5. The order of these four
MPL questions were randomized at the participant level.
                                                      59
            Figure C.11: Unincentivized Question if Censored at W T P $1.50




Participants who chose the option on the left in the last row of the MPL, indicating they
valued reminders more than $1.50, were shown this screen to elicit an unincentivized will-
ingness to pay beyond the $1.50 maximum. Participants were required to enter a number
that was greater than 150 cents.



           Figure C.12: Unincentivized Question if Censored at W T P  -$1.50




Participants who chose the option on the right in the first row of the MPL, indicating
they valued reminders less than -$1.50, would have been shown this screen to elicit an
unincentivized willingness to pay beyond the -$1.50 minimum. Participants would have
been required to enter a number that was greater than 150 cents. In practice, no participant
was censored in this way and so this screen was never shown to participants.



                                            60
                     Figure C.13: Final Screen: No reminder emails




Participants who would not receive reminder emails were shown this screen at the end of
part 1 of the study.



                       Figure C.14: Final Screen: Reminder emails




Participants who would receive reminder emails were shown this screen at the end of part 1
of the study.




                                           61
