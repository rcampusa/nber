                                   NBER WORKING PAPER SERIES




PRICE AND REAL OUTPUT MEASURES FOR THE EDUCATION FUNCTION OF GOVERNMENT:
        EXPLORATORY ESTIMATES FOR PRIMARY & SECONDARY EDUCATION

                                           Barbara M. Fraumeni
                                           Marshall B. Reinsdorf
                                           Brooks B. Robinson
                                           Matthew P. Williams

                                           Working Paper 14099
                                   http://www.nber.org/papers/w14099


                         NATIONAL BUREAU OF ECONOMIC RESEARCH
                                  1050 Massachusetts Avenue
                                    Cambridge, MA 02138
                                         June 2008




   This paper was written when all four authors were at the U.S. Bureau of Economic Analysis (BEA).
   The analysis and results in this paper are those of the authors, not of BEA. The views expressed herein
   are those of the author(s) and do not necessarily reflect the views of the National Bureau of Economic
   Research.

   NBER working papers are circulated for discussion and comment purposes. They have not been peer-
   reviewed or been subject to the review by the NBER Board of Directors that accompanies official
   NBER publications.

   © 2008 by Barbara M. Fraumeni, Marshall B. Reinsdorf, Brooks B. Robinson, and Matthew P. Williams.
   All rights reserved. Short sections of text, not to exceed two paragraphs, may be quoted without explicit
   permission provided that full credit, including © notice, is given to the source.
Price and Real Output Measures for the Education Function of Government: Exploratory Estimates
for Primary & Secondary Education
Barbara M. Fraumeni, Marshall B. Reinsdorf, Brooks B. Robinson, and Matthew P. Williams
NBER Working Paper No. 14099
June 2008
JEL No. O40

                                              ABSTRACT

In a previous paper, the authors took the first step in their research on measuring the education function
of government by estimating real output measures (Fraumeni, et. al. 2004). In this paper, chain-type
Fisher quantity indexes for those output measures are calculated to be more consistent with Bureau
of Economic Analysis (BEA) methodology and the real output measures presented in the previous
paper are refined. In addition, and more importantly, implicit price deflators are presented to give a
more complete picture. Alternative price and real output measures are compared; it is clear that methodology
choice matters. Price change is always greater than quantity change for the periods given; however,
price changes are overstated to the extent quality changes are not captured in the quantity indexes.
Quality-adjustments continue to be the most challenging aspect of decomposing nominal expenditures
for government-provided education into price and quantity components.


Barbara M. Fraumeni                                  Brooks B. Robinson
Muskie School of Public Service                      U.S. PACOM
University of Southern Maine                         HQ, USPACOM J5
P.O. Box 9300                                        Box 64015
Portland, ME 04104-9300                              Camp H. M Smith, HI 96861-4015
and NBER                                             brooks.robinson@pacom.mil
Barbara.Fraumeni@maine.edu
                                                     Matthew P. Williams
Marshall B. Reinsdorf                                105 E. 100th St.
Bureau of Economic Analysis                          New York, NY 10029
US Department of Commerce                            mpw2109@columbia.edu
1441 L Street, NW, Mail Stop BE-40
Washington, DC 20230
marshall.reinsdorf@bea.gov
    Price and Real Output Measures for the Education Function of Government:
            Exploratory Estimates for Primary & Secondary Education
    Barbara M. Fraumeni, Marshall B. Reinsdorf, Brooks B. Robinson, & Matthew P. Williams1

Abstract:

In a previous paper, the authors took the first step in their research on measuring the
education function of government by estimating real output measures (Fraumeni, et. al.
2004). In this paper, chain-type Fisher quantity indexes for those output measures are
calculated to be more consistent with Bureau of Economic Analysis (BEA) methodology
and the real output measures presented in the previous paper are refined. In addition, and
more importantly, implicit price deflators are presented to give a more complete picture.
Alternative price and real output measures are compared; it is clear that methodology
choice matters. Price change is always greater than quantity change for the periods
given; however, price changes are overstated to the extent quality changes are not
captured in the quantity indexes. Quality-adjustments continue to be the most
challenging aspect of decomposing nominal expenditures for government-provided
education into price and quantity components.


Introduction



This paper presents new measures of real output for government-provided education in

the United States. The research refines the measures in our previous experimental work

(Fraumeni, et. al. 2004) and also takes important steps forward by calculating chain-type

Fisher quantity indexes and implicit price deflators.



Measuring the education output of the government is difficult, even though education is a

near-market activity. For services, defining nominal output measures can be problematic

and measuring real output is challenging (Griliches, 1994). Education is a service with

significant nonmarket inputs, notably student and parent time, and the outcome of

1
  This paper represents views of the authors and is not an official position of the Bureau of Economic
Analysis or the Department of Commerce. All of the authors were employees of the Bureau of Economic
Analysis when this paper was drafted. Marshall B. Reinsdorf is the only author currently at the Bureau of
Economic Analysis. We thank Michael Christian and Barbara Silk, both formerly employees of BEA, for
their assistance on this project.


                                                                                                            1
education depends upon factors outside the control of providers of educational services,

such as student ability, family, peer group, and neighborhood factors (Rivkin, 2000).

Accordingly, isolating the contribution of providers of educational services is not easy.

In addition, not all benefits of education are measurable, because it has broader effects on

the welfare of individuals and of society than, for example, just raising earnings. As this

research continues, the exploratory measures presented may be substantially altered and

refined, and will be expanded to include other levels and types of education.



The objective of the government is to educate all individuals of school age, including

those least and most able. The cost of educating students will vary substantially across

students, with the cost particularly high for special education students and those requiring

supplemental help beyond that available in a typical classroom. A recent National

Education Association (NEA) report indicates that the average U.S. cost per special

education student is more than twice the average cost across all students. As well, the

report notes that the number of special education students has risen 30 percent over the

last 10 years.2 Educating these students is clearly more expensive than other types of

students. Bringing about marginal improvements in their educational attainment is

probably also more expensive than for more able students. Our current experimental

output measures do not adjust for student composition except to reflect the number of

students in high school vs. lower grades. Accordingly, given the growth in special

education students and the associated higher costs, it is not surprising that the price

measures presented in this paper grow at a faster rate than the gross domestic product

(GDP) or gross domestic purchases price indexes. In addition, to the extent that our
2
    National Education Association (2004).


                                                                                              2
measures do not capture all quality improvements occurring over time, quantity changes

may be underestimated and price changes may be overestimated.



In 2001, the education function of government accounted for approximately 5 percent of

nominal GDP, as measured by final expenditures, ranking it with health and income

security as among the three largest government function categories.3 BEA began

publishing functional tables with quantity and/or price indexes for government in 2004.

However, these output quantity and price indexes are estimated with a cost-of-inputs-

based approach as is currently performed for total government, Federal, and state and

local.4 Such input-based approaches do not recognize changes in output resulting from

intangible inputs or from varying relationships between inputs and outputs such as those

arising from qualitative changes, and they do not allow for a meaningful estimate of

productivity change.



Output-based measures of government output are preferred to input-based measures of

output, but are difficult to develop and implement. In recent years national income

accountants in other countries have looked to volume indicators using an output approach

to improve measures of government education output (Powell & Pritchard, 2002; Konijn

& Kleima, 2000a; Australian Bureau of Statistics, OECD 2000).5 The emphasis in all


3
  See tables 1.1.5, 3.15.5, and 3.16 of the Bureau of Economic Analysis’ (BEA’s) National Income and
Product Accounts (NIPA). As government by function tables (3.15 and 3.16) appear later than other NIPA
tables (last published in the October of 2002 Survey of Current Business (pp. 12-13)), the data cited in this
paper do not reflect results of the NIPA comprehensive revision published in December 2003.
4
  The 2003 comprehensive revision new NIPA table family 3.10 presents an alternative breakout of
consumption expenditures.
5
  Following the international System of National Accounts (see OECD 1993), most countries use the term
“volume” to refer to what U.S economists typically call “quantity.” In this paper, the terms are used
interchangeably.


                                                                                                            3
cases has been on real output, or volume, measures rather than on price measures. These

volume indicators, such as those based on number of pupils or hours spent in school, may

or may not be quality adjusted. Others have suggested an outcome-based approach

directly to adjust for quality change, such as those based on test scores or incremental

earnings (O’Mahony & Stevens, 2004; Jorgenson & Fraumeni, 1992).6 A third approach

is a housing value approach, such as those that look at differential prices paid for houses

near borders of school districts with differential performance ratings (Black, 1998).



Volume indicators using an output approach are commonly not really independent of

input measures. For example, teacher experience and pupil-teacher quality-adjustments

both depend upon an input measure. Although measures are becoming less reliant on

input measures than previously as real output is not set equal to real input, education

output measures still frequently rely on input measures.



This exploratory paper begins by presenting a simple education production function and

discussing the issue of outputs versus outcomes. It next summarizes and analyzes the

progress made by other countries to measure the education output of government to set

the stage for a description of the U.S. initial efforts.7 It then focuses on a few possible

quality-adjusted volume indicators for the United States for primary and secondary public

education.8 Subsequent research at BEA will continue this line of investigation and will



6
  Currie and Thomas (1999) show the relationship between test scores and future educational attainment
and labor market outcomes.
7
  Recent attempts to measure the education output of government in the national accounts from the output-
side (as opposed to the input side) began outside the United States.
8
  In this paper, primary education refers to kindergarten through grade 8 education and secondary education
refers to grade 9 through 12 education.


                                                                                                         4
look at quality-adjusted volume indicators for public higher education, libraries, and

other education, and at the other output-based approaches for all subcategories of the

education function of government. The sample of possible quality-adjusted volume

indicator alternatives to BEA’s current methodology is presented within the context of

the literature and empirical estimates are developed.




Prior Research on Output-Based Measures of Education Services of Government

As part of a general movement in the international statistical community toward using an

output-based approach to measuring government output in their national accounts, a

number of countries have implemented or experimented with the output-based measures

of real government educational services.9, 10 The Office of National Statistics (ONS) in

the United Kingdom has gone the furthest with this approach, with nearly 70 percent of

government expenditure being measured using direct volume measures.11 New Zealand

measures over 60 percent of government expenditure in a similar fashion. Australia, the

Netherlands, and Italy have also followed suit, each measuring up to 50 percent of

government expenditures using direct volume measures. Other countries, such as Canada,

Italy, Germany, Norway, Finland, Sweden, and Israel, have also developed real output

measures, either having recently implemented them for a small portion of government

expenditures or currently considering doing so in the near future. Education and health

9
  See Jenkinson (2003) p. 2 and Pritchard (2002b) p. 3.
10
   See Jenkinson (2003) and other documents such as those authored by the Australian Bureau of Statistics,
the Office for National Statistics (ONS), including Alwyn Pritchard of ONS and Caplan formerly of ONS,
and Algera and Kleima of Statistics Netherlands (CBS) and Konijn formerly of CBS.
11
   In an 11/25/03 email from Alwyn Pritchard of the ONS, unconfirmed by the Australian Bureau of
Statistics (ABS), implementing current ABS research on direct volume measures for justice (police, courts,
and prisons), taxation, and social security would bring coverage of government output (using real
measures) to 90 percent, making the ABS the world leader.


                                                                                                         5
are the two functions of government most commonly measured with an output-based

approach.



A topic of debate in these efforts is the extent to which output measures can be based on

outcomes. Sherwood (1994) gives the example of a teacher who faces a class of poor

students. If the students learn nothing, is the output of the school for that class zero?

Sherwood suggests that whether a service output, such as education, should be quality-

adjusted with an outcome measure, as opposed to being derived from a pure transactions

count approach, depends upon the particular service. He points out that the price that

individuals pay for a market service, such as a football game, in part depends upon the

expected outcome of the game.



A few countries have experimented with output measures that use data on outcomes to

quality adjust a quantity index of student-years of education. Test scores are one such

measure, and the Atkinson Report suggests using real earnings growth in an experimental

measure of education output.12 Refinements to the use outcomes data that remove the

influence of non-school factors from the output measure have not yet been developed.

Below we review prior results on measuring the output for the education function of

government employed by statistical agencies of the United Kingdom, Australia, the

Netherlands, and other countries.




12
  See Atkinson Commission (2005) and ONS (2005). Note that this approach requires either a strong
assumption that the change in the price of education in the labor market can be measured by a general
deflator, or the availability of a customized deflator to separate out the price and volume components of the
increment to earnings resulting from additional education.


                                                                                                           6
        United Kingdom

In the United Kingdom (UK), the Office of National Statistics (ONS) produces both an

official and an experimental quality-adjusted volume measure of the education function

of government. Both use full-time equivalent number of pupils as the volume indicator

under the assumption that hours in school per pupil are constant across time although it is

recognized that pupil hours would be preferred. Both exclude higher education.



The official volume indicator is quality adjusted for all education categories. A 0.25

percent quality-adjustment factor per year is utilized since “there is significant evidence

that educational standards have been rising over a number of years” and “there is

evidence that the quality of teaching is rising.”13 This is justified by General Certificate

of Secondary Education (GCSE) examination results, which show a pattern of increases

in the average point scores of pupils over a period of 11 years. An index of the number of

pupils enrolled in nursery schools, primary schools, secondary schools, further education,

and special education is constructed with weights proportional to the expenditure on

education in the base period to form the official volume indicator.



Pritchard (2002a) introduced the idea of using a lesson quality adjustment. In the UK,

government inspectors make assessments regarding the quality of lessons. Powell and

Pritchard note that weights could be assigned to the three ratings categories for lessons:

Good/very good lessons, satisfactory lessons, and unsatisfactory/poor lessons. If these


13
  Caplan (1998) p. 48. Caplan also indicates that there may be a declining proportion of students
completing higher education courses, therefore an upward quality-adjustment for these students may not be
justified. Ronald Ehrenberg of Cornell University in a recent discussion indicated that, in his opinion, an
upward quality-adjustment may not be justified for US higher education.


                                                                                                          7
assessments were used to form a lesson-quality adjustment, the rate of growth of the

volume indicator would be raised over the period 1995-2000. However, Powell and

Pritchard say that they would prefer a “more coherent basis for estimates,”14 so this

adjustment is not used as part of the official measure, although it is part of the unofficial

measure.15



The official and experimental ONS estimates show how sensitive results can be to

methodology. From 1995-2000 the annual rate of growth of the volume indicator for the

experimental estimates with the “quality of lessons received” adjustment is 1.74

percent.16 The comparable figure for the official index with the 0.25 percent quality

adjustment is 0.88 percent.17



From 1995-2000 the annual rate of growth of the implicit price deflator for the official

estimate is 3.76 percent. This reflects a typical pattern seen for the few countries,

including the U.S. (see Table 3 and Chart 2), for which a price can be calculated based

upon published information. In all these countries, the rates of growth in the prices

account for at least two-thirds of the rates of growth of nominal expenditures, with the

UK being on the high side for that time period at approximately 80 percent.18 In contrast,

the rates of growth of the GDP price may account for less than half of the rates of growth


14
   See Powell and Pritchard (2002) p. 8.
15
   See Powell and Pritchard (2002) for a description of the official measure and Pritchard (2002a) p. 30 for
the unoffical measure estimates.
16
   See Pritchard (2002a), p. 30, table 10.
17
   See Pritchard (2002b), Annex B, p. 11 .
18
   Rough estimates of a volume indicator for the U.S. covering higher education as well as primary and
secondary education were calculated using quality-unadjusted enrollment for higher education to compare
with the Australian, Netherlands, and UK estimates. From 1990-2001, the rate of growth of prices is about
three-quarters of the rate of growth of nominal expenditures.


                                                                                                           8
of nominal GDP for the countries for which education prices can be calculated:

Australia, the UK, the Netherlands, and the United States.19 However, to the extent that

quality improvements occurring over time are not captured in the quality adjustments

made to the education volume indicators, the price growth rates are overestimated and the

volume indicator growth rates are underestimated. Measuring the output of services is

difficult, and measuring quality changes in the output of services is even more difficult.

Accordingly, it is reasonable to assume that quality is imperfectly estimated in the

education volume indicators of all countries.



         Australia

The Australian Bureau of Statistics (ABS) examined a variety of approaches when

researching possible volume indicators for education.20 These approaches included a

volume indicator with and without quality adjustment and a modified incremental

earnings approach. The quality adjustments considered include quality indicators such as

class size, examination results, the quantity and quality of research publications, grants

received, and the number of student research completions. In the official index, class size

was not adopted as a quality adjuster because of uncertainty about the relationship

between class size and the quality of education received. Examination results are not

adopted as a quality adjuster because of concern about the comparability of scores over

time, particularly because of external factors, such as social capital, which can affect



19
   The comparison is made here to GDP prices rather than gross domestic purchases prices as the former are
available for all of the countries. Frequently the term gross domestic final expenditures prices is the term
used by other countries for the term gross domestic purchases prices used by BEA.
20
   The Australian Bureau of Statistics (ABS) does not distinguish between public and private education in
its estimates, therefore it is not possible to separate the government education function from the private
education function.


                                                                                                          9
these scores.21 The modified incremental earnings approach, if ever adopted in the

national accounts, would indirectly infer the “direction and size of long-term quality

change” from a human capital model similar to that developed by Jorgenson and

Fraumeni (1992), a model which has been implemented in a satellite account for

Australia by ABS.22



The official volume indicator now used by ABS does not quality-adjust the output

indicator. For primary and secondary education, the output index is an index of

enrollment numbers converted to an equivalent full-time student unit (EFTSU) basis. For

vocational education, module hours are used. For higher education, enrollments are

converted to an EFTSU basis and weighted by the Higher Education Contribution

Scheme (HECS) charges levied on students to form the final higher education index. A

university research index depends upon the number of publications and student research

completions. Other education services, such as pre-school education, are still measured

using input price indexes. All of the individual indexes are weighted together using cost

shares.23



The new education output method results in growth rates that are higher and more stable

than the previous input method. For years ending June 30th, the average annual rate of




21
   Examination results were considered according to ABS (1998) p. 4, but were not used according to ABS
(2002b) p. 3.
22
   ABS (1998) p. 4 and ABS (2002b) p. 3.
23
   ABS (2001a) pp. 4-5 and ABS (1999) p. 13, the latter for the definition of a equivalent full-time student
unit.


                                                                                                          10
growth of gross value-added from 1993-1994 to 1999-2000 under the new method is 1.9

percent compared to 1.5 percent under the previous method.24 25



For years ending June 30th, the 1994-2003 annual rate of growth of the implicit price

deflator is 3.3 percent. This accounts for two-thirds of the rate of growth of nominal

expenditures.



        The Netherlands

Statistics Netherlands (CBS)26 experimented with five possible volume indicators to

replace their current input-based output index (Konijn and Kleima, 2000a). Education is

divided into ten levels, from primary through university, and each level’s output is

measured using an appropriate index. Two indexes depend only on number of pupils.

One is an unweighted index; the other weights number of pupils by expenditures per

type. Three combination indexes use number of pupils for primary education along with

number of pupils, number of pupils moving up, and/or number of graduates for other

levels or types of education. In some cases a two-year moving average of number of

graduates is used to smooth the series. For several categories of secondary education and

above, part-time students are counted as 0.5 in the pupil count.




24
   ABS (2002b) pp. 3-4.
25
   David Bain of ABS provided the authors on 4/30/04 with a worksheet containing education nominal and
chain volume measures through the year ending June 30th of 2003. (All ABS annual economic statistics are
calculated from July 1st through June 30th.) Growth rates for the nominal and implicit price deflator were
calculated by the authors.
26
   Both Statistics Netherlands and the Central Bureau of Statistics of Israel are referred to as “CBS.”


                                                                                                       11
Other quality adjustments are considered. For primary education, these include: The

composition of the pupil stock, the percentage of pupils that move up each year, and the

scores of the level test, which is the National Institute for Educational Measurement

(CITO) test. A quality adjustment for the composition of the pupil stock incorporates

information on the share of students who have a lower level of education for two

subcategories: Those whose parents are Dutch and those whose parents are foreigners.

This quality adjustment was not included in an estimate because of the uncertainty in the

resource cost factors to serve these different students. A quality adjustment for pupils

moving up was not incorporated into the estimate because these adjustments would be

almost constant over the years. The CITO test results changed little during 1995-2000 so

they are not employed as a quality adjustment, but new tests may be a fruitful source of

quality adjustment in later versions of a volume indicator. Pupils moving up was not

used as an indicator for any of the university education volume indicators because the

financial grant period was shortened during this time period, accordingly the study

duration decreased. Pupils moving up was not used as an indictor for vocational colleges

because data were not available on this for the whole period 1990-1998.



The conclusion of Konijn and Kleima is that the best volume indicators are those which

go beyond just tracking the number of pupils. Of the three combination indexes, the

index that uses the number of pupils for primary, secondary and vocational education, a

two-period moving average of the number of graduates of vocational colleges and

universities, and pupils moving up as the quality adjuster for all other categories of




                                                                                           12
education is their first choice.27 Cost shares are used as weights. A volume indicator very

similar to this index is now used in their national accounts.28



Konijn and Kleima estimate volume indicators with the current input method and five

alternative indexes; however, they indicate that current input method estimates for 1996-

1998 may not be reliable. In 1995 the volume of labor input to education was adjusted

upward by 15 percent. Unrevised estimates for 1990-1995 show labor input to education

almost constant.29 The 1991-1995 annual growth rates of the volume indicators vary

from 0.34 percent for the two pupil numbers indexes to 1.42 percent for the preferred

combination index. The 1991-1995 annual growth rate of the current input method is

0.86 percent. The 1991-1998 annual growth rates of the volume indicators vary from 0.23

percent for the weighted pupil numbers index to 1.25 percent for the preferred

combination index.30 From 1991-1997 the annual rate of growth of the implicit price

deflator is 2.41 percent, which is two-thirds of the rate of growth of nominal

expenditures.



Chart 1 shows the implicit price deflator for all levels of the government function of

education for Australia, the Netherlands, and the UK. As the implicit price deflators for

these countries are not available separately for the category primary and secondary

education, the U.S implicit price deflator is not shown. As there are only at most five



27
   Konijn and Kleima (2000a), pp. 19, 25. The two-period moving average is used to mitigate the effect of a
small absolute change looking large in relative terms compared to a small population of graduates.
28
   Email from Kleima 11/14/03.
29
   Konijn and Kleima (2000a),p. 23.
30
   Calculations based on estimates in Ibid., p. 22.


                                                                                                        13
overlap years, always including the base year 1996, it is difficult to make comparisons.31
32




                                                    Chart 1
                           UK, the Netherlands, and Australia All Levels of Education
                  1.30
                                                 Price Deflator
                  1.25

                  1.20

                  1.15
     (1996=1.0)




                  1.10
        Price




                  1.05

                  1.00

                  0.95

                  0.90

                  0.85

                  0.80
                         1991   1992    1993   1994   1995   1996   1997   1998   1999   2000   2001   2002   2003
                                                                    Year


                                       UK Price Deflator 1995-2000
                                       Netherlands Price Deflator 1991-1997
                                       Australian Price Deflator 1994-2003 (Years ending June 30th)


                  Other Countries

While a half dozen or so other countries have developed real output measures for a

portion of government expenditures, in many cases it is difficult to get a clear indication

of the approaches used, much less a full description of the methodologies. Much of the

work is still in developmental stages and published explanations are hard to come by.

Nevertheless, other approaches to measuring the output of the education function of

government by other nations contribute valuable insight.




31
   The base year is set to 1996 in this paper because the U.S. data used in this paper is the pre-NIPA
comprehensive revision data that has 1996 as its base year.
32
   David Bain of ABS on 4/30/04 provided the authors with a worksheet containing education nominal and
chain volume measures through the year ending June 30th of 2003. Implicit price deflators were calculated
by the authors.


                                                                                                                     14
A number of countries use the number of pupils as a volume indicator, with quality-

adjustment factors under consideration or actually adopted. Among the countries that

have adopted this approach are: Statistics Canada (Stat Can), the Federal Statistical

Office of Germany (DESTATIS), the National Institute of Statistics of Italy (ISTAT), and

the Central Bureau of Statistics of Israel (CBS). Canada and Germany use or plan to use

the number of pupils without a quality adjustment. 33 Italy uses the number of pupils for

education-based functions with some qualitative adjustments related to class size as

captured by a congestion measure. Weights used in output aggregation also in some

cases adjust for the use of equipment and teaching aids. For service-based functions in

education, Italy uses the number of employees, the number of users of the services, or the

number of services provided.34 Israel may be quality adjusting the number of pupils with

a variety of indicators of quality change for higher education: The percentage of students

succeeding in their studies each year, the number of students receiving diplomas or

academic degrees, the percentage of students studying towards first, second, and third

degrees, and the number of students in various study disciplines.35



Statistics Finland uses a variety of volume indicators. Teaching hours are the volume

indicator for 99 percent of educational services produced by municipalities, which

include services provided by vocational institutes and community colleges as well as

primary and secondary education institutions. The number of degrees completed,

generally separated into graduate and postgraduate, measures university education output.

Either the number of days of study or courses completed measures adult and continuing

33
   Jenkinson (2003) p. 4.
34
   Malizia (1998) pp. 18-24.
35
   Hadar, Madler, and Barzel (1998) pp. 9-13.


                                                                                          15
education, depending upon the university. The number of publications is used for the

output of research, the number of visitors for libraries.36



Experimental Estimates for the U.S.



           Introduction

In this section quality-adjusted volume indicators are presented that might serve as a

basis for measurement of output of government educational services. Each begins with

the number of pupils enrolled as the base index, then considers possible quality

adjustments to this base index. The list of possible quality adjusters is not exhaustive and

improvements to these experimental estimates are still underway. Accordingly, these

estimates should not be taken as an indication of what measure (if any) may be adopted in

the future by BEA.



Estimates are presented for 1980-2001 for primary and secondary education.37 Quality

adjustments presented in this paper include adjustments by: Teaching staff composition

indexes, pupil-teacher ratios, and high school drop-out rates.



           Defining a Production Function for Education

           A difficult question in the development of an output volume measure for the

education function of government is whether outcome ought to be distinguished from

output. “Outcome” generally refers to the level of knowledge or skills possessed by


36
     Niemi (1998).
37
     The estimates do not incorporate revised data from the December 2003 NIPA comprehensive revision.


                                                                                                         16
those who have received education. Outcome can be affected by a host of factors other

than schools themselves, e.g., ability, parental support, the quality of home life, and

social capital in general. “Output” generally refers only to the impact of schools on the

level of knowledge and skills of students. For example, test scores or graduation rates are

frequently used to quality adjust volume indicators for education, yet these are often

affected by factors other than schools. Cipollone and Rosolia (2007), for example, find

that conscription prospects and peer group outcomes affect graduation rates. Students’

ability and prior preparation also affect current educational outcomes. Finally, families

provide inputs into the learning process, so, for example, students from families that do

not speak English will generally require more educational services to achieve the same

outcome as measured by test scores than native speakers of English.

       Some of the services that schools provide are in areas besides education itself,

such as athletics and socialization, but for the sake of convenience, we will refer to our

quality adjustment factor for outcomes as “learning.” Let θit denote the average learning

outcome by a student at education level i (primary, secondary, or higher education) and

let qit denote the number of students completing a year of education at level i in year t.

Then volume of learning in year t is:


                                   Qt = ∑ i θitqit                                           (1)


       Changes in θit result both from changes in the educational services produced by

schools using inputs of teachers, other staff, supplies, and capital stock and from changes

in non-school factors. Let teacher inputs be represented by the vector of Ti, where the

elements of Ti are the numbers of teachers of each experience and education level

teaching at school type i. Also, let Ai represent administrative and support staff, let Ki


                                                                                              17
represent the capital stock, and let Mi represent intermediate inputs such as supplies.

Finally, let the factors that influence outcomes but not output be ei, an index of factors

other than teachers or schools that influence student effort levels and bi, an index of

student background and ability levels. Then the outcome function at educational level i

for learning per pupil is:


                               θi = fi(Ti, Ai, Ki, Mi, ei ,bi,qi)                                 (2)


If θi is measured by average test scores, fi(⋅) equals the maximum score achievable by the

school with inputs Ti, Ai, Ki, Mi, given the external factors ei and bi and student body size

of qi.

         Over the relevant range for the arguments of equation (2), the average amount of

learning by a student at level i is increasing in Ti, ei and bi. Also, fi(⋅) is decreasing in qi

in the region where we expect schools to operate, meaning that the marginal effect of a

rise in the student-teacher ratio is to reduce learning per student. It is also increasing in

Ai, Ki and Mi in some local region (though in the case of Ai, it is not always clear that

schools are operating in that region.) Finally, we assume that fi(0, 0, 0, 0, ei ,bi,qi) = 0.

         Because fi(Ti, Ai, Ki, Mi, ei ,bi,qi) > fi(Ti, Ai, Ki, Mi, 0 ,0,qi) for ei > 0 and bi > 0,

the average product of the inputs Ti, Ai, Ki, Mi depends on the levels of ei and bi. To

measure the per-student educational output produced by the inputs into production, we

must therefore condition on some set of reference values of ei and bi. If ei and bi are

constant over time, we can use their actual values as the reference values, and treat the

observed θi as a measure of the educational services produced by Ti, Ai, and Ki.

Otherwise, we must choose some level of reference values of ei and bi, such as their initial



                                                                                                      18
                                                                                               ^
level, their final level, or some average in between these. Letting ^ei and bi denote these

reference values, the conditional education production function is defined as:

                                               ^                                    ^
               φi(Tit, Ait, Kit, Mit, qit; ^ei,bi) = qit fi(Tit, Ait, Kit, Mit, ^ei,bi,qit).          (3)


Equation (3) can be used to measure the change in output of educational services from
                                                       ^                                       ^
time t to time s as φi(Tis,Ais,Kis, Mis, qis; ^ei, bi) – φi(Tit, Ait, Kit, Mit, qit; ^ei,bi). To estimate

this change, the observed θit and θis must be adjusted for the effect of substituting ^ei and
^
bi for the actual values eit and bit and of eis and bis.

        We make no such an adjustment in this paper, however. As a result, the change in

outcome probably understates of the growth in output of educational services in the

recent past. In particular, increasing numbers of special education students and students

whose parents do not speak English have probably had adverse effects on student

outcomes.

        The basic measures developed here lay the foundation for future research on

adjustment of outcomes to reflect changes in non-school factors. Furthermore, outcome

is the appropriate variable for some important questions. Perhaps for this reason, in other

industries where external factors heavily influence outcomes, the convention is to ignore

the external factors and accept outcomes as measures of output. In agriculture, for

example, weather is a crucial determinant of the size of the harvest, and the spread across

borders of disease-causing organisms can affect deliveries of animal products to industry

customers. No provision is made for these effects in the calculation of real agricultural

output for national accounts purposes.




                                                                                                       19
           Use of Input Quantity and Quality to infer Changes in Output

Empirical research has shown that some input quantity and quality measures are linked to

improved educational outcomes as measured by test scores. These include pupil-teacher

ratios and teaching staff composition measures such as years of education and

experience. When direct measures of educational outcomes are unavailable, counting the

expected change in educational outcomes that would arise from changes in input

quantities or quality in the output measure is better than assuming that output per student

educated is constant. A common practice is, therefore, to quality-adjust volume

indicators by factors that measure the amount or quality of inputs that have been shown

to have an important effect on output. The difficult part is to estimate the precise value of

the change in output resulting from a given change in inputs. For example; if class sizes

drop by 10 percent, does θit increase by 10 percent? Furthermore, if the quantitative

impact of inputs on output is estimated at some point in time, changes in other factors,

such as the composition of the student body, might alter the relationship.



           Enrollment Data

U.S. Census Bureau (Census) Current Population Survey (CPS) student enrollment

statistics are used in preference to other sources such as the U.S. National Center for

Education Statistics (NCES). NCES enrollment data were incomplete in some years.

While considered superior, the Census enrollment figures used are also imperfect. Over

the time period we consider (1980-2001),38 three adjustments to the data had to be made:

The data for 1981-1992 are revised to be consistent with the 1990 Census estimates,


38
     See Williams (2003) “Appendix B-1” for a note on the time series.


                                                                                           20
interpolation is used for 1980 to deal with the lack of a public/private breakdown of

students, and estimates of students age 35 and over are added in for years before 1994,

because these students are not included in the Census enrollment figures.39




39
     See Williams (2003) “Appendix B-2” for full explanation of adjustments.


                                                                                          21
                             Table 1: Adjusted Census Enrollment Figures

                                               (in thousands)


                                   Primary   Primary   Secondary             College
                                     and
                         Year     Secondary Grades K-8 Grades 9-12

                          2001      47,775        32,945            14,830   12,421
                         .2000      46,982        32,551            14,431   12,008
                         .1999      47,069        32,431            14,638   11,659
                         .1998      46,551        32,252            14,299   11,984
                         .1997      47,213        32,579            14,634   12,091
                         .1996      45,618        31,506            14,113   12,014
                         .1995      45,308        31,558            13,750   11,372
                          1994      44,948        31,409            13,539   11,694
                         .1993      44,852        31,867            12,985   11,594
                         .1992      43,878        31,201            12,677   11,765
                         .1991      43,182        30,738            12,444   11,436
                         .1990      42,605        30,446            12,159   11,166
                         .1989      41,947        29,661            12,287   10,644
                         .1988      41,649        29,281            12,368   10,624
                         .1987      41,365        28,549            12,816   10,368
                         .1986      40,755        27,805            12,950    9,803
                         .1985      40,220        27,286            12,934    9,916
                         .1984      40,140        27,282            12,857    9,886
                         .1983      39,960        27,066            12,894    9,466
                         .1982      40,304        27,232            13,072    9,547
                         .1981      40,983        27,426            13,557    9,254
                         .1980      40,548        27,088            13,460    8,785




           Teaching Staff Composition

The U.S. Department of Education’s National Center for Education Statistics’

“Monitoring School Quality: An Indicators Report” (2000) found that “students learn

more from teachers with strong academic skills and classroom experience than they do

from teachers with weak academic skills and less experience.”40 Rivkin, Hanushek, and

Kain’s (2001) analysis “identifies large differences in the quality of schools in a way that
40
     National Center for Education Statistics (NCES) (2000) p. i.


                                                                                          22
rules out the possibility that they are driven by non-school factors… we conclude that the

most significant [source of achievement variation] is… teacher quality.”41 Hanushek

(1998) states that the “differences in student achievement with a good versus a bad

teacher can be more than 11/2 grade levels of achievement within a single school year.”42

The NCES report identified 13 indicators of school quality that recent research suggests

are related to school learning; of these, four relate to the quality of teachers: Teacher

academic skills, teacher assignment, teacher experience, and professional development.43



Data produced by the National Education Association “Status of the American Public

School Teacher” provide information on teacher educational attainment. Although

educational attainment does not perfectly predict how well a person will teach, there is

“broad agreement that teachers’ academic skills are linked to student learning.”44

Students appear to learn more from teachers with strong academic training, for example,

Darling-Hammond (2000) concludes that “The most consistent highly significant

predictor of student achievement in reading and mathematics in each year tested is the

proportion of well-qualified teachers in a state”.45 Surveys by NEA and NCES separate

teachers with no degree, a bachelor’s degree, a master’s degree, a professional diploma,

and a doctor’s (Ph.D.) degree. Indicating quality change, results show that from 1961 to

1996 the percentage of public elementary and secondary school teachers with a master’s

degree, specialist’s degree, or a doctor’s degree almost doubled.46



41
   Rivkin, Hanushek, and Kain (2001), p. 32.
42
   Hanushek (1998) p. 35.
43
   NCES (2000), p. 4.
44
   Ibid., p. 5.
45
   See Darling-Hammond (2000), p. 27.
46
   NCES (2003), p. 82.


                                                                                            23
Independent of educational attainment, teacher assignment can directly affect student

learning and the quality of education. Many teachers are currently teaching courses in

disciplines other than those in which they have been formally trained and the student

achievement has suffered.47 The NCES Report states, “Given the apparent benefits

students receive from being taught by well-qualified teachers, it is worth assessing the

extent to which students are taught by teachers who are teaching without proper

qualifications.”48 While teacher assignment is an important indicator of school quality,

defining a teacher “qualified” versus “unqualified” is difficult and meaningful data are

not available.



Studies show that students also learn more when taught by more experienced teachers.

Rivkin, Hanushek, and Kain (2002) show that 4th and 5th grade students in Texas whose

teachers had more than two years of experience increased their math and reading test

scores by between 0.12 and 0.19 standard deviations more over the course of a year than

those whose teachers had fewer than 2 years of experience. The NEA and NCES surveys

report detailed information regarding teacher experience.



Even though experts would likely agree that professional development should enhance

student learning, there is no concrete statistical evidence of such an association.49

Conceptually, professional development opportunities seem important to help retain

quality teachers but research is needed to document such a relationship.

47
   NCES (2000), p. 12.
48
   Ibid, p. 11.
49
   Ibid, p. 14.


                                                                                           24
Of the four indicators of school quality associated with teachers, teacher academic skills

(educational attainment) and teacher experience offer the best hope of empirically

capturing quality change. Using NEA and NCES survey data that are available for

selected school years, the Government Division of BEA computes a quality-adjusted

constant-dollar estimate of labor compensation for education. Educational attainment and

experience are taken into account to adjust average real compensation estimates to

represent changes in the teaching staff composition. Specifically, annual estimates of the

number of teachers cross-classified by experience categories and highest degree obtained

are multiplied by 1996 average wages for these same groups, and then divided by the

total number of teachers in each year to derive an estimate of an annual real average

wage.50 This series, normalized to 1.0 in 1996, is an index of teaching staff composition.

It is used in this paper as a quality-adjuster, under the assumption that differences in

average wages paid reflect teacher quality differences.51 Table 2 shows that, although this

index of teaching staff composition increased for the period as a whole and for the first

sub period, 1980-1990, it decreased during the 1990-2001 sub-period. This is probably a

reflection of the significant changes in teacher experience shown between the 1990/1991

and 1999/2000 NCES surveys of teachers. This indicator of teaching staff composition

change is applied to both primary and secondary education, as there is no evidence of a

differing impact upon different grades.

50
   NEA and NCES provided BEA with their survey data cross-classified by experience and highest degree
obtained categories. Experience categories include less than 5 years of experience, 5-10 years of
experience, 11-15 years of experience, 16-20 years of experience, 21-25 years of experience, and over 25
years of experience. Highest degree obtained categories include no degree, two-year degree, bachelor’s
degree, master’s degree, and doctor’s degree.
51
   Experience-based adjustments to labor input indexes implicitly assume that wage differentials reflect
actual relative marginal productivity differences (perhaps as determined by a merit pay system) as opposed
to wage differentials primarily arising from seniority-based wage systems.


                                                                                                       25
       Table 2: Annual Rates of Growth in Prospective Quality-Adjustment Factors

                                       (Percentages)

                                         1980-2001 1980-1990 1990-2001
          Teaching Staff Composition        0.13      0.49     -0.20
          Pupil-teacher Ratio               -.77      -.83     -0.71
          High School Drop-out Rate        -1.31     -1.52     -1.11
          College Enrollment Rate           1.07      2.00      0.24


         Class size

Does size matter? Intuition says it must. If class size did not matter it would be perfectly

logical to increase a second grade class from 30 to 60 students, or to 120 for that matter.

Supplemental, out of class tutoring would be just as effective when done in groups of ten

students as with one-on-one instruction. Although intuition necessitates this conclusion,

the measurable impact of class size variation is tough to measure and debatable.



Finn (1998a) summarizes the findings of some pivotal studies on class size.52 Glass and

Smith’s (1978) statistical meta-analysis of the findings of over 80 empirical studies show

that “reduced class size can be expected to produce increased academic achievement.”53

The Educational Research Service analyzed a much larger set of studies, finding mixed

results.54 One of Robinson’s conclusion’s is that the class size effects are more apparent

with early primary education. Tennessee’s Project STAR (Student-Teacher Achievement

Ratio), a controlled scientific experiment that assigned over 10,000 students to small and

large classes at random then tracked their progress over four years, “provided educators


52
   Finn (1998a).
53
   Glass and Smith (1978) p. iv.
54
   Robinson (1990).


                                                                                           26
with definitive answers about the impact of small classes in the primary grades.” Project

Star found that statistically significant differences existed among the students in the

different size classes on every achievement measure for every year of the study.55 After

being returned to regular size classes, the students of Project STAR were subsequently

tracked by the Lasting Benefits Study (LBS). It found small, but positive, carryover

effects through at least 8th grade.56 Finn’s study (1998a, p. 4) concludes that “small

classes (17 pupils or below) are more effective academically than larger classes (22 and

above) in the primary grades in all subject areas.” Class sizes seem especially important,

as “teachers spend more time in direct instruction and less time in classroom management

when the number of students is small (Finn, 1998a, p. 4).”



Ivor Pritchard (1999) also synthesized previous studies, concluding “the pattern of

research findings points more and more clearly toward the beneficial effects of reducing

class size.”57 He noted Slavin’s (1989) findings that “reduced class size had a small

positive effect on students that did not persist after their reduced class experience.”58

Robinson and Wittebols (1986) found that the clearest evidence of the positive effects of

smaller classes is in the primary grades. Ferguson (1991), using data on more than 800

districts and 2.4 million students in Texas, found that in grades one through seven

“district student achievement fell as the student/teacher ratio increased for every student


55
   Finn reaches this conclusion (1998a, p. 4). Mosteller (1995) and Krueger (1999) both support the
conclusion that Project STARs results show that class size does matter, especially with younger and more
economically disadvantaged children.
56
   This is the conclusion of Ivor Pritchard (1999, p. 4) who cites Finn’s (1998b) citation of Nye, et. al.
(1995).
57
   Ivor Pritchard (1999) p. 1.
58
   Ivor Pritchard ((1999), p. 2) gives Slavin’s conclusion, citing Finn (1998) as the source. Finn’s
bibliography does not give a cite for Slavin (1989) as a sole author source. Finn’s bibliography includes a
1989 article by Slavin and Madden and a 1989 book edited by Slavin.


                                                                                                         27
above an 18 to 1 ratio.”59 Krueger (1998), “in an external re-analysis of the Project STAR

data, reconfirmed the original finding that ‘students in small classes scored higher on

standardized tests than students in regular classes’ even when the data analysis took into

account adjustments for school effects, attrition, re-randomization after kindergarten,

nonrandom transitions, and variability in actual class size.”60 Ivor Pritchard makes the

following conclusions to his synthesis:

            •    Existing research shows that smaller classes in the early grades leads to
                 higher achievement
            •    Reducing class size from over 20 students to under 20 students moves the
                 average student from the 50th percentile to the 60th percentile in
                 achievement measures
            •    Students, teachers, and parents all agreed that smaller classes increase the
                 quality of classroom activity


On the other side of the debate, Hanushek (1998) claims that in 277 independent studies,

only 15 percent found a statistically significant correlation.61 “The evidence about

improvements in student achievement that can be attributed to smaller classes turns out to

be meager and unconvincing.”62 The results suggest that while some factors, such as

teacher, quality do affect the output of education, class size does not. Using National

Assessment of Educational Progress (NAEP) standardized tests data in conjunction with

aggregate data on national pupil-teacher ratios over time, Hanushek concluded that

smaller classes simply do not outperform larger classes on a consistent basis, and that the

data do not support the assertion that smaller classes ensure a higher level of output.




59
   As cited and quoted in Ivor Pritchard (1999), p. 2.
60
   Op. cit., p. 5.
61
   Krueger (2002) disputes Hanushek’s conclusions after reviewing the same studies covered in Hanushek
(1998).
62
   Hanushek (1998), abstract.


                                                                                                     28
Hanushek (2002) suggests possible explanations for the lack of correlation between small

classes and improved performance. One is that intra-school class sizes are not decided at

random: Schools put their lower achieving students that need extra resources in smaller

classes. Also, identification of exogenous determinants of class size is extremely

difficult; accordingly the generalizability of any findings may be jeopardized. As an

example he cites a study by Lazear (2001). Lazear looks at the probability that a student

may impede his own learning or other’s learning and suggests that higher quality teachers

may be more capable of keeping students on track. This study raises the question in

Hanushek’s mind of whether the probability of disruption should be considered an

exogenous factor or dependent upon the teacher’s classroom management ability.63

Except for a few scientifically controlled studies such as Project STAR, the bulk of the

studies have no way to control for exogenous factors and simply compare achievement

by class size. Other experiments (California, 1996; Indiana’s Prime Time Project, 1994;

Burke County, North Carolina, 1990; Wisconsin’s Student Achievement Guarantee in

Education (SAGE) Program, 1996) that systematically reduce class size across a school,

district, or state may miss some of the benefits of having smaller classes because they

required hiring new, inexperienced teachers to accomplish the class size reductions.64



Actual class sizes are unavailable, but pupil-teacher ratios, which are available, are a

proxy for class size.65 We, therefore, use pupil-teacher ratios for quality adjustment.

Primary and secondary education pupil-teacher ratios have declined from 18.7 in 1980 to

63
   Hanushek (2002) pp. 48-51.
64
   Ivor Pritchard (1999) p. 9.
65
   Pupil teacher ratios are not the best measure of class size, but are the best data available. See Hanushek
(1998, p. 16) for reasons that the two measures differ, such as effect of special education teachers and aids
on pupil-teacher ratios.


                                                                                                           29
15.9 in 2001.66, 67 Table 2 shows the rate of decline in this ratio for the whole period and

two sub periods. Ceteris paribus, this trend improves the quality of education, resulting in

an increase in the output. Because of the controversy regarding the link between pupil-

teacher ratios and the quality of education, we damp the effect of pupil-teacher ratios by

raising them to the 0.1 power, a conservative assumption. 68 Letting ρit denote the

student-teacher ratio in year t and wi0 denote a weight proportional to expenditures on

educational level i (where the levels are primary and secondary), we can define a

Laspeyres index of the educational services volume measure in equation (1):

                                  QLaspeyres = ∑ i wi0 (qit/qi0)(ρit / ρi0)−0.1                         (4)

With this quality adjustment, a 10 percent decrease in class size results in a 1 percent

increase in the output measure. Pupil-teacher ratios are applied as a quality adjustment

just for primary education (grades K-8), because an effect on primary education output

has greater support in the literature than an effect on both primary and secondary

education output.




         High School Completion Factor


Two additional quality-adjustment factors that are worth considering are the percentage

of the relevant population who complete high school, and the percentage who go on to

higher education. Two possible proxies for these factors were considered briefly: The



66
   Eventually it would be preferred to substitute pupil-teacher ratios for K-8, but these are not readily
available even through NCES or other sources.
67
   NCES, Digest of Educational Statistics, 2002 (2003), table 65.
68
   Krueger (1999) shows that a 1/3 reduction in class size over four years produced an average gain of 0.2
standard deviations in student achievement. See Hanushek (2002, p. 65).


                                                                                                         30
high school drop-out rate and college enrollment rates. Additional research is needed to

identify and quantify these and other possible quality-adjusters.


Research literature needs to be examined to answer two basic questions. To what extent

are drop-out rates determined by what schools do as opposed to other factors such as

social (including cultural) capital? Are rising college enrollment rates primarily a sign of

schools preparing students better for higher education, e.g., producing higher quality

students, or is this phenomenon mainly a function of changing labor market conditions?

To give a sense of how important these potential quality adjustments might be, volume

indicators are calculated with and without a drop-out rate quality adjustment. The rates

of growth of drop-outs and college enrollments for recent high school graduates are

shown in Table 2.69 The drop-out rate quality adjustment is implemented at a 0.1 power

as drop-out rates are taken to be an indicator of success for a portion of the high school

population.70 If the college enrollment quality-adjustment is incorporated at a later date

after further research, it also might be incorporated at a rate less than 1:1. Table 2 shows

that the high school drop-out rate reduction is larger in absolute value terms (if employed

at a 1:1 rate instead of a 10:1 rate) than any other possible quality-adjustment factor,

where a decrease in the drop-out rate would produce a higher adjustment than any other

shown, with the exception of college enrollment rates for 1980-1990.71 Over the 1980-

2001 period, the increase in the college enrollment rate (again if employed at a 1:1 rate)


69
   NCES, Digest of Educational Statistics, 2002 (2003) table 108 and table 183. A caveat to the dropout
rate table states: "Because of changes in data collection procedures, data may not be comparable with
figures for earlier years."
70
   The high school drop-out rate for persons 16 to 24 years of age varies from a high of 14.1 percent in
1980 to a low of 10.7 percent in 2001. This rate is the average rate across public and private school
students. See NCES (2002) table 108.
71
   As with the pupil-teacher ratio, the quality adjustment factor for the drop-out rate is the negative of the
growth rates shown in table 2.


                                                                                                             31
would have the next largest impact, however in 1990-2001 this possible quality-

adjustment factor would have a significantly smaller effect as college enrollment rates

peaked in 1997 at 67.0 percent before dropping to 61.7 percent in 2001.72




        Prices and Volume Indicators


Table 3 presents annual growth rates of a number of alternative prices and volume

indicators for selected periods. These fall into three categories: 1) Unweighted quality-

unadjusted total enrollment, 2) Quality-unadjusted enrollment where the volume

indicators are chain-type Fisher quantity indexes, and 3) Quality-adjusted enrollment

where the volume indicators are chain-type Fisher quantity indexes. In all cases the

prices are implicit price deflators. The third category of volume indicators is being used

or under consideration in the most countries. The first two categories of volume

indicators are presented in this paper mainly for purposes of comparison.




72
  The economy may explain the drop in 2001 or even 2000, but the drop in 1998 and 1999 cannot be
explained by an economic slowdown.


                                                                                                   32
                         Table 3: Annual Rates of Growth in Prices and Quantities (Volume Indicators)
                          for Primary and Secondary Public Education and Gross Domestic Purchases
                                                       (Percentages)

                                                                                         1980-2001        1980-1990        1990-2001
                                                                                      Price Quantity   Price Quantity   Price Quantity
1     Quality-unadjusted Enrollment, Unweighted – Total                               6.02      0.78   7.29      0.50   4.88      1.05
2      Primary Growth Rate                                                            6.17      0.94   7.33      1.18   5.12      0.72
3      Secondary Growth Rate                                                          5.83      0.46   7.55     -1.01   4.29      1.82

      Enrollment, chain-type Fisher quantity indexes and implicit price deflators
4       Quality-unadjusted Enrollment – Total*                                        6.05    0.76     7.41    0.38     4.83    1.10
5         Primary Contribution                                                        4.19    0.65     4.88    0.80     3.57    0.51
6         Secondary Contribution                                                      1.90    0.14     2.52    -0.33    1.34    0.56
        Quality-adjusted Enrollment – Totals
7         Adjusted by Teaching Staff Composition Index                                5.91    0.89     6.88    0.88     5.04    0.90
8         Adjusted by .1 × Pupil-Teacher Ratio                                        6.00    0.81     7.35    0.44     4.78    1.15
9         Adjusted by .1 × High School Drop-out Rate                                  5.99    0.81     7.34    0.45     4.79    1.14
10        Adjusted by Teaching Staff Composition Index & .1 × Pupil-teacher Ratio     5.86    0.94     6.82    0.93     4.99    0.95
11        Adjusted by Teaching Staff Composition Index, .1 × Pupil-teacher Ratio, &   5.80    0.99     6.75    1.00     4.95    0.99
             .1 × High School Drop-out Rate

12    Gross Domestic Purchases                                                        3.05    3.29     4.16    3.35     2.06    3.23


    * The sum of row 5 and 6 may not equal the total in row 4 because of rounding.




                                                                                                                                         33
Each one of the methods used in table 3 can be criticized. An unweighted quality-

unadjusted enrollment volume indicator assumes that all pupils in primary and secondary

education receive the same quantity of education, e.g., that the output of schools is the

same whether they are educating a kindergartner or a twelfth grader. Also, it assumes

that the quantity of education represented by a pupil-year does not change over time.

Clearly these are simplifying assumptions. The growth rates shown for primary

education versus secondary education are the unweighted growth rates for these

subcategories, accordingly they do not add up to the growth rate for the total. The

methodology underlying the second and third category, quality-unadjusted and quality-

adjusted enrollment where the volume indicators are chain-type Fisher quantity indexes,

is preferred to the methodology underlying the first category because, under certain

assumptions, including one that public schools allocate their budget between primary and

secondary education to maximize the output produced, cost shares used in Fisher indexes

reflect relative marginal products of resources devoted to primary versus secondary

education. The growth rates shown for primary education versus secondary education are

Fisher index decompositions for these subcategories, accordingly they do add up to the

growth rate for the total. As is true for the first category of indicators, using a quality-

unadjusted volume indicator assumes that the quantity of educational output per pupil-

year within either primary education or secondary education has not changed over time.

This seems unlikely even during the 21 year period examined.




                                                                                               34
The preferred approach uses a chain-type Fisher quantity index and includes adjustments

for quality changes. However, which quality indicators to include in the measure of

quality change and how to specify the equations for their effect are difficult questions.

Table 3 shows the prices and volume indicators implied by three possible indicators and

by two possible combinations of these indicators. At this time, because further research

needs to be performed on the use of high school completion as a quality indicator, the

enrollment volume indicator quality adjusted by an index of teaching staff composition

and pupil-teacher ratios and the implicit price derived from the volume indicators are

favored. However, all measures are exploratory.




The second and third category of alternative volume indicators can be written as follows:

Let zp,y represent enrollment in primary school in year y and zs,y represent enrollment in

secondary education in year y. The enrollment growth rates for primary and secondary

education are calculated as GR(zp,1980, 2001) = (zp,2001 / zp,1980)1/21 – 1 and GR(zs,1980, 2001) =

(zs,2001 / zs,1980)1/21 – 1 respectively.



Let GR(TSCI1980, 2001) denote the growth rate of the teacher composition index, and let

GR(zp,1980, 2001) and GR(zs,1980, 2001) denote the growth rates of primary and secondary

school enrollment . Then the growth rate of the volume indicator with a teaching staff

composition adjustment for primary education is:

        GR(zp,1980, 2001,TSCI1980, 2001) = GR(zp,1980, 2001) + GR(TSCI1980, 2001)            (5)




                                                                                                   35
and the growth rate of the volume indicator with a teaching staff composition adjustment

for secondary education is:

       GR(zs,1980, 2001,TSCI1980, 2001) = GR(zs,1980, 2001) + GR(TSCI1980, 2001).     (6)

The growth rate of the volume indicator with a pupil-teacher ratio (PTR) adjustment for

primary education is:

       GR(zp,1980, 2001,PTR1980, 2001) = GR(zp,1980, 2001) - 0.1 GR(PTR1980, 2001)

       (7)

and the growth rate for secondary education is GR(zs,1980, 2001) as calculated above as the

pupil-teacher adjustment is only applied to primary education. The growth rate of the

pupil-teacher ratio is entered with a negative, as an increase in the ratio is associated with

a decline in output quality and a decrease is associated with a rise in output quality.



To adjust for changes in the drop-out rate (DOR), the growth rate of the volume indicator

adjusted for changes in school completion rates for secondary education as proxied by the

changes in the drop-out rate is:

       GR(zs,1980, 2001,DOR1980, 2001) = GR(zs,1980, 2001) - 0.1 GR(DOR1980, 2001)

       (8)



The growth rate in the drop-out rate is entered with a negative as an increase in the rate is

associated with a decline in output quality and a decrease is associated with a rise in

output quality. The growth rate for primary education is GR(zp,1980, 2001) as calculated

above as the drop-out rate adjustment is only applied to secondary education.




                                                                                            36
The growth rate of the primary education volume indicator adjusted for changes in

teaching staff composition and the pupil-teacher ratio is:


   GR(zp,1980, 2001,TSCI1980, 2001,PTR1980,2001) = GR(zp,1980, 2001) + GR(TSCI1980, 2001)

                                                   – 0.1 GR(PTR1980, 2001).            (9)


The growth rate for secondary education is GR(zs,1980, 2001,TSCI1980, 2001) as calculated

above as the pupil-teacher adjustment is only applied to primary education.



The growth rate of the volume indicator adjusted for changes in teaching staff

composition, the pupil-teacher ratio, and the high school drop-out rate for secondary

education is:


   GR(zs,1980, 2001,TSCI1980, 2001, PTR1980,2001,DOR1980,2001) = GR(zs,1980, 2001) +

                           GR(TSCI1980, 2001) – 0.1 GR(PTR1980,2001) – 0.1 GR(DOR1980, 2001).

                           (10)


The growth rate for primary education is GR(zp,1980, 2001,TSCI1980, 2001,PTR1980,2001) as

calculated above as the high school drop-out rate adjustment is only applied to secondary

education.



Quality-adjusted volume indicators are then calculated for primary and secondary

education by applying the quality-adjusted growth rates to a 1996 base set equal to

enrollment in 1996. Implicit price indexes are estimated by dividing nominal

expenditures by the volume indicators. The resulting implicit price index is normalized

to 1.0 in 1996. The final step is to calculate a chain-type Fisher quantity index with


                                                                                             37
quality-adjusted enrollment and implicit prices for primary and secondary education as

the inputs and to calculate the implicit price index associated with the chain-type Fisher

quantity index.73


Decomposing the Fisher chain-type indexes allow for estimation of the contribution of

the subcomponents: primary and secondary education, to growth in prices and quantities

for the aggregate. The results of a decomposition for the quality-unadjusted estimates for

the preferred indexes (that which uses teaching staff composition and the pupil-teacher

ratio to adjust enrollment) is shown in the middle panel of Table 3.



The growth rate of the decomposition of the chain-type Fisher quality-unadjusted volume

index, ci, is calculated as

                                  qi y+1
                  GR(ciy) = s-iy ( q – 1)                                                          (11)
                                     iy


for i=primary education or secondary education, where

                               FPpiy qiy + pi y+1 qiy
                  s-iy =                                     ,                                     (12)
                           FP[∑ j pjy qjy] + ∑ j pjy+1 qjy

FP is a chain-type Fisher price index for year y, pi y+1 qit represents expenditures on

education level i in year y adjusted for price change between year y and year y+1 and s-iy

may be interpreted as a weighted average of the expenditure share for education level i in

year y and its hypothetical share at year y+1 if only prices had changed. The quality-

unadjusted chain-type Fisher quantity indexes for primary and secondary education are

then calculated from the growth rates in the same manner as described above.

73
  For an explanation of how chain-type Fisher indexes are constructed and a discussion of their properties,
see Young (1992), Diewert (1993), and Triplett (1992). Because of the properties of Fisher indexes, the
implicit price indexes are Fisher price indexes.


                                                                                                          38
The decomposition of the chain-type Fisher quality-unadjusted price index is calculated

using equations (10) and (11) above, with the price relative substituted for the quantity

relative in equation (10) and chain-type Fisher quantity indexes, FQ, substituted in for the

Fisher price indexes in equation (11). The quality-unadjusted chain-type Fisher price

indexes and implicit price indexes for primary and secondary education are then

calculated in a manner parallel to the quality-unadjusted chain-type Fisher quantity

indexes and implicit price indexes, with appropriate normalization.



Table 3 shows that price change is always greater than quantity change for the periods

listed, with the price change typically being in the ballpark of twice the U.S. gross

domestic purchases price change. When making comparisons, it should be remembered

that the price changes in Table 3 are probably overstated and the quantity changes

understated. This is because of quality improvements occurring over time that have not

yet been, or perhaps never will be (due to lack of data), captured in the estimates and

because of other factors leading to higher expenditures per pupil such as the increase in

the number of special education students. For example, has the quality of education

received in high school increased as evidenced by an increase in advanced placement

courses? The comparison is made to gross domestic purchases prices rather than to GDP

prices to exclude exports, which are included in GDP and excluded in gross domestic

purchases, and to include imports, which are excluded in GDP and included in gross

domestic purchases. Chart 2 plots the preferred price deflator (derived from the volume

indicator that uses the teaching staff composition index and the pupil-teacher ratio to




                                                                                            39
adjust enrollment) against the gross domestic purchases price deflator. Except for a brief

period during the early nineties, the preferred price deflator rises at a rate faster than the

gross domestic purchase price deflator. The decomposition of the price deflators derived

from chain-type Fisher quality-unadjusted enrollment indexes in the middle panel of

Table 3 show that this is primarily because of the significantly higher contribution of

primary education price change (4.19 percent vs. 1.90 percent, 4.88 percent vs. 2.52

percent, and 3.57 percent vs. 1.34 percent). The rate of price change did moderate

significantly in the last period, 1990-2001, compared to the first period, 1980-1990.

                                                                                      C h art 2
                                                                        P referred Im plicit P rice D eflator
                                                                                         vs.
                                                                 th e G ross D om estic P u rch ases P rice D eflator
                      1 .35

                      1 .25

                      1 .15

                      1 .05
   Price (1996=1.0)




                      0 .95

                      0 .85

                      0 .75

                      0 .65

                      0 .55

                      0 .45

                      0 .35
                              1980   1981   1982   1983   1984    1985   1986   1987   1988   1989   1990    1991   1992   1993   1994   1995   1996   1997   1998   1999   2000   2001
                                                                                                        Y e ar




                                 P re fe rre d im p lic it p ric e d e fla to r (d e rive d fro m a c h a in -typ e F is h er q u a n tity in d e x )

                                 G ro s s d o m e s tic p u rc h a s e s p rice d e fla to r




Enrollment data, which are the foundation for all volume indicators, show the influence

of demographics. Noticeable is the decline in the population of high school students

during 1980-1990, which ripples through all measures, but it is most apparent in the

unweighted quality-unadjusted enrollment growth rates for secondary education in the




                                                                                                                                                                                          40
top panel of Table 3. Total enrollments nonetheless have increased during all three

periods.




The difference between the top panel and the middle panel total growth rates reflect the

fact that it is substantially more expensive to educate a secondary school student than a

primary school student. The average expenditure per secondary student is estimated to be

significantly higher than that per primary student.74 On average only either 30 percent or

31 percent of all primary and secondary students attend secondary school. Relative

expenditures enter in to the Fisher index calculation.




Looking at the middle panel of Table 3, the total growth rates for the quality-unadjusted

measures can be compared directly to the quality-adjusted enrollment volume indicators

growth rates. Note that the change in the quantity index is offset by a change in the

opposite direction in the price deflators.75 This fact again highlights the sensitivity of the

price results to quality adjustment of the quantity indexes. It is easiest to compare the

quality unadjusted estimates with those adjusted by the teaching staff composition index,

as this difference, except for rounding, is exactly equal to the growth rate for the teaching



74
   It is difficult to estimate expenditure per student for primary versus secondary students because
expenditures may be reported on a school district basis aggregating across primary and secondary schools
and because of different school formats, e.g., middle schools versus junior high schools. Our expenditure
per student estimates are based on Digest of Educational Statistics tables. See National Center for
Education Statistics, Digest of Educational Statistic, various issues.
75
   With Fisher indexes, the growth rates are related by the following equation:
           (1 + n) = (1 + p) * (1 + q),
where n is the nominal growth rate, p is the price growth rate, q is the quantity growth rate, and the growth
rates are in decimal format, e.g., a 6.00 percent growth rate appears as .0600.


                                                                                                           41
staff composition index shown in Table 2. However, as the pupil-teacher ratio and high

school drop-out rate quality adjustments affect only one part of enrollments, not all

enrollments as with the teaching staff composition index, it is much more difficult to

make a direct comparison. The impact of both are reduced because the weights are less

than 1 and because minus the pupil teacher ratio and the drop-out rate are both entered at

a 0.1 power. Accordingly, even though the absolute value of the rates of growth of the

pupil-teacher ratio and the drop-out rate are greater than that for the teaching staff

composition index (see Table 2), the volume indicators with the pupil-teacher ratio and

the drop-out rate adjustments grow at a slower rate for 1980-2001 than that with the

teaching staff composition adjustment.76




These estimates show that quality adjusting a volume indicator can have a significant

effect on estimated output and prices. The difference between the growth rates for the

quality-unadjusted measure and the preferred quality-adjusted measure (that using the

teaching staff composition index and the pupil-teacher ratio) is 0.18 percent, 0.55 percent,

and -0.15 percent for 1980-2001, 1980-1990, and 1990-2001, respectively.77 The impact

on output is greater than the impact on prices as the rates of growth of quantities are

much smaller than the rates of growth of prices. Chained BEA 2000 dollar estimates for

primary and secondary education using an input cost-based output approach became

76
   The fact that the growth rates for the volume indicator with a pupil-teacher quality adjustment and that
for the volume indicator with a high-school drop-out rate quality adjustment are almost identical is
coincidental. The product of the (higher) expenditure weight for primary school with the absolute value of
the (lower) rate of growth for the pupil teacher growth rate is equal the the product of the (lower)
expenditure weight for secondary school with the absolute value of the (higher) rate of growth for the high
school drop-out rate.
77
   Recall that changes in the experience distribution seem to be driving the decline in the teaching staff
composition index over the 1990-2001 period. See table 2.


                                                                                                         42
available in October of 2004. A comparison can be made between those estimates and the

quality-adjusted output estimates presented here.78




Conclusion


Given its goal of continuously improving the U.S. national accounts, BEA is examining a

number of possible changes to the way it measures the output of the government sector.

This exploratory paper looks at one possible methodology that might be adopted if a

change is made. Focusing on prices particularly highlights that much additional research

needs to be undertaken, both for primary and secondary education, and for other

components of the government education function, e.g., for higher education and

libraries. For primary and secondary education, beyond looking at high school

completion factors, additional research is needed. This includes research on trends in

numbers of teaching specialists; and the number and sizes of special education classes,

English as a Second Language (ESL) classes and other special classes to interpret or

modify the pupil-teacher ratios; research on the impact and growth of school-sponsored

activities; and research on the composition of the student body as it affects learning, to

name a few possible avenues of future work. As the title of the paper indicates, this paper

is exploratory.




78
     The relevant BEA category is titled “elementary and secondary education.”


                                                                                             43
References

Abraham, Katharine G. and Christopher Mackie, eds. 2005. Beyond the Market:
      Designing Nonmarket Accounts for the United States. Washington, DC: The
      National Academies Press.

Algera, Symon. 1998. “The Measurement of Price and Volume Changes of Government
       Output: The Case of the Netherlands.” Joint ESCAP-OECD Meeting. 1993
       System of National Accounts: Five Years On. Bangkok, May 4-8.

Ashaye, Timi. 2001. “Recent Developments in the Measurement of General Government
      Output.” Economic Trends, No. 576. November, pp. 41-44.

Atkinson Commission. 2005. Atkinson Review: Final Report, Measurement of
       Government Output and Productivity for the National Accounts, Palgrave
       Macmillan, Hampshire, England, January.

Australian Bureau of Statistics. 1998. “Measuring Non-Market Sector Output: Recent
       Work by the Australian Bureau of Statistics.” Paper presented at OECD Meeting
       of National Account Experts. Paris, September 22-25, STD/NA(98)3.

Australian Bureau of Statistics. 1999. “Non-Market Output—Recent Work by the
       Australian Bureau of Statistics.” Paper presented at OECD Meeting of National
       Account Experts. Paris, September 21-24, STD/NA(99)41.

Australian Bureau of Statistics. 2000a. “Non-Market Output—Recent Work by the
       Australian Bureau of Statistics.” Paper presented at OECD Meeting of National
       Account Experts. Paris, September 26-29. STD/NA/RD(2000)04.

Australian Bureau of Statistics. 2000b. “Australian National Accounts: Concepts, Sources
       and Methods.” ABS Publication No. 5216.0. November.

Australian Bureau of Statistics. 2001a. “New Chain Volume Estimates for the Services
       Sector.” ABS National Income, Expenditure and Product, Australian National
       Accounts. Publication No. 5206. March.

Australian Bureau of Statistics. 2001b. “Experimental Estimates of Human Capital for
       Australia.” OECD Meeting of National Accounts Experts. Paris, October 9-12.

Australian Bureau of Statistics. 2002a. “Australian National Accounts: Non-Profit
       Institutions Satellite Account.” ABS Publication No. 5256.0. November.

Australian Bureau of Statistics, 2002b. “New Volume Estimates for Health and
       Education Services.” Australia Now, Year Book Australia 2002. National
       Accounts.




                                                                                       44
Australian Bureau of Statistics. 2003. “Chapter 9: Outputs and Outcomes.” Measuring
       Learning in Australia, A Framework for Education and Training Statistics.

Bain, David. 2004. ABS Education Worksheets, received April 30, 2004.

Ballou, Dale. 1996. “Do Public Schools Hire the Best Applicants.” Quarterly Journal of
       Economics. Vol. 111, No. 1, February, pp. 97-133.

Baxter, Michael. 2000. “Developments in the Measurement of General Government
       Output.” Economic Trends. No. 562. September, pp. 3-5.

Black, Sandra. 1998. “Measuring the Value of Better Schools.” FRBNY Economic Policy
       Review. Vol. 4, No. 1. March.

Bournay, Jacques. 2002. “An Alternative Way of Treating Government Consumption
      Expenditure in the National Accounts.” Prepared for the 27th General Conference
      of The International Association for Research in Income and Wealth. Djurhamn,
      Sweden, August 18-24.

Bureau of Economic Analysis. 1997. “Statistical Conventions Used for the NIPA
      Estimates.” National Income and Product Accounts of the United States, 1929-97.
      p. M-25.

Bureau of Economic Analysis. 1998. “Government Transactions: Constant-Dollar
      Purchases of Goods and Services.” Survey of Current Business. November,
      p. 100.

Bureau of Economic Analysis. 2002. “Annual NIPA Revision, Newly Available Tables.”
      Survey of Current Business. October, pp. 12-19.

Bureau of Economic Analysis. 2002. “Table 3.15, Government Consumption
      Expenditures and Gross Investment by Function.” National Income and Product
      Accounts. Revised October 16, 2002. Available from
      http://www.bea.gov/bea/dn/nipaweb/SelectTable.asp?Selected=N#S1. Internet;
      accessed December 2, 2003.

Bureau of Economic Analysis. 2003. “Table 1.1, Gross Domestic Product.” National
      Income and Product Accounts. Revised November 23, 2003. Available from
      http://www.bea.gov/bea/dn/nipaweb/SelectTable.asp?Selected=N#S1. Internet;
      accessed December 2, 2003.

Caplan, David. 1998. “Measuring the Output of Non-Market Services.” Economic
       Trends. No. 539. October, pp. 45-49.




                                                                                      45
Card, David, and Alan Krueger. 1992. “Does School Quality Matter? Returns to
       Education and the Characteristics of Public Schools in the United States.” Journal
       of Political Economy, Vol. 100, No. 11.

Cipollone, Piero and Rosolia, Alfonso. 2007. “Social Interactions in High School:
       Lessons from an Earthquake. American Economic Review 97, no. 3 (June): 948-
       965.

Commission of the European Communities, International Monetary Fund, Organisation
     for Economic Cooperation and Development, United Nations, and World Bank.
     1993. System of National Accounts 1993. Brussels/Luxembourg, New York,
     Paris, Washington, DC.

Committee on National Statistics. 2003. “Designing Nonmarket Accounts for the United
     States: Interim Report.” National Research Council. The National Academies
     Press: Washington, DC.

Currie, Janet and Duncan Thomas, "Early Test Scores, Socioeconomic Status and Future
        Outcomes," NBER Working Paper No. 6943, February 1999.

Darling-Hammond, Linda. 2000. “Teacher Quality and Student Achievement: A Review
       of State Policy Evidence. Education Policy Analysis Archives. 8(1).
       http://epaa.asu.edu/epaa/v8n1.

Denny, Charlotte. 2003. “Public Sector Inflation ‘Wasted’ 7.5 Billion Sounds, Says
      CBI.” Guardian Unlimited. November 10.

Department for Education and Skills, United Kingdom,. 2005 “Measuring Output from
      the Education System, Measuring Education Output in the National Accounts, An
      Overview of Several Methods Developed as Part of the Atkinson Review,”
      October.

Diewert, W. Erwin. 1993. “Fisher Ideal Output, Input and Productivity Indexes
      Revisited.” In W. Erwin Diewert and Alice O. Nakamura, eds. Essays in Index
      Number Theory Volume 1. North-Holland. Amsterdam. pp. 317-354.

Diewert, W. Erwin. 1995. “Price and Volume Measures in the System of National
      Accounts.” NBER Working Paper No. 5103. May.

Economic and Social Commission for Asia and the Pacific, Organisation for Economic
      Cooperation and Development. 1998. “Report.” Joint ESCAP-OECD Meeting on
      National Accounts. 1993 System of National Accounts: Five Years On. Bangkok,
      May 4-8.




                                                                                      46
Edwards, Rob, Peter Comisari, and Tony Johnson. 2002. “Beyond 1993: The System of
      National Accounts and the New Economy.” IAOS Conference on the New
      Economy. London, August 27-29.

Ehrenberg, Ronald G., and Dominic J. Brewer. 1994. “Do School Characteristics Matter?
      Evidence from High School and Beyond.” Economics of Education Review.
      13(1). pp. 1-17.

Ehrenberg, Ronald G., and Dominic J. Brewer. 1995. “Did Teachers’ Verbal Ability and
      Race Matter in the 1960s? Coleman Revisited.” Economics of Education Review.
      14(1). pp. 1-21.

Eurostat. 1998. “Final Report of the Task Force: Prices and Volumes for Education.”
       September.

Eurostat. 2001. “Handbook on Price and Volume Measures in National Accounts.”
       Version prepared for the seminar on Price and Volume Measures. Statistics
       Netherlands: Voorburg, March 14-16.

Eurostat. 2002. “Handbook on Price and Volume Measures in National Account:
       Methods and Nomenclature.” BEA’s Strategic Plan for 2001-2005, Survey of
       Current Business, Vol. 82, No. 5. May.

Ferguson, Ronald F. 1991. “Paying for Public Education: New Evidence on How and
       Why Money Matters.” Harvard Journal on Legislation. 28(2). pp. 465-498.

Ferguson, Ronald F., and Helen F. Ladd. 1996. “Additional Evidence on How and Why
       Money Matters: A Production Function Analysis of Alabama Schools.” In Helen
       F. Ladd (ed.) Holding Schools Accountable: Performance Based Approaches to
       School Reform. The Brookings Institution. Washington, DC.

Finn, Jeremy D. 1998a. “Class Size: What Does Research Tell Us?” Laboratory for
        Student Success, Spotlight on Student Success. No. 207.

Finn, Jeremy D. 1998b. Class size and students at risk: What is known? What is next?
        Washington, DC: U.S. Department of Education, Office of Educational Research
        and Improvement, National Institute on the Education of At-Risk Students.

Fisk, Donald, and Darlene Forte. 1997. “The Federal Productivity Measurement Program:
       Final Results.” Monthly Labor Review. May, pp. 19-28.

Fraumeni, Barbara M. 2000. “The Output of the Education Sector as Determined by
      Education’s Effect on Lifetime Income.” Paper presented at the Workshop on
      Measuring the Output of the Education Sector. Brookings Institution:
      Washington, DC. April 7.




                                                                                      47
Fraumeni, Barbara M. 2001. “Imputing the Services of Government Capital.” Prepared
      for the BEA Advisory Committee Meeting. November 30.

Fraumeni, Barbara M., and Alan Krueger. 2003. “Chapter on Education Accounts.”
      National Science Foundation: Panel on Nonmarket Accounts. Unpublished
      Preliminary Draft—For discussion purposes only.

Fraumeni, Barbara M., Marshall B. Reinsdorf, Brooks B. Robinson, & Matthew P.
      Williams. 2004. “Real Output Measures for the Education Function of
      Government: A First Look at Primary & Secondary Education.” Paper presented
      at the Public Services Performance Workshop. National Institute of Economic and
      Social Research, London, UK. March 2.

Glass, Gene, and Mary Lee Smith. 1978. Meta-Analysis of the Relationship of Class Size
       and Student Achievement. San Francisco. Far West Laboratory for Educational
       Research.

Goldhaber, Dan D. and Dominic J. Brewer. 1997. “Evaluating the Effect of Teacher
      Degree Level on Educational Performance. In William Fowler (ed.)
      Developments in School Finance, 1996. U.S. Department of Education. National
      Center for Education Statistics. (NCES 97-535). Washington, DC. U.S.
      Government Printing Office. pp. 197-210.

Griliches, Zvi. "Productivity, R&D, and the Data Constraint," The American Economic
       Review, March 1994, pp. 1-23.

Haan, Mark de, and Myriam van Rooijen-Horsten, with contributions from Dirk van den
      Bergen and Ronald de Jong. 2003. “Knowledge Indicators Based on Satellite
      Accounts: Final Report for NESIS-Work Package 5.1.” Statistics Netherlands.

Hadar, Ezra, Pablo Mandler, and Ayelet Barzel. 1998. “Indicators for Changes in Output
       of Non-market Services.” OECD Meeting of National Accounts Experts. Paris,
       September 22-25.

Hanushek, Eric A. 1986. “The Economics of Schooling: Production and Efficiency in
      Public Schools.” Journal of Economic Literature. Vol. 24. September,
      pp. 1141-1177.

Hanushek, Eric A. 1996. “Measuring Investment in Education.” Journal of Economic
      Perspectives. Volume 10, Number 4. Fall, pp. 9-30.

Hanushek, Eric A. 1998. “The Evidence on Class Size.” Occasional Paper 98-1. W. Allen
      Institute of Political Economy, University of Rochester. February, pp. 1-40.

Hanushek, Eric A. 2002. “Publicly Provided Education.” NBER Working Paper 8799.
      http://www.nber.org/papers/w8799.



                                                                                    48
Hanushek, Eric A, Steven G. Rivkin, and Lori L. Taylor. 1996. “Aggregation and the
      Estimated Effects of School Resources.” The President and Fellows of Harvard
      College and the Massachusetts Institute of Technology. pp. 611-627.

Hedges, Larry V., Richard D. Laine, and Rob Greenwald. 1994. “Does Money Matter? A
      Meta-Analysis of Studies of the Effects of Differential School Inputs on Student
      Outcomes.” Educational Researcher. 23(3). pp. 5-14

Hoxby, Caroline M. 2002. “School Choice and School Productivity (Or Could School
      Choice be a Tide that Lifts All Boats?)” National Bureau of Economic Research
      Working Paper 8873. http://www.nber.org/papers/w8873.

Hughes, Andrew. 2002. “Guide to the Measurement of Government Productivity.”
      International Productivity Monitor. No. 5. Fall, pp. 64-77.

Jenkinson, Graham. 2003. “Measuring the Volume of Government Outputs.” OECD
       National Accounts Experts Meeting. Paris, October 7-10.

Joel Popkin and Company. 2003a. “Part 1, Nonprofit Services: Conceptual Issues
       Associated With the Measurement of Output and Prices.” Draft Report on
       Nonprofit Services.

Joel Popkin and Company. 2003b. “Progress Report: Part 4, Data for Nonprofit
       Services.” Draft Report on Nonprofit Services.

Joel Popkin and Company. 2003c. “Progress Report: Parts 2 and 3, Methodologies
       Currently Implemented and Research Being Conducted by Statistical Agencies
       for the Measurement of Prices and Real Output: Nonprofit Services.” Draft
       Report on Nonprofit Services.

Jorgenson, Dale W., and Barbara M. Fraumeni. 1992. “The Output of the Education
       Sector.” In Zvi Griliches (ed.) Output Measurement in the Service Sectors.
       Chicago: National Bureau of Economic Research, Studies in Income and Wealth,
       Vol. 56. University of Chicago Press.

Karras, Georgios. 1996. “The Optimal Government Size: Further International Evidence
       on the Productivity of Government Services.” Western Economic Association
       International. Economic Inquiry. Vol. XXXIV. April, pp. 193-203.

Kazemier, Brugt. 1997. “Volume Measurement of Government Output: The Dutch
      Practice Since Revision 1987.” STD/NA(97)16.

Konijn, Paul and Foske Kleima, 2000a. “Volume Measurement of Education.” Paper
       presented at OECD Meeting of National Account Experts. Paris,
       September 26-29. STD/NA(2000)27.



                                                                                     49
Konijn, Paul and Foske Kleima, 2000b. “Volume Measurement of Education.” Statistics
       Netherlands, Division Macro-economic Statistics and Dissimilation, Sector
       Support and Development. November.

Krueger, Alan B. 1997. “Experimental Estimates of Educational Production Functions.”
      NBER Working Paper w6051. June. http://www.nber.org/papers/w6051.

Krueger, Alan B. 1998. “Experimental Estimates of Educational Production Functions.”
      March, unpublished.

Krueger, Alan B. 1999. “Experimental Estimates of Educational Production Functions.”
      Quarterly Journal of Economics. Vol. 114(2). pp. 497-532.

Krueger, Alan B. 2002. “Economic Considerations and Class Size.” NBER Working
      Paper w8875. April. http://www.nber.org/papers/w8875.

Landefeld, Steven J., and Stephanie H. McCulla. 2000. “Accounting for Nonmarket
      Household Production Within a National Accounts Framework,” The Review of
      Income and Wealth. Series 46, No. 3, September.

Lazear, Edward P. 1999. “Educational Production.” NBER Working Paper w7349.
       http://www.nber.org/papers/w7349.

Lazear, Edward P. 2001. “Educational Production.” Quarterly Journal of Economics.
       116, no. 3, August, pp. 777-803.

Lum, Sherlene K.S., Brian C. Moyer, and Robert E. Yuskavage. 2001. “Improved
      Estimates of Gross Product by Industry for 1947-98.” Survey of Current Business,
      June, pp. 24-54.

Malizia, Raffaele. 1998. “The Estimation of General Government Services at Constant
       Prices: Methodology and Application Proposal for Italy.” Joint ESCAP-OECD
       Meeting on National Accounts. 1993 System of National Accounts: Five Years
       On. Bangkok, May 4-8.

Monk, David H. and Jennifer King. 1994. “Multi-level Teacher Resource Effects on
      Pupil Performance in Secondary Mathematics and Science: The Role of Teacher
      Subject Matter Preparation.” In Ronald Ehrenberg (ed.) Contemporary Policy
      Issues: Choices and Consequences in Education. Ithaca, NY. ILR Press. pp. 29-
      58.

Moss, David, and Sarah Brennan. 2002. “National Economic Accounting: Past, Present,
      and Future.” National Economic Accounting. President and Fellows of Harvard
      College. Boston, MA. December 4.




                                                                                      50
Mosteller, Frederick. 1995. “The Tennessee Study of Class Size in the Early School
       Grades.” The Future of Children: Critical Issues for Children and Youths. Vol. 5,
       No. 2. Summer/Fall, pp. 113-127.

Mosteller, Frederick, and Daniel P. Moynihan (eds.) 1972. On Equality of Educational
       Opportunity. New York. Random House.

National Center for Education Statistics. 2000. “Monitoring School Quality: An
       Indicators Report.” U.S. Department of Education, Office of Educational
       Research and Improvement. December.

National Center for Education Statistics. 2003. Digest of Educational Statistics, 2002.
       June. http://www.nces.ed.gov/programs/digest/.

National Center for Policy Analysis. 1998. “Hanushek Study: Is Class Size Important?”
       http://www.ncpa.org/pi/edu/july98b.html.

National Education Association. 2004. “Special Education and the Individuals with
       Disabilities Act.” http://www.nea.org/specialed/ .

National Research Council, Committee on National Statistics. 1998. “Measuring the
       Government Sector of the U.S. Economic Accounts.” National Academy Press.
       Washington, DC.

Neuburger, Henry, and David Caplan. 1998. “The Measurement of Real Public Sector
      Output in the National Accounts.” Economic Trends, No. 531. February,
      pp. 29-35.

Niemi, Mervi. 1998. “Measuring Government Sector Output and Productivity in Finland-
       Application of the Output Indicator Method.” OECD Meeting of National
       Accounts. Paris, September 22-25.

Nordhaus, William. 1997. “Do Real-Output and Real-Wage Measures Capture Reality?
      The History of Lighting Suggests Not.” in Timothy Bresnahan and Robert Gordon
      (eds.) The Economics of New Goods. Chicago: National Bureau of Economic
      Research, Studies in Income and Wealth, Vol. 58. University of Chicago Press.

Nordhaus, William, and Edward Kokkelenberg (eds.) 1999. Nature’s Numbers:
      Expanding the National Economic Accounts to Include the Environment.
      Washington, DC: National Academy Press.

Nye, Barbara, B. DeWayne Fulton, Jayne Boyd-Zaharias, and Van A. Cain. 1995. The
      Last Benefits Study, Eighth Grade Technical Report. Nashville, TN: Center of
      Excellence for Research in Basic Skills, Tennessee State University.




                                                                                          51
OECD. 1997. “Productivity Measurement for Non-Market Services.” Paper presented at
     The Joint ECE/OECD/Eurostat Meeting on National Accounts. Paris, June 3-6.
     STD/NA(97)14.

OECD. 1998. “Measuring Output for Non-market Services.” Joint ESCAP-OECD
     Meeting on National Accounts, 1993 System of National Accounts: Five Years
     On. Bangkok, May 4-8.

OECD Public Management Service. 1999. “Measuring Public Sector Productivity: Case
     Studies Presented to the PUMA Expert Group Meeting on Measuring Productivity
     in the Government Sector.” Paper presented at OECD Meeting of National
     Account Experts. Paris, September 21-24. STD/NA/RD(99)2 and
     PUMA/SBO(99)6.

OECD Public Management Service. 1999. “Report on the PUMA Expert Group Meeting
     on Measuring Productivity in the Government Sector.” Paper presented at OECD
     Meeting of National Account Experts. Paris, September 21-24.
     STD/NA/RD(99)1 and PUMA/HRM/M(99)1.

OECD, 2002. Handbook on Nonprofit Institutions in the System of National Accounts.

Office for National Statistics, United Kingdom. 1997. “The Deflation of Government
        Consumption.” Paper presented at The Joint ECE/OECD/Eurostat Meeting on
        National Accounts. Paris, June 3-6. STD/NA(97)31.

Office for National Statistics, UK Centre for the Measurement of Government
        Productivity, United Kingdom. 2005. “Public Service Productivity, Education,”
        October.

Office for National Statistics, United Kingdom. 1997. “Use of Productivity Estimates in
        the United Kingdom Output Measure of Gross Domestic Product.” Paper
        presented at The Joint ECE/OECD/Eurostat Meeting on National Accounts. Paris,
        June 3-6, STD/NA(97)26.

Office for National Statistics, United Kingdom. 2000. “Development in the Measurement
        of General Government Output.” OECD Meeting of National Accounts Experts.
        Paris, September 26-29.

Oi, Walter Y. 2003. “Ciphers, Seconds, and Firsts (A Reflection on Hard to Measure
      Sectors).” Rochester, New York: November 10.

O’Mahony, Mary, and Philip Stevens. 2003. “International Comparisons of Performance
     in the Provision of Public Services: Outcome Based Measures for Education.”
     National Institute of Economic and Social Research. November.




                                                                                        52
O’Mahony, Mary, and Philip Stevens. 2004. “International Comparisons of Performance
     in the Provision of Public Services: Outcome Based Measures for Education.”
     Presentation by O’Mahony at the Public Services Performance Workshop.
     National Institute of Economic and Social Research, London, UK. March 2.

Powell, Matthew, and Alwyn Pritchard. 2002. “Measuring Government Output –
       Mystical or Misunderstood?” Paper prepared for the 27th General Conference of
       The International Association for Research in Income and Wealth. Djurhamn,
       Sweden, August 18-24.

Pritchard, Alwyn. 2002a. “Measuring Productivity Change in the Provision of Public
       Services.” Economic Trends. No. 582. May, pp. 20-32.

Pritchard, Alwyn. 2002b. “Measuring Productivity Change in the Provision of Public
       Services.” Paper produced for the NIESR Conference on Productivity and
       Performance in the Provision of Public Services, November 19.

Pritchard, Alwyn. 2003. “Understanding Government Output and Productivity.”
        Economic Trends. No. 596. July, pp. 27-40.

Pritchard, Alwyn, and Matthew Powell. 2001. “Direct Measures of Government Output:
       A Few Conceptual and Practical Issues.” Office of National Statistics – United
       Kingdom. OECD Meeting of National Accounts Experts. Paris, October 9-12.


Pritchard, Ivor. 1999. “Reducing Class Size, What Do We Know?.” National Institute of
       Student Achievement, Curriculum and Assessment. Office of Educational
       Research and Improvement, U.S. Department of Education. Revised March.
       http://www.ed.gov/pubs/ReducingClass/index.html.

Public Management Service, Public Management Committee (PUMA). 1999.
       “Measuring Public Sector Productivity.” 20th Annual Meeting of Senior Budget
       Officials. Paris, June 3-4.

“Public Sector Productivity, Governing Costs.” 2003. The Guardian. Leader,
       November 10.

Rivkin, Steven G. 2000. “The Estimation of Productivity Change in Education.” Amherst
       College. April.

Rivkin, Steven G., Eric A. Hanushek, and John F. Kain. 2001. “Teachers, Schools and
       Academic Achievement.” NBER Working Paper w6691 revised.

Rivkin, Steven G., Eric A. Hanushek, and John F. Kain. 2002. “Teachers, Schools and
       Academic Achievement.” July 1998 NBER Working Paper 6691 revised, July.




                                                                                      53
Robinson, G. E. 1990. “Synthesis of Research on Effects of Class Size.” Educational
      Leadership. 47(7). pp. 80-90.

Robinson, Glen E. and James H. Wittebols. 1986. “Class Size Research: A Related
      Cluster Analysis for Decision-Making.” Arlington, VA. Education Research
      Service.

Schwartz, Amy, and Benjamin Scafidi. 2000. “Quality Adjusted Price Indices for Four
      Year Colleges.” Paper presented at the Bureau of Labor Statistics’ Conference on
      Issues in Measuring Price Change and Consumption. Washington, DC, June 5-8.

Sherwood, Mark K. 1994. “Difficulties in the Measurement of Service Outputs,” Monthly
      Labor Review, March, pp. 11-19.

Slavin, Robert E., and Madden, Nancy A. 1989. "What Works for Students at Risk: A
        Research Synthesis." Educational Leadership. 46(5). pp. 4-13.

Statistics Canada, Input-Output Division. 1993. “Services Industries in the Canadian
         Input-Output Accounts: Sources of Data and Methods of Estimation (Current
         Prices).” Statistics Canada Catalogue No. 15-601E, Vol. 2. January.

Statistics Directorate. 1999. “Report of the PUMA Expert Group Meeting on Measuring
         Productivity in the Government Sector.” OECD Public Management Service,
         OECD Meeting of National Accounts Experts. Paris, September 21-24.

Statistics Netherlands, National Accounts Branch. 2003. “Inventory of Sources and
        Methods for Price and Volume Measures in the Dutch National Accounts.” BPA
        No. 02254-02-MNR.

Statistics Norway. 2000. “National Accounts 1992-1999: Institutional Sector Accounts.”
        November.

United Nations. 2003. “Chapter 6: Measuring NPI Output.” Handbook on Non-Profits
       Institutions in the System of National Accounts. ST/ESA/STAT/SER.F/91. New
       York.

Triplett, Jack E. 1992. “Economic Theory and BEA’s Alternative Quantity and Price
        Indexes.” Survey of Current Business. April, pp. 49-52.

U.S. Census Bureau. 2002. Statistical Abstract of the United States: 2002.

Ward, Michael P. 2002. “Government Statistics; Information or Pro Forma Data?”
      unpublished paper.

Williams, Matthew P. 2003. “Real Output Measures for the Education Function of
       Government: A More Relevant Approach,” draft, December.



                                                                                       54
Young, Allan H. 1992. “Alternative Measures of Change in Real Output and Prices.”
      Survey of Current Business. April, pp. 32-48.




                                                                                    55
