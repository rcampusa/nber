                              NBER WORKING PAPER SERIES




      IDENTIFICATION AND EFFICIENCY BOUNDS FOR THE AVERAGE MATCH
          FUNCTION UNDER CONDITIONALLY EXOGENOUS MATCHING

                                        Bryan S. Graham
                                        Guido W. Imbens
                                          Geert Ridder

                                      Working Paper 22098
                              http://www.nber.org/papers/w22098


                     NATIONAL BUREAU OF ECONOMIC RESEARCH
                              1050 Massachusetts Avenue
                                Cambridge, MA 02138
                                     March 2016




This paper was presented at the 2012 European Summer Meetings of the Econometric Society,
the December 2012 SFB 884 Research Conference on the Evaluation of Political reforms at the
University of Mannheim, and at seminars hosted by the University of California - Berkeley,
University of California - Davis, University of Wisconsin - Madison, Northwestern University
and the University of Southern California. We thank these seminar audiences for useful feedback.
We are also grateful to Stephane Bonhomme, Richard Blundell, Konrad Menzel and James
Powell for useful discussions. Financial support from the National Science Foundation (SES
#0820361) for the first author's contribution is gratefully acknowledged. All the usual disclaimers
apply. The views expressed herein are those of the authors and do not necessarily reflect the
views of the National Bureau of Economic Research.

At least one co-author has disclosed a financial relationship of potential relevance for this
research. Further information is available online at http://www.nber.org/papers/w22098.ack

NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.

© 2016 by Bryan S. Graham, Guido W. Imbens, and Geert Ridder. All rights reserved. Short
sections of text, not to exceed two paragraphs, may be quoted without explicit permission
provided that full credit, including © notice, is given to the source.
Identification and Efficiency Bounds for the Average Match Function under Conditionally
Exogenous Matching
Bryan S. Graham, Guido W. Imbens, and Geert Ridder
NBER Working Paper No. 22098
March 2016

                                          ABSTRACT

Consider two heterogenous populations of agents who, when matched, jointly produce an output,
Y. For example, teachers and classrooms of students together produce achievement, parents raise
children, whose life outcomes vary in adulthood, assembly plant managers and workers produce a
certain number of cars per month, and lieutenants and their platoons vary in unit effectiveness. Let
W ∈ =      {w1,..., wJ } and X ∈  =      {x 1,..., x K } denote agent types in the two populations.
Consider the following matching mechanism: take a random draw from the W = w j subgroup of
the first population and match her with an independent random draw from the X = xk subgroup of
the second population. Let β (w j , x k ) , the average match function (AMF), denote the expected
output associated with this match. We show that (i) the AMF is identified when matching is
conditionally exogenous, (ii) conditionally exogenous matching is compatible with a pairwise
stable aggregate matching equilibrium under specific informational assumptions, and (iii) we
calculate the AMF's semiparametric efficiency bound.




Bryan S. Graham                                     Geert Ridder
University of California - Berkeley                 Department of Economics
530 Evans Hall #3880                                University of Southern California
Berkeley, CA 94720-3880                             Kaprielian Hall
and NBER                                            Los Angeles, CA 90089
bgraham@econ.berkeley.edu                           ridder@usc.edu

Guido W. Imbens
Graduate School of Business
Stanford University
655 Knight Way
Stanford, CA 94305
and NBER
Imbens@stanford.edu
There are two populations, say teachers and classrooms of students.1 Let W 2 W =
{w1 , . . . , wJ } denote the observable type of a teacher and U 2 U unobserved teacher at-
tributes.2 The dimension of U is unrestricted. The J support points of W may encode, for
example, diﬀerent unique combinations of years of teaching experience, levels of education,
race and gender; U corresponds to unobserved dimensions of teacher quality. Teachers are
heterogenous. Let R denote a vector of observable “proxies” for U ; R may have both discrete
and continuous components. We clarify the properties of R further below. All diversity in
the population of teachers is captured by the triple (W, R, U ). We index a random draw
from this population by the subscript i, such that (Wi , Ri , Ui ) corresponds to the ith random
draw (teacher). A generic random draw is denoted by (W, R, U ) (i.e., subscripts omitted).
Let X 2 X = {x1 , . . . , xK } be the observable type of a classroom and V 2 V unobserved
classroom attributes. The dimension of V is unrestricted. The K types of classroom could
enumerate diﬀerent unique combinations of classroom size and/or student gender/ethnicity
mixes. Let S denote an observed vector of proxies for V . We index a random draw from the
population of classrooms by the superscript h, such that X h , S h , V h equals measured and
unmeasured characteristics of the hth random draw (classroom). The sub- and super-script
notation emphasizes the two-population aspect of our setup.
Teachers and classrooms of students are matched (i.e., paired with one another) through
some process. In this paper we only consider one-to-one matching. Restrictions on this
process will be imposed in Section 1 below. Once paired they jointly produce the output,
Y 2 Y ⇢ R, say, student achievement.
Associated with each teacher-classroom pair is a potential or conjectural output (Holland,
1986; Manski, 2007; Imbens and Rubin, 2015). Let Yh (i) denote the potential output when
teacher i matches with classroom h; in production function form,

                                     Yh (i) = g Wi , X h , Ui , V h .                                 (1)

Now consider two teachers, i and i0 , both of type Wi = Wi0 = w. A key feature of our set-up
is that there is no representation of the potential outcome for classroom h in terms of its
assigned teacher’s type, w, alone. This follows because, in general, Ui 6= Ui0 and hence

                    Yh (i) = g w, X h , Ui , V h 6= g w, X h , Ui0 , V h = Yh (i0 ) ,
   1
     We maintain this running example for much of what follows for expository reasons, but our results are
not restricted to this case. See Boyd, Lankford, Loeb and Wyckoﬀ (2013) for empirical context and Graham
(2011a) for other empirical examples and references.
   2
     In what follows random variables are denoted by capital Roman letters, specific realizations by lower
case Roman letters and their support by blackboard bold Roman letters. That is Y , y and Y respectively
denote a generic random draw of, a specific value of, and the support of, Y .


                                                    2
due to heterogeneity in unobserved teacher quality. Consequently, we cannot write Yh (i) =
Yh (w). Although teachers i and i0 may be of the same observed type, Wi = Wi0 = w, they
would typically diﬀer in terms of their unobserved “quality”, Ui 6= Ui0 .
Equation (1) is a production function with two heterogenous inputs, W and X. This contrasts
with the standard single agent production function, where output across heterogeneous firms
varies with the level of a homogenous input (e.g., Chamberlain, 1984, Griliches and Mairesse,
1996; Olley and Pakes, 1996). If we set Ui = ū for all i we recover this familiar single
agent problem. Such a restriction ensures that, conditional on their type, teachers are a
homogenous input. We can then write a classroom’s conjectural output as a function of it
assigned teacher type alone:

                        Yh (i) = Yh (w) = g w, X h , V h = g w, X h , ū, V h .                          (2)

In (2) achievement (output) across heterogeneous classrooms (firms) is a function of the
level of the homogenous input, teacher type (capital), Wi = w. In this paper we instead
consider the non-standard case, where observed output is generated according to (1). Loosely
speaking, both the “firm” (classroom) and the “input” (teacher) are heterogenous in our set-
up. This raises new issues.3
Let h = m (i) equal the classroom assigned to teacher i under the status quo (i.e., observed)
matching. (so that m 1 (h) = i). For simplicity, we assume that (i) the populations of
teachers and classrooms are the same size and (ii) that all classrooms are assigned a teacher
in the status quo matching. Observed output is therefore given by

                                     Yi = g Wi , X m(i) , Ui , V m(i) .                                  (3)

In what follows we write Yi = Ym(i) (i), Xi = X m(i) and Vi = V m(i) to simplify the notation.
Put diﬀerently, the i subscript will be used to index both teachers and teacher-classroom
matches (the latter in the status quo assignment only). Let {Zi }Ni=1 denote a random sample
of size N , from the status quo distribution of matches, of Zi = (Xi , Wi , Ri0 , Si0 , Yi )0 .
The econometrician seeks to use this random sample to make inferences about average out-
put across diﬀerent counterfactual reallocations of teachers to classrooms. Specifically we
   3
     If we consider classroom h’s “treatment” to be the assignment to a specific teacher, then the fact that
classroom h has a diﬀerent potential outcome when assigned teacher i versus teacher i0 is not a violation of
SUTVA (cf., Imbens and Rubin, 2015). However, there is a violation of SUTVA if, instead, we consider the
type of i as the treatment (e.g., assignment to an inexperienced vs. experienced teacher). This follows, as
explained in the main text, because teachers of the same observed type may vary in terms of unobserved,
output-eﬀecting, attributes. Another violation of SUTVA implicit in our set-up is that of no treatment
inference. Interference in our setting arises because if classroom h is assigned to teacher i, then classroom
h0 cannot be assigned to teacher i; matching is one-to-one and rivalrous.


                                                     3
consider the following thought experiment. A social planner takes a random draw from
the subpopulation of type Wi = w teachers. She then takes an independent random draw
from the subpopulation of type X h = x classrooms. The expected outcome associated with
pairing together these two draws is
                                 ˆ ˆ
                           def
                    (w, x) ⌘           g (w, x, u, v) f U |W ( u| w) f V |X ( v| x) dudv.        (4)

We call (4) the average match function (AMF) (cf., Graham, 2011a). The AMF is a building
block for conducting inference on counterfactual reallocations. Observe that we make no
presumption of independence between W and U or X and V . The distribution of teacher
ability, U , may vary systematically with observed years of teacher experience, W . Because
reallocations leave the joint distributions of (Wi , Ui ) and X h , V h unchanged, F U |W and
F V |X are the correct distributions to integrate over in (4).
In contrast, dependence between U and V , given (W = w, X = x), generates a wedge be-
tween observed average output under the status quo matching, E [ Y | W = w, X = x], and the
AMF, (w, x), due to matching on unobservables. Say we wish to learn about the average
match output when experienced teachers, Wi = w, are assigned to classrooms with low prior
achievement, X h = x. If, in the status quo matching, among experienced teachers, those
with high ability, Ui , are matched to high ability, V h , classrooms (among those with the low
prior achievement), then there will be dependence between U and V given (W = w, X = x).
The AMF is defined with reference to a hypothetical matching scheme which rules out such
dependence by construction. This is analogous to the conceptual role played by random
assignment in the program evaluation literature.


Main contributions

In this paper we present three results. First, we show that (w, x) is identified under a condi-
tionally exogenous matching assumption. Our assumption is a multi-agent generalization of
the “selection on observables” or “unconfoundedness” assumption familiar from the program
evaluation literature (e.g., Heckman, Smith and Clements, 1997; Imbens, 2004). Second,
we show that, under certain assumptions about agents’ information sets, our conditionally
exogenous matching assumption is consistent with pairwise stability in an aggregate trans-
ferable utility (TU) matching market of the type introduced by Choo and Siow (2006a,b) and
recently extended by a number of authors (e.g., Chiappori, Salanié and Weiss, 2015; Dupuy
and Galichon, 2014; Galichon and Salanié, 2015; Graham, 2013).4 This result provides guid-
  4
    See Dagvisk (2000), Galichon and Hsieh (2015) and Menzel (2015) for related contributions to non-
transferrable utility (NTU) matching problems.


                                                     4
ance to practitioners interested in applying our results outside of quasi-experimental settings.
In particular, it suggests what types of variables should be included in the proxy vectors R
and S. Third, we characterize the semiparametric eﬃciency bound for (w, x). The bound
is complex, involving several integral equations, but nevertheless provides insights useful for
eﬃcient estimation.
This paper is related to Graham, Imbens and Ridder (2014), which presented explicit es-
timators for various reallocation eﬀects. In that paper (i) teacher and classroom types
were assumed continuously-valued, (ii) methods for covariate adjustment were not presented
(limiting the included formal results to experimental settings) and (iii) the heterogenous two-
agent aspect of the problem was not explicitly developed. Finally, no analysis of semipara-
metric eﬃciency was undertaken. Our results are also related to the very large literature on
eﬃcient covariate adjustment in program evaluation problems (see Imbens and Wooldridge
(2009) for a recent review). While covariate adjustment is a well-studied problem in single
agent models, going back at least to the work of Yule (1897) on the causes of pauperism in
late 19th century England, we are aware of no prior research on covariate adjustment for
multi-agent models.



1    Identification
We assume that the econometrician is able to collect a random sample of output measure-
ments and agent observables from a status quo population of matches.

Assumption 1. (Random Sampling) Let Zi = (Xi , Wi , Ri0 , Si0 , Yi ); {Zi }N
                                                                           i=1 is a random
sequence drawn from the status quo population of matches with distribution function F .

Our key identifying assumption restricts the structure of the status quo matching.

Assumption 2. (Conditionally Exogenous Matching)

                (X, V, S) ? U | W = w, R = r, (W, U, R) ? V | X = x, S = s

for all (w, r) 2 W ⇥ R and all (x, s) 2 X ⇥ S.

Assumption 2 implies that, conditional on teacher observed attributes (W, R), her unobserved
quality, U , has no predictive power for classroom characteristics. Likewise, conditional on
observed classroom attributes (X, S), unobserved classroom attributes, V , have no predictive
power for teacher characteristics. In Section 2 we show that Assumption 2 is consistent with a


                                               5
Choo-Siow aggregate matching market equilibrium under specific assumptions about agents’
pre-match information sets.
To better understand Assumption 2 we first prove the following factorization lemma. This
lemma features in the proof of our main identification result, Proposition 1 below.

Lemma 1. (Factorization) Under Assumption 2

                 f U,V |W,X,R,S ( u, v| w, x, r, s) = f U |W,R ( u| w, r) f V |X,S ( v| x, s) .

Proof. The first part of Assumption 2 gives the joint density factorization

      fW,X,R,S,U,V (w, x, r, s, u, v) = f X,V,S|W,R ( x, v, s| w, r) f U |W,R ( u| w, r) fW,R (w, r) ,

while the second part gives

       fW,X,R,S,U,V (w, x, r, s, u, v) = f W,U,R|X,S ( w, u, r| x, s) f V |X,S ( v| x, s) fX,S (x, s)

Conditioning on all observables therefore gives the pair of equalities

          f U,V |W,X,R,S ( u, v| w, x, r, s) = f V |W,X,R,S ( v| w, x, r, s) f U |W,R ( u| w, r)
                                               = f V |X,S ( v| x, s) f U |W,X,R,S ( u| w, x, r, s) .

Integrating over u then gives

                             f V |W,X,R,S ( v| w, x, r, s) = f V |X,S ( v| x, s) ,                       (5)

which after substitution gives

                 f U,V |W,X,R,S ( u, v| w, x, r, s) = f U |W,R ( u| w, r) f V |X,S ( v| x, s)

as claimed.

Equation (5), in the proof to Lemma 1, highlights a key implication of Assumption 2: con-
ditional on a classroom’s observed attributes, X and S, the observed attributes of their
assigned teacher, W and R, do not predict unobserved classroom attributes, V . Conversely,
conditional on W and R, classroom characteristics, X and S, do not predict unobserved
teacher attributes, U . Assumption 2 implies that within W = w, R = r by X = x, S = s
cells, there is no matching on unobservables between teachers and classrooms.


                                                       6
Note that
                                 ˆ ˆ
    f U,V |W,X ( u, v| w, x) =          f U |W,R ( u| w, r) f V |X,S ( v| x, s) f R,S|W,X ( r, s| w, x) drds
                             6= f U |W ( u| w) f V |X ( v| x) ,                                                (6)

so that Assumption 2 does allow for matching on unobservables within the coarser W = w
by X = x cells. However within W = w, R = r by X = x, S = s cells matching is ‘as if’
random. We call this conditionally exogenous matching.
Assumption 2 may hold for two reasons. First, it can hold by design. In that case the
researcher chooses a feasible joint distribution for (W, X, R, S), but forms a (W, R) = (w, r)
to (X, S) = (x, s) match by taking a random draw from the subpopulation of teachers
homogenous in (W, R) = (w, r) and matching her with an independent random draw from
the subpopulation of classrooms homogenous in (X, S) = (x, s). This is a doubly randomized
assignment scheme (cf., Graham, 2008, 2011a). Note, as indicated by (6), this scheme does
allow for sorting on unobservables within W = w by X = x cells. As shown below, the
presence of the proxies R and S allows the researcher to “undo” this sorting in order to
recover the AMF.
Second, Assumption 2 is also an equilibrium property of a Choo and Siow (2006a,b) type
aggregate matching market (under certain restrictions on agents’ information sets). We
develop this result in Section 2 below.
Identification of    (w, x) also requires a support condition.

Assumption 3. (Support)
(i) If f S|X ( s| x) f R|W ( r| w) > 0, then f R,S|W,X ( r, s| w, x) > 0,
(ii) ⇡xw = Pr (W = w, X = x) > 0.

Note that, under part (ii) of Assumption 3, the reverse of the implication stated in part (i)
holds as well. Assumption 3 therefore requires that the support of f R,S|W,X ( r, s| w, x) equals
the product of the supports of f R|W ( r| w) and f S|X ( s| x). Observe that the set

                        Sfeasible
                         RS       (w, x) = r, s : f R|W ( r| w) f S|X ( s| x) > 0

equals the feasible joint support of R and S across the set of W = w to X = x matches. This
set contains all logically possible combinations of R = r and S = r that might be observed
in a W = w to X = x match. Identification requires that the actual support

                          Sactual
                           RS     (w, x) = r, s : f R,S|W,X ( r, s| w, x) > 0

                                                      7
and the feasible one overlap.
It is useful to connect this assumption to the familiar overlap condition found in the program
evaluation literature. Doing so also allows us to introduce some notation that will be used
in the eﬃciency bound calculation presented in Section 3. Under Assumption 3 Bayes’ Law
gives

                  pw (r) fR (r)                   px (s) fS (s)                             pwx (r, s) fR,S (r, s)
f R|W ( r| w) =                 , f S|X ( s| x) =               , f R,S|W,X ( r, s| w, x) =                        ,
                       ⇢w                                x                                           ⇡wx

where we define the conditional probabilities

                               pw (r) = Pr ( W = w| R = r)
                               px (s) = Pr ( X = x| S = s)
                           pwx (r, s) = Pr ( W = w, X = x| R = r, S = s) ,

and also the unconditional probabilities ⇢w = Pr (W = w),                        x   = Pr (X = x) and ⇡wx =
Pr (W = w, X = x).5
Under Assumption 3 we have

        Sfeasible
         RS       (w, x) = {r, s : pw (r) px (s) > 0} , Sactual
                                                         RS     (w, x) = {r, s : pwx (r, s) > 0} .                (7)

The equalities in (7) suggest the following reformulation of Assumption 3:

Assumption 4. (Strong Overlap) pwx (r, s)                          > 0 for all (r, s) 2 Sfeasible
                                                                                          RS       (w, x) .

As suggested by its label, Assumption 4, is related to the overlap assumption made in the
program evaluation literature (e.g., Hahn, 1998; Imbens, 2004). It ensures that all logically
possible combinations of R and S that could be observed in a W = w to X = x match are
in fact observed in the set of status quo w-to-x matches. It would useful to develop tests,
heuristic or formal, for assessing Assumption 4 in practice.
Consider the mean regression of Y given W, X, R, S

                                                                       def
                          E [ Y | W = w, X = x, R = r, S = s] ⌘ q (w, x, r, s) .                                  (8)

Note that q (w, x, r, s) is a structural object under (1) and Assumptions 1, 2 and 3. Specifi-
cally, the diﬀerence
                                   q (w, x0 , r, s0 ) q (w, x, r, s) ,
   5
     In certain instances we will also use the notation pj (r) = Pr ( W = wj | R = r), pk (s) =
Pr ( X = xk | S = s), and pjk (r, s) = Pr ( W = wj , X = xk | R = r, S = s) for j = 1, . . . , J and k = 1, . . . , K.


                                                          8
gives the expected change in output when a teacher with characteristics (W, R) = (w, r) is
assigned to a classroom with characteristics (X, S) = (x, s) instead of one with characteristics
(X, S) = (x0 , s0 ) .
Our main identification result is

Proposition 1. (Identification) Under (1) and Assumptions 1, 2 and 3

                                               1
                                                       ˆ ˆ
               (w, x) =                                         q (w, x, r, s) pw (r) px (s) fR (r) fS (s) drds                     (9)
                                       ⇢
                                       ˆ wˆ x
                               =                       q (w, x, r, s) f R|W ( r| w) f S|X ( s| x) drds.                            (10)
                                           s       r


Proof. First, observe that under Assumption 2 we have, invoking Lemma 1,
                                        ˆ ˆ
           q (w, x, r, s) =                                g (w, x, u, v) f U,V |W,X,R,S ( u, v| w, x, r, s) dudv
                                        ˆv ˆu
                               =                           g (w, x, u, v) f U |W,R ( u| w, r) f V |X,S ( v| x, s) dudv.
                                               v       u

                                                                     pw (r)fR (r)                         px (s)fS (s)
Second, from Bayes’ Rule f R|W ( r| w) =                                  ⇢w
                                                                                    and f S|X ( s| x) =         x
                                                                                                                         . This and the
second equality above yields

                       1
                               ˆ ˆ
                                           q (w, x, r, s) pw (r) px (s) fR (r) fS (s) drds
                  ⇢
                  ˆ wˆ x        s      r

              =     q (w, x, r, s) f R|W ( r| w) f S|X ( s| x) drds
                   s       r
                ˆ ˆ ˆ ˆ
              =            g (w, x, u, v) f U |W,R ( u| w, r) f V |X,S ( v| x, s) dudv
                   s       r       v    u
                ⇥f R|W ( r| w) f S|X ( s| x) drds
                ˆ ˆ ˆ ˆ
              =             g (w, x, u, v) f U,R|W ( u, r| w) f V,S|X ( v, s| x) dudrdvds
                 s
                ˆ ˆ r  v  u

              =       g (w, x, u, v) f U |W ( u| w) f V |X ( v| x) dudv
                   v       u
              =     (w, x) .

Note that for (9) and (10) to be well-defined we require Assumption 3. Since all the compo-
nents to the right of the equalities in (9) and (10) are asymptotically revealed under random
sampling (Assumption 1), the result follows.

We discuss some implications of Proposition 1 for estimation after presenting its semipara-
metric eﬃciency bound in Section 3 below.


                                                                          9
2     Pairwise stability and exogeneity
In this section we relate our exogenous matching condition (Assumption 2) to the notion of
pairwise stability in transferable utility (TU) one-to-one matching problems (e.g., Shapley
and Shubik, 1971; Becker, 1973). Our point of departure is the aggregate matching setup
introduced by Choo and Siow (2006a,b). In this framework the econometrician observes
the match frequencies ⇡jk = Pr Wi = wj , X m(i) = xk for j = 1, . . . , J and k = 1, . . . , K
and, from this joint distribution of match types and equilibrium restrictions, seeks to recover
(features of) the distribution of unobserved agent preferences. Match surplus and transfers
are unobserved.
We add to this set-up the observable match output Yi . Match output will generally covary
with the match surplus agents’ actually care about, but it need not be coincident with it.
For example the surplus associated with a specific marriage may vary with (expected) child
outcomes, but would generally not be coincident with them. Our question is: under what
restrictions on agents preferences and information sets will the observed matching be both
(i) pairwise stable and (ii) satisfy Assumption 2? Our conclusion is that Assumption 2 can
hold in settings where agents purposively choose match partners. More constructively, our
analysis provides guidance as to what types of measures to include in the proxy variable
vectors Ri and S h .
For what follows, maintaining the assumption of one-to-one matching, it is pedagogically
convenient to think of the first population as consisting of firms (teachers) and the second
of workers (classrooms). As before, we let i index firms with observable type Wi 2 W =
{w1 , . . . , wJ } and h workers with observable type X h 2 X = {x1 , . . . , xK }. The match output
associated with the pairing of firm i and worker h is now restricted to equal

                     g Wi , X h , Ui , V h =   Wi , X h + Ui X h + V h (Wi )                     (11)
                                                           PK
where (w, x) is an unrestricted function, Ui (x) =           k=1 1 (x = xk ) Uki and V (w) =
                                                                                          h
PJ
   j=1 1 (w = wj ) Vj . We normalize Ui (x) to be conditionally mean zero (i.e., E [ Ui (x)| Wi ] =
                     h

0 for all x 2 X) and impose the analogous restriction on V h (w). These normalizations imply
that the average match function (AMF) equals (w, x). Note that average output across
observed W = w to X = x matches may not equal the AMF. Matching bias is possible.
Equation (11) is restrictive, ruling out interactive eﬀects in the unobservable productivity
                                                               0
vectors Ui = (U1i , . . . , UKi )0 and V h = V1h , . . . , VJh . This type of separability restriction
plays an essential role in the empirical structural matching literature (cf., Assumption 2 of
Galichon and Salanié (2015)). To better understand the content of (11) consider two firms,


                                                 10
i and i0 , of the same type, say w, and two workers, h and h0 , also of the same type, say x.
Under (11) the aggregate output associated with the i-to-h and i0 -to-h0 matching equals

                                                                            0
                            2 (w, x) + Ui (x) + V h (w) + Ui0 (x) + V h (w) ,

which exactly coincides with that of the alternative i-to-h0 and i0 -to-h matching. Any re-
arrangement of matches within a W = w and X = x cell leaves aggregate output unchanged
(although individual match output may, of course, change).
We now turn to firm and worker preferences. In the Choo and Siow (2006a,b) (henceforth
CS) setup, the surplus firm i gets from matching with worker h equals

                    ⇧i X h =         Wi , X h     ⌧ Wi , X h + Ui X h + "˜i X h ,

where ⌧ (wj , xk ) equals the equilibrium transfer a type Wi = wj firm “pays” a type X h = xk
                                                    P
worker (transfers may be negative), and "˜i (x) = K                    ˜ki is an additional source
                                                       k=1 1 (x = xk ) "
of unobserved firm-specific heterogeneity. We introduce this term to allow for a divergence
between the net match output of interest to the econometrician, and the net match surplus
agents’ actually care about. When these two objects coincide "˜i (x) will equal zero for all
x 2 X.6
The surplus worker h gets from matching with firm i equals

                             ⌅h (Wi ) = ⌧ Wi , X h + V h (Wi ) + ˜h (Wi ) ,
                 PJ
with ˜h (w) =       j=1   1 (w = wj ) ˜jh introduced for the same reason as "˜i (x).
We impose the following informational structure: prior to matching firms and workers observe
their own and candidate partners’ types, know the form of (w, x), and also observe transfers.
                                                                                                0
While agents also observe the vectors "˜i = (˜ "1i , . . . , "˜Ki )0 and ˜h = ˜1h , . . . , ˜Jh , they do not
                                                                0
observe Ui = (U1i , . . . , UKi )0 and V h = V1h , . . . , VJh . This means that the ex post utility
associated with any given match is imperfectly known to agents ex ante. While Ui and V h
are unobserved, agents have at their disposal the signals Ri and S h . We assume that these
signals satisfy:

Assumption 5. ( Signals) (i) Prior to matching firms and workers observe Wi , X h , Ri , S h , "˜i , ˜h ,
   6
    The development in this section employs a variant of the notation used in Graham (2013), we gloss over
several interesting subtleties of the CS framework, referring the reader to, for example, Galichon and Salanié
(2015) for a rigorous and comprehensive exposition.




                                                     11
(ii)

                                 V h ? (Wi , Ri , "˜i )| X h , S h , ˜h                            (12)
                                  Ui ?       X h , S h , ˜h   Wi , Ri , "˜i ,                      (13)

and (iii)

                                         Ui ? "˜i | Wi , Ri                                        (14)
                                        V h ? ˜h X h , S h .                                       (15)

Part (i) of Assumption 5 defines what is observed by agents i and h when they match (but
prior to realizing match output). Parts (ii) and (iii) restrict the relationship between what
is known and unknown by agents at the time of matching. Consider conditions (12) and
(14), which restrict the predictability of worker and firm productivity, respectively V h =
 V1h , . . . , VJh and Ui = (Ui1 , . . . , UiK )0 . We omit a discussion of conditions (13) and (15), as
they are analogous to (12) and (14).
Condition (12) implies that firm i’s own attributes – Wi , Ri , "˜i – have no predictive power
for worker h0 s unobserved productivity, V h , conditional on her attributes – X h , S h , ˜h . In
words, conditional on what the two agents know about the worker, what is additionally
known about the firm cannot be used to predict worker productivity. This appears to be
a natural assumption in our context. Note that (12) alone does not impose restrictions
on the joint distribution of (W, X, R, S, "˜, ˜) . This implies, for example, that agents could
assortatively match on "˜k and ˜j within a W = wj , R = r by X = xk , S = s match cell if
they so wanted. Recall, further, that "˜k and ˜j are unobserved by the econometrician.
Condition (14) is restrictive. It implies that W and R contain all variables which simul-
taneously predict U and "˜ or, equivalently, all variables which predict match surplus, and
hence choice, and also aﬀect match output. This condition is analogous to the ‘selection
on observables’ assumption familiar from the program evaluation literature. In that context
treatment exogeneity requires that the set of pre-treatment conditioning variables used by
the econometrician include all joint predictors of the treatment and outcome. The appropri-
ateness of condition (14) is context specific. It will be violated, for example, if there exists a
component of "˜, which is part of the firm’s information set, that covaries with productivity,
U , conditional of those parts of the information sets that are observed by the econometri-
cian (i.e., W and R). If R plausibly approximates those components of an firm’s pre-match
information set that are also likely to predict productivity, U , then invoking Assumption 5
is reasonable.


                                                   12
Under Assumption 5 we prove the following Lemma.

Lemma 2. (Factorization with Signals) Under Assumption 5

               f U,V |W,X,R,S,˜",˜ ( u, v| w, x, r, s, "˜, ˜) = f U |W,R ( u| w, r) f V |W,R ( v| x, s) .    (16)

Proof. See Appendix A. The argument is similar to that used to show Lemma 1.

Agents directly act on their knowledge of Wi , X h , Ri , S h , "˜i , ˜h when matching, inducing
a specific equilibrium match density fW,X,R,S,˜",˜ (w, x, r, s, "˜, ˜) in the process. Observe that
Assumption 5 alone does not restrict this match density (beyond the requirements of fea-
sibility). As noted above, sorting on "˜ and ˜, for example, is allowed. However Lemma 2
shows that an implication of Assumption 5 is that any such sorting does not induce sorting
on U and V conditional on W, X, R, S, "˜ and ˜.
Using Lemma 2 we compute firm i0 s expected utility from matching with worker h as
           ⇥                                              ⇤
          E ⇧i X h          Wi , X h , Ri , S h , "˜i , ˜h =          Wi , X h   ⌧ Wi , X h + "¯i X h ,      (17)

where "¯i X h is the two agents’ forecast of Ui X h + "˜i X h

                                         K
                                         X
                             "¯i (x) =         1 (x = xk ) E [ Uki | Wi , Ri ] + "˜i (x) .                   (18)
                                         k=1


Note that the utility firm i expects to receive when matching with worker h depends on
worker h0 s type alone. Although the firm also observes the worker attributes S h and ˜h , its
expected utility is invariant to them. This result is an implication of the separable form of
the CS utility function, something we inherit from the structural matching literature, as well
as our assumption about agent information sets.
Similarly we compute worker h’s expected utility from matching with firm i as

                               ⌅h (Wi ) = ⌧ Wi , X h + V h (Wi ) + ˜h (Wi ) ,                                (19)

with the corresponding forecast of ˜h (w) + V h (w) for worker h equal to

                                                              J
                                                              X
           ⇥                                          ⇤                          ⇥             ⇤
         E ¯ (w) Wi , X , Ri , S , "˜i , ˜
                h               h        h        h
                                                          =         1 (w = wj ) E Vjh X h , S h + ˜h (w) .   (20)
                                                              j=1


Worker h’s expected utility from matching with firm i depends on firm i’s type alone. Al-
though the worker also observes the firm attributes Ri and "˜i , her expected utility is invariant

                                                               13
to them.
Under (17) and (19) firm and worker partner choice respectively satisfies
                                    ✓                                                                     ◆
                       D
                      ⇡jk   = Pr k = arg            max           [ (wj , x)    ⌧ (wj , x) + "¯ (x)]          (21)
                                                x2{x1 ,...,xK }


and                                        ✓                                                     ◆
                             S
                            ⇡jk ,=      Pr j = arg          max          [⌧ (w, xk ) + ¯ (w)]                 (22)
                                                       w2{w1 ,...,wJ }

which coincide with the choice rules of the generalized CS model. The transfers, ⌧ (w, x),
adjust so as to ensure that ⇡jkD     S
                                 = ⇡jk  for all j, k in equilibrium. This ensures that the
‘demand’ for type X = x workers by type Wi = w firms coincides with the ‘supply’ of type
                    h

X h = x workers to type Wi = w firms (e.g., Graham, 2013). Galichon and Salanié (2015,
Theorems 1 and 2) show such an that equilibrium exists and is unique as long as "¯i (x) for
all x 2 X and ¯h (w) for all w 2 W have suﬃciently large support (which we assume here).
The CS equilibrium induces a particular type of sorting. Consider the subpopulation of type
Wi = w firms. Among these firms the subset that matches with type X h = x workers will
diﬀer from the subset that matches with type X h = x0 workers. Specifically, from the choice
rule (21), the distribution of "¯ (x) and "¯ (x0 ) will diﬀer between the two groups. Because Ri
covaries with "¯i – see (18) above – the distribution of Ri may diﬀer across the two groups as
well. Finally, because Ri covaries with Ui , the distribution of Ui may diﬀer across the two
groups. Consequently average output across W = w to X = x matches will not generally
coincide with the AMF. This is because the distribution of firm productivity in this cell
may diﬀer from that across the entire subpopulation of W = w firms in a CS equilibrium.
A similar reasoning can be used to describe how, among workers of the same type, the
distribution of worker ability will vary with the chosen type of the matched firm.
Under Assumption 5 the availability of R and S is suﬃcient to ‘undo’ any biases caused by
CS matching. Together (11) and Lemma 2 imply7 that the proxy variable regression function
(8) equal

             q (wj , xk , r, s) =       (wj , xk ) + E [ Uk | W = w, R = r] + E [ Vj | X = x, S = s]          (23)

for all combinations of j = 1, . . . , J and k = 1, . . . , K. Plugging (23) into the right-hand-side
  7
      Multiplying both sides of (16) by f ( "˜, ˜| w, x, r, s) and integrating over "˜ and ˜ gives

                         f U,V |W,X,R,S ( u, v| w, x, r, s) = f U |W,R ( u| w, r) f V |W,R ( v| x, s) .




                                                              14
of (9) or (10) and evaluating then gives
                     ˆ ˆ
                               q (wj , xk , r, s) f ( r| wj ) f ( s| xk ) drds =   (wj , xk ) .
                       s   r


Lemma 2 and (23) thus give:
Proposition 2. ( CS Equilibrium & Exogeneity) When match surplus and output takes
the form described above, and agents’ pre-match information sets satisfy Assumption 5,
agents will (i) match according to (21) and (22), (ii) transfers adjust to clear the market,
and (iii) the equilibrium matching will satisfy the conditionally exogenous matching condition
(Assumption 2).

For an empirical researcher contemplating invoking Assumption 2 in a setting where agents
choose match partners in a decentralized way (with transferable utility), assessing the plausi-
bility of Assumption 5 is key. This condition is analogous to conditions for input exogeneity
in single-agent models (e.g., Chamberlain, 1984; Olley and Pakes, 1996).8 Consider the
teacher-to-classroom matching problem introduced in the introduction. In that example Ri
should include attributes that correlate with teacher productivity, Ui . Likewise S h should
include student characteristics that are associated with high levels of achievement, V h . It
may be that there are additional (unobserved) teacher and student attributes that influ-
ence the matching process, for example some teachers may especially prefer to work close to
where they live. In that case condition (14) would require that conditional on Ri , a teacher’s
commuting tastes do not help to predict her unobserved productivity.
To be clear our conclusion is not that Assumption 2 is suitable for routine use in all observa-
tional settings, rather it is that it (i) can be appropriate in certain well-defined settings and
(ii) it is possible to reason about such settings in a ways familiar from the single agent obser-
vational context (e.g, Heckman, Smith and Clements, 1997; Imbens, 2004). As in the single
agent context, articulating the relationship between the agents’ and the econometrician’s
information sets is central.
Research designs based on conditional exogeneity assumptions (i.e., ‘selection on observ-
ables’, ‘unconfoundedness’, etc.) have proved to be a very durable, albeit controversial, part
of the researcher’s toolkit (e.g., Chamberlain, 1984; Griliches and Mairesse, 1998; Olley and
Pakes, 1996). Our view, shaped by the observation that Assumption 2 is compatible with the
leading empirical model of one-to-one matching under certain informational assumptions, is
that covariate adjustment can play a similar role in multi-agent production problems.
   8
    In Chamberlain’s (1984) example the farmer knows land quality (unobserved by the econometrician)
when choosing her input level, but is unable to forecast weather. Weather influences farm output, but only
after input choices are made.


                                                         15
3       Semiparametric eﬃciency bound
Our final result, Theorem 1, characterizes the semiparametric eﬃciency bound for (w, x)
under (1) and Assumptions 1, 2 and 3. As in Graham (2011b) a multinomial approximation
(not reported) was used to conjecture the form of the bound, with the formal result following
from a pathwise derivative calculation, as in Newey (1990).9
Let Dw (W ) = Dw = 1 if W = w and zero otherwise. Let Ex (X) = Ex = 1 if X = x and
zero otherwise. Let Twx (W, X) = Twx = 1 if W = w and X = x and zero otherwise. Let
 wx =   (w, x) and define the candidate eﬃcient influence function

             0    (Z,   wx , h (Z))   =   0   (Z, (w, x)) +    R   (Z, (w, x)) +        S   (Z, (w, x))   (24)

where

                   h (Z) = (f (R, S) , f ( R| W ) , f ( S| X) , ⇢! ,         x,

                                , pwx (R, S) , q (w, x, R, S) , eS (w, x, R) , eR (w, x, S))0

and

                                 f ( R| W ) f ( S| X)    Twx
              0   (Z,   wx , h (Z))   =                          (Y                     q (w, x, R, S))
                                       f (R, S)       pwx (R, S)
                                 Dw
             R (Z, wx , h (Z)) =      (eS (w, x, R)     (w, x))
                                 ⇢w
                                 Ex
             S (Z, wx , h (Z)) =     (eR (w, x, S)     (w, x))
                                               x


with
                                                     ˆ
                                eS (w, x, r) =           q (w, x, r, s) f ( s| x) ds
                                                     ˆ
                                eR (w, x, s) =           q (w, x, r, s) f ( r| w) dr.

    9
    We have also verified our hderivation of the eﬃcient influence function
                                                                          i using the method of Newey (1994b)
and the moment condition E ⇢w1 x f f(R)f   (S) pw (R)px (S)
                                        (R,S)   pwx (R,S)   T wx Y (w, x)  = 0.




                                                         16
Define the candidate variance bound
                      "⇢                                            #
                                                     2 2
              1          f ( R| W = w) f ( S| X = x)      wx (R, S)
    I0 ( wx )   = E                                                                                     (25)
                                  f (R, S)             pwx (R, S)
                       1 ⇥                      2        ⇤
                    + E (eS (w, x, R)       wx ) W = w
                      ⇢w
                       1 ⇥                      2      ⇤
                    + E (eR (w, x, S)       wx ) X = x
                           x
                            ⇡wx
                      +2        E [ (eS (w, x, R)        wx ) (eR   (w, x, S)   wx )| W   = w, X = x]
                           ⇢w x

with

                       2
                       wx   (r, s) = V ( Y | W = w, X = x, R = r, S = s) .

Theorem 1. The semiparametric eﬃciency bound for wx in the problem defined by (1)
and Assumptions 1, 2 and 3 is equal to I0 ( wx ) with an eﬃcient influence function of
 0 (Z, wx , h (Z)) .


Proof. See Appendix A.

Both the eﬃcient influence function and the variance bound have straightforward inter-
pretations. Consider first the influence function. Its first term, 0 (Z, wx , h (Z)), reflects
the asymptotic penalty associated with not knowing conditional distribution of Y given
(W, X, R, S). The second and third terms, R (Z, wx , h (Z)) and S (Z, wx , h (Z)), reflect
the contributions of uncertainty about, respectively, the conditional distributions of R given
W and S given X. The interpretation of I0 ( wx ) 1 is analogous, with its last term arising
from covariance between R (Z, wx , h (Z)) and S (Z, wx , h (Z)).



4        Further research directions
In this paper we have characterized a method of covariate adjustment appropriate for two-
agent models.10 When matching is conditionally exogenous our approach to covariate adjust-
ment recovers a well-defined causal object: the average match function (AMF). Although,
as in other areas of applied social science research, the econometrician may be interested
in ‘controlling for’ observed covariate diﬀerences even if Assumption 2 does not hold (ex-
actly) (cf., Keiding and Clayton, 2014). Our eﬃciency bound calculation characterizes the
    10
    The extension to settings with more than two agents appears to be straightforward. Graham, Imbens
and Ridder (2010) provide one motivating example for such an extension.


                                                    17
maximum asymptotic precision possible when undertaking such covariate adjustment. The
bound is valid for the estimand defined by the right-hand-side of (10) irrespective of whether
it also coincides with the AMF.
Recovering structural objects via covariate adjustment can be controversial in some settings
(cf., Freedman, 1997). Proposition 2 relates our conditionally exogenous matching assump-
tion to the structural TU matching model of Choo and Siow (2006a,b). This model has been
an object of intense study, development and application in recent years (see Chiappori and
Salanié (forthcoming) for a survey). Proposition 2 shows that a status quo matching can
both satisfy our key identifying assumption (Assumption 2) and be consistent with a TU
matching equilibrium. This result requires maintaining certain assumptions about agents’
information sets (Assumption 5) and provides guidance regarding which types of measures
should be included in the teacher and classroom proxy variables, respectively R and S.
We have not presented an estimator for the AMF, instead we leave this exercise to future
research. However, the structure of the eﬃcient influence function in Theorem 1 suggest
several possibilities. Perhaps the most obvious is the following “double average” estimator

                                 2
                                     PN PN
                             N        i=1j=1 1 (Wi = w) 1 (Xj = x) q̂ (w, x, Ri , Sj )
              ˆDA (w, x) =               P     PN                                      ,            (26)
                                      N 2 Ni=1   j=1 1 (W i = w) 1 (X j = x)
where q̂ (w, x, Ri , Sj ) is a preliminary nonparametric estimate. This estimator is similar to
the partial mean estimator introduced by Newey (1994a), but instead requires “integration”
with respect to a product of two marginal distributions (as opposes to integrating with
respect to a single joint distribution). This feature is reflected in the V-Statistic structure
of (26). Statistically, (26) corresponds to the random matching estimator introduced in
Graham, Imbens and Ridder (2014) for the special case where Wi = w for all i and Xj = x
for all j (i.e., when there is only one type of teacher and only one type of classroom).11
In that case the eﬃcient influence function given in Theorem 1 also corresponds to the
influence function derived (by brute force) in Graham, Imbens and Ridder (2014). This
suggests that an eﬃcient estimator for the AMF could be constructed by adapting the
regularity conditions and specific estimation procedures presented there (likewise it implies
semiparametric eﬃciency of the random matching estimator). Empirical researchers might
consider using a flexible parametric estimate of q̂ (w, x, r, s) in practice.
The form of the eﬃcient influence function also suggests an inverse probability weighting
  11
    Graham, Imbens and Ridder (2014) estimate q̂ (w, x, r, s) by a particular kernel regression estimator
designed to deal with boundary bias.




                                                   18
type (IPW) estimator. In particular, under (1) and Assumptions 1, 2 and 4, we have
                                      
                                            1 f (R) f (S) pw (R) px (S)
                         (w, x) = E                                     Twx Y .
                                          ⇢w x f (R, S) pwx (R, S)

This suggests an estimator, akin the one studied by Hirano, Imbens and Ridder (2003) for
the single agent case, of

                                          XN ˆ
                  ˆIPW (w, x) = 1 1          f (Ri ) fˆ (Si ) p̂w (Ri ) p̂x (Si )
                                                                                  Twx,i Yi .          (27)
                                N ⇢ˆw ˆ x i=1 fˆ (Ri , Si ) p̂wx (Ri , Si )

It would also be of interest to construct locally eﬃcient, doubly robust, estimators, as has
been done in the program evaluation context (cf., Graham, Pinto and Egel (2012, 2016) and
the references cited therein).
The AMF provides information on how match output varies across diﬀerent types of agent
pairings. We close our paper by briefly outlining how to integrate the AMF into an explicit
social planning problem. We assume the social planner knows (w, x) for all (w, x) 2
W ⇥ X (perhaps up to sampling uncertainty). She also knows the marginal distributions
of teacher and classroom types, respectively ⇢ = (⇢1 , . . . , ⇢J )0 for ⇢j = Pr (Wi = wj ) and
  = ( 1 , . . . , K )0 for k = Pr X h = xk (again perhaps up to sampling uncertainty). She
                                           0
does not observe (Ri0 , Ui0 )0 or S h , V h or is unable/unwilling to act on this knowledge if
she does. Put diﬀerently, the planner is constrained to consider only doubly randomized
reallocations (Graham, 2008; 2011a).
Recall that ⇡jk = Pr (W = wj , X = xk ) for j = 1, . . . J and k = 1, . . . , K. The planner’s
problem is to choose a ⇡ = (⇡11 , . . . , ⇡1K , . . . , ⇡J1 , . . . , ⇡JK )0 that maximizes expected output

                                               J X
                                               X K
                                     ✓ (⇡) =               (wj , xk ) ⇡jk                             (28)
                                               j=1 k=1


subject to the J + K feasibility constraints:

                                    K
                                    X
                                          ⇡jk = ⇢j , j = 1, . . . , J                                 (29)
                                    k=1
                                    XJ
                                          ⇡jk =      k,   k = 1, . . . , K.
                                    j=1


See Graham, Imbens and Ridder (2007).



                                                     19
                                                 Table 1: The structure of feasible assignments
         Teachers/Classrooms                    x1 · · ·                xK 1                                                       xK     fW (w)
                                                                                                                          PK   1
         w1                                  ⇡11 · · ·                        ⇡1K   1                                ⇢1     k=1    ⇡1k           ⇢1
         ..                                    .. . .                               ..                                               ..           ..
          .                                     .     .                              .                                                .            .
                                                                                                                       PK 1
         wJ 1                             ⇡       ···                    ⇡J                                      ⇢J 1     k=1 ⇡J 1k         ⇢J
20




                                        PJ 1J 11                    PJ  1
                                                                               1K 1
                                                                                             PJ 1        PK  1       PJ 1 P K 1
                                                                                                                                                  1
         wJ                         1    j=1 ⇡j1  ···       K 1       j=1     ⇡jK   1    1    j=1   ⇢j    k=1    k +  j=1   k=1 ⇡jk              ⇢J
         fX (x)                                  1                             K 1                                                  K
     Notes: The j = 1, . . . , J types of teachers are enumerated in the first column, with the marginal frequency of each type given in the last
     column. The k = 1, . . . , K types of classrooms are enumerated in the first row, with the marginal frequency of each type given in the last
     row. The joint distribution of teachers and classrooms is characterized by the interior probabilities. The feasibility constraints are used
     to reduce the parameterization of an assignment to (J 1) (K 1) probabilities.
       P P
Since Jj=1 K   k=1 ⇡jk = 1, one constraint is redundant. Table 1 depicts the structure of a
feasible assignment. By substituting out the feasibility constraints, an assignment can be
represented in terms of (J 1) (K 1) probabilities.
Graham (2011a) shows that the diﬀerence between two doubly randomized allocations, ⇡ 0
and ⇡ is given by

                         J 1K
                         X  X1
         0                              0
  ✓ (⇡ )       ✓ (⇡) =                 ⇡jk   ⇡ jk ( (wJ , xK )       (wJ , xk )     [ (wj , xK )     (wj , xk )]) .
                             j=1 k=1
                                                                                          (30)
Equation (30) indicates that the average outcome properties of an allocation depend critically
on the complementarity properties of the average match function (AMF). Of particular
interest is the diﬀerence between a candidate assignment ⇡ and the completely random
matching ⇡jkrdm
                = ⇢j k for all j = 1, . . . , J and k = 1, . . . , K:

                             J 1K
                             X  X1
    0              rdm                  0
✓ (⇡ )       ✓ ⇡         =             ⇡jk   ⇢j   k   ( (wJ , xK )     (wJ , xk )     [ (wj , xK )     (wj , xk )]) .
                             j=1 k=1
                                                                                            (31)
Equation (31) suggests that outcome-maximizing assignments will tend to be assortative
(⇡jk
  0
     > ⇢j k ) in regions of complementarity ( (wJ , xK )    (wJ , xk ) [ (wj , xK )  (wj , xk )] >
0) and anti-assortative (⇡jk < ⇢j k ) in regions of substitutability ( (wJ , xK )
                              0
                                                                                    (wJ , xk )
[ (wj , xK )     (wj , xk )] < 0).
The semiparametric eﬃciency bound for ✓ (⇡), for a given fixed assignment, ⇡, should follow
relatively easily from Theorem 1. Likewise an eﬃcient estimate, ✓ˆ (⇡), should be straight-
forward to construct, given the availability on an eﬃcient estimate of the AMF at all points
in (w, x) 2 W ⇥ X. There remain interesting decision theoretic questions regarding how to
implement an optimal assignment on the basis of sample information alone.



A            Proofs
Proof of Lemma 2

To economize on the notation we drop subscripts from densities in what follows. Recall the
                                                                 0
                "1i , . . . , "˜Ki )0 and ˜h = ˜1h , . . . , ˜Jh . We begin by factoring the joint density
notation "˜i = (˜




                                                           21
of all firm and worker attributes as

        f (u, v, w, x, r, s, "˜, ˜) = f ( v, x, s, ˜| u, w, r, "˜) f ( u, "˜| w, r) f (w, r)
                                     = f ( v, x, s, ˜| u, w, r, "˜) f ( "˜| w, r) f ( u| w, r) f (w, r) ,   (32)

where the second equality follows from (14) in the main text. An analogous calculation gives
the parallel factorization

          f (u, v, w, x, r, s, "˜, ˜) = f ( u, w, r, "˜| v, x, s, ˜) f ( ˜| x, s) f ( v| x, s) f (x, s) .   (33)

Dividing (32) by f (w, x, r, s, "˜, ˜) yields

                                       f ( v, x, s, ˜| u, w, r, "˜) f ( "˜| w, r) f ( u| w, r) f (w, r)
        f ( u, v| w, x, r, s, "˜, ˜) =
                                                      f ( x, s, ˜| w, r, "˜) f (w, r, "˜)
                                       f ( v, x, s, ˜| u, w, r, "˜) f ( u| w, r)
                                     =
                                               f ( x, s, ˜| u, w, r, "˜)
                                     = f ( v| w, x, r, s, u, "˜, ˜) f ( u| w, r) ,                          (34)

where the second equality follows from (13) of the main text.
Dividing (33) by f (w, x, r, s, "˜, ˜) and invoking (12) yields the parallel result

                     f ( u, v| w, x, r, s, "˜, ˜) = f ( u| w, x, r, s, v, "˜, ˜) f ( v| x, s) .             (35)

Integrating (35) with respect to u gives

                                   f ( v| w, x, r, s, u, "˜, ˜) = f ( v| x, s) .                            (36)

Substituting (36) into (34) yields the density factorization

                            f ( u, v| w, x, r, s, "˜, ˜) = f ( u| w, r) f ( v| x, s)

as claimed.


Proof of Theorem 1

In calculating the semiparametric eﬃciency bound for the model defined by (1) and Assump-
tions 1 to 4 above we follow the general approach of Bickel, Klaassen, Ritov and Wellner
(1993) and, especially, Newey (1990, Section 3). First, we characterize the nuisance tan-
gent space. Second, we demonstrate pathwise diﬀerentiability of the average match function

                                                        22
 jk = (wj , xk ). The eﬃcient influence function is the projection of the pathwise derivative
onto the nuisance tangent space. In the present case the pathwise derivative is an element of
the tangent space and therefore coincides with the required projection (i.e., jk is a param-
eter of an unrestricted distribution and hence the pathwise derivative is unique; cf. Newey
(1994b)). The main result then follows from Theorem 3.1 of Newey (1990, p. 106).
Step 1: Characterization of tangent space
The joint density function of Z = (W, X, Y, R0 , S 0 )0 , recalling that

                         pjk (r, s) = Pr ( W = wj , X = xk | R = r, S = s) ,

⇢j = Pr (W = wj ) and        k   = Pr (X = xk ), is conveniently factorized as follows:

                      J Y
                      Y K
f (y, w, x, r, s) =               f ( y| wj , xk , r, s)dj ek f ( r, s| wj , xk )dj ek Pr (W = wj , X = xk )dj ek
                      j=1 k=1
                      J Y
                      Y K                                                                                                    dj ek
                                                           dj ek         f (wj , xk , r, s)
                 =                f ( y| wj , xk , r, s)                                     f ( r| wj ) f ( s| xk ) ⇢j   k
                      j=1 k=1
                                                                       f (wj , r) f (xk , s)
                      J Y
                      Y K                                                                                                              dj e k
                                                           dj ek        pjk (r, s) f (r, s)
                 =                f ( y| wj , xk , r, s)                                         f ( r| wj ) f ( s| xk ) ⇢j       k               ,
                      j=1 k=1
                                                                       pj (r) pk (s) f (r) f (s)

where we suppress the functional dependence of dj on w and ek on x.12 Recall also that
pj (r) = Pr ( W = wj | R = r) and pk (s) = Pr ( X = xk | S = s) .
Consider a regular parametric submodel with f (y, w, x, r, s; ⌘) = f (y, w, x, r, s) at ⌘ = ⌘0 .
The submodel joint density is given by

                         J Y
                         Y K
f (y, w, x, r, s; ⌘) =               f ( y| wj , xk , r, s; ⌘)dj ek
                         j=1 k=1
                                                                                                                                     dj ek
                                    pjk (r, s; ⌘)       f (r, s; ⌘)
                         ⇥                                             f ( r| wj ; ⌘) f ( s| xk ; ⌘) ⇢j (⌘)               k (⌘)               .
                                 pj (r; ⌘) pk (s; ⌘) f (r; ⌘) f (s; ⌘)
  12
    That is Dj = Dj (W ) = 1 if W = wj and zero otherwise and Ek = Ek (X) = 1 if X = xk and zero
otherwise.




                                                              23
The submodel log likelihood is

                                  J X
                                  X K
      ln f (y, w, x, r, s; ⌘) =              dj ek ln f ( y| wj , xk , r, s; ⌘)
                                  j=1 k=1
                                      J X
                                      X K                   ⇢
                                                                   pjk (r, s; ⌘)       f (r, s; ⌘)
                                  +              dj ek ln
                                      j=1 k=1
                                                                pj (r; ⌘) pk (s; ⌘) f (r; ⌘) f (s; ⌘)
                                      J X
                                      X K                                          J X
                                                                                   X K
                                  +              dj ek ln f ( r| wj ; ⌘) +                    dj ek ln f ( s| xk ; ⌘)
                                      j=1 k=1                                      j=1 k=1
                                      J X
                                      X K                               J X
                                                                        X K
                                  +              dj ek ln ⇢j (⌘) +                   dj ek ln      k   (⌘)
                                      j=1 k=1                           j=1 k=1
                                  J X
                                  X K
                            =                dj ek ln f ( y| wj , xk , r, s; ⌘)
                                  j=1 k=1
                                      J X
                                      X K                   ⇢
                                                                   pjk (r, s; ⌘)       f (r, s; ⌘)
                                  +              dj ek ln
                                      j=1 k=1
                                                                pj (r; ⌘) pk (s; ⌘) f (r; ⌘) f (s; ⌘)
                                      J
                                      X                                K
                                                                       X
                                  +         dj ln f ( r| wj ; ⌘) +            ek ln f ( s| xk ; ⌘)
                                      j=1                              k=1
                                      J
                                      X                         K
                                                                X
                                  +         dj ln ⇢j (⌘) +            ek ln    k   (⌘) ,
                                      j=1                       k=1


with an associated score vector of
                                             J X
                                             X K
                s⌘ (y, w, x, r, s; ⌘) =                  dj ek s⌘ ( y| wj , xk , r, s; ⌘)
                                             j=1 k=1
                                                 J X
                                                 X K
                                             +               dj ek k⌘ (wj , xk , r, s; ⌘)
                                                 j=1 k=1
                                                 J
                                                 X                             K
                                                                               X
                                             +         dj t⌘ ( r| wj ; ⌘) +           ek t⌘ ( s| xk ; ⌘)
                                                 j=1                           k=1
                                                 J
                                                 X                      K
                                                                        X
                                             +         dj ⇢j,⌘ (⌘) +          ek     k,⌘   (⌘) ,                        (37)
                                                 j=1                    k=1




                                                        24
where

          s⌘ ( y| wj , xk , r, s; ⌘) = r⌘ ln f ( y| wj , xk , r, s; ⌘)
             k⌘ (wj , xk , r, s; ⌘) = r⌘ ln pjk (r, s; ⌘)              r⌘ ln pj (r; ⌘)          r⌘ ln pk (s; ⌘)
                                           +r⌘ ln f (r, s; ⌘)          r⌘ ln f (r; ⌘)           r⌘ ln f (s; ⌘)
                   t⌘ ( r| wj ; ⌘) = r⌘ ln f ( r| wj ; ⌘)
                   t⌘ ( s| xk ; ⌘) = r⌘ ln f ( s| xk ; ⌘)
                                     @ ln ⇢j (⌘)
                         ⇢j,⌘ (⌘) =
                                         @⌘
                                     @ ln k (⌘)
                          k,⌘ (⌘) =              .
                                         @⌘

By the usual conditional mean zero property of the score function,

                              E [ s⌘ ( Y | W, X, R, S)| W, X, R, S] = 0
                                                     E [k⌘ (W, X, R, S)] = 0
                                                       E [ t⌘ ( R| W )| W ] = 0
                                                           E [ t⌘ ( S| X)| X] = 0,                                    (38)

where the suppression of ⌘ in a function means that it is evaluated at is population value
(e.g., t⌘ ( S| X) = t⌘ ( S| X; ⌘0 )).
From (37) and (38) the tangent set is therefore given by
                        (   J X
                              K                                        J X
                                                                         K
                            X                                          X
               T   =                  dj ek s ( y| wj , xk , r, s) +                dj ek k (wj , xk , r, s)
                            j=1 k=1                                    j=1 k=1
                            J                        K                        J                K
                                                                                                              )
                            X                        X                        X                X
                        +         dj t ( r| wj ) +         ek t ( s| xk ) +         d j aj +         e k bk       ,   (39)
                            j=1                      k=1                      j=1              k=1


where aj and bk are finite constants for j = 1, . . . , J and k = 1, . . . , K and s ( y| wj , xk , r, s),
k (wj , xk , r, s), t ( r| wj ) and t ( s| xk ) satisfy

                               E [ s ( Y | W, X, R, S)| W, X, R, S] = 0
                                                     E [k (W, X, R, S)] = 0
                                                       E [ t ( R| W )| W ] = 0
                                                           E [ t ( S| X)| X] = 0.



                                                            25
Step 2: Demonstration of Pathwise Diﬀerentiability
Under the parametric submodel         (⌘) is identified by
                            ˆ ˆ ˆ
              (w, x; ⌘) =            yf ( y| w, x, r, s; ⌘) dy f ( r| w; ⌘) f ( s| x; ⌘) drds.

Diﬀerentiating under the integral and evaluating at ⌘ = ⌘0 gives

    @ (w, x; ⌘0 )
                        ˆ ˆ
                  =         E [ Y s⌘ ( Y | W, X, R, S)| w, x, r, s] f ( r| w; ⌘0 ) f ( s| x; ⌘0 ) drds
       @⌘ 0
                        ˆ ˆ
                      +       q (w, x, r, s) t⌘ ( r| w)0 f ( r| w; ⌘0 ) f ( s| x; ⌘0 ) drds
                        ˆ ˆ
                      +       q (w, x, r, s) t⌘ ( s| x)0 f ( r| w; ⌘0 ) f ( s| x; ⌘0 ) drds
                      ˆ ˆ
                    =       E [ Y s⌘ ( Y | W, X, R, S)| w, x, r, s] f ( r| w; ⌘0 ) f ( s| x; ⌘0 ) drds
                        ˆ
                      + eS (w, x, r) t⌘ ( r| w)0 f ( r| w; ⌘0 ) dr
                        ˆ
                      + eR (w, x, s) t⌘ ( s| x)0 f ( s| x; ⌘0 ) ds
                      ˆ ˆ
                    =       E [ Y s⌘ ( Y | W, X, R, S)| w, x, r, s] f ( r| w; ⌘0 ) f ( s| x; ⌘0 ) drds
                          ⇥                               ⇤
                      +E eS (w, x, R) t⌘ ( R| w)0 w
                          ⇥                             ⇤
                      +E eR (w, x, S) t⌘ ( S| x)0 x ,                                                  (40)

where
                                             ˆ
                            eS (w, x, r) =       q (w, x, r, s) f ( s| x; ⌘0 ) ds
                                             ˆ
                            eR (w, x, s) =       q (w, x, r, s) f ( r| w; ⌘0 ) dr.


To demonstrate pathwise diﬀerentiability of jk = (wj , xk ) we require F (Y, wj , xk , R, S)
such that
               @ (wj , xk ; ⌘0 )     ⇥                                                   0⇤
                                 = E   F (Y, w j , x k , R, S) s ⌘ (Y, w j , x k , R, S)    . (41)
                   @⌘ 0
With some work it is possible to show that condition (41) holds for

                                  f ( R| wj ) f ( S| xk ) Dj Ek
        F (Y, wj , xk , R, S) =                                      (Y q (wj , xk , R, S))
                                        f (R, S)         pjk (R, S)
                                    Dj                               Ek
                                  +     (eS (wj , xk , R)     jk ) +     (eR (wj , xk , S)       jk ) .   (42)
                                    ⇢j                                 k


                                                   26
We evaluate the covariance of each of the three terms in (42) with s⌘ (Y, wj , xk , R, S) in turn.
We begin with
    
        f ( R| wj ) f ( S| xk ) Dj Ek
E
              f (R, S)         pjk (R, S)
              ⇥ (Y q (w1 , x1 , R, S))
                                                  
                                                  f ( R| wj ) f ( S| xk ) Dj Ek
           ⇥Dj Ek s⌘ (Y, wj , xk , R, S)] = E
                                                        f (R, S)         pjk (R, S)
                                              ⇥ (Y q (wj , xk , R, S)) Dj Ek s⌘ ( Y | wj , xk , R)]
                                                  
                                                    f ( R| wj ) f ( S| xk ) Dj Ek
                                              +E
                                                          f (R, S)          pjk (R, S)
                                                                                    @ log f (wj , xk , R, S; ⌘0 )
                                              ⇥ (Y q (wj , xk , R, S)) Dj Ek
                                                                                                 @⌘ 0
                                                
                                                  f ( R| wj ) f ( S| xk ) Dj Ek
                                            = E                                        Y s⌘ ( Y | wj , xk , R, S)
                                                        f (R, S)         pjk (R, S)
                                                
                                                  f ( R| wj ) f ( S| xk ) Dj Ek
                                            = E
                                                        f (R, S)         pjk (R, S)
                                              ⇥E [ Y s⌘ ( Y | wj , xk , R, S; ⌘0 )| wj , xk , R, S]]
                                              ˆ ˆ X   J X K
                                                              f ( r| wj ) f ( s| xk ) Dj (wl ) Ek (xm )
                                            =
                                                     l=1 m=1
                                                                     f (r, s)             pjk (r, s)
                                              ⇥E [ Y s⌘ ( Y | wj , xk , R, S)| wj , xk , r, s] plm (r, s)] f (r, s) drds
                                              ˆ ˆ
                                            =      E [ Y s⌘ ( Y | wj , xk , R, S)| wj , xk , r, s]
                                                ⇥f ( r| wj ) f ( s| xk ) drds,

which coincides with the first component of (40). The second equality above follows by
iterated expectations and the conditional mean zero property of the score function. The
third and fourth equalities follow from applications of iterated expectations.
To evaluate the covariance of the second two terms in (42) with s⌘ (Y, wj , xk , R, S; ⌘0 ) the
following alternative density factorizations will prove useful:

                          f (w, x, r, s; ⌘) = f ( r| w; ⌘) f ( x, s| w, r; ⌘) f (w; ⌘)
                          f (w, x, r, s; ⌘) = f ( s| x; ⌘) f ( w, r| x, s; ⌘) f (x; ⌘) .

These give, in an abuse of notation, the score decompositions

s⌘ (Y, W, X, R, S; ⌘) = s⌘ ( Y | W, X, R, S; ⌘) + t⌘ ( R| W ; ⌘) + s⌘ ( X, S| W, R; ⌘) + s⌘ (W ; ⌘)
s⌘ (Y, W, X, R, S; ⌘) = s⌘ ( Y | W, X, R, S; ⌘) + t⌘ ( S| X; ⌘) + s⌘ ( W, R| X, S; ⌘) + s⌘ (X; ⌘) .


                                                        27
By the conditional mean zero property of the score function
                                 
                                     Dj
                             E          (eS (wj , xk , R)       jk ) s⌘   ( Y | W, X, R, S) = 0.
                                     ⇢j

Using iterated expectations further yields
                                                                                  
      Dj                                                                       Dj
    E    (eS (wj , xk , R)                jk ) s⌘   (W )   = E s⌘ (W ) E           (eS (wj , xk , R)     jk ) W
      ⇢j                                                                       ⇢j
                                                           = s⌘ (wj ) E [ (eS (wj , xk , R)      jk )| W = wj ]

                                                           = s⌘ (wj ) (        jk       jk )

                                                           = 0.

Similarly, using iterated expectations and the conditional mean zero property of the score
function, yields
                                                                          
  Dj                                                                    Dj
E    (eS (wj , xk , R)                jk ) s⌘   ( X, S| W, R)    = E       (eS (wj , xk , R)       jk ) E [ s⌘   ( X, S| W, R)| W, R]
  ⇢j                                                                    ⇢j
                                                                      
                                                                        Dj
                                                                 = E       (eS (wj , xk , R)       jk )   ·0
                                                                        ⇢j
                                                                 = 0.

Finally
                                                                 
  Dj                                                            Dj                                            0
E    (eS (wj , xk , R)                jk ) t⌘   ( R| W )   = E     (eS (wj , xk , R)        jk ) t⌘ ( R| W )
  ⇢j                                                            ⇢j
                                                               
                                                                   Dj                                            0
                                                           = E E        (eS (wj , xk , R)        jk ) t⌘ ( R| W ) W
                                                                    ⇢j
                                                              ⇥                                          0         ⇤
                                                           = E (eS (wj , xk , R)      jk ) t⌘ ( R| W ) W = wj
                                                              ⇥                                             ⇤
                                                           = E eS (wj , xk , R) t⌘ ( R| W )0 W = wj ,

again using the conditional mean zero property of the score function. Putting these results
together gives
    
        Dj                                                               ⇥                                    ⇤
E          (eS (wj , xk , R)           jk ) s⌘   (Y, W, X, R, S; ⌘0 ) = E eS (wj , xk , R) t⌘ ( R| W )0 W = wj .
        ⇢j

Analogous calculations yield the expression
    
        Ek                                                              ⇥                                        ⇤
E            (eR (wj , xk , S)        jk ) s⌘   (Y, W, X, R, S; ⌘0 ) = E eR (wj , xk , S; ⌘0 ) t⌘ ( S| X)0 X = xk .
         k



                                                                28
These expressions coincide with the second and third components of (40). Condition (41)
then holds for F (Y, wj , xk , R, S) as defined in (42).
Step 3: Calculation of projection
The semiparametric variance bound for jk is the expected square of the projection of
F (Y, wj , xk , R, S) onto T . Since F (Y, wj , xk , R, S) 2 T it coincides with the required pro-
jection and is therefore the eﬃcient influence function as claimed. Here

                         f ( R| wj ) f ( S| xk ) Dj Ek
                                                           (Y         q (wj , xk , R, S))
                               f (R, S)         pjk (R, S)
                    PJ     PK
plays the role of    j=1     k=1   dj ek s ( y| wj , xk , r, s) and

                                         Dj
                                            (eS (wj , xk , R)         jk )
                                         ⇢j

and
                                         Ek
                                               (eR (wj , xk , S)      jk )
                                           k

the roles of, respectively, dj t ( r| wj ) and ek t ( s| xk ) . Zeros plays the role of the remaining
terms.



References
 [1] Becker, Gary S. (1973). “A theory of marriage: part I,” Journal of Political Economy
     81 (4): 813 – 846.

 [2] Bickel, Peter J., Chris A.J. Klaassen, Ya’acov Ritov and Jon A. Wellner. (1993). Eﬃcient
     and Adaptive Estimation for Semiparametric Models. New York: Springer-Verlag.

 [3] Boyd, Donald, Hamilton Lankford, Susanna Loeb and James Wyckoﬀ. (2013). “Analyz-
     ing the determinants of the matching of public school teachers to jobs: disentangling
     the preferences of teachers and employers,” Journal of Labor Economics 31 (1): 83 -
     117.

 [4] Chamberlain, Gary. (1984). “Panel data,” Handbook of Econometrics 2: 1247 - 1318 (Z.
     Griliches & M.D. Intriligator, Eds.). Amsterdam: North Holland.

 [5] Chiappori, Pierre-André and Bernard Salanié. (forthcoming). “The economics of match-
     ing models,” Journal of Economic Literature.


                                                        29
 [6] Chiappori, Pierre-André, Bernard Salanié and Yoram Weiss. (2015). “Partner choice
     and the marital college premium,” Mimeo, Columbia University.

 [7] Choo, Eugene and Aloysius Siow. (2006a). “Who marries whom and why?” Journal of
     Political Economy 114 (1): 175 - 201.

 [8] Choo, Eugene and Aloysius Siow. (2006b). “Estimating a marriage matching model with
     spillover eﬀects,” Demography 43 (3): 464 - 490.

 [9] Dagsvik, John. K. (2000). “Aggregation in matching markets,” International Economic
     Review 41 (1): 27 – 58.

[10] Dupuy, Arnaud and Alfred Galichon. (2014). “Personality traits and the marriage mar-
     ket.” Journal of Political Economy 122 (6): 1271–1319.

[11] Freedman, David. (1997). “From association to causation via regression,” Advances in
     Applied Mathematics 18 (1): 59 – 110.

[12] Galichon, Alfred and Bernard Salanié. (2015). “Cupid’s invisible hand: social surplus
     and identification in matching models,” Mimeo, New York University.

[13] Galichon, Alfred and Yuwei Hsieh. (2015). “Love and chance: equilibrium and identifi-
     cation in a large NTU matching market with stochastic choice,” Mimeo, University of
     Souther California.

[14] Griliches, Zvi and Jacques Mairesse. (1998). “Production functions: the search for iden-
     tification,” Econometrics and Economic Theory in the 20th Century: The Ragner Frisch
     Memorial Symposium: 169 - 203 (S. Strom, Ed.). Cambridge: Cambridge University
     Press.

[15] Graham, Bryan S. (2008). "Identifying social interactions through conditional variance
     restrictions," Econometrica 76 (3): 643 - 660.

[16] Graham, Bryan S. (2011a). “Econometric methods for the analysis of assignment prob-
     lems in the presence of complementarity and social spillovers," Handbook of Social Eco-
     nomics 1B: 965 - 1052 (J. Benhabib, A. Bisin, & M. Jackson, Eds.). Amsterdam: North-
     Holland.

[17] Graham, Bryan S. (2011b). “Eﬃciency bounds for missing data models with semipara-
     metric restrictions,” Econometrica 79 (2): 437 - 452.



                                             30
[18] Graham, Bryan S. (2013). “Comparative static and computational methods for an empir-
     ical one-to-one transferable utility matching model,” Advances in Econometrics: Struc-
     tural Econometric Models 31 (1): 151 - 179.

[19] Graham, Bryan S., Guido W. Imbens and Geert Ridder. (2007). “Redistributive eﬀects
     for discretely-valued inputs,” IEPR Working Paper No. 07.7.

[20] Graham, Bryan S., Guido W. Imbens and Geert Ridder. (2010). “Measuring the eﬀects
     of segregation in the presence of social spillovers: a nonparametric approach,” NBER
     Working Paper No. 16499.

[21] Graham, Bryan S., Guido W. Imbens and Geert Ridder. (2014). "Complementarity and
     aggregate implications of assortative matching: a nonparametric analysis," Quantitative
     Economics 5 (1), 29 - 66.

[22] Graham, Bryan S., Cristine Campos de Xavier Pinto, Daniel Egel. (2012). “Inverse
     probability tilting for moment condition models with missing data,” Review of Economic
     Studies 79 (3): 1053 - 1079.

[23] Graham, Bryan S., Cristine Campos de Xavier Pinto, Daniel Egel. (2016). “Eﬃcient esti-
     mation of data combination models by the method of auxiliary-to-study tilting (AST),”
     Journal of Business and Economic Statistics 34 (2): XXX - XXX.

[24] Hahn, Jinyong. (1998). “On the role of the propensity score in eﬃcient semiparametric
     estimation of average treatment eﬀects,” Econometrica 66 (2): 315 - 331.

[25] Heckman, James J., Jeﬀrey Smith and Nancy Clements. (1997). “Making the most
     out of programme evaluations and social experiments: accounting for heterogeneity in
     programme impacts,” Review of Economic Studies 64 (4): 487 – 535.

[26] Hirano, Keisuke, Guido W. Imbens and Geert Ridder. (2003). “Eﬃcient estimation of
     average treatment eﬀects using the estimated propensity score,” Econometrica 71 (4):
     1161 - 1189.

[27] Holland, Paul W. (1986). “Statistics and causal inference,” Journal of the American
     Statistical Association 81 (396): 945 - 960.

[28] Imbens, Guido W. (2004). “Nonparametric estimation of average treatment eﬀects under
     exogeneity: a review,” Review of Economics and Statistics 86 (1): 4 - 29.

[29] Imbens, Guido W. and Donald B. Rubin. (2015). Causal Inference for Statistics, Social,
     and Biomedical Sciences: An Introduction. Cambridge: Cambridge University Press.

                                            31
[30] Imbens, Guido W. and Jeﬀrey Wooldridge. (2009). “Recent developments in the econo-
     metrics of program evaluation,” Journal of Economic Literature 2009 47 (1): 5 - 86.

[31] Keiding, Niels and David Clayton. (2014). “Standardization and control for confounding
     in observational studies: a historical perspective,” Statistical Science 29 (4): 529 - 558.

[32] Manski, Charles F. (2007). Identification for Prediction and Decision. Cambridge, MA:
     Harvard University Press.

[33] Menzel, Konrad. (2015). “Large matching markets as two-sided demand systems,”
     Econometrica 83 (3): 897 - 941.

[34] Newey, Whitney K. (1990). “Semiparametric eﬃciency bounds,” Journal of Applied
     Econometrics 5 (2): 99 - 135.

[35] Newey, Whitney K. (1994a). “Kernel estimation of partial means and a general variance
     estimator,” Econometric Theory 10 (2): 233 - 253.

[36] Newey, Whitey K. (1994b). “The asymptotic variance of semiparametric estimators,”
     Econometrica 62 (6): 1349 - 1382.

[37] Olley, G. Steven and Ariel Pakes. (1996). “The dynamics of productivity in the telecom-
     munications equipment industry,” Econometrica 64 (6): 1263 - 1297.

[38] Shapley, Lloyd S. and Martin Shubik. (1971) "The assignment game I: The core," In-
     ternational Journal of Game Theory 1 (1): 111 - 130.

[39] Wooldridge, Jeﬀrey. (2005). “Unobserved heterogeneity and the estimation of average
     partial eﬀects,” Identification and Inference for Econometric Models: Essays in Honor
     of Thomas Rothenberg: 27 - 55 (D.W.K. Andrews & J.H. Stock, Eds.). Cambridge:
     Cambridge University Press.

[40] Yule, G. Udny. (1897). “An Investigation into the causes of changes in pauperism in
     England, chiefly during the last two intercensal decades (Part I.),” Journal of the Royal
     Statistical Society 62 (2): 249 - 295.




                                              32
