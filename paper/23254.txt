                             NBER WORKING PAPER SERIES




                               MYOPIA AND DISCOUNTING

                                        Xavier Gabaix
                                        David Laibson

                                     Working Paper 23254
                             http://www.nber.org/papers/w23254


                    NATIONAL BUREAU OF ECONOMIC RESEARCH
                             1050 Massachusetts Avenue
                               Cambridge, MA 02138
                                   March 2017




For very good research assistance we are grateful to Omeed Maghzian, and for helpful comments
to Ned Augenblick, Eric Budish, Stefano Dellavigna, Sam Gershman, Emir Kamenica, Ben
Lockwood, Muriel Niederle, Matthew Rabin, Daphna Shohamy, Bruno Strulovici, Dmitry
Taubinsky, Michael Woodford, and seminar participants at Berkeley, Chicago, Columbia,
Edinburgh, Harvard, Hebrew U., NYU, SITE, and Stirling. We are grateful to the Pershing
Square Fund for Research on the Foundations of Human Behavior for financial support. The
views expressed herein are those of the authors and do not necessarily reflect the views of the
National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.

© 2017 by Xavier Gabaix and David Laibson. All rights reserved. Short sections of text, not to
exceed two paragraphs, may be quoted without explicit permission provided that full credit,
including © notice, is given to the source.
Myopia and Discounting
Xavier Gabaix and David Laibson
NBER Working Paper No. 23254
March 2017
JEL No. D03,D14,E03,E23

                                          ABSTRACT

We assume that perfectly patient agents estimate the value of future events by generating noisy,
unbiased simulations and combining those signals with priors to form posteriors. These posterior
expectations exhibit as-if discounting: agents make choices as if they were maximizing a stream
of known utils weighted by a discount function, D(t). This as-if discount function reflects the fact
that estimated utils are a combination of signals and priors, so average expectations are optimally
shaded toward the mean of the prior distribution, generating behavior that partially mimics the
properties of classical time preferences. When the simulation noise has variance that is linear in
the event's horizon, the as-if discount function is hyperbolic, D(t)=1/(1+at). Our agents exhibit
systematic preference reversals, but have no taste for commitment because they suffer from
imperfect foresight, which is not a self-control problem. In our framework, agents that are more
skilled at forecasting (e.g., those with more intelligence) exhibit less discounting. Agents with
more domain-relevant experience exhibit less discounting. Older agents exhibit less discounting
(except those with cognitive decline). Agents who are encouraged to spend more time thinking
about an intertemporal tradeoff exhibit less discounting. Agents who are unable to think carefully
about an intertemporal tradeoff – e.g., due to cognitive load – exhibit more discounting. In our
framework, patience is highly unstable, fluctuating with the accuracy of forecasting.


Xavier Gabaix
Department of Economics
Harvard University
Littauer Center
1805 Cambridge St
¸˛Cambridge, MA 02138
  and NBER
  xgabaix@fas.harvard.edu

David Laibson
Department of Economics
Littauer M-12
Harvard University
Cambridge, MA 02138
and NBER
dlaibson@gmail.com
1         Introduction
Most people appear to act as if they have a strong preference for earlier rewards over later
rewards. For the last century economists have usually assumed that this type of behavior
re‡ects (fundamental) time preferences, which economists model with discount factors that
multiplicatively weight utils. If the one-period-ahead discount factor is , then                   utils expe-
rienced now are as valuable as one util experienced next period. If                      < 1; economic agents
prefer a current util to a delayed util.
        However, such time preferences are only one of many ways to explain the empirical
regularity that intertemporal choices are characterized by declining sensitivity as utils are
moved further away in time. Diminishing sensitivity to future utils is also explained by
imperfect information. For example, Böhm-Bawerk (1889) wrote that “we possess inadequate
power to imagine and to abstract, or that we are not willing to put forth the necessary e¤ort,
but in any event we limn a more or less incomplete picture of our future wants and especially
of the remotely distant ones. And then, there are all of those wants that never come to mind
at all.” Pigou (1920) similarly observed “that our telescopic faculty is defective, and that
we, therefore, see future pleasures, as it were, on a diminished scale. That this is the right
explanation is proved by the fact that exactly the same diminution is experienced when,
apart from our tendency to forget ungratifying incidents, we contemplate the past.”1 Pigou
believed that our imperfect ability to forecast the future mirrors our imperfect ability to
recall the past.
        To gain intuition for the role of imperfect forecasting, consider a driver who sees an
upcoming pothole and estimates that it is small. A few moments later, he realizes that the
pothole is large, swerves to avoid it, and crashes.                This accident is likely a re‡ection of
imperfect foresight, not procrastination or laziness. In this case, large delayed consequences
are misperceived by an imperfectly farsighted driver. We probably wouldn’t infer that the
driver didn’t care about the impending crash because it was in the “future.”If the driver had
foreseen the consequences, he would have braked earlier. In general, people will not respond
optimally to future consequences that they do not anticipate (or only partially anticipate).
        Likewise, consider a sailor who sees a few clouds forming on the horizon and doesn’t
immediately take the costly action of charting a new course. When her vessel is lashed by
    1
        For a review of the history of theories of discounting see Loewenstein (1992).



                                                         2
a violent storm the next day, it is not clear whether she was lazy the previous night, or just
mistaken in her forecast about the upcoming weather.
      Decision-making is rife with situations in which a current action/inaction causes a stream
of current and future consequences, many of which are hard to foresee. If delayed conse-
quences are typically harder to foresee than immediate consequences, then decision-makers
will appear to be impatient.
      The role of imperfect information is also apparent in the seemingly impatient behavior
of non-human animals. When monkeys are given an abstract intertemporal choice task on
a computer, they act as if they discount delayed rewards at the rate of 10% per second.
When the same monkeys are given a temporally analogous foraging task (also presented
on a computer screen), the monkeys show very little discounting (Blanchard and Hayden
2015). Animal behavior appears to be impatient in completely novel domains and patient
in domains that are evolutionarily relevant. As Blanchard and Hayden (2015) conclude,
“Seemingly impulsive behavior in animals is an artifact of their di¢ culty understanding the
structure of intertemporal choice tasks.”
      In the current paper, we argue that behavior that arises from imperfect foresight is hard
to distinguish from behavior that arises from time preferences. We study a Bayesian decision-
maker with perfectly patient time preferences who receives noisy signals about the future.
The resulting signal-extraction problem leads the Bayesian agent to behave in a way that
is easy to misinterpret as a time preference; we call this seemingly impatient behavior as-if
discounting. Our analysis shows that lack of foresight generates behavior that has most of
the same characteristics of behavior that arises from deep time preferences. In other words,
a perfectly patient Bayesian decision-maker who receives noisy signals about the future will
behave as if they have time preferences.
      Opthalmic myopia arises when people can’t clearly see distant objects.       But myopia
also means a “lack of foresight or discernment.”2 Such forecasting limitations matter when
agents need to judge the value of events that will occur at a temporal distance.         In this
paper, we show that imperfect foresight – i.e., myopia – generates as-if discounting, even
when the actors’ true preferences are perfectly patient.        More generally, we show that
imperfect foresight makes agents appear to behave more impatiently than implied by their
deep time preferences.
  2
      Merriam-Webster.


                                                3
   Our formal model assumes that agents receive noisy, unbiased signals about future events
and combine these signals with their priors to generate posterior beliefs about future events.
Our key assumption is that the forecasting noise increases with the horizon of the forecast.
We give special attention to the case in which the variance of the forecasting noise rises
linearly with the forecasting horizon.
   We provide an illustrative example of our framework in Section 2, where we study a
binary choice problem: an actor chooses between an early reward and a mutually exclusive
later reward. We show that when the variance of forecasting noise rises linearly with the
event horizon, Bayesian agents will act as if they are hyperbolic discounters, even though
their deep time preferences are perfectly patient.
   In Section 3, we describe the broader implications of our framework, and identify predic-
tions that distinguish our framework from time preference models. First, we show that our
(perfectly patient) agents exhibit preference reversals of the same kind that are exhibited
by agents with hyperbolic discount functions. However, these preference reversals do not
re‡ect a self-control problem. The preference reversals arise because the agents obtain less
noisy information with the passage of time. Accordingly, our agents do not wish to commit
themselves; they act as-if they are naive hyperbolic discounters (Strotz 1957, Akerlof 1992,
O’Donoghue and Rabin 1999) rather than sophisticated ones (Laibson 1997).
   In the cross-section, our framework implies that agents with greater intelligence exhibit
less as-if discounting –their superior forecasting ability enables them to make choices that
are more responsive to future utility ‡ows.
   In addition, our agents exhibit as-if discounting that is domain speci…c. They exhibit
less as-if discounting (i) when they have more overall life experience, (ii) when they are more
experienced in the speci…c choice domain, (iii) when they have more time to think about an
intertemporal choice (e.g., Imas, Khun, and Mironova, 2016), and (iv) when they have more
cognitive bandwidth to think about their choice (e.g., Benjamin and Shapiro, 2015).
   In Section 4, we generalize our example by making the action set continuous. We provide
su¢ cient conditions that imply that perfectly patient agents who are imperfect forecasters
will act as if they are naive hyperbolic discounters.
   In Section 5, we discuss connections between our framework and related literatures on
myopia, Bayesian cognition, risk, and discounting. Section 6 concludes.



                                              4
2     A Basic Case: Binary Choice
Our approach can be explained with a simple example of a binary choice.          Consider an
agent at time zero, who must choose (irreversibly) between two mutually exclusive rewards:
Early and Late. Reward Early would be experienced at date, t          0: Reward Late would
be experienced at date, t +    > t (i.e.,   > 0): The agent doesn’t know the true value of
Early and Late, respectively denoted, ut and ut+ : To simplify exposition and without loss of
generality, we assume that these utility events are deterministic, though they were originally
generated from a prior distribution that we will characterize below. (Note that any non-
deterministic, zero-mean component is irrelevant because we are operating in utility space
and we assume that our agents have classical expected utility preferences.)
    Although the agent doesn’t know the value of ut and ut+ ; the agent can mentally simulate
these deterministic rewards and thereby generate unbiased signals of their value:


                                       st = ut + "t

                                     st+ = ut+ + "t+ :


In the …rst equation, ut is the true value of the Early utils and "t is the simulation noise.
In the second equation, ut+ is the true value of the Late utils and "t+ is the associated
simulation noise. For tractability, we assume that the simulation noise is Gaussian. To
simplify exposition, we assume that the correlation between "t and "t+ is zero.


2.1    Simulation Noise

We assume that the longer the horizon, the greater the variance of the simulation noise.
Intuitively, the further away the event, the harder it is to accurately simulate the event’s
utility. Because our set-up assumes that t < t + ; this assumption implies that


                                    var("t ) < var("t+ ):                                 (1)


We will also sometimes assume that


                                      lim var("t ) = 1;
                                      t!1




                                              5
however this property is not important for our qualitative results.
     We will pay particular attention to the special case of simulation noise that has a variance
that is proportional to the simulation horizon:

                                                  2            2
                                    var("t ) =    "t   =       "t                                      (2)
                                                  2                 2
                                  var("t+ ) =     "t+      =        "   (t + ) :                       (3)


This linearity assumption engenders a speci…c (hyperbolic) functional form in the analysis
that follows. But this linearity assumption is not necessary for our qualitative results. We
provide a complete characterization of noise functions below: i.e., necessary and su¢ cient
=conditions for the noise function to generate as-if discounting with declining discount rates
as the horizon increases. The case of linear variance is a special case in this larger class of
noise functions.


2.2      Bayesian Priors and Posteriors

The agents in our model combine Bayesian priors with their signals (st and st+ ) to generate
a Bayesian posterior. We model the Bayesian prior over utility events (in whatever class of
                                                                                                  2
events we are studying) as a Gaussian density with mean                            and variance   u:


                                                               2
                                           u     N( ;          u ):                                    (4)


Here     is the average value in this class of utility events (e.g., visits to Philadelphia), whereas
 2
 u   is the overall variance within the class (e.g., some trips are great –Philadelphia in June
–and some trips are much less great –Philadelphia in January).
     In the appendix, we derive the agent’s Bayesian posterior distribution of ut , which is
generated by combining her prior (4) and her signal st :
                                   0                   0                           1    1
                                     st                                   1
                           ut   N@ +             2    ; @1                    2
                                                                                   A   2A
                                                                                       u  :            (5)
                                                 "t                           "t
                                     1+          2                  1+        2
                                                 u                            u



We summarize these results with the following propostion.

Proposition 1 If the agent generates a mental simulation st ; then her Bayesian posterior


                                                      6
will be
                                                                                       2
                           ut      N         + D(t)(st         ); (1           D(t))   u   ;

where
                                                               1
                                                D(t) =             2    ;                                         (6)
                                                                   "t
                                                            1+     2
                                                                   u

                                                    2                                                      2
the variance of her simulation noise is             "t   and her prior distribution is u            N( ;   u ):


     For reasons that will become apparent below, we refer to D(t) as the agent’s as-if discount
                                                                        2
function. Because we assume that simulation noise,                      "t ;   is increasing in t; D(t) is decreasing
in t; which is a standard property of a discount function. If limt!1 var("t ) = 1; then
limt!1 D(t) = 0, another common property of a discount function. In this case, the posterior
expectation of ut converges to the mean of the prior as the horizon increases. In notation,


                                              lim E0 [ut j st ] = :
                                              t!1



     It is helpful to integrate posteriors over agents in the economy.                          We assume that the
signals st are unbiased, so they are equal to ut on average. Accordingly, the average forecast
of ut will be            Z
                                E0 [ut j st ] dF (st j ut ) =           + D(t)(ut          ):
                           st

In general, the mean of the prior will be less extreme than the actual values of ut : To model
this statistical property, consider the illustrative case in which the prior is approximately
equal to zero. (We will relax this restriction later.) Under this restriction, the average belief
is                                Z
                                         E0 [ut j st ] dF (st j ut ) = D(t)ut :
                                    st

We now have an expression that looks like a discounted utility framework: D(t) is a decreasing
function and it multiplies the actual utility value ut :


2.3       Hyperbolic As-if Discounting

We explore a benchmark case: noise that is linear in the horizon.




                                                           7
                                                   2          2
Lemma 1 When we assume that var("t ) =             "t   =     " t;   we obtain hyperbolic as-if discounting:

                                                         1
                                          D(t) =                                                          (7)
                                                       1+ t

where
                                                        2
                                                        "
                                                  =     2
                                                          ;                                               (8)
                                                        u

which is the (one-period) noise-to-signal variance ratio.

                                        1
   The discount function, D(t) =      1+ t
                                           ;   implies an instantaneous discount rate

                                                 D0 (t)       (1+ t)2
                        discount rate =                 =        1       =          :
                                                 D(t)          1+ t
                                                                             1+ t

At horizon 0, the as-if discount rate is : The as-if discount rate falls with t: As t ! 1, the
as-if discount rate converges to 0.


2.4      An Example When the Mean Priori Is not Zero ( 6= 0)

As we noted above, actual utility events will tend to be more extreme than priors.                        To
capture this property, we previously set the mean of the prior distribution equal to zero:
  = 0: We now relax this restriction and illustrate the general case with an example in
which the mean of the prior distribution is             = 1: For this example, we assume that the
                                                                                          2
simulation variance is linear in the time horizon and the variance ratio is               "
                                                                                          2   = 0:1: Figure 1
                                                                                          u

plots the population level expectations of ut for three values of ut (holding the mean of the
prior distribution …xed at   = 1):


                                       ut =      + 10 = 11

                                       ut =         1=2 = 1=2

                                       ut =         10 =        9:


   When the three utility events are in the present (t = 0), the three expectations are equal
to the true value of each utility event, respectively 11, 1/2, and -9. However, as the three
utility events recede into the distant future, the three expectations revert to the mean of the
prior,   = 1: This discounting towards the mean of the prior is hyperbolic because we are


                                                   8
Figure 1: Plot of the average perceived value ut , given for three di¤erent true utilities ut
(ut 2 f 9; 1=2; 11g), as a function of the time horizon t. This average perceived value is:
ut = + ut "2 . The …gure uses "2 = u2 = 0:1.
         1+   2t
              u




assuming linear variance (see subsection 2.3).
   The ut = 11 curve is characterized by standard discounting.         The further ahead the
utility event is shifted, the lower the perceived value of the event. The ut =      9 curve is
also characterized by standard discounting on most of its domain. As the event is moved
further into the future, its value declines toward zero. However, at t = 90; the perceived
value crosses the x-axis and continues asymptoting toward        =   1: Finally, the ut = 1=2
line displays anti-discounting. The further the value is moved into the future, the higher its
perceived value, as it asymptotes to the prior mean of    = 1:
   These three lines illustrate the three types of cases that arise in our framework, including
the special case of anti-discounting. Note that anti-discounting arises when the true value
of ut lies between 0 and the mean of the prior distribution, :


2.5    Probabilistic Choice

Our framework implies that choice is probablistic, because agents receive noisy signals about
the value of future rewards. In our example, the agent chooses Early if and only if


                                   D(t)st    D(t + )st+ ;




                                              9
where D(t) is the as-if discount function introduced above and st and st+ are the (unbiased)
signals of the respective values of the Early and Late rewards.
   From the perspective of an observer who knows the values of ut and ut+ , the probability
that that agent chooses the Early reward is


               P(choose Early) = P [D (t) (ut + "t )              D (t + ) (ut+ + "t+ )]
                                          1
                                  =           [D(t)ut       D(t + )ut+ ] ;


where     is a Gaussian CDF and        is a scaling factor:

                                p
                            =    D(t)2 var("t ) + D(t + )2 var("t+ ):


   This probabilistic choice function has natural properties. If t = 0 (i.e., the Early reward
is an immediate reward), then,


                         P(choose Early) = P [u0             D ( ) (u + " )]
                                                        1
                                              =             [u0     D( )u ] :


If we let the time delay between the Early reward and the Late reward go to in…nity (i.e.,
 ! 1), then
                                 lim P(choose Early) = 1u0 >0 :
                                  !1


This implies that the agent chooses the Early reward with probability one if three properties
hold: (i) the Early reward is available immediately (t = 0), (ii) the Late reward is available
arbitrarily far in the future ( ! 1), and (iii) the Early reward is strictly positive (u0 > 0).
In other words, the agent behaves as if she places no value on the (in…nitely) delayed Late
reward.
   Now assume that the Early reward is available with some delay, so that t > 0 (i.e., the
Early reward is not immediate), then


                            lim P(choose Early) = P [ut + "t > 0]
                            !1
                                                                  ut
                                                        =              :



                                                  10
Accordingly, if the Late reward is available arbitrarily far in the future ( ! 1), then the
agent chooses the Early reward with the same probability that she perceives the Early
reward to have positive value. Once again, the agent behaves as if she places no value on
the (in…nitely) delayed Late reward.


2.6        Preference Reversals without Commitment

In our setting, an observer who knows the values of ut and ut+ will be able to predict
(probabilistic) preference reversals. For example, consider the case of linear variances. In
addition, assume that ut+ > ut > 0; and


                                              ut > D( )ut+ :


When the two options are su¢ ciently far in the future (large t), a majority of agents (if
forced to choose) will prefer Late over Early, because

                                                     1
                        P(choose Early) =                [D(t)ut    D(t + )ut+ ] :


Note that
                                                            ut          ut+
                          D(t)ut     D(t + )ut+ =                               :
                                                           1+ t      1 + (t + )
For su¢ ciently large values of t; ut+ > ut implies,

                                        ut            ut+
                                                              < 0:
                                       1+ t        1 + (t + )

       However, with the passage of time, all agents will eventually choose Early because ut >
D( )ut+ : More precisely, if agents were not forced to choose in advance, but were instead
given the chance to choose at time t; all would choose Early.
       In many economic models, such preference reversals are a sign of dynamic inconsistency
in preferences.3 That is not the case here. The agents in the current model have imperfect
information, not dynamically inconsistent time preferences.                  Their externally predictable
   3
    See McGuire and Kable (2012, 2013) for a setting in which preference reversals arise because of rational
learning dynamics. If a delayed reward that was probabilitistically expected does not arrive after a period
of waiting, the agent infers that the hazard rate of arrival is low and further waiting is not likely to pay o¤,
and therefore reverts to choosing the immediate reward.



                                                      11
preference reversals are a result of their imperfect information. Accordingly, the agents in
our model will not desire to limit their own choice sets. Preference reversals arise from their
inference problems, not self-control problems.


2.7     More General Discounting Functions

We can provide necessary and su¢ cient conditions for the as-if discount function, D(t);
to exhibit decreasing impatience. In other words, we can derive necessary and su¢ cient
conditions for the property that the instantaneous as-if discount rate

                                                               D0 (t)
                                             (t) :=
                                                               D(t)

is decreasing in the horizon t:

Proposition 2 The as-if discount function D(t) exhibits strictly decreasing impatience at
time horizon t if and only if
                                                     d    2             2
                                   d2 "2t                 "t
                                                               =   2
                                                         dt        u
                                    dt2
                                      2                            2         < 0:
                                                                   "t
                                      u              1+            2
                                                                   u


                                                                                                 d    2
                                                                                                      "t
     This proposition is proved in the appendix.                        Because we assume that       dt
                                                                                                           > 0; this
proposition yields an immediate corollary.

Lemma 2 The as-if discount function D(t) exhibits strictly decreasing impatience if the
                                              2
variance of simulation noise, var("t ) =      "t ;   is a weakly concave function of time.

     Accordingly, our model generates as-if discount rates that decrease as the horizon in-
creases in many cases. We next study a boundary case.


Exponential As-if Discounting Our framework can also be reverse-engineered to gen-
erate exponential discounting as a special case. However, this requires assumptions on the
variance function that we believe are heroic.

Lemma 3 The as-if discount function D(t) exhibits a constant discount rate, ; if and only
if
                                      2                                       2
                                      "t    = [exp( t)                  1]    u:




                                                         12
                                                                                             2
    Accordingly, the discount rate is exponential if and only if the simulation variance,    "t ;

rises exponentially. This Lemma is proven by setting

                                                 1
                                 D(t) =              2        = exp( t):
                                                     "t
                                          1+         2    t
                                                     u



This sort of cognitive discounting is useful because of the tractability it induces (see for
instance Gabaix 2016a,b).



3      Implications
We now discuss the key predictions of our model, emphasizing several ways that our model
of myopia di¤ers from other models in the intertemporal choice literature.      As discussed
above, our myopic agent acts as if she is maximizing a utility function with an as-if discount
function, D( ), where
                                                              1
                                      D( ) =                      2    :
                                                                  "
                                                     1+            2
                                                                   u


When the variance of the forecasting noise is weakly concave in the simulation horizon, the
discounting function is characterized by an instantaneous discount rate that falls with the
                                                                                     2       2
horizon. When the forecasting noise is linear in the simulation horizon, so that     "   =   "

then the discount function is hyperbolic:

                                                              1
                                      D( ) =                      2    :
                                                     1+           "
                                                                  2
                                                                  u



These as-if discounting functions arise because of the imperfect information that the agent
has when she generates forecasts. If she were asked to describe her preferences, she would
say that she has no times preferences. In other words, she is trying to maximize

                                          TPt
                                                 u(at+ ):
                                            =0


Her as-if discounting behavior arises because she doesn’t have perfect foresight regarding the
future utility ‡ows u(at+ ):




                                                  13
3.1       Absence of Commitment

The agents in this model have a forecasting problem, not a self-control problem. Therefore
they are never willing to reduce their choice set (unless they are paid to do so). This absence
of a willingness to pay for commitment may explain the lack of commitment technologies in
markets. In real markets there is little commitment for commitment’s sake.4 Personal train-
ers and website blocking apps are frequently mentioned exceptions, but such technologies
are not commonly used.
       By contrast, economists have been able to elicit commitment in experiments (see Cohen,
Ericson, Laibson, and White 2017, for a review). However, most of these experiments elicit
only a weak taste for a commitment and little or no willingness to pay for commitment (e.g.,
Augenblick, Niederle and Sprenger 2015, Sado¤, Samek and Sprenger 2016).
       Our myopia model predicts that agents will exhibit as-if hyperbolic discounting with
preference reversals and no willingness to pay for commitment.                In this sense, our model
reproduces the predictions of the standard hyperbolic discounting model with naive beliefs
(see O’Donoghue and Rabin 1999, 2001, Laibson 2015, and Ericson forthcoming). However,
it also generate further implications, to which we now turn.


3.2       Intelligence Is Associated with Less As-if Discounting

Our model predicts that agents with less forecasting noise will exhibit less as-if discounting.
Because of this mechanism, agents that are more intelligent will exhibit less as-if discounting.5
       To see this formally, let H represent human capital and assume that the variance of
forecasting noise is declining in human capital:

                                              d "2 (H)
                                                       < 0:
                                                dH

The as-if discount rate is given by

                                             D0 (t)
                                                    =      ;
                                             D(t)     1+ t
   4
     However, there is a great deal of ancillary commitment, like mortgage contracts, which create a forced
savings system as a by-product of a stream of loan/principal repayments.
   5
     The underlying assumption is that more intelligent agents simulate the future with less noise— for in-
stance because they generate more simulations. If they run n simulations, the variance of the average
simulation will be 1=n, so it will be lower.


                                                    14
where
                                                 2
                                                 " (H)
                                            =       2
                                                       :
                                                    u
                                           2
The as-if discount rate is increasing in   " (H);    so as-if discounting is decreasing in human
capital, H:
   The available evidence supports this prediction. Measured discount rates are negatively
correlated with scores on IQ tests: see Benjamin, Brown, and Shapiro (2013), Burk et al.
(2009) and Shamosh and Gray (2008). Indeed, such e¤ects also arise across species. Tobin
and Logue (1994) show that patience increases as the study population switches from pigeons,
to rats, to humans.


3.3     Myopia Is Domain Speci…c

These comparative statics on cognitive function generate a wider set of predictions when
forecasting ability varies across domains. For example, our framework predicts that agents
with more domain-relevant experience, and hence better within-domain forecasting ability,
will exhibit less discounting. Read, Frederick and Scholten (2013) report that people exhibit
more patience when an intertemporal choice is posed as an investment rather than a (seem-
ingly novel) money-now-vs-money-later decision. Relatedly, recall our earlier discussion of
the monkey experiments reported by Blanchard and Hayden (2015): when an intertemporal
choice is presented as a reward-now-vs-reward-later decision, monkeys choose far more im-
patiently then they do when a foraging problem is used to frame the intertemporal tradeo¤s.
   Likewise, our framework predicts that older agents –who generally have more life experi-
ence and consequently better forecasting skills –will exhibit less discounting. This prediction
is supported by Green, Fry, and Myerson (1994). Relatedly, Addessi et al (2014) show that
replacing one-for-one representations of future reward with more abstract one-for-many rep-
resentations of the same future rewards, leads capuchin monkeys and (human) children to
exhibit more impatience. In contrast, adults, who have more experience using abstract sym-
bols, do not behave more impatiently when one-for-one representations of future reward are
replaced with one-for-many representations. The Addessi et al (2014) experimental evidence
implies that childhood impatience is due, at least in part, to children’s less developed ability
to cognitively represent (abstract) future rewards. Our framework also predicts that people
who experience cognitive decline (e.g., due to normal aging) will exhibit more discounting;

                                                15
see James, Boyle, Yu, Han, and Bennett 2015 for supporting evidence.
    Our framework predicts that agents who are encouraged to spend more time thinking
about a future tradeo¤ will exhibit less discounting.    Imas, Kuhn, and Mironova (2016)
robustly measure such an e¤ect experimentally. In their experiment, some subjects decide
at time 0 how to divide an e¤ort task between time 0 and time t: Other subjects are given a
preceding hour to decide how to divide the e¤ort task between time 0 and time t: Subjects
in the latter condition choose more patiently: their measured discount rate is 16 percentage
points lower.
    Our framework also predicts that rewards delivered in future periods that are cognitively
well-simulated will exhibit less discounting. Peters and Büchel (2010) exogenously manip-
ulate the salience of various future periods and …nd that higher salience/imagery of future
reward periods increases the value of rewards delivered during those periods.
    Our framework predicts that agents who are unable to think carefully about an intertem-
poral tradeo¤ – e.g., due to a cognitive load manipulation or the e¤ects of alcohol – will
exhibit more discounting. Steele and Josephs (1990), Shiv and Fedorikhin (1999), Hinson,
Jameson, and Whitney (2003), and Benjamin, Brown and Shapiro (2013) document such
e¤ects.   This prediction is closely related to the theory of cognitive scarcity: see Spears
(2012), Eldar and Sha…r (2013), and Schilbach et al (2016).
    Finally, our framework predicts that discounting behavior will only be weakly correlated
across domains because discounting is not a domain general preference, but rather the result
of imperfect forecasting, which will naturally vary across domains. Chapman (1996) and
Chabris et al (2008) document the low level of correlation in discount rates that are measured
in di¤erent decision-making domains.



4     Extension to a Continuous Action
Until now we have studied the case in which the agent has two mutually exclusive actions:
choose an Early reward or a Late reward. We now generalize the action space to a con-
tinuum. We then provide su¢ cient conditions that enable us to apply our framework to a
general, multi-period intertemporal choice problem.
    The upshot of this section is that the economics of the binary action case still goes
through, though with more complex mathematics.

                                             16
4.1      Modelling How Agents Observe with Noise a Whole Utility
         Function

Suppose that an action a leads to a true payo¤ u (a). However, the agent observes this
noisily: we suppose that the agent observes the “noised up”version of the utility function:


                                              s = (s (a))a2A                                         (9)


of the whole function u = (u (a))a2A , where A = [a; a] is the support of the action, which is
assumed to contain 0 (this is just a normalization). This noised-up version is assumed to
take the form:
                                 s (a) = u (a) +    "t W   (a) +    "t 0                            (10)

for all a 2 A. There is a continuous noise W (a), modelled as standard Brownian motion
with W (0) = 0 except that W is “two-sided”, i.e. runs to the left and right of 0.6;7 The noise
is modelled as proportional to "t when the utility is seen from a distance t. For instance, the
                      p
linear case is "t = " t: The term "t 0 ensures that the value at a = 0 is also perceived
with noise (    0   = 1,   is a parameter).
      Given this perceived curve s, what’s his posterior about u (a)? We will see the under the
“right”assumptions (to be speci…ed soon), we simply have


                                      E [u (a) j s] = D (t) s (a)


with the same D (t) as in the binary case. This means that the representative agent just
dampens the true function.


4.2      Assumptions for Our Result

Here we detail the assumptions we use for the results. But the reader may wish to skip to
the result itself, in the next subsection 4.3.

                                                                                      u( ) u(0)
Assumption 1 (Wiener decomposition) We suppose that function v ( ) :=                     u
                                                                                                  is per-
  6
    Formally, W (x)x 0 and W ( x)x 0 are independent Brownian motions.
  7
    See Callender and Hummel (2014) for a recent model using inference on Brownian paths, though with a
signal structure di¤erent from ours.



                                                   17
                                                                                                       2 2
ceived as drawn from the Wiener measure, and u (0) is drawn as u (0)                           N (0;     u)   inde-
pendent of v. We call
                                                              1
                                                  D (t) =         2                                           (11)
                                                                  "t
                                                            1+    2
                                                                  u


where     "t   is as in (10).

      Let us state this assumption in more user-friendly language. The value of u (0) is also
seen as random— and we index its randomness by                         . The rest of the function u, outside
                                                                             u(a) u(0)
the intercept, is also random. To specify this, we v (a) :=                      u
                                                                                         , which is u normalized
to have 0 intercept, and with standardized size (so E v (1)2 = 1). We view v a a “random
function” drawn from a distribution. For simplicity, we consider that it’s drawn from the
simplest distribution of random functions –the so-called Wiener measure (Brownian motions
are typical instances of such functions).8 Basically, the assumption is that the component
of du (a) are drawn as i.i.d. normal increments, like a Brownian motion, with square width
 2
 u da.   Note that this refers to the distribution assumed by the agent when he performs his
Bayesian inference, not necessarily the true distribution.
      The appendix (section 7.2) proposes a variant, Assumption 2, with polynomial utility,
that uses more elementary mathematics, at the cost of heavier notations and proofs.


4.3      Perceived Utility Function Given the True Utility

We can now derive the utility perceived by the agent, given she agent sees the whole noised-up
function s (equation (10)).

Proposition 3 (Perceived utility for a continuous utility function) Make Assumption 1 or
2. Then, the perceived utility is proportional to the signal:


                                             E [u (a) j s] = D (t) s (a)

                      1
where D (t) =             2
                          "t
                               . As a result, the average perceived utility u (a), de…ned as:
                   1+     2
                          u



                                            u (a) := E [E [u (a) j s] j u]
  8
    We could imagine a number of variants, e.g. u00 would be drawn from this distribution; or, to keep u
concave, we could have ln ( u00 ) drawn from that distribution. This becomes quickly more mathematically
involved, so we leave this to a separate investigation, and focus on what we view as the simplest case.


                                                         18
satis…es:
                                             u (a) = D (t) u (a)                                           (12)

       This means that the average perceived utility is D (t) u (a) rather than plainly u (a),
exactly like in the simple two-action (consume / don’t consume) case.


4.4       The Representative Agent Perspective

4.4.1      Assumptions for a Tractable Generalization

To cleanly study dynamic problem, we assume the following (in addition to the assumptions
of Proposition 3).9

 A1. The agent treats the noise at all simulation horizons as uncorrelated.

 A2. The agent has Gaussian priors with 0 mean (and no correlation between ut ; ut+ ).

 A3. The agent acts as if she won’t learn new simulation information in the future.10

       The notion of “average behavior” is potentially messy with non-linear utilities. Hence,
we …nd it useful to de…ne the following form of “representative agent”version of the model.
We study the equilibrium path in which all simulation noise happens to be realized as zero
(but the agent doesn’t know this). In our illustrative example, this corresponds to "t = 0.
For instance, we had st = ut + "t and E[ut j st ] = D(t)st . The representative agent draws
noise "t = 0; so for the representative agent, E[ut j st ] = D (t) ut .

Proposition 4 (Dynamic choices of the representative agent) Assume that the agent has
dynamically consistent preferences
                                                    X
                                                    T
                                                          u(at ):
                                                    t=0

Then A1-A3 imply that at each time period t 2 f0; :::; T g the representative agent acts as if
she is trying to maximize
                                              X
                                              T t
                                                    D( )u(at+ )
                                               =0

   9
     There are many alternative ways to generate variances that are linear in the forecasting horizon, including
new signals that contain all of the information of the old signals.
  10
     This is the assumption of the “anticipated utility” framework of Kreps (1998) used also by Cogley and
Sargent (2008).


                                                          19
where
                                                         1
                                          D( ) =              2    :
                                                              "
                                                      1+       2
                                                               u


Corollary 1 Assume that simulation variance is linear in the horizon of the simulation:
 2        2
 "   =    ".   Then, at each time period t 2 f0; :::; T g the representative agent acts as if she is
trying to maximize
                                          X
                                          T t
                                                D( )u(at+ );
                                           =0

where

                                                        1
                                           D( ) =
                                                      1+
                                                        2
                                                        "
                                                  =     2
                                                          :
                                                        u


     Proposition 4 shows that our basic results extend to arbitrary utility functions with
continuous actions.



5        Literatures on Related Mechanisms
We now review other lines of research that are related to this paper and on which this paper
builds. We review three literatures: models of myopia, Bayesian foundations of imperfect
and costly cognition, risk-based models of as-if discounting (including risk-based models with
probability distortions).


5.1      Myopia

Political economists, psychologists, and other social scientists have long posited that impa-
tient behavior was due in part to imperfect foresight. These ideas were informally described
by political economists, including Böhm-Bawerk (1889), and economists, including Pigou
(1921), both of whom are quoted in the introduction of this paper.
     These informal explanations have been joined by formal, mathematical de…nitions, mod-
els, and analyses of that incorporate various formulations of myopia. For example, Brown
and Lewis (1981) provide an axiomatic de…nition of myopia. Feldstein (1985) evaluates the
optimality of social security under the assumptions of myopia and partial myopia (modeled

                                                   20
as a low discount factor in a two-period decision problem). Phillippe Jéhiel (1995) studies
two-player games in which players have limited forecasting horizons. Spears (2012) gener-
ate a forecasting horizon that is endogenous because forward-looking calculations are costly.
Gabaix, Laibson, Moloche, and Weinberg (2006) report experimental evidence that supports
a model in which agents choose an endogenous forecasting horizon at which the cognitive cost
and estimated utility bene…t of marginally increasing the forecasting horizon are equalized.
This optimal forecasting framework generates a complex option value problem with respect
to information acquisition (see also Fudenberg, Strack, and Strzalecki 2016).
       In the current paper, we assume that the agent has noisy signals about the future, which
engenders Bayesian forecasts that have “myopic” properties: i.e., declining sensitivity to
future events. When the noise is linear in the forecasting horizon, the as-if discounting takes
a simple hyperbolic form. Accordingly, our paper introduces a tractable microfoundation
for myopia.


5.2       Bayesian Models of Attention and Cognition

The current paper assumes that agents are Bayesian, which adopts the approach of early
decision-theory pioneers like Rai¤a and Schlaifer (1961). There is a growing body of litera-
ture (in economics, cognitive psychology, and neuroscience) that studies the e¤ects of noisy
perception and Bayesian inference, and uses this combination to explain seemingly subop-
timal behaviors. One of the pioneering examples is the work of Commons, Woodford and
Ducheny (1982), and Commons, Woodford and Trudeau (1991) who use this approach to
generate a theory of hyperbolic memory recall – in their framework, the noisy signals are
memories, whereas the noisy signals in our model are simulations of the future. The literature
on attention allocation assumes that agents have limited information, which is mathemat-
ically equivalent to the assumption that agents have noisy signals about the state of the
world. Geanakoplos and Milgrom (1991), Sims (2003), Kamenica (2008), Woodford (2009),
Gabaix (2014), Schwartzstein (2014), Hanna, Mullainathan and Schwartzstein (2014), All-
cott and Taubinsky (2015), Matejka, Steiner and Stewart (forth.), Taubinsky and Rees-Jones
(2016a,b), study agents who allocate their limited attentional bandwidth to the activities
that they believe are the most valuable.11 Steiner and Stewart (2016) and Khaw, Li, and
  11
   Another strand of the literature uses non-Bayesian rules to govern attention and salience (e.g. Bordalo,
Gennaioli and Shleifer 2012, 2013), though it might be probably be given some quasi-Bayesian interpretation.


                                                    21
Woodford (2017) study an environment in which agents react to the noise in their probability
perceptions by (optimally) distorting their perceived probabilities in a way that mimics the
probability mapping in prospect theory (Kahneman and Tversky 1979).
   Our paper adopts the approach that uni…es the work above: noisy signals plus Bayesian
inference jointly produce as-if behavior that appears to be imperfectly rational. Speci…cally,
in our case, this combination generates as-if hyperbolic discounting.


5.3    Risk-Based Models of As-if Discounting

It has long been recognized that time preferences engender the same kind of behavior that is
associated with risk or mortality (e.g., Yaari 1965). For example, if promised future rewards
may be permanently withdrawn/lost at a constant hazard rate, ; then a perfectly patient
decision-maker should be indi¤erent between 1 util at time zero and exp(     ) utils a time .
In this example, risk induces a perfectly patient agent to appear to discount the future with
exponential discount rate :
   This type of risk-based discounting can also produce hyperboloid discount functions un-
der speci…c assumptions about a non-constant hazard rate (see Sozou 1998, Azfar 1999,
Weitzman 2001, Halevy 2004, and Dasgupta and Maskin 2005). For instance, Azfar, Sozou
and Weitzman assume that the hazard rate that governs the withdrawal of rewards is itself
drawn from a distribution and has a value that can only be inferred from the observed data.
This assumption produces preferences that are characterized by a declining discount rate as
the horizon increases –the more time passes without a withdrawal, the more likely that one
of the low hazard rates is the hazard rate that was drawn from the distribution at the start
of time, implying a lower e¤ective discount rate at longer horizons. Risk can also produce
hyperboloid discount functions because of probability transformations that are characterized
by a certainty e¤ect, whereby a certain present reward is discretely more valuable than an
even slightly uncertain delayed reward (see the non-expected utility frameworks in Prelec
and Loewenstein 1991, Quiggin and Horowitz 1995, Keren and Roelofsma 1995, Weber and
Chapman 2005, Halevy 2008, Epper, Fehr-Duda and Bruhin 2011, Baucells and Heukamp
2012, Andreoni and Sprenger 2012, and Fehr-Duda and Epper 2015).
   Our model works o¤ a related but di¤erent risk mechanism than those listed above. The
uncertainty in our model is due to noise that is generated by the forecaster herself.     For


                                             22
example, our mechanism predicts that an expert would exhibit little as-if discounting in
her domain of expertise (she forecasts the future with little or no noise) while a non-expert
would exhibit substantial as-if discounting in the same domain (she forecasts the future
with relatively more noise than the expert). Likewise, our framework predicts that cognitive
load should increase as-if discounting because it reduces an agent’s ability to forecast accu-
rately. Accordingly, our noise-based discounting mechanism is not propagated by external
risk (like mortality or the likelihood of default), but rather by noise associated with the
limited forecasting ability of the decision maker.
    Finally, our framework is consistent with Bayesian decision-making and expected utility
theory.   Accordingly, our agent will not be dynamically inconsistent and will not pay for
commitment. In our framework, preference reversals re‡ect classical information acquisition,
not weakness of will.
    Our key assumption is that the agent has (unbiased) noise in her signals about the
future. This noise leads our agent to optimally down-weight her simulations of the future
and therefore place more weight on her priors. Consequently, she ends up being (rationally)
imperfectly responsive to future contingencies and therefore behaves as if she discounts the
future. As her expertise and experience improves (over her lifetime, or as she gains domain-
speci…c knowledge), she shifts her behavior and acts as-if she has become more patient.



6     Conclusion
We assume that perfectly patient agents estimate the value of future events by generating
noisy, unbiased simulations of those events. Our agents combine these noisy signals with
their priors, thereby forming posterior utility expectations. We show that these expectations
exhibit a property that we call as-if discounting. Speci…cally, the agent makes choices as if
she were maximizing a stream of known utils weighted by an as-if discount function, D(t):
This as-if discount function adjusts for the fact that future utils are not actually known by
the agent and must be estimated with noisy signals and priors.       This estimation shades
the estimated utils toward the mean of the prior distribution, creating behavior that largely
mimics the e¤ect of classical time preferences.
    When the simulation noise has a variance that is linear in the event’s horizon, the as-if



                                              23
discount function is hyperbolic:
                                                   1
                                        D(t) =        ;
                                                 1+ t
where    is the ratio of the variance of (per-period) simulation noise to the variance of events
in the agent’s prior distribution.
   Our model generates several predictions that match the known empirical evidence. Our
agents exhibit systematic preference reversals. Our agents have no intrinsic taste for com-
mitment, because they su¤er from an imperfect forecasting problem, not a self-control prob-
lem. Our agents will exhibit comparative statics with respect to cognitive function: people
who are more skilled at forecasting (e.g., those with greater intelligence) will exhibit less
discounting.
   Our framework predicts many domain-speci…c discounting e¤ects.           Agents with more
domain-relevant experience will exhibit less discounting.       Older agents will exhibit less
discounting (except those with cognitive decline, who will exhibit more discounting). Agents
who are encouraged to spend more time thinking about a future tradeo¤ will exhibit less
discounting.    Finally, agents who are unable to think carefully about an intertemporal
tradeo¤ –e.g., due to a cognitive load manipulation –will exhibit more discounting.
   Our framework predicts that discounting is a highly variable/plastic phenomenon that
arises from imperfect forecasting of future rewards/costs. Our model provides a complemen-
tary alternative to the classical assumption that discounting arises from a deep preference
for known rewards (costs) to be moved earlier (later) in time.




                                              24
References

Addessi, Elsa, Francesca Bellagamba, Alexia Del…no, Francesca De Petrillo, Valentina Fo-
caroli, Luigi Macchitella, Valentina Maggiorelli, et al. 2014. “Waiting by Mistake: Symbolic
Representation of Rewards Modulates Intertemporal Choice in Capuchin Monkeys, Preschool
Children and Adult Humans.”Cognition 130 (3): 428–41.
   Allcott, Hunt, and Dmitry Taubinsky. “Evaluating Behaviorally-Motivated Policy: Ex-
perimental Evidence from the Lightbulb Market.” American Economic Review 105 (8):
2501–38.
   Andreoni, James, and Charles Sprenger. 2012. “Risk Preferences Are Not Time Prefer-
ences.”American Economic Review 102 (7): 3357–76.
   Augenblick, Ned, Muriel Niederle, and Charles Sprenger. 2015. “Working Over Time:
Dynamic Inconsistency in Real E¤ort Tasks.”Quarterly Journal of Economics 130 (3): 1067–
115.
   Azfar, Omar. 1999. “Rationalizing Hyperbolic Discounting.” Journal of Economic Be-
havior & Organization 38 (2): 245–52.
   Baucells, Manel, and Franz H. Heukamp. 2012. “Probability and Time Trade-O¤.”
Management Science 58 (4): 831–42.
   Blanchard, Tommy C., and Benjamin Y. Hayden. 2015. “Monkeys Are More Patient in a
Foraging Task than in a Standard Intertemporal Choice Task.”PLoS ONE 10 (2): e0117057.
   Bordalo, Pedro, Nicola Gennaioli, and Andrei Shleifer. 2012. “Salience Theory of Choice
Under Risk.”The Quarterly Journal of Economics 127 (3): 1243–85.
   Bordalo, Pedro, Gennaioli, Nicola and Andrei Shleifer. 2013. “Salience and Consumer
Choice.”Journal of Political Economy 121(5): 803-43.
   Brown, Donald J. and Lucinda M. Lewis. 1981. “Myopic Economic Agents.”Economet-
rica 49 (2): 359-68.
   Callander, Steven, and Patrick Hummel. 2014. “Preemptive Policy Experimentation.”
Econometrica 82 (4): 1509–28.
   Chakraborty, Anujit, Evan Calford, Guidon Fenig and Yoram Halevy. 2015. “Exter-
nal and Internal Consistency of Choices made in Convex Time Budgets.” Working paper,
University of British Columbia.



                                            25
   Cogley, Timothy and Thomas J. Sargent. 2008. “Anticipated Utililty and Rational Ex-
pectations as Approximations of Bayesian Decision Making.”International Economic Review
49 (1): 185–221.
   Commons, Michael L., Michael Woodford, and John R. Ducheny. 1982. “How Reinforcers
Are Aggregated in Reinforcement-Density Discrimination and Preference Experiments.” In
Quantitative Analyses of Behavior: Vol. 2, Matching and Maximizing Accounts, edited by
Michael L. Commons, Richard J. Herrnstein, and Howard Rachlin, 25–78. Cambridge, MA:
Ballinger.
   Commons, Michael L., Michael Woodford, and Edward J. Trudeau. 1991. “How Each
Reinforcer Contributes to Value: ‘Noise’Must Reduce Reinforcer Value Hyperbolically.” In
Signal Detection: Mechanisms, Models, and Applications, edited by Michael L. Commons,
John A. Nevin, and Michael C. Davison, 139–68. Hillsdale, NJ: Lawrence Erlbaum Asso-
ciates.
   Dasgupta, Partha and Eric Maskin. 2005. “Uncertainty and Hyperbolic Discounting.”
American Economic Review 95(4): 1290-99.
   De Martino, Benedetto, Dharshan Kumaran, Ben Seymour, and Raymond J. Dolan.
2006. “Frames, Biases, and Rational Decision Making in the Human Brain.” Science 313
(5787): 684–87.
   Doll, Bradley B., Katherine D. Duncan, Dylan A. Simon, Daphna Shohamy, and Nathaniel
D. Daw. 2015. “Model-Based Choices Involve Prospective Neural Activity.” Nature Neuro-
science 18 (5): 767–72.
   Epper, Thomas, and Helga Fehr-Duda. 2015. “The Missing Link: Unifying Risk Taking
and Time Discounting.”Working Paper no. 096, University of Zurich.
   Epper, Thomas, Helga Fehr-Duda, and Adrian Bruhin. 2011. “Viewing the Future
Through a Warped Lens: Why Uncertainty Generates Hyperbolic Discounting.”Journal of
Risk and Uncertainty 43 (3): 169–203.
   Ericson, Keith M. Forthcoming. “On the Interaction of Memory and Procrastination:
Implications for Reminders, Deadlines, and Empirical Estimation.”Journal of the European
Economic Association.
   Feldstein, Martin. 1985. “The Optimal Level of Social Security Bene…ts.”The Quarterly
Journal of Economics 100 (2): 303–20.
   Fudenberg, Drew, Philipp Strack, and Tomasz Strzalecki. “Stochastic Choice and Opti-

                                           26
mal Sequential Sampling.”Working paper, Harvard University.
   Gabaix, Xavier. 2014. “A Sparsity-Based Model of Bounded Rationality.”The Quarterly
Journal of Economics 129 (4): 1661–710.
   Gabaix, Xavier. 2016a. “Behavioral Macroeconomics via Sparse Dynamic Program-
ming.”Working Paper no. 21848, NBER, Cambridge, MA.
   Gabaix, Xavier. 2016b. “A Behavioral New Keynesian Model.” Working Paper no.
22954, NBER, Cambridge, MA.
   Gabaix, Xavier, David Laibson, Guillermo Moloche, and Stephen Weinberg.        2006.
“Costly Information Acquisition: Experimental Analysis of a Boundedly Rational Model.”
American Economic Review 96 (4): 1043–68.
   Geanakoplos, John and Paul Milgrom. 1991. “A Theory of Hierarchies Based on Limited
Managerial Attention.”Journal of the Japanese and International Economies 5 (3): 205–25.
   Halevy, Yoram. 2004. “Diminishing Impatience: Disentangling Time Preference from
Uncertain Lifetime.”Working paper, University of British Columbia.
   Halevy, Yoram. 2008. “Strotz Meets Allais: Diminishing Impatience and the Certainty
E¤ect.” American Economic Review 98 (3): 1145–62.
   Halevy, Yoram. 2014. “Some Comments on the Use of Monetary and Primary Rewards
in the Measurement of Time Preferences.”Working paper, University of British Columbia.
   Halevy, Yoram. 2015. “Time Consistency: Stationarity and Time Invariance.” Econo-
metrica 83 (1): 335–52.
   Hanna, Rema, Sendhil Mullainathan, and Joshua Schwartzstein. 2014. “Learning Through
Noticing: Theory and Evidence from a Field Experiment.” The Quarterly Journal of Eco-
nomics 129 (3): 1311–53.
   Imas, Alex, Michael A. Kuhn, and Vera Mironova. 2016. “Waiting to Choose.”Working
Paper no. 6162, CESifo Group, Munich, Germany.
   James, Bryan D., Patricia A. Boyle, Lei Yu, S. Duke Han, and David A. Bennett. 2015.
“Cognitive Decline is Associated with Risk Aversion and Temporal Discounting in Older
Adults Without Dementia.”PLoS ONE 10 (4): e0121900.
   Jéhiel, Phillippe. 1995. “Limited Horizon Forecast in Repeated Alternate Games.”Jour-
nal of Economic Theory 67 (2): 497–519.
   Kahneman, Daniel and Amos Tversky. 1979. “Prospect Theory: An Analysis of Decision
Under Risk.”Econometrica 47 (2): 263–91

                                          27
   Kamenica, Emir. 2008. “Contextual Inference in Markets: On the Informational Content
of Product Lines.”American Economic Review 98 (5): 2127–49.
   Khaw, Mel Win, Ziang Li, and Michael Woodford. 2017. “Risk Aversion as a Perceptual
Bias.”Working paper, Columbia University.
   Keren, Gideon, and Peter Roelofsma. 1995. “Immediacy and Certainty in Intertemporal
Choice.”Organizational Behavior and Human Decision Processes 63 (3): 287–97.
   Kreps, David M. 1998. “Anticipated Utility and Dynamic Choice.” In Frontiers of Re-
search in Economic Theory: The Nancy L. Schwartz Memorial Lectures, 1983–1997. Edited
by D. Jacobs, E. Kalai and M. Kamien. Econometric Society Monographs, 242–74. Cam-
bridge, UK: Cambridge University.
   Laibson, David. 1997. “Golden Eggs and Hyperbolic Discounting.”The Quarterly Jour-
nal of Economics 112 (2): 443–77.
   Lang, John C, Daniel M Abrams, and Hans De Sterck. 2015. “The In‡uence of Societal
Individualism on a Century of Tobacco Use: Modelling the Prevalence of Smoking.” BMC
Public Health 15 (1280).
   Lockwood, Benjamin. 2016. “Optimal Income Taxation with Present Bias.” Working
paper, University of Pennsylvania.
   Matejka, Filip, Jakub Steiner and Colin Stewart. Forthcoming. “Rational Inattention
Dynamics: Inertia and Delay in Decision-Making.”Econometrica.
   McGuire, Joseph T., and Joseph W. Kable. 2013. “Rational Temporal Predictions Can
Underlie Apparent Failures to Delay Grati…cation.”Psychological Review 120 (2): 395–410.
   McGuire, Joseph T., and Joseph W. Kable. 2012. “Decision Makers Calibrate Behavioral
Persistence on the Basis of Time-Interval Experience.”Cognition 124 (2): 216–26.
   Mullainathan, Sendhil, and Eldar Sha…r, 2013. Scarcity: Why Having Too Little Means
So Much, New York: Henry Holt & Company.
   O’Donoghue, Ted, and Matthew Rabin. “Doing It Now or Later.” American Economic
Review (1999): 103–24.
   O’Donoghue, Ted, and Matthew Rabin. 1999. “Incentives for Procrastinators.” The
Quarterly Journal of Economics 114 (3): 769-816.
   O’Donoghue, Ted, and Matthew Rabin. 2011. “Choice and Procrastination.”The Quar-
terly Journal of Economics 116 (1): 121–60.
   Peters, Jan, and Christian Büchel. 2010. “Episodic Future Thinking Reduces Reward

                                              28
Delay Discounting Through an Enhancement of Prefrontal-Mediotemporal Interactions.”
Neuron 66 (1): 138–48.
   Prelec, Drazen and George Loewenstein. 1991. “Decision Making over Time and Under
Uncertainty: A Common Approach.”Management science, 37 (7): 770–86.
   Quiggin, John, and John Horowitz. 1995. “Time and Risk.”Journal of Risk and Uncer-
tainty 10 (1): 37-55.
   Rai¤a, Howard and Robert Schlaifer. 1961. Applied Statistical Decision Theory. Cam-
bridge, MA: Harvard University and MIT Press.
   Schilbach, Frank, Heather Scho…eld, and Sendhil Mullainathan. 2016. “The Psycho-
logical Lives of the Poor.” American Economic Review: Papers and Proceedings 106 (5):
435–40.
   Schwartzstein, Joshua. 2014. “Selective Attention and Learning.” Journal of the Euro-
pean Economic Association 12 (6): 1423–52.
   Sims, Christopher A. 2003. “Implications of Rational Inattention.”Journal of Monetary
Economics 50 (3): 665–690.
   Sozou, Peter D. 1998. “On Hyperbolic Discounting and Uncertain Hazard Rates.” Pro-
ceedings of the Royal Society B: Biological Sciences 265 (1409): 2015–20.
   Spears, Dean. 2012. “Cognitive Limits, Apparent Impatience, and Monthly Consumption
Cycles: Theory and Evidence from the South Africa Pension.”Working paper.
   Sprenger, Charles, Sally Sado¤ and Anya Samek. 2014. “Dynamic Inconsistency in
Food Choice: Experimental Evidence from a Food Desert.” Working paper, University of
California San Diego.
   Steele, Claude M., and Robert A. Josephs. 1990. “Alcohol Myopia: Its Prized and
Dangerous E¤ects.”American Psychologist 45 (8): 921–33.
   Steiner, Jakub, and Colin Stewart. 2016. “Perceiving Prospects Properly.” American
Economic Review 106 (7): 1601–31.
   Taubinsky, Dmitry and Alex Rees-Jones. 2016a. “Attention Variation and Welfare:
Theory and Evidence from a Tax Salience Experiment.”Working Paper no. 22545, NBER,
Cambridge, MA.
   Taubinsky, Dmitry and Alex Rees-Jones. 2016b. “Heuristic Perceptions of the Income
Tax: Evidence and Implications.”Working Paper no. 22884, NBER, Cambridge, MA.
   Weber, Bethany J., and Gretchen B. Chapman. 2005. “The Combined E¤ects of Risk and

                                             29
Time on Choice: Does Uncertainty Eliminate the Immediacy E¤ect? Does Delay Eliminate
the Certainty E¤ect?”Organizational Behavior and Human Decision Processes 96 (2): 104–
18.
      Weber, Elke U, Sharoni Sha…r, and Ann-Renee Blais. 2004. “Predicting Risk Sensitivity
in Humans and Lower Animals: Risk as Variance or Coe¢ cient of Variation.”Psychological
Review 111 (2): 430–45.
      Weitzman, Martin. 2001. “Gamma Discounting.” American Economic Review. 91 (1):
260–71.
      Woodford, Michael. 2009. “Information-Constrained State-Dependent Pricing.”Journal
of Monetary Economics 56 (S): 100–24.
      Yaari, Menahem E. 1965. “Uncertain Lifetime, Life Insurance, and the Theory of Con-
sumer.”Review of Economic Studies, 32 (1): 37–50.




                                             30
7     Appendix: Proofs and Complements

7.1        Omitted Proofs

    Proof of Proposition 1 This proof is very elementary, but for completeness we provide
its calculations. We normalize            = 0 without loss of generality (for instance, by considering
u0t = ut      and s0t = st       ). It is well-known that ut j st is Gaussian distributed, and can
be represented:
                                                         ut = st +           t                                   (13)

for some , and some Gaussian variable                     t     independent of st , so that E [st t ] = 0. Multiplying
(13) by st on both sides and taking the expectations gives: E [ut st ] = E [s2t ], i.e.

                     E [ut st ]   E [ut (ut + "t )]      E [u2t ]
                    =           =                   =                as E [ut "t ] = 0
                      E [s2t ]    E (ut + "t )2       E [u2t + "2t ]
                          2
                                      1
                    = 2 u 2 =            2  = D (t) .
                      u + "t       1 + "2t
                                                     u



    Next, taking the variance of both sides of (13), we have

                                                 2               2 2
                                                 u   =             s   + var( t )

                                  2        2             2                           2        2
as cov (st ; t ) = 0 and with     s   =    u     +       "t .    So, using           s   =    u,


                                            2            2 2             2       2                     2
                            var( t ) =      u              s      =      u       u   = (1          )   u:



                                          2
Hence, ut j st    N ( st ; (1         )   u ),   as announced.


    Proof of Proposition 3 It is a corollary of Proposition 5 (for Assumption 1) and
Proposition 6 (for Assumption 2) below.


    Proof of Proposition 4 Given our assumptions, the agent at time t will want to
maximize                         "T t                           #
                                  X                                                  X
                                                                                     T t
                     max E                u(at+ ) j s = max                                  D ( ) st+ (at+ )
                   (at+ )    0                                         (at+ )    0
                                   =0                                                =0




                                                                    31
where s = (st (y) ; :::; st+ (y))y2A . Assumption A1-A3 allows us to remove expected values.
For our representative agent, we have st+ (a) = ut+ (a). Hence, this representative agent
maximizes at time t:
                                                                X
                                                                T t
                                                   max                D ( ) u(at+ ):
                                                 (at+ )     0
                                                                =0


7.2     Complements to the Continuous Actions Case

Here are some complements to Section 4. To simplify the notations, we set                                   =   "t .



7.2.1    Result for the Wiener case

Proposition 5 (Bayesian updating with functions) Under Assumption 1, we have


                                                     E [u (a) j s] = s (a)

                 1
with    =   1+   2    2   . This means that we can do Bayesian updating on this space of functions.
                 "t = u




   Proof. Take the increments:


                                                 ds (a) = du (a) + dW (a)


The key observation is that the ds (a)’s are all Gaussians innovations, independent of the
value of the functions are other points y 6= a. So, by the formulation for Gaussian updating
we used before:
                                             E [du (a) j ds (a)] = ds (a)

                 1
with    =   1+   2= 2     . Next, because the du (a) and dW (a) are independent,
                    u




                                  E [du (a) j s] = E [du (a) j ds (a)] = ds (a) :                                      (14)


   Next, the behavior at 0 needs a special treatment.                                    Because s (0) = u (0) +         0,
                                                            1                   1
E [u (0) j s (0)] =        0 s (0),   with   0   =        var(    0)
                                                                       =        2 2   = . Then, E [u (0) j s (0)] = s (0),
                                                     1+    var(u0 )        1+   2 2
                                                                                  u
and by independence:
                                                     E [u (0) j s] = s (0) :                                           (15)



                                                                      32
Hence, integrating from 0 to a, we get
                                            Z       a                                             Z    a
             E [u (a) j s] = E u (0) +                      du (y) j s = E [u (0) j s] +                   E [du (y) j s]
                                                   y=0                                             y=0
                                     Z         a
                           = s (0) +                ds (y)
                                           0

                          = s (a) :


7.2.2       Polynomial utility

Here we provide assumptions that are a little more elementary, but apply only when the
utility function u (a) is a polynomial in a. For instance, we want to capture that u (a) =
b0 + b1 a + b2 a2 with unknown coe¢ cients bi , that the agent wants to learn from noisy signals.


       Assumptions for the polynomial utility case We shall use the Legendre Pi (a)
polynomials as a basis, as they are more convenient than the plain monomials ai . We have
for instance:12

                                                                           1                                 1
         P0 (a) = 1;       P1 (a) = a;               P2 (a) =                3a2      1 ;      P3 (a) =        5a3     3a :
                                                                           2                                 2

Using the product                                                Z    1
                                         hf j gi :=                        f (a) g (a) da;                                    (16)
                                                                      1
                                                               1
we have the standard result: hPi j Pj i =                     i+ 21
                                                                      1i=j . So we de…ne qi to be a rescaled version of
the standard Legendre polynomial:
                                                                  r
                                                                          1
                                               qi (a) :=               i + Pi (a) ;                                           (17)
                                                                          2

so that
                                                         hqi j qj i = 1i=j .                                                  (18)

Polynomial qi has degree i, and the qi ’s form an orthogonal basis for polynomial functions.
       We can now state our assumption.

                                                        h                  i
  12                                      1 di                         i
       More generally we have Pi (a) =   2i i! dai          a2    1            by Rodrigues’formula.




                                                                      33
Assumption 2 (Utility function as drawn from a random distribution on polynomial basis)
We decompose the true utility function u (a) as:

                                                                        X
                                                                        1
                                                              u (a) =           fi Qi (a)                                          (19)
                                                                        i= 1

                                                                         Ra
where Q      1   (a)         1 and for i                 0, Qi (a) =        0
                                                                                qi (y) dy, where qi (y) is the i-th normalized
Legrendre polynomial (17). We assume that a …nite subset I such that coe¢ cients ffi gi2I
                                                                                                                 2
are nonzero and that the fi for i 2 I are i.i.d. and follow a N (0;                                              u)   distribution. Also
           2                 2 2
assume     f     1
                     =         u.


      We note that, in the limit where all coe¢ cients are non-zero, we get the “Wiener”case.


      Result We prove a more general proposition.

Proposition 6 Suppose that coe¢ cients fi are drawn from the Gaussian N 0; f2i , and
                                                                         h                        i
jointly Gaussian and uncorrelated. Then, the posterior E [u (a) j s] := E u (a) j (s (y))y2[ 1;1]
is:
                                                                        X
                                                                        1
                                                E [u (a) j s] =                 E [fi j s] Qi (a)
                                                                        i= 1

where, for i             1
                                                                                      Z   1
                                       E [fi j s] =           i hqi j dsi =       i           qi (a) ds (a)
                                                                                       a= 1
                                                                        2
                                                     i   = 1= 1 +           =var (fi )

                                                                                2 2
while E [f       1   j s] =         1 s (0)   with        1   = 1= (1 +               =var (f 1 )). This implies that the average
posterior is:
                                                                                          X
                                                                                          1
                                         u (a) := E [[u (a) j s] j f ] =                       i fi Qi   (a) :                     (20)
                                                                                       i= 1

                                                                                                                                    2
      In particular, take the case of ‡at priors of Assumption 2, and call                                              = 1= 1 +    2   .
                                                                                                                                    u

Then,
                                                                 u (a) = u (a)                                                     (21)

i.e. we obtain uniform dampening.



                                                                         34
   Proof of Proposition 6 Suppose that we have a function u (a), and we observe, as in
(10),
                                         s (a) = u (a) + W (a) +                    0                               (22)

where W (a) is a Brownian motion and ~0 =                                0   is a Gaussian variable of mean zero.
Di¤erentiate:


                                 ds (a) = u0 (a) da + dW (a)
                                               X
                                               1                         X
                                                                         1
                                     0
                                 u (a) =                fj Q0j   (a) =         fj qj (a) :
                                               j= 1                      j=0


Hence:
                                                  X
                                                  1
                                     ds (a) =           fj qj (a) da + dW (a):
                                                  j=0

   The agent wants to infer u given s, i.e. f given ds (we consider the intercept u (0) at the
end). Multiplying the previous equation by qi (a) and integrating between                                 1 and 1 gives:


                                      Si := hqi j dsi                                                               (23)
                                           X
                                         =      fj hqi j qj i + hqi j dW i
                                              j

                                           = fi + hqi j dW i


because of (18).
   Hence we can write the signal Si := hqi j dsi as


                                                   Si = fi + "i                                                     (24)

                          R1
with "i := hqi j dW i =         q
                               1 i
                                     (a) dWa satis…es E ["i ] = 0. In addition

                                 Z                          Z                           Z
             E ["i "j ] = E              qi (a) dWa              qj (a) dWa         =        qi (a) qj (a) da

                      = 1i=j :


   Hence, the signal-extraction problem E [fi j s] is quite simple, as only Si is informative




                                                             35
about fi : E [fi j s] = E [fi j Si ]. Given (24),


                                    E [fi j s] =              i Si                                                                      (25)
                                                                                2
                                                   i   = 1= 1 +                     =var (fi ) :                                        (26)


   Hence, we have

                                                                  X
                                                                  1
                                            0
                                    E [u (a) j s] =                     E [fi j s] qi (a)
                                                                  i=0
                                                                  X
                                                                  1
                                                              =         E [fi j s] Q0i (a)
                                                                  i=0


   We next study the intercept in (22), u (0). Given s (0) = u (0) +                                                 0    and u (0) = f 1 ,


                        E [u (0) j s] = E [f              1   j s (0)] =                  1 s (0)   =     1S 1



                                                       2 2
where S   1   := s (0) and   1   = 1= (1 +                    =var (f 1 )). Integrating,
                                                                            Z       a
                    E [u (a) j s] = E [u (0) j s] + E                                   u0 (b) db j s
                                                                             0
                                                              X
                                                              1                                 X
                                                                                                1
                                    =           1S 1   +              i Si Qi           (a) =           i Si Qi   (a) :
                                                              i=0                               i= 1


   In addition, the average perception is:

                                                                            X
                                                                            1
                        u (a) := E [[u (a) j s] j u] =                                    i E [Si   j f ] Qi (a)
                                                                            i= 1
                                    X
                                    1
                                =               i fi Qi   (a)                                                                           (27)
                                    i= 1


   If we assume a “‡at” prior of Assumption 2, where var (fi ) is independent of i (if
var (fi ) > 0), we have for i       0

                                                                  1                        1
                                        i   =      =                    2           =           2   :
                                                          1+      var(fi )
                                                                                         1+     2
                                                                                                u




                                                                  36
                     2           2 2
Furthermore, as      f   1
                             =     u,
                                                                 1
                                                 1   =               2 2        = :
                                                         1+   var(f        1)


Hence,   i   =   for all i        1, and (27) implies:

                                        X
                                        1                            X
                                                                     1
                             u (a) =          i fi Q i   (a) =                  fi Qi (a) = u (a) :
                                       i= 1                          i= 1




                                                              37
