                                 NBER WORKING PAPER SERIES




     VALUE-ADDED TO WHAT? HOW A CEILING IN THE TESTING INSTRUMENT
                 INFLUENCES VALUE-ADDED ESTIMATION

                                               Cory Koedel
                                               Julian Betts

                                          Working Paper 14778
                                  http://www.nber.org/papers/w14778


                       NATIONAL BUREAU OF ECONOMIC RESEARCH
                                1050 Massachusetts Avenue
                                  Cambridge, MA 02138
                                      March 2009




The authors thank Andrew Zau and many administrators at San Diego Unified School District, in particular
Karen Bachofer and Peter Bell, for helpful conversations and assistance with data issues. We also
thank Yixiao Sun, Julie Cullen, Nora Gordon and Dale Ballou for their useful comments and suggestions,
and the Spencer Foundation and the National Center for Performance Incentives for research support.
The underlying project that provided the data for this study has been funded by a number of organizations
including The William and Flora Hewlett Foundation, the Public Policy Institute of California, The
Bill and Melinda Gates Foundation, the Atlantic Philanthropies and the Girard Foundation. None of
these entities has funded the specific research described here, but we warmly acknowledge their contributions
to the work needed to create the database underlying the research. The views expressed herein are
those of the author(s) and do not necessarily reflect the views of the National Bureau of Economic
Research.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2009 by Cory Koedel and Julian Betts. All rights reserved. Short sections of text, not to exceed
two paragraphs, may be quoted without explicit permission provided that full credit, including © notice,
is given to the source.
Value-Added to What? How a Ceiling in the Testing Instrument Influences Value-Added
Estimation
Cory Koedel and Julian Betts
NBER Working Paper No. 14778
March 2009
JEL No. I2,I21,I22,J08,J33,J45

                                              ABSTRACT

Value-added measures of teacher quality may be sensitive to the quantitative properties of the student
tests upon which they are based. This paper focuses on the sensitivity of value-added to test-score-ceiling
effects. Test-score ceilings are increasingly common in testing instruments across the country as education
policy continues to emphasize proficiency-based reform. Encouragingly, we show that over a wide
range of test-score-ceiling severity, teachers' value-added estimates are only negligibly influenced
by ceiling effects. However, as ceiling conditions approach those found in minimum-competency
testing environments, value-added results are significantly altered. We suggest a simple statistical
check for ceiling effects.


Cory Koedel
Dept. of Economics
University of Missouri
118 Professional Building
Columbia, MO 65211
koedelc@missouri.edu

Julian Betts
Department of Economics, 0508
University of California, San Diego
9500 Gilman Drive
La Jolla, CA 92093-0508
and NBER
jbetts@ucsd.edu
        Teacher performance pay is quickly gaining momentum in the United States. In fact,

some districts, and even entire states, are already implementing performance-pay programs for

teachers that involve sizeable public expenditures. For example, the Texas Governor’s Educator

Excellence Award Programs (GEEAP) allot a large fraction of their combined 330 million dollar

annual budget to directly reward classroom teachers based on performance (Podgursky and

Springer, 2007).

        The aspect of teacher performance that has received the most attention from

policymakers of late, and is perhaps the most contentious, is value-added to students’ test scores.

While the literature overwhelmingly indicates that there are important differences in teacher

quality measured by value-added, there is little consensus on the best approach for estimating

value-added. Furthermore, there is ample evidence that value-added measures of teacher quality

are noisy, which creates some concern about the feasibility of using value-added for large-scale

teacher evaluation.1      In addition to these unresolved issues, value-added estimates may be

sensitive to the quantitative properties of the testing instruments upon which they are based.

        This paper evaluates the sensitivity of value-added to a particularly relevant testing-

instrument property – the severity of test-score-ceiling effects. We refer to the tendency for

gains in a student’s test score to be smaller if the student’s initial score is toward the top end of

the distribution, simply because the student has little room for improvement given the difficulty

level of the test, as a “ceiling effect”. Ceiling effects will be most pronounced in minimum-

competency or proficiency-based tests, which are being used increasingly across the United

States. For example, 22 states nationwide use high school exit exams that are typically pitched at

1
  See, for example, Aaronson, Barrow and Sander (2007), Hanushek, Kain, O’Brien and Rivkin (2005), Koedel and
Betts (2007) and Rockoff (2004). In addition, Rothstein (2008) shows that value-added estimates may be biased by
student-teacher sorting.

                                                       1
a middle-school or lower high-school level.2 Furthermore, because federal No Child Left Behind

(NCLB) legislation focuses largely on proficiency, mainstream proficiency-based testing is also

becoming increasingly common.

           The increased focus on proficiency in education coincides with the growing interest from

researchers and policymakers in value-added as a tool for measuring teacher performance. The

impending collision of ceiling-affected testing instruments with value-added-based teacher

evaluations motivates our analysis. Do ceiling effects influence value-added estimation? If so,

how important are ceiling effects and how severe must they be to significantly alter value-added

results?

           We answer these questions using a testing instrument where there is no evidence of a

test-score ceiling. Starting with our no-ceiling baseline, we simulate test-score ceilings that vary

in severity and evaluate their effects on teacher value-added.                    Our findings are generally

encouraging - over a wide range of test-score-ceiling severity we find that value-added estimates

are roughly impervious to ceiling effects. However, ceiling conditions approaching the severity

of those found in minimum-competency testing environments noticeably alter value-added

results.

       I. Test-Score Ceilings: Introduction and Measurement

           Test-score ceilings structurally restrict students’ test-score gains as test-score levels rise.

Because a test-score ceiling directly influences the tool by which value-added is measured, it is

intuitive that it will influence results. For example, consider a testing instrument where a large

fraction of the student population is at or near the maximum possible score. Teachers teaching

these students will have little opportunity to add value to test scores. Furthermore, they are



2
    The nationwide count applies to 2006 and was calculated based on information in Warren (2007).

                                                          2
likely to use advanced curricula that focus at least partly on material that goes beyond the scope

of the test, making their evaluations based on the test uninformative.

        In practice it might be quite important whether a district uses a norm-referenced or a

criterion-referenced test for the purpose of evaluating teaching effectiveness. A norm-referenced

test is a standardized test that is meant to estimate where a student ranks against the test-score

distribution of the reference group, typically the national student population. Such a test, if well-

designed, should exhibit few ceiling effects because it must include questions with a range of

difficulty so that distinctions can be made among students throughout the test-score distribution.

Such tests have been in use for many decades.

        More recently, partly as a consequence of NCLB, many states are using testing systems

designed to measure student understanding of the content standards set by the state’s Department

of Education. We speculate that these “criterion-referenced” tests are more likely to exhibit

ceiling effects, particularly when a state exam is intended, either explicitly or implicitly, to serve

as a “minimum-competency” test. For example, in Mississippi the state-level test appears to be

targeted at a fairly low level. In 2006-07, 90 percent of fourth-grade students scored at or above

the “proficient” level in reading on the state-level Mississippi Curriculum Test (MCT).

However, just 19 percent of these students scored at or above the proficient level on the National

Assessment of Education Progress (NAEP).3,4

        One way to evaluate the impact of ceiling effects on teacher value-added would be to find

a population of students that had been tested in several consecutive years using two testing


3
 From the US Department of Education, Mapping Mississippi’s Educational Progress 2008.
4
  Cullen and Loeb (2004) illustrate another source of ceiling effects that is directly associated with NCLB –
reporting requirements that require states to document the percentage of students who are “proficient”. Their Figure
12c provides a graphical representation of the mechanical relationship between underlying proficiency levels and
growth in proficiency. Clearly, if value-added were estimated based on simple pass-fail measures of student
achievement, as emphasized by NCLB, ceiling effects would be severe.

                                                         3
systems – one that lacked a ceiling effect and another that suffered from a ceiling effect.

However, it is likely that the different tests in such a scenario would also differ in terms of

content, confounding the ceiling effect.        A second approach is to use a test that can be

demonstrated not to suffer from ceiling effects, and then to simulate test-score ceiling effects

using that instrument. This creates a counterfactual of what would have happened had the test

been right-censored. We adopt this approach by using Stanford 9 math test scores for fourth

grade students in the San Diego Unified School District. The Stanford 9 is a nationally norm-

referenced test. For the population we study we find no evidence of a ceiling effect (see below).

It thus provides a way of comparing measures of teacher value-added with and without a test-

score ceiling.

       The first step in our analysis is to provide a reliable measure of test-score ceiling severity.

An intuitive approach would be to evaluate the strength of the negative relationship between test-

score levels and subsequent test-score gains. However, this approach is problematic because a

negative relationship will exist due to regression to the mean even in the absence of a test-score

ceiling. Furthermore, in cases where a test-score ceiling does exist, there is no obvious way to

dissect the negative relationship between test-score levels and test-score gains to isolate the

ceiling effect. As an alternative, we propose that the distribution of students’ test scores can be

used to measure test-score-ceiling severity. Specifically, we can use the degree of negative

skewness in the test-score distribution as originally suggested by Roberts (1978). We define

                                       E ( x − E ( x ))
                                                          3
                                                                  μ3
skewness as the sample analog of                              ≡      , where μ3 is the third moment about
                                                      2 3/2
                                    ⎡ E ( x − E ( x )) ⎤          σ3
                                    ⎢⎣                 ⎥⎦

the mean and σ is the standard deviation.            Under the assumption that underlying student

achievement in the population is symmetrically distributed, skewness provides an intuitive and

                                                    4
straightforward measure of test-score ceiling severity. In Section VII, we provide suggestive

(although not exhaustive) evidence that skewness is a robust measure of ceiling severity.

        Figure 1 displays the frequency distributions of students’ lagged (grade 3) and current

(grade 4) math test scores from our data, gathered from San Diego Unified School District. As

mentioned above, there is no evidence of a test-score ceiling. In fact, the test-score distributions

from our sample are skewed mildly positively. The figure shows kernel-density plots of the

distributions of actual scores contrasted with normally-distributed overlays. The skewness in the

lagged and current-score distributions in our data are 0.25 and 0.17, respectively. Notice that

although both of these distributions are skewed slightly positively, they both closely mirror their

normally distributed analogs.

        In our test-score-ceiling simulations, what is the relevant range of skewness to consider?

We answer this question using two large-scale, state-level tests: (1) the Texas Assessment of

Academic Skills (TAAS) and (2) the Florida Comprehensive Assessment Test (FCAT).5 The

TAAS was administered in Texas from 1991 to 2003 and prior to 1991 was known as the Texas

Educational Assessment of Minimum Skills. The minimum-competency-based design of the

TAAS makes it a useful test upon which to base our most severe test-score-ceiling simulations.

The FCAT was first administered in 1998 in Florida and continues to serve as the state-level

standardized test there.

        We simulate test-score-ceiling conditions based on the skewness in the test-score

distributions of the math portions of the TAAS and FCAT from 2002 and 2007, respectively.

Figure 2 shows kernel-density plots of third and fourth-grade mathematics scores on the TAAS

compared to normally-distributed overlays based on 2002 test scores (statewide). The skewness


5
 Statewide distributions of test scores for the TAAS were provided online by the Texas Education Agency
(http://www.tea.state.tx.us). FCAT scores were provided by the Florida Department of Education.

                                                       5
in these score distributions is large and negative, at -1.60 and -2.08, respectively. Similarly, the

top panel of Figure 3 shows kernel density plots of third and fourth-grade mathematics scores on

the 2007 FCAT (statewide). The skewness in these score distributions is also negative but much

milder, at -0.46 and -0.55. Finally, the bottom panel of Figure 3 shows the distributions of scores

for ninth and tenth-grade students on the FCAT in 2007 where the skewness in the test-score

distributions becomes increasingly negative. The ninth and tenth-grade score distributions from

the FCAT have skewness of -0.94 and -1.99, respectively.6

         Starting with our no-ceiling baseline, we create counterfactual testing environments

where students’ scores are impeded by test-score ceilings of varying severity. Our most severe

ceiling simulation is designed to mimic the testing conditions from the fourth-grade TAAS. For

simplicity, we simulate what we will refer to as “hard” test-score ceilings, where students’ scores

are restricted at a specific maximum score. An alternative would be to simulate “soft” test-score

ceilings that restrict student performance throughout the test-score distribution. For example,

students’ scores might taper off as they approach a maximum score. Soft test-score ceilings

appear to characterize more accurately the true distributions of test scores in Figures 2 and 3.

However, there are literally an infinite number of possible soft-ceiling structures that could

generate the observed skewness in the TAAS and FCAT distributions, making such an analysis

infeasible. Instead, we focus on hard test-score ceilings and compare the results that we obtain

from our simulations to a set of results generated using one possible soft-ceiling structure. This

analysis is detailed in Section VII and suggests that similarly skewed test-score distributions



6
  Students in Florida must pass the math portion of the tenth-grade FCAT to receive a high-school diploma. It is
possible that the exam is targeted at a lower level because of this. Also, students are allowed to take the test more
than once. The distribution of tenth-grade FCAT scores reported in Figure 3 is for all tests taken in 2007 (provided
by the Florida Department of Education), which will include retaken exams. The retaken exams could either
positively or negatively skew the distribution.

                                                          6
have similar implications for value-added results, regardless of whether a hard or soft ceiling

generates the ceiling effect.

       Finally, we distinguish two mechanisms by which test-score ceiling effects will influence

value-added estimation. First, most straightforwardly, ceiling effects represent lost information

about student learning. The more severe is the test-score ceiling, the greater the amount of lost

information. Second, ceiling effects will result in model misspecification. A test-score ceiling is

a data censor, and as such the typical value-added approach is improperly specified in the

presence of a ceiling. In practice this is a non-trivial problem because the underlying data-

censoring structure will be unknown.       Furthermore, the censoring problem is even more

complicated in the value-added framework than in the typical dependent-variable censoring

problem because lagged test scores will also be censored. In the general value-added approach

(where current test scores are regressed on lagged test scores), this means that there will be

censoring of an independent variable in addition to the censoring of the dependent variable.

Converting to gainscores does not circumvent the problem because censoring will be ill-defined -

censored gainscores will have zero or near zero gains, but non-censored scores can also have

zero, near zero, or even negative gains.

       The current state of the data-censoring literature in econometrics and statistics is such

that there is no solution to the data-censoring problem in this context. Therefore, distortionary

test-score ceiling effects can be thought of as the product of both of these problems – lost

information and model misspecification. For this reason, our primary results are from standard

value-added models estimated by least squares. In Section VIII, we further consider the data-

censoring problem and provide some evidence on the extent to which model misspecification

alone drives our ceiling-effect results.


                                                7
   II. Background

       Only a fraction of the recent studies measuring teacher value-added have considered the

potential importance of test-score ceiling effects. Furthermore, none have explicitly evaluated

the direct implications of ceiling effects for value-added results. Hanushek, Kain, O’Brien and

Rivkin (2005) provide the most provocative documentation of ceiling effects in the recent value-

added literature. These authors estimate value-added using the TAAS where scale scores are

such that a gain of zero implies “typical” progress. They divide the exam into ten equal test-

score intervals and assign each student to one of ten bins based on his or her period-(t-1) test-

score level. There is a strong negative relationship between students’ period-(t-1) test-score

levels and period-t gains, which is suggestive of a ceiling effect (although mean reversion could

also explain the documented relationship). More importantly, approximately two-thirds of the

students in their sample are assigned to a bin where the average test-score gain is negative.

Where “typical” progress is purported to correspond to a gain of zero, and in the absence of a

ceiling effect, mean reversion in both directions would suggest that approximately equal shares

of students should experience positive and negative gains. That such a large fraction of students

show negative gains suggests that ceiling effects are an important concern. Their analysis is one

of only a few that carefully considers test-score-ceiling effects, although a direct analysis of

ceiling effects is beyond the scope of their study.

       Of the other recent test-score-based studies of teacher quality, there is little mention of

ceiling effects. Koedel (2008) and Koedel and Betts (2007) acknowledge the potential for test-

score ceiling effects and report information on the relationship between students’ gains and

lagged test-score levels. Aaronson, Barrow and Sander (2007) measure value-added using two

tests that differ substantially in terms of the distributions of scores, which they thoroughly


                                                  8
document, but they do not explicitly consider ceiling effects.                      Rockoff (2004), who estimates

teacher effects outside of the value-added framework, reports that 3 to 6 percent of the students

in his sample attain the maximum possible score but he does not go into further detail.

Lockwood et al. (2007) show that teacher effects are quite sensitive to different testing

instruments but they do not consider the extent to which ceiling effects might be driving their

results. Nye, Konstantopoulos and Hedges (2005) do not discuss test-score-ceiling effects.

    III. Value-Added Models

        We    estimate    teacher      value-added           using     three        different   student-achievement

specifications. Each specification implies tradeoffs in estimation. We focus on the general

value-added model where current test scores are regressed on lagged test scores. It is somewhat

common in the literature to use a specific form of the value-added model, the gainscore model,

where the coefficient on the lagged test score is forced to one and the lagged-score term is

moved to the left side of the equation. Although we do not present results from gainscore

models, our findings are nearly identical using the gainscore framework. Results from the

gainscore analogs to the below specifications are available from the authors upon request.

        The first model that we consider, and the simplest, is a basic value-added model that

allows for the comparison of teacher effects across schools:

                                    Yit = φt + Yi (t −1)φ1 + X itφ2 + Titθ + ε it                               (1)

In (1), Yit is the test score for student i in year t, φt is a year-specific intercept, Xit is a vector of

fixed and time-varying student-specific characteristics (see Table 1) and Tit is a vector of teacher

indicator variables where the entry for the teacher who teaches student i in year t is set to one.

The coefficients of interest are in the Jx1 vector of teacher effects, θ.




                                                         9
        We refer to equation (1) as the basic model. The most obvious omission from the model

is school-level information, whether in the form of school fixed effects or time-varying controls.

Researchers have generally incorporated this information because of concerns that students and

teachers are sorting into schools non-randomly. This sorting, along with the direct effects of

school-level inputs on student achievement (peers, for example), will generate omitted-variables

bias in the value-added results for teachers in equation (1).

        While the omitted-variables-bias concern is certainly relevant, any model that includes

school-level information will not allow for a true comparison of teacher effectiveness across

schools. For example, if school fixed effects are included in equation (1) then each teacher’s

comparison group will be restricted to the set of teachers who teach at the same school.

Furthermore, even in the absence of school fixed effects, the inclusion of school-level controls

will restrict teachers’ comparison groups to some extent because teachers may sort themselves

based on school-level characteristics. If this is the case, controls meant to capture school quality

will also partly capture school-level teacher quality, limiting inference from across-school

comparisons of teachers.

        For most researchers, concerns about omitted-variables bias dominate concerns about

shrinking teacher comparison groups. This leads to the second model that we consider, the

within-schools model, which is more commonly estimated in the literature and includes time-

varying school-level covariates and school fixed effects.7

                          Yit = β t + Yi (t −1) β1 + X it β 2 + Sit β 3 + Tit γ +ν it                    (2)




7
  Although teacher effectiveness cannot be compared across schools straightforwardly using value-added estimates
from (2), this may be acceptable from a policy perspective. For example, policymakers may wish to identify the
best and worst teachers on a school-by-school basis regardless of any teacher sorting across schools.

                                                             10
In (2), Sit is a vector that includes school indicator variables and time-varying school-level

information for the school attended by student i in year t. The controls in the vector Sit are

detailed in Table 1. The benefit of including school-level information is a reduction in omitted-

variables bias, including sorting bias generated by students and teachers selecting into specific

schools.

          Finally, we incorporate student fixed effects into the student achievement specification.

This approach is suggested by Harris and Sass (2006), Koedel (2008) and Koedel and Betts

(2007):

                            Yit = α i + α t + Yi (t −1)α1 + X itα 2 + Sitα 3 + Titδ + uit                        (3)

In going from equation (2) to equation (3) we add the student fixed effects, αi. The inclusion of

the student fixed effects also limits the entries in the vector Xit to include only time-varying

student characteristics. The benefit of the within-students approach is that teacher effects will

not be biased by within-school student sorting across teachers based on time-invariant student

characteristics (such as ability, parental involvement, etc.). However, again there are tradeoffs.

Equation (3) further narrows teachers’ comparison groups to those with whom they share

students. Thus, identification comes from comparing test-score gains for individual students

when they were in the third and fourth grades. In addition, the incorporation of the student fixed

effects makes the model considerably noisier.8 Finally, the inclusion of the student fixed effects

restricts the size of the student population that can be considered because a student record must

contain at least three contiguous test scores, instead of just two, to be included in the analysis.9



8
  In fact, a test for the statistical significance of the student fixed effects in equation (3) fails to reject the null
hypothesis of joint insignificance. However, the test is of low power given the large-N, small-T panel dataset
structure (typical of most value-added analyses), limiting inference.
9
  Equation (3) also introduces a potential endogeneity concern if teacher assignments are correlated with the time-
varying error-term component across years. See Rothstein (2008) and Koedel and Betts (2008).

                                                             11
        Despite these concerns, econometric theory suggests that student fixed effects will be an

effective way to remove within-school sorting bias as long as students and teachers are sorting

based on time-invariant characteristics.            We estimate the within-students model by first-

differencing equation (3) and instrumenting for students’ lagged test-score gains with their

second-lagged levels. This general approach was developed by Anderson and Hsiao (1981) and

has been recently used by Harris and Sass (2006), Koedel (2008) and Koedel and Betts (2007) to

estimate teacher value-added.10

        Two key issues distinguish the within-students model from the other models that we

consider. First, to completely first-difference equation (3) we must incorporate students’ lagged

teacher assignments, which will appear in the period-(t-1) version of equation (3). That is, we

are comparing the effectiveness of students’ current and previous-year teachers. Second, the

requirement that each student record contain three contiguous test scores in the within-students

model not only limits the sample size overall but also restricts the student population to less-

transient students.      Because these students tend to be higher achievers (see, for example,

Rumberger and Larson, 1998; Ingersoll, Scamman and Eckerling, 1989), a given test-score

ceiling will have a stronger effect on the restricted student sample. This issue will be revisited

when we present our results.

     IV. Data

        We evaluate ceiling effects using administrative data from fourth-grade students in San

Diego (San Diego Unified School District) who started the fourth grade between 1998-1999 and

2001-2002. We chose the fourth grade because it is an elementary-level grade (so that each

student is linked to just one teacher) and because our student-fixed-effects model requires at least


10
  Although all three of these studies use the same basic methodology, Harris and Sass (2006) estimate their model
using GMM while Koedel (2008) and Koedel and Betts (2007) use 2SLS. We use 2SLS here.

                                                        12
three contiguous test-score records per student (students are first tested in the second grade). The

standardized test that we use to measure student achievement is the Stanford 9 mathematics test.

The Stanford 9 is designed to be vertically scaled such that a one-point gain in student

performance at any point in the schooling process is meant to correspond to the same amount of

learning. As discussed in Section I, there is no evidence of a ceiling effect in the test-score data.

         Students who have fourth grade test scores and lagged test scores are included in our

analysis. In our student-fixed-effects models, we also require students to have second-lagged

test scores. For each model, we estimate value-added for teachers who teach at least 20 students

across the data panel and restrict our student sample to the set of students taught by these

teachers.11 In the models without student fixed effects, we evaluate test-score records for 30,354

students taught by 596 teachers. Our sample size falls to 15,592 students taught by 389 teachers

in the student-fixed-effects model. The large reduction in sample size is the result of (1) the

requirement of three contiguous test-score records per student instead of just two, which in

addition to removing more transient students also removes one year-cohort of students because

we do not have test-score data prior to 1997-1998 (that is, students in the fourth grade in 1998-

1999 can have lagged scores but not second-lagged scores) and (2) requiring the remaining

students be assigned to one of the 389 fourth-grade teachers who teach at least 20 students with

three test-score records or more.12 We include students who repeat the fourth grade because our

objective is to inform policy and it is unlikely that grade repeaters would be excluded from

teacher evaluations in practice (because of moral hazard concerns). In our original sample of

30,354 students with current and lagged test-score records, just 199 are grade repeaters.


11
   This restriction is imposed because of concerns about sampling variation (see Kane and Staiger, 2002). Our
results are not sensitive to reasonable adjustments to the 20-student threshold.
12
   Only students who repeated the 4th grade in the latter two years of our panel could possibly have had more than
three test-score records. There are 32 students with four test-score records in our dataset.

                                                         13
         The degree of student-teacher sorting will influence the magnitude of test-score-ceiling

effects. At one extreme, random assignment of students to teachers will mitigate ceiling effects

insofar as they determine teacher rankings regardless of which model from Section III is used

(although ceiling effects may still lead to an understatement the importance of teacher quality

overall, and increase the noise associated with value-added estimation).13 At the other, a test-

score ceiling where there is strong student-teacher sorting should lead to a large shift in teacher

rankings based on value-added.14

         One benefit of our analysis is that we can use real student-teacher matches from a real

school district, rather than attempting to simulate student-teacher sorting. This is important

because there is no consensus in the literature as to how students and teachers are actually

assigned to one another, making it impossible to artificially generate student-teacher matches.

However, if parents, students, teachers and administrators in San Diego act similarly to parents,

students, teachers and administrators in other similar school districts, our results will

generalize.15

         We document observable student-teacher sorting in our data by comparing the average

realized within-teacher standard deviation of students’ lagged test scores to analogous measures

based on simulated student-teacher matches that are either randomly generated or perfectly

sorted. This approach follows Aaronson, Barrow and Sander (2007). Table 2 details our results,

which are presented as ratios of the standard deviation of interest to the total within-grade


13
   If within-teacher student samples are small enough, random assignment will not be sufficient to entirely mitigate
ceiling effects on teacher rankings.
14
   In addition to differential student-teacher sorting across districts and schools, there will also be differential sorting
across schooling levels. Ceilings will have larger distortionary effects in higher grade levels if student-teacher
sorting is stronger.
15
   The San Diego Unified School District (SDUSD) is the eighth largest school district in the nation, with
considerable student diversity. The one notable difference between SDUSD and some other districts is that there is
a larger-than-average share of English learners at SDUSD. For basic demographic information about the population
of students and teachers at SDUSD see Betts, Zau and Rice (2003).

                                                            14
standard deviation of the test (calculated based on our student sample). Note that while there

does appear to be some student sorting based on lagged test-score performance, this sorting is

relatively mild.

     V.    Test-Score-Ceiling Simulations and Basic Results

          Our ceiling simulations are based on the distribution of students’ test scores in the fourth

grade. For example, one of our simulations imposes a ceiling where the maximum score is set at

the 95th percentile of the fourth grade test-score distribution. Because the Stanford 9 is vertically

scaled, this ceiling definition spills over to third grade scores. That is, if a student in the third

grade scores above the 95th percentile in the distribution of fourth-grade scores, her third-grade

score is set at the maximum. Our approach generates negative skewness in the test-score

distributions for each grade. The skewness will be more pronounced in the fourth grade relative

to the third grade, and in the third grade relative to the second grade. After imposing each test-

score ceiling on our data, we re-standardize students’ test scores within grades to have a mean of

zero and a variance of one.16,17




16
   An alternative approach would have been to separately set the ceilings in the 2nd, 3rd and 4th grades such that each
ceiling is imposed at the 95th percentile of its respective distribution. However, this approach is inconsistent with
the evidence from the TAAS and, more mildly, the FCAT, where later-grade test-score distributions are more
skewed. We do, however, evaluate such a ceiling structure when we consider the importance of differences in
skewness across grades. See Table 6.
17
   Mechanically, the standardization of scores for each grade has no effect on results from the basic and within-
schools models. In the within-students model, using within-grade standardized scores reduces the distortionary
impacts of the test-score ceilings, albeit mildly. This occurs because the first-differenced test scores in the within-
students models are scaled by their respective standard deviations before differencing, and the standard devation of
fourth-grade scores is smaller than the standard deviation of third-grade scores. This effectively upweights test
scores for students in the current year relative to the lagged year. Because ceilings are defined by skewness in the
test-score distribution, a larger share of students have above average test-scores as ceiling severity increases across
years. In our analysis, the relative up-weighting of these scores generated by the standardization appears to partially
offset the dampening effect of the test-score ceiling. For any test-score distortions characterized by increased
skewness over time (positive or negative), standardization should be somewhat helpful, although we note that the
standardization question is of little practical importance here. Results from models of scaled scores analogous to
those from standardized-scores models in the within-students analysis are available from the authors upon request.
These results suggest even stronger distortionary ceiling effects.

                                                          15
        We create each test-score ceiling by imposing a maximum possible score that we do not

allow students’ scores to exceed. We consider test-score ceilings where the maximum score

ranges from the 97th percentile to the 33rd percentile of the original distribution of fourth-grade

scores. This latter ceiling generates skewness in the current and lagged test-score distributions

comparable to skewness from the third and fourth-grade TAAS exams in 2002, as well as the

ninth and tenth-grade FCAT exams in 2007.18                  For each ceiling simulation, we report the

skewness of the generated test-score distributions.

        Tables 3, 4 and 5, respectively, show results from the three value-added models discussed

above: the basic, within-schools and within-students models. When the ceilings are imposed,

these models are misspecified because the data are censored. Therefore, the results from the

tables document the combined effects of lost information and model misspecification. Again,

because of the complications associated with properly modeling the data censoring given a real-

world test-score ceiling, these results offer the most pragmatic representation of the influence of

ceiling effects. We separately consider the data-censoring problem in more detail in Section

VIII.

        Each column in the tables shows results from a different test-score ceiling. The ceilings

are increasing in severity moving from left to right, and the first column in each table shows

results from our no-ceiling baseline for comparison. The negative skewness measures reported

in rows (2) and (3) of each table (and in row (4) in Table 5) indicate the degree of ceiling

severity.    The eighth column of the tables shows results from our most severely skewed

simulation, which we refer to as the “minimum-competency-equivalent” ceiling.                           For each

ceiling simulation we report three measures of interest in addition to the skewness measures: (1)

18
  The lagged-score distribution is less-skewed than the distribution of 3rd-grade scores on the TAAS and more
skewed than the distribution of 9th-grade scores on the FCAT. We evaluate the extent to which differences in
skewness across grades influence value-added estimation in Table 6 and find very mild effects.

                                                        16
the correlation between teachers’ ceiling-affected value-added estimates and estimates from the

baseline model without ceiling effects, (2) the estimation-error share of the variance of the

teacher effects and (3) the adjusted and unadjusted effect sizes, by which we mean the predicted

change in student achievement, as a proportion of one standard deviation of test scores, resulting

from a one-standard-deviation increase in teacher quality. The correlations between the ceiling-

affected and baseline estimates provide a quick gauge of the distortionary impacts of the ceilings.

Teacher effect sizes are commonly used in the literature to evaluate the importance of differences

in teacher quality. The unadjusted effect size is just the square root of the raw variance in

teacher effects, while the adjusted measure accounts for estimation error in the individual

teacher-effect estimates. These estimates are reported as ratios of the standard deviation of the

teacher-effect distribution to the standard deviation of the censored test-score distribution for

each ceiling simulation. This metric has a straightforward interpretation. For example, the

southwest-most entry in Table 3, if taken at face value, suggests that a one-standard-deviation

improvement in teacher quality corresponds to a 0.24-standard-deviation improvement in test

scores.    The estimation-error shares of the teacher-effect variances and the corresponding

adjusted variance measures are estimated following Koedel (2008), who separates the variance

of the estimated teacher effects into signal and noise components.19

          The three tables show that teachers’ value-added estimates are roughly impervious to

test-score-ceiling effects over a wide-range of ceiling severity in each model. This can be seen

by looking at the correlations between the teacher effects estimated using the actual test-score

data and those estimated after the ceilings are imposed. Notice that even the ceiling that affects


19
  For the within-students model we also report the skewness in the second-lagged test-score distribution. In the
between and within-schools models we cluster standard errors at the student level. Because only grade-repeaters
have more than one record, the clustered standard errors are essentially typical robust standard errors. Our within-
students model is estimated using robust standard errors.

                                                         17
students’ test scores starting at the 75th percentile is largely inconsequential (skewness ≈ -0.64),

as evidenced by the fairly high correlation between teachers’ baseline value-added estimates and

their value-added estimates from this ceiling simulation. So, for example, policymakers should

feel comfortable using FCAT scores from the third and fourth grades, where the skewness in the

test-score distributions are around -0.5, to measure teacher value-added at least insofar as ceiling

effects are a concern. However, value-added results begin to respond to ceiling effects as the

ceilings continue to increase in severity. For instance, when the ceiling begins at the 50th

percentile of the fourth-grade test-score distribution, the correlation between the teacher-effect

estimates from the actual data and the data with the ceiling imposed ranges from about 0.85 for

the basic and within-schools models to 0.80 for the within-students model. The correlations drop

further when we impose the ceiling at the 33rd percentile, with the lowest correlation being 0.72

in the within-students model.      As ceiling conditions approach those found in minimum-

competency testing environments, value-added results are non-negligibly altered.

       Two other observations from Tables 3, 4 and 5 are noteworthy. First, the estimation-

error share of the variance of teacher effects increases as ceiling severity increases, which surely

explains part of the pattern in correlations discussed above. Second, there is a negligible change

in the adjusted variance of teacher quality regardless of ceiling severity, which may initially

seem counterintuitive. However, note that the test-score ceilings are reducing the raw variance

of test scores overall, and that the teacher-effect variance measures are scaled by this underlying

variance. That is, although the standard deviation of the teacher-effect distribution is reduced

when a ceiling is imposed, the standard deviation of the distribution of test scores is also

reduced. In fact, our analysis likely understates test-score-ceiling effects on the measurable




                                                18
variance of teacher quality because it removes variability in test-scores more precisely than

would be observed in a real-world ceiling.20

         Thus far we have focused on the effect of the magnitude of skewness on value-added

estimation. But differences in skewness across grades may also affect value-added results. We

evaluate this possibility in Table 6 by simulating variations of our most severe test-score ceiling.

In the first column of Table 6 for each model, we impose a ceiling at the 33rd percentile of the

test-score distribution for each grade level separately, rather than basing the ceiling on the

fourth-grade distribution of scores as in the previous analysis. This generates more closely

aligned skewness in the test-score distributions across grades. In columns (2) and (3), we

simulate less severe ceilings in the lagged-score distributions – these ceilings approach the

severity of the ceiling in the lagged-score distribution from our initial simulation at the 33rd

percentile of fourth-grade scores. Table 6 shows that regardless of changes in the difference-in-

skewness across grades, the effect of the test-score ceiling on value-added results is roughly

constant.21 This can be most easily seen by comparing the correlations between the ceiling-

affected value-added estimates and those from the no-ceiling baseline in row (7) of Table 6.

         Finally, note that the test-score ceilings induce more skewness in the test-score

distributions from the within-students sample (Table 5) relative to the larger student sample used

in the basic and within-schools models (Tables 3 and 4). As mentioned in Section III, this is

because the restricted student sample used for the within-students model is disproportionately

affected by the test-score ceiling (that is, the set of students who have three contiguous test

20
   Our simulations allow students to demonstrate that they are far above the cutoff, and then we restrict their scores
ex post. This removes additional variability in test scores that would be found near the highest possible score in a
real-world test-score ceiling. For example, we might observe a student scoring at the 80th percentile of the actual
distribution of test scores and restrict her score to the 50th percentile such that she obtains the maximum possible
score in our simulation. However, with a real-world ceiling where she would have to answer every question
correctly to score at the maximum, she might bubble in a wrong answer by accident, read a question incorrectly, etc.
This would add to the underlying variability in test scores, but of course would not be explained by teacher effects.
21
   That is, over the fairly wide range of differences in skewness that we consider.

                                                         19
scores is higher achieving, on average, than the set of students who have just two test scores).

Interestingly, the influence of each test-score ceiling on value-added is similar across the three

models despite the fact that each ceiling is more strongly felt by students in the within-students

model. It appears that the stronger skewness in the test-score distributions for the restricted

student sample is roughly offset by the benefit of looking within students, where ceiling effects

will be partially controlled for by the first-differencing procedure.

   VI. Implications of Minimum-Competency Testing for Value-Added Analysis

       We further evaluate the sensitivity of teacher value-added to the imposition of our most

severe test-score ceiling, designed to replicate minimum-competency testing conditions, using

transition matrices to compare teacher rankings before and after the test-score-ceiling

transformation. The transition matrices provide an alternative documentation of the correlations

reported in the final columns of Tables 3, 4 and 5.

       To construct the transition matrices, we estimate each model before and after the ceiling

is imposed. In each case, we keep the vector of estimated teacher effects and rank them from 1

to J, 1 being the lowest and J being the highest. We divide teachers into quintiles based on their

value-added rankings, where quintile-5 teachers are those with the highest value-added. The

transition matrices compare the stability of these quintile assignments before and after the ceiling

is imposed. This type of analysis is particularly relevant in the context of teacher accountability.

For example, an accountability system might reward the top 20 percent of teachers and sanction

the bottom 20 percent as measured by value-added. Our results are reported in Table 7 for each

of the value-added specifications described in Section III.

       The vertical dimension of the transition matrices represents teachers’ quintile rankings

without the ceiling and the horizontal dimension teachers’ rankings after the ceiling is imposed.


                                                 20
Each cell in Table 7 indicates the percentage of teachers who fall into a given quintile set, where

a quintile set is defined by the pair of quintile-rankings for a given teacher with and without the

ceiling (e.g., the set (1,4) would indicate a quintile ranking of “1” in the no-ceiling case and a

quintile ranking of “4” after the ceiling is imposed). If ceiling effects did not influence value-

added rankings, the diagonal entries in Table 7 would all equal 100 percent and the off-diagonal

entries would all equal zero.

        The transition matrices show that ceiling effects alone can significantly influence value-

added rankings. For example, across the three models, just 49 to 56 percent of the teachers who

are identified as being in the top 20 percent of the value-added distribution based on students’

actual test scores are also identified as being in this group once test scores are transformed.

Furthermore, 14 to 17 percent of these teachers are pushed below the 60th percentile of the

distribution of teacher effects.

        In an omitted analysis available upon request, we also consider whether certain types of

teachers are helped or harmed in terms of their value-added rankings by minimum-competency

testing. The mechanism through which we might expect an effect is student-teacher sorting

within and across schools.         For example, if teachers with master’s degrees teach a

disproportionate share of high-achieving students, their value-added rankings will be more

adversely affected by test-score-ceiling effects. Not surprisingly, we find that more qualified

teachers, teachers with higher salaries, and teachers who teach at more advantaged schools are

harmed by test-score ceiling effects in value-added rankings (the latter result related to the

socioeconomic advantage of students across schools, of course, is only applicable in the basic

value-added model).




                                                21
      VII.     Robustness of the Negative Skewness Measure

          In this section we evaluate the robustness of the negative skewness measure by

evaluating whether differentially constructed test-score ceilings that produce similar negative

skewness have similar implications for value-added results. In particular, we construct a set of

soft test-score ceilings that are designed to replicate the negative skewness in some of the hard-

ceiling simulations and look to see if the soft-ceiling design has different implications for value-

added results. We stress that our analysis here is far from exhaustive - for any given level of

negative skewness in a test-score distribution, there are literally an infinite number of soft test-

score ceiling structures that could generate the skewness. We focus on just one possibility here,

creating soft test-score ceilings using a spline such that for a student with uncensored test score

Yi:


Y%i = Yi , Yi ≤ S1
Y%i = S1 + X 1 (Yi − S1 ), S1 < Yi ≤ S 2
Y%i = S1 + X 1 ( S 2 − S1 ) + X 2 (Yi − S 2 ), S 2 < Yi ≤ S3                         (4)
M
Y%i = S1 + X 1 ( S 2 − S1 ) + X 2 (Yi − S 2 ) + ... + X n (Yi − S n ), Yi > S n −1



In (4), Y%i is the transformed score for student i, S n > Sn −1 > K > S1 where the Sj, j=1,…,n

represent the test-score levels at which the n knots appear, and 1 ≥ X 1 ≥ X 2 ≥ K X n −1 ≥ X n ,

meaning that the test-score ceiling is non-decreasing in severity as students’ test scores rise.

Specifically we define Sn as the score at the nth decile of the fourth-grade test-score distribution

for these simulations. For student i whose score falls between S2 and S3, her transformed score

can be written (where Yi is her observed test score):




                                                                 22
Y%i = S1 + X 1 ( S2 − S1 ) + X 2 (Yi − S2 )                                                (5)

        We generate three soft test-score ceilings using this basic structure. These ceilings are

designed to produce skewnesss in the distributions of test scores comparable to those from our

hard-ceiling simulations imposed at the 75th, 50th and 33rd percentiles. Table 8 displays the Xn

vectors for each of these three ceilings.

        Table 9 displays the effects of the three soft ceilings on value-added estimates from each

of the three models discussed in Section III. The results are comparable to those in columns (6),

(7) and (8) in Tables 3, 4 and 5. Although the effects of the soft ceilings are slightly more mild

than those from their hard-ceiling counterparts, the results suggest that similarly skewed test-

score distributions have similar implications for value-added estimation.

    VIII. The Model Misspecification Problem

        Finally, we explicitly consider the model-misspecification problem, which has partly

driven our results thus far. A least-squares approach (and variants thereof), which is typically

used in the value-added literature, will be misspecified when there is a test-score ceiling because

the ceiling acts as a data censor. When ceiling effects are severe, the misspecification problem

will be amplified.

        In theory one could estimate a censored-data model, such as a tobit model, to correct this

misspecification. However, as a practical matter, there are three complications that arise with

respect to resolving the model-misspecification problem in the value-added context where a test-

score ceiling is detected. First, the censor points in a real-world test-score ceiling will be

unknown and, in fact, discontinuous censor points may not even exist. Evidence from Carson

and Sun (2007) suggests that mis-identifying the censor points will produce substantially biased




                                                23
estimates of the model parameters, meaning that “guessing” at the censor points based on some

observed distribution of scores is unlikely to resolve the problem.22

         A second complication of data censoring in the value-added context is that both current

and lagged scores are likely to be censored. In the general value-added model, this means that an

independent variable will be censored in addition to the dependent variable. The gainscore

framework does not solve this problem because the censoring in a gainscore model is ill-defined

(censored gains will be zero or near zero, but non-censored gains can also be zero, near zero, or

even negative). Although dependent-variable data censoring has received considerable attention

in research, there is a much smaller literature that considers independent-variable data censoring.

Austin and Brunner (2003) and Austin and Hoch (2004) provide MLE solutions to the

independent-variable censoring problem with a known censor point, but their solutions are

sensitive to an assumption about the joint distribution of the independent variables. Where

possible, even these authors strongly recommend circumventing the censoring problem

altogether by obtaining uncensored data, or if the sample size permits, restricting the analysis

only to uncensored observations.23

         A third complication in the context of teacher value-added is that as the data censoring

gets more severe, more and more teachers teach fewer and fewer students whose scores are not

censored. At extreme ceiling severities, some teachers do not teach any students whose scores


22
   There has been some work in the econometrics literature that looks at data censoring when the censor points are
unknown, but this literature is inapplicable to the case of a test-score ceiling because a key assumption required to
overcome the unknown censoring process is that the censoring is independent of the underlying value of the
censored variable (see Chen, 2002; GØrgens and Horowitz, 1999). This assumption obviously does not apply when
the dependent variable, the test score, is subject to a ceiling effect.
23
   Whereas thus far we have treated ceiling effects as a ‘problem’ for value-added estimation, an alternative view is
that ceiling effects simply signify a shift in the objective function of administrators toward helping students whose
scores are not impacted by the ceiling. In such cases, modeling student achievement only for students whose scores
are below the ceiling, if such a ceiling can be reasonably identified, will be a viable option. However, if school
administrators do not want to shift disproportionate weight to low-achieving students in teachers’ value-added
estimates, the ceiling ‘problem’ resurfaces.

                                                         24
are not censored. Clearly, as a larger fraction of the student population’s scores are censored,

inference for more and more teacher effects becomes unreliable. Thus, where ceiling effects are

mild and the misspecification issue has little bearing on the results, a model that appropriately

treats the censored data could in principle be informative for most if not all of the teacher effects.

However, as ceiling effects become increasingly severe, and therefore the data-censoring

correction would be most useful, the estimates for more and more teachers become

uninformative.

         Overall, these three issues suggest that a statistical solution to the misspecification

problem, although theoretically possible, is unlikely to be successful. If a severe test-score

ceiling is detected, the most reasonable solution is to find a different testing instrument. The

results from this analysis can be useful for determining whether a test-score ceiling is sufficiently

severe such that an alternative test should be considered.

         Despite these practical difficulties, as a thought experiment it may be of interest to

identify the separate impacts of lost information and model misspecification on value-added

results. In Table 10, we briefly evaluate this question at the level of school effects (for our

baseline sample of fourth-grade students) using a basic tobit model.24 We focus on school

effects to circumvent the problem that at the teacher level, some teachers teach only students

with censored scores in the most severe ceiling simulations (where this analysis is most

interesting). In all schools, there are at least some students below the cutoff in all of our

simulations.      In addition, we avoid the added complication of independent-variable data


24
  Unlike OLS, heteroskedasticity in the case of tobit implies inconsistency in the coefficient estimates, and there is
substantial heteroskedasticity here. There is some argument in the literature as to how important this is as a practical
matter (see, for example, Arabmazar and Schmidt, 1981; Brown and Moffit, 1983; Hurd, 1979), but in our case a
tobit that directly models the heteroskedasticity in the data performs worse than a simple tobit. We can only
speculate as to the cause in our context - one possibility is that in the heteroskedastic tobit, the large number of
(sometimes imprecisely) estimated heteroskedasticity parameters upon which the parameter-estimates of interest are
based may be problematic.

                                                          25
censoring by only censoring current scores (in practice this has a negligible effect on results).

Although our partial-censoring approach to estimating school effects is not directly comparable

to the preceding analysis, it provides a straightforward setting in which to evaluate separately the

information-loss and model-misspecification components of test-score-ceiling effects.           For

brevity, Table 10 reports only the correlations of school effects across models. In our school-

effect models we control for the student-level covariates documented in Table 1 (that is, we

replace the vector of teacher indicator variables with a vector of school indicator variables in the

basic value-added model).

       Table 10 shows that the tobit specification improves model performance, and

substantially so. For example, even in the minimum-competency-equivalent simulation where a

significant amount of test-score information is lost, modeling the censored data dramatically

improves performance. Although the correlation between the baseline school effects and the

ceiling-influenced school effects is still far from one in the most severe ceiling simulation, it is

much improved (going from 0.78 to 0.90).              This exercise suggests that the model-

misspecification problem is an important contributor to the ceiling-effect distortions documented

in our primary analysis.

   IX. Concluding Remarks

       In the current climate of proficiency-based educational reform, test-score ceilings are

likely to be increasingly common. We evaluate the extent to which ceiling effects influence the

estimation of teacher value-added. There are two mechanisms by which ceiling effects distort

value-added results. First, most straightforwardly, a test-score ceiling represents lost information

about student learning. Second, a ceiling generally results in model misspecification. Although




                                                26
in theory this latter issue can be resolved by properly modeling the censored data, in practice a

statistical solution to the data censoring problem is unlikely to be feasible.

       Our analysis properly treats the test-score-ceiling problem as a combination of these two

distortionary influences. Overall, our findings are generally encouraging – given a wide range

of test-score ceiling conditions, some of which might be casually identified as severe, value-

added estimates are only negligibly affected. However, researchers and policymakers should be

concerned when working in minimum-competency or proficiency-based testing environments.

We show that ceiling conditions in such environments can significantly alter value-added

assessments for individual teachers.




                                                 27
                                                                             Figures

Figure 1. Frequency Distributions of Lagged and Current Math Test Scores from our Data Contrasted with Normal-
Distribution Overlays
          .01




                                                                                              .01
          .008




                                                                                              .008
            .006




                                                                                                .006
                                                                                          Density
      Density
   .004




                                                                                       .004
          .002




                                                                                              .002
                                                                                              0
          0




                   400   500              600              700              800                        400   500            600              700              800

LEFT: Kernel-density plot of lagged test score distribution – skewness ≈ 0.25
RIGHT: Kernel-density plot of current test score distribution – skewness ≈ 0.17
In each graph the solid line represents the distribution of actual scores and the dotted line the normal-distribution overlay. Estimates are calculated using the
Epanechnikov kernel with a bandwidth equal to 2.5 percent of the range of test scores.




                                                                                  28
Figure 2. Frequency Distributions of Third and Fourth-Grade Math Scores from the TAAS in 2002 Contrasted With Normal-
Distribution Overlays




                                                                                           .08
                                                                                           .06
      .06




                                                                                        Density
   Density




                                                                                         .04
    .04




                                                                                           .02
      .02




                                                                                           0
      0




             0         20            40           60            80           100                  0     20            40           60           80           100

LEFT: Kernel-density plot of third-grade test score distribution – skewness ≈ -1.60
RIGHT: Kernel-density plot of fourth-grade test score distribution – skewness ≈ -2.08
In each graph the solid line represents the distribution of actual scores and the dotted line the normal-distribution overlay. Estimates are calculated using the
Epanechnikov kernel with a bandwidth equal to 2.5 percent of the range of test scores.




                                                                                   29
Figure 3. Frequency Distributions of Third, Fourth, Ninth and Tenth-Grade Math Scores
from the FCAT in 2007 Contrasted With Normal-Distribution Overlays
     .008




                                                                 .008
     .006




                                                                 .006
  Density




                                                            Density
   .004




                                                             .004
     .002




                                                                 .002
     0




                                                                 0
            100   200        300         400        500                 100   200      300        400        500




                                                               .01
     .008
    .006




                                                             Density
  Density




                                                           .005
     .004
     .002




                                                               0
     0




            100   200        300        400         500                 100   200     300        400        500


UPPER LEFT: Kernel-density plot of third-grade test score distribution – skewness ≈ -0.46
UPPER RIGHT: Kernel-density plot of fourth-grade test score distribution – skewness ≈ -0.55
LOWER LEFT: Kernel-density plot of ninth-grade test score distribution – skewness ≈ -0.94
LOWER RIGHT: Kernel-density plot of tenth-grade test score distribution – skewness ≈ -1.99
In each graph the solid line represents the distribution of actual scores and the dotted line the normal-distribution
overlay. Estimates are calculated using the Epanechnikov kernel with a bandwidth equal to 2.5 percent of the range
of test scores.




                                                          30
                                                    Tables


Table 1. Controls from Value-Added Models

Student-Level Controls (Xit)                             School (and Classroom)-Level Controls (Sit)

English-Learner (EL) Status                              School Fixed Effects
Change from EL to English-Proficient                     Classroom-level Peer Performance in Year (t-1)
Expected and Unexpected School Changer                   Class Size
Parental Education
Race                                                     Percentage of Student Body:
Gender                                                   by Race
Designated as Advanced Student                           by English Learner Status
Percentage of School Year Absent*                        by Free/Reduced-Price Lunch Status
                                                         by School-Changer Status

*The share of days missed by students is sometimes considered endogenous. Fourth-grade students, however, are
not likely to have much influence over their attendance decisions.




Table 2. Average Within-Teacher Standard Deviations of Students’ Period (t-1) Test
Scores
                                      Within Schools                 Across District
                       Actual      Random         Perfect        Random            Perfect
                                 Assignment       Sorting      Assignment          Sorting

Standard Deviations of
Lagged Scores                    0.81             0.90               0.32               0.99               <0.01
Note: The numbers above report the average standard deviation of test scores within the classroom, for various
scenarios, each divided by the overall standard deviation of test scores district-wide. In the “Perfect Sorting”
columns students are sorted by period (t-1) test-score levels in math, first within school and in the final column
across the district. For the randomized assignments, students are assigned to teachers based on randomly generated
numbers from a uniform distribution. In the second column of numbers, students are not re-assigned across schools;
in the fourth column, students are re-assigned across schools. The random assignments are repeated 25 times and
estimates are averaged across all random assignments and all teachers. The estimates from the simulated random
assignments are very stable across simulations.




                                                         31
     Table 3. Test-Score-Ceiling Effects on Value-Added Results: Basic Specification

                                                  (1)*         (2)          (3)           (4)          (5)          (6)           (7)          (8)

Percentile of Fourth-Grade Test-Score            99.96         97            95           90           85            75           50            33
Distribution Where Ceiling is Set

Skewness of Period-t Score                        0.17        -0.02        -0.07        -0.25         -0.37        -0.64         -1.31        -2.00
Distribution

Skewness of Period-(t-1) Score                    0.25        0.11          0.07        -0.05         -0.13        -0.32         -0.83        -1.32
Distribution

Correlation Between Ceiling-Restricted              -         0.99         0.99          0.98         0.97          0.94         0.85          0.77
Value-Added Estimates and Baseline

Estimation-error share of variance of             0.11        0.11         0.11          0.11         0.12          0.13         0.17          0.24
teacher fixed effects

Unadjusted Effect Size of Teacher                 0.26        0.25         0.25          0.25         0.25          0.25         0.26          0.26
Quality

Adjusted Effect Size of Teacher                   0.24        0.24         0.24          0.24         0.24          0.24         0.23          0.23
Quality
     * Column (1) shows results from the no-ceiling baseline. Of course, a ceiling is not “set” here – 0.04 percent of the student population attains the maximum
     possible score. The last two rows show the unadjusted and adjusted estimates of the number of standard deviations by which student achievement is predicted to
     change after a one standard deviation increase in teacher quality.




                                                                                   32
     Table 4. Test-Score-Ceiling Effects on Value-Added Results: Within-Schools Specification

                                                  (1)*         (2)          (3)           (4)          (5)          (6)           (7)          (8)

Percentile of Fourth-Grade Test-Score            99.96         97            95           90           85            75           50            33
Distribution Where Ceiling is Set

Skewness of Period-t Score                        0.17        -0.02        -0.07        -0.25         -0.37        -0.64         -1.31        -2.00
Distribution

Skewness of Period-(t-1) Score                    0.25        0.11          0.07        -0.05         -0.13        -0.32         -0.83        -1.32
Distribution

Correlation Between Ceiling-Restricted              -         0.99         0.99          0.97         0.96          0.93         0.84          0.73
Value-Added Estimates and Baseline

Estimation-error share of variance of             0.24        0.24         0.24          0.25         0.26          0.28         0.35          0.44
teacher fixed effects

Unadjusted Effect Size of Teacher                 0.28        0.27         0.27          0.27         0.27          0.27         0.29          0.30
Quality

Adjusted Effect Size of Teacher                   0.24        0.24         0.24          0.24         0.23          0.23         0.23          0.22
Quality
     * Column (1) shows results from the no-ceiling baseline. Of course, a ceiling is not “set” here – 0.04 percent of the student population attains the maximum
     possible score. The last two rows show the unadjusted and adjusted estimates of the number of standard deviations by which student achievement is predicted to
     change after a one standard deviation increase in teacher quality.




                                                                                   33
     Table 5. Test-Score-Ceiling Effects on Value-Added Results: Within-Students Specification

                                                  (1)*         (2)          (3)           (4)          (5)           (6)          (7)          (8)

Percentile of Fourth-Grade Test-Score            99.96         97            95           90           85            75           50            33
Distribution Where Ceiling is Set

Skewness of Period-t Score                        0.17        -0.10        -0.16        -0.36         -0.49        -0.79         -1.58        -2.39
Distribution

Skewness of Period-(t-1) Score                    0.25        0.07          0.02        -0.13         -0.22        -0.43         -1.03        -1.62
Distribution

Skewness of Period-(t-2) Score                    0.15        0.12         0.11          0.07         0.04         -0.04         -0.32        -0.63
Distribution

Correlation Between Ceiling-Restricted              -         0.99         0.99          0.97         0.96          0.92         0.80          0.72
Value-Added Estimates and Baseline

Estimation-error share of variance of             0.33        0.33         0.33          0.34         0.34          0.37         0.45          0.51
teacher fixed effects

Unadjusted Effect Size of Teacher                 0.29        0.29         0.29          0.29         0.29          0.30         0.32          0.35
Quality

Adjusted Effect Size of Teacher                   0.23        0.24         0.24          0.24         0.24          0.24         0.24          0.25
Quality
     * Column (1) shows results from the no-ceiling baseline. Of course, a ceiling is not “set” here – 0.04 percent of the student population attains the maximum
     possible score. The last two rows show the unadjusted and adjusted estimates of the number of standard deviations by which student achievement is predicted to
     change after a one standard deviation increase in teacher quality.




                                                                                   34
          Table 6. Effects of Differences in Skewness across Grades on Value-Added Estimates from Each Model, Based on our Most
          Severe Ceiling Simulation
                                                           Basic Model                                  Within-Schools Model                          Within-Students Model**
                                                (1)       (2)      (3)        (4)*               (1)        (2)       (3)    (4)*              (1)          (2)       (3)     (4)*

Share of Fourth-Grade Students at Highest       67        67         67        67                67         67       67        67               67          67       67        67
Score (%)

Share of Lagged Scores at Highest Score         67        60         50        47                67         60       50        47               67          60       50        47
(%)

Share of Second-Lagged Scores at Highest        N/A       N/A       N/A        N/A              N/A        N/A      N/A       N/A               67          60       50        25
Score (%)

Skewness of Period-t Score Distribution        -2.00     -2.00     -2.00      -2.00             -2.00      -2.00    -2.00     -2.00           -2.39       -2.39     -2.39     -2.39

Skewness of Period-(t-1) Score                 -2.21     -1.84     -1.45      -1.32             -2.21      -1.84    -1.45     -1.32           -2.77       -2.27     -1.78     -1.62
Distribution

Skewness of Period-(t-2) Score                  N/A       N/A       N/A        N/A              N/A        N/A      N/A       N/A             -2.17       -1.79     -1.48     -0.63
Distribution

Correlation Between Ceiling-Restricted         0.79      0.79       0.78      0.77              0.73       0.73     0.73      0.73             0.72        0.69     0.70      0.72
Value-Added Estimates and No-Ceiling
Baseline

Estimation-error share of variance of          0.19      0.21       0.23      0.24              0.45       0.44     0.44      0.44             0.54        0.51     0.48      0.51
teacher fixed effects

Unadjusted Effect Size of Teacher Quality      0.26      0.26       0.26      0.26              0.30       0.30     0.30      0.30             0.36        0.36     0.36      0.35

Adjusted Effect Size of Teacher Quality         0.24     0.23       0.23      0.23             0.22       0.22        0.22      0.22             0.24      0.25       0.26     0.25
         * The fourth column for the basic, within-schools, and within-students models shows the results from the eighth column of Tables 3, 4 and 5, respectively, for
         comparison. That is, 47 percent of third grade students attain the score at the 33rd percentile of the fourth-grade test-score distribution.
         ** In the within-students panel, the ceilings in the fourth-grade and lagged-score distributions are designed based on the full student sample to make the ceilings
         as directly comparable as possible across models (i.e., to replicate as closely as possible the effect of a true ceiling). Note that a higher share of the restricted
         within-students sample will score at the maximum in each grade level (see Section III for discussion). Only the ceilings in the second-lagged-score distributions
         are based on the restricted within-students sample. This causes slightly less skewness in the period-(t-2) distribution of scores in columns (1), (2) and (3) of the
         within-students panel relative to the lagged-score distribution.
         *** The last two rows show the unadjusted and adjusted estimates of the number of standard deviations by which student achievement is predicted to change
         after a one standard deviation increase in teacher quality.
                                                                                           35
Table 7. Transition Matrices Documenting the Stability of Teachers’ Value-Added Rankings,
by Quintile, Before and After the Minimum-Competency-Equivalent Ceiling is Imposed.

Basic Model

                                                   Ceiling-Affected Quintile Assignments

                                        1                   2         3            4       5 (best)
 Baseline
  Quintile            1                76                  17          6            1         0
Assignments           2                22                  43         25           10        0
                      3                 2                  33         34           23        8
                      4                 0                   7         20           36        38
                   5 (best)             0                   2         14           30        54
Notes: Cells report percentage of teachers in each quintile set.



Within-Schools Model

                                                   Ceiling-Affected Quintile Assignments

                                        1                   2         3            4       5 (best)
 Baseline
  Quintile            1                71                  17          8            3         0
Assignments           2                24                  39         24           11        3
                      3                 4                  29         33           21        13
                      4                 1                  13         19           39        28
                   5 (best)             0                   2         16           26        56
Notes: Cells report percentage of teachers in each quintile set.


Within-Students Model

                                                   Ceiling-Affected Quintile Assignments

                                        1                   2         3            4       5 (best)
 Baseline
  Quintile            1                58                  24         13            4         1
Assignments           2                35                  35         23           6         1
                      3                 5                  21         31           28        15
                      4                 3                  12         17           37        32
                   5 (best)             0                   9         17           25        49
Notes: Cells report percentage of teachers in each quintile set.




                                                            36
       Table 8. Soft-Ceiling Simulations Designed to Mimic Hard Ceilings at the 75th, 50th and 33rd Percentiles of the Distribution of
       Fourth-Grade Test Scores.

                          Soft-Ceiling 1                       Soft-Ceiling 2                       Soft-Ceiling 3

Description:     Mimics the hard ceiling set at       Mimics the hard ceiling set at       Mimics the hard ceiling set at
                 the 75th percentile of the fourth-   the 50th percentile of the fourth-   the 33rd percentile of the fourth-
                 grade test-score distribution        grade test-score distribution        grade test-score distribution

X1 :                            1                                      1                                   1
X2 :                            1                                     1                                  0.60
X3 :                            1                                    0.90                                0.40
X4 :                            1                                    0.70                                0.20
X5 :                           0.90                                  0.30                                0.10
X6 :                           0.70                                  0.10                                0.10
X7 :                           0.50                                  0.10                                  0
X8 :                           0.30                                    0                                   0
X9 :                           0.10                                    0                                   0




                                                                                37
        Table 9. Soft Ceiling Results.
                                                  Basic Model                             Within-Schools Model                     Within-Students Model

Soft Ceiling Number                     (1)          (2)           (3)             (1)            (2)             (3)       (1)            (2)             (3)
(from Table B.1)

Comparable to hard ceiling              75th         50th          33rd         75th              50th         33rd         75th           50th         33rd
imposed at                           Percentile   Percentile    Percentile   Percentile        Percentile   Percentile   Percentile     Percentile   Percentile

Share of Fourth-Grade Students at      0.04         22.40         30.10           0.04           22.40           30.10     0.04           22.40        30.10
Highest Score (%)

Skewness of Period-t Score             -0.62        -1.30         -1.96           -0.62          -1.30           -1.96     -0.77          -1.55        -2.30
Distribution

Skewness of Period-(t-1) Score         -0.34        -0.84         -1.35           -0.34          -0.84           -1.35     -0.45          -1.04        -1.64
Distribution

Skewness of Period-(t-2) Score         N/A           N/A          N/A             N/A            N/A             N/A       -0.07          -0.35        -0.26
Distribution

Correlation Between Ceiling-           0.96          0.88         0.82            0.95           0.86            0.77      0.94            0.84            0.74
Restricted Value-Added Estimates
and Baseline

Estimation-error share of variance     0.12          0.16         0.20            0.27           0.34            0.40      0.36            0.43            0.46
of teacher fixed effects

Unadjusted Effect Size of Teacher      0.25          0.26         0.26            0.28           0.28            0.30      0.30           0.32             0.35
Quality

Adjusted Effect Size of Teacher        0.24          0.23         0.23            0.24           0.24            0.23      0.24           0.24             0.26
Quality




                                                                             38
     Table 10. Test-Score-Ceiling Effects on Value-Added Results for School-Level Effects (N=116) – Tobit Versus OLS

                                                  (1)*         (2)      (3)          (4)     (5)     (6)     (7)     (8)

Percentile of Fourth-Grade Test-Score            99.96          97      95           90      85      75      50      33
Distribution Where Ceiling is Set

Skewness of Period-t Score                        0.17        -0.02    -0.07        -0.25   -0.37   -0.64   -1.31   -2.00
Distribution

Skewness of Period-(t-1) Score                    0.25         0.25    0.25         0.25    0.25    0.25    0.25    0.25
Distribution (not censored)

Correlation Between Ceiling-Restricted               -         1.00    1.00         0.99    0.98    0.95    0.86    0.78
Value-Added Estimates estimated by
OLS and Baseline

Correlation Between Ceiling-Restricted               -         1.00    1.00         1.00    0.99    0.99    0.95    0.90
Value-Added Estimates estimated by
Tobit and Baseline

     * Again, column (1) shows results from the no-ceiling baseline.




                                                                               39
                                      References

Aaronson, Daniel, Lisa Barrow and William Sander 2007. Teachers and Student
Achievement in the Chicago Public High Schools. Journal of Labor Economics 25:95-135.

Arabmazar, Abbas and Peter Schmidt 1981. Further Evidence on the Robustness of the Tobit
Estimator to Heteroskedasticity. Journal of Econometrics 17:253-258.

Austin, Peter C. and Lawrence J. Brunner 2003. Type I Error Inflation in the Presence of a
Ceiling Effect. The American Statistician 57:97-104

Austin, Peter C. and Jeffrey S. Hoch 2004. Estimating Linear Regression Models in the
Presence of a Censored Independent Variable. Statistics in Medicine 23:411-429.

Anderson T.W. and Cheng Hsiao 1981. Estimation of Dynamic Models with Error
Components. Journal of the American Statistical Association. 76:598-609.

Betts, Julian, Andrew Zau, and Lorien Rice 2003. Determinants of Student Achievement,
New Evidence from San Diego. Public Policy Institute of California.

Brown, Charles and Robert Moffitt 1983. The Effect of Ignoring Heteroscedasticity on
Estimates of the Tobit Model. Technical Working Paper no. 27. National Bureau of
Economic Research, Cambridge, MA.

Carson, Richard T. and Yixiao Sun 2007. The Tobit Model with a Non-Zero Threshold.
Econometrics Journal 10:488-502

Chen, Songhian 2002. Rank Estimation of Transformation Models. Econometrica 70:1683-
1697.

Cullen, Julie Berry and Susanna Loeb 2004. School finance feform in Michigan: Evaluating
Proposal A, in J. Yinger (Ed.), Helping Children Left Behind: State Aid and the Pursuit of
Educational Equity. Cambridge, MA: MIT Press. pp. 215-250.

GØrgens, Tue and Joel L. Horowitz 1999. Semiparametric Estimation of a Censored
Regression Model with an Unknown Transformation of the Dependent Variable. Journal of
Econometrics 90:155-191.

Hanushek, Eric, John Kain, Daniel O’Brien and Steven Rivkin 2005. The Market for Teacher
Quality. Working Paper no. 11154, National Bureau of Economic Research, Cambridge,
MA.

Harris, Douglas and Tim R. Sass 2006. Value-Added Models and the Measurement of
Teacher Quality. Unpublished manuscript, Department of Economics, Florida State
University, Tallahassee.

                                           40
Hurd, Michael 1979. Estimation in Truncated Samples When There is Heteroskedasticity.
Journal of Econometrics 11:247-258.

Ingersoll, Gary M., James P. Scamman and Wayne D. Eckerling. 1989. Geographic Mobility
and Student Achievement in an Urban Setting. Educational Evaluation and Policy Analysis
11:143-149.

Kane, Thomas and Douglas Staiger. 2002. The Promise and Pitfalls of Using Imprecise
School Accountability Measures. Journal of Economic Perspectives 16:91-114.

Koedel, Cory. forthcoming. An Empirical Analysis of Teacher Spillover Effects in
Secondary School. Economics of Education Review.

Koedel, Cory and Julian R. Betts. 2007. Re-Examining the Role of Teacher Quality in the
Educational Production Function. Working Paper, University of Missouri, Columbia.

   -- 2008. How Reliable is Value-Added Modeling? An Extended Analysis of the
   Rothstein Critique. Working Paper, University of Missouri, Columbia.

Lockwood, J.R., Daniel F. McCaffrey, Laura S. Hamilton, Brian Stecher, Vi-Nhuan Le and
Jose Felipe Martinez. 2007. The Sensitivity of Value-Added Teacher Effect Estimates to
Different Mathematics Achievement Measures. Journal of Educational Measurement,
44:47-67.

Nye, Barbara, Spyros Konstantopoulos and Larry V. Hedges. 2004. How Large are Teacher
Effects? Educational Evaluation and Policy Analysis, 26:237-257.

Podgursky, Michael J. and Mathew G. Springer. 2007. Teacher Performance Pay: A Survey.
Journal of Policy Analysis and Management, 26:909-950.

Roberts, Sarah Jane. 1978. Test Floor and Ceiling Effects. ESEA Title I Evaluation and
Reporting System. Research report, RMC Research Corporation.

Rockoff, Jonah. 2004. The Impact of Individual Teachers on Student Achievement: Evidence
from Panel Data. American Economic Review, Papers and Proceedings.

Rothstein, Jesse. 2008. Teacher Quality in Educational Production: Tracking, Decay, and
Student Achievement. Unpublished Manuscript, Princeton University.

Rumberger, Russell W. and Katherine A. Larson. 1998. Student Mobility and the Increased
Risk of High School Dropout. American Journal of Education 107:1-35.

Warren, John Robert, State High School Exit Examinations for Graduating Classes Since
1977. Minneapolis, MN: Minnesota Population Center, 2007. Available at
http://www.hsee.umn.edu.

                                          41
